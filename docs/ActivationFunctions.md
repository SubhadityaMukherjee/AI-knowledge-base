# Activation Functions
- [[Initialization]]

- [[Sigmoid]]

- [[Relu]]
- [[Elu]]
- [[Maxout]] 

- [[Tanh]] 

- [[Softmax]]

- [[Softplus]]

- [[Swish]]

## Refs
<<<<<<< HEAD
- [mlmastery](https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/)



## Backlinks
* [[Elu]]
	* [[Activation Functions]]
* [[Maxout]]
	* [[Activation Functions]]

## ...
=======
- [mlmastery](https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/)## Backlinks
* [[Backprop]]
	* Solved by [[ActivationFunctions]]
* [[DeepLearning]]
	* [[ActivationFunctions]]

>>>>>>> 1dd38fd29e2ea89a9d6c64b1ecd9e965740dd3c9
