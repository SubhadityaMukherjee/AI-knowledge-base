---
title: He Initialization

tags: regularize
---

# He Initialization
- bring the variance of those outputs to approximately one
- However, Kumar indeed proves mathematically that for the ReLU activation function, the best weight initialization strategy is to initialize the weights randomly but with this variance:
	- $$\begin{equation} v^{2} = 2/N \end{equation}$$
- For Sigmoid based activation functions


## Backlinks

> - [Initialization](Initialization.md)
>   - [[He Initialization]]

_Backlinks last generated 2022-07-26 19:32:48_
