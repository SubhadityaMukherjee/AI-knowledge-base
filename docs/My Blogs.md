---
title: Articles
toc: true
tags: 
date modified: Wednesday 16th July 2025, Wed
date created: Wednesday 16th July 2025, Wed
---

# Articles sorted by recency :)
- Click the link to navigate to the full thing


- [[DRAFT BookTalk - Piranesi]]

```md
- piranesi means trapped
- what was it about
- house with ancient statues and sea
- Piranesi wakes up with no real memory of who he is and just knows he loves the "house"
- writing style reminded me of the book The Slow Regard of Silent Things by Patrick Rothfuss
...
```

- [[Summary - Notes on a Nervous Planet by Matt Haig]]

```md

Notes on a nervous planet is a book that brings a lot of important points about the collective overwhelm that we as a “modern” society face. Social media, lack of sleep, odd working hours, loneliness. Neither of these were aspects that we were really prepared to deal with. But before treating a disease, we must know what the symptoms are and why they showed up in the first place. (At the end of the article, there is a long is list of what you can do to avoid overwhelm.)

This is a summary and my thoughts (chapter wise, mostly) on the book. I try to cover what stuck out to me the most and in turn, hope that it helps someone out. If you like the summary, you will like the book more.

...
```

- [[Commission FAQ]]

```md

## Name
Full form article about anything related to AI/Neural networks/Technical writing.

## Description
...
```

- [[Writing your own Markdown to LaTEX parser]]

```md

## What we want

## Disclaimers

...
```

- [[What I learnt from an AI Masters Part 2]]

```md
If you are a programmer, this part might not be something you are particularly afraid of. But if you are not as comfortable with programming, it would help you to set up your environment before you start. This might include things like creating a GitHub repository, installing all the packages that you need, making sure that you have the resources that you might require (such as a GPU) etc. In future parts of the article, I will try to detail as much of this as I can based on my experience. (Depending on when you are reading this, these parts of the article might be already posted, so refer to them.)

### Setting up your writing environment
A thesis is a rather long document, but by this point, you hopefully have written enough assignments to be comfortable with writing technical articles. If you have not, or you are reading this before doing a Master, it would serve you to be a little familiar with LaTEX. It's took me about a few hours to get everything set up and running, but it might take you longer. (Refer to a detailed guide in future articles.)

...
```

- [[Ai_for_startups]]

```md

Are you part of a startup, or want to start one? Want to have AI superpowers but don't know where to start? Want to make sure you don't sink your boat before it floats? Make sure you hire the right people and avoid blowing your budget out of the water. This article is for you.
A helpful guide on what to focus on, resources you need, and punching common myths in the face. Sounds interesting?
Read on.

...
```

- [[Sigh, Gone - My Thoughts on the Book]]

```md

At some point in life, we stop reading and shift to faster content - articles, videos, and now “reels.” This year, I decided to go the other way and read as many books as possible. And perhaps write about some of them. Summarizing a book, though, is an exercise in futility and encourages more “fast content,” which is the opposite of what I want to do. Instead, I want to explore some of my thoughts while reading these books as a note to you, my dear reader, and future me.

## Introduction
This article is about the book *Sigh, Gone: A Misfit's Memoir of Great Books, Punk Rock, and the Fight to Fit In* by *Phuc Tran*. The book is a journey through the author’s life as a second-generation immigrant in America and his struggle to fit in. Phuc talks about many things: kind families, racism, trying to blend in, books, and what influenced him growing up. I loved the writing style and how every chapter was the name of a book. It was an enjoyable read, although some parts did make me want to cry.
...
```

- [[What I learnt from an AI Masters Part 1]]

```md

Part 1 : Starting  Troubles - Finding a Project, Supervisors ; Creating a proposal (This article)
Part 2 : Timelines  - Optimal Planning and Breakdowns
Part 3 : Setup and Tools - Setting up Writing, Programming, and research to prevent tears
Part 4 : Writing and Programming Advice
...
```

- [[Duolingo is a platform for language learning]]

```md
Chip Huyen is one of my favourite authors in the space of MLOps. She has some great blogs, and a really useful book. In one of them, she asks the [[https://huyenchip.com/machine-learning-systems-design/exercises.html#exercises-rWl8SQW]]. This blog post is my answer to the ones I felt I could contribute interesting points to. Since there are quite a few, I will probably split them up into parts.

Note: These are my views on these questions. They are not a comprehensive resource by any means. Just me thinking out loud on how I would go about solving the problem. Like any research project, as more time passes, these answers might change. They are here as a way for someone starting out to get a feel for approaching problems posed to them.

1. Duolingo is a platform for language learning. When a student is learning a new language, Duolingo wants to recommend increasingly difficult stories to read.
...
```

- [[Sklearn and OpenML hackathon]]

```md
<!--section: 2-->
## Intro

Scikit-learn is a free and open-source machine learning library for the Python programming language. If you are reading this article though, chances are that you have probably used it already.

...
```

- [[Professional Reports and Papers with LaTEX]]

```md
At the heart of everything, what you are struggling with is the issue of change. If you had a small article, with barely any changes or styling, Word is great. But for anything more than 5 pages? Ouch.
LaTeX is a complete document preparation system, with the added advantage of a different “language” that makes your life a lot easier. It sounds and looks very strange at first. But once you get the hang of it, it will change the way you write content. Think of it as a more advanced template that is infinitely customisable. All you do is set things up once. Then you can focus on your content.
Oh no! Something changed? That’s okay. Add a few lines to your document and you are good to go.
I do all my reports, articles, homework, projects everything using it and it has saved me days of effort.
It looks professional right off the bat.
...
```

- [[Parsing and Querying Tensorboard logs - A Mini Tutorial]]

```md

So, you wanted to parse your Tensorboard logs, didn’t you? Did you try using GPT-3? OH! GPT-4? Well. Guess that didn’t give you what you wanted.
Yeah, me neither. So here we are.
Read on and you will find out how to take all the runs you logged to Tensorboard, clean them up, and put them in a single DataFrame. From there, you can query it as you would any other table.

...
```

- [[Tree of Thoughts Explained Simply (LLMs Dehyped - 1)]]

```md
> “Programmers are, in their hearts, architects, and the first thing they want to do when they get to a site is to bulldoze the place flat and build something grand.” : Joel Spolsky, co-founder of Stack Overflow

## Introduction
Large language models like GPT4 have taken over the world and has left every second IT person to scramble towards the next great AI solution. Given the monetary benefits, a lot of research has popped up in a very short time. While this is very exciting, it is wise to look a little deeper beneath the hype. The “new groundbreaking research from {X} that will change your company” is sometimes a tiny change or a creative use of an existing concept.
This is not to pour cold water over your dreams, but a way to help you understand these concepts better. If you did not come from a technical computer science background, this constant influx of “groundbreaking research” can get very overwhelming.
...
```

- [[AI and Doom]]

```md

## Ideas
- Why do we work : society
- The pursuit of art
- Luddites
...
```

- [[Easier Deep Learning Research for Beginners]]

```md
{{TOC}}
So, you took your first steps into Deep Learning. Maybe you read a few articles, did a course or two, and watched a bunch of videos. Or maybe you just heard so much about it that you wanted to learn more.
Welcome.
This is a beautiful world. It is also very overwhelming. There is so much to learn and understand. But we need to start somewhere.
This is your ticket. Enjoy the ride.
...
```

- [[Notes - Machine Learning System Design]]

```md

These are my notes on [this article](https://github.com/chiphuyen/machine-learning-systems-design/blob/master/build/build1/consolidated.pdf) by [Chip Huyen](https://huyenchip.com/).

The article that these notes are based on, talks about some factors that are involved in designing machine learning systems, and what to watch out for in interviews on the same. I wanted to write a summary of what I read and add my take on it for future reference.

...
```

- [[GRAD CAM]]

```md
Note: This is a slightly advanced article. If you are not comfortable with training neural networks, this is probably not for you yet. Start [here](https://msubhaditya.medium.com/easier-deep-learning-research-for-beginners-935c1a6721c3?source=your_stories_page-------------------------------------) instead.

## Intro
So you want to train a Neural Network to classify images. Woah. That’s awesome! How well did it do? Did you get a good score? Oh? You want to do better? I hear you.
What if you could see what the network sees to make the choice? That would help understand how to make it perform better right? Read on!
...
```

- [[Experiments with LLM temperature]]

```md

Over the past few months at OpenML, we have been experimenting with LLM models in an attempt to improve the search experience for our users. While our existing implementation uses ElasticSearch, we wanted to also have the option of having a more "semantic" search experience.

Aside from the usual RAG pipeline that everyone and their grandparents seems to be using these days, we also wanted to experiment with using an LLM to semi-automatically generate filters for our search queries. While it may not seem like a big feature, it is something that has always been a bit of an annoyance for some of our users.

...
```

- [[Implementing DCGAN to generate CIFAR images]]

```md

## What are we building?
Creating novel images is one of the strengths of Generative Adversarial Networks (GANs). In this article, we will build a DCGAN for image generation using the [[CIFAR]] dataset. The DCGAN is a type of GAN that builds upon the Vanilla GAN and addresses some of its issues. The DCGAN is a good choice if the image data size is larger than 28x28. This network also leads to fewer chances of a mode collapse and is thus a better network than a standard GAN.
Here, we want the network to create realistic images to resemble any of the ten classes of the [[CIFAR]] dataset. We will create the DCGAN from scratch using PyTorch, train it and write scripts to generate our images.

...
```

- [[Generating Images using GANs in Tensorflow]]

```md
:::section{.abstract}
# Overview
This article explains using a Generative Adversarial Network (GAN) to generate new images of handwritten digits. A GAN is a machine-learning model consisting of a generator and a discriminator. The generator creates novel images from random, while the Discriminator attempts to prove that the images generated are fake. The GAN is trained on the MNIST dataset of handwritten digits and is evaluated by testing it on unseen data and creating new images using the generator. The final output of the GAN is a batch of images that look like handwritten digits. The article provides code for reading the dataset, creating the required architecture, computing loss functions, training the network, and testing the network.

:::
...
```

- [[Transfer Learning and Fine-tuning]]

```md

:::section{.abstract}

## Overview
Training deep learning models requires a massive amount of labeled data. In most cases, this data needs to be made available or easier to clean up. Many approaches for working with limited data sets have been created over the years, **Transfer Learning** being one of the breakthroughs. Transfer learning enables us to **fine-tune** a model pre-trained on a large dataset on our task.
...
```

- [[Autoencoders with Convolutions]]

```md

:::section{.abstract}

## Overview
In the absence of labels in a dataset, only a few models can perform well. The Convolutional Autoencoder is a model that can be used to **re-create** images from a dataset, creating an unsupervised classifier and an image generator in the process. This model uses an Encoder-Bottleneck-Decoder architecture to understand the **latent space** of the dataset and re-create the images.
...
```

- [[CycleGAN #scalar]]

```md
:::section{.abstract}

## Overview
The field of computer vision has been trying to create AI that creates never seen before images for decades. Generative networks such as the CycleGAN are part of a long line of such research, but one that performs extremely well in tasks ranging from converting images to paintings to changing the weather in images. The CycleGAN is rather different from many approaches before it as it is an unpaired Image2Image translation task with these tasks being cyclic in nature. In this article, we will explore what all these terms mean and how to put them into practise in a CycleGAN.

...
```

- [[Masked Language Modeling in BERT #scalar]]

```md
masked language model explained

:::section{.abstract}

## Overview
...
```

- [[Improving Model Accuracy in Image Classification]]

```md

:::section{.abstract}

## Overview
Improving image classification accuracy is one of the biggest hurdles in deep learning. Apart from using a deeper network and better data, many techniques have been developed to optimize network performance. Some techniques, such as **[[Dropout]]**, are more focused on improving the overall pipeline.
...
```

- [[Reconstructing the MNIST images using an autoencoder]]

```md

:::section{.abstract}

## Overview
Given noisy images, an **Autoencoder** is a family of models that can convert these noisy images to their original form. These models are unsupervised and use an **Encoder-Decoder** architecture to re-create the original images given **noisy** variants of the same. In the process of re-creation, the model also learns to model the latent space of the dataset.
...
```

- [[Building the Word2Vec Model in Gensim]]

```md

:::section{.abstract}

## Overview
Word2Vec is a family of models that can take large corpora and represent them in vector space. These representations, also known as word embeddings, are extremely useful as they help us perform many tasks in NLP. From recommender systems to analysing sentiments from internet feeds to large-scale chatbots, word embeddings have brought life to the field of NLP for decades. Word2Vec, one of the older models, is relatively simple to implement. After implementing it we will use word embedding [[visualization]] to further understand how the model works.
...
```

- [[Image Classification using TensorFlow]]

```md

:::section{.abstract}

## Overview
Image Classification is one of the basic tasks in Deep Learning. Given a dataset with images of different categories, we create a Deep Learning model and a pipeline to classify these images. We can create models in any library, but Tensorflow is a good starting point for beginners, and we will use this library to create an image classifier.
...
```

- [[DCGAN – Adding convolution to a GAN]]

```md

:::section{.abstract}

## Overview

...
```

- [[Siamese Network]]

```md

:::section{.abstract}

## Overview
The lack of data is a huge hurdle for any Deep learning task. Finding large datasets is nearly impossible in domains such as facial recognition, signature verification, text similarity, and many others. In many cases, a single example of each class is present. A regular CNN fails to perform in these cases, but if a network learns to minimize a similarity metric between images, it can easily perform this task. The Siamese Network is a class of architectures that excel at this one-shot learning task.
...
```

- [[Differences between Discriminative and Generative models]]

```md

:::section{.abstract}

## Overview
Machine learning models can be broadly classified into discriminative and generative. Discriminative models, such as logistic regression, support vector machines, and decision [[Trees]], while discriminative models are more commonly used for classification and regression.
...
```

- [[Time series forecasting using LSTM]]

```md

:::section{.abstract}

## Overview
Any temporal data can be framed as a time series task. Data such as heart rates, stock market prices, sensor logs and many others fall under the category of time series data. There are many Deep Learning architectures that are used to model such data, LSTMs being one of them. This article focuses on building an LSTM time series model.
...
```

- [[Scalar Articles]]

```md

## Done
- [Word2Vec] with [gensim](Word2Vec] with [gensim.md)
- [CycleGAN](CycleGAN.md)
- [Masked Language Modeling] with [BERT](Masked Language Modeling] with [BERT.md)
...
```

- [[Building a GAN from scratch]]

```md

:::section{.abstract}

## Overview
Generating images from scratch is a huge deal in computer vision. A **Generative Adversarial Network**(GAN) was one of the first models to generate new images in an unsupervised manner efficiently. A GAN is not a single model but a family of different architectures used for image generation.
...
```

- [[Intro to Conditional GANS]]

```md

:::section{.abstract}

## Overview
Generating novel images from an image dataset has been a dream in Computer Vision. Being able to influence the generation of these images was made possible by a family of GANs named **Conditional GANs**. The following article explores these CGANs and shows how the DCGAN was modified to have the ability to control latent space traversal to a certain extent. We also look at the training paradigm and cover some challenges we might encounter while training a CGAN on our data.
...
```

- [[StackGAN]]

```md
:::section{.abstract}

## Overview

In image generation, the GAN architecture is one of the best ones. The StackGAN architecture addresses some of the flaws of basic GANs by decomposing the task of generating images into multiple parts. This article will focus on the training paradigm proposed by StackGAN and take an in-depth look at its architecture.
...
```

- [[Obisidian Daily Notes]]

```md

Obsidian is one of my favorite productivity tools, but there are some things that I don't fully like about it. One of them that being the daily notes feature. I use Obsidian for journaling and getting an overview of my day while making notes and so being able to use this function efficiently would be very helpful to me.
In this article, I talk about how I use daily notes in Obsidian, the tweaks I have made, the plugins I use, and the hotkeys I have set up.

Note: None of the plugins I discuss are sponsored and I am sharing them only because I use almost them every day.
...
```

- [[Pomodoro - A means but not the end]]

```md

In our quest for productivity, many ideas, tools, and theories have sprung up over the ages. Some work, some don’t, and some are versions of previous ones with a “modern” twist. The concept of the Pomodoro timer seems to have stood the test of time. The hustle culture especially has glorified its existence and made it out to be something of a magic pill that either works for you or does absolutely nothing.
I think it is a very valuable tool. But in time, we have stopped thinking of it as a tool and made it out to be more of a rule.
So how do we make it work for us?

...
```

- [[Obsidian Plugins]]

```md
Obsidian is my favourite program for taking notes. Be it for research, general things I learn, summaries from papers, lecture notes and the like. Out of the box, it does so many things really well.
But, its real power lies in the vast number of plugins it has. Most of these are user created, and you can even make your own (or hack one together)! In this sea of functionality, these are the top few that I use. Grouped by the type of task for easier lookup.

(Disclaimer : I am not sponsored by either Obsidian or any of the authors of the plugins mentioned here. These are personal preferences.)

...
```

- [[Obsidian tutorial]]

```md


## Hello There
- ![](Pasted%20image%2020240514132517.png)

...
```

- [[CheckEmptySections]]

```md

python
import os
from pathlib import Path
import argparse as ap
...
```

- [[Extracting Highlights from a PDF easily]]

```md
Convert highlights like in this document on the left to notes with minimal effort.
I use a note application called [Obsidian](https://obsidian.md), but anything you use works here. As long as it supports text.

## Existing Programs and… their flaws
Many PDF viewers offer the ability to export notes into text. Like [Skim](https://skim-app.sourceforge.io) for Mac, [Adobe Acrobat](https://get.adobe.com/uk/reader/), [OneNote](https://www.onenote.com/) etc
...
```

- [[Extensions that make writing research papers easier]]

```md

While researching anything, we tend to heavily rely on our browser. To make this process more efficient, quite a few “plugins” have been created over the years. Every major browser has hundreds, if not thousands of such plugins/extensions/add-ons. Like any infinite list of things, it’s often overwhelming to find helpful ones.

For my work, I read a lot of articles and skim through many web pages, blogs, you name it. In the process, I need to store this information somewhere. Either to write a research paper, or a blog like this one, or just for my knowledge. The following plugins have made my life a lot easier and so I thought I would share them with you.

...
```

- [[Taking notes from websites to markdown - A workflow]]

```md

## What we want

## Obsidian Quick intro

...
```

