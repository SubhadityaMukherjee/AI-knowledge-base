---
title: Attention Based Distillation

categories: ['knowledgedistillation']
date modified: Monday, October 10th 2022, 2:02:09 pm
date created: Tuesday, October 4th 2022, 11:51:17 am
---

# [Attention](Attention.qmd) Based Distillation
- That is to say, knowledge about feature embedding is transferred using [attention](Attention.qmd) map functions. Unlike the [attention](Attention.qmd) maps, a different attentive knowledge distillation method was proposed by Song et al. (2018). An [attention](Attention.qmd) mechanism is used to assign different confidence rules (Song et al., 2018).



