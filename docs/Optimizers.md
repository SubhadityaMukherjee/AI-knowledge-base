---
title: Optimization
tags: regularize 
---

# Optimization
- [[Gradient Descent gradients]]

- [[Adagrad]]

- [[Rmsprop]]

- [[Adam]]

- [[Learning Rate Decay tricks]]

- [[Early Stopping tricks]]

## â€¦


































































































## Backlinks

> - [Non [[Relational Inductive Bias]]](Non Relational Inductive Bias.md)
>   - [[Optimizers]]
>    
> - [](DeepLearning.md)
>   - [[Optimizers]]
>    
> - [Bias Vs Variance](Bias Variance Dilemma.md)
>   - Tune on [[Emperical Risk]] instead using [[Optimizers]]
>    
> - [Vanishing/exploding #gradients](Vanishingexploding gradients.md)
>   - [[Regularization]] , [[Optimizers]] , [[Architectures]]

_Backlinks last generated 2022-07-26 20:33:15_
