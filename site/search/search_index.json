{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"SiteMap","text":""},{"location":"#welcome","title":"Welcome!","text":"<ul> <li>This is my little knowledge base</li> <li>If there is something you are looking for, just type it into the search bar<ul> <li>Of course, since this is not google, it is not a one stop shop</li> <li>In essence, it will have something on things that I learn</li> </ul> </li> <li>How to go about finding things?<ul> <li>Scroll the sidebar and pick something you like. Click the links inside it to go about. Choose your own adventure.</li> </ul> </li> <li>Major topics (These index pages are auto generated and might have errors sometimes)<ul> <li>Articles</li> <li>Deep learning<ul> <li>Deep Learning architectures</li> <li>Loss Functions</li> <li>Datasets</li> <li>Pytorch_Tricks</li> <li>Regularizing stuff</li> </ul> </li> <li>Others<ul> <li>Robotics</li> <li>Brain</li> <li>Visualization</li> <li>Language</li> <li>Medical</li> <li>Psychology</li> </ul> </li> </ul> </li> </ul>"},{"location":"#links","title":"Links","text":"<ul> <li>My Github</li> <li>Blogs</li> <li>Medium -&gt; More blogs</li> <li>My Linkedin</li> <li>My Art</li> <li>Email</li> </ul>"},{"location":"#anchor","title":"anchor","text":""},{"location":"0-1%20Loss/","title":"0-1 Loss","text":"<ul> <li> \\[\\begin{cases} 1 &amp; f(x)=y \\\\ 0 &amp; f(x)\\neq y\\end{cases}\\] </li> <li>Classification</li> </ul>"},{"location":"1D%20piecewise%20linear%20interpolation/","title":"1D Piecewise Linear Interpolation","text":""},{"location":"1D%20piecewise%20linear%20interpolation/#1d-piecewise-linear-interpolation_1","title":"1D Piecewise Linear Interpolation","text":""},{"location":"2%20X%202%20Study/","title":"2 X 2 Study","text":"<ul> <li>First factor has 2 levels (The vs each)</li> <li>Second factor has 2 levels (Distributive vs. Collective situations)</li> <li>We are trying to see how these fixed factors effect responses</li> <li>We control the fixed factors</li> <li>We carefully design them so we know what category an item presented to a participant belongs to</li> </ul>"},{"location":"2%20byte%20character%20set/","title":"2 Byte Character Set","text":"<ul> <li>65,536 unique characters Pairs of Bytes for a Single Character</li> <li>Sometimes single byte letters, spaces and punctuations will be interspersed with two-byte characters</li> <li>Chinese characters are encoded in two format:<ul> <li>Big-5 - Complex Mandarin</li> <li>GB - Simple form</li> </ul> </li> </ul>"},{"location":"8%20bit%20character%20set/","title":"8 Bit Character Set","text":"<ul> <li>First 128 characters are reserved for ASCII</li> <li>ISO-8859 series of 10+ Character Sets for most European Languages</li> <li>Results in large number of overlapping character sets for different languages</li> </ul>"},{"location":"A%20declarative%20modular%20framework%20for%20representing%20and%20applying%20ethical%20principles/","title":"A Declarative Modular Framework for Representing and Applying Ethical Principles.","text":"<ul> <li>Fiona Berreby, Gauvain Bourgne, and JeanGabriel Ganascia</li> <li>high level action language for designing ethical agents in an attempt to shift the burden of moral reasoning to the autonomous agents</li> <li>collects action, event and situation information to enable an agent to simulate the outcome of various courses of actions</li> <li>event traces are then passed to the causal engine to produce causal traces</li> <li>ethical specifications and priority of ethical considerations under a given situation are used to compute the goodness assessment on the consequences</li> <li>combined with deontological specifications (duties, obligations, rights) to produce a final rightfulness assessment</li> </ul>"},{"location":"A%20declarative%20modular%20framework%20for%20representing%20and%20applying%20ethical%20principles/#backlinks","title":"Backlinks","text":"<ul> <li>Building Ethics into Artificial Intelligence</li> <li>[[A declarative modular framework for representing and applying ethical principles.]]</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"A%20low-cost%20ethics%20shaping%20approach%20for%20designing%20reinforcement%20learning%20agents/","title":"A Low-cost Ethics Shaping Approach for Designing Reinforcement Learning Agents","text":"<ul> <li>Yueh-Hua Wu and Shou-De Lin.</li> <li>authors investigated how to enable RL to take ethics into account ethics shaping</li> <li>assuming that the majority of observed human behaviours are ethical, the proposed approach learns ethical shaping policies from available human behaviour data in given application domains</li> <li>rewards positive ethical decisions, punishes negative ethical decisions, and remains neutral when ethical considerations are not involved</li> </ul>"},{"location":"A%20low-cost%20ethics%20shaping%20approach%20for%20designing%20reinforcement%20learning%20agents/#backlinks","title":"Backlinks","text":"<ul> <li>Building Ethics into Artificial Intelligence</li> <li>A low-cost ethics shaping approach for designing reinforcement learning agents</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"A%20matter%20of%20ambiguity%3F%20Using%20eye%20movements%20to%20examine%20collective%20vs%20distributive%20interpretations%20of%20plural%20sets/","title":"A Matter of Ambiguity? Using Eye Movements to Examine Collective Vs. Distributive Interpretations of Plural Sets","text":"<ul> <li>Christine Boylan, Dimka Atanassov, Florian Schwarz, John Trueswell</li> </ul>"},{"location":"A%20matter%20of%20ambiguity%3F%20Using%20eye%20movements%20to%20examine%20collective%20vs%20distributive%20interpretations%20of%20plural%20sets/#previous-work","title":"Previous Work","text":"<ul> <li>Frazier et al. (1999) used eye-tracking reading times to compare processing loads of sentences that were explicitly distributive (involving the adverb each), explicitly collective (involving the adverb together), and locally indeterminate at the predicate.</li> <li>Having found evidence for increased processing load associated with distributive sentences, they concluded that the processor initially pursues a collective reading, and thus the distributive / collective distinction was one of ambiguity and not vagueness.</li> <li>However, an increased processing load for the distributive reading might be expected regardless of whether the underlying representation is vague or ambiguous</li> <li>processor must stipulate a distributive operator D (the spell-out of which is each) to interpret a distributive meaning, which may incur processing delays</li> <li>Moreover, increased reading times at the point of disambiguation preclude conclusions about exactly when listeners may have committed to a distributive reading during the processing the underdetermined predicate.</li> </ul>"},{"location":"A%20matter%20of%20ambiguity%3F%20Using%20eye%20movements%20to%20examine%20collective%20vs%20distributive%20interpretations%20of%20plural%20sets/#introduction","title":"Introduction","text":"<ul> <li>eye movements of listeners were recorded to investigate the representation of collective vs. distributive interpretations of plural subjects in light of the Minimal Semantic Commitment (MSC) hypothesis</li> <li>Minimal Semantic Commitment</li> <li>The crucial difference between these two proposed representation types is that an ambiguous representation forces a decision about an interpretation, while a vague representation tolerates unspecified features.</li> <li>Given the prediction that an ambiguous item will prompt the processor to converge on one interpretation even in the absence of disambiguating information, we tested whether sentences underdetermined for collectivity / distributivity would nonetheless cause listeners to converge on a single interpretation.</li> <li>Rather than relying on processing times to infer representational commitments, we employed the visual world paradigm to track which representations subjects considered over the course of hearing a sentence</li> </ul>"},{"location":"A%20matter%20of%20ambiguity%3F%20Using%20eye%20movements%20to%20examine%20collective%20vs%20distributive%20interpretations%20of%20plural%20sets/#method","title":"Method","text":"<ul> <li>The eye movements of 24 participants were recorded as they listened to explicit/indeterminate collective/distributive sentences while they considered collective and distributive acts depicted on a computer screen</li> <li>An earlier switch in gaze to one of the two images would indicate a processing preference for one interpretation over the other. Results and Discussion</li> <li>Explicitly collective sentences prompted looks to the collective scenario at the point of disambiguation (i.e. together) and explicitly distributive sentences (using each) prompted looks to the distributive scenario (Fig. 2a)</li> <li>Crucially, however, the indeterminate, nulldisambiguator sentences patterned along the same trajectory as together sentences: the predicate alone prompted looks to the collective, prior to hearing the final word of the sentence</li> <li>We also compared the proportion of looks averaged across two time windows: an 800- ms interval before the onset of the predicate and an 800-ms time window following the predicate onset (Fig. 2b)</li> <li>Since the distinction between ambiguity and vagueness lies at the interface of semantics and pragmatics, it is particularly important that we find a psychometric realization of the difference between these two types of representations.</li> <li>Here we presented a method by which to investigate the time courses of these representations as they relate to distributivity, and we suggest this method may further contribute to the study of the semantics-pragmatics interface at large.</li> </ul>"},{"location":"A%20matter%20of%20ambiguity%3F%20Using%20eye%20movements%20to%20examine%20collective%20vs%20distributive%20interpretations%20of%20plural%20sets/#results","title":"Results","text":"<ul> <li>In an ANOVA of proportion of looks to collective and distributive scenes, we found significant interactions between disambiguator and time window.</li> <li>In a targeted analysis of disambiguator effects in each time window, we found significant differences between together and each sentences and the null and each sentences after the predicate onset but not before</li> <li>Moreover, the together sentences did not significantly differ from the null form.</li> <li>Thus, despite a lack of explicit disambiguating information, the indeterminate, nulldisambiguator sentences nonetheless prompted looks to the collective scenario, which was reliably different from the time course of distributive-directed each sentences.</li> <li>This provides evidence that the listener has committed to the collective interpretation in the absence of disambiguating information.</li> <li>This is consistent with a theory that treats the collective / distributive distinction as ambiguous rather than vague.</li> <li>The results also indicate that this processing commitment is essentially immediate; i.e., as soon as listeners begin hearing the ambiguous predicate, they show a preference for the collective interpretation.</li> </ul>"},{"location":"A%20matter%20of%20ambiguity%3F%20Using%20eye%20movements%20to%20examine%20collective%20vs%20distributive%20interpretations%20of%20plural%20sets/#pictures","title":"Pictures","text":""},{"location":"A%20survey%20on%20Image%20Data%20Augmentation%20for%20Deep%20Learning/","title":"Data Augmentation with Curriculum Learning","text":"<ul> <li>Connor Shorten and Taghi M. Khoshgoftaar</li> </ul>"},{"location":"A%20survey%20on%20Image%20Data%20Augmentation%20for%20Deep%20Learning/#methods","title":"Methods","text":"<ul> <li>Geometric Transformations</li> <li>Flipping</li> <li>Color Space Transform</li> <li>Cropping</li> <li>Noise Injection</li> <li>Color Space Transformations</li> <li>Kernel Filters</li> <li>Feature Space Augmentation</li> <li>SMOTE </li> <li>GAN\u2010based Data Augmentation</li> <li>Meta Learning Data Augmentations</li> <li>Neural Augmentation</li> <li>Smart Augmentation</li> <li>AutoAugment</li> <li>Augmented Random Search</li> <li>Test-time Augmentation</li> <li>SamplePairing</li> <li>Data Augmentation with Curriculum Learning</li> <li>Alleviating Class Imbalance with Data Augmentation</li> </ul>"},{"location":"A%20survey%20on%20Image%20Data%20Augmentation%20for%20Deep%20Learning/#discussion","title":"Discussion","text":"<ul> <li>It is easy to explain the benefit of horizontal Flipping or random cropping</li> <li>However, it is not clear why mixing pixels or entire images together such as in PatchShuffle regularization or SamplePairing is so effective.</li> <li>dditionally, it is difficult to interpret the representations learned by neural networks for GAN-based augmentation, variational auto-encoders, and meta-learning.</li> <li>An interesting characteristic of these augmentation methods is their ability to be combined together.</li> <li>The GAN framework possesses an intrinsic property of recursion which is very interesting</li> <li>Samples taken from GANs can be augmented with traditional augmentations such as lighting filters, or even used in neural network augmentation strategies such as Smart Augmentation or Neural Augmentation to create even more samples. These samples can be fed into further GANs and dramatically increase the size of the original dataset.</li> <li>An interesting question for practical Data Augmentation is how to determine postaugmented dataset size.</li> <li>no consensus about the best strategy for combining data warping and oversampling techniques</li> <li>One important consideration is the intrinsic bias in the initial, limited dataset</li> <li>There are no existing augmentation techniques that can correct a dataset that has very poor diversity with respect to the testing data</li> </ul>"},{"location":"A%20voting-based%20system%20for%20ethical%20decision%20making/","title":"A Voting-based System for Ethical Decision Making","text":"<ul> <li>Ritesh Noothigattu, Snehalkumar \u2018Neil\u2019 S. Gaikwad, Edmond Awad, Sohan Dsouza, Iyad Rahwan, Pradeep Ravikumar, and Ariel D. Procaccia</li> <li>voting-based system for autonomous entities to make collective ethical decisions leverages data collected from the Moral Machine project</li> <li>Self reported preference over different outcomes under diverse ethical dilemmas are used to learn models of preference for the human voters over different alternative outcomes.</li> <li>individual models are then summarized to form a model that approximates the collective preference of all voters</li> </ul>"},{"location":"A%20voting-based%20system%20for%20ethical%20decision%20making/#backlinks","title":"Backlinks","text":"<ul> <li>Building Ethics into Artificial Intelligence</li> <li>A voting-based system for ethical decision making</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"ACT-R%20Chunk/","title":"ACT-R Chunk","text":"<ul> <li>id</li> <li>Attribute<ul> <li>Attributes point to other chunks (eg: fact3+2)</li> <li>number of attributes</li> </ul> </li> <li>Activation<ul> <li>determines priority of retrieval</li> <li>if below threshold, chunk cannot be retrieved</li> <li>determines time of retrieval</li> <li>decays with time</li> <li>depends on<ul> <li>How often retrieved or recreated</li> <li>context</li> </ul> </li> </ul> </li> </ul>"},{"location":"ACT-R%20Chunk/#backlinks","title":"Backlinks","text":"<ul> <li>Declarative Memory</li> <li>ACT-R Chunk</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"ACT-R/","title":"ACT-R","text":"<ul> <li>Explain human behavior</li> <li>Manual Control<ul> <li>[[Motor Cortex]]</li> </ul> </li> <li>Visual Perception<ul> <li>Visual Cortex</li> </ul> </li> <li>Problem State<ul> <li>Parietal lobe</li> <li>Store intermediate results</li> </ul> </li> <li> <p>Declarative memory</p> </li> <li> <p>Perception</p> </li> <li>Control State<ul> <li>[[Anterior Cingulate]]</li> <li>Keep track of goals</li> <li>What? How far?</li> </ul> </li> <li>[[Time perception]]</li> <li>Ideas always get reinforced if retrieve</li> </ul>"},{"location":"ADE20K/","title":"ADE20K","text":"<p>title: ADE20K</p> <p>tags: dataset</p>"},{"location":"ADE20K/#ade20k","title":"ADE20K","text":""},{"location":"ADVENT/","title":"ADVENT","text":"<ul> <li>blog</li> <li>paper</li> <li></li> <li>ADVENT is a flexible technique for bridging the gap between two different domains through entropy minimization</li> <li>models trained only on source domain tend to produce over-confident, i.e., low-entropy, predictions on source-like images and under-confident, i.e., high-entropy, predictions on target-like ones</li> <li>Consequently by minimizing the entropy on the target domain, we make the feature distributions from the two domains more similar.</li> <li>More annotated data has been shown to always improve performance of DNNs</li> <li>Here we are working on Unsupervised DA (UDA), which is a more challenging task where we have access to labeled source samples and only unlabeled target samples. We use as source, data generated by a simulator or video game engine, while for target we consider real-data from car-mounted cameras.</li> <li>The main approaches for UDA include discrepancy minimization between source and target feature distributions usually achieved via adversarial training , , self-training with pseudo-labels  and generative approaches , .</li> <li>We present our two proposed approaches for entropy minimization using (i) an unsupervised entropy loss and (ii) adversarial training. To build our models, we start from existing semantic segmentation frameworks and add an additional network branch used for domain adaptation.</li> <li>Direct entropy minimization</li> <li>Entropy minimization by adverarial learning</li> <li>GTA5</li> <li>SYNTHIA</li> <li>Cityscapes</li> </ul>"},{"location":"ALBERT/","title":"ALBERT","text":"<ul> <li>Lite BERT</li> <li></li> <li>blog #Roam-Highlights</li> <li>perhaps even better when scaled to the same number of parameters as BERT</li> <li>Factorized Embedding Parameters</li> <li>cross-layer parameter sharing</li> <li>inter-sentence coherence loss</li> <li>We can see that the ALBERT base model attempts to mimic BERT base, with a hidden state size of 768, parameter sharing and a smaller embedding size due to factorization explained above. Contrary to the 108 million parameters, it has only 12 million. This makes a big difference when training the model.</li> <li>Another model, ALBERT xxlarge (extra-extra large) has 235 million parameters, with 12 encoder segments, 4096-dimensional hidden state and 128-dimensional embedding size. It also includes parameter sharing.</li> <li>GLUE</li> <li>SQuAD</li> <li>RACE</li> <li>For Factorized Embedding Parameters, the authors report good performance. Both the case where cross-layer parameters were not shared and where they were, are reported. Without sharing, larger embedding sizes give better performance. With sharing, performance boosts satisfy at an embedding size of 128 dimensions. That's why the 128-size embeddings were used in the table above.</li> <li>For cross-layer parameter sharing, the authors looked at not performing cross-layer sharing, performing cross-layer sharing for the feedforward segments only, performing sharing for the attention segments, and performing sharing for all subsegments. It turns out that sharing the parameters for the attention segments is most effective, while sharing the feedforward segment parameters does not contribute significantly. This clearly illustrates the important role of the attention mechanism in Transformer models. Because, however, all-segment sharing significantly decreases the number of parameters, at only slightly worse performance compared to attention-only sharing, the authors to perform all-segment sharing instead.</li> <li>For the SOP task, we can read that if NSP is performed on a SOP task, performance is poor. NSP on NSP of course performs well, as well as SOP on SOP. However, if SOP is performed on NSP, it performs really well. This suggests that SOP actually captures sentence coherence whereas NSP might not, and that SOP yields a better result than NSP.</li> </ul>"},{"location":"ANOVA/","title":"ANOVA","text":"<ul> <li>For an ANOVA you aggregate the data</li> <li>so that you get the mean responses per condition for each individual, look at the differences between their means, and then check if the differences for all the</li> </ul>"},{"location":"ASCII/","title":"ASCII","text":"<ul> <li>7 bit character set</li> <li>Roman, Latin</li> </ul>"},{"location":"AUC-Borji/","title":"AUC-Borji","text":"<ul> <li>\"PR is calculated in the same way as AUC-Judd\"</li> <li>\"a location-based metric\"</li> <li>FPR is obtained by calculating the proportion of negatives in the thresholded region, where the negatives are collected uniformly at random.</li> <li>\"AUC for the curve is calculated as AUC-Borji.\"</li> <li>\"TPR is calculated in the same way as AUC-Judd\"</li> <li>\"location-based metric\"</li> </ul>"},{"location":"AUC-Judd/","title":"AUC-Judd","text":"<ul> <li>\"location-based metric\"</li> <li>The true positive rate (TPR) is calculated as the proportion of fixations falling into the thresholded saliency map</li> <li>The false positive rate (FPR) is calculated as the proportion of no-fixated pixels in the thresholded saliency map.</li> <li>After calculating TPR and FPR at each threshold, the area under the curve (AUC) is calculated for the curve of TPR against FPR.</li> </ul>"},{"location":"Absolute%20Error/","title":"Absolute Error","text":"<ul> <li> \\[\\lvert y-f(x)\\rvert\\] </li> <li>Penalize large errors</li> </ul>"},{"location":"Acceleration/","title":"Acceleration","text":"<ul> <li>Change in Velocity wrt Time</li> <li> \\[a_{avg}= \\Delta v/\\Delta t\\] </li> </ul>"},{"location":"Action%20Component/","title":"Action Component","text":""},{"location":"Action%20Component/#action-component","title":"Action Component","text":"<ul> <li>reactively dispatches and monitors the execution of actions.</li> </ul>"},{"location":"Action%20Component/#backlinks","title":"Backlinks","text":"<ul> <li>Action Component</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Action%20Potential/","title":"Action Potential","text":"<ul> <li>Sometimes called a \u201cspike\u201d or described as a neuron \u201cfiring,\u201d an action potential occurs when there is a significant increase in the electrical activity along the membrane of a nerve cell. It is associated with neurons passing electrochemical messages down the axon, releasing neurotransmitters to neighboring cells in the synapse.</li> </ul>"},{"location":"Action%20Transitive%20verb/","title":"Action Transitive Verb","text":"<ul> <li>a verb has a direct object + verb</li> <li>I made her lower her head or body</li> </ul>"},{"location":"Activation%20Functions/","title":"Activation Functions","text":"<ul> <li>general rule \"which is better\"</li> <li>SELU &gt; Elu &gt; Leaky Relu &gt; Relu &gt; Tanh &gt; Sigmoid</li> </ul>"},{"location":"Active%20Compliant%20Robot/","title":"Active Compliant Robot","text":"<ul> <li>An active compliant robot is one in which motion modification during the performance of a task is initiated by the control system. The induced motion modification is slight, but sufficient to facilitate the completion of a desired task.</li> </ul>"},{"location":"Active%20tracking/","title":"Active Tracking","text":"<ul> <li>hz evaluated moment-to-moment  </li> <li>Is it already time to start preparing?</li> <li>Makes your models run much slower!</li> </ul>"},{"location":"Active%20tracking/#backlinks","title":"Backlinks","text":"<ul> <li>Cogntition Hazard Rates</li> <li>Active tracking</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Actuator/","title":"Actuator","text":"<ul> <li>A power mechanism used to effect motion, or maintain position of the robot (for example, a motor which converts Electrical Energy to effect motion of the robot) (R15.07). The actuator responds to a signal received from the control system.</li> </ul>"},{"location":"Acute/","title":"Acute","text":"<ul> <li>A condition that is often severe but starts and ends quickly</li> </ul>"},{"location":"AdaDelta/","title":"AdaDelta","text":"<ul> <li> \\[RMS[\\Delta \\theta]_{t}= \\sqrt{E[\\Delta \\theta^{2}]_{t}+ \\epsilon}$$ $$\\begin{align}\\\\ &amp; \\Delta \\theta_{t}= -\\frac{RMS[\\Delta \\theta]_{t-1}}{RMS[g]_{t}}g_{t}\\\\ &amp; \\theta_{t+1}= \\theta_{t}+ \\Delta \\theta_{t} \\end{align}\\] </li> </ul>"},{"location":"AdaIn/","title":"AdaIn","text":"<ul> <li>paper</li> <li> \\[AdaIN(x,y) = \\sigma(y) \\big( \\frac{x-\\mu(x)}{\\sigma (x)} \\big)\\] </li> <li>Adaptive Instance Normalization\u00a0is a normalization method that aligns the mean and variance of the content features with those of the style features.</li> <li>no learnable affine features</li> <li>Adaptively computes affine params from style input</li> </ul>"},{"location":"Adagrad/","title":"Adagrad","text":"<ul> <li>Past squared grads as scaling factor for learning rate</li> <li> \\[\\begin{align}&amp; g_{t,i} = \\nabla_\\theta J(\\theta_{t,i}) \\\\ &amp; \\theta_{t+1, i} = \\theta_{t,i} - \\eta \\cdot g_{t,i} \\\\ &amp; \\theta_{t+1, i} = \\theta_{t,i} - \\frac{\\eta}{\\sqrt{G_{t,ii} + \\epsilon}} \\cdot g_{t,i} \\end{align}\\] </li> <li>Doesnt forget past gradients</li> </ul>"},{"location":"Adam/","title":"Adam","text":"<ul> <li>Supervised learning</li> <li>Rmsprop + Momentum</li> <li>Corrects bias in exponentially weighted averages</li> <li>Struggles with large no of params -&gt; Over smooths the gradient</li> <li> \\[\\begin{align} &amp; s_n = \\rho_1 s_{n-1} + (1-\\rho_1) g_n \\\\ &amp; r_n = \\rho_2 r_{n-1} + (1-\\rho_2) g_n \\odot g_n \\\\ &amp; \\Theta_{n+1} = \\Theta_n - \\alpha \\frac{s_n}{\\epsilon + \\sqrt{r_n}} \\frac{1-\\rho_2^n}{1-\\rho^n_1} \\end{align}\\] </li> <li>First and second moments</li> </ul>"},{"location":"Adaptive%20Gradient%20Clipping/","title":"Adaptive Gradient Clipping","text":"<ul> <li>clips gradients to the ratio between weight gradient and weight value</li> <li>Clipping parameter is more robust than in traditional GC</li> <li>Swapping Batch Normalisation for AGC<ul> <li>faster training for equally sized models</li> <li>Allows for even larger batch size training</li> </ul> </li> </ul>"},{"location":"Adaptive%20Input%20Representation/","title":"Adaptive Input Representation","text":"<ul> <li>Adaptive Input Representations for Neural Language Modeling</li> <li>varying the size of input word embeddings for neural language modeling</li> <li>improve accuracy while drastically reducing the number of model parameters</li> <li>more than twice as fast to train than the popular character input CNN while having a lower number of parameters</li> <li>English Wikipedia</li> <li>Billion Word</li> </ul>"},{"location":"Adaptive%20Whitening%20Saliency/","title":"Adaptive Whitening Saliency","text":"<ul> <li>is based on the whitening of low-level features and has shown good performance for saliency map estimation.</li> <li>uses the features extracted by the model pre-trained for scene recognition.</li> </ul>"},{"location":"Adding%20noise/","title":"Noise","text":"<ul> <li>To training input data<ul> <li>Gaussian random noise</li> <li>Augmentation</li> </ul> </li> <li>While the algo runs<ul> <li>Dropout</li> </ul> </li> <li>Stochastic ensemble learning</li> </ul>"},{"location":"Additive%20Attention/","title":"Additive Attention","text":"<ul> <li>Bahdanau et al., 2015</li> <li>Uses a one layer feedforward network to calculate Attention Alignment</li> <li>Oh, basically it is the same as Bahdanau Attention</li> </ul>"},{"location":"Adjective/","title":"Adjective","text":"<ul> <li>Properties of objects</li> </ul>"},{"location":"Adrenal%20Glands/","title":"Adrenal Glands","text":"<ul> <li>Located on top of each kidney, these two glands are involved in the body\u2019s response to stress and help regulate growth, blood glucose levels, and the body\u2019s metabolic rate. They receive signals from the brain and secrete several different hormones in response, including cortisol and adrenaline.</li> </ul>"},{"location":"Adrenaline/","title":"Adrenaline","text":"<ul> <li>Also called epinephrine, this hormone is secreted by the adrenal glands in response to stress and other challenges to the body. The release of adrenaline causes a number of changes throughout the body, including the metabolism of carbohydrates to supply the body\u2019s energy demands and increased arousal or alertness.</li> </ul>"},{"location":"Advantages%20of%20Federated%20Learning/","title":"Advantages of Federated Learning","text":"<ul> <li>All your information is locally stored and is never sent anywhere</li> <li>Saves your personalized data from being leaked</li> <li>Removes all connections to you</li> <li>Allows the model to be updated and become better without compromizing on privacy</li> <li>Nobody \"owns\" your data except you</li> </ul>"},{"location":"Adverb/","title":"Adverb","text":"<ul> <li>Properties of verbs</li> <li>slowly, frequently, nally</li> </ul>"},{"location":"Adversarial%20Distillation/","title":"Adversarial Distillation","text":"<ul> <li>Furthermore, an effective intermediate supervision, i.e., the squeezed knowledge, was used by Shu et al. (2019) to mitigate the capacity gap between the teacher and the student.</li> <li>In the third category, adversarial knowledge dis- tillation is carried out in an online manner, i.e., the teacher and the student are jointly optimized in each it- eration (Wang et al., 2018e; Chung et al., 2020)</li> </ul>"},{"location":"Adversarial%20Learning/","title":"Adversarial Learning","text":"<ul> <li>Consider data in a Manifold. The PDF is concentrated along a low dim Manifold \\(\\mathcal{M}\\)</li> <li>Now the original picture is a point on the Manifold (dim = output layer size)</li> <li>Add noise to the image such that the image now appears to be in a direction orthogonal to \\(\\mathcal{M}\\) -&gt; value of PDF shrinks dramatically</li> <li>Then the network has never seen this before and will return a random classification</li> </ul>"},{"location":"Adversarial%20Loss/","title":"Adversarial Loss","text":"<ul> <li>We apply Adversarial Loss to both the Generators, where the Generator tries to generate the images of it's domain, while its corresponding discriminator distinguishes between the translated samples and real samples.</li> <li>Generator aims to minimize this loss against its corresponding Discriminator that tries to maximize it.</li> </ul>"},{"location":"Adversarial%20Loss/#backlinks","title":"Backlinks","text":"<ul> <li>CycleGAN</li> <li>Adversarial Loss</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Adversarial%20Spatial%20Dropout%20for%20Occlusion/","title":"Adversarial Spatial Dropout for Occlusion","text":"<ul> <li>Crops region pixels to generate hard positives for object detection by learning key image regions</li> <li>Within the proposed region only 1/3 pixels are dropped after sorting based on magnitudes</li> <li>Dropped values are non-contiguous here as compared to previously discussed methods</li> </ul>"},{"location":"Afferent/","title":"Afferent","text":"<ul> <li>Sensory Division</li> </ul>"},{"location":"Affine%20Function/","title":"Affine Function","text":"<ul> <li>b is a bias term which is padded at the end, size 1</li> <li>Function + bias</li> </ul>"},{"location":"Affordance%20Detection%20Task%20Specific/","title":"Affordance Detection Task Specific","text":"<ul> <li>Kokic, Mia, et al. \u201cAffordance Detection for Task-Specific Grasping using Deep Learning.\u201d Humanoids 2017.</li> <li></li> <li></li> <li>Affordance Score vs Contact Constraint</li> </ul>"},{"location":"Afib/","title":"Afib","text":"<ul> <li>Atrial fibrillation, irregular and rapid heartbeats</li> </ul>"},{"location":"Agglutinating%20words/","title":"Agglutinating Words","text":"<ul> <li>Words divide into smaller units with clear boundaries</li> <li>Compound words are hyphenated (as in English or not as in German)</li> <li>Nachkriegszeit ; Nichtraucher</li> <li>Single token words - end-of-line</li> <li>Multi-token words \u2013 Delhi-based</li> <li>String of Morphology Affix</li> </ul>"},{"location":"Akaike%20Information%20Criterion/","title":"Akaike Information Criterion","text":"<ul> <li>Considers goodness-of-fit to the data and penalizes complexity of the model</li> <li> \\[AIC=\u22122log\u2061(L)+2q\\] </li> <li>where:</li> <li>L: likelihood function for a particular model</li> <li>q: number of variables of this model</li> <li>If error terms \\(\\epsilon\\) follows Normal Distribution , expected value 0 + constant variance \\(\\(AIC = \\frac{1}{\\eta \\sigma^{2}}(RSS + 2p \\hat \\sigma^2)\\)\\)</li> </ul>"},{"location":"Aleatoric/","title":"Aleatoric","text":"<ul> <li>Uncertainty part of the data</li> <li>Sensor noise etc</li> <li>Simplest noise : additive noise \\(\\(f(x) = x^{3}+ \\epsilon\\)\\)</li> <li> \\[\\epsilon \\sim \\mathcal{N}(0, \\sigma^{2})\\] </li> <li>Homoscedatic</li> <li>Heteroscedatic</li> <li></li> </ul>"},{"location":"Alex%20Net/","title":"Alex Net","text":"<ul> <li>Dropout + Relu</li> <li>No of filters increase according to depth</li> <li></li> </ul>"},{"location":"Algebra%20Cognitive%20Tutor/","title":"Algebra Cognitive Tutor","text":"<ul> <li>Anderson's group (Anderson, Corbett, Koedinger, &amp; Pelletier, 1995), and has been extended and marketed by Carnegie Learning ( <li>Because it is so widely used and has undergone so many positive evaluations, it is arguably the most successful ITS in the world at this time.</li> <li>Inner loop: The inner loop monitors the student's steps while solving an algebra problem.</li> <li>Although most problems, including the one in Figure 2, involve using multiple representational tools (graphs, tables, etc.) to analyze a problem scenario, some problems focus only on specific tools, such as the equation solver. In Figure 2, the problem to be solved is shown in the upper left window. This problem has four parts, labeled 1 through 4. When the student began, the cells of the table in the worksheet window at the lower left were all empty. The student has filled every cell with a number, text or an algebraic formula. In the process of figuring out what to put in the cells, the student used the solver window (upper right) and the graphing window (lower right). Each time the student filled a table cell, plotted a point on the graph, entered an equation in the solver window, etc, the tutor gave immediate feedback that told the student whether the step was correct or incorrect.</li> <li>Outer loop: The outer loop in the Algebra I Algebra Cognitive Tutor selects an algebra problem for the student to do and makes sure that the student submits a solution. The tutor uses its fine-grained assessment to select a task that exercises a few knowledge components that the student has not yet mastered. When the student has mastered all the knowledge components in a unit (all the bars turn gold), the student is advanced to the next unit in the algebra curriculum.</li> <li>Step analysis: The tutor analyzes each student step in terms of a set of anticipated steps (S. Ritter, Blessing, &amp; Wheeler, 2003). The set of anticipated steps for a problem is precomputed by solving the problem in all acceptable ways by running a rule-based problem solver. The rules are written to correspond to knowledge components. Each step is associated with the rules that were used to generate it during the precomputation. During tutoring, the student's step is matched against these anticipated steps.</li> <li>When a student's step matches an anticipated step, the student is credited in the assessment with having applied the associated knowledge components.</li>"},{"location":"Allele/","title":"Allele","text":"<ul> <li>One of two or more varying forms of a gene due to genetic mutation.</li> <li>Differing alleles, which can be found at the same spot on a chromosome, produce variation in inherited characteristics such as hair color or blood type.</li> </ul>"},{"location":"Alleviating%20Class%20Imbalance%20with%20Data%20Augmentation/","title":"Alleviating Class Imbalance with Data Augmentation","text":"<ul> <li>Data Augmentation falls under a Data-level solution to class imbalance and there are many different strategies for implementation.</li> <li>A naive solution to oversampling with Data Augmentation would be a simple random oversampling with small geometric transformations such as a 30\u00b0 rotation</li> <li>One problem of oversampling with basic image transformations is that it could cause overfitting on the minority class which is being oversampled</li> <li>The biases present in the minority class are more prevalent post-sampling with these techniques.</li> <li>Neural Style Transfer is an interesting way to create new images. These new images can be created either through extrapolating style with a foreign style or by interpolating styles amongst instances within the dataset.</li> <li>Oversampling with GANs can be done using the entire minority class as 'real' examples, or by using subsets of the minority class as inputs to GANs</li> <li>The use of evolutionary sampling to find these subsets to input to GANs for class sampling is a promising area for future work.</li> </ul>"},{"location":"Allomorph/","title":"Allomorph","text":"<ul> <li>Variants of the same morpheme but cant be replaced by another</li> <li>Un \u2013 happy gives unhappy</li> <li>In-comprehensible gives incomprehensible</li> </ul>"},{"location":"Alpha%20Waves/","title":"Alpha Waves","text":"<ul> <li>9-14 Hz</li> <li>Drowsy/Inhibition</li> <li></li> </ul>"},{"location":"Alzheimer%E2%80%99s%20Disease/","title":"Alzheimer\u2019s Disease","text":"<ul> <li>A debilitating form of dementia, this progressive and irreversible neurodegenerative disease results in the development of protein plaques and tangles that damages neurons and interfere with neural signaling, ultimately affecting memory and other important cognitive skills</li> </ul>"},{"location":"Amdahl%27s%20Law/","title":"Amdahl's Law","text":"<ul> <li>Maximum expected improvement to an overall system when only part of the system is improved</li> <li>Theoretical maximum speedup using multiple processors</li> <li> \\[Speedup = \\frac{\\text{Execution time before improvement}}{\\text{Execution time after improvement}}\\] </li> <li> \\[Speedup = ((1-f_{E})+(\\frac{f_{E}}{f_{I}}))^{-1}\\] <ul> <li>\\(f_E\\) is fraction enhanced</li> <li>\\(f_{I}\\) is factor of improvement</li> <li>\\(S_{e}\\) is speedup enhanced</li> </ul> </li> <li> \\[ExecutionTime_{new} = ExecutionTime_{old}\\times [(1-f_{E})+f_{E}/S_{e}]\\] </li> <li> \\[MaximumSpeedupp = n/(1+(n-1)f )\\] <ul> <li>n is no of processors</li> </ul> </li> </ul>"},{"location":"Amino%20Acid/","title":"Amino Acid","text":"<ul> <li>A type of small organic molecule that has a variety of biological roles but is best known as the \u201cbuilding block\u201d of proteins.</li> </ul>"},{"location":"Amygdala/","title":"Amygdala","text":"<ul> <li>Part of the brain\u2019s limbic system, this primitive brain structure lies deep in the center of the brain and is involved in emotional reactions, such as anger or fear, as well as emotionally charged memories. It also influences behavior such as feeding, sexual interest, and the immediate \u201cfight or flight\u201d stress reaction that helps ensure the person\u2019s needs are met.</li> </ul>"},{"location":"Amyloid%20Plaque/","title":"Amyloid Plaque","text":"<ul> <li>The sticky, abnormal accumulations of amyloid-beta protein aggregate around neurons and synapses in the memory and intellectual centers of the brain, in people with Alzheimer\u2019s. These are sometimes referred to as neuritic plaques or senile plaques. While amyloid plaques have long been considered markers of Alzheimer\u2019s, they are also found to some extent in many cognitively normal elderly people. The plaques\u2019 role in Alzheimer\u2019s neurodegeneration remains unclear.</li> </ul>"},{"location":"Amyloid-beta%20%28A%CE%B2%29%20Protein/","title":"Amyloid-beta (A\u03b2) Protein","text":"<ul> <li>A naturally occurring protein in brain cells. Large, abnormal clumps of this protein form the amyloid plaques that are a physiological hallmark of Alzheimer\u2019s disease.</li> </ul>"},{"location":"Amyotrophic%20Lateral%20Sclerosis%20%28ALS%29/","title":"Amyotrophic Lateral Sclerosis (ALS)","text":"<ul> <li>Also known as Lou Gehrig\u2019s disease, this neurodegenerative disease results in the death of brain cells that control the muscles.</li> </ul>"},{"location":"Analysis%20of%20Explainers%20of%20Black%20Box%20Deep%20Neural%20Networks%20for%20Computer%20Vision%20A%20Survey/","title":"Analysis of Explainers of Black Box Deep Neural Networks for Computer Vision A Survey","text":"<ul> <li>Vanessa Buhrmester , David M\u00fcnch and Michael Arens</li> </ul>"},{"location":"Analysis%20of%20Explainers%20of%20Black%20Box%20Deep%20Neural%20Networks%20for%20Computer%20Vision%20A%20Survey/#explaining-systems","title":"Explaining Systems","text":"<ul> <li>GAM</li> <li>Partial Dependence Plot</li> <li>Salience Map</li> <li>Explanator</li> <li>Comprehensibility</li> <li>Causality</li> <li>Causability</li> <li>Bayesian Rule List</li> <li>TREPAN</li> </ul>"},{"location":"Analysis%20of%20Explainers%20of%20Black%20Box%20Deep%20Neural%20Networks%20for%20Computer%20Vision%20A%20Survey/#selected-dnn-explainers","title":"Selected DNN Explainers","text":"<ul> <li>Counterfactual Impact Evaluation</li> <li>DeconvNet</li> <li>Layerwise Relevance Propagation</li> <li>Parent Approximations</li> <li>RETAIn</li> <li>SP-LIME</li> <li>Deep Visual Explanation</li> <li>Prediction Difference Analysis</li> <li>Smooth-Grad</li> <li>Multimodal Explanation</li> <li>Summit</li> <li>DeepFool</li> <li>DeconvNet</li> <li>LIME</li> </ul>"},{"location":"Andes/","title":"Andes","text":"<ul> <li>Outer loop: The Andes physics tutoring system (http://www.andes.pitt.edu/) helps students learn how to solve physics problems (K. VanLehn et al., 2005; K. VanLehn et al., 2002).</li> <li>Inner loop: The student solves the problem by making steps similar to the ones they would make if solving the problem with pencil and paper. One kind of step is to type an equation into one of the numbered boxes in the right window. Another kind of step is to draw a Cartesian coordinate system, such as the one showing in the lower left. A third kind of step is to sketch a vector, then fill out a dialogue box that defines it. A vector-drawing operation was in progress at the time the screen shot was taken, so its dialogue box covers part of the screen.</li> <li>Every time the student makes a step, Andes gives immediate feedback. For most types of steps, Andes merely colors the step green if it is correct and red if it is incorrect.</li> <li>Step analysis: Like the Algebra Cognitive Tutor, Andes analyzes non-equation steps by precomputing anticipated steps and matching the student's step against them</li> <li>However, this method does not work for the equation steps because there are too many anticipated equations to generate</li> </ul>"},{"location":"Angina/","title":"Angina","text":"<ul> <li>Intermittent chest pain normally caused by insufficient blood flow to the heart</li> </ul>"},{"location":"Angiography/","title":"Angiography","text":"<ul> <li>A medical imaging technique that allows clinicians to visualize the interior of blood vessels, arteries, veins, and the heart.</li> </ul>"},{"location":"Anthropomorphic/","title":"Anthropomorphic","text":"<ul> <li>Human like in shape</li> <li>Very large number of joint</li> <li>Very large number of DOF</li> <li>Compact design</li> </ul>"},{"location":"Apoptosis/","title":"Apoptosis","text":"<ul> <li>A form of programmed cell death that occurs as part of normal growth and development.</li> </ul>"},{"location":"Appendectomy/","title":"Appendectomy","text":"<ul> <li>Surgical procedure to remove the appendix</li> </ul>"},{"location":"Application%20dependence/","title":"Application Dependence","text":"<ul> <li>Word and sentence segmentation are necessary</li> <li>Tokenizer</li> </ul>"},{"location":"Applications%20of%20Knowledge%20Distillation/","title":"Applications of Knowledge Distillation","text":"<ul> <li>Specifically, in (Luo et al., 2016), the knowledge from the chosen informative neurons of top hint layer of the teacher network is trans- ferred into the student network</li> <li>A recursive knowledge distillation method was designed by using a previous student network to ini- tialize the next one (Yan et al., 2019). Since most face recognition methods perform the open-set recognition, i.e., the classes/identities on test set are unknown to the training set, the face recognition criteria are usually distance metrics between feature representations of positive and negtive samples, e.g., the angular loss in (Duong et al., 2019) and the correlated embedding loss in (Wu et al., 2020).</li> <li>Specifically, Ge et al. (2018) proposed a selective knowledge distillation method, in which the teacher network for high-resolution face recognition selectively transfers its informative facial features into the student network for low-resolution face recognition through sparse graph optimization. In (Kong et al., 2019), cross- resolution face recognition was realized by designing a resolution invariant model unifying both face halluci- nation and heterogeneous recognition sub-nets. To get efficient and effective low resolution face recognition model, the multi-kernel maximum mean discrepancy between student and teacher networks was adopted as the feature loss (Wang et al., 2019c).</li> <li>KD-based face recognition can be extended to face alignment and verification by changing the losses in knowledge distillation (Wang et al., 2017).</li> <li>For incomplete, ambiguous and redundant image labels, the label refinery model through self-distillation and label progression is proposed to learn soft, informative, collective and dynamic labels for complex image classi- fication (Bagherinezhad et al., 2018).</li> <li>To address catas- trophic forgetting with CNN in a variety of image clas- sification tasks, a learning without forgetting method for CNN, including both knowledge distillation and lifelong learning is proposed to recognize a new image task and to preserve the original tasks (Li and Hoiem, 2017).</li> <li>For improving image classification accuracy, Chen et al. (2018a) proposed the feature maps-based knowledge distillation method with GAN.</li> <li>Similar to the KD-based low-resolution face recognition, Zhu et al. (2019) proposed deep feature distillation for the low-resolution image classification, in which the output features of a student match that of teacher.</li> <li>Gordon and Duh (2019) explained the good perfor- mance of sequence-level knowledge distillation from the perspective of data augmentation and regulariza- tion. In (Kim and Rush, 2016), the effective word-level knowledge distillation is extended to the sequence- level one in the sequence generation scenario of NMT. The sequence generation student model mimics the sequence distribution of the teacher. To overcome the multilingual diversity, Tan et al. (2019) proposed multi teacher distillation, in which multiple individual models for handling bilingual pairs are teacher and a mul- tilingual model is student.</li> <li>To improve the transla- tion quality, an ensemble of mutiple NMT models as teacher supervise the student model with a data filtering method Freitag et al. (2017).</li> <li>(Wei et al., 2019) proposed a novel online knowledge distillation method, which addresses the unstableness of the training process and the decreasing performance on each validation set.</li> <li>The proposed pre- trained distillation performs well in sentiment classifi- cation, natural language inference, textual entailment. For a multi-task distillation in the context of natu- ral language understanding, Clark et al. (2019) pro- posed the single-multi born-again distillation, which is based on born-again neural networks (Furlanello et al., 2018).</li> <li>In (Perez et al., 2020), a audio-visual multi- modal knowledge distillation method is proposed. knowl- edge is transferred from the teacher models on visual and acoustic data into a student model on audio data.</li> <li>Pan et al. (2019) designed a enhanced collaborative denoising autoencoder (ECAE) model for recommender systems via knowledge distillation to capture useful knowledge from user feedbacks and to reduce noise. The unified ECAE framework contains a generation network, a retraining network and a distillation layer that trans- fers knowledge and reduces noise from the generation network.</li> </ul>"},{"location":"Approximately%20Compositional%20Semantic%20Parsing/","title":"Approximately Compositional Semantic Parsing","text":"<ul> <li>which Semantic Analysis processing is applied to the result of performing a syntactic parse</li> </ul>"},{"location":"Arbitrary%20Relation%20Bias/","title":"Arbitrary Relation Bias","text":"<ul> <li>To solve problems related to a group of things or people, it might be more informative to see them as a Graphs. The graph structure imposes arbitrary relationships between the entities, which is ideal when there\u2019s no clear sequential or local relation in the model:</li> <li></li> </ul>"},{"location":"Area%20Minimization/","title":"Area Minimization","text":"<ul> <li>Small areas preferable</li> <li>aspect ratio can play role</li> <li></li> </ul>"},{"location":"Articulated%20Manipulator/","title":"Articulated Manipulator","text":"<ul> <li>A manipulator with an arm that is broken into sections (links) by one or more joints. Each of the joints represents a degree of freedom in the manipulator system and allows translation and rotary motion.</li> </ul>"},{"location":"Articulation/","title":"Articulation","text":"<ul> <li>Describes a jointed device, such as a jointed manipulator. The joints provide rotation about a vertical axis, and elevation out of the horizontal plane. This allows a robot to be capable of reaching into confined spaces.</li> </ul>"},{"location":"Assembly%20Robot/","title":"Assembly Robot","text":"<ul> <li>A robot designed specifically for mating, fitting, or otherwise assembling various parts or components into completed products. Primarily used for grasping parts and mating or fitting them together, such as in assembly line production.</li> </ul>"},{"location":"Astrocyte/","title":"Astrocyte","text":"<ul> <li>A star-shaped glial cell that supports neurons, by helping to both feed and remove waste from the cell, and otherwise modulates the activity of the neuron. Astrocytes also play critical roles in brain development and the creation of synapses.</li> </ul>"},{"location":"Asymptotic%20Decider/","title":"Asymptotic Decider","text":"<ul> <li>Consider the bilinear interpolant within cell</li> <li>the true isolines within a cell are hyperbolas</li> <li>investigate order of intersection points along x or y axis</li> <li>build pairs of first two and last two intersections</li> </ul>"},{"location":"Attention%20Alignment/","title":"Attention Alignment","text":"<ul> <li>If there are sequences \\(x, y\\)<ul> <li>Encoder is any Recurrent with a forward state \\(\\(\\overrightarrow h^{T}\\)\\) and \\(\\(\\overleftarrow h^{T}\\)\\) for backward</li> <li>Concat them represents the preceding and following word annotations<ul> <li>\\(\\(h_{i}= [\\overrightarrow h_{i}^{T}; \\overleftarrow h_{i}^{T}]\\)\\), \\(i = 1, \u2026, n\\)</li> <li>Decoder has hidden state \\(s_{t}= f(s_{t-1}, y_{t-1}, c_{t})\\) for the output word at position t for \\(t = 1, \u2026, m\\)<ul> <li>Context vector \\(c_{t}\\) is a sum of hidden states of the input seq, weighted by alignment scores</li> <li> \\[c_{t}= \\Sigma_{i=1}^{n}\\alpha_{t,i}h_{i}\\] </li> <li>How well the two words are aligned is given by</li> <li> \\[\\alpha_{t,i} = align(y_{t}, x_{i})\\] </li> <li>Taking softmax<ul> <li> \\[\\frac{exp(score(s_{t-1}, h_{i}))}{\\Sigma_{i'-1}^{n}exp(score(s_{t-1}, h_{i}'))}\\] </li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> <li> \\[f_{att}(h_{i}, s_{j}) = v_{a}^{T}tanh(W_{a}[h_{i};s_{j}])\\] </li> <li>\\(v_{a}\\) and \\(W_{a}\\) are the learned Attention params</li> <li>\\(h\\) is the hidden state for the encoder</li> <li>\\(s\\) is the hidden state for the decoder</li> <li>Matrix of alignment<ul> <li></li> <li>Final scores calculated with a Softmax</li> </ul> </li> </ul>"},{"location":"Attention%20Based%20Distillation/","title":"Attention Based Distillation","text":"<ul> <li>That is to say, knowledge about feature embedding is transferred using attention map functions. Unlike the attention maps, a different attentive knowledge distillation method was proposed by Song et al. (2018). An attention mechanism is used to assign different confidence rules (Song et al., 2018).</li> </ul>"},{"location":"Attention%20NMT/","title":"Attention NMT","text":"<ul> <li>Effective Approaches to Attention-based Neural Machine Translation</li> <li> proposed an Attention mechanism to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation</li> <li>a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time</li> <li>BLEU</li> </ul>"},{"location":"Attention/","title":"Attention","text":"<ul> <li>Model can decide where to look in the input</li> <li>Self Attention</li> <li>Additive Attention</li> <li>Dot Product Attention</li> <li>Location Aware Attention</li> <li>Relative Multi Head Self Attention</li> <li>Soft Attention</li> <li>Scaled Dot Product Attention</li> <li>Encoder Decoder Attention</li> <li>Multi Head Attention</li> <li>Strided Attention</li> <li>Fixed Factorization Attention</li> <li>Sliding Window Attention</li> <li>Dilated Sliding Window Attention</li> <li>Global and Sliding Window Attention</li> <li>Content Based Attention</li> <li>Location Base Attention</li> <li>Mixed chunk attention</li> </ul>"},{"location":"Attentions%20and%20salience/","title":"Attentions and Salience","text":"<ul> <li>Learning is related to attention</li> <li>We give more attention to salient items</li> <li>Selective attention leads to overshadowing</li> <li>Two cues presented together jointly predict outcomes</li> <li>salience (and then attention) leads to one cue being strongly associated</li> <li>ther cue is only weakly associated (overshadowed)</li> <li>Overshadowing leads to blocking</li> <li>Blocking could model interference</li> <li>Children learn temporal adverbs like hier late (Dale and Fenson 1996)</li> <li>L2 learners go through phases where time is marked with adverbials alone (Bardovi-Harlig 1992; Meisel 1987)</li> <li>This seems to block acquisition of other cues</li> <li>temporal adverbs are highly salient (and easier) so they get stronger associations</li> <li>even though they co-occur with different verb forms, these are hard to learn</li> </ul>"},{"location":"Attentive%20CutMix/","title":"Attentive CutMix","text":"<ul> <li>builds up on CutMix.</li> <li>Instead of random pasting, it identifies attentive patches for cutout and pastes them at the same location in the other image.</li> <li>avoids the problem of selecting a background region not important for the network and updating the label information</li> <li>A separate pre-trained network is employed to extract attentive regions.</li> <li>The attention output is mapped back onto the original image</li> </ul>"},{"location":"AttributeMix/","title":"AttributeMix","text":"<ul> <li>augments images based on se- mantically extracted image attributes</li> <li>Image is divided into a grid of patches where highly activated six responses are pasted onto the training image. These image pairs are selected randomly in every training iteration.</li> <li>attribute classifier by extracting k attributes (e.g., leg, head, and wings of a bird) from each image.</li> <li>The attribute mining procedure for every image is performed repetitively k times, whereas for each iteration, an attribute is masked out from the original image based on the most discriminative region in the attention map.</li> <li>attribute-level classifier is trained to generate new images for the actual classification model.</li> </ul>"},{"location":"AudioSet%20classification/","title":"AudioSet Classification","text":""},{"location":"AugMix/","title":"AugMix","text":"<ul> <li>using the input image itself</li> <li>t transforms (translate, shear, rotate and etc) the input image and mixes it with the original image</li> <li>Image transformation involves series of randomly selected augmentation operations applied with three parallel augmentation chains.</li> <li>Each chain has a composition of operations that could involve applying, for example, translation on input image followed by shear and so on</li> <li>The output of these three chains is three images mixed to form a new image.</li> <li>This new image is later mixed with the original image to generate the final augmented output image,</li> </ul>"},{"location":"Augmented%20Random%20Search/","title":"Augmented Random Search","text":"<ul> <li>The authors point out that the sub-policies learned from AutoAugment are inherently flawed because of the discrete search space.</li> <li>They convert the probability and magnitude of augmentations into a continuous space and search for sub-policies with ARS.</li> </ul>"},{"location":"Auto%20Encoders/","title":"AutoEncoder","text":"<ul> <li>Regression by predicting a reconstruction of the data</li> <li>Encoder \\(\\(E : \\mathscr{X} \\rightarrow \\mathscr{F}\\)\\)</li> <li>Decoder \\(\\(\\mathscr{F} \\rightarrow \\mathscr{D}\\)\\)</li> <li>$\\(E_\\theta, D_\\theta = argmin_{E_\\theta, D\\theta}||X-D(E(X))||^2\\)<ul> <li>Learn using Gradient Descent gradients</li> </ul> </li> <li>Compressed rep of data -&gt; Good for Classification or Regression</li> <li>MSE : Unsupervised</li> </ul>"},{"location":"Auto%20Encoders/#difficulties","title":"Difficulties","text":"<ul> <li>dim \\(\\mathscr{F} \\lt \\mathscr{X}\\)<ul> <li>Cannot learn the identity function</li> </ul> </li> <li>usages<ul> <li>data compression / dimensionality reduction</li> <li>encoder to obtain features (use the latent variable as feature)</li> <li>denoising autoencoders<ul> <li>input noisy image and try to obtain image without noise</li> </ul> </li> <li>sparse auto-encoder</li> <li>contractive autoencoder</li> </ul> </li> </ul>"},{"location":"Auto%20Encoders/#types","title":"Types","text":"<ul> <li>Denoising Autoencoder</li> <li>VAE</li> </ul>"},{"location":"AutoAugment/","title":"AutoAugment","text":"<ul> <li>developed by Cubuk et al.</li> <li>much different approach to meta-learning than Neural Augmentation or Smart Augmentation</li> <li>AutoAugment is a Reinforcement Learning algorithm that searches for an optimal augmentation policy amongst a constrained set of geometric transformations with miscellaneous levels of distortions. For example, \u2018translateX 20 pixels\u2019 could be one of the transformations in the search space</li> <li>In Reinforcement Learning algorithms, a policy is analogous to the strategy of the learning algorithm. This policy determines what actions to take at given states to achieve some goal. The AutoAugment approach learns a policy which consists of many subpolicies, each sub-policy consisting of an image transformation and a magnitude of transformation</li> <li>Reinforcement Learning is thus used as a discrete search algorithm of augmentations.</li> </ul>"},{"location":"AutoAugment/#backlinks","title":"Backlinks","text":"<ul> <li>RandAugment</li> <li>RandAugment is able to achieve comparable or better performance compared to other automated augmentation methods, such as AutoAugment, without the need for a separate proxy task.</li> <li>Conversely, a policy learned on a proxy task (such as AutoAugment) provides a fixed distortion magnitude (Figure 1b, dashed line) for all architectures that is clearly sub-optimal.</li> <li>In previous work, scaling learned data augmentation to larger dataset and models have been a notable obstacle. For example, AutoAugment and Fast AutoAugment could only be optimized for small models on reduced subsets of data</li> <li> <p>In previous work, scaling learned data augmentation to larger dataset and models have been a notable obstacle. For example, AutoAugment and Fast AutoAugment could only be optimized for small models on reduced subsets of data</p> </li> <li> <p>Augmented Random Search</p> </li> <li> <p>The authors point out that the sub-policies learned from AutoAugment are inherently flawed because of the discrete search space.</p> </li> <li> <p>KeepAugment</p> </li> <li>Keep- Augment identifies the salient area in an image and assures the image generated by the augmenta- tion strategies, for example, Cutout, RandAugment [14], CutMix [82] or AutoAugment [13], contains salient region in it.</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"AutoDistill/","title":"AutoDistill","text":"<ul> <li>AutoDistill: an End-to-End Framework to Explore and Distill Hardware-Efficient Language Models</li> <li>(NLP) tasks but they are expensive to serve due to long serving latency and large memory usage</li> <li>compress these models, knowledge distillation</li> <li>handling fast evolving models, considering serving performance, and optimizing for multiple objectives.</li> <li>end-to-end model distillation framework integrating model architecture exploration and multi-objective optimization for building hardware-efficient NLP pre-trained models</li> <li>Bayesian Optimization to conduct multi-objective Neural Architecture Search for selecting student model architectures</li> <li>proposed search comprehensively considers both prediction accuracy and serving latency on target hardware</li> <li>TPUv4i</li> <li>MobileBERT</li> <li>GLUE</li> <li>higher than BERT_BASE, DistillBERT, TinyBERT, NAS-BERT, and MobileBERT</li> </ul>"},{"location":"AutoTutor/","title":"AutoTutor","text":"<ul> <li>Outer loop: AutoTutor (http://demo.autotutor.org/) teaches by engaging students in a natural language (English) dialogue</li> <li>For AutoTutor, a task corresponds to a single question, such as the one shown in the upper right of Figure 4, that has a complex answer. Its outer loop consists of selecting such a question and working with the student to get it completely answered.</li> <li>Inner loop: The inner loop starts with the student typing in an initial answer to the top level question (see Figure 4; the student types into the lower right window; the whole dialogue is displayed in the lower left window).</li> <li>AutoTutor has been used to compare output modalities.</li> <li>An AutoTutor dialogue is composed of tutor turns alternating with student turns. On most of the student turns, the student makes a small contribution toward completing the whole task. Those student turns count as steps, because they are a user interface event that contributes to a solution of the whole task</li> <li>Step analysis:</li> <li>These are conclusions that are produced by applying knowledge components. For instance, the first two items above correspond to distinct learning events, wherein the student has applied the same Knowledge Component,</li> <li>In addition to having a list of all anticipated correct learning events, such as the ones mentioned above, AutoTutor has a list of several of the most important incorrect learning events</li> <li>To find out which learning events underlie the student's step, AutoTutor measures the semantic similarity between the text of the Learning Event and the text of the step. It uses a measure called Latent Semantic Analysis</li> </ul>"},{"location":"Automation%20Bias/","title":"Automation Bias","text":"<ul> <li>When a human decision maker favors recommendations made by an automated decision-making system over information made without automation, even when the automated decision-making system makes errors.</li> </ul>"},{"location":"Autonomic/","title":"Autonomic","text":"<ul> <li>Involuntary</li> <li>Sympathetic + Parasympathetic</li> </ul>"},{"location":"Autoregressive/","title":"Autoregressive","text":"<ul> <li>predict the future by past of TIme Series</li> <li>Multi Variate AR</li> </ul>"},{"location":"Autoregressive/#backlinks","title":"Backlinks","text":"<ul> <li>Generative vs Discriminative Models</li> <li>Autoregressive</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Average%20Filter/","title":"Average Filter","text":"<ul> <li>Each grey value is replaced by the average value in the kernel in a local surrounding</li> <li>linear</li> <li>flatten edges</li> </ul>"},{"location":"Average%20Number%20of%20Stored%20Instances%20per%20Category/","title":"Average Number of Stored Instances per Category","text":""},{"location":"Average%20Number%20of%20Stored%20Instances%20per%20Category/#average-number-of-stored-instances-per-category","title":"Average Number of Stored Instances per Category","text":"<ul> <li>memory resource required for learning</li> </ul>"},{"location":"Average%20Number%20of%20Stored%20Instances%20per%20Category/#backlinks","title":"Backlinks","text":"<ul> <li>Average Number of Stored Instances per Category</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Axon%20Terminal/","title":"Axon Terminal","text":"<ul> <li>The very end of the axon, where electrochemical signals are passed through the synapse to neighboring cells by means of neurotransmitters and other neurochemicals. A collection of axons coming from, or going to, a specific brain area may be called a white matter fiber tract.</li> </ul>"},{"location":"Axon/","title":"Axon","text":"<ul> <li>A long, single nerve fiber that transmits messages, via electrochemical impulses, from the body of the neuron to dendrites of other neurons, or directly to body tissues such as muscles.</li> </ul>"},{"location":"BART/","title":"BART","text":"<ul> <li>BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</li> <li>denoising autoencoder</li> <li>pretraining sequence-to-sequence</li> <li>trained by corrupting text with an arbitrary noising function, and learning a model to reconstruct the original text</li> <li>generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder),</li> <li>finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token</li> <li>With BERT, random tokens are replaced with masks, and the document is encoded bidirectionally. Missing tokens are predicted independently, so BERT cannot easily be used for generation.</li> <li>With GPT, tokens are predicted auto-regressively (generation of a new token is conditioned on the prior tokens), meaning GPT can be used for generation.</li> <li>noising schemes to an input document and thus corrupts it by replacing spans of text with mask symbols</li> <li>effective when finetuned for text generation but also works well for comprehension tasks</li> <li>matches the performance of RoBERTa with comparable training resource</li> <li>GLUE</li> <li>SQuAD</li> </ul>"},{"location":"BCE%20with%20Logits/","title":"BCE Logits","text":"<ul> <li>Cross Entropy + logits \\(\\(\\left( - \\mathrm{sum}\\left( y \\cdot \\mathrm{logsoftmax}\\left( \u0177 \\right) \\cdot weight \\right) \\right) \\cdot \\mathrm{//}\\left( 1, \\mathrm{size}\\left( y, 2 \\right) \\right)\\)\\)</li> </ul>"},{"location":"BCE%20with%20Logits/#_1","title":"\u2026","text":""},{"location":"BERT/","title":"BERT","text":"<ul> <li>Bidirectional Encoder rep from transformers</li> <li>Uses Token Embedding</li> <li>Self Supervised</li> <li>Masked language modeling, next sentence prediction</li> <li></li> <li>[CLS] : start of classification task, [SEP] between sentences, [MASK] : masked token</li> <li> <p>christianversloot #Roam-Highlights</p> <ul> <li>BERT base \\(\\text{BERT}_\\text{BASE}\\), which has 12 Encoder Segments stacked on top of each other, has 768-dimensional intermediate state, and utilizes 12 attention heads (with hence 768/12 = 64-dimensional attention heads).</li> <li>BERT large (\\(\\text{BERT}_\\text{LARGE}\\)), which has 24 Encoder Segments, 1024-dimensional intermediate state, and 16 attention heads (64-dimensional attention heads again).</li> <li>BERT utilizes the encoder segment, meaning that it outputs some vectors \\(T_i\\) for every token. The first vector, \\(T_0\\), is also called \\(C\\) in the BERT paper: it is the \"class vector\" that contains sentence-level information (or in the case of multiple sentences, information about the sentence pair). All other vectors are vectors representing information about the specific token.</li> <li>In other words, structuring BERT this way allows us to perform sentence-level tasks and token-level tasks. If we use BERT and want to work with sentence-level information, we build on top of the \\(C\\) token.</li> <li> <p>Masked Language Modeling</p> </li> <li> <p>Next Sentence Prediction (NSP)</p> <ul> <li>This task ensures that the model learns sentence-level information. It is also really simple, and is the reason why the BERT inputs can sometimes be a pair of sentences. NSP involves textual entailment, or understanding the relationship between two sentences.</li> <li>Constructing a training dataset for this task is simple: given an unlabeled corpus, we take a phrase, and take the next one for the 50% of cases where BERT has a next sentence. We take another phase at random given A for the 50% where this is not the case (Devlin et al., 2018). This way, we can construct a dataset where there is a 50/50 split between 'is next' and 'is not next' sentences.</li> </ul> </li> <li>BooksCorpus</li> <li>English Wikipedia</li> <li>It thus does not matter whether your downstream task involves single text or text pairs: BERT can handle it.<ul> <li>Sentence pairs in paraphrasing tasks.</li> <li>Hypothesis-premise pairs in textual entailment tasks.</li> <li>Question-answer pairs in question answering.</li> <li>Text-empty pair in text classification.</li> </ul> </li> <li>Yes, you read it right: sentence B is empty if your goal is to fine-tune for text classification. There simply is no sentence after the token.</li> <li>Fine-tuning is also really inexpensive</li> </ul> </li> </ul>"},{"location":"BEiT/","title":"BEiT","text":"<p>title: BEiT</p> <p>tags: architecture</p>"},{"location":"BEiT/#beit","title":"BEiT","text":"<ul> <li>BEiT: BERT Pre-Training of Image Transformers<ul> <li>Self Supervised pre-trained representation model</li> <li>Bidirectional Encoder Decoder Attention representations from Vision Transformer</li> <li>masked image modeling task to pretrain vision Transformers</li> <li>each image has two views in their pre-training</li> <li>the embeddings of which are calculated as linear projections of flattened patches</li> <li>visual tokens</li> <li>discrete VAE (dVAE) which acts as an \u201cimage tokenizer\u201d learnt via autoencoding-style reconstruction</li> <li>input image is tokenized into discrete visual tokens obtained by the latent codes of the discrete VAE</li> <li>proposed method is critical to make BERT like pre-training (i.e., auto-encoding with masked input) work well for image Transformers</li> <li>automatically acquired knowledge about semantic regions, without using any human-annotated data</li> <li>randomly masks some image patches and feeds them into the backbone Transformer</li> <li>pre-training objective is to recover the original visual tokens based on the corrupted image patches</li> <li>directly fine-tune the model parameters on downstream tasks by appending task layers upon the pretrained encoder</li> <li>ImageNet</li> <li>outperforming from-scratch DeiT</li> </ul> </li> </ul>"},{"location":"BLEU/","title":"BLEU","text":""},{"location":"BOLD/","title":"BOLD","text":"<ul> <li>Blood oxygenation level dependant signal<ul> <li>Indirect measure of neural activity</li> <li>Blood goes to a place</li> </ul> </li> <li>When neurons fire or increase their firing rate, they draw on oxygen and various nutrients.</li> <li>The circulatory system of the brain reacts by sending the region that just fired more highly-oxygenated blood than is needed. This results in an increased blood oxygen level in the activated region.</li> <li>With right pulse sequence, an MRI scanner is able to detect this difference in blood oxygen level</li> <li>Factors such as drugs, substances \u00a0and excitation \u00a0have been shown to increase BOLD response. Conversely, age and brain pathology \u00a0have been shown to decrease BOLD response</li> </ul>"},{"location":"BUCC/","title":"BUCC","text":""},{"location":"BYOL%20Loss/","title":"BYOL Loss","text":"<p>title: BYOL Loss</p> <p>tags: loss</p>"},{"location":"BYOL%20Loss/#byol-loss","title":"BYOL Loss","text":"<ul> <li>Similarity loss between \\(q_\\theta (z_\\theta)\\) and \\(sg(z^{'}_{\\xi})\\)</li> <li>\\(\\theta\\) is trained weights</li> <li>\\(\\xi\\) is exponentially moving average of \\(\\theta\\) and sg is stop gradient</li> <li>\\(f_\\theta\\) is discarded, \\(y_\\theta\\) is used as image representation</li> <li></li> </ul>"},{"location":"BYOL/","title":"BYOL","text":"<p>title: BYOL</p> <p>tags: architecture</p>"},{"location":"BYOL/#byol","title":"BYOL","text":"<ul> <li>Bootstrap Your Own Latent: a New Approach to Self-supervised Learning<ul> <li>Self Supervised image representation learning</li> <li>predicting previous versions of its outputs, without using negative pairs</li> <li>two neural networks, referred to as online and target networks</li> <li>that interact and learn from each other</li> <li>From an augmented view of an image, they train the Online Learning network to predict the target network representation of the same image under a different augmented view</li> <li>update the target network with a slow-moving average of the online network</li> <li>ImageNet</li> <li>Res Net</li> <li>dependent on existing sets of Augmentation that are specific to vision applications</li> <li>BYOL Loss</li> </ul> </li> </ul>"},{"location":"Back%20To%20Front%20Raycasting/","title":"Back To Front Raycasting","text":"<ul> <li>blending over operator for semi-transparent geometry</li> </ul>"},{"location":"Backprop/","title":"Backprop","text":"<ul> <li>Gradient (\\(\\nabla l(\\theta) = [\\frac{\\partial l}{\\partial \\theta_1}(\\theta) , \u2026 , \\frac{\\partial L}{\\partial \\theta_L}(\\theta)]\\)\\)<ul> <li>partial derivs of the loss wrt weights  Forward pass<ul> <li>Store result of operation in u</li> </ul> </li> <li>Backward Pass<ul> <li>Traverse the graph backwards<ul> <li>Chain Rule : \\(\\(\\frac{dl}{d\\theta_i} = \\Sigma_{k \\in parents(l)} \\frac{\\partial l}{\\partial u_k} \\frac{\\partial u_k}{\\partial \\theta_i}\\)\\)</li> <li> \\[\\begin{align} &amp;\\frac{d\\hat y}{d\\mathbf{W_1}}\\\\ &amp;= \\frac{\\partial \\hat y}{\\partial u_2} \\frac{\\partial u_2}{\\partial h_1} \\frac{\\partial h_1}{\\partial u_1} \\frac{\\partial u_1}{\\partial \\mathbf{W_1}} \\\\ &amp;= \\frac{\\partial \\sigma (u2)}{\\partial u_2} \\frac{\\partial \\mathbf{W}^T_2 h_1}{\\partial h_1} \\frac{\\partial \\sigma (u1)}{\\partial u_1} \\frac{\\partial \\mathbf{W}^T_1 x}{\\partial \\mathbf{W}_1} \\end{align}\\] </li> <li>Collecting all the (\\(\\partial \\sigma(u_i)\\)\\) wrt params -&gt; #gradients exponentially decreases wrt depth of the network : Vanishing<ul> <li>Solved by Activation Functions</li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"Bag%20of%20Words%20robotics/","title":"Bag of Words Robotics","text":""},{"location":"Bag%20of%20Words%20robotics/#bag-of-words-robotics","title":"Bag of Words Robotics","text":"<ul> <li>Recognizing objects using local descriptors would be computationally expensive.</li> <li>The number of local features for a given object mainly depends on the size of the object, and therefore, varies for different objects.</li> <li>The key idea for fast 3D object recognition is to use mechanisms for representing objects in a compact and uniform format (e.g., histogram).</li> <li>If we represent objects in a uniform format, then we can apply ML algorithms</li> <li>Compute local features for all the discovered objects and make a pool of features.</li> <li>A dictionary is generated via clustering of the pool of features into N clusters (the number of the clusters is the codebook size).</li> <li>Visual word are then defined as the centres of the extracted clusters.</li> <li>Finally, each object is described (abstracted) by a histogram of occurrences of these visual words.</li> <li></li> <li></li> </ul>"},{"location":"Bag%20of%20Words%20robotics/#backlinks","title":"Backlinks","text":"<ul> <li>Bag of Words robotics</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Bag%20of%20n-grams/","title":"Bag of N-grams","text":"<ul> <li>consider word phrases of length n to represent documents as fixed-length vectors to capture local word order</li> <li>suffer from data sparsity and high dimensionality.</li> </ul>"},{"location":"Bag%20of%20n-grams/#backlinks","title":"Backlinks","text":"<ul> <li>Bag of Words</li> <li>Somewhat solved by Bag of n-grams</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Bag%20of%20words/","title":"Bag of Words","text":""},{"location":"Bag%20of%20words/#explained","title":"Explained","text":"<ul> <li>Count based conversion of document into fixed length vectors of integers</li> <li><code>John likes to watch movies. Mary likes movies too.</code><ul> <li><code>[1, 2, 1, 1, 2, 1, 1, 0, 0, 0, 0]</code></li> </ul> </li> <li><code>John also likes to watch football games. Mary hates football.</code><ul> <li><code>[1, 1, 1, 1, 0, 1, 0, 1, 2, 1, 1]</code></li> </ul> </li> <li>Order is arbitary</li> </ul>"},{"location":"Bag%20of%20words/#disadvantages","title":"Disadvantages","text":"<ul> <li>Lose all info about word order</li> <li>Does not learn meaning of the words, so distance isnt very accurate</li> <li>Somewhat solved by Bag of n-grams</li> <li>Curse Of Dimensionality</li> </ul>"},{"location":"Bag%20of%20words/#backlinks","title":"Backlinks","text":"<ul> <li>CBOW</li> <li>Continous implementation of Bag of words</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Bahdanau%20Attention/","title":"Bahdanau Attention","text":"<ul> <li>Neural Machine Translation by Jointly Learning to Align and Translate<ul> <li>Attention mechanism (borrowed from the field of information retrieval) within the context of NLP</li> </ul> </li> <li>Same as Additive Attention</li> <li></li> </ul>"},{"location":"Barycentric%20Interpolation/","title":"Barycentric Interpolation","text":"<ul> <li>d+1 points</li> <li>Point x is an Affine Function of \\(x_i\\)</li> </ul>"},{"location":"Basal%20Ganglia/","title":"Basal Ganglia","text":"<ul> <li>includes the caudate, putamen and globus pallidus. These nuclei work with the cerebellum to coordinate fine motions, such as fingertip movements.</li> <li>Hypothalamus</li> <li>Pituitary gland</li> <li>Pineal gland</li> <li>Thalamus</li> <li>Limbic system</li> </ul>"},{"location":"Basal%20Ganglia/#basal-ganglia_1","title":"Basal Ganglia","text":"<ul> <li>A group of structures below the cortex involved in motor, cognitive, and emotional functions.</li> </ul>"},{"location":"Base%20Link/","title":"Base Link","text":"<ul> <li>The stationary base structure of a robot arm that supports the first joint.</li> </ul>"},{"location":"Basic%20GAN/","title":"Basic GAN","text":"<ul> <li>Generative Adversarial Networks</li> <li>Learn a prob distribution directly from data generated by that distribution</li> <li>no need for any Markov Chain or unrolled approximate inference networks during either training or generation of samples</li> <li>Adversarial Learning</li> <li></li> <li>Min Max game \\(\\(max_D min_G V(G,D)\\)\\) where \\(\\(V(G,D) = \\mathbb{E}_{p_{data}(x)}logD(x) + \\mathbb{E}_{p_{data}(x)}log(1-D(x))\\)\\)</li> <li>G : Gradient Descent gradients</li> <li>D : Gradient Ascent</li> <li>Discriminator Loss (Given Generator)<ul> <li> \\[L_{disc}(D_\\theta) =-\\frac{1}{2}(\\mathbb{E}_{x\\sim p_{real}(x)}[log(D_{theta}(x))] + \\mathbb{E}_{x\\sim p_{latent}(x)}[log(1- D_\\theta(G_\\phi(z)))])\\] </li> </ul> </li> <li>Generator Loss (Given Discriminator)<ul> <li> \\[L_{gen}(G_{\\phi})= - \\mathbb{E}_{z\\sim p_{latent}(z)}[log(D_\\theta(G_\\phi(z)))]\\] </li> <li>This is low if Discriminator is fooled by Gen, \\(D_{\\theta}(x_{gen}) \\approx 1\\)</li> </ul> </li> </ul>"},{"location":"Basic%20GAN/#training","title":"Training","text":"<ul> <li>pick mini batch of samples\u00a0</li> <li>update discriminator with\u00a0Gradient Descent based** on discriminator loss with generator obtained from previous update</li> <li>update the generator with\u00a0Gradient Descent based on generator loss with the discriminator from the previous step</li> </ul>"},{"location":"Basic%20GAN/#issues","title":"Issues","text":"<ul> <li>Mode Collapse</li> </ul>"},{"location":"Basic%20GAN/#backlinks","title":"Backlinks","text":"<ul> <li>Generative vs Discriminative Models</li> <li> <p>Basic GAN</p> </li> <li> <p>Scalar Articles</p> </li> <li> <p>Basic GAN</p> </li> <li> <p>Generative_Models</p> </li> <li>Basic GAN</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Basic%20RNN%20Architectures/","title":"Basic RNN Architectures","text":"<ul> <li>Recurrent</li> <li>SRN</li> <li>Stacking RNN</li> <li>Bi Directional RNN</li> <li>Seq2Seq</li> <li>Temporal Conv</li> <li>[[GRU)](Gated Recurrent Unit (GRU|Gated Recurrent Unit (GRU|GRU)]].md)</li> <li>[[LSTM)](Long Short Term Memory (LSTM|Long Short Term Memory (LSTM|LSTM)]].md)</li> </ul>"},{"location":"Basic%20Transformer/","title":"Basic Transformer","text":"<ul> <li>Feed forward blocks, are two Dense MLPs with Relu. Residual connections in between</li> <li>Uses Attention</li> <li>Embedding Layers transform between 1 hot and vector rep</li> <li>Position Encoding + Token Embedding</li> <li>Position Wise Feed Forward</li> </ul>"},{"location":"Basics%20of%20Federated%20Learning/","title":"Basics of Federated Learning","text":"<ol> <li>Get data (Hopefully a lot)</li> <li>Preprocess (aka clean up) the data</li> <li>Find/create an architecture</li> <li>Train the model using the data(1) and the architecture(3). This step is done once. And then periodically updated as the data changes over time. Keep this in mind.</li> <li>Push the model out to n users</li> <li>Collect data about how well the model did. (Bye bye privacy)</li> <li>Send this data back to the main model. 7 (#new). Find the difference between the original model and the personalized one's parameters. Do this for multiple users. Remove identifiable information. 7.1 (#new). Aggregate (eg. average) the information and then send that to the main model</li> <li>Retrain the model on new data</li> </ol>"},{"location":"Basilar%20Artery/","title":"Basilar Artery","text":"<ul> <li>Located at the base of the skull, the basilar artery is a large, specialized blood vessel that supplies oxygenated blood to the brain and nervous system.</li> </ul>"},{"location":"Batch%20Normalization/","title":"Batch Normalization","text":"<ul> <li>bias=False for Linear/Conv2D for input and True for output #tricks</li> <li>Normalizes #activations</li> <li>Input distributions change per layer -&gt; Make sure they stay similar</li> <li>Reduces co variate shift because now the network must adapt per layer</li> <li>During testing : use stats saved during training</li> <li>Simplifies learning dynamics<ul> <li>Can use larger learning rate</li> <li>Higher order interactions are suppressed because the mean and std are independant of the activations which makes training easier</li> </ul> </li> <li>Cant work with small batches. Not great with RNN</li> <li></li> <li> \\[\\mu_j \\leftarrow \\frac{1}{m}\\Sigma_{i=1}^m x_{ij}\\] </li> <li> \\[\\sigma^2_j \\leftarrow \\frac{1}{m}\\Sigma^m_{i=1}(x_{ij}-\\mu_j)^2\\] </li> <li> \\[\\hat x_{ij} \\leftarrow \\frac{x_{ij}-\\mu_j}{\\sqrt{\\sigma^2_j + \\epsilon}}\\] </li> <li> \\[\\hat x_{ij} \\leftarrow \\gamma \\hat x_{ij} + \\beta\\] </li> </ul>"},{"location":"Batch%20Normalization/#backlinks","title":"Backlinks","text":"<ul> <li>DCGAN</li> <li>Transposed Conv , Batch Normalization and Relu</li> <li>Batch Normalization AFTER Transposed Conv is super important as it helps with flow of gradients</li> <li>Strided Conv, Batch Normalization, and Leaky Relu</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Bayes%20Prediction/","title":"Bayes Prediction","text":"<ul> <li> \\[P(y|x) = \\int_{w}P(y|w,x)P(w|x)dw\\] </li> <li>w is model parameters</li> <li>basically gets y with different model parms w</li> <li>prob of those params given input x</li> <li>Model averaging</li> </ul>"},{"location":"Bayes%20Rule/","title":"Bayes Rule","text":"<ul> <li> \\[h(\\theta|D) = \\frac{p_{\\otimes_{i}}x(D|\\theta)h(\\theta)}{p(D)}\\] </li> </ul>"},{"location":"Bayesian%20Information%20Criterion/","title":"Bayesian Information Criterion","text":"<ul> <li> \\[BIC = -2 log(L) + 2log(n)q = \\frac{1}{n}(RSS + log(n) p \\hat \\sigma^{2})\\] </li> <li>L: denotes the likelihood function for a particular model</li> <li>q: number of estimated parameters of the model</li> </ul>"},{"location":"Bayesian%20Model%20Estimation/","title":"Bayesian Model Estimation","text":"<ul> <li>Unlike frequentist, sometimes things like sample mean is not a good metric because it has a high variance. Might give different results with different trials in a real valued distribution</li> <li>The task is to estimate \\(\\theta\\) from the data</li> <li>Bayesian Prior</li> <li>Now there are two sources of info about the true distribution \\(p_{X}(\\theta)\\)<ul> <li>The likelihood \\(p_{\\otimes_{i}}x(D|\\theta)\\) of \\(\\theta\\) . Empirical data</li> <li>Prior plausibility in \\(h(\\theta)\\)</li> <li>Since these are independant sources we can combine them by multiplication: \\(p_{\\otimes_{i}}x(D|\\theta)h(\\theta)\\)<ul> <li>High values -&gt; Candidate model \\(\\theta\\) is a good estimate</li> <li>Bayesian Posterior</li> <li>Posterior Mean estimate</li> </ul> </li> </ul> </li> </ul>"},{"location":"Bayesian%20Model%20Estimation/#advantages","title":"Advantages","text":"<ul> <li>If priors are well chosen -&gt; Better than frequentists with small sample sizes</li> </ul>"},{"location":"Bayesian%20Model%20Estimation/#disadvantages","title":"Disadvantages","text":"<ul> <li>Integrating over millions of params and performing multiple preds for each param -&gt; infeasible</li> <li>How to encode or represent Bayesian Posterior as very high dim<ul> <li>No closed form representation over weights</li> <li>Represent data with histograms and use Monte Carlo</li> </ul> </li> </ul>"},{"location":"Bayesian%20Model%20Estimation/#example","title":"Example","text":"<ul> <li>Green : prior , Red: Posterior</li> <li>The Posterior Mean estimate is obtained by integrating \\(\\int_{\\mathbb{R}}\\mu h(\\mu|D)d\\mu\\)</li> <li>Since this is different from sample mean -&gt; Prior distribution really does influence the models</li> </ul>"},{"location":"Bayesian%20Model%20Estimation/#protein-modeling","title":"Protein Modeling","text":""},{"location":"Bayesian%20Model%20Estimation/#backlinks","title":"Backlinks","text":"<ul> <li>Generative vs Discriminative Models</li> <li>Bayesian Model Estimation</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Bayesian%20Neural%20Network/","title":"Bayesian Neural Network","text":"<ul> <li>Bayesian Model Estimation</li> <li>Generally we want to learn Joint Probability distribution \\(P(y|x)\\) but this does not use the model parameters w</li> <li>We need (\\(P(w|D) = \\frac{P(D|w)P(w)}{P(D)}\\)\\)<ul> <li>D is the labelled dataset</li> <li>Model is now defined by structure and parameters</li> </ul> </li> <li>The parameters encode information about Uncertainty<ul> <li>Can be understood using Bayesian Predictive Posterior</li> </ul> </li> </ul>"},{"location":"Bayesian%20Posterior/","title":"Bayesian Posterior","text":"<ul> <li>When D is fixed though, this becomes a function of Model Candidates</li> <li>Non negative on K dim param space</li> <li>Not a PDF but if we divide it by its integral -&gt; PDF .<ul> <li> \\[\\frac{p_{\\otimes_{i}}x(D|\\theta)h(\\theta)}{\\int_{\\mathbb{R}^K}p_{\\otimes_{i}}x(D|\\theta)h(\\theta)d\\theta}\\] </li> <li>Prob distrib over candidate models</li> </ul> </li> <li>If the denominator is replaced written as \\(p(D)\\) then it looks like the Bayes Rule</li> <li>Shape : \\(P(D|\\theta)h(\\theta)\\)<ul> <li>Integral not 1</li> <li>Proto Distributions on \\(\\theta\\) space</li> </ul> </li> </ul>"},{"location":"Bayesian%20Predictive%20Posterior/","title":"Bayesian Predictive Posterior","text":"<ul> <li>Follows from Bayesian Posterior</li> <li>Marginalizing over all possible model parameters w</li> <li>Computes predictions y with different model parameters w and weights them by the Probability of those params given an input x</li> <li>Bayesian Model Averaging</li> <li> \\[P(y|x) = \\int_{w}P(y|w,x)P(w|x)dx\\] </li> </ul>"},{"location":"Bayesian%20Prior/","title":"Bayesian Prior","text":"<ul> <li>Use prior knowledge as beliefs (param vectors \\(\\theta\\)). Cast in the form of a Probability distribution over the space \\(\\Theta\\) .<ul> <li>Weak knowledge most times</li> <li>For a K parametric PDF \\(p_{x}\\) , \\(\\Theta \\in \\mathbb{R}^{K}\\) .</li> <li>Not connected to Random variable(RVS).</li> <li>Does not model outcomes. Instead has \"beliefs\" about true distribution \\(P_{X_{i}}\\)</li> <li>Each \\(\\theta \\in \\mathbb{R}^{K}\\) corresponds to one specific PDF \\(p_{X}(\\theta)\\) -&gt; single candidate distribution \\(\\hat P_{X}\\) for values \\(x_i\\) (In frequentist, it models single data points)</li> <li>Since this is a distribution over distributions, it is a hyperdistribution</li> <li>N dim PDF \\(p_{\\otimes}x_{i}: \\mathbb{R}^{N} \\rightarrow \\mathbb{R}^{\\geq 0}\\) for the distribution of \\(RV \\otimes_{i}X_{i}\\)<ul> <li>\\(p_{\\otimes_{i}}x_{i}((x_{i},\u2026, x_{N})) = p_{x_{1}}, \u2026, p_{x_{N}}(x_{N}) = \\Pi_{i}p_{X}(x_{i})\\)</li> <li>\\(p_{\\otimes_{i}}x(D|\\theta)\\) -&gt; PDF values on a data sample D \\(p_{\\otimes_{i}}x_{i}((x_{i},\u2026, x_{N})) = p_{\\otimes_{i}}(\\theta)(D)\\)</li> </ul> </li> <li>When \\(\\theta\\) is fixed then \\(p_{\\otimes_{i}}x(D|\\theta)\\) is a function of data vectors D. For each sample, it describes how probable this distribution is assuming the true distribution of X is \\(p_{X}(\\theta)\\)</li> <li>When D is fixed, then it is a function of \\(\\theta\\). But this does not really measure anything.<ul> <li>Integral over \\(\\theta\\) is not 1</li> <li>It is a function of \\(\\theta\\) and so it is a likelihood function. MLE</li> <li>If given data D -&gt; it can show which models are more likely than others.</li> <li>Higher values of  \\(p_{\\otimes_{i}}x(D|\\theta)\\) are better</li> </ul> </li> </ul> </li> </ul>"},{"location":"Bayesian%20Rule%20List/","title":"Bayesian Rule List","text":"<ul> <li>The BRL is a generative model that yields a posterior distribution over possible decision lists, which consist of a series of if-then statements that discretize a high-dimensional, multivariate feature space into a series of simple, readily interpretable decision statements. The if statements define a partition of a set of features, and the then statements correspond to the predicted outcome of interest.</li> <li>According to the authors, their experiments showed that the BRL has predictive accuracy on par with the current top algorithms for prediction in Machine Learning</li> <li>The BRL is able to be used to produce highly accurate and interpretable medical scoring systems</li> </ul>"},{"location":"Bayesian/","title":"Bayesian","text":"<ul> <li>Subjective</li> <li>Bayes Prediction</li> <li>Bayes Rule</li> <li>Bayesian Model Estimation</li> <li>Probability density function</li> </ul>"},{"location":"Beam%20search/","title":"Beam Search","text":"<ul> <li>(from)</li> <li>at each step, keep track of the\u00a0k\u00a0mos probable translation hypotheses (k, beam size)</li> <li>examine the\u00a0k\u00a0most probable words for each hypothesis, compute their entire scores, keep\u00a0k\u00a0best ones</li> <li>not guaranteed to find optimal solution, but more efficient than exhaustive search</li> <li>it does not only take the best word, it rather takes the best\u00a0B\u00a0(user specified) and generates multiple hypothesis, which will then be evaluated and the best one at each step is chosen for the next ones</li> <li>not guaranteed to find the optimal solution and is therefore an approximate search</li> <li>problems:<ul> <li>when multiplying a lot of probabilities of very unlikely word (e.g. almost 0 but not exactly), the result will get very small and the system can no longer represent it. results in numerical underflow --&gt; instead of multiplying, summing the log of probabilities (more numerical stable)</li> <li>if the sentence is very long, the probabilities get very low, therefore it rather takes smaller translations --&gt; normalize the output by the number of word in the translation (average of the log of each word)</li> </ul> </li> <li>how to choose beam width\u00a0\\(B\\)?<ul> <li>the smaller, the fewer probabilities are considered (worse result, faster)</li> <li>the larger, the more are considered, but the more computing expensive it is (better results, slower)</li> <li>try out different values and cross check</li> </ul> </li> </ul>"},{"location":"Belief-Desire-Intention/","title":"Belief-Desire-Intention","text":"<ul> <li>To judge the ethics of an agent's own actions, the awareness process generates the beliefs that describe the current situation facing the agent and the goals of the agent.</li> <li>Based on the beliefs and goals, the evaluation process generates the set of possible actions and desirable actions</li> <li>goodness process then computes the set of ethical actions based on the agent's beliefs, desires, actions, and moral value rules</li> <li>rightness process evaluates whether or not executing a possible action is right under the current situation and selects an action which satisfies the rightfulness requirement</li> </ul>"},{"location":"Belief-Desire-Intention/#backlinks","title":"Backlinks","text":"<ul> <li>Building Ethics into Artificial Intelligence</li> <li>Belief-Desire-Intention</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Belmont%20Principles/","title":"Belmont Principles","text":"<ul> <li>The three principles\u2014</li> <li>beneficence, distributive justice, and respect for persons\u2014which the 1976 Belmont Report concluded should underlie all conduct in biomedical and behavioral research in order to protect human participants.</li> </ul>"},{"location":"Belmont%20Report/","title":"Belmont Report","text":"<ul> <li>An influential report that identified and defined the basic ethical principles (the Belmont principles) that should govern research studies involving human participants.</li> </ul>"},{"location":"Benchmark%20LLM/","title":"Benchmark LLM","text":"<ul> <li>Large Language Models Still Can\u2019t Plan (A Benchmark for LLMs on Planning and Reasoning about Change)))</li> <li>The recent advances in large language models (LLMs) have transformed the field of natural language processing (NLP)</li> <li>From GPT3 to PaLM, the state-of-the-art performance on natural language tasks is being pushed forward with every new large language model</li> <li>current benchmarks are relatively simplistic and the performance over these benchmarks cannot be used as an evidence to support</li> <li>extensible assessment framework motivated by the above gaps in current benchmarks to test the abilities of LLMs on a central aspect of human intelligence, which is reasoning about actions and change</li> <li>multiple test cases</li> </ul>"},{"location":"Bend%20Minimization/","title":"Bend Minimization","text":"<ul> <li>Curved lines easier to follow than edged lines Gestalt Laws</li> <li>domain specific constraints and traditions have to be acknowledged</li> <li></li> </ul>"},{"location":"Beneficence/","title":"Beneficence","text":"<ul> <li>One of the three Belmont principles, the requirement that physicians and researchers provide, to the best of their ability, positive benefits for patients that participate in clinical trials, including good health and the prevention and removal of harmful conditions.</li> </ul>"},{"location":"Benford%27s%20Law/","title":"Benford's Law","text":"<ul> <li>In a genuine dataset of numbers<ul> <li>1 will be the leading digit 30.1% of the time</li> <li>2 will be the leading digit 17.6% of the time</li> <li>rest with decreasing frequency</li> </ul> </li> </ul>"},{"location":"Benign/","title":"Benign","text":"<ul> <li>Refers to a tumor that is neither cancerous nor malignant</li> </ul>"},{"location":"Berkeley%20et%20al/","title":"Berkeley Et Al","text":"<ul> <li>576 input patterns</li> <li>5793 epochs needed to reach convergence</li> <li>Model frozen, stimulus set presented again</li> <li>Activation of each hidden unit recorded</li> <li>Single unit recording</li> <li>Why do these bands appear?</li> <li>Gaussian activation function</li> <li>But banding patterns have been found with sigmoidal activation as well</li> <li>Effect = units only respond to a limited number of inputs</li> <li>Bands appear when weights into HUs cancel each other out</li> <li>Activations of hidden neurons can be organized into bands</li> <li>Bands are associated with interpretable features</li> <li>Lesion studies show bands are essential to solving problem</li> <li>For some problems under some circumstances, neural networks develop highly selective hidden units</li> <li>Looks like localist coding (grandmother cells)</li> <li>Patterns of activation can be ambiguous on their own</li> <li>But realistically, more than one pattern might be activated simultaneously</li> <li>Superimposing two or more patterns over same units leads to an ambiguous blend</li> <li>Problem for localist representations, but even more serious with distributed representations</li> </ul>"},{"location":"Bernoulli%20Distribution/","title":"Bernoulli Distribution","text":"<ul> <li>Only two possible outcomes</li> <li>PMF : \\(\\(p(s_{i}) = \\begin{cases} 1-q, &amp; \\text{for i = 1} \\\\ q,&amp; \\text{for i =2} \\end{cases}\\)\\)</li> <li>Given : Data - \\({x_{1}, .., x_{N}}\\) and \\(x_{i} \\in {s_{1}, s_{2}}\\) then \\(\\(\\hat q = \\frac{1}{N}|\\{i|x_{i}= s_{2}\\}\\)\\)</li> </ul>"},{"location":"Best%20Maching%20Unit/","title":"Best Matching Unit","text":"<ul> <li>Neuron whose weight vector best matches input pattern</li> </ul>"},{"location":"Beta%20Distribution/","title":"Beta Distribution","text":"<ul> <li>[0,1]</li> <li>Parameterized by two positive shape parameters \\(\\alpha, \\beta\\)</li> <li>Exponents of the random variable and control the shape of the distribution</li> <li>Multiple variables is Dirichlet Distribution</li> </ul>"},{"location":"Beta%20Waves/","title":"Beta Waves","text":"<ul> <li>Movement</li> <li></li> </ul>"},{"location":"Beware%20of%20Inmates%20Running%20the%20Asylum/","title":"Beware of Inmates Running the Asylum","text":"<ul> <li>Tim Miller\u2217 and Piers Howe\u2020 and Liz Sonenberg</li> </ul>"},{"location":"Beware%20of%20Inmates%20Running%20the%20Asylum/#tldr","title":"TL;DR","text":"<ul> <li>Essentially proposes to look at behavioral science research as well. Not particularly useful but is a good reminder to look at other research because XAI is meant for people and not programmers.</li> </ul>"},{"location":"Beware%20of%20Inmates%20Running%20the%20Asylum/#abstract","title":"Abstract","text":"<ul> <li>programmers design software for themselves, rather than for their target audience; a phenomenon he refers to as the \u2018inmates running the asylum\u2019.</li> <li>This paper argues that explainable AI risks a similar fate.</li> <li>evaluation of these models is focused more on people than on technology</li> <li>considerable scope to infuse more results from the social and behavioural sciences into explainable AI, and present some key results from these fields that are relevant to explainable AI.</li> </ul>"},{"location":"Beware%20of%20Inmates%20Running%20the%20Asylum/#explainable-ai-survey","title":"Explainable AI Survey","text":""},{"location":"Beware%20of%20Inmates%20Running%20the%20Asylum/#survey-method","title":"Survey Method","text":"<ul> <li> <p>On topic</p> <ul> <li>Each paper was categorised as either being about explainable AI or not, based on our understanding of the topic</li> <li>Data Driven</li> <li>Each paper was given a score from 0\u20132 inclusive.</li> <li>A score of 1 was given if and only if one or more of the references of the paper was an article on explanation in social science</li> <li>Validation</li> <li>Each paper was given a binary 0/1. A score of 1 was given if and only if the evaluation in the survey article (note, not the referenced article) was based on data from human behavioural studies.</li> </ul> </li> </ul>"},{"location":"Beware%20of%20Inmates%20Running%20the%20Asylum/#results","title":"Results","text":"<ul> <li>These results show that for the on-topic papers, only four articles referenced relevant social science research, and only one of them truly built a model on this</li> <li>Further, serious human behavioural experiments are not currently being undertaken.</li> <li>For off topic papers, the results are similar: limited input from social sciences and limited human behavioural experiments.</li> <li>Where to? A Brief Pointer to Relevant Work Contrastive Explanation</li> <li>explanations are contrastive why\u2013questions are contrastive</li> <li>That is, why\u2013questions are of the form \u201cWhy P rather than Q?\u201d, where P is the fact that requires explanation, and Q is some foil case that was expected</li> <li>Importantly, the contrast case helps to frame the possible answers and make them relevant</li> <li>This is a challenge for explainable AI, because it may not be easy to elicit a contrast case from an observer.</li> <li>However, it is also an opportunity: as Lipton [1990] argues, answering a contrastive question is often easier than giving a full cause attribution because one only needs to understand the difference between the two cases, so one can provide a complete explanation without determining or even knowing all causes of the event.</li> </ul>"},{"location":"Beware%20of%20Inmates%20Running%20the%20Asylum/#attribution-theory","title":"Attribution Theory","text":"<ul> <li>study of how people attribute causes to events Social Attribution</li> <li>The book from Malle [2004], based on a large body of work from himself and other researchers in the field, describes a mature model of how people explain behaviour of others using folk psychology</li> <li>people attribute behaviour based on the beliefs, desires, intentions, and traits of people</li> <li>important for systems in which intentional action will be cited as a cause important for systems doing deliberative reasoning</li> </ul>"},{"location":"Beware%20of%20Inmates%20Running%20the%20Asylum/#causal-connection","title":"Causal Connection","text":"<ul> <li>Research on how people connect causes shows that they do so by undertaking a mental simulation of what would have happened had some other event turned out differently</li> </ul>"},{"location":"Beware%20of%20Inmates%20Running%20the%20Asylum/#backlinks","title":"Backlinks","text":"<ul> <li>05:17 Anyway Beware of Inmates Running the Asylum</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Bhattacharya%20Distance/","title":"Bhattacharya Distance","text":"<ul> <li>In statistics, the\u00a0Bhattacharyya distance\u00a0measures the similarity of two Distributions. It is normally used to measure the separability of classes in classification.</li> <li> \\[D_{B}(p,q) = -ln(BC(p,q))\\] </li> <li> \\[BC(p,q) = \\Sigma_{x\\in X}\\sqrt{p(x)q(x)}\\] </li> </ul>"},{"location":"Bi%20Directional%20RNN/","title":"Bi Directional RNN","text":"<ul> <li>Not causal</li> <li>Looks at the forward timestep dimension and also the backward<ul> <li>Both combined to make a prediction</li> </ul> </li> <li></li> </ul>"},{"location":"Bias%20Variance%20Dilemma/","title":"Bias Vs Variance","text":"<ul> <li><ul> <li>m is choices of PC vectors</li> <li>as m increases, weight matrices grow by \\(10\\cdot m\\) . Aka more flexible models.</li> <li>Increasing tail of MSEtest -&gt; overfitting. too flexible</li> <li>Increasing flexibility -&gt; decrease of empirical risk</li> <li>Inc : very low to very high -&gt; less and less underfitting then overfitting</li> <li>Best: min point in curve. But it is defined on test data which we do not have</li> </ul> </li> <li></li> <li></li> <li>Decision function should minimize LossFunctions and yield a function with risk h. This is hopeless \\(\\(R(h) = E[L(h(X), Y)]\\)\\)</li> <li>Tune on Emperical Risk instead using Optimizers</li> <li>\\(\\mathcal{H}\\) is hypothesis space (related to Fitting).</li> </ul>"},{"location":"Bias%20Variance%20Dilemma/#why-is-this-a-dilemma","title":"Why is This a Dilemma","text":"<ul> <li>Any learning algo \\(\\mathcal{A}\\)</li> <li>If we run \\(\\mathcal{A}\\) repeatedly but for different \"fresh\" sampled data -&gt; \\(\\hat h\\) varies from trial to trial</li> <li>For any fixed x, \\(\\hat h(x)\\)<ul> <li>is a random variable</li> <li>value determined by drawn training samples</li> <li>rep by distribution \\(P_{X,Y}\\) (which we cannot really know)</li> <li>Expectation \\(E_{retrain}[\\hat h(x)]\\) . aka taken over ALL possible training runs with sampled data</li> </ul> </li> <li>Quadratic Loss (risk) is minimized by the function (\\(\\Delta(x) = E_{Y|X=x}[Y]\\)\\)<ul> <li>Expectation of Y given x.</li> </ul> </li> <li></li> <li></li> <li>Bias measures how strongly the avg result deviates from optimal value</li> <li>Variance measures how strongly the results vary around the expected value \\(E_{retrain}\\)</li> <li>When flexibility is too low -&gt; bias dominates(too good in train and horrible later) and underfits</li> <li>When flexibility is too high -&gt; variance dominates -&gt; overfitting</li> </ul>"},{"location":"Bias%20Variance%20Dilemma/#tuning-model-flexibility","title":"Tuning Model Flexibility","text":""},{"location":"Bias%20nodes/","title":"Bias Nodes","text":"<ul> <li>Bias nodes give a defaults activation to other nodes<ul> <li>Usually included, the bias node does not connect to input nodes but to the output nodes</li> </ul> </li> </ul>"},{"location":"Big%20Bird/","title":"Big Bird","text":"<ul> <li>Big Bird: Transformers for Longer Sequences</li> <li>imitation of Transformer-based models is the quadratic complexity</li> <li>sparse attention mechanism that reduces this quadratic complexity to linear</li> </ul>"},{"location":"Big-Bench/","title":"Big-Bench","text":"<ul> <li>Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models</li> <li>present and near-future capabilities and limitations of language models</li> <li>Beyond the Imitation Game benchmark (BIG-bench)</li> <li>benchmark that can measure progress well beyond the current state-of-the-art</li> <li>204 tasks, contributed by 442 authors across 132 institutions</li> <li>Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development</li> <li>tasks that are believed to be beyond the capabilities of current language models</li> <li>valuate the behavior of OpenAI\u2019s GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters</li> </ul>"},{"location":"Bilinear%20Interpolation/","title":"Bilinear Interpolation","text":"<ul> <li> \\[f(x,y) = (1-\\beta)(1-\\alpha)f_{i,j}+(1-\\beta)\\alpha f_{i+1,j} + \\beta(1-\\alpha)f_{i,j+1}+\\beta \\alpha f_{i+1,j+1}\\] </li> <li>Quadratic</li> <li>ik, jkl , il -&gt; ij</li> </ul>"},{"location":"Billion%20Word/","title":"Billion Word","text":""},{"location":"Binary%20Cross%20Entropy/","title":"Binary Cross Entropy","text":"\\[-(ylog(p)+(1-y)log(1-p))\\]"},{"location":"Binary%20Cross%20Entropy/#backlinks","title":"Backlinks","text":"<ul> <li>Cross [Entropy](./Entropy.md)</li> <li>Binary Cross Entropy</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Binary%20pattern/","title":"Binary Pattern Encoding","text":"<ul> <li>If symbol alphabet has a large size k<ul> <li>One hot is too huge</li> </ul> </li> <li>Encode into binary vector of length \\(\\(\\lceil log_{2} \\rceil\\)\\)</li> <li>{a,b,c,d} -&gt; {[0,0]', [0,1]', [1,0]', [1,1]'}</li> <li>Non linear effort as it is a arbitrary encoding</li> <li>Too intensive</li> </ul>"},{"location":"Binary%20pattern/#_1","title":"\u2026","text":""},{"location":"BinaryBERT/","title":"BinaryBERT","text":"<ul> <li>BinaryBERT: Pushing the Limit of BERT Quantization</li> <li>demand for model compression techniques</li> <li>weight binarization</li> <li>binary BERT is hard to be trained directly than a ternary counterpart due to its steep and complex loss landscape</li> <li>ternary weight splitting</li> <li>initializes BinaryBERT by equivalently splitting from a half-sized ternary network, followed by fine-tuning for further refinement</li> <li>binary model thus inherits the good performance of the ternary one, and can be further enhanced by fine-tuning the new architecture after splitting</li> <li>tailor the size of BinaryBERT based on the edge device constraints</li> <li>GLUE</li> <li>SQuAD</li> </ul>"},{"location":"Binning/","title":"Binning","text":"<ul> <li>k segments</li> <li>Transform each symbol</li> <li>Types<ul> <li>Simplest -&gt; k equal bins</li> <li>Approx Equal no of data points</li> <li>Reduced precision devices perform as well as the high precision ones</li> <li>Continuous range -&gt; adaptive bin boundaris Decision Trees</li> </ul> </li> </ul>"},{"location":"Binomial%20Distribution/","title":"Binomial Distribution","text":"<ul> <li>Bernoulli Distribution repeated for N independant trials with success Probability q</li> <li>Aka N times with only 2 outcomes</li> <li>PMF: \\(\\(p(s) = \\binom{N}{s}q^{s}(1-q)^{N-s} = \\frac{N!}{s!(N-s)}q^{s}(1-q)^{N-s}\\)\\)</li> <li>s = 0,1,2..N</li> <li>\\(N\\choose s\\) is binomial coefficient</li> <li> \\[X \\sim Bi(N,s)\\] </li> <li></li> </ul>"},{"location":"Biological%20Neuron/","title":"Biological Neuron","text":"<ul> <li>(from)</li> <li>composed of\u00a0<ul> <li>cell body</li> <li>dendrites: many branching extensions</li> <li>axon: very long extension that splits off at its tip into many branches called synaptic terminals</li> </ul> </li> <li>composed in a network (e.g. brain) by synaptic terminals of one neuron connected to dendrites of other neurons</li> <li>electrical impulses (signals) are sent from other neurons via these synapses</li> <li>if a neuron receives a sufficient number of signal from other neurons within a few milliseconds, it is exited and fires its own signals (activation)</li> <li>connectivity in a biological neural system is huge, human brain:<ul> <li>number of neurons: \\(\\approx 10^{11}\\)</li> <li>number of connections per neuron:\u00a0\\(\\approx 10^4\\)</li> </ul> </li> <li>networks are organized into hierarchical structures</li> <li>Irreplacable</li> <li>Requires constant supply of Glucose</li> </ul>"},{"location":"Biomarkers/","title":"Biomarkers","text":"<ul> <li>A measurable physiological indicator of a biological state or condition.</li> </ul>"},{"location":"Biopsy/","title":"Biopsy","text":"<ul> <li>Removal of a small tissue sample for testing</li> </ul>"},{"location":"Block%20Sparse%20Kernel/","title":"Block Sparse Kernel","text":"<ul> <li>For networks with block sparse weights</li> <li>Can choose amount of sparsity</li> <li>Can replace normal Dense Layers with sparse and wide or sparse and deep</li> <li></li> <li>Enables wider and deeper networks</li> <li>Only compute on non zero blocks</li> <li></li> <li>Connectivity is unaffected in the spatial dimensions</li> <li>Compute cost is only prop to number of non zero weights</li> <li>Small World graphs</li> <li>Also useful for compression</li> </ul>"},{"location":"Block%20Sparse%20Kernel/#refs","title":"Refs","text":"<ul> <li>openai</li> </ul>"},{"location":"BlockNeRF/","title":"BlockNeRF","text":"<ul> <li>Block-NeRF: Scalable Large Scene Neural View Synthesis</li> <li>variant of Neural Radiance Field</li> <li>reconstruct large-scale environments</li> <li>scaling NeRF to render city-scale scenes spanning multiple blocks, it is vital to decompose the scene into individually trained NeRFs that can be optimized independently.</li> <li>this decomposition decouples rendering time from scene size</li> <li>allows per-block updates of the environment</li> <li>data collected will necessarily have transient objects and variations in appearance</li> <li>modifying the underlying NeRF architecture to make NeRF robust to data captured over months under different environmental conditions</li> <li>appearance Embedding, learned pose refinement, and controllable exposure to each individual NeRF</li> <li>procedure for aligning appearance between adjacent NeRFs so that they can be seamlessly combined</li> <li>building an entire neighborhood in San Francisco from 2.8M images using a grid of Block-NeRFs, forming the largest neural scene representation to date</li> </ul>"},{"location":"Blood%20Culture/","title":"Blood Culture","text":"<ul> <li>Test to reveal the existence of fungi or bacteria in the blood, possibly indicating an infection</li> </ul>"},{"location":"Blood%20Lancet/","title":"Blood Lancet","text":"<ul> <li>A double-edged blade or needle used to obtain blood samples</li> </ul>"},{"location":"Blood%20Swab/","title":"Blood Swab","text":"<ul> <li>Taking a blood sample using a cotton-tipped stick</li> </ul>"},{"location":"Blood-brain%20Barrier/","title":"Blood-brain Barrier","text":"<ul> <li>A protective barrier that separates the brain from the blood circulating across the body. The blood-brain barrier is semipermeable, meaning it allows the passage of water as well as molecules like glucose and other amino acids that help promote neural function.</li> </ul>"},{"location":"Boltzmann%20Distribution/","title":"Boltzmann Distribution","text":"<ul> <li>PDF \\(\\(p(s|T)= \\frac{1}{\\int_{s}e^{-C(s)/T}ds}e^{-C(s)/T}\\)\\)</li> <li>Energy function \\(E: S \\rightarrow \\mathbb{R}^{\\geq 0}\\)</li> <li>Markov Random Field</li> </ul>"},{"location":"BooksCorpus/","title":"BooksCorpus","text":""},{"location":"Bottom%20Up%20Parsing/","title":"Bottom Up Parsing","text":""},{"location":"Bound%20morpheme/","title":"Bound Morpheme","text":"<ul> <li>morphemes that cannot appear as a word by itself</li> <li>e.g., +ing, +s, +ness,ly,ed</li> </ul>"},{"location":"Brain%20Areas/","title":"Brain Areas","text":"<ul> <li>Cerebrum</li> <li>Cerebellum</li> <li>Brainstem</li> </ul>"},{"location":"Brain%20Cortex/","title":"Brain Cortex","text":"<ul> <li>Also called Cerebral Cortex</li> <li>It has a folded appearance with hills and valleys</li> <li>The nerve cell bodies color the cortex grey-brown giving it its name \u2013 gray matter</li> <li>Beneath the cortex are long nerve fibers (axons) that connect Brain Areas to each other \u2014 called white matter</li> <li></li> <li>Gyrus</li> <li>Basal Ganglia</li> <li>Divided into parts<ul> <li>Medial Prefrontal Cortex, and the Posterior Cingulate Cortex with the nearby Precuneus and Lateral Parietal Cortex **</li> </ul> </li> </ul>"},{"location":"Brain%20Organoid/","title":"Brain Organoid","text":"<ul> <li>A research model that uses pluripotent stem cells (iPSCs) to grow structures that resemble brains in some ways, but are grown in a lab dish made of neurons and other brain tissues.</li> </ul>"},{"location":"Brain%20Oscillations/","title":"Brain Oscillations","text":"<ul> <li>Periodic</li> <li>Brain waves</li> <li>Delta Waves</li> <li>Theta Waves</li> <li>Alpha Waves</li> <li>Beta Waves</li> <li>Gamma Waves</li> <li>Spectrogram</li> <li></li> </ul>"},{"location":"Brain-derived%20Neurotrophic%20Factor%20%28BDNF%29/","title":"Brain-derived Neurotrophic Factor (BDNF)","text":"<ul> <li>Sometimes referred to as \u201cbrain fertilizer,\u201d BDNF is a protein that helps promote the growth, maintenance, and survival of neurons.</li> </ul>"},{"location":"BrainWave%20Coherence/","title":"BrainWave Coherence","text":"<ul> <li>Correlation in the frequency domain</li> <li>Unlike synchronization, this also depends on signal amplitude</li> <li><ul> <li>coherence vs freq</li> <li>decent coherence between CZ and O1</li> <li>O1 and PZ has little coherence</li> </ul> </li> </ul>"},{"location":"BrainWave%20CrossFrequency%20Coupling/","title":"BrainWave CrossFrequency Coupling","text":"<ul> <li>Low frequency + Superimposed High freq signal</li> <li></li> <li>-<ul> <li>Electrode with low freq + high freq</li> </ul> </li> </ul>"},{"location":"BrainWave%20Synchronization/","title":"BrainWave Synchronization","text":"<ul> <li>Consistency of phase difference<ul> <li>If 0 then perfect</li> </ul> </li> <li></li> <li>Phase Locking Value</li> </ul>"},{"location":"Brainstem/","title":"Brainstem","text":"<ul> <li>relay center connecting the cerebrum and cerebellum to the spinal cord. It performs many automatic functions such as breathing, heart rate, body temperature, wake and sleep cycles, digestion, sneezing, coughing, vomiting, and swallowing</li> </ul>"},{"location":"Branch%20Prediction/","title":"Branch Prediction","text":"<ul> <li>avoid delays cause of control dependencies to be resolved.</li> <li>determines whether a conditional branch (jump) in the instruction flow of a program is likely to be taken or not</li> </ul>"},{"location":"Broadcasting/","title":"Broadcasting","text":"<ul> <li>Expanding the shape of an operand in a matrix math operation to dimensions compatible for that operation. For instance, linear algebra requires that the two operands in a matrix addition operation must have the same dimensions. Consequently, you can't add a matrix of shape (m, n) to a vector of length n. Broadcasting enables this operation by virtually expanding the vector of length n to a matrix of shape (m,n) by replicating the same values down each column.</li> </ul>"},{"location":"Brocas%20Area/","title":"Brocas Area","text":"<ul> <li>If this area is damaged, one may have difficulty moving the tongue or facial muscles to produce the sounds of speech. The person can still read and understand spoken language but has difficulty in speaking and writing</li> <li>Broca's aphasia</li> <li>Discovered by French physician Paul Broca in the late 19th century, this small region in the left frontal lobe has been linked to speech production.</li> </ul>"},{"location":"Broden/","title":"Broden","text":"<ul> <li>Broadly and Densely Labeled Dataset</li> <li>unifies several densely labeled image data sets: ADE [43], OpenSurfaces [4], Pascal-Context [19], Pascal-Part [6], and the Describable Textures Dataset [7]</li> <li>These data sets contain examples of a broad range of objects, scenes, object parts, textures, and materials in a variety of contexts</li> <li>segmented down to the pixel level except textures and scenes which are given for full-images</li> <li>every image pixel in the data set is annotated with one of the eleven common color names according to the human perceptions classified by van de Weijer</li> <li>The concept labels in Broden are normalized and merged from their original data sets so that every class corresponds to an English word</li> <li>Labels are merged based on shared synonyms, disregarding positional distinctions such as 'left' and 'top'</li> </ul>"},{"location":"Broden/#backlinks","title":"Backlinks","text":"<ul> <li>Network Dissection Quantifying Interpretability of Deep Visual Representions</li> <li>Broden</li> <li>evaluates every individual convolutional unit in a CNN as a solution to a binary segmentation task to every visual concept in Broden</li> <li>For every input image x in the Broden dataset, the activation map \\(A_{k}(x)\\) of every internal convolutional unit k is collected.</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Bucketing/","title":"Bucketing","text":"<ul> <li>Converting a (usually continuous) feature into multiple binary features called buckets or bins, typically based on value range. For example, instead of representing temperature as a single continuous floating-point feature, you could chop ranges of temperatures into discrete bins. Given temperature data sensitive to a tenth of a degree, all temperatures between 0.0 and 15.0 degrees could be put into one bin, 15.1 to 30.0 degrees could be a second bin, and 30.1 to 50.0 degrees could be a third bin.</li> </ul>"},{"location":"Building%20Ethics%20into%20Artificial%20Intelligence/","title":"Building Ethics into Artificial Intelligence","text":"<ul> <li>Han Yu, Zhiqi Shen, Chunyan Miao, Cyril Leung, Victor R. Lesser, Qiang Yang</li> </ul>"},{"location":"Building%20Ethics%20into%20Artificial%20Intelligence/#abstract","title":"Abstract","text":"<ul> <li>taxonomy which divides the field into four areas: 1) exploring ethical dilemmas; 2) individual ethical decision frameworks; 3) collective ethical decision frameworks; and 4) ethics in human-AI interactions</li> </ul>"},{"location":"Building%20Ethics%20into%20Artificial%20Intelligence/#types","title":"Types","text":"<ul> <li>Consequentialist ethics</li> <li>Utilitarian ethics</li> <li>Deontological ethics</li> <li>Virtue ethics</li> <li>Ethical dilemmas</li> </ul>"},{"location":"Building%20Ethics%20into%20Artificial%20Intelligence/#exploring-ethical-dilemmas","title":"Exploring Ethical Dilemmas","text":"<ul> <li>explore the ethical dilemmas in the target application scenarios [Anderson and Anderson, 2014]</li> <li>GenEth</li> <li>Moral Machine project</li> </ul>"},{"location":"Building%20Ethics%20into%20Artificial%20Intelligence/#individual-ethical-decision-frameworks","title":"Individual Ethical Decision Frameworks","text":"<ul> <li>AI research community largely agrees that generalized frameworks are preferred over ad-hoc rules</li> <li>if updates are provided by people, some review mechanisms should be put in place to prevent abuse</li> <li>moral decision-making by humans not only involves utilitarian considerations, but also moral rules.</li> <li>Such rules often involve protected values (a.k.a. sacred values)</li> <li>MoralDM</li> <li>Belief-Desire-Intention</li> <li>blind ethical judgement</li> <li>partially informed ethical judgement</li> <li>fully informed ethical judgement</li> <li>Moral decision making frameworks for artificial intelligence</li> <li>Preferences and ethical principles in decision making</li> <li>[[A declarative modular framework for representing and applying ethical principles.]]</li> <li>A low-cost ethics shaping approach for designing reinforcement learning agents</li> <li>Even angels need the rules AI, roboethics, and the law</li> <li>Norms as a basis for governing sociotechnical systems</li> <li>Embedding ethical principles in collective decision support systems</li> <li>A voting-based system for ethical decision making</li> <li>swap-dominance</li> <li>satisfying consequentialist ethics Ethics in Human-AI Interactions Belmont Report</li> <li>[Luckin, 2017; Yu et al., 2017b]</li> <li>1) people's personal autonomy should not be violated (they should be able to maintain their free will when interacting with the technology); 2) benefits brought</li> <li>about by the technology should outweigh risks; and 3) the benefits</li> <li>and risks should be distributed fairly among the users (people should not be discriminated based on their personal backgrounds such as race, gender and religion)</li> <li>persuasion agents</li> <li>[Kang et al., 2015; Rosenfeld and Kraus, 2016]</li> <li>[Stock et al., 2016]</li> <li>large-scale study to investigate human perceptions on the ethics of persuasion by an AI agent</li> <li>trolley scenario</li> <li>authors tested three persuasive strategies: 1) appealing to the participants emotionally; 2) presenting the participants with utilitarian arguments; and 3) lying</li> <li>participants hold a strong preconceived negative attitude towards the persuasion agent, and argumentation-based and lying-based persuasion strategies work better than emotional persuasion strategies</li> <li>did not show significant variation across genders or cultures</li> <li>adoption of persuasion strategies should take into account differences in individual personality, ethical attitude and expertise in the given domain.</li> <li>Coping Theory</li> <li>Argumentation-based explainable AI</li> <li>[Fan and Toni, 2015; Langley et al., 2017] well suited to the consequentialist ethics</li> <li>depending on how the explanations are used, researchers need to strike a balance on the level of details to be included</li> <li>Full transparency may be too overwhelming if the objective is to persuade a user to follow a time-critical recommendation</li> <li>useful as a mechanism to trace the AI decision process afterwards not enough transparency may hamper users' trust in the AI</li> </ul>"},{"location":"Building%20Ethics%20into%20Artificial%20Intelligence/#backlinks","title":"Backlinks","text":"<ul> <li>2023-01-18</li> <li>Building Ethics into Artificial Intelligence</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Burn-in/","title":"Burn-in","text":"<ul> <li>Burn-In is a robot testing procedure where all components of the robot are operated continuously for an extended period of time. This is done to test movement and movement programming of the robot at early stages to avoid malfunctions after deployment.</li> </ul>"},{"location":"C-section/","title":"C-section","text":"<ul> <li>Caesarian section, where a baby is delivered through an abdominal and uterine incision</li> </ul>"},{"location":"CAM/","title":"CAM","text":"<ul> <li>Zhou et al. 2016</li> </ul> <ul> <li>Class Activation Mapping</li> <li>Similar to [[Network In Network]]</li> <li>zeroes out the negative grads during backward pass to provide more visually appealing results</li> <li>Uses Global Average Pooling</li> <li></li> <li> \\[\\alpha_{k}^{c}= \\overbrace{\\frac{1}{Z}\\Sigma_{i}\\Sigma_{j}}^\\text{global avg pool} \\underbrace{\\frac{\\partial y^{c}}{\\partial A^{k}_{ij}}}_\\text{grads via backprop}\\] </li> <li>k is the index of the activation map in the last convolutional layer, and c is the class of interest. Alpha computed above shows the importance of feature map k for the target class c.</li> <li>Finally, we multiply each activation map by its importance score (i.e. alpha) and sum the values</li> </ul>"},{"location":"CAM/#chatgpt","title":"ChatGPT","text":"<ul> <li>The paper, \"Learning Deep Features for Discriminative Localization\" by Zhou et al. (2016) introduces the concept of Class Activation Mapping (CAM) as a way to visualize which regions of an image are most important for a given classification task. CAM is a technique for generating heatmaps that highlight the regions in an image that are most important for a specific classification. The authors propose to use global average pooling (GAP) in the final convolutional layer to generate a feature map, followed by a linear combination of the feature map and the class weight vector to generate a single class activation map.</li> <li>The authors apply CAM to the ResNet architecture and show that it outperforms the traditional fully-connected layer approach in terms of localization performance. They test the CAM on the image classification task using the ILSVRC-2012 dataset. The authors showed that by using CAM, they could identify the specific regions of an image that were important for a given classification, rather than just a \"black box\" decision made by the model. The authors also demonstrate how CAM can be used for fine-grained recognition, where the model is trained to identify sub-categories within a larger class.</li> <li>Additionally, the authors also show that the CAM can be used to improve the interpretability of deep neural networks by providing a visual representation of the model's decision-making process. They also use CAM to identify misclassifications and analyze the model's decision-making process. The authors test CAM on other architectures such as VGG and GoogleNet and show that it can be applied.</li> <li>The authors also use CAM for multi-label classification and show that it can identify the regions in an image that are relevant to multiple labels. They also use CAM for action classification in video and show that it can identify the regions of video frames that are important for a given action. They use CAM for object detection and show that it can be used to identify the regions of an image that contain an object of interest.</li> <li>The authors use CAM for fine-tuning a pre-trained model on a new dataset and show that it can be used to improve the performance of the model on the new dataset. They also use CAM for unsupervised feature learning and show that it can be used to learn features that are useful for a wide range of tasks. They use CAM for zero-shot learning and show that it can be used to identify the regions of an image that are relevant to a class that the model has never seen before.</li> <li>Finally, the authors use CAM for domain adaptation and show that it can be used to identify the regions of an image that are important for a specific task, even when the model has been trained on a different dataset. They also use CAM for weakly-supervised object localization and show that it can be used to identify the regions of an image that contain an object of interest, even when only image-level labels are available. They use CAM for multi-modal learning and show that it can be used to identify the regions of an image that are important for a given task, even when multiple modalities (e.g. image and text) are available.</li> <li>The authors conclude that the CAM is a powerful technique for visualizing the decision-making process of a deep neural network and can be used to improve the interpretability, performance, and robustness of deep models.</li> </ul>"},{"location":"CBOW/","title":"CBOW","text":"<ul> <li>Continous implementation of Bag of words</li> <li>tries to predict the current target word (the center word) based on the source context words (surrounding words)</li> <li>\u201cthe quick brown fox jumps over the lazy dog\u201d, this can be pairs of\u00a0(context_window, target_word)\u00a0where if we consider a context window of size 2, we have examples like\u00a0([quick, fox], brown), ([the, brown], quick), ([the, dog], lazy)\u00a0and so on</li> <li>context window</li> <li></li> <li>several times faster to train than the skip-gram, slightly better accuracy for the frequent words.</li> <li>CBOW is prone to overfit frequent words because they appear several time along with the same context.</li> <li>tends to find the probability of a word occurring in a context</li> <li>it generalizes over all the different contexts in which a word can be used</li> <li>also a 1-hidden-layer neural network</li> <li>The synthetic training task now uses the average of multiple input context words, rather than a single word as in skip-gram, to predict the center word.</li> <li>Again, the projection weights that turn one-hot words into averageable vectors, of the same width as the hidden layer, are interpreted as the word embeddings.</li> </ul>"},{"location":"CBOW/#backlinks","title":"Backlinks","text":"<ul> <li>Word2Vec</li> <li>Skip Gram or CBOW</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"CDF/","title":"CDF","text":"<ul> <li>get cumulative density function \\(\\varphi : \\mathbb{R} \\rightarrow [0,1]\\)</li> </ul>"},{"location":"CIFAR/","title":"CIFAR","text":"<ul> <li>60'000 images</li> <li>10 classes with 6'000 images</li> <li>image size: 32x32x3</li> <li>50'000 training, 10'000 testing</li> </ul>"},{"location":"CIFAR/#backlinks","title":"Backlinks","text":"<ul> <li>Real Time Image Saliency for Black Box Classifiers</li> <li>CIFAR</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"CLIP/","title":"CLIP","text":"<ul> <li>Learning Transferable Visual Models from Natural Language Supervision</li> <li>introduces CLIP, a pre-training task which efficiently learns visual concepts from natural language supervision</li> <li>performs language-guided image generation</li> <li>uses vision and language encoders trained in isolation and uses a contrastive loss to bring similar image-text pairs closer, while pulling apart dissimilar pairs as a part of pretaining</li> <li>can be applied to any visual classification benchmark by simply providing the names of the visual categories to be recognized, similar to the \u201czero-shot\u201d capabilities of GPT and GPT3</li> <li>pre-trains an image encoder and a text encoder to predict which images were paired with which texts in our dataset</li> <li>zero-shot classifier</li> <li>they convert all of a dataset\u2019s classes into captions such as \u201ca photo of a dog\u201d and predict the class of the caption CLIP estimates best pairs with a given image</li> </ul>"},{"location":"COCO/","title":"COCO","text":""},{"location":"CRISPR%20%28clustered%20Regularly-interspaced%20Short%20Palindromic%20repeats%29/","title":"CRISPR (clustered Regularly-interspaced Short Palindromic repeats)","text":"<ul> <li>A relatively precise and reliable DNA-editing technique.</li> </ul>"},{"location":"CTC/","title":"CTC","text":"<ul> <li>Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks</li> <li>Many real-world sequence learning tasks require the prediction of sequences of labels from noisy, unsegmented input data</li> <li>Recurrent neural networks (RNNs) are powerful sequence learners that would seem well suited to such task</li> <li>hey require pre-segmented training data, and post-processing to transform their outputs into label sequences, their applicability has so far been limited</li> <li>temporal classification</li> <li>label unsegmented sequences directly</li> <li>probabilistic principles</li> <li>TIMIT speech corpus</li> </ul>"},{"location":"CUB-200-2011%204/","title":"CUB-200-2011","text":"<ul> <li>Caltech-UCSD Birds 200 (CUB-200) is a dataset of images of birds, with 200 different species of birds and 11,788 images in total.</li> </ul>"},{"location":"CUB-200-2011%204/#backlinks","title":"Backlinks","text":"<ul> <li>Fine Grained Datasets</li> <li>CUB-200-2011</li> </ul> <p>Backlinks last generated 2023-01-25 01:31:24</p>"},{"location":"CUB-200-2011/","title":"CUB-200-2011","text":"<ul> <li>Caltech-UCSD Birds 200 (CUB-200) is a dataset of images of birds, with 200 different species of birds and 11,788 images in total.</li> </ul>"},{"location":"CUB-200-2011/#backlinks","title":"Backlinks","text":"<ul> <li>Fine Grained Datasets</li> <li>CUB-200-2011</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Cache%20Coherence/","title":"Cache Coherence","text":"<ul> <li>Individual CPU caches or memories can become out of synch with each other</li> <li>if one processor updates a location in shared memory, all the other processors know about the update</li> </ul>"},{"location":"Calibration%20Layer/","title":"Calibration Layer","text":"<ul> <li>A post-prediction adjustment, typically to account for prediction bias. The adjusted predictions and probabilities should match the distribution of an observed set of labels.</li> </ul>"},{"location":"Candidate%20Sampling/","title":"Candidate Sampling","text":"<ul> <li>A training-time optimization in which a probability is calculated for all the positive labels, using, for example, softmax, but only for a random sample of negative labels. For example, if we have an example labeled beagle and dog candidate sampling computes the predicted probabilities and corresponding loss terms for the beagle and dog class outputs in addition to a random subset of the remaining classes (cat, lollipop, fence). The idea is that the negative classes can learn from less frequent negative reinforcement as long as positive classes always get proper positive reinforcement, and this is indeed observed empirically. The motivation for candidate sampling is a computational efficiency win from not computing predictions for all negatives.</li> </ul>"},{"location":"Capacitance/","title":"Capacitance","text":"<ul> <li>Charge stored/Potential Difference</li> <li> \\[C= Q/V\\] </li> </ul>"},{"location":"Capture%20bias/","title":"Capture Bias","text":"<ul> <li>photographers tending to take pictures of objects in similar ways</li> <li>Searching for \"mug\" on Google Image Search will reveal another kind of capture bias: almost all the mugs has a right-facing handle</li> <li>Beyond better data sampling strategies, one way to deal with this is to perform various data transformations to reduce this bias</li> </ul>"},{"location":"Capture%20bias/#backlinks","title":"Backlinks","text":"<ul> <li>Unbiased Look at Dataset Bias</li> <li>Capture bias</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Cardinality%20Principle/","title":"Cardinality Principle","text":""},{"location":"Cardinality%20Principle/#cardinality-principle","title":"Cardinality Principle","text":"<ul> <li>how each step in the counting sequence (1,2,3,4,5,6\u2026.) means an increase of one individual</li> </ul>"},{"location":"Carousel/","title":"Carousel","text":"<ul> <li>A rotating platform that delivers objects to a robot and serves as an object queuing system. This carousel delivers the objects, or work pieces, to the loading/unloading station of the robot.</li> </ul>"},{"location":"Case%20Grammar/","title":"Case Grammar","text":"<ul> <li>The structure that is built by the parser contains some semantic information, although further interpretation may also be necessary</li> </ul>"},{"location":"Causability/","title":"Causability","text":"<ul> <li>measures how exact an interpretable model is in imitating the behavior of a black box. fidelity</li> <li>It is measured in terms of the accuracy score, but with respect to the outcome of the black box, similarly to the model accuracy.</li> </ul>"},{"location":"Causal%201D%20Conv/","title":"Causal 1D Conv","text":"<ul> <li>Only past info used for prediction</li> <li>Conv works in both directions and can leak future information into predictions</li> </ul>"},{"location":"Causal%20Dilated%20Conv/","title":"Causal Dilated Conv","text":"<ul> <li>Receptive field is how much of the input sequence is needed for one prediction</li> </ul>"},{"location":"Causal%20Language%20Model/","title":"Causal Language Model","text":"<ul> <li>Unlike Masked Language Modeling, this is uni-directional.</li> <li>Can consider words to its left</li> <li>Better for generating text</li> </ul>"},{"location":"Causal%20Systems/","title":"Causal Systems","text":"<ul> <li>Does not depend on future input</li> <li>Has memory if current input not fully determined by previous one but influenced by earlier inputs</li> <li>TIme Series</li> </ul>"},{"location":"Causality/","title":"Causality","text":"<ul> <li>An understandable explanation must be created by a machine in a given time (e.g., one hour or one day) and can be comprehended by a user, who need not to be an expert, but has an educational background</li> <li>The user keeps asking a finite number of questions of the machine until he/she can no longer ask why or how because he/she has a satisfactory answer; we say he/she has comprehended.</li> <li>This is the relationship between cause and effect; it is not a synonym for causability</li> <li>Causability is about measuring and ensuring the quality of an explanation and refers to a human model</li> </ul>"},{"location":"CenterNet/","title":"CenterNet","text":"<p>title: CenterNet</p> <p>tags: architecture</p>"},{"location":"CenterNet/#centernet","title":"CenterNet","text":"<ul> <li>paper</li> <li>center point-based object detection approach</li> <li>end-to-end differentiable</li> <li>bounding box based detectors</li> <li>Anchorless</li> <li>keypoint estimation networks to find center points   id:: 62a89b04-c7bf-4205-9695-39da04c2aafb</li> <li>Linear Regression to all other properties   id:: 62a89d01-53af-46f6-82d4-362fab069b46</li> <li>COCO</li> <li>Single stage   id:: 62a89d42-312d-4f3d-8235-c54a9dfafadf</li> </ul>"},{"location":"Central%20Limit%20Theorem/","title":"Central Limit Theorem","text":"<ul> <li>When random effects of many independant small sized causes sum up to large scale observable effects : one gets the Normal Distribution</li> <li>Let \\((X_{i})_{i\\in N}\\) is a seq of independant, real valued, [(X_{i}- EX_{i}%20=%20E%5B%5BX_%7Bi%7D-%20E%5BX_%7Bi%7D) \\(P_{S_{n}}\\) of standardized sum variables converge weakly to \\(\\mathscr{N}(0,1|[Square Integrable]]\\) . (\\(S_{n}= \\frac{\\Sigma_{i= 1}^{n}(X_{i}- E[X_{i}])}{\\sigma(\\Sigma^{n}_{i=1}X_{i})}\\)\\)<ul> <li>Converge weakly : \\(\\(lim_{n\\rightarrow\\infty}\\int f(x)P_{n}(dx) = \\int f(x)P(dx)\\)\\) for all \\(f: \\mathbb{R} \\rightarrow \\mathbb{R}\\)</li> <li>Lebesgue Integrals</li> </ul> </li> </ul>"},{"location":"Central%20Limit%20Theorem/#x_i-are-identically-distributed","title":"\\(X_{i}\\) Are Identically Distributed","text":"<ul> <li>Regardless of shape of each \\(X_{i}\\), distribution of normalized sum converges to \\(\\mathscr{N}(0,1)\\)</li> <li>Uniformly bounded</li> <li>None of the \\(X_{i}\\) dominates the other \"washing out\"</li> </ul>"},{"location":"Central%20Nervous%20System/","title":"Central Nervous System","text":"<ul> <li>brain + Spinal Cord</li> </ul>"},{"location":"Central%20Sulcus/","title":"Central Sulcus","text":"<ul> <li>The primary groove in the brain\u2019s cerebrum, which separates the frontal lobe in the front of the brain from the parietal and occipital lobes in the rear of the brain.</li> </ul>"},{"location":"Centrifugal%20Force/","title":"Centrifugal Force","text":"<ul> <li>When a body rotates about an axis other than one at it's center of mass, it exerts an outward radial force called centrifugal force upon the axis, which restrains it from moving in a straight tangential line. To offset this force, the robot must exert an opposing torque at the joint of rotation.</li> </ul>"},{"location":"Centripetal%20Force/","title":"Centripetal Force","text":"<ul> <li>centripetal force =\u00a0mass x speed2\u00a0  radius of path</li> <li>\\(\\(F_{C}= \\frac{mv^{2}}{r}\\)\\) </li> </ul>"},{"location":"Centroid/","title":"Centroid","text":"<ul> <li>The center of a cluster as determined by a k-means or k-median algorithm. For instance, if k is 3, then the k-means or k-median algorithm finds 3 centroids.</li> </ul>"},{"location":"Cerebellar%20Artery/","title":"Cerebellar Artery","text":"<ul> <li>The major blood vessel providing oxygenated blood to the cerebellum.</li> </ul>"},{"location":"Cerebellum/","title":"Cerebellum","text":"<ul> <li>Its function is to coordinate muscle movements, maintain posture, and balance</li> <li>relays information to the Basal Ganglia.</li> <li>It stores automatic learned memories like tying a shoe, playing an instrument, or riding a bike.</li> </ul>"},{"location":"Cerebral%20Palsy/","title":"Cerebral Palsy","text":"<ul> <li>A developmental disorder resulting from damage to the brain before or during birth, usually characterized by impaired muscle coordination and body movements, but can also include impaired cognition and social behavior.</li> </ul>"},{"location":"Cerebrospinal%20Fluid%20%28CSF%29/","title":"Cerebrospinal Fluid (CSF)","text":"<ul> <li>The clear, colorless liquid found surrounding the brain and spinal cord. This fluid can be analyzed to detect diseases.</li> </ul>"},{"location":"Cerebrum/","title":"Cerebrum","text":"<ul> <li>largest part of the brain</li> <li>performs higher functions like interpreting touch, vision and hearing, as well as speech, reasoning, emotions, learning, and fine control of movement</li> <li>Divided by Corpus callosum</li> <li>Surface is called the Brain Cortex</li> <li></li> <li>Frontal lobe</li> <li>Parietal lobe</li> <li>Occipital lobe</li> <li>Temporal lobe</li> </ul>"},{"location":"Chain%20of%20Thought/","title":"Chain of Thought","text":"<ul> <li>Chain of Thought Prompting Elicits Reasoning in Large Language Models</li> <li>ability of language models to generate a coherent chain of thought</li> <li>series of short sentences that mimic the reasoning process a person might have when responding to a question</li> <li>the more complex the task of interest is (in the sense of requiring multi-step reasoning approach), the bigger the boost from the chain of thought prompting!</li> <li>chain of thought processing is an emergent property of model scale that can be induced via prompting and can enable sufficiently large language models to better perform reasoning tasks that otherwise have flat scaling curves.</li> </ul>"},{"location":"Challenges%20of%20Words-and-rules/","title":"Challenges of Words-and-rules","text":"<ul> <li>Words-and-Rules fits a lot of the data, but is vague on<ul> <li>Exactly what innate structures are available to a child learner</li> <li>Exactly how learning proceeds</li> </ul> </li> </ul>"},{"location":"Change%20Blindness/","title":"Change Blindness","text":"<ul> <li>Difficulty detecting changes in separated scenes even after careful inspection</li> <li>Once found viewers agree that it was trivial</li> <li>Not due to limited visual acuity but inappropriate attentional guidance</li> <li>Temporal separation (instead of spatial)</li> <li>sudden changes within a static scene are easily perceived (cf. preattentiveness of motion)</li> <li>Change blindness also in animations if temporal separation spans multiple scenes</li> <li>A scene that should be the same but differs between cuts is known as continuity error</li> </ul>"},{"location":"Change%20in%20Gravitational%20Potential%20Energy/","title":"Change in Gravitational Potential Energy","text":"<ul> <li>mass x gravitational field strenth x\u00a0 difference in height</li> <li> \\[DGPE = mgDh\\] </li> </ul>"},{"location":"Character-set%20dependence/","title":"Character-set Dependence","text":"<ul> <li>ASCII</li> <li>8 bit character set</li> <li>2 byte character set</li> <li>Unicode 5.0</li> </ul>"},{"location":"Characteristics%20of%20Visual%20Variables/","title":"Characteristics of Visual Variables","text":"<ul> <li>Visual Selective</li> <li>Visual Associative</li> <li>Visual Ordered</li> <li>Visual Quantitative</li> <li>Visual Length </li> </ul>"},{"location":"Charge/","title":"Charge","text":"<ul> <li>Current x time</li> <li> \\[DQ = IDt\\] </li> </ul>"},{"location":"Chebyshev%20Distance/","title":"Chebyshev Distance","text":"<ul> <li> \\[D(x,y) = max_{i}(|x_{i}-y_{i}|)\\] </li> <li>greatest of difference between two vectors along any coordinate dimension</li> <li>simply the maximum distance along one axis.</li> <li>Chessboard distance since the minimum number of moves needed by a king to go from one square to another is equal to Chebyshev distance</li> <li>can be used to extract the minimum number of moves needed to go from one square to another</li> <li>warehouse logistics as it closely resembles the time an overhead crane takes to move an object</li> </ul>"},{"location":"CheckList/","title":"CheckList","text":"<ul> <li>Beyond Accuracy: Behavioral Testing of NLP Models with CheckList</li> <li>ML systems can run to completion without throwing any errors (indicating functional correctness) but can still produce incorrect outputs (indicating behavioral issues)</li> <li>CheckList</li> <li>model-agnostic and task-agnostic methodology for testing NLP models inspired by principles of behavioral testing</li> <li>matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation</li> <li>Minimum Functionality Test (MFT): A Minimum Functionality Test (MFT) uses simple examples to make sure the model can perform a specific task well. For example, they might want to test the performance of a sentiment model when dealing with negations</li> <li>Invariance Test: Besides testing the functionality of a model, they might also want to test if the model prediction stays the same when trivial parts of inputs are slightly perturbed. These tests are called Invariance Tests (IV)</li> <li>Directional Expectation Test: In the Invariance Test, they expect the outputs after the perturbation to be the same. However, sometimes they might expect the output after perturbation to change. That is when Directional Expectation Tests comes in handy</li> </ul>"},{"location":"Chi%20Squared%20Distance/","title":"Chi Squared Distance","text":"<ul> <li> \\[d= \\Sigma_{i}\\frac{p_{i}-q_{i}}{p_{i}+q_{i}}\\] </li> </ul>"},{"location":"Chimera/","title":"Chimera","text":"<ul> <li>A single organism with cells from more than one distinct genotype.</li> </ul>"},{"location":"Chinchilla/","title":"Chinchilla","text":"<ul> <li>Training Compute-Optimal Large Language Models</li> <li>given a 10x increase in computational budget, model size should increase 5.5x, and the number of tokens should only increase 1.8x</li> <li>model and data size should increase in accordance</li> <li>collecting high-quality datasets will play a key role in further scaling of LLMs</li> <li>optimal model size and number of tokens for training a transformer language model under a given compute budget</li> <li>By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, they find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled</li> <li>significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks</li> <li>ubstantially less compute for fine-tuning and inference, greatly facilitating downstream usage</li> <li>MMLU</li> </ul>"},{"location":"Chronic%20Encephalopathy%20Syndrome%20%28CES%29/","title":"Chronic Encephalopathy Syndrome (CES)","text":"<ul> <li>Symptoms, including memory issues, depression, and impulsive behavior, that manifest themselves after repeated brain traumas. Over time, CES can result in a diagnosis of chronic traumatic encephalopathy (CTE)).md).</li> </ul>"},{"location":"Chronic%20Traumatic%20Encephalopathy%20%28CTE%29/","title":"Chronic Traumatic Encephalopathy (CTE)","text":"<ul> <li>Once known as dementia pugilistica and thought to be confined largely to former boxers, this neurodegenerative disease, with symptoms including impulsivity, memory problems, and depression, affects the brains of individuals who have suffered repeated concussions and traumatic brain injuries.</li> </ul>"},{"location":"Chronic/","title":"Chronic","text":"<ul> <li>Describes a condition that is persistent or recurring</li> </ul>"},{"location":"Circular%20Motion%20Type/","title":"Circular Motion Type","text":"<ul> <li>A calculated path that the robot executes, and is circular in shape.</li> </ul>"},{"location":"Circumfix/","title":"Circumfix","text":"<ul> <li>precede and follow the stem</li> </ul>"},{"location":"Cityscapes/","title":"Cityscapes","text":""},{"location":"Clamp/","title":"Clamp","text":"<ul> <li>An end-effector which serves as a pneumatic hand that controls the grasping and releasing of an object. Tactile, and feed-back force sensors are used to manage the applied force to the object by the clamp.</li> </ul>"},{"location":"Clamping/","title":"Clamping","text":"<ul> <li>The maximum permissible force acting on a body region, resulting from a robot collision where the period of contact results in a plastic deformation of a person\u2019s soft tissue.</li> </ul>"},{"location":"Class%20Conditional%20distribution/","title":"Class Conditional Distribution","text":"<ul> <li>\\(f_{i}\\) is the PDF for \\(P_{X|Y=c_{i}}\\)</li> </ul>"},{"location":"Class%20Size/","title":"Class Size","text":"<ul> <li>Class inclusion seq : Set of candidate models with increasing flexibility</li> <li> \\[\\mathcal{H}_{1} \\subset \\mathcal{H}_{2} \\subset, \u2026, \\subset \\mathcal{H}_{l} \\] </li> </ul>"},{"location":"Classification%20Ray%20Casting/","title":"Classification Ray Casting","text":"<ul> <li>Transfer Function</li> <li>Pre Classification</li> <li>Post Classification</li> </ul>"},{"location":"Classifier%20Gradients/","title":"Classifier Gradients","text":"<ul> <li>For example, if we want to add sunglasses to an image of a face, we can used a trained classifier that identifies if a personal has that feature.</li> <li>To do this, we can take a batch of noise vector Z that goes through the generator.</li> <li>We then pass this image through a classifier, in this case a sunglasses classifier, which will tell us if the output has that feature.</li> <li>We the use this information to modify the Z vectors, without modifying the weights of the generator at all.</li> <li>To do so, we modify the Z vectors by moving in the direction of the gradient with the costs that will penalize the model for images classified as not having sunglasses. </li> <li>We then repeat this process until the images are classified with the desired feature.</li> <li>The downside with this method is that we need a pre-trained classifier that can detect the desired feature, which may not always be readily available.</li> </ul>"},{"location":"Classifier%20Gradients/#backlinks","title":"Backlinks","text":"<ul> <li>Conditional GAN</li> <li>Classifier Gradients</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Classifying%20a%20specific%20image%20region%20using%20convolutional%20nets%20with%20an%20ROI%20mask%20as%20input/","title":"Classifying a Specific Image Region Using Convolutional Nets with an ROI Mask as Input","text":"<ul> <li>Eppel, Sagi. \u201cClassifying a Specific Image Region Using Convolutional Nets with an ROI Mask as Input,\u201d n.d., 8.</li> </ul>"},{"location":"Classifying%20a%20specific%20image%20region%20using%20convolutional%20nets%20with%20an%20ROI%20mask%20as%20input/#intro","title":"Intro","text":"<ul> <li>In some cases, it is desirable to classify only a specific region of the image that corresponds to a certain object.</li> <li>Hence, assuming that the region of the object in the image is known in advance and is given as a binary region of interest (ROI) mask, the goal is to classify the object in this region using a convolutional neural net.</li> <li>This goal is achieved using a standard image classification net with the addition of a side branch, which converts the ROI mask into an attention map. This map is then combined with the image classification net</li> <li>focus the attention on the object region while still extracting contextual cues from the background</li> <li>COCO</li> <li>OpenSurfaces materials dataset</li> </ul>"},{"location":"Classifying%20a%20specific%20image%20region%20using%20convolutional%20nets%20with%20an%20ROI%20mask%20as%20input/#network","title":"Network","text":"<ul> <li>combining the attention map at the first layer of the net gave better results than combining it at higher layers of the net</li> <li>An alternative approach is to generate an attention map, which can be used by the net to extract features from both objects and the background using the ROI mask as an additional input to the net</li> <li>An attention map can easily be generated from the input ROI mask using a convolution layer</li> <li>This attention map is then combined with one or more layers of the main branch, either by element-wise addition or multiplication</li> <li>The combined layer is then used as an input for the next layer of the main branch</li> <li>In order to allow element-wise addition or multiplication, the attention map must be the same size as the layer with which it is combined. To achieve this, the ROI mask was first resized to match the size of the layer with which it was merged, and a convolution layer was then applied with the same number of filters as the depth of the target layer.</li> <li>For cases where the attention maps were combined with more than one layer , a separate attention map was generated using different convolution filters for each layer</li> </ul>"},{"location":"Classifying%20a%20specific%20image%20region%20using%20convolutional%20nets%20with%20an%20ROI%20mask%20as%20input/#net-initiation","title":"Net Initiation","text":"<ul> <li>The convolution layer of the side branch was initialized as follows: if the attention map was to be merged by element-wise addition, both the weights and the bias were initialized to zero; if the attention map was to be merged multiplication, the bias was set to one and the filter weights to zero</li> <li>This weights initiation method promise that the initial effect of the attention branch on the classification branch is zero at the outset and increases gradually during training.</li> </ul>"},{"location":"Classifying%20a%20specific%20image%20region%20using%20convolutional%20nets%20with%20an%20ROI%20mask%20as%20input/#datasets","title":"Datasets","text":"<ul> <li>The nets were also trained using the OpenSurfaces material classification dataset1 0 ; in this case, the ROI was generated by taking a connected region of the image corresponding to a single material, and the output was the material type.</li> </ul>"},{"location":"Classifying%20a%20specific%20image%20region%20using%20convolutional%20nets%20with%20an%20ROI%20mask%20as%20input/#results","title":"Results","text":"<ul> <li>It can be seen that methods based on generating an attention map and combining it with the main branch net branch gave considerably better accuracy than hard attention methods based on blacking out the background region3</li> <li>The difference in accuracy is particularly large for the classification of small segments where background information is more important in classification.</li> <li>Merging the attention map with the first layer of the net gave significantly better results than merging at higher layers</li> <li>This probably due to the fact that higher layers of the net suffer from a loss of high-resolution information that is relevant in the classification of small objects.</li> <li>Generating several attention maps and merging them with multiple layers of the net gave the same or worse results than generating a single attention map and merging it with the first layer</li> </ul>"},{"location":"Classifying%20a%20specific%20image%20region%20using%20convolutional%20nets%20with%20an%20ROI%20mask%20as%20input/#images","title":"Images","text":""},{"location":"Clear%20Thinking/","title":"Clear Thinking","text":"<ul> <li>Developing good abstractions, notations, visualizations, and so forth, is improving the user interfaces for ideas.</li> <li>This helps both with understanding ideas for the first time and with thinking clearly about them.</li> <li>Conversely, if we can\u2019t explain an idea well, that\u2019s often a sign that we don\u2019t understand it as well as we could.</li> </ul>"},{"location":"Clustering/","title":"Clustering","text":"<ul> <li>KMeans</li> <li>SOMs</li> </ul>"},{"location":"Clutter%20In%20Visualisation/","title":"Clutter In Visualisation","text":""},{"location":"Co%20adaptation/","title":"Co Adaptation","text":"<ul> <li>Computing the gradient is done with respect to the error, but also with respect to what all other units are doing (Srivastava et al., 2014). This means that certain neurons, through changes in their weights, may fix the mistakes of other neurons. These, Srivastava et al. (2014) argue, lead to complex co-adaptations that may not generalize to unseen data, resulting in overfitting.</li> </ul>"},{"location":"Co-Mixup/","title":"Co-Mixup","text":"<ul> <li>salient image mixing on a batch of input images to generate a batch of augmented images</li> <li>This technique maximizes saliency in output images by penalizations to ensure local data smoothness and diverse image regions</li> </ul>"},{"location":"Co-training/","title":"Co-training","text":"<ul> <li>A semi-supervised learning approach particularly useful when all of the following conditions are true</li> <li>The ratio of unlabeled examples to labeled examples in the dataset is high.</li> <li>This is a classification problem (binary or multi-class).</li> <li>The dataset contains two different sets of predictive features that are independent of each other and complementary.</li> <li>Co-training essentially amplifies independent signals into a stronger signal.</li> </ul>"},{"location":"Coarse-grained%20assessment/","title":"Coarse-grained Assessment","text":"<ul> <li>If the required assessment is coarse grained, such as a single number reporting the student's competence on the current unit, then it is usually computed from several measures such as:</li> <li>A measure of progress and coverage, such as the number of problems solved or the number of correct steps.</li> <li>A measure of the amount of help given, such as the number of hint sequences started and the proportion that ended with a bottom-out hint.</li> <li>Some measure of competence, such as the frequency of incorrect initial steps, the time required to enter a correct step, or the number of attempts at a step before it is entered correctly.</li> <li>Psychometrics is the field that studies how to do this for conventional tests (e.g., multiple choice tests)</li> <li>One of their main tools is item-response theory IRT</li> </ul>"},{"location":"Cochlea/","title":"Cochlea","text":"<ul> <li>The part of the inner ear that transforms sound vibrations into neural impulses.</li> </ul>"},{"location":"CogMod%20Final%20Paper/","title":"CogMod Final Paper","text":"<ul> <li>The Reward Experiment</li> <li>An ACT-R model that explains at least one of these effects (well)  </li> <li>Report to justify design choices</li> <li>\u2018cognitive\u2019 interpretation, that is justifiable &amp; plausible</li> </ul>"},{"location":"CogMod%20Final%20Paper/#to-test","title":"To test","text":"<ul> <li>linear decrease in goal activation </li> </ul>"},{"location":"CogMod%20Final%20Paper/#papers","title":"Papers","text":"<ul> <li>[[Sequential effects within a short foreperiod context: Evidence for the conditioning account of temporal preparation]] : The sequential effects and fanning. But properly</li> <li>[[Traces of times past: Representations of temporal intervals in memory]] : Explain the fanning effect</li> <li>[[The warning stimulus as retrieval cue: The role of associative memory in temporal preparation]] : temporal preparation , sloping effects</li> <li>[[Modeling motivation using goal competition in mental fatigue studies]] : performance in reward vs non reward , distraction, linear decrease in goal activation</li> <li>[[The neural correlates of mental fatigue and reward processing: A task-based fMRI study]] : studies the physical brain effect of reward and fatigue</li> <li>[[Implicitly learning when to be ready: From instances to categories]] : fanning , maybe the graphs can also be explained by categorical association instead of just instance based</li> <li>[[On the Distinction Between Perceived Duration and Event Timing: Towards a Unified Model of Time Perception]] : brain uses temporal expectations to bias perception in a way that stimuli are \u2018regularized\u2019</li> <li>[[Change of VariableForeperiod Effects within an Experiment: A Bayesian Modeling Approach]] : sequential modulation, which is attributed to feature binding and retrieval by the BRAC framework, could have different underlying mechanisms depending on the task scenario.</li> </ul>"},{"location":"CogMod%20Final%20Paper/#backlinks","title":"Backlinks","text":"<ul> <li>02:37 CogMod Final Paper</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Cognition%20Hazard%20Rates/","title":"Cogntition Hazard Rates","text":"<ul> <li>{Proven wrong} : Cognitive fMTP</li> <li>Mathematical construct about probability</li> <li>Continuously tracking the odds the event appeads rn given it has not happened yet</li> <li>Idea : Use this \"hazard rate\" to decide when to prepare</li> <li>RT is proportional to hazard</li> <li>Optimally prepared if certain</li> </ul>"},{"location":"Cognition%20Hazard%20Rates/#distributions-used-pdf","title":"Distributions Used (PDF)","text":"<ul> <li>Constant, exponential, flipped exponential</li> <li>Hazard rate is this pdf by 1-F</li> <li> \\[h(t) = \\frac{f(t)}{1-F(t)}\\] </li> <li></li> </ul>"},{"location":"Cognition%20Hazard%20Rates/#hazard-rates","title":"Hazard Rates","text":"<ul> <li>How does that translate to RT?</li> <li>Proposed<ul> <li>\\(RT = c- h(t)\\) : linear effect</li> <li>\\(RT = c+ \\frac{1}{h(t)}\\) : inverse relation</li> </ul> </li> <li>dashed : \\(\\frac{1}{hazard}\\)</li> <li></li> <li></li> </ul>"},{"location":"Cognition%20Hazard%20Rates/#vs-act-r","title":"Vs ACT-R","text":"<ul> <li>Prepare for \u2018the right moment\u2019  <ul> <li>\u2018degree of preparation\u2019 given by moment-to-moment hz  </li> </ul> </li> <li>\u2018the right moment\u2019 is estimated based on time (pulses) and memory (DM)  <ul> <li>No \u2018time\u2019, no explicit memory?  </li> </ul> </li> <li>If we are prepared\u2192 benefit, else cost  <ul> <li>Scaled benefits (useful for assignment)</li> <li>Does not specify why/how; i.e., what preparation is  </li> </ul> </li> <li>No active process during the interval  <ul> <li>Active tracking</li> </ul> </li> <li>Once we are prepared, it doesn\u2019t \u2018go away\u2019  <ul> <li>A by-product of the Hazard rate</li> </ul> </li> <li>No memory model  <ul> <li>Such mathematical models give no mechanism for how the pdf is stored in memory, retrieved, or used\u2026</li> </ul> </li> </ul>"},{"location":"Cognition%20Hazard%20Rates/#problems","title":"Problems","text":"<ul> <li>Does not explain preparation</li> <li>How do particpants \u2018learn\u2019 the distribution?</li> <li>Do participants truly track \u2018conditional probabilities\u2019 throughout the foreperiod</li> </ul>"},{"location":"Cognition%20Hazard%20Rates/#extending","title":"Extending","text":"<ul> <li>Does not store PDFs in memory, which sucks<ul> <li>Does not keep track of time as well</li> </ul> </li> <li>Subjective hazard/ anticipation function<ul> <li>Temporal uncertainty</li> <li>Blur the pdf such that later points are less certain using a Gaussian filter that gets wider for later points in time</li> <li> \\[f'(t) = \\frac{1}{\\theta t \\sqrt{2 \\pi}} \\int_{-\\infty}^{\\infty}f(\\tau)e^{-\\frac{(r-t)^{2}}{2 \\theta^{2}t^{2}}}d \\tau\\] </li> <li>Climb to 1 after a while</li> <li>Hazard is more even though probs are equal in classical. This equates them and makes them less blurred out</li> <li></li> </ul> </li> </ul>"},{"location":"Cognition%20Hazard%20Rates/#images","title":"Images","text":""},{"location":"Cognition%20Hazard%20Rates/#backlinks","title":"Backlinks","text":"<ul> <li>Cognitive Foreperiod</li> <li> <p>Cognition Hazard Rates</p> </li> <li> <p>The Reward Experiment</p> </li> <li> <p>Cognition Hazard Rates</p> </li> <li> <p></p> </li> <li>01:13 Cognition Hazard Rates</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Cognitive%20Foreperiod/","title":"Cognitive Foreperiod","text":"<ul> <li>Time before stimulus</li> <li>Prepare to act</li> </ul>"},{"location":"Cognitive%20Foreperiod/#constant-fp","title":"Constant FP","text":"<ul> <li>No uncertainty</li> <li>Up to ~150ms : RT decrease, then increase</li> <li>People prepare if warning - faster</li> <li> <p>if longer intervals</p> <ul> <li>avg response time increases</li> <li>Temporal estimates get noiser : Temporal Uncertainty<ul> <li>not really prepared</li> </ul> </li> </ul> </li> <li> <p>Faster if less time to prepare</p> </li> </ul>"},{"location":"Cognitive%20Foreperiod/#variable-fp","title":"Variable FP","text":"<ul> <li>Faster if more time to prepare</li> <li>Asymptotic decrease : plateau</li> <li>Try your best based on exp to be prepared<ul> <li>But stay prepared if you already are</li> </ul> </li> <li>\"Strategic\" : aim to be prepared as late<ul> <li>Why? Dunno. Maybe energy conservation</li> </ul> </li> </ul>"},{"location":"Cognitive%20Foreperiod/#distribution-effects","title":"Distribution Effects","text":"<ul> <li>Uniform</li> <li>Exponential<ul> <li>Many shorts</li> </ul> </li> <li>Anti Exponential<ul> <li>Many long</li> </ul> </li> <li>Prep strategy altered based on which type of distribution</li> </ul>"},{"location":"Cognitive%20Foreperiod/#sequential-effects","title":"Sequential Effects","text":"<ul> <li>Prep dependent on previous trials<ul> <li>If prev short, present longer : RTs are slow</li> </ul> </li> <li>Traces of gradually forgetting previous trials<ul> <li>in the shape of prep effects</li> </ul> </li> <li>Cannot just be accounted for by n-1 trials</li> </ul>"},{"location":"Cognitive%20Foreperiod/#transfer-effects","title":"Transfer Effects","text":"<ul> <li>Start with uniform - then something else - then uniform again</li> <li>Long lasting effects</li> <li>Even if participants were informed that things changed</li> <li>Even a week later</li> <li></li> </ul>"},{"location":"Cognitive%20Foreperiod/#motivation-determined-by","title":"Motivation Determined by","text":"<ul> <li>Time</li> <li>Memory</li> <li> <p>Motivation - still works on earlier prep?</p> </li> <li> <p>Cognition Hazard Rates</p> </li> </ul>"},{"location":"Cognitive%20Foreperiod/#backlinks","title":"Backlinks","text":"<ul> <li>Cognitive Preparation</li> <li>Cognitive Foreperiod</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Cognitive%20Multitasking/","title":"Cognitive Multitasking","text":"<ul> <li>No overlap between areas of brain in fMRI with similar tasks concurrently</li> <li>Threaded Cognition</li> <li>Natural way of cognition, need to be prepared for a new \"task\"</li> <li>If unused brain resources, put it to use</li> <li>Mental Fatigue</li> </ul>"},{"location":"Cognitive%20Multitasking/#backlinks","title":"Backlinks","text":"<ul> <li>Cognitive Preparation</li> <li>Cognitive Multitasking</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Cognitive%20Preparation/","title":"Cognitive Preparation","text":"<ul> <li>How brains use time to make decisions</li> <li>Reflects implicit learning mechanisms<ul> <li>relies to optimize behavior</li> </ul> </li> <li>Many models assume brain can time but not about how time is implemented</li> <li>Prepartion effects are present on different time scales</li> <li>Within trial</li> <li>Block of trials</li> <li>Across blocks of trials</li> <li>Cognitive Multitasking</li> </ul>"},{"location":"Cognitive%20Preparation/#design","title":"Design","text":"<ul> <li>Task has to be boring -.-</li> </ul>"},{"location":"Cognitive%20Preparation/#terms","title":"Terms","text":"<ul> <li>Cognitive Foreperiod</li> </ul>"},{"location":"Cognitive%20Preparation/#backlinks","title":"Backlinks","text":"<ul> <li>01:07 Cognitive Preparation</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Cognitive%20fMTP/","title":"Cognitive fMTP","text":"<ul> <li>Preparation effects manifest in the motor system</li> <li>Preparation is a balance between inhibition and activation</li> <li>A neural representation of time  </li> <li>A (crude) model of the motor system  </li> <li>Hebbian associations + Forgetting &amp; Retrieval</li> </ul>"},{"location":"Cognitive%20fMTP/#between-fore-and-start","title":"Between Fore and Start","text":"<ul> <li>Timing<ul> <li>Layer of time cells</li> <li></li> </ul> </li> <li>Preparation<ul> <li>The motor system \u2018stages\u2019 a response, but holds it under inhibition..</li> <li>When the Go-stimulus (S2) arrives: activation</li> <li></li> </ul> </li> </ul>"},{"location":"Cognitive%20fMTP/#learning","title":"Learning","text":"<ul> <li>Different 'Time cells' and 'motor inhibiton &amp; activation' are active at the same time: this leads to hebbian learning</li> <li>Fire together, wire together</li> <li>Forms memory traces : aka chunk</li> <li></li> </ul>"},{"location":"Cognitive%20fMTP/#retrieval","title":"Retrieval","text":"<ul> <li>At the start, high degree of inhibition and low degree of activation</li> <li>If prev trial is short, inhibition short and more activation</li> <li> \\[RT = \\frac{I}{A}\\] </li> <li>More activation retrieved : faster</li> <li>Recency weighted</li> <li>Declarative Memory Blending</li> <li>Preparation := Ratio of retrieved I vs. A</li> <li></li> </ul>"},{"location":"Cognitive%20fMTP/#vs-act-r","title":"Vs ACT-R","text":"<ul> <li>Prepare for \u2018the right moment\u2019  <ul> <li>Moment-to-moment balance of I and A  </li> </ul> </li> <li>\u2018the right moment\u2019 is estimated based on time (pulses) and memory (DM)  <ul> <li>Similar; but memory \u2018chunks\u2019 contain I- and A-traces not a single moment at which one should be prepared  </li> </ul> </li> <li>If we are prepared\u2192 benefit, else cost  <ul> <li>Scaled benefits  </li> <li>Inhibiton increases RT; activation decreases RT  </li> </ul> </li> <li>No active process during timing  <ul> <li>Continuously retrieving associated memories?  </li> </ul> </li> <li>Once we are prepared, it doesn\u2019t \u2018go away\u2019  <ul> <li>Consequence of \u2018more A, less I retrieved\u2019</li> </ul> </li> </ul>"},{"location":"Cognitive%20fMTP/#backlinks","title":"Backlinks","text":"<ul> <li>The Reward Experiment</li> <li> <p>Cognitive fMTP</p> </li> <li> <p>Cogntition Hazard Rates</p> </li> <li>{Proven wrong} : Cognitive fMTP</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Collaborative%20Recommender/","title":"Collaborative Recommender","text":"<ul> <li>Clusters users according to behavior</li> <li>Match with other users</li> <li>eg : netflix</li> </ul>"},{"location":"Collaborative%20Topic%20Regression/","title":"Collaborative Topic Regression","text":"<ul> <li>Collaborative Deep Learning for Recommender Systems</li> <li>Collaborative filtering (CF) is a successful approach commonly used by many recommender systems</li> <li>Conventional CF-based methods use the ratings given to items by users as the sole source of information for learning to make recommendation</li> <li>However, the ratings are often very sparse in many applications, causing CF-based methods to degrade significantly in their recommendation performance</li> <li>To address this sparsity problem, auxiliary information such as item content information may be utilized</li> <li>Collaborative topic regression (CTR) is an appealing recent method taking this approach which tightly couples the two components that learn from two different sources of information</li> <li>Nevertheless, the latent representation learned by CTR may not be very effective when the auxiliary information is very sparse.</li> <li>generalizing recent advances in deep learning from i.i.d input to non-i.i.d (CF-based) input and propose a hierarchical Bayesian model called collaborative deep learning (CDL), which jointly performs deep representation learning for the content information and collaborative filtering for the ratings (feedback) matrix.</li> </ul>"},{"location":"Collective%20Ethical%20Decision%20Frameworks/","title":"Collective Ethical Decision Frameworks","text":"<ul> <li>advocates the need of primary rules governing social norms and allowing the creation, modification and suppression of the primary rules with secondary rules as situations evolve.</li> </ul>"},{"location":"Collective%20Ethical%20Decision%20Frameworks/#backlinks","title":"Backlinks","text":"<ul> <li>Even Angels Need the Rules AI, Roboethics, and the Law</li> <li>Collective Ethical Decision Frameworks</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Collective%20Interpretation/","title":"Collective Interpretation","text":""},{"location":"Collective%20Interpretation/#collective-interpretation","title":"Collective Interpretation","text":"<ul> <li>No scopal relation</li> <li>Three aliens are holding two flags.</li> <li>Both Np's are interpreted individually and connected to each other</li> </ul>"},{"location":"Collectivity%2C%20Distributivity%2C%20and%20the%20Interpretation%20of%20Plural%20Numerical%20Expressions%20in%20Child%20and%20Adult%20Language/","title":"Collectivity, Distributivity, and the Interpretation of Plural Numerical Expressions in Child and Adult Language","text":"<ul> <li>Kristen Syrett, Ph.D. and Rutgers, The State University of New Jersey, Linguistics, New Brunswick, United States</li> <li>Julien Musolino : Rutgers, The State University of New Jersey, Psychology, Piscataway, United States</li> </ul>"},{"location":"Collectivity%2C%20Distributivity%2C%20and%20the%20Interpretation%20of%20Plural%20Numerical%20Expressions%20in%20Child%20and%20Adult%20Language/#intro","title":"Intro","text":"<ul> <li>Sentences containing plural numerical expressions (e.g., two boys) can give rise to two interpretations (collective and distributive), arising from the fact that their representation admits of a part-whole structure</li> <li>designed to explore children\u2019s understanding of this distinction and its implications for the acquisition of linguistic expressions with number words.</li> <li>preschoolers access both interpretations, indicating that they have the requisite linguistic and conceptual machinery to generate the corresponding representations.</li> <li>hift their interpretation in response to structural and lexical manipulations.</li> <li>unlike adults, they are drawn to the distributive interpretation, and are not yet aware of the lexical semantics of each and together, which should favor one or another interpretation.</li> <li>Here, we take a different approach, and use numerically quantified expressions to study how children acquire a fundamental semantic property shared by a range of plurality-denoting expressions</li> </ul>"},{"location":"Collectivity%2C%20Distributivity%2C%20and%20the%20Interpretation%20of%20Plural%20Numerical%20Expressions%20in%20Child%20and%20Adult%20Language/#findings","title":"Findings","text":"<ul> <li>Our findings demonstrate that the ability to generate collective and distributive interpretations of sentences such as (1) is part of the semantic repertoire of children as young as three (Experiment 1). However, we also uncover intriguing differences in the preferences preschoolers and adults have for resolving the collective/distributive ambiguity: whereas adults strongly prefer the collective interpretation, preschoolers prefer the distributive one (Experiment 2).</li> <li>Following analyses by Link (1983, 1987) and others more recently, we will assume that these sentences are truly ambiguous, and not merely underspecified, and that the source of the ambiguity in our target sentences is the VP predicate. Here, we adopt a default semantics approach in order to illustrate how two different interpretations may be generated.</li> <li>When the predicate in our example sentence is applied to the individuals, the derived reading is the distributive one</li> <li>When the predicate is applied to the group, however, a collective reading is derived, and the extension is an atomic joint \u2018car pushing\u2019 event in which the boys collectively push the car.</li> <li>Beginning with the latter, Musolino (2009) was primarily concerned with the range of readings arising from the interaction of two numerically quantified expressions in so-called relational plural sentences such as (3).</li> </ul>"},{"location":"Collectivity%2C%20Distributivity%2C%20and%20the%20Interpretation%20of%20Plural%20Numerical%20Expressions%20in%20Child%20and%20Adult%20Language/#judgment-task-with-ambiguous-sentences","title":"Judgment Task with Ambiguous Sentences","text":"<ul> <li>The results demonstrate that both children and adults were able to access both the collective and distributive interpretations of the target sentences. While there was a significant main effect of age (p= .02), there was no main effect of context (p=.75) and no interaction between age group and context (p=.26).</li> <li>This difference stems from the fact that while four-year-olds were near ceiling in their acceptance of the sentences in the distributive context, adults\u2019 acceptance rates were slightly suppressed.</li> </ul>"},{"location":"Collectivity%2C%20Distributivity%2C%20and%20the%20Interpretation%20of%20Plural%20Numerical%20Expressions%20in%20Child%20and%20Adult%20Language/#ambiguous-sentences-that-yield-either-interpretation","title":"Ambiguous Sentences That Yield Either Interpretation.","text":"<ul> <li>As the results demonstrate, adults overwhelmingly preferred the collective version of the event</li> <li>In this experiment, we found that while adults robustly prefer the collective context as a match for the ambiguous target sentences, children display a slight preference in the opposite direction, leaning towards preference for the distributive context.</li> <li>structural manipulation of passivization will lead participants to prefer the collective context.</li> <li>As predicted, adults consistently accepted the passive test sentences in the collective context, but largely rejected them in the distributive context. Most children also followed this pattern, although the difference between acceptances in the two contexts was not as striking for children as it was for the adults.</li> <li>whether participants can recruit lexical semantic information provided by individual words to disambiguate the target sentence and assign either a collective or distributive interpretation, depending on the lexical item.</li> <li>As predicted, adults were guided by the presence of the additional lexical item in their interpretation of these sentences, accepting the test sentences with each in the distributive context, but rejecting them in the collective context</li> <li>In place of the ambiguous sentences, children heard sentences with a post-verbal together (</li> <li>Interestingly, despite children\u2019s acceptance of the together sentences in both contexts of the judgment task of Experiment 4, children appeared to be aware of the collectivizing force of together in the current preference task.</li> </ul>"},{"location":"Color%20Compositing/","title":"Color Compositing","text":"<ul> <li> \\[C_{i}= c_{i}+ (1-o_{i})C_{i-1}\\] </li> <li>where</li> <li> \\[c_{i}= o_{i}c_{i}'\\] </li> <li></li> <li>First is same as Marching Cubes</li> <li>$$I(p) = \\begin{cases*}</li> </ul> <p>f(\\sigma)&amp; \\(\\exists t \\in [0,T], s(t) = \\sigma\\) \\</p> <p>I_{o}&amp;otherwise</p> <p>\\end{cases*}$$</p> <ul> <li>Higher pixel accurate quality</li> </ul>"},{"location":"Color%20Space%20Transformations/","title":"Color Space Transformations","text":"<ul> <li>Image data is encoded into 3 stacked matrices, each of size height\u00d7width. These matrices represent pixel values for an individual RGB color value</li> <li>Lighting biases are amongst the most frequently occurring challenges to image recognition problems</li> <li>A quick fix to overly bright or dark images is to loop through the images and decrease or increase the pixel values by a constant value.</li> <li>Another quick color space manipulation is to splice out individual RGB color matrices.</li> <li>Another transformation consists of restricting pixel values to a certain min or max value.</li> <li>Similar to geometric transformations, a disadvantage of color space transformations is increased memory, transformation costs, and training time.</li> <li>Additionally, color transformations may discard important color information and thus are not always a label-preserving transformation.</li> <li>For example, when decreasing the pixel values of an image to simulate a darker environment, it may become impossible to see the objects in the image.</li> <li>Digital image data is usually encoded as a tensor of the dimension (height \u00d7 width \u00d7 color channels)</li> <li>Performing augmentations in the color channels space is another strategy that is very practical to implement.</li> <li>Very simple color augmentations include isolating a single color channel such as R, G, or B.</li> <li>An image can be quickly converted into its representation in one color channel by isolating that matrix and adding 2 zero matrices from the other color channels. Additionally, the RGB values can be easily manipulated with simple matrix operations to increase or decrease the brightness of the image.</li> <li>More advanced color augmentations come from deriving a color histogram describing the image</li> </ul>"},{"location":"Color%20Spaces/","title":"Color Spaces","text":"<ul> <li>Divide Oriented</li> <li>Intuitive Color spaces</li> </ul>"},{"location":"ColorMap/","title":"ColorMap","text":"<ul> <li>Color Spaces</li> <li></li> </ul>"},{"location":"CommonCrawl/","title":"CommonCrawl","text":""},{"location":"Complex%20Geometry/","title":"Challenge of Complex Geometry","text":"<ul> <li>Manifold</li> </ul>"},{"location":"Compliant%20Robot/","title":"Compliant Robot","text":"<ul> <li>A robot that performs tasks, with respect to external forces, by modifying its motions in a manner that minimizes those forces. The indicated or allowed motion is accomplished through lateral (horizontal), axial (vertical) or rotational compliance.</li> </ul>"},{"location":"Comprehensibility/","title":"Comprehensibility","text":"<ul> <li>An understandable explanation must be created by a machine in a given time (e.g., one hour or one day) and can be comprehended by a user, who need not to be an expert, but has an educational background</li> <li>The user keeps asking a finite number of questions of the machine until he/she can no longer ask why or how because he/she has a satisfactory answer; we say he/she has comprehended.</li> <li>This is the relationship between cause and effect; it is not a synonym for causability</li> </ul>"},{"location":"Computational%20Graph/","title":"Computational Graph","text":"<ul> <li>patterns in backward flow<ul> <li>add gate: gradient distributor</li> <li>max gate: gradient router</li> <li>mul gate: gradient switcher</li> <li>branches: sum up gradients</li> </ul> </li> <li>pros<ul> <li>intuitive interpretation of gradient</li> <li>easily define new nodes using forward/backward pattern (i.e. only these two functions must be implemented)</li> <li>any complex learning architecture can be composed of atomic nodes (node composition or factorization)</li> <li>no need to compute manually complex gradients</li> <li>loss function can be seen as extra nodes in the end of the graph</li> </ul> </li> </ul>"},{"location":"Conceptual%20Parsing/","title":"Conceptual Parsing","text":"<ul> <li>Syntactic Analysis and Semantic Analysis knowledge are combined into a single interpretation system that is driven by the semantic knowledge</li> </ul>"},{"location":"Concurrency/","title":"Concurrency","text":"<ul> <li>The number of tasks that can be executed in parallel is the degree of concurrency of a decomposition.</li> <li>always equal to the number of leaves in the tree</li> <li>Both the maximum and the average degrees of concurrency usually increase as the Parallel Granularity of tasks becomes smaller (finer)</li> <li> \\[CriticalPath = \\frac{\\text{Total amount of work}}{\\text{Critical path length}}\\] </li> <li></li> </ul>"},{"location":"Concussion/","title":"Concussion","text":"<ul> <li>A type of mild traumatic brain injury resulting from a blow or hit to the head that causes the brain to move rapidly back and forth inside the skull.</li> </ul>"},{"location":"Conditional%20GAN/","title":"Conditional GAN","text":"<ul> <li>Image2Image, Face Aging, Text to Image</li> <li>Generate images with certain extra conditions or attributes</li> <li>The Generator and Discriminator both receive some additional conditioning input information. This could be the class of the current image or some other property.</li> <li>add an additional input layer with values of one-hot-encoded image labels</li> <li>Adding a vector of features controls the output and guide Generator figure out what to do.</li> <li>Such a vector of features should derive from a image which encode the class(like an image of a woman or a man if we are trying to create faces of imaginary actors) or a set of specific characteristics we expect from the image (in case of imaginary actors, it could be the type of hair, eyes or complexion).</li> <li>Whereas conditional generation uses labels during training, controllable generation focuses on controlling the features that you want in the output examples.</li> <li>We can incorporate the information into the images that will be learned and also into the Z input, which is not completely random anymore.</li> <li>This can be done by adjusting the input noise vector z that is fed into the generator after it has been trained.</li> <li>We can use the same DCGANs and imposed a condition on both Generator\u2019s and Discriminator\u2019s inputs. The condition should be in the form of a one-hot vector version of the digit. This is associated with the image to Generator or Discriminator as real or fake.</li> <li>Typically this is done with a one-hot vector, meaning there are zeros in every position except for the position of the class we want to generate</li> </ul>"},{"location":"Conditional%20GAN/#architecture","title":"Architecture","text":"<ul> <li>GAN Z Space</li> </ul>"},{"location":"Conditional%20GAN/#the-discriminators-network","title":"The Discriminator\u2019s network","text":"<ul> <li>Discriminator\u2019s evaluation is done not only on the similarity between fake data and original data but also on the correspondence of the fake data image to its input label (or features)</li> <li>Same as DCGAN except one hot vector for conditioning</li> </ul>"},{"location":"Conditional%20GAN/#the-generators-network","title":"The Generator\u2019s network","text":"<ul> <li>To create an image that looks as \u201creal\u201d as possible to fool the Discriminator.</li> <li>Same as DCGAN except one hot vector.</li> <li></li> </ul>"},{"location":"Conditional%20GAN/#loss-functions","title":"Loss functions","text":"<ul> <li>We need to calculate two losses for the Discriminator. The sum of the \u201cfake\u201d image and \u201creal\u201d image loss is the overall Discriminator loss. So the loss function of the Discriminator is aiming at minimizing the error of predicting real images coming from the dataset and fake images coming from the Generator given their one-hot labels.</li> </ul>"},{"location":"Conditional%20GAN/#gen","title":"Gen","text":"<ul> <li>The loss function of the Generator minimizes the correct prediction of the Discriminator on fake images conditioned on the specified one-hot labels.</li> <li> \\[\\mathcal{L}^{(G)}(\\theta^{(G)}, \\theta^{(D)}) = - \\mathbb{E}_{z} log \\mathcal{D} (\\mathcal{G} (z|y\u2019))\\] </li> </ul>"},{"location":"Conditional%20GAN/#disc","title":"Disc","text":"<ul> <li>has to correctly label real images which are coming from training data set as real.</li> <li>has to correctly label generated images which are coming from Generator as fake.</li> <li> \\[ \\mathcal{L}^{(D)}(\\theta^{(G)}, \\theta^{(D)})= - \\mathbb{E}_{x \\sim p_{data}}log \\mathcal{D}(x|y) - \\mathbb{E}_{z} log (1- \\mathcal{D}(\\mathcal{G}(z|y')))\\] </li> </ul>"},{"location":"Conditional%20GAN/#training","title":"Training","text":"<ul> <li>The Discriminator is trained using real and fake data and generated </li> <li>After the Discriminator has been trained, both models are trained together.</li> <li>First, the Generator creates some new examples.</li> <li>The Discriminator\u2019s weights are frozen, but its gradients are used in the Generator model so that the Generator can update its weights.</li> </ul>"},{"location":"Conditional%20GAN/#training-flow","title":"Training Flow","text":"<ul> <li>For the Disc </li> <li>For the Gen </li> </ul>"},{"location":"Conditional%20GAN/#challenges-with-conditional-generation","title":"Challenges with Conditional generation","text":"<ul> <li>Not strictly unsupervised. Needs labels</li> <li>With a conditional GAN, you get a random example from the class you specify</li> <li>With conditional generation, you have to train the GAN with labeled datasets.</li> <li>Feature Correlationa</li> <li>Z-Space Entanglement</li> <li>Classifier Gradients</li> </ul>"},{"location":"Conditional%20GAN/#dcgan-vs-cgan","title":"DCGAN vs CGAN","text":"DCGAN CGAN Output features are not controllable Output features can be controlled Unsupervised Semi-Supervised Discriminator does not receive labels Discriminator requires labels Discriminator evaluates similarity between input and target images Discriminator considers input and target images and their respective labels"},{"location":"Conditional%20GAN/#backlinks","title":"Backlinks","text":"<ul> <li>Scalar Articles</li> <li> <p>Conditional GAN</p> </li> <li> <p></p> </li> <li>02:04 Conditional GAN</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Conditional%20Independence/","title":"Conditional Independence","text":"<ul> <li>A Naive Bayes classifier assumes that the attribute values are independent of each other given the class. Normally distributed: many statistical methods assume that data is normally distributed.</li> </ul>"},{"location":"Conductance/","title":"Conductance","text":"<ul> <li>Kedar Dhamdhere, Mukund Sundararajan, Qiqi Yan</li> </ul>"},{"location":"Conductance/#summary-by-chatgpt","title":"Summary by ChatGPT","text":"<ul> <li>This paper introduces the concept of conductance as a way to understand the importance of hidden units in deep networks. Conductance is defined as the flow of Integrated Gradients' attribution via a hidden unit, and is used to understand the importance of a hidden unit to the prediction for a specific input or over a set of inputs. The effectiveness of conductance is evaluated in multiple ways, including theoretical properties, ablation studies, and a feature selection task using the Inception network over ImageNet data and a sentiment analysis network over reviews. The properties of conductance include completeness, linearity and insensitivity to variations in inputs or hidden unit values. The paper also discusses the issue of saturation in neural networks, where the gradient of the output with respect to the input can be near-zero, and how conductance addresses this issue. The authors also compare conductance with other methods of understanding hidden unit importance and find it to be more intuitive and accurate.</li> </ul>"},{"location":"Conductance/#abstract","title":"Abstract","text":"<ul> <li>We introduce the notion of conductance to extend the notion of attribution to the understanding the importance of hidden units</li> <li>conductance of a hidden unit of a deep network is the flow of attribution via this hidden unit</li> <li>conductance to understand the importance of a hidden unit to the prediction for a specific input, or over a set of inputs</li> <li>We evaluate the effectiveness of conductance in multiple ways, including theoretical properties, ablation studies, and a feature selection task</li> <li>Inception network over ImageNet data, and a sentiment analysis network over reviews</li> <li>Informally, the conductance of a hidden unit of a deep network is the flow of Integrated Gradients' attribution via this hidden unit</li> <li>The key idea behind conductance is to decompose the computation of Integrated Gradients via the chain rule</li> </ul>"},{"location":"Conductance/#conductance_1","title":"Conductance","text":"<ul> <li>Integrated Gradients produces attributions for base features</li> <li>There is a natural way to 'lift' these attributions to a neuron in a hidden layer. Consider a specific neuron y in a hidden layer of a network</li> <li>\\(\\(F:R^{n} \\rightarrow [0,1]\\)\\) represents a deep network.</li> <li>\\(x \\in R^{n}\\) is input, \\(x' \\in R^{n}\\) is baseline input</li> <li>Integrated Gradients is path integral of gradient along straightline path from baseline \\(x'\\) to input \\(x\\). The function F varies from a near zero value for the informationless baseline to its final value. The gradients of F with respect to the image pixels explain each step of the variation in the value of F</li> <li>The integration (sum) over the gradients cumulates these micro explanations and accounts for the net difference between the baseline prediction score (near zero) and the prediction value at the input x.</li> <li>\\(\\(IG_{i}(x) ::== (x_{i}- x_{i}') \\int_{\\alpha=0}^{1} \\frac{\\partial F(x' + \\alpha(x-x'))}{\\partial x_{i}}d \\alpha\\)\\) where \\(\\frac{\\partial F(x)}{\\partial x_{i}}\\) is grad of F along i^th dimension at x</li> <li>Conductance of neuron y for attribution to input variable i is \\(\\(Cond_{i}^{y}(x) ::== (x_{i}- x_{i}') \\int_{\\alpha=0}^{1} \\frac{\\partial F(x' + \\alpha(x-x'))}{\\partial y} \\cdot \\frac{\\partial y}{\\partial x_{i}} d \\alpha\\)\\)</li> </ul>"},{"location":"Conductance/#evaluation-of-conductance","title":"Evaluation of Conductance","text":"<ul> <li>Activation: The value of the hidden unit is the feature importance score.</li> <li>\\(Gradient\\times Activation\\) : \\(\\(y \\times \\frac{\\partial F(x' + \\alpha \\times (x-x'))}{\\partial y} d \\alpha\\)\\)</li> <li>Internal Influence : \\(\\(Int Inf ^{y}(x) ::= \\int^{1}_{\\alpha=0} \\frac{\\partial F(x' + \\alpha(x-x'))}{\\partial y} d \\alpha\\)\\)</li> <li>The premise is that hidden units that are important across a set of inputs from a class should be predictive of this input class.</li> </ul>"},{"location":"Conductance/#properties-of-conductance","title":"Properties of Conductance","text":""},{"location":"Conductance/#completeness","title":"Completeness","text":"<ul> <li>conductances for any single hidden layer add up to the difference between the predictions \\(F(x) - F(x')\\)</li> <li>conductances thus satisfy the Layerwise Conservation Principle</li> </ul>"},{"location":"Conductance/#linearity","title":"Linearity","text":"<ul> <li>So do internal influence and gradient*activations</li> <li>Suppose that we linearly compose hidden neurons f1 and f2 to form the final network that models the function \\(a \\times f_{1} + b \\times f_{2}\\). Then, the conductances of the two hidden neurons will be \\(a \\times (f_{1}(x) f_{1}(x_{0}))\\) and \\(b \\times (f_{2}(x) f_{2}(x'))\\) respectively.</li> <li>This is a sanity-check because if the action of a network is mostly linear from a hidden layer, the conductances will match what is intuitively the obvious solution.</li> </ul>"},{"location":"Conductance/#insensitive","title":"Insensitive","text":"<ul> <li>If varying the values of a hidden unit does not change the network's prediction, it has zero conductance</li> <li>If varying the inputs does not change value of the hidden unit, the hidden unit has zero conductance</li> <li>Based on \\(\\frac{\\partial F}{\\partial y_{j}}\\) and \\(\\frac{\\partial y_{j}}{\\partial x_{i}}\\) being 0</li> </ul>"},{"location":"Conductance/#saturation-of-neural-networks","title":"Saturation of Neural Networks","text":"<ul> <li>Basically, for a network, or a sub-network, even when the output crucially depends on some input, the gradient of the output w.r.t. the input can be near-zero.</li> <li>As an artificial example, suppose the network first transforms the input x linearly to y = 2x, and then transforms it to z = max(y, 1). Suppose the input is x = 1 (where z is saturated at value 1), with 0 being the baseline. Then for the hidden unit of y, gradient of z w.r.t. y is 0. Gradient*activation would be 0 for y, which does not reflect the intuitive importance of y. Like in Integrated Gradients, in computing conductance, we consider all extrapolated inputs for x between 0 and 1, and look at the gradients of output w.r.t. y at these points. This takes the non-saturated region into account, and ends up attributing 1 to y, as desired.</li> <li>wrong Polarity/Sensitivity</li> </ul>"},{"location":"Conductance/#methods","title":"Methods","text":"<ul> <li>we compare against can yield scores that have signs and magnitudes that are intuitively incorrect</li> <li>This is intuitively because each misses terms/paths that our method considers.</li> <li>Activation values for a ReLU based network are always positive. However, ReLU nodes can have positive or negative influence on the output depending on the upstream weights. Here, Activation does not distinguish the sign of the influence, whereas condutance can.</li> <li>Gradient*Activation as a linear projection can overshoot</li> <li>Certain hidden units that actually have near zero influence can be assigned high</li> <li>importance scores.</li> <li>For example, suppose that the network is the composition of two functions f (x) = x and a weighted ReLU g(y) = max(y 1, 0). Again, the network computes the composition g(f(x)). Suppose that the baseline is x = 0 and the input is x = 1 . The output of the network is 0. But the feature importance of the unit f is deemed to be 1 (activation) times 1 (gradient), which is 1 . Notice that this is the only unit in its layer, so the fact that its influence does not agree in magnitude with the output is undesirable. In contrast, conductance assigns all hidden units a score of zero. The example can be extended to show that the feature importance score can disagree in sign with the actual direction of influence.</li> <li>Suppose that the network is the composition two functions f(x) = x and g(y) = y, i.e., the network computes the composition g(f(x)). Suppose that the baseline is x = 0 and the input is x = 1. The output of the network is 1. But the internal influence of the unit represented by the function g is +1 (regardless of the choice of the input or the path). Notice that this is the only unit in</li> <li>its layer, so the fact that its influence does not agree in sign with the output is highly undesirable. In contrast, conductance assigns an influence score of 1.</li> </ul>"},{"location":"Conductance/#applying-conductance-to-an-object-recognition-model","title":"Applying Conductance to an Object Recognition Model","text":"<ul> <li>We use conductance as a measure to identify influential filters in hidden layers in the Inception network.</li> <li>Given an input image, we identify the top predicted label</li> <li>For the pre-softmax score for this label, we compute the conductance for each of the filters in each of the hidden layers</li> <li>The visualization is done by aggregating the conductance along the color channel and scaling the pixels in the actual image by the conductance values.</li> </ul>"},{"location":"Conductance/#ablation-study","title":"Ablation Study","text":"<ul> <li>Next we studied how many filters we need to ablate in the network in order for the network to change its prediction. We found that, it is sufficient to ablate 3.7 on an average for the network to change its prediction for an image. Only 3 out of 100 images needed more than 10 filter ablations to change the predicted label. The maximum was 16. This provides further evidence that using conductance we can identify filters that are important for the prediction.</li> <li>We compare this to the filters with highest internal influence. Out of the 100 sample images, the network prediction changed for only 5 images when their top 10 filters</li> </ul>"},{"location":"Conductance/#division-of-labour","title":"Division of Labour","text":"<ul> <li>We notice that almost all the filters either capture positive sentiment or negative sentiment, but not both.</li> <li>We substantiate via Figure 3, which is a clustered heatmap of signs of conductances of the 256 filters (columns) for around four thousand examples (rows) from the Stanford Sentiment Tree Bank [24]. Notice that very few filters have</li> <li>both negative and positive conductance. Negation</li> <li>Negation is commonly used in expressing sentiments, in phrases like \"this is not good\" or \"this is not bad\". Does the sentiment network understand negation? Does it have hidden units dedicated to implement the logic of negation? We first identify high conductance filters for the input \"this is not good\" that have a high attribution to the pattern \"not good\".</li> <li>Sentences with high conductance for filters that have high conductance for the phrase \"not bad\". These filters are largerly focussed on negation.</li> </ul>"},{"location":"Conductance/#images","title":"Images","text":""},{"location":"Conductance/#backlinks","title":"Backlinks","text":"<ul> <li>Vision Explainibility</li> <li>Conductance</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Cone/","title":"Cone","text":"<ul> <li>A type of photoreceptor cell responsible for color vision that is found in the retina.</li> </ul>"},{"location":"Confirmation%20Bias/","title":"Confirmation Bias","text":"<ul> <li>fairness</li> <li>The tendency to search for, interpret, favor, and recall information in a way that confirms one's preexisting beliefs or hypotheses. Machine learning developers may inadvertently collect or label data in ways that influence an outcome supporting their existing beliefs. Confirmation bias is a form of implicit bias.</li> <li>Experimenter's bias is a form of confirmation bias in which an experimenter continues training models until a preexisting hypothesis is confirmed.</li> <li> </li> <li>Using a dataset not gathered scientifically in order to run quick experiments. Later on, it's essential to switch to a scientifically gathered dataset</li> <li> </li> <li>The process of using mathematical techniques such as gradient descent to find the minimum of a convex function. A great deal of research in machine learning has focused on formulating various problems as convex optimization problems and in solving those problems more efficiently.</li> </ul>"},{"location":"Confirmation%20Bias/#convenience-sampling","title":"convenience sampling","text":""},{"location":"Confirmation%20Bias/#convex-optimization","title":"convex optimization","text":""},{"location":"Conformer/","title":"Conformer","text":"<ul> <li>Conformer: Convolution-augmented Transformer for Speech Recognition</li> <li>Transformer models are good at capturing content-based global interactions, while CNNs exploit local features effectively</li> <li>integrating components from both CNNs and Transformers for end-to-end speech recognition to model both local and global dependencies of an audio sequence in a parameter-efficient way</li> <li>importance of each component, and demonstrated that the inclusion of convolution modules is critical to the performance of the Conformer model</li> <li>propose the convolution-augmented transformer for speech recognition, named Conformer</li> <li>LibriSpeech</li> </ul>"},{"location":"Confusion%20Matrix/","title":"Confusion Matrix","text":"<ul> <li>Measures the test performance of a classification system on a per-class basis by indicating the number of samples of actual class\u00a0a\u00a0predicted as class\u00a0b.</li> <li>The rows relate to the actual class labels\u00a0a\u00a0and the columns to the predicted class labels\u00a0b.</li> </ul>"},{"location":"Connectionism/","title":"Connectionism","text":"<ul> <li>So called symbols like noun, verb or noun-phrase are just epiphenomenal misunderstandings we have of learned through network arrangements of non-linguistic primitive elements</li> </ul>"},{"location":"Connectionist%20Networks/","title":"Connectionist Networks","text":"<ul> <li>Intelligence 'emerges' through the changes in the connections (weights)</li> <li>Basically deep learning</li> </ul>"},{"location":"Connectives/","title":"Connectives","text":"<ul> <li>connect words, phrases (and, but, when)</li> </ul>"},{"location":"Connectome/","title":"Connectome","text":"<ul> <li>the graph of how neurons in a brain connect</li> </ul>"},{"location":"Connectome/#connectome_1","title":"Connectome","text":""},{"location":"Consequentialist%20ethics/","title":"Consequentialist Ethics","text":"<ul> <li>an agent is ethical if and only if it weighs the consequences of each choice and chooses the option which has the most moral outcomes</li> </ul>"},{"location":"Consequentialist%20ethics/#backlinks","title":"Backlinks","text":"<ul> <li>Building Ethics into Artificial Intelligence</li> <li>Consequentialist ethics</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Conservation%20Of%20Momentum/","title":"Conservation Of Momentum","text":"<ul> <li> \\[\\Sigma p_{i}= \\Sigma p _{f}\\] </li> <li> \\[m_{1}u_{1}+m_{2}u_{2}= m_{1}v_{1}+ m_{2}v_{2}\\] </li> <li>p is momentum</li> <li>v and d are velocity</li> <li>m is mass</li> </ul>"},{"location":"Contact%20Sensor/","title":"Contact Sensor","text":"<ul> <li>A device that detects the presence of an object or measures the amount of applied force or torque applied on the object through physical contact with it. Contact sensing can be used to determine location, identity, and orientation of work pieces.</li> </ul>"},{"location":"Content%20Based%20Attention/","title":"Content Based Attention","text":"<ul> <li>Graves2014</li> <li>Attention Alignment score \\(score(s_{t}, h_{i}) = cosine[s_{t}, h_{i}]\\)$</li> </ul>"},{"location":"Content%20Based%20Recommender/","title":"Content Based Recommender","text":"<ul> <li>User actions linked to content</li> <li>User model rep as info context</li> <li>eg: Google</li> </ul>"},{"location":"Content%20Morpheme/","title":"Content Morpheme","text":"<ul> <li>carry some semantic content</li> <li>e.g. able, un, van</li> </ul>"},{"location":"Content%20words/","title":"Content Words","text":"<ul> <li>Identifies part of a word</li> <li>Noun</li> <li>Adjective</li> <li>Verb</li> <li>Adverb</li> </ul>"},{"location":"Context%20Free%20Grammar/","title":"Context Free Grammar","text":"<ul> <li>formal system that describes a language by specifying how any legal text can be derived from a distinguished symbol called the axiom, or sentence symbol.</li> <li>It consists of a set of productions, each of which states that a given symbol can be replaced by a given sequence of symbols</li> <li>Types of Words</li> <li></li> <li></li> <li>Top Down Parsing</li> <li>Bottom Up Parsing</li> </ul>"},{"location":"Continous%20-%3E%20Discrete/","title":"Continous -&gt; Discrete","text":""},{"location":"Continous%20-%3E%20Discrete/#binning","title":"Binning","text":""},{"location":"Continous%20-%3E%20Discrete/#hierarchial-refinement","title":"Hierarchial Refinement","text":""},{"location":"Continous%20-%3E%20Discrete/#vector-quantization","title":"Vector Quantization","text":""},{"location":"Continous%20-%3E%20Discrete/#neural-dynamics","title":"Neural Dynamics","text":""},{"location":"Continuous%20Path/","title":"Continuous Path","text":"<ul> <li>Describes the process where by a robot is controlled over the entire path traversed, as opposed to a point-to-point method of traversal. This is used when the trajectory of the end-effector is most important to provide a smooth movement, such as in spray painting etc</li> </ul>"},{"location":"Contour/","title":"Contours","text":"<ul> <li>For nD \\(\\(\\{x \\in \\mathbb{R}^{n}|f(x)=c\\}\\)\\)</li> <li>Always closed curves</li> <li>Never self instersect</li> <li>Nested</li> <li>Contours cut the plane into values smaller or larger than the isovalue c</li> <li>Isoline</li> <li>Isosurface</li> <li>Countouring with Transparency</li> </ul>"},{"location":"Contrastive%20Loss/","title":"Contrastive Loss","text":"<ul> <li>Minimize distance between similar inputs Gradient Descent gradients, maximize between dissimilar Gradient Ascent</li> <li>Learn Embedding/Feature space using neighbors</li> <li>dim(Embedding d) &lt; dim(input Space D)</li> <li>Encoded using a learnable function(NN) \\(\\(G_\\theta(x) : \\mathcal{R}^D \\rightarrow \\mathcal{R}^d\\)\\)</li> <li>Binary labels : similar or not</li> <li>$\\(D_\\theta(x_1, x_2) = ||G_\\theta(x_1) - G_\\theta(x_2)||_2\\)</li> <li> \\[L(\\theta, y, x_1, x_2) = \\frac{(1-y)(D_\\theta(x_1, x_2))^2}{2} + \\frac{y(max(0,m-D\\theta(x_1, x_2)))^2}{2}\\] <ul> <li>m is enforced margin between similar and dissimilar (m&gt;0)</li> <li>Labeled points \\(\\((y,x_1,x_2)\\)\\) are generated</li> </ul> </li> </ul>"},{"location":"Contrastive%20Predictive%20Coding/","title":"Contrastive Predictive Coding","text":"<ul> <li>Representation Learning with Contrastive Predictive Coding</li> <li>Contrastive Predictive Coding</li> <li>framework for extracting compact latent representations to encode predictions over future observations</li> <li>learn such representations by predicting the future in latent space by using powerful autoregressive models</li> <li>probabilistic contrastive loss based on NCE, which both the encoder and autoregressive model are trained to jointly optimize, which they call InfoNCE</li> <li>InfoNCE induces the latent space to capture information that is maximally useful to predict future samples</li> <li>combines autoregressive modeling and noise-contrastive estimation with intuitions from predictive coding to learn abstract representations in an unsupervised fashion</li> <li>negative sampling</li> </ul>"},{"location":"Contributions%20of%20Shape%2C%20Texture%2C%20and%20Color%20in%20Visual%20Recognition%20Abstract/","title":"Contributions of Shape, Texture, and Color in Visual Recognition Abstract","text":"<ul> <li>Yunhao Ge, Yao Xiao, Zhi Xu, Xingrui Wang, and Laurent Itti</li> </ul>"},{"location":"Contributions%20of%20Shape%2C%20Texture%2C%20and%20Color%20in%20Visual%20Recognition%20Abstract/#abstract","title":"Abstract","text":"<ul> <li>humanoid vision engine (HVE) that explicitly and separately computes shape, texture, and color features from images</li> <li>resulting feature vectors are then concatenated to support the final classification</li> <li>HVE can summarize and rankorder the contributions of the three features to object recognition.</li> <li>We use human experiments to confirm that both HVE and humans predominantly use some specific features to support the classification of specific classes</li> <li>To demonstrate more usefulness of HVE, we use it to simulate the open-world zeroshot learning ability of humans with no attribute labeling</li> <li>Finally, we show that HVE can also simulate human imagination ability with the combination of different features.</li> </ul>"},{"location":"Contributions%20of%20Shape%2C%20Texture%2C%20and%20Color%20in%20Visual%20Recognition%20Abstract/#introduction","title":"Introduction","text":"<ul> <li>A widely accepted intuition about the success of CNNs on perceptual tasks is that CNNs are the most predictive models for the human ventral stream object recognition</li> <li>To understand which feature is more important for CNN-based recognition, recent paper shows promising results: ImageNet-trained CNNs are biased towards texture while increasing shape bias improves accuracy and robustness [33]</li> <li>Here, inspired by HVS, we wish to find a general way to understand how shape, texture, and color contribute to a recognition task by pure data-driven learning.</li> <li>It has been shown by neuroscientists that there are separate neural pathways to process these different visual features in primate</li> <li>Among the many kinds of features crucial to visual recognition in humans, the shape property is the one that we primarily rely on in static object recognition [16]. Meanwhile, some previous studies show that surface-based cues also play a key role in our vision system</li> <li>For example, [21] shows that scene recognition is faster for color images compared with grayscale ones</li> <li>Humanoid Vision Engine</li> </ul>"},{"location":"Contributions%20of%20Shape%2C%20Texture%2C%20and%20Color%20in%20Visual%20Recognition%20Abstract/#image-parsing-and-foreground-identification","title":"Image Parsing and Foreground Identification.","text":"<ul> <li>we use the entity segmentation method [41] to simulate the process of parsing objects from a scene in our brain.</li> <li>Entity segmentation is an open-world model and can segment the object from the image without labels.</li> <li>This method aligns with human behavior, which can (at least in some cases; e.g., autostereograms [29]) segment an object without deciding what it is</li> <li>After we get the segmentation of the image, we use a pre-trained CNN and Grad-CAM [47] to find the foreground object among all masks.</li> <li>We design three different feature extractors after identifying the foreground object segment: shape extractor, texture extractor, and color extractor, similar to the separate neural pathways in the human brain which focus on specific property</li> </ul>"},{"location":"Contributions%20of%20Shape%2C%20Texture%2C%20and%20Color%20in%20Visual%20Recognition%20Abstract/#shape-feature-extractor","title":"Shape Feature Extractor","text":"<ul> <li>want to keep both 2D and 3D shape information while eliminating the information of texture and color</li> <li>first use a 3D depth prediction model [44,43] to obtain the 3D depth information of the whole image</li> <li>After element-wise multiplying the 3D depth estimation and 2D mask of the object, we obtain our shape feature</li> <li>We can notice that this feature only contains 2D shape and 3D structural information (the 3D depth) and without color or texture information</li> </ul>"},{"location":"Contributions%20of%20Shape%2C%20Texture%2C%20and%20Color%20in%20Visual%20Recognition%20Abstract/#texture-feature-extractor","title":"Texture Feature Extractor","text":"<ul> <li>want to keep both local and global texture information while eliminating shape and color information.</li> <li>to remove the color information, we convert the RGB object segmentation to a grayscale image</li> <li>cut this image into several square patches with an adaptive strategy (the patch size and location are adaptive with object sizes to cover more texture information)</li> <li>If the overlap ratio between the patch and the original 2D object segment is larger than a threshold \u03c4, we add that patch to a patch pool (we set \u03c4 to be 0.99 in our experiments, which means the over 99% of the area of the patch belongs to the object</li> <li>Since we want to extract both local (one patch) and global (whole image) texture information, we randomly select 4 patches from the patch pool and concatenate them into a new texture image</li> </ul>"},{"location":"Contributions%20of%20Shape%2C%20Texture%2C%20and%20Color%20in%20Visual%20Recognition%20Abstract/#color-feature-extractor","title":"Color Feature Extractor","text":"<ul> <li>The first method is phase scrambling</li> </ul>"},{"location":"Contributions%20of%20Shape%2C%20Texture%2C%20and%20Color%20in%20Visual%20Recognition%20Abstract/#phase-scrambling","title":"Phase Scrambling","text":"<ul> <li>transforms the image into the frequency domain using the fast Fourier transform (FFT)</li> <li>In the frequency domain, the phase of the signal is then randomly scrambled, which destroys shape information while preserving color statistics</li> <li>Then we use IFFT to transfer back to image space</li> <li>We also used simple color histograms (see suppl.) as an alternative, but the results were not as good, hence we focus here on the phase scrambling approach for color representation.</li> </ul>"},{"location":"Contributions%20of%20Shape%2C%20Texture%2C%20and%20Color%20in%20Visual%20Recognition%20Abstract/#humanoid-neural-network","title":"Humanoid Neural Network","text":"<ul> <li>After preprocessing, we have three features</li> <li>To simulate the separate neural pathways in humans' brains for different feature information [1,11], we design three feature representation encoders for shape, texture, and color, respectively</li> <li>ResNet-18 [24] as the backbone for all feature encoders to project the three types of features to the corresponding well-separated embedding spaces.</li> <li>hard to define the ground-truth label of the distance between features.</li> <li>Given that the objects from the same class are relatively consistent in shape, texture, and color, the encoders can be trained in the classification problem independently instead, with the supervision of class labels.</li> <li>fter training our encoders as classifiers, the feature map of the last convolutional layer will serve as the final feature representation</li> <li>We also propose a gradient-based contribution attribution method to interpret the contributions of shape, texture, and color to the classification decision,</li> <li>Take the shape feature as an example, given a prediction p and the probability of</li> <li>class k, namely pk, we compute the gradient of pk with respect to the shape feature Vs</li> <li>gradient as shape importance weights \u21b5sk</li> <li>In other words, Ssk represents the \"contribution\" of shape feature to classifying this</li> <li>image as class k</li> </ul>"},{"location":"Contributions%20of%20Shape%2C%20Texture%2C%20and%20Color%20in%20Visual%20Recognition%20Abstract/#effectiveness-of-feature-encoders","title":"Effectiveness of Feature Encoders","text":"<ul> <li>handcrafted three subsets of ImageNet</li> <li>Shape-biased dataset containing 12 classes, where the classes were chosen which intuitively are strongly determined by shape</li> <li>Texture-biased dataset uses 14 classes which we believed are more strongly determined by texture</li> <li>Color-biased dataset includes 17 classes</li> <li>After pre-processing the original images and getting their feature images, we input the feature images into feature encoders and get the T-SNE</li> <li>Each row represents one feature-biased dataset and each column is bounded with one feature encoder, each image shows the results of one combination</li> </ul>"},{"location":"Contributions%20of%20Shape%2C%20Texture%2C%20and%20Color%20in%20Visual%20Recognition%20Abstract/#effectiveness-of-humanoid-neural-network","title":"Effectiveness of Humanoid Neural Network","text":"<ul> <li>As these classifiers classify images based on corresponding feature representation, we call them feature nets.</li> <li>If we combine these three feature nets with the interpretable aggregation module, the classification accuracy is very close to the upper bound, which means our vision system can classify images based on these three features almost as well as based on the full original color images.</li> </ul>"},{"location":"Contributions%20of%20Shape%2C%20Texture%2C%20and%20Color%20in%20Visual%20Recognition%20Abstract/#more-humanoid-applications-with-hve-open-world-zero-shot-learning-with-hve","title":"More Humanoid Applications with HVE Open-world Zero-shot Learning with HVE","text":"<ul> <li>Most current methods [37,32,13] need humans to provide detailed attribute labels for each image, which is costly in time and energy. However, given an image from an unseen class, humans can still describe it with their learned knowledge</li> <li>First, to represent learnt knowledge, we use feature extractors</li> <li>To retrieve learnt classes as description, we calculate the average distance dkm</li> <li>between Iun and images of other class k in the latent space on feature m Open-world classification</li> <li>To further predict the actual class of Iun based on the feature-wise description, we use ConceptNet as common knowledge to conduct reasoning</li> <li>We form a reasoning root pool R\u21e4 consisting of feature roots Rs, Rt, Rc obtained during image description, and shared attribute roots Ras , Rat , Rac . The reasoning roots will be our evidence for reasoning</li> <li>We humans can intuitively imagine an object when seeing one aspect of a feature, especially when this feature is prototypical (contribute most to classification)</li> <li>For instance, we can imagine a zebra when seeing its stripe (texture). This process is similar but harder than the classical image generation task since the input features modality here dynamic which can be any feature among shape, texture, or color</li> </ul>"},{"location":"Contributions%20of%20Shape%2C%20Texture%2C%20and%20Color%20in%20Visual%20Recognition%20Abstract/#cross-feature-retrieval","title":"Cross Feature Retrieval","text":"<ul> <li>In order to reasonably retrieve the most possible other two corresponding features given only one feature (among shape, texture, or color), we learn a feature agnostic encoder that projects the three features into one same feature space and makes sure that the features belonging to the same class are in the nearby regions.</li> <li>In the retrieval process, given any feature of any object, we can map it into the cross feature embedding space by the corresponding encoder net and the feature agnostic net</li> <li>Then we apply the 2 norm to find the other two features closest to the input one as output. The output is correct if they belong to the same class as the input.</li> </ul>"},{"location":"Contributions%20of%20Shape%2C%20Texture%2C%20and%20Color%20in%20Visual%20Recognition%20Abstract/#cross-feature-imagination","title":"Cross Feature Imagination","text":"<ul> <li>To stimulate imagination, we propose a crossfeature imagination model to generate a plausible final image with the input and retrieved features</li> <li>Inspired by the pixel2pixel GAN[26] and AdaIN[25] in the style transfer, we design a crossfeature pixel2pixel GAN model to generate the final image.</li> </ul>"},{"location":"Contributions%20of%20Shape%2C%20Texture%2C%20and%20Color%20in%20Visual%20Recognition%20Abstract/#pictures","title":"Pictures","text":""},{"location":"Contributions%20of%20Shape%2C%20Texture%2C%20and%20Color%20in%20Visual%20Recognition%20Abstract/#backlinks","title":"Backlinks","text":"<ul> <li>Contributions of Shape, Texture, and Color in Visual Recognition Abstract</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Conv%20Based%20Noise%20Reduction/","title":"Conv Based Noise Reduction","text":"<ul> <li>Noise is high frequency component, suppress via low-pass filters</li> <li>Ideal low-pass filter</li> <li>multiply with box filter in frequency domain</li> <li>convolution with sinc in spatial domain (impractical: infinite extent)</li> <li></li> <li></li> <li>Spatially narrow (wide) filter has wide (narrow) spectrum and low (high) smoothing effect</li> </ul>"},{"location":"Conv/","title":"Convnd","text":"<ul> <li> \\[A\\ast B\\] </li> <li>Connect a neighbor only to spatial neighborhood -&gt; spatial order<ul> <li>Some rotation and illumination invariance</li> </ul> </li> <li>Slide over -&gt; Same weights independant of location -&gt; less weights</li> <li>Subsample after conv</li> <li>multiple 2d feature maps</li> <li>Similar to Gabor filters after learning</li> <li>Some might collapse to 0</li> <li> \\[out(x,y) = \\Sigma_i \\Sigma_j input(x+i, y+j) kernel(i,j)\\] </li> <li> \\[Z^j = \\Sigma_{i=0}^{l-1}W^{ji} \\ast X^i + b^{ij}\\] </li> <li> \\[Y^j = g(Z^j)\\] </li> <li>Output shape : (\\(\\frac{()_i-f+2p}{s}\\)\\)<ul> <li>If \\(\\(p = \\frac{f-1}{2}\\)\\) and \\(\\(s=1\\)\\) then dimensions maintained</li> </ul> </li> <li>One operation repeated over and over starting with raw</li> <li>Padded Conv</li> <li>Strided</li> <li>Depthwise Separable</li> <li>Causal 1D Conv</li> <li>Causal Dilated Conv</li> </ul>"},{"location":"ConvBERT/","title":"ConvBERT","text":"<ul> <li>Convolutional BERT (ConvBERT) improves the original BERT by replacing some Multi Head Attention Self Attention segments with cheaper and naturally local operations, so-called span-based dynamic convolutions. These are integrated into the self-attention mechanism to form a mixed attention mechanism, allowing Multi-headed Self-attention to capture global patterns; the Convolutions focus more on the local patterns, which are otherwise captured anyway. In other words, they reduce the computational intensity of training BERT.</li> </ul>"},{"location":"ConvNeXt/","title":"ConvNeXt","text":"<ul> <li>A ConvNet for the 2020s<ul> <li>modifying a standard Res Net , following design choices closely inspired by Vision Transformer</li> <li>A vanilla ViT, on the other hand, faces difficulties when applied to general computer vision tasks such as object detection and semantic segmentation</li> <li>hierarchical Transformers (e.g., Swin Transformer ) that reintroduced several Conv priors, making Transformers practically viable as a generic vision backbone</li> <li>effectiveness of such hybrid approaches is still largely credited to the intrinsic superiority of Transformers, rather than the inherent inductive biases of convolutions</li> <li>extending the number of epochs, using AdamW optimizer, Stochastic Depth, Label Smoothing</li> <li>number of blocks in each stage (stage compute ratio), which was adjusted from (4, 4, 6, 3) to (3, 3, 9, 3)</li> <li>The second is the stem cell configuration, which in the original ResNet consisted of 7\u00d77 convolutions with stride 2 followed by a max-Pooling layer. This was substituted by a more Transformer-like \u201cpatchify\u201d layer which utilizes 4\u00d74 non-overlapping convolutions with stride 4</li> <li>Depthwise Separable , which are interestingly similar to self-Attention as they work on a per-channel basis</li> <li>higher number of channels (from 64 to 96)</li> <li>Inverted Bottleneck: An essential configuration of Transformers is the expansion-compression rate in the MLP block (the hidden dimension is 4 times higher than the input and output dimension)</li> <li>input is expanded using 1 \\times 1 convolutions and then shrunk through depthwise convolution and 1 \\times 1 convolutions</li> <li>move the depthwise convolution before the convolution</li> <li>7 \\times 7 window (higher values did not bring any alterations in the results</li> <li>GELU instead of Relu , a single activation for each block (the original Transformer module has just one activation after the MLP), fewer normalization Layers, Batch Normalization substituted by Layer Normalization , and separate downsampling layer</li> <li>ImageNet</li> <li>COCO</li> <li>ADE20K</li> <li>A case in point is multi-modal learning, in which a cross-attention module may be preferable for modeling feature interactions across many modalities</li> <li>Transformers may be more flexible when used for tasks requiring discretized, sparse, or structured outputs</li> </ul> </li> </ul>"},{"location":"Convolutional%20RNN/","title":"Convolutional RNN","text":"<ul> <li> \\[h_t = \\sigma_h(W_{hh}\\star h_{t-1} + W_{xh}\\star x_t + b_h)\\] </li> <li> \\[y_t = \\sigma_y(W_{hy}\\star h_t + b_y)\\] </li> <li>\\(\\(\\star\\)\\) is spatial Conv</li> <li>5D shapes -&gt; [samples, timesteps, width, height, channels]</li> <li>Very memory intensive</li> <li> \\[x^{2}+x\\] </li> </ul>"},{"location":"Coping%20Theory/","title":"Coping Theory","text":"<ul> <li>[Marsella and Gratch, 2003]</li> <li>allow agents to deal with strong negative emotions by changing the appraisal of the given situation was proposed</li> <li>agent assesses the ethical effects of its own actions and other agents' actions</li> <li>If its own action violates a given moral value, the shame emotion is triggered which serves to lower the priority of continuing with the given action</li> <li>If another agent's action violates a given moral value, the reproach emotion is triggered in the observing agent which serves to increase social distance with the given agent</li> <li>similar to existing individual ethical decision frameworks implicit reward</li> <li>humans in the loop</li> </ul>"},{"location":"Coping%20Theory/#backlinks","title":"Backlinks","text":"<ul> <li>Building Ethics into Artificial Intelligence</li> <li>Coping Theory</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Coronary%20Bypass/","title":"Coronary Bypass","text":"<ul> <li>Surgical transplant of a healthy blood vessel into the heart to bypass or replace an unhealthy vessel</li> </ul>"},{"location":"Corpus%20callosum/","title":"Corpus Callosum","text":"<ul> <li>bundle of fibers that transmits messages from one side to the other</li> </ul>"},{"location":"Corpus%20dependence/","title":"Corpus Dependence","text":"<ul> <li>Misspellings</li> <li>Erroneous Punctuations and Spacing</li> <li>Difficult to write rules to govern the corpora from the Internet</li> <li>Di\ufb03cult to prescribe rules governing the use of a written language</li> <li>Punctuations mean \u2013 Suprasegmentals in Spoken Language but might not be the same for corpora</li> <li>Algorithms may expect corpora need to obey some rules</li> </ul>"},{"location":"Correlation/","title":"Correlation","text":"<p>title: Correlation</p> <p>tags: statistics</p>"},{"location":"Correlation/#correlation","title":"Correlation","text":"<ul> <li>How strong a relationship is between data.</li> <li>The formulas return a value between -1 and 1, where:<ul> <li>1 indicates a strong positive relationship.</li> <li>-1 indicates a strong negative relationship.</li> <li>A result of zero indicates no relationship at all.</li> <li></li> </ul> </li> </ul>"},{"location":"Cortical%20Homunculus/","title":"Cortical Homunculus","text":""},{"location":"Cortisol/","title":"Cortisol","text":"<ul> <li>A steroid hormone produced by the adrenal glands that controls how the body uses fat, protein, carbohydrates, and minerals, and helps reduce inflammation. Cortisol is released in the body\u2019s stress response; scientists have found that prolonged exposure to cortisol has damaging effects on the brain</li> </ul>"},{"location":"Cosine%20Distance%204/","title":"Cosine Distance","text":"<ul> <li>Complement of Cosine Similarity</li> <li> \\[D_{c}(A,B) := 1- S_{c}(A,B)\\] </li> </ul>"},{"location":"Cosine%20Distance%204/#backlinks","title":"Backlinks","text":"<ul> <li>Cosine Similarity</li> <li>Cosine Distance</li> </ul> <p>Backlinks last generated 2023-01-25 01:31:24</p>"},{"location":"Cosine%20Distance/","title":"Cosine Distance","text":"<ul> <li>Complement of Cosine Similarity</li> <li> \\[D_{c}(A,B) := 1- S_{c}(A,B)\\] </li> </ul>"},{"location":"Cosine%20Distance/#backlinks","title":"Backlinks","text":"<ul> <li>Cosine Similarity</li> <li>Cosine Distance</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Cosine%20Learning%20Rate%20Decay/","title":"Cosine Learning Rate Decay","text":"<ul> <li>Instead of Learning Rate Warmup and then decay</li> <li> \\[\\eta_{\\mathrm{t}}=\\frac{1}{2}\\left(1+\\cos\\left(\\frac{\\mathrm{t}\\pi}{\\mathrm{\\mathrm{T}}}\\right)\\right)\\eta\\] </li> <li>Rate decreases slowly at first, then almost linear in the middle and slows down again in the end</li> <li></li> </ul>"},{"location":"Cosine%20Similarity/","title":"Cosine Similarity","text":"<ul> <li>Lp Regularization l2norm aka p = 2</li> <li> \\[S_{c}(A,B) := cos(\\theta) = \\frac{A\\cdot B}{||A|| ||B||} = \\frac{\\Sigma_{i=1}^{n}A_{i}B_{i}}{\\sqrt{\\Sigma_{i=1}^{n}A^{2}_{i}} \\sqrt{\\Sigma_{i=1}^{n}B_{i}^{2}}}\\] </li> <li>ranges from -1 : exactly opposite, 1 : exactly same, 0: orthogonal/not correlated, intermediate</li> <li>Cosine Distance</li> <li>Cosine similarity is $$ - \\mathrm{sum}\\left( \\mathrm{l2norm}\\left( y \\right) \\cdot \\mathrm{l2norm}\\left( \u0177 \\right) \\right)$$</li> <li></li> <li>magnitude of vectors is not taken into account, merely their direction</li> <li>In practice, this means that the differences in values are not fully taken into account</li> <li>If you take a recommender system, for example, then the cosine similarity does not take into account the difference in rating scale between different users</li> <li>high-dimensional data and when the magnitude of the vectors is not of importance</li> </ul>"},{"location":"Cosine%20Similarity/#backlinks","title":"Backlinks","text":"<ul> <li>Cosine Distance</li> <li> <p>Complement of Cosine Similarity</p> </li> <li> <p>Word Vectors</p> </li> <li>eg : Cosine Similarity</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Counterfactual%20Fairness/","title":"Counterfactual Fairness","text":"<ul> <li>A fairness metric that checks whether a classifier produces the same result for one individual as it does for another individual who is identical to the first, except with respect to one or more sensitive attributes. Evaluating a classifier for counterfactual fairness is one method for surfacing potential sources of bias in a model.</li> <li>See \"When Worlds Collide Integrating Different Counterfactual Assumptions in Fairness\" for a more detailed discussion of counterfactual fairness.</li> </ul>"},{"location":"Counterfactual%20Images%203/","title":"Counterfactual Images","text":"<ul> <li>Concepts in images, removing which would increment networks confidence about the target (aka competing class)</li> </ul>"},{"location":"Counterfactual%20Images%203/#backlinks","title":"Backlinks","text":"<ul> <li>GradCAM</li> <li>To identify Counterfactual Images, flip the signs</li> </ul> <p>Backlinks last generated 2023-01-25 01:31:24</p>"},{"location":"Counterfactual%20Images/","title":"Counterfactual Images","text":"<ul> <li>Concepts in images, removing which would increment networks confidence about the target (aka competing class)</li> </ul>"},{"location":"Counterfactual%20Images/#backlinks","title":"Backlinks","text":"<ul> <li>GradCAM</li> <li>To identify Counterfactual Images, flip the signs</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Counterfactual%20Impact%20Evaluation/","title":"Counterfactual Impact Evaluation","text":"<ul> <li>local method of comparison for different predictions. Counterfactuals are contrastive. They explain why a decision was made instead of another. A counterfactual explanation of a prediction may be defined as the smallest change to the feature values that changes the prediction to a predefined output.</li> </ul>"},{"location":"Countouring%20with%20Transparency/","title":"Countouring with Transparency","text":"<ul> <li>draw several contours for several isovalues</li> <li>assign \u201cadequate\u201d transparency</li> </ul>"},{"location":"Covariance/","title":"Covariance","text":"<p>title: Covariance</p> <p>tags: statistics</p>"},{"location":"Covariance/#covariance","title":"Covariance","text":"<ul> <li>how much two\u00a0random variables\u00a0vary together</li> <li></li> <li> \\[Cov(X,Y) = \\frac{\\Sigma(x_{i}- \\bar x)(y_{i}- \\bar y)}{N-1}\\] </li> <li>\\(\\bar x\\) is the mean of x</li> </ul>"},{"location":"Coverage%20Bias/","title":"Coverage Bias","text":"<ul> <li>The population represented in the dataset does not match the population that the machine learning model is making predictions about.</li> </ul>"},{"location":"CowMask/","title":"CowMask","text":"<ul> <li>semi-supervised learning</li> <li>original and augmented images are brought closer during training</li> <li>CowMask suggests two types of mixing approaches 1) erasing and 2) mixing two images similar to cutmix</li> <li>The mask here is of irregular shape rather than rectangular and generated by masking or keeping a proportion of image pixels through thresholding.</li> <li>Gaussian filter is applied to remove noise before thresholding.</li> <li>Pixel values below the threshold are either erased or replaced by the pixel values of the randomly selected image at the corresponding locations.</li> </ul>"},{"location":"Crash%20Blossom/","title":"Crash Blossom","text":"<ul> <li>A sentence or phrase with an ambiguous meaning. Crash blossoms present a significant problem in natural language understanding. For example, the headline Red Tape Holds Up Skyscraper is a crash blossom because an NLU model could interpret the headline literally or figuratively.</li> </ul>"},{"location":"Critical%20Points/","title":"Critical Points","text":"<ul> <li>sink (attracting node): all vectors converge  </li> <li>source (repelling node): all vectors diverge  </li> <li>saddle point: slopes are zero in orthogonal directions, no extremum \u2022 center point: embedded by (circular) flow around  </li> <li>attracting focus: flow is attracted in a spiral pattern</li> <li>repelling focus, where the flow is repelled in a spiral pattern</li> <li></li> </ul>"},{"location":"Cropping/","title":"Cropping","text":"<ul> <li>Cropping images can be used as a practical processing step for image data with mixed height and width dimensions by cropping a central patch of each image</li> <li>Additionally, random cropping can also be used to provide an effect very similar to translations.</li> <li>whereas translations preserve the spatial dimensions of the image</li> <li>Depending on the reduction threshold chosen for cropping, this might not be a label-preserving transformation. Rotation</li> <li>Rotation augmentations are done by rotating the image right or left on an axis between 1\u00b0 and 359\u00b0</li> <li>The safety of rotation augmentations is heavily determined by the rotation degree parameter.</li> <li>as the rotation degree increases, the label of the data is no longer preserved post-transformation. Translation</li> <li>Shifting images left, right, up, or down can be a very useful transformation to avoid positional bias in the data</li> <li>For example, if all the images in a dataset are centered, which is common in face recognition datasets, this would require the model to be tested on perfectly centered images as well.</li> <li>remaining space can be filled with either a constant value such as 0 s or 255 s, or it can be filled with random or Gaussian noise</li> </ul>"},{"location":"Cross%20Entropy/","title":"Cross Entropy","text":"<ul> <li>Entropy</li> <li>Cross-entropy is a measure from the field of information theory, building upon entropy and generally calculating the difference between two Probability distributions.</li> <li>It is closely related to but is different from KL Divergence that calculates the relative entropy between two Probability distributions, whereas cross-entropy can be thought to calculate the total entropy between the distributions.</li> <li>implicit distribution \\(\\(p(Y|x;\\theta)\\)\\) -&gt; use CE</li> <li> \\[\\mathscr{L}(\\theta) = -\\mathbb{E}_{(x,y) \\sim P(X,Y)} log (p_{model}(Y|x))\\] <ul> <li>Categorical CE<ul> <li>Classification</li> <li> \\[\\mathscr{L}(\\theta) = -\\mathbb{E}_{(x,y) \\sim P(X,Y)} \\Sigma_{i=1}^C 1(y=i)log (p_{model}f_i(x|\\theta))\\] </li> <li>C is no of classes</li> </ul> </li> <li>MSE<ul> <li>Regression</li> <li> \\[\\mathscr{L}(\\theta) = \\frac{1}{2}\\mathbb{E}_{(x,y) \\sim P(X,Y)}||y-f(x;\\theta)||^2$\\] </li> </ul> </li> </ul> </li> <li>Binary Cross Entropy</li> </ul>"},{"location":"Cross%20Minimization/","title":"Cross Minimization","text":"<ul> <li>planar graph: can be drawn on a plane without edge crossings</li> <li>from Euler\u2019s formula - the maximum number of edges for planar graphs: \\(\\(e \\leq 3v-6\\)\\)</li> <li></li> </ul>"},{"location":"Cross%20Modal%20Distillation/","title":"Cross Modal Distillation","text":"<ul> <li>Moreover, Do et al. (2019) proposed a knowledge distillation-based visual question answering method, in which knowledge from trilinear interaction teacher model with image-question-answer as inputs is distilled into the learning of a bilinear interaction student model with image-question as inputs</li> </ul>"},{"location":"Cross%20Validation/","title":"Cross Validation","text":""},{"location":"Cross%20Validation/#kfold","title":"KFold","text":"<ul> <li>Repeat for m = 1..L<ul> <li>Split data into roughly equal sizes. Disjoint subsets</li> <li>Get model with min Emperical Risk</li> <li>Test it with validation set</li> <li>Avg it for the folds for this value of m</li> </ul> </li> <li>Find optimal class for that m that had min avg validation risk (aka training error)</li> <li>Compute \\(h_{opt}\\) using the original training data</li> </ul>"},{"location":"Cross%20Validation/#leave-one-out","title":"Leave One Out","text":"<ul> <li>Each D contains a single training example</li> <li>For tiny datasets</li> </ul>"},{"location":"Cross%20angle%20Maximization/","title":"Cross Angle Maximization","text":"<ul> <li>avoid ambiguities</li> <li></li> </ul>"},{"location":"Cross-dataset%20generalization/","title":"Cross-dataset Generalization","text":"<ul> <li>virtually no papers demonstrating cross-dataset generalization, e.g. training on ImageNet, while testing on PASCAL VOC</li> <li>if our datasets were truly representative of the real world, this would be a very easy thing to do, and would give access to more of the much needed labelled data</li> <li>But from our perspective, all the datasets are really trying to represent the same domain \u2013 our visual world \u2013 and we would like to measure how well or badly they do it.</li> <li>Overall the results look rather depressing, as little generalization appears to be happening beyond the given dataset</li> </ul>"},{"location":"Cross-dataset%20generalization/#backlinks","title":"Backlinks","text":"<ul> <li>Unbiased Look at Dataset Bias</li> <li>Cross-dataset generalization</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Cross-situational%20learning/","title":"Cross-situational Learning","text":"<ul> <li>Speakers 'take statistics' about word-concept co-occurrences</li> <li>Predicts gradual learning</li> </ul>"},{"location":"Cuboids/","title":"Cuboids","text":""},{"location":"Cumulative%20Interpretation/","title":"Cumulative Interpretation","text":"<ul> <li>No scopal relation</li> <li>Three aliens are holding two flags.</li> </ul>"},{"location":"Curl%20And%20Vorticity/","title":"Curl And Vorticity","text":"<ul> <li>Helmholtz Theorem</li> </ul>"},{"location":"Curriculum%20Learning/","title":"Curriculum Learning","text":"<ul> <li>Formal Mathematics Statement Curriculum Learning</li> <li>neural theorem prover using GPT-f</li> <li>solve a curriculum of increasingly difficult problems out of a set of formal statements of sufficiently varied difficulty</li> <li>high-school Math Olympiad problems</li> <li>language model to find proofs of formal statements</li> <li>formal mathematics</li> <li>at same compute budget, expert iteration, by which they mean proof search interleaved with learning, dramatically outperforms proof search only</li> <li>expert iteration is capable of finding and solving a curriculum of increasingly difficult problems, without the need for associated ground-truth proofs</li> <li>miniF2F</li> <li>automatically solving multiple challenging problems drawn from high school olympiads</li> <li>lack of self-play in the formal mathematics setup can be effectively compensated for by automatically as well as manually curated sets of formal statement</li> <li>cheaper to formalize than full proofs</li> </ul>"},{"location":"Curse%20Of%20Dimensionality/","title":"Curse of Dimensionality","text":"<ul> <li>In an n dim hypercube -&gt; greatest possible distance is \\(\\sqrt{n}\\)</li> <li>Aka the higher the dimension -&gt; wider the training points from each other</li> <li>But there are fewer data points than dimensions and the distances are huge</li> <li>Dimensionality Reduction</li> </ul>"},{"location":"Curse%20Of%20Dimensionality/#backlinks","title":"Backlinks","text":"<ul> <li>Bag of Words</li> <li>Curse Of Dimensionality</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Cut%20and%20Delete/","title":"Cut and Delete","text":"<ul> <li>data augmentation by deleting image patches randomly or semantically.</li> <li>learn in case of occlusions</li> <li>This kind of dropout is different from conventional dropout because it drops contiguous image regions, whereas values in traditional dropout work at noncontiguous locations</li> </ul>"},{"location":"Cut%20and%20Mix/","title":"Cut and Mix","text":"<ul> <li>instead of deleting a patch, the patch is replaced with some other image region</li> <li>y this approach, an image shares multiple class labels, whereas the major class label belongs to the original class label</li> <li>Hence, the model learns to differentiate between two classes within a single image.</li> </ul>"},{"location":"Cut%2C%20Paste%20and%20Learn/","title":"Cut, Paste and Learn","text":"<ul> <li>generates new data by extracting object instances and pasting them on randomly selected background images.</li> <li>Instances are blended with various blending approaches, for example, gaussian blurring and poison blending, to reduce pixel artifacts around the augmented object boundaries.</li> <li>Added instances are also rotated, oc- cluded, and truncated to make the learning algo- rithm robust.</li> </ul>"},{"location":"CutMix/","title":"CutMix","text":"<ul> <li>images are augmented by sampling patch coordinates, x, y, h, w from a uniform distribution</li> <li>selected patch is replaced at the</li> <li>corresponding location with a patch from the other randomly picked image from the current mini-batch during training.</li> <li>M is the image mask, xa and xb are images, \u03bb is the proportion of label, and ya and yb are the labels of images.</li> <li> \\[x_{new}= M.x_{a}+(1-M).x_{b}\\] </li> <li> \\[y_{new}= \\lambda.y_{a}+ (1-\\lambda).y_{b}\\] </li> </ul>"},{"location":"Cutout/","title":"Cutout","text":"<ul> <li>removes constant size square patches randomly by replacing them with any constant value.</li> <li>The selection of region is performed by selecting a pixel value randomly and placing a square around it</li> </ul>"},{"location":"Cutout/#backlinks","title":"Backlinks","text":"<ul> <li>KeepAugment</li> <li> <p>Keep- Augment identifies the salient area in an image and assures the image generated by the augmenta- tion strategies, for example, Cutout, RandAugment [14], CutMix [82] or AutoAugment [13], contains salient region in it.</p> </li> <li> <p>GridMask</p> </li> <li> <p>The algorithm tries to overcome drawbacks of Cutout, Random Erasing, and Hide and Seek that are prone to deleting important information entirely or leaving it untouched without making it harder for the algorithm to learn.</p> </li> <li> <p>SmoothMix</p> </li> <li> <p>matching closely with the Cutout and CutMix techniques</p> </li> <li> <p>Image Mixing and Deletion</p> </li> <li>Cutout and CutMix argues that hindering image regions enforces the classifier to learn from the partially visible objects and understand the overall structure</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"CvT/","title":"CvT","text":"<p>title: CvT</p> <p>tags: architecture</p>"},{"location":"CvT/#cvt","title":"CvT","text":"<ul> <li>CvT: Introducing Convolutions to Vision Transformers<ul> <li>improves Vision Transformer</li> <li>introducing Conv</li> <li>a hierarchy of Transformers containing a new convolutional token Embedding</li> <li>convolutional Transformer block leveraging a convolutional projection</li> <li>shift, scale, and distortion invariance</li> <li>dynamic Attention , global context, and better generalization</li> <li>ImageNet</li> <li>Position Encoding , a crucial component in existing Vision Transformers, can be safely removed in our model</li> <li>potential advantage for adaption</li> <li>built-in local context structure introduced by convolutions, CvT no longer requires a position embedding</li> </ul> </li> </ul>"},{"location":"Cycle%20Consistency%20Loss/","title":"Cycle Consistency Loss","text":"<ul> <li>For two domains X, Y mapping \\(G: X \\rightarrow Y\\), \\(F: Y \\rightarrow X\\)</li> <li>trying to enforce the intuition that these mappings should be reverses of each other and that both mappings should be bijections</li> <li>Encourages \\(\\(F(G(x)) \\approx x \\text{ and } G(F(y)) \\approx y\\)\\)</li> <li>reduces the space of possible mapping functions by enforcing forward and backwards consistency</li> <li> \\[L_{cyc}(G,F) = \\mathbb{E}_{x \\sim p_{data}(x)}[||F(G(x))-x)||_{1}] + \\mathbb{E}_{x \\sim p_{data}(y)}[||G(F(x))-x)||_{1}]\\] </li> <li> \\[\\mathcal{L}_{cyc}(G, F, X, Y) = \\frac{1}{m}\\Sigma_{i=1}^{m}[(F(G(x_{i})-x_{i})+ (G(F(y_{i}))-y_{i})]\\] </li> </ul>"},{"location":"Cycle%20Consistency%20Loss/#backlinks","title":"Backlinks","text":"<ul> <li>CycleGAN</li> <li>\\(\\mathcal{L}_{cyc}\\) Cycle Consistency Loss</li> <li>Adversarial Loss + Cycle Consistency Loss</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"CycleGAN/","title":"CycleGAN","text":"<ul> <li>Unpaired image2image</li> <li>2 Mapping functions G, F : generator and \\(D_{X}, D_{Y}\\) as generators</li> <li>Adversarial Loss</li> <li>\\(\\mathcal{L}_{cyc}\\) Cycle Consistency Loss</li> <li>Full objective<ul> <li>Adversarial Loss + Cycle Consistency Loss</li> <li>\\(\\lambda\\) is a hyperparam. Generally set to 10</li> <li> \\[\\mathcal{L}_{GAN}(G, F, D_{X}, D_{Y}) = \\mathcal{L}_{GAN}(G, D_{Y}, X, Y) + \\mathcal{L}_{GAN}(F, D_{X}, X, Y) + \\lambda \\mathcal{L}_{cyc}(G,F)\\] </li> </ul> </li> <li>To Solve</li> <li> \\[G^{*},F^{*} =\\underset{G,F}{argmin} \\underset{D_{X}, D_{Y}}{max} \\mathcal{L}_{GAN}(G, F, D_{X}, D_{Y})\\] </li> <li>two stride-2 convolutions, several residual blocks, and two fractionally strided convolutions with stride \\(\\frac{1}{2}\\)</li> <li>Instance Normalization</li> </ul>"},{"location":"CycleGAN/#architecture","title":"Architecture","text":""},{"location":"CycleGAN/#generator","title":"Generator","text":""},{"location":"CycleGAN/#encoder","title":"Encoder","text":"<ul> <li>The encoder extracts features from the input image by using Convolutions and compressed the representation of image but increase the number of channels</li> <li>The encoder consists of 3 convolution that reduces the representation by 1/4 th of actual image size</li> </ul>"},{"location":"CycleGAN/#transforming-block","title":"Transforming Block","text":"<ul> <li>The transformer contains 6 or 9 residual blocks based on the size of input.</li> <li>The output of transformer is then passed into the decoder which uses 2 -deconvolution block of fraction strides to increase the size of representation to original size.</li> </ul>"},{"location":"CycleGAN/#discriminator","title":"Discriminator","text":"<ul> <li>PatchGAN</li> </ul>"},{"location":"CycleGAN/#applications","title":"Applications","text":"<ul> <li>Style Transfer<ul> <li>Unlike other works on neural style transfer, CycleGAN learns to mimic the style of an entire collection of artworks, rather than transferring the style of a single selected piece of art</li> </ul> </li> <li>Object Transformation<ul> <li>CycleGAN can transform object from one ImageNet class to another such as: Zebra to Horses and vice-versa, Apples to Oranges and vice versa etc</li> </ul> </li> <li>Season Transfer<ul> <li>CycleGAN can also transfer images from Winter Season to Summer season and vice-versa. For this the model is trained on 854 winter photos and 1273 summer photos of Yosemite from Flickr.</li> </ul> </li> <li>Photo Generation from Painting<ul> <li>can also be used to transform photo from paintings and vice-versa</li> <li>Identity Loss</li> </ul> </li> </ul>"},{"location":"CycleGAN/#limits","title":"Limits","text":"<ul> <li>applied to perform geometrical transformation, CycleGAN does not perform very well. This is because of the generator architecture which is trained to perform appearance changes in the image.</li> </ul>"},{"location":"CycleGAN/#reduce-model-oscillation","title":"Reduce Model Oscillation","text":"<ul> <li>To prevent the model from changing drastically from iteration to iteration, the discriminators were fed a history of generated images, rather than just the ones produced by the latest versions of the generator.</li> <li>To do this we keep storing the 50 most recently generated images. Based on this technique we reduce the model Oscillation as well as model overfitting.</li> </ul>"},{"location":"CycleGAN/#technical-implementation","title":"Technical Implementation","text":"<p>The CycleGAN paper provides a number of technical details regarding how to implement the technique in practice.</p> <p>The generator network implementation is based on the approach described for style transfer by\u00a0Justin Johnson\u00a0in the 2016 paper titled \u201cPerceptual Losses for Real-Time Style Transfer and Super-Resolution.\u201d</p> <p>The generator model starts with best practices for generators using the deep convolutional GAN, which is implemented using multiple residual blocks (e.g. from the\u00a0ResNet).</p> <p>The discriminator models use PatchGAN, as described by\u00a0Phillip Isola, et al. in their 2016 paper titled \u201cImage-to-Image Translation with Conditional Adversarial Networks.\u201d</p> <p>This discriminator tries to classify if each NxN patch in an image is real or fake. We run this discriminator convolutionally across the image, averaging all responses to provide the ultimate output of D.</p> <p>\u2014\u00a0Image-to-Image Translation with Conditional Adversarial Networks, 2016.</p> <p>PatchGANs are used in the discriminator models to classify 70\u00d770 overlapping patches of input images as belonging to the domain or having been generated. The discriminator output is then taken as the average of the prediction for each patch.</p> <p>The adversarial loss is implemented using a least-squared loss function, as described in\u00a0Xudong Mao, et al\u2019s 2016 paper titled \u201cLeast Squares Generative Adversarial Networks.\u201d</p> <p>[\u2026] we propose the Least Squares Generative Adversarial Networks (LSGANs) which adopt the least squares loss function for the discriminator. The idea is simple yet powerful: the least squares loss function is able to move the fake samples toward the decision boundary, because the least squares loss function penalizes samples that lie in a long way on the correct side of the decision boundary.</p> <p>\u2014\u00a0Least squares generative adversarial networks, 2016.</p> <p>Additionally, a buffer of 50 generated images is used to update the discriminator models instead of freshly generated images, as described in\u00a0Ashish Shrivastava\u2019s\u00a02016 paper titled \u201cLearning from Simulated and Unsupervised Images through Adversarial Training.\u201d</p> <p>[\u2026] we introduce a method to improve the stability of adversarial training by updating the discriminator using a history of refined images, rather than only the ones in the current minibatch.</p> <p>\u2014\u00a0Learning from Simulated and Unsupervised Images through Adversarial Training, 2016.</p> <p>The models are trained with the\u00a0Adam version of stochastic gradient descent\u00a0and a small learning rate for 100 epochs, then a further 100 epochs with a learning rate decay. The models are updated after each image, e.g. a batch size of 1.</p>"},{"location":"CycleGAN/#backlinks","title":"Backlinks","text":"<ul> <li>Scalar Articles</li> <li> <p>CycleGAN</p> </li> <li> <p></p> </li> <li>12:57 Have to write an article about CycleGAN</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Cyclic%20Learning%20Rate/","title":"Cyclic Learning Rate","text":"<ul> <li>With respect to local minima and saddle points, one could argue that you could simply walk \"past\" them if you set steps that are large enough. Having a learning rate that is too small will thus ensure that you get stuck.</li> <li>Now,\u00a0Cyclical Learning Rates\u00a0- which were introduced by Smith (2017) - help you fix this issue. These learning rates are indeed cyclical, and ensure that the learning rate moves back and forth between a\u00a0minimum value\u00a0and a\u00a0maximum value\u00a0all the time.</li> <li></li> <li></li> <li></li> </ul>"},{"location":"Cyclo%20Drive/","title":"Cyclo Drive","text":"<ul> <li>A brand name for a speed reduction device that converts high speed low torque to low speed high torque, usually used on the major (larger) axis.</li> </ul>"},{"location":"Cylinders/","title":"Cylinders","text":""},{"location":"Cylindrical%20Topology/","title":"Cylindrical Topology","text":"<ul> <li>A topology where the arm follows a radius of a horizontal circle, with a prismatic joint to raise or lower the circle. Not popular in industry</li> </ul>"},{"location":"DALL-E%203/","title":"DALL-E 2","text":"<ul> <li>Hierarchical Text-Conditional Image Generation with CLIP Latents</li> <li>DALL-E 2, generates more realistic and accurate images with 4x greater resolution, better caption matching and photorealism</li> <li>Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style</li> <li>two-stage model: a prior that generates a CLIP image embedding given a text caption, and a \u201cunCLIP\u201d decoder that generates an image conditioned on the image embedding</li> <li>explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity</li> <li>decoder, which is conditioned on image representations, can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation</li> <li>diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples</li> </ul>"},{"location":"DALL-E/","title":"DALL-E","text":"<ul> <li>AdaIn</li> <li>it was capable of generating text that could not be distinguished from human-written text</li> <li>named after Salvador Dal\u00ed and Pixar's WALL\u00b7E</li> <li>based on the GPT3</li> <li>Previous approaches like BERT and the original GPT model followed the fine-tuning approach.</li> <li>GPT-2 and GPT-3 recognized that even while pretraining already provided lots of benefits compared to training from scratch, so-called zero-shot learning - where the model is finetuned and then applied to language tasks, without pretraining - could be the way forward.</li> <li>DALL\u00b7E is capable of performing a variety of tasks:<ul> <li>Controlling attributes, instructing the model what particular attributes of an object should look like. For example: \"a collection of glasses is sitting on a table\" (OpenAI, 2021). Here, we instruct the model about the glasses, and more precisely, their location.</li> <li>Drawing multiple objects is also possible, but is more challenging, because it can be unknown whether certain characteristics belong to one object or another (OpenAI, 2021). DALL\u00b7E is however also capable of performing that task, but at the risk of making mistakes - once again due to the issue mentioned previously. The success rate decreases rapidly when the number of objects increases.</li> <li>Visualizing perspective and three-dimensionality, meaning that DALL\u00b7E can be instructed to take a particular \"perspective\" when generating the image (OpenAI, 2021).</li> <li>Visualizing across many levels, from \"extreme close-up\" to \"higher-level concepts\" (OpenAI, 2021).</li> <li>Inferring context, meaning that particular elements can be added to an image that normally do not belong to a particular context (e.g. the OpenAI logo in the image above; this is normally not displayed on a store front).</li> </ul> </li> <li>Uses<ul> <li>Industrial and interior design, to aid designers when creating a variety of household and other objects.</li> <li>Architecture, to guide the creation of buildings and other forms of constructions.</li> <li>Photography, to create an image specifically tailored to one's requirements.</li> <li>Graphic design, with e.g. the creation of a variety of icons.</li> </ul> </li> <li>Zero-Shot Text-to-Image Generation</li> <li>DALL-E which offers a simple approach for text-to-image generation based on an autoregressive transformer which models the text and image tokens as a single stream of data</li> <li>simple decoder-only transformer that receives both the text and the image as a single stream of 1280 tokens\u2014256 for the text and 1024 for the image\u2014and models all of them autoregressively</li> <li>They find that sufficient data and scale can lead to improved generalization, both in terms of zero-shot performance relative to previous domain-specific approaches</li> <li>and in terms of the range of capabilities that emerge from a single generative model.</li> </ul>"},{"location":"DCGAN/","title":"DCGAN","text":""},{"location":"DCGAN/#architecture","title":"Architecture","text":""},{"location":"DCGAN/#weight-init","title":"Weight Init","text":"<ul> <li>If conv Random Normal with mean = 0 and std.dev = 0.02</li> <li>If BatchNorm with mean = 1.0 and std.dev = 0.02, Bias = 0</li> </ul>"},{"location":"DCGAN/#generator","title":"Generator","text":"<ul> <li>Map latent space vector z to data space</li> <li>Creating RGB with same size as training image</li> <li>Transposed Conv , Batch Normalization and Relu</li> <li>Output is 3x64x64</li> <li>Output passed through Tanh to return it to [-1,1]</li> <li>Batch Normalization AFTER Transposed Conv is super important as it helps with flow of gradients</li> <li>Notice, how the inputs we set in the input section (nz,\u00a0ngf, and\u00a0nc) influence the generator architecture in code.\u00a0nz\u00a0is the length of the z input vector,\u00a0ngf\u00a0relates to the size of the feature maps that are propagated through the generator, and\u00a0nc\u00a0is the number of channels in the output image (set to 3 for RGB images)</li> <li></li> </ul>"},{"location":"DCGAN/#discriminator","title":"Discriminator","text":"<ul> <li>Strided Conv, Batch Normalization, and Leaky Relu</li> <li>3x64x64 input</li> <li>Binary classification network - outputs prob of real/fake</li> <li>Final is a Sigmoid layer</li> <li>For downsampling, good Practise to use Strided rather than Pooling as it lets the network learn it's own pooling function</li> <li>Almost a direct inverse of the Generator</li> </ul>"},{"location":"DCGAN/#special-features","title":"Special Features","text":"<ul> <li>Explicitly uses convolutional layers in the discriminator and transposed-convolutional layers in the generator</li> <li>Further the discriminator uses batch norm layers and\u00a0Leaky Relu\u00a0activations while the generator uses\u00a0Relu\u00a0activations</li> <li>The input is a latent vector drawn from a standard normal distribution and the output is a\u00a0\\(3 \\times 32 \\times 32\\)\u00a0RGB image</li> <li>In this implementation, I also added in\u00a0Label Smoothing</li> </ul>"},{"location":"DCGAN/#loss-functions","title":"Loss functions","text":""},{"location":"DCGAN/#discriminator-loss","title":"Discriminator loss","text":"<p>The Discriminator penalizes wrongly classifying a real image as a fake or a fake image as real. This can be thought of as maximizing the following function. \\(\\(\\nabla_{\\theta_{d}} \\frac{1}{m} \\Sigma_{i=1}^{m}[log D(x^{(i)}) + log(1-D(G(z^{(i)})))]\\)\\)</p>"},{"location":"DCGAN/#generator-loss","title":"Generator loss","text":"<ul> <li> <p>The Generator loss takes the output of the Discriminator into account and rewards it if the Generator is fooled into thinking the fake image is real. If this condition is not satisfied, the Generator is penalized.</p> </li> <li> <p>This can be thought of as minimizing the following function. \\(\\(\\nabla_{\\theta_{g}} \\frac{1}{m} \\Sigma_{i=1}^{m}log(1-D(G(z^{(i)})))\\)\\)</p> </li> </ul>"},{"location":"DCGAN/#backlinks","title":"Backlinks","text":"<ul> <li>Conditional GAN</li> <li>Same as DCGAN except one hot vector for conditioning</li> <li> <p>Same as DCGAN except one hot vector.</p> </li> <li> <p>Scalar Articles</p> </li> <li> <p>DCGAN</p> </li> <li> <p></p> </li> <li>DCGAN</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"DLRM/","title":"DLRM","text":"<ul> <li>Deep Learning Recommendation Model for Personalization and Recommendation Systems</li> <li>DLRM</li> <li>The DLRM model handles continuous (dense) and categorical (sparse) features that describe users and products</li> <li>wide range of hardware and system components, such as memory capacity and bandwidth, as well as communication and compute resources</li> <li>design a specialized parallelization scheme utilizing model parallelism on the embedding tables to mitigate memory constraints while exploiting data parallelism to scale-out compute from the fully-connected layers</li> <li>it computes the feature interactions explicitly while limiting the order of interaction to pairwise interactions.</li> <li>treats each embedded feature vector (corresponding to categorical features) as a single unit, whereas other methods (such as Deep and Cross) treat each element in the feature vector as a new unit that should yield different cross terms</li> <li>These design choices help reduce computational/memory cost while maintaining competitive accuracy</li> </ul>"},{"location":"DT%20Tutor/","title":"DT Tutor","text":"<ul> <li>DT Tutor (Murray, VanLehn, &amp; Mostow, 2004) implements a version of this ideal tutoring policy.</li> <li>The tutor applies decision theory to make its choice about whether to give a hint.</li> <li>For each tutor action it can make (e.g., to give a hint, and which kind of hint), it uses a probabilistic model of the student to predict all possible student reactions to the tutor's action and their probability.</li> <li>The predicted student state includes the likelihood of learning, of becoming frustrated, of entering the next step correctly, etc.</li> <li>DT Tutor evaluates the utility of each of the predicted student states, multiplies the state's utility by the state's probability, and eventually produces the expected utility of each proposed tutor action.</li> <li>It then takes the tutor action with the highest expected utility. Although advances in probabilistic reasoning make it feasible for DT Tutor to perform this calculation in real time, considerable data from human students is needed in order to set the parameters in its model of student learning</li> </ul>"},{"location":"Data%20Augmentation%20with%20Curriculum%20Learning/","title":"Data Augmentation with Curriculum Learning","text":"<ul> <li>Curriculum learning decisions are especially important for One-Shot Learning systems such as FaceNet</li> <li>In this sense, the concept of curriculum learning shares many similarities with adversarial search algorithms or learning only on hard examples.</li> <li>originally coined by Bengio et al.</li> <li>Plotting out training accuracy over time across different initial training subsets could help reveal patterns in the data that dramatically speed up training time.</li> </ul>"},{"location":"Data%20Augmentation%20with%20Curriculum%20Learning/#backlinks","title":"Backlinks","text":"<ul> <li>[[Data Augmentation with Curriculum Learning]]</li> <li> </li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Data%20Augmentation%20with%20Curriculum%20Learning/#data-augmentation-with-curriculum-learning_1","title":"Data Augmentation with Curriculum Learning","text":""},{"location":"Data%20Free%20Distillation/","title":"Data Free Distillation","text":"<ul> <li>Just as \u201cdata free\u201d implies, there is no training data. Instead, the data is newly or synthetically generated.</li> <li>Specifically, in (Chen et al., 2019a; Ye et al., 2020; Micaelli and Storkey, 2019; Yoo et al., 2019; Hu et al., 2020), the transfer data is generated by a GAN. In the proposed data-free knowledge distillation method (Lopes et al., 2017), the transfer data to train the student network is reconstructed by using the layer ac- tivations or layer spectral activations of the teacher net- work.</li> <li>Yin et al. (2020) proposed DeepInversion, which uses knowledge distillation to generate synthesized images for data-free knowledge transfer. Nayak et al. (2019) proposed zero-shot knowledge distillation that does not use existing data.</li> <li>The transfer data is pro- duced by modelling the softmax space using the pa- rameters of the teacher network. In fact, the target data in (Micaelli and Storkey, 2019; Nayak et al., 2019) is generated by using the information from the fea- ture representations of teacher networks.</li> <li>distilling knowl- edge from a teacher model into a student neural network (Kimura et al., 2018; Shen et al., 2021).</li> <li>data distillation, which is similar to data-free distillation (Radosavovic et al., 2018; Liu et al., 2019d; Zhang et al., 2020d). In data distillation, new training annotations of unlabeled data generated from the teacher model are employed to train a student model.</li> </ul>"},{"location":"Data%20Structures/","title":"Data Structures","text":"<ul> <li>Grids</li> </ul>"},{"location":"Data%20aug%20for%20spoken%20language/","title":"Data Aug for Spoken Language","text":"<ul> <li>Comparing Data Augmentation and Annotation Standardization to Improve End-to-end Spoken Language Understanding Models</li> <li>All-neural end-to-end (E2E) Spoken Language Understanding (SLU) models can improve performance over traditional compositional SLU models, but have the challenge of requiring high-quality training data with both audio and annotations</li> <li>they struggle with performance on \u201cgolden utterances\u201d, which are essential for defining and supporting features, but may lack sufficient training data</li> <li>using data augmentation to compare two data-centric AI methods to improve performance on golden utterances</li> <li>improving the annotation quality of existing training utterances and augmenting the training data with varying amounts of synthetic data</li> <li>both data-centric approaches to improving E2E SLU achieved the desired effect, although data augmentation was much more powerful than annotation standardization.</li> <li>leads to improvement in intent recognition error rate (IRER) on their golden utterance test set by 93% relative to the baseline without seeing a negative impact on other test metrics</li> </ul>"},{"location":"Decision%20Boundaries/","title":"Decision Boundaries","text":"<ul> <li>Minimal risk decision function is unique and must be represented in terms of Distributions of data generating RVs X and Y<ul> <li>A is some subvolume of P. (n dimensional hypercubes or volume bodies)</li> <li>\\(P_{X,Y}\\) is ground truth<ul> <li>Function that assigns every choice of \\(A \\subseteq P , c \\in C\\) the number P</li> </ul> </li> </ul> </li> <li>Decision function \\(h: P \\rightarrow {c_{1}, \u2026, c_{k}}\\) partitions pattern space into k disjoint decision regions \\(R_{1}, \u2026, R_{k}\\) by \\(\\(R_{i}= \\{x \\in P | h(x) = c_{i}\\}\\)\\)</li> <li>If a test pattern falls into \\(R_{i}\\) it is classified as class i</li> </ul>"},{"location":"Decision%20Boundaries/#finding-decision-regions","title":"Finding Decision Regions","text":"<ul> <li>which yields the lowerst misclassification rate or highest Probability of correct classification</li> <li>\\(f_{i}\\) be the PDF for Class Conditional distribution</li> <li>Probability of obtaining a correct classification for \\(R_{i}\\) is \\(\\(\\Sigma_{i=1}^{k}P(X \\in R_{i}, Y = c_{i})\\)\\)</li> <li></li> <li>This region has curved boundaries aka decision boundaries<ul> <li>Folded and on higher dims : very complex and fragmented</li> </ul> </li> <li>x is a vector</li> <li>For patterns on these boundaries, two or more classifications are equally probable</li> <li>Maximal if \\(\\(R_{i}= \\{x \\in P| i = argmax_{j} P(Y=c_{j})f_{j}(x)\\}\\)\\)</li> <li>Then \\(\\(h_{opt}: P \\rightarrow C_{j}x \\rightarrow c_{argmax_{j}P(Y=c_{j})f_{j}(x)}\\)\\)</li> <li>Algo learns estimates of the Class Conditional distribution and class probabilities aka priors</li> <li>The separator between classes learned by a model in a binary class or multi-class classification problems. For example, in the following image representing a binary classification problem, the decision boundary is the frontier between the orange class and the blue class</li> </ul>"},{"location":"Decision%20Trees/","title":"DT","text":""},{"location":"Decision%20Trees/#backlinks","title":"Backlinks","text":"<ul> <li>Generative vs Discriminative Models</li> <li>Decision Trees</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Declarative%20Memory%20Blending/","title":"Declarative Memory Blending","text":"<ul> <li>Like \"weighted avg\"</li> <li>store no of pulses</li> <li>activation decays in time<ul> <li> \\[A(t) = log(t-t_{creation})^{-d}+\\text{mismatchpenalty}\\] </li> <li>Retrieval probability<ul> <li>Softmax</li> <li> \\[P_{i}= \\frac{e^{\\frac{A_{t}}{t}}}{\\Sigma_{i}e^{\\frac{A_{t}}{t}}}\\] </li> </ul> </li> <li>Adds up to 1</li> <li> \\[Result = \\Sigma_{j}P_{j}V_{j}\\] </li> <li>t controls noise<ul> <li>if t is high : 1/no of competitors , more prob of retrieval</li> </ul> </li> <li>Looking for long interval (partial matching)</li> <li>Penalty for short intervals</li> <li>Apply \\(P_{i}\\)</li> <li>Weighted avg \\(Result\\)</li> </ul> </li> <li>Too short is positive. else negative, correct is 0</li> <li>no of pulses to wait : duration + feedback from memory</li> </ul>"},{"location":"Declarative%20Memory%20Blending/#fit","title":"Fit","text":"<ul> <li>Exp done on generated data as well</li> <li>Compares if same as when run on original</li> <li>Does well with unmodified mode    </li> <li></li> </ul>"},{"location":"Declarative%20Memory%20Blending/#backlinks","title":"Backlinks","text":"<ul> <li>Dikes and Rivers</li> <li> <p>ACTR declarative memory + Declarative Memory Blending</p> </li> <li> <p>Cognitive fMTP</p> </li> <li>Declarative Memory Blending</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Declarative%20memory/","title":"Declarative Memory","text":""},{"location":"Declarative%20memory/#info","title":"Info","text":"<ul> <li>All decisions are based on knowledge</li> <li>Depends on frequency and recency of use</li> <li>Semantic , episodic</li> <li>Representation similar to semantic networks</li> <li>No inheritence</li> <li>Partially sub-symbolic</li> <li>Related to [[Prefrontal cortex]]</li> </ul>"},{"location":"Declarative%20memory/#programming","title":"Programming","text":"<ul> <li>ACT-R Chunk</li> </ul>"},{"location":"Declarative%20memory/#backlinks","title":"Backlinks","text":"<ul> <li>ACT-R</li> <li> <p>Declarative memory</p> </li> <li> <p></p> </li> <li>Course - Declarative memory , [[Time perception]]</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"DeconvNet/","title":"DeconvNet","text":"<ul> <li>Zeiler, Fergus</li> <li>Arxiv</li> </ul> <ul> <li></li> <li></li> <li>Basically invert operations between input and the chosen layer.<ul> <li>Conv -&gt; Deconv</li> <li>Pool -&gt; Unpooling</li> <li>ReLU -&gt; ReLU with negative valyes clamped going backward from the activation space to image space</li> <li>Pooling is non invertible, but uses a switch module : recover positions of maxima in the forward pass</li> </ul> </li> <li>DeconvNet is a calculation of a backward convolutional network that reuses the weights at each layer from the output layer back to the input image</li> <li>The employed mechanisms are deconvolution and unpooling, which are especially designed for CNNs with convolutions, max-pooling, and Rectified Linear Units (ReLUs). The method makes it possible to create feature maps of an input image that activates certain hidden units most, linked to a particular prediction</li> <li>With their propagation technique, they identified the most responsible patterns for this output. The patterns are visualized in the input space</li> <li>DeconvNet is limited to max-pooling layers, but the unpooling uses an approximate inverse</li> </ul>"},{"location":"DeconvNet/#backlinks","title":"Backlinks","text":"<ul> <li>Vision Explainibility</li> <li>Building up on Deep Inside Convolutional Networks and DeconvNet</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Deductive%20Approaches/","title":"Deductive Approaches","text":"<ul> <li>Settling on one hypothesis by eliminating all others</li> </ul>"},{"location":"Deep%20Brain%20Stimulation/","title":"Deep Brain Stimulation","text":"<ul> <li>A method of treating various neuropsychiatric and neurodegenerative disorders through small, controlled electric shocks administered from a special battery-operated neurostimulation implant. The implant, sometimes called a \u201cbrain pacemaker,\u201d is placed within deep brain regions such as the globus pallidus or subthalamus.</li> </ul>"},{"location":"Deep%20Inside%20Convolutional%20Networks/","title":"Deep Inside Convolutional Networks","text":"<ul> <li>Karen Simonyan, Andrea Vedaldi, Andrew Zisserman</li> </ul> <ul> <li>The paper \"Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps\" by Simonyan, Vedaldi, and Zisserman (2014) proposes a method for visualizing and interpreting the internal representations of deep convolutional neural networks.</li> <li>A method is proposed for visualizing the internal representations of a deep CNN by backpropagating the output of the network to the input image.</li> <li>The method is used to visualize the internal representations of a CNN trained on the ImageNet dataset.</li> <li>The internal representations of the CNN are shown to have a hierarchical structure, with early layers learning simple features such as edges and textures, and later layers learning more complex features such as object parts and entire objects.</li> <li>A method is proposed for visualizing the saliency of an image region, which is defined as the gradient of the output of the network with respect to the input image.</li> <li>Saliency maps are generated by this method, which highlight the regions of an image that are most important for a given classification.</li> <li>The saliency maps generated by the method are shown to be qualitatively similar to the attention maps generated by humans.</li> <li>The saliency maps can be used to generate class-specific saliency maps, which highlight the regions of an image that are most important for a specific class.</li> <li>The method is used to visualize the internal representations of a CNN trained on the COCO dataset, which is a dataset of images with multiple objects and classes.</li> <li>The method can be used to visualize the internal representations of a CNN trained on a wide range of datasets and architectures.</li> <li>The method is used to visualize the internal representations of a CNN trained on a dataset of natural images and the internal representations of the network are shown to be similar to the representations of the primary visual cortex of the brain.</li> <li>The method is used to visualize the internal representations of a CNN trained on a dataset of images with natural scenes and the internal representations of the network are shown to be similar to the representations of the higher visual areas of the brain.</li> <li>The method is used to visualize the internal representations of a CNN trained on a dataset of images with faces and the internal representations of the network are shown to be similar to the representations of the fusiform face area of the brain.</li> <li>The saliency maps can be used to generate class-specific saliency maps, which highlight the regions of an image that are most important for a specific class.</li> <li>The method is used to analyze the internal representations of a CNN trained on a dataset of images with faces and the internal representations of the network are shown to be similar to the representations of the fusiform face area of the brain.</li> <li>The saliency maps can be used to generate class-specific saliency maps, which highlight the regions of an image that are most important for a specific class.</li> <li>The method is used to visualize the internal representations of a CNN trained on a dataset of images with faces and the internal representations of the network are shown to be similar to the representations of the fusiform face area of the brain.</li> <li>The saliency maps can be used to generate class-specific saliency maps, which highlight the regions of an image that are most important for a specific class.</li> <li>The method is concluded to be a powerful tool for visualizing and interpreting the internal representations of deep convolutional neural networks.</li> </ul>"},{"location":"Deep%20Inside%20Convolutional%20Networks/#backlinks","title":"Backlinks","text":"<ul> <li>Salience Map</li> <li>Use backprop to compute the gradients of logits wrt input : Deep Inside Convolutional Networks</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Deep%20Visual%20Explanation/","title":"Deep Visual Explanation","text":"<ul> <li>They captured the discriminative areas of the input image by considering the activation of high and low spatial scales in the Fourier space.</li> </ul>"},{"location":"DeepFM/","title":"DeepFM","text":"<ul> <li>DeepFM: a Factorization-Machine Based Neural Network for CTR Prediction</li> <li>Learning sophisticated feature interactions behind user behaviors is critical in maximizing CTR for recommender systems</li> <li>existing methods seem to have a strong bias towards low- or high-order interactions, or require expertise feature engineering</li> <li>an end-to-end learning model that emphasizes both low- and high-order feature interactions</li> <li>DeepFM is a Factorization-Machine (FM) based Neural Network for CTR prediction, to overcome the shortcomings of the state-of-the-art models and to achieve better performance.</li> <li>DeepFM trains a deep component and an FM component jointly and models low-order feature interactions through FM and models high-order feature interactions through the DNN</li> <li>DeepFM can be trained end-to-end with a shared input to its \u201cwide\u201d and \u201cdeep\u201d parts, with no need of feature engineering besides raw features.</li> <li>1) it does not need any pre-training; 2) it learns both high- and low-order feature interactions; 3) it introduces a sharing strategy of feature embedding to avoid feature engineering</li> <li>combines the power of factorization machines for recommendation and deep learning for feature learning in a new neural network architecture</li> <li>Criteo</li> </ul>"},{"location":"DeepFool/","title":"DeepFool","text":""},{"location":"DeepLIFT/","title":"DeepLIFT","text":""},{"location":"DeepLIFT/#backlinks","title":"Backlinks","text":"<ul> <li>Vision Explainibility</li> <li>DeepLIFT</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"DeepLearning/","title":"Index","text":"<ul> <li>Features</li> <li>Fundamentals</li> <li>Issues</li> <li>Layers</li> <li>Architectures</li> <li>Optimizers</li> <li>Regularization</li> <li>LossFunctions</li> <li>Activation Functions</li> <li>Initialization</li> <li>Augmentation</li> <li>Uncertainty</li> <li>Optimizing Code</li> <li>Useful Codes</li> <li>Federated Learning</li> <li>Reinforcement Learning</li> <li>Refs</li> </ul> <p>#anchor</p>"},{"location":"DeepNet/","title":"DeepNet","text":"<ul> <li>DeepNet: Scaling Transformers to 1,000 Layers</li> <li>allows train extremely deep transformers with 1000L+ layers</li> <li>fundamental, effective and simple</li> <li>can be used in any Transformer architecture (encoder, decoder, encoder-decoder) which covers almost all different tasks across AI areas (language, vision, speech, multimodal, and beyond)</li> <li>newly proposed normalization function</li> <li>DeepNorm</li> <li>It works alongside a dedicated initialization scheme based on Xavier initialization.</li> <li>These two tricks lead to greater stability during the training which allows the authors to scale their modified Transformer architecture (DeepNet) up to 1000 layers</li> </ul>"},{"location":"DeepNorm/","title":"DeepNorm","text":"<ul> <li>which modifies the residual connection in Transformers</li> <li>theoretical justification of bounding the model update by a constant which makes stable training possible in a principled way</li> <li>DeepNorm modifies the residual connection in the Transformer architecture by up-scaling it before performing layer normalization</li> </ul>"},{"location":"DeepPERF/","title":"DeepPERF","text":"<ul> <li>DeepPERF: a Deep Learning-Based Approach for Improving Software Performance</li> <li>Performance bugs may not cause system failure and may depend on user input, so detecting them can be challenging</li> <li>harder to fix than non-performance bugs</li> <li>performance bug detection approaches have emerged to help developers identify performance issues</li> <li>Building rule-based analyzers is a non-trivial task, as it requires achieving the right balance between precision and recall</li> <li>Once developed, maintaining these rules can also be costly</li> <li>large transformer model to suggest changes at application source code level to improve its performance</li> <li>first pretrain the model using masked language modelling (MLM) tasks on English text and source code taken from open source repositories on GitHub, followed by finetuning on millions of performance commits made by .NET developers</li> <li>recommend patches to provide a wide-range of performance optimizations in C<code>#</code> applications</li> <li>Most suggested changes involve modifications to high-level constructs like API/Data Structure usages or other algorithmic changes, often spanning multiple methods, which cannot be optimized away automatically by the C<code>#</code> compiler and could, therefore, lead to slow-downs on the user\u2019s side</li> </ul>"},{"location":"Default%20mode%20network/","title":"Default mode network","text":"<p>title: Default mode network</p> <p>tags: brain, psychology</p>"},{"location":"Default%20mode%20network/#default-mode-network","title":"Default Mode Network","text":"<ul> <li>Brain is organized into coherent spatio-temporal networks such as this one</li> <li>Brain areas in the Brain Cortex that constantly decreased their activity while performing highly demanding task</li> <li>Some studies revealed task-induced activations in the DMN, e.g. when internally directed/self-related cognition is required</li> <li>Altered with addictions</li> <li>Functional connectivity within DMN may predict successful quitting, the intensity of withdrawal-induced craving and the degree of cognitive decline in addictions</li> <li>They ^1 (as key node of the cognitive control network) and the Anterior Cingulate/Prefrontal cortex (as key nodes of the DMN)</li> </ul> <p>^1 associated with individual differences in Internet tendency in healthy young adults , Neuropsychologia</p>"},{"location":"Defibrillator/","title":"Defibrillator","text":"<ul> <li>A device that discharges an electric current to the heart to correct cardiac arrhythmia or arrest</li> </ul>"},{"location":"Degrees%20of%20Freedom/","title":"Degrees of Freedom","text":"<ul> <li>The number of independent directions or joints of the robot (R15.07), which would allow the robot to move its end effector through the required sequence of motions. For arbitrary positioning, 6 degrees of freedom are needed: 3 for position (left-right, forward-backward and up- down), and 3 for orientation (yaw, pitch and roll).</li> </ul>"},{"location":"DeiT/","title":"DeiT","text":"<p>title: DeiT</p> <p>tags: architecture</p>"},{"location":"DeiT/#deit","title":"DeiT","text":"<ul> <li>paper</li> <li>blog</li> <li>Conv free Transformer, Vision Transformer</li> <li>does not require very large amount of data   id:: 62a8a66a-941e-4a6d-918a-bb49cd496b15</li> <li>Knowledge Distillation</li> <li>teacher-student strategy specific to transformers</li> <li>Distillation Token</li> <li>ConvNet as teacher through Attention   id:: 62a8a6b2-abf4-4869-934e-c75d05884304</li> <li>ImageNet</li> <li> </li> </ul>"},{"location":"DeiT/#begin_caution","title":"+BEGIN_CAUTION","text":"Heh. Didnt they say no convs?   #+END_CAUTION"},{"location":"Delta%20Waves/","title":"Delta Waves","text":"<ul> <li>2-4 Hz</li> <li>sleep</li> <li></li> </ul>"},{"location":"Demographic%20Parity/","title":"Demographic Parity","text":"<ul> <li>A fairness metric that is satisfied if the results of a model's classification are not dependent on a given sensitive attribute.</li> <li>For example, if both Lilliputians and Brobdingnagians apply to Glubbdubdrib University, demographic parity is achieved if the percentage of Lilliputians admitted is the same as the percentage of Brobdingnagians admitted, irrespective of whether one group is on average more qualified than the other.</li> <li>Contrast with equalized odds and equality of opportunity, which permit classification results in aggregate to depend on sensitive attributes, but do not permit classification results for certain specified ground-truth labels to depend on sensitive attributes. See \"Attacking discrimination with smarter machine learning\" for a visualization exploring the tradeoffs when optimizing for demographic parity.</li> </ul>"},{"location":"Dendrites/","title":"Dendrites","text":"<ul> <li>Short nerve fibers that project from a neuron, generally receiving messages from the axons of other neurons and relaying them to the cell\u2019s nucleus.</li> </ul>"},{"location":"Denoising%20Autoencoder/","title":"Denoising Autoencoder","text":"<ul> <li>Corrupt inputs with noise</li> <li>Salt pepper noise</li> <li>Normal Distribution</li> <li>Compare outputs to clean inputs</li> <li>$\\(L(X) = n^{-1}\\Sigma_i||x_i - D(E(\\tilde x))||^2\\)</li> </ul>"},{"location":"Dense%20Net/","title":"Dense Net","text":"<ul> <li>Generalized Res Net</li> <li>Skip connections inside the Dense block itself</li> <li>![im](images/Dense Skip Connections]]</li> <li>Transition layer -&gt; Dense -&gt; 1x1 Conv , 2x2 avg pool -&gt; Dense</li> </ul>"},{"location":"Dense%20Skip%20Connections/","title":"Dense Skip Connections","text":"<ul> <li> \\[x_i = F(x_0,x_1 ,\u2026 ,x_{i-1})\\] <ul> <li>F : 3x3 Conv + Relu -&gt; k feature maps</li> <li>no of feature maps : \\(\\(k(i-1) + k_0\\)\\) where k is growth rate (hyperparam)</li> </ul> </li> <li>Skip Connection</li> </ul>"},{"location":"Dense/","title":"Dense","text":"<ul> <li>Weighted LinearRegression</li> <li>Forward<ul> <li> \\[z = W\\cdot x + b$$ , $$y=g(z)\\] </li> </ul> </li> <li>Backward<ul> <li> \\[\\delta = g'(z)\\circ \\nabla_y E\\] </li> <li> \\[\\nabla_WE = \\delta \\cdot x^T$$ , $$\\nabla_bE = \\delta\\] </li> <li> \\[\\nabla_xE = W^T\\cdot \\delta\\] </li> </ul> </li> </ul>"},{"location":"Density/","title":"Density","text":"<ul> <li>mass / vol</li> <li> \\[r - m/V\\] </li> </ul>"},{"location":"Deontological%20ethics/","title":"Deontological Ethics","text":"<ul> <li>an agent is ethical if and only if it respects obligations, duties and rights related to given situations</li> <li>act in accordance to established social norms</li> </ul>"},{"location":"Deontological%20ethics/#backlinks","title":"Backlinks","text":"<ul> <li>Building Ethics into Artificial Intelligence</li> <li>Deontological ethics</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Depthwise%20Separable/","title":"Depthwise Separable","text":"<ul> <li>Only transforms the input once and saves computation -&gt; elongate it to more channels</li> <li>From C -&gt; F channels : Use F instances of a 1x1xC filter</li> <li></li> </ul>"},{"location":"Derivational%20Morphology/","title":"Derivational Morphology","text":"<ul> <li>creates new word by changing the POS tag</li> </ul>"},{"location":"Detailed%20Balance/","title":"Detailed Balance","text":"<ul> <li>To find a transition kernel T(x|y) for a homogenous, Ergodic Markov Chain</li> <li>If we pick some state x with the Probability given by g and multiply its prob g(x) with the transition Probability density T(x|y) (weighted by Probability density of x) then its the same as the reverse weighted transiting Probability density from y to x</li> <li> \\[\\forall x,y \\in \\mathbb{R}^{k}: T(y|x)g(x) = T(x|y)g(y)\\] </li> <li>If T(x|y) has detailed balance wrt g, then it is an Invariant Distribution</li> <li> \\[\\int_{\\mathbb{R}^{k}}T(x|y)g(y)dy = \\int_{\\mathbb{R}^{k}}T(y|x)g(x)dy = g(x)\\int_{\\mathbb{R}^{k}}P(y|x)dy = g(x)\\] </li> </ul>"},{"location":"Determiners/","title":"Determiners","text":"<ul> <li>indicate specific object (a, the,that)</li> </ul>"},{"location":"DiTransitive%20verb/","title":"DiTransitive Verb","text":"<ul> <li>a verb has two noun objects</li> <li>I cooked a duck for her</li> </ul>"},{"location":"Dialyser/","title":"Dialyser","text":"<ul> <li>A machine that replaces the function of the kidneys by removing solutes, excess water and toxins from the blood</li> </ul>"},{"location":"Dialysis/","title":"Dialysis","text":"<ul> <li>Process to filter the blood, usually performed as a result of kidney failure</li> </ul>"},{"location":"Dice%20Score/","title":"Dice Score","text":"<ul> <li>2 * the Area of Overlap divided by the total number of pixels in both images</li> </ul>"},{"location":"Dictionary%20Learning/","title":"Dictionary Learning","text":"<ul> <li>Given : N unlabeled data points \\(\\(x_i \\in \\mathcal{R}^d\\)\\)</li> <li>To find:<ul> <li>Linear rep of these points based on set of basis vectors<ul> <li> \\[x = \\Sigma_i^k r_i \\cdot d_i = Dr\\] </li> <li>dimension d</li> <li>r are repr weights corresponding to basis vector d</li> <li>D is a dict with basis vectors</li> <li>R contains weights. Scalar</li> <li>$\\(||d_i|| \\leq 1\\)</li> </ul> </li> </ul> </li> <li>Sparse Dictionary Learning Loss</li> <li>After learning, these can be used as discriminative features<ul> <li>Expensive to compute</li> </ul> </li> </ul>"},{"location":"Diffusion%20LM/","title":"Diffusion LM","text":"<ul> <li>Diffusion-LM Improves Controllable Text Generation</li> <li>Controlling the behavior of language models (LMs) without re-training is a major open problem in natural language generation</li> <li>non-autoregressive language model based on continuous diffusions</li> <li>substantial departure from the current paradigm of discrete autoregressive generation</li> <li>iteratively denoises a sequence of Gaussian vectors into word vectors, yielding a sequence of intermediate latent variables</li> <li>continuous, hierarchical nature of these intermediate variables enables a simple gradient-based algorithm to perform complex, controllable generation tasks</li> <li>successful control of Diffusion-LM for six challenging fine-grained control tasks</li> </ul>"},{"location":"Diffusion%20Tensor/","title":"Diffusion Tensor","text":""},{"location":"Digital%20Phenotyping/","title":"Digital Phenotyping","text":"<ul> <li>The use of data collected from personal electronic devices like smart phones to diagnose and monitor medical and psychiatric conditions.</li> </ul>"},{"location":"Dikes%20and%20Rivers/","title":"Dikes and Rivers","text":"<ul> <li>Subjects alternate producing time intervals of a short and a long duration.  </li> <li>Initially, short is 2 seconds and long is 3.1 seconds</li> <li>They receive feedback on whether their estimate is within +/- 12.5% of the target, and receive \u201ctoo short\u201d or \u201ctoo long\u201d as feedback otherwise.</li> <li>After a number of trials, the criterion for the long interval starts to change</li> <li></li> <li>Short intervals are also effected and vice versa</li> </ul>"},{"location":"Dikes%20and%20Rivers/#model","title":"Model","text":"<ul> <li>ACTR declarative memory + Declarative Memory Blending</li> </ul>"},{"location":"Dikes%20and%20Rivers/#backlinks","title":"Backlinks","text":"<ul> <li>Dikes and Rivers</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Dilated%20Sliding%20Window%20Attention/","title":"Dilated Sliding Window Attention","text":"<ul> <li>Analgous to dilated CNN</li> <li>Assuming a fixed \\(d\\) and \\(w\\) for all layers, receptive field is \\(l \\times d \\times w\\) which can reach tens of thousands of tokens even with small values of \\(d\\)</li> <li></li> </ul>"},{"location":"Dimensionality%20Reduction/","title":"Dimensionality Reduction","text":"<ul> <li>Given<ul> <li>\\(\\((x_i)_{i = 1, \u2026,N}\\)\\) raw data points, \\(\\(x_i \\in \\mathbb{R}^n\\)\\) : High dim</li> </ul> </li> <li>To get<ul> <li>Low dim \\(\\(x_i \\in \\mathbb{R}^m\\)\\) where \\(\\(m &lt;n\\)\\)</li> </ul> </li> <li>f(x) is composed of m component functions aka features<ul> <li>\\(\\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\)\\) : scalar characteristic</li> <li>m such features : feature map<ul> <li> \\[(f_1 , \u2026, f_m)' =: f : \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\] </li> <li>maps input vectors to feature vectors</li> </ul> </li> </ul> </li> <li>KMeans</li> <li>PCA</li> <li>SOMs</li> </ul>"},{"location":"Dimensionality%20Reduction/#anchor","title":"anchor","text":""},{"location":"Dirac%20Delta/","title":"Dirac Delta","text":"<p>-\\(\\(P(X \\in A) = \\begin{cases}1&amp; \\text{if 0}\\in A\\\\[2ex] 0&amp; \\text{if } 0 \\notin A \\end{cases}\\)\\)</p> <ul> <li> \\[P(X \\in A) = \\int_{A}\\delta(x)dx\\] </li> </ul>"},{"location":"Dirac%20Delta/#in-mathbbrn","title":"In \\(\\mathbb{R}^{n}\\)","text":"<ul> <li>it is a PDF which describes prob concentrated in the origin</li> <li>Multi Point Distribution -&gt; combine dirac deltas</li> <li></li> </ul>"},{"location":"Direct%20entropy%20minimization/","title":"Direct Entropy Minimization","text":"<ul> <li>On the source domain we train our model,</li> <li>as usual using a supervised loss</li> <li>For the target domain, we do not have annotations and we can no longer use the segmentation loss to train</li> <li>supervision signal that could leverage visual information from the target samples, in spite of the lack of annotations</li> <li>constrain</li> <li>to produce high-confident predictions on target samples similarly to source samples</li> <li>entropy loss \\(\\mathcal{L}_{ent}\\) to maximize directly the prediction confidence in the target domain.</li> <li>Shannon Entropy</li> </ul>"},{"location":"Direct-drive/","title":"Direct-drive","text":"<ul> <li>Joint actuation, including no transmission elements (i.e., the link is bolted onto the output of the motor.)</li> </ul>"},{"location":"Dirichlet%20Distribution/","title":"Dirichlet Distribution","text":"<ul> <li>PDF</li> <li> \\[h(\\theta|\\alpha) = \\frac{1}{Z(\\alpha)} \\Pi_{j=1}^{l}\\theta_{j}^{a_{j}-1}\\] </li> <li>\\(\\(Z(\\alpha) = \\int_{\\mathcal{H}}\\Pi_{j=1}^{l}\\theta_{j}^{\\alpha_{j}-1}d\\theta\\)\\) is normalization constant. Ensures integral of h over \\(\\mathcal{H}\\) is 1</li> </ul>"},{"location":"Discrete%20-%3E%20Continuous/","title":"Discrete -&gt; Continous Transforms","text":""},{"location":"Discrete%20-%3E%20Continuous/#one-hot","title":"One hot","text":""},{"location":"Discrete%20-%3E%20Continuous/#binary-pattern","title":"Binary pattern","text":""},{"location":"Discrete%20-%3E%20Continuous/#linear-scale","title":"Linear scale","text":""},{"location":"Discrete%20-%3E%20Continuous/#word-vectors","title":"Word Vectors","text":""},{"location":"Discrete%20Cosine%20Transform/","title":"Discrete Cosine Transform","text":"<ul> <li>machine-learning-articles/cnns-and-feature-extraction-the-curse-of-data-sparsity.md at main \u00b7 christianversloot/machine-learning-articles #Roam-Highlights</li> <li>expresses a finite sequence of data points in terms of a sum of cosine functions oscillating at different frequencies<ul> <li>you make the CNN blind to the unique aspects represented by the numbers\u2026 despite the fact that they are already in there</li> <li>In my opinion, this can be explained by looking at the internals of a convolutional layer. It works as follows. You specify a number of filters which, during training, learn to recognize unique aspects of the image-like data. They can then be used to classify new samples - quite accurately, as we have seen with raw MNIST data. This means that the convolutional layer already makes your data representation sparser. What's more, this effect gets even stronger when layers like Pooling are applied</li> <li>But when you downsample the data first by e.g. applying the DCT, you thus effectively apply sparsening twice. My only conclusion can thus be that by consequence, the convolutional filters can no longer learn the unique aspects within the image-like data, as they are hidden in the data set made compact. Only then, I literally found out why people always suggest to input your image data into CNNs as untransformed as possible.</li> <li>Besides the architectural differences between them, one must also conclude that CNNs make data essentially sparser while SVMs do not.</li> </ul> </li> </ul>"},{"location":"Disparate%20Impact/","title":"Disparate Impact","text":"<ul> <li>Making decisions about people that impact different population subgroups disproportionately. This usually refers to situations where an algorithmic decision-making process harms or benefits some subgroups more than others.</li> <li>For example, suppose an algorithm that determines a Lilliputian's eligibility for a miniature-home loan is more likely to classify them as \u201cineligible\u201d if their mailing address contains a certain postal code. If Big-Endian Lilliputians are more likely to have mailing addresses with this postal code than Little-Endian Lilliputians, then this algorithm may result in disparate impact.</li> </ul>"},{"location":"Disparate%20Treatment/","title":"Disparate Treatment","text":"<ul> <li>Factoring subjects' sensitive attributes into an algorithmic decision-making process such that different subgroups of people are treated differently.</li> <li>For example, consider an algorithm that determines Lilliputians\u2019 eligibility for a miniature-home loan based on the data they provide in their loan application. If the algorithm uses a Lilliputian\u2019s affiliation as Big-Endian or Little-Endian as an input, it is enacting disparate treatment along that dimension.</li> <li>Contrast with disparate impact, which focuses on disparities in the societal impacts of algorithmic decisions on subgroups, irrespective of whether those subgroups are inputs to the model.</li> <li>Because sensitive attributes are almost always correlated with other features the data may have, explicitly removing sensitive attribute information does not guarantee that subgroups will be treated equally. For example, removing sensitive demographic attributes from a training data set that still includes postal code as a feature may address disparate treatment of subgroups, but there still might be disparate impact upon these groups because postal code might serve as a proxy for other demographic information.</li> </ul>"},{"location":"Displacement/","title":"Displacement","text":"<ul> <li> \\[\\Delta x = x_{f}-x_{i}\\] </li> </ul>"},{"location":"Distance%20Measures/","title":"Distance Measures","text":"<ul> <li>Euclidean Distance</li> <li>Cosine Similarity</li> <li>Hamming Distance</li> <li>Manhattan Distance</li> <li>Chebyshev Distance</li> <li>Hausdorff Distance</li> <li>Chi Squared Distance</li> <li>Bhattacharya Distance</li> <li>Minkowski Distance</li> <li>Jaccard Distance</li> <li>Haversine Distance</li> <li>S\u00f8rensen-Dice Index</li> </ul>"},{"location":"DistillBERT/","title":"DistillBERT","text":"<ul> <li>DistilBERT, a Distilled Version of BERT: Smaller, Faster, Cheaper and Lighter</li> <li>Huggingface</li> <li>general-purpose pre-trained version of BERT</li> <li>40% smaller, 60% faster, cheaper to pre-train, and retains 97% of the language understanding capabilities</li> <li>knowledge distillation during the pre-training phase</li> <li>triple loss combining language modeling, distillation and cosine-distance losses</li> </ul>"},{"location":"Distillation%20Algorithms/","title":"Distillation Algorithms","text":"<ul> <li>Adversarial Distillation</li> <li>Multi Teacher Distillation</li> <li>Cross Modal Distillation</li> <li>Graph Based Distillation</li> <li>Attention Based Distillation</li> <li>Data Free Distillation</li> <li>Quantized Distillation</li> </ul>"},{"location":"Distillation%20Loss/","title":"Distillation Loss","text":"<ul> <li> \\[\\mathscr{l}(p, softmax(z))+T^{2}\\mathscr{l}(softmax(\\frac{r}{T}), softmax(\\frac{z}{T}))\\] </li> <li>Negative Cross Entropy + other</li> <li>p is the true probability Distributions</li> <li>z,r are outputs of the student and teacher model</li> <li>T is the temperature to make Softmax smoother</li> </ul>"},{"location":"Distillation%20Schemes/","title":"Distillation Schemes","text":"<ul> <li>Offline Distillation</li> <li>Self Distillation</li> </ul>"},{"location":"Distillation%20Token/","title":"Distillation Token","text":"<p>title: Distillation Token</p> <p>tags: architecture</p>"},{"location":"Distillation%20Token/#distillation-token","title":"Distillation Token","text":"<ul> <li>A learned vector that flows through the network along with the transformed image data</li> <li>cues the model for its distillation output, which can differ from its class output</li> <li>Specific to Transformers</li> </ul>"},{"location":"Distilling%20the%20Knowledge%20in%20a%20Neural%20Network/","title":"Distilling the Knowledge in a Neural Network","text":"<ul> <li>Geoffrey Hinton, Oriol Vinyals, Jeff Dean</li> </ul>"},{"location":"Distilling%20the%20Knowledge%20in%20a%20Neural%20Network/#intro","title":"Intro","text":"<ul> <li>compress the knowledge in an ensemble into a single model which is much easier to deploy</li> <li>new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse</li> <li>Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel</li> <li>Many insects have a larval form that is optimized for extracting energy and nutrients from the environment and a completely different adult form that is optimized for the very different requirements of traveling and reproduction</li> <li>training must extract structure from very large, highly redundant datasets but it does not need to operate in real time and it can use a huge amount of computation. Deployment to a large number of users, however, has much more stringent requirements on latency and computational resources.</li> <li>The cumbersome model could be an ensemble of separately trained models or a single very large model trained with a very strong regularizer such as dropout</li> <li>we tend to identify the knowledge in a trained model with the learned parameter values and this makes it hard to see how we can change the form of the model but keep the same knowledge.</li> <li>learned</li> <li>where T is a temperature that is normally set to 1</li> </ul>"},{"location":"Distilling%20the%20Knowledge%20in%20a%20Neural%20Network/#observations","title":"Observations","text":"<ul> <li>This net achieved 67 test errors whereas a smaller net with two hidden layers of 800 rectified linear hidden units and no regularization achieved 146 errors</li> <li>soft targets can transfer a great deal of knowledge to the distilled model, including the knowledge about how to generalize that is learned from translated training data even though the transfer set does not contain any translations.</li> <li>When the distilled net had 300 or more units in each of its two hidden layers, all temperatures above 8 gave fairly similar results</li> <li>But when this was radically reduced to 30 units per layer, temperatures in the range 2.5 to 4 worked significantly better than higher or lower temperatures.</li> </ul>"},{"location":"Distilling%20the%20Knowledge%20in%20a%20Neural%20Network/#automatic-speech-recognition","title":"Automatic Speech Recognition","text":"<ul> <li>State-of-the-art ASR systems currently use DNNs to map a (short) temporal context of features derived from the waveform to a probability distribution over the discrete states of a Hidden Markov Model (HMM)</li> <li>DNN produces a probability distribution over clusters of tri-phone states at each time and a decoder then finds a path through the HMM states that is the best compromise between using high probability states and producing a transcription that is probable under the language model.</li> <li>There is, however, another important objection to ensembles: If the individual models are large neural networks and the dataset is very large, the amount of computation required at training time is excessive, even though it is easy to parallelize.</li> </ul>"},{"location":"Distilling%20the%20Knowledge%20in%20a%20Neural%20Network/#jft","title":"JFT","text":"<ul> <li>JFT is an internal Google dataset that has 100 million labeled images with 15,000 labels. When we did this work, Google\u2019s baseline model for JFT was a deep convolutional neural network that had been trained for about six months using asynchronous stochastic gradient descent on a large number of cores.</li> <li>When the number of classes is very large, it makes sense for the cumbersome model to be an ensemble that contains one generalist model trained on all the data and many 'specialist' models, each of which is trained on data that is highly enriched in examples from a very confusable subset of the classes (like different types of mushroom)</li> <li>The softmax of this type of specialist can be made much smaller by combining all of the classes it does not care about into a single dustbin class.</li> <li>To reduce overfitting and share the work of learning lower level feature detectors, each specialist model is initialized with the weights of the generalist model</li> <li>These weights are then slightly modified by training the specialist with half its examples coming from its special subset and half sampled at random from the remainder of the training set. After training, we can correct for the biased training set by incrementing the logit of the dustbin class by the log of the proportion by which the specialist class is oversampled.</li> <li>In order to derive groupings of object categories for the specialists, we decided to focus on categories that our full network often confuses.</li> <li>Even though we could have computed the confusion matrix and used it as a way to find such clusters, we opted for a simpler approach that does not require the true labels to construct the clusters. In particular, we apply a clustering algorithm to the covariance matrix of the predictions of our generalist model, so that a set of classes Sm that are often predicted together will be used as targets for one of our specialist models, m.</li> <li>on-line version of the K-means algorithm to the columns of the covariance matrix, and obtained reasonable clusters</li> <li>One of our main claims about using soft targets instead of hard targets is that a lot of helpful information can be carried in soft targets that could not possibly be encoded with a single hard target.</li> <li>It is even more remarkable to note that we did not have to do early stopping: the system with soft targets simply 'converged' to 57%. This shows that soft targets are a very effective way of communicating the regularities discovered by a model trained on all of the data to another model.</li> <li>The specialists that we used in our experiments on the JFT dataset collapsed all of their non-specialist classes into a single dustbin class. If we allow specialists to have a full softmax over all classes, there may be a much better way to prevent them overfitting than using early stopping. A specialist is trained on data that is highly enriched in its special classes.</li> <li>This means that the effective size of its training set is much smaller and it has a strong tendency to overfit on its special classes.</li> <li>The use of specialists that are trained on subsets of the data has some resemblance to mixtures of experts which use a gating network to compute the probability of assigning each example to each expert</li> <li>At the same time as the experts are learning to deal with the examples assigned to them, the gating network is learning to choose which experts to assign each example to based on the relative discriminative performance of the experts for that example.</li> <li>Using the discriminative performance of the experts to determine the learned assignments is much better than simply clustering the input vectors and assigning an expert to each cluster, but it makes the training hard to parallelize: First, the weighted training set for each expert keeps changing in a way that depends on all the other experts and second, the gating network needs to compare the performance of different experts on the same example to know how to revise its assignment probabilities.</li> </ul>"},{"location":"Distilling%20the%20Knowledge%20in%20a%20Neural%20Network/#conclusions","title":"Conclusions","text":"<ul> <li>These difficulties have meant that mixtures of experts are rarely used in the regime where they might be most beneficial: tasks with huge datasets that contain distinctly different subsets.</li> <li>It is much easier to parallelize the training of multiple specialists</li> <li>e first train a generalist model and then use the confusion matrix to define the subsets that the specialists are trained on</li> <li>We have shown that distilling works very well for transferring knowledge from an ensemble or from a large highly regularized model into a smaller, distilled model.</li> <li>For really big neural networks, it can be infeasible even to train a full ensemble, but we have shown that the performance of a single really big net that has been trained for a very long time can be significantly improved by learning a large number of specialist nets, each of which learns to discriminate between the classes in a highly confusable cluster. We have not yet shown that we can distill the knowledge in the specialists back into the single large net.</li> </ul>"},{"location":"Distributive%20Interpretation%20%282%29/","title":"Distributive Interpretation (2)","text":""},{"location":"Distributive%20Interpretation%20%282%29/#distributive-interpretation-2","title":"Distributive Interpretation (2)","text":"<ul> <li>Object takes scope over the subject:</li> <li>Three aliens are holding two flags. = (Two flags, and then three aliens hold them)</li> </ul>"},{"location":"Distributive%20Interpretation/","title":"Distributive Interpretation","text":""},{"location":"Distributive%20Interpretation/#distributive-interpretation","title":"Distributive Interpretation","text":"<ul> <li>Subject takes scope over the object: Three aliens are holding two flags.</li> <li>Both Np's are interpreted individually and connected to each other No indicators of how they are connected.</li> </ul>"},{"location":"Distributive%20units/","title":"Distributive Units","text":"<ul> <li>each unit responds to multiple categories</li> <li>You must see the entire pattern over a collection of units, to uniquely categorize an input.</li> <li>The states of individual units are uninterpretable</li> </ul>"},{"location":"Divergence/","title":"Divergence","text":"<ul> <li>Flow field transports mass \\(\\(v:\\mathbb{R}^{3}\\rightarrow \\mathbb{R}^{3}\\)\\)</li> <li>Increase/loss of mass at point p</li> <li> \\[div_v: \\mathbb{R}^{3} \\rightarrow \\mathbb{R} = \\nabla \\cdot v = \\frac{\\partial v_{x}}{\\partial x} + \\frac{\\partial v_{y}}{\\partial y} + \\frac{\\partial v_{z}}{\\partial z}\\] </li> <li></li> </ul>"},{"location":"Divide%20Oriented/","title":"Divide Oriented","text":"<ul> <li>corresponds to physical realization (screen,printer), e.g., RGB, CMYK</li> </ul>"},{"location":"Document%20Triage/","title":"Document Triage","text":"<ul> <li>Characters in file must be MACHINE READABLE (Character Encoding)</li> <li>Character Encoding Identification (ASCII, UNICODE..)</li> <li>Language Identification (English, French,..)</li> <li>Text Sectioning</li> </ul>"},{"location":"Dopamine/","title":"Dopamine","text":"<p>title: Dopamine</p> <p>tags: brain, psychology</p>"},{"location":"Dopamine/#dopamine","title":"Dopamine","text":"<ul> <li>Dopamine is chemically expressed as C2H11NO2.</li> <li>It is a neuro-chemical created in various parts of the brain and is critical for all kinds of brain functions including thinking, carrying, sleeping, mood, attention, motivation, seeking and rewarding.</li> <li>The dopamine is responsible for the feeling of pleasure.</li> <li>When a person eats, drinks or performs a pleasurable action, dopamine is stimulated in his brain to repeat the action.</li> <li>Unexpected rewards increase the activity of dopamine neurons, acting as positive feedback signals for the brain regions associated with the preceding behavior.</li> <li>As learning takes place, the timing of activity will shift until it occurs upon the cue alone, with the expected reward having no additional effect.<ul> <li>And should the expected reward not be received, dopamine activity drops, sending a negative feedback signal to the relevant parts of the brain, weakening the positive association</li> </ul> </li> </ul>"},{"location":"Dot%20Product%20Attention/","title":"Dot Product Attention","text":"<ul> <li>Luong et al., 2015</li> <li> \\[f_{att}(h_{i}, s_{j}) = h_{i}^{T}s_{j}\\] </li> <li>Equivalent to Multiplicative Attention with no trainable weight matrix. Performs better at larger dimensions</li> <li>Identity matrix</li> <li>\\(h\\) is hidden state for encoder and \\(s\\) is hidden state for decoder</li> <li>A type of Attention Alignment</li> <li>Final scores after Softmax</li> <li></li> </ul>"},{"location":"Double%20Descent/","title":"Double Descent","text":"<ul> <li>When increasing the model size or the number of epochs, performance on the test set initially improves, then worsens but then again starts to improve and finally saturates.  </li> <li>This phenomena is against conventional wisdom, because the test error should not be decreasing again after increasing.</li> <li>occurs often in the over-parameterization regime<ul> <li>models which have a lot of parameters</li> <li>models that have huge complexity</li> </ul> </li> </ul>"},{"location":"Down%20Syndrome/","title":"Down Syndrome","text":"<ul> <li>A genetic disorder characterized by intellectual impairment and physical abnormalities that arises from the genome having an extra copy of chromosome 21.</li> </ul>"},{"location":"Downsampling/","title":"Downsampling","text":"<ul> <li>Overloaded term that can mean either of the following</li> <li>Reducing the amount of information in a feature in order to train a model more efficiently. For example, before training an image recognition model, downsampling high-resolution images to a lower-resolution format.</li> <li>Training on a disproportionately low percentage of over-represented class examples in order to improve model training on under-represented classes. For example, in a class-imbalanced dataset, models tend to learn a lot about the majority class and not enough about the minority class. Downsampling helps balance the amount of training on the majority and minority classes.</li> </ul>"},{"location":"DrawBench/","title":"DrawBench","text":"<ul> <li>comprehensive and challenging benchmark for text-to-image models</li> </ul>"},{"location":"Drop%20Delivery/","title":"Drop Delivery","text":"<ul> <li>A method of introducing an object to the workplace by Gravity. Usually, a chute or container is so placed that, when work on the part is finished, it will fall or drop into a chute or onto a conveyor with little or no transport by the robot.</li> </ul>"},{"location":"Dropout/","title":"Dropout","text":"<ul> <li>Applied to Dense Layers</li> <li>Training : Randomly (Bernoulli, p = 0.5 say) set #activations to 0</li> <li>Generally p = 0.1, 0.5</li> <li>Testing: Reweight by p<ul> <li>Because after training values will increase by \\(\\(1/(1-p)\\)\\)</li> </ul> </li> <li>Reduces co dependence between neurons</li> <li>Decreases overfitting</li> <li>Start with small rate : 20 %</li> <li>Helps with small datasets</li> <li>Reducing Co adaptation by making the presence of other hidden [neurons] unreliable</li> <li>The authors found that there is a trade-off between when Dropout is necessary, and when it's no longer useful. First, to cover the case where the dataset is extremely small: even Dropout does not improve performance in that case, simply because the dataset size is too small. The same is true for datasets that are large enough: Dropout then does no longer improve the model, but rather, model performance gets worse.</li> </ul>"},{"location":"Dual-memory%20Approach/","title":"Dual-memory Approach","text":""},{"location":"Dual-memory%20Approach/#dual-memory-approach","title":"Dual-memory Approach","text":"<ul> <li>Combination of several memories specialized for storing different types of data and supporting different functionalities</li> </ul>"},{"location":"Dual-memory%20Approach/#triplestores","title":"Triplestores","text":"<ul> <li>The contents of this memory system is semantic in nature (small)</li> </ul>"},{"location":"Dual-memory%20Approach/#leveldb","title":"LevelDB","text":"<ul> <li>key-value storage database, operate in RAM (developed by Google)</li> <li>Interpretation includes computing spatial relations between objects to keep an updated relational model of the scene around the robot</li> </ul>"},{"location":"Dual-memory%20Approach/#backlinks","title":"Backlinks","text":"<ul> <li>Dual-memory Approach</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Dynamic%20Eager%20Execution/","title":"Dynamic Eager Execution","text":"<ul> <li>operations are executed immediately</li> <li>more readable code and easier to debug</li> <li>lower performance than\u00a0Static Graph Execution</li> <li>graph architecture can evolve dynamically</li> </ul>"},{"location":"Dynamic%20Sparsity/","title":"Dynamic Sparsity","text":"<ul> <li>train intrinsically sparse neural networks from scratch using only a small proportion of parameters and FLOPs</li> <li>Dynamic sparsity enables training sparse models from scratch, hence the training and inference FLOPs and memory requirements are only a small fraction of the dense models.</li> <li>models built with dynamic sparsity can be trained from scratch to match their dense counterparts without involving any pre-training or dense training</li> </ul>"},{"location":"Dynamic%20visual%20attention/","title":"Dynamic Visual Attention","text":"<ul> <li>Incremental coding length (ICL) using the features of local image patches is proposed to maximise the entropy of the sampled visual features</li> <li>unexpected features elicit entropy gain in the perception state and are therefore assigned high energy</li> <li>The probability function of feature activities of this model is updated dynamically</li> </ul>"},{"location":"EEG%20Artifacts/","title":"EEG Artifacts","text":"<ul> <li>ICA</li> <li></li> </ul>"},{"location":"EEG%20Artifacts/#drift","title":"Drift","text":"<ul> <li>A</li> </ul>"},{"location":"EEG%20Artifacts/#probably-disconnected","title":"Probably Disconnected","text":""},{"location":"EEG%20Artifacts/#periodic-probably-from-ecg","title":"Periodic - Probably From ECG","text":"<ul> <li>Arteries in neck exposed when person is nervous</li> </ul>"},{"location":"EEG%20Baseline%20Correction/","title":"EEG Baseline Correction","text":"<ul> <li>Signal drifts</li> <li>Adjust pre stimulus value by averaging across pre stimulus points during baseline period</li> <li>Subtract average value from post stimulus period</li> <li>Period where nothing is going on</li> <li>100-200 ms</li> <li></li> </ul>"},{"location":"EEG%20Cap/","title":"EEG Cap","text":"<ul> <li>![im](images/Pasted%20Image%2020220510230806.png|]]</li> <li></li> </ul>"},{"location":"EEG%20Cluster%20Testing/","title":"EEG Cluster Testing","text":"<ul> <li>If done for each point, same test repeated and false positives increase</li> <li>eg: t test in each electrode</li> <li>A result is more believable if it occurs in a set of adjacent channels:  <ul> <li>Threshold data of statistical test  </li> <li>Compute clusters  </li> <li>Threshold randomized data  </li> <li>Compute clusters for 100 or so distr of randomized data</li> <li>Decide if its rare</li> </ul> </li> <li> \\[sumT = \\text{sum of all t stats}\\] </li> <li><ul> <li>There was a significant difference between easy and more difficult trials between 712 ms post-stimulus and 768 ms post-stimulus. This difference was initially localized to a few central electrodes but over time spread out more posteriorly. This is consistent with previous studies that have shown</li> </ul> </li> </ul>"},{"location":"EEG%20Filtering/","title":"EEG Filtering","text":"<ul> <li>Remove 50/60Hz notch filter for line noise</li> <li>Might introduce distortion</li> </ul>"},{"location":"EEG%20Statistical%20Analysis/","title":"EEG Statistical Analysis","text":"<ul> <li>EEG Cluster Testing</li> </ul>"},{"location":"EEG/","title":"EEG","text":"<ul> <li>Electrical activity on the surface of the brain</li> <li>Frequencies</li> <li>Pyramidal cell</li> <li>Electrode nomenclature</li> <li>Cheaper than fMRI</li> <li>Fast signals</li> <li>Low anatomical specificity<ul> <li>Cant find where its coming from</li> <li>Lots of noise</li> </ul> </li> <li>EEG Artifacts</li> <li>EEG Filtering</li> <li>ERP</li> <li>EEG Baseline Correction</li> <li>EEG Statistical Analysis</li> <li>EEG Cap</li> <li>This might be related to fMRI</li> </ul>"},{"location":"ELECTRA/","title":"ELECTRA","text":"<ul> <li>ELECTRA: Pre-training Text Encoders As Discriminators Rather Than Generators</li> <li>Pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK] and then train a model to reconstruct the original tokens.</li> <li>sample-efficient pre-training alternative task called replaced token detection</li> <li>self-supervised task for language representation learning</li> <li>Instead of masking the input, their approach corrupts it by replacing some tokens with plausible alternatives sampled from a small generator network</li> <li>Then, instead of training a model that predicts the original identities of the corrupted tokens, the key idea is training a discriminative text encoder model to distinguish input tokens from high-quality negative samples produced by an small generator network</li> <li>more compute-efficient and results in better performance on downstream tasks</li> <li>particularly strong for small models</li> <li>GLUE</li> <li>performs comparably to [[RoBERTa|[RoBERTa](./RoBERTa.md) and XLNet while using less than 1/4 of their compute and outperforms them when using the same amount of compute.</li> </ul>"},{"location":"ELMO/","title":"ELMO","text":"<ul> <li>Deep Contextualized Word Representations</li> <li>context-sensitive word embeddings using the [[LSTM)](Long Short Term Memory (LSTM|Long Short Term Memory (LSTM|LSTM)]].md)-based Embedding from Language Models (ELMo) architecture</li> </ul>"},{"location":"ERP/","title":"ERP","text":"<ul> <li>Event related potentials</li> <li>-</li> <li></li> <li></li> </ul>"},{"location":"Eager%20Execution/","title":"Eager Execution","text":"<ul> <li>A TensorFlow programming environment in which operations run immediately. By contrast, operations called in graph execution don't run until they are explicitly evaluated. Eager execution is an imperative interface , much like the code in most programming languages. Eager execution programs are generally far easier to debug than graph execution programs.</li> </ul>"},{"location":"Early%20Ray%20Termination/","title":"Early Ray Termination","text":"<ul> <li>General acceleration idea: neglect regions with irrelevant information</li> </ul>"},{"location":"Early%20Stopping%20tricks/","title":"Early Stopping","text":"<ul> <li>No of epochs is a hyper parameter : to prevent overfitting</li> <li>Early Stopping is a regularization technique that improves image classification accuracy by intentionally stopping the training when validation loss increases. Training is stopped as training a model for too many epochs sometimes causes Overfitting.</li> <li>In Early Stopping, the number of epochs becomes a tunable hyperparameter. We continuously store the best parameters during training, and when these parameters no longer change for several epochs, we stop training.</li> </ul>"},{"location":"Earth%20Mover%27s%20Distance%20%28EMD%29/","title":"Earth Mover's Distance (EMD)","text":"<ul> <li>A measure of the relative similarity between two documents. The lower the value, the more similar the documents.</li> <li>Spatial Distance between two PDF</li> </ul>"},{"location":"Edema/","title":"Edema","text":"<ul> <li>Swelling as a result of fluid retention or build-up</li> </ul>"},{"location":"Effect%20Of%20Depth/","title":"Effect of Depth","text":"<ul> <li>Adding skip connections make the loss surface smoother</li> <li></li> </ul>"},{"location":"Effect%20Of%20Depth/#deeper-architectures","title":"Deeper Architectures","text":"<ul> <li>Makes more uneven and chaotic</li> <li></li> </ul>"},{"location":"Effect%20Of%20Depth/#wider-architectures","title":"Wider Architectures","text":"<ul> <li>Makes landscape smoother and flatter</li> <li></li> </ul>"},{"location":"Effects%20of%20Contextual%20Cues%20on%20Inferring%20and%20Remembering%20Meanings%20of%20New%20Word/","title":"Effects of Contextual Cues on Inferring and Remembering Meanings of New Word","text":"<ul> <li>xiaolongli</li> <li>Li, X. (1988). Effects of contextual cues on inferring and remembering meanings of new words. Applied linguistics, 9(4), 402-413.</li> </ul>"},{"location":"Effects%20of%20Contextual%20Cues%20on%20Inferring%20and%20Remembering%20Meanings%20of%20New%20Word/#abstract","title":"Abstract","text":"<ul> <li>This study tested four directional hypotheses: Compared with those receiving cue- inadequate sentences, subjects receiving cue-adequate sentences will (1) report greater ease in word inference, and (2) score higher in inferring and remembering the contextual meanings of unfamiliar words. (3) Contextual cues being equally adequate, subjects reading, in contrast to listening to, the sentences will better infer and remember the contextual meanings ofunfamiliar words. (4) The higher the scores of word inference, the better the retention of the contextual meanings of the target word</li> <li>An approach combining schema theory and the generative model of comprehension was usedfor the rationale of this study and the discussion of its findings.</li> </ul>"},{"location":"Effects%20of%20Contextual%20Cues%20on%20Inferring%20and%20Remembering%20Meanings%20of%20New%20Word/#literature-review","title":"LITERATURE REVIEW","text":"<ul> <li>Inferring, or 'inferencing', the meanings of unfamiliar words in context can be seen as 'a process of identifying and acquiring' new vocabulary by utilizing 'attributes and contexts that are familiar'</li> <li>In language learning, inferring word meanings while reading or listening is a process of vocabulary acquisition which has an important influence upon comprehension either in a first language (Kruse 1979) or in a second language (Yorio 1971).</li> <li>Contextual cues can affect the process and outcome of word inference.</li> <li>Carton (1971) hypothesized that in the process of identifying and acquiring unfamiliar words in context, greater certainty results from guesses based on many cues than on few.</li> <li>However, for contextual cues to be of real help for word inference, they must (1) be perceptually and conceptually familiar to the text-receiver, and (2) contain the information available for the text-receiver to find the relevant schemata in order to (a) account for the oncoming input in the text, and (b) identify unfamiliar stimuli in context.</li> <li>Memories are, in a sense, natural effects of the comprehension process (Rumelhart and Ortony 1977) which, by nature, is schematic (Bartlett 1932</li> <li>memory performance is enhanced to the extent that the encoding context forms an integrated unit with the to-be-remembered word</li> </ul>"},{"location":"Effects%20of%20Contextual%20Cues%20on%20Inferring%20and%20Remembering%20Meanings%20of%20New%20Word/#this-study-purpose-and-hypotheses","title":"THIS STUDY: PURPOSE AND HYPOTHESES","text":"<ul> <li>The present study was conducted among second language learners</li> <li>It not only focused on the effects of cue adequacy on inferring and remembering the meanings of new words in discrete, semantically disconnected sentences, but also aimed at an empirical exploration concerning the relationship between word inference and retention.</li> <li>this study compared the effects of cue adequacy in both reading and listening contexts</li> <li>cue-adequate sentences were compared with their cue- inadequate counterparts for testing four directional hypotheses. These were: Compared with those receiving cue-inadequate sentences, subjects receiving cue-adequate sentences will (1) report greater ease in inferring the meanings of new words, and (2) score higher in inferring and remembering the meanings of new words. (3) Contextual cues being equally adequate, subjects reading, in contrast to listening to, the sentences will better infer and remember the meanings of unfamiliar words. (4) The higher the scores of word inference, the better the retention of the meanings of the target words.</li> <li>a sentence with certain input information that contains clues sufficient for inferring the contextual meaning of a target word was defined as a cue-adequate sentence, while a sentence without such input information was defined as a cue-inadequate one</li> <li>For example, the sentence John took out a collapsible bicycle and rode to school was treated as a cue- inadequate sentence, for, in this sentence, there was no input information signaling any clue to the contextual meaning of the target word collapsible</li> <li>However, the sentence John took out a collapsible bicycle, unfolded it, and rode to school was treated as a cue-adequate one, for the word unfolded provided the clue to the approximate meaning of the target word.</li> </ul>"},{"location":"Effects%20of%20Contextual%20Cues%20on%20Inferring%20and%20Remembering%20Meanings%20of%20New%20Word/#method","title":"METHOD","text":""},{"location":"Effects%20of%20Contextual%20Cues%20on%20Inferring%20and%20Remembering%20Meanings%20of%20New%20Word/#participant","title":"Participant","text":"<ul> <li>Forty-eight advanced trainees from an EAP (English for Academic Purposes) center in China were involved in this study.</li> <li>Their average age was 35 (SD = 7), ranging from 22 to 48</li> <li>They were randomly assigned into four treatment groups, namely</li> <li>LC\u2014 (i.e. listening group with inadequate cues); RC\u2014 (i.e. reading group with inadequate cues); LC + (i.e. listening group with adequate cues), and RC+ (i.e. reading group with adequate cues). Of the four groups, two (i.e. LC\u2014 and RC\u2014) received cue-inadequate sentences, and the others (i.e. LC+ and RC+), cue- adequate sentences. In each pair of groups that received the same sentences, one group (i.e. LC\u2014 and LC+) took the listening test, and the other (i.e. RC\u2014 and RC+), the reading test</li> <li>There were two independent variables. The first one, text, had two levels- sentences with adequate cues versus inadequate cues. The second one, language skills, also had two levels\u2014reading versus listening dependent</li> <li>These were group means in terms of: (1) measures of word inference (i.e. inferring the meanings of unfamiliar words); (2) ratings of degrees of difficulty of word inference, and (3) measures of word retention (i.e. recall of the inferred meanings of the target words).</li> </ul>"},{"location":"Effects%20of%20Contextual%20Cues%20on%20Inferring%20and%20Remembering%20Meanings%20of%20New%20Word/#task","title":"Task","text":"<ul> <li>Sixty discrete, semantically disconnected sentences were constructed for the experiment.</li> <li>They formed two sets of counterparts. Each set was composed of 30 sentences.</li> <li>One set consisted of cue-adequate sentences, and the other of cue-inadequate sentences</li> <li>A target word was defined as a perceptually, not conceptually, unfamiliar term.</li> <li>Since the target words were only perceptually unfamiliar, it would not be a prerequisite for the subjects to acquire any new concept to perform the task for this experiment.</li> <li>By the same token, the topic of all the test items was based on common knowledge; thus, there was no need to turn to any biased or specialized frame of reference for inferring word meanings in this experiment</li> <li>Furthermore, no meaning of any target word for this study could be deduced simply by applying morphological knowledge in terms of stems, affixes, or other devices of word formation.</li> <li>In the pretest, the subjects were asked to write down (either in English or Chinese) the common meanings of the target words they knew.</li> <li>Three more tasks were performed after the pretest. The first one, word inference, was to infer the contextual meanings of the target words based on the input information in the sentences in which the target words were embedded.</li> <li>Both tapes, one containing sentences with adequate, and the other sentences with inadequate cues, were produced by a native English speaker at a speed of about 90 words per minute.</li> <li>The subjects listened to the sentences one by one, with each sentence repeated three times.</li> <li>Sentences were shown one by one on the screens by using a mask, and presented at the same rate as the corresponding items on the tapes for the listening groups.</li> <li>The tests were presented in an open-ended, not in a multiple-choice, form</li> <li>After reading or listening to each sentence, the subjects were asked to state (either in English or Chinese) their guesses of the contextual meaning of the target word in the sentence</li> <li>The second task after the pretest was to rate the degrees of difficulty in terms of word inferences</li> <li>The last task, word retention, was a cued recall of the target words' inferred contextual meanings.</li> <li>Each target word was cued by another word from the same sentence that had been processed for inferring the contextual meaning of the target word.</li> <li>The target words were listed in exactly the same order as they appeared in the tests for word inference.</li> </ul>"},{"location":"Effects%20of%20Contextual%20Cues%20on%20Inferring%20and%20Remembering%20Meanings%20of%20New%20Word/#data-analyses","title":"DATA ANALYSES","text":"<ul> <li>Hypothesis 1 was tested by a Chi-square, Hypothesis 2 by two separate one-way ANOVAs, Hypothesis 3 by two separate Duncan's Multiple Range Tests, and Hypothesis 4 by a Correlation Test</li> <li>Since the Chi-square is a test especially designed for nominal data, it was decided beforehand that the nine-point scale should be dichotomized: ratings less than 5 were defined as 'difficult' (to infer the contextual meanings of the target words from the discrete sentences), while ratings equal to or greater than 5 were defined as 'easy'.</li> <li>Cronbach's Alpha was used for computing the test reliability. Reliability coefficients for the four tests of word inference were 0.60 for LC\u2014, 0.54 for RC-, 0.68 for LC+, and 0.64 for RC+, which were rather low.</li> <li>However, they can be considered as being acceptable for this study, for both the sample size (12 per cell) and the number of test items (30 for each test) were very small.</li> </ul>"},{"location":"Effects%20of%20Contextual%20Cues%20on%20Inferring%20and%20Remembering%20Meanings%20of%20New%20Word/#results","title":"RESULTS","text":"<ul> <li>Data analyses indicated that all the four hypotheses were confirmed with statistical significance.</li> </ul>"},{"location":"Effects%20of%20Contextual%20Cues%20on%20Inferring%20and%20Remembering%20Meanings%20of%20New%20Word/#hypothesis-1","title":"Hypothesis 1","text":"<ul> <li>no significant difference between the four groups in rating degrees of difficulty of word inference</li> <li>result of the Chi-square Test presented null hypothesis could be rejected</li> </ul>"},{"location":"Effects%20of%20Contextual%20Cues%20on%20Inferring%20and%20Remembering%20Meanings%20of%20New%20Word/#hypothesis-2","title":"Hypothesis 2","text":"<ul> <li>subjects in the four groups performed differently on both tasks of word inference and word retention</li> <li>showed that RC+ scored significantly higher than LC+, and that both RC+ and LC+</li> <li>scored significantly higher than RC\u2014 and LC\u2014</li> <li>However, there was no significant difference between RC\u2014 and LC\u2014</li> <li>showed both RC+ and LC+ scored significantly higher than RC\u2014 and LC\u2014, and that R C + scored significantly higher than LC+. However, there was no significant difference between RC\u2014 and LC-.</li> </ul>"},{"location":"Effects%20of%20Contextual%20Cues%20on%20Inferring%20and%20Remembering%20Meanings%20of%20New%20Word/#hypothesis-3","title":"Hypothesis 3","text":"<ul> <li>both in word inference and retention, R C + scored significantly higher than LC+; however, in either word inference or retention, there was no significant difference between RC\u2014 and LC\u2014</li> </ul>"},{"location":"Effects%20of%20Contextual%20Cues%20on%20Inferring%20and%20Remembering%20Meanings%20of%20New%20Word/#hypothesis-4","title":"Hypothesis 4","text":"<ul> <li>there was a positive correlation of statistical significance between word inference and word retention</li> </ul>"},{"location":"Effects%20of%20Contextual%20Cues%20on%20Inferring%20and%20Remembering%20Meanings%20of%20New%20Word/#discussion-and-conclusion","title":"DISCUSSION AND CONCLUSION","text":"<ul> <li>ubjects receiving cue- adequate sentences, in contrast to cue-inadequate sentences, not only reported greater ease in word inference, but also scored significantly higher in inferring and remembering the meanings of unfamiliar words in context</li> <li>existed a positive correlation between word inference and word retention. That is, the higher the group means in inferring the contextual meanings of unfamiliar words, the better the performance in remembering the meanings of those words</li> <li>contextual cues being equally adequate, subjects reading the sentences scored significantly higher in both inferring and remembering the contextual meanings of unfamiliar words than those listening to the sentences.</li> <li>This finding further sustained Carton's (1971) hypothesis that texts with adequate contextual cues minimize errors in the process of identifying and acquiring new words in a natural context</li> <li>The presence of contextual cues means 'bridging information' (Garrod and Sanford 1981), grammatical and/or semantic, conceptual as well as perceptual.</li> <li>Without adequate bridging information, it would seem next to impossible to infer and recall the contextual meaning of any unfamiliar word.</li> <li>This explains why the LC\u2014 and RC\u2014 groups scored so low on both word inference and word retention.</li> <li>the target words associated with more powerful retrieval cues were more recallable than those associated with less powerful retrieval cues</li> <li>Probably, more powerfully associated retrieval cues better triggered the schematic memory, which created a 'short cut' that linked the process needed for recalling the contextual meaning of the target word and the initial process involved in inferring the contextual meaning of that target word.</li> <li>contextual cues being equally adequate (not inadequate), subjects in the reading group scored significantly higher in both word inference and word retention than subjects in the listening group.</li> <li>not clear why this was so</li> <li>That is, the subjects might be more competent in reading than in listening contextual cues presented visually were more accessible</li> </ul>"},{"location":"Effects%20of%20Contextual%20Cues%20on%20Inferring%20and%20Remembering%20Meanings%20of%20New%20Word/#implications","title":"Implications","text":"<ul> <li>irst, since adequate cues in context can relieve learners of English as a second language from the anxiety of unfamiliar words, it might follow that reasonably sufficient contextual cues should be provided in texts for second language learners, so that enough information can be created for them to play the 'psychoUnguistic guessing game' (Goodman 1983)</li> <li>contextual cues can enhance inferring and remembering the meanings of unfamiliar words in context,</li> <li>since contextual cues being equally adequate, subjects can, within the same amount of time, better acquire vocabulary through visual patterns of learning than through oral patterns, it might follow that learners whose learning styles are congruent or similar to the subjects in this study, may well enlarge their vocabulary for reading in a more efficient way through visual ways of learning</li> </ul>"},{"location":"Effects%20of%20Contextual%20Cues%20on%20Inferring%20and%20Remembering%20Meanings%20of%20New%20Word/#pictures","title":"Pictures","text":""},{"location":"Effects%20of%20Regularization/","title":"Effects of Regularization","text":"<ul> <li>The Effects of Regularization and Data Augmentation are Class Dependent</li> <li>Current Deep Networks heavily rely on regularizers such as data Augmentation (DA) or Weight Decay, and employ structural risk minimization, i.e., Cross Validation, to select the optimal regularization hyper-parameters</li> <li>weight decay increases the average test performances at the cost of significant performance drops on some specific classes</li> <li>unfair across classes</li> <li>By focusing on maximizing aggregate performance statistics we have produced learning mechanisms that can be potentially harmful, especially in transfer learning tasks</li> <li>optimal amount of DA or weight decay found from cross-validation leads to disastrous model performances on some classes</li> <li>only by introducing random crop DA during training</li> <li>such performance drop also appears when introducing uninformative regularization techniques such as weight decay</li> <li>ur search for ever increasing generalization performance \u2013 averaged over all classes and samples \u2013 has left us with models and regularizers that silently sacrifice performances on some classes.</li> <li>varying the amount of regularization employed during pre-training of a specific dataset impacts the per-class performances of that pre-trained model on different downstream tasks e.g. an ImageNet pre-trained ResNet50 deployed on INaturalist sees its performances fall from 70% to 30% on a particular classwhen introducing random crop DA during the Imagenet pre-training phase</li> <li>designing novel regularizers without class-dependent bias remains an open research question</li> <li>Categories largely identifiable by color or texture (for e.g., yellow bird, textured mushroom) are unaffected by aggressive cropping, while categories identifiable by shape (for e.g., corkscrew) see a performance degradation with aggressive cropping that only contains part of the object</li> <li>Conversely, color jitter does not affect shape or texture-based categories (for e.g., zebra), but affects color-based categories (for e.g., basket ball)</li> </ul>"},{"location":"Efferent/","title":"Efferent","text":"<ul> <li>Motor Division</li> <li>Instructions from brain to muscle and glands</li> <li>Also includes Somatic + Autonomic</li> </ul>"},{"location":"EfficientNet/","title":"EfficientNet","text":"<p>title: EfficientNet</p> <p>tags: architecture</p>"},{"location":"EfficientNet/#efficientnet","title":"EfficientNet","text":"<ul> <li>LATER</li> </ul>"},{"location":"Eigenvector/","title":"Eigenvector","text":""},{"location":"Einsum/","title":"Einsum","text":"<ul> <li>Matrix transpose<ul> <li>ij -&gt; ji</li> </ul> </li> <li>Sum<ul> <li>ij -&gt;</li> </ul> </li> <li>Column sum<ul> <li>ij -&gt; j</li> </ul> </li> <li>Row sum<ul> <li>ij -&gt; i</li> </ul> </li> <li>Matrix vector multiply<ul> <li>ik, k -&gt; i</li> </ul> </li> <li>Matrix matrix multiply<ul> <li>ik, kj -&gt; ij</li> </ul> </li> <li>Dot product<ul> <li>i,i -&gt;</li> <li>ij, ij -&gt;</li> </ul> </li> <li>Hadmard product<ul> <li>ij, ij -&gt; ij</li> </ul> </li> <li>Outer product<ul> <li>i,j -&gt; ij</li> </ul> </li> <li>Batch matrix multiply<ul> <li>ijk, ikl -&gt; ijl</li> </ul> </li> <li>Tensor Contraction<ul> <li>pqrs , tuvr -&gt; pstuv</li> </ul> </li> </ul>"},{"location":"Electrical%20Energy/","title":"Electrical Energy","text":"<ul> <li>Electrical energy changed into heat = potential difference x current x time</li> <li> \\[E= \\frac{V}{t}\\] </li> </ul>"},{"location":"Electroconvulsive%20Therapy%20%28ECT%29/","title":"Electroconvulsive Therapy (ECT)","text":"<ul> <li>A therapeutic treatment for depression and other mental illnesses that sends small electric currents over the scalp to trigger a brief seizure.</li> </ul>"},{"location":"Electrode%20nomenclature/","title":"Electrode nomenclature","text":"<p>5---</p> <p>title: Electrode nomenclature</p>"},{"location":"Electrode%20nomenclature/#electrode-nomenclature","title":"Electrode Nomenclature","text":"<ul> <li>Bathing cap</li> <li>Top of head</li> <li>Cz : middle of head</li> <li>Left : odd</li> <li>Right : even</li> <li>Frontal : F</li> <li>Frontal Polar : Fp</li> <li>Temporal : T</li> <li>Posterior : Pz</li> <li>Occipital : Oz</li> </ul>"},{"location":"Elements%20of%20sets/","title":"Elements of Sets","text":"<ul> <li>The stickers we have in stocks are stars, the moons, item and a flag.</li> <li>I\u2019ll take two moons.</li> <li>The moons in the 2nd sentences should be understood to be some of the moons mentioned in the 1st sentence.</li> <li>Notice that to understand the 2nd sentence at all requires that we use the context of the first sentence to establish that the word \u2018moons\u2019 means moon stickers.</li> </ul>"},{"location":"Ellipsoids/","title":"Ellipsoids","text":"<ul> <li>Fractional Anisotropy</li> <li></li> <li>Linear, Planar, Spherical</li> <li>2D projection can convey ambiguous 3D orientation</li> </ul>"},{"location":"Elman%201990/","title":"Elman 1990","text":"<ul> <li>The network learned generalizations</li> <li>examine hidden unit activation pattern for each word measure distance between each pattern and every other pattern (Euclidean distance)</li> <li>Use this to create a hierarchical cluster.</li> <li>Network learned semantic classes</li> <li>If the input to a simulation is preselected to avoid problems, one has instantiated an expert filtering system.</li> <li>in order to accomplish the goal of creating word classes by surface structure alone, it appears that the input must be filtered in just the right way.</li> <li>Instead of semantic representations, semantics gets replaced with distributional information<ul> <li>This is not what humans know about word classes.</li> <li>If the simulation's goals are accomplished by avoiding pronouns then we have the equivalent of a pronoun filter</li> </ul> </li> <li>Some strings in English are both nouns and verbs, e.g. smell, break</li> <li>The simulation did not learn what children learn</li> <li>Yes, the input was oversimplified, but it's not clear that adding these additional features will make the model perform worse</li> <li>language is very redundant, so certain simplifications actually remove helpful features</li> <li>Categories can 'emerge' via statistical regularities</li> <li>Basic RNN Architectures can find these</li> </ul>"},{"location":"Elman%201991/","title":"Elman 1990","text":"<ul> <li>The network learned generalizations</li> <li>examine hidden unit activation pattern for each word measure distance between each pattern and every other pattern (Euclidean distance)</li> <li>Use this to create a hierarchical cluster.</li> <li>Network learned semantic classes</li> <li>If the input to a simulation is preselected to avoid problems, one has instantiated an expert filtering system.</li> <li>in order to accomplish the goal of creating word classes by surface structure alone, it appears that the input must be filtered in just the right way.</li> <li>Instead of semantic representations, semantics gets replaced with distributional information<ul> <li>This is not what humans know about word classes.</li> <li>If the simulation's goals are accomplished by avoiding pronouns then we have the equivalent of a pronoun filter</li> </ul> </li> <li>Some strings in English are both nouns and verbs, e.g. smell, break</li> <li>The simulation did not learn what children learn</li> <li>Yes, the input was oversimplified, but it's not clear that adding these additional features will make the model perform worse</li> <li>language is very redundant, so certain simplifications actually remove helpful features</li> <li>Categories can 'emerge' via statistical regularities</li> <li>Basic RNN Architectures can find these</li> </ul>"},{"location":"Elman%201992/","title":"Elman 1990","text":"<ul> <li>The network learned generalizations</li> <li>examine hidden unit activation pattern for each word measure distance between each pattern and every other pattern (Euclidean distance)</li> <li>Use this to create a hierarchical cluster.</li> <li>Network learned semantic classes</li> <li>If the input to a simulation is preselected to avoid problems, one has instantiated an expert filtering system.</li> <li>in order to accomplish the goal of creating word classes by surface structure alone, it appears that the input must be filtered in just the right way.</li> <li>Instead of semantic representations, semantics gets replaced with distributional information<ul> <li>This is not what humans know about word classes.</li> <li>If the simulation's goals are accomplished by avoiding pronouns then we have the equivalent of a pronoun filter</li> </ul> </li> <li>Some strings in English are both nouns and verbs, e.g. smell, break</li> <li>The simulation did not learn what children learn</li> <li>Yes, the input was oversimplified, but it's not clear that adding these additional features will make the model perform worse</li> <li>language is very redundant, so certain simplifications actually remove helpful features</li> <li>Categories can 'emerge' via statistical regularities</li> <li>Basic RNN Architectures can find these</li> </ul>"},{"location":"Elman%201993/","title":"Elman 1990","text":"<ul> <li>The network learned generalizations</li> <li>examine hidden unit activation pattern for each word measure distance between each pattern and every other pattern (Euclidean distance)</li> <li>Use this to create a hierarchical cluster.</li> <li>Network learned semantic classes</li> <li>If the input to a simulation is preselected to avoid problems, one has instantiated an expert filtering system.</li> <li>in order to accomplish the goal of creating word classes by surface structure alone, it appears that the input must be filtered in just the right way.</li> <li>Instead of semantic representations, semantics gets replaced with distributional information<ul> <li>This is not what humans know about word classes.</li> <li>If the simulation's goals are accomplished by avoiding pronouns then we have the equivalent of a pronoun filter</li> </ul> </li> <li>Some strings in English are both nouns and verbs, e.g. smell, break</li> <li>The simulation did not learn what children learn</li> <li>Yes, the input was oversimplified, but it's not clear that adding these additional features will make the model perform worse</li> <li>language is very redundant, so certain simplifications actually remove helpful features</li> <li>Categories can 'emerge' via statistical regularities</li> <li>Basic RNN Architectures can find these</li> </ul>"},{"location":"Elu/","title":"Elu","text":"<ul> <li> \\[f(x) = max(x, a \\cdot (e^x-1))\\] </li> <li></li> </ul>"},{"location":"Embedding%20Human%20Knowledge%20into%20Deep%20Neural%20Network%20via%20Attention%20Map/","title":"Embedding Human Knowledge into Deep Neural Network via Attention Map","text":"<ul> <li>Mitsuhara, Masahiro, Hiroshi Fukui, Yusuke Sakashita, Takanori Ogata, Tsubasa Hirakawa, Takayoshi Yamashita, and Hironobu Fujiyoshi. \u201cEmbedding Human Knowledge into Deep Neural Network via Attention Map.\u201d arXiv, December 19, 2019. http://arxiv.org/abs/1905.03540.</li> </ul>"},{"location":"Embedding%20Human%20Knowledge%20into%20Deep%20Neural%20Network%20via%20Attention%20Map/#intro","title":"Intro","text":"<ul> <li>focus on the attention mechanism of an attention branch network (ABN)</li> <li>propose a fine-tuning method that utilizes a single-channel attention map which is manually edited by a human expert.</li> <li>Our fine-tuning method can train a network so that the output attention map corresponds to the edited ones. As a result, the fine-tuned network can out- put an attention map that takes into account human knowl- edge</li> <li>ImageNet</li> <li>CUB-200-2010</li> <li>IDRiD</li> <li>human intuitive edit- ing via a visual interface and suggest new possibilities for human-machine cooperation in addition to the improvement of visual explanations.</li> <li>Typical visual explanation approaches in- clude class activation mapping (CAM) and Grad-CAM</li> <li>However, an inconsistency between the tar- get region of the recognition result, namely the ground truth (GT), and an attention region may occur.</li> <li>To this end, we focus on the visual explanation and the attention mechanism of ABN</li> <li>ABN applies an atten- tion map for visual explanation to the attention mechanism.</li> <li>We propose a fine-tuning method based on the characteristics of ABN and an edited attention map</li> <li>The proposed method fine-tunes the attention and perception branches of ABN to output the same attention map as the edited one.</li> </ul>"},{"location":"Embedding%20Human%20Knowledge%20into%20Deep%20Neural%20Network%20via%20Attention%20Map/#related-work","title":"Related Work","text":""},{"location":"Embedding%20Human%20Knowledge%20into%20Deep%20Neural%20Network%20via%20Attention%20Map/#embedding-human-knowledge","title":"Embedding Human Knowledge","text":"<ul> <li>human-in-the- loop (HITL)</li> <li>Branson et al. [4] pro- posed an interactive HITL approach that helps to train a decision tree by using a question and answer with respect to a specific bird.</li> <li>Deng et al. [7] used a bubble, that is, a circular bounding box, as human knowl- edge. This bubble information is annotated from an atten- tion region when a user distinguishes the two types of birds. By annotating the bubble with various pairs and users, char- acteristic regions of bird images can be obtained when we recognize bird categories.</li> <li>Linsley et al. [18] proposed a method that incorpo- rates human knowledge into large-scale deep neural net- works using the HITL framework. This method added a spatial attention mechanism into the attention mecha- nism [19, 15, 13, 2, 20, 35, 33, 37, 39, 40] of squeeze-and- excitation networks (SENet) [13] and trained the network by using a ClickMe map that introduces human knowledge to the weights of the attention mechanism.</li> </ul>"},{"location":"Embedding%20Human%20Knowledge%20into%20Deep%20Neural%20Network%20via%20Attention%20Map/#editing-the-attention-map","title":"Editing the Attention Map","text":"<ul> <li>In this experiment, we used an ABN whose backbone is 152-layer ResNet [12] (ResNet152+ABN) as a network mode</li> <li>Then, we selected the 1k misclassified samples from the validation samples and edited the maps</li> <li>btain the attention map from the at- tention branch, where the size of the attention map is 14\u00d714 pixels. Then, we edit the obtained attention map manually. Note that the attention map is resized to 224\u00d7224 pixels and is overlaid with the input image for ease of manual editing. The edited attention map is resized to 14 \u00d7 14 pixels and used for an attention mechanism to infer classification re- sults from the perception branch.</li> <li>By training the attention and percep- tion branches with the edited attention map including hu- man knowledge, ABN can output an attention map that con- siders this knowledge and thereby improve the classification performance.</li> <li>During the fine-tuning process, we update the parameters of the attention and perception branches by using the loss calculated from the attention map obtained from ABN and the edited attention map in addition to the loss of ABN</li> <li>To make an attention map from the bubbles, we use a kernel density estimation with multiple bubbles</li> <li>A dense region of bubbles indicates an impor- tant region for recognizing the bird category.</li> <li>In contrast, the proposed method highlights the local characteristic regions, such as the color and the head of the bird. In addition, the proposed method removes noise from the attention map by fine-tuning. Thus, the proposed method can also improve the performance of fine-grained recognition.</li> <li>Consequently, our method can gener- ate a more interpretable attention map and successfully em- bed human knowledge.</li> </ul>"},{"location":"Embedding%20Human%20Knowledge%20into%20Deep%20Neural%20Network%20via%20Attention%20Map/#fine-tuning-branches","title":"Fine Tuning Branches","text":"<ul> <li>\\(x_i\\) is the i-th sample</li> <li> \\[L_{abn}(x_{i})=L_{att}(x_{i})+L_{per}(x_{i})\\] <ul> <li>where \\(L_{arr}, L_{per}\\) are conventional cross entropy losses for the attention and perception branches, respectively</li> </ul> </li> <li> \\[L(x_{i})=L_{abn}(x_{i})+L_{map}(x_{i})\\] </li> <li>Edited map : M'<ul> <li> \\[L_{map}(x_{i})=\\gamma||M'(x_{i})-M(x_{i})||_{2}\\] <ul> <li>\\(\\gamma\\) is a scale factor</li> <li>\\(L_{map}\\) is larger than the others, hence needs to be scaled</li> </ul> </li> </ul> </li> </ul>"},{"location":"Embedding%20Human%20Knowledge%20into%20Deep%20Neural%20Network%20via%20Attention%20Map/#images","title":"Images","text":""},{"location":"Embedding%20ethical%20principles%20in%20collective%20decision%20support%20systems/","title":"Embedding Ethical Principles in Collective Decision Support Systems","text":"<ul> <li>Joshua Greene, Francesca Rossi, John Tasioulas, Kristen Brent Venable, and Brian Williams.</li> <li>authors envisioned a possible way forward to enable human-agent collectives [Jennings et al., 2014] to make ethical collective decisions</li> <li>By imbuing individual agents with ethical decision-making mechanisms (such as those mentioned in the previous section), a population of agents can take on different roles when evaluating choices of action with moral considerations in a given scenario</li> <li>Based on a set of initial ethics rules, more complex rules can be acquired gradually through learning.</li> <li>Their evaluations, manifested in the form of preferences and limited by feasibility constraints, can be aggregated to reach a collective decision</li> <li>need for new forms of preference representation in collective ethical decisionmaking</li> <li>potential candidate actions to choose from can vastly outnumber the number of agents involved which is very different from multi-agent voting scenarios.</li> <li>candidate actions may not be independent from each other, some of them may share certain features which describe their ethical dilemma situations</li> </ul>"},{"location":"Embedding%20ethical%20principles%20in%20collective%20decision%20support%20systems/#backlinks","title":"Backlinks","text":"<ul> <li>Building Ethics into Artificial Intelligence</li> <li>Embedding ethical principles in collective decision support systems</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Embedding/","title":"Embedding","text":"<ul> <li>More complex than 1 hot</li> <li>Lookup table is an example.<ul> <li> \\[token\\_embedding(i) = gather(W, i)\\] </li> </ul> </li> <li>Say vocabulary is (the cat walks)<ul> <li>Embedding vector v that will be learnt</li> <li>Values like : \\(v_{the}\\), \\(v_{cat}\\), \\(v_{walks}\\)</li> </ul> </li> </ul>"},{"location":"Embolism/","title":"Embolism","text":"<ul> <li>A clot caused by blood, fat, air or other types of fluid, gas or foreign material</li> </ul>"},{"location":"Emergentism/","title":"Emergentism","text":"<ul> <li>What seems symbolic emerges from distributed representations<ul> <li>qualitatively newand more complex structures can emerge from simpler, basic facts</li> <li>Language structure can emerge from simply listening and producing speech</li> <li>Structural properties of language, e.g. part-of-speech (nouns, verbs) can emerge from serial order, distributional properties and procedural memory.</li> </ul> </li> </ul>"},{"location":"Emperical%20Risk/","title":"Emperical Risk","text":"<ul> <li>TRAINING ERROR. Mean loss computed over training examples</li> <li> \\[R(f) = \\mathbb{E} _{(X,Y) \\sim P(X,Y)}[l(y, f(x))]\\] </li> <li> \\[R^{emp}(h) = \\frac{1}{N}\\Sigma_{i=1}^{N}L(h(x_{i}), y_{i})\\] </li> <li> <p>joint prob distribution \\(P(X\\in A,Y=c)\\) is unknown</p> <ul> <li>Decision Boundaries</li> </ul> </li> <li> <p>Learning set \\(\\(\\mathcal L\\)\\) is finite</p> </li> <li>Need an estimator to evaluate it<ul> <li>Supervised Learning<ul> <li>Compute \\(\\(\\mathcal L_{train}\\)\\)</li> <li>Risk train = (1/M)(sum of loss values for (y, f(x)))</li> <li>This is an unbiased estimator, so we can use it to approximate the optimal function f* that minimizes \\(\\(\\mathbb{R}\\)\\)</li> <li>This means that we find \\(\\(argmin_{f\\in F} \\hat R(f, \\mathcal{L}_Train)\\)\\) (out of all the possible functions)</li> <li>\\(\\(lim_{M\\rightarrow \\infty}(f^*_{\\mathcal{L}_Train}) = f^*\\)\\) : converges to the fn that minimizes emprical risk</li> </ul> </li> <li>Ordinary least squares regression</li> </ul> </li> </ul>"},{"location":"Enabling%20Device/","title":"Enabling Device","text":"<ul> <li>A manually operated device which when continuously activated, permits motion. Releasing the device shall stop robot motion and motion of associated equipment that may present a hazard.</li> </ul>"},{"location":"Encoder%20Decoder%20Attention/","title":"Encoder Decoder Attention","text":"<ul> <li>Q comes from prev decoder</li> <li>K,V from encoder</li> </ul>"},{"location":"Encodings/","title":"Encoding","text":""},{"location":"Encodings/#discrete-continuous","title":"Discrete -&gt; Continuous","text":""},{"location":"Encodings/#continous-discrete","title":"Continous -&gt; Discrete","text":""},{"location":"End-effector/","title":"End-effector","text":"<ul> <li>An accessory device or tool, specifically designed for attachment to the robot wrist or tool mounting plate to enable the robot to perform its intended task. (Examples may include: gripper, spot weld gun, arc weld gun, spray point gun or any other application tools.)</li> </ul>"},{"location":"Endoscope/","title":"Endoscope","text":"<ul> <li>An optical instrument containing a tube with a lighted end used for internal examinations</li> </ul>"},{"location":"Endpoint/","title":"Endpoint","text":"<ul> <li>The nominal commanded position that a manipulator will attempt to achieve at the end of a path of motion. The end of the distal link.</li> </ul>"},{"location":"Energy%20Transferred%20in%20a%20Component/","title":"Energy Transferred in a Component","text":"<ul> <li>charge passing through it x potential difference acorss it</li> <li> \\[W = QV\\] </li> </ul>"},{"location":"English%20Wikipedia/","title":"English Wikipedia","text":""},{"location":"Ensemble%20Distillation/","title":"Ensemble Distillation","text":"<ul> <li>Distilling the Knowledge in a Neural Network</li> <li>training many different models on the same data and then to average their predictions</li> <li>making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets</li> <li>compress the knowledge in an ensemble into a single model</li> <li>MNIST</li> <li>ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse</li> <li>specialist models can be trained rapidly and in parallel</li> <li>distillation works remarkably well even when the transfer set that is used to train the distilled model lacks any examples of one or more of the classes</li> <li>performance of a single really big net that has been trained for a very long time can be significantly improved by learning a large number of specialist nets, each of which learns to discriminate between the classes in a highly confusable cluster.</li> </ul>"},{"location":"Ensemble%20Distillation/#backlinks","title":"Backlinks","text":"<ul> <li>Stoch Ensemble Learning</li> <li>Ensemble Distillation</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Ensemble%20of%20Shape%20Functions/","title":"Ensemble of Shape Functions","text":"<ul> <li>ESF</li> <li></li> </ul>"},{"location":"Ensemble%20of%20Shape%20Functions/#backlinks","title":"Backlinks","text":"<ul> <li>Stoch Ensemble Learning</li> <li> <p>Ensemble of Shape Functions</p> </li> <li> <p></p> </li> <li>Ensemble of Shape Functions</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Entities%20involving%20in%20actions/","title":"Entities Involving in Actions","text":"<ul> <li>Her house was broken into last week.</li> <li>They took the TV and the stereo.</li> <li>The pronoun \u2018they\u2019 should be recognized as referring to the burglars who broke into the house.</li> </ul>"},{"location":"Entourage%20Plot/","title":"Entourage Plot","text":""},{"location":"Entropy%20minimization%20by%20adverarial%20learning/","title":"Entropy Minimization by Adverarial Learning","text":"<ul> <li>A limitation of the entropy loss is related to the absence of structural dependencies between local semantics.</li> <li>This is caused by the aggregation of the pixel-wise prediction entropies by summation.</li> <li>unified adversarial training framework which minimizes indirectly the entropy of target data, by encouraging it to become similar to the source one.</li> <li>minimizing distribution distance between source and target on the weighted self-information space</li> <li>We perform the adversarial adaptation on weighted self-information maps using a fully-convolutional discriminator network</li> <li>the discriminator produces domain classification outputs, i.e., class label for the source (resp. target) domain.</li> <li>discriminate outputs coming from source and target images, and at the same time, train the segmentation network to fool the discriminator.</li> </ul>"},{"location":"Entropy/","title":"Entropy","text":"<ul> <li>Measure of information content</li> <li> \\[H = -\\Sigma_{x}P(x)logP(x) = \\Sigma_{x}P(x)log \\frac{1}{P(x)}\\] </li> <li>Units : bits of \\(log_{2}\\)</li> <li>Uniform Distribution maximizes entropy. Results harder to predict</li> </ul>"},{"location":"Ependymal%20Cell/","title":"Ependymal Cell","text":"<ul> <li>Line cavities</li> <li>Create, secrete and circulate Cerebrospinal Fluid (CSF)).md)</li> </ul>"},{"location":"Epigenetics/","title":"Epigenetics","text":"<ul> <li>A subset of genetics that focuses on how specific environmental factors can influence where, when, and how a gene is expressed, resulting in variation in the gene\u2019s related traits.</li> </ul>"},{"location":"Epilepsy/","title":"Epilepsy","text":"<ul> <li>A neurological disorder characterized by abnormal electrical activity in the brain, leading to seizures.</li> </ul>"},{"location":"Epistemic/","title":"Epistemic","text":"<ul> <li>Uncertainty produced by the model</li> <li>Class imbalance etc</li> <li>Reduce by adding more info</li> <li></li> </ul>"},{"location":"Equal%20And%20Opposite%20Force%20Pairs/","title":"Equal And Opposite Force Pairs","text":"<ul> <li>\"When one body exerts a force on a second body, the second body simultaneously exerts a force equal in magnitude and opposite in direction on the first body.\"</li> <li> \\[F_{1}= -F_{2}\\] </li> </ul>"},{"location":"Equality%20of%20Opportunity/","title":"Equality of Opportunity","text":"<ul> <li>fairness</li> <li>A fairness metric that checks whether, for a preferred label (one that confers an advantage or benefit to a person) and a given attribute, a classifier predicts that preferred label equally well for all values of that attribute. In other words, equality of opportunity measures whether the people who should qualify for an opportunity are equally likely to do so regardless of their group membership.</li> <li>For example, suppose Glubbdubdrib University admits both Lilliputians and Brobdingnagians to a rigorous mathematics program. Lilliputians\u2019 secondary schools offer a robust curriculum of math classes, and the vast majority of students are qualified for the university program. Brobdingnagians\u2019 secondary schools don\u2019t offer math classes at all, and as a result, far fewer of their students are qualified. Equality of opportunity is satisfied for the preferred label of \"admitted\" with respect to nationality (Lilliputian or Brobdingnagian) if qualified students are equally likely to be admitted irrespective of whether they're a Lilliputian or a Brobdingnagian.</li> <li>For example, let's say 100 Lilliputians and 100 Brobdingnagians apply to Glubbdubdrib University, and admissions decisions are made as follows</li> </ul>"},{"location":"Equalized%20Odds/","title":"Equalized Odds","text":"<ul> <li>A fairness metric that checks if, for any particular label and attribute, a classifier predicts that label equally well for all values of that attribute.</li> <li>For example, suppose Glubbdubdrib University admits both Lilliputians and Brobdingnagians to a rigorous mathematics program. Lilliputians' secondary schools offer a robust curriculum of math classes, and the vast majority of students are qualified for the university program. Brobdingnagians' secondary schools don\u2019t offer math classes at all, and as a result, far fewer of their students are qualified. Equalized odds is satisfied provided that no matter whether an applicant is a Lilliputian or a Brobdingnagian, if they are qualified, they are equally as likely to get admitted to the program, and if they are not qualified, they are equally as likely to get rejected.</li> </ul>"},{"location":"Equations%20of%20motion/","title":"Equations of Motion","text":"<ul> <li>\\(\\Delta x\\) is displacement</li> <li>$\\Delta t $ is time</li> <li>v is final velocity</li> <li>u is initial velocity</li> <li>a is acceleration</li> <li> \\[v = u + a\\Delta t\\] </li> <li> \\[\\Delta x = u \\Delta t + \\frac{1}{2}a \\Delta t^{2}\\] </li> <li> \\[\\Delta x = \\frac{1}{2}(v+u) \\Delta t\\] </li> <li> \\[v^{2}= u^{2}+2a \\Delta x\\] </li> </ul>"},{"location":"Equivalent%20Current%20Dipole/","title":"Equivalent Current Dipole","text":"<ul> <li>Generates an electric field</li> <li></li> <li>Perpendicular is Magnetic field - MEG</li> </ul>"},{"location":"Ergodic/","title":"Ergodic","text":"<ul> <li>If only one Invariant Distribution</li> <li>Sequence of distributions \\(g^{(n)}\\) converges to g from any initial distribution</li> <li>Asymptotic, stationary, equilibrium distribution</li> </ul>"},{"location":"Ethical%20dilemmas/","title":"Ethical Dilemmas","text":"<ul> <li>situations in which any available choice leads to infringing some accepted ethical principle and yet a decision has to be made</li> </ul>"},{"location":"Ethical%20dilemmas/#backlinks","title":"Backlinks","text":"<ul> <li>Building Ethics into Artificial Intelligence</li> <li>Ethical dilemmas</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Euclidean%20Distance/","title":"Euclidean Distance","text":"<ul> <li> \\[d = \\sqrt{\\Sigma_{i=1}^{n}(p_{i}-q_{i})^{2}}\\] </li> <li>It is a distance measure that best can be explained as the length of a segment connecting two points.</li> <li>calculated from the cartesian coordinates of the points using the Pythagorean theorem</li> <li>Euclidean distance is not scale in-variant which means that distances computed might be skewed depending on the units of the features. Typically, one needs to\u00a0normalize\u00a0the data before using this distance measure.</li> <li>Moreover, as the dimensionality increases of your data, the less useful Euclidean distance becomes. This has to do with the curse of dimensionality</li> <li>works great when you have low-dimensional data and the magnitude of the vectors is important to be measured</li> </ul>"},{"location":"Eugenics/","title":"Eugenics","text":"<ul> <li>A 19th century scientific theory that advocated for selective mating of people with desirable hereditary traits.</li> </ul>"},{"location":"Euler%20Integration/","title":"Euler Integration","text":"<ul> <li> \\[\\frac{dx}{dt} = f(x,t), x(t_{0}) = x_{0}\\] </li> <li>First order integration</li> <li>Midpoint Method</li> <li>Runge Kutta</li> </ul>"},{"location":"Eulerian%20Grid/","title":"Eulerian Grid","text":"<ul> <li>Focus on domain</li> <li>Properties given on a grid  </li> <li>(Position of particles is implicit)</li> <li></li> </ul>"},{"location":"Europarl-ST/","title":"Europarl-ST","text":"<ul> <li>multilingual speech translation corpus</li> <li>based on speeches and debates in the European parliament between 2008-2013</li> <li>Creative Common Non-Commercial license</li> <li>data belongs to the European Union</li> <li>by releasing, authors want to improve speech translation</li> <li>consists of audio, transcript and translation of the transcript</li> <li>72 translation directions</li> <li>includes also noisy samples</li> </ul>"},{"location":"Even%20angels%20need%20the%20rules%20AI%2C%20roboethics%2C%20and%20the%20law/","title":"Even Angels Need the Rules AI, Roboethics, and the Law","text":"<ul> <li>Ugo Pagallo</li> <li>Collective Ethical Decision Frameworks</li> </ul>"},{"location":"Even%20angels%20need%20the%20rules%20AI%2C%20roboethics%2C%20and%20the%20law/#backlinks","title":"Backlinks","text":"<ul> <li>Building Ethics into Artificial Intelligence</li> <li>Even angels need the rules AI, roboethics, and the law</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Evidence%20For%20Distributivity%20Effects%20in%20Comprehension/","title":"Evidence For Distributivity Effects in Comprehension","text":"<ul> <li>Nikole D. Patson and Tessa Warren</li> </ul>"},{"location":"Evidence%20For%20Distributivity%20Effects%20in%20Comprehension/#intro","title":"Intro","text":"<ul> <li>In the current paper, we introduce a new methodology for detecting whether a word in a sentence is conceptually represented as plural and use it to shed light on a debate about whether comprehenders interpret singular indefinite noun phrases within a distributed predicate as plural during on-line reading.</li> <li>self-paced reading on a sentence presented in one- and two-word chunk</li> <li>indicated that participants were slower to judge that one word was on the screen when the word was plural (e.g., cats) than when it was singular (e.g., cat)</li> <li>build different conceptual representations for distributed versus collective predicates, and interpret a singular indefinite noun phrase within a distributed predicate as plura</li> </ul>"},{"location":"Evidence%20For%20Distributivity%20Effects%20in%20Comprehension/#results","title":"Results","text":"<ul> <li>When the group was conceptually distributed (5a), participants incorrectly (in American English) used a plural verb (were) more often in their continuations than when the group was conceptually collective (5b). This indicates that participants were more likely to treat the gang as a plural when its individuals were more salient (5a) rather than when the group was the relevant referent (5b). This suggests that distributivity can make grammatically singular lexical items that have plural referents (e.g., gang, group) functionally plural during language production.</li> <li>This production and off-line comprehension work suggests that readers build different conceptual representations for collective and distributed predicates, and is consistent with the hypothesis that singular indefinite noun phrases within distributed predicates are often treated as conceptually plural.</li> </ul>"},{"location":"Evidence%20For%20Distributivity%20Effects%20in%20Comprehension/#experiment-1","title":"Experiment 1","text":"<ul> <li>Experiment 1 was conducted to test whether the Berent et al. (2005) methodology could be extended to sentences. It is possible that the added complexity involved in building and maintaining a sentence representation during the number judgment task, or the task demands of simultaneously carrying out self-paced reading and number judgments, might make participants less sensitive to interference than they were in Berent et al. (2005). Experiment 1 is also important because in order to use the paradigm to test ambiguous cases (as in Experiment 2), we must first establish that the paradigm works on simple, unambiguous sentences.</li> <li>The critical measure was the reaction time for the number judgment for correct number judgment trials only. There was a significant main effect of noun type such that 'one' responses were slower when the target word was plural</li> <li>Experiment 1 confirmed that even in sentential contexts, semantic plural information on a word interferes with singular number judgments.</li> <li>Specifically, if the distributing quantifier takes wide scope over the indefinite, and comprehenders build conceptual representations for distributed predicates that contain multiple exemplars of the referent introduced by the singular indefinite noun phrase, then one-word judgment times should be slower for indefinite noun phrases in distributed predicates than collective predicates.</li> <li>The experiment had a 2 \u00d7 2 within-participants design. The first factor was the quantifier type and was either distributed (a) or collective (b)</li> <li>They were asked to rate on a scale of 1 \u2013 5 (where 1 was 'definitely one' and 5 was 'definitely more than one') whether the last word in the sentence referred to one or more than one object. Results indicated that the singular-marked distributed items were indeed biased toward a plural interpretation of the noun phrase. Participants rated the singular-marked distributed items as being closer to the 'definitely more than one' end of the scale</li> <li>than the singular-marked collective items</li> </ul>"},{"location":"Evidence%20For%20Distributivity%20Effects%20in%20Comprehension/#experiment-2","title":"Experiment 2","text":"<ul> <li>The results of Experiment 2 confirm the hypothesis that singular indefinite noun phrases in distributed predicates can indeed be treated as conceptually plural during reading</li> <li>There was no reliable effect of distributivity and no reliable difference between the plural-marked conditions, indicating that the difference in the singular-marked conditions was unlikely to be the result of one kind of predicate being more costly to compute than the other. These results indicate that in these items the distributing quantifier took wide scope over the indefinite</li> <li>indicate that conceptual plurality interferes with number judgments during sentence comprehension</li> <li>These findings (Filik et al., 2004; Paterson et al., 2008) indicate that comprehenders do not build conceptually plural referents on-line for indefinite noun phrases in distributed structures that off-line norming had indicated were likely to be interpreted as plural.</li> </ul>"},{"location":"Explanator/","title":"Explanator","text":"<ul> <li>synonym for an explaining system or explaining process that gives answers to questions in understandable terms, which could, computationally, be considered a program execution trace.</li> <li>or instance, if the question is how a machine is working, the explainer makes the internal structure of a machine more transparent to humans</li> </ul>"},{"location":"Exponential%20Distribution/","title":"Exponential Distribution","text":"<ul> <li>How long you have to wait for something after the it has happened once already</li> <li>Average rate /unit reference time</li> <li>PDF \\(\\(p(x) = \\lambda e^{-\\lambda x}\\)\\) and \\(x \\geq 0\\)</li> <li>Expectation \\(\\(E(X) = \\frac{1}{\\lambda}\\)\\)</li> <li></li> <li>Rate : \\(\\(\\hat \\lambda = \\frac{1}{N-1}\\Sigma_{i = 1, \u2026, N}t_{i+1}-t_{i}\\)\\)</li> <li>Spiking Networks</li> </ul>"},{"location":"Extensions%20to%20SlimStampen/","title":"Extensions to SlimStampen","text":"<ul> <li>Konteksti<ul> <li>Semantic similarity</li> </ul> </li> <li>Increase activation of similar facts</li> <li>Dual-lingo<ul> <li>Word + Picture based cues</li> </ul> </li> <li>Type of info<ul> <li>Eg vocabulary</li> </ul> </li> <li>Perfect pitch<ul> <li>auditory stimuli</li> </ul> </li> <li>Fun with flags<ul> <li>Improve scheduling by computing a continous stimuli</li> </ul> </li> <li>CramDroid<ul> <li>App that helps with between-sessions</li> <li>Track decay functions and send notif</li> </ul> </li> <li>Space Times<ul> <li>Game based memorization of tables</li> </ul> </li> <li>Vocab Warrior<ul> <li>play against an ACT-R</li> <li>answer faster</li> </ul> </li> </ul>"},{"location":"Extra-position/","title":"Extra-position","text":"<ul> <li>Did anyone who you expected to help actually help?</li> <li>Did anyone actually help who you expected to help?</li> </ul>"},{"location":"Eye%20Tracking/","title":"Eye Tracking","text":"<ul> <li>Gaze position</li> </ul>"},{"location":"Eye-to-hand%20System/","title":"Eye-to-hand System","text":""},{"location":"Eye-to-hand%20System/#eye-to-hand-system","title":"Eye-to-hand System","text":"<ul> <li>The task in visual servoing is to use visual information to control the robot's endeffector relative to a target object.</li> <li>provide feedback to the robot controller at each time step</li> <li></li> </ul>"},{"location":"Eye-to-hand%20System/#backlinks","title":"Backlinks","text":"<ul> <li>Eye-to-hand System</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"FGSM/","title":"FGSM","text":"<ul> <li>FGSM is a method of generating noise in the direction of the cost function gradient concerning the data</li> <li>Given original input image x, label y, model parameter \u03b8, and loss J.  </li> <li> \\[adv_{x}= x+ \\epsilon \\ast sign(\\nabla_{x}(J(\\theta, x, y)))\\] </li> <li>this gives us the perturbations</li> </ul>"},{"location":"FGVC%20Aircraft/","title":"FGVC Aircraft","text":"<ul> <li>This dataset contains images of 100 different types of aircrafts, with a total of 10,000 images.</li> </ul>"},{"location":"FGVC%20Aircraft/#backlinks","title":"Backlinks","text":"<ul> <li>Fine Grained Datasets</li> <li>FGVC Aircraft</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"FGVCx/","title":"FGVCx","text":"<ul> <li>FGVCx is a dataset that includes a number of fine-grained datasets, such as the FGVC Aircraft, Stanford Cars and Stanford Dogs datasets.</li> </ul>"},{"location":"FGVCx/#backlinks","title":"Backlinks","text":"<ul> <li>Fine Grained Datasets</li> <li>FGVCx</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"FLASH/","title":"FLASH","text":"<ul> <li>Transformer Quality in Linear Time</li> <li>weaknesses in handling long sequences</li> <li>FLASH</li> <li>performant layer (gated linear unit) and by combining it with an accelerator-efficient approximation strategy (mixed chunk attention)</li> <li>GAU</li> <li>Mixed chunk attention</li> <li>outperforms three baselines: vanilla Transformer, Performer and Combiner in terms of quality and efficiency</li> <li>Wiki</li> <li>PG-19</li> </ul>"},{"location":"FLAVA/","title":"FLAVA","text":"<ul> <li>FLAVA: a Foundational Language and Vision Alignment Model</li> <li>foundational vision and language alignment model that performs well on all three target modalities: 1) vision, 2) language, and 3) vision &amp; language</li> <li>use a single holistic universal model, as a \u201cfoundation\u201d, that targets all modalities at once</li> <li>wide range of 35 tasks spanning these target modalities</li> </ul>"},{"location":"FP16%20training/","title":"FP16 Training","text":"<ul> <li>Reduced precision has a narrower range that might make the results more out of range and worsen the training progress</li> <li>Can store all parameters and activations in FP16 and then use that for gradients.</li> <li>Also copy to FP32 for parameter updates</li> <li>Multiply scalar to loss to align range of FP16</li> </ul>"},{"location":"FTSwish/","title":"FTSwish","text":"<ul> <li>Relu + Sigmoid</li> <li> \\[\\begin{equation} FTSwish: f(x) = \\begin{cases} T, &amp; \\text{if}\\ x &lt; 0 \\\\ \\frac{x}{1 + e^{-x}} + T, &amp; \\text{otherwise} \\\\ \\end{cases} \\end{equation}\\] </li> <li>As we can see, the sparsity principle is still true - the neurons that produce negative values are taken out.</li> <li>What we also see is that the derivative of FTSwish is smooth, which is what made Swish theoretically better than ReLU in terms of the loss landscape</li> <li>However, what I must note is that this function does not protect us from the dying ReLU problem: the gradients for \\(x &lt; 0\\) are zero, as with ReLU.</li> </ul>"},{"location":"FaceNet/","title":"FaceNet","text":"<ul> <li>FaceNet: a Unified Embedding for Face Recognition and Clustering</li> <li>mapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity</li> <li>Optimize the embedding itself</li> <li>FaceNet directly trains its output to be a compact 128-D embedding using a Triplet Loss function</li> <li>Choosing which triplets to use turns out to be very important for achieving good performance<ul> <li>inspired by curriculum learning</li> <li>online negative exemplar mining strategy which ensures consistently increasing difficulty of triplets as the network trains</li> <li>also explore hard-positive mining techniques which encourage spherical clusters for the embeddings of a single person</li> </ul> </li> <li>squared Lp Regularization L2 distance, in the embedding space directly correspond to face similarity: faces of the same person have small distances and faces of distinct people have large distances</li> <li>face verification simply involves thresholding the distance between the two embeddings; recognition becomes a KNN classification problem</li> <li>Labeled Faces in the Wild</li> <li>Zeiler Fergus</li> <li>Inception</li> <li>Harmonic Embedding</li> </ul>"},{"location":"Factorized%20Embedding%20Parameters/","title":"Factorized Embedding Parameters","text":"<ul> <li>Factorization of these parameters is achieved by taking the matrix representing the weights of the word embeddings \\(E\\) and decomposing it into two different matrices. Instead of projecting the one-hot encoded vectors directly onto the hidden space, they are first projected on some-kind of lower-dimensional embedding space, which is then projected to the hidden space (Lan et al, 2019). Normally, this should not produce a different result, but let's wait.</li> <li>Another thing that actually ensures that this change reduces the number of parameters is that the authors suggest to reduce the size of the embedding matrix.</li> <li>In BERT, the shape of the vocabulary/embedding matrix E equals that of the matrix for the hidden state H.</li> <li>First of all, theoretically, the matrix E captures context-independent information</li> <li>whereas the hidden representation H captures context-dependent information</li> <li>ALBERT solves this issue by decomposing the embedding parameters into two smaller matrices, allowing a two-step mapping between the original word vectors and the space of the hidden state. In terms of computational cost, this no longer means \\(\\text{O(VxH)}\\) but rather \\(\\text{O(VxE + ExH)}\\), which brings a significant reduction when \\(\\text{H &gt;&gt; E}\\).</li> </ul>"},{"location":"Factors%20for%20MC%20estimate/","title":"Factors for MC Estimate","text":"<ul> <li>Amount of computation required to simulate transition kernel</li> <li>Time for chain to converge to equilibrium -&gt; no of states that must be discarded</li> <li>No of transitions needed to move from one state in the equilibrium to another that is independant<ul> <li>Redundant information</li> <li>First value will depend to a decreasing degree on the distance from this timestep to the previous ones. Then washout (because old ones are too far away)</li> </ul> </li> </ul>"},{"location":"Fairness%20Constraint/","title":"Fairness Constraint","text":"<ul> <li>Applying a constraint to an algorithm to ensure one or more definitions of fairness are satisfied. Examples of fairness constraints include</li> </ul>"},{"location":"Familar%20Object%20Grasping%20Object%20Viiew%20recog/","title":"Familar Object Grasping Object Viiew Recog","text":"<ul> <li>Shafii, Nima, S. Hamidreza Kasaei, and Lui\u0301s Seabra Lopes. \"Learning to grasp familiar objects using object view recognition and template matching.\" IROS 2016.</li> <li>Grasp template :</li> <li>Local shape feature for graspable regions<ul> <li>spin-image feature</li> </ul> </li> <li>Global feature, the radius (distance from the CoM to the selected keypoint)<ul> <li>important to represent a stable grasp</li> </ul> </li> <li>Finger configuration</li> <li>New objects that are similar to known ones (i.e. they are familiar) can be grasped in a similar way<ul> <li>As an example, if the robot knows how to grasp a pen, it may use the same grasp temple to take a marker</li> </ul> </li> </ul>"},{"location":"Fashion%20MNIST/","title":"Fashion MNIST","text":"<ul> <li>https://github.com/zalandoresearch/fashion-mnist</li> <li>alternative to\u00a0MNIST<ul> <li>60'000 train images</li> <li>10'000 test images</li> <li>28x28x1 grayscale</li> <li>10 classes</li> </ul> </li> <li>bit more challenging than\u00a0MNIST</li> </ul>"},{"location":"FastText/","title":"FastText","text":"<pre><code>def FastTextNew(vocab_size, embedding_dim, output_dim):\n    return nn.Sequential(\n        Rearrange('t b -&gt; t b'),\n        nn.Embedding(vocab_size, embedding_dim),\n        Reduce('t b c -&gt; b c', 'mean'),\n        nn.Linear(embedding_dim, output_dim),\n        Rearrange('b c -&gt; b c'),\n    )\n</code></pre>"},{"location":"Fastai%20Blocks/","title":"Fastai Blocks","text":""},{"location":"Fastai%20Blocks/#building-blocks","title":"Building Blocks","text":"<pre><code># Image Classification\ndls = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=[Resize(192, method='squish')]\n).dataloaders(path)\n\ndls.show_batch(max_n=6)\nlearn = cnn_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(8)\n</code></pre> <pre><code># Label regex\npets = DataBlock(blocks = (ImageBlock, CategoryBlock),\n                 get_items=get_image_files, \n                 splitter=RandomSplitter(seed=42),\n                 get_y=using_attr(RegexLabeller(r'(.+)_\\d+.jpg$'), 'name'),\n                 item_tfms=Resize(460),\n                 batch_tfms=aug_transforms(size=224, min_scale=0.75))\ndls = pets.dataloaders(path/\"images\")\n</code></pre> <ul> <li>DataBlock is more general<ul> <li>list models</li> </ul> </li> </ul> <pre><code>timm.list_models('convnex*')\n</code></pre> <pre><code># Segmentation\npath = untar_data(URLs.CAMVID_TINY)\ndls = SegmentationDataLoaders.from_label_func(\n    path, bs=8, fnames = get_image_files(path/\"images\"),\n    label_func = lambda o: path/'labels'/f'{o.stem}_P{o.suffix}',\n    codes = np.loadtxt(path/'codes.txt', dtype=str)\n)\nlearn = unet_learner(dls, resnet34)\nlearn.fine_tune(8)\n</code></pre> <ul> <li>Segmentation Dataloaders is just another abstraction of the DataBlock for a specific case. Can use the DataBlock as well</li> </ul> <pre><code># Tabular\ndls = TabularDataLoaders.from_csv(path/'adult.csv', path=path, y_names=\"salary\",\n    cat_names = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race'],\n    cont_names = ['age', 'fnlwgt', 'education-num'],\n    procs = [Categorify, FillMissing, Normalize])\nlearn = tabular_learner(dls, metrics=accuracy)\nlearn.fit_one_cycle(2)\n</code></pre> <ul> <li>Fitting because pretrained models are not going to be there</li> </ul> <pre><code># Collaborative [Filtering](./Filtering.md)\ndls = CollabDataLoaders.from_csv(path/'ratings.csv')\nlearn = collab_learner(dls, y_range=(0.5,5.5))\nlearn.fine_tune(8)\n</code></pre> <ul> <li>Range is for the output (Since its not a binary output)</li> <li>Saving a model</li> </ul> <pre><code>learn.export('model.pkl')\n</code></pre>"},{"location":"Fastai%20Deployment/","title":"Fastai Deployment","text":"<ul> <li>Gradio</li> </ul>"},{"location":"Fastai%20Deployment/#save","title":"Save","text":"<pre><code>from fastai.vision.widgets import *\n</code></pre> <pre><code>path = Path()\npath.ls(file_exts='.pkl')\n\nlearn_inf = load_learner(path/'export.pkl')\nlearn_inf.predict('images/grizzly.jpg')\nlearn_inf.dls.vocab\n</code></pre>"},{"location":"Fastai%20Interpretation/","title":"Fastai Interpretation","text":""},{"location":"Fastai%20Interpretation/#classification-interpretation","title":"Classification Interpretation","text":"<pre><code>interp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\ninterp.plot_top_losses(5, nrows=1)\n</code></pre> <ul> <li>Ordered by loss</li> <li>If predicted correctly but still shown, then low confidence</li> </ul>"},{"location":"Fastai%20Interpretation/#cleaner","title":"Cleaner","text":"<pre><code>cleaner = ImageClassifierCleaner(learn)\ncleaner\n\nfor idx in cleaner.delete(): cleaner.fns[idx].unlink()\nfor idx,cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat)\n</code></pre>"},{"location":"Fastai%20Interpretation/#get-all-classes-and-their-probabilities","title":"Get All Classes and Their Probabilities","text":"<pre><code>def classify_image(img):\n    pred,idx,probs = learn.predict(img)\n\nreturn dict(zip(categories, map(float,probs)))\n\nclassify_image(im)\n</code></pre>"},{"location":"Fastai%20Tricks/","title":"Fasai Tricks","text":""},{"location":"Fastai%20Tricks/#batched-map","title":"Batched Map","text":"<pre><code>tok_ds = ds.map(tok_func, batched=True)\n</code></pre>"},{"location":"Fastai%20Tricks/#learning-rate-finder","title":"Learning Rate Finder","text":"<pre><code>learn.lr_find(suggest_funcs=(slide, valley))\n</code></pre>"},{"location":"Fastai%20Tricks/#test-dataset","title":"Test Dataset","text":"<pre><code>tst_dl = learn.dls.test_dl(tst_df)\npreds,_ = learn.get_preds(dl=tst_dl)\n</code></pre>"},{"location":"Fastai%20Tricks/#ensemble","title":"Ensemble","text":"<pre><code>def ensemble():\n    learn = tabular_learner(dls, metrics=accuracy, layers=[10,10])\n    with learn.no_bar(),learn.no_logging(): learn.fit(16, lr=0.03)\n    return learn.get_preds(dl=tst_dl)[0]\nlearns = [ensemble() for _ in range(5)]\n\nens_preds = torch.stack(learns).mean(0) # stack and mean\n</code></pre>"},{"location":"Faster%20RCNN/","title":"Faster RCNN","text":"<ul> <li>Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</li> <li>SPNet</li> <li>Fast-RCNN + Region Proposal</li> <li>Attention is also used</li> <li>Vgg</li> <li>PASCAL VOC, ILSVRC, COCO</li> </ul>"},{"location":"Feature%20Based%20Knowledge/","title":"Feature Based Knowledge","text":"<ul> <li>Huang and Wang (2017) using neuron selectivity trans- fer. Passalis and Tefas (2018) transferred knowledge by matching the probability distribution in feature space.</li> <li>Kim et al. (2018) introduced so called \u201cfactors\u201d as a more understandable form of intermediate repre- sentations. To reduce the performance gap between teacher and student, Jin et al. (2019) proposed route constrained hint learning, which supervises student by outputs of hint layers of teacher. Recently, Heo et al. (2019c) proposed to use the activation boundary of the hidden neurons for knowledge transfer. Interestingly, the parameter sharing of intermediate layers of the teacher model together with response-based knowledge is also used as the teacher knowledge (Zhou et al., 2018).</li> <li>To match the semantics between teacher and stu- dent, Chen et al. (2021) proposed cross-layer knowledge distillation, which adaptively assigns proper teacher layers for each student layer via attention allocation.</li> <li>Though feature-based knowledge transfer provides favorable information for the learning of the student model, how to effectively choose the hint layers from the teacher model and the guided layers from the student model remains to be further investigated (Romero et al., 2015).</li> <li>Distillation Schemes</li> <li>Teacher Student Architecture</li> <li>Distillation Algorithms</li> <li>Applications of Knowledge Distillation</li> <li>For example, on one hand, some recent works find that the student model can learn little from some teacher models due to the model capac- ity gap between the teacher model and the student model (Zhang et al., 2019b; Kang et al., 2020); On the other hand, from some early theoretical analysis on the capacity of neural networks, shallow networks are capable of learning the same representation as deep neural networks (Ba and Caruana, 2014).</li> </ul>"},{"location":"Feature%20Correlationa/","title":"Feature Correlationa","text":"<ul> <li>If certain features in a dataset have a high correlation in a dataset, it becomes difficult to control specific features without changing the closely correlated ones.</li> <li>For example, let's say you have a dataset of \u00a0face images and want to add facial hair to an image of a woman, it's likely that you'll end up modifying more features as this feature is highly correlated with a male's face.</li> </ul>"},{"location":"Feature%20Correlationa/#backlinks","title":"Backlinks","text":"<ul> <li>Conditional GAN</li> <li>Feature Correlationa</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Feature%20Learning/","title":"Feature Learning","text":"<ul> <li>Dictionary Learning</li> <li>Methods for Feature Learning</li> <li>Contrastive Loss</li> <li>Max Margin Loss</li> <li>Triplet Loss</li> </ul>"},{"location":"Feature%20Space%20Augmentation/","title":"Feature Space Augmentation","text":"<ul> <li>The sequential processing of neural networks can be manipulated such that the intermediate representations can be separated from the network as a whole. The lower-dimensional representations of image data in fully-connected layers can be extracted and isolated.</li> <li>DeVries and Taylor tested their feature space augmentation technique by extrapolating between the 3 nearest neighbors per sample to generate new data and compared their results against extrapolating in the input space and using affine transformations in the input space</li> <li>Vector representations are then found by training a CNN and then passing the training set through the truncated CNN. These vector representations can be used to train any machine learning model from Naive Bayes, Support Vector Machine, or back to a fully-connected multilayer network.</li> <li>A disadvantage of feature space augmentation is that it is very difficult to interpret the vector data.</li> </ul>"},{"location":"Feature%20Spec/","title":"Feature Spec","text":"<ul> <li>Describes the information required to extract features data from the tf.Example protocol buffer. Because the tf.Example protocol buffer is just a container for data, you must specify the following<ul> <li>the data to extract (that is, the keys for the features)</li> <li>the data type (for example, float or int)</li> <li>The length (fixed or variable)</li> </ul> </li> </ul>"},{"location":"Features/","title":"Features","text":""},{"location":"Features/#dimensions","title":"Dimensions","text":""},{"location":"Features/#wide","title":"Wide","text":"<ul> <li>Had to train</li> <li>More number of neurons</li> <li>Easy parallel</li> <li>Infinitely wide -&gt; Gaussian process</li> </ul>"},{"location":"Features/#deep","title":"Deep","text":"<ul> <li>Easier to train</li> <li>Less data</li> <li>Linear amount</li> <li>Difficult to parallelize</li> </ul>"},{"location":"Features/#why","title":"Why","text":"<ul> <li>Domain Adaptation</li> <li>Structure exploitation</li> <li>Relevant features</li> </ul>"},{"location":"Features/#random-things","title":"Random Things","text":"<ul> <li>1 hidden layer Perceptron -&gt; Universal fn estimator</li> <li>Best generalization -&gt; First order optimization</li> </ul>"},{"location":"Federated%20Learning/","title":"Federated Learning","text":"<ul> <li>Basics of Federated Learning</li> <li>Advantages of Federated Learning</li> <li>Federated Updates</li> </ul>"},{"location":"Federated%20Learning/#refs","title":"Refs","text":"<ul> <li>OpenMined Blog</li> <li>Nvidia</li> <li>Unite.ai</li> <li>Google blog</li> <li>Wiki</li> <li>Digital health : Rieke, N., Hancox, J., Li, W. et al. The future of digital health with federated learning. npj Digit. Med. 3, 119 (2020). https://doi.org/10.1038/s41746-020-00323-1</li> <li>Gboard : Chen, M., Mathews, R., Ouyang, T., &amp; Beaufays, F. (2019). Federated learning of out-of-vocabulary words. arXiv preprint arXiv:1903.10635. Paper</li> <li>Chen, M., Mathews, R., Ouyang, T., &amp; Beaufays, F. (2019). Federated learning of out-of-vocabulary words. arXiv preprint arXiv:1903.10635.</li> </ul>"},{"location":"Federated%20Updates/","title":"Federated Updates","text":"<ul> <li>Structured Update</li> <li>Sketched Update</li> </ul>"},{"location":"Feedback%20Loop/","title":"Feedback Loop","text":"<ul> <li>In machine learning, a situation in which a model's predictions influence the training data for the same model or another model. For example, a model that recommends movies will influence the movies that people see, which will then influence subsequent movie recommendation models.</li> </ul>"},{"location":"Few%20Shot%20Order%20Sensitivity/","title":"Few Shot Order Sensitivity","text":"<ul> <li>Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity</li> <li>When primed with only a handful of training samples, very large, pretrained language models such as GPT-3 have shown competitive results when compared to fully-supervised, fine-tuned, large, pretrained language models</li> <li>few-shot prompts suffer from order sensitivity</li> <li>for the same prompt the order in which samples are provided can make the difference between state-of-the-art and random performance \u2013 essentially some permutations are \u201cfantastic\u201d and some not</li> <li>problem is prevalent across tasks, model sizes (even for the largest current models), prompt templates, it is not related to a specific subset of samples, number of training samples, and that a given good permutation for one model is not transferable to another.</li> <li>novel probing method that exploits the generative nature of language models to construct an artificial development set</li> <li>identity performant permutations for prompts using entropy-based statistics over this set, which yields a 13% relative improvement for GPT-family models across eleven different established text classification tasks</li> </ul>"},{"location":"Filter%20Bubble%20Problem/","title":"Filter Bubble Problem","text":"<ul> <li>Shows only things that you have seen before</li> </ul>"},{"location":"Filtering/","title":"Filtering","text":"<ul> <li>Noise Suppression</li> </ul>"},{"location":"Final%20Paper%20Language%20Modeling/","title":"Final Paper LM","text":""},{"location":"Final%20Paper%20Language%20Modeling/#tips","title":"Tips","text":"<ul> <li>Identify a Research Question that links with previous research</li> <li>Identify a Method that will let you answer the research question, or at least partially</li> <li>Develop an experiment (or two) that would then test this research question</li> <li>Predict what the results will look like given current theory/theories     If your experiment tests the predictions of more than one theory then you should have one set of predictions for each theory  </li> <li>What are the consequences of certain results for our understanding of the phenomena studied?</li> <li>What should following research do given certain results?</li> <li>How will you analyze the results statistically? What methods, which tests, what does your data look like?</li> <li>So write a 2-3 page paper about an experiment that would answer an open question about quantification.</li> <li>This paper can be written as if you are proposing it (e.g. \"We would then test x children with \u2026.\" ) or you could write it as if you already did the experiment, imagining the results, e.g. \"We tested 30 Spanish speakers \u2026\".</li> <li>Give it the kind of title you would give to a paper. (Don't call it \"My Gedankenexperiment\" !;)).</li> <li>Give an introduction to the research area, summarize the results you already know, using references to relevant papers. This could be the background section. Explain what's still missing in our knowledge and how we should test it.</li> <li>In the methods section I need to know the details of the experimental design, including examples of sentences that will be tested and pictures that might be used (use clip art, or I also don't mind simple drawings. Don't worry if you are not very artistic!).</li> <li>Explain what kind of participants you will need to test, how many, and what features they need to have. Be realistic. Just because it's fantasy, you shouldn't propose testing 1000 children.</li> <li>Do you also want to do additional testing? (working memory, inhibition?) Make sure you motivate it.</li> </ul>"},{"location":"Final%20Paper%20Language%20Modeling/#idea","title":"Idea.","text":"<ul> <li>Shortcuts to Quantifier Interpretation in Children and Adults But studies and comparisons were weird. Change?</li> <li>They were salty about This feature of their design provided children with unambiguous cues as to which set of entities was the focus of attention. : Crain and Thornton (1998)</li> <li>Crain et al.\u2019s (1996) claim that preschoolers have full competence with uni- versal quantifiers would seem to be undermined by the fact that even older school-age children make errors identifyi- 04:40 ng the domain of a universal quantifier.</li> <li></li> <li>Brooks and Braine (1996, Experiment 1) tested adults with actional scenes and found no errors. Their data, however, came from 10 undergraduates at a highly selective private university (Carnegie Mellon) and thus may not be representative of adults in general.</li> <li>Test same thing as Shorts. but different data</li> <li>Pictures</li> </ul>"},{"location":"Final%20Paper%20Language%20Modeling/#abstract","title":"Abstract","text":"<ul> <li>Ostensive cues</li> <li>Locative bias</li> <li>Mouse tracking</li> </ul>"},{"location":"Final%20Paper%20Language%20Modeling/#participants","title":"Participants","text":"<ul> <li>We recruited and tested twelve 5-year-olds (M = 5;5, range = 5;2\u20135;11), twelve 6-year-olds (M = 6;6, range = 6;2\u20136;10), twelve 7-yearolds (M = 7;6, range = 7;1\u20137;11), twelve 8-year-olds (M = 8;6, range = 8;0\u20138;11), and twelve 9-year-olds (M = 9;6, range = 9;1\u20139;11), twelve adults at private elementary schools and after-school programs in Atlanta, Georgia.</li> <li>adults from RUG, kiddos from some school</li> </ul>"},{"location":"Final%20Paper%20Language%20Modeling/#procedure","title":"Procedure","text":"<ul> <li>single, 20-min session conducted in a quiet room of their school</li> <li>We showed children two pictures at a time and asked them to point to the picture that went best with a sentence read aloud</li> <li>After the child looked at both pictures, the experimenter read the corresponding sentence and asked the child to point to the picture that went best with the sentence.</li> <li>Same for adults (Contrary to this paper itself)</li> <li>the teddy thing was nice and cute too</li> </ul>"},{"location":"Final%20Paper%20Language%20Modeling/#experimental-design","title":"Experimental Design","text":"<ul> <li>16 test items</li> <li>2x2 study (Picture: Collective vs. Distributive) and Sentence: (With marker (\"each\" or \"together\") or without)</li> <li>2 practice trials</li> <li>7 controls</li> <li>3 fillers</li> </ul>"},{"location":"Final%20Paper%20Language%20Modeling/#what-do-we-expect","title":"What Do We Expect","text":"<ul> <li>Both children and adults make errors</li> <li>Only 9 y/o were consistent</li> <li>7 y/o : extra animals/objects vs containers</li> <li>Better performance :<ul> <li>Quantifier modifying the containers vs subject (Disprove Kang et al.)</li> <li>Children , Not Adults</li> </ul> </li> <li>Prefer locative scenes with all filled containers (Drozd et al.)<ul> <li>Children , Not Adults</li> </ul> </li> <li>The rate of rejection of the sentences as true descriptions of the visually represented situations significantly correlated with the type of the visual representation.</li> <li>The sentence\u2013 drawing pairs were rejected in 10.53% of the cases.</li> <li>n the case of the sentence\u2013photo pairs, the rate of rejection was a mere 3.51%.</li> <li>Just as in Pint\u00e9r's (2016) experiment, the rate of rejection (i.e., the rate of the exhaustive interpretation of the sentences) was slightly even higher in the adult control group: 13.33% in the case of sentence\u2013drawing pairs, and 8.88% in the case of sentence\u2013photo p</li> <li>Crucially, the photos contained many more extra agents and extra objects than the drawings, still they elicited significantly less rejections.</li> <li>What made the presence of extra objects in the drawings ostensive was the minimality of the drawings, suggesting that everything irrelevant had been eliminated from them.</li> <li>This suggests that the problem does not reside in the child's syntax, given the similarities in sentence structures used across studies, but in</li> <li>stead has to do with the difficulty of selecting the appropriate set of entities and avoiding distraction by salient objects.</li> <li>Taken together, the experiments suggest that it was the collective scenes as opposed to the use of all that improved children's performance in Experiment 1. Collective scenes were easier presumably because the group depiction aided the child in isolating one set of entities relative to the other</li> <li>We suspect that both children and adults make errors in comprehension because they engage in shallow processing that causes inaccurate mapping between syntactic and semantic representations.</li> <li>Shallow processing also provides a straightforward explanation of the errors made by adults in Experiment 3. Their high error rates suggest that adult listeners often do not tax their limited information-processing capacities by conducting exhaustive syntactic analyses of sentences but rather make use of simpler strategies in generating reasonable guesses</li> </ul>"},{"location":"Final%20Paper%20Language%20Modeling/#literature","title":"Literature","text":"<ul> <li>Quantifier spreading children misled by ostensive cues</li> <li>Shortcuts to Quantifier Interpretation in Children and Adults</li> <li>A matter of ambiguity? Using eye movements to examine collective vs. distributive interpretations of plural sets</li> </ul>"},{"location":"Final%20Paper%20Language%20Modeling/#experiment","title":"Experiment","text":"<ul> <li>Replace doodles with images , Use more</li> <li>Combine locative bias fix + old version of exp</li> <li>Mouse tracking</li> </ul>"},{"location":"Final%20Paper%20Language%20Modeling/#sentences","title":"Sentences","text":"<ul> <li>Each of the (people) is (verb)ing an (object), for example, Each of the men is washing a bear.</li> <li>There is a (person) (verb)ing each of the (objects), for example, There is a man washing each of the bears.</li> <li>Every (person) is (verb)ing an (object), for example, Every man is washing a bear.</li> <li>There is a (person) (verb)ing every (object), for example, There is a man washing every bear.</li> <li>All of the (people) are (verb)ing an (object), for example, All of the men are washing a bear.</li> <li>There is a (person) (verb)ing all of the (objects), for example, There is a man washing all of the bears.</li> <li>All of the (objects) are in a (container), for example, All of the alligators are in a bathtub.</li> <li>All of the (containers) have an (object) in them, for example, All of the bathtubs have an alligator in them.</li> <li>There is an (object) in all of the (containers), for example, There is an alligator in all of the bathtubs.</li> <li>Each of the (objects) is in a (container), for example, Each of the alligators is in a bathtub.</li> <li>Each of the (containers) has an (object) in it, for example, Each of the bathtubs has an alligator in it.</li> <li>There is an (object) in each of the (containers), for example, There is an alligator in each of the bathtubs.</li> <li>Every (object) is in a (container), for example, Every alligator is in a bathtub.</li> <li>Every (container) has an (object) in it, for example, Every bathtub has an alligator in it.</li> <li>There is an (object) in every (container), for example, There is an alligator in every bathtub.</li> </ul>"},{"location":"Final%20Paper%20User%20Models/","title":"Final Paper User Models","text":""},{"location":"Final%20Paper%20User%20Models/#literature","title":"Literature","text":"<ul> <li>van den Broek, G. S., Takashima, A., Segers, E., &amp; Verhoeven, L. (2018). Contextual richness and word learning: Context enhances comprehension but retrieval enhances retention. Language learning, 68(2), 546-585.</li> <li>Effects of Contextual Cues on Inferring and Remembering Meanings of New Word</li> <li>SlimStampen</li> <li>The Behavior of Tutoring Systems</li> <li>The Effect of Three Consecutive Context Sentences on EFL Vocabulary-Learning</li> <li>Second Language Vocabulary Learning , The role of context  versus translation</li> <li>Learning L2 German Vocabulary Through Reading</li> </ul>"},{"location":"Final%20Paper%20User%20Models/#members","title":"Members","text":"<ul> <li>Hila Schwartz</li> <li>Juliette Bruin</li> <li>Isabelle Tilleman</li> <li>Subhaditya Mukherjee</li> </ul>"},{"location":"Final%20Paper%20User%20Models/#concept","title":"Concept","text":"<ul> <li>Outlines the general concept of our system, meaning how we tried to adjust and improve the slimstampen system, and any additional improvements we made to the system.</li> </ul>"},{"location":"Final%20Paper%20User%20Models/#main-idea","title":"Main Idea","text":"<ul> <li>The main idea we have for this project is to improve the SlimStampen system by presenting vocabulary in context. The way we would like to do this, is by presenting users with the word they need to learn in a sentence. As the learner gets better, the system will increase the difficulty by presenting words without context. If the words are presented out of context and the user makes a mistake, the correct answer will also be shown without context.</li> <li>An example of this would be as follows:</li> <li>Prompt: Wij kopen een huis</li> <li>Correct answer: to buy OR buy OR buying</li> <li>This idea is based on research by van den Broek et al. (2018), which showed that presenting novel words in context during the initial learning phase, and then reducing context later on improves long-term word retention. Next to that, Li (1988) found that learning words in context improves understanding of the word.</li> <li>Based on previous feedback, we have decided to have two contexts for each word. If a user gets a word wrong consistently, they will be shown a different context. This is based on the idea that they perhaps do not understand the first context they were provided with, and that perhaps the second context will provide them with the information they need to get the word correct. Unlike previously stated, this addition will not be tested in a separate condition. The only conditions we will test are no contexts at all and two contexts.</li> </ul>"},{"location":"Final%20Paper%20User%20Models/#params","title":"Params","text":"<ul> <li>WORD_THRESHOLD = 0.29</li> <li>CONTEXT2_THRESHOLD = 0.35</li> <li>DEFAULT_ALPHA = 0.3</li> </ul>"},{"location":"Final%20Paper%20User%20Models/#fact","title":"Fact","text":"<pre><code>Fact(fact_id = 6,\u00a0\n\u00a0question = 'gemiddeld',\u00a0\n\u00a0context_1 = \"Deze opleiding heeft 'gemiddeld' \u2026\",\u00a0\n\u00a0context_2 = \"De temperatuur is 'gemiddeld' \u2026\",\u00a0\n\u00a0answer = 'average',\u00a0\n\u00a0chosen_context = \"Deze opleiding heeft 'gemiddeld' \u2026\",\n\u00a0encounter_2 = True)\n</code></pre>"},{"location":"Final%20Paper%20User%20Models/#additional-improvements","title":"Additional Improvements","text":"<ul> <li>We added a few things to the system of which we will not test the effects:</li> </ul>"},{"location":"Final%20Paper%20User%20Models/#1-ui-improvements","title":"1 UI Improvements","text":"<ul> <li>The UI was updated for a cleaner look and feel, as well as to improve readability for dyslexic participants.</li> </ul>"},{"location":"Final%20Paper%20User%20Models/#2-gamification","title":"2 Gamification","text":"<ul> <li>We have decided to add a few small gamification elements. One of those is that through the use of colors (green/red). Another is displaying a score which shows how many answers you got correct out of the total number of trials you have done so far.</li> </ul>"},{"location":"Final%20Paper%20User%20Models/#3-multiple-translations","title":"3 Multiple Translations","text":"<ul> <li>For some of the words, we added multiple correct answers. This means that multiple different answers can be counted as correct. This is especially useful for verbs, since there are different ways to translate those which do not really change the meaning of the words. An example of this is the word vergeten, which can be translated to forget, to forget, and forgot. Out-of-context, all three of these meanings make sense. Having multiple options be marked as correct also helped with gathering context sentences, since you are not as restricted to one specific use of the word.</li> </ul>"},{"location":"Final%20Paper%20User%20Models/#system","title":"System","text":"<ul> <li>Explains how the system itself works, meaning how it switches between words and context, and how it decided which word to show next.</li> </ul>"},{"location":"Final%20Paper%20User%20Models/#word-order","title":"Word Order","text":"<ul> <li>Similarly to the set-up of slimstampen, we let the system decide which words to show and how based on rate of forgetting. What we changed is that we have different thresholds for when it shows the context, which context it shows, and when it shows just the word.</li> <li>Once it enters the second context, it will not go back to the first. This is based on the assumption that the first context did not help the participant figure out the meaning of the word. Thresholds were decided through trial-and-error, based on what felt like a natural progression of conditions.</li> <li>In the original system, the same word (and in this case also context) was shown thrice without changing at all based on input. We changed that to twice in our system.</li> </ul>"},{"location":"Final%20Paper%20User%20Models/#testing","title":"Testing","text":"<ul> <li>Explains how we aim to test the system, including each of the conditions and the full experimental procedure.</li> </ul>"},{"location":"Final%20Paper%20User%20Models/#stimuli-and-design","title":"Stimuli and Design","text":"<ul> <li>To test whether the improvements we made to the SlimStampen system have an effect on word retrieval, we want to do a within-participant study. For each condition, we will vary whether words are presented in a sentence, and how many sentences are available. The independent variables in this study are accuracy and response time. The dependent variable is number of contexts available.</li> </ul>"},{"location":"Final%20Paper%20User%20Models/#conditions","title":"Conditions","text":"<ul> <li>We are planning to have two conditions:</li> <li>Baseline condition: In this condition, participants are only shown words without context.</li> <li>Two-context condition: In this condition, participants will be presented with words in-context as well as out of context. If participants repeatedly give the wrong answer when an item is shown in-context, they will be shown a different context.</li> </ul>"},{"location":"Final%20Paper%20User%20Models/#components","title":"Components","text":"<ul> <li>The experiment consists of different components:</li> <li>Questionnaire: This will ask the participants for some basic info: age, their native language, and their previous experience with learning Dutch.</li> <li>Level Evaluation: Participants will be shown 20 Dutch words and will be asked to translate them to English. We can use this data to compare the Dutch level of each participant, in case we see some weird results. The words were chosen by taking vocabulary from different levels of Dutch (a1-b2).</li> <li>Training: Participants will try to learn new Dutch words using our adapted SlimStampen system. This is where the conditions come into play. This segment lasts either 150 trials or until they have seen all words.</li> <li>Distractor: A small dot-counting task that functions as a distractor task. Participants need to do this 10 times between training and testing. The number of blue dots that are shown is between 10 and 20.</li> <li>Test: Participants will be tested on the words learned during training using a simple translation task. In this test, they are only presented with words that they saw during training. They immediately receive feedback on how they did.</li> <li>Break: Participants will be asked to take a small break between testing and starting the training in a new condition.</li> <li>We will present each participant with the same two word/context lists, but they will be randomized over the conditions. The order of the conditions will also be randomized.</li> </ul>"},{"location":"Final%20Paper%20User%20Models/#procedure","title":"Procedure","text":"<ul> <li>The experiment will be performed on our personal laptops using OpenSesame in a quiet place. Participants will be presented with a small introduction to the experiment. Then, they will answer a small questionnaire about their Dutch level. After that, they will do a small Dutch test. All participants will be asked to do the same Dutch test. The words in this test will not be used in any of the conditions. This test will provide a frame of reference for each participant's Dutch level, in case we see unexpected results. After the test, there is a small break.</li> <li>Next, participants will be asked to practice a word list in one of the two conditions. This finishes after 150 trials or after a participant has seen all of the words. Next, participants will do a small distractor test. In this distractor test, they are asked to count dots. They need to do this 10 times. Then, we will test their word retrieval with a simple single-word translation test. After this, participants are asked to take a small break. This process is repeated a total of 2 times, once for each of the conditions.</li> <li>After each participant has done each of the two conditions, we will end the experiment by thanking them for their efforts.</li> </ul>"},{"location":"Final%20Paper%20User%20Models/#results","title":"Results","text":"<ul> <li>Not significant ):</li> <li>Not effective for all learners\u00a0</li> <li>Short time frame</li> <li>Effect : Using words in sentences vs Retrieval (Prince, 1996)</li> </ul>"},{"location":"Final%20Paper%20User%20Models/#limitations","title":"Limitations","text":"<ul> <li>Very small participant pool</li> <li>Variation in participant backgrounds</li> <li>Ceiling effect</li> <li>No external motivation to do well</li> <li>Not enough attention to the context</li> </ul>"},{"location":"Final%20Paper%20User%20Models/#future-directions","title":"Future Directions","text":"<ul> <li>Additional focus on gamification</li> <li>Longer term studies (Like SlimStampen)</li> <li>Harder words : Ceiling Effect</li> <li>More informed context in sentences (van den Broek et al., 2018)\u00a0</li> <li>Other ways of testing context</li> <li>Investigate thresholds</li> </ul>"},{"location":"Final%20Paper%20User%20Models/#pictures","title":"Pictures","text":""},{"location":"Fine%20Grained%20assesment/","title":"Fine Grained Assesment","text":""},{"location":"Fine%20Grained%20assesment/#counting-learning-events","title":"Counting Learning Events","text":"<ul> <li>We also assumed that a step is the product of one or more learning events, and defined a Learning Event to be a mental event based on a Knowledge Component. Learning events and knowledge components are not directly observable, but steps are.</li> <li>Mastery really means the probability that a Knowledge Component will be applied when it should be applied. If the student's competence was frozen instead of constantly changing due to learning and forgetting, then mastery could be estimated by counting the number of times a Knowledge Component was applied and dividing by the number of times it should have been applied. Thus, we need to discuss three issues: (1) How to detect applications of a Knowledge Component, (2) how to detect times when a Knowledge Component should have been applied, and (3) how to adjust for instruction, learning and forgetting.</li> </ul>"},{"location":"Fine%20Grained%20assesment/#counting-failures","title":"Counting Failures","text":"<ul> <li>Counting only the successful applications of a Knowledge Component is not enough; we need to know how many times the student failed to apply it as well.</li> <li>One approach is to detect failed attempts at steps. Suppose for simplicity that there is a one-to-one correspondence between a step and a Learning Event</li> <li>If the student fails to make the step, then the student must lack the knowledge that underlies the Learning Event.</li> </ul>"},{"location":"Fine%20Tuning%20Based%20Pruning/","title":"Fine Tuning Based Pruning","text":"<ul> <li>Some store weights before Pruning and use that to continue training.</li> <li>Others somehow try to rewind to a previous state and reinitialize the network entirely</li> </ul>"},{"location":"Fine%20grained%20datasets/","title":"Fine Grained Datasets","text":"<ul> <li>CUB-200-2011</li> <li>Stanford Dogs</li> <li>FGVC Aircraft</li> <li>FGVCx</li> <li>iNaturalist</li> <li>PlantCLEF</li> </ul>"},{"location":"Fine%20grained%20datasets/#backlinks","title":"Backlinks","text":"<ul> <li>Fine grained datasets</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Fine-grained%20Object%20Recognition/","title":"Fine-grained Object Recognition","text":""},{"location":"Fine-grained%20Object%20Recognition/#fine-grained-object-recognition","title":"Fine-grained Object Recognition","text":"<ul> <li>An object can be represented by:</li> <li>a shared (generic) dictionary, which is used to describe the content of all categories (basic-level)</li> <li>and a set of category-specific dictionaries for highlighting the small diversity within the different categories (fine-grained )</li> <li></li> </ul>"},{"location":"Fine-grained%20Object%20Recognition/#backlinks","title":"Backlinks","text":"<ul> <li>Fine-grained Object Recognition</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Finite%20Differences/","title":"Finite Differences","text":"<ul> <li> \\[f'(x) = \\frac{df}{dx} \\rightarrow \\frac{\\Delta f}{\\Delta x}\\] </li> <li></li> <li>Forward differences \\(\\(f'(x) = \\frac{f(x_{i+1})-f(x_{i})}{\\Delta x}\\)\\)</li> <li>Non Isotropic</li> <li>Backward differences \\(\\(f'(x) = \\frac{f(x_{i})-f(x_{i-1})}{\\Delta x}\\)\\)</li> <li>Non Isotropic</li> <li>Central differences \\(\\(f'(x) = \\frac{f(x_{i+1})-f(x_{i-1})}{2\\Delta x}\\)\\)</li> <li>High pass filter</li> <li>Non isotropic</li> </ul>"},{"location":"First%20order%20generalization/","title":"First Order Generalization","text":"<ul> <li>Present model with an example, ask it to choose which of three objects most likely of the same category</li> </ul>"},{"location":"First%20order%20integration/","title":"First Order Integration","text":"<ul> <li> \\[x(t+ \\Delta t) = x(t)+ \\Delta t f(x,t)\\] </li> <li>Global error proportional to \\(\\Delta t\\)</li> <li>Not stable</li> <li></li> <li></li> </ul>"},{"location":"Fisher%20Spanish-English/","title":"Fisher Spanish-English","text":""},{"location":"Fitting/","title":"Fitting","text":"<ul> <li>Bayes risk<ul> <li>Minimal expected risk over set of all functions \\(\\(R_B = min_{f\\in y^X} R(f)\\)\\)</li> <li>If minimized -&gt; Best possible function</li> <li>Capacity of hypothesis space \\(\\mathcal{H}\\)</li> <li>It is essentally all possible things. In reg, all possible affine linear fns. In neural networks, all possible specific connection structure.<ul> <li>If low, \\(\\(\\mathscr{F} = R(f) - R_B\\)\\) is large : Underfitting (Huge difference between best risk and current risk)</li> <li>If high, \\(\\(\\mathscr{F} = R(f) - R_B\\)\\) is small : Overfitting (Tiny difference between best risk and current risk)</li> </ul> </li> </ul> </li> </ul>"},{"location":"Fixed%20Factorization%20Attention/","title":"Fixed Factorization Attention","text":"<ul> <li>paper</li> <li>Specific cells summarize previous locations and propagate to all future cells.</li> <li>Part of Sparse Transformer</li> <li>Fixed attention pattern with c = 1 limits expressivity</li> <li>many representations in the network are only used for one block whereas a small number of locations are used by all blocks.</li> <li>Choosing $c \\in 8, 16, 32</li> <li>when using multiple heads, having them attend to distinct subblocks of length \\(c\\) within the block of size \\(l\\) was preferable to having them attend to the same subblock</li> <li></li> </ul>"},{"location":"Fixed%20Factors/","title":"Fixed Factors","text":"<ul> <li>Another term: independent variables</li> <li>This is a \"Between subjects ANOVA\"</li> <li>participants go in the same directions\u2026</li> <li>BUT, because item is also a random factor, we have to check that too</li> <li>This is a \"Between items ANOVA\"</li> <li>Check that for each item, what's the difference between the conditions, and check if they go in the same direction.</li> </ul>"},{"location":"Flamingo/","title":"Flamingo","text":"<ul> <li>Flamingo: a Visual Language Model for Few-Shot Learning</li> <li>large-scale pre-training followed by task-specific fine-tuning has emerged as a standard approach, but the fine-tuning step still requires a lot of samples.</li> <li>building models that can be rapidly adapted to numerous tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research</li> <li>family of Visual Language Models (VLM) which seek to train a multi-modal model (i.e., with the ability to understand different types of input \u2013 visual, audio, text etc.) in a few-shot learning approach (which refers to the ability to learn a new task with just a few samples for training).</li> <li>bridge powerful pretrained vision-only and language-only models</li> <li>handle sequences of arbitrarily interleaved visual and textual data</li> <li>seamlessly ingest images or videos as inputs</li> <li>Interleave cross-attention layers with language-only self-attention layers (frozen).</li> <li>Perceiver-based architecture that transforms the input sequence data (videos) into a fixed number of visual token</li> <li>Large-scale (web) multi-modal data by scraping webpages which has inter-leaved text and images</li> <li>Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities</li> </ul>"},{"location":"Flickr30K/","title":"Flickr30K","text":""},{"location":"Flipping/","title":"Flipping","text":"<ul> <li>Horizontal axis flipping is much more common than flipping the vertical axis.</li> <li>On datasets involving text recognition such as MNIST or SVHN, this is not a label-preserving transformation.</li> </ul>"},{"location":"Flynn%27s%20Taxonomy/","title":"Flynn's Taxonomy","text":"<ul> <li>Classify multi processor architectures</li> <li>SISD</li> <li>SIMD</li> <li>MISD</li> <li>MIMD</li> </ul>"},{"location":"Focal%20Loss/","title":"Focal Loss","text":"<ul> <li>two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations.</li> <li>In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far.</li> <li>Extreme foreground-background class imbalance encountered during training of dense detectors is the central cause</li> <li>modulating term to Cross Entropy in order to focus learning on hard misclassified examples</li> <li>scaling factor decays to zero as confidence in the correct class increases</li> <li>training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training</li> <li>RetinaNet</li> </ul>"},{"location":"Foley/","title":"Foley","text":"<ul> <li>A catheter inserted into the bladder to help with urinary drainage</li> </ul>"},{"location":"Force%20Directed%20Graph%20Layout/","title":"Force Directed Graph Layout","text":"<ul> <li>Model a graph as rings and springs</li> <li>Attractive forces between adjacent nodes</li> <li>edges are modeled as springs with uniform length</li> <li>Repulsive forces between non-adjacent nodes could be seen as springs of infinite length or repelling forces of electrically charged metal spheres</li> <li></li> </ul>"},{"location":"Force/","title":"Force","text":"<ul> <li>\"The vector sum of the external forces F on an object is equal to the mass m of that object multiplied by the acceleration vector of the object.\"</li> <li> \\[\\Sigma F = ma\\] </li> <li>mass times Acceleration</li> </ul>"},{"location":"Forceps/","title":"Forceps","text":"<ul> <li>A hinged instrument, like scissors, used to grasp and hold objects</li> </ul>"},{"location":"Forgetting/","title":"Forgetting","text":""},{"location":"Forgetting/#forgetting","title":"Forgetting","text":"<ul> <li>If our memories are too precise and overfitted, then we can't actually use them to make predictions about future situations</li> <li>Forgetting is an essential component of adaptive system</li> <li>Simple memories that store the gist of our experiences and avoid complicated details will be better for generalizing to future events.</li> <li>orgetting has been regarded as a passive decay over time of the information stored in the memory.</li> </ul>"},{"location":"Forgetting/#passive-forgetting","title":"Passive Forgetting","text":"<ul> <li>Concepts stored in the memory can be forgotten \"passively\" based on: Decay over time (fading factor)</li> <li>Loss of context cue</li> <li>Retrieval interference</li> </ul>"},{"location":"Forgetting/#active-forgetting","title":"Active Forgetting","text":"<ul> <li>may be more potent at erasing memory than the passive forgetting mechanisms Motivated forgetting</li> <li>forgetting is often more intentional</li> <li>unpleasant memories (categories) Intrinsic forgetting</li> <li>redundant data Interference-based forgetting samples that cause interference</li> </ul>"},{"location":"Forgetting/#backlinks","title":"Backlinks","text":"<ul> <li>Forgetting</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Forward%20Backward%20Matching/","title":"Forward Backward Matching","text":"<ul> <li>Matching proceeds from the end of the string of characters</li> <li>Results are compared</li> <li>Optimised Segmentation occurs</li> <li>Language-specific heuristics are used later</li> </ul>"},{"location":"Forward%20Kinematic%20Solution/","title":"Forward Kinematic Solution","text":"<ul> <li>The calculation required to find the endpoint position, given the joint positions. For most robot topologies this is easier than finding the inverse kinematic solution.</li> </ul>"},{"location":"Forward%20Kinematics/","title":"Forward Kinematics","text":"<ul> <li>Computational procedures which determine where the end-effector of a robot is located in space. The procedures use mathematical algorithms along with joint sensors to determine its location.</li> <li> <ol> <li>For a robot with n joints, what is the endeffector pose (\u03be ), given the joint angles (q)</li> <li>\u03be = \u03ba(q) : q = {qi,i \u2208 [1,\u2026,n]},</li> </ol> </li> </ul>"},{"location":"Fractional%20Anisotropy/","title":"Fractional Anisotropy","text":"<ul> <li> \\[FA = \\sqrt{\\frac{3}{2}}\\frac{\\sqrt{\\Sigma_{i=1}^{3}(\\lambda_{1}-\\mu)^{2}}}{\\Sigma_{i=1}^{3}\\lambda_{i}^{2}}\\] </li> <li></li> </ul>"},{"location":"Fracture/","title":"Fracture","text":"<ul> <li>A cracked or broken bone</li> </ul>"},{"location":"Free%20morpheme/","title":"Free Morpheme","text":"<ul> <li>can appear as a word by itself, often combined with other morphemes too.</li> <li>e.g., house (houses) , walk (walked ) of ,or,the</li> </ul>"},{"location":"Freedom/","title":"Freedom","text":"<ul> <li>(N, D, P) N samples, D degrees of freedom</li> <li>If N&lt;D , then ill posed</li> <li>Need N &gt;&gt; D</li> <li>If P learnable params , \\(\\(P&lt;N\\)\\) : underspecified</li> <li>If \\(\\(P &gt;&gt; N\\)\\) : overparameterized</li> <li>No of params not a good indicator of overfitting</li> <li>Solution : Regularization</li> </ul>"},{"location":"Frequentist/","title":"Frequentist","text":"<ul> <li>Measure probablity -&gt; Counting</li> <li>Repeat an experiment n times and get the estimate : \\(\\hat P\\) (estimate based on finite amount of data)</li> <li>Law of large numbers</li> <li>Random variable X which takes values in a sample space S.</li> <li>Measurement process hard to carry out in reality</li> <li>What does unbiased means? Especially because most things related to future input that we do not have yet</li> <li>Distibution of data points</li> <li>MLE</li> </ul>"},{"location":"Friction/","title":"Friction","text":"<ul> <li>Friction scales linearly with the normal force.</li> <li>Friction is not affected by the area of contact between surfaces.</li> <li>Stationary objects have more friction than sliding objects.</li> <li>Sliding friction is not affected by sliding velocity.</li> <li>You can look up the magnitude of friction for each pair of materials</li> <li>Static Friction</li> <li>Kinetic Friction</li> </ul>"},{"location":"Front%20to%20Back%20Raycasting/","title":"Front to Back Raycasting","text":"<ul> <li>Color Compositing</li> </ul>"},{"location":"Frontal%20Operculum/","title":"Frontal Operculum","text":"<ul> <li>The part of the frontal lobe that sits over the insula.</li> </ul>"},{"location":"Frontal%20lobe/","title":"Frontal Lobe","text":"<ul> <li>Personality, behavior, emotions</li> <li>Judgment, planning, problem solving</li> <li>Speech: speaking and writing (Brocas Area)</li> <li>Body movement (motor strip)</li> <li>Intelligence, concentration, self awareness</li> </ul>"},{"location":"Function%20words/","title":"Function Words","text":"<ul> <li>Glues words and phrases together</li> <li>Determiners</li> <li>Quantifiers</li> <li>Prepositions</li> <li>Connectives</li> </ul>"},{"location":"Functional%20Connectivity/","title":"Functional Connectivity","text":""},{"location":"Functional%20Connectivity/#symmetric","title":"Symmetric","text":"<ul> <li>BrainWave Synchronization</li> <li>BrainWave Coherence</li> <li>BrainWave CrossFrequency Coupling</li> </ul>"},{"location":"Functional%20Connectivity/#directedasymmetric","title":"Directed/Asymmetric","text":"<ul> <li>Granger Causallity</li> </ul>"},{"location":"Functional%20Morpheme/","title":"Functional Morpheme","text":"<ul> <li>provides grammatical information</li> <li>e.g. s (plural ) third person singular</li> </ul>"},{"location":"Functional%20correlates/","title":"Functional correlates","text":"<p>title: Functional correlates</p> <p>tags: statistics</p>"},{"location":"Functional%20correlates/#functional-correlates","title":"Functional Correlates","text":"<ul> <li>Dimensionality Reduction technique used to quantify the Correlation and dependence between two variables when the data is functional</li> <li>Relations between the surface and phenomena that influence or are influenced by the topography.</li> </ul>"},{"location":"Fundamentals/","title":"Fundamentals","text":"<ul> <li>Emperical Risk</li> <li>LinearRegression</li> <li>TemporalLearning</li> <li>Dimensionality Reduction</li> <li>Unsupervised Learning</li> <li>Semi Supervised</li> <li>Self Supervised</li> <li>Encodings</li> <li>Probability</li> <li>Universal Approximation Theorem</li> <li>Sampling</li> <li>Distributions</li> </ul>"},{"location":"GAM/","title":"GAM","text":"<ul> <li>Early explaining systems for ML black boxes go back to 1986 with Generalized Additive Models (GAM)</li> <li>GAMs are global statistic models that use smooth functions, which are estimated using a scatterplot smoother</li> <li>The technique is applicable to any likelihood-based regression model, provides a flexible method for identifying nonlinear covariate effects in exponential family models and other likelihood-based regression models, and has the advantage of being completely automatic</li> <li>In its most general form, the algorithm can be applied to any situation in which a criterion is optimized involving one or more smooth functions</li> </ul>"},{"location":"GAN%20Z%20Space/","title":"GAN Z Space","text":""},{"location":"GAN%20Z%20Space/#vector-algebra-in-z-space","title":"Vector Algebra in Z-Space","text":"<ul> <li>Controllable generation is somewhat similar to interpolation.</li> <li>With interpolation, you get intermediate examples between two generated observations.</li> <li>These intermediate examples between to two targets by manipulating the inputs from Z-space, which is the same idea behind controllable generation.</li> <li>In order to get intermediate values between two images, for example, you can make an interpolation between their two input vectors v1 and v2 in the Z-space.</li> <li>Controllable generation also uses changes in Z-space and makes use of how adjustments to the noise vector are reflected in the output from the generator.</li> <li>Differences in the features generated, for example different hair colors, occur due to changes in the direction that you have to move in Z-space to modify the features of the image.</li> <li>If image output of \\(g(v_{1})\\) , new controlled output with \\(g(v_{1}+d)\\)</li> </ul>"},{"location":"GAN%20Z%20Space/#backlinks","title":"Backlinks","text":"<ul> <li>Conditional GAN</li> <li>GAN Z Space</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"GAN%E2%80%90based%20Data%20Augmentation/","title":"GAN\u2010based Data Augmentation","text":"<ul> <li>Bowles et al. describe GANs as a way to 'unlock' additional information from a dataset</li> <li>Another useful strategy for generative modeling worth mentioning is variational auto-encoders. The GAN framework can be extended to improve the quality of samples produced with variational auto-encoders</li> <li>Using CycleGANs to translate images from the other 7 classes into the minority classes was very effective in improving the performance of the CNN model on emotion recognition.</li> <li>As exciting as the potential of GANs is, it is very difficult to get high-resolution outputs from the current cutting-edge architectures. Increasing the output size of the images produced by the generator will likely cause training instability and non-convergence</li> </ul>"},{"location":"GAU/","title":"GAU","text":"<ul> <li>gated attention unit; a generalization of GLU - gated linear unit</li> <li>allows for better and more efficient approximation of multi-head attention than many other efficient attention methods by using a weaker single-head attention with minimal quality loss</li> </ul>"},{"location":"GE2E/","title":"GE2E","text":"<ul> <li>Generalized End-to-end Loss for Speaker Verification</li> <li>new loss function</li> <li>training of speaker verification models more efficient</li> <li>Unlike TE2E, the GE2E loss function updates the network in a way that emphasizes examples that are difficult to verify at each step of the training process</li> <li>pushes the embedding towards the centroid of the true speaker, and away from the centroid of the most similar different speaker</li> <li>does not require an initial stage of example selection</li> <li>MultiReader technique</li> </ul>"},{"location":"GELU/","title":"GELU","text":"<ul> <li>Paper</li> <li>Smoother Relu</li> <li>\\(\\(x\\Phi(x)\\)\\) where \\(\\Phi(x)\\) is the Normal Distribution CDF</li> <li>Weights inputs by percentile, rather than by sign like ReLU</li> <li> \\[GELU(x) = xP(X \\leq x) = x\\Phi(x) = x. \\frac{1}{2}\\left[ 1+erf\\left( \\frac{x}{\\sqrt{ 2 }} \\right) \\right]\\] </li> <li>If \\(X \\sim \\mathscr{N}(0,1)\\)</li> <li>Used in GPT3, Transformer, Vision Transformer, BERT</li> <li></li> </ul>"},{"location":"GGCNN/","title":"GGCNN","text":"<ul> <li>Learning an object agnostic function to grasp objects</li> <li>Grasping using uni-modal data (depth image).  </li> <li>Generate pixel-wise grasp configuration for the given input.  </li> <li>The gripper approaches the target object in top-down manner. Uses a shallow network, and an eye-in-hand camera configuration.</li> <li></li> <li>Morrison, Douglas, Peter Corke, and Ju\u0308rgen Leitner. \"Closing the loop for robotic grasping: A real-time, generative grasp synthesis approach.\" RSS (2018).</li> </ul>"},{"location":"GLOW/","title":"GLOW","text":"<pre><code>def unsqueeze2d_new(input, factor=2):\n    return rearrange(input, 'b (c h2 w2) h w -&gt; b c (h h2) (w w2)', h2=factor, w2=factor)\n\ndef squeeze2d_new(input, factor=2):\n    return rearrange(input, 'b c (h h2) (w w2) -&gt; b (c h2 w2) h w', h2=factor, w2=factor)\n</code></pre>"},{"location":"GLUE/","title":"GLUE","text":""},{"location":"GOMS/","title":"GOMS","text":""},{"location":"GOMS/#parts","title":"Parts","text":"<ul> <li>Goals</li> <li>Operations</li> <li>Methods</li> <li>Selection Rules</li> </ul>"},{"location":"GOMS/#rest","title":"Rest","text":"<ul> <li>Human Computer Interaction</li> <li>Atomic</li> <li>Reactive vs proactive</li> <li>Performance vs control</li> </ul>"},{"location":"GPT/","title":"GPT","text":"<ul> <li>Pretrained using Unsupervised Learning and finetuned</li> <li>Log Likelihood Loss</li> <li></li> </ul>"},{"location":"GPT3/","title":"GPT3","text":"<ul> <li>Language Models are Few-Shot Learners</li> <li>shows that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches</li> <li>autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting</li> <li>without any gradient updates or fine-tuning</li> <li>on-the-fly reasoning or domain adaptation</li> <li>methodological issues related to training on large web corpora</li> <li>can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans</li> </ul>"},{"location":"GRConvNet/","title":"GRConvNet","text":"<ul> <li>Generative Residual Convolutional Neural Network</li> <li>Learning an object agnostic function to grasp objects</li> <li>Uses multi modal input data (RGB + depth images).</li> <li>Generates pixel-wise antipodal grasp configuration.</li> <li>State-of-the-art performance (97% on Cornell dataset).</li> <li>Use eye-to-hand camera configuration.</li> <li>Sulabh Kumra, et al. \"Antipodal robotic grasping using generative residual convolutional neural network.\" IROS 2020.</li> <li></li> </ul>"},{"location":"GTA5/","title":"GTA5","text":""},{"location":"Game%20Based%20Learning/","title":"Game Based Learning","text":""},{"location":"Game%20Based%20Learning/#game-based-learning","title":"Game Based Learning","text":"<ul> <li>gamified learning tasks</li> <li>improve engagement</li> <li>Learning progress</li> <li>Interactivity<ul> <li>Seductive details<ul> <li>interesting, but irrelevant</li> </ul> </li> <li>Should support a specific function</li> </ul> </li> <li>Does it work?<ul> <li>Mixed</li> <li>Studies use many game elements in a study - makes it hard to understand if any work</li> <li>Not a lot of directed, guided</li> <li>Makes it fun though</li> </ul> </li> </ul>"},{"location":"Gamification/","title":"Gamification","text":"<ul> <li>Use of game-like design elements</li> <li>Non game concepts</li> <li>Interface</li> <li>Mechanics</li> <li>Design</li> <li>Serious Games</li> <li>Game Based Learning</li> </ul>"},{"location":"Gaming%20addiction/","title":"Gaming addiction","text":"<p>title: Gaming addiction</p> <p>tags: brain, psychology</p>"},{"location":"Gaming%20addiction/#gaming-addiction","title":"Gaming Addiction","text":"<ul> <li>fMRI was performed while showing game images to online game addicts.</li> <li>According to Brain Areas control group, right orbitofrontal cortex, right nucleus accumbens, bilateral anterior cingulate and medial frontal cortex, right dorsolateral prefrontal cortex and right caudate nucleus activation were observed.</li> <li>These areas are the rewarding areas</li> <li>The results show that the same addiction to substance can share the same neuro-biological mechanisms with the extreme gaming demands of online gaming addiction.</li> <li>Paper</li> </ul>"},{"location":"Gamma%20Waves/","title":"Gamma Waves","text":"<ul> <li>28-90 Hz</li> <li>Attention/Consciousness</li> <li></li> </ul>"},{"location":"Gamma-aminobutyric%20Acid%20%28GABA%29/","title":"Gamma-aminobutyric Acid (GABA)","text":"<ul> <li>A neurotransmitter implicated in brain development, muscle control, and reduced stress response</li> </ul>"},{"location":"Gantry%20Robot/","title":"Gantry Robot","text":"<ul> <li>A robot which has three degrees of freedom along the X, Y and Z coordinate system. Usually consists of a spooling system (used as a crane), which when reeled or unreeled provides the up and down motion along the Z axis. The spool can slide from left to right along a shaft which provides movement along the Z axis. The spool and shaft can move forward and back along tracks which provide movement along the Y axis. Usually used to position its end effector over a desired object and pick it up.</li> </ul>"},{"location":"Gas%20Law/","title":"Gas Law","text":"<ul> <li>combination of Boyle's Law and Charles' Law</li> <li> \\[\\frac{P_{1}V_{1}}{T_{1}}= \\frac{P_{2}V_{2}}{T_{2}}\\] </li> <li>Temp must be in Kelvin</li> <li>pressure x volume of a gas = number of moles x molar gas constant x absolute temperature</li> <li> \\[pV = nRT\\] </li> </ul>"},{"location":"Gated%20Recurrent%20Unit%20%28GRU%29/","title":"Gated Recurrent Unit (GRU)","text":"<ul> <li>Simplified [[LSTM)](Long Short Term Memory (LSTM|Long Short Term Memory (LSTM|LSTM)]].md)</li> <li>It has an input and forget gate, no output gate</li> <li>Faster than LSTM in training, but does not perform well in many tasks</li> <li>Tries to forget what is not important</li> </ul>"},{"location":"Gated%20Recurrent%20Unit%20%28GRU%29/#the-math","title":"The Math","text":"<ul> <li>Two gates, Sigmoid<ul> <li>Reset : \\(\\(g_r = \\sigma(W_{hr}h_{t-1} + W_{xr}x_t + b_r)\\)\\)</li> <li>Update : \\(\\(g_u = \\sigma(W_{hu}h_{t-1} + W_{xu}x_t + b_u)\\)\\)</li> </ul> </li> <li>Hidden state proposal<ul> <li> \\[\\hat h_t = tanh(W_{xh}x_t + W_{hh}g_r\\cdot h_{t-1} + b_h)\\] </li> </ul> </li> <li>Final hidden state<ul> <li>Linear Interpolation between last hidden state and proposal</li> <li> \\[h_t = (1-g_u)\\cdot h_{t-1} + g_u \\cdot \\hat h_t\\] </li> </ul> </li> </ul>"},{"location":"Gato/","title":"Gato","text":"<ul> <li>A Generalist Agent</li> <li>Gato</li> <li>single generalist agent beyond the realm of text outputs, inspired by progress in large-scale language modeling</li> <li>multi-modal, multi-task, multi-embodiment generalist policy</li> <li>same network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens</li> <li>To enable processing this multi-modal data from different tasks and modalities, it is serialized into a flat sequence of tokens</li> <li>In this representation, Gato can be trained and sampled from akin to a standard large-scale language model</li> <li>Masking is used such that the loss function is applied only to target outputs, i.e text and various actions</li> <li>During deployment, sampled tokens are assembled into dialogue responses, captions, button presses, or other actions based on the context</li> <li>Transformer sequence models are effective as multi-task multi-embodiment policies, including for real-world text, vision and robotics tasks</li> </ul>"},{"location":"Gaussian%20Filter/","title":"Gaussian Filter","text":"<ul> <li>Filtering with a discretized Gaussian function</li> <li>Weights follow \\(\\(G(x) = e^{-ax^{2}}\\)\\)</li> </ul>"},{"location":"Gaze%20position/","title":"Gaze Position","text":"<ul> <li>Where the subject is looking</li> <li>Process how that moves with new stimuli</li> <li>Pupil size</li> <li>Gaze direction is a good metric of Attention</li> <li>Pupil Dilation</li> </ul>"},{"location":"GenEth/","title":"GenEth","text":"<ul> <li>ethical dilemma analyzer</li> <li>ethical issues related to intelligent systems are likely to exceed the grasp of the original system designers, and designed GenEth to include ethicists into the discussion process in order to codify ethical principles in given application domains.</li> <li>Features: denoting the presence or absence of factors (e.g.,harm,benefit) with integer values;</li> <li>Duties: denoting the responsibility of an agent to minimize/maximize a given feature;</li> <li>Actions: denoting whether an action satisfies or violates certain duties as an integer tuple;</li> <li>Cases: used to compare pairs of actions on their collective ethical impact</li> <li>Principles: denoting the ethical preference among different actions as a tuple of integer tuples.</li> </ul>"},{"location":"GenEth/#backlinks","title":"Backlinks","text":"<ul> <li>Building Ethics into Artificial Intelligence</li> <li>GenEth</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Gene%20Expression/","title":"Gene Expression","text":"<ul> <li>The process by which a gene</li> <li>\u2019s nucleotide sequence is transcribed into the form of RNA</li> <li>\u2014often as a prelude to being translated into a protein.</li> </ul>"},{"location":"Generalization%20Curve/","title":"Generalization Curve","text":"<ul> <li>A loss curve showing both the training set and the validation set. A generalization curve can help you detect possible overfitting. For example, the following generalization curve suggests overfitting because loss for the validation set ultimately becomes significantly higher than for the training set.</li> </ul>"},{"location":"Generalizing%20Adversarial%20Explanations%20with%20Grad-CAM/","title":"Generalizing Adversarial Explanations with Grad-CAM","text":"<ul> <li>Chakraborty, Tanmay, Utkarsh Trehan, Khawla Mallat, and Jean-Luc Dugelay. \u201cGeneralizing Adversarial Explanations with Grad-CAM.\u201d In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 186\u201392. New Orleans, LA, USA: IEEE, 2022. https://doi.org/10.1109/CVPRW56347.2022.00031.</li> <li>Adversarial Learning</li> </ul>"},{"location":"Generalizing%20Adversarial%20Explanations%20with%20Grad-CAM/#intro","title":"Intro","text":"<ul> <li>The drawback of Grad-CAM is that it cannot be used to generalize CNN behaviour.</li> <li>extends Grad-CAM from example-based explanations to a method for explaining global model behaviour</li> <li>These metrics are computed by comparing a Normalized Inverted Structural Similarity Index (NISSIM) metric of the Grad-CAM generated heatmap for samples from the original test set and samples from the adversarial test set.</li> <li>We observe a consistent shift in the region highlighted in the Grad-CAM heatmap, reflecting its participation to the decision making, across all models under adversarial attacks.</li> <li>These adversarial attacks display specific properties, i) They are not perceptible to the human eye, ii) They are controllable, and iii) Transferability, i.e., an attack designed for one model is capable of attacking multiple models</li> <li>There are mainly two kinds of attacks: targeted and non-targeted attacks. Targeted attack makes a model predict a certain label for the adversarial example, while for non-targeted attacks the labels for adversarial examples are not important, as long as the model is wrong</li> <li>These attacks can also be subdivided into black-box attacks and white-box attacks. Black-box attacks have no information about the target model, training procedure, architecture, whereas white-box attacks know the target model, training procedure, architecture, parameters.</li> <li>The CKA-similarity algorithm was used to compare the hidden representations of broad and deep models . They found that when the model capacity is large compared to the training set, a block structure emerges, which shows that the models propagate the main component of their hidden representation.</li> <li>More recent methods leverage explainability of machine learning and use SHAP based signatures to detect adversarial attacks</li> <li>As a result, we observed a global pattern displayed by all models. The shifting in the region of participation can be defined as when a model sees adversarial examples. Some parts of the input image no longer participate in the decision-making, while new parts do participate.</li> <li>These changes are not deterministic, and given an adversarial example, there is no way to tell how it will affect the shift</li> <li>FGSM</li> </ul>"},{"location":"Generalizing%20Adversarial%20Explanations%20with%20Grad-CAM/#new-metrics","title":"New Metrics","text":"<ul> <li>Normalized Inverted Structural Similarity Index</li> <li>Mean Observed Dissimilarity</li> <li>Variation in Dissimilarity Variation in Dissimilarity</li> </ul>"},{"location":"Generalizing%20Adversarial%20Explanations%20with%20Grad-CAM/#face-dataset-case-study","title":"Face Dataset Case Study","text":"<ul> <li>VGGFace2</li> <li>First, we preprocess the dataset to align and crop the faces. Then, the dataset is split into 80% training, 10% testing, and 10% validation sets.</li> <li>Once the training step is completed, the stored models are loaded and used to generate perturbations from the test set using FGSM.</li> <li>Then the test set is attacked with different values of \\(\\epsilon\\) from the stored perturbations and these counterexamples are stored as perturbed test sets</li> <li>Finally, Grad- CAM was used to generate heatmaps for every layer in each model and each \u03f5 in the perturbed test set.</li> <li>VGG 16: we can observe clearly that all the attacks were successful and illustrates a clear shift of participating regions as the \u03f5 increases.</li> <li>ResNet50 : the number of layers are too many to pin point out some example, yet if observed very carefully the hidden layers as the \u03f5 increases, we can find a shifting in the region of participation.</li> <li>In ResNet101: it seems more resilient there are some observable region shifts, but overall much less.</li> <li>InceptionNet v3 : seems to have learnt something different, the focus was more on forehead than face, but the overall shifting is much higher for this model, we even see focus regions getting inverted as the \u03f5 increases.</li> <li>For XceptionNet : the phenomenon is more clear, some regions get expanded, and background areas are being highlighted.</li> <li>We use the heatmap obtained from the original image (without adversarial perturbation) as our ground truth heatmap, i.e., what the model expects to see in order to make a decision, then the second heatmap is generated from the adversarially attacked image, and we create a dataframe with all the NISSIM values for the entire test set comparing with the adversarial test set for all values of \u03f5.</li> <li>We can also use this metric to explain the performance of VGG16. Since the shift was smaller, the model was less likely to fail.</li> <li>We also find that deep networks perform better than wide networks for similar shifts</li> <li>The main idea, that is examined here, is that the lower the shift in distribution, more the model is robust to adversarial attacks</li> <li>This indicates that VGG16 is a stable model for this task, over the other models.</li> </ul>"},{"location":"Generalizing%20Adversarial%20Explanations%20with%20Grad-CAM/#observations","title":"Observations","text":"<ul> <li>We see a shift in the focus of the model in different directions, sometimes backgrounds get highlighted, other times, participation region expands or shrinks.</li> <li>Deeper models are much robust to this changes, for similar amount of shift, deeper models provide better performance than wider models.</li> <li>neural networks fail because of a shifting behaviour in the region of participation to the decision-making, when the model sees adversarial examples, its focus changes and it now sees a different hidden representation</li> <li>The main observation to keep in mind is, as \u03f5 increases, the dissimilarity increases, indicating that the focus of the model is diverted when it is presented an adversarial example, this value indicates that the more the examples differ, the more likely the model will fail.</li> <li>We can observe a pattern that wide models fail more than deep models as the \u03f5 increases</li> </ul>"},{"location":"Generalizing%20Adversarial%20Explanations%20with%20Grad-CAM/#images","title":"Images","text":""},{"location":"Generative%20Models/","title":"Generative_Models","text":"<ul> <li>Basic GAN</li> <li>[[GAN]]</li> </ul>"},{"location":"Generative%20RNN/","title":"Generative RNN","text":"<ul> <li>initial sequence is used as seed and output is sampled\u00a0<ul> <li>random or argmax to sample</li> <li>normally not taking argmax but sample with respective\u00a0Softmax probabilities -&gt; allows to generate something different than input</li> </ul> </li> <li>new output is used as seed to generate next\u00a0</li> <li>repeat until termination criterion</li> </ul>"},{"location":"Generative%20Spoken%20Language%20Modeling/","title":"Generative Spoken Language Modeling","text":"<ul> <li>Generative Spoken Language Modeling from Raw Audio</li> <li>learns speech representations from CPC, Wav2Vec2.0, and HuBERT for synthesizing speech</li> <li>task of learning the acoustic and linguistic characteristics of a language from raw audio</li> <li>et of metrics to automatically evaluate the learned representations at acoustic and linguistic levels for both encoding and generation</li> <li>set up baseline systems consisting of a discrete speech encoder (returning pseudo-text units)</li> <li>generative language model (trained on pseudo-text)</li> <li>speech decoder (generating a waveform from pseudo-text)</li> <li>trained without supervision</li> <li>number of discrete units (50, 100, or 200) matters in a task-dependent and encoder-dependent way, and that some combinations approach text-based systems</li> </ul>"},{"location":"Generative%20vs%20Discriminative%20Models/","title":"Generative vs Discriminative Models","text":""},{"location":"Generative%20vs%20Discriminative%20Models/#generative","title":"Generative","text":"<ul> <li>LDA</li> <li>Bayesian Model Estimation</li> <li>[[HMM]]</li> <li>Autoregressive</li> <li>Basic GAN</li> </ul>"},{"location":"Generative%20vs%20Discriminative%20Models/#discriminative","title":"Discriminative","text":"<ul> <li>[[Logistic Regression]]</li> <li>[[SVM]]</li> <li>Decision Trees</li> <li>[[Random Forest]]</li> </ul>"},{"location":"Generative%20vs%20Discriminative%20Models/#backlinks","title":"Backlinks","text":"<ul> <li>Scalar Articles</li> <li>Generative vs Discriminative Models</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Geometric%20Transformations/","title":"Geometric Transformations","text":"<ul> <li>The safety of a Data Augmentation method refers to its likelihood of preserving the label post-transformation.</li> <li>A non-label preserving transformation could potentially strengthen the model\u2019s ability to output a response indicating that it is not confident about its prediction. However, achieving this would require refined labels post-augmentation.</li> <li>Due to the challenge of constructing refined labels for post-augmented data, it is important to consider the \u2018safety\u2019 of an augmentation. This is somewhat domain dependent</li> </ul>"},{"location":"Gestalt%20Laws/","title":"Gestalt Laws","text":"<ul> <li>Good form can dominate other laws</li> <li>crossing swarms in our visual field are perceived as different swarms</li> </ul>"},{"location":"Glia/","title":"Glia","text":"<ul> <li>The supporting cells of the central nervous system. They may contribute to the transmission of nerve impulses and play a critical role in protecting and nourishing neurons.</li> <li>Previously thought of as protective covering</li> <li>Central Nervous System<ul> <li>Astrocyte</li> <li>Microglia</li> <li>Ependymal Cell</li> <li>Ogliodendrocytes</li> </ul> </li> <li>Peripheral Nervous System<ul> <li>Satellite Cell</li> <li>Schwann Cell</li> </ul> </li> </ul>"},{"location":"Glioblastoma/","title":"Glioblastoma","text":"<ul> <li>An invasive brain tumor made up of glial tissue, blood vessels, and dead neurons.</li> </ul>"},{"location":"Glioma/","title":"Glioma","text":"<ul> <li>A tumor that arises from the brain\u2019s glial tissue.</li> </ul>"},{"location":"GloVE/","title":"GloVE","text":""},{"location":"GloVE/#glove","title":"GloVE","text":""},{"location":"GloVE/#explanation","title":"Explanation","text":"<ul> <li>GloVe: Global Vectors for Word Representation</li> <li>Word2Vec relies only on local information of language. That is, the semantics learnt for a given word, is only affected by the surrounding words.</li> <li>Unsupervised Learning algorithm which captures both global statistics and local statistics of a corpus</li> <li>aggregated global word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space</li> <li>whether distributional word representations are best learned from count-based methods or from prediction-based methods</li> <li>probe the underlying co-occurrence statistics of the corpus</li> <li>reformulated word2vec optimizations as a special kind of factorization for word co-occurence matrices</li> <li>Note that GloVe does not use neural networks</li> <li>utilizes this main benefit of count data while simultaneously capturing the meaningful linear substructures prevalent in recent log-bilinear prediction-based methods like word2vec</li> <li>global log-bilinear LinearRegression model for the unsupervised learning of word representations</li> <li></li> <li>There\u2019s a straight red column through all of these different words. They\u2019re similar along that dimension (and we don\u2019t know what each dimensions codes for)</li> <li>There are clear places where \u201cking\u201d and \u201cqueen\u201d are similar to each other and distinct from all the others. Could these be coding for a vague concept of royalty?</li> </ul>"},{"location":"GloVE/#analogies","title":"Analogies","text":""},{"location":"GloVE/#backlinks","title":"Backlinks","text":"<ul> <li>Word Vectors</li> <li>GloVE</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Global%20Average%20Pooling/","title":"Global Average Pooling","text":"<p>```mermaid graph TD;</p> <p>E1[Averages activations of each feature map] --&gt; E2[concatenates them] --&gt; E3[outputs as a vector] ```</p>"},{"location":"Global%20Average%20Pooling/#backlinks","title":"Backlinks","text":"<ul> <li>CAM</li> <li>Uses Global Average Pooling</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Global%20Classification%20Accuracy/","title":"Global Classification Accuracy","text":""},{"location":"Global%20Classification%20Accuracy/#global-classification-accuracy","title":"Global Classification Accuracy","text":"<ul> <li>an accuracy computed using all predictions in a complete experiment</li> </ul>"},{"location":"Global%20Classification%20Accuracy/#backlinks","title":"Backlinks","text":"<ul> <li>Global Classification Accuracy</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Global%20Gradient%20Magnitude%20Based%20Pruning/","title":"Global Gradient Magnitude Based Pruning","text":"<ul> <li>Identifies lowest absolute value \\((weight*gradient)\\) in the whole network and removes them</li> </ul>"},{"location":"Global%20Magnitude%20Based%20Pruning/","title":"Global Magnitude Based Pruning","text":"<ul> <li>Takes the lowest values in the entire network. Drops them.</li> </ul>"},{"location":"Global%20and%20Sliding%20Window%20Attention/","title":"Global and Sliding Window Attention","text":"<ul> <li>Sliding Window Attention and Dilated Sliding Window Attention are not always enough</li> <li>global attention\u201d on few pre-selected input locations.</li> <li>This attention is operation symmetric: that is, a token with a global attention attends to all tokens across the sequence, and all tokens in the sequence attend to it</li> <li></li> </ul>"},{"location":"Glucose/","title":"Glucose","text":"<ul> <li>A natural sugar that is carried in the blood and is the principal source of energy for the cells of the brain and body.</li> </ul>"},{"location":"Glymphatic%20System/","title":"Glymphatic System","text":"<ul> <li>The system that helps clear debris from the brain. During sleep, special glial cells called astrocytes form a network of conduits that allow cerebrospinal fluid to flush unwanted and unnecessary proteins out of the brain.</li> </ul>"},{"location":"Glyphs/","title":"Glyphs","text":"<ul> <li>Alpha Blending</li> </ul>"},{"location":"Goodhart%27s%20Law/","title":"Goodhart's Law","text":"<ul> <li>\u201cWhen a measure becomes a target, it ceases to be a good measure.\u201d</li> <li>Proxy Objective</li> <li>Rejection Sampling</li> </ul>"},{"location":"Goodhart%27s%20Law/#refs","title":"Refs","text":"<ul> <li>openai</li> </ul>"},{"location":"Google%20Conceptual%20Captions/","title":"Google Conceptual Captions","text":""},{"location":"Google%20NMT/","title":"Google NMT","text":"<ul> <li>Google\u2019s Neural Machine Translation System: Bridging the Gap Between Human and Machine Translation<ul> <li>deep [[LSTM)](Long Short Term Memory (LSTM|Long Short Term Memory (LSTM|LSTM)]].md) network with 8 encoder and 8 decoder layers using attention and residual connections</li> <li>improve parallelism and therefore decrease training time, their attention mechanism connects the bottom layer of the decoder to the top layer of the encoder</li> <li>low-precision arithmetic during inference computations (FP16 training ???)</li> <li>improve handling of rare words, we divide words into a limited set of common sub-word units</li> <li>good balance between the flexibility of \u201ccharacter\u201d-delimited models and the efficiency of \u201cword\u201d-delimited models</li> <li>Beam search technique employs a length-normalization procedure and uses a coverage penalty</li> </ul> </li> </ul>"},{"location":"Google%20voice%20search%20task/","title":"Google Voice Search Task","text":""},{"location":"Grad-CAM/","title":"GradCAM","text":"<ul> <li>Grad-CAM: Visual Explanations from Deep Networks Via Gradient-based Localization</li> <li>Modified CAM</li> <li>Importance of feature map k for target class c<ul> <li>A is input</li> <li>\\(Y_{c}=\\text{score of class c}\\) : value of output before softmax</li> <li>grad of \\(Y_{c}\\) wrt A and take avg</li> <li> \\[\\alpha_{k}^{c}= average(\\partial \\frac{Y_{c}}{\\partial A^{k}_{ij}})\\] </li> <li>If avg is high : important</li> <li>0 : not</li> <li>neg : background/ others</li> </ul> </li> <li>Weighted combination -&gt; relu<ul> <li> \\[L^{c}_{GRADCAM}=Resize(ReLU(\\Sigma_{k}(\\alpha^{c}_{k}A^{k})))\\] </li> <li>This is a coarse heatmap because the image is resized</li> <li>ReLU used because we only care about positive values (actualy image pixel)</li> </ul> </li> <li>To identify Counterfactual Images, flip the signs<ul> <li> \\[\\alpha_{k}^{c}=average(- \\partial \\frac{Y_{c}}{\\partial A^{k}_{ij}})\\] </li> <li></li> </ul> </li> <li>Followed by Guided GradCAM</li> </ul>"},{"location":"Grad-CAM/#predicting-failure-modes","title":"Predicting Failure Modes","text":""},{"location":"Grad-CAM/#adversarial-noise","title":"Adversarial Noise","text":"<ul> <li>robust to</li> <li></li> </ul>"},{"location":"Grad-CAM/#removing-biasness","title":"Removing Biasness","text":""},{"location":"Grad-CAM/#other-stuff","title":"Other Stuff","text":"<ul> <li>producing \u2018visual explanations\u2019 for decisions from a large class of CNN-based models, making them more transparent and explainable</li> <li>Gradient-weighted Class Activation Mapping</li> <li>uses the gradients of any target concept</li> <li>flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept</li> <li>lend insights into failure modes of these models (showing that seemingly unreasonable predictions have reasonable explanations)</li> <li>are robust to adversarial perturbations</li> <li>are more faithful to the underlying model</li> <li>help achieve model generalization by identifying dataset bias</li> <li>identify important neurons through GradCAM and combine it with neuron names to provide textual explanations for model decisions</li> </ul>"},{"location":"Grad-CAM/#gradcam-vs-cam","title":"GradCAM Vs CAM","text":"<ul> <li>Gradient-weighted Class Activation Mapping (Grad-CAM) is an improvement over Class Activation Mapping (CAM) that provides a more detailed and accurate visualization of the regions of an image that are important for a given classification.</li> <li>CAM generates heatmaps by using global average pooling (GAP) in the final convolutional layer to generate a feature map, followed by a linear combination of the feature map and the class weight vector to generate a single class activation map. However, this approach does not take into account the gradients of the class scores with respect to the feature maps, which can provide additional information about the contribution of different regions of the image to the final classification decision.</li> <li>Grad-CAM, on the other hand, uses the gradients of the class scores with respect to the feature maps in order to generate heatmaps. Specifically, it uses the gradients of the class scores with respect to the final feature maps of the network, which are then upsampled to the same size as the input image. The resulting heatmap highlights the regions of the input image that are most important for the given classification.</li> <li>In summary, Grad-CAM is an improvement over CAM because it provides a more detailed and accurate visualization of the regions of an image that are important for a given classification by using gradients of the class scores with respect to the feature maps, providing additional information about the contribution of different regions of the image to the final classification decision.</li> </ul>"},{"location":"Gradient%20Accumulation/","title":"Gradient Accumulation","text":"<ul> <li>helps when the model is not able to be trained with a big enough batch size</li> <li>often caused by memory limitations of the GPU</li> <li>Accumulate the gradients (for each trainable model value) of several forward passes and after some steps use the accumulated gradients to update the weights</li> <li>Is then equal to using a large batch size</li> <li>example with \\(\\(SGD: \\theta_{i}=\\theta_{i}\u22121\u2212 \\alpha\\ast(\\Sigma_{i=0}^{N}grad_{\\theta_{i}})\\)\\)</li> </ul>"},{"location":"Gradient%20Ascent/","title":"Gradient Ascent","text":"<ul> <li>To maximize loss function unlike Gradient Descent gradients</li> <li>Proportional to positive of gradient</li> <li> \\[\\theta_{t+1} = \\theta{t} + \\eta_t \\Sigma_{n=1}^N(\\nabla l_n(\\theta_t))^T\\] </li> </ul>"},{"location":"Gradient%20Boosting/","title":"Gradient Boosting","text":"<ul> <li>A training algorithm where weak models are trained to iteratively improve the quality (reduce the loss) of a strong model. For example, a weak model could be a linear or small decision tree model. The strong model becomes the sum of all the previously trained weak models.</li> <li>In the simplest form of gradient boosting, at each iteration, a weak model is trained to predict the loss gradient of the strong model.</li> </ul>"},{"location":"Gradient%20Checkpointing/","title":"Gradient Checkpointing","text":"<ul> <li>https://spell.ml/blog/gradient-checkpointing-pytorch-YGypLBAAACEAefHs</li> </ul>"},{"location":"Gradient%20Clipping/","title":"Gradient Clipping","text":"<ul> <li>Limit the value or the norm of a gradient to a fixed Hyperparameter \u03bb.</li> <li>mitigate the Vanishing &amp; Exploding Gradients, exploding ones</li> <li>idea is to clip the gradients during Backpropagation to a certain threshold (limit the value)</li> <li>most often used in RNN or GAN, where Batch Normalisation is tricky to use</li> <li>methods<ul> <li>clip by norm<ul> <li>clip the whole gradient if its L2 norm is greater than the threshold</li> <li>remains the orientation</li> </ul> </li> <li>clip by value<ul> <li>clip the gradient by a fixed value</li> <li>problem: orientation of the gradient may change due to clipping<ul> <li>example: [0.9,100.0]\u2192[0.9,1.0]</li> <li>however, this works well in practice</li> </ul> </li> </ul> </li> </ul> </li> <li>pros:<ul> <li>larger batch sizes</li> </ul> </li> <li>cons:<ul> <li>sensible to tuning Hyperparameter \u03bb</li> </ul> </li> <li>Adaptive Gradient Clipping</li> </ul>"},{"location":"Gradient%20Descent%20gradients/","title":"Gradient Descent","text":"<ul> <li>Backprop</li> <li> <p>Gradient Direction</p> </li> <li> <p>Gradient Magnitude</p> </li> <li>Edge Strength $\\(||\\triangledown f|| = \\sqrt{(\\frac{\\partial f}{\\partial x})^{2} + (\\frac{\\partial f}{\\partial y})^{2}}\\)</li> <li>Params \\(\\(\\theta\\)\\)</li> <li>Minimize loss function \\(\\(\\mathscr{L}(\\theta) = \\Sigma^N_{n=1}l_n(\\theta)\\)\\)</li> <li>Simple Gradient Descent</li> <li>SGD</li> <li>Mini Batch GD</li> <li>SGD Momentum</li> <li>Adagrad</li> <li>Nesterov Momentum</li> <li>AdaDelta</li> <li>Rmsprop</li> <li>Adam</li> </ul>"},{"location":"Gradient%20Descent%20gradients/#_1","title":"\u2026","text":""},{"location":"Gradient%20Direction/","title":"Gradient Direction","text":"<ul> <li>Direction of Steepest Descent \\(\\(\\theta = tan^{-1}(\\frac{\\frac{\\partial f}{\\partial y}}{\\frac{\\partial f}{\\partial x}})\\)\\)</li> </ul>"},{"location":"Gradient%20Direction/#backlinks","title":"Backlinks","text":"<ul> <li>Gradient Descent</li> <li>Gradient Direction</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Gradio/","title":"Gradio","text":"<pre><code>from fastai.vision.all import *\npath = untar_data(URLs.PETS)\ndls = ImageDataLoaders.from_name_re(path, get_image_files(path/'images'), pat='(.+)_\\d+.jpg', item_tfms=Resize(460), batch_tfms=aug_transforms(size=224, min_scale=0.75))\nlearn = vision_learner(dls, models.resnet50, metrics=accuracy)\nlearn.fine_tune(1)\nlearn.path = Path('.')\nlearn.export()\n\nlearn = load_learner('export.pkl')\n\nlabels = learn.dls.vocab\ndef predict(img):\n    img = PILImage.create(img)\n    pred,pred_idx,probs = learn.predict(img)\n    return {labels[i]: float(probs[i]) for i in range(len(labels))}\n\ntitle = \"Pet Breed Classifier\"\ndescription = \"A pet breed classifier trained on the Oxford Pets dataset with [fastai](./fastai.md). Created as a demo for Gradio and HuggingFace Spaces.\"\narticle=\"&lt;p style='text-align: center'&gt;&lt;a href='https://tmabraham.github.io/blog/gradio_hf_spaces_tutorial' target='_blank'&gt;Blog post&lt;/a&gt;&lt;/p&gt;\"\nexamples = ['siamese.jpg']\ninterpretation='default'\nenable_queue=True\n\ngr.Interface(fn=predict,inputs=gr.inputs.Image(shape=(512, 512)),outputs=gr.outputs.Label(num_top_classes=3),title=title,description=description,article=article,examples=examples,interpretation=interpretation,enable_queue=enable_queue).launch()\n</code></pre>"},{"location":"Gram%20matrix/","title":"Gram Matrix","text":"<pre><code>def gram_matrix_new(y):\n    b, ch, h, w = y.shape\n    return torch.einsum('bchw,bdhw-&gt;bcd', [y, y]) / (h * w)\n</code></pre>"},{"location":"Granger%20Causallity/","title":"Granger Causallity","text":"<ul> <li>Autoregressive</li> <li>If significant then electrode Granger-causes another</li> <li>Theres some causality but not sure if physical or causal</li> <li></li> <li>Partial Directed Coherence</li> <li>Directed Transfer Function</li> <li><ul> <li>Magnitude vs freq</li> <li>Undirected</li> <li>From O1 to PZ is different from PZ to O1</li> <li>How well can activity in one channel predict one in another</li> </ul> </li> </ul>"},{"location":"Graph%20Based%20Distillation/","title":"Graph Based Distillation","text":"<ul> <li>Lee and Song (2019) analysed intra-data rela- tions using a multi-head graph, in which the vertices are the features from different layers in CNNs. Park et al. (2019) directly transferred the mutual relations of data samples, i.e., to match edges between a teacher graph and a student graph. Tung and Mori (2019) used the similarity matrix to represent the mutual relations of the activations of the input pairs in teacher and student models. The similarity matrix of student matches that of teacher.</li> <li>Peng et al. (2019a) not only matched the response-based and feature-based knowl- edge, but also used the graph-based knowledge. In (Liu et al., 2019g), the instance features and instance relationships are modeled as vertexes and edges of the graph, respectively.</li> <li>Specifically, Luo et al. (2018) considered the modal- ity discrepancy to incorporate privileged information from the source domain. A directed graph, referred to as a distillation graph is introduced to explore the relationship between different modalities. Each vertex represent a modality and the edges indicate the connection strength between one modality and another.</li> <li>Minami et al. (2019) proposed a bidirectional graph-based diverse collaborative learning to explore diverse knowledge transfer patterns. Yao et al. (2020) introduced GNNs to deal with the knowledge trans- fer for graph-based knowledge.</li> <li>Besides, using knowl- edge distillation, the topological semantics of a graph convolutional teacher network as the topology-aware knowledge are transferred into the graph convolutional student network (Yang et al., 2020b)</li> </ul>"},{"location":"Graph-based%20visual%20saliency/","title":"Graph-based Visual Saliency","text":"<ul> <li>exploits channel-wise feature maps computed by linear filtering followed by a nonlinear transformation.</li> <li>To estimate saliency maps, the feature maps are transformed into activation maps and normalised by using the fully-connected directed graph of the feature maps.</li> </ul>"},{"location":"Graphs/","title":"Graphs","text":"<ul> <li>Graph \\(G= (V,E)\\) where</li> <li>edges \\(E \\subseteq V \\times V\\)</li> <li>vertices \\(V\\)</li> <li>Small World graphs</li> </ul>"},{"location":"Grasp%20Point%20Detection/","title":"Grasp Point Detection","text":"<ul> <li>Choose a grasp point either from above or from side by considering:<ul> <li>Size of object\u2019s bounding box</li> <li>Principle axes</li> <li>Projections/Views</li> </ul> </li> <li>GGCNN, GRConvNet, MVGrasp, Unet Grasping, Learning to Detect Grasp Affordance, Volumetric Grasping Network , Affordance Detection Task Specific</li> <li>Kinesthetic Teaching</li> </ul>"},{"location":"Gravity%20Loading/","title":"Gravity Loading","text":"<ul> <li>The force exerted downward, due to the weight of the robot arm and/or the load at the end of the arm. The force creates an error with respect to position accuracy of the end effector. A compensating force can be computed and applied bringing the arm back to the desired position.</li> </ul>"},{"location":"Gravity/","title":"Gravity","text":"<ul> <li>Mass\u00a0is a measure of an object's inertia.\u00a0Mass\u00a0also determines the strength of gravity. Because of gravity all objects are attracted to each other, but we mostly notice the attraction towards the Earth because it is so large and so close.</li> <li> \\[F_{g}= mg\\] </li> <li>\\(g = 9.8 m/s^2\\)</li> </ul>"},{"location":"Greedy%20Policy/","title":"Greedy Policy","text":"<ul> <li>In reinforcement learning, a policy that always chooses the action with the highest expected return.</li> </ul>"},{"location":"Grey%20sheep%20problem/","title":"Grey Sheep Problem","text":"<ul> <li>Fall outside the bounds in something like clustering</li> <li>gets assigned to something close by but not really related</li> </ul>"},{"location":"GridMask/","title":"GridMask","text":"<ul> <li>The algorithm tries to overcome drawbacks of Cutout, Random Erasing, and Hide and Seek that are prone to deleting important information entirely or leaving it untouched without making it harder for the algorithm to learn.</li> <li>To handle this, GridMask creates multiple blacked-out regions in evenly spaced grids to maintain a good balance between deletion and retention of critical information</li> <li>The number of masking grids and their sizes are tuneable</li> </ul>"},{"location":"Grids/","title":"Grids","text":""},{"location":"Gripper/","title":"Gripper","text":"<ul> <li>An end effector that is designed for seizing and holding (ISO 8373) and \"grips\" or grabs an object. It is attached to the last link of the arm. It may hold an object using several different methods, such as: applying pressure between its \"fingers\", or may use magnetization or vacuum to hold the object, etc</li> <li></li> </ul>"},{"location":"Group%20Modeling%20Approach/","title":"Group Modeling Approach","text":"<ul> <li>Take all users -&gt; Split them into groups</li> <li>Not personalized</li> <li>Easy to classify a user</li> <li>Grey sheep problem</li> </ul>"},{"location":"Guided%20BackProp/","title":"Guided BackProp","text":"<ul> <li>Striving for simplicity, the All conv net</li> </ul> <ul> <li>Instead of masking importance based on position of neg values , mask each signal to see if the case occurs or not</li> <li></li> <li></li> </ul>"},{"location":"Guided%20BackProp/#backlinks","title":"Backlinks","text":"<ul> <li>Salience Map</li> <li>Guided BackProp</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Guided%20GradCAM/","title":"Guided Grad-CAM","text":"<ul> <li>Pointwise multiply betwen CAM and Grad-CAM</li> <li>Class Discriminative</li> <li>High resolution</li> <li>Similar to [[Occlusion Map]] but faster</li> <li>Guided Grad-CAM is a variation of Grad-CAM that combines the gradients of the class scores with respect to the feature maps with the gradients of a guided backpropagation algorithm. Guided backpropagation is a method for visualizing the internal representations of a neural network by backpropagating the output of the network to the input image, while only propagating the positive gradients.</li> <li>The main difference between Grad-CAM and Guided Grad-CAM is that while Grad-CAM focuses on finding the regions of an image that are most important for a given classification, Guided Grad-CAM also takes into account the positive gradients of the guided backpropagation algorithm, in order to provide a more fine-grained visualization of the internal representations of the network. This can make Guided Grad-CAM more effective for understanding how the model is making its decisions, and for identifying the specific features of an image that the model is using for a given classification.</li> <li></li> </ul>"},{"location":"Guided%20GradCAM/#backlinks","title":"Backlinks","text":"<ul> <li>GradCAM</li> <li>Followed by Guided GradCAM</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Gyrus/","title":"Gyrus","text":"<ul> <li>The folding of the cortex increases the brain\u2019s surface area allowing more neurons to fit inside the skull and enabling higher functions</li> <li>Each groove between folds is called a sulcus.</li> <li>There are names for the folds and grooves that help define specific brain regions.</li> </ul>"},{"location":"H3%20View/","title":"H3 View","text":""},{"location":"Hallucination%20Text%20Generation/","title":"Hallucination Text Generation","text":"<ul> <li>Survey of Hallucination in Natural Language Generation</li> <li>often produces false statements that are disconnected from reality because such models are not grounded in reality</li> <li>hallucinated texts</li> </ul>"},{"location":"Hallucination/","title":"Hallucination","text":"<ul> <li>The production of plausible-seeming but factually incorrect output by a generative model that purports to be making an assertion about the real world. For example, if a dialog agent claims that Barack Obama died in 1865, the agent is hallucinating.</li> </ul>"},{"location":"Hamming%20Distance/","title":"Hamming Distance","text":"<ul> <li> \\[d = \\Sigma_{i}|p_{i}- q_{i}|\\] </li> <li>Hamming distance is the number of values that are different between two vectors</li> <li>It is typically used to compare two binary strings of equal length.</li> <li>difficult to use when two vectors are not of equal length</li> </ul>"},{"location":"Hand%20Guiding/","title":"Hand Guiding","text":"<ul> <li>allows an operator to hand guide the robot to a desired position. This task can be achieved by utilizing additional external hardware mounted directly to the robot or by a robot specifically designed to support this feature. Both solutions will require elements of functional safety to be utilized. A risk assessment shall be used to determine if any additional safeguarding is necessary to mitigate risks within the robot system.</li> </ul>"},{"location":"Harmonic%20Drive/","title":"Harmonic Drive","text":"<ul> <li>Compact lightweight speed reducer that converts high speed low torque to low speed high torque. Usually found on the minor (smaller) axis.</li> </ul>"},{"location":"Harness/","title":"Harness","text":"<ul> <li>Usually several wires, bundled together to deliver power and/or signal communications to/from devices. For example, the robot motors are connected to the controller through a wire harness.</li> </ul>"},{"location":"Hashing/","title":"Hashing","text":"<ul> <li>In machine learning, a mechanism for bucketing categorical data, particularly when the number of categories is large, but the number of categories actually appearing in the dataset is comparatively small.</li> <li>For example, Earth is home to about 60,000 tree species. You could represent each of the 60,000 tree species in 60,000 separate categorical buckets. Alternatively, if only 200 of those tree species actually appear in a dataset, you could use hashing to divide tree species into perhaps 500 buckets.</li> </ul>"},{"location":"Hausdorff%20Distance/","title":"Hausdorff Distance","text":"<ul> <li> \\[d= max_{i}(|p_{i}-q_{i}|)\\] </li> </ul>"},{"location":"Haversine%20Distance/","title":"Haversine Distance","text":"<ul> <li> \\[d = 2r\\times arcsin(\\sqrt{sin^{2}(\\frac{\\varphi_{2}-\\varphi_{1}}{2})+cos(\\varphi_{1})cos(\\varphi_{2})sin^{2}(\\frac{\\lambda_{2}-\\lambda_{1}}{2}))}\\] </li> <li>Haversine distance is the distance between two points on a sphere given their longitudes and latitudes</li> <li>The main difference is that no straight line is possible since the assumption here is that the two points are on a sphere.</li> <li>ssumed the points lie on a sphere</li> <li>As you might have expected, Haversine distance is often used in navigation</li> <li>calculate the distance between two countries when flying between them</li> <li>Note that it is much less suited if the distances by themselves are already not that large. The curvature will not have that large of an impact.</li> </ul>"},{"location":"He%20Initialization/","title":"He Initialization","text":"<ul> <li>bring the variance of those outputs to approximately one</li> <li>However, Kumar indeed proves mathematically that for the ReLU activation function, the best weight initialization strategy is to initialize the weights randomly but with this variance:<ul> <li> \\[\\begin{equation} v^{2} = 2/N \\end{equation}\\] </li> </ul> </li> <li>For Sigmoid based activation functions</li> </ul>"},{"location":"Heaviside/","title":"Heaviside","text":"<ul> <li>$$\\begin{cases} 1, \\text{if } z \\geq 0\\ 0, \\text{if } z &lt; 0</li> </ul> <p>\\end{cases}$$</p> <ul> <li>used in Rosenblatt's\u00a0Perceptron</li> <li>not\u00a0Differentiable\u00a0-&gt;\u00a0SGD\u00a0not possible</li> <li>no practical use</li> </ul>"},{"location":"Height%20Plots/","title":"Height Plots","text":"<ul> <li>2D scalar field</li> <li> \\[\\{(x,y, f(x,y))|(x,y)\\in \\mathbb{R}^{2}\\}\\] </li> <li>Displacement along \\(z = f(x,y)\\)</li> <li></li> </ul>"},{"location":"Heightmaps%20Kinesthetic/","title":"Heightmaps Kinesthetic","text":"<ul> <li>Herzog, Alexander, et al. \"Learning of grasp selection based on shape-templates.\" Autonomous Robots 36: pp. 51-65, 2014</li> <li></li> <li></li> </ul>"},{"location":"Helmholtz%20Theorem/","title":"Helmholtz Theorem","text":""},{"location":"Help%20Abuse/","title":"Help Abuse","text":"<p>Some students ask for help even when they don't need it. In the extreme cases, some students ask for help on every step.</p>"},{"location":"Help%20Refusal/","title":"Help Refusal","text":"<p>Some students refuse to ask for help even when they need it. They enter a long series of incorrect steps, which may be guesses, instead of clicking on the help button.</p>"},{"location":"Heteroscedatic/","title":"Heteroscedatic","text":"<ul> <li>if \\(\\sigma^{2}\\) is a function of the input or variable in \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma^{2})\\)</li> <li></li> </ul>"},{"location":"HiFI-GAN%20Denoising/","title":"HiFI-GAN_Denoising","text":"<ul> <li>HiFi-GAN: High-Fidelity Denoising and Dereverberation Based on Speech Deep [Features](./Features.md) in Adversarial Networks&gt;</li> <li>Real-world audio recordings are often degraded by factors such as noise, reverberation, and equalization distortion</li> <li>transform recorded speech to sound as though it had been recorded in a studio</li> <li>end-to-end feed-forward WaveNet architecture, trained with multi-scale adversarial discriminators in both the time domain and the time-frequency domain</li> <li>relies on the deep feature matching losses of the discriminators to improve the perceptual quality of enhanced speech</li> </ul>"},{"location":"HiFI-GAN%20Synthesis/","title":"HiFI-GAN Synthesis","text":"<ul> <li>HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis</li> <li>synthesis</li> <li>As speech audio consists of sinusoidal signals with various periods, they demonstrate that modeling periodic patterns of an audio is crucial for enhancing sample quality</li> <li>shows a significant improvement in terms of synthesis speed.</li> <li>MOS</li> <li>characteristic of speech audio that consists of patterns with various periods and applied it to neural networks, and verified that the existence of the proposed discriminator greatly influences the quality of speech synthesis through the ablation study</li> <li>generalize to the mel-spectrogram inversion of unseen speakers and synthesize speech audio comparable to human quality from noisy inputs in an end-to-end setting</li> <li>progress towards on-device natural speech synthesis, which requires low latency and memory footprint</li> <li>generators of various configurations can be trained with the same discriminators and learning mechanism</li> <li>possibility of flexibly selecting a generator configuration according to the target specifications without the need for a time-consuming hyper-parameter search for the discriminators</li> </ul>"},{"location":"Hide%20and%20Seek/","title":"Hide and Seek","text":"<ul> <li>divides an image into a specified number of grids and turns on and off each grid with an assigned probability</li> <li>various image regions are deleted, and they can be connected or disconnected from each other</li> <li>Values in turned-off regions are replaced with the average of all the pixel values in the entire dataset.</li> </ul>"},{"location":"Hide%20and%20Seek/#backlinks","title":"Backlinks","text":"<ul> <li>GridMask</li> <li>The algorithm tries to overcome drawbacks of Cutout, Random Erasing, and Hide and Seek that are prone to deleting important information entirely or leaving it untouched without making it harder for the algorithm to learn.</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Hierarchial%20Refinement/","title":"Heirarchial Refinement","text":"<ul> <li>Split n dim volume -&gt; finite set of discrete regions</li> <li>n dim hypercubes</li> <li>Construct regions where there is a higher point density</li> <li>Fine grained info encoded -&gt; smaller hypercubes to increase resolution</li> <li>n = 2 : quadtree</li> <li>n = 3 : octree</li> <li></li> <li>Mesh refinement</li> </ul>"},{"location":"Hierarchial%20Refinement/#_1","title":"\u2026","text":""},{"location":"Hierarchical%20Edge%20Bundling/","title":"Hierarchical Edge Bundling","text":"<ul> <li>Exploit the hierarchical structure to bundle non-hierarchical edges visually together</li> <li>conceptual similarity to bundling streamlines</li> <li></li> </ul>"},{"location":"High%20pass%20filter/","title":"High Pass Filter","text":"<ul> <li>That passes signals with a frequency higher than a certain cutoff frequency and attenuates signals with frequencies lower than the cutoff frequency.</li> </ul>"},{"location":"Highway%20Convolutions/","title":"Highway Convolutions","text":"<ul> <li>Conv</li> </ul> <pre><code>class HighwayConv1dNew(nn.Conv1d):\n    def forward(self, inputs):\n        L = super().forward(inputs)\n        H1, H2 = rearrange(L, 'b (split c) t -&gt; split b c t', split=2)\n        torch.sigmoid_(H1)\n        return H1 * H2 + (1.0 - H1) * inputs\n</code></pre>"},{"location":"Hinge%20Loss/","title":"Hinge","text":"<ul> <li>Classification</li> <li>SVMs</li> <li>the w are weights of the model</li> </ul> \\[\\mathrm{max}\\left( 0, 1 + \\mathrm{max}\\left( w_{y} \\cdot x - w_{t} \\cdot x \\right) \\right)\\]"},{"location":"Hinge%20Loss/#_1","title":"\u2026","text":""},{"location":"Hit%20list/","title":"Hit List","text":"<ul> <li>A shape feature which is powerful for Retrieval may not be strong in Recognition!</li> <li>Feature B: hit list should provide nice, intuitive rank in a satisfying \u2018hit list\u2019</li> <li>Feature A: target word class should survive competit with the other word classes (emerging needle from the heterogeneous hay stack)</li> </ul>"},{"location":"Holdout%20Data/","title":"Holdout Data","text":""},{"location":"Homoscedatic/","title":"Homoscedatic","text":"<ul> <li>if \\(\\sigma^{2}\\) is constant in \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma^{2})\\)</li> <li></li> </ul>"},{"location":"Hopfield%20networks/","title":"Hopfield Networks","text":"<ul> <li>from</li> <li>Older architecture used to store and retrieve patterns</li> <li>Building blocks<ul> <li>Data (list of patterns)</li> <li>Network<ul> <li>Nodes</li> <li>All nodes are connected with each other</li> </ul> </li> <li>Retrieval<ul> <li>Input: partial pattern</li> <li>Output: full pattern (retrieved)<ul> <li>\"best match\" partial pattern to entire data</li> <li>Filling out the missing nodes with the best pattern is called\u00a0update rule</li> </ul> </li> </ul> </li> <li>Energy Function<ul> <li>Theoretical concept, similar to a loss function</li> <li>Is not being optimized directly but trough update function</li> </ul> </li> <li>Update function<ul> <li>Optimizes the pattern that will be retrieved to best match the partial pattern</li> </ul> </li> </ul> </li> </ul>"},{"location":"Huber/","title":"Huber/Smooth L1/Smooth MAE","text":"<ul> <li>It is less sensitive to outliers than the MSE and in some cases prevents exploding #gradients</li> <li>Fast-RCNN</li> </ul> <p>if $$\\left( \\left|y - \u0177\\right| \\lt 1.0 \\right) &gt;1 $$</p> \\[\\frac{1}{\\mathrm{length}\\left( y \\right)} \\cdot \\mathrm{sum}\\left( 0.5 \\cdot \\left( y - \u0177 \\right)^{2} \\right)\\] <p>else</p> \\[\\frac{1}{\\mathrm{length}\\left( y \\right)} \\cdot \\mathrm{sum}\\left( \\left\\|y - \u0177\\right\\| - 0.5 \\right)\\]"},{"location":"Huber/#_1","title":"\u2026","text":""},{"location":"Humanoid%20Vision%20Engine/","title":"Humanoid Vision Engine","text":"<ul> <li>HVE</li> <li>summarize the contribution of shape, texture, and color in a given task (dataset) by separately computing the three features to support image classification</li> <li>end-to-end learning with backpropagation to simulate the learning process of humans and to summarize the contribution of shape, texture, and color</li> <li>advantage of end-to-end training is that we can avoid introducing human bias, which may influence the objective of contribution attribution</li> </ul>"},{"location":"Humanoid%20Vision%20Engine/#backlinks","title":"Backlinks","text":"<ul> <li>Contributions of Shape, Texture, and Color in Visual Recognition Abstract</li> <li>Humanoid Vision Engine</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Huntington%E2%80%99s%20Disease/","title":"Huntington\u2019s Disease","text":"<ul> <li>A neurodegenerative disorder that causes progressive death of neurons in the brain, resulting in severe movement and cognitive problems. The disorder is caused by the mutation of a single gene</li> <li>\u2014and symptoms typically present when an individual is in his or her 30\u2019s or 40\u2019s.</li> </ul>"},{"location":"Hybrid%20Word%20Segmentation/","title":"Hybrid Word Segmentation","text":"<ul> <li>combination</li> <li>weighted Finite State Transducer to identify dictionary entries</li> </ul>"},{"location":"HyperStreamlines/","title":"HyperStreamlines","text":""},{"location":"Hypertension/","title":"Hypertension","text":"<ul> <li>Unusually high blood pressure</li> </ul>"},{"location":"Hypodermic%20Needle/","title":"Hypodermic Needle","text":"<ul> <li>A very thin, hollow needle used with a syringe to inject substances into the body or to extract blood</li> </ul>"},{"location":"Hypotension/","title":"Hypotension","text":"<ul> <li>Unusually low blood pressure</li> </ul>"},{"location":"Hypothalamus/","title":"Hypothalamus","text":"<ul> <li>is located in the floor of the third ventricle and is the master control of the autonomic system.</li> <li>It plays a role in controlling behaviors such as hunger, thirst, sleep, and sexual response.</li> <li>It also regulates body temperature, blood pressure, emotions, and secretion of hormones.</li> </ul>"},{"location":"Hysterectomy/","title":"Hysterectomy","text":"<ul> <li>Surgical procedure to remove the uterus</li> </ul>"},{"location":"ICA%20Noise%20Removal/","title":"ICA Noise Removal","text":"<ul> <li>Notch filter</li> <li>High pass filter</li> </ul>"},{"location":"ICA/","title":"ICA","text":"<ul> <li>Independant Component Analysis</li> <li>Unmix combinations of signals</li> <li>Look for rotations of data into maximally independant components</li> <li>Not always orthogonal</li> <li>Better after noise removal</li> <li>Remove fewer than 20%</li> <li>Remove really bad parts first</li> <li></li> <li>ICA Noise Removal</li> </ul>"},{"location":"IDRiD/","title":"IDRiD","text":"<ul> <li>IDRiD is con- cerned with the disease grade recognition of retina images, and the presence or absence of diseases is recognized from exudates and hemorrhages.</li> <li>IDRiD includes a segmentation label of disease regions annotated by a specialist</li> </ul>"},{"location":"IID/","title":"IID","text":"<ul> <li>Neural networks assumes that the data points are independent and identically distributed.</li> </ul>"},{"location":"ILSVRC/","title":"ILSVRC","text":""},{"location":"IMDB/","title":"IMDB","text":"<ul> <li>Movie reviews</li> </ul>"},{"location":"IRT/","title":"IRT","text":"<ul> <li>assumes that every test item has a difficulty, and different items have different difficulties (Embretson &amp; Reise, 2000)</li> <li>Unfortunately, IRT generally assumes that test items are conditionally independent given the student's competence. This is seldom true of the raw measures collected at the step level by tutoring systems</li> <li>Although IRT has powerful features, such as calibration algorithms that empirically determine item difficulties and other parameters, considerable work is needed before it can be applied to tutoring systems.</li> </ul>"},{"location":"ISIC%202018/","title":"ISIC 2018","text":"<ul> <li>2386 dermoscopy images, all of them annotated with patterns on skin lesions unanimously recognized as indicators of potential malignancy</li> <li>binary masks highlighting the presence of five \"features\" at pixel-level</li> <li>globules, streaks, pigment network, negative network, and milia-like cysts.</li> <li>The performances on the feature extraction task are measured using the Jaccard index.</li> </ul>"},{"location":"ITM%20Loss/","title":"ITM Loss","text":"<ul> <li>ITM loss is an alignment loss that encompasses cross-modality interaction between image and text</li> <li>ITM requires positive and negative pairs</li> </ul>"},{"location":"Ideas%20for%20Fact%20Learning/","title":"Ideas for Fact Learning","text":"<ul> <li>Type of information<ul> <li>Methods of presentation</li> <li>Types of feedback</li> <li>Time pressure</li> <li>Study time</li> <li>Visualize progress</li> <li>Decide when an item is mastered</li> </ul> </li> <li>Increase system info about the user<ul> <li>Reaction times</li> <li>accuracy</li> <li>Eye Tracking</li> <li>biometric</li> <li>pupil dilation</li> <li>Detect learning styles</li> </ul> </li> <li>Framework<ul> <li>Gamification</li> </ul> </li> </ul>"},{"location":"Identity%20Loss/","title":"Identity Loss","text":"<ul> <li> \\[L_{identity}(G,F) = \\mathbb{E}_{y \\sim p_{data}(y)}[||G(y)-y)||_{1}] + \\mathbb{E}_{x \\sim p_{data}(x)}[||F(x)-x)||_{1}]\\] </li> <li>The identity loss is used to preserve the color and prevent reverse color in the result.</li> <li>This loss can regularize the generator to be near an identity mapping when real samples of the target domain are provided. If something already looks like from the target domain, you should not map it into a different image.</li> <li>The model will be more conservative for unknown content.</li> <li>In general, it can help bette preserve the content if that is your priority.</li> </ul>"},{"location":"Identity%20Loss/#backlinks","title":"Backlinks","text":"<ul> <li>CycleGAN</li> <li>Identity Loss</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Image%20Data/","title":"Image Data","text":"<ul> <li>Cant use MLPs<ul> <li>Too many weights to learn</li> <li>No translation equi-invariance</li> </ul> </li> </ul>"},{"location":"Image%20Mixing%20and%20Deletion/","title":"Image Mixing and Deletion","text":"<ul> <li>Cutout and CutMix argues that hindering image regions enforces the classifier to learn from the partially visible objects and understand the overall structure</li> <li>CutMix verifies this argument by showing enhanced focus towards the target class in</li> <li>Opposite to this, MixUp has shown to improve classifier's calibration and reduced prediction uncertainity</li> <li>mean of predictions vs accuracy where the confidence distribution for MixUp trained model is evenly distributed against the standard model whose disovertribution is towards higher conficence i.e. confidence</li> <li>Similarly, the loss contours obtained for a network trained with MixUp are smooth as compared to sharp contours in standarad training</li> <li>better generalization and robustness of MixUp against adversarial attacks.</li> <li>Mixup</li> <li>Cut and Delete</li> <li>Cutout</li> <li>Random Erasing</li> <li>Hide and Seek</li> <li>GridMask</li> <li>Adversarial Spatial Dropout for Occlusion</li> <li>Cut and Mix</li> <li>CutMix</li> <li>Attentive CutMix</li> <li>AttributeMix</li> <li>RICAP</li> <li>Mixed Example</li> <li>CowMask</li> <li>ResizeMix</li> <li>SaliencyMix</li> <li>Intra-Class Part Swapping</li> <li>SnapMix</li> <li>KeepAugment</li> <li>Visual Context Augmentation</li> <li>Cut, Paste and Learn</li> <li>Manifold MixUp</li> <li>AugMix</li> <li>SmoothMix</li> <li>Co-Mixup</li> <li>Sample Pairing</li> <li>Puzzle Mix</li> <li>ReMix</li> </ul>"},{"location":"ImageNet/","title":"ImageNet","text":"<p>title: ImageNet</p> <p>tags: dataset</p>"},{"location":"ImageNet/#imagenet","title":"ImageNet","text":""},{"location":"ImageNet/#backlinks","title":"Backlinks","text":"<ul> <li>Real Time Image Saliency for Black Box Classifiers</li> <li>ImageNet</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Imagen/","title":"Imagen","text":"<ul> <li>better top-1 accuracy on ImageNet than EfficientNet at similar latency</li> <li>Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding</li> <li>text-to-image diffusion model</li> <li>large Transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation</li> <li>Imagen produces \\(1024 \\times 1024\\) samples with unprecedented photorealism and alignment with text</li> <li>generic large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis</li> <li>increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model</li> <li>FID score</li> <li>COCO</li> <li>DrawBench</li> <li>With DrawBench, we compare Imagen with recent methods including VQGAN+CLIP, Latent Diffusion Models, and DALL-E, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment</li> </ul>"},{"location":"Implicit%20Bias/","title":"Implicit Bias","text":"<ul> <li>The unconscious attitudes, beliefs, or stereotypes we hold that have the power to affect our perceptions, actions, and decisions.</li> </ul>"},{"location":"Implicit%20Bias/#implicit-bias_1","title":"Implicit Bias","text":"<ul> <li>Automatically making an association or assumption based on one\u2019s mental models and memories. Implicit bias can affect the following<ul> <li>How data is collected and classified.</li> <li>How machine learning systems are designed and developed.</li> </ul> </li> <li>For example, when building a classifier to identify wedding photos, an engineer may use the presence of a white dress in a photo as a feature. However, white dresses have been customary only during certain eras and in certain cultures.</li> </ul>"},{"location":"Impulse/","title":"Impulse","text":"<ul> <li>An impulse is defined as a force applied over a period of time. Applying a larger force or longer lasting force produces a larger impulse. A large impulse produces a large change in momentum.</li> <li> \\[J = \\Delta p \\] </li> <li> \\[F \\Delta t = \\Delta p\\] </li> <li> \\[F \\Delta t = mv - mu\\] </li> <li> \\[F \\Delta t = m \\Delta v\\] </li> <li> \\[\\Delta v = \\frac{F}{m} \\Delta t\\] </li> <li>J is impulse in kg m/s</li> <li>F is force</li> <li>p is momentum</li> <li>v,u is velocity</li> </ul>"},{"location":"In%20Silico/","title":"In Silico","text":"<ul> <li>An experimental method to study brain or neural function using computer modeling or computer simulation.</li> </ul>"},{"location":"In%20Vitro/","title":"In Vitro","text":"<ul> <li>An experimental method to study brain or neural function by looking at cells outside a living organism, for example, in a test tube or petri dish.</li> </ul>"},{"location":"In%20Vivo/","title":"In Vivo","text":"<ul> <li>An experimental method allowing scientists to study brain or neural function in a living organism.</li> </ul>"},{"location":"In-group%20Bias/","title":"In-group Bias","text":"<ul> <li><code>#</code>fairness</li> <li>Showing partiality to one's own group or own characteristics. If testers or raters consist of the machine learning developer's friends, family, or colleagues, then in-group bias may invalidate product testing or the dataset.</li> <li>In-group bias is a form of group attribution bias.</li> </ul>"},{"location":"Inattentional%20Blindness/","title":"Inattentional Blindness","text":"<ul> <li>Viewers can fail to perceive visual elements or activities caused by an absence of attention to the unseen object</li> <li>Related to Change Blindness</li> </ul>"},{"location":"Inception/","title":"Inception","text":"<ul> <li>Going Deeper with Convolutions</li> <li>Rethinking the Inception Architecture for Computer Vision</li> </ul>"},{"location":"Inception/#v1","title":"V1","text":"<ul> <li>Conv at different filter scales to find different kinds of features -&gt; stack them up</li> <li>Increasing both the depth and width of the network while keeping computations at a manageable level</li> <li>Human visual system wherein information is processed at multiple scales and then aggregated locally</li> <li>channel dimensionality reduction, by reducing the output channels of the input</li> <li>To enable concatenation of features convolved with different kernels, they pad the output to make it the same size as the input.<ul> <li>without dilation</li> <li>padding \\(p = (k-1)/2p\\)</li> <li>since \\(out = in +2p -k +1\\)</li> </ul> </li> <li></li> </ul>"},{"location":"Inception/#v2v3","title":"V2/V3","text":"<ul> <li>nxn Conv -&gt; 1xn followed by nx1 Conv</li> <li>5x5, 7x7 -&gt; 2 and three 3x3 seq Conv</li> <li>More filters (wider)</li> <li>Distributed the computational budget in a balanced way between the depth and width of the network</li> <li>Added Batch Normalization</li> <li></li> </ul>"},{"location":"Inception/#v4","title":"V4","text":"<ul> <li>(from) Paper:\u00a0https://arxiv.org/pdf/1602.07261.pdf Year: 2016 Summary: New Residual\u00a0Inception\u00a0Architecture (deep\u00a0CNN)</li> <li>Why?<ul> <li>Introduction of residual connections on traditional architectures yielded SOTA performance (2015)</li> </ul> </li> <li>Research question<ul> <li>Are there benefits when combining residual connections with the\u00a0Inception\u00a0architecture?</li> </ul> </li> <li>Findings<ul> <li>Residual connections accelerates training of the\u00a0Inception\u00a0architecture</li> <li>Residual\u00a0Inception\u00a0outperforming similar architecture only close</li> <li>When the number of filters were higher than\u00a01\u2032000\u00a0the residual variants of the network died early in the training (e.g. outputted only zeros)<ul> <li>was not able to fix with lowering the\u00a0Learning Rate\u00a0or\u00a0Batch Normalization</li> </ul> </li> <li>Scaling down the residuals before adding them with the residual connection stabilized the training (factor:\u00a00.1\u22120.3)</li> </ul> </li> <li>General Ideas<ul> <li>Parallel convolutions: Similar to the GoogLeNet architecture within their\u00a0modules\u00a0the authors simultaneously use multiple convolutional branches with different receptive field sizes on the same input activation maps and again concatenate those activations for further processing.</li> <li>Reduction modules: Instead of simply applying a single max pooling or a 2-stride convolution to downsize the spatial dimensions, the authors dedicated whole modules to this task again employing parallel branches.\u00a0</li> <li>Strong usage of small convolutional kernels(e.g.\u00a03\u00d73): Throughout the network the authors pefer smaller convolutional kernel size over larger ones, as this enables the same receptive field with less parameters (e.g. a single\u00a05\u00d75convolution \u223c25\u00a0params  as 2 consecutive\u00a03\u00d73convolutions [\u223c18\u00a0params ], but the later has less parameters)</li> <li>Factorization of convolutions: They factorize convolutions of filter size\u00a0n\u00d7n\u00a0to a combination of\u00a01\u00d7n\u00a0and\u00a0n\u00d71\u00a0convolutions, in order to reduce the nr of parameters even further (e.g.\u00a07\u00d77\u00a0[\u223c49\u00a0params ] results in\u00a01\u00d77\u00a0and\u00a07\u00d71\u00a0[\u223c14\u00a0params ]!)</li> <li>Residual connections: In the\u00a0<code>Inception-ResNet-v1</code>\u00a0and\u00a0<code>Inception-ResNet-v2</code>\u00a0the authors employ the usage of residual connections. Although the residual version of the networks converge faster, the final accuracy seems to mainly depend on the model size.</li> <li>Usage of bottleneck layers: In order to reduce the cost of the individual convolutional branches within their modules, they apply\u00a01\u00d71convolutions at the beginning to reduce the depth of the input activation maps.</li> </ul> </li> <li>Remarks<ul> <li>Authors disagree with residual paper one some points\u00a0<ul> <li>Residual connections are nessecary for training deep convolutional models<ul> <li>They show that it is not hard to train very deep models which achieve high performance without residual connections</li> <li>They argue that residual connections do only speed up the training greatly</li> </ul> </li> <li>\"Warm up\" phases (pre-training with very low LR followed by a high LR) do not help to stabilize training very deep networks<ul> <li>high LR had the chance to destroy already learnt features</li> <li>Scaling should be used instead</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"Inceptionism/","title":"Inceptionism","text":"<ul> <li>Google AI Blog: Inceptionism: Going Deeper into Neural Networks #Roam-Highlights</li> <li>Le Net</li> <li></li> <li></li> <li></li> <li></li> <li></li> <li></li> <li></li> <li>One of the challenges of neural networks is understanding what exactly goes on at each layer</li> <li>We know that after training, each layer progressively extracts higher and higher-level features of the image, until the final layer essentially makes a decision on what the image shows</li> <li>For example, the first layer maybe looks for edges or corners</li> <li>Intermediate layers interpret the basic features to look for overall shapes or components, like a door or a leaf</li> <li>The final few layers assemble those into complete interpretations\u2014these neurons activate in response to very complex things such as entire buildings or trees.</li> <li>One way to visualize what goes on is to turn the network upside down and ask it to enhance an input image in such a way as to elicit a particular interpretation.</li> <li>Why is this important? Well, we train networks by simply showing them many examples of what we want them to learn, hoping they extract the essence of the matter at hand (e.g., a fork needs a handle and 2-4 tines), and learn to ignore what doesn\u2019t matter (a fork can be any shape, size, color or orientation)</li> <li>But how do you check that the network has correctly learned the right features? It can help to visualize the network\u2019s representation of a fork.</li> <li>Indeed, in some cases, this reveals that the neural net isn\u2019t quite looking for the thing we thought it was.</li> <li>Instead of exactly prescribing which feature we want the network to amplify, we can also let the network make that decision</li> <li>In this case we simply feed the network an arbitrary image or photo and let the network analyze the picture</li> <li>We then pick a layer and ask the network to enhance whatever it detected.</li> <li>If we choose higher-level layers, which identify more sophisticated features in images, complex features or even whole objects tend to emerge</li> <li>Again, we just start with an existing image and give it to our neural net.</li> <li>We ask the network: \u201cWhatever you see there, I want more of it!\u201d This creates a feedback loop: if a cloud looks a little bit like a bird, the network will make it look more like a bird</li> <li>This in turn will make the network recognize the bird even more strongly on the next pass and so forth, until a highly detailed bird appears, seemingly out of nowhere.</li> <li>The results are intriguing\u2014even a relatively simple neural network can be used to over-interpret an image, just like as children we enjoyed watching clouds and interpreting the random shapes</li> <li>This network was trained mostly on images of animals, so naturally it tends to interpret shapes as animals</li> <li>But because the data is stored at such a high abstraction, the results are an interesting remix of these learned features.</li> <li>Of course, we can do more than cloud watching with this technique</li> <li>We can apply it to any kind of image</li> <li>The results vary quite a bit with the kind of image, because the features that are entered bias the network towards certain interpretations</li> <li>For example, horizon lines tend to get filled with towers and pagodas</li> <li>Rocks and trees turn into buildings</li> <li>Birds and insects appear in images of leaves.</li> <li>We must go deeper: Iterations If we apply the algorithm iteratively on its own outputs and apply some zooming after each iteration, we get an endless stream of new impressions, exploring the set of things the network knows about</li> <li>We can even start this process from a random-noise image, so that the result becomes purely the result of the neural network, as seen in the following images:</li> <li>The techniques presented here help us understand and visualize how neural networks are able to carry out difficult classification tasks, improve network architecture, and check what the network has learned during training</li> <li>It also makes us wonder whether neural networks could become a tool for artists\u2014a new way to remix visual concepts\u2014or perhaps even shed a little light on the roots of the creative process in general.</li> </ul>"},{"location":"Indirect%20Volume%20Visualization/","title":"Indirect Volume Visualization","text":"<ul> <li>Isosurface</li> </ul>"},{"location":"Individual%20Fairness/","title":"Individual Fairness","text":"<ul> <li>A fairness metric that checks whether similar individuals are classified similarly. For example, Brobdingnagian Academy might want to satisfy individual fairness by ensuring that two students with identical grades and standardized test scores are equally likely to gain admission.</li> <li>Note that individual fairness relies entirely on how you define \"similarity\" (in this case, grades and test scores), and you can run the risk of introducing new fairness problems if your similarity metric misses important information (such as the rigor of a student\u2019s curriculum).</li> </ul>"},{"location":"Individual%20Modeling/","title":"Individual Modeling","text":"<ul> <li>More personalized</li> <li>Less data per category</li> <li>Ramp up problem</li> </ul>"},{"location":"Induced%20Pluripotent%20Stem%20Cell%20%28iPSC%29/","title":"Induced Pluripotent Stem Cell (iPSC)","text":"<ul> <li>A cell that has been taken from adult tissue and genetically modified to behave like an embryonic stem cell, with the ability to develop into any type of cell found in the body, including nerve cells.</li> </ul>"},{"location":"Inductive%20Bias/","title":"Inductive Bias","text":"<ul> <li>Set of assumptions that the learner uses to predict outputs of given inputs that it has not yet encountered</li> <li>In Bayesian<ul> <li>Bayesian Prior can shape the Bayesian Posterior in the way that it can be a similar distribution to the former</li> </ul> </li> <li>In KNN<ul> <li>we assume that similar data points are clustered near each other away from the dissimilar ones</li> </ul> </li> <li>in LinearRegression<ul> <li>we assume that the variable Y is linearly dependent on the explanatory variables X.</li> <li>Therefore, the resulting model linearly fits the training data. However, this assumption can limit the model\u2019s capacity to learn non-linear functions.</li> </ul> </li> <li>in Logistic Regression<ul> <li>assume that there\u2019s a hyperplane that separates the two classes from each other. This simplifies the problem, but one can imagine that if the assumption is not valid, we won\u2019t have a good model.</li> </ul> </li> <li>Non Relational Inductive Bias</li> <li>Relational Inductive Bias</li> </ul>"},{"location":"Inductive%20Learning/","title":"Inductive Learning","text":"<ul> <li>Bayesian is inductive learning</li> <li>Learning is identifying which hypothesis set is a concept</li> <li>Hypotheses don't disappear, they just become less likely</li> <li>Learning develops through more experience</li> <li>One challenge of Bayesian learning is that any small subset is consistent with many hypotheses</li> <li>Different hypotheses have different likelihoods based on the examples we are exposed to</li> <li>But in the end we also prefer smaller hypotheses over larger ones: The size principle</li> <li>Simple clustering methods can be used to get the data to automatically create the hypothesis space needed for Bayesian modelling</li> <li>Probabilities of different sets then match with human judgments surprisingly well</li> <li>Clustering based on biology worked worse!</li> <li>Clustering using linguistic co-occurrences with Latent Semantic Analysis also worked worse!</li> <li>Human subject judgements of similarity worked best</li> <li>Suggests some human reasoning relies on probability</li> <li>Bayesian learning can also learn categories</li> <li>Models are capable of making generalizations about the specific objects as well as the appropriate generalizations about categorization (superordinate categories!) in general.</li> <li>Advanced learning means learn constraints on what is a possible hypothesis</li> <li>Hierarchical Bayesian Modelling (HBM) can explain how we acquire overhypotheses</li> <li>using observations from the lowest level (data) and calculating statistical inferences</li> </ul>"},{"location":"Inductive%20Sensor/","title":"Inductive Sensor","text":"<ul> <li>The class of proximity sensors, which has half of a ferrite core, whose coil is part of an oscillator circuit. When a metallic object enters this field, at some point, the object will absorb enough energy from the field to cause the oscillator to stop oscillating. This signifies that an object is present in a given proximity.</li> </ul>"},{"location":"Inertia/","title":"Inertia","text":"<ul> <li>\"A body either remains at rest or continues to move at a constant velocity, unless acted upon by a net external force.\"</li> </ul>"},{"location":"Inference%20Path/","title":"Inference Path","text":"<ul> <li>In a decision tree, during inference, the route a particular example takes from the root to other conditions, terminating with a leaf.</li> </ul>"},{"location":"Infix/","title":"Infix","text":"<ul> <li>inserted inside the stem</li> </ul>"},{"location":"Inflectional%20Morphology/","title":"Inflectional Morphology","text":"<ul> <li>creates the new forms of the same word</li> <li>e.g.bring, brought, brings</li> </ul>"},{"location":"Inflectional%20words/","title":"Inflectional Words","text":"<ul> <li>boundaries unclear, can express more than one grammar meaning</li> </ul>"},{"location":"Influence%20of%20image%20classification%20accuracy%20on%20saliency%20map%20estimation/","title":"Influence of Image Classification Accuracy on Saliency Map Estimation","text":"<ul> <li>Oyama, Taiki, and Takao Yamanaka. 2018. \u201cInfluence of Image Classification Accuracy on Saliency Map Estimation.\u201d CAAI Transactions on Intelligence Technology 3(3):140\u201352. doi: 10.1049/trit.2018.1012.</li> </ul>"},{"location":"Influence%20of%20image%20classification%20accuracy%20on%20saliency%20map%20estimation/#intro","title":"Intro","text":"<ul> <li>Saliency map estimation in computer vision aims to estimate the locations where people gaze in images.</li> <li>Since people tend to look at objects in images, the parameters of the model pre-trained on ImageNet for image classification are useful for the saliency map estimation</li> <li>no research on the relationship between the image classification accuracy and the performance of the saliency map estimation</li> <li>strong correlation between image classification accuracy and saliency map estimation accuracy</li> <li>It the models pre-trained on ImageNet are useful for saliency map estimation the parameters of is known that</li> <li>This would be because a human tends to look at the centres of objects , which are learned to be recognised in the pre-trained model for the ImageNet classification task.</li> <li>Although the model based on DenseNet has achieved the state-of-the-art performance in the ACPR 2017 paper, this additional study led to even better performance using the model based on dual path networks (DPNs)</li> </ul>"},{"location":"Influence%20of%20image%20classification%20accuracy%20on%20saliency%20map%20estimation/#related-work","title":"Related Work","text":"<ul> <li>uses the coefficients of Attention based on Information the basis Maximization (AIM) calculated by the independent component analysis (ICA) in local image patches</li> <li>The distribution of the coefficients is estimated by the kernel density estimation, which is used for estimating saliency maps based on the the local patches self-information of</li> <li>Graph-based visual saliency</li> <li>Saliency using natural statistics</li> <li>Dynamic visual attention</li> <li>Adaptive Whitening Saliency</li> <li>SAM-ResNet</li> </ul>"},{"location":"Influence%20of%20image%20classification%20accuracy%20on%20saliency%20map%20estimation/#components-of-readout-net","title":"Components of Readout Net","text":"<ul> <li>The operation attempts to directly minimise the reconstruction error of the input image under a sparsity constraint on an over-complete set of feature maps</li> <li>The mini-batch size and learning rate were set to 1 and 10\u22125 during training, respectively.</li> <li>We subtract the per-channels mean value of training images from each image as pre-processing</li> </ul>"},{"location":"Influence%20of%20image%20classification%20accuracy%20on%20saliency%20map%20estimation/#dc","title":"DC","text":"<ul> <li>\"DC is also called as transposed convolution\"</li> <li>When DC is used for the up-sampling layers in Readout Net, the first, second, and third DC followed by a ReLU layer in Readout Net reduce the channels to 128, 64, and 32, respectively. Then, the 1 \u00d7 1 convolution reduces the channel to 1 to predict the saliency map. The filter size of DC was set to 4 \u00d7 4.</li> <li>method to recover a high-resolution image from its additional low resolution counterpart with little computational cost, by rearranging the data along the channel into feature maps with a convolution operation.</li> </ul>"},{"location":"Influence%20of%20image%20classification%20accuracy%20on%20saliency%20map%20estimation/#spc","title":"SPC","text":"<ul> <li>When SPC is used for the up-sampling layers, each SPC layer reduces the channels to one forth, followed by a 3 \u00d7 3 convolution and a ReLU layer. Then, the 1 \u00d7 1 convolution predicts the saliency map from the output of the last SPC layer.</li> <li>each BI layer in the up-sampling network resizes a feature map twice while maintaining the feature-map channels, GPU was out of memory when three up-sampling layers were used for all channels of outputs of Main Net</li> </ul>"},{"location":"Influence%20of%20image%20classification%20accuracy%20on%20saliency%20map%20estimation/#bi","title":"BI","text":"<ul> <li>the order of up-sampling and projection (1 \u00d7 1 convolution) can be inverted without any influence on the output</li> <li>the concatenated feature maps from Main Nets are first processed by the 1 \u00d7 1 convolution to output feature map, followed by the BI up-sampling layers. the 1-channel</li> </ul>"},{"location":"Influence%20of%20image%20classification%20accuracy%20on%20saliency%20map%20estimation/#datasets","title":"Datasets","text":"<ul> <li>Salicon dataset</li> <li>OSIE</li> <li>PASCAL-S</li> <li>MIT1003</li> <li>MIT300</li> </ul>"},{"location":"Influence%20of%20image%20classification%20accuracy%20on%20saliency%20map%20estimation/#metrics","title":"Metrics","text":"<ul> <li>AUC-Judd</li> <li>AUC-Borji</li> <li>Shuffled-AUC</li> </ul>"},{"location":"Influence%20of%20image%20classification%20accuracy%20on%20saliency%20map%20estimation/#conclusions","title":"Conclusions","text":"<ul> <li>strong correlation between image classification accuracy and saliency map estimation accuracy.</li> <li>not only the architecture but also the initialisation strategy using the weights pre-trained with the ImageNet classification task were important for estimating the saliency maps</li> <li>model which is pre-trained with the ImageNet classification and has achieved high</li> <li>\"for performance on the classification task is also useful the\"</li> <li>\"saliency map estimation task\"</li> <li>human fixations often concentrate on objects in the image, while the model pre-trained on ImageNet can react on many objects in images because ImageNet has a wide variety of object categories.</li> <li>If the model is initialised with random weights and is trained on a fixation dataset with the limited categories of objects for saliency map estimation, to the objects in the training dataset the model would overfit</li> <li>if the model is trained for the image classification task which includes a wide variety of categories, overfitting for the objects in the training dataset would be suppressed owing to a large number of categories.</li> </ul>"},{"location":"Influence%20of%20image%20classification%20accuracy%20on%20saliency%20map%20estimation/#images","title":"Images","text":""},{"location":"Information%20Gain/","title":"Information Gain","text":"<ul> <li>In decision forests, the difference between a node's entropy and the weighted (by number of examples) sum of the entropy of its children nodes. A node's entropy is the entropy of the examples in that node.</li> </ul>"},{"location":"Information%20Visualization/","title":"Information Visualization","text":"<ul> <li>Visualization of abstract data</li> <li>Visual mappings often have to be learned</li> <li>spatial layout is chosen</li> <li>Perception</li> <li>Visual Encoding</li> </ul>"},{"location":"Inhibitory%20Control%20Network/","title":"Inhibitory Control Network","text":"<p>title: Inhibitory Control Network</p> <p>tags: brain, psychology</p>"},{"location":"Inhibitory%20Control%20Network/#inhibitory-control-network","title":"Inhibitory Control Network","text":"<ul> <li>Brain areas related to response inhibition ability</li> <li>inferior Frontal Gyri and Medial Frontal Gyri, the Opercular Cingulate, Insular Cingulate, Orbital Posterior Cingulate and Posterior Parietal Cortex</li> </ul>"},{"location":"Initialization/","title":"Initialization","text":"<ul> <li>Xavier Initialization , He Initialization , LeCun Init</li> </ul>"},{"location":"Instance%20Normalization/","title":"Instance Normalization","text":"<ul> <li>Contrast Normalization</li> <li> \\[     y_{tijk} = \\frac{x_{tijk} - \\mu_{ti}}{\\sqrt{\\sigma_{ti}^2 + \\epsilon}},     \\quad     \\mu_{ti} = \\frac{1}{HW}\\sum_{l=1}^W \\sum_{m=1}^H x_{tilm},     \\quad     \\sigma_{ti}^2 = \\frac{1}{HW}\\sum_{l=1}^W \\sum_{m=1}^H (x_{tilm} - mu_{ti})^2 \\] </li> <li>This prevents instance-specific mean and covariance shift simplifying the learning process.</li> <li>Intuitively, the normalization process allows to remove instance-specific contrast information from the content image in a task like image stylization, which simplifies generation.</li> <li></li> </ul>"},{"location":"Instance%20Normalization/#backlinks","title":"Backlinks","text":"<ul> <li>AdaIn</li> <li> <p>Adaptive Instance Normalization\u00a0is a normalization method that aligns the mean and variance of the content features with those of the style features.</p> </li> <li> <p>CycleGAN</p> </li> <li>Instance Normalization</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Instance-based%20Learning/","title":"Instance-based Learning","text":""},{"location":"Instance-based%20Learning/#instance-based-learning","title":"Instance-based Learning","text":"<ul> <li>an object category is represented by a set of known instances a nearest neighbor classifier is used</li> <li>IBL considers category learning as a process of learning about the instances of the category:</li> <li>The training phase is very fast</li> <li>IBL can recognize objects using a very small number of experiences IBL is a baseline approach to evaluate object representations Simple and easy to implement</li> <li>Memory usage in instance-based systems is continuously growing. Computational complexity grows with the number of training instances</li> <li>The computational complexity of classifying a single new instance is O(n), where n is number of instances stored in perceptual memory.</li> <li>Salience and forgetting mechanisms can be used to bound the memory usage which are also useful for reducing the risk of overfitting to noise in the training set.</li> <li>Overfitting</li> <li>Sensitive to noise</li> <li></li> </ul>"},{"location":"Instance-based%20Learning/#backlinks","title":"Backlinks","text":"<ul> <li>Instance-based Learning</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Instant%20NeRF/","title":"Instant NeRF","text":"<ul> <li>Instant Neural Graphics Primitives with a Multiresolution Hash Encoding</li> <li>Neural Radiance Field</li> <li>Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate</li> <li>rely on task specific data structures</li> <li>new input encoding that permits the use of a smaller network without sacrificing quality</li> <li>educing the number of floating point and memory access operations</li> <li>near-instant training of neural graphics primitives on a single GPU for multiple tasks</li> <li>small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through Gradient Descent gradients</li> <li>automatically focuses on relevant detail, independent of task at hand</li> <li>low overhead</li> <li>In a gigapixel image, they represent an image by a neural network. SDF learns a signed distance function in 3D space whose zero level-set represents a 2D surface</li> <li>2D images and their camera poses to reconstruct a volumetric radiance-and-density field that is visualized using ray marching.</li> <li>neural volume learns a denoised radiance and density field directly from a volumetric path tracer.</li> <li>only vary the hash table size which trades off quality and performance</li> <li>disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern GPUs</li> <li>parallelism</li> <li>fully-fused Operator Fusion CUDA kernels with a focus on minimizing wasted bandwidth and compute operations</li> </ul>"},{"location":"Instruction%20Bandwidth/","title":"Instruction Bandwidth","text":"<ul> <li>Bandwidth is the maximum amount of data that can travel through a 'channel'</li> </ul>"},{"location":"Instruction%20Cycle/","title":"Instruction Cycle","text":"<ul> <li>The time it takes for a robot controller system's cycle to decode a command or instruction before it is executed. The Instruction Cycle must be analyzed very closely by robotic programmers to enable speedy and proper reaction to varying commands.</li> </ul>"},{"location":"Instruction%20Latency/","title":"Instruction Latency","text":"<ul> <li>Amount of time to complete a task(time , seconds)</li> <li>function of how long it takes the data to get sent all the way from the start point to the end</li> </ul>"},{"location":"Instruction%20Pipelining/","title":"Instruction Pipelining","text":"<ul> <li>used in the design of modern microprocessors, microcontrollers and CPUs to increase their Instruction Throughput for the entire workload</li> <li>divide the processing of a CPU instruction into a series of independent steps o microinstructions with storage at the end of each step.</li> <li>This allows the CPUs control logic to handle instructions at the processing rate of the slowest step, which is much faster than the time needed to process the instruction as a single step</li> <li>IF: Instruction Fetch</li> <li>ID: Instruction Decode, register fetch</li> <li>EX: Execution</li> <li>MEM: Memory Access</li> <li>WB: Register write Back</li> </ul>"},{"location":"Instruction%20Throughput/","title":"Instruction Throughput","text":"<ul> <li>the number of instructions that can be executed in a unit of time</li> <li>(Jobs/Hour)</li> <li>how much data actually does travel through the 'channel' successfully</li> </ul>"},{"location":"Instruction%20level%20programming/","title":"Instruction Level Programming","text":"<ul> <li>ILP</li> <li>allows the compiler and the processor to overlap the execution of multiple instructions or even to change the order in which instructions are executed</li> <li>Instruction Pipelining</li> <li>SuperScalar</li> <li>Out of Order Execution</li> <li>Register Renaming</li> <li>Speculative Execution</li> <li>Branch Prediction</li> </ul>"},{"location":"Insula/","title":"Insula","text":"<ul> <li>Sometimes referred to as the insular cortex, this small region of the cerebrum is found deep within the lateral sulcus, and is believed to be involved in consciousness, emotion, and keeping the body in balance.</li> </ul>"},{"location":"Integral%20Lines/","title":"Integral Lines","text":"<ul> <li>Seed particles in a flow field  </li> <li>Let the particles move in the flow field</li> <li>Compute and show the trajectories</li> <li></li> <li></li> <li>Streamlines</li> <li>Pathlines</li> </ul>"},{"location":"Integrated%20Gradients/","title":"Integrated Gradients","text":""},{"location":"Integrated%20Gradients/#backlinks","title":"Backlinks","text":"<ul> <li>Conductance</li> <li>This paper introduces the concept of conductance as a way to understand the importance of hidden units in deep networks. Conductance is defined as the flow of Integrated Gradients' attribution via a hidden unit, and is used to understand the importance of a hidden unit to the prediction for a specific input or over a set of inputs. The effectiveness of conductance is evaluated in multiple ways, including theoretical properties, ablation studies, and a feature selection task using the Inception network over ImageNet data and a sentiment analysis network over reviews. The properties of conductance include completeness, linearity and insensitivity to variations in inputs or hidden unit values. The paper also discusses the issue of saturation in neural networks, where the gradient of the output with respect to the input can be near-zero, and how conductance addresses this issue. The authors also compare conductance with other methods of understanding hidden unit importance and find it to be more intuitive and accurate.</li> <li>Informally, the conductance of a hidden unit of a deep network is the flow of Integrated Gradients' attribution via this hidden unit</li> <li>The key idea behind conductance is to decompose the computation of Integrated Gradients via the chain rule</li> <li>Integrated Gradients produces attributions for base features</li> <li>Integrated Gradients is path integral of gradient along straightline path from baseline \\(x'\\) to input \\(x\\). The function F varies from a near zero value for the informationless baseline to its final value. The gradients of F with respect to the image pixels explain each step of the variation in the value of F</li> <li> <p>As an artificial example, suppose the network first transforms the input x linearly to y = 2x, and then transforms it to z = max(y, 1). Suppose the input is x = 1 (where z is saturated at value 1), with 0 being the baseline. Then for the hidden unit of y, gradient of z w.r.t. y is 0. Gradient*activation would be 0 for y, which does not reflect the intuitive importance of y. Like in Integrated Gradients, in computing conductance, we consider all extrapolated inputs for x between 0 and 1, and look at the gradients of output w.r.t. y at these points. This takes the non-saturated region into account, and ends up attributing 1 to y, as desired.</p> </li> <li> <p>Vision Explainibility</p> </li> <li> <p>Integrated Gradients</p> </li> <li> <p>Smooth-Grad</p> </li> <li>reduces visual noise and, hence, improves visual explanations about how a DNN is making a classification decision. Comparing their work to several gradient-based sensitivity map methods such as LRP, DeepLift, and Integrated Gradients (IG) [96], which estimate the global importance of each pixel and create saliency maps, showed that Smooth-Grad focuses on local sensitivity and calculates averaging maps with a smoothing effect made from several small perturbations of an input image. The effect is enhanced by further training with these noisy images and finally having an impact on the quality of sensitivity maps by sharpening them.</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Inter-rater%20Agreement/","title":"Inter-rater Agreement","text":"<ul> <li>A measurement of how often human raters agree when doing a task. If raters disagree, the task instructions may need to be improved. Also sometimes called inter-annotator agreement or inter-rater reliability.</li> </ul>"},{"location":"Interneuron/","title":"Interneuron","text":"<ul> <li>Association neuron</li> <li>impulse moves between sensory and motor neurons</li> <li>mostly multipolar</li> </ul>"},{"location":"Interpolation/","title":"Interpolation","text":"<ul> <li>1D piecewise linear interpolation</li> <li>Bilinear Interpolation</li> <li>Barycentric Interpolation</li> </ul>"},{"location":"Interpretability%20vs%20Neuroscience/","title":"Interpretability Vs Neuroscience","text":"<ul> <li>Interpretability vs Neuroscience (rough note) -- colah's blog%20--%20colah's%20blog)%20--%20colah's%20blog)</li> </ul>"},{"location":"Interpretability%20vs%20Neuroscience/#you-can-get-the-responses-of-all-neurons-for-arbitrarily-many-stimuli","title":"You Can Get the Responses of All Neurons for Arbitrarily Many Stimuli","text":"<ul> <li>In neuroscience, one is limited in the number of neurons they can record from, their ability to select the neurons they record, and the number of stimuli they can record responses to.</li> <li>For artificial neural networks, we can record the responses of all neurons to arbitrarily many stimuli</li> <li>Turn arounds are much faster than biological experiments</li> <li>There's no recording noise, No synaptic fatigue.</li> </ul>"},{"location":"Interpretability%20vs%20Neuroscience/#not-only-do-you-have-the-connectome-you-have-the-weights","title":"Not Only Do You Have the Connectome, You Have the Weights!","text":"<ul> <li>A major undertaking in neuroscience is the attempt to access the connectome</li> <li>Even if they succeed, they won\u2019t know the weights of those connections</li> <li>With artificial neural networks, all the connections and weights are simply there for us to look at.</li> <li>And since we also know how these artificial neurons are computed, in principle we have everything we need to just reason through and understand the neural network.</li> </ul>"},{"location":"Interpretability%20vs%20Neuroscience/#weight-tying-massively-reduces-the-number-of-unique-neurons","title":"Weight-tying Massively Reduces the Number of Unique Neurons!","text":"<ul> <li>weight-tying,</li> <li>force many neurons to have the same weights</li> <li>he most common use of this is in convolutional neural networks, where each neuron has translated copies of itself with the same weights.</li> <li>in ImageNet conv nets, weight-tying often reduces the number of unique neurons in early vision by 10,000x or even more</li> <li>This results in artificial neural networks having many fewer neurons for early vision than their biological counterparts</li> <li>This means we can just literally study every single neuron.</li> </ul>"},{"location":"Interpretability%20vs%20Neuroscience/#establishing-causality-by-optimizing-the-input","title":"Establishing Causality by Optimizing the Input","text":"<ul> <li>one of the thorniest issues in understanding neurons in artificial networks is separating correlation from causation.</li> <li>Does a neuron detect a dog head? Or does it just detect part of a dog head?</li> <li>There's a second very closely related problem: we don't know what the space of likely functions a neuron might perform is.</li> <li>this is also a challenge in neuroscience.</li> <li>We create stimuli \u201cfrom scratch\u201d to strongly activate neurons (or combinations of neurons) in artificial neural networks, by starting with random noise and optimizing the input.</li> <li>The key property of feature visualization is that anything in the resulting visualization there because it caused the neuron to fire more</li> <li>If feature visualization gives you a fully formed dog head with eyes and ears arranged appropriately, it must be detecting an entire dog head</li> <li>If it just gives an eye, it's probably only (or at least primarily) responding to that.</li> <li>Recent efforts in neuroscience have tried to develop similar methods [], by using an artificial neural network as a proxy for a biological one.</li> <li>unclear they give you the same ability to establish a causal link.</li> <li>It seems hard to exclude the possibility that the resulting stimulus might have content which causes the artificial neurons predicting the biological neuron to fire more, but aren't causally necessary for the biological neuron to fire.</li> </ul>"},{"location":"Interpretability%20vs%20Neuroscience/#interventions-ablations-and-edits","title":"Interventions, Ablations, and Edits","text":"<ul> <li>Optogenetics has been a major methodological advance for neuroscience in allowing neuroscientists to temporarily ablate neurons, or to force them to activate.</li> <li>Artificial neural networks are trivial to manipulate at the level of neurons</li> <li>One can easily ablate neurons or set them to particular activations</li> <li>But one can also do more powerful \"circuit editing\" where one modifies parameters at a finer grained level.</li> <li>In image generation, Bau et al., 2018 show that you can ablate neurons to remove objects like tress and windows from generated images</li> <li>In RL, Hilton et al., 2020 show that you can ablate features to blind an agent to a particular enemy while leaving other competencies in tact</li> <li>More recently, Cammarata et al, 2021 reimplements a large chunk of neural network from scratch, and then splices it into a model.</li> </ul>"},{"location":"Interpretability%20vs%20Neuroscience/#we-can-study-the-exact-same-model","title":"We Can Study the Exact Same Model.","text":"<ul> <li>Neuroscientists might study a model organism species, but each brain they study has different neurons</li> <li>If one neuroscientist reports on an interesting neuron they found, other neuroscientists can't directly study that same neuron</li> <li>In fact, the neuroscientists studying the original neuron will quickly lose access to it: probes can't be left in indefinitely, organisms die, human subjects leave, and even setting that aside neurons change over time.</li> <li>Studying artificial networks, we can collaboratively reverse engineer the same \u201cbrain\", building on each other.</li> <li>we have a shared web of thousands of \"footholds\" into InceptionV1, consisting of neurons we understand fairly well and know the connections between, which makes it massively easier to explore</li> </ul>"},{"location":"Interpretation%20of%20Neural%20networks%20is%20fragile/","title":"Interpretation of Neural Networks is Fragile","text":"<ul> <li>Ghorbani et al</li> </ul>"},{"location":"Interpretation%20of%20Neural%20networks%20is%20fragile/#backlinks","title":"Backlinks","text":"<ul> <li>Vision Explainibility</li> <li>Interpretation of Neural networks is fragile</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Interpreting%20Attention/","title":"Interpreting Attention","text":"<ul> <li>Attention Interpretability Across NLP Tasks</li> <li>empirically prove the hypothesis that attention weights are interpretable and are correlated with feature importance measures</li> <li>n both single and pair sequence tasks, the attention weights in samples with original weights do make sense in general</li> <li>However, in the former case, the attention mechanism learns to give higher weights to tokens relevant to both kinds of sentiment.</li> <li>They show that attention weights in single sequence tasks do not provide a reason for the prediction, which in the case of pairwise tasks, attention do reflect the reasoning behind model output</li> <li>BertViz repo</li> </ul>"},{"location":"Interpretive%20Labor/","title":"Interpretive Labor","text":"<ul> <li>There\u2019s a tradeoff between the energy put into explaining an idea, and the energy needed to understand it.</li> <li>On one extreme, the explainer can painstakingly craft a beautiful explanation, leading their audience to understanding without even realizing it could have been difficult</li> <li>On the other extreme, the explainer can do the absolute minimum and abandon their audience to struggle.</li> <li>That is, really outstanding tutorials, reviews, textbooks, and so on.</li> <li>we often have a group of researchers all trying to understand each other</li> <li>Just like before, the cost of explaining stays constant as the group grows, but the cost of understanding increases with each new member</li> <li>At some size, the effort to understand everyone else becomes too much.</li> <li>As a defense mechanism, people specialize, focusing on a narrower area of interest.</li> <li>The maintainable size of the field is controlled by how its members trade off the energy between communicating and understanding.</li> <li>Research debt is the accumulation of missing interpretive labor.</li> <li>It\u2019s extremely natural for young ideas to go through a stage of debt, like early prototypes in engineering.</li> <li>The problem is that we often stop at that point.</li> <li>Young ideas aren\u2019t ending points for us to put in a paper and abandon.</li> <li>When we let things stop there the debt piles up.</li> <li>It becomes harder to understand and build on each other\u2019s work and the field fragments.</li> </ul>"},{"location":"Intra%20cluster%20variance/","title":"Intra Cluster Variance","text":"<ul> <li>$\\(J = \\Sigma_{j=1}^K \\Sigma_{x \\in S_j} ||x - \\mu_j||^2\\)</li> <li>Measure of how much the points in a given cluster spread</li> </ul>"},{"location":"Intra-Class%20Part%20Swapping/","title":"Intra-Class Part Swapping","text":"<ul> <li>replaces most attentive regions of one image by the other</li> <li>Attentive regions are extracted using a classification activation map (CAM), thresholded for the most prominent region</li> <li>The attentive region in the source image is scaled and translated according to the attentive region of the target image for region replacement</li> <li>The label information of the output is similar to the target image as this approach relies on augmenting similar class images.</li> </ul>"},{"location":"Intra-Class%20Part%20Swapping/#mastersthesis","title":"mastersthesis","text":""},{"location":"Intravenous/","title":"Intravenous","text":"<ul> <li>Administration of medication or fluids by vein</li> </ul>"},{"location":"Intubation/","title":"Intubation","text":"<ul> <li>Medical insertion of a tube into the body, for example, into the throat to assist with breathing</li> </ul>"},{"location":"Intuitive%20Color%20spaces/","title":"Intuitive Color Spaces","text":""},{"location":"Invariant%20Distribution/","title":"Invariant Distribution","text":"<ul> <li>If g is the PDF. T(x|y) is the PDF of the transition kernel. Homogenous Markov Chain. Then g is PDF of an invariant distribution of T(x|y) if</li> <li> \\[g(x) = \\int_{\\mathbb{R}^{k}}T(x|y)g(y)dy\\] </li> <li>Atleast one invariant distribution</li> <li>Ergodic</li> </ul>"},{"location":"Inverse%20Kinematics/","title":"Inverse Kinematics","text":"<ul> <li>which joint movements (q) are needed achieve a particular robot end effector pose (\u03be)  </li> <li>q = \u03ba\u22121(\u03be) : q = {qi,i \u2208 [1,\u2026,n]}</li> </ul>"},{"location":"Inverse%20Reinforcement%20Learning/","title":"Inverse Reinforcement Learning","text":"<ul> <li>Basically, IRL is about studying from humans.</li> <li>Inverse reinforcement learning is the sphere of studying an agent\u2019s objectives, values, or rewards with the aid of using insights of its behavior.</li> <li>We can fit a reward function with the use of professional demonstrations. Once a reward feature is fitted, we are able to use Policy Gradient, Model-based RL or different RL to locate the ideal policy.</li> <li>For example, we are able to compute the policy gradient with the use of the reward feature as opposed to sampled rewards. With the policy gradient calculated, we optimize the policy closer to the finest rewards gain.</li> <li>As part of the IRL, the task is to collect a set of human-generated driving data and extract an approximation of that human's reward function for the task. Of course, this approximation necessarily relates to a simplified driving model.</li> <li>As Ng and Russell put it, \"the reward function, rather than the guideline, is the most concise, robust, and transferable definition of the task\" because it quantifies how good or bad certain actions are. Once we have the right reward function, the problem is finding the right guideline and can be solved using standard reinforcement learning methods.</li> <li>For our autonomous car example, we would use human driving data to automatically learn the correct functional weights for the reward. Since the task is fully described by the reward function, we don't even need to know the details of human politics as long as we have the right reward function to optimize.</li> </ul>"},{"location":"Inverse%20Square%20Law/","title":"Inverse Square Law","text":"<ul> <li>for force on a charge in an electric field of another charge: Force is proportional to the product of the charges and inversely proportional to the square of the distance between them</li> <li> \\[F_{E}= \\frac{\\frac{1}{4pe_{0}}Q_{1}Q_{2}}{r^{2}}\\] </li> <li>force on a mass in a gravitational field of another mass: Force is proportional to the product of the masses and inversely proportional to the square of the distance between them</li> <li> \\[F_{G}= -G \\frac{m_{1}m_{2}}{r^{2}}\\] </li> </ul>"},{"location":"Ion%20Channel/","title":"Ion Channel","text":"<ul> <li>A pore in the membrane of a neuron that allows ions to pass through, helping to shape action potentials.</li> </ul>"},{"location":"Isolating%20words/","title":"Isolating Words","text":"<ul> <li>words do not divide into smaller units</li> </ul>"},{"location":"Isoline/","title":"Isoline","text":"<ul> <li>2D</li> <li>Contour line</li> <li> \\[\\{(x,y|f(x,y)=c\\}\\] </li> <li>Curve along function has constant value c</li> <li></li> <li>Marching Squares</li> <li></li> <li>No isoline inside cells with same signs</li> <li>only consider cells with different signs</li> <li>access look-up table for respective case</li> </ul>"},{"location":"Isosurface/","title":"Isosurface","text":"<ul> <li>Marching Cubes</li> <li>Marching Tetrahedra</li> <li>Fractional Anisotropy</li> <li></li> </ul>"},{"location":"Isotropic%20Architectures/","title":"Isotropic Architectures","text":"<ul> <li>(of an object or substance) having a physical property which has the same value when measured in different directions.</li> <li>And precisely that is what an isotropic architecture is. Isotropic architectures do not produce pyramid shaped data transformations, but rather\u00a0fixed\u00a0ones where data does not change in shape and size</li> <li>In other (simpler) words, when you take a look at the value going through an\u00a0isotropic\u00a0network, it doesn't change in size.</li> <li>Like Transformer , MLP-Mixer</li> </ul>"},{"location":"Issues/","title":"Issues","text":"<ul> <li>Multiple Local Minima</li> <li>Saddle Points</li> <li>Vanishingexploding gradients</li> <li>Image Data</li> <li>Fitting</li> <li>Freedom</li> <li>Bias Variance Dilemma</li> <li>Complex Geometry</li> <li>Lack of information</li> </ul>"},{"location":"Iterative%20Closest%20Point/","title":"Iterative Closest Point","text":""},{"location":"Iterative%20Closest%20Point/#iterative-closest-point","title":"Iterative Closest Point","text":"<ul> <li>Start from initial guess</li> <li>Iterate</li> <li>For each point on M, find closest point on P</li> <li>Find best transform for this correspondence Transform M</li> <li>Good initial guess -&gt; Converges to global minimum</li> <li>The ICP is applicable when we have a relatively good starting point in advance.</li> <li>Otherwise, it will be trapped into the first local minimum and the solution will be useless.</li> <li>Without pose information, ICP-based approaches are unable to recover the proper transformations because of the ambiguity in surface matching.</li> <li></li> </ul>"},{"location":"Iterative%20Closest%20Point/#backlinks","title":"Backlinks","text":"<ul> <li>Iterative Closest Point</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Jaccard%20Distance/","title":"Jaccard Distance","text":"<ul> <li> \\[D(x,y) = 1- \\frac{x\\cup y}{x\\cap y}\\] </li> <li>The Jaccard index (or Intersection over Union) is a metric used to calculate the similarity and diversity of sample sets. It is the size of the intersection divided by the size of the union of the sample sets.</li> <li>In practice, it is the total number of similar entities between sets divided by the total number of entities.</li> <li>To calculate the Jaccard distance we simply subtract the Jaccard index from 1</li> <li>highly influenced by the size of the dat</li> <li>Large datasets can have a big impact on the index as it could significantly increase the union whilst keeping the intersection similar</li> <li>The Jaccard index is often used in applications where binary or binarized data are used</li> <li>deep learning model predicting segments of an image</li> <li>text similarity analysis to measure how much word choice overlap there is between documents</li> </ul>"},{"location":"Joint%20Factor%20Analysis/","title":"Joint Factor Analysis","text":"<ul> <li>Front-end Factor Analysis for Speaker Verification</li> <li>Joint Factor Analysis (JFA)</li> <li>eature extractor to learn a low-dimensional speaker representation for speaker verification, which is also used to model session and channel effects/variabilities</li> <li>In this new space, a given speech utterance is represented by a new vector named total factors (called the identity-vector or the \u201ci-vector\u201d)</li> <li>The i-vector is thus a feature that represents the characteristics of the frame-level features\u2019 distributive pattern</li> <li>dimensionality reduction of the GMM supervector (although the GMM supervector is not extracted when computing the i-vector)</li> <li>extracted in a similar manner with the eigenvoice adaptation scheme or the JFA technique</li> <li>extracted per sentence</li> <li>Support-Vector-Machine-based system that uses the cosine kernel to estimate the similarity between the input data</li> <li>cosine similarity as the final decision score</li> <li>removed the SVM from the decision proces</li> <li>no speaker enrollment</li> <li>EER</li> <li>MinDCF</li> <li>NIST 2008 Speaker Recognition Evaluation dataset</li> <li>Up until d-vectors, the state-of-the-art speaker verification systems were based on the concept of i-vectors</li> </ul>"},{"location":"Joint%20Interpolated%20Motion/","title":"Joint Interpolated Motion","text":"<ul> <li>A method of coordinating the movement of the joints, such that all joints arrive at the desired location simultaneously. This method of servo control produces a predictable path regardless of speed and results in the fastest pick and place cycle time for a particular move.</li> </ul>"},{"location":"Joint%20Motion%20Type/","title":"Joint Motion Type","text":"<ul> <li>Also known as Point-to-Point Motion, Joint Motion Type is a method of path interpolation that commands the movement of the robot by moving each joint directly to the commanded position so that all axis arrive to the position at the same time. Although the path is predictable, it will not be linear.</li> </ul>"},{"location":"Joint%20Space/","title":"Joint Space","text":"<ul> <li>a. Joint Space (or Joint Coordinates) is just a method of defining the position of the robot in terms of the value of each axis instead of as a TCP position. For example, the Home Position of a robot is often defined in Joint Space as each axis being at 0 degrees.</li> <li>b. The set of joint positions.</li> </ul>"},{"location":"Joint%20Velocity/","title":"Joint Velocity","text":""},{"location":"Joint%20Velocity/#joint-velocity","title":"Joint Velocity","text":"<ul> <li>Joint space trajectory is generally smoother than task space trajectory</li> <li></li> </ul>"},{"location":"Joint%20Velocity/#backlinks","title":"Backlinks","text":"<ul> <li>Joint Velocity</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"KL%20Divergence/","title":"KL Divergence","text":"<ul> <li>Classification</li> <li>Entropy + Cross Entropy</li> <li>Distribution Based metric</li> <li>Measures difference between two PDF</li> <li>We first define xlogx for a weird edge case \\(\\(x \\cdot \\log\\left( x \\right)\\)\\)</li> </ul> <p>Then entropy \\(\\(\\mathrm{sum}\\left( \\mathrm{xlogx}\\left( y \\right) \\right) \\cdot \\mathrm{//}\\left( 1, \\mathrm{size}\\left( y, 2 \\right) \\right)\\)\\)</p> <p>Then cce as defined before $$ - \\mathrm{sum}\\left( y \\cdot \\log\\left( \u0177 \\right) \\right)$$</p> <p>Finally KLD \\(\\(entropy + crossentropyloss\\)\\)</p> <ul> <li> \\[KL(p,q) = \\Sigma_x p(x) log\\frac{p(x)}{q(x)}\\] </li> </ul>"},{"location":"KL%20Divergence/#backlinks","title":"Backlinks","text":"<ul> <li>Stack GAN</li> <li>KL Divergence between the standard Gaussian distribution and the conditioning Gaussian distribution</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"KMeans/","title":"K Means","text":"<ul> <li>Codebook vectors. No manifolds.</li> <li>Given: \\(\\((x_i)_{i= 1,..,N} \\in \\mathbb{R}^n\\)\\)</li> <li>Need : K clusters \\(\\(C_1 , \u2026 , C_K\\)\\)</li> <li>Randomly assign training points to K sets : \\(\\(S_j (j = 1, \u2026, K)\\)\\)</li> <li>Repeat:<ul> <li>For each set (\\(S_j\\)\\)<ul> <li>Mean \\(\\(\\mu_j = |S_j|^{-1} \\Sigma_{x \\in S_j} x\\)\\)</li> <li>Create new sets by putting points into set where \\(||x_i-\\mu_j||\\) is minimal</li> <li>If empty, dismiss and reduce K to K'</li> </ul> </li> </ul> </li> <li>Error quantity does not increase<ul> <li>Intra cluster variance</li> <li>Clusters are bounded by line Decision Boundaries and forms a Voronoi Cell</li> </ul> </li> <li>Does not work for curved boundaries</li> </ul>"},{"location":"KMeans/#codebook-vector","title":"Codebook Vector","text":"<ul> <li>Each cluster represented by it</li> <li>Vector pointing to the mean of all vectors in the cluster</li> <li>Center of Gravity</li> </ul>"},{"location":"Kalman%20Filter/","title":"Kalman Filter","text":""},{"location":"Kalman%20Filter/#kalman-filter","title":"Kalman Filter","text":"<ul> <li>The standard Kalman filter is the optimum estimator when your system is linear and the system noise is Gaussian.</li> <li>linear systems with Gaussian noise</li> </ul>"},{"location":"Kalman%20Filter/#backlinks","title":"Backlinks","text":"<ul> <li>Kalman Filter</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"KeepAugment/","title":"KeepAugment","text":"<ul> <li>Keep- Augment identifies the salient area in an image and assures the image generated by the augmenta- tion strategies, for example, Cutout, RandAugment [14], CutMix [82] or AutoAugment [13], contains salient region in it.</li> </ul>"},{"location":"Kernel%20Filters/","title":"Kernel Filters","text":"<ul> <li>sharpen and blur images</li> <li>These filters work by sliding an n \u00d7 n matrix across an image with either a Gaussian blur filter, which will result in a blurrier image, or a high contrast vertical or horizontal edge filter which will result in a sharper image along edges</li> <li>Intuitively, blurring images for Data Augmentation could lead to higher resistance to motion blur during testing</li> <li>Additionally, sharpening images for Data Augmentation could result in encapsulating more details about objects of interest.</li> <li>Kang et al. experiment with a unique kernel filter that randomly swaps the pixel values in an n\u00d7n sliding window. They call this augmentation technique PatchShuffle Regularization</li> </ul>"},{"location":"Kernel%20Support%20Vector%20Machines%20%28KSVMs%29/","title":"Kernel Support Vector Machines (KSVMs)","text":"<ul> <li>A classification algorithm that seeks to maximize the margin between positive and negative classes by mapping input data vectors to a higher dimensional space. For example, consider a classification problem in which the input dataset has a hundred features. To maximize the margin between positive and negative classes, a KSVM could internally map those features into a million-dimension space. KSVMs uses a loss function called hinge loss.</li> </ul>"},{"location":"Kernel%20Support%20Vector%20Machines%20%28KSVMs%29/#backlinks","title":"Backlinks","text":"<ul> <li>Generative vs Discriminative Models</li> <li>[[SVM]]</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Ketamine/","title":"Ketamine","text":"<ul> <li>A powerful anesthetic drug, originally manufactured for veterinary use, that has been shown to be an effective treatment for major depressive disorder, especially in patients who do not respond well to traditional antidepressant medications.</li> </ul>"},{"location":"Kinesthetic%20Teaching/","title":"Kinesthetic Teaching","text":"<ul> <li>H. Kasaei et al., \u201cInteractive open-ended object, affordance and grasp learning for robotic manipulation.\u201d ICRA 2019.</li> <li>Formulate object grasping as a supervised learning problem,  </li> <li>An appropriate grasp configuration can be learned from human demonstrations</li> <li>Primary assumption -&gt; familiar objects can be grasped in a similar way.</li> <li>Heightmaps Kinesthetic</li> <li>Familar Object Grasping Object Viiew recog</li> </ul>"},{"location":"Kinetic%20Energy/","title":"Kinetic Energy","text":"<ul> <li>half x mass x (velocity squared)</li> <li> \\[E_{K}= \\frac{1}{2}mv^{2}\\] </li> </ul>"},{"location":"Kinetic%20Friction/","title":"Kinetic Friction","text":"<ul> <li> \\[F_{k} \\leq \\mu _{k}F_{N}\\] </li> <li>\\(F_{k}\\) is kinetic friction</li> <li>\\(F_{N}\\) is normal force</li> <li>\\(\\mu _s\\) is coefficient of friction</li> </ul>"},{"location":"Knowledge%20Component/","title":"Knowledge Component","text":"<ul> <li>A knowledge component can be a principle, a concept, a rule, a procedure, a fact, an association or any other fragment of task-specific information.</li> <li>Despite the connotations of knowledge, a knowledge component can be incorrect, in that instructors would rather that students not apply this knowledge component while achieving a task.</li> </ul>"},{"location":"Knowledge%20Distillation%20Survey%202021/","title":"Knowledge Distillation Survey 2021","text":"<ul> <li>model compression and acceleration techniques</li> <li>Low-rank factorization</li> <li>Transferred compact convolutional filters</li> <li>To address this issue, Bucilua et al. (2006) first proposed model compression to transfer the information from a large model or an ensem- ble of models into training a small model without a significant drop in accuracy. The knowledge transfer between a fully-supervised teacher model and a stu- dent model using the unlabeled data is also intro- duced for semi-supervised learning (Urner et al., 2011).</li> <li>The learning of a small model from a large model is later formally popularized as knowledge distilla- tion (Hinton et al., 2015). In knowledge distillation, a small student model is generally supervised by a large teacher model (Bucilua et al., 2006; Ba and Caruana, 2014; Hinton et al., 2015; Urban et al., 2017).</li> <li>The main idea is that the student model mimics the teacher model in order to obtain a competitive or even a superior performance. The key problem is how to transfer the knowledge from a large teacher model to a small student model. Basically, a knowledge distillation system is composed of three key components: knowledge, dis- tillation algorithm, and teacher-student architecture.</li> <li>Successful distillation relies on data geometry, optimization bias of distillation objective and strong monotonicity of the student classifier</li> <li>quantified the extraction of visual concepts from the intermediate layers of a deep neural network, to explain knowledge distillation (Cheng et al., 2020). Ji &amp; Zhu theoretically explained knowledge distillation on a wide neural network from the respective of risk bound, data efficiency and imperfect teacher (Ji and Zhu., 2020).</li> <li>Knowledge distillation has also been explored for label smoothing, for assessing the accuracy of the teacher and for obtaining a prior for the optimal output layer geometry (Tang et al., 2020).</li> <li>Furthermore, the knowledge transfer from one model to another in knowledge distillation can be extended to other tasks, such as adversar- ial attacks (Papernot et al., 2016), data augmenta- tion (Lee et al., 2019a; Gordon and Duh, 2019), data privacy and security (Wang et al., 2019a).</li> <li>A vanilla knowledge distillation uses the logits of a large deep model as the teacher knowledge (Hinton et al., 2015; Kim et al., 2018; Ba and Caruana, 2014; Mirzadeh et al., 2020)</li> <li>Further- more, the parameters of the teacher model (or the connections between layers) also contain another knowl- edge (Liu et al., 2019c)</li> <li>Response Based Knowledge</li> <li>Feature Based Knowledge</li> </ul>"},{"location":"Knowledge%20Distillation/","title":"Knowledge Distillation","text":"<ul> <li>Teacher model to help train the student model</li> <li>Teacher is often pre trained</li> <li>Student tries to imitate teacher</li> <li>Distillation Loss</li> <li>Knowledge Distillation Survey 2021</li> <li>Distilling the Knowledge in a Neural Network</li> </ul>"},{"location":"Kvasir%20Dataset/","title":"Kvasir Dataset","text":"<ul> <li>Simula Datasets - Kvasir #Roam-Highlights</li> <li>dataset containing images from inside the gastrointestinal (GI) tract</li> <li>The collection of images are classified into three important anatomical landmarks and three clinically significant findings.</li> <li>two categories of images related to endoscopic polyp removal</li> <li>The dataset consist of the images with different resolution from 720x576 up to 1920x1072 pixels</li> <li>Some of the included classes of images have a green picture in picture illustrating the position and configuration of the endoscope inside the bowel, by use of an electromagnetic imaging system</li> </ul>"},{"location":"LASER/","title":"LASER","text":"<ul> <li>Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond</li> <li>joint multilingual sentence representations</li> <li>LASER</li> <li>Language-Agnostic SEntence Representations</li> <li>93 languages, belonging to more than 30 different families and written in 28 different scripts</li> <li>universal language agnostic sentence embeddings</li> <li>train a single encoder to handle multiple languages, so that semantically similar sentences in different languages are close in the embedding space</li> <li>single BiLSTM encoder with a shared BPE vocabulary for all languages</li> <li>coupled with an auxiliary decoder and trained on publicly available parallel corpora</li> <li>learn a classifier on top of the resulting embeddings using English annotated data only, and transfer it to any of the 93 languages without any modification</li> <li>XNLI</li> <li>MLDoc</li> <li>BUCC</li> <li>test set of aligned sentences in 112 languages</li> </ul>"},{"location":"LASER/#laser_1","title":"Laser","text":"<ul> <li>Acronym for Light Amplification by Stimulated Emission of Radiation. A device that produces a coherent monochromatic beam of light which is extremely narrow and focused but still within the visible light spectrum. This is commonly used as a non-contact sensor for robots. Robotic applications include: distance finding, identifying accurate locations, surface mapping, bar code scanning, cutting, welding etc.</li> </ul>"},{"location":"LDA/","title":"LDA","text":""},{"location":"LDA/#steps","title":"Steps","text":"<ul> <li>Compute the\u00a0dd-dimensional mean vectors for the different classes from the dataset.</li> <li>Compute the scatter matrices (in-between-class and within-class scatter matrix).</li> <li>Compute the eigenvectors \\((e_1,e_2,...,e_de_1,e_2,...,e_d)\\) and corresponding eigenvalues \\((\u03bb_1,\u03bb_2,...,\u03bb_d\u03bb_1,\u03bb_2,...,\u03bb_d)\\) for the scatter matrices.</li> <li>Sort the eigenvectors by decreasing eigenvalues and choose\u00a0\\(k\\)\u00a0eigenvectors with the largest eigenvalues to form a\u00a0\\(d\u00d7k\\)\u00a0dimensional matrix\u00a0\\(W\\)\u00a0(where every column represents an eigenvector).</li> <li>Use this\u00a0\\(d\u00d7k\\)\u00a0eigenvector matrix to transform the samples onto the new subspace. This can be summarized by the matrix multiplication:\u00a0\\(Y=X\u00d7WY=X\u00d7W\\)\u00a0(where\u00a0\\(X\\)\u00a0is a\u00a0\\(n\u00d7d\\)-dimensional matrix representing the\u00a0\\(n\\)\u00a0samples, and\u00a0\\(y\\)\u00a0are the transformed\u00a0\\(n\u00d7k\\)-dimensional samples in the new subspace).</li> </ul>"},{"location":"LIME/","title":"LIME","text":"<ul> <li>\u201cWhy Should I Trust You?\u201d Explaining the Predictions of Any Classifier</li> <li>novel model-agnostic modular and extensible explanation technique that explains the predictions of any classifier in an interpretable and faithful manner</li> <li>learning an interpretable model locally around the prediction</li> <li>SP-LIME</li> <li>method to explain models by selectingrepresentative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem and providing a global view of the model to users</li> <li>flexibility of these methods by explaining different models for text (e.g random forests) and image classification (e.g neural networks)</li> <li>usefulness of explanations is shown via novel experiments, both simulated and with human subjects</li> <li></li> </ul>"},{"location":"Label%20Encoding/","title":"Label Encoding","text":"<ul> <li>also called Integer Encoding</li> <li>Each unique category is assigned an integer value</li> <li>e.g. \"red\" \u2192 0, \"blue\" \u2192 1, \u2026</li> <li>easy reversible</li> <li>can only be used when a ordinal relationship between the labels exist, e.g. winner ranking in string (\"first\", \"second\", \"third\")<ul> <li>if not and still used, can result in poor performance and unexpected results</li> </ul> </li> <li>numeric representations have a natural ordered relationship between each other and the models are able to understand that relationship</li> </ul>"},{"location":"Label%20Smoothing/","title":"Label Smoothing","text":"<ul> <li>Dense layer is generally the last one and combined with soft max leads to a Probability distribution</li> <li>Assume true label to be y, then a truth Probability distribution would be \\(p_i=1\\) If i=y and 0 otherwise</li> <li>During training, minimize negative Cross Entropy loss to make these distributions similar</li> <li>We know, \\(\\(\\mathscr{l}(p,q) = -log p_y = -z_y + log(\\Sigma^{K}_{i=1}exp(z_i))\\)\\)</li> <li>Where the optimal solution is \\(z^{\\ast}_{y}=\\inf\\)<ul> <li>The output scores are encouraged to be distinctive which leads to overfitting</li> <li>Leads to</li> </ul> </li> <li>Instead \\(\\(\\cases{1-\\epsilon&amp; if i=1\\\\\\frac{\\epsilon}{(K-1)} &amp; \\text{otherwise}}\\)\\)</li> <li>The optimal Solution is<ul> <li>\\(log((K-1)(1-\\epsilon)/ \\epsilon)+\\alpha\\) if \\(i=y\\)</li> <li>\\(\\alpha\\) otherwise<ul> <li>Any real number</li> <li>Finite output from the last layer that generalizes well</li> </ul> </li> </ul> </li> <li>If \\(\\epsilon =0\\) , \\(log((k-1)\\frac{1-\\epsilon}{\\epsilon})\\) is \\(\\infty\\)</li> <li>As \\(\\epsilon\\) increases, the gap decreases</li> <li>If \\(\\epsilon=\\frac{K-1}{K}\\), all optimizal \\(z^{\\ast}_{i}\\) are identical</li> </ul>"},{"location":"Label%20Smoothing/#backlinks","title":"Backlinks","text":"<ul> <li>DCGAN</li> <li>In this implementation, I also added in\u00a0Label Smoothing</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Label%20bias/","title":"Label Bias","text":"<ul> <li>This comes from the fact that semantic categories are often poorly defined, and different labellers may assign differing labels to the same type of object [11] (e.g. \"grass\" vs. \"lawn\", \"painting\" vs. \"picture\").</li> </ul>"},{"location":"Label%20bias/#backlinks","title":"Backlinks","text":"<ul> <li>Unbiased Look at Dataset Bias</li> <li>Label bias</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Labeled%20Faces%20in%20the%20Wild/","title":"Labeled Faces in the Wild","text":"<ul> <li>vis-www.cs.umass.edu/lfw/</li> </ul>"},{"location":"Lack%20of%20information/","title":"Lack of Information","text":"<ul> <li>Data does not show how to extract optimal info</li> <li>Curse Of Dimensionality</li> </ul>"},{"location":"Ladle%20Gripper/","title":"Ladle Gripper","text":"<ul> <li>An end-effector, which acts as a scoop. It is commonly used to scoop up liquids, transfer it to a mold and pour the liquid into the mold. Common for handling molten metal under hazardous conditions</li> </ul>"},{"location":"Lagrangian%20Coherent%20Structure/","title":"Lagrangian Coherent Structure","text":"<ul> <li>Lagrangian Grid</li> <li></li> </ul>"},{"location":"Lagrangian%20Grid/","title":"Lagrangian Grid","text":"<ul> <li>Focus on individual particles</li> <li>Attached are position, velocity, and other properties</li> <li>Explicit position</li> <li></li> </ul>"},{"location":"Language%20Identification/","title":"Language Identification","text":"<ul> <li>Identifying the language of the document</li> <li>Documents could be multilingual at the sentence level or paragraph level too</li> <li>Unique Character Set</li> <li>Shared Character Set</li> <li>Byte Range Distribution used for Character Set Identification</li> <li>sort the bytes in a \ufb01le by frequency count and use the sorted list as a signature vector for comparison via an n-gram model</li> </ul>"},{"location":"Language%20dependence/","title":"Language Dependence","text":"<ul> <li>Range of orthographic conventions used in written languages to denote the boundaries between linguistic units such as syllables, words, or sentences</li> <li>Language Identification</li> </ul>"},{"location":"Laplacian%20Grid%20Smoothing/","title":"Laplacian Grid Smoothing","text":"<ul> <li>new position is based on neighbor positions</li> <li> \\[p_{i}=\\frac{1}{N}\\Sigma_{i\u2026j}p_{j}\\] </li> </ul>"},{"location":"Large%20Batch%20Training/","title":"Large Batch Training","text":"<ul> <li>Generally slows down training</li> <li>If convex, convergence rate decreases with increase in batch size</li> <li>Learning Rate Scheduling</li> <li>Modified Batch Normalization with \\(\\gamma=0\\) for all BNs at the end of a residual block that micmics networks with less Layers and is easier to train at the start</li> <li>No bias decay</li> </ul>"},{"location":"Large%20Kernel%20in%20Attention/","title":"Large Kernel in Attention","text":"<ul> <li>self-attention can be viewed as a global depth-wise kernel that enables each layer to have a global receptive field.</li> <li>Swin Transformer (Liu et al., 2021e) is a ViTs variant that adopts local attention with a shifted window manner</li> <li>greatly improve the memory and computation efficiency with appealing performance</li> <li>Since the size of attention windows is at least 7, it can be seen as an alternative class of large kernel</li> <li>recent work (Guo et al., 2022b) proposes a novel large kernel attention module that</li> <li>uses stacked depthwise, small convolution, dilated convolution as well as pointwise convolution to capture both local and global structure</li> </ul>"},{"location":"Large%20Kernel%20in%20Convolution/","title":"Large Kernel in Convolution","text":"<ul> <li>Global Convolutional Network (GCNs) (Peng et al., 2017) enlarges the kernel size to 15 by employing a combination of 1\u00d7M + M\u00d71 and M\u00d71 + 1\u00d7M convolutions.</li> <li>However, the proposed method leads to performance degradation on ImageNet</li> <li>or the utilization of varying convolutional kernel sizes to learn spatial patterns at different scales. With the popularity of VGG (Simonyan &amp; Zisserman, 2014), it has been common over the past decade to use a stack of small kernels (1\u00d71 or 3\u00d73) to obtain a large receptive</li> <li>field</li> <li>However, the performance improvement plateaus when further expanding the kernel size</li> <li>Han et al. (2021b) find that dynamic depth-wise convolution (7x7) performs on par with the local attention mechanism if we substitute the latter with the former in Swin Transformer</li> <li>Liu et al. (2022b) imitate the design elements of Swin Transformer (Liu et al., 2021e) and design ConvNeXt employed with 7x7 kernels, surpassing the performance of the former</li> <li>Lately, Chen et al. (2022) reveal large kernels to be feasible and beneficial for 3D networks too.</li> <li>Prior works have explored the idea of paralleling (Peng et al., 2017; Guo et al., 2022a) or stacking (Szegedy et al., 2017) two complementary Mx1 and 1xM kernels</li> <li>However, they limit the shorter edge to 1 and do not scale the kernel size beyond 51x51</li> </ul>"},{"location":"Latent%20Dirchlet%20Allocation/","title":"Latent Dirchlet Allocation","text":"<ul> <li>Discovers topics into a collection of documents</li> <li>Tags each document with topics</li> <li></li> </ul>"},{"location":"Latent%20Semantic%20Analysis/","title":"Latent Semantic Analysis","text":"<ul> <li>Roughly speaking, a Learning Event is considered to be included in the student's step if the degree of semantic similarity is above a certain threshold.</li> </ul>"},{"location":"Law%20of%20large%20numbers/","title":"Law of Large Numbers","text":"<ul> <li>If one carries out an infinite seq of independantly repeating same the same numerical mesurement and gets a sequence of measurement values \\(x_{1}, .. , x_{n}\\) where \\(x_{i} \\in \\mathbb{R}\\), then the mean value of the inital seq upto N will almost always converge to the same number \\(\\(\\mu_{N} = \\frac{1}{N}\\Sigma_{i=1}^{N}x_{i}\\)\\) which is the EXPECTATION of X \\(\\(\\mu_{N}=E[X]\\)\\)</li> <li>Kolmogorov axioms</li> </ul>"},{"location":"Layer%20Normalization/","title":"Layer Normalization","text":"<ul> <li>For RNNs etc</li> <li>Mean and variance calculated independantly for each element of the batch by aggregating over the features dimensions.</li> <li> (Compared to Batch Normalization)</li> <li>$$ \\begin{align*}\\</li> </ul> <p>&amp;\\mu_{\\mathcal{B}} \\leftarrow \\frac{1}{m}\\Sigma_{i=1}^{m}x_{i}\\</p> <p>&amp;\\sigma^{2}{\\mathcal{B}} \\leftarrow \\frac{1}{m}\\Sigma{i=1}{m}(x_{i}-\\mu_{\\mathcal{B}}){2}\\</p> <p>&amp;\\hat x_{i} \\leftarrow \\frac{x_{i}-\\mu_{\\mathcal{B}}}{\\sqrt{\\sigma^{2}_{\\mathcal{B}} + \\epsilon}}\\</p> <p>&amp;y_{i}= \\gamma \\hat x_{i}+ \\beta</p> <p>\\end{align*}</p> <p>$$</p>"},{"location":"Layers/","title":"Layers","text":""},{"location":"Layers/#notation","title":"Notation","text":"<ul> <li>Grad of a function f wrt A : \\(\\(\\nabla_Af\\)\\)</li> <li>Neuron Pre activation : Z</li> <li>Activations : Y</li> <li>Tensor shape : (w,h,c)</li> <li>Matrix multi : \\(\\(A\\cdot B\\)\\)</li> <li>Hadmard prod (coeff wise) : \\(\\(A \\circ B\\)\\)</li> </ul>"},{"location":"Layerwise%20Conservation%20Principle/","title":"Layerwise Conservation Principle","text":"<ul> <li>which says that \"a network's output activity is fully redistributed through the layers of a DNN onto the input variables, i.e., neither positive nor negative evidence is lost.\"</li> </ul>"},{"location":"Layerwise%20Conservation%20Principle/#backlinks","title":"Backlinks","text":"<ul> <li>Conductance</li> <li>conductances thus satisfy the Layerwise Conservation Principle</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Layerwise%20Gradient%20Magnitude%20Based%20Pruning/","title":"Layerwise Gradient Magnitude Based Pruning","text":"<ul> <li>Finds the lowest absolute value per layer and removes them</li> </ul>"},{"location":"Layerwise%20Magnitude%20Based%20Pruning/","title":"Layerwise Magnitude Based Pruning","text":"<ul> <li>Takes the lowest values per layer in the network and prunes.</li> <li>Modifying the global layerwise and applying it per layer instead.</li> <li>To do this, we first make a copy of the weights. Then for every layer in the array, we find the least n values, take the nth value and set all the others to 0.</li> <li>As an edge case, if the number of elements entered is greater than the total length of the layer, then the entire layer is set to 0.</li> </ul>"},{"location":"Layerwise%20Relevance%20Propagation/","title":"Layerwise Relevance Propagation","text":"<ul> <li>It relies on a conservation principle to propagate the outcome decision back without using gradients. The idea behind it is a decomposition of prediction function as a sum of layerwise relevance values. When LRP is applied to deep ReLU networks, LRP can be understood as a deep Taylor decomposition of the prediction. This principle ensures that the prediction activity is fully redistributed through all the layers onto the input variables</li> <li>suffers from the shattered gradients problem</li> </ul>"},{"location":"Le%20Net/","title":"Le Net","text":"<ul> <li>Spatial dims reduce with depth, no of neurons increase</li> <li></li> </ul>"},{"location":"LeCun%20Init/","title":"LeCun Init","text":"<ul> <li> \\[\\frac{1}{fan_{in}}\\] </li> </ul>"},{"location":"Lead%20Test/","title":"Lead Test","text":"<ul> <li>A test to reveal the quantity of lead in the bloodstream</li> </ul>"},{"location":"Leaky%20Relu/","title":"Leaky Relu","text":"<ul> <li>Andrew L. Maas, Awni Y. Hannun, Andrew Y. Ng (2014). Rectifier Nonlinearities Improve Neural Network Acoustic Models.</li> <li>has a small slope for negative values instead of a flat slope. The slope coefficient is determined before training, i.e. it is not learnt during training. This type of activation function is popular in tasks where we we may suffer from sparse gradients, for example training generative adversarial networks.</li> <li>The reasons can be numerous, but in order to fight the situation when suddenly lot\u2019s of neurons in the network simply do nothing</li> <li> \\[max(0.01x,x)\\] </li> <li></li> </ul>"},{"location":"Leaky%20Relu/#backlinks","title":"Backlinks","text":"<ul> <li>DCGAN</li> <li>Strided Conv, Batch Normalization, and Leaky Relu</li> <li>Further the discriminator uses batch norm layers and\u00a0Leaky Relu\u00a0activations while the generator uses\u00a0Relu\u00a0activations</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Learning%20Component/","title":"Learning Component","text":""},{"location":"Learning%20Component/#learning-component","title":"Learning Component","text":"<ul> <li>analyses the trace of activities and extracts and conceptualizes possibly interesting experiences.</li> </ul>"},{"location":"Learning%20Component/#backlinks","title":"Backlinks","text":"<ul> <li>Learning Component</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Learning%20Event/","title":"Learning Event","text":"<ul> <li>A learning event is the construction or application of a Knowledge Component, often while trying to achieve the task. Learning events are mental events, whereas steps are physical events. A learning event occurs in the mind of the student where it cannot be observed, whereas a step occurs on the user interface and the computer can observe it. In the algebra example mentioned earlier, the step x=18.46 can be considered to result from three learning events: \\(\\(2.3*x=42.45 (cid:198) x=42.45/2.3 (cid:198) x=18.4565\u2026 (cid:198) x=18.46\\)\\)</li> </ul>"},{"location":"Learning%20L2%20German%20Vocabulary%20Through%20Reading/","title":"Learning L2 German Vocabulary Through Reading","text":"<ul> <li>Elke Peters , Jan H. Hulstijn , Lies Sercu , Madeline Lutjeharms</li> </ul> <ul> <li>This study investigated three techniques designed to increase the chances that second language (L2) readers look up and learn unfamiliar words during and after reading an L2 text</li> <li>They could look up the meaning of unfamiliar words in an online dictionary</li> <li>Test announcement and word relevance substantially prompted participants to use the online dictionary more.</li> <li>Only test announcement and vocabulary task (not word relevance) affected performance in the word recognition test positively</li> <li>oth word relevance and postreading vocabulary task substantially affected word retention in the recall posttests</li> <li>low incidence of vocabulary acquisition through reading (\"input only\") can be substantially boosted by techniques that make students look up the meaning of unknown words, process their form-meaning relationship elaborately, and process them again after reading (\"input plus\").</li> </ul>"},{"location":"Learning%20L2%20German%20Vocabulary%20Through%20Reading/#backlinks","title":"Backlinks","text":"<ul> <li>Final Paper User Models</li> <li>Learning L2 German Vocabulary Through Reading</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Learning%20Rate%20Decay%20tricks/","title":"Learning Rate Decay #tricks","text":"<ul> <li>Scale of loss landscape changes</li> <li>Reduce step size near optima</li> <li>Factor \\(\\(\\alpha_{i+1} = d\\cdot \\alpha_i\\)\\)</li> <li>Cosine Learning Rate Decay</li> </ul>"},{"location":"Learning%20Rate%20Decay%20tricks/#_1","title":"\u2026","text":""},{"location":"Learning%20Rate%20Range%20Test/","title":"Learning Rate Range Test","text":"<ul> <li>Smith, LN (2018) A disciplined approach to neural network hyper-parameters: Part 1--learning rate, batch size, momentum, and weight decay arXiv preprint arXiv:1803.09820</li> <li>It is relatively straight-forward: in a test run, one starts with a very small learning rate, for which one runs the model and computes the loss on the validation data. One does this iteratively, while increasing the learning rate exponentially in parallel. One can then plot their findings into a diagram representing loss at the y axis and the learning rate at the x axis. The x value representing the lowest y value, i.e. the lowest loss, represents the optimal learning rate for the training data.</li> <li>The learning rate at this extrema is the largest value that can be used as the learning rate for the maximum bound with cyclical learning rates but a smaller value will be necessary when choosing a constant learning rate or the network will not begin to converge.</li> <li>Smoothed loss changes</li> </ul> <pre><code>for i in range(moving_average, len(learning_rates)):\n    loss_changes.append((losses[i] - losses[i - moving_average]) / moving_average)\n</code></pre>"},{"location":"Learning%20Rate%20Scheduling/","title":"Learning Rate Scheduling","text":"<ul> <li>Learning Rate Decay tricks</li> <li>Gradient Descent gradients</li> <li>Increasing the batch size, reduces noise in the #gradients so a larger learning rate is okay</li> <li>Linear Learning Rate Scaling</li> <li>Learning Rate Warmup</li> </ul>"},{"location":"Learning%20Rate%20Warmup/","title":"Learning Rate Warmup","text":"<ul> <li>Small learning rate at the start and then a larger learning rate when the training is stabilized</li> <li>Linearly from 0 to initial rate</li> <li>First m batches to warm up and if the initial learning rate is \\(\\eta\\) then at batch i, \\(1 \\leq i \\leq m\\) , learning rate is \\(\\(\\frac{i\\eta}{m}\\)\\)</li> </ul>"},{"location":"Learning%20to%20Detect%20Grasp%20Affordance/","title":"Learning to Detect Grasp Affordance","text":"<ul> <li>Yikun Li, et al., Learning to Detect Grasp Affordances of 3D Objects using Deep Convolutional Neural Networks, Task-Informed Grasping workshop (TIG-II), RSS2019, Germany 2019.</li> <li></li> </ul>"},{"location":"Left%20psuedo%20inverse/","title":"Left psuedo inverse","text":"<ul> <li> \\[(A'A)^{-1}A'\\] </li> </ul>"},{"location":"Left%20psuedo%20inverse/#backlinks","title":"Backlinks","text":"<ul> <li>Linear Regression</li> <li>Left psuedo inverse</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Lemmatization/","title":"Lemmatization","text":"<ul> <li>Word-&gt; lemma</li> <li>saw : {see, saw}</li> <li>Morphological analysis : word-&gt; set of {lemma, tag}</li> </ul>"},{"location":"Length%20Optimization/","title":"Length Optimization","text":""},{"location":"Lesion/","title":"Lesion","text":"<ul> <li>An injury, area of disease, or surgical incision to body tissue. Much of what we know about the functions of brain structures or pathways comes from lesion mapping studies, where scientists observe the behavior of people with an injury to a distinct area of the brain or analyze the behavior of a laboratory animal resulting from a lesion made in the brain.</li> </ul>"},{"location":"Lesion/#lesion_1","title":"Lesion","text":"<ul> <li>Damage or change to tissue, such as a cut, a wound or a sore</li> </ul>"},{"location":"Lexical%20Ambiguity/","title":"Lexical Ambiguity","text":"<ul> <li>Lexical Disambiguation</li> <li>Will will Will\u2019s will</li> <li>Buffalo buffalo Buffalo buffalo</li> <li>Rose rose roes rows</li> </ul>"},{"location":"Lexical%20Disambiguation/","title":"Lexical Disambiguation","text":"<ul> <li>process of determining the correct meaning of an individual word</li> <li>Word sense disambiguation</li> <li>Semantic Markers</li> </ul>"},{"location":"Lexical%20Word%20Segmentation/","title":"Lexical Word Segmentation","text":"<ul> <li>rule-based \u2013 syntax; semantics; morphological rules</li> </ul>"},{"location":"Lexically%20Collective/","title":"Lexically Collective","text":"<ul> <li>Each knight gathered at the castle.</li> </ul>"},{"location":"Lexically%20Distributive/","title":"Lexically Distributive","text":"<ul> <li>Each girl smiled</li> </ul>"},{"location":"Lexicon/","title":"Lexicon","text":"<ul> <li>a module that tells what words there are and what properties they have</li> </ul>"},{"location":"LibriSpeech/","title":"LibriSpeech","text":""},{"location":"Limbic%20system/","title":"Limbic System","text":"<ul> <li>is the center of our emotions, learning, and memory.</li> <li>Included in this system are the cingulate gyri, hypothalamus, amygdala (emotional reactions) and hippocampus (memory).</li> </ul>"},{"location":"Limbic%20system/#limbic-system_1","title":"Limbic System","text":"<ul> <li>A group of evolutionarily older brain structures that encircle the top of the brain stem. The limbic structures play complex roles in emotions, instincts, and appetitive behaviors.</li> </ul>"},{"location":"Line%20Integral%20Convolution/","title":"Line Integral Convolution","text":"<ul> <li>Mimic physical experiment: oil drops on surface, apply flow (wind)</li> <li>Intensity distribution along streamlines shows high correlation</li> <li>No correlation between neighboring streamlines</li> <li></li> <li></li> </ul>"},{"location":"Linear%20Classifier%20Probes/","title":"Linear Classifier Probes","text":"<ul> <li>Understanding Intermediate Layers Using Linear Classifier Probes</li> <li>Black box</li> <li>monitor the features at every layer of a model and measure how suitable they are for classification</li> <li>\"Probes\"</li> <li>trained entirely independently of the model</li> <li>observe experimentally that the linear separability of features increase monotonically along the depth of the model</li> </ul>"},{"location":"Linear%20Interpolated%20Motion/","title":"Linear Interpolated Motion","text":"<ul> <li>Is a method of path interpolation that commands the movement of the robot by moving each joint in a coordinated motion so that all axis arrive to the position at the same time. The path of the Tool Control Point (TCP) is predictable and will be linear.</li> </ul>"},{"location":"Linear%20Learning%20Rate%20Scaling/","title":"Linear Learning Rate Scaling","text":"<ul> <li>If [He Initialization ]] is used, 0.1 is a good learning rate for batch size 256 and for a larger b, \\(0.1\\times\\frac{\\mathrm{b}}{256}\\) is okay</li> </ul>"},{"location":"Linear%20scale/","title":"Linear Scale Encoding","text":"<ul> <li>Eg Likert scale</li> <li>A = {certainly not, rather not, dont know}</li> </ul>"},{"location":"Linear%20scale/#_1","title":"\u2026","text":""},{"location":"LinearRegression/","title":"Linear Regression","text":"<ul> <li>Minimization problem (\\((w,b) = argmin_{w^{\\ast} , b^{\\ast}} \\Sigma^N_{i-1}(w^{\\ast}x_{i} +b ^{\\ast} - y_{i})^2\\)\\)<ul> <li>Affine Function</li> <li> \\[w = argmin_{w^\\ast} || (\\Sigma^n_{j = 1} w^\\ast_j \\phi_j) - y || ^2\\] <ul> <li>w* is just \\(\\((w^\\ast_1 \u2026 w^\\ast_n)\\)\\)</li> <li> \\[u_j$$ of U forms orthonormal basis of $$\\mathscr{F}\\] </li> </ul> </li> </ul> </li> <li>X : nxN matrix , y : N dim vector</li> <li>Solution : \\(\\([w, b] \\in \\mathbb{R} ^{n+1}\\)\\)</li> <li> \\[w' = (XX')^{-1}X y\\] <ul> <li>If y is has vector data too (size k)<ul> <li> \\[W' = (XX')^{-1}XY\\] </li> <li>Y : N x k matrix</li> </ul> </li> </ul> </li> <li></li> <li>(\\(\\phi _1 , \\phi_2 \u2026\\)\\) form a subspace (\\(\\mathscr{F}\\)\\) with dim = n<ul> <li>linearly independant vectors. If not, drop as many as possible</li> </ul> </li> <li>The optimal solution y_opt is the projection of y on that subspace and has the smallest distance from y<ul> <li> \\[y_{opt} = w_1 \\phi_1 + w_2 \\phi_2\\] </li> </ul> </li> <li> \\[(\\Sigma^n_{j = 1} w^\\ast_j \\phi_j)$$ is a vector on $$\\mathscr{F}\\] </li> <li>Ridge Regression</li> <li>Window Based Regression</li> </ul>"},{"location":"LinearRegression/#general-defination","title":"General Defination","text":"<ul> <li>Training data : \\(\\((x_i, y_i)_{i= 1,..,N}\\)\\) and \\(\\(\\in \\mathbb{R}^k\\)\\)</li> <li>Search space H<ul> <li>Candidate functions \\(\\(h: \\mathbb{R}^n \\rightarrow \\mathbb{R}^k\\)\\)</li> </ul> </li> <li>Loss function (\\(L : \\mathbb{R}^k \\times \\mathbb{R}^k \\rightarrow \\mathbb{R}^{n \\geq 0}\\)\\)<ul> <li>Quadratic Loss</li> </ul> </li> <li> <p>Solution : \\(\\(h_{opt} = argmin_{h \\in \\mathcal{H}}\\Sigma_{i=1}^N L(h(x_i), y_i)\\)\\)</p> <ul> <li> \\[\\mathcal{H}$$ is all linear functions from $$\\mathbb{R}^n$$ to $$\\mathbb{R}^k\\] </li> </ul> </li> <li> <p>Left psuedo inverse</p> </li> </ul>"},{"location":"Linguistic%20details/","title":"Linguistic Details","text":"<ul> <li>Phonetics</li> <li>Phonology</li> <li>Morphology</li> <li>Syntactic Analysis</li> <li>Semantic Analysis</li> <li>Pragmatics</li> </ul>"},{"location":"Lisht/","title":"Lisht","text":"<ul> <li>Derivatives<ul> <li></li> </ul> </li> <li>blog #Roam-Highlights<ul> <li>Linearly Scaled Hyperbolic Tangent</li> <li>his activation function simply uses the Tanh function and scales it linearly, as follows</li> <li> \\[LiSHT(x) = x \\times tanh(x)\\] </li> <li>Essentially, LiSHT looks very much like Swish in terms of the first-order derivative. However, the range is expanded into the negative as well, which means that the vanishing gradient problem is reduced even further - at least in theory.</li> <li>In their work, Roy et al. (2019) report based on empirical testing that indeed, the vanishing gradient problems is reduced compared to Swish and traditional ReLU. Additional correlations between network learning and the shape of e.g. the LiSHT loss landscape were identified.</li> </ul> </li> </ul>"},{"location":"Listen%20Attend%20Spell/","title":"Listen Attend Spell","text":"<ul> <li>Listen, Attend and Spell</li> <li>LAS</li> <li>learns to transcribe speech utterances to characters</li> <li>nlike traditional DNN-HMM models, this model learns all the components of a speech recognizer jointly</li> <li>sequence-to-sequence framework</li> <li>trained end-to-end and has two main components: a listener (encoder) and a speller (decoder)</li> <li>listener is a pyramidal RNN encoder that accepts filter bank spectra as inputs, transforms the input sequence into a high level feature representation and reduces the number of timesteps that the decoder has to attend to.</li> <li>The speller is an attention-based RNN decoder that attends to the high level features and spells out the transcript one character at a time</li> <li>The proposed system does not use the concepts of phonemes, nor does it rely on pronunciation dictionaries or HMMs</li> <li>bypass the conditional independence assumptions of CTC, and show how they can learn an implicit language model that can generate multiple spelling variants given the same acoustics</li> <li>producing character sequences without making any independence assumptions between the characters is the key improvement of LAS over previous end-to-end CTC models</li> <li>used samples from the softmax classifier in the decoder as inputs to the next step prediction during training</li> <li>show how a language model trained on additional text can be used to rerank their top hypotheses</li> <li>Google voice search task</li> </ul>"},{"location":"Load%20Cycle%20Time/","title":"Load Cycle Time","text":"<ul> <li>A manufacturing or assembly line process term, which describes the complete time to unload the last work piece and load the next one.</li> </ul>"},{"location":"Load%20balancing/","title":"Load Balancing","text":"<ul> <li>divide the work equally among the available processors</li> </ul>"},{"location":"Local%20Descriptor/","title":"Local Descriptor","text":""},{"location":"Local%20Descriptor/#local-descriptor","title":"Local Descriptor","text":"<ul> <li>set of spin-images</li> <li>A spin-image feature is computed for every keypoint:</li> <li>The tangent plane is estimated</li> <li>A 2D histogram is computed along the a and b dimensions in the neighborhood of the keypoint</li> <li>Spin-image represents a small area of an object around a specific keypoints</li> <li></li> <li></li> </ul>"},{"location":"Local%20Descriptor/#backlinks","title":"Backlinks","text":"<ul> <li>Local Descriptor</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Local%20Reference%20Frame/","title":"Local Reference Frame","text":""},{"location":"Local%20Reference%20Frame/#local-reference-frame","title":"Local Reference Frame","text":"<ul> <li>Three principal axes of a given object's point cloud are firstly determined based on eigenvectors analysis (PCA)</li> <li></li> </ul>"},{"location":"Local%20Reference%20Frame/#backlinks","title":"Backlinks","text":"<ul> <li>Local Reference Frame</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Local-LDA%20Object%20Representation/","title":"Local-LDA Object Representation","text":""},{"location":"Local-LDA%20Object%20Representation/#local-lda-object-representation","title":"Local-LDA Object Representation","text":"<ul> <li>A variant of Latent Dirichlet Allocation (Local-LDA)</li> <li>learn structural semantic features (i.e. topics) from low-level feature cooccurrences for each category independently and incrementally.</li> <li></li> </ul>"},{"location":"Local-LDA%20Object%20Representation/#backlinks","title":"Backlinks","text":"<ul> <li>Local-LDA Object Representation</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Localist%20units/","title":"Localist Units","text":"<ul> <li>In a localist representation, a localist unit (neuron) is most active to one meaningful category</li> <li>In Input: when we build a network and let each input unit represent a a specific word</li> <li>In Output: when we allow outputs of single unit to be interpreted</li> <li>In the hidden units: is there evidence of localist encoding developing?</li> </ul>"},{"location":"Locality/","title":"Locality","text":"<ul> <li>Localist units</li> <li>In order to process an image, we start by capturing the local information. One way to do that is the use of a convolutional layer. It can capture the local relationship between the pixels of an image. Then, as we go deeper in the model, the local feature extractors help to extract the global features</li> <li></li> </ul>"},{"location":"Location%20Aware%20Attention/","title":"Location Aware Attention","text":"<ul> <li>Chorowski et al., 2015</li> </ul>"},{"location":"Location%20Base%20Attention/","title":"Location Base Attention","text":"<ul> <li>Luong2015</li> <li>Attention Alignment score \\(\\alpha_{t,i} = softmax(W_{\\alpha}s_{t})\\)</li> </ul>"},{"location":"Log%20Likelihood%20Loss/","title":"Log Likelihood Loss","text":"<ul> <li> \\[L(U) = \\Sigma_i log P(u_i| u_{i-k} ,\u2026, u_{i-1} )\\] </li> <li>k is size of context window of past tokens</li> </ul>"},{"location":"Log-odds/","title":"Log-odds","text":"<ul> <li>The logarithm of the odds of some event.</li> <li>If the event refers to a binary probability, then odds refers to the ratio of the probability of success (p) to the probability of failure (1-p).</li> </ul>"},{"location":"LogCosh/","title":"Log Cosh","text":"<ul> <li>works like the MSE, but is smoothed towards large errors (presumably caused by outliers) so that the final error score isn\u2019t impacted thoroughly.</li> </ul> <p>We first define the Softplus function \\(\\(\\log\\left( e^{x} + 1 \\right)\\)\\)</p> <p>Then , \\(\\(x = \u0177 - y\\)\\)</p> <p>logcosh = \\(\\(\\mathrm{mean}\\left( x + \\mathrm{softplus}\\left( -2 \\cdot x \\right) - \\log\\left( 2.0 \\right) \\right)\\)\\)</p>"},{"location":"LogCosh/#_1","title":"\u2026","text":""},{"location":"Logarithm/","title":"Logarithm","text":"<ul> <li>Logarithms are sort of a measure of the \u201cbigness\u201d of a number; 1\u201310 is small (say, 0..1), 10\u2013100 are medium (1..2), 100\u20131000 are big (2..3). But, it makes a pretty huge difference if we\u2019re thinking about\u00a0\\(log(x)\\)\u00a0with\u00a0\\(x\\)\u00a0between 1 and 10, or with\u00a0\\(x\\)\u00a0above 10, or with\u00a0\\(x\\)\u00a0less than 1. An\u00a0\\(x\\)\u00a0between zero and 1 turns into a negative number</li> </ul>"},{"location":"Logits/","title":"Logits","text":"<ul> <li>The vector of raw (non-normalized) predictions that a classification model generates, which is ordinarily then passed to a normalization function.</li> </ul>"},{"location":"Long%20Short%20Term%20Memory%20%28LSTM%29/","title":"Long Short Term Memory (LSTM)","text":"<ul> <li>Smaller chance of exploding or vanishing #gradients</li> <li>Better ability to model long term dependencies</li> <li>Gated connections</li> <li>Gates that learn to forget some aspects, and remember others better</li> <li>Splitting state into parts -&gt; output pred and feature learning</li> <li>At the end of the day, these could not handle too long sequences. Therefore -&gt; Transformer</li> </ul>"},{"location":"Long%20Short%20Term%20Memory%20%28LSTM%29/#the-math","title":"The Math","text":"<ul> <li>Gates<ul> <li>Forget (\\(g_f = \\sigma(W_{hf}h_{t-1} + W_{xf}x_t + b_f)\\)\\)<ul> <li>How much of the previous cell state is used</li> </ul> </li> <li>Input (\\(g_i = \\sigma(W_{hi}h_{t-1} + W_{xi}x_t + b_i)\\)\\)<ul> <li>How proposal is added to the state</li> </ul> </li> <li>Output (\\(g_o = \\sigma(W_{ho}h_{t-1} + W_{xo}x_t + b_o)\\)\\)<ul> <li>Component wise products</li> </ul> </li> </ul> </li> <li>Hidden state<ul> <li>(\\(C_t\\)\\) to model cross timestep dependencies<ul> <li>Cell state proposal : \\(\\(\\hat C = tanh(W_{hc}h_{t-1} + W_{xc}x_t + b_c)\\)\\)</li> <li>Final cell state : \\(\\(C_t = g_f \\cdot C_{t-1} + g_i\\cdot \\hat C\\)\\)</li> </ul> </li> <li>(\\(h_t\\)\\) to predict output<ul> <li> \\[h_t = g_o \\cdot \\sigma_y(C_t)\\] </li> </ul> </li> </ul> </li> </ul>"},{"location":"Long%20Term%20Potentiation%20%28LTP%29/","title":"Long Term Potentiation (LTP)","text":"<ul> <li>The persistent strengthening of a synapse with increased use, thought to underlie learning and memory.</li> </ul>"},{"location":"Longformer/","title":"Longformer","text":"<ul> <li>Longformer: the Long-Document Transformer</li> <li>Transformer</li> <li>Sliding Window Attention</li> <li>Dilated Sliding Window Attention</li> <li>Global and Sliding Window Attention</li> <li>attention mechanism that scales linearly with sequence length</li> <li>drop-in replacement for the standard self-attention</li> <li>local windowed attention with a task motivated global attention</li> <li>text8</li> <li>enwik8</li> <li>consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA</li> <li>Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset</li> </ul>"},{"location":"Loop%20Tiling/","title":"Loop Tiling","text":"<ul> <li>Hardware memory layout in consideration</li> </ul>"},{"location":"Low-rank%20factorization/","title":"Low-rank Factorization","text":"<ul> <li>These methods identify re- dundant parameters of deep neural networks by em- ploying the matrix and tensor decomposition (Yu et al., 2017; Denton et al., 2014).</li> </ul>"},{"location":"Lp%20Regularization/","title":"Lp Regularization","text":"<ul> <li>![images/Pasted image 20220626150853.png)</li> <li>Tikhonov</li> <li>Penalty considering weights</li> <li> \\[L^\\ast(\\theta) = L(\\theta) + \\lambda \\Sigma_i |\\theta_i|^p\\] <ul> <li>Lasso<ul> <li>p = 1</li> <li>Sparse</li> <li>With linear model : feature selection</li> </ul> </li> <li>Weight Decay<ul> <li>p = 2</li> <li>Bayesian</li> <li>Encourages optimization trajectory perpendicular to isocurves</li> <li></li> </ul> </li> </ul> </li> <li>Tune (\\(\\lambda\\)\\)<ul> <li>Grid search : log scale</li> <li>Too large : underfit, too small : overfit</li> <li>Cross Validation required</li> </ul> </li> </ul>"},{"location":"Lumbar%20Puncture%20or%20Spinal%20Tap/","title":"Lumbar Puncture or Spinal Tap","text":"<ul> <li>Drawing of cerebrospinal fluid from the lumbar region of the back using a hollow needle</li> </ul>"},{"location":"MAPE/","title":"MAPE","text":"<ul> <li>mean absolute % error</li> </ul> \\[\\frac{1}{\\mathrm{length}\\left( y \\right)} \\cdot \\mathrm{sum}\\left( \\left\\|\\frac{y - \u0177}{y}\\right\\| \\right)\\]"},{"location":"MCMC%20Sampling/","title":"MCMC Sampling","text":"<ul> <li>Complex distribution with only a proto PDF \\(g_{0}\\) that is known</li> <li>Markov Chain</li> <li>Detailed Balance<ul> <li>Sufficient but not necessary for Markov Chain to be a Sampler for g</li> </ul> </li> <li>Factors for MC estimate</li> </ul>"},{"location":"MILAN/","title":"MILAN","text":"<ul> <li>Natural Language Descriptions of Deep Visual Features</li> <li>Some neurons in deep networks specialize in recognizing highly specific perceptual, structural, or semantic features of inputs</li> <li>identifying neurons that respond to individual concept categories</li> <li>richer characterization of neuron-level computation</li> <li>mutual-information-guided linguistic annotation of neurons</li> <li>generate open-ended, compositional, natural language descriptions of individual neurons in deep networks</li> <li>generates a description by searching for a natural language string that maximizes pointwise mutual information with the image regions in which the neuron is active</li> <li>MILANNOTATIONS</li> <li>fine-grained descriptions that capture categorical, relational, and logical structure in learned features</li> <li>characterizing the distribution and importance of neurons selective for attribute, category, and relational information in vision models.</li> <li>auditing, surfacing neurons sensitive to protected categories like race and gender in models trained on datasets intended to obscure these features</li> <li>editing, improving robustness in an image classifier by deleting neurons sensitive to text features spuriously correlated with class labels</li> </ul>"},{"location":"MILANNOTATIONS/","title":"MILANNOTATIONS","text":"<ul> <li>a dataset of fine-grained image annotations</li> </ul>"},{"location":"MIMD/","title":"MIMD","text":"<ul> <li>Multiple instruction, multiple data</li> <li>Synchronous/Async , deterministic/non deterministic</li> <li>Most supercomputers</li> <li>Grids</li> <li>Multi processor SMP computers</li> <li>Also include SIMD sub components</li> <li></li> </ul>"},{"location":"MISD/","title":"MISD","text":"<ul> <li>Multiple instructions on single data</li> <li>Real time computers need to be fault tolerant where several processors execute the same data for producing the redundant data</li> <li>N-version programming</li> <li></li> </ul>"},{"location":"MIT1003/","title":"MIT1003","text":"<ul> <li>\"779 landscape images and 228 portrait images.\"</li> <li>The fixations were measured while 15 observers looked at an image for 3 s.</li> </ul>"},{"location":"MIT300/","title":"MIT300","text":"<ul> <li>was the first data set with held-out human eye movements and is used as benchmark test data in MIT Saliency Benchmark</li> <li>\"300 natural\"</li> <li>The fixations were measured while 39 observers looked at an image for 3 s.</li> <li>\"indoor and outdoor scenes.\"</li> </ul>"},{"location":"MLDoc/","title":"MLDoc","text":""},{"location":"MLIM/","title":"MLIM","text":"<ul> <li>MLIM: Vision-and-language Model Pre-training with Masked Language and Image Modeling</li> <li>Vision-and-Language Pre-training (VLP) improves model performance for downstream tasks that require image and text inputs</li> <li>Typically, in addition to the Masked Language Modeling (MLM) loss, alignment-based objectives are used for cross-modality interaction, and RoI feature regression and classification tasks for Masked ImageRegion Modeling (MIRM)</li> <li>Alignment-based objectives require pairings of image and text and heuristic objective functions</li> <li>Masking policies either do not take advantage of multi-modality or are strictly coupled with alignments generated by other models</li> <li>pre-trained using two pre-training tasks as a multi-loss objective given a mini-batch of image-text pairs: Masked Language Modeling (MLM) loss (as in BERT) for text, and image reconstruction (RECON) loss for image, coupled with Modality Aware Masking (MAM)</li> <li>determines the masking probability and applies masking to both word and image embedding</li> <li>based on BERT predict the masked words from available words and image regions</li> <li>follow BERT for this task: two-layer MLP MLM head outputting logits over the vocabulary</li> <li>MLM loss is negative log-likelihood for masked word</li> <li>RECON loss is an an average of pixel-wise sum of squared errors (SSE)</li> <li>Both image and word masking is realized by replacing an embedding with the embedding of <code>[MASK]</code></li> <li>transformer layers recognize <code>[MASK]</code></li> <li>\u2019s embedding as a special embedding that needs to be \u201cfilled in\u201d, independent of the modality, by attending to other vectors in the layer inputs</li> <li>unlike other architectures (LXMERT, UNiTER, ViLBERT, VLP, VL-BERT, VisualBERT, etc.), image masking is not based on image regions detected by the object detector, but a shallow CNN as an image embedder which is much more lightweight than deep models like ResNet and is designed to be masking friendly</li> <li>MLM + RECON losses apply only to the masked text/image areas and measure reconstructed text and image quality.</li> <li>no specific alignment loss</li> <li>Modality Aware Masking (MAM) to boost cross-modality interaction and take advantage of MLM and RECON losses that separately capture text and image reconstruction quality</li> <li>Since the the task of finding closely-matching (CM) item pairs requires a pair of image+text inputs, they exploit this multi-modality by employing Modality Dropout</li> <li>text-only, image-only, and image-text mode</li> <li>However, RECON instead of ITM loss offers better PR AUC</li> <li>Similarly, using the ITM loss together with MLM and RECON does not change the performance</li> </ul>"},{"location":"MLM/","title":"MLM (Masked Language Modeling)","text":"<ul> <li>from</li> <li>15% of the words in each sequence are replaced by\u00a0<code>[MASK]</code></li> <li>model tries to predict original values of the masked words</li> <li>uses the context provided by the other non-masked words in the sequences</li> <li>loss function only considers the predictions of the masked words, ignores non-masked ones<ul> <li>leads to slower convergence than with directional models</li> </ul> </li> <li>additions to standard architecture:<ul> <li>classification layer on top of the encoder output</li> <li>multiplying the encoders output vectors with the embedding matrix -&gt; transforms them into the vocabulary dimension</li> <li>calculating probability of each word in the vocabulary using\u00a0Softmax</li> </ul> </li> </ul>"},{"location":"MMLU/","title":"MMLU","text":""},{"location":"MNIST/","title":"MNIST","text":"<ul> <li>10 classes</li> <li>2 channels</li> <li>dataset consisting of handwritten digits</li> <li>28x28 pixels</li> <li>70'000 images</li> </ul>"},{"location":"MRI/","title":"MRI","text":"<ul> <li>Studies brain anatomy</li> <li>1 image</li> <li>1mm</li> <li>1) Put subject in big magnetic field (leave him there)  </li> <li>2) Transmit radio waves into subject [about 3 ms]  </li> <li>3) Turn off radio wave transmitter  </li> <li>4) Receive radio waves re-transmitted by subject  <ul> <li>Manipulate re-transmission with magnetic fields during this readout interval [10-100 ms: MRI is not a snapshot]  </li> </ul> </li> <li>5) Store measured radio wave data vs. time  <ul> <li>Now go back to 2) to get some more data  </li> </ul> </li> <li>Process raw data to reconstruct images  </li> <li>Allow subject to leave scanner</li> </ul>"},{"location":"MSCOCO/","title":"MSCOCO","text":""},{"location":"MSE/","title":"MSE","text":"<ul> <li>$\\(L(x) = \\Sigma_i ||D(E(x_i))||^2\\)</li> <li> \\[MSE = \\frac{1}{N} \\Sigma^N_{i=1}(p(x_i) - y_i)^2\\] </li> </ul>"},{"location":"MSLE/","title":"MSLE","text":"<ul> <li>MSE log error</li> <li>Use MSLE when doing regression, believing that your target, conditioned on the input, is normally distributed, and you don\u2019t want large errors to be significantly more penalized than small ones, in those cases where the range of the target value is large.</li> </ul> \\[\\frac{1}{\\mathrm{length}\\left( y \\right)} \\cdot \\mathrm{sum}\\left( \\left( \\log\\left( y + 1 \\right) - \\log\\left( \u0177 + 1 \\right) \\right)^{2} \\right)\\]"},{"location":"MUSAN/","title":"MUSAN","text":"<ul> <li>which consists of over 900 noises, 42 hours of music from various genres and 60 hours of speech from twelve languages</li> </ul>"},{"location":"MVCNN/","title":"MVCNN","text":""},{"location":"MVCNN/#mvcnn","title":"MVCNN","text":"<ul> <li>Multi view CNN for 3D object recognition</li> <li>Limitations</li> <li>Generating multi-views is time consuming process</li> <li>Objects are partially visible due to (self) occlusion</li> <li>Number of categories should be defined in advance.</li> <li></li> </ul>"},{"location":"MVCNN/#backlinks","title":"Backlinks","text":"<ul> <li>MVCNN</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"MVGrasp/","title":"MVGrasp","text":"<ul> <li>H. Kasaei, et al. \"MVGrasp: Real-Time Multi-View 3D Object Grasping in Highly Cluttered Environments.\" arXiv preprint arXiv:2103.10997 (2021).</li> <li>Render multiple views of objects and use a next best view view selection algorithm Generate pixel-wise grasp configuration for the given object view.  </li> <li>The gripper approaches the target object in an arbitrary direction.  </li> <li>Use a shallow network, and an eye-to-hand camera configuration.</li> <li></li> <li>Mixed autoencoder (CAE + DAE)  </li> <li>optimizer: RMSprop, learning_rate = 0.001  </li> <li>metrics: Intersection over Union (IoU) and reconstruction error loss: mean squared error</li> <li></li> <li>Which view is suitable?<ul> <li>Depends on the pose of the target object and other objects  </li> <li>Most objects are graspable from either top or side -&gt; orthographic setup</li> <li>View entropy is used as the metric for selecting the best view</li> </ul> </li> </ul>"},{"location":"Macroadaptation/","title":"Macroadaptation","text":"<ul> <li>Among four common designs for outer loops, the most complex is based on a pedagogy called macroadaptation (Corbett &amp; Anderson, 1995; Shute, 1993)</li> <li>For each task that the tutoring system can assign, it knows which knowledge components are exercised by the task. For each Knowledge Component, the tutor maintains an estimate of the student's degree of mastery of that Knowledge Component</li> <li>When a student has completed a task and the tutor needs to select the next one, it chooses one based on the overlap between the tasks' knowledge components and the student's mastered knowledge components</li> <li>For example, it might assign a task that requires many knowledge components that are already mastered by the student and just two components that are not yet mastered.</li> <li>Some tutoring systems represent not only correct and incorrect knowledge components, but also other stable traits of students. They might represent learning styles and preferences, such as a preference for visual or verbal explanations, so they can choose tasks that are marked as compatible with the student's style or preference.</li> <li>For the outer loop to function correctly across multiple tasks and sessions, the information about the student must be stored on a server or on the student's computer's disk. This persistent information is often called a student model. Exactly what it contains depends on the type of outer loop</li> </ul>"},{"location":"Magical%20maybe/","title":"Magical maybe","text":"<p>title: Magical maybe</p> <p>tags: brain, psychology</p>"},{"location":"Magical%20maybe/#magical-maybe","title":"Magical Maybe","text":"<ul> <li>Robert Sapolsky</li> <li>According to this idea; the individual may or may not find a notification when looking on the phone. There is a large increase in Dopamine levels when the indication is seen.</li> </ul>"},{"location":"Magnetic%20Detectors/","title":"Magnetic Detectors","text":"<ul> <li>Robot sensors that can sense the presence of ferromagnetic material. Solid-state detectors with appropriate amplification and processing can locate a metal object to a high degree of precision.</li> </ul>"},{"location":"Malignant/","title":"Malignant","text":"<ul> <li>Refers to the presence of cancerous cells in a tumor or growth</li> </ul>"},{"location":"Mallows%20Cp%20Statistic/","title":"Mallows Cp Statistic","text":"<ul> <li> \\[C_{p}= \\frac{1}{n}(RSS + 2 p \\hat \\sigma^{2})\\] </li> </ul>"},{"location":"Manhattan%20Distance/","title":"Manhattan Distance","text":"<ul> <li>Taxicab distance or City Block distance, calculates the distance between real-valued vectors</li> <li> \\[D(x,y) = \\Sigma_{i=1}^{k}|x_{i}-y_{i}|\\] </li> <li>There is no diagonal movement involved in calculating the distance.</li> <li>Manhattan distance seems to work okay for high dim data, it is a measure that is somewhat less intuitive than euclidean distance, especially when using in high-dimensional data</li> <li>more likely to give a higher distance value than euclidean distance since it does not the shortest path possible.</li> <li>When your dataset has discrete and/or binary attributes, Manhattan seems to work quite well since it takes into account the paths that realistically could be taken within values of those attributes.</li> </ul>"},{"location":"Manifold%20MixUp/","title":"Manifold MixUp","text":"<ul> <li>mixes feature values generated from intermediate neural network layers.</li> <li>feedforward up to k layer of the network where the output feature maps are mixed</li> <li>The mixed feature maps are given input to the next layer and forward propagated up to the last layer.</li> <li>After the forward propagation, backward propagation is performed in the standard way with updated labels</li> </ul>"},{"location":"Manifold/","title":"Manifold","text":"<ul> <li>Data manifolds are an abstraction</li> <li>Only geometric insights are important</li> <li>Locally around some point c where the PDF is large -&gt; It will stay large only for a small fraction of directions<ul> <li>Those directions span a low dimensional hyperplane around c</li> <li>\"low dimensional sheets\"</li> <li>curved path</li> </ul> </li> <li>In an n dimensional real vector space \\(\\mathbb{R}^{n}\\) . Embedding space<ul> <li>\\(m \\leq n\\) is a positive integer</li> <li>An m dim manifold \\(\\mathcal{M}\\) is a subset of the vector space where one can smoothly map a neighborhood of that point to a neighborhood of the origin in m dim Euclidean space<ul> <li>Locally represents Euclidean space</li> </ul> </li> </ul> </li> <li>Only surface and not interior</li> <li>No sharp edges or spikes</li> <li>Can be exploited by Adversarial Learning</li> <li>Examples<ul> <li>1 dim -&gt; Lines in some high dim figure : B</li> <li>2 dim -&gt; Surfaces : A</li> <li></li> <li></li> <li></li> <li></li> </ul> </li> </ul>"},{"location":"Manifold/#refs","title":"Refs","text":"<ul> <li>tds</li> <li>way more stuff : bjlkeng #todo</li> </ul>"},{"location":"Manipulator/","title":"Manipulator","text":"<ul> <li>Gripper, hand, arm or other part of the body that can effect and move objects in the robot\u2019s environment</li> <li>Is an End-effector</li> </ul>"},{"location":"Mapping%20to%20Geometry/","title":"Mapping to Geometry","text":"<ul> <li>Height Plots</li> <li>Contour</li> </ul>"},{"location":"Marching%20Cubes/","title":"Marching Cubes","text":"<ul> <li>3D version of Marching Squares</li> <li>Cell consists of 8 node values: (i+{0,1}, j+{0,1}, k+{0,1})</li> <li> <ol> <li>Consider a cell</li> </ol> </li> <li> <ol> <li>Classify each vertex as inside or outside</li> </ol> </li> <li> <ol> <li>Build an index</li> </ol> </li> <li> <ol> <li>Get edge list from table[index]</li> </ol> </li> <li> <ol> <li>Interpolate the edge location</li> </ol> </li> <li> \\[x = i + \\frac{(c-v[i])}{(v[i+1]-v[i])}\\] </li> <li> <ol> <li>Compute gradients</li> </ol> </li> <li>Finite Differences Central</li> <li> <ol> <li>Consider ambiguous cases</li> </ol> </li> <li>Midpoint Decider</li> <li>Asymptotic Decider</li> <li> <ol> <li>Go to next cell</li> </ol> </li> <li></li> </ul>"},{"location":"Marching%20Cubes/#limitations","title":"Limitations","text":"<ul> <li>Produces many triangles</li> <li>Cannot represent sharp edges</li> <li>Produces \u201cugly\u201d (thin) triangles</li> <li>Produces ringing artifacts!</li> </ul>"},{"location":"Marching%20Squares/","title":"Marching Squares","text":"<ul> <li>Also uses Interpolation</li> <li>Symmetries</li> <li></li> <li>Asymptotic Decider</li> <li>Midpoint Decider</li> </ul>"},{"location":"Marching%20Tetrahedra/","title":"Marching Tetrahedra","text":"<ul> <li>Unstructured Grids</li> <li>May split other cell types into tetrahedra, however, at the cost of introduced error</li> <li>One - and three + or Two - and two +</li> </ul>"},{"location":"Margin%20Ranking/","title":"Margin Ranking","text":"<ul> <li>Creates a criterion that measures the loss given inputs x1x1x1 , x2x2x2 , two 1D mini-batch Tensors, and a label 1D mini-batch tensor yyy (containing 1 or -1).</li> <li>If y=1y = 1y=1 then it assumed the first input should be ranked higher (have a larger value) than the second input, and vice-versa for y=\u22121y = -1y=\u22121 .</li> <li>take avg</li> </ul> \\[\\frac{1}{\\mathrm{length}\\left( y \\right)} \\cdot \\mathrm{sum}\\left( \\mathrm{max}\\left( 0, \\left( - y \\right) \\cdot x1 - x2 + margin \\right) \\right)\\]"},{"location":"Markov%20Chain/","title":"Markov Chain","text":"<ul> <li>Sequence of random variables such as \\(X_{n+1}\\) only depends on \\(X_{n}\\)</li> <li>Discrete time</li> <li>Stochastic process without memory</li> <li>Finite interval : [0, 1, \u2026 n]</li> <li>Right infinite : n = 1, 2, 3, \u2026</li> <li>Left right infinite : Integers<ul> <li>No start</li> </ul> </li> <li>Markov Initial Distribution</li> <li>Markov Transition Kernel</li> <li>Markov for Continuous Distributions</li> </ul>"},{"location":"Markov%20Initial%20Distribution/","title":"Markov Initial Distribution","text":"<ul> <li>\\(P_{X}\\)</li> <li>Not needed for left right infinite ones</li> </ul>"},{"location":"Markov%20Property/","title":"Markov Property","text":"<ul> <li>A property of certain environments, where state transitions are entirely determined by information implicit in the current state and the agent\u2019s action.</li> </ul>"},{"location":"Markov%20Random%20Field/","title":"Markov Random Field","text":"<ul> <li>Generalized Markov Chain</li> </ul>"},{"location":"Markov%20Transition%20Kernel/","title":"Markov Transition Kernel","text":"<ul> <li>\\(\\(T_{n}(x|y) = P_{n}(X_{n+1}= x | X_{n}= y)\\)\\) for all \\(x,y \\in S\\)</li> <li>Homogenous if \\(\\(T_{n}(x|y) = T_{n'}(x|y)\\)\\) for all n,n'</li> <li>First get a value from a random drow from \\(P_{X_{1}}\\)</li> <li>Then get the next from the distribution which is specified by the transition kernel</li> <li></li> </ul>"},{"location":"Markov%20for%20Continuous%20Distributions/","title":"Markov for Continuous Distributions","text":"<ul> <li>Family of PDF</li> <li>If a Markov Chain with state set S, matrix M is executed m times . The transition probabilities transmit between states and then, \\(\\(P(X_{n+m}=s_{j}|X_{n}= s_{i}) = M^{m}(i, j)\\)\\)</li> <li>where \\(M^{m}= M \\cdot M \\cdot M \u2026 \\cdot M\\) (m times)</li> <li>To get the PDF \\(\\(g^{n+1}(x) = \\int_{\\mathbb{R}^{k}}T(x|y)g^{n}(y)dy\\)\\)</li> <li>Invariant Distribution</li> </ul>"},{"location":"Masked%20Autoencoders/","title":"Masked Autoencoders","text":"<ul> <li>Masked Autoencoders are Scalable Vision Learners</li> <li>simple Self Supervised</li> <li>ImageNet and in Transfer Learning that an Auto Encoders \u2014- a simple self-supervised method similar to techniques in NLP \u2013 provides scalable benefits</li> <li>mask random patches of the input image and reconstruct the missing pixels</li> <li>asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens</li> <li>images and languages are signals of a different nature</li> <li>Images are merely recorded light without a semantic decomposition into the visual analogue of words</li> <li>The word (or subword) analog for images are pixels</li> <li>But decomposing the image into patches (like Vision Transformer reduces the quadratic computation cost of transformers compared to operating at the pixel level</li> <li>remove random patches that most likely do not form a semantic segment</li> <li>Likewise, MAE reconstructs pixels, which are not semantic entities</li> <li>hey find that masking a high proportion of the input image, e.g., 75%, yields a nontrivial and meaningful self-supervisory task</li> <li>train and throw away the decoder and fine-tune the encoder for downstream tasks</li> <li>Vanilla ViT-Huge model (ViTMAE) achieves the best accuracy</li> <li>ImageNet</li> <li>Transfer performance in downstream tasks outperforms supervised pre-training and shows promising scaling behavior</li> <li>semantics</li> <li>Occurs by way of a rich hidden representation inside the MAE</li> <li></li> </ul>"},{"location":"Masked%20Language%20Modeling/","title":"Masked Language Modeling","text":"<ul> <li>In Masked Language Modeling, an input sequence of tokens is provided, but with some of these tokens masked. The goal of the model is then to learn to predict the correct tokens that are hidden by the mask. If it can do so, it can learn token-level information given the context of the token.</li> <li>In BERT, this is done as follows. 15% of all word embedded tokens is masked at random. From this 15%, 80% of the tokens is represented with a token called , 10% is replaced with a random token and 10% is left alone. This ensures that masking is both relatively random and that the model does not zoom in to the token, which is available during pretraining but not during fine-tuning.</li> <li>This model is also capable of predicting words using the two masked sentences. It concatenates two masked words and tries to predict.</li> <li>these models where we are required to predict the context of words. Since the words can have different meanings in different places the model needs to learn deep and multiple representations of words.</li> <li>These models have shown improved performance levels in the downstream tasks such as syntactic tasks that require lower layer representation of certain models in place of a higher layer representation.</li> <li>We may also find their use in learning the deep bidirectional representations of words. The model should be able to learn the context of words from the start of the sentence as well as from the behind.</li> </ul>"},{"location":"Masked%20Language%20Modeling/#tokenizing-data-bert","title":"Tokenizing Data BERT","text":"<ul> <li>In the tokenizer method, text_lst is the text corpus, max_length suggests the maximum number of allowable input tokens (the maximum is 512 for BERT base), and truncation set to True indicates that if the input size is more than the max_length, then the token from index number equal to max_length would be truncated i.e., for our example input tokens from index 100 would be dropped, padding set to True indicates the input length shorter than the max_length are padded, with padding token 0 and lastly, return_tensors indicates in what format do we want the output tensor and tf suggests that we expect tensorflow tensor. The tokenizer here returns three fields, as we have mentioned earlier.</li> <li>Now if we look at the \u201cinputs\u201d with the code print(inputs), we can see that the input_ids tensor is of shape 1567\u00d7100, and each row starts with the token 101, which is the id for the Special token [CLS] and ends with 0 which is the padding token indicating that the sentence length is less than 100. Also, there is a Special token 102, the [SEP] token, which is not visible, indicating the end of a sentence. Secondly, the token_type_ids are all 0 as there is only a single sentence as input. Finally, the attention_mask has ones at locations for the actual input tokens and zeros for the padding tokens.</li> </ul>"},{"location":"Masked%20Language%20Modeling/#masking-input-tokens-bert","title":"Masking Input Tokens BERT","text":"<ul> <li>In the original research paper, 15% of the input tokens were masked, of which 80% were replaced with [MASK] tokens, 10% were replaced with random tokens, and another 10% were left as is. However, in our fine-tuning task, we are replacing 15% of the input tokens except for the special ones with only [MASK] i.e., we will not replace token numbers 101,102, and 0 with mask token 103. In the following lines of codes, the same logic is implemented</li> </ul>"},{"location":"Masked%20Language%20Modeling/#backlinks","title":"Backlinks","text":"<ul> <li>Causal Language Model</li> <li> <p>Unlike Masked Language Modeling, this is uni-directional.</p> </li> <li> <p>BERT</p> </li> <li> <p>Masked Language Modeling</p> </li> <li> <p>Scalar Articles</p> </li> <li> <p>Masked Language Modeling with BERT</p> </li> <li> <p></p> </li> <li>11:05 Bunch of things today. First I have a thesis presentation Vision Explainibility, then an article on Masked Language Modeling and then Cogmod</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Mastectomy/","title":"Mastectomy","text":"<ul> <li>Surgical procedure to remove part or all of the breast</li> </ul>"},{"location":"Mastery%20learning/","title":"Mastery Learning","text":"<ul> <li>The outer loop implements a pedagogy called mastery learning (Bloom, 1984).</li> <li>The curriculum is structured as a sequence of units or a sequence of difficulty levels. When a student is working on a unit (or level of difficulty), the tutoring system keeps assigning tasks from that unit until the student has mastered the unit's knowledge. Only then does it allow the student to proceed to the next unit</li> <li>Thus, some students finish the curriculum having done fewer tasks than other students.</li> <li>This design for the outer loop is mostly used in self-paced courses.</li> <li>It is seldom used for a class-paced course, where it is important that all students stay together as they move through the curriculum.</li> <li>A common mistake is to develop a tutoring system with a fancy outer loop, then discover that instructors cannot use its features due to the class-paced nature of the course.</li> </ul>"},{"location":"Material%20Processing%20Robot/","title":"Material Processing Robot","text":"<ul> <li>A robot designed and programmed so that it can machine, cut, form or change the shape, function or properties of materials it handles between the time the materials are first grasped and the time they are released in a manufacturing process.</li> </ul>"},{"location":"Max%20Margin%20Loss/","title":"Max Margin Loss","text":"<ul> <li>Makes sure only dissimilar pairs with minimum distance m contribute to the loss</li> <li>Spring mass system</li> <li>Hinge Loss probably ??</li> </ul>"},{"location":"Maximum%20Matching%20Algorithm/","title":"Maximum Matching Algorithm","text":"<ul> <li>Greedy</li> <li>Starts with first character</li> <li>Searches for the longest word in list starting with this character. If match is found, boundary is marked</li> </ul>"},{"location":"Maxout/","title":"Maxout","text":"<ul> <li> \\[f(x) = max(x, x\\cdot a)\\] </li> </ul>"},{"location":"Mean%20Diffusivity/","title":"Mean Diffusivity","text":"<ul> <li> \\[\\mu = \\frac{\\lambda _{1}+ \\lambda_{2}+ \\lambda_{3}}{3}\\] </li> <li></li> </ul>"},{"location":"Mean%20Observed%20Dissimilarity/","title":"Mean Observed Dissimilarity","text":"<ul> <li>is the mean of the NISSIM dissimilarity over the adversarial test set for similar levels of attack.</li> <li>So for every adversarial set X \u2217, calculate NISSIM value for all samples in that set, and divide by the total number of samples.</li> <li>(0,1], such that 0 indicates total similarity while 1 indicates total dissimilarity</li> <li> \\[MOD_{advset}= \\frac{1}{N}\\Sigma (NISSIM_{i})\\] </li> </ul>"},{"location":"Median%20Filter/","title":"Median Filter","text":"<ul> <li>Values are replaced by the median in a local surrounding</li> <li>non linear</li> <li>preserves edges</li> </ul>"},{"location":"Mediatic%20Behavior/","title":"Mediatic Behavior","text":"<p>title: Mediatic Behavior</p> <p>tags: brain, psychology</p>"},{"location":"Mediatic%20Behavior/#mediatic-behavior","title":"Mediatic Behavior","text":"<ul> <li>Many individuals tend to resemble a role model.</li> <li>For example, in order to resemble the character in a television series, he or she unintentionally wears clothes similar to those she wore and uses his or her lines in daily life.</li> <li>This is basic type of media behavior</li> </ul>"},{"location":"Memory%20Coupling/","title":"Memory Coupling","text":"<ul> <li>TIghtly coupled</li> </ul>"},{"location":"Memory%20to%20Memory%20Architecture/","title":"Memory to Memory Architecture","text":"<ul> <li>For all vector operation, operands are fetched directly from main memory, then routed to the functional unit</li> <li>Results are written back to main memory</li> <li>Large startup time</li> </ul>"},{"location":"Memory-based%20learning/","title":"Memory-based Learning","text":"<ul> <li>Lazy learning</li> <li>All encountered examples are stored in memory in a multi-dimensional array, positioned according to relevant features</li> <li>New items are classified (comprehension) or generated (production) by searching for an example in memory that is closest to the target</li> <li>Because examplars are represented by their features even novel forms can be classified</li> <li>A generalization of the knn (k-nearest neighbors) algorithm</li> <li>Don't remove any infrequent or even solo forms. You might need the info</li> <li>Don't trim down the number of examples of a frequent form you have in the model. This effects it.</li> <li>Learning is storing, classification is analogy</li> <li>multiple long-distance dependencies</li> </ul>"},{"location":"Mental%20Fatigue/","title":"Mental Fatigue","text":"<ul> <li>Resource depletion occurs</li> <li>Drop in motivation does not really happen</li> </ul>"},{"location":"Mental%20Fatigue/#backlinks","title":"Backlinks","text":"<ul> <li>Cognitive Multitasking</li> <li>Mental Fatigue</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Mesh%20Smoothing/","title":"Mesh Smoothing","text":"<ul> <li>Noisy volume data leads to a noisy surface grid:</li> <li>Smooth the volume data first, or</li> <li>Smooth the grid in post-processing</li> <li>Eventually simplifies the grid</li> </ul>"},{"location":"Mesh%20refinement/","title":"Mesh Refinement","text":""},{"location":"Mesolimbic%20Pathway/","title":"Mesolimbic Pathway","text":"<ul> <li>A specialized brain circuit implicated in the processing of risk and reward information.</li> </ul>"},{"location":"Meta%20Learning%20Data%20Augmentations/","title":"Meta Learning Data Augmentations","text":"<ul> <li>The concept of meta-learning in Deep Learning research generally refers to the concept of optimizing neural networks with neural networks.</li> <li>This approach has become very popular since the publication of NAS</li> </ul>"},{"location":"Methods%20for%20Feature%20Learning/","title":"Methods for Feature Learning","text":"<ul> <li>Start with a random dict and rep. Update D while keeping r fixed -&gt; find best r while keeping D fixed. Repeat until convergence.</li> <li>Move D in a direction to minimize loss and project it back<ul> <li>Gradient Descent gradients or LinearRegression</li> </ul> </li> </ul>"},{"location":"Microbiota/","title":"Microbiota","text":"<ul> <li>The community of various microorganisms found in the digestive tract. Scientists are now learning that microbes found in the microbiota can influence brain development, mood, and behavior.</li> </ul>"},{"location":"Microglia/","title":"Microglia","text":"<ul> <li>A small, specialized glial cell that operates as the first line of immune defense in the central nervous system.</li> </ul>"},{"location":"Micromarriage/","title":"Micromarriage","text":"<ul> <li>Micromarriages -- colah's blog</li> <li>A micromarriage is a one in a million chance that an action will lead to you getting married, relative to your default policy.</li> <li>Note that some actions, such as dying, have a negative number of micromarriages associated with them.</li> <li>Most people do not include being coercively forced into a marriage when calculating micromarriages.</li> </ul>"},{"location":"Midpoint%20Decider/","title":"Midpoint Decider","text":"<ul> <li>check value in cell center and decide accordingly</li> <li></li> </ul>"},{"location":"Midpoint%20Method/","title":"Midpoint Method","text":""},{"location":"Milin%20et%20al/","title":"Milin Et Al.","text":"<ul> <li>Towards cognitively plausible data science in language research (2016), Milin, Divjak, Dimitrijevic and Baayen</li> <li>Identify difficult and easy forms (from lemma to plural form)</li> <li>Check if human participants also react differently to independently identified difficult and easy forms Compare NDL learning model to TiMBL and human results</li> <li>MDVM computes the distance between two values of a feature to reflect their patterns of co-occurrence with categories</li> <li>Using MDVM adds an unsupervised learning component to MBL Hoste (2005) because essentially it clusters feature values and uses that information</li> <li>Using larger values of k with MDVM is helpful</li> <li>Easy words that are frequent tokens (forms) are reacted to faster</li> <li>Maybe this interaction doesn't occur with difficult words because there is less variation in the frequency of the difficult words?</li> <li>This seems similar to results with regular past tense forms in English:</li> <li>Strikingly, TiMBL's inflectional class probabilities turn out to be predictive in production and comprehension, i. e., for lexical decision latencies.</li> <li>Two Grapheme to Lexeme Measures</li> <li>Diversity Sum of the absolute values of the activations of all possible outcomes, given a set of input cues.</li> <li>Input cues that activate many different outcomes give rise to a highly diverse activation vector, which in turn indicates a high degree of uncertainty about the intended outcome.</li> <li>G2L-Prior Sum of the absolute values of the weights on the connections from all cues to a given outcome.</li> <li>independent of the actual cues encountered in the input</li> <li>reflects the prior availability of an outcome, its entrenchment in the learning network</li> <li>TiMBL assigns higher probabilities to forms belonging to lemmas with letter trigraphs that yield more diverse activations</li> <li>Those trigraphs belong to a rich exemplar space in the memory</li> <li>it would be expected that higher probabilities would result in shorter response latencies</li> <li>However, NDL's G2L-Diversity was in fact positively correlated with RTs, indicating inhibition, i. e. slower recognition.</li> <li>TiMBL probabilities are intended to capture the likelihood of a form's occurrence in production.</li> <li>in comprehension (lexicality judgments) high trigraphs diversity may hurt results</li> <li>Spontaneous recovery from extinction</li> <li>After a CS is learned to associated with a given Conditioned Response (CR), this association is unlearned</li> <li>Theoretically, it can not arise again without retraining</li> <li>But in real life, sometimes seemingly completely forgotten associations are reactivated</li> <li>shows extinction is not unlearning</li> <li>responses that disappear are not necessarily forgotten</li> <li>Suggests loss of activation is not simply the mirror of acquiring associations</li> <li>Given two conditions stimuli, (CS) where one is more salient, the more salient CS will develop a strong association with the CR (Conditioned Response)</li> <li>Some linguistic things can be learned with NDL and this might show use something about the problem</li> <li>What made NDL so nice for animal learning might not scale up to linguistic phenomena</li> <li>Inductive approaches to cognition</li> </ul>"},{"location":"Mini%20Batch%20GD/","title":"Mini Batch GD","text":"<ul> <li> \\[\\theta= \\theta-\\eta \\cdot \\nabla_{\\theta}J(\\theta; x^{i:i+n};y^{i;i+n})\\] </li> </ul>"},{"location":"Minimal%20Semantic%20Commitment/","title":"Minimal Semantic Commitment","text":"<ul> <li>(Frazier et al., 1999)</li> <li>The MSC hypothesis distinguishes between two types of mental representations the processor might entertain upon encountering an underdetermined semantic constituent: if the representation is ambiguous, the processor will commit to just one interpretation and later revise it if necessary, but if the representation is vague, the processor refrains from committing to an interpretation, leaving some features underdetermined until further information is made available.</li> </ul>"},{"location":"Minimizing%20Communication/","title":"Minimizing Communication","text":"<ul> <li>Reduce the number of messages passed</li> <li>Reduce amount of data passed in messages</li> </ul>"},{"location":"Minkowski%20Distance/","title":"Minkowski Distance","text":"<ul> <li> \\[D(x,y) = (\\Sigma_{i=1}^{n}|x_{i}-y_{i}|^{p})^{\\frac{1}{p}}\\] </li> <li>It is a metric used in Normed vector space (n-dimensional real space), which means that it can be used in a space where distances can be represented as a vector that has a length.<ul> <li>Zero Vector \u2014 The zero vector has a length of zero whereas every other vector has a positive length. For example, if we travel from one place to another, then that distance is always positive. However, if we travel from one place to itself, then that distance is zero.</li> <li>Scalar Factor \u2014 When you multiple the vector with a positive number its length is changed whilst keeping its direction. For example, if we go a certain distance in one direction and add the same distance, the direction does not change.</li> <li>Triangle Inequality \u2014 The shortest distance between two points is a straight line.</li> </ul> </li> <li>Most interestingly about this distance measure is the use of parameter p. We can use this parameter to manipulate the distance metrics to closely resemble others.</li> <li>Common values of p are:<ul> <li>p=1 \u2014 Manhattan Distance</li> <li>p=2 \u2014 Euclidean Distance</li> <li>p=\\(\\infty\\) \u2014 Chebyshev Distance</li> </ul> </li> <li>The upside to p is the possibility to iterate over it and find the distance measure that works best for your use case.</li> </ul>"},{"location":"Mirman%20et%20al/","title":"Mirman Et Al.","text":"<ul> <li>train syllables in words, predicting the next syllable</li> <li>use network to train on different types of individual words, matching them with one of five objects, simulating word learning</li> <li>75 epocs 1000 syllable sequence, then it predicted almost perfectly the next syllable (teaching phonotactics of the language)</li> <li>Model trained to recognize one of five objects for each of five different two-syllable input patterns of three types 1. words (100% transitional probability) 2. partwords (25% probability transitions) 3. nonwords (0% transitions)</li> <li>Model is better at mapping two-syllable sequences to words when it has already been exposed to those sequences and they had high probabilities</li> <li>Novel-sequence non-word labels initially learned nearly as fast as word up to intermediate point.</li> <li>exposure to familiarization input allowed network to created distinct hidden representations for each syllable</li> <li>SRN can show how statistical learning supports word learning, showing a link</li> <li>Humans are good at learning sequences, even when the data is presented implicitly and even when the relationships are non-adjacent</li> <li>We aren't just sensitive to frequency: we are sensitive to actual transitional probabilities</li> <li>SRNs with very simple assumptions model non-adjacent learning and transitional probabilities</li> <li>Biological arguments for distributed representations</li> <li>Makes more sense that neurons get randomly assigned to be active for different inputs</li> <li>We can start with randomness and with learning it will become structured</li> <li>concepts are just bundles of features, that together become something</li> <li>Prevents catastrophic failures</li> </ul>"},{"location":"Mirror%20Shift%20Function/","title":"Mirror Shift Function","text":"<ul> <li>With the Mirror Shift Function, a job is converted to the job in which the path is symmetrical to that of the original job.</li> </ul>"},{"location":"Misyak%20et%20al%202010/","title":"Misyak Et Al 2010","text":"<ul> <li>Does the ability to learn statistical non-adjacent dependencies correlate with the ability to process non-adjacent dependencies in language?</li> <li>Can we model non-adjacent dependency learning with simple SRNs?</li> <li>allows us to see the continuous timecourse of statistical processing</li> <li>Uses both linguistic stimulus tokens and auditory cues</li> <li>on-line non-adjacency learning</li> <li>Investigation of Individual differences in language processing and statistical learning</li> <li>Participants trained in blocks of three word sequence trials.</li> <li>First and second word were random, but the third word was dependent on the first word.<ul> <li>Intervening second word creates non-adjacency</li> </ul> </li> <li>After final block: Prediction task where participants had to say what the third word was from two word sequences</li> <li>People can learn non-adjacent sequences with only implicit exposure</li> <li>SRN can capture performance on AGL tasks</li> <li>SRNs can deal with temporal structures and associations</li> <li>Localist representations: 30 input and output units, each unique unit corresponding to each nonword</li> <li>Standard backpropagation with a learning rate of 0.1 and momentum at 0.8</li> <li>The higher the prediction task accuracy (x-axis) the shorter reading times for object relatives.</li> <li>Even the people who are bad at sequential learning are still fluent speakers and listeners</li> <li>Is it possible that sequential learning and language learning are unrelated</li> <li>Maybe children are better at sequential learning, which helps them acquire languag</li> <li>Adults then lose this ability</li> </ul>"},{"location":"Mixed%20Effect%20Models/","title":"Mixed Effect Models","text":"<ul> <li>Are able to combine fixed factors and multiple random factors in one analysis No longer necessary to do two ANOVAs</li> </ul>"},{"location":"Mixed%20Example/","title":"Mixed Example","text":"<ul> <li>experimented with 14 different types of augmentation approaches. The output image is generated using the following techniques: vertical concatenation, horizontal concatenation, mixed concatenation, random 2x2, VH- mixup (vertical concatenation, horizontal concate- nation, and mixup), VH-BC+ (vertical concatena- tion, horizontal concatenation, and between-class), random square, random column interval, random row interval, random rows, random columns, ran- dom pixels, random elements, and noisy mixup.</li> <li>From all these approached, VHmixup has the best performance.</li> </ul>"},{"location":"Mixed%20chunk%20attention/","title":"Mixed Chunk Attention","text":"<ul> <li>an efficient linear approximation method that combines the benefits from partial and linear attention mechanisms, which is accelerator-friendly and highly competitive in quality.</li> <li>The method works on chunks of tokens and leverages local (within chunk) and global (between chunks) attention spans</li> </ul>"},{"location":"Mixup/","title":"Mixup","text":"<ul> <li>Randomly sample two examples \\((x_{i}, y_{i})\\) and \\((x_{j}, y_{j})\\)</li> <li>New example by weighted 1D piecewise linear interpolation</li> <li> \\[\\hat x = \\lambda x_{i}+(1-\\lambda)x_{j}\\] </li> <li> \\[\\hat y = \\lambda y_{i}+(1-\\lambda)y_{j}\\] </li> <li>$\\lambda \\in 0,1</li> <li>New example \\((\\hat x, \\hat y)\\)</li> </ul>"},{"location":"MoCO/","title":"MoCO","text":"<p>title: MoCO</p> <p>tags: architecture</p>"},{"location":"MoCO/#moco","title":"MoCO","text":"<ul> <li>Momentum Contrast for Unsupervised Visual Representation Learning</li> <li>unsupervised visual representation learning</li> <li>contrastive learning as dictionary look-up, MoCo builds a dynamic dictionary with a queue and a moving-averaged encoder</li> <li>large and consistent dictionary on-the-fly</li> <li>ImageNet</li> <li>transfer well to downstream tasks.</li> <li>PASCAL VOC</li> <li>COCO</li> <li>visual representation encoder by matching an encoded query</li> <li>to a dictionary of encoded keys using a contrastive loss</li> <li>dictionary is built as a queue, with the current mini-batch enqueued</li> <li>oldest mini-batch dequeued</li> <li>slowly progressing encoder</li> <li>momentum update with the query encoder</li> <li></li> <li></li> </ul>"},{"location":"Mobile%20Net/","title":"Mobile Net","text":"<ul> <li>Depthwise Separable</li> </ul>"},{"location":"MobileOne/","title":"MobileOne","text":"<ul> <li>An Improved One Millisecond Mobile Backbone</li> <li>extensive analysis of different metrics by deploying several mobile friendly networks on a mobile device</li> <li>identify and analyze architectural and optimization bottlenecks</li> <li>many times faster on mobile</li> <li>Inspired byRepVGG</li> <li>Either ReLU or SE-ReLU is used as activation. The trivial over-parameterization factor \\(k\\) is a hyperparameter which is tuned for every variant.</li> <li>better top-1 accuracy on ImageNet than EfficientNet at similar latency</li> <li></li> </ul>"},{"location":"Modality%20Dropout/","title":"Modality Dropout","text":"<ul> <li>MDO improves fine-tuning by randomly dropping one of the modalities</li> </ul>"},{"location":"Modality/","title":"Modality","text":"<ul> <li>A high-level data category. For example, numbers, text, images, video, and audio are five different modalities.</li> </ul>"},{"location":"Mode%20Collapse/","title":"Mode Collapse","text":"<ul> <li>Generator collapses and only predicts mean/median/mode of data instead of the prob distribution</li> </ul>"},{"location":"Mode%20Switch/","title":"Mode Switch","text":"<ul> <li>As per safety standards, an industrial robot has three distinct modes of operation. These are Teach (also called Manual) and Play (also called Automatic) and Remote. Switching between these modes is performed using a key switch on the teach pendant and is called Mode Switch.</li> </ul>"},{"location":"Modeling%20Driver%20Behavior%20with%20Cognitive%20Architecture/","title":"Modeling Driver Behavior with Cognitive Architecture","text":"<ul> <li>Salvucci, Dario D. \"Modeling driver behavior in a cognitive architecture.\" Human factors 48.2 (2006): 362-380.</li> </ul>"},{"location":"Modeling%20Driver%20Behavior%20with%20Cognitive%20Architecture/#intro","title":"Intro","text":"<p>This paper explores the development of a rigorous computational model of driver behavior in a cognitive architecture \u2013 a computational framework with underlying psychological theories that incorporate basic properties and limitations of the human system</p> <p>An integrated driver model developed in the ACT-R (Adaptive Control of Thought-Rational) cognitive architecture is described that focuses on the component processes of control, monitoring, and decision making in a multilane highway environment</p> <p>This model accounts for the steering profiles, lateral position profiles, and gaze distributions of human drivers during lane keeping, curve negotiation, and lane changing.</p> <p>The model demonstrates how cognitive architectures facilitate understanding of driver behavior in the context of general human abilities and constraints and how the driving domain benefits cognitive architectures by pushing model development toward more complex, realistic tasks</p>"},{"location":"Modeling%20Driver%20Behavior%20with%20Cognitive%20Architecture/#driving-and-integrated-driver-modeling","title":"Driving and Integrated Driver Modeling","text":"<p>useful to view driving and driver modeling in the context of the embodied cognition, task, and artifact (ETA) framework (Byrne, 2001; Gray, 2000; Gray &amp; Boehm-Davis, 2000).</p> <p>As the name suggests, this framework emphasizes three components of an integrated modeling effort: the task that a person attempts to perform, the artifact</p> <p>by which the person performs the task, and the embodied cognition by which the person perceives, thinks, and acts in the world through the artifact</p> <p>A sound understanding of each component is critical to developing rigorous integrated models of driver behavior.</p> <p>Michon (1985) identified three classes of task processes for driving: operational processes that involve manipulating control inputs for stable driving, tactical processes that govern safe interactions with the environment and other vehicles, and strategic processes for higher level reasoning and planning</p> <p>Some tasks are not continual but intermittent, arising in specific situations \u2013 for instance, parking a vehicle at a final destination.</p> <p>Between cognition and the vehicle lies the embodiment of the driver, namely the perceptual processes (visual, aural, vestibular, etc.) and motor processes (hands, feet) that provide the input from and output to the external world.</p> <p>Not surprisingly, there can be parallelism in this integrated system \u2013 for instance, moving the hand while visually encoding the lead car \u2013 but there are also capacity constraints and/or bottlenecks that sometimes result in degraded performance.</p>"},{"location":"Modeling%20Driver%20Behavior%20with%20Cognitive%20Architecture/#pictures","title":"Pictures","text":""},{"location":"Modeling%20Transfer/","title":"Modeling Transfer","text":"<ul> <li>given levels of mastery on a set of knowledge components, predict the performance on a new problem (Singley &amp; Anderson, 1989).</li> <li>For instance, suppose a student has mastered 15 of the 20 knowledge components required to do a task.</li> <li>This predicts the student's behavior on the task\u2014where the student will ask for help, how long it will take to do the task, which errors occur, etc</li> <li>However, these predictions can be inaccurate if the assumed knowledge components are not accurate reflections of how the student actually understands the task domain</li> </ul>"},{"location":"Momentum/","title":"Momentum","text":"<ul> <li>Momentum is the velocity of a body multiplied by its mass. A small force can quickly stop an object with low momentum, but a large or prolonged force is required to stop an object with high momentum.</li> <li> \\[p = mv\\] </li> <li>mass times velocity</li> </ul>"},{"location":"Monk/","title":"Monk","text":"<ul> <li>Hit list</li> <li></li> </ul>"},{"location":"Moral%20Machine%20project/","title":"Moral Machine Project","text":"<ul> <li>MIT</li> <li>wisdom of the crowd to find resolutions for ethical dilemmas</li> <li>studying the perception of autonomous vehicles (AVs) which are controlled by AI and has the potential to harm pedestrians and/or passengers if they malfunction</li> <li>allows participants to judge various ethical dilemmas facing AVs which have malfunctioned, and select which outcomes they prefer.</li> <li>saving more lives</li> <li>protecting passengers</li> <li>upholding the law</li> <li>avoiding intervention</li> <li>gender preference</li> <li>species preference</li> <li>age</li> <li>social value preference.</li> <li>people generally prefer the AV to make sacrifices if more lives can be saved.</li> <li>self-reported preferences often do not align well with actual behaviours</li> </ul>"},{"location":"Moral%20Machine%20project/#backlinks","title":"Backlinks","text":"<ul> <li>Building Ethics into Artificial Intelligence</li> <li>Moral Machine project</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Moral%20decision%20making%20frameworks%20for%20artificial%20intelligence/","title":"Moral Decision Making Frameworks for Artificial Intelligence","text":"<ul> <li>Vincent Conitzer, Walter Sinnott- Armstrong, Jana Schaich Borg, Yuan Deng, and Max Kramer.</li> <li>developing a general ethical decision making framework for AI based on game theory and machine learning</li> <li>For the game theory based framework, the authors suggest the extensive form (a generalization of game trees) as a foundation scheme to represent dilemmas</li> <li>current extensive form does not account for protected values in which an action can be treated as unethical regardless of its consequence</li> <li>extend the extensive form representation with passive actions for agents to select in order to be ethical</li> <li>machine learning based ethical decision-making</li> <li>classify whether a given action under a given scenario is morally right or wrong</li> <li>The main challenge in machine learning based moral decision-making is to design a generalizable representation of ethical dilemmas</li> <li>Game theory and machine learning can be combined into one framework in which game theoretic analysis of ethics is used as a feature to train machine learning approaches</li> </ul>"},{"location":"Moral%20decision%20making%20frameworks%20for%20artificial%20intelligence/#backlinks","title":"Backlinks","text":"<ul> <li>Building Ethics into Artificial Intelligence</li> <li>Moral decision making frameworks for artificial intelligence</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"MoralDM/","title":"MoralDM","text":"<ul> <li>enables an agent to resolve ethical dilemmas by leveraging on two mechanisms</li> <li>1) first-principles reasoning, which makes decisions based on well-established ethical rules (e.g., protected values); and 2) analogical reasoning, which compares a given scenario to past resolved similar cases to aid decision-making.</li> <li>the exhaustive comparison approach by MoralDM is expected to become computationally intractable</li> <li>[Blass and Forbus, 2015], MoralDM is extended with structure mapping which trims the search space by computing the correspondences, candidate inferences and similarity scores between cases to improve the efficiency of analogical generalization</li> </ul>"},{"location":"MoralDM/#backlinks","title":"Backlinks","text":"<ul> <li>Building Ethics into Artificial Intelligence</li> <li>MoralDM</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Morpheme%20Generation/","title":"Morpheme Generation","text":"<ul> <li>See +past.verb = saw</li> </ul>"},{"location":"Morpheme%20Segmentation/","title":"Morpheme Segmentation","text":"<ul> <li>De-nation-al-iz-ation</li> </ul>"},{"location":"Morpheme/","title":"Morpheme","text":"<ul> <li>words are built from smaller meaningful units called morphemes</li> <li>Allomorph</li> <li>Morphology Stem</li> <li>Morphology Affix</li> <li>Content Morpheme</li> <li>Functional Morpheme</li> <li>Morpheme Segmentation</li> <li>Morpheme Generation</li> <li>Morphotactic</li> </ul>"},{"location":"Morphology%20Affix/","title":"Morphology Affix","text":"<ul> <li>Bits and pieces that adhere to stems to change their meanings and grammatical functions</li> <li>Bound morpheme</li> <li>Prefix</li> <li>Suffix</li> <li>Infix</li> <li>Circumfix</li> </ul>"},{"location":"Morphology%20Stem/","title":"Morphology Stem","text":"<ul> <li>The core meaning bearing units \u2013 Main morpheme of the word</li> <li>Free morpheme</li> </ul>"},{"location":"Morphology/","title":"Morphology","text":"<ul> <li>structure of words</li> <li>Morpheme</li> <li>It is concerned with inflection.</li> <li>It is also concerned with derivation of new words from existing ones, eg. lighthouse (formed from light &amp; house)</li> <li>Needs a Lexicon</li> <li>Inflectional Morphology</li> <li>Derivational Morphology</li> <li>Suppletion</li> <li>Word Compounding</li> <li>Word Blending</li> <li>Word Clipping</li> <li>Lemmatization</li> </ul>"},{"location":"Morphotactic/","title":"Morphotactic","text":"<ul> <li>Which class of morphemes follow other class of morphemes</li> <li>Plural morphemes follow noun</li> <li>some endings go only on certain words not on everything.</li> <li>Do + er : doer</li> <li>Be + er :beer</li> </ul>"},{"location":"Motor%20Memories/","title":"Motor Memories","text":"<ul> <li>motor memory is unique</li> <li>Some studies on Alzheimer\u2019s disease included participants who were previously musicians and couldn\u2019t remember their own families, but they could still play beautiful music. Clearly, there\u2019s a huge difference in the way that motor memories are formed</li> <li>Memories are thought to be encoded in the brain in the pattern of activity in networks of hundreds or thousands of neurons, sometimes distributed across distant brain regions</li> <li>memory trace</li> <li>When the researchers tested the animals\u2019 memory of this new skill weeks later, they found that those mice that still remembered the skill showed increased activity in the same neurons that were first identified during the learning period, showing that these neurons were responsible for encoding the skill</li> <li>two-photon microscopy</li> <li>\u201cengram neurons\u201d reprogram themselves as the mice learned</li> <li>Motor cortex engram cells took on new synaptic inputs \u2014 potentially reflecting information about the reaching movement \u2014 and themselves formed powerful new output connections in a distant brain region called the dorsolateral striatum \u2014 a key waystation through which the engram neurons can exert refined control over the animal\u2019s movements.</li> <li>These findings suggest that, in addition to being dispersed, motor memories are highly redundant.</li> <li>The researchers say that as we repeat learned skills, we are continually reinforcing the motor engrams by building new connections \u2014 refining the skill. It\u2019s what is meant by the term muscle memory \u2014 a refined, highly redundant network of motor engrams used so frequently that the associated skill seems automatic.</li> <li>Current thinking is that Parkinson\u2019s disease is the result of these motor engrams being blocked, but what if they\u2019re actually being lost and people are forgetting these skills?</li> </ul>"},{"location":"Multi%20Head%20Attention/","title":"Multi Head Attention","text":"<ul> <li>ZihangDai et al., 2019</li> <li>which computes self-attention over the inputs, then adds back the residual and layer normalizes everything. The attention head can be split into multiple segments, hence the name\u00a0multi-head</li> <li>Multiple attention instances, each focusing on a different part of the input</li> <li>Words can mean different things in context<ul> <li>If using Self Attention, then this just gets summed up. Which is not very nice</li> <li>Several attention heads -&gt; different output vectors</li> <li>Concatenate them and pass through a linear transform -&gt; dimension back to k</li> </ul> </li> <li> \\[MultiHead(Q,K,V) = Concat(head_1, head_2, \u2026., head_h)W^O\\] <ul> <li> \\[head_i = Attention(QW_i^Q, KW_i^K , VW_i^V)\\] </li> </ul> </li> <li>W is learnable projections for attention params</li> <li></li> <li>To improve efficiency<ul> <li>Cut the incoming vector into chunks -&gt; no of attention heads</li> </ul> </li> </ul> <pre><code>class MultiHeadAttentionNew(nn.Module):\n    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):\n        super().__init__()\n        self.n_head = n_head\n\n        self.w_qs = nn.Linear(d_model, n_head * d_k)\n        self.w_ks = nn.Linear(d_model, n_head * d_k)\n        self.w_vs = nn.Linear(d_model, n_head * d_v)\n\n        nn.init.normal_(self.w_qs.weight, mean=0, std=np.sqrt(2.0 / (d_model + d_k)))\n        nn.init.normal_(self.w_ks.weight, mean=0, std=np.sqrt(2.0 / (d_model + d_k)))\n        nn.init.normal_(self.w_vs.weight, mean=0, std=np.sqrt(2.0 / (d_model + d_v)))\n\n        self.fc = nn.Linear(n_head * d_v, d_model)\n        nn.init.xavier_normal_(self.fc.weight)\n        self.dropout = nn.Dropout(p=dropout)\n        self.layer_norm = nn.LayerNorm(d_model)\n\n    def forward(self, q, k, v, mask=None):\n        residual = q\n        q = rearrange(self.w_qs(q), 'b l (head k) -&gt; head b l k', head=self.n_head)\n        k = rearrange(self.w_ks(k), 'b t (head k) -&gt; head b t k', head=self.n_head)\n        v = rearrange(self.w_vs(v), 'b t (head v) -&gt; head b t v', head=self.n_head)\n        attn = torch.einsum('hblk,hbtk-&gt;hblt', [q, k]) / np.sqrt(q.shape[-1])\n        if mask is not None:\n            attn = attn.masked_fill(mask[None], -np.inf)\n        attn = torch.softmax(attn, dim=3)\n        output = torch.einsum('hblt,hbtv-&gt;hblv', [attn, v])\n        output = rearrange(output, 'head b l v -&gt; b l (head v)')\n        output = self.dropout(self.fc(output))\n        output = self.layer_norm(output + residual)\n        return output, attn\n</code></pre>"},{"location":"Multi%20Teacher%20Distillation/","title":"Multi Teacher Distillation","text":"<ul> <li>The multiple teacher networks can be individually and integrally used for distillation during the period of training a student network.</li> <li>To transfer knowledge from multiple teachers, the simplest way is to use the averaged response from all teachers as the supervision signal (Hinton et al., 2015)</li> <li>In addi- tion to the averaged logits from all teachers, You et al. (2017) further incorporated features from the inter- mediate layers in order to encourage the dissimilarity among different training samples.</li> <li>Fukuda et al. (2017) randomly selected one teacher from the pool of teacher networks at each it- eration. To transfer feature-based knowledge from mul- tiple teachers, additional teacher branches are added to the student networks to mimic the intermediate features of teachers (Park and Kwak, 2020; Asif et al., 2020). Born again networks address multiple teach- ers in a step-by-step manner, i.e., the student at the t step is used as the teacher of the student at the t+1 step (Furlanelloetal., 2018)</li> <li>To effi- ciently perform knowledge transfer and explore the power of multiple teachers, several alternative meth- ods have been proposed to simulate multiple teach- ers by adding different types of noise to a given teacher (Sau and Balasubramanian, 2016) or by us- ing stochastic blocks and skip connections (Lee et al., 2019c). Using multiple teacher models with feature ensembles, knowledge amalgamation is designed in (Shen et al., 2019a; Luo et al., 2019; Shen et al., 2019b; Luo et al., 2020). Through knowledge amalgamation, many public available trained deep models as teachers can be reused.</li> </ul>"},{"location":"Multi%20Variate%20AR/","title":"Multi Variate AR","text":"<ul> <li>predict future from past of another time series</li> </ul>"},{"location":"MultiReader%20technique/","title":"MultiReader Technique","text":"<ul> <li>which allows domain adaptation</li> <li>training a more accurate model that supports multiple keywords (i.e., \u201cOK Google\u201d and \u201cHey Google\u201d) as well as multiple languages/dialects</li> </ul>"},{"location":"Multimodal%20Explanation/","title":"Multimodal Explanation","text":"<ul> <li>The visual explanation was created by an attention mechanism that conveyed knowledge about what region of the image was important for the decision. This explanation guides the generation of the textual justification out of a LSTM feature, which is a prediction of a classification problem over all possible justifications.</li> </ul>"},{"location":"Multinomial%20Distribution/","title":"Multinomial","text":"<ul> <li> \\[P(D|\\theta) = \\frac{N!}{n_{1}!\u2026n_{l}!}\\Pi_{j=1}^{l}\\theta_{j}^{n_{j}}\\] </li> <li>\\(N = n_{1}+ \u2026.+ n_{l}\\)</li> <li>Generalized Binomial Distribution</li> <li>PMF</li> </ul>"},{"location":"Multiple%20Local%20Minima/","title":"Multiple Local Minima","text":""},{"location":"Multiple%20Sclerosis/","title":"Multiple Sclerosis","text":"<ul> <li>A progressive neurodegenerative disease involving damage to the protective myelin sheaths of nerve cells in the brain and spinal cord. Symptoms include impaired movement, pain, and fatigue.</li> </ul>"},{"location":"Multiple%20constraint-based%20theories/","title":"Multiple Constraint-based Theories","text":"<ul> <li>describe language comprehension as an interactive process whereby all possible syntactic representations are simultaneously partially active and competing for more activation across time</li> <li>Unlike the syntax-first models, multiple sources of information, be they syntactic or non-syntactic, integrate immediately to determine the amount of activation provided to each of the competing alternatives</li> </ul>"},{"location":"Multiplicative%20Attention/","title":"Multiplicative Attention","text":"<ul> <li> \\[f_{att}(h_{i}, s_{j}) = h_{i}^{T}W_{a}s_{j}\\] </li> <li>Since Additive Attention performs better for scale, use a factor Scaled Dot Product Attention</li> </ul>"},{"location":"Myelin/","title":"Myelin","text":"<ul> <li>The fatty substance that encases most nerve cell</li> <li>axons, helping to insulate and protect the nerve fiber and effectively speeding up the transmission of nerve impulses.</li> </ul>"},{"location":"Myocardial%20Infarction/","title":"Myocardial Infarction","text":"<ul> <li>Also known as a heart attack, where the heart is deprived of blood due to arterial blockage</li> </ul>"},{"location":"N-dim%20Normal/","title":"N Dim Normal Distribution","text":"<ul> <li>Normal Distribution</li> <li>If data points are vectors \\(x = (x_{1}, \u2026, x_{n})'\\) and RVs X_i fulfill the Central Limit Theorem,</li> <li>PDF \\(\\(p(x) = \\frac{1}{(2\\pi)^{n/2}det(\\Sigma)^{\\frac{1}{2}}}exp\\left(-\\frac{1}{2}(x-\\mu)'\\Sigma^{-1}(x-\\mu)\\right)\\)\\)</li> <li>\\(\\mu\\) is expectation $E[X_{1}, \u2026, X_{n})'x</li> <li> \\[\\Sigma(i,j) = E[(X_{i} - E[X_{i}])(X_{j}-E[X_{j}])]\\] </li> <li></li> <li> \\[\\hat \\mu = \\frac{1}{N}\\Sigma_{i}x_{i}$$ and $$\\hat \\Sigma = \\frac{1}{N-1}\\Sigma_{i}(x_{i}-\\hat\\mu)(x_{i}-\\hat\\mu)'\\] </li> </ul>"},{"location":"NCE/","title":"NCE","text":"<p>title: NCE</p> <p>tags: architecture</p>"},{"location":"NCE/#nce","title":"NCE","text":"<ul> <li>Conditional Negative Sampling for Contrastive Learning of Visual Representations</li> <li>Contrastive Learning</li> <li>noise-contrastive estimation</li> <li>bound on mutual information between two views of an image</li> <li>randomly sampled negative examples to normalize the objective</li> <li>choosing difficult negatives, or those more similar to the current instance, can yield stronger representation</li> <li>Conditional Noise Contrastive Estimator</li> <li>sample negatives conditionally</li> <li>in a \u201cring\u201d around each positive, by approximating the partition function using samples from a class of conditional distributions</li> <li>hese estimators lower-bound mutual information</li> <li>higher bias but lower variance than NCE Bias Vs Variance</li> <li>Applying these estimators as objectives in contrastive representation learning</li> <li>transferring features to a variety of new image distributions from the meta-dataset collection</li> <li>Contrastive Loss</li> </ul>"},{"location":"NIST%202008%20Speaker%20Recognition%20Evaluation%20dataset/","title":"NIST 2008 Speaker Recognition Evaluation Dataset","text":""},{"location":"NIST%20SRE%202016%20Cantonese/","title":"NIST SRE 2016 Cantonese","text":""},{"location":"NLVR2%203/","title":"NLVR2","text":""},{"location":"NUMA/","title":"NUMA","text":"<ul> <li>Shared memory</li> <li>Often made by physically linking two or more SMP</li> <li>One SMP can directly access memory of another SMP</li> <li>Not all processors have equal access time to all memories</li> <li>Memory access across link is slower</li> <li>Cache Coherence</li> </ul>"},{"location":"NaN%20Trap/","title":"NaN Trap","text":"<ul> <li>When one number in your model becomes a NaN during training, which causes many or all other numbers in your model to eventually become a NaN.</li> </ul>"},{"location":"Names%20of%20individuals/","title":"Names of Individuals","text":"<ul> <li>Dan went to the movies.</li> <li>Dan should be understood to be some person named Dan. Although there are many, the speaker had one particular one in mind and the discourse context should tell us which.</li> </ul>"},{"location":"Nasnet/","title":"Nasnet","text":"<ul> <li>Neural Architecture Search</li> <li>Controller RNN (Basic RNN Architectures) produces architectures and evaluated until convergence</li> </ul>"},{"location":"Nativists/","title":"Nativists","text":"<ul> <li>noun and verb and determiner phrase and various sentence constituents are not only real mental primitives, but are innately given inherently linguistic primitives of the mind.</li> </ul>"},{"location":"Nebulizer/","title":"Nebulizer","text":"<ul> <li>A device used to deliver medication in an aerosol form through inhalation</li> </ul>"},{"location":"Negative%20Log%20Likelihood/","title":"Negative Log Likelihood","text":"<ul> <li>Classification, Smaller quicker training, Simple tasks.</li> </ul> \\[ - \\mathrm{sum}\\left( \\log\\left( y \\right) \\right)\\] <ul> <li>Log Likelihood Loss</li> </ul>"},{"location":"Negative%20Sampling/","title":"Negative Sampling","text":"<ul> <li>introduce samples of words that are not neighbors</li> <li></li> <li></li> </ul>"},{"location":"Negative%20Sampling/#backlinks","title":"Backlinks","text":"<ul> <li>Skip Gram</li> <li> <p>also uses Negative Sampling</p> </li> <li> <p>Word2Vec</p> </li> <li>This simple switch changes the model we need from a neural network, to a logistic regression model \u2013 thus it becomes much simpler and much faster to calculate. + Negative Sampling</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Negative%20Set%20Bias/","title":"Negative Set Bias","text":"<ul> <li>Datasets define a visual phenomenon (e.g. object, scene, event) not just by what it is (positive instances), but also by what it is not (negative instances)</li> <li>the space of all possible negatives in the visual world is astronomically large, so datasets are forced to rely on only a small sample</li> <li>ImageNet benefits from a large variability of negative examples and does not seem to be affected by a new external negative set, whereas Caltech and MSRC appear to be just too easy</li> <li>Unfortunately, it's not at all easy to stress-test the sufficiency of a negative set in the general case since it will require huge amounts of labelled (and unbiased) negative data.</li> <li>One remedy, proposed in this paper, is to add negatives from other datasets</li> <li>Another approach, suggested by Mark Everingham, is to use a few standard algorithms (e.g. bag of words) to actively mine hard negatives as part of dataset construction from a very large unlabelled set, and then manually going through them to weed out true positives. The down side is that the resulting dataset will be biased against existing algorithms.</li> </ul>"},{"location":"Negative%20Set%20Bias/#backlinks","title":"Backlinks","text":"<ul> <li>Unbiased Look at Dataset Bias</li> <li>Negative Set Bias</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Nesterov%20Momentum/","title":"Nesterov Momentum","text":"<ul> <li>$$\\begin{align}</li> </ul> <p>&amp;v_{t}= \\gamma v_{t+1}+\\eta \\cdot \\nabla_{\\theta}J(\\theta - \\gamma v_{t-1}) \\</p> <p>&amp;\\theta = \\theta- v_{t}\\</p> <p>\\end{align}$$</p>"},{"location":"Network%20Dissection%20Quantifying%20Interpretability%20of%20Deep%20Visual%20Representions/","title":"Network Dissection Quantifying Interpretability of Deep Visual Representions","text":"<ul> <li>David Bau\u2217, Bolei Zhou\u2217, Aditya Khosla, Aude Oliva, and Antonio Torralba</li> </ul> <ul> <li>Quantifying the interpretability of latent representations of CNNs by evaluating the alignment between individual hidden units and a set of semantic concepts</li> <li>score the semantics of hidden units at each intermediate convolutional layer.</li> <li>The units with semantics are given labels across a range of objects, parts, scenes, textures, materials, and colors.</li> <li>interpretability of units is equivalent to random linear combinations of units</li> <li>analyze the effect of training iterations, compare networks trained with different initializations, examine the impact of network depth and width, and measure the effect of dropout and batch normalization on the interpretability of deep visual representations</li> </ul>"},{"location":"Network%20Dissection%20Quantifying%20Interpretability%20of%20Deep%20Visual%20Representions/#introduction","title":"Introduction","text":"<ul> <li>The emergence of interpretable structure suggests that deep networks may be learning disentangled representations spontaneously.</li> <li>A disentangled representation aligns its variables with a meaningful factorization of the underlying problem structure</li> <li>Broden</li> </ul>"},{"location":"Network%20Dissection%20Quantifying%20Interpretability%20of%20Deep%20Visual%20Representions/#network-dissection","title":"Network Dissection","text":"<ul> <li>Our measurement of interpretability for deep visual representations proceeds in three steps: 1. Identify a broad set of human-labeled visual concepts. 2. Gather hidden variables' response to known concepts. 3. Quantify alignment of hidden variableconcept pairs.</li> <li>In a fully interpretable local coding such as a one-hotencoding, each variable will match exactly with one humaninterpretable concept.</li> <li>Therefore we measure the alignment between single units and single interpretable concepts</li> <li>This does not gauge the discriminative power of the representation; rather it quantifies its disentangled interpretability.</li> <li>We then measure the alignment of each hidden unit of the CNN with each concept by evaluating the feature activation of each individual unit as a segmentation model for each concept</li> <li>To quantify the interpretability of a layer as a whole, we count the number of distinct visual concepts that are aligned with a unit in the layer</li> <li>Broden</li> </ul>"},{"location":"Network%20Dissection%20Quantifying%20Interpretability%20of%20Deep%20Visual%20Representions/#scoring-unit-interpretability","title":"Scoring Unit Interpretability","text":"<ul> <li>evaluates every individual convolutional unit in a CNN as a solution to a binary segmentation task to every visual concept in Broden</li> <li>applied to any CNN using a forward pass without the need for training or backpropagation.</li> <li>For every input image x in the Broden dataset, the activation map \\(A_{k}(x)\\) of every internal convolutional unit k is collected.</li> <li>Then the distribution of individual unit activations ak is computed</li> <li>For each unit k, the top quantile level \\(T_{k}\\) is determined such that \\(P(a_{k} &gt; T_{k} = 0.005\\) over every spatial location of the activation map in the data set.</li> <li>input-resolution annotation mask \\(L_{c}\\) for some concept c</li> <li>the activation map is scaled up to the mask resolution \\(S_{k}(x)\\) from \\(A_{k}(x)\\) using bilinear interpolation, anchoring interpolants at the center of each unit's receptive field</li> <li>\\(S_{k}(x)\\) is then thresholded into a binary segmentation: \\(M_{k}(x) \\equiv S_{k}(x) \\leq T_{k}\\), selecting all regions for which the activation exceeds the threshold Tk. These segmentations are evaluated against every concept c in the data set by computing intersections \\(M_{k}(x) \\cap L_{c}(x)\\), for every (k, c) pair.</li> <li>The score of each unit k as segmentation for concept c is reported as a data-set-wide intersection over union score \\(\\(I_{o}U_{k,c}=\\frac{\\Sigma|M_{k}(x) \\cap L_{c}(x|}{\\Sigma|M_{k}(x) \\cup L_{c}(x)|}\\)\\)</li> <li>where | \u00b7 | is the cardinality of a set.</li> <li>The value of \\(IoU_{k,c}\\) is the accuracy of unit k in detecting concept c; we consider one unit k as a detector for concept c if IoUk,c exceeds a threshold</li> <li>Our qualitative results are insensitive to the IoU threshold: different thresholds denote different numbers of units as concept detectors</li> <li>For our comparisons we report a detector if IoUk,c &gt; 0.04.</li> <li>one unit might be the detector for multiple concepts; for the purpose of our analysis, we choose the top ranked label</li> <li>The IoU evaluating the quality of the segmentation of a unit is an objective confidence score for interpretability that is comparable across networks</li> <li>Note that network dissection works only as well as the underlying data set</li> <li>We conclude that interpretability is neither an inevitable result of discriminative power, nor is it a prerequisite to discriminative power.</li> <li>Instead, we find that interpretability is a different quality that must be measured separately to be understood.</li> </ul>"},{"location":"Network%20Dissection%20Quantifying%20Interpretability%20of%20Deep%20Visual%20Representions/#measure-of-axis-aligned-interpretability","title":"Measure of Axis Aligned Interpretability","text":""},{"location":"Network%20Dissection%20Quantifying%20Interpretability%20of%20Deep%20Visual%20Representions/#disentangled-concepts-by-layer","title":"Disentangled Concepts by Layer","text":"<ul> <li>Confirming intuition, color and texture concepts dominate at lower layers conv1 and conv2 while more object and part detectors emerge in conv5.</li> </ul>"},{"location":"Network%20Dissection%20Quantifying%20Interpretability%20of%20Deep%20Visual%20Representions/#network-architectures-and-supervisions","title":"Network Architectures and Supervisions","text":"<ul> <li>In terms of network architecture, we find that interpretability of ResNet &gt; VGG &gt; GoogLeNet &gt; AlexNet</li> <li>Deeper architectures appear to allow greater interpretability. Places &gt; ImageNet.</li> <li>Self-supervised models create many texture detectors but relatively few object detectors; apparently, supervision from a self-taught primary task is much weaker at inferring interpretable concepts than supervised training on a large annotated data set</li> <li>The form of self-supervision makes a difference: for example, the colorization model is trained on colorless images, and almost no color detection units emerge</li> <li>We hypothesize that emergent units represent concepts required to solve the primary task.</li> </ul>"},{"location":"Network%20Dissection%20Quantifying%20Interpretability%20of%20Deep%20Visual%20Representions/#training-conditions-vs-interpretability","title":"Training Conditions Vs. Interpretability","text":"<ul> <li>We can see that object detectors and part detectors begin emerging at about 10,000 iterations (each iteration processes a batch of 256 images)</li> <li>We do not find evidence of transitions across different concept categories during training</li> <li>For example, units in conv5 do not turn into texture or material detectors before becoming object or part detectors.</li> <li>Comparing different random initializations, the models converge to similar levels of interpretability, both in terms of the unique detector number and the total detector number; this matches observations of convergent learning</li> <li>For the network without dropout, more texture detectors emerge but fewer object detectors</li> <li>Batch normalization seems to decrease interpretability significantly.</li> <li>The batch normalization result serves as a caution that discriminative power is not the only property of a representation that should be measured.</li> <li>batch normalization 'whitens' the activation at each layer, which smooths out scaling issues and allows a network to easily rotate axes of intermediate representations during training</li> <li>While whitening apparently speeds training, it may also have an effect similar to random rotations analyzed in Sec. 3.2 which destroy interpretability</li> <li>interpretability is neither a prerequisite nor an obstacle to discriminative power</li> <li></li> <li></li> <li></li> </ul>"},{"location":"Network%20Dissection%20Quantifying%20Interpretability%20of%20Deep%20Visual%20Representions/#discrimination-vs-interpretability","title":"Discrimination Vs. Interpretability","text":"<ul> <li>For each trained model, we extract the representation at the highest convolutional layer, and train a linear SVM with C = 0.001 on the training data for action40 action recognition task</li> <li>Thus the supervision tasks that encourage the emergence of more concept detectors may also improve the discrimination ability of deep features.</li> <li>accuracy on a representation when applied to a task is dependent not only on the number of concept detectors in the representation, but on the suitability of the set of represented concepts to the transfer task.</li> <li></li> </ul>"},{"location":"Network%20Dissection%20Quantifying%20Interpretability%20of%20Deep%20Visual%20Representions/#layer-width-vs-interpretability","title":"Layer Width Vs. Interpretability","text":"<ul> <li>Depth has been shown to be important to high discrimination ability</li> <li>increasing the number of convolutional units at a layer significantly increases computational cost while yielding only marginal improvements in classification accuracy</li> <li>carefully designed wide residual network can achieve classification accuracy superior to the commonly used thin and deep counterparts.</li> <li>This may indicate a limit on the capacity of AlexNet to separate explanatory factors; or it may indicate that a limit on the number of disentangled concepts that are helpful to solve the primary task of scene classification.</li> <li></li> </ul>"},{"location":"Network%20Dissection%20Quantifying%20Interpretability%20of%20Deep%20Visual%20Representions/#backlinks","title":"Backlinks","text":"<ul> <li>01:26Network Dissection Quantifying Interpretability of Deep Visual Representions</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Neural%20Augmentation/","title":"Neural Augmentation","text":"<ul> <li>The Neural Style Transfer algorithm requires two parameters for the weights of the style and content loss. Perez and Wang presented an algorithm to meta-learn a Neural Style Transfer strategy called Neural Augmentation.</li> <li>e Neural Augmentation approach takes in two random images from the same class. The prepended augmentation net maps them into a new image through a CNN with 5 layers, each with 16 channels, 3\u00d73 filters, and ReLU activation functions. The image outputted from the augmentation is then transformed with another random image via Neural Style Transfer.</li> <li>This style transfer is carried out via the CycleGAN extension of the GAN framework</li> <li>These images are then fed into a classification model and the error from the classification model is backpropagated to update the Neural Augmentation net.</li> <li>The Neural Augmentation network uses this error to learn the optimal weighting for content and style images between different images as well as the mapping between images in the CNN</li> <li>The Neural Augmentation techniques tested consist of three levels based on the design of the loss function for the augmentation net (Content loss, Style loss via gram matrix, and no loss computer at this layer)</li> </ul>"},{"location":"Neural%20Augmentation/#backlinks","title":"Backlinks","text":"<ul> <li>AutoAugment</li> <li> <p>much different approach to meta-learning than Neural Augmentation or Smart Augmentation</p> </li> <li> <p>[[Data Augmentation with Curriculum Learning]]</p> </li> <li> <p>Samples taken from GANs can be augmented with traditional augmentations such as lighting filters, or even used in neural network augmentation strategies such as Smart Augmentation or Neural Augmentation to create even more samples. These samples can be fed into further GANs and dramatically increase the size of the original dataset.</p> </li> <li> <p>Smart Augmentation</p> </li> <li>utilizes a similar concept as the Neural Augmentation technique</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Neural%20Chimera/","title":"Neural Chimera","text":"<ul> <li>A research model where human stem cells are transplanted into an animal embryo to follow the genetic, molecular, and functional processes of brain cells as they grow.</li> </ul>"},{"location":"Neural%20Dynamics/","title":"Neural Dynamics","text":"<ul> <li>Continous -&gt; Discrete seq of words</li> <li>Use NN to generate hypothesis outputs vectors<ul> <li>As many components as possible target symbols</li> </ul> </li> </ul>"},{"location":"Neural%20Induction/","title":"Neural Induction","text":"<ul> <li>A developmental process where ectodermal cells \u201cdecide\u201d to form the neural plate, the basis of what will eventually become the organism\u2019s nervous system.</li> </ul>"},{"location":"Neural%20Network%20Architecture%20Cheat%20Sheet/","title":"Neural Network Architecture Cheat Sheet","text":"<ul> <li>Spiking Networks</li> <li>Hidden Models</li> <li>Capsule Network</li> <li>Probability</li> <li>Recurrent</li> <li>Conv</li> </ul>"},{"location":"Neural%20Probabilistic%20Model/","title":"Neural Probabilistic Model","text":"<ul> <li>A Neural Probabilistic Language Model</li> <li>more compact and smoother representations based on distributed representations that can accommodate far more conditioning variables</li> <li>learning the joint probability function of sequences of words in a language was intrinsically difficult because of the curse of dimensionality</li> <li>learning a distributed representation for words which allows each training sentence to inform the model about an exponential/combinatorial number of semantically neighboring sentences</li> <li>The model learns simultaneously (i) a distributed representation for each word along with (ii) the probability function for word sequences, expressed in terms of these representations</li> <li>Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar</li> <li>significantly improves on state-of-the-art n gram models</li> </ul>"},{"location":"Neural%20Radiance%20Field/","title":"Neural Radiance Field","text":"<ul> <li>NeRF: Representing Scenes As Neural Radiance Fields for View Synthesis<ul> <li>synthesizing novel views of complex scenes</li> <li>optimizing an underlying continuous volumetric scene function using a sparse set of input views</li> <li>single continuous 5D coordinate (spatial location (x,y,z) and viewing direction (\u03b8,\u03d5))</li> <li>output is the volume density and view-dependent emitted radiance at that spatial location</li> <li>querying 5D coordinates along camera rays</li> <li>volume rendering techniques to project the output colors and densities into an image</li> <li>volume rendering is naturally differentiable</li> <li>set of images with known camera poses</li> <li>They describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes</li> </ul> </li> </ul>"},{"location":"Neural%20Text%20Degeneration/","title":"Neural Text Degeneration","text":"<ul> <li>The Curious Case of Neural Text Degeneration</li> <li>deep analysis into the properties of the most common decoding methods for open-ended language generation</li> <li>surprising distributional differences between human text and machine text</li> <li>decoding strategies alone can dramatically effect the quality of machine text, even when generated from exactly the same neural language model</li> <li>likelihood maximizing decoding causes repetition and overly generic language usage</li> <li>sampling methods without truncation risk sampling from the low-confidence tail of a model\u2019s predicted distribution</li> <li>Nucleus Sampling</li> </ul>"},{"location":"Neuroaesthetics/","title":"Neuroaesthetics","text":"<ul> <li>A field within cognitive neuroscience that examines the neural underpinnings of what humans find visually appealing or beautiful.</li> </ul>"},{"location":"Neurogenesis/","title":"Neurogenesis","text":"<ul> <li>The production of new, maturing neurons by neural stem and progenitor cells. Rapid and widespread neurogenesis obviously occurs in the fetal brain in humans and other animals, but neuroscientists long believed that neurogenesis essentially does not occur in the adult human brain.</li> <li>However, over the past two decades, research has shown that it does in fact occur in the dentate gyrus of the hippocampus and possibly other brain regions. This \u201cadult neurogenesis\u201d appears to be vital for normal learning and memory, and may help protect the brain against stress and depression.</li> </ul>"},{"location":"Neuroplasticity/","title":"Neuroplasticity","text":"<ul> <li>Also referred to as brain plasticity or neural plasticity, this is the ability of the brain to change throughout the lifespan, forming new synapses and neural connections in response to the environment.</li> </ul>"},{"location":"Newtons%20Laws/","title":"Newtons Laws","text":"<ul> <li>Inertia</li> <li>Force</li> <li>Equal And Opposite Force Pairs</li> </ul>"},{"location":"No%20bias%20decay/","title":"No Bias Decay","text":"<ul> <li>No Learning Rate Decay tricks</li> <li>Equivalent to Lp Regularization L2 to all parameters to drive the values towards 0</li> <li>Only apply Regularization to the weights</li> <li>Leave Batch Normalization Layers alone</li> <li>LARS</li> </ul>"},{"location":"Node%20Distribution/","title":"Node Distribution","text":"<p>-</p>"},{"location":"Node%20Link%20Diagram/","title":"Node LInk Diagram","text":"<ul> <li>Vertices (Nodes) are mapped to graphical shapes circles, squares, triangles, etc.</li> <li>Edges (Links) are mapped to straight or curved lines</li> <li>Nodes can freely be positioned</li> <li>Cross Minimization</li> <li>Bend Minimization</li> <li>Area Minimization</li> <li>Cross angle Maximization</li> <li>Length Optimization</li> <li>Symmetries Node Link</li> <li>Node Distribution</li> <li>Force Directed Graph Layout</li> <li>Hierarchical Edge Bundling</li> </ul>"},{"location":"Noise%20Injection/","title":"Noise Injection","text":"<ul> <li>injecting a matrix of random values usually drawn from a Gaussian distribution</li> <li>Adding noise to images can help CNNs learn more robust features.</li> </ul>"},{"location":"Noise%20Suppression/","title":"Noise Suppression","text":"<ul> <li>reduce the intensity variation in big structures (such as organs in medical imaging data) -improve the detectability of edges between big structures,</li> <li>preserve small scale structures - Conv Based Noise Reduction</li> <li>Average Filter</li> <li>Gaussian Filter</li> <li>Mesh Smoothing</li> <li>Laplacian Grid Smoothing</li> </ul>"},{"location":"Noise%20Tunnel/","title":"Noise Tunnel","text":""},{"location":"Noise%20Tunnel/#backlinks","title":"Backlinks","text":"<ul> <li>Vision Explainibility</li> <li>Noise Tunnel</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Noisy%20Relu/","title":"Noisy Relu","text":"<ul> <li> \\[f(x) = max(0, x+Y) $$ where $$Y\\in Normal(0,1)\\] </li> </ul>"},{"location":"Non%20Relational%20Inductive%20Bias/","title":"Non Relational Inductive Bias","text":"<ul> <li>Activation Functions<ul> <li>allow the model to capture the non-linearity hidden in the data</li> </ul> </li> <li>Dropout<ul> <li>helps the network avoid memorizing the data by forcing random subsets of the network to each learn the data pattern. As a result, the obtained model, in the end, is able to generalize better</li> </ul> </li> <li>Weight Decay<ul> <li>puts constraints on the model\u2019s weights</li> </ul> </li> <li>Batch Normalization , Layer Normalization , Instance Normalization<ul> <li>Reduces Covariate Shift</li> </ul> </li> <li>Augmentation</li> <li>Optimizers</li> </ul>"},{"location":"Non-adjacent%20dependencies/","title":"Non-adjacent Dependencies","text":"<ul> <li>Wh-dependencies</li> <li>Extra-position</li> <li>Object-relative clauses</li> <li>Subject relative</li> <li>Subject-verb agreement</li> </ul>"},{"location":"Non-response%20Bias/","title":"Non-response Bias","text":"<ul> <li>(also called participation bias)</li> <li>Users from certain groups opt-out of surveys at different rates than users from other groups.</li> </ul>"},{"location":"Nonstationarity/","title":"Nonstationarity","text":"<ul> <li>A feature whose values change across one or more dimensions, usually time. For example, the number of swimsuits sold at a particular store demonstrates nonstationarity because that number varies with the season. As a second example, the quantity of a particular fruit harvested in a particular region typically shows sharp nonstationarity over time.</li> </ul>"},{"location":"Nootropics/","title":"Nootropics","text":"<ul> <li>Drugs or supplements that are marketed as ways to improve cognitive functions like memory, attention, or creativity.</li> </ul>"},{"location":"Normal%20Distribution/","title":"Normal Distribution","text":"<ul> <li> \\[p(x) = \\frac{1}{\\sqrt{2\\pi\\sigma}}e^{-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}}\\] </li> <li>Mean \\(\\mu\\) and std \\(\\sigma\\). \\(\\mu\\) is max and \\(\\mu \\pm \\sigma\\) is locations of zeros of second derivative</li> <li></li> <li>\\(\\mathcal{N}(0,1)\\)</li> <li>Central Limit Theorem</li> </ul>"},{"location":"Normal%20Distribution/#properties","title":"Properties","text":"<ul> <li>Linear combinations of normal distributed independant RVs are normal distributed</li> <li>X,Y have means \\(\\mu\\) and v and variances \\(\\sigma^{2}\\) and \\(\\tau^{2}\\). Then \\(aX + bY\\) is normally distributed and has mean : \\(a\\mu + bv\\) and variance \\(\\alpha^{2}\\sigma^{2}+b^{2}\\tau^{2}\\)</li> </ul>"},{"location":"Normal%20Distribution/#computing-the-value","title":"Computing the Value","text":"<ul> <li> \\[\\int_{a}^{b} \\frac{1}{\\sqrt{2\\pi\\sigma}}e^{-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}}dx\\] </li> <li>Transform \\(\\mathscr{N}(\\mu, \\sigma^{2})\\) to \\(\\mathscr{N}(0,1)\\)</li> <li> \\[Z = \\frac{X-\\mu}{\\sigma}\\] </li> <li> \\[\\int_{\\frac{a-\\mu}{\\sigma}}^{\\frac{b-\\mu}{\\sigma}}\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{(x)^{2}}{2}}dx\\] </li> <li>Compute by using Cumulative density function \\(\\phi\\)</li> <li>Iterative solvers</li> <li> \\[\\phi(\\frac{b-\\mu}{\\sigma})-\\phi(\\frac{a-\\mu}{\\sigma})\\] </li> <li> \\[\\hat \\mu = \\frac{1}{N}\\Sigma_{i}(x_{i})$$ $$\\hat \\sigma^{2}= \\frac{1}{N-1}\\Sigma_{i}(x_{i}-\\hat\\mu)^2\\] </li> </ul>"},{"location":"Normalized%20Inverted%20Structural%20Similarity%20Index/","title":"Normalized Inverted Structural Similarity Index","text":"<ul> <li>metric is calculated from Structural Similarity Index by inverting the range and then normalizing it.</li> <li>NISSIM bounds to (0, 1] where 0 means similar and 1 means dissimilar. Ideally we want this value as close to 0 as possible</li> <li> \\[NISSIM_{i}= \\frac{1-SSIM_{i}}{2}\\] </li> </ul>"},{"location":"Norms%20as%20a%20basis%20for%20governing%20sociotechnical%20systems/","title":"Norms as a Basis for Governing Sociotechnical Systems","text":"<ul> <li>Munindar P. Singh</li> <li>framework that uses social norms to govern autonomous entities' (e.g., AI agents' or human beings') behaviours</li> <li>inherently distributed rather than relying on a central authority</li> <li>Individuals maintain their autonomy through executing their own decision policies, but are subjected to social norms defined by the collective through roles</li> <li>Social norms are defined through a template containing codified commitment, authorization, prohibition, sanction and power</li> </ul>"},{"location":"Norms%20as%20a%20basis%20for%20governing%20sociotechnical%20systems/#backlinks","title":"Backlinks","text":"<ul> <li>Building Ethics into Artificial Intelligence</li> <li>Norms as a basis for governing sociotechnical systems</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Notch%20filter/","title":"Notch Filter","text":"<ul> <li>A notch filter is a type of band-stop filter, which is a filter that attenuates frequencies within a specific range while passing all other frequencies unaltered</li> </ul>"},{"location":"Nucleotide%20Sequence/","title":"Nucleotide Sequence","text":"<ul> <li>A specific and ordered array of nucleotides that make up a specific genetic variant or allele.</li> </ul>"},{"location":"Nucleotide/","title":"Nucleotide","text":"<ul> <li>Sometimes referred to as a nucleic acid, these are the biological building blocks of DNA.</li> </ul>"},{"location":"Nucleus%20Accumbens/","title":"Nucleus Accumbens","text":"<ul> <li>Part of the brain\u2019s reward circuitry, or mesolimbic pathway, this small region in the midbrain releases dopamine in response to rewarding experiences.</li> </ul>"},{"location":"Nucleus%20Sampling/","title":"Nucleus Sampling","text":"<ul> <li>Nucleus (or top-p) Sampling, a simple but effective method that captures the region of confidence of language models effectively to draw the best out of neural generation</li> <li>By sampling text from the dynamic nucleus of the probability distribution, which allows for diversity while effectively truncating the less reliable tail of the distribution, the resulting text better demonstrates the quality of human text, yielding enhanced diversity without sacrificing fluency and coherence.</li> </ul>"},{"location":"Numerically%20Quantified%20Expressions/","title":"Numerically Quantified Expressions","text":""},{"location":"Numerically%20Quantified%20Expressions/#numerically-quantified-expressions","title":"Numerically Quantified Expressions","text":"<ul> <li>NQEs express<ul> <li>Plurality</li> <li>Cardinality</li> </ul> </li> <li>Can be in scopal relations with other expressions</li> </ul>"},{"location":"OPT/","title":"OPT","text":"<ul> <li>OPT: Open Pre-trained Transformer Language Models</li> <li>Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning</li> <li>collection of auto-regressive/decoder-only pre-trained transformer-based language models ranging in size from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers</li> <li>replicate the performance and sizes of the GPT-3 class of models, while also applying the latest best practices in data curation and training efficiency</li> <li>OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop</li> </ul>"},{"location":"OSIE/","title":"OSIE","text":"<ul> <li>700 natural indoor and outdoor scenes, aesthetic photographs from Flickr and Google.</li> <li>The fixations were measured while 15 observers looked at the image for 3 s.</li> <li>the fixations for all observers were collected and blurred using the Gaussian filter with the standard deviation equivalent to 1\u00b0 in the visual angle</li> </ul>"},{"location":"Object-relative%20clauses/","title":"Object-relative Clauses","text":"<ul> <li>[The report] that the senator attacked [] admitted the error.</li> <li>[The senator] that the report attacked [] admitted the error.</li> </ul>"},{"location":"Oblique%20Slicing/","title":"Oblique Slicing","text":"<ul> <li>Resample the data on arbitrarily oriented slices</li> <li>Exploit 3D texture mapping functionality</li> <li>Store volume in 3D texture</li> </ul>"},{"location":"Occipital%20lobe/","title":"Occipital Lobe","text":"<ul> <li>Interprets vision (color, light, movement)</li> </ul>"},{"location":"Occlusion/","title":"Occlusion","text":""},{"location":"Occlusion/#occlusion","title":"Occlusion","text":"<ul> <li>occurs if a target object is hidden (occluded) by other objects Self-occlusion</li> <li>from a certain viewpoint, one part of an object is occluded by another part.</li> </ul>"},{"location":"Occult%20Blood%20Screen/","title":"Occult Blood Screen","text":"<ul> <li>Use of a chemically treated card or pad to test for blood hidden in a stool sample</li> </ul>"},{"location":"Offline%20Distillation/","title":"Offline Distillation","text":"<ul> <li>The first stage in offline distillation is usually not discussed as part of knowledge distillation, i.e., it is assumed that the teacher model is pre-defined. Little at- tention is paid to the teacher model structure and its re- lationship with the student model</li> <li>The main advantage of offline methods is that they are simple and easy to be implemented. For example, the teacher model may contain a set of mod- els trained using different software packages, possibly located on different machines. The knowledge can be extracted and stored in a cache.</li> <li>The offline distillation methods usually employ one- way knowledge transfer and two-phase training pro- cedure. However, the complex high-capacity teacher model with huge training time can not be avoided, while the training of the student model in offline distillation is usually efficient under the guidance of the teacher model.</li> <li>Moreover, the capacity gap between large teacher and small student always exists, and student often largely relies on teacher.</li> </ul>"},{"location":"Ogliodendrocytes/","title":"Ogliodendrocytes","text":"<ul> <li>Wrap and insulate, forms Myelin sheath</li> </ul>"},{"location":"Ohms%20Law/","title":"Ohms Law","text":"<p>- Potential difference = current x resistance \u00a0- \\(\\(V= IR\\)\\) \u00a0- Ohm's Law applied to the full circuit: Electromotive force = current x (sum of the circuit resistance and the internal resistance of the cell) \u00a0- \\(\\(EMF = I(R+r)\\)\\)</p>"},{"location":"On%20the%20overlap%20between%20Grad-CAM%20saliency%20maps%20and%20explainable%20visual%20features%20in%20skin%20cancer%20images/","title":"On the Overlap Between Grad-CAM Saliency Maps and Explainable Visual Features in Skin Cancer Images","text":"<ul> <li>Nunnari, Fabrizio, Md Abdul Kadir, and Daniel Sonntag. 2021. \u201cOn the Overlap Between Grad-CAM Saliency Maps and Explainable Visual Features in Skin Cancer Images.\u201d Pp. 241\u201353 in Machine Learning and Knowledge Extraction. Vol. 12844, Lecture Notes in Computer Science, edited by A. Holzinger, P. Kieseberg, A. M. Tjoa, and E. Weippl. Cham: Springer International Publishing.</li> </ul>"},{"location":"On%20the%20overlap%20between%20Grad-CAM%20saliency%20maps%20and%20explainable%20visual%20features%20in%20skin%20cancer%20images/#intro","title":"Intro","text":"<ul> <li>Dermatologists recognize melanomas by inspecting images in which they identify human-comprehensible visual features.</li> <li>investigate to what extent such features correspond to the saliency areas identified on CNNs trained for classification</li> <li>Saliency maps are images that indicate the pixels areas contributing to a certain classification decision. Saliency maps are normally encoded as greyscale images or converted to heatmaps for visual inspection.</li> <li>to what extent saliency maps can be used to identify visual features of skin lesions</li> </ul>"},{"location":"On%20the%20overlap%20between%20Grad-CAM%20saliency%20maps%20and%20explainable%20visual%20features%20in%20skin%20cancer%20images/#related-work","title":"Related Work","text":"<ul> <li>ISIC 2018</li> <li>RISE</li> <li>Jahanifar et al. also propose a modified DRFI (Discriminative Regional Feature Integration) technique for a similar task for multi-level segmentation task</li> <li>By combining multiple segmentation masks, they produce a more accurate mask.</li> <li>During the generation of the mask, they use a threshold value of 0.5, but they did not provide a reason for which they choose this value.</li> </ul>"},{"location":"On%20the%20overlap%20between%20Grad-CAM%20saliency%20maps%20and%20explainable%20visual%20features%20in%20skin%20cancer%20images/#classification-architectures-and-models","title":"Classification Architectures and Models","text":"<ul> <li>RESNET50</li> <li>VGG16</li> </ul>"},{"location":"On%20the%20overlap%20between%20Grad-CAM%20saliency%20maps%20and%20explainable%20visual%20features%20in%20skin%20cancer%20images/#data-preparation","title":"Data Preparation","text":"<ul> <li>As an additional feature, we compute the pixels-wise union of all the features</li> <li>In our experiments, we ignore the skin lesion samples with no features.</li> <li>The generation of the saliency maps consists of running the Grad-CAM algorithm on each skin lesion picture with non-black union mask</li> <li>We repeat the procedure for both the VGG16 and the RESNET50 models, generating the SV and SR greyscale picture sets</li> <li>To compare the saliency maps with ground truth maps, we scaled up SV and SR to the resolution of the original images using a nearest neighbour filter.</li> <li>We can observe that all distributions are strongly right skewed, and all \\(J_{s}\\) are mostly below 0.2, with the exception of a peak in performance for the pigment network clas</li> </ul>"},{"location":"On%20the%20overlap%20between%20Grad-CAM%20saliency%20maps%20and%20explainable%20visual%20features%20in%20skin%20cancer%20images/#first-experiment","title":"First Experiment","text":"<ul> <li>With the first experiment we aim at identifying the threshold value that leads to a maximization of the overlap between saliency maps and ground truth</li> <li>To do so, we converted each saliency map into 11 binary maps using thresholds from 0.0 to 1.0 with steps of 0.1</li> <li>Then, we proceed by computing the Jaccard indices J between the ground truth and all of the processed saliencies S x V and S x R.</li> <li>For VGG16, among the features classes, the best threshold ranges between 0.4 and 0.7. The minimum J index is 0.0 on all categories, meaning that among all samples there is always at least one map with zero-overlap with the ground truth. The highest average (J=0.141) and maximum (J=0.797) belong to the pigmented network class.</li> <li>When switching to RESNET50, the best thresholds range between 0.3 and 0.7. With respect to VGG16, pigmented network and streaks present the worse performance, while the average J increases for the other three classes</li> <li>Surprisingly, the Jaccard indices measured with the RESNET50 maps, which have a resolution limited to 8x8 pixels, are comparable to the ones extracted from the VGG16 models (24x24 pixels)</li> <li>The second hypothesis is that the lower resolution of the RESNET50 maps is compensated by the higher accuracy of the classification model, i.e., a better overall overlap.</li> </ul>"},{"location":"On%20the%20overlap%20between%20Grad-CAM%20saliency%20maps%20and%20explainable%20visual%20features%20in%20skin%20cancer%20images/#second-experiment","title":"Second Experiment","text":"<ul> <li>diving the samples into Melanoma and Nevus, and into correctly vs. wrongly classified samples.</li> <li>Here, the Jaccard indices are calculated using the union feature and using the best threshold identified in the first experiment, hence on S 0.5 R V and S 0.3</li> <li>For VGG16, we can observe that the mean J for correctly classified melanomas (0.135) is similar to the union class average (0.132).</li> <li>However, when melanomas are wrongly classified, the Jaccard index drops to 0.086, meaning that the saliency maps diverges from the ground truth</li> </ul>"},{"location":"On%20the%20overlap%20between%20Grad-CAM%20saliency%20maps%20and%20explainable%20visual%20features%20in%20skin%20cancer%20images/#observation","title":"Observation","text":"<ul> <li>This could effectively help doctors is spotting a wrong classification</li> <li>The idea is that: if the classifier tells the doctor that the sample is a melanoma, but then the reported saliency areas diverge a lot from what would be manually marked, then doctors can be more easily induced to think that the system is misclassifying the image</li> </ul>"},{"location":"On%20the%20overlap%20between%20Grad-CAM%20saliency%20maps%20and%20explainable%20visual%20features%20in%20skin%20cancer%20images/#discussion","title":"Discussion","text":"<ul> <li>Among the five features, only Pigment Network reaches the same level of accuracy of the union class.</li> <li>maximum J=0.136</li> <li>This is a huge annotation overhead when compared to labeling images with their diagnose class.</li> <li>The value of the threshold to reach the best J index varies among datasets and features. Since it is not possible to analytically foresee the best threshold of a given dataset, we suggest the development of interactive exploratory visual interfaces, where dermatologists can autonomously control the saliency threshold value in an interactive fashion for exploration.</li> <li>However, from a decomposition between classes and correctness of classification, it appears that, for higher resolution maps (24x24 pixels on VGG16), saliency maps overlap much better with ground truth features when the classifier is correctly classifying a melanoma (J=0.135) and performance drops when the prediction is incorrect (J=0.086).</li> <li>Further, we would like to investigate on better options for thresholding. In this paper, a global threshold, in the range of 0.0 to 1.0, was simultaneously searched and applied to all the saliency map.</li> <li>This allows for an \"emersion\" of the most relevant region of interests of a global scale</li> <li>However, there might be regions of saliency below the global threshold which are relevant with respect to the local surrounding area</li> <li>To spot local maxima, we could split the maps into tiles, or super-pixels, and iteratively identify multiple local threshold values based on the range of saliency values of each region.</li> <li>Finally, the current implementation of Grad-CAM returns saliency maps whose range is filled by stretching the range of activation values of the target convolution layer.</li> <li>Each saliency map is forced to use the full activation range, independent of other samples.</li> <li>In so doing, regions of interests are \"forced\" to emerge, even when the activation values of the inner layer are lower when compared to other images.</li> </ul>"},{"location":"On%20the%20overlap%20between%20Grad-CAM%20saliency%20maps%20and%20explainable%20visual%20features%20in%20skin%20cancer%20images/#future-work","title":"Future Work","text":"<ul> <li>As future work, we could consider performing saliency normalization according to global statistics (mean and variance) on the tested set.</li> </ul>"},{"location":"On%20the%20overlap%20between%20Grad-CAM%20saliency%20maps%20and%20explainable%20visual%20features%20in%20skin%20cancer%20images/#images","title":"Images","text":""},{"location":"One%20hot/","title":"One Hot Encoding","text":"<ul> <li>Given \\(A = {a_{1}, \u2026 , a_{k}}\\)</li> <li>Turn each \\(a_{v}\\) into k dim binary vector \\(v_{v} \\in {0,1}^{k}\\) which is 0 everywhere execpt at position v</li> <li>Symbolic input</li> <li>k dim one hot vector</li> <li></li> </ul>"},{"location":"Opacity%20Correction/","title":"Opacity Correction","text":"<ul> <li>Opacity component \\(o\\) in transfer function stored with respect to a standard step size \\(\\Delta t\\)</li> <li>Different step sizes \\(\\Delta t^{\\ast}\\)</li> <li>Dynamic step sizes</li> <li>generate pre-integrated function</li> <li> \\[o^{\\ast} = 1- (1-o)^{\\frac{\\Delta t^{\\ast}}{\\Delta t}}\\] </li> <li>evaluate after obtaining \\(o\\) from transfer function</li> <li>apply before compositing`</li> </ul>"},{"location":"Operator%20Fusion/","title":"Operator Fusion","text":"<ul> <li>Some DirectML operators support a concept known as fusion. Operator fusion is a way to improve performance by merging one operator (typically, an activation function) into a different operator so that they are executed together without requiring a roundtrip to memory.</li> <li>https://arxiv.org/abs/2108.13342</li> </ul>"},{"location":"Ophthalmoscope/","title":"Ophthalmoscope","text":"<ul> <li>An instrument used to examine the eye's fundus, retina and other structures</li> </ul>"},{"location":"Opportunistic%20Learning/","title":"Opportunistic Learning","text":""},{"location":"Opportunistic%20Learning/#opportunistic-learning","title":"Opportunistic Learning","text":"<ul> <li>apart from learning from a batch of labelled training data at predefined times or according to a predefined training schedule, the robot must be prepared to accept a new example when it is observed or becomes available.</li> </ul>"},{"location":"Opportunistic%20Learning/#backlinks","title":"Backlinks","text":"<ul> <li>Opportunistic Learning</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Optical%20Encoder/","title":"Optical Encoder","text":"<ul> <li>A detection sensor, which measures linear or rotary motion by detecting the movement of markings past a fixed beam of light. This can be used to count revolutions, identify parts, etc.</li> </ul>"},{"location":"Optical%20Proximity%20Sensors/","title":"Optical Proximity Sensors","text":"<ul> <li>Robot sensors which measure visible or invisible light reflected from an object to determine distance. Lasers are used for greater accuracy.</li> </ul>"},{"location":"Optimizers/","title":"Optimization","text":"<ul> <li>Gradient Descent gradients</li> <li>Adagrad</li> <li>Rmsprop</li> <li>Adam</li> <li>Learning Rate Decay tricks</li> <li>Early Stopping tricks</li> </ul>"},{"location":"Optimizers/#_1","title":"\u2026","text":""},{"location":"Optimizing%20Code/","title":"Optimizing Work","text":"<ul> <li>Vectorization</li> <li>Parallelization</li> <li>Loop Tiling</li> <li>Operator Fusion</li> <li>Block Sparse Kernel</li> </ul>"},{"location":"Optogenetics/","title":"Optogenetics","text":"<ul> <li>An innovative neuroscientific technique that uses light to turn genetically modified neurons on and off at will, in live animals.</li> </ul>"},{"location":"Organoid/","title":"Organoid","text":"<ul> <li>A research model that uses pluripotent stem cells (iPSCs) to grow structures made of organ-specific cell types.</li> </ul>"},{"location":"Orthogonal%20Initialization/","title":"Orthogonal Initialization","text":"<ul> <li>simple approach to solving the problem Vanishingexploding gradients</li> <li>when applying repeated matrix multiplications, the eigenvalues are what dictate the growth or death of the result</li> <li>eigenvalues of orthogonal matrices have an absolute value of 1</li> <li>no matter how many matrix multiplications the result doesen't explode nor vanishes</li> </ul>"},{"location":"Orthogonal%20Slicing/","title":"Orthogonal Slicing","text":"<ul> <li>Interactively resample the data on slices perpendicular to x-,y-,z-axis</li> <li>Use visualization techniques for Isoline, Height Plots</li> </ul>"},{"location":"OrthographicNet/","title":"OrthographicNet","text":""},{"location":"OrthographicNet/#orthographicnet","title":"OrthographicNet","text":"<ul> <li>Orthographic projection is used as a universal language among people in engineering professions:</li> <li>Projection lines are parallel to each other and are perpendicular to the plane, An accurate outline of the visible face of the object is obtained.</li> <li></li> <li></li> </ul>"},{"location":"OrthographicNet/#backlinks","title":"Backlinks","text":"<ul> <li>OrthographicNet</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Ostension/","title":"Ostension","text":"<ul> <li>The notion of ostension in Relevance Theory and in Natural Pedagogy Ostension is a notion of the Relevance Theory of Sperber and Wilson</li> <li>Ostension is the behaviour when the communicator makes manifest his/her intention to make something manifest, i.e., perceptible or inferable, to the listener</li> <li>Ostensive Information</li> <li>Humans try to obtain from every item of information as great a contextual effect as possible for as small a processing effort as possible.</li> <li>Children tend to give more credit to information derived from ostensive communication than to information obtained via direct experience</li> </ul>"},{"location":"Ostensive%20Information/","title":"Ostensive Information","text":"<ul> <li>Ostensive communication provides two kinds of information<ul> <li>information changing the listener's cognitive state </li> <li>information communicating that the first layer of information is presented intentionally</li> </ul> </li> </ul>"},{"location":"Ostensive%20Information/#backlinks","title":"Backlinks","text":"<ul> <li>Ostension</li> <li>Ostensive Information</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Otoscope%20or%20Auriscope/","title":"Otoscope or Auriscope","text":"<ul> <li>A device for examining the external ear cavity</li> </ul>"},{"location":"Out%20of%20Order%20Execution/","title":"Out of Order Execution","text":"<ul> <li>allow the processor to avoid a class of delays that occur when the data needed to perform an operation are unavailable</li> <li>Instruction fetch.</li> <li>Instruction dispatch to an instruction queue (also called instruction buffer)</li> <li>The instruction waits in the queue until its input operands are available.</li> <li>The instruction is issued to the appropriate functional unit and executed by that unit.</li> <li>The results are queued (Re-order Buffer).</li> <li>Only after all older instructions have their results written back to the register file, then this result is written back to the register.</li> </ul>"},{"location":"Out-group%20Homogeneity%20Bias/","title":"Out-group Homogeneity Bias","text":"<ul> <li>The tendency to see out-group members as more alike than in-group members when comparing attitudes, values, personality traits, and other characteristics. In-group refers to people you interact with regularly; out-group refers to people you do not interact with regularly. If you create a dataset by asking people to provide attributes about out-groups, those attributes may be less nuanced and more stereotyped than attributes that participants list for people in their in-group.</li> </ul>"},{"location":"Out-of-bag%20Evaluation%20%28OOB%20evaluation%29/","title":"Out-of-bag Evaluation (OOB evaluation)","text":"<ul> <li>A mechanism for evaluating the quality of a decision forest by testing each decision tree against the examples</li> <li>not used during training of that decision tree.</li> <li>Out-of-bag evaluation is a computationally efficient and conservative approximation of the cross-validation mechanism</li> </ul>"},{"location":"Overhypotheses/","title":"Overhypotheses","text":"<ul> <li>Parse trees for sentences,</li> <li>These can be explained related to grammar</li> <li>Grammars are structured according to general princples in Universal grammar</li> <li>Do children generalize learned object names as representing shape rather than other features? Yes!</li> <li>First order generalization</li> <li>Second order generalization</li> <li>Suggests that learning of overhypotheses can also be modeled with Bayesian learning</li> </ul>"},{"location":"Oxytocin/","title":"Oxytocin","text":"<ul> <li>Sometimes referred to as the \u201ccuddle chemical,\u201d this hormone can work as a neurotransmitter in the brain and has been linked to social attachment and parental care. While there are \u201clove\u201d sprays on the market that are said to contain oxytocin, there is no evidence that these concoctions have any effect on social relationships.</li> </ul>"},{"location":"PASCAL%20VOC/","title":"PASCAL VOC","text":""},{"location":"PASCAL-S/","title":"PASCAL-S","text":"<ul> <li>\"subset of the validation data in the PASCAL VOC 2010\"</li> <li>\"850 natural images\"</li> <li>The fixations were measured while eight observers looked at an image for 2 s.</li> </ul>"},{"location":"PCA/","title":"PCA","text":"<ul> <li>m dim affine hyperplace spanned by first m eigenvectors. Only manifolds and no codebook vectors</li> <li>Be able to reconstruct x from f(x) : decoding function \\(\\(x \\approx d \\circ f(x)\\)\\)</li> <li></li> </ul>"},{"location":"PCA/#steps","title":"Steps","text":"<ol> <li>Center data (A)<ul> <li>Subtract their mean from each pattern.</li> <li> \\[\\mu = \\frac{1}{N}\\Sigma_{i}x_{i}$$ and getting patterns $$\\hat x_{i}=x_{i}-\\mu\\] </li> <li>Point cloud with center of Gravity : origin<ul> <li>Extend more in some \"directions\" characterized by unit norm direction vectors \\(u \\in \\mathbb{R}^n\\) .</li> <li>Distance of a point from the origin in the direction of u : projection of \\(\\bar x_i\\) on u aka inner product \\(u'\\bar x_i\\)</li> <li>Extension of cloud in direction u : Mean square dist to origin.</li> <li>Largest extension : \\(\\(u_{1}= argmax_{u_{1}, ||u|| = 1} \\frac{1}{N}\\Sigma_{i}(u '\\bar x_i)^2\\)\\)</li> <li>Since centered: mean is 0 and \\(\\frac{1}{N}\\Sigma_{i}(u'\\bar x_i)^2\\) is the variance</li> <li>\\(u_1\\) is the longest direction : First PC : PC1</li> </ul> </li> </ul> </li> <li>Project points (B)<ul> <li>Find orthogonal (90deg) subspace . (n-1) dim linear</li> <li>Map all points \\(\\bar x\\) to \\(\\(\\bar x ^{\\ast}=\\bar x- (u' \\bar x_i^\\ast)^2\\)\\)- Second PC : PC2</li> </ul> </li> <li>Rinse and repeat (C)</li> <li>New PCs plotted in original cloud (D)</li> <li>For featurres \\(f_{k}: \\mathbb{R}^{n}\\rightarrow \\mathbb{R}\\) , \\(x \\rightarrow u'_{k}\\bar x\\)</li> <li>Reconstruction : (\\(x= \\mu + \\Sigma_{k= 1, \u2026,n}f_{k}(x)u_{k}\\)\\)<ul> <li>First few PCs till index m<ul> <li>\\((f_{1}(x), \u2026, f_{m}(x))'\\)</li> <li>Decoding function \\(\\(d: (f_{1}(x), \u2026, f_{m}(x))' \\rightarrow \\mu + \\Sigma_{k= 1}f_{k}(x)u_{k}\\)\\)</li> </ul> </li> </ul> </li> <li>How good is the reconstruction<ul> <li> \\[\\Sigma_{k=m+1}^{n} (\\frac{1}{N}\\Sigma_if_{k}(x_i)^2)\\] </li> <li>Relative amount of dissimilarity to mean empirical variance of patterns - 1<ul> <li> \\[\\frac{\\Sigma_{k=m+1}^{n} (\\frac{1}{N}\\Sigma_if_{k}(x_i)^2)}{\\Sigma_{k=1}^{n} (\\frac{1}{N}\\Sigma_if_{k}(x_i)^2)}\\] </li> <li>Ratio very small as index k grows. Very little info lost by reducing dims. Aka good for very high dim stuff.</li> </ul> </li> </ul> </li> <li>Compute SVD<ul> <li>\\(u_{1,}\u2026u_{n}\\) form orthonormal, real eigenvectors</li> <li>variances \\(\\sigma_{1}^{2,}\u2026, \\sigma_{n}^2\\) are eigenvalues</li> <li>\\(C = U\\Sigma U'\\) to get PC vectors \\(u_k\\) lined up in U and variances \\(\\sigma_k^2\\) as eigenvalues in \\(\\Sigma\\)</li> <li>If we want to preserve 98% variance : Rhs of (1) st. ratio is (1-0.98)</li> </ul> </li> </ol>"},{"location":"PDF/","title":"Probability Density Function","text":"<ul> <li>If X is a random variable(RV) that takes values in \\(S \\subseteq \\mathbb{R}^{n}\\). PDF is a fn \\(f:S \\rightarrow \\mathbb{R}^{\\geq0}\\) that satisfies:<ul> <li>For every subvolume \\(A \\subseteq S\\) of S, the prob \\(P(X \\in A)\\) that X gives a value in A is \\(\\(P(X\\in A) = \\int_Af(x)dx\\)\\)</li> </ul> </li> <li>Function which maps from a random process to a quantified version<ul> <li>eg. 1 when heads and 0 when tails</li> </ul> </li> </ul>"},{"location":"PIUQ/","title":"PIUQ","text":"<p>title: PIUQ</p> <p>tags: brain cogneuro psychology</p>"},{"location":"PIUQ/#piuq","title":"PIUQ","text":"<ul> <li>Problematic Internet Use Questionnaire (PIUQ)</li> <li>a validated self-report scale with good reliability and validity characteristics</li> <li>The questionnaire contains 18 items, each scored on a 5-point Likert-type scale ranging from 1 (never) to 5 (always)</li> </ul>"},{"location":"PMF/","title":"Probability Mass Function","text":"<ul> <li>pmf</li> <li>Given a discrete sample space S<ul> <li>S is a function \\(p : S \\rightarrow [0,1]\\) whos total mass is 1</li> <li>satisfies \\(\\Sigma_{s \\in S}p(s) = 1\\)</li> </ul> </li> </ul>"},{"location":"PRelu/","title":"Parametric Relu","text":"<ul> <li> \\[max(\\alpha x,x)\\] </li> <li></li> </ul>"},{"location":"PaLM/","title":"PaLM","text":"<ul> <li>PaLM: Scaling Language Modeling with Pathways</li> <li>single 540 billion parameter dense Transformer language model</li> <li>few-shot language understanding and generation</li> <li>drastically reduces the number of task-specific training examples needed to adapt the model to a particular application</li> <li>Pathways Language Model</li> <li>6144 TPU v4 chips</li> <li>breakthrough performance on reasoning tasks, which require multi-step logical inference</li> <li>combination of scale and chain-of-thought prompting, where the model is explicitly prompted to generate a natural language logical inference chain before making its predictio</li> <li>write explicit logical inference chains to both explain jokes and answer complex questions about scenarios</li> <li>Big-Bench</li> <li>suggest that the improvements from scale for few-shot language understanding have not yet plateaued</li> <li>When they compare results from PaLM 540B to our own identically trained 62B and 8B model variants, improvements are typically log-linear.</li> <li>certain capabilities of language models only emerge when trained at sufficient scale, and there are additional capabilities that could emerge from future generations of models</li> <li>demonstrating that prompting the model to generate explicit inference chains can drastically increase the quality of the predictions themselves</li> <li>model\u2019s generation (rather than just understanding) capabilities can be immensely beneficial even for tasks that are modeled as categorical prediction or regression, which typically do not require significant language generation</li> <li>comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale</li> <li>ethical considerations related to large language models and discuss potential mitigation strategies</li> </ul>"},{"location":"Padded%20Conv/","title":"Padded Conv","text":"<ul> <li> \\[(N_i, N_o, C, F)\\] </li> <li>Filters transform from C -&gt; F channels</li> <li>Mirror, Reflect</li> </ul>"},{"location":"Palletizing/","title":"Palletizing","text":"<ul> <li>The process of stacking packages (i.e., boxes, bags, containers, etc.) in an organized fashion on a pallet.</li> </ul>"},{"location":"Parallel%20Coordinate%20Plots/","title":"Parallel Coordinate Plots","text":"<ul> <li>Enhancement<ul> <li>Permute axes (horizontally) to and swap their direction (vertically) minimize crossings</li> <li>Add histograms on axes to show lines per unit data value</li> <li>Visually group/cluster polylines histograms</li> </ul> </li> </ul>"},{"location":"Parallel%20Granularity/","title":"Parallel Granularity","text":"<ul> <li>Ration of computation to communication</li> <li>Coarse : Large computation between communication</li> <li>Fine : Small computation between communication</li> </ul>"},{"location":"Parallel%20Processing/","title":"Parallel Processing","text":"<ul> <li>Load balancing</li> <li>Minimizing Communication</li> <li>Overlap Communication</li> </ul>"},{"location":"Parallel%20Runner/","title":"Parallel Runner","text":"<pre><code>import random\nimport numpy as np\nimport concurrent\nfrom typing import *\nfrom concurrent.futures import ProcessPoolExecutor\nfrom types import SimpleNamespace\nimport os\nfrom pathlib import Path\n\ndef ifnone(a, b):\n    \"\"\"\n    Return if None\n    \"\"\"\n    return b if a is None else a\n\n\ndef listify(o):\n    \"\"\"\n    Convert to list\n    \"\"\"\n    if o is None:\n        return []\n    if isinstance(o, list):\n        return o\n    if isinstance(o, str):\n        return [o]\n    if isinstance(o, Iterable):\n        return list(o)\n    return [o]\n\n\ndef num_cpus() -&gt; int:\n    \"Get number of cpus\"\n    try:\n        return len(os.sched_getaffinity(0))\n    except AttributeError:\n        return os.cpu_count()\n\n\n_default_cpus = min(16, num_cpus())\ndefaults = SimpleNamespace(\n    cpus=_default_cpus, cmap=\"viridis\", return_fig=False, silent=False\n)\n\n\ndef parallel(func, arr: Collection, max_workers: int = None, leave=False):  # %t\n    \"Call `func` on every element of `arr` in parallel using `max_workers`.\"\n    max_workers = ifnone(max_workers, defaults.cpus)\n    if max_workers &lt; 2:\n        results = [\n            func(o, i)\n            for i, o in tqdm.tqdm(enumerate(arr), total=len(arr), leave=leave)\n        ]\n    else:\n        with ProcessPoolExecutor(max_workers=max_workers) as ex:\n            futures = [ex.submit(func, o, i) for i, o in enumerate(arr)]\n            results = []\n            for f in tqdm.tqdm(\n                concurrent.futures.as_completed(futures), total=len(arr), leave=leave\n            ):\n                results.append(f.result())\n    if any([o is not None for o in results]):\n        return results\n\n</code></pre>"},{"location":"Parallel%20Shift%20Function/","title":"Parallel Shift Function","text":"<ul> <li>Parallel Shift refers to the shifting of an object from a fixed position in such a way that all points within the object move an equal distance.</li> </ul>"},{"location":"Parallelization/","title":"Parallel","text":"<ul> <li>Independant work chunks -&gt; operate simultaneously</li> </ul>"},{"location":"Parasympathetic/","title":"Parasympathetic","text":"<ul> <li>Relaxes the body</li> </ul>"},{"location":"Parent%20Approximations/","title":"Parent Approximations","text":"<ul> <li>It learns a compact two-level decision set in which each rule explains parts of the model behavior unambiguously and is a combined objective function to optimize these aspects: high agreement between the explanation and the model; little overlap between the decision rules in the explanation; the explanation decision set is lightweight and small.</li> </ul>"},{"location":"Parietal%20lobe/","title":"Parietal Lobe","text":"<ul> <li>Interprets language, words</li> <li>Sense of touch, pain, temperature (sensory strip)</li> <li>Interprets signals from vision, hearing, motor, sensory and memory</li> <li>Spatial and visual perception</li> </ul>"},{"location":"Parietal%20lobe/#parietal-lobe_1","title":"Parietal Lobe","text":"<ul> <li>The area of the brain\u2019s cerebrum located just behind the central sulcus. It is concerned primarily with the reception and processing of sensory information from the body and is also involved in map interpretation and spatial orientation (recognizing one\u2019s position in space in relation to other objects or places).</li> </ul>"},{"location":"Parietal%20lobe/#backlinks","title":"Backlinks","text":"<ul> <li>ACT-R</li> <li>Parietal lobe</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Parkinson%E2%80%99s%20Disease/","title":"Parkinson\u2019s Disease","text":"<ul> <li>A neurodegenerative disorder characterized by tremor, slowed movement, and speech changes due to the death of dopamine</li> <li>neurons located in the substantia nigra.</li> </ul>"},{"location":"Partial%20Dependence%20Plot/","title":"Partial Dependence Plot","text":"<ul> <li>Another approach [29] shows the marginal effect of one or two features on the prediction of learning techniques using Partial Dependence Plots</li> <li>The method gives a statement about the global relationship of a feature and whether its relation to the outcome is linear, monotonic, or more complex.</li> <li>The PDP is the average of Individual Conditional Expectation (ICE) over all features</li> <li>ICE [30] points to how the prediction changes if a feature changes.</li> <li>The PDP is limited to two features.</li> </ul>"},{"location":"Participation%20Bias/","title":"Participation Bias","text":"<ul> <li>Synonym for non-response bias</li> </ul>"},{"location":"Particle%20Filter/","title":"Particle Filter","text":""},{"location":"Particle%20Filter/#particle-filter","title":"Particle Filter","text":"<ul> <li>Particle filter algorithm works for any arbitrary distribution and not just Gaussian. Particle filter is computationally more expensive than Kalman filter</li> <li>non linear systems.</li> </ul>"},{"location":"Particle%20Filter/#backlinks","title":"Backlinks","text":"<ul> <li>Particle Filter</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Particle%20Visualization/","title":"Particle Visualization","text":"<ul> <li>points with only a (3D) coordinate</li> <li>potentially enriched with attributes like radius, velocity, etc.</li> <li>scattered data Data Structures</li> <li>lack of topological information (neighborhood)</li> <li>typically many particles (e.g. atoms, galaxies)</li> <li>Atomistic visualization</li> <li>via glyphs (i.e. spheres)</li> <li>explicit (geometry) or implicit Raycasting</li> <li>Surface-based visualization</li> <li>extract surface geometry</li> <li>particles: point samples describing a surface (e.g. in fluids or molecules)</li> <li>Volume-based visualization</li> <li>particles: point samples of a volume</li> <li>Raycasting and hierarchical Data Structures</li> </ul>"},{"location":"Parts%20of%20action/","title":"Parts of Action","text":"<ul> <li>Lynn went on a business trip to New York.</li> <li>She left on an early morning flight.</li> <li>Taking a flight should be recognized as part of going on a trip.</li> </ul>"},{"location":"Parts%20of%20entities/","title":"Parts of Entities","text":"<ul> <li>Tracy opened the book she just bought.</li> <li>The title page was torn.</li> <li>The phrase \u2018the title page\u2019 should be recognized as being part of the book tat was just bought.</li> </ul>"},{"location":"PatchGAN/","title":"PatchGAN","text":"<ul> <li>Type of discriminator</li> <li>only penalizes structure at the scale of local image patches</li> <li>tries to classify if each \\(N \\times N\\) patch in an image is real or fake</li> <li>discriminator is run convolutionally across the image, averaging all responses to provide the ultimate output of \\(D\\)</li> <li>effectively models the image as a Markov random field</li> <li>assuming independence between pixels separated by more than a patch diameter</li> <li>type of texture/style loss</li> <li>rather the regular GAN maps from a 256\u00d7256 image to a single scalar output, which signifies \u201creal\u201d or \u201cfake\u201d, whereas the PatchGAN maps from 256\u00d7256 to an NxN (here 70\u00d770) array of outputs X, where each \\(X_{ij}\\) signifies whether the patch ij in the image is real or fake.</li> <li></li> </ul>"},{"location":"PatchGAN/#backlinks","title":"Backlinks","text":"<ul> <li>CycleGAN</li> <li>PatchGAN</li> <li>The discriminator models use PatchGAN, as described by\u00a0Phillip Isola, et al. in their 2016 paper titled \u201cImage-to-Image Translation with Conditional Adversarial Networks.\u201d</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Pathlines/","title":"Pathlines","text":""},{"location":"Pearson%20Correlation/","title":"Pearson Correlation","text":"<p>title: Pearson Correlation</p> <p>tags: statistics</p>"},{"location":"Pearson%20Correlation/#pearson-correlation","title":"Pearson Correlation","text":"<ul> <li>One type of Correlation</li> <li> \\[r = \\frac{n(\\Sigma xy) - (\\Sigma x)(\\Sigma y)}{\\sqrt{[n\\Sigma x^{2} - (\\Sigma x)^{2}][n\\Sigma y^{2} - (\\Sigma y)^{2}]}}\\] </li> </ul>"},{"location":"Pendant%20Teaching/","title":"Pendant Teaching","text":"<ul> <li>The mapping and recording of the position and orientation of a robot and/or manipulator system as the robot is manually moved in increments from an initial state along a path to a final goal state. The position and orientation of each critical point (joints, robot base, etc.) is recorded and stored in a database for each taught position the robot passes through on its path toward its final goal. The robot may now repeat the path on its own by following the path stored in the database.</li> </ul>"},{"location":"People%20Art%20Dataset/","title":"People Art Dataset","text":""},{"location":"Perception%20Component/","title":"Perception Component","text":""},{"location":"Perception%20Component/#perception-component","title":"Perception Component","text":"<ul> <li>processes all momentary information coming from sensors.</li> </ul>"},{"location":"Perception%20Component/#backlinks","title":"Backlinks","text":"<ul> <li>Perception Component</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Perception/","title":"Perception","text":"<ul> <li>Perception \u2014 process by which we interpret the things around us through sensory stimuli</li> <li>Cognition \u2014 mental processes assisting us to remember, think, know, judge, solve problems, etc.</li> <li>Preattentive Processing</li> <li>Gestalt Laws</li> <li>Postattentive Amnesia</li> <li>Change Blindness</li> <li>Inattentional Blindness</li> </ul>"},{"location":"Perceptron/","title":"Perceptron","text":"<ul> <li> \\[f(x)=sign(\\Sigma _i w_ix_i +b) = sign(\\mathbf{w^Tx}+b)\\] <ul> <li>\\(\\(sign(x) = \\begin{cases} 1 &amp; x\\geq0 \\\\ 0 &amp; otherwise\\end{cases}\\)\\) </li> </ul> </li> <li>computational graph</li> <li>Multi layer<ul> <li>Stack multiple perceptrons</li> <li> \\[\\begin{align} \\\\&amp; h_0 = x h1= sign(\\mathbf{w_1^T}+b_1) \\\\ &amp;\u2026\\\\&amp; h1= sign(\\mathbf{w_{L-1}^T}+b_L) \\end{align}\\] </li> </ul> </li> </ul>"},{"location":"Perceptual%20Messages/","title":"Perceptual Messages","text":""},{"location":"Perceptual%20Messages/#perceptual-messages","title":"Perceptual Messages","text":"<ul> <li>large (e.g., point cloud)</li> <li>data flows continuously at the sensor output frequency (30Hz).</li> </ul>"},{"location":"Perceptual%20Messages/#backlinks","title":"Backlinks","text":"<ul> <li>Perceptual Messages</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Perceptually%20Uniform/","title":"Perceptually Uniform","text":"<ul> <li>Euclidean distance corresponds to perceptual difference</li> <li>e.g., CIELUV,CIELAB, (L,a,b*).</li> </ul>"},{"location":"Peripheral%20Nervous%20System/","title":"Peripheral Nervous System","text":"<ul> <li>All the nerves that branch off from the brain</li> <li>Both directions</li> <li>Allow Central Nervous System to communicate with the body</li> <li>Afferent + Efferent</li> </ul>"},{"location":"Perplexity/","title":"Perplexity","text":"<ul> <li>Perplexity is defined as the exponentiated average negative log-likelihood of a sequence.</li> <li>If we have a tokenized sequence \\(X = (x_0, x_1, \\dots, x_t)\\), then the perplexity of \\(X\\) is, \\(\\(\\text{PPL}(X) = \\exp \\left\\{ {-\\frac{1}{t}\\sum_i^t \\log p_\\theta (x_i|x_{&lt;i}) } \\right\\}\\)\\)where \\(\\log p_\\theta (x_i|x_{&lt;i})\\) is the log-likelihood of the ith token conditioned on the preceding tokens \\(x_{&lt;i}\\) according to our model.</li> <li>Intuitively, it can be thought of as an evaluation of the model's ability to predict uniformly among the set of specified tokens in a corpus.</li> <li>Importantly, this means that the tokenization procedure has a direct impact on a model's perplexity which should always be taken into consideration when comparing different models.</li> <li>This is also equivalent to the exponentiation of the Cross Entropy between the data and model predictions</li> </ul>"},{"location":"Pharmacotherapy/","title":"Pharmacotherapy","text":"<ul> <li>The use of pharmaceutical drugs for therapeutic purposes.</li> </ul>"},{"location":"Phases%20of%20Simulated%20User%20Experiments/","title":"Phases of Simulated User Experiments","text":""},{"location":"Phases%20of%20Simulated%20User%20Experiments/#phases-of-simulated-user-experiments","title":"Phases of Simulated User Experiments","text":"<ul> <li>Evolution<ul> <li>The classification performance should be improved as the number of examples per category increases while NO new categories are introduced.</li> </ul> </li> <li>Recovery<ul> <li>By increasing the number of categories, it is expected that the prediction accuracy decreases. The time spent in system evolution until correcting and adjusting all current categories defines recovery.</li> </ul> </li> <li>Breakpoint<ul> <li>Eventually the learning agent reaches to a breakpoint where the agent is no longer able to learn more categories.</li> </ul> </li> </ul>"},{"location":"Phases%20of%20Simulated%20User%20Experiments/#backlinks","title":"Backlinks","text":"<ul> <li>Phases of Simulated User Experiments</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Phenotype/","title":"Phenotype","text":"<ul> <li>A set of traits or characteristics resulting from the interaction of one\u2019s genes with the environment.</li> </ul>"},{"location":"Phonetics/","title":"Phonetics","text":"<ul> <li>deals with the physical building blocks of a language sound system.</li> <li>eg. sounds of \u2018k\u2019, \u2018t\u2019 and \u2018e\u2019 in \u2018kite</li> </ul>"},{"location":"Phong%20Lighting/","title":"Phong Lighting","text":""},{"location":"Phonology/","title":"Phonology","text":"<ul> <li>organisation of speech sounds within a language.</li> <li>eg. (1) different \u2018k\u2019 sounds in \u2018kite\u2019 vs \u2018coat\u2019</li> <li>(2) different \u2018t\u2019 and \u2018p\u2019 sounds in \u2018top\u2019 vs \u2018pot\u2019</li> </ul>"},{"location":"Phrase%20Representation%20Learning/","title":"Phrase Representation Learning","text":"<ul> <li>Learning Phrase Representations Using RNN Encoder\u2013Decoder for Statistical Machine Translation</li> <li>two recurrent neural networks Basic RNN Architectures that is together able to learn the mapping from a sequence of an arbitrary length to another sequence, possibly from a different set, of an arbitrary length.</li> <li>either score a pair of sequences (in terms of a conditional probability) or generate a target sequence given a source sequence</li> <li>jointly trained to maximize the conditional probability of a target sequence given a source sequence</li> <li>reset gate and an update gate that adaptively control how much each hidden unit remembers or forgets while reading/generating a sequenc</li> <li>RNN Encoder\u2013Decoder to score each phrase pair in the phrase table</li> <li>capture linguistic regularities in the phrase pairs well</li> <li>BLEU</li> </ul>"},{"location":"Picasso%20Dataset/","title":"Picasso Dataset","text":""},{"location":"Pick%20and%20Place%20Cycle/","title":"Pick and Place Cycle","text":"<ul> <li>The amount of time it takes for a manipulator to pick up an object and place it in a desired location, then return to its rest position. This includes time during the acceleration and deceleration phases of a particular task. The robot movement is controlled from one point location in space to another in a Point-to-Point (PTP) motion system. Each point is programmed into the robot's control memory and then played back during the work cycle.</li> </ul>"},{"location":"Picky%20Puppet%20Method/","title":"Picky Puppet Method","text":"<ul> <li>For children</li> <li>Ask if a \"puppet\" would like it</li> </ul>"},{"location":"Pinch%20Points/","title":"Pinch Points","text":"<ul> <li>A pinch point is any point at which it is possible for a person or part of a person\u2019s body to be caught between moving parts of a machine, or between the moving and stationary parts of a machine, or between material and any part of the machine. A pinch point does not have to cause injury to a limb or body part, although it might cause injury \u2013 it only has to trap or pinch the person to prevent them from escaping or removing the trapped part from the pinch point.</li> </ul>"},{"location":"Pineal%20gland/","title":"Pineal Gland","text":"<ul> <li>It is located behind the third ventricle.</li> <li>It helps regulate the body\u2019s internal clock and circadian rhythms by secreting melatonin.</li> <li>It has some role in sexual development.</li> </ul>"},{"location":"Pipes/","title":"Pipes","text":"<ul> <li>Allows vector operation to be performed in parallel on multiple elements of the vector</li> </ul>"},{"location":"Pituitary%20gland/","title":"Pituitary Gland","text":"<ul> <li>lies in a small pocket of bone at the skull base called the sella turcica.</li> <li>The pituitary gland is connected to the hypothalamus of the brain by the pituitary stalk.</li> <li>Known as the \u201cmaster gland,\u201d it controls other endocrine glands in the body.</li> <li>It secretes hormones that control sexual development, promote bone and muscle growth, and respond to stress.</li> </ul>"},{"location":"Pituitary%20gland/#pituitary-gland_1","title":"Pituitary Gland","text":"<ul> <li>An endocrine organ at the base of the brain that is closely linked with the hypothalamus. The pituitary gland is composed of two lobes, the anterior and posterior lobes, and secretes hormones that regulate the activity of the other endocrine organs in the body.</li> </ul>"},{"location":"Pix2Seq/","title":"Pix2Seq","text":"<ul> <li>Pix2seq: a Language Modeling Framework for Object Detection</li> <li>generic framework for object detection</li> <li>object detection as a language modeling task conditioned on the observed pixel inputs</li> <li>Object descriptions (e.g., bounding boxes and class labels) are expressed as sequences of discrete tokens, and we train a neural network to perceive the image and generate the desired sequence</li> <li>COCO</li> <li>output can be represented by a relatively concise sequence of discrete tokens (e.g., keypoint detection, image captioning, visual question answering)</li> <li>autoregressive</li> <li>stop inference when the ending token is produced</li> <li>applying it to offline inference, or online scenarios where the objects of interest are relatively sparse</li> <li>entirely based on human annotation</li> </ul>"},{"location":"PlantCLEF/","title":"PlantCLEF","text":"<ul> <li>PlantCLEF dataset is a collection of images of plants, with a total of around 57,000 images and over 200 different plant species.</li> </ul>"},{"location":"PlantCLEF/#backlinks","title":"Backlinks","text":"<ul> <li>Fine Grained Datasets</li> <li>PlantCLEF</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Pluripotency/","title":"Pluripotency","text":"<ul> <li>The quality of certain undifferentiated cells that allows them to develop into one of many different cell types.</li> </ul>"},{"location":"Point%20Cloud/","title":"Point Cloud Data","text":"<ul> <li>PointNet++</li> </ul>"},{"location":"Point%20Distribution/","title":"Point Distribution","text":"<ul> <li>PDF is impossible to use</li> <li>Probability mass is concentrated in a few points</li> <li>Dirac Delta</li> <li>Hyperdistributions</li> </ul>"},{"location":"Point-to-Point/","title":"Point-to-Point","text":"<ul> <li>Manipulator motion in which a limited number of points along a projected path of motion is specified. The manipulator moves from point to point rather than a continuous smooth path.</li> </ul>"},{"location":"PointNet%2B%2B/","title":"PointNet++","text":""},{"location":"PointNet%2B%2B/#todo","title":"todo","text":""},{"location":"Poisson%20Distribution/","title":"Possion Distribution","text":"<ul> <li>Probability that an event occurs k times within a given time interval</li> <li>Eg:</li> <li>k meteors within 100 years</li> <li>k calls in an hour</li> <li>Also can count spatially circumscribed events</li> <li>no of dust particles in a mm of air</li> <li>no of diamons in ton of ore</li> <li>Expected no of events \\(E[X]\\) : rate \\(\\lambda\\)</li> <li>PMF : \\(\\(p(k) = \\frac{\\lambda^{k}e^{-k}}{k!}\\)\\)</li> <li></li> <li>Eg:</li> <li>N 1-hour protocols for calls : \\(n_{i} (i = 1, \u2026, N)\\)</li> <li> \\[\\hat\\lambda =\\frac{1}{N}\\Sigma_{i}n_{i}\\] </li> </ul>"},{"location":"Poisson%20Loss/","title":"Poisson Loss","text":"<ul> <li>When data is from Poisson Distribution</li> </ul> \\[\\frac{1}{\\mathrm{length}\\left( y \\right)} \\cdot \\mathrm{sum}\\left( \u0177 - \\log\\left( \u0177 \\right) \\right)\\]"},{"location":"Poisson%20Process/","title":"Poisson Process","text":"<ul> <li>Waiting times between two consecutive spikes are Exponential Distribution</li> </ul>"},{"location":"Polynomial%20Trajectories/","title":"Polynomial Trajectories","text":""},{"location":"Polynomial%20Trajectories/#polynomial-trajectories","title":"Polynomial Trajectories","text":""},{"location":"Polynomial%20Trajectories/#backlinks","title":"Backlinks","text":"<ul> <li>Polynomial Trajectories</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Polysynthetic%20words/","title":"Polysynthetic Words","text":"<ul> <li>complex words that function as a sentence (Chukchi and Inuktitut)</li> </ul>"},{"location":"Pooling/","title":"Pooling","text":"<ul> <li>Summarize low level features</li> <li>Reduce input dims</li> <li>Max/Avg</li> <li>Too much pooling reduces performance<ul> <li>Multiple convs first</li> </ul> </li> <li>Max pool + dilated/Strided convs control effective receptive field size</li> </ul>"},{"location":"Population%20Correlation/","title":"Population Correlation","text":"<p>title: Population Correlation</p> <p>tags: statistics</p>"},{"location":"Population%20Correlation/#population-correlation","title":"Population Correlation","text":"<ul> <li> \\[\\rho_{xy}= \\frac{\\sigma_{xy}}{\\sigma_{x}\\sigma_{y}}\\] </li> <li>\\(\\sigma\\) is the Standard Deviation</li> <li>\\(\\sigma_{xy}\\) is the Covariance</li> </ul>"},{"location":"Position%20Encoding/","title":"Position Encoding","text":"<ul> <li>Transformers are feed forward. So need a way to inject position into seq</li> <li> \\[PE(pos, 2i) = sin(\\frac{pos}{10000^\\frac{2i}{d_{model}}})\\] </li> <li> \\[PE(pos, 2i+1) = cos(\\frac{pos}{10000^\\frac{2i}{d_{model}}})\\] </li> <li>Conceptually, adding word order to a sentence<ul> <li>Something like (\"Hello\", 1) , (\"from\",2) , (\"me\", 3)</li> </ul> </li> </ul>"},{"location":"Position%20Wise%20Feed%20Forward/","title":"Position Wise Feed Forward","text":"<ul> <li> \\[FFN(x) = max(0, xW_{1}+b_{1})W_{2}+b_{2}\\] </li> <li>Dense Layers are applied along the last (512) dims</li> </ul>"},{"location":"Positron%20Emission%20Tomography%20%28PET%29/","title":"Positron Emission Tomography (PET)","text":"<ul> <li>An imaging technique, often used in brain imaging. For a PET scan of the brain, a radioactive \u201cmarker\u201d that emits, or releases, positrons (parts of an atom that release gamma radiation) is injected into the bloodstream. Detectors outside of the head can sense these \u201cpositron emissions,\u201d which are then reconstructed using sophisticated computer programs to create computer images. Since blood flow and metabolism increase in brain regions at work, those areas have higher concentrations of the marker, and researchers can see which brain regions activate during certain tasks or exposure to sensory stimuli. Ligands can be added to a PET scan to detect pathological entities such as amyloid or tau deposits.</li> </ul>"},{"location":"Post%20Classification/","title":"Post Classification","text":"<ul> <li>Interpolation of scalars at several vertices</li> <li>Classification via Transfer Function</li> <li></li> </ul>"},{"location":"Post-processing%20Your%20Model%27s%20Output/","title":"Post-processing Your Model's Output.","text":"<ul> <li>Altering the loss function to incorporate a penalty for violating a fairness metric.</li> <li>Directly adding a mathematical constraint to an optimization problem.</li> <li>A synthetic feature formed by crossing (taking a Cartesian product of) individual binary features obtained from categorical data or from continuous features via bucketing. Feature crosses help represent nonlinear relationships.</li> </ul>"},{"location":"Postattentive%20Amnesia/","title":"Postattentive Amnesia","text":"<ul> <li>No additional information is saved in the visual system between different scenes</li> </ul>"},{"location":"Posterior%20Mean%20estimate/","title":"Posterior Mean Estimate","text":"<ul> <li>If need a single, definite model estimate -&gt; Get mean value of posterior \\(\\(\\hat \\theta = \\theta^{PME} = \\int_{\\mathbb{R}^{K}}\\theta h(\\theta|D)d\\theta\\)\\)</li> </ul>"},{"location":"Postsynaptic%20Cell/","title":"Postsynaptic Cell","text":"<ul> <li>The neuron on the receiving end of a nerve impulse transmitted from another neuron.</li> </ul>"},{"location":"Power%20and%20Force%20Limiting%20%28PFL%29/","title":"Power and Force Limiting (PFL)","text":"<ul> <li>Collaborative feature that allows both the operator and robot to work in proximity to one another by ensuring the robot will slow down and stop before a contact situation occurs. In order for this feature to be safely implemented, functional safety and additional detection hardware must be used. A risk assessment shall be used determine if any additional safeguarding is necessary to mitigate risks within the robot system.</li> </ul>"},{"location":"Power/","title":"Power","text":"<ul> <li>current x potential difference</li> <li> \\[P = IV\\] </li> </ul>"},{"location":"Pragmatics/","title":"Pragmatics","text":"<ul> <li>Important relationships that may hold between phrases and parts of their discourse context</li> <li>Parts of entities</li> <li>Parts of action</li> <li>Entities involving in actions</li> <li>Elements of sets</li> <li>Names of individuals</li> </ul>"},{"location":"Pre%20Classification/","title":"Pre Classification","text":"<ul> <li>Classification of scalars at each sample via Transfer Function</li> <li>Interpolation of RGBA values</li> <li></li> </ul>"},{"location":"Pre%20Integrated%20Volume%20Rendering/","title":"Pre Integrated Volume Rendering","text":"<ul> <li>Assume<ul> <li>Linear interpolations of scalar values in a ray segment</li> <li>Constant length of ray segment L</li> </ul> </li> <li>Pre computation from a slab</li> <li> \\[s_{L}(t) = s_{b}+ \\frac{t}{L}(s_{f}-s_{b})\\] </li> <li>becomes</li> <li> \\[c_{i}= \\int_{0}^{L}g(t)e^{-\\int_{t}^{L}{\\kappa(t')d_{t}}}dt' \\] </li> <li> \\[o_{i}= e^{\\int_{0}^{L}\\kappa(t)}d_{t} \\] </li> <li></li> </ul>"},{"location":"Preattentive%20Processing/","title":"Preattentive Processing","text":"<ul> <li>some visual properties are detected very rapidly and in parallel by low level visual processes</li> <li></li> </ul>"},{"location":"Precision%20Recall%20Curve/","title":"Precision Recall Curve","text":"<ul> <li>Precision + Recall</li> <li>appropriate when dataset imbalanced</li> <li>no-skill line changes based on the distribution of the positive to negative classes<ul> <li>horizontal line with the value of the ratio of positive cases in the dataset</li> <li>balanced this is 0.5</li> </ul> </li> <li>skilful model is represented by a curve that bows towards (1,1) above the flat line of no skill</li> </ul>"},{"location":"Precision/","title":"Precision","text":"<ul> <li> \\[\\frac{TP}{TP+FP}\\] </li> <li>How many samples are actually positive out of the total number of predicted positive samples? -&gt; How precise is the model in predicting positive samples?</li> </ul>"},{"location":"Predicate/","title":"Predicate","text":"<ul> <li>the part of a sentence or clause containing a verb and stating something about the subject (e.g.\u00a0went home\u00a0in\u00a0John went home\u00a0)</li> </ul>"},{"location":"Predicting%20Student%20learning%20Curve/","title":"Predicting Student Learning Curve","text":"<ul> <li>(Croteau, Heffernan, &amp; Koedinger, 2004).</li> <li>The learning curve of a Knowledge Component graphs the durations of its learning events (or their probability of error).</li> <li>a learning curve should be a smoothly descending power-law or exponential curve</li> <li>For instance, the first Learning Event for a Knowledge Component may take 50 seconds, because the student is constructing the Knowledge Component by referring to the textbook and asking the tutoring system for help. The next Learning Event might take only 25 seconds because the student is reconstructing the Knowledge Component. On the third Learning Event, the student recalls the Knowledge Component after a brief struggle, so the event takes 12 seconds. The fourth event takes 6 seconds, the fifth takes 3 seconds, and so on. However, if the representation of knowledge is inaccurate, a Knowledge Component's learning curve may have a huge jump in the middle or be quite jagged</li> </ul>"},{"location":"Prediction%20Difference%20Analysis/","title":"Prediction Difference Analysis","text":"<ul> <li>Their goal was to improve and interpret DNNs. Their technique was based on the univariate approach of [94] and the idea that the relevance of an input feature with respect to a class can be estimated by measuring how the prediction changes if the feature is removed. Zintgraf et al. removed several features at one time using their knowledge about the images by strategically choosing patches of connected pixels as the feature sets. Instead of going through all individual pixels, they considered all patches of a special size implemented in a sliding window fashion. They visualized the effects of different window sizes and marginal versus conditional sampling and displayed feature maps of different hidden layers and top-scoring classes.</li> </ul>"},{"location":"Prediction%20assumption/","title":"Prediction Assumption","text":"<ul> <li>every model that aims to predict an output Y from an input X makes the assumption that it's possible to predict Y based on X.</li> </ul>"},{"location":"Predictive%20Parity/","title":"Predictive Parity","text":"<ul> <li>A fairness metric that checks whether, for a given classifier, the precision rates are equivalent for subgroups under consideration.</li> </ul>"},{"location":"Predictive%20Uncertainty/","title":"Predictive Uncertainty","text":""},{"location":"Preferences%20and%20ethical%20principles%20in%20decision%20making/","title":"Preferences and Ethical Principles in Decision Making","text":"<ul> <li>Andrea Loreggia, Nicholas Mattei, Francesca Rossi, and Kristen Brent Venable.</li> <li>leverage the CP-net formalism to represent the exogenous ethics priorities and endogenous subjective preferences</li> <li>distance between CPnets so as to enable AI agents to make decisions using their subjective preferences if they are close enough to the ethical principles</li> </ul>"},{"location":"Preferences%20and%20ethical%20principles%20in%20decision%20making/#backlinks","title":"Backlinks","text":"<ul> <li>Building Ethics into Artificial Intelligence</li> <li>Preferences and ethical principles in decision making</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Prefix/","title":"Prefix","text":"<ul> <li>precede the stem: do / undo</li> </ul>"},{"location":"Prepositions/","title":"Prepositions","text":"<ul> <li>relates phrases (at, on, of, about)</li> </ul>"},{"location":"Presence-sensing%20Safeguarding%20Device/","title":"Presence-sensing Safeguarding Device","text":"<ul> <li>A device designed, constructed and installed to create a sensing field to detect an intrusion into such field by people, robots or objects</li> </ul>"},{"location":"Pressure%20%3D%20ForceArea/","title":"Pressure = Force/Area","text":"<ul> <li> \\[P = \\frac{F}{A}\\] </li> </ul>"},{"location":"Presynaptic%20Cell/","title":"Presynaptic Cell","text":"<ul> <li>In synaptic transmission, the neuron that sends a nerve impulse across the synaptic cleft to another neuron.</li> </ul>"},{"location":"Prion/","title":"Prion","text":"<ul> <li>A protein aggregate that can multiply itself, inducing the formation of new aggregates from individual copies of the protein it encounters. Prions have the potential to spread within the body and brain, and even from one organism to another\u2014\u201cinfectiously,\u201d like a virus. The first prions described were hardy aggregates of PrP, the prion protein. They are responsible for a set of rapid, fatal, and potentially transmissible neurodegenerative diseases including Creutzfeldt-Jakob disease and bovine spongiform encephalopathy (\u201cmad cow disease\u201d). Many researchers now argue that protein aggregates in other neurodegenerative diseases, such as the A\u03b2 and tau plaques of Alzheimer\u2019s, have such similar properties that they also deserve to be called prions.</li> </ul>"},{"location":"Prismatic%20Joint/","title":"Prismatic Joint","text":"<ul> <li>Linear movement like a piston</li> </ul>"},{"location":"Probability/","title":"Probability","text":"<ul> <li>Frequentist</li> <li>Bayesian</li> </ul>"},{"location":"Programmable%20Logical%20Controller%20%28PLC%29/","title":"Programmable Logical Controller (PLC)","text":"<ul> <li>A solid-state control system, which has a user programmable memory for storage of instructions to implement specific functions such as: I/O control logic, timing, counting arithmetic and data manipulation</li> <li>A PLC consists of a central processor, input/output interface, memory and programming device, which typically uses relay equivalent symbols.</li> </ul>"},{"location":"Propose-but-verify/","title":"Propose-but-verify","text":"<ul> <li>Speakers guess a word-concept association</li> <li>Keep guess until they encounter contradicting information</li> <li>Seems to correctly model competitions between features/cues</li> </ul>"},{"location":"Protein%20Folding/","title":"Protein Folding","text":"<ul> <li>The process by which the chain of amino acids that make up a protein assumes its functional shape. The protein clumps and tangles that occur in some neurodegenerative disorders are thought to be triggered when proteins \u201cmisfold.\u201d</li> </ul>"},{"location":"Protein%20Modeling/","title":"Protein Modeling","text":"<ul> <li>Using Bayesian models</li> <li>Task : Estimate Probability mass function(because discrete) for a finite, discrete distribution -&gt; given a histogram from a sample</li> <li>Large number of categories and small number of observations</li> <li><ul> <li>Estimate Probability distrib of amino acids in each column in a protein class. 20 dim PMF (one for each site)</li> <li>Can be aligned</li> <li>High chances of class not being present in data<ul> <li>MLE will assign 0 Probability to X</li> <li>Wrong decision made for a lot of them that were not in the training set</li> <li>Cannot use</li> </ul> </li> </ul> </li> <li>20 dim PMF for amnio acid distrib : \\(\\theta = (\\theta_{1}, \u2026 ,\\theta_{20})' = (P(X=A), \u2026, P(X=Y))'\\)<ul> <li>count vectors of amino acids found in a given site in training data D</li> <li>Distributed according to Multinomial Distribution with l = 20</li> </ul> </li> </ul>"},{"location":"Protein%20Modeling/#using-prior","title":"Using Prior","text":"<ul> <li>0 probabilities should not occur. (\\(\\mathcal{H} = (\\theta_{1}, \u2026, \\theta_{20})' \\in \\mathbb{R}^{20}|\\theta_{j} \\in (0,1)\\)\\) and $$ \\Sigma_{j} \\theta_{j}=1$$<ul> <li>19 dim hypervolume</li> <li>Continuous space and so can use PDF</li> <li>Dirichlet Distribution is used to represent it because parameterized with l = 20</li> </ul> </li> <li></li> <li>\\(\\alpha\\)s fixed beforehand</li> </ul>"},{"location":"Proto%20Distributions/","title":"Proto Distributions","text":"<ul> <li>Distributions</li> <li>Occur in Bayesian</li> </ul>"},{"location":"Proto%20Distributions/#continuous-spaces","title":"Continuous Spaces","text":"<ul> <li>Proto PDF</li> <li> \\[p(x|\\theta) = \\frac{1}{\\int_{\\mathbb{R}^{k}}p_{0}(x| \\theta)dx}p_{0}(x|\\theta)\\] </li> <li>\\(p_{0}\\) gives the shape of the PDF</li> <li>\\(\\frac{1}{\\int_{\\mathbb{R}^{k}}p_{0}(x| \\theta)dx}\\) is a normalization so it integrates to 1</li> <li></li> <li>Most of the time we dont know a distribution but only its proto distribution. This is actually enough sometimes</li> </ul>"},{"location":"Proto%20Distributions/#discrete-spaces","title":"Discrete Spaces","text":"<ul> <li>Proto PMF</li> </ul>"},{"location":"Proto%20PDF/","title":"Proto PDF","text":"<ul> <li>Generalized PDF</li> <li>Proto PDF \\(g_{0}\\)</li> <li>Any non negative function \\(g_{0}: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}\\) that has a finite integral \\(\\(\\int_{\\mathbb{R}^{n}}g_{0}(x)dx\\)\\)</li> <li>If we divide \\(g_{0}\\) by its integral -&gt; we get a normal PDF g</li> </ul>"},{"location":"Proto%20PMF/","title":"Proto PMF","text":"<ul> <li>Generalized PMF</li> <li> \\[p(x|\\theta) = \\frac{1}{\\Sigma_{x \\in S}p_{0}(s)}p_{0}(x)\\] </li> <li>S is huge if there are many random variables</li> </ul>"},{"location":"Proximity%20Sensor/","title":"Proximity Sensor","text":"<ul> <li>A non-contact sensing device used to sense when objects are a short distance away, and it can determine the distance of the object.</li> </ul>"},{"location":"Proxy%20Attention/","title":"Proxy Attention","text":""},{"location":"Proxy%20Attention/#comparisons","title":"Comparisons","text":"<ul> <li>[[M2Det]] <ul> <li>The Multi-level Feature Pyramid Network has similar ideas but does not use the outputs of XAI algorithms. </li> <li>This uses channel wise attention : ours is independant of that</li> <li>This takes images and passes them through multiple networks and then aggregates the features obtained from each of those networks. : ours uses a trained network and is independent of all that. Unlike the former, ours does not use a compressed feature map but uses a trained network to predict an explainability map instead.</li> </ul> </li> </ul>"},{"location":"Proxy%20Attention/#backlinks","title":"Backlinks","text":"<ul> <li>Vision Explainibility</li> <li>Proxy Attention</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Proxy%20Objective/","title":"Proxy Objective","text":"<ul> <li>Easier to change or measure than the actual objective</li> <li>Suppose we have some sample space \\(S\\) (such as the set of possible question-answer pairs), some Probability distribution \\(P\\) over \\(S\\), a true objective (or \u201creward\u201d) \\(R_{true}: S \\to \\mathbb{R}\\) , proxy objective \\(R_{proxy}:S \\to \\mathbb{R}\\) and we optimize \\(R_{proxy}\\) to get a new distribution \\(P'\\)</li> <li>\\(E_{x'\\sim P'}[Rtrue(x\u2032)]\\) is how well the true objective is optimized<ul> <li>Monte Carlo estimator used</li> <li>If \\(N \\geq n\\) samples from P, simultaneously consider every possible subset of these samples of size nnn, weight each sample by the number of subsets for which it is the best according to the proxy objective, and then take the weighted average true objective \\(\\(\\binom{k-1}{n-1}\\)\\) where k is the rank of the sample under the proxy objective, from 1 (worst) up to N (best)</li> <li>Can reuse samples of n</li> </ul> </li> <li>KL Divergence \\(P' || P\\) measures how much optimization is done<ul> <li>As long as Continous , \\(\\(n - \\frac{n-1}{n}\\)\\)</li> </ul> </li> </ul>"},{"location":"Proxy%20Objective/#refs","title":"Refs","text":"<ul> <li>openai</li> </ul>"},{"location":"Pruning/","title":"Pruning","text":"<ul> <li>Mainly that of being able to reduce the size, cost and computational requirements of my models, all while maintaning the accuracy (sort of atleast).</li> <li>Generally this comes about by removing parameters in some form or fashion.</li> <li>Rather than taking a mask, we can prune certain parts of the network by setting them to 0 or by dropping them if required. (aka weights and biases)</li> <li>In most cases, the network is first trained for a while. Then pruned. Which reduces its accuracy and is thus trained again (fine tuning). This cycle is repeated until we get the results we require.</li> <li>Major Types of Pruning Methods</li> <li>Structure Based Pruning</li> <li>Scoring Pruning Approaches</li> <li>Scheduling</li> <li>Fine Tuning Based Pruning</li> <li>Global Magnitude Based Pruning</li> <li>Global Gradient Magnitude Based Pruning</li> <li>Layerwise Gradient Magnitude Based Pruning</li> <li>Random Pruning</li> <li>Layerwise Magnitude Based Pruning</li> </ul>"},{"location":"Psycholinguistics/","title":"Psycholinguistics","text":"<ul> <li>The study of how the brain processes or produces language</li> <li>Linguistics proper is more concerned with what the structure of language is</li> <li>Also examines how linguistic processing interacts with executive functions, e.g. \u2022 Working memory \u2022 Inhibition</li> </ul>"},{"location":"Psychosis/","title":"Psychosis","text":"<ul> <li>A severe symptom of mental illness in which a person\u2019s thoughts and perceptions are so disordered that the individual loses touch with reality.</li> </ul>"},{"location":"Pulse%20Coordinates/","title":"Pulse Coordinates","text":"<ul> <li>Yaskawa robots define robot joint axes position in degrees for revolute joints. Pulse is also another way to specify robot joint position, and it does so in robot motor encoder pulse counts.</li> </ul>"},{"location":"Pulse%20Oximeter/","title":"Pulse Oximeter","text":"<ul> <li>A small device that clips to the finger, toe or earlobe used to measure blood oxygen saturation</li> </ul>"},{"location":"Punctuation/","title":"Punctuation","text":"<ul> <li>Punctuation characters are treated as separate tokens \u2013 usually</li> </ul>"},{"location":"Pupil%20Dilation/","title":"Pupil Dilation","text":"<ul> <li>Pupil diameter responds to more than just light<ul> <li>When something was visually pleasing</li> <li>Harder problems - thinking time</li> <li>Reaches an asymptote when the task is too difficult , processing load</li> </ul> </li> <li>Measure of resource allocation</li> <li></li> <li>Light based response - parasympathetic</li> <li>Increases with emotional stimulation</li> <li></li> <li>Speech replacement</li> </ul>"},{"location":"Puzzle%20Mix/","title":"Puzzle Mix","text":"<ul> <li>learns to augment two images optimally based on saliency.</li> <li>Images are divided into regions for the mixup</li> <li>The algorithm learns to transport the salient region of one image such that the output image has the maximized saliency from both images.</li> </ul>"},{"location":"Pyramidal%20cell/","title":"Pyramidal Cell","text":"<ul> <li><ul> <li>Folds on sides have same charge</li> <li>Adds up so can be measured</li> <li>Equivalent Current Dipole</li> </ul> </li> </ul>"},{"location":"Pytorch%20Tricks/","title":"Pytorch Tricks","text":"<ul> <li>Also look at fastai</li> </ul>"},{"location":"Pytorch%20Tricks/#get-params-of-a-layer","title":"Get Params of a Layer","text":"<pre><code>m = learn.model\nl = m.get_submodule('0.model.stem.1')\nlist(l.parameters())\n</code></pre>"},{"location":"Pytorch%20Tricks/#interact","title":"Interact","text":"<pre><code>from ipywidgets import interact\n@interact(m=1.5, b=1.5)\ndef plot_relu(m,b):\n    plot_function(partial(relu, m, b), ylim = (-1, 4))\n</code></pre>"},{"location":"Pytorch%20Tricks/#set-dataset-directory","title":"Set Dataset Directory","text":"<pre><code>import os\nos.environ[\"TORCH_HOME\"] = \"/media/hdd/Datasets/\"\nos.environ[\"FASTAI_HOME\"] = \"/media/hdd/Datasets/\"\n</code></pre>"},{"location":"Quadratic%20Loss/","title":"Quadratic Loss","text":"<ul> <li>$\\(W = argmin_{W^{\\ast}}\\Sigma^N_{i=1} ||W^{\\ast} x_i - y_i||^2\\)</li> <li>\\(\\Delta : \\mathbb{R}^{n} \\rightarrow \\mathbb{R}, x \\rightarrow E[Y|X = x]\\) is the gold standard for minimizing this. But \\(\\Delta\\) is unknown</li> </ul>"},{"location":"Quadratic%20Potential%20Field/","title":"Quadratic Potential Field","text":""},{"location":"Quadratic%20Potential%20Field/#quadratic-potential-field","title":"Quadratic Potential Field","text":"<ul> <li>make attractive potential to the goal \\(\\(U_{att}(q)=\\frac{1}{2}D(q,q_{goal})\\)\\)</li> <li></li> </ul>"},{"location":"Quadratic%20Potential%20Field/#backlinks","title":"Backlinks","text":"<ul> <li>Quadratic Potential Field</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Quantifier%20spreading%20children%20misled%20by%20ostensive%20cues/","title":"Quantifier Spreading Children Misled by Ostensive Cues","text":"<ul> <li>Katalin \u00c9. Kiss and Tam\u00e1s Z\u00e9t\u00e9nyi</li> </ul> <ul> <li>TL;DR : Use real images instead of Drawings</li> <li>economy of the stimulus employed in child language experiments may lend an increased ostensive effect to the message communicated to the child</li> <li>Thus, when the visual stimulus in a sentence-picture matching task is a minimal model abstracting away from the details of the situation, children often regard all the elements of the stimulus as ostensive clues to be represented in the corresponding sentence</li> <li>The use of such minimal stimuli is mistaken when the experiment aims to test whether or not a certain element of the stimulus is relevant for the linguistic representation or interpretation</li> <li>It is claimed that children find a universally quantified sentence like Every girl is riding a bicycle to be a false description of a picture showing three girls riding bicycles and a solo bicycle because they are misled to believe that all the elements in the visual stimulus are relevant, hence all of them are to be represented by the corresponding linguistic description.</li> <li>When the iconic drawings were replaced by photos taken in a natural environment rich in accidental details, the occurrence of quantifier spreading was radically reduced.</li> <li>It is shown that an extra object in the visual stimulus can lead to the rejection of the sentence also in the case of sentences involving no quantification, which gives further support to the claim that the source of the problem is not (or not only) the grammatical or cognitive difficulty of quantification but the unintended ostensive effect of the extra object.</li> <li>The reason for the unexpected reactions is that the experimental stimulus presented to the child is devoid of any episodic details; it merely contains a few iconic symbols, which suggests to the child that the irrelevant details have been omitted; hence every element of the stimulus, including the one whose relevance the experiment aims to test, is to be interpreted as an ostensive signal, i.e., every element of the stimulus is significant.</li> <li>Ostension</li> </ul>"},{"location":"Quantifier%20spreading%20children%20misled%20by%20ostensive%20cues/#quantifier-spreading-as-an-ostensive-effect","title":"Quantifier Spreading as an Ostensive Effect","text":"<ul> <li>The phenomenon</li> <li>Every girl is riding a bicycle.</li> <li>Although every one of the three girls in the picture is riding a bicycle, many children find the sentence false</li> <li>When asked \"Why?\", they point at the solo bicycle, and say something like \"Not that bicycle\", i.e., they show 'Exhaustive Pairing' under an extra object condition.</li> <li>Quantifier spreading also has a somewhat less common variant, called \"Perfectionist Response\".1 It occurs when a universally quantified sentence like (2a) is to be matched with a picture like Figure 2, which contains an element that is neither identical with the referent of the subject, nor identical with the referent of the VP-internal complement.2 (2) a. Every dog is eating a bone. b. No, not that one.</li> </ul>"},{"location":"Quantifier%20spreading%20children%20misled%20by%20ostensive%20cues/#theories-of-quantifier-spreading","title":"Theories of Quantifier Spreading","text":"<ul> <li>In fact, children are not fully consistent in assigning to universally quantified sentences interpretations of type (3b); the adult interpretation illustrated in (3a),</li> <li>too, appears to be accessible also to those favoring the spreading reading.</li> <li>The event quantification analysis of Philip (1995) has been criticized on several grounds. For example, it predicts that quantifier spreading is only attested in the case of eventive sentences. In fact, as shown by Philip (2011), it also occurs with sentences of type (4), which contain no event variable:</li> <li>Furthermore, as Crain et al. (1996) point out, the analysis of every as an event quantifier does not account for the \"perfectionist\" mistake, i.e., for the case when the sentence questioned in (1a) is found false in the presence of an extra participant that is neither a girl, nor a bicycle</li> <li>The fact that children have initially access to two interpretations of universally quantified sentences (those of type (3b) and (3a)), one of which is later eliminated, raises a learnability problem, as well \u2013 under the assumption that children acquiring their mother tongue only have access to positive evidence.</li> <li>Several experiments on quantifier spreading have shown that the rate of spreading is affected by pragmatic factors, e.g., a rich linguistic or visual context reduces spreading (cf. Crain et al. 1996</li> <li>However, some of the evidence concerning the role of extra elements appears to be contradictory; e.g., in the case of quantifier spreading, both the increasing of the number of extra objects (Freeman, Sinha &amp; Stedmon 1982), and the decreasing of the size of the extra object (Philip 2011: 377) have been found to reduce the proportion of spreading, which has not been given a principled explanation.</li> <li>Relevance Account</li> <li>Salient Object Strategy</li> <li>Quantifier spreading is due to the increased ostensive effect of iconic stimuli</li> <li>We hypothesized that quantifier spreading is elicited in experimental situations where the stimulus is not embedded in a context, and is devoid of episodic details, as a consequence of which it gains a \u2013 potentially misleading \u2013 concentrated ostensive effect</li> <li>Crucially, however, when the stimulus only contains a few iconic symbols, every one of its elements gains an ostensive effect.</li> </ul>"},{"location":"Quantifier%20spreading%20children%20misled%20by%20ostensive%20cues/#experiment","title":"Experiment","text":""},{"location":"Quantifier%20spreading%20children%20misled%20by%20ostensive%20cues/#participants","title":"Participants","text":"<ul> <li>We tested 82 children from 5 Budapest kindergartens, whose mean age was 5;3 years (SD=0.73).</li> <li>We also carried out the experiment with an adult control group consisting of 24 university students, whose mean age was 21 years (SD=1.61).</li> </ul>"},{"location":"Quantifier%20spreading%20children%20misled%20by%20ostensive%20cues/#procedure","title":"Procedure","text":"<ul> <li>The child, the experimenter, and a helper were seated at a table in front of a laptop in a quiet room of the kindergarten.</li> <li>The helper held a teddy bear</li> <li>The experimenter told the child that they would look at pictures on the computer screen together.</li> <li>They would listen to what the bear said about each picture, and the experimenter would ask the subject whether or not it was true.</li> </ul>"},{"location":"Quantifier%20spreading%20children%20misled%20by%20ostensive%20cues/#materials","title":"Materials","text":"<ul> <li>16 sentence\u2013picture pairs (8 fillers and 8 test pairs) were presented to the subjects Each test sentence involved the universal quantifier minden \u201bevery'</li> <li>Four sentence\u2013picture pairs were of the type which can elicit the Exhaustive Pairing mistake, i.e., they involved an extra object (see example (7) and Figures 3, 4), and four sentence\u2013picture pairs were of the type which can elicit the Perfectionist Response, i.e., they contained an extra element neither identical with the referent of the subject, nor identical with the referent of the VPinternal complement (see example (8) and Figures 5, 6)</li> </ul>"},{"location":"Quantifier%20spreading%20children%20misled%20by%20ostensive%20cues/#results","title":"Results","text":"<ul> <li>The stimuli consisting of a quantified sentence and a drawing elicited quantifier spreading in 27% of the children's answers. In the case of the stimuli consisting of a quantified sentence and a photo, the rate of quantifier spreading dropped to 15%. Among the adults, the rate of quantifier spreading was 6% and 5%, respectively</li> </ul>"},{"location":"Quantifier%20spreading%20children%20misled%20by%20ostensive%20cues/#discussion","title":"Discussion","text":"<ul> <li>In language acquisition experiments, experimenters tend to use iconic visual stimuli</li> <li>in order to eliminate irrelevant distractors, and to ensure that children only react to the controlled factor(s)</li> <li>Our results suggest that this method is mistaken when the experiment aims to test whether or not an element in the stimulus is relevant for the linguistic representation</li> <li>If the visual stimulus is a minimal model devoid of episodic details, children tend to interpret all of its elements as ostensive clues to be represented linguistically</li> <li>If the ostensive effect is diminished by the use of photos taken in natural environments, the proportion of QS is reduced by nearly 50%.</li> <li>The only photo which elicited a relatively high proportion (36%) of quantifier spreading answers (Figure 8) is a picture of a fairly artificial-looking setup with remarkably few details:</li> <li>Decreasing the size of the extra object makes the object less salient; but increasing the number of the extra objects does not necessarily decrease their salience, and what is more, it is not clear why an increase in the salience of the extra object should result in the increased frequency of quantifier spreading responses.</li> <li>Misleading ostensive effect in other types of acquisition experiments An example: A test of exhaustivity</li> <li>t has been tested in several experiments (e.g., Beaver &amp; Onea 2011; Kas &amp; Luk\u00e1cs 2013; Pint\u00e9r 2016) whether the exhaustivity of the preverbal focus of the Hungarian sentence (corresponding roughly to an English cleft constituent) is an inherent semantic property or a cancellable pragmatic implicature</li> <li>The tasks involved truth value judgements; experimenters aimed to find out whether children and adults accept a focus construction like (11) as a true description of a non-exhaustive situation like that in Figure 11 (both cited from Pint\u00e9r 2016):</li> </ul>"},{"location":"Quantifier%20spreading%20children%20misled%20by%20ostensive%20cues/#results_1","title":"Results","text":"<ul> <li>The rate of rejection of the sentences as true descriptions of the visually represented situations significantly correlated with the type of the visual representation.</li> <li>The sentence\u2013 drawing pairs were rejected in 10.53% of the cases.</li> <li>n the case of the sentence\u2013photo pairs, the rate of rejection was a mere 3.51%.</li> <li>Just as in Pint\u00e9r's (2016) experiment, the rate of rejection (i.e., the rate of the exhaustive interpretation of the sentences) was slightly even higher in the adult control group: 13.33% in the case of sentence\u2013drawing pairs, and 8.88% in the case of sentence\u2013photo pairs (see Figure 18). When we asked the subjects giving</li> <li>negative answers why e.g. (14) was not true of Figure 14, they consistently gave answers of the following type: \"Because the woman is also feeding the ducks\".</li> <li>15 children (39%) gave at least one negative answer</li> </ul>"},{"location":"Quantifier%20spreading%20children%20misled%20by%20ostensive%20cues/#discussion_1","title":"Discussion","text":"<ul> <li>The sentences tested in this experiment involved no special linguistic or cognitive difficulty; they were simple declarative sentences with no quantification, let alone universal quantification; nevertheless, 10.53% of the preschoolers evaluated them as false descriptions of the drawings intended to represent them visually.</li> <li>Since this rate is not high (though it is comparable to the 12% of partial rejection obtained by Pint\u00e9r 2016 in this age group), we might be tempted to attribute it to noise (children's failure to pay attention, etc.)</li> <li>However, if the 10.53% rate of rejection had been due to noise, it would not have dropped to 3.51% when the visual stimuli were represented by photos.</li> <li>The comments of the children giving negative answers made it clear that they rejected the given sentence\u2013picture pair because the picture contained extra objects that were not repre-</li> <li>sented linguistically</li> <li>Crucially, the photos contained many more extra agents and extra objects than the drawings, still they elicited significantly less rejections.</li> <li>What made the presence of extra objects in the drawings ostensive was the minimality of the drawings, suggesting that everything irrelevant had been eliminated from them.</li> </ul>"},{"location":"Quantifier%20spreading%20children%20misled%20by%20ostensive%20cues/#pictures","title":"Pictures","text":""},{"location":"Quantifiers/","title":"Quantifiers","text":"<ul> <li>how many are identified (all, some, none) A type of word that indicates quantity</li> <li>Nominal</li> <li>Each, Every, All, Many, Some, No, Few, Most</li> <li>Adverbial</li> <li>Quantify over time: Always, Usually, Never, Rarely</li> <li>Quantify over space: Everywhere, Somewhere, Nowhere</li> <li>Others</li> <li>Modals are used to show that we believe something is certain, probable or possible</li> <li>Modal verbs: Can, Could, May, Might, Must</li> <li>Modal quantifiers: Perhaps, Necessarily, Maybe</li> </ul>"},{"location":"Quantifying%20Uncertainty/","title":"Quantifying Uncertainty","text":""},{"location":"Quantile%20Bucketing/","title":"Quantile Bucketing","text":"<ul> <li>Distributing a feature's values into buckets so that each bucket contains the same (or almost the same) number of examples.</li> </ul>"},{"location":"Quantized%20Distillation/","title":"Quantized Distillation","text":"<ul> <li>Network quantization reduces the computation com- plexity of neural networks by converting high-precision networks (e.g., 32-bit floating point) into low-precision networks (e.g., 2-bit and 8-bit). Meanwhile, knowledge distillation aims to train a small model to yield a performance comparable to that of a complex model.</li> <li>Specifically, Polino et al. (2018) proposed a quan- tized distillation method to transfer the knowledge to a weight-quantized student network. In (Mishra and Marr, 2018), the proposed quantized KD is called the \u201cap- prentice\u201d. A high precision teacher network transfers knowledge to a small low-precision student network. To ensure that a small student network accurately mim- ics a large teacher network, the full-precision teacher network is first quantized on the feature maps, and then the knowledge is transferred from the quantized teacher to a quantized student network (Wei et al., 2018).</li> </ul>"},{"location":"Quasi-static%20Clamping/","title":"Quasi-static Clamping","text":"<ul> <li>A type of contact between a person and part of a robot system where the body part can be clamped between the moving part of the robot system &amp; another fixed or moving part of the robot cell</li> </ul>"},{"location":"RACE/","title":"RACE","text":""},{"location":"RAHP/","title":"RAHP","text":"<ul> <li>Recall at high precision</li> </ul>"},{"location":"RANSAC/","title":"RANSAC","text":""},{"location":"RANSAC/#ransac","title":"RANSAC","text":"<ul> <li>It is an iterative method to estimate parameters of a mathematical model from a set of observed data</li> <li>A simple example is fitting a line to a set of observations.</li> <li>Outliers are points that don't \"fit\" the model and points that do fit are called \"inliers\"</li> <li>Table detection<ul> <li>The algorithm starts by generating plane hypotheses based on three unique points.</li> <li>For each plane hypothesis, distances from all points in the point cloud to the plane are computed.</li> <li>The plane hypotheses are then scored based on counting the number of inlier points, e.g., distance to the plane \uf8ff 20mm.</li> <li></li> </ul> </li> <li>The RANSAC algorithm is repeated for a certain number of iterations, e.g., n = 200.</li> <li>Object detection<ul> <li>It is now possible to extract the points which lie directly above it.</li> <li>By removing the table, we have a point cloud where all the objects that are on top of the table are included.</li> <li>The obtained point cloud is then segmented into individual clusters Each small group of points will be treated as an object candidate.</li> <li></li> </ul> </li> <li></li> </ul>"},{"location":"RANSAC/#backlinks","title":"Backlinks","text":"<ul> <li>RANSAC</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"RETAIn/","title":"RETAIn","text":"<ul> <li>REverse Time AttentIoN mechanism</li> <li>The approach mimics physician practice by attending to the EHR data. Two RNNs are trained in a reverse time order with the goal of efficiently generating the appropriate attention variables. It is based on a two-level neural attention generation process that detects influential past visits and significant clinical variables to improve accuracy and interpretability.</li> <li>for application to Electronic Health Record (EHR) data.</li> </ul>"},{"location":"RETRO/","title":"RETRO","text":"<ul> <li>Improving Language Models by Retrieving from Trillions of Tokens</li> <li>Retrieval-Enhanced Transformer</li> <li>RETRO</li> <li>enhances auto-regressive language models by conditioning on document chunks retrieved from a large corpus</li> <li>based on local similarity with preceding tokens</li> <li>comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25x fewer parameters</li> <li>frozen BERT retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training</li> <li>Wikitext103</li> <li>Pile</li> <li>improving semi-parametric language models through explicit memory can provide an orthogonal, more efficient approach than raw parameter scaling as they seek to build more powerful language models</li> </ul>"},{"location":"RICAP/","title":"RICAP","text":"<ul> <li>Random image cropping and patching</li> <li>cropping four regions from randomly sampled images and augmenting them to create a new image.</li> <li>The generated image has mixed labels proportional to the pasted area</li> <li>The area of cropped regions in the output image is determined by sampling through uniform distribution</li> <li>anywhere-RICAP (origin can be anywhere), center-RICAP (origin can only be in the middle of the image), and corner-RICAP (origin can only be in corners</li> <li>Corner-RICAP has shown the best performance because a larger region of one image is visible to the network to learn</li> </ul>"},{"location":"RISE/","title":"RISE","text":"<ul> <li>based on a stochastic approach</li> <li>Input images are iteratively altered via random noise, and the final saliency map is composed by accumulating the partial estimations</li> <li>However, its application requires much more computational power, as it needs to run hundreds of thousands of prediction cycles</li> <li>it seems that RISE is not able to highlight regions of interest of skin lesion images with the same reliability as on pictures of real-world objects</li> <li>In the first step, it creates a segmentation mask and applies it to the dermoscopic image. Secondly, it creates a structure segmentation mask to identify the structure of the dermoscopic image. After masking, the original segmented image and some nonvisual metadata are fed into a convolutional neural network for classification</li> </ul>"},{"location":"ROC%20Curve/","title":"ROC Curve","text":"<ul> <li>appropriate when the data is balanced</li> <li>x-axis : False Positive</li> <li>y-axis : True Positive</li> <li>area under the curve (AUC) can be used as a summary of the model performance<ul> <li>assign a higher probability to a randomly chosen real positive occurrence than a negative occurrence on average</li> </ul> </li> <li>Interpretation<ul> <li>Smaller values on the x-axis of the plot indicate lower false positives and higher true negatives.</li> <li>Larger values on the y-axis of the plot indicate higher true positives and lower false negatives.</li> </ul> </li> </ul>"},{"location":"Radial%20Plot/","title":"Radial Plot","text":"<ul> <li>number of associated attributes limited  </li> <li>distinct distinguishable values per attribute limited  </li> <li>visual mappings must be learned for correct interpretation</li> </ul>"},{"location":"Ramp%20up%20problem/","title":"Ramp up Problem","text":"<ul> <li>No data for a user -&gt; picked one</li> <li>Broad generalization</li> </ul>"},{"location":"RandAugment/","title":"RandAugment","text":"<ul> <li>Randaugment: Practical automated data augmentation with a reduced search space</li> <li>Ekin D. Cubuk \u2217, Barret Zoph\u2217, Jonathon Shlens, Quoc V. Le</li> </ul>"},{"location":"RandAugment/#chatgpt-summary","title":"ChatGPT Summary","text":"<ul> <li>Large-scale adoption of data augmentation methods is hindered by the need for a separate and expensive search phase.</li> <li>Commonly, a smaller proxy task is used to overcome the expense of the search phase, but it is not clear if the optimized hyperparameters found on the proxy task are also optimal for the actual task.</li> <li>The process of designing automated augmentation strategies is being rethought.</li> <li>It is proposed to only search for a single distortion magnitude that jointly controls all operations, which reduces computational expense and eliminates the need for a separate proxy task.</li> <li>The proposed method was tested on various datasets including CIFAR-10, CIFAR 100, SVHN, ImageNet, and COCO, and showed improvement in performance without the use of a proxy task.</li> <li>The proposed method, RandAugment, uses a parameter-free procedure of always selecting a transformation with uniform probability from a set of K=14 available transformations, and a single distortion magnitude that jointly controls all operations.</li> <li>RandAugment is able to achieve comparable or better performance compared to other automated augmentation methods, such as AutoAugment, without the need for a separate proxy task.</li> <li>The results suggest that the optimal data augmentation policies may depend on the specific model and dataset size, and a small proxy task may not provide the best indicator of performance on a larger task.</li> </ul>"},{"location":"RandAugment/#abstract","title":"Abstract","text":"<ul> <li>An obstacle to a large-scale adoption of these methods is that they require a separate and expensive search phase</li> <li>A common way to overcome the expense of the search phase was to use a smaller proxy task.</li> <li>However, it was not clear if the optimized hyperparameters found on the proxy task are also optimal for the actual task.</li> <li>rethink the process of designing automated augmentation strategies</li> <li>it is sufficient to only search for a single distortion magnitude that jointly controls all operations</li> <li>propose a simplified search space that vastly reduces the computational expense of automated augmentation, and permits the removal of a separate proxy task.</li> <li>CIFAR-10</li> <li>CIFAR 100</li> <li>SVHN</li> <li>ImageNet</li> <li>COCO datasets</li> </ul>"},{"location":"RandAugment/#systematic-failures-of-a-separate-proxy-task","title":"Systematic Failures of a Separate Proxy Task","text":"<ul> <li>A central premise of learned data augmentation is to construct a small, proxy task that may be reflective of a larger task</li> <li>Although this assumption is sufficient for identifying learned augmentation policies to improve performance, it is unclear if this assumption is overly stringent and may lead to sub-optimal data augmentation policies.</li> <li>two separate dimensions that are commonly restricted to achieve a small proxy task: model size and dataset size</li> <li>First, we train a family of Wide-ResNet architectures, where the model size may be systematically altered through the widening parameter governing the number of convolutional filters</li> <li>For each of these networks, we train the model on CIFAR-10 and measure the final accuracy compared to a baseline model trained with default data augmentations (i.e. horizontal flips and pad-and-crop)</li> <li>The Wide-ResNet models are trained with the additional K=14 data augmentations (see Section 3) over a range of global distortion magnitudes M parameterized on a uniform linear scale ranging from [0, 30]</li> <li>Namely, larger networks demand larger data distortions for regularization</li> <li>Conversely, a policy learned on a proxy task (such as AutoAugment) provides a fixed distortion magnitude (Figure 1b, dashed line) for all architectures that is clearly sub-optimal.</li> <li>A second dimension for constructing a small proxy task is to train the proxy on a small subset of the training data</li> <li>We first observe that models trained on smaller training sets may gain more improvement from data augmentation</li> <li>see that the optimal distortion magnitude is larger for models that are trained on larger datasets.</li> <li>optimal distortion magnitude increases monotonically with training set size</li> <li>One hypothesis for this counter-intuitive behavior is that aggressive data augmentation leads to a low signal-to-noise ratio in small datasets</li> <li>learned augmentation may learn an augmentation strength more tailored to the proxy task instead of the larger task of interest.</li> <li>The dependence of augmentation strength on the dataset and model size indicate that a small proxy task may provide a sub-optimal indicator of performance on a larger task.</li> </ul>"},{"location":"RandAugment/#automated-data-augmentation-without-a-proxy-task","title":"Automated Data Augmentation without a Proxy Task","text":"<ul> <li>The reason we wish to remove the search phase is because a separate search phase significantly complicates training and is computationally expensive.</li> <li>In order to remove a separate search phase, we aspire to fold the parameters for the data augmentation strategy into the hyper-parameters for training a model.</li> <li>Indeed, previous work enumerated a policy in terms of choosing which transformations to apply out of K=14 available transformations, and probabilities for applying each transformation:</li> <li>age diversity, we replace the learned policies and probabilities for applying each</li> <li>transformation with a parameter-free procedure of always selecting a transformation with uniform probability K1</li> <li>Given N transformations for a training image, RandAugment may thus express KN potential policies.</li> <li>magnitude of the each augmentation distortion.</li> <li>Briefly, each transformation resides on an integer scale from 0 to 10 where a value of 10 indicates the maximum scale for a given transformation</li> <li>A data augmentation policy consists of identifying an integer for each augmentation.</li> <li>and postulate that a single global distortion M may suffice for parameterizing all transformations</li> <li>We experimented with four methods for the schedule of M during training: constant magnitude, random magnitude, a linearly increasing magnitude, and a random magnitude with increasing upper bound</li> <li>The resulting algorithm contains two parameters N and M</li> <li>Both parameters are human-interpretable such that larger values of N and M increase regularization strength</li> <li>In order to reduce the parameter space but still maintain imInvestigating the dependence on the included transformations</li> <li>RandAugment is largely insensitive to the selection of transformations for different datasets.</li> <li>We see that while geometric transformations individually make the most difference, some of the color transformations lead to a degradation of validation accuracy on average</li> <li>Surprisingly, rotate can significantly improve performance and lower variation even when included in small subsets of RandAugment transformations, while posterize seems to hurt all subsets of all sizes.</li> </ul>"},{"location":"RandAugment/#learning-the-probabilities-for-selecting-image-transformations","title":"Learning the Probabilities for Selecting Image Transformations","text":"<ul> <li>For K=14 image transformations and N =2 operations, \u03b1ij constitutes 28 parameters. We initialize all weights such that each transformation is equal probability (i.e. RandAugment), and update these parameters based on how well a model classifies a held out set of validation images distorted by \u03b1ij.</li> <li>This approach was inspired by density matching [19], but instead uses a differentiable approach in lieu of Bayesian optimization.</li> <li>We label this method as a 1st-order density matching approximation.</li> <li>The 1st -order method improves accuracy by more than 3.0% for both models on reduced CIFAR-10 compared to the baseline of flips and pad-and-crop</li> <li>Although the density matching approach is promising, this method can be expensive as one must apply all K transformations N times to each image independently.</li> <li>Hence, because the computational demand of KN transformations is prohibitive for large images, we reserve this for future exploration.</li> <li>learning the probabilities through density matching may improve the performance on small-scale tasks and reserve explorations to larger-scale tasks for the future.</li> <li>RandAugment selects all image transformations with equal probability</li> <li>This opens up the question of whether learning K probabilities may improve performance further.</li> <li>Most of the image transformations (except posterize, equalize, and autoContrast) are differentiable, which permits backpropagation to learn the K probabilities</li> </ul>"},{"location":"RandAugment/#discussion","title":"Discussion","text":"<ul> <li>not tailoring the number of distortions and the distortion magnitude to the dataset size nor the model size leads to sub-optimal performance</li> <li>In previous work, scaling learned data augmentation to larger dataset and models have been a notable obstacle. For example, AutoAugment and Fast AutoAugment could only be optimized for small models on reduced subsets of data</li> <li>The proposed method scales quite well to datasets such as ImageNet and COCO while incurring minimal computational cost (e.g. 2 hyper-parameters), but notable predictive performance gains.</li> </ul>"},{"location":"RandAugment/#images","title":"Images","text":""},{"location":"RandAugment/#backlinks","title":"Backlinks","text":"<ul> <li>KeepAugment</li> <li> <p>Keep- Augment identifies the salient area in an image and assures the image generated by the augmenta- tion strategies, for example, Cutout, RandAugment [14], CutMix [82] or AutoAugment [13], contains salient region in it.</p> </li> <li> <p>SLAK</p> </li> <li>We share the (pre-)training settings of SLaK on ImageNet-1K in this section. We train SLaK for 300 epochs (Section 5.1) and 120 epochs (Section 4) using AdamW (Loshchilov &amp; Hutter, 2019) with a batch size of 4096, and a weight decay of 0.05. The only differnce between models training for 300 epochs and 120 epochs is the training time. The learning rate is 4e-3 with a 20-epoch linear warmup followed by a cosine decaying schedule. For data augmentation, we use the default setting of RandAugment (Cubuk et al., 2020) in Timm (Wightman, 2019) \u2013 \"rand-m9-mstd0.5- inc1\", Label Smoothing (Szegedy et al., 2016) coefficient of 0.1, Mixup (Zhang et al., 2017) with \u21b5 = 0.8, Cutmix (Yun et al., 2019) with \u21b5 = 1.0, Random Erasing (Zhong et al., 2020) with p = 0.25, Stochastic Depth with drop rate of 0.1 for SLaK-T, 0.4 for SLaK-S, and 0.5 for SLaK-B, Layer Scale (Touvron et al., 2021c) of initial value of 1e- 6, and EMA with a decay factor of 0.9999. We train SLaK-T with NVIDIA A100 GPUs and the rest of models are trained with NVIDIA V100.</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Random%20Erasing/","title":"Random Erasing","text":"<ul> <li>deletes contiguous rectangular image regions similar to cutout with minor differences in region selection procedure.</li> <li>Opposite to cutout, where deletion is applied on all the images, here it is performed with a probability of either applying it or not</li> <li>In every iteration, region size is defined randomly with upper and lower limits on region area and aspect ratio.</li> <li>Additional to this, random erasing provides region-aware deletion for object detection and person identification tasks</li> <li>Regions inside the object bounding boxes are randomly erased to generate occlusions</li> </ul>"},{"location":"Random%20Erasing/#backlinks","title":"Backlinks","text":"<ul> <li>GridMask</li> <li> <p>The algorithm tries to overcome drawbacks of Cutout, Random Erasing, and Hide and Seek that are prone to deleting important information entirely or leaving it untouched without making it harder for the algorithm to learn.</p> </li> <li> <p>SLAK</p> </li> <li>We share the (pre-)training settings of SLaK on ImageNet-1K in this section. We train SLaK for 300 epochs (Section 5.1) and 120 epochs (Section 4) using AdamW (Loshchilov &amp; Hutter, 2019) with a batch size of 4096, and a weight decay of 0.05. The only differnce between models training for 300 epochs and 120 epochs is the training time. The learning rate is 4e-3 with a 20-epoch linear warmup followed by a cosine decaying schedule. For data augmentation, we use the default setting of RandAugment (Cubuk et al., 2020) in Timm (Wightman, 2019) \u2013 \"rand-m9-mstd0.5- inc1\", Label Smoothing (Szegedy et al., 2016) coefficient of 0.1, Mixup (Zhang et al., 2017) with \u21b5 = 0.8, Cutmix (Yun et al., 2019) with \u21b5 = 1.0, Random Erasing (Zhong et al., 2020) with p = 0.25, Stochastic Depth with drop rate of 0.1 for SLaK-T, 0.4 for SLaK-S, and 0.5 for SLaK-B, Layer Scale (Touvron et al., 2021c) of initial value of 1e- 6, and EMA with a decay factor of 0.9999. We train SLaK-T with NVIDIA A100 GPUs and the rest of models are trained with NVIDIA V100.</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Random%20Factors/","title":"Random Factors","text":"<ul> <li>These are factors that can affect the outcome that we do not design, and cannot control</li> <li>Two common types</li> <li>Participant<ul> <li>People differ. Some people are very strict. Some people accept everything.</li> </ul> </li> <li>Item<ul> <li>Some sentences are weird with monkeys. Some situations are hard to draw so the picture is a bit odd.</li> </ul> </li> <li>When we analyze the data we have to take into account that there will be variation present because of these random factors.</li> </ul>"},{"location":"Random%20Pruning/","title":"Random Pruning","text":"<ul> <li>Each weight independantly considered and dropped with a fraction of network required</li> <li>For this we first take the number of values to prune by identifying the total size of the weights and then multiplying it by the fraction of values to remove.</li> </ul>"},{"location":"Rank%20%28Tensor%29/","title":"Rank (Tensor)","text":"<ul> <li>The number of dimensions in a Tensor. For instance, a scalar has rank 0, a vector has rank 1, and a matrix has rank 2.</li> </ul>"},{"location":"Rapid%20Eye%20Movement%20%28REM%29%20Sleep/","title":"Rapid Eye Movement (REM) Sleep","text":"<ul> <li>A stage of sleep occurring approximately 90 minutes after sleep onset characterized by increased brain activity, rapid eye movements, and muscle relaxation.</li> </ul>"},{"location":"Rational%20inference/","title":"Rational Inference","text":"<ul> <li>Uses all Available cues</li> </ul>"},{"location":"Raycasting/","title":"Raycasting","text":"<ul> <li>Volume Rendering Equation</li> <li>Back To Front Raycasting</li> <li>Color Compositing</li> <li>Front to Back Raycasting</li> <li>Sampling Ray Casting</li> <li>Classification Ray Casting</li> <li>Slice Based Volume Rendering</li> <li>Voxel Projection</li> </ul>"},{"location":"ReMix/","title":"ReMix","text":"<ul> <li>addresses the issue of class imbalance by generating mixed images for minority classes.</li> <li>in the case of label assignment, it sets the label of the output image to the minority class.</li> </ul>"},{"location":"Real%20Time%20Image%20Saliency%20for%20Black%20Box%20Classifiers/","title":"Real Time Image Saliency for Black Box Classifiers","text":"<ul> <li>Piotr Dabkowski</li> <li>Yarin Gal</li> </ul>"},{"location":"Real%20Time%20Image%20Saliency%20for%20Black%20Box%20Classifiers/#tldr","title":"TL;DR","text":"<ul> <li>New metric to judge how good a saliency map is using the largest rectangle that can define it. Training to reduce adversarial artefacts introduced due to masking with non smooth masks</li> </ul>"},{"location":"Real%20Time%20Image%20Saliency%20for%20Black%20Box%20Classifiers/#abstract","title":"Abstract","text":"<ul> <li>fast saliency detection method that can be applied to any differentiable image classifier</li> <li>masking model</li> <li>manipulate the scores of the classifier by masking salient parts of the input image</li> <li>requires a single forward pass to perform saliency detection</li> <li>CIFAR</li> <li>ImageNet</li> <li>new metric for saliency</li> </ul>"},{"location":"Real%20Time%20Image%20Saliency%20for%20Black%20Box%20Classifiers/#image-saliency-and-introduced-evidence","title":"Image Saliency and Introduced Evidence","text":"<ul> <li>no single obvious metric that could measure the quality of the produced map</li> <li>In simple terms, the saliency map is defined as a summarised explanation of where the classifier \"looks\" to make its prediction.</li> <li></li> <li>SSR</li> <li>SDR</li> <li>In order to be as informative as possible we would like to find a region that performs well as both SSR and SDR.</li> <li>Both SDR and SSR remove some evidence from the image</li> <li>there are few ways of removing evidence, for example by blurring the evidence, setting it to a constant colour, adding noise, or by completely cropping out the unwanted parts</li> <li>Unfortunately, each one of these methods introduces new evidence that can be used by the classifier as a side effec</li> </ul>"},{"location":"Real%20Time%20Image%20Saliency%20for%20Black%20Box%20Classifiers/#fighting-the-introduced-evidence","title":"Fighting the Introduced Evidence","text":"<ul> <li>by manipulating the image we always introduce some extra evidence applying a mask M to the image X to obtain the edited image E</li> <li>the simplest case we can simply multiply X and M element-wise:</li> <li> \\[E = X \\odot M\\] </li> <li>This operation sets certain regions of the image to a constant \"0\" colour</li> <li>While setting a larger patch of the image to \"0\" may sound rather harmless (perhaps following the assumption that the mean of all colors carries very little evidence), we may encounter problems when the mask M is not smooth</li> <li>in the worst case, can be used to introduce a large amount of additional evidence by generating adversarial artifacts</li> <li>Adversarial artifacts generated by the mask are very small in magnitude and almost imperceivable for humans, but they are able to completely destroy the original prediction of the classifier</li> <li>we may change the way we apply a mask to reduce the amount of unwanted evidence due to specifically-crafted masks</li> <li> \\[E = X \\odot M + A \\odot (1-M)\\] </li> <li>where A is an alternative image</li> <li>A can be chosen to be for example a highly blurred version of X</li> <li>In such case mask M simply selectively adds blur to the image X and therefore it is much harder to generate high-frequency-high-evidence artifacts</li> <li>Unfortunately, applying blur does not eliminate existing evidence very well, especially in the case of images with low spatial frequencies like a seashore or mountains.</li> <li>Another reasonable choice of A is a random constant colour combined with highfrequency noise. This makes the resulting image E more unpredictable at regions where M is low and therefore it is slightly harder to produce a reliable artifact.</li> <li>encourage smoothness of the mask M for example via a total variation (TV) penalty</li> <li>We can also directly resize smaller masks to the required size as resizing can be seen as a smoothness mechanism.</li> </ul>"},{"location":"Real%20Time%20Image%20Saliency%20for%20Black%20Box%20Classifiers/#a-new-saliency-metric","title":"A New Saliency Metric","text":"<ul> <li>In order to make sure that the preserved region is free from adversarial artifacts instead of masking we can crop the image.</li> <li>We propose to find the tightest rectangular crop that contains the entire salient region and to feed that rectangular region to the classifier to directly verify whether it is able to recognise the requested class</li> <li> \\[s(a,p) = log(\\overset{\\sim}a)- log(p)\\] </li> <li>\\(\\overset{\\sim}a = max(a, 0.05)\\)</li> <li>Here a is the area of the rectangular crop as a fraction of the total image size and p is the probability of the requested class returned by the classifier based on the cropped region.</li> <li>The metric is almost a direct translation of the SSR</li> <li>We threshold the area at 0.05 in order to prevent instabilities at low area fractions.</li> <li>Good saliency detectors will be able to significantly reduce the crop size without reducing the classification probability, and therefore a low value for the saliency metric is a characteristic of good saliency detectors.</li> <li>this measure can be seen as the relative amount of information between an indicator variable with probability p and an indicator variable with probability a\u2014or the concentration of information in the cropped region.</li> <li>Because most image classifiers accept only images of a fixed size and the crop can have an arbitrary size, we resize the crop to the required size disregarding aspect ratio</li> </ul>"},{"location":"Real%20Time%20Image%20Saliency%20for%20Black%20Box%20Classifiers/#the-saliency-objective","title":"The Saliency Objective","text":"<ul> <li>want to find a mask M that is smooth and performs well at both [[[[SSR]]]] and SDR</li> <li>given class c of interest, and an input image X, to find a saliency map M for class c, our objective function L is given by</li> <li> \\[L(M) = \\lambda_{1}TV(M) + \\lambda_{2}AV(M) - log(f_{c}(\\Phi(X,M)))+\\lambda_{3}f_{c}(\\Phi(X, 1-M))^{\\lambda_{4}}\\] </li> <li>fc is a softmax probability of the class c of the black box image classifier and TV(M) is the total variation of the mask defined simply as</li> <li> \\[TV(M) = \\Sigma_{i,j}(M_{ij}-M_{ij+1})^{2}+ \\Sigma_{ij}(M_{ij}-M_{i+1j})^{2}\\] </li> <li>AV(M) is the average of the mask elements, taking value between 0 and 1, \\(\\lambda_{i}\\) are regularisers</li> <li>function \\(\\Phi\\) removes the evidence from the image as introduced in the previous section</li> <li> \\[\\Phi(X, M) = X \\odot M + A \\odot (1-M)\\] </li> <li>In total, the objective function is composed of 4 terms. The first term enforces mask smoothness, the second term encourages that the region is small. The third term makes sure that the classifier is able to recognise the selected class from the preserved region. Finally, the last term ensures that the probability of the selected class, after the salient region is removed, is low</li> <li>Setting \\(\\lambda_{4}\\) to a value smaller than 1 (e.g. 0.2) helps reduce this probability to very small values.</li> </ul>"},{"location":"Real%20Time%20Image%20Saliency%20for%20Black%20Box%20Classifiers/#masking-model","title":"Masking Model","text":"<ul> <li>The mask can be found iteratively for a given image-class pair by directly optimising the objective function</li> <li>Unfortunately, iteratively finding the mask is not only very slow, as normally more than 100 iterations are required, but it also causes the mask to greatly overfit to the image and a large TV penalty is needed to prevent adversarial artifacts from forming</li> <li>Therefore, the produced masks are blurry, imprecise, and overfit to the specific</li> <li>image rather than capturing the general behaviour of the classifie</li> <li>develop a trainable masking model that can produce the desired masks in a single forward pass without direct access to the image classifier after training</li> <li>The masking model receives an image and a class selector as inputs and learns to produce masks that minimise our objective function</li> <li>In order to succeed at this task, the model must learn which parts of the input image are considered salient by the black box classifier</li> <li>n theory, the model can still learn to develop adversarial masks that perform well on the objective function, but in practice it is not an easy task, because the model itself acts as some sort of a \"regulariser\" determining which patterns are more likely and which are less.</li> <li>Unet Architecture</li> <li>so that the masking model can use feature maps from multiple resolutions</li> <li>The ResNet-50 model contains feature maps of five different scales, where each subsequent scale block downsamples the input by a factor of two</li> <li>The purpose of the feature filter is to attenuate spatial locations which contents do not correspond to the selected class.</li> <li>Therefore, the feature filter performs the initial localisation, while the following upsampling blocks fine-tune the produced masks</li> <li>The output of the feature filter Y at spatial location i, j is given by:</li> <li> \\[Y_{ij}= X_{ij}\\sigma(X_{ij}^T C_{s})\\] </li> <li>Xij is the output of the Scale 5 block at spatial location i, j; Cs is the embedding of the selected class s and \\(\\sigma(\\cdot)\\) is the sigmoid nonlinearity. Class embedding C can be learned as part of the overall objective.</li> <li>The upsampler blocks take the lower resolution feature map as input and upsample it by a factor of two using transposed convolution</li> <li>afterwards they concatenate the upsampled map with the corresponding feature map from ResNet and follow that with three bottleneck blocks</li> <li>Finally, to the output of the last upsampler block (Upsampler Scale 2) we apply 1x1 convolution to produce a feature map with with just two channels</li> <li>The mask Ms is obtained from</li> <li> \\[M_{s}= \\frac{abs(C_{o})}{abs(C_{o})+ abs(C_{1})}\\] </li> <li>We use this nonstandard nonlinearity because sigmoid and tanh nonlinearities did not optimise properly and the extra degree of freedom from two channels greatly improved training</li> </ul>"},{"location":"Real%20Time%20Image%20Saliency%20for%20Black%20Box%20Classifiers/#training-process","title":"Training Process","text":"<ul> <li>train the masking model to directly minimise the objective function</li> <li>he weights of the pre-trained ResNet encoder (red blocks in figure 4) are kept fixed</li> <li>during the training.</li> <li>sometimes supply a class selector for a fake class and to apply only the area penalty term of the objective function.</li> <li>Under this setting the model must pay attention to the class selector, as the only way it can reduce loss in case of a fake label is by setting the mask to zero</li> <li>During training, we set the probability of the fake label occurrence to 30%</li> <li>One can also greatly speed up the embedding training by ensuring that the maximal value of \\(\\sigma(X_{ij}^{T}C_{s})\\) from equation 7 is high in case of a correct label and low in case of a fake label.</li> <li>evidence removal function \\(\\Phi(X,M)\\)</li> <li>In order to prevent the model from adapting to any single evidence removal scheme the alternative image A is randomly generated every time the function is called</li> <li>In 50% of cases the image A is the blurred version of X (we use a Gaussian blur with = \\(\\sigma=10\\) to achieve a strong blur) and in the remainder of cases, A is set to a random colour image with the addition of a Gaussian noise.</li> <li>Such a random scheme greatly improves the quality of the produced masks as the model can no longer make strong assumptions about the final look of the image.</li> <li>ImageNet</li> <li>three different black-box classifiers: AlexNet [6], GoogLeNet [15] and ResNet-50 [4]</li> <li>These models are treated as black boxes</li> <li>The selected parameters of the objective function are \\(\\lambda_{1} = 10, \\lambda_{2} = 103, \\lambda_{3} = 5, \\lambda_{4} = 0.3\\)</li> <li>The first upsampling block has 768 output channels and with each subsequent upsampling block we reduce the number of channels by a factor of two. We train each masking model as described in section 4.1 on 250,000 images from the ImageNet training set.</li> <li>The masks produced by models trained on GoogLeNet and ResNet are sharp and precise and would produce accurate object segmentations. The saliency model trained on AlexNet produces much stronger and slightly larger saliency regions, possibly because AlexNet is a less powerful model which needs more evidence for successful classification.</li> </ul>"},{"location":"Real%20Time%20Image%20Saliency%20for%20Black%20Box%20Classifiers/#results","title":"Results","text":""},{"location":"Real%20Time%20Image%20Saliency%20for%20Black%20Box%20Classifiers/#future-research","title":"Future Research","text":"<ul> <li>modifying the approach to produce high quality, weakly supervised, image segmentations</li> <li>Moreover, because our model can be run in real-time, it can be used for video saliency detection to instantly explain decisions made by black-box classifiers such as the ones used in autonomous vehicles</li> <li>Lastly, our model might have biases of its own \u2014 a fact which does not seem to influence the model performance in finding biases in other black boxes according to the various metrics we used</li> </ul>"},{"location":"Real%20Time%20Image%20Saliency%20for%20Black%20Box%20Classifiers/#images","title":"Images","text":""},{"location":"Real%20Time%20Image%20Saliency%20for%20Black%20Box%20Classifiers/#backlinks","title":"Backlinks","text":"<ul> <li>Real Time Image Saliency for Black Box Classifiers</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Reasoning%20Component/","title":"Reasoning Component","text":""},{"location":"Reasoning%20Component/#reasoning-component","title":"Reasoning Component","text":"<ul> <li>updates the world model and determines plans to achieve goals.</li> </ul>"},{"location":"Reasoning%20Component/#backlinks","title":"Backlinks","text":"<ul> <li>Reasoning Component</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Recall/","title":"Recall","text":"<ul> <li> \\[\\frac{TP}{TP+FN}\\] </li> <li>Model needs to remember the features such that it does not miss-classify a positive case as a negative one. -&gt; How many positive samples does the model remember?</li> <li># positive classes correctly predicted / total # positive classes</li> </ul>"},{"location":"Receptive%20field/","title":"Receptive Field","text":"<ul> <li>Not for Dense, only local connected layers like Conv, Pooling</li> <li>A neuron's receptive field is the patch of the total field of view that a single neuron has access to</li> <li>Almost a logarithmic relationship between classification accuracy and receptive field size<ul> <li>Large fields are almost necessary for high level recognition tasks, but with diminishing rewards</li> </ul> </li> <li> \\[r(i-1) = s_{i}\\times r_{i} + (k_{i}-s_{i})\\] </li> <li></li> <li>Recursive<ul> <li> \\[r_{0} = \\Sigma^{L}_{i=1}((k_{i}-1)\\Pi_{j=1}^{l-1}s_{j})+1\\] </li> </ul> </li> <li>How to increase receptive field<ul> <li>Add more Conv</li> <li>Add pooling or higher stride</li> <li>Causal Dilated Conv</li> <li>Depthwise Separable</li> </ul> </li> </ul>"},{"location":"Recessive/","title":"Recessive","text":"<ul> <li>A genetic trait or disease that appears only in patients who have received two copies of a mutant gene, one from each parent.</li> </ul>"},{"location":"Recommender%20System/","title":"Recommender System","text":"<ul> <li>Group Modeling Approach</li> <li>Individual Modeling</li> <li>Collaborative Recommender</li> <li>Content Based Recommender</li> </ul>"},{"location":"Recurrent/","title":"Recurrent","text":"<ul> <li>Sequences as inputs/outputs</li> <li>Sequential processing</li> <li>Turing complete</li> <li>memory through state persisted between timesteps<ul> <li>operation invariant to the sequence</li> <li>reduces no of params needed</li> </ul> </li> <li>Output comes back as input<ul> <li></li> </ul> </li> <li>variable sized inputs and outputs : encoder decoder</li> <li>Three weight matrices and two bias vectors.</li> <li> \\[h_t = \\sigma_h(W_{hh}h_{t-1} + W_{xh}x_t + b_h)\\] </li> <li> \\[y_t = \\sigma_y(W_{hy}h_t + b_y)\\] </li> <li>Stateful : hidden state kept across batches of inputs</li> <li>Activation usually Sigmoid or Tanh</li> <li>BPTT<ul> <li></li> <li> </li> </ul> </li> <li>Training stuff<ul> <li>Softmax but on every output vector simultaneously<ul> <li>If  is lower (eg between 0 and 0.5). It becomes more confident and hence more conservative</li> <li>Near 0 is very diverse and less confident</li> </ul> </li> <li>Feed a char into the RNN -&gt; distribution over characters that comes next -&gt; Sample from it -&gt; Feed it back</li> </ul> </li> <li>Some basic patterns from here<ul> <li>The model first discovers the general word-space structure and then rapidly starts to learn the words.</li> <li>First starting with the short words and then eventually the longer ones.</li> <li>Topics and themes that span multiple words (and in general longer-term dependencies) start to emerge only much later.</li> </ul> </li> </ul>"},{"location":"Recurrent/#gradients","title":"gradients","text":"<ul> <li>If eigen decomposition \\(\\(W = Q\\wedge^tQ\\)\\), then \\(\\(h_t = Q^T\\wedge^tQ\\)\\)</li> <li>If less than 0 then will converge to 0 or if bigger then will explore to infinity -&gt; long sequences</li> <li>Element wise clipping #tricks<ul> <li>Clip if bigger than value</li> </ul> </li> <li>Norm clipping<ul> <li>Clip if $\\(||g|| &gt;v\\) set \\(g = \\frac{gv}{||g||}\\)</li> <li>v can be decided by trial and error</li> </ul> </li> </ul>"},{"location":"Reflex%20Hammer/","title":"Reflex Hammer","text":"<ul> <li>A specially designed hammer used to test deep tendon or motor reflexes</li> </ul>"},{"location":"RegNet/","title":"RegNet","text":"<ul> <li>network design paradigm</li> <li>discover design principles that generalize across settings</li> <li>arrives at a low-dimensional design space consisting of simple, regular networks</li> <li>widths and depths of good networks can be explained by a quantized linear function</li> <li>outperforms EfficientNet</li> </ul>"},{"location":"Region%20Growing/","title":"Region Growing","text":"<ul> <li>Automatic Segmentation</li> <li>Requires seed point</li> <li>Leakage through holes in Contour</li> <li></li> </ul>"},{"location":"Region%20Proposal/","title":"Region Proposal","text":"<ul> <li>region proposal generation that shares full-image convolutional features with the detection network,</li> <li>nearly cost-free region proposals</li> <li>haring convolutional features with the down-stream detection network</li> <li>simultaneously predicts object bounds and objectness scores at each position</li> </ul>"},{"location":"Register%20Renaming/","title":"Register Renaming","text":"<ul> <li>used to avoid unnecessary serialization of program operations caused by the reuse of registers by those operations, in order to enable out-of-order execution</li> <li></li> </ul>"},{"location":"Register%20to%20Register%20Architecture/","title":"Register to Register Architecture","text":"<ul> <li>All vector operations occur between vector registers</li> <li>If necessary, operands are fetched from main memory into a set of vector registers (load-store unit)</li> <li>SIMD based on this</li> <li></li> </ul>"},{"location":"Regularization%20Rate/","title":"Regularization Rate","text":"<ul> <li>A scalar value, represented as lambda, specifying the relative importance of the regularization function.</li> <li>Raising the regularization rate reduces overfitting but may make the model less accurate.</li> <li> </li> <li>The final stage of a recommendation system, during which scored items may be re-graded according to some other (typically, non-ML) algorithm. Re-ranking evaluates the list of items generated by the scoring phase, taking actions such as<ul> <li>Eliminating items that the user has already purchased.</li> <li>Boosting the score of fresher items.</li> </ul> </li> </ul>"},{"location":"Regularization%20Rate/#re-ranking","title":"re-ranking","text":""},{"location":"Regularization%20Term/","title":"Term","text":"<ul> <li>Penalty term</li> <li>Cost function that penalizes model params \\(\\theta\\) with a high degree of flexibility</li> <li> \\[h_{opt} = argmin_{h \\in \\mathcal{H}} \\frac{1}{N}\\Sigma_{i=1}^N L(h(x_i), y_i)+ \\alpha^{2}R(\\theta_h)\\] </li> <li>\\(\\alpha^2\\) determines how much regularizer affects the model<ul> <li>Larger : soft models</li> <li>Increasing -&gt; Down regulating flexibility</li> <li>0 = overfitting and unregularized risk</li> <li>\\(\\infty\\) does not care about training data at all. Only cares about minimal penalty<ul> <li>Dead model</li> </ul> </li> </ul> </li> </ul>"},{"location":"Regularization%20Term/#types","title":"Types","text":"<ul> <li>Lp Regularization for p =2<ul> <li>Soft models</li> <li>Squared sum of model params</li> </ul> </li> </ul>"},{"location":"Regularization/","title":"Regularization","text":"<ul> <li>Regularization Term</li> <li>Dropout</li> <li>VariationalRecurrent Dropout</li> <li>Batch Normalization</li> <li>Layer Normalization</li> <li>Augmentation</li> <li>Lp Regularization</li> <li>Pruning</li> <li>Effects of Regularization</li> </ul>"},{"location":"Reinforcement%20Learning/","title":"Reinforcement Learning","text":""},{"location":"Reinforcement%20Learning/#anchor","title":"anchor","text":""},{"location":"Rejection%20Sampling/","title":"Rejection Sampling","text":"<ul> <li>Also called Best-of-n sampling</li> <li>No CDF with a simple inverse</li> <li>Importance sampling</li> <li>Use a simpler distribution which is somewhat related to the target PDF<ul> <li>Sample by transformation</li> </ul> </li> <li>Now if we can take a Proto PDF \\(g_{0} \\geq f\\) where we can sample from the PDF g</li> <li>Take a point from g with Probability \\(f(\\tilde x)/g_{0}(\\tilde x)\\)<ul> <li>Either accept or reject if satisfies Probability</li> <li>If accepted then return the sample</li> </ul> </li> <li>Drop a point from \\(g_{0}(x)\\) with that Probability</li> <li>Depends on how close g is to f of course</li> <li><ul> <li>If the ratio \\(\\frac{f}{g_{0}}\\) is small. (aka f is bigger), then there are many rejections and the algo will be slow. Impossible to not do in high dim spaces</li> </ul> </li> </ul>"},{"location":"Relational%20Inductive%20Bias/","title":"Relational Inductive Bias","text":"<ul> <li>Weak Relation Bias</li> <li>Locality</li> <li>Sequential Relation Bias</li> <li>Arbitrary Relation Bias</li> </ul>"},{"location":"Relative%20Multi%20Head%20Self%20Attention/","title":"Relative Multi Head Self Attention","text":"<ul> <li>ZihangDai et al., 2019</li> </ul>"},{"location":"Relevance%20Account/","title":"Relevance Account","text":"<ul> <li>(Philip 2011)</li> <li>pragmatic extension of the theory of Drozd and Loosbroek (2006)</li> <li>Philip claims that the problem that children have to solve when assigning a domain to a universal quantifier is which objects in the context should be taken as relevant.</li> <li>Adults rely on their world knowledge in identifying the presupposed set</li> <li>As formulated in the Normal World Constraint, \"if an object is contextually relevant, then there is a normal situation that it is part of.\"</li> </ul>"},{"location":"Relevance%20Theory/","title":"Relevance Theory","text":"<ul> <li>Relevance Theory is built on the assumption that attention and thought processes automatically turn toward information that seems relevant, capable of yielding cognitive effects</li> <li>As the Principle of Relevance states, communicated information comes with a guarantee of relevance.</li> <li>The higher the cognitive effect of the information, and/or the more economically it is communicated, the greater its relevance.</li> </ul>"},{"location":"Relu/","title":"Relu","text":"<ul> <li> \\[ReLU(x) = max(0,x)\\] </li> <li> \\[\\frac{d}{d_x}ReLU(X) = \\begin{cases}0 &amp; x \\geq 0 \\\\ 1 &amp; otherwise \\end{cases}\\] </li> <li>He init</li> <li>MLP, CNN : Hidden</li> <li></li> <li>Leaky Relu</li> <li>PRelu</li> <li>Noisy Relu</li> </ul>"},{"location":"Remission/","title":"Remission","text":"<ul> <li>Describes a disease that is not getting worse</li> </ul>"},{"location":"RepLKNet/","title":"RepLKNet","text":"<ul> <li>impressively manages to scale the kernel size to 31\u00d731 with improved performance</li> <li>the performance starts to saturate as the kernel size continues growing, compared to the scaling trend of ad- vanced ViTs such as Swin Transformer</li> <li>explore the possibility of training extreme convolutions larger than 31\u00d731 and test whether the performance gap can be eliminated by strategically enlarging convolutions.</li> </ul>"},{"location":"RepVGG/","title":"RepVGG","text":"<p>title: RepVGG</p> <p>tags: architecture</p>"},{"location":"RepVGG/#repvgg","title":"RepVGG","text":"<ul> <li>RepVGG: Making Vgg-style ConvNets Great Again<ul> <li>stack of \\(3\\times3\\) Conv and Relu during inference time</li> <li>training-time model has a multi-branch topology</li> <li>decoupling of the training-time and inference-time architecture is realized by a structural re-parameterization technique</li> <li>5 stages and conducts down-Sampling via stride-2 convolution at the beginning of a stage</li> <li>identity and 1 \\times 1 branches, but only for training</li> <li>ImageNet</li> <li>higher accuracy and show favorable accuracy-speed trade-off compared to the state-of-the-art models like EfficientNet and RegNet</li> <li></li> </ul> </li> </ul>"},{"location":"Res%20Net%20D/","title":"Res Net D","text":""},{"location":"Res%20Net/","title":"Res Net","text":"<ul> <li>Deep Residual Learning for Image Recognition</li> <li>Deeper Networks have Issues because of vanishing #gradients</li> <li>Propagate gradients forward for deeper networks</li> <li>Skip connections</li> <li>output of F(x) has the same dims as x -&gt; add</li> <li>If only spatial dims match (aka not channels) -&gt; concat</li> <li>less params than VGG</li> <li>Skip Connection</li> <li>Sadly, one of the creators Jian Sun passed away yesterday. (16-6-22)</li> </ul> <pre><code>def make_layer(inplanes, planes, block, n_blocks, stride=1):\n    downsample = None\n    if stride != 1 or inplanes != planes * block.expansion:\n        # output size won't match input, so adjust residual\n        downsample = nn.Sequential(\n            nn.Conv2d(inplanes, planes * block.expansion,\n                      kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n    return nn.Sequential(\n        block(inplanes, planes, stride, downsample),\n        *[block(planes * block.expansion, planes) for _ in range(1, n_blocks)]\n    )\n\ndef ResNetNew(block, layers, num_classes=1000):    \n    e = block.expansion\n\n    resnet = nn.Sequential(\n        Rearrange('b c h w -&gt; b c h w', c=3, h=224, w=224),\n        nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n        nn.BatchNorm2d(64),\n        nn.ReLU(inplace=True),\n        nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n        make_layer(64,      64,  block, layers[0], stride=1),\n        make_layer(64 * e,  128, block, layers[1], stride=2),\n        make_layer(128 * e, 256, block, layers[2], stride=2),\n        make_layer(256 * e, 512, block, layers[3], stride=2),\n        # combined AvgPool and view in one averaging operation\n        Reduce('b c h w -&gt; b c', 'mean'),\n        nn.Linear(512 * e, num_classes),\n    )\n\n    # [initialization](./Initialization.md)\n    for m in resnet.modules():\n        if isinstance(m, nn.Conv2d):\n            n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n            m.weight.data.normal_(0, math.sqrt(2. / n))\n        elif isinstance(m, nn.BatchNorm2d):\n            m.weight.data.fill_(1)\n            m.bias.data.zero_()\n    return resnet\n</code></pre>"},{"location":"Rescorla-Wagner%20Algorithm/","title":"Rescorla-Wagner Algorithm","text":"<ul> <li>Rescorla &amp; Wagner (1972): animals and humans also learn associations by paying attention to what is not associated.</li> <li> \\[\\Delta V = \\alpha \\beta_{1}\\beta_{2}(\\lambda = \\Sigma V)\\] </li> <li>\u25b6 V = association strength \u25b6 \u2206 V : Change in association strength \u25b6 \u03bb = maximum values of the unconditional stimulus \u25b6 Set to 1: when US is present (food) \u25b6 Set to 0: when not present \u25b6 \u03b1 = learning rate \u25b6 \u03b2 = varies the effects of negative or positive evidence \u25b6 \u03a3V = sum of associated strengths for all cues/features/conditions stimuli</li> <li>negative instances are also useful to learning</li> <li>Logical Problem of Lang Acquisition</li> <li>Children don't get negative evidence = must be innate</li> <li>Cross-situational learning</li> <li>Propose-but-verify</li> <li>Rescorla-Wagner Blocking</li> <li>Rescorla-Wagner = error-driven</li> <li>After a strong association is made, as long as it is confirmed by data, no new learning will occur</li> <li>The model only learns when the predicted outcome differs from actual outcome</li> </ul>"},{"location":"Rescorla-Wagner%20Blocking/","title":"Rescorla-Wagner Blocking","text":"<ul> <li>If an outcome is already strongly associated with a stimuli (cue or feature), the presence of other features, regardless of how consistent they are, will not lead to a new association</li> </ul>"},{"location":"Research%20Debt/","title":"Research Debt","text":"<ul> <li>Research Debt</li> <li>Achieving a research-level understanding of most topics is like climbing a mountain.</li> <li>Aspiring researchers must struggle to understand vast bodies of work that came before them, to learn techniques, and to gain intuition</li> <li>Upon reaching the top, the new researcher begins doing novel work, throwing new stones onto the top of the mountain and making it a little taller for whoever comes next.</li> <li>People expect the climb to be hard</li> <li>It reflects the tremendous progress and cumulative effort that\u2019s gone into mathematics</li> <li>The climb is seen as an intellectual pilgrimage, the labor a rite of passage</li> <li>But the climb could be massively easier</li> <li>It\u2019s entirely possible to build paths and staircases into these mountains.</li> <li>That is, really outstanding tutorials, reviews, textbooks, and so on.</li> <li>technical debt</li> <li>institutional debt</li> <li>Poor Exposition</li> <li>Interpretive Labor</li> <li>Clear Thinking</li> <li>Research Distillation</li> </ul>"},{"location":"Research%20Distillation/","title":"Research Distillation","text":"<ul> <li>It can be incredibly satisfying, combining deep scientific understanding, empathy, and design to do justice to our research and lay bare beautiful insights.</li> <li>Distillation is also hard</li> <li>It\u2019s tempting to think of explaining an idea as just putting a layer of polish on it, but good explanations often involve transforming the idea.</li> <li>This kind of refinement of an idea can take just as much effort and deep understanding as the initial discovery.</li> <li>We can\u2019t solve research debt by having one person write a textbook</li> </ul>"},{"location":"Research%20Intimacy/","title":"Research Intimacy","text":"<ul> <li>Internalizing obscure knowledge, equations, relationships, and ways of thinking related to a research topic.</li> <li>link</li> </ul>"},{"location":"Resistance/","title":"Resistance","text":"<ul> <li>resistance =\u00a0resistivity x length\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 cross sectional area</li> <li> \\[R = r/A\\] </li> </ul>"},{"location":"ResizeMix/","title":"ResizeMix","text":"<ul> <li>performs random image cropping and pasting</li> <li>ResizeMix solves the random region cropping problem that misallocates the output image label in certain cases where the pasted region does not contain any object informa- tion</li> <li>scaling down (scale rate is sampled from a uniform dis- tribution) the selected image completely and past- ing it randomly on the target image</li> <li>modified labels of the output image are always accurate and proportionate to the mixing.</li> </ul>"},{"location":"Response%20Based%20Knowledge/","title":"Response Based Knowledge","text":"<ul> <li>neural response of the last output layer of the teacher model. The main idea is to directly mimic the final prediction of the teacher model.</li> <li>Given a vector of logits z as the outputs of the last fully connected layer of a deep model, the Distillation Loss for response-based knowledge can be formulated as</li> <li>response in object detection task may contain the logits together with the offset of a bounding box (Chen et al., 2017). In semantic landmark localization tasks, e.g., human pose estimation, the response of the teacher model may include a heatmap for each landmark (Zhang et al., 2019a)</li> </ul>"},{"location":"Restricted%20Boltzmann%20Machine/","title":"Restricted Boltzmann Machine","text":"<ul> <li>Start of Deep learning</li> </ul>"},{"location":"RetinaNet/","title":"RetinaNet","text":"<ul> <li>Simple dense detector for Focal Loss</li> </ul>"},{"location":"Reuptake/","title":"Reuptake","text":"<ul> <li>A process by which released neurotransmitters are absorbed for subsequent re-use.</li> </ul>"},{"location":"Revolute%20Joint/","title":"Revolute Joint","text":"<ul> <li>The joints of a robot, which are capable of rotary motion.</li> </ul>"},{"location":"Ridge%20Regression/","title":"Ridge Regression","text":"<ul> <li>LinearRegression</li> <li>\\(\\((XX')\\)\\) suffers from numerical instability when almost singular (real world data. sparse I think)</li> <li>Add a tiny term<ul> <li> \\[w'_{opt} = (XX' + \\alpha ^2 I_{nxn})^{-1}XY\\] </li> <li>Solution to overfitting</li> </ul> </li> </ul>"},{"location":"Risk%20Mitigation/","title":"Risk Mitigation","text":"<ul> <li>A secondary step in the risk assessment process that involves reducing the level of risk for the identified tasks, by applying risk reduction measures in order to eliminate or mitigate the hazards.</li> </ul>"},{"location":"Rmsprop/","title":"Rmsprop","text":"<ul> <li>RL</li> <li>More stable than Adagrad</li> <li>Moving exponential avg : older grads given less weight</li> <li>$$\\begin{align}\\</li> </ul> <p>&amp; E[g^{2}]{t}= 0.9E[g^{2}]{t-1}+ 0.1g^{2}_{t}\\</p> <p>&amp; \\theta_{t+1}= \\theta_{t}- \\frac{\\eta}{\\sqrt{E[g^{2}]){t}+\\epsilon}}g{t}</p> <p>\\end{align}</p> <p>$$ - Suggested \\(\\gamma=0.9\\) and \\(\\eta= 0.001\\)</p>"},{"location":"RoBERTa/","title":"RoBERTa","text":"<ul> <li>RoBERTa: a Robustly Optimized BERT Pretraining Approach</li> <li>evaluates a number of design decisions when pretraining BERT models</li> <li>They find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it.</li> <li>performance can be substantially improved by training the model longer, with bigger batches over more data; removing the next sentence prediction objective; training on longer sequences; and dynamically changing the masking pattern applied to the training data</li> <li>GLUE</li> <li>RACE</li> <li>SQuAD</li> <li>only the masked language model objective</li> </ul>"},{"location":"Robot%20Range%20Limit%20Monitoring/","title":"Robot Range Limit Monitoring","text":"<ul> <li>Monitors the manipulator arm or its tool to be in the designated safety area</li> </ul>"},{"location":"Robotic%20Joints/","title":"Robotic Joints","text":"<ul> <li>Joints connect the manipulator links</li> <li>A joint normally provides one Controllable Degree of Freedom (CDOF)</li> <li>For each CDOF one separate actuator is needed</li> <li>An endeffector with many (C)DOFs needs a lot of actuators!</li> <li>Rotary Joint , Prismatic Joint</li> </ul>"},{"location":"Robust%20RegNet/","title":"Robust RegNet","text":"<p>title: Robust RegNet</p> <p>tags: architecture</p>"},{"location":"Robust%20RegNet/#robust-regnet","title":"Robust RegNet","text":"<ul> <li>Vision Models are More Robust and Fair When Pretrained on Uncurated Images Without Supervision</li> <li>Unsupervised Learning</li> <li>Discriminative Self Supervised learning allows training models on any random group of internet images, and possibly recover salient information that helps differentiate between the images</li> <li>ImageNet</li> <li>object-centric features that perform on par with supervised features on most object-centric downstream tasks</li> <li>learn any salient and more representative information present in diverse unbounded set of images from across the globe</li> <li>without any data pre-processing or prior assumptions about what we want the model to learn</li> <li>RegNet</li> <li>scaled to a dense 10 billion parameters</li> <li>pre-trained using the SwaV self-supervised method on a large collection of 1 billion randomly selected public images from Instagram with a diversity of gender, ethnicity, cultures, and locations</li> <li>captures well semantic information</li> <li>captures information about artistic style and learns salient information such as geo-locations and multilingual word embeddings based on visual content only.</li> <li>large-scale self-supervised pre-training yields more robust, fair, less harmful, and less biased results than supervised models or models trained on object centric datasets such as ImageNet</li> </ul>"},{"location":"Rod/","title":"Rod","text":"<ul> <li>A type of photoreceptor, usually found on the outer edges of the retina, that helps facilitate peripheral vision.</li> </ul>"},{"location":"Roll/","title":"Roll","text":"<ul> <li>Rotation of the robot end effector in a plane perpendicular to the end of the manipulator arm.</li> </ul>"},{"location":"Rotary%20Joint/","title":"Rotary Joint","text":""},{"location":"Rotary%20Vector%20Drive%20%28RV%29/","title":"Rotary Vector Drive (RV)","text":"<ul> <li>A brand name for a speed reduction device that converts high speed low torque to low speed high torque, usually used on the major (larger) axis . See Cyclo Drive and Harmonic Drive.</li> </ul>"},{"location":"Rotational%20Invariance/","title":"Rotational Invariance","text":"<ul> <li>In an image classification problem, an algorithm's ability to successfully classify images even when the orientation of the image changes. For example, the algorithm can still identify a tennis racket whether it is pointing up, sideways, or down. Note that rotational invariance is not always desirable; for example, an upside-down 9 should not be classified as a 9.</li> </ul>"},{"location":"Runge%20Kutta/","title":"Runge Kutta","text":"<ul> <li>Fourth Order</li> <li></li> <li></li> </ul>"},{"location":"S2ST/","title":"S2ST","text":"<ul> <li>Direct Speech-to-speech Translation with Discrete Units</li> <li>direct speech-to-speech translation (S2ST) model that translates speech from one language to speech in another language without relying on intermediate text generation</li> <li>self-supervised discrete speech encoder on the target speech</li> <li>training a sequence-to-sequence speech-to-unit translation</li> <li>model to predict the discrete representations of the target speech</li> <li>When target text transcripts are available, they design a joint speech and text training framework that enables the model to generate dual modality output (speech and text) simultaneously in the same inference pass</li> <li>Fisher Spanish-English</li> </ul>"},{"location":"SAM-ResNet/","title":"SAM-ResNet","text":"<ul> <li>uses LSTM to compute an attention map. The feature map extracted by ResNet is input to attentive convolutional LSTM that focuses on the most salient regions of the input image to iteratively refine the predicted saliency map.</li> </ul>"},{"location":"SBU%20Captions/","title":"SBU Captions","text":""},{"location":"SDR/","title":"SDR","text":"<ul> <li>Smallest destroying region</li> <li>smallest region of the image that when removed, prevents a confident classification.</li> </ul>"},{"location":"SDR/#backlinks","title":"Backlinks","text":"<ul> <li>Real Time Image Saliency for Black Box Classifiers</li> <li>SDR</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"SELU/","title":"SELU","text":"<ul> <li> \\[selu(x) = \\lambda x , \\text{if } x &gt;0 , \\text{else }, \\alpha(e^{x}-1)\\] </li> </ul>"},{"location":"SELU/#information","title":"Information","text":"<ul> <li>Paper:\u00a0https://arxiv.org/pdf/1706.02515.pdf</li> <li>scaled variant of the\u00a0Elu\u00a0function</li> <li>does internal normalization (\"self-normalizing\")<ul> <li>each layer preserves the mean and the variance from the previous one</li> <li>normalization happens within the activation function</li> <li>to work:<ul> <li>input features must be standardized</li> <li>architecture must be sequential<ul> <li>self-normalizing not guaranteed otherwise</li> </ul> </li> <li>SELU as activation\u00a0</li> <li>custom\u00a0Initialization<ul> <li>zero mean</li> <li>standard deviation:\u00a0\\(\\sqrt{\\frac{1}{ \\#inputs}}\\)</li> </ul> </li> <li>if all layers are dense (in paper), but other research showed that it also works for CNNs</li> </ul> </li> </ul> </li> <li>has two fixed parameters\u00a0\u03b1\u00a0and\u00a0\u03bb<ul> <li>not hyperparameters nor learnt parameters</li> <li>derived from the inputs (\u03bc=0,\u00a0std=1)</li> <li>\u03b1\u22481.6732,\u00a0\u03bb\u22481.0507</li> </ul> </li> <li>Pros:<ul> <li>no\u00a0Vanishingexploding gradients</li> <li>cannot die as\u00a0Relu</li> <li>converges faster and to a better result than other activation functions</li> <li>significantly outperformed other activation functions for deep networks</li> </ul> </li> <li>Cons:<ul> <li>Computational heavier</li> </ul> </li> </ul>"},{"location":"SGD%20Momentum/","title":"SGD Momentum","text":"<ul> <li>$$\\begin{align}</li> </ul> <p>&amp;v_{t}= \\gamma v_{t+1}+\\eta \\cdot \\nabla_{\\theta}J(\\theta) \\</p> <p>&amp;\\theta = \\theta- v_{t}\\</p> <p>\\end{align}$$</p>"},{"location":"SGD/","title":"SGD","text":"<ul> <li>instead of taking the whole dataset for each iteration, we randomly select the batches of data</li> <li>The procedure is first to select the initial parameters w and learning rate n. Then randomly shuffle the data at each iteration to reach an approximate minimum.</li> <li>full of noise</li> <li>Due to an increase in the number of iterations, the overall computation time increases.</li> <li> \\[\\theta = \\theta - \\eta \\cdot \\nabla_{\\theta} J(\\theta ; x^{i}; y^{i} )\\] <ul> <li>For each example \\(x^{i}\\) and label \\(y^{i}\\)</li> </ul> </li> </ul>"},{"location":"SHAP/","title":"SHAP","text":"<ul> <li>A Unified Approach to Interpreting Model Predictions</li> <li>help users interpret the predictions of complex models</li> <li>unclear how these methods are related and when one method is preferable over another</li> <li>unified framework for interpreting predictions</li> <li>SHAP</li> <li>SHapley Additive exPlanations</li> <li>game theoretic approach to explain the output of any machine learning model</li> <li>connects optimal credit allocation with local explanations using the classic Shapley values from game theory and their related extensions</li> <li>assigns each feature an importance value for a particular prediction</li> <li>identification of a new class of additive feature importance measures</li> <li>theoretical results showing there is a unique solution in this class with a set of desirable properties</li> <li>notable because several recent methods in the class lack the proposed desirable properties</li> <li>present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches</li> </ul>"},{"location":"SIMD/","title":"SIMD","text":"<ul> <li>Single instruction on multiple data</li> <li>Graphics, Image processing</li> <li>Synchronous, Deterministic</li> <li>GPU</li> <li></li> <li>Vector Processor</li> <li>Limited by Amdahl's Law</li> <li>More energy efficient than MIMD</li> <li>Time space duality</li> </ul>"},{"location":"SISD/","title":"SISD","text":"<ul> <li>Single Instruction, Single Data</li> <li>Deterministic</li> <li>-</li> </ul>"},{"location":"SLAK/","title":"SLAK","text":"<ul> <li>MORE CONVNETS IN THE 2020S: SCALING UP KERNELS BEYOND 51 \u00d7 51 USING SPARSITY</li> <li>Shiwei Liu, Tianlong Chen, Xiaohan Chen, Xuxi Chen, Qiao Xiao, Boqian Wu, Tommi Ka \u0308 rkka \u0308 inen, Mykola Pechenizkiy, Decebal Constantin Mocanu, Zhangyang Wang</li> </ul> <ul> <li>The dominant role of convolutional neural networks (CNNs) seems to be challenged by increasingly effective transformer-based models.</li> <li>advanced convolutional models strike back with large ker- nels motivated by the local-window attention mechanism, showing appealing perfor- mance and efficiency</li> <li>RepLKNet</li> <li>This study ends up with a recipe for applying extremely large kernels from the perspective of sparsity, which can smoothly scale up kernels to 61\u00d761 with better performance</li> <li>Built on this recipe, we propose Sparse Large Kernel Network (SLaK), a pure CNN architec- ture equipped with sparse factorized 51\u00d751 kernels that can perform on par with or better than state-of-the-art hierarchical Transformers and modern ConvNet architec- tures like ConvNeXt and RepLKNet</li> </ul>"},{"location":"SLAK/#related-work","title":"RELATED WORK","text":"<ul> <li>Large Kernel in Attention</li> <li>Large Kernel in Convolution</li> <li>Dynamic Sparsity</li> <li>Sparse Evolutionary Training</li> </ul>"},{"location":"SLAK/#failures-of-existing-approaches-to-go-beyond-31x31-kernels","title":"FAILURES OF EXISTING APPROACHES TO GO BEYOND 31x31 KERNELS","text":"<ul> <li>It is important to note that all models are trained for a reduced length of 120 epochs in this section, just to sketch the scaling trends of large kernel sizes.</li> <li>Following the design in RepLKNet, we set the kernel size of each stage as [51, 49, 47, 13] and [61, 59, 57, 13],</li> <li>naively enlarging kernel size from 7x7 to 31x31 decreases the performance although the receptive field may be enlarged by using extremely large kernels, it</li> <li>might fail to maintain the desirable property of locality.</li> <li>Since the stem cell in standard ResNet (He et al., 2016) and ConvNeXt results in a</li> <li>4x downsampling of the input images, extreme kernels with</li> </ul>"},{"location":"SLAK/#a-recipe-for-extremely-large-kernels-beyond-31x31","title":"A RECIPE FOR EXTREMELY LARGE KERNELS BEYOND 31x31","text":"<ul> <li>Decomposing a large kernel into two rectangular, parallel kernels smoothly scales the kernel size up to 61x61</li> <li>Although using convolutions with medium sizes (e.g., 31x31) seemingly can directly avoid this problem, we want to investigate if we can further push the performance of CNNs by using (global) extreme convolutions</li> <li>approximate the large MxM kernel with a combination of two parallel and rectangular convolutions whose kernel size is MxN and NxM (where N &lt; M), respectively, as shown in Figure 1. Following Ding et al. (2022), we keep a 5x5 layer parallel to the large kernels and summed up their outputs after a batch norm layer.</li> <li>This decomposition balances between capturing long-range dependencies and extracting local detail features</li> <li>In stark contrast, the overhead of our method increases just linearly with the kernel size</li> <li>As the decomposition reduces learnable parameters and FLOPs, it is no surprise to observe our network to initially sacrifice accuracy slightly compared to the original RepLKNet at medium kernel sizes i.e. 31x31</li> <li>However, as the convolution size continues to increase, our method can scale kernel size up to 61x61 with improved performance.</li> </ul>"},{"location":"SLAK/#use-sparse-groups-expand-more-width","title":"Use Sparse Groups, Expand More Width","text":"<ul> <li>significantly boosts the model capacity.</li> <li>Instead of using the standard group convolution, ConvNeXt simply employs depthwise convolutions with an increased width to achieve the goal of \"use more groups, expand width\". In this paper, we attempt to extend this principle from a sparsity-inspired perspective \u2013 \"use sparse groups, expand more width\".</li> <li>replace the dense convolutions with sparse convolutions, where the sparse kernels are randomly constructed based on the layer-wise sparsity ratio of SNIP (Lee et al., 2019)</li> <li>After construction, we train the sparse model with dynamic sparsity (Mocanu et al., 2018; Liu et al., 2021b), where the sparse weights are dynamically adapted during training by pruning the weights with the lowest magnitude and growing the same number of weights randomly.</li> <li>Doing so enables dynamic adaptation of sparse weights, leading to better local features.</li> <li>As kernels are sparse throughout training, the corresponding parameter count and training/inference FLOPs are only proportional to the dense models.</li> <li>dynamic sparsity notably reduces more than 2.0 GFLOPs, despite causing temporary performance degradation.</li> <li>Dynamic sparsity allows us to computation-friendly scale the model size up</li> <li>For example, using the same sparsity (40%), we can expand the model width by 1.3x while keeping the parameter count and FLOPs roughly the same as the dense model</li> </ul>"},{"location":"SLAK/#large-kernels-generalize-better-than-small-kernels-with-our-recipe","title":"Large Kernels Generalize Better Than Small Kernels with Our Recipe","text":"<ul> <li>performance consistently increases with kernel size, up to 51x51</li> <li>Applying each part of our proposed recipe to 7x7 kernels leads to either no gain or marginal gains compared to our 51x51 kernels. This break-down experiment justifies our claim: large kernel is the root of power, and our proposed recipe helps unleash such power from large kernels.</li> </ul>"},{"location":"SLAK/#slak_1","title":"SLAK","text":"<ul> <li>SLaK is built based on the architecture of ConvNeXt</li> <li>The design of the stage compute ratio and the stem cell are inherited from ConvNeXt</li> <li>The number of blocks in each stage is [3, 3, 9, 3] for SLaK-T and [3, 3, 27, 3] for SLaK-S/B</li> <li>The stem cell is simply a convolution layer with 4x4 kernels and 4 strides. Page 6</li> <li>We first directly increase the kernel size of ConvNeXt to [51, 49, 47, 13] for each stage, and replace each MxM kernel with a combination of Mx5 and 5xM kernels</li> <li>We find that adding a BatchNorm layer directly after each decomposed kernel is crucial before summing the output up</li> <li>urther sparsify the whole network and expand the width of stages by 1.3x, ending up with SLaK</li> </ul>"},{"location":"SLAK/#evaluation-of-slak-imagenet-1k","title":"EVALUATION OF SLAK ImageNet-1K","text":"<ul> <li>ADE20K</li> <li>PASCAL VOC 2007</li> <li>COCO</li> <li>SLaK is not only able to capture long-range dependence but also the local context features.</li> <li>In comparison, high-contribution pixels of SLaK spread in a much larger ERF, and some high-contribution pixels emerge in non-center areas.</li> <li>SLaK balances between capturing long-range dependencies and focusing on the local details.</li> <li>SLaK seems to automatically recover the inductive bias of peripheral vision (Lettvin et al., 1976; Min et al., 2022) in the human vision system: the entire visual field is partitioned into multiple regions from near the gaze center to distant areas; humans have high- resolution processing near the gaze center (central and para-central regions), and decrease the resolution of processing for mid and far peripheral regions.</li> </ul>"},{"location":"SLAK/#kernel-scaling-efficiency","title":"KERNEL SCALING EFFICIENCY","text":"<ul> <li>We simply replace all the kernels in stages of ConvNeXt-T with a set of kernel sizes from 7 to 151 and report the required GFLOPs and the number of parameters</li> <li>One can clearly see the big gap between full-kernel scaling (yellow lines) and kernel decomposition (green lines) as the kernel size increases beyond 31x31.</li> <li>Even using the ultra-large 151x151 kernels, using our methods would require fewer FLOPs and parameters, compared to full-kernel scaling with 51x51 kernel</li> <li>EFFECTIVE RECEPTIVE FIELD (ERF)</li> </ul>"},{"location":"SLAK/#experiment","title":"Experiment","text":""},{"location":"SLAK/#settings-imagenet-1k","title":"SETTINGS IMAGENET-1K","text":"<ul> <li>We share the (pre-)training settings of SLaK on ImageNet-1K in this section. We train SLaK for 300 epochs (Section 5.1) and 120 epochs (Section 4) using AdamW (Loshchilov &amp; Hutter, 2019) with a batch size of 4096, and a weight decay of 0.05. The only differnce between models training for 300 epochs and 120 epochs is the training time. The learning rate is 4e-3 with a 20-epoch linear warmup followed by a cosine decaying schedule. For data augmentation, we use the default setting of RandAugment (Cubuk et al., 2020) in Timm (Wightman, 2019) \u2013 \"rand-m9-mstd0.5- inc1\", Label Smoothing (Szegedy et al., 2016) coefficient of 0.1, Mixup (Zhang et al., 2017) with \u21b5 = 0.8, Cutmix (Yun et al., 2019) with \u21b5 = 1.0, Random Erasing (Zhong et al., 2020) with p = 0.25, Stochastic Depth with drop rate of 0.1 for SLaK-T, 0.4 for SLaK-S, and 0.5 for SLaK-B, Layer Scale (Touvron et al., 2021c) of initial value of 1e- 6, and EMA with a decay factor of 0.9999. We train SLaK-T with NVIDIA A100 GPUs and the rest of models are trained with NVIDIA V100.</li> </ul>"},{"location":"SLAK/#semantic-segmentation-on-ade20k","title":"SEMANTIC SEGMENTATION ON ADE20K","text":"<ul> <li>We follow the training setting used in Ding et al. (2022); Liu et al. (2022b) using UperNet (Xiao et al., 2018) implemented by MMSegmentation (Contributors, 2020) with the 80K/160K-iteration training schedule. We conduct experiments with both short and long training procedures. The backbones are pre-trained on ImageNet-1K with 224x224 input for 120/300 epochs and then are finetuned with UperNet (Xiao et al., 2018) for 80K/160K iterations, respectively. We report the mean Intersection over Union (mIoU) with single-scale. All the hyperparameters are the exactly the same as the ones used in the official GitHub repository of ConvNeXt (con, 2021).</li> </ul>"},{"location":"SLAK/#object-detection-and-segmentation-on-coco","title":"OBJECT DETECTION AND SEGMENTATION ON COCO","text":"<ul> <li>For COCO experiments, we follow the training settings used in BEiT, Swin, and ConvNeXt using MMDetection (Chen et al., 2019) and MMSegmentation (Contributors, 2020) toolboxes. The final model weights are adopted (instead of EMA weights) from ImageNet-1K pre-training with 224x224 input. We also conduct experiments with both short and long training procedures. The backbones are pre- trained on ImageNet-1K with 224x224 input for 120/300 epochs and then are finetuned with Cascade Mask R-CNN (Cai &amp; Vasconcelos, 2018) for 12/36 epochs, respectively. All the hyperparameters are the exactly the same as the ones used in the official GitHub repository of ConvNeXt (con, 2021).</li> </ul>"},{"location":"SLAK/#object-detection-on-pascal-voc-2007","title":"OBJECT DETECTION ON PASCAL VOC 2007","text":"<ul> <li>We follow (Liu et al., 2021e) and finetune Faster-RCNN on PASCAL VOC dataset with SLaK-T as the backbone. We use multi-scale setting (Carion et al., 2020; Sun et al., 2021) which leads to the length of the shorter side between 480 and 800 and the ones of the longer side at most 1333. The model is trained with AdamW for 36 epochs with a learning rate of 0.0001, a weight decay of 0.05, and a batch size of 16.</li> </ul>"},{"location":"SLAK/#some-more-effects","title":"Some More Effects","text":""},{"location":"SLAK/#trade-off-between-sparsity-and-width","title":"TRADE-OFF BETWEEN SPARSITY AND WIDTH","text":"<ul> <li>As we expected, the model's performance keeps increasing as model width</li> <li>increases until the width factor reaches 1.5x, after which increasing width further starts to hurt the performance apparently due to the training difficulties associated with highly sparse neural networks.</li> </ul>"},{"location":"SLAK/#effect-of-the-shorter-edge-n-on-slak","title":"EFFECT OF THE SHORTER EDGE N ON SLAK","text":"<ul> <li>We vary the shorter edge N 2 [3, 5, 7] and report the accuracy. All models were trained with AdamW on ImageNet-1K for 120 epochs. We empirically find that N=5 give us the best results, whereas N = 3 and N = 7 has slightly lower accuracy. We hence think it reasonable to choose N = 5 as the default option.</li> </ul>"},{"location":"SLAK/#erf-quantitation-of-models-with-different-kernel-sizes","title":"ERF QUANTITATION OF MODELS WITH DIFFERENT KERNEL SIZES","text":"<ul> <li>Larger r suggests a smoother distribution of high-contribution pixels. We can see that with global kernels, SLaK naturally considers a larger range of pixels to make decisions than ConvNeXt and RepLKNet.</li> </ul>"},{"location":"SLAK/#configurations-of-dynamic-sparsity","title":"CONFIGURATIONS OF DYNAMIC SPARSITY","text":"<ul> <li>Following Liu et al. (2021c), we specifically tune two factors for SLaK-T that control the strength of weight adaptation, adaptation frequency f and adaptation rate p. Adaptation frequency determines after how many training iterations we adjust the sparse weights, and the latter controls the ratio of the weight that we adjust at each adaptation</li> <li>f = 2000</li> <li>and p = 0.5 works best for SLak-T. For SLak-S/B, we directly choose f = 100 and p = 0.3 without careful tuning.</li> </ul>"},{"location":"SLAK/#limitations","title":"LIMITATIONS","text":"<ul> <li>sparse architecture is implemented with binary masks due to the limited support of sparse neural networks by the commonly used hardware such as GPU and TPU</li> <li>Therefore, the inference FLOPs reported in the main paper are the theoretical values.</li> <li>Once this great potential is supported in the future, it can have a significant positive impact on our planet by saving a huge amount of energy and reducing overall total carbon emissions.</li> <li>Although not the focus of this current work, it would be interesting for future work to examine the speedup of sparse large kernels, using such specialized hardware accelerators, as we see much improvement room of promise here.</li> </ul>"},{"location":"SMOTE/","title":"SMOTE","text":"<ul> <li>is a popular augmentation used to alleviate problems with class imbalance. This technique is applied to the feature space by joining the k nearest neighbors to form new instances.</li> </ul>"},{"location":"SMP/","title":"SMP","text":"<ul> <li>Architecture where multiple processors share a single address space and access to all resources</li> <li>Shared memory computing</li> <li>Connected by a bus</li> </ul>"},{"location":"SOMs/","title":"Self Organizing Maps","text":"<ul> <li>Kohonen maps</li> <li>Crumbled up grid of SOM neurons</li> <li> </li> <li>Essentially : need to map a high dim space to a grid of neurons while trying to preserve the neighborhood relations from the high dim space. This is technically impossible so compromise.</li> <li>First initialized with small random values</li> <li>For each new pattern, identify Best Maching Unit based on current vectors. Reduce the value of r. And pull the point to the part of the grid with similar weight vectors.<ul> <li>Update weights \\(\\(w(v_{kl}) \\leftarrow w(v_{kl}) + \\lambda f_r(d(v_{kl}, v_{BMU}))(x-w(v_{kl}))\\)\\)</li> <li>\\(\\lambda\\) is learning rate</li> <li>d is Euclidean Distance between two neurons in grid.</li> <li>\\(f_{r}(0)=1\\). Tends to 0 as argument grows. r is radius. Greater values, will spread it out.</li> <li>Regulated by \\(f_r(d(v_{kl}, v_{BMU}))\\)</li> </ul> </li> <li>Eventually this will lead to an organization. Covered evenly after a while. Eventually neighbors of \\(v_0\\) would have weights towards \\(w(v_0)\\) . And \\(w(v_{0)} \\approx mean(all patterns x)\\)</li> <li>Repeat until response stops. Each members BMU rate is too low to expand.</li> <li>Start with large r and then slow down.</li> </ul>"},{"location":"SOMs/#neuromorphic","title":"neuromorphic","text":""},{"location":"SP-LIME/","title":"SP-LIME","text":"<ul> <li>model judges whether you can trust the whole model or not. It selects a picked diverse set of representative instances with LIMEs via submodular optimization. The user should evaluate the black box by regarding the feature words of the selected instances. It is conceivable that it also recognizes bias or systematic susceptibility to adversarial examples. With this knowledge, it is also possible to improve a bad model. SP-LIME was researched with text data, but the authors claimed that it can be transferred to models for any data type.</li> <li>This was inspired by CAM and Grad-CAM and tested the explanator on randomly chosen images from the COCO dataset [91], applied to the pre-trained neural network VGG-16 using the Kullback\u2013Leibler (KL) divergence</li> </ul>"},{"location":"SQL-Tutor/","title":"SQL-Tutor","text":"<ul> <li>Outer loop: SQL-Tutor (http://www.aw-bc.com/databaseplacedemo/sqltutor.html) teaches students how to write a query to a relational database (B. Martin &amp; Mitrovic, 2002; Mitrovic, 2003; Mitrovic &amp; Ohlsson, 1999). Each task consists of a database and information to be retrieved from it. In Figure 5, for instance, the student has been given a database of movie information and has been asked to write a query that will List the titles and numbers of all movies that have won at least one Academy Award and have been made in or after 1988.</li> <li>Inner loop: Students write a query in the SQL language by clicking on buttons and filling in blanks. This may take several minutes. At any point, they can press the Submit Answer button. The tutor, which has been completely silent up until now, analyzes the student's query to find its flaws. It gives a variety of levels of feedback and hints.</li> <li>One way to think of SQL-Tutor's inner loop is that the student takes multiple steps, each comprised of filling in a blank in the query</li> <li>Unlike tutors that give feedback as soon as a student had taken a step, SQL-Tutor delays its feedback until the student requests it.</li> <li>Step analysis: In order to analyze steps, SQL-Tutor has a set of constraints, where a constraint consists of a relevance condition and a satisfaction condition. If the relevance condition is false of the students' step, then the constraint is irrelevant, so the tutor says nothing about it. If the constraint has a true relevance condition and a true satisfaction condition, then the constraint is satisfied and the tutor says nothing about it.</li> <li>If the relevance condition is true, and the satisfaction condition is false, then the student's step violates the constraint and the tutor has identified a topic worth talking about. In particular, every constraint has two messages.</li> <li>Depending on the feedback level selected by the tutor or the student, one of them may be presented to the student when the constraint is violated. One message describes the constraint and its violation briefly. The other presents more details.</li> <li>Although the constraints are task independent, many of them refer to a correct solution of the problem, which is stored in the tutoring system.</li> <li>The relationship between steps, learning events and constraints is quite simple in the SQL-Tutor. Each constraint corresponds to a Knowledge Component.</li> </ul>"},{"location":"SQuAD/","title":"SQuAD","text":""},{"location":"SRN/","title":"SRN","text":"<ul> <li>Just a simple RNN Cell</li> <li></li> <li></li> <li>Vanishingexploding gradients , in Backprop, they break down when sequences are long.</li> <li>Distance between the relevant words are too long</li> <li>Followed up [[LSTM)](Long Short Term Memory (LSTM|Long Short Term Memory (LSTM|LSTM)]].md)</li> </ul>"},{"location":"SSR/","title":"SSR","text":"<ul> <li>Smallest sufficient region</li> <li>smallest region of the image that alone allows a confident classification</li> </ul>"},{"location":"SSR/#backlinks","title":"Backlinks","text":"<ul> <li>Real Time Image Saliency for Black Box Classifiers</li> <li>SSR</li> <li>In order to be as informative as possible we would like to find a region that performs well as both SSR and SDR.</li> <li>Both SDR and SSR remove some evidence from the image</li> <li>The metric is almost a direct translation of the SSR</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"SVHN/","title":"SVHN","text":"<p>title: SVHN</p> <p>tags: dataset</p>"},{"location":"SVHN/#svhn","title":"SVHN","text":""},{"location":"SYNTHIA/","title":"SYNTHIA","text":""},{"location":"Saddle%20Points/","title":"Saddle Points","text":""},{"location":"Safeguard/","title":"Safeguard","text":"<ul> <li>A barrier guard, device or safety procedure designed for the protection of personnel</li> </ul>"},{"location":"Safety%20Integrity%20Level/","title":"Safety Integrity Level","text":"<ul> <li>Safety Integrity Level (SIL) is IEC\u2019s method for determining the performance level of a safety system. SIL 2 corresponds to ISO Performance Level \u201cd\u201d, and SIL 3 corresponds to ISO Performance Level \u201ce\u201d. ISO 10218 allows for the use of either.</li> </ul>"},{"location":"Safety%20Logic%20Circuit/","title":"Safety Logic Circuit","text":"<ul> <li>The safety logic circuit monitors safety critical external devices such as the light curtains and FSU generated signals. The safety logic circuit is programmed via an intuitive user interface that is supported on the Yaskawa programming pendant. It enables to set up the logical operations, such as stopping the manipulator or outputting a signal if the servos are on.</li> </ul>"},{"location":"Saffran%2C%20Aslin%20and%20Newport/","title":"Saffran, Aslin and Newport","text":"<ul> <li>8-month-olds can segment a continuous stream of speech syllables, containing no acoustic or prosodic cues to word boundaries, into wordlike units after only 2 min of listening experience</li> </ul>"},{"location":"Salicon%20dataset/","title":"Salicon Dataset","text":"<ul> <li>\"10,000 images for training and 5000 images for validation\"</li> <li>\"Each image was viewed by 60 observers\"</li> <li>this dataset Different from other fixation datasets, is large-scale mouse-tracking data through Amazon Mechanical Turk</li> <li>Although this dataset is not the fixation dataset, it is well known that the distribution of mouse tracking points is similar to the distribution of fixations and that the parameters of the model trained on Salicon dataset are useful for saliency map estimation</li> </ul>"},{"location":"Salience%20Map/","title":"Salience Map","text":""},{"location":"Salience%20Map/#explained","title":"Explained","text":"<ul> <li>Specifies parts of the image that contribute the most to the activity of a specific layer or the entire decision</li> <li>\\(Y_{c}=\\text{score of class c}\\) : value of output before softmax</li> <li> \\[saliency = max_{r,g,b}(|\\frac{\\partial Y_{c}}{\\partial I}|)\\] </li> <li>grad of \\(Y_{c}\\) wrt input - matrix with shape similiar to input</li> <li>if value close to 0 : small changes in input have no effect on output</li> <li>if high magnitude : small changes can have a major impact</li> <li>positive : roughly the location of the target object</li> <li>negative : competing class objects : background or instance</li> <li>Abs value for heatmap</li> <li>grads : backprop on \\(Y_{c}\\) instead of loss</li> </ul>"},{"location":"Salience%20Map/#three-approaches","title":"Three Approaches","text":""},{"location":"Salience%20Map/#dencov","title":"Dencov","text":"<ul> <li>Use DeconvNet</li> <li>Use backprop to compute the gradients of logits wrt input : Deep Inside Convolutional Networks</li> <li>Guided BackProp</li> </ul>"},{"location":"Salience%20Map/#features","title":"Features","text":"<ul> <li>One of the oldest interpretation methods</li> <li>Salience maps of important features are calculated, and they show superpixels that have influenced the prediction most, for example</li> <li>To create a map of important pixels, one can repeatedly feed an architecture with several portions of inputs and compare the respective output, or one can visualize them directly by going rearwards through the inverted network from an output of interest;</li> <li>Grouped in this category as well is exploiting neural networks with activation atlases through feature inversion. This method can reveal how the network typically represents some concepts</li> <li>Considering image or text portions that maximize the activation of interesting neurons or whole layers can lead to the interpretation of the responsible area of individual parts of the architecture.</li> </ul>"},{"location":"Saliency%20using%20natural%20statistics/","title":"Saliency Using Natural Statistics","text":"<ul> <li>applies a Bayesian framework using local feature maps to estimate saliency maps.</li> <li>The probability distribution of features is learned not from each individual test image but from statistics calculated over the training set of natural images.</li> </ul>"},{"location":"SaliencyMix/","title":"SaliencyMix","text":"<ul> <li>extracts salient regions and pastes them on the corresponding location in the target image</li> <li>The salient region is extracted around the maximum intensity pixel location in the saliency map</li> </ul>"},{"location":"Salient%20Object%20Strategy/","title":"Salient Object Strategy","text":"<ul> <li>\"if an object is contextually relevant, then it is salient\" (Philip 2011: 370\u2013371)</li> </ul>"},{"location":"Sample%20Correlation/","title":"Sample Correlation","text":"<p>title: Sample Correlation</p> <p>tags: statistics</p>"},{"location":"Sample%20Correlation/#sample-correlation","title":"Sample Correlation","text":"<ul> <li>A type of Correlation</li> <li> \\[r_{xy}= \\frac{s_{xy}}{s_{x}s_{y}}\\] </li> <li>\\(s_{x}\\), \\(s_y\\) are the sample Standard Deviation</li> <li>\\(s_y\\) is the sample Covariance</li> </ul>"},{"location":"Sample%20Pairing/","title":"Sample Pairing","text":"<ul> <li>merges two images by averaging their pixel intensities</li> <li>The new image has the same training image label opposite to MixUp and other approaches where labels are updated according to the proportion of image mixing.</li> <li>one epoch on ImageNet and 100 epochs on other datasets are completed without SamplePairing before mixed image data is added to the training</li> <li>Once the SamplePairing images are added to the training set, they run in cycles between 8:2 epochs, 8 with SamplePairing images, 2 without.</li> </ul>"},{"location":"Sampler/","title":"Sampler","text":"<ul> <li>Given :<ul> <li>\\(P_{X}\\) is a distribution on a measure space (E,B)</li> <li>A seq of \\(X_{1}, X_{2}, \u2026\\) of random variables is a sampler if for all \\(A \\in B\\)</li> <li> \\[P_{X}(A) = lim_{N \\rightarrow \\infty} \\frac{1}{N}\\Sigma_{i=1}^{N}1_{A}\\circ X_{i}\\] </li> <li>\\(1_A\\) is an indicator function for A</li> </ul> </li> <li>\\(X_{1}, X_{2}, \u2026\\) need not have the same distribution or need to be independant</li> <li>Dream : All \\(X_{i}\\) are uniformly distributed on [0,1]. (Impossible)</li> </ul>"},{"location":"Sampling%20Bias/","title":"Sampling Bias","text":"<ul> <li>Data is not collected randomly from the target group.</li> </ul>"},{"location":"Sampling%20Ray%20Casting/","title":"Sampling Ray Casting","text":"<ul> <li>selection of positions along the ray</li> <li>Early Ray Termination</li> <li></li> </ul>"},{"location":"Satellite%20Cell/","title":"Satellite Cell","text":"<ul> <li>Surround neuron cell bodies</li> <li>Similar to Astrocyte</li> </ul>"},{"location":"Satisficing%20Heuristic/","title":"Satisficing Heuristic","text":"<ul> <li>Good enough</li> <li>Less time</li> <li>Less knowledge</li> </ul>"},{"location":"Scalar%20Articles/","title":"Scalar Articles","text":""},{"location":"Scalar%20Articles/#done","title":"Done","text":"<ul> <li>Word2Vec with gensim</li> <li>CycleGAN</li> <li>Masked Language Modeling with BERT</li> <li>DCGAN</li> <li>Conditional GAN</li> <li>Stack GAN</li> <li>Basic GAN</li> </ul>"},{"location":"Scalar%20Articles/#in-progress","title":"In Progress","text":"<ul> <li>Generative vs Discriminative Models</li> </ul>"},{"location":"Scalar%20Articles/#backlinks","title":"Backlinks","text":"<ul> <li> <p>I decided to put all my articles for scalar in Scalar Articles</p> </li> <li> <p>2022-12-21</p> </li> <li>02:47 man these articles are all the same arent they. Scalar Articles</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Scalar%20Color%20Coding/","title":"Scalar Color Coding","text":"<ul> <li>Mean Diffusivity</li> <li>Fractional Anisotropy</li> <li>Eigenvector</li> </ul>"},{"location":"Scalar%20Register/","title":"Scalar Register","text":"<ul> <li>Single elements for interconnecting Vector Functional Units, Vector Load Store Units, and registers</li> </ul>"},{"location":"Scaled%20Dot%20Product%20Attention/","title":"Scaled Dot Product Attention","text":"<ul> <li>Vaswani et al., 2017</li> <li>Q is query, K is key V is value. Same dims</li> <li>\\(q_{i}= W_{q}x_i\\) , \\(k_{i}= W_{k}x_{i}\\) , \\(v_{i}= W_{v}x_{i}\\)<ul> <li>\\(w_{ij}' = q_{i}^{T}k_{j}\\)</li> <li>\\(y_{i}= \\Sigma_{j}w_{ij}v_{j}\\)</li> </ul> </li> <li>Softmax is sensitive to large values. Which sucks for the #gradients</li> <li>The avg value of the dot product grows with embedding dimension k. So scale back.<ul> <li>\\(\\sqrt{k}\\) . Vector in \\(\\mathbb{R}^{k}\\) with all values as c</li> <li>Euclidean length is \\(\\sqrt{kc}\\)</li> </ul> </li> <li> \\[Attention(Q, K,V) = softmax(\\frac{QK^T}{\\sqrt{d_{k}}})V\\] </li> <li>Generalization of Soft Attention</li> <li></li> <li>Attention Alignment score \\(\\(\\alpha_{t,i} = \\frac{s_{t}^{T}h_{i}}{\\sqrt{n}}\\)\\)</li> </ul>"},{"location":"Scaled%20benefits/","title":"Scaled Benefits","text":"<ul> <li>The more prepared - the more benefits</li> </ul>"},{"location":"Scaled%20benefits/#backlinks","title":"Backlinks","text":"<ul> <li>Cogntition Hazard Rates</li> <li>Scaled benefits (useful for assignment)</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Scatter%20and%20Gather/","title":"Scatter and Gather","text":"<ul> <li>Retrieves data elements scattered thorughout memory and packs them into sequential vectors in vector registers</li> <li>Promotes data locality and reduces data pollution</li> </ul>"},{"location":"Scene%20based%20text%20to%20image%20generation/","title":"Scene Based Text to Image Generation","text":"<ul> <li>Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors</li> <li>text-to-image generation</li> <li>enabling a simple control mechanism complementary to text in the form of a scene</li> <li>introducing elements that substantially improve the tokenization process by employing domain-specific knowledge over key image regions</li> <li>adapting classifier-free guidance for the transformer use case</li> <li>They attempt to progress text-to-image generation towards a more interactive experience, where people can perceive more control over the generated outputs, thus enabling real-world applications such as storytelling</li> <li>focus on improving key image aspects that are significant in human perception, such as faces and salient objects, resulting in higher favorability of their method in human evaluations and objective metrics</li> <li>Through scene controllability, they introduce several new capabilities: (i) scene editing, (ii) text editing with anchor scenes, (iii) overcoming out-of-distribution text prompts, and (iv) story illustration generation</li> </ul>"},{"location":"Scheduling/","title":"Scheduling","text":"<ul> <li>Some prune all the weights at once</li> <li>Others prune iteratively using loops or some other condition</li> </ul>"},{"location":"Schwann%20Cell/","title":"Schwann Cell","text":"<ul> <li>Insulate , helps form Myelin</li> <li>Similar to Ogliodendrocytes</li> </ul>"},{"location":"Scoring%20Pruning%20Approaches/","title":"Scoring Pruning Approaches","text":"<ul> <li>Like all networks, scoring becomes essential when we try to choose which parameter to get rid of.</li> <li>Some authors suggest removing based on absolute values, others decide to prune based on the contributions of that parameter to the entire network.</li> <li>Others remove based on a score given.</li> <li>Some perform Pruning locally, while others perform it globally across the network.</li> </ul>"},{"location":"Second%20Language%20Vocabulary%20Learning%20%2C%20The%20role%20of%20context%20%20versus%20translation/","title":"Second Language Vocabulary Learning , The Role of Context Versus Translation","text":"<ul> <li>PETER PRINCE</li> </ul> <ul> <li>A widespread view of vocabulary learning is that it is advisable to make the shift away from learning words with their translations and to rely on second language (L2) context as soon as possible</li> <li>Such faith in context learning has not always received experimental support, however, nor is it commonly shared by L2 learners</li> <li>An experiment in which subjects were tested on their recall of newly learned words was conducted to determine the relative advantages and disadvantages of both context learning and translation learning as a function of learner proficiency</li> </ul>"},{"location":"Second%20Language%20Vocabulary%20Learning%20%2C%20The%20role%20of%20context%20%20versus%20translation/#results","title":"Results","text":"<ul> <li>reveal a superiority of translation learning in terms of quantity, but an inability on the part of weaker learners to transfer their knowledge into L2 contexts</li> <li>suggested that alternative learning strategies that combine the advantages of the two techniques should be explored.</li> </ul>"},{"location":"Second%20Language%20Vocabulary%20Learning%20%2C%20The%20role%20of%20context%20%20versus%20translation/#backlinks","title":"Backlinks","text":"<ul> <li>Final Paper User Models</li> <li>Second Language Vocabulary Learning , The role of context  versus translation</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Second%20order%20generalization/","title":"Second Order Generalization","text":"<ul> <li>Present model with novel example (not seen in training)</li> </ul>"},{"location":"SegNet/","title":"Seg Net","text":"<ul> <li>Precursor to Unet</li> <li>No Skip connections</li> </ul>"},{"location":"Selection%20Bias/","title":"Selection Bias","text":"<ul> <li>Errors in conclusions drawn from sampled data due to a selection process that generates systematic differences between samples observed in the data and those not observed. The following forms of selection bias exist</li> <li>datasets often prefer particular kinds of images</li> <li>However, getting images from the Internet does not in itself guarantee a fair sampling, since keyword-based searches will return only particular types of images</li> <li>Obtaining data from multiple sources</li> <li>even better to start with a large collection of unannotated images and label them by crowd-sourcing</li> </ul>"},{"location":"Self%20Attention%20GAN/","title":"Self Attention GAN","text":"<ul> <li>Self Attention + Generative Models</li> </ul> <pre><code>class Self_Attn_New(nn.Module):\n    \"\"\" Self attention Layer\"\"\"\n    def __init__(self, in_dim):\n        super().__init__()\n        self.query_conv = nn.Conv2d(in_dim, out_channels=in_dim//8, kernel_size=1)\n        self.key_conv = nn.Conv2d(in_dim, out_channels=in_dim//8, kernel_size=1)\n        self.value_conv = nn.Conv2d(in_dim, out_channels=in_dim, kernel_size=1)\n        self.gamma = nn.Parameter(torch.zeros([1]))\n\n    def forward(self, x):\n        proj_query = rearrange(self.query_conv(x), 'b c h w -&gt; b (h w) c')\n        proj_key = rearrange(self.key_conv(x), 'b c h w -&gt; b c (h w)')\n        proj_value = rearrange(self.value_conv(x), 'b c h w -&gt; b (h w) c')\n        energy = torch.bmm(proj_query, proj_key)\n        attention = F.softmax(energy, dim=2)\n        out = torch.bmm(attention, proj_value)\n        out = x + self.gamma * rearrange(out, 'b (h w) c -&gt; b c h w',\n                                         **parse_shape(x, 'b c h w'))\n        return out, attention\n</code></pre>"},{"location":"Self%20Attention/","title":"Self Attention","text":"<ul> <li>paper</li> <li>Basically Scaled Dot Product Attention</li> <li>Q,K,V all from same module but prev layer</li> <li>Weighted average over all input vectors (\\(y_{i}= \\Sigma_{j}w_{ij}x_{j}\\)\\)<ul> <li>j is over the sequence</li> <li>weights sum to 1 over j</li> <li>\\(w_{ij}\\) is derived \\(w^{'}_{ij}=x_{i}^{T}x_{j}\\)<ul> <li>Any value between -inf to +inf so Softmax is applied</li> </ul> </li> <li>\\(x_i\\) is the input vector at the same pos as the current output vector \\(y_i\\)</li> </ul> </li> <li>Propagates info between vectors</li> <li></li> <li>The process<ul> <li>Assign every word t in the vocabular an Embedding</li> <li>Feeding this into a self attention layer we get another seq of vectors \\(y_{the}\\) , \\(y_{cat}\\) etc</li> <li>each of the \\(y_{something}\\) is a weighted sum over all the embedding vectors in the first seq weighted by their normalized dot product with \\(v_{something}\\)</li> <li>the dot product shows how related the vectors are in the sequence<ul> <li>weights determined by them</li> <li>Self-Attention layer may give more weights to those input vectors that are more similar to each other when generating the output vectors</li> </ul> </li> </ul> </li> <li>Properties<ul> <li>Inputs are a set (not sequence)</li> <li>If input seq is permuted, the output is too</li> <li>Ignores the sequential nature of input by itself</li> </ul> </li> <li>Code</li> </ul> <pre><code>def attention(K, V, Q):\n    _, n_channels, _ = K.shape\n    A = torch.einsum('bct,bcl-&gt;btl', [K, Q])\n    A = F.softmax(A * n_channels ** (-0.5), 1)\n    R = torch.einsum('bct,btl-&gt;bcl', [V, A])\n    return torch.cat((R, Q), dim=1)\n</code></pre>"},{"location":"Self%20Attention/#ref","title":"Ref","text":"<ul> <li>perterbloem</li> </ul>"},{"location":"Self%20Distillation/","title":"Self Distillation","text":"<ul> <li>To be specific, Yuan et al. proposed teacher-free knowledge distillation meth- ods based on the analysis of label smoothing reg- ularization (Yuan et al., 2020). Hahn and Choi pro- posed a novel self-knowledge distillation method, in which the self-knowledge consists of the predicted probabilities instead of traditional soft probabilities (Hahn and Choi, 2019).</li> <li>These predicted probabilities are defined by the feature representations of the train- ing model. They reflect the similarities of data in feature embedding space. Yun et al. proposed class- wise self-knowledge distillation to match the output distributions of the training model between intra- class samples and augmented samples within the same source with the same model (Yun et al., 2020).</li> <li>In addition, the self-distillation proposed by Lee et al. (2019a) is adopted for data augmentation and the self- knowledge of augmentation is distilled into the model itself. Self distillation is also adopted to optimize deep models (the teacher or student networks) with the same architecture one by one (Furlanello et al., 2018; Bagherinezhad et al., 2018)</li> <li>both self-distillation and online distillation are properly in- tegrated via the multiple knowledge transfer frame- work (Sun et al., 2021).</li> </ul>"},{"location":"Self%20Supervised%20Vision%20Transformers/","title":"Self Supervised Vision Transformers","text":"<p>title: Self Supervised Vision Transformers</p> <p>tags: architecture</p>"},{"location":"Self%20Supervised%20Vision%20Transformers/#self-supervised-vision-transformers","title":"Self Supervised Vision Transformers","text":"<ul> <li>An Empirical Study of Training Self-Supervised Vision Transformers<ul> <li>recipes for Vision Transformer are yet to be built</li> <li>Self Supervised</li> <li>instability is a major issue that degrades accuracy, and it can be hidden by apparently good results</li> <li>improved when training is made more stable</li> <li>MoCO v3, a framework which offers an incremental improvement of MoCO</li> <li></li> </ul> </li> </ul>"},{"location":"Self%20Supervised/","title":"Self Supervised","text":"<ul> <li>Subset of Unsupervised Learning</li> <li>ConvNet trained with supervisory signals that are generated from data itself</li> <li>Very much Transfer Learning</li> </ul>"},{"location":"Self%20Supervised/#anchor","title":"anchor","text":""},{"location":"Semantic%20Analysis/","title":"Semantic Analysis","text":"<ul> <li>Meaning of the language</li> <li>Meanings of the word to extend and perhaps disambiguate the result returned by the syntactic parse</li> <li>Look up the individual words in a dictionary (or Lexicon) and extract their meanings</li> <li>But many words have several meanings<ul> <li>Lexical Disambiguation</li> </ul> </li> <li>Sentence level processing</li> </ul>"},{"location":"Semantic%20Data/","title":"Semantic Data","text":""},{"location":"Semantic%20Data/#semantic-data","title":"Semantic Data","text":"<ul> <li>represents the world by a set of subject-predicate-object triple Therefore, the size of the messages is small.</li> </ul>"},{"location":"Semantic%20Data/#backlinks","title":"Backlinks","text":"<ul> <li>Semantic Data</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Semantic%20Grammar/","title":"Semantic Grammar","text":"<ul> <li>combine syntactic, semantic and pragmatic knowledge into a single set of rules in the form of a grammar</li> </ul>"},{"location":"Semantic%20Markers/","title":"Semantic Markers","text":"<ul> <li>PHYSICAL-OBJECT</li> <li>ANIMATE-OBJECT</li> <li>ABSTRACT-OBJECT</li> <li>Unfortunately, to solve Lexical Disambiguation problem complete, it becomes necessary to introduce more and more finely grained semantic markers</li> </ul>"},{"location":"Semantics%20influences%20form/","title":"Semantics Influences Form","text":"<ul> <li>Past tense choices mediated by perceived semantic similarity to neighbors, e.g. drank</li> <li>Adults under time pressure also make overgeneralization errors at rates from 6% to 31%</li> </ul>"},{"location":"Semi%20Supervised/","title":"Semi Supervised","text":"<ul> <li>Obtain weak labels instead of class labels</li> <li>Eg: \"similar\"</li> <li>Contrastive Loss</li> <li>Triplet Loss</li> <li>Max Margin Loss</li> </ul>"},{"location":"Semi%20Supervised/#anchor","title":"anchor","text":""},{"location":"Sense-Plan-Act%20Model/","title":"Sense-Plan-Act Model","text":""},{"location":"Sense-Plan-Act%20Model/#sense-plan-act-model","title":"Sense-Plan-Act Model","text":"<ul> <li>Deliberative planning has three main steps that are performed in sequence: Sensing,</li> <li>Planning</li> <li>Acting (executing the plan)</li> </ul>"},{"location":"Sense-Plan-Act%20Model/#backlinks","title":"Backlinks","text":"<ul> <li>Sense-Plan-Act Model</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Sensitivity/","title":"Sensitivity","text":"<ul> <li> \\[TPR = \\frac{TP}{TP + FN}\\] </li> </ul>"},{"location":"Sensory%20Feedback/","title":"Sensory Feedback","text":"<ul> <li>Variable data measured by sensors and relayed to the controller in a Closed-loop System. If the controller receives feedback that lies outside an acceptable range, then an error has occurred. The controller sends an error signal to the robot. The robot makes the necessary adjustments in accordance with the error signal.</li> </ul>"},{"location":"Sentence%20Segmentation/","title":"Sentence Segmentation","text":"<ul> <li>identify the processing unit, consists of one or more words</li> <li>Sentence boundary detection</li> <li>Sentence boundary disambiguation</li> <li>Sentence boundary recognition</li> </ul>"},{"location":"Sentence%20level%20processing/","title":"Sentence Level Processing","text":"<ul> <li>Semantic Grammar</li> <li>Case Grammar</li> <li>Conceptual Parsing</li> <li>Approximately Compositional Semantic Parsing</li> </ul>"},{"location":"Sentiment%20Neuron/","title":"Sentiment Neuron","text":"<ul> <li>Linear model + L1 Lp Regularization</li> <li>Uses very few learned units</li> <li>Single sentiment neuron that predicts the sentiment value</li> <li></li> <li>Can be useful for Unsupervised Learning</li> </ul>"},{"location":"Sentiment%20Neuron/#refs","title":"Refs","text":""},{"location":"SentimentAnalysis/","title":"SentimentAnalysis","text":"<ul> <li>Sentiment Neuron</li> <li>Block Sparse Kernel</li> </ul>"},{"location":"Sepsis/","title":"Sepsis","text":"<ul> <li>An imbalance in the body's response to infection that injures the body's tissues and organs</li> </ul>"},{"location":"Seq2Seq/","title":"Seq2Seq","text":"<ul> <li>Basic RNN Architectures</li> <li>Long term dependency Issues</li> <li>Even if hidden state vector has a high dimensionality, cannot hold all info</li> <li>Sequence to Sequence Learning with Neural Networks</li> <li>encoder-decoder learning to map sequences to sequences</li> <li>multilayered Long Short-Term Memory [[LSTM)](Long Short Term Memory (LSTM|Long Short Term Memory (LSTM|LSTM)]].md)</li> <li>large deep LSTM with a limited vocabulary can outperform a standard statistical machine translation (SMT)-based system whose vocabulary is unlimited on a large-scale MT task</li> <li>WMT14</li> <li>BLEU score</li> <li>reversing the order of the words in all source sentences (but not target sentences) improved the LSTM\u2019s performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier</li> </ul>"},{"location":"Sequential%20Relation%20Bias/","title":"Sequential Relation Bias","text":"<ul> <li>Sometimes our data has a sequential characteristic. For instance, time series and sentences consist of sequential elements that appear one after another. To model this pattern, we can introduce a Recurrent layer to our network:</li> <li></li> </ul>"},{"location":"Serious%20Games/","title":"Serious Games","text":""},{"location":"Serious%20Games/#serious-games","title":"Serious Games","text":"<ul> <li>Support learning</li> <li>Simulators of real world tasks</li> <li>Eg: response simulator, kerbal space program, sustainable energy management</li> </ul>"},{"location":"Serotonin/","title":"Serotonin","text":"<ul> <li>A neurotransmitter believed to play many roles, including, but not limited to, temperature regulation, sensory perception, and the onset of sleep. Neurons using serotonin as a transmitter are found in the brain and in the gut. A number of antidepressant medications are targeted to brain serotonin systems.</li> </ul>"},{"location":"Servo%20Control/","title":"Servo Control","text":"<ul> <li>The process by which the control system of the robot checks if the attained pose of the robot corresponds to the pose specified by the motion planning with required performance and safety criteria.</li> </ul>"},{"location":"Servo%20Motor/","title":"Servo Motor","text":"<ul> <li>An electrical power mechanism used to effect motion or maintains position of the robot</li> </ul>"},{"location":"Servo%20Pack/","title":"Servo Pack","text":"<ul> <li>An alternating, current electrical power mechanism that is controlled through logic to convert electrical supply power that is in a sine wave form to a Pulse Width Modulated (PWM) square form, delivered to the motors for motor control: speed, direction, acceleration, deceleration and braking control.</li> </ul>"},{"location":"Servo-controlled%20Robot/","title":"Servo-controlled Robot","text":"<ul> <li>The control of a robot through the use of a Closed-loop Servo-system, in which the position of the robot axis is measured by feedback devices and is stored in the controller's memory</li> </ul>"},{"location":"Servo-system/","title":"Servo-system","text":"<ul> <li>A system in which the controller issues commands to the motors, the motors drive the arm, and an encoder sensor measures the motor rotary motions and signals the amount of the motion back to the controller.</li> </ul>"},{"location":"Shading/","title":"Shading","text":""},{"location":"Shapes%20Dataset/","title":"Shapes Dataset","text":"<ul> <li>Convention is that the first dimension of the dataset belongs to the features and the second to the number of samples. The operations can then be systematically applied from the left.  </li> <li>Exception!, in TensorFlow and PyTorch it is the other way around!</li> <li>\\((\\#features, \\#samples)\\)</li> </ul>"},{"location":"Shared%20Character%20Set/","title":"Shared Character Set","text":"<ul> <li>helps in narrowing down to a small set of languages</li> <li>Arabic &amp; Persian<ul> <li>Share same characters but one has supplemental characters</li> </ul> </li> <li>Russian &amp; Ukrainian<ul> <li>Same Character Set - Different Frequencies</li> </ul> </li> <li>Norwegian &amp; Swedish</li> </ul>"},{"location":"Shepard%20Interpolation/","title":"Shepard Interpolation","text":"<ul> <li>$$</li> </ul> <p>\\begin{cases}\\Sigma^{N}{i=1}w{i}(x)f_{i}&amp; \\text{if } d(x,x_{i}) \\neq 0 \\forall i\\f_{i}&amp; \\text{if } d(x, x_{i})=0\\</p> <p>\\end{cases}</p> <p>$$ - \\(w_{i}(x) = \\frac{1}{d(x,x_{i})^{p}}\\) - Neighborhood N determines points aka radius</p>"},{"location":"Sherlock/","title":"Sherlock","text":"<ul> <li>Outer loop: Sherlock (Katz et al., 1998) tutors the troubleshooting of a large piece of simulated electrical equipment, an avionics test station.</li> <li>It provides many views on the equipment and its schematics, so no screenshots are included here.</li> <li>Inner loop: Troubleshooting the equipment requires taking many steps, such as measuring voltages and replacing suspect parts. Sherlock gives unsolicited feedback only if a step is unsafe, that is, if the step would cause serious damage to the equipment or the student if it were done in the real world</li> <li>Sherlock also makes extensive feedback available after the student has solved the problem. Sherlock's default post-solution feedback is to display a side-by-side summary of the student's solution and Sherlock's ideal solution.</li> </ul>"},{"location":"Shock%20Detection%20Function/","title":"Shock Detection Function","text":"<ul> <li>Shock detection is a function supported by the Yaskawa robot controller that reduces the impact of a robot collision by stopping the manipulator without any external sensor when the tool or the manipulator collide with a peripheral device.</li> </ul>"},{"location":"Shortcuts%20to%20Quantifier%20Interpretation%20in%20Children%20and%20Adults/","title":"Shortcuts to Quantifier Interpretation in Children and Adults","text":"<ul> <li>Patricia J. Brooks &amp; Irina Sekerina</li> </ul>"},{"location":"Shortcuts%20to%20Quantifier%20Interpretation%20in%20Children%20and%20Adults/#intro","title":"Intro","text":"<ul> <li>Summarized in 2022-10-10</li> <li>Errors involving universal quantification are common in contexts depicting sets of individuals in partial, one-to-one correspondence</li> <li>quantifier-spreading errors are more common with distributive quantifiers each and every than with all.</li> <li>pairs of pictures</li> <li>selected one corresponding to a sentence containing a universal quantifier</li> <li>not in correspondence, with correct sentence interpretation requiring their attention</li> <li>Children younger than 9 years made numerous errors</li> <li>with poorer performance in distributive contexts than collective ones</li> <li>21 native, English-speaking adults, given a similar task with the distributive quantifier every, also made childlike errors undermines accounts positing immature syntactic structures as the error source</li> <li>errors seemingly reflect inaccurate syntax to semantics mapping, with adults and children alike resorting to processing shortcuts.</li> <li>Quantificational terms such as all, usually, and most are a crucial type of linguistic device used to indicate which sets of individuals or events have which properties and relationships</li> <li>acquisition may be delayed relative to other sorts of lexical items (e.g., nouns and verbs) because their complex patterns of usage often result in interpretive ambiguities</li> <li>96 children (5- to 9-year-olds)</li> <li>Both pictures showed extra objects</li> <li>In spoken language, however, the intensifier interpretation is predominant in which it is conventional to say all even when it does not imply exhaustivity</li> <li>I left all my money at home would not preclude their having money in the bank</li> <li>all is used primarily as an adverbial intensifier in both child language and child-directed speech</li> <li>every and each involve additional lexical complexity</li> <li>Every appears inside compounds\u2014for example, everybody\u2014and it vacillates between collective and distributive interpretations when used as a quantifier</li> <li>Each is more uniformly distributive but has the further conceptual requirement that the individuals modified by each be successively scanned</li> <li>Every and each both occur only rarely in either childdirected or child speech, which limits opportunities for children to acquire their patterns of usage.</li> <li>overexhaustive search</li> <li>involves failure to properly restrict the domain of the universal quantifier to the</li> <li>exhaustive pairing</li> <li>classic spreading</li> <li>noun phrase (NP) it modifies</li> <li>complementary error referred to as underexhaustive search</li> <li>Very young children occasionally make other, more surprising errors in interpreting universal quantifiers, such as answering no to the question Is every bunny eating a carrot? when shown, for example, a picture of three rabbits, each eating a carrot, along with a dog eating a bone. This error is referred to as bunny spreading</li> <li>Children's errors with universal quantification have led to controversies with respect to how to explain them. One view is that the errors stem from children's deficient syntactic representations (Kang (2001), Philip (1995; 1996), Roeper and de Villiers (1993), Roeper and Matthei (1975), Roeper et al. (2005)). Philip (1995; 1996), following Roeper and Matthei (1975) suggested that the classic spreading error is due to the fact that children syntactically misinterpret distributive universal quantifiers (e.g., each or every in English, cada in Spanish or Portuguese) as sentential adverbials that range over events as opposed to individuals. Roeper et al. (2005) described a sequence of steps of how children start with a general syntactic representation of every as an adverbial intensifier that gets progressively more specific as every changes its position in the syntactic representation.</li> <li>Rather, the errors are presumed to involve shallow processing, resulting in inaccurate mapping between syntactic and semantic representations</li> <li>First, Crain et al. (1996) did not systematically vary the position of the universal quantifier in the test questions or statements while holding constant the introductory story and scenario</li> <li>prototypical scenario</li> <li>This feature of their design provided children with unambiguous cues as to which set of entities was the focus of attention.</li> <li>Crain et al.'s (1996) claim that preschoolers have full competence with universal quantifiers would seem to be undermined by the fact that even older school-age children make errors identifying the domain of a universal quantifier.</li> <li>First, there have been great discrepancies in error rates across the many studies that have almost exclusively utilized the Truth Value Judgment Task, ranging from near perfect performance in Crain et al. (1996) to extremely high error rates in Kang (2001), that is, over 80% errors in 6- to 7-year-olds</li> <li>Our sentence\u2013picture matching task does not have the same demand characteristics as the Truth Value Judgment Task, and in our opinion, it provides a more accurate way of evaluating whether children's interpretations of sentences with universal quantifiers vary systematically as a function of the position of the</li> <li>quantifier in the sentence and the type of scene</li> <li>examine whether there is an asymmetry in the distribution of errors as a function of the syntactic position of the universal quantifier</li> <li>address the controversy as to whether children perform better in tasks with collective universal quantifiers (cf. Brooks and Braine (1996)) or with distributive ones (cf. Drozd (1996))</li> <li>Finally, because accounts positing syntactic deficits as the source of quantifierspreading errors (e.g., Kang (2001), Philip (1996), Roeper et al. (2005)) generally assume that adults are essentially error free in their comprehension of basic sentences containing universal quantifiers (i.e., their syntax is perfect), we tested a group of adults on a version of our task (Experiment 3) to evaluate this claim and to allow a more complete investigation of the developmental trajectory of quantifier acquisition from 5-year-olds to adults.</li> </ul>"},{"location":"Shortcuts%20to%20Quantifier%20Interpretation%20in%20Children%20and%20Adults/#experiment-1","title":"EXPERIMENT 1","text":""},{"location":"Shortcuts%20to%20Quantifier%20Interpretation%20in%20Children%20and%20Adults/#participants","title":"Participants","text":"<ul> <li>We recruited and tested twelve 5-year-olds (M = 5;5, range = 5;2\u20135;11), twelve 6-year-olds (M = 6;6, range = 6;2\u20136;10), twelve 7-yearolds (M = 7;6, range = 7;1\u20137;11), twelve 8-year-olds (M = 8;6, range = 8;0\u20138;11), and twelve 9-year-olds (M = 9;6, range = 9;1\u20139;11) at private elementary schools and after-school programs in Atlanta, Georgia.</li> <li>24 pairs of pictures depicting people involved in various activities such as carrying boxes, washing pets, or watering plants.</li> <li>Four types of picture pairs were constructed.</li> <li>three people individually engaged in an activity with three distinct objects or animals</li> <li>three people engaged in an activity with three objects</li> <li>Collective picture pair types 3 and 4 were variations of collective picture pair type 2, with new foils created to match the distributive pairs in terms of target\u2013foil similarity</li> <li>Across picture pairs, a variety of contexts with transitive actional verbs were used so that each sentence type could be presented multiple times without repeating any pictures</li> <li>All of the contexts involved humans acting on animate or inanimate objects</li> <li>Six sentence types were used</li> <li> <ul> <li>(1) Each of the (people) is (verb)ing an (object), for example, Each of the men is washing a bear. (2) There is a (person) (verb)ing each of the (objects), for example, There is a man washing each of the bears. (3) Every (person) is (verb)ing an (object), for example, Every man is washing a bear. (4) There is a (person) (verb)ing every (object), for example, There is a man washing every bear. (5) All of the (people) are (verb)ing an (object), for example, All of the men are washing a bear.</li> </ul> </li> <li> <ul> <li>(6) There is a (person) (verb)ing all of the (objects), for example, There is a man washing all of the bears.</li> </ul> </li> <li>Twelve additional pairs of pictures served as filler items.</li> </ul>"},{"location":"Shortcuts%20to%20Quantifier%20Interpretation%20in%20Children%20and%20Adults/#procedure","title":"Procedure","text":"<ul> <li>single, 20-min session conducted in a quiet room of their school</li> <li>We showed children two pictures at a time and asked them to point to the picture that went best with a sentence read aloud</li> <li>After the child looked at both pictures, the experimenter read the corresponding sentence and asked the child to point to the picture that went best with the sentence.</li> <li>without providing any corrective feedback</li> <li>Across trials, we randomized the position of the correct picture</li> <li>Children made no errors on filler sentences, and these trials were not examined further.</li> <li>mixed-design analysis of variance</li> <li>The dependent variable was the proportion of correct picture choices for Sentence Types 1 through 4</li> <li>arcsine transformed these proportions and all others</li> </ul>"},{"location":"Shortcuts%20to%20Quantifier%20Interpretation%20in%20Children%20and%20Adults/#analysis","title":"Analysis","text":"<ul> <li>The analysis showed significant main effects of syntactic position, F(1, 55) = 19.03, p &lt; .001, and age, F(4, 55) = 7.22, p &lt; .001. No other main effects or interactions were significant</li> <li>As shown in Table 1, comparisons against chance performance (50%) revealed that only the 9-year-olds as a group were above chance in selecting the correct pictures (Figure 1b) for sentences with the universal quantifier modifying the direct object.</li> <li>At this criterion, one 5-year-old (8%), two 6-year-olds (17%), five 7- year-olds (42%), five 8-year-olds (42%), and nine 9-year-olds (75%) performed above chance.</li> <li>Overall, children were correct in 95.8% of their picture selections when the original collective pictures of Brooks and Braine were used (see Figure 2) but only 83.1% of trials with the modified collective pictures (see Figures 3 and 4).</li> <li>children's responses became more consistently correct with age, with only children 7 years and older performing above chance as a group in the modified task when the quantifier modified the direct object.</li> <li>To examine whether individual children were above chance in selecting the appropriate collective picture on the modified task with all, we again used the binomial distribution ( p &lt; .05), with above-chance performance requiring 6 out of 6 correct responses</li> <li>At this criterion, zero 5-year-olds (0%), five 6-year-olds (42%), seven 7-year-olds (58%), seven 8-year-olds (58%), and seven 9-year-olds (58%) performed above chance.</li> <li>f we adopt a more lenient criterion (5 of 6), which is proportionally comparable to the 10 of 12 required for above-chance performance with sentences containing each or every, then six 5-year-olds (50%), eight 6-yearolds (67%), ten 7-year-olds (83%), eight 8-year-olds (67%), and ten 9-year-olds (83%) showed consistently strong individual performance.</li> <li>we conducted one additional mixed-design ANOVA with Quantifier (all, each, every) and Syntactic Position as within-subjects factors and Age as a between-subjects factor.</li> <li>The main effect of syntactic position was significant, F(1, 55) = 16.38, p &lt; .001, but was qualified by an interaction of quantifier and age, F(1, 55) = 7.13, p &lt; .01</li> <li>comprehension performance was more accurate when the universal quantifier modified the subject of the sentence.</li> <li>This effect of syntactic position, however, was highly significant for sentences with each or every but only marginally significant for sentences with all (see above for F tests for main effects of syntactic position in separate analyses by quantifier). No other interactions were significant.</li> <li>In general, their picture selections were more accurate for sentences with a universal quantifier modifying the subject in comparison to the direct object of a transitive actional verb.</li> <li>Although the Philip (1996) and Kang studies have shown a similar pattern of subject/object asymmetry to this study and Brooks and Braine (1996), we note that children performed at much higher levels of accuracy in our sentence\u2013picture matching task compared to the Truth Value Judgment Task. In none of our conditions, at any age, were children significantly below chance in their picture selections. This contrasts especially with Kang, who reported error rates over 80% in both English-speaking and Korean-speaking 6- and 7-year-olds</li> <li>However, by age 7, children were consistently correct in their picture choices regardless of the syntactic position of all in the sentence.</li> <li>This suggests that the collective groupings may have helped focus their at</li> <li>tention on the relevant set of entities modified by the quantifier.</li> <li>More generally, the Truth Value Judgment Task allows children to reject a picture for a variety of reasons (and it is often hard to discern the basis for children's pattern of responding)</li> <li>In Experiment 1, we used a sentence\u2013picture matching procedure in which children needed only to find the picture that matched the sentence. This task eliminated opportunities for participants to consider whether a collective versus distributive interpretation of the sentence was preferred and furthermore allowed us to carefully match our collective and distributive pictures with respect to the composition of the foils.</li> </ul>"},{"location":"Shortcuts%20to%20Quantifier%20Interpretation%20in%20Children%20and%20Adults/#experiment-2","title":"Experiment 2","text":"<ul> <li>We designed Experiment 2 to eliminate this interpretive confound through the use of locative scenes.</li> <li>locative pictures with animals and other entities shown in containers of various sorts (e.g., bananas in baskets, bears in beds).</li> <li>universal quantifiers in three syntactic constructions that support distributive interpretations in a locative context</li> <li>Across constructions, we systematically varied both the syntactic position of the universal quantifier and whether the subject of the sentence referred to the containers or the entities in them</li> </ul>"},{"location":"Shortcuts%20to%20Quantifier%20Interpretation%20in%20Children%20and%20Adults/#method","title":"Method","text":"<ul> <li>Participants. Twelve 7-year-olds (M = 7;6, range = 7;1\u20137;10), twelve 8-year-olds (M = 8;6, range = 8;0\u20138;11), and twelve 9-year-olds (M = 9;5, range = 9;0\u20139;10) took part in the experiment. We recruited and tested these children at the same schools as in Experiment 1. None of the children in Experiment 2 participated in the previous experiment.</li> </ul>"},{"location":"Shortcuts%20to%20Quantifier%20Interpretation%20in%20Children%20and%20Adults/#materials","title":"Materials","text":"<ul> <li>27 pairs of pictures depicting various entities arranged in containers (e.g., alligators in bathtubs, turtles in tanks, apples in bowls)</li> <li>The pictures showed distributive arrangements with the entities and containers in partial, one-to-one correspondence with each other.</li> <li>Both pictures depicted three entities each located in a unique container. One picture showed two extra empty containers (see Figure 5a), and the other picture showed two objects that were not in containers (see Figure 5b).</li> <li>nine sentence types</li> <li> <ul> <li>(7) All of the (objects) are in a (container), for example, All of the alligators are in a bathtub. (8) All of the (containers) have an (object) in them, for example, All of the bathtubs have an alligator in them.</li> </ul> </li> <li>There is an (object) in all of the (containers), for example, There is an alligator in all of the bathtubs. Each of the (objects) is in a (container), for example, Each of the alligators is in a bathtub. Each of the (containers) has an (object) in it, for example, Each of the bathtubs has an alligator in it. There is an (object) in each of the (containers), for example, There is an alligator in each of the bathtubs. Every (object) is in a (container), for example, Every alligator is in a bathtub. Every (container) has an (object) in it, for example, Every bathtub has an alligator in it. There is an (object) in every (container), for example, There is an alligator in every bathtub.</li> </ul>"},{"location":"Shortcuts%20to%20Quantifier%20Interpretation%20in%20Children%20and%20Adults/#analysis_1","title":"Analysis","text":"<ul> <li>The main effect of quantifier and all of the interactions involving quantifiers were not significant.</li> <li>Across syntactic constructions, 7-year-olds preferred the picture with the extra animals or objects as opposed to the picture with the extra containers.</li> <li>Both the 7- and 8-year-olds made correct picture selections at an above-chance level only for sentences with the universal quantifier modifying the noun correspond</li> <li>ing to the containers irrespective of the syntactic construction.</li> <li>n contrast, the majority of 9-year-olds correctly varied their picture selections in accordance with the varying syntactic constructions and performed above chance as a group for all sentence types.</li> <li>Experiment 2 replicated one of the main findings of Experiment 1: Only 9-yearolds as a group consistently identified the domain of the universal quantifier and selected the appropriate picture at above-chance levels for distributive events in which sets of objects were in partial, one-to-one correspondence.</li> <li>Rather, irrespective of the syntactic construction, they showed better performance on sentences with the quantifier modifying the containers</li> <li>The observed bias to prefer locative scenes in which all of the containers were filled (the so-called garage-centered bias) has been observed many times; see Drozd (2001) for a review</li> <li>These results are difficult to reconcile with Kang's</li> <li>Moreover, all of the age groups failed to show any effect of quantifier in Experiment 2 in contrast to Experiment 1</li> <li>That is, the children failed to show a familiarity effect with better performance for sentences with all.</li> <li>he differing results for the two experiments indicate that the collective scenes used with all in Experiment 1 were easier than the distributive ones used in Experiment 2 (see also Brinkmann et al. (1996)).</li> <li>It appears that these scenarios involving partial, one-toone correspondence pose considerable challenges for children.</li> <li>Although Experiment 2 provided no evidence that children distinguished the quantifier all from each or every, we emphasize that previous work (Brooks et al. (2001), Brooks et al. (1998)) has shown that children do readily distinguish these quantifiers on semantic grounds</li> </ul>"},{"location":"Shortcuts%20to%20Quantifier%20Interpretation%20in%20Children%20and%20Adults/#experiment-3","title":"Experiment 3","text":"<ul> <li>In Experiment 3, we examined whether adults would make errors restricting the domain of a universal quantifier in a similar picture-selection task with distributive, locative scenes.</li> <li>Testing adults is important because syntactic accounts do not readily predict errors in syntactically competent adults</li> <li>Brooks and Braine (1996, Experiment 1) tested adults with actional scenes and found no errors. Their data, however, came from 10 undergraduates at a highly selective private university (Carnegie Mellon) and thus may not be representative of adults in general.</li> <li>Here we tested monolingual English-speaking undergraduates at a highly diverse public university. This experiment constituted pilot work to establish a paradigm suitable for an eye-tracking study.</li> <li>First, we included two filler pictures along with each target and foil picture, and second, we presented sentences with the quantifier every but did not test each or all.</li> <li>Participants. We recruited 22 monolingual, adult native speakers of English (16 women, 6 men; M age = 26 years, range = 18\u201349) from introductory psychology classes at the College of Staten Island, City University of New York, who received extra credit for their participation.</li> <li>We created PowerPoint\u00ae slides comprising four pictures that were presented simultaneously (see Figure 6 for an example). These slides depicted two sets of objects in partial, one-to-one correspondence</li> <li>Each array contained two pictures similar to those used in Experiment 2 (compare Figures 5 and 6), along with two filler pictures</li> <li>We presented each sentence type six times, in randomized order, for a total of 12 trials</li> <li>To permit naming, we numbered the pictures from 1 to 4, with the position of the target randomized across trials. We used a tape recorder to record participants' responses.</li> </ul>"},{"location":"Shortcuts%20to%20Quantifier%20Interpretation%20in%20Children%20and%20Adults/#observations","title":"Observations","text":"<ul> <li>Across all participants, no errors were made on filler sentences indicating that the participants were generally compliant with the task instructions</li> <li>performance on the task was not at ceiling, with adults making errors on an average of 21% of the trials</li> <li>This error rate, which is numerically higher than the rate observed with the 9-year-olds of Experiments 1 and 2, is likely due to the added complexity of the task involving four pictures as opposed to two.</li> <li>Across the trials involving sentences with every, adult participants never selected either of the filler pictures. This indicates that their response set was effectively the same as that of the children in Experiment 2.</li> <li>Thus, unlike the children in Experiment 2, the adults did not show a preference for locative scenes in which all of the containers were filled.</li> <li>A further examination of data indicated that two adults selected the same picture on 12 of 12 trials indicating no sensitivity to the position of the quantifier in the sentence</li> <li>The fact that half of our adult participants made considerable numbers of errors in restricting every to its domain is not readily explicated by syntactic accounts positing immature syntactic representations as the source of children's quantifier-spreading errors</li> <li>Our findings that both children and adults make errors in quantifier interpretation are more readily explained by the underspecification account of Sanford and Sturt (2002).</li> <li>The crucial step involves deficient mapping from syntactic structure to a semantic representation,</li> <li>Although grammatically competent adults are capable of construing correct and fully specified semantic representations of utterances with quantifiers, it does not always happen</li> <li>The results demonstrate that many school-age children and adults had considerable difficulty in restricting the domain of a universal quantifier, especially when two sets of entities were in partial, one-to-one correspondence. This result contrasts most dramatically with the near perfect performance of preschool children in Crain et al. (1996)</li> <li>This suggests that the problem does not reside in the child's syntax, given the similarities in sentence structures used across studies, but in</li> <li>stead has to do with the difficulty of selecting the appropriate set of entities and avoiding distraction by salient objects.</li> <li>Taken together, the experiments suggest that it was the collective scenes as opposed to the use of all that improved children's performance in Experiment 1. Collective scenes were easier presumably because the group depiction aided the child in isolating one set of entities relative to the other</li> <li>Conversely, the distributive scenes were more difficult because the pictures were more visually symmetric. The observed difference in performance for collective versus distributive scenes seems to undermine syntactic accounts of children's errors given that the structures of the corresponding sentences were essentially the same.</li> <li>The fact that children's errors in Experiment 2 were not randomly distributed indicates that they noticed the extra objects and/or containers in the distributive pictures</li> <li>Again, only 9-yearolds consistently varied their picture selections in accordance with the varying syntactic constructions and did not show a strong preference for one picture configuration at the expense of the other.</li> <li>Their performance on a modified version of the sentence\u2013picture matching task was not only below ceiling, but their error rate was numerically higher than that of the 9-year-olds of Experiments 1 and 2. Note, however, that in contrast to the 7- and 8-year-olds' patterns, the adults' errors were equally distributed between the locative pictures with extra animals or objects versus extra containers.</li> <li>We suspect that both children and adults make errors in comprehension because they engage in shallow processing that causes inaccurate mapping between syntactic and semantic representations.</li> <li>In interpreting universal quantifiers, children construct underspecified representations using simpler processing strategies and then rely on pragmatics to solve the task.</li> <li>Adults also may construct underspecified representations as a first step that may or may not be followed by the application of algorithm. We speculate that adults stop at an underspecified representation when there are other demands on attention under conditions of working memory load, fatigue, or lack of cognitive effort.</li> <li>Another possibility with respect to the results of Experiment 2 is that the children may have gradually picked up on the fact that the universal quantifier modified the noun corresponding to the containers in two of three of the sentences.</li> <li>In either case, once children were fixated on a particular picture configuration, they perseverated and were reluctant to consider a competing picture as a possible alternative, even in the face of a conflicting sentence structure</li> <li>The suggestion that the children processed the sentences deterministically is not a new one.</li> <li>have indicated that children tend not to revise their initially incorrect interpretations of temporary syntactic or referential ambiguities even when disambiguating information becomes available.</li> <li>Shallow processing also provides a straightforward explanation of the errors made by adults in Experiment 3. Their high error rates suggest that adult listeners often do not tax their limited information-processing capacities by conducting exhaustive syntactic analyses of sentences but rather make use of simpler strategies in generating reasonable guesses</li> <li>These findings led Ferreira et al. (2002) to conclude that the meaning people obtain for a sentence is often not a reflection of its true content (p. 11) and that language processing often yields a merely good enough representation of a sentence's meaning</li> <li>This statement is an apt characterization of the performance of many school-age children and adults in our experiments. More generally, the comprehension of universal quantifiers seems an ideal domain for exploring the dynamics of attention allocation, and cognitive effort, in language processing.</li> </ul>"},{"location":"Shrinkage/","title":"Shrinkage","text":"<ul> <li>A hyperparameter in gradient boosting that controls overfitting. Shrinkage in gradient boosting is analogous to learning rate in gradient descent. Shrinkage is a decimal value between 0.0 and 1.0. A lower shrinkage value reduces overfitting more than a larger shrinkage value.</li> </ul>"},{"location":"ShuffleNet/","title":"ShuffleNet","text":"<ul> <li>Channel Shuffle</li> </ul> <pre><code>def channel_shuffle_new(x, groups):\n    return rearrange(x, 'b (c1 c2) h w -&gt; b (c2 c1) h w', c1=groups)\n</code></pre> <pre><code>class ShuffleUnitNew(nn.Module):\n    def __init__(self, in_channels, out_channels, groups=3, \n                 grouped_conv=True, combine='add'):\n        super().__init__()\n        first_1x1_groups = groups if grouped_conv else 1\n        bottleneck_channels = out_channels // 4\n        self.combine = combine\n        if combine == 'add':\n            # ShuffleUnit Figure 2b\n            self.left = Rearrange('...-&gt;...') # identity\n            depthwise_stride = 1\n        else:\n            # ShuffleUnit Figure 2c\n            self.left = nn.AvgPool2d(kernel_size=3, stride=2, padding=1)\n            depthwise_stride = 2\n            # ensure output of concat has the same channels as original output channels.\n            out_channels -= in_channels\n            assert out_channels &gt; 0\n\n        self.right = nn.Sequential(\n            # Use a 1x1 grouped or non-grouped convolution to reduce input channels\n            # to bottleneck channels, as in a ResNet bottleneck module.\n            conv1x1(in_channels, bottleneck_channels, groups=first_1x1_groups),\n            nn.BatchNorm2d(bottleneck_channels),\n            nn.ReLU(inplace=True),\n            # channel shuffle\n            Rearrange('b (c1 c2) h w -&gt; b (c2 c1) h w', c1=groups),\n            # 3x3 depthwise convolution followed by batch \n            conv3x3(bottleneck_channels, bottleneck_channels,\n                    stride=depthwise_stride, groups=bottleneck_channels),\n            nn.BatchNorm2d(bottleneck_channels),\n            # Use 1x1 grouped convolution to expand from \n            # bottleneck_channels to out_channels\n            conv1x1(bottleneck_channels, out_channels, groups=groups),\n            nn.BatchNorm2d(out_channels),\n        )        \n\n    def forward(self, x):\n        if self.combine == 'add':\n            combined = self.left(x) + self.right(x)\n        else:\n            combined = torch.cat([self.left(x), self.right(x)], dim=1)\n        return F.relu(combined, inplace=True)\n</code></pre>"},{"location":"Shuffled-AUC/","title":"Shuffled-AUC","text":"<ul> <li>FPR is calculated based on the negatives which are determined by fixation points of all the other images in the dataset.</li> <li>\"AUC for the curve is calculated as sAUC.\"</li> </ul>"},{"location":"Sigmoid/","title":"Sigmoid","text":"<ul> <li> \\[\\sigma(x) = \\frac{1}{1+exp(-x)}\\] </li> <li> \\[\\frac{d\\sigma}{dx}(x) = \\sigma(x)(1-\\sigma(x))\\] <ul> <li>max : 0.25</li> </ul> </li> <li>Logistic</li> <li>Xavier/Glorot init</li> <li>RNN : Hidden</li> <li>Bernoulli Distribution over a binary variable</li> <li></li> </ul>"},{"location":"Sigmoid/#backlinks","title":"Backlinks","text":"<ul> <li>DCGAN</li> <li>Final is a Sigmoid layer</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"SimCLR/","title":"SimCLR","text":"<p>title: SimCLR</p> <p>tags: architecture</p>"},{"location":"SimCLR/#simclr","title":"SimCLR","text":"<ul> <li>A Simple Framework for Contrastive Learning of Visual Representations<ul> <li>contrastive learning of visual representations</li> <li>without requiring specialized architectures or a memory bank</li> <li>composition of data augmentations plays a critical role in defining effective predictive tasks</li> <li>introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations</li> <li>contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning</li> <li>use of a nonlinear head at the end of the network, and the loss function</li> <li>Res Net</li> <li>Two separate data augmentation operators are sampled from the same family of augmentations</li> <li>applied to each data example to obtain two correlated views</li> <li>After training is completed, they throw away the projection head and use the encoder for downstream tasks</li> <li>head \\(g(\\cdot)\\)</li> <li>encoder \\(f(\\cdot)\\)</li> <li>representation \\(h\\)</li> <li></li> </ul> </li> </ul>"},{"location":"Simple%20Gradient%20Descent/","title":"Simple Gradient Descent","text":"<ul> <li> \\[\\theta = \\theta - \\eta \\cdot \\nabla_{\\theta} J(\\theta)\\] </li> <li>It starts with some coefficients, sees their cost, and searches for cost value lesser than what it is now.</li> <li>It moves towards the lower weight and updates the value of the coefficients.</li> <li>The process repeats until the local minimum is reached. A local minimum is a point beyond which it can not proceed.</li> </ul>"},{"location":"Simulations%20Of%20language/","title":"Mirman Et Al","text":"<p>Basic RNN Architectures</p> <ul> <li>Simple rnn performed like human learners<ul> <li>sensitive to transitional prob and freq</li> </ul> </li> </ul>"},{"location":"Single%20unit%20recording/","title":"Single Unit Recording","text":"<ul> <li>Authors predictions: units will either be on or off (no activation or nearly fully activated)</li> </ul>"},{"location":"Singularity/","title":"Singularity","text":"<ul> <li>A configuration where two joints of the robot arm become co-axial (aligned along a common axis). In a singular configuration, smooth path following is normally impossible and the robot may lose control. The term originates from the behavior of the Jacobian matrix, which becomes singular (i.e., has no inverse) in these configurations.</li> </ul>"},{"location":"Sketched%20Update/","title":"Sketched Update","text":"<ul> <li>Learn a full model update, then compress it before sending to the server.</li> <li>First computes the full Hit during local training without any constraints, and then approximates, or encodes, the update in a (lossy) compressed form before sending to the server. The server decodes the updates before doing the aggregation.</li> <li>Subsampling - Instead of sending Hit , each client only communicates matrix H\u0302it which is formed from a random subset of the (scaled) values of Hit.</li> <li>Quantize the weights -Improving the quantization by structured random rotations. The above 1-bit and multi-bit quantization approach work best when the scales are approximately equal across different dimensions.</li> <li>In the decoding phase, the server needs to perform the inverse rotation before aggregating all the updates.</li> </ul>"},{"location":"Sketching/","title":"Sketching","text":"<ul> <li>In unsupervised machine learning, a category of algorithms that perform a preliminary similarity analysis on examples. Sketching algorithms use a locality-sensitive hash function</li> <li>to identify points that are likely to be similar, and then group them into buckets.</li> <li>Sketching decreases the computation required for similarity calculations on large datasets. Instead of calculating similarity for every single pair of examples in the dataset, we calculate similarity only for each pair of points within each bucket.</li> </ul>"},{"location":"Skip%20Connection/","title":"Skip Connection","text":"<ul> <li> \\[x_i = F(x_{i-1}) + x_{i-1}\\] </li> <li>Effect Of Depth</li> <li>Previous layer gradient carried to next module untouched -&gt; loss surface is smoother</li> <li>Transfer #gradients to prevent Vanishingexploding gradients</li> <li>Learns the difference (residual) \\(\\(F(x) = H(x)-x\\)\\)</li> </ul>"},{"location":"Skip%20Gram/","title":"Skip Gram","text":"<ul> <li>the distributed representation of the input word is used to predict the context.</li> <li>tries to predict the neighbors of a word</li> <li>works well with a small amount of the training data, represents well even rare words or phrases.</li> <li>Skip-gram rely on single words input, it is less sensitive to overfit frequent words, because even if frequent words are presented more times that rare words during training, they still appear individually</li> <li>tends to study different contexts separately</li> <li>needs more data to be trained contains more knowledge about the context.</li> <li>takes in pairs (word1, word2) generated by moving a window across text data, and trains a 1-hidden-layer neural network based on the synthetic task of given an input word, giving us a predicted probability distribution of nearby words to the input. </li> <li>A virtual one hot encoding of words goes through a \u2018projection layer\u2019 to the hidden layer; these projection weights are later interpreted as the word embeddings. </li> <li>So if the hidden layer has 300 neurons, this network will give us 300-dimensional word embeddings.</li> <li>also uses Negative Sampling</li> </ul>"},{"location":"Skip%20Gram/#backlinks","title":"Backlinks","text":"<ul> <li>Word2Vec</li> <li>Skip Gram or CBOW</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Slice%20Based%20Volume%20Rendering/","title":"Slice Based Volume Rendering","text":"<ul> <li>assign transparency inversely proportional to the number of slices</li> <li></li> </ul>"},{"location":"Sliding%20Window%20Attention/","title":"Sliding Window Attention","text":"<ul> <li>Given the importance of local context, the sliding window attention pattern employs a fixed-size window attention surrounding each token<ul> <li>multiple stacked layers of such windowed attention results in a large receptive field, where top layers have access to all input locations and have the capacity to build representations that incorporate information across the entire input</li> <li>But a model with typical multiple stacked transformers will have a large receptive field. This is analogous to CNNs where stacking layers of small kernels leads to high level features that are built from a large portion of the input (receptive field)</li> <li>Depending on the application, it might be helpful to use different values of \\(w\\) for each layer to balance between efficiency and model representation capacity.</li> </ul> </li> <li>Given a fixed window size \\(w\\), each token attends to \\(\\frac{1}{2}w\\) tokens on each size</li> <li>Complexity is \\(O(n \\times w)\\)<ul> <li>\\(w\\) should be small compared to \\(n\\)</li> </ul> </li> <li>With \\(l\\) layers, receptive field size is \\(l \\times w\\)</li> <li></li> </ul>"},{"location":"SlimStampen/","title":"SlimStampen","text":"<ul> <li>Predicting University Students' Exam Performance Using a Model-Based Adaptive Fact-Learning System</li> </ul>"},{"location":"SlimStampen/#literature","title":"Literature","text":"<ul> <li>Digital learning systems allow learners to track their progress and make study decisions informed by data1.</li> <li>For example, Duolingo, a language- learning tool, shows learners an overview of their mastery of each lesson in a dashboard (Figure 1(a) in Settles &amp; Meeder, 2016). Rosetta Stone, another language-learning tool, has a similar dashboard and includes a suggested next study activity (Ridgeway, Mozer, &amp; Bowles, 2017).</li> <li>Adaptive learning systems take this a step further by assuming control over some study choices that might otherwise be made by learners. Using an internal model of the learner that is informed by the learner\u2019s performance, such systems can adapt the learning experience in real time (VanLehn, 2006).</li> <li>The adaptation can include changing the difficulty of the problems presented to the learner, changing the amount of feedback that the learner receives, and changing the scheduling of repetitions within and between learning sessions</li> <li>What type and degree of adaptivity are most beneficial is an empirical question and depends on whether the adaptive system accurately traces the acquisition and forgetting of knowledge over time. If implemented well, adaptive learning systems can help students achieve more effective study behaviour by facilitating spaced repetition, active study, and other effective techniques.</li> </ul>"},{"location":"SlimStampen/#slimstampen-a-model-based-adaptive-fact-learning-system-app","title":"SlimStampen: A Model-Based Adaptive Fact-Learning System App","text":"<ul> <li>Following correct answers, the next trial commenced after one second. For incorrect answers, feedback remained on the screen until the learner pressed the \u201cNext\u201d button at the bottom of the screen (see Figures 1b and 1d), making the feedback similar to the study trials.</li> </ul>"},{"location":"SlimStampen/#scheduling-algorithm","title":"Scheduling Algorithm","text":"<ul> <li>extension of the adaptive item-learning model by Pavlik and Anderson (2005; 2008) and has been tested in laboratory settings (van Rijn, van Maanen, &amp; van Woudenberg, 2009; Sense, Behrens, Meijer, &amp; van Rijn, 2016; Sense, Meijer, &amp; van Rijn, 2018) but has not been deployed in a university course before.</li> <li>his model capitalizes on the spacing effect (see Dempster, 1988, for a review) within a single session by scheduling repetitions as far apart as possible, while also</li> <li>optimizing for the testing effect (see van den Broek et al., 2016, for a review) by repeating items soon enough that most responses are correct.</li> <li>The model represents every encountered item by a unique memory chunk, based on the ACT-R theory of declarative memory (Anderson, 2007).</li> <li>Each chunk has an activation\u2014a representation of the ease with which that item could be retrieved\u2014that receives a boost whenever an item is re-encoded and that decays over time</li> <li>The activation A of a chunk i at time t, given n previous encounters at t1,\u2026,tn seconds ago, is<ul> <li> \\[A_{i}(t) = ln(\\Sigma_{j=1}^{n}t_{j}^{-d_{i}(t)}\\] </li> <li> \\[d_{i}(t)=c \\ast e^{A_{i}(t_{n-1})}+\\alpha_{i} \\] </li> </ul> </li> <li>When a new trial commences, the model determines the activation of all items 15 seconds in the future, and if the item with the lowest activation has an activation value below a retrieval threshold, that item will be scheduled for presentation</li> <li>If all predicted activations are above the retrieval threshold, the model will introduce a new item</li> <li>y selecting items on the basis of their activation, items will be repeated with as much spacing as possible, while ensuring that, theoretically, a correct response can still be given.</li> <li>The decay of the activation (parameter d in Equation (1)) varies between items to account for differences in difficulty. The higher this decay, the faster a chunk\u2019s activation will decrease, causing it to be repeated sooner than an item with a lower decay.</li> <li>The decay d of a chunk i at time t depends on the activation of the chunk at the time of its previous encounter, as well as an offset that we label the rate of forgetting, \u03b1</li> <li>The model assumes that each item has a standard initial rate of forgetting when it is first presented. However, this value is updated during learning</li> <li>At each presentation, the model calculates an expected response time, E(RT ), based on the activation at the time of the presentation (e\u2212Ai , based on Equation (5) in Anderson, Bothell, Lebiere, &amp; Matessa, 1998) and an estimated reading time of the prompt (based on the number of characters in the prompt; see Section 2.2.1 in Nijboer, 2011, for details).</li> <li>The accuracy of the response and the mismatch between expected and observed response time are used to update the value of the rate-of-forgetting parameter.</li> <li>Using both accuracy and response time to update the model allows for adjustment of the parameter estimate after any response, not just after an incorrect response.</li> <li>A correct but slower-than-expected response signals that the memory trace has decayed further than assumed, meaning that the item\u2019s true rate of forgetting is higher than the current estimate.</li> <li>That is, when a learner arrives at the right answer but takes longer than anticipated, they likely struggled to recall the information</li> <li>Conversely, an incorrect or missing response suggests that the activation of the</li> <li>item\u2019s memory trace actually dropped below the retrieval threshold, which means that the true rate of forgetting should be higher because this item\u2019s activation was expected to be above the threshold (which was fixed at ACT-R\u2019s default value).</li> <li>An unexpectedly fast correct response, on the other hand, indicates a stronger- than-expected memory trace and implies that the estimated rate of forgetting should be adjusted downward.</li> <li>Since interruption or distraction can cause disproportionately large response times, observed response times are capped before their mismatch with the expected response time is calculated.</li> <li>To update the rate of forgetting after each trial, the model uses a binary search in a small window around the previous value to identify the rate of forgetting that minimizes the mismatch between E(RT) and RT\u2032</li> </ul>"},{"location":"SlimStampen/#usage-of-the-system","title":"Usage of the System","text":"<ul> <li>Most students exhibited strong \u201ccramming\u201d behaviour, with much higher SlimStampen usage in the days leading up to the exam: in both cohorts, we observed a sharp increase in activity starting around 10 days before the exam and peaking on the last day. As the exam neared, usage intensified throughout the day and extended into the night.</li> </ul>"},{"location":"SlimStampen/#exam-performance","title":"Exam Performance","text":"<ul> <li>In both cohorts, students that used SlimStampen (92.6% of students) obtained higher grades than those that did not\u2014averaging 6.91 compared to 5.86, respectively</li> <li>a direct comparison of these groups is problematic due to selection effects and the imbalanced distributio</li> </ul>"},{"location":"SlimStampen/#amount-of-practice","title":"Amount of Practice","text":"<ul> <li>number of study trials completed was positively correlated with the final grade completing more trials was associated with higher grades on the exam</li> <li>The number of unique days on which a learner engaged with the tool\u2014an index of spaced practice\u2014was also positively correlated with exam grades (r = 0.27, t(283) = 4.81, p &lt; 0.001)</li> <li>the two measures of engagement were strongly and positively correlated (r = 0.75, t(283) = 18.78, p &lt; 0.001).</li> </ul>"},{"location":"SlimStampen/#studied-versus-non-studied-items","title":"Studied Versus Non-studied Items","text":"<ul> <li>We observed a large difference between exam questions that learners had used the system to study and questions that they had not3: students\u2019 accuracy was 83.7% on studied items but only 53.6% on unstudied items</li> <li>A mixed-effects logistic regression (with random intercepts for learners and items) confirmed that encountering an item during SlimStampen rehearsal considerably increased the chances of a correct answer on the exam (bstudied/not studied = 1.70, SE = 0.18, z = 9.06, p &lt; 0.001).</li> </ul>"},{"location":"SlimStampen/#rates-of-forgetting-and-grades","title":"Rates of Forgetting and Grades","text":"<ul> <li>The rate of forgetting, which was initially estimated for each learner\u2013item combination, was converted into a learner-specific rate of forgetting by averaging over all studied items</li> <li>The negative correlation shows that a learner who was estimated to forget material more slowly also tended to obtain higher grades.</li> <li>In practice, a possible relationship between someone\u2019s rate of forgetting and eventual exam performance would be most useful if it could be detected ahead of time rather than on the day of the exam\u2014when it is too late to potentially help struggling students, for example</li> <li>This pattern could be driven by additional learners that start at the last minute and demonstrate poor learning performance and poor grades</li> </ul>"},{"location":"SlimStampen/#predicting-performance-on-individual-exam-questions","title":"Predicting Performance on Individual Exam Questions","text":"<ul> <li>The results reported so far confirm that the expected patterns emerged in the aggregate: a learner\u2019s average rate of forgetting was strongly related to their average performance on the exam</li> <li>A step-wise backward elimination procedure was used to find the best model: starting with the full model, the term with the lowest absolute z-value was removed until the simpler model was no longer preferred on the basis of BIC and AIC (Gelman &amp; Hill, 2006).</li> <li>Additionally, the estimated rate of forgetting modulated the effect such that learner\u2013 item combinations with very low rates of forgetting have a higher chance of yielding a correct answer.</li> <li>The differences in rates of forgetting are especially pronounced at a low number of repetitions due to the non-linear mapping between the predictors and the predicted probability introduced by the logit function</li> </ul>"},{"location":"SlimStampen/#predicting-performance-on-the-exam","title":"Predicting Performance on the Exam","text":"<ul> <li>We used lasso regression (Tibshirani, 1996) to predict grades using nine predictors: a student\u2019s accuracy during study, their cohort, their cumulative usage time, the number of days on which they used the system, the number of items they studied, the number of sessions they recorded, the number of trials they completed, their estimated rate of forgetting, and their median response time</li> <li>The advantage of lasso regression is that the shrinkage term handles multicollinearity between the predictors by shrinking their coefficients</li> <li>The shrinkage is achieved by imposing a cost function on the magnitude of the</li> <li>coefficients themselves: the best fit is achieved by the model that minimizes the OLS with the smallest coefficients. In fact, coefficients are shrunk entirely if they do not explain sufficient variance to justify inclusion in the model. In lasso regression, predictors must be normalized to ensure that the shrinkage term affects all predictors equally. A convenient consequence of normalized predictors is that their post-shrinkage</li> <li>coefficients directly indicate their importance: since all predictors are on the same scale, the most important predictor retains the largest (absolute) coefficient.</li> <li>250-fold cross-validation procedure</li> </ul>"},{"location":"SlimStampen/#comparing-self-reported-and-recorded-study-times","title":"Comparing Self-Reported and Recorded Study Times","text":"<ul> <li>This means that students who used SlimStampen more did not necessarily self- report studying more overall. Thus, the positive association between more SlimStampen usage and higher grades was unlikely to be a consequence of higher motivation alone.</li> <li>This suggests, unsurprisingly, that general studiousness led to higher exam performance</li> <li>More interestingly, time spent studying with SlimStampen was time well spent, as the expected gain in grades associated with additional hours of study was 0.11 points, compared to only 0.03 points gained by an hour of unspecified study time.</li> </ul>"},{"location":"SlimStampen/#discussion","title":"Discussion","text":"<ul> <li>Students\u2019 rates of forgetting, estimated by the system during use, were correlated with exam performance up to two weeks before the exam (Figure 2), even though &lt; 5% of the data were available at that point</li> <li>Furthermore, rate-of-forgetting estimates for individual facts were predictive of learners\u2019 performance on the associated exam questions, along with the number of times these facts were repeated during study</li> <li>One limitation of the sample was that we did not know what other study methods students may have used alongside the system. It is possible that the spike in activity in the days preceding the exam was caused by students verifying that they had retained the knowledge obtained through other study activities</li> </ul>"},{"location":"SlimStampen/#implications","title":"Implications","text":"<ul> <li>controlling within-session study decisions through the adaptive fact-learning system, leaving other study decisions\u2014when to study, which chapter to study, how long to study, and whether to study with open response or multiple-choice questions\u2014to the learner</li> <li>students still made sub-optimal decisions about when to repeat a lesson that they had studied previously.</li> <li>Alternatively, the system could suggest the lesson that would yield the largest learning gain at the moment a student decides to start a session</li> </ul>"},{"location":"SlimStampen/#pictures","title":"Pictures","text":""},{"location":"Small%20World%20graphs/","title":"Small World Graphs","text":"<ul> <li>Any two nodes in the graph are connected via a smalll number of steps</li> </ul>"},{"location":"Smart%20Augmentation/","title":"Smart Augmentation","text":"<ul> <li>utilizes a similar concept as the Neural Augmentation technique</li> <li>However, the combination of images is derived exclusively from the learned parameters of a prepended CNN, rather than using the Neural Style Transfer algorithm.</li> <li>another approach to meta-learning augmentations</li> <li>This is done by having two networks, Network-A and Network-B. Network-A is an augmentation network that takes in two or more input images and maps them into a new image or images to train Network-B. The change in the error rate in Network-B is then</li> <li>backpropagated to update Network-A.</li> <li>Additionally another loss function is incorporated into Network-A to ensure that its outputs are similar to others within the class. Network-A uses a series of convolutional layers to produce the augmented image</li> <li>The conceptual framework of Network-A can be expanded to use several Networks trained in parallel. Multiple Network-As could be very useful for learning class-specific augmentations via meta-learning</li> </ul>"},{"location":"Smart%20Augmentation/#backlinks","title":"Backlinks","text":"<ul> <li>AutoAugment</li> <li> <p>much different approach to meta-learning than Neural Augmentation or Smart Augmentation</p> </li> <li> <p>[[Data Augmentation with Curriculum Learning]]</p> </li> <li>Samples taken from GANs can be augmented with traditional augmentations such as lighting filters, or even used in neural network augmentation strategies such as Smart Augmentation or Neural Augmentation to create even more samples. These samples can be fed into further GANs and dramatically increase the size of the original dataset.</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Smooth-Grad/","title":"Smooth-Grad","text":"<ul> <li>reduces visual noise and, hence, improves visual explanations about how a DNN is making a classification decision. Comparing their work to several gradient-based sensitivity map methods such as LRP, DeepLift, and Integrated Gradients (IG) [96], which estimate the global importance of each pixel and create saliency maps, showed that Smooth-Grad focuses on local sensitivity and calculates averaging maps with a smoothing effect made from several small perturbations of an input image. The effect is enhanced by further training with these noisy images and finally having an impact on the quality of sensitivity maps by sharpening them.</li> <li>a local, post hoc approach gave visual and textual justifications of the predictions with the help of two novel explanation datasets through crowd sourcing.</li> </ul>"},{"location":"Smooth-Grad/#backlinks","title":"Backlinks","text":"<ul> <li>Vision Explainibility</li> <li>Smooth-Grad</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"SmoothGrad%20Square/","title":"SmoothGrad Square","text":""},{"location":"SmoothGrad%20Square/#backlinks","title":"Backlinks","text":"<ul> <li>Vision Explainibility</li> <li>SmoothGrad Square</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"SmoothMix/","title":"SmoothMix","text":"<ul> <li>mask-based approach</li> <li>matching closely with the Cutout and CutMix techniques</li> <li>the mask has soft edges with gradually decreasing intensity</li> <li>the mixing strategy is the same</li> <li>The augmented image has mixed pixel values depending on the strength of the mask</li> <li> \\[\\lambda= \\frac{\\Sigma_{i=1}^{W}\\Sigma_{j=1}^{H}G_{ij}}{WH}\\] </li> <li>Gij is the pixel value of mask G, H is height, and W is width</li> <li>xnew = G.xa + (1 \u2212 G).xb</li> <li>ynew = \u03bb.ya + (1 \u2212 \u03bb).yb</li> </ul>"},{"location":"Smoothness/","title":"Smoothness","text":"<ul> <li>Every supervised machine learning method assumes that there's a set of functions that can transform inputs into outputs such that similar inputs are transformed into similar outputs. If an input X produces an output Y, then an input close to X would produce an output proportionally close to Y.</li> </ul>"},{"location":"SnapMix/","title":"SnapMix","text":"<ul> <li>augments training images by ex- tracting and merging random image regions of dif- ferent sizes, where the region size is drawn through the beta distribution for both the images.</li> <li>Gen- erated image label is assigned based on semantic composition from normalized (sum to one) CAMs.</li> <li>However, the summation of label coefficients can exceed beyond one depending on the semantic composition of the output image.</li> </ul>"},{"location":"Soft%20Attention/","title":"Soft Attention","text":"<ul> <li>For a simple Seq2Seq, all hidden state vectors \\(h_t\\) across timesteps are linearly combined</li> <li> \\[c_i = \\Sigma_{j=1}^T \\alpha_{ij} h_j\\] </li> <li> \\[a_{ij} = \\frac{exp(e_{ij})}{\\Sigma_{k=1}^T exp(e_{ij})}\\] </li> <li></li> </ul>"},{"location":"Softlimit%20Setting%20Function/","title":"Softlimit Setting Function","text":"<ul> <li>The Softlimit Setting Function is a function to set the axis travel limit range of the manipulator motion in software.</li> </ul>"},{"location":"Softmax/","title":"Softmax","text":"<ul> <li>Output : probabilities</li> <li> \\[p = \\frac{1}{\\Sigma_{i = 1, .., n}e^{\\frac{\\alpha y_{i}}{T}}}(e^{\\frac{\\alpha y_{1}}{T}} , e^{\\frac{\\alpha y_{2}}{T}} , \u2026, e^{\\frac{\\alpha y_{n}}{T}})'\\] </li> <li>Softer argmax (0,1)</li> <li>Multinoulli</li> </ul>"},{"location":"Softmax/#entropy","title":"Entropy","text":"<ul> <li>\\(\\alpha\\) determines entropy</li> <li>If it is 0, and Uniform Distribution and limit to infinity -&gt; binary vector which is 0 everywhere except at position i when y is maximal</li> </ul>"},{"location":"Softmax/#temperature","title":"Temperature","text":"<ul> <li>Higher the T -&gt; Softer it the distribution. Aka less confident about distribution</li> <li>Lower -&gt; Harder. More confident</li> <li></li> </ul>"},{"location":"Softmax/#backlinks","title":"Backlinks","text":"<ul> <li>Declarative Memory Blending</li> <li>Softmax</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Softplus/","title":"Softplus","text":"<ul> <li> \\[\\ln(1+e^x)\\] </li> </ul>"},{"location":"Somatic/","title":"Somatic","text":"<ul> <li>Voluntary</li> <li>Skeletal movement</li> </ul>"},{"location":"Somatosensory%20Cortex/","title":"Somatosensory Cortex","text":"<ul> <li>Located in the parietal lobe, this region of the brain processes touch, pressure, and pain information.</li> </ul>"},{"location":"Sono-stimulation/","title":"Sono-stimulation","text":"<ul> <li>The activation of neural networks using ultrasound.</li> </ul>"},{"location":"Sonogenetics/","title":"Sonogenetics","text":"<ul> <li>A novel investigative approach that turns genetically modified neurons on and off using ultrasonic waves.</li> </ul>"},{"location":"Sparse%20Dictionary%20Learning%20Loss/","title":"Sparse Dict Learning Loss","text":"<ul> <li>$\\(L(X) = n^{-1}\\Sigma_i^n ||x_i - Dr_i ||^2 + \\lambda \\Sigma_i |r_i|\\)</li> <li>\\(\\lambda \\Sigma_i |r_i|\\) is Lasso/L1 Lp Regularization</li> <li>Predictions : \\(r = argmin_r ||x- Dr_i ||^2 + \\lambda \\Sigma_i |r_i|\\)</li> </ul>"},{"location":"Sparse%20Evolutionary%20Training/","title":"Sparse Evolutionary Training","text":"<ul> <li>(Mocanu et al., 2018; Liu et al., 2021b)</li> <li>which randomly initializes the sparse connectivity between layers randomly and dynamically adjusts the sparse connectivity via a parameter prune-and-grow scheme during the course of training</li> <li>The parameter prune-and-grow scheme allows the model's sparse structure to gradually evolve, achieving better performance than naively training a static sparse network</li> </ul>"},{"location":"Sparse%20Transformer/","title":"Sparse Transformer","text":"<ul> <li>paper</li> <li>Uses Strided Attention</li> </ul>"},{"location":"Sparsity/","title":"Sparsity","text":"<ul> <li>The number of elements set to zero (or null) in a vector or matrix divided by the total number of entries in that vector or matrix. For example, consider a 10x10 matrix in which 98 cells contain zero.</li> </ul>"},{"location":"Spatial%20Transformer/","title":"Spatial Transformer","text":"<ul> <li>Transformer</li> </ul> <pre><code>class SpacialTransformNew(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        # Spatial [[Transformer|[transformer](./Transformer.md) localization-network\n        linear = nn.Linear(32, 3 * 2)\n        # Initialize the weights/bias with identity transformation\n        linear.weight.data.zero_()\n        linear.bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n\n        self.compute_theta = nn.Sequential(\n            nn.Conv2d(1, 8, kernel_size=7),\n            nn.MaxPool2d(2, stride=2),\n            nn.ReLU(True),\n            nn.Conv2d(8, 10, kernel_size=5),\n            nn.MaxPool2d(2, stride=2),\n            nn.ReLU(True),\n            Rearrange('b c h w -&gt; b (c h w)', h=3, w=3),\n            nn.Linear(10 * 3 * 3, 32),\n            nn.ReLU(True),\n            linear,\n            Rearrange('b (row col) -&gt; b row col', row=2, col=3),\n        )\n\n    # Spatial transformer network forward function\n    def stn(self, x):\n        grid = F.affine_grid(self.compute_theta(x), x.size())\n        return F.grid_sample(x, grid)\n</code></pre>"},{"location":"Speaker%20Verification/","title":"Speaker Verification","text":"<ul> <li>Deep Neural Networks for Small Footprint Text-dependent Speaker Verification</li> <li>nvestigates the use of deep neural networks (DNNs) to train speaker embeddings for a small footprint text-dependent speaker verification task</li> <li>stacked filterbank features as input</li> <li>During speaker enrollment, the trained DNN is used to extract speaker-specific features/embeddings by averaging the activations from the last hidden layer (called deep-vectors or \u201cd-vectors\u201d for short), which is taken as the speaker model</li> <li>d-vector is extracted for each utterance and compared to the enrolled speaker model to make a verification decision by calculating the cosine distance between the test d-vector and the claimed speaker\u2019s d-vector, similar to the i-vector framework</li> <li>A verification decision is made by comparing the distance to a threshold</li> <li>DNN based speaker verification system achieves good performance compared to a popular i-vector system on a small footprint text-dependent speaker verification task</li> </ul>"},{"location":"Speakers%20in%20the%20Wild/","title":"Speakers in the Wild","text":""},{"location":"SpecAugment/","title":"SpecAugment","text":"<ul> <li>SpecAugment: a Simple Data Augmentation Method for Automatic Speech Recognition</li> <li>simple data augmentation method for speech recognition</li> <li>applied directly to the feature inputs of a neural network</li> <li>warping the features, masking blocks of frequency channels, and masking blocks of time steps</li> <li>apply SpecAugment on Listen, Attend and Spell (LAS) networks for end-to-end speech recognition tasks</li> <li>LibriSpeech</li> <li>Swichboard</li> <li>end-to-end LAS networks by augmenting the training set using simple handcrafted policies</li> <li>converts ASR from an over-fitting to an under-fitting problem, and they are able to gain performance by using bigger networks and training longer</li> </ul>"},{"location":"Specificity/","title":"Specificity","text":"<ul> <li> \\[Specificity = \\frac{TN}{TN+FP}\\] </li> </ul>"},{"location":"Spectrogram/","title":"Spectrogram","text":""},{"location":"Speculative%20Execution/","title":"Speculative Execution","text":"<ul> <li>Allow the execution of complete instructions or parts of instructions before being sure whether this execution is required</li> </ul>"},{"location":"Speculum/","title":"Speculum","text":"<ul> <li>An instrument used when examining body orifices to help widen the opening</li> </ul>"},{"location":"Speech%20Emotion%20Recognition/","title":"Speech Emotion Recognition","text":"<ul> <li>GAN-based Data Generation for Speech Emotion Recognition</li> <li>form of speech emotion spectrograms</li> <li>used for training speech emotion recognition networks</li> <li>nvestigate the usage of GANs for capturing the data manifold when the data is eyes-off, i.e., where they can train networks using the data but cannot copy it from the clients</li> <li>CNN-based GAN with spectral normalization on both the generator and discriminator, both of which are pre-trained on large unlabeled speech corpora</li> <li>even after the data on the client is lost, their model can generate similar data that can be used for model bootstrapping in the future</li> </ul>"},{"location":"Speech%20Recognition/","title":"Speech Recognition","text":"<ul> <li>Recurrent Neural Network Based Language Model<ul> <li>50% reduction of Perplexity</li> <li>mixture of several Basic RNN Architectures</li> <li>Wall Street Journal task</li> <li>connectionist language models are superior to standard n gram techniques, except their high computational (training) complexity</li> <li>break the myth that language modeling is just about counting n-grams, and that the only reasonable way how to improve results is by acquiring new training dat</li> </ul> </li> <li>Towards End-To-End Speech Recognition with Recurrent Neural Networks<ul> <li>character-level speech recognition system that directly transcribes audio data with text using a recurrent neural network</li> <li>combination of the deep bidirectional LSTM recurrent neural network architecture and a modified Connectionist Temporal Classification (CTC) objective function</li> <li>word error rate</li> <li>Wall Street Journal task</li> </ul> </li> </ul>"},{"location":"Speech%20Resynthesis/","title":"Speech Resynthesis","text":"<ul> <li>Speech Resynthesis from Discrete Disentangled Self-Supervised Representations</li> <li>self-supervised discrete representations for the task of speech resynthesis</li> <li>separately extract low-bitrate representations for speech content, prosodic information, and speaker identity</li> <li>This allows to synthesize speech in a controllable manner</li> <li>evaluate the F0 reconstruction, speaker identification performance (for both resynthesis and voice conversion), recordings\u2019 intelligibility, and overall quality using subjective human evaluation</li> <li>ultra-lightweight speech codec</li> </ul>"},{"location":"Spiking%20Networks/","title":"Spiking Networks","text":"<ul> <li>Poisson Process</li> </ul>"},{"location":"Spirometer/","title":"Spirometer","text":"<ul> <li>A device that measures the amount of air breathed in and out by the lungs</li> </ul>"},{"location":"Spline%20Motion%20Type/","title":"Spline Motion Type","text":"<ul> <li>A calculated path that the robot executesthat may be parabolic in shape. A spline motion may also accomplish a free form curve with mixtures of circular and parabolic shapes.</li> </ul>"},{"location":"Spline/","title":"Spline","text":"<ul> <li>A smooth, continuous function used to approximate a set of functions that are uniquely defined on a set of sub-intervals. The approximating function and the set of functions being approximated intersect at a sufficient number of points to insure a high degree of accuracy in the approximation. The purpose for the smooth function is to allow a robot manipulator to complete a task without jerky motion.</li> </ul>"},{"location":"Square%20Integrable/","title":"Square Integrable","text":"<ul> <li>Real valued RV with PDF p is square integrable if the Uncentered Second moment is finite</li> <li>\\(\\(E[X^{2}] = \\int_\\mathbb{R} x^{2}p(x)dx\\)\\) is finite</li> </ul>"},{"location":"Squared%20Error/","title":"Squared Error","text":"<ul> <li> \\[(y- f(x))^2\\] </li> <li>Regression</li> </ul>"},{"location":"Squared%20Hinge/","title":"Squared Hinge","text":"<ul> <li>Hinge Loss</li> <li>problems involving yes/no (binary) decisions and when you\u2019re not interested in knowing how certain the classifier is about the classification</li> <li>Tanh for last layer</li> <li>maximum margin</li> </ul> \\[\\mathrm{sum}\\left( \\left( \\mathrm{max}\\left( 0, 1 - y \\cdot \u0177 \\right) \\right)^{2} \\right)\\]"},{"location":"Stack%20GAN/","title":"Stack GAN","text":"<ul> <li>Text to Image synthesis</li> <li>StackGAN decomposes the hard problem into more manageable sub-problems through a sketch-refinement process.</li> <li>The Stage-I GAN sketches the primitive shape and colors of the object based on the given text description, yielding Stage-I low-resolution images.</li> <li>The Stage-II GAN takes Stage-I results and text descriptions as inputs and generates high-resolution images with photo-realistic details. It is able to rectify defects in Stage-I results and add compelling details with the refinement process</li> <li>Multi Modal. Large no of ims that fit the given text</li> </ul>"},{"location":"Stack%20GAN/#todo","title":"todo","text":""},{"location":"Stack%20GAN/#architecture","title":"Architecture","text":"<ul> <li> <p>256\u00d7256 photo-realistic images conditioned on text descriptions. sketch-refinement process.</p> </li> <li>Conditioning Augmentation technique that encourages smoothness in the latent conditioning manifold.</li> </ul>"},{"location":"Stack%20GAN/#introduction","title":"Introduction","text":"<ul> <li>Generating photo-realistic images from text is an important problem and has tremendous applications, including photo-editing, computer-aided design, etc</li> <li>very difficult to train GAN to generate high-resolution photo-realistic images from text descriptions</li> <li>Simply adding more upsampling layers in state-ofthe-art GAN models for generating high-resolution (e.g., 256\u00d7256) images generally results in training instability</li> <li>supports of natural image distribution and implied model distribution may not overlap in high dimensional pixel space</li> <li>more severe as the image resolution increases In analogy to how human painters draw</li> <li>By conditioning on the Stage-I result and the text again, Stage-II GAN learns to capture the text information that is omitted by Stage-I GAN and draws more details for the object.</li> </ul>"},{"location":"Stack%20GAN/#conditioning-augmentation","title":"Conditioning Augmentation","text":"<ul> <li>Conditioning Augmentation technique to produce additional conditioning variables c\u02c6</li> <li>we randomly sample the latent variables c\u02c6 from an independent Gaussian distribution \\(\\mathcal{N}(\\mu(\\varphi_{t}), \\Sigma(\\varphi_{t}))\\), where the mean \\(\\mu(\\varphi_{t})\\) and diagonal covariance matrix \\(\\Sigma(\\varphi_{t})\\) are functions of the text embedding \\(\\varphi_{t}\\)</li> <li>The proposed Conditioning Augmentation yields more training pairs given a small number of imagetext pairs, and thus encourages robustness to small perturbations along the conditioning manifold</li> <li>regularization term to the objective of the generator during training \\(\\(D_{KL}(\\mathcal{N}(\\mu(\\varphi_{t}), \\Sigma(\\varphi_{t})) || \\mathcal{N}(0,I))\\)\\)</li> <li>KL Divergence between the standard Gaussian distribution and the conditioning Gaussian distribution</li> <li>The randomness introduced in the Conditioning Augmentation is beneficial for modeling text to image translation as the same sentence usually corresponds to objects with various poses and appearances.</li> </ul>"},{"location":"Stack%20GAN/#stage-i-gan","title":"Stage-I GAN","text":"<ul> <li>\\(\\varphi_{t}\\) be the text embedding of the given description</li> <li>The Gaussian conditioning variables \\(\\hat c_{0}\\) for text embedding are sampled from N(\u03bc0(\u03c6t),\u01a90(\u03c6t)) to capture the meaning of \\(\\varphi_{t}\\) with variations</li> <li>Conditioned on c\u02c60 and random variable z, Stage-I GAN trains the discriminator D0 and the generator G0 by alternatively maximizing \\(\\mathcal{L}_{D_{0}}\\) in Eq. (3) and minimizing \\(\\mathcal{L}_{G_{0}}\\)</li> <li> \\[\\mathcal{L}_{D_{0}} = \\mathbb{E}_{(I_{0},t)\\sim p_{data}}[log D_{0}(I_{0}, \\varphi_{t})]+\\mathbb{E}_{z \\sim p_{z}, t \\sim p_{data}}[log(1- D_{0}(G_{0}(z , \\hat{c_{0}}, \\varphi_{t})))]\\] </li> <li> \\[\\mathcal{L}_{G_{0}}= \\mathbb{E}_{z \\sim p_{z}, t \\sim p_{data}}[log(1- D_{0}(G_{0}(z, \\hat{c_{0}}),\\varphi_{t}))] + \\lambda D_{KL}(\\mathcal{N}(\\mu_{0}(\\varphi_{t}), \\Sigma_{0}(\\varphi_{t}))|| \\mathcal{N}(0, I))\\] </li> <li>where the real image I0 and the text description t are from the true data distribution pdata</li> <li>z is a noise vector randomly sampled from a given distribution pz (Gaussian distribution in this paper</li> <li>\u03bb is a regularization parameter that balances the two terms</li> <li>\u03bb = 1 for all the exps.</li> <li>both \\(\\mu_{0}(\\varphi_{t})\\) and \\(\\Sigma_{0}(\\varphi_{t})\\) are learned jointly with the rest of the network.</li> <li>For the generator G0, to obtain text conditioning variable c\u02c60, the text embedding \u03c6t is first fed into a fully connected layer to generate \u03bc0 and \u03c30 (\u03c30 are the values in the diagonal of \u01a90) for the Gaussian distribution N(\u03bc0(\u03c6t),\u01a90(\u03c6t)</li> <li>\u02c60 are then sampled from the Gaussian distribution c\u02c6 = \u03bc +\u03c3 \u2299\u03b5</li> <li>trained by alternatively maximizing LD in Eq. (5) and minimizing LG in Eq. (6),</li> <li>concatenated with a Nz dimensional noise vector to generate a W0 \u00d7 H0 image by a series of up-sampling blocks</li> <li>the text embedding \u03c6t is first compressed to Nd dimensions using a fully-connected layer</li> <li>and then spatially replicated to form a Md \u00d7 Md \u00d7 Nd tensor.</li> <li>the image is fed through a series of down-sampling blocks until it has Md \u00d7 Md spatial dimension</li> <li>Then, the image filter map is concatenated along the channel dimension with the text tensor</li> <li>The resulting tensor is further fed to a 1\u00d71 convolutional layer to jointly learn features across the image and the text.</li> <li>Finally, a fullyconnected layer with one node is used to produce the decision score.</li> </ul>"},{"location":"Stack%20GAN/#stage-ii-gan","title":"Stage-II GAN","text":"<ul> <li>Low-resolution images generated by Stage-I GAN usually lack vivid object parts and might contain shape distortions.</li> <li>is conditioned on low-resolution images and also the text embedding again to correct defects in Stage-I results</li> <li>The Stage-II GAN completes previously ignored text information to generate more photo-realistic details.</li> <li>Conditioning on the low-resolution result s0 = G0(z, c\u02c60) and Gaussian latent variables c\u02c6</li> <li>Different from the original GAN formulation, the random noise z is not used in this stage with the assumption that the randomness has already been preserved by s0</li> <li>Gaussian conditioning variables c\u02c6 used in this stage and c\u02c60 used in Stage-I GAN share the same pre-trained text encoder, generating the same</li> <li></li> <li>text embedding \u03c6t.</li> <li>StageI and Stage-II Conditioning Augmentation have different fully connected layers for generating different means and standard deviations</li> <li>In this way, Stage-II GAN learns to capture useful information in the text embedding that is omitted by Stage-I GAN.</li> </ul>"},{"location":"Stack%20GAN/#model-architecture","title":"Model Architecture.","text":"<ul> <li>Stage-II generator as an encoder-decoder network with residual blocks</li> <li>text embedding \u03c6t is used to generate the Ng dimensional text conditioning vector c\u02c6</li> <li>spatially replicated to form a Mg \u00d7Mg \u00d7Ng tensor</li> <li>Stage-I result s0 generated by Stage-I GAN is fed into several downsampling blocks (i.e., encoder) until it has a spatial size of Mg \u00d7 Mg</li> <li>The image features and the text features are concatenated along the channel dimension</li> <li>The encoded image features coupled with text features are fed into several residual blocks, which are designed to learn multi-modal representations across image and text feature</li> <li> <p>series of up-sampling layers</p> </li> <li> <p>are used to generate a W \u21e5H high-resolution</p> </li> <li>Such a generator is able to help rectify defects in the input image while add</li> <li>more details to generate the realistic high-resolution image.</li> <li>For the discriminator, its structure is similar to that of Stage-I discriminator with only extra down-sampling blocks since the image size is larger in this stage</li> <li>To explicitly enforce GAN to learn better alignment between the image and the conditioning text, rather than using the vanilla discriminator, we adopt the matching-aware discriminator</li> <li>During training, the discriminator takes real images and their corresponding text descriptions as positive sample pairs, whereas negative sample pairs consist of two groups</li> <li>Implementation details</li> <li>up-sampling blocks consist of the nearest-neighbor upsampling followed by a 3\u21e53 stride 1 convolution</li> <li></li> <li>Batch normalization [11] and ReLU activation are applied after every convolution except the last one</li> <li>The residual blocks consist of 3\u21e53 stride 1 convolutions, Batch normalization and ReLU. Two residual blocks are used in 128\u21e5128 StackGAN models while four are used in 256\u21e5256 models. The down-sampling blocks consist of 4\u21e54 stride 2 convolutions, Batch normalization and LeakyReLU, except that the first one does not have Batch normalization.</li> <li>Bydefault,Ng =128,Nz =100,Mg =16,Md =4, Nd = 128, W0 = H0 = 64 and W = H = 256</li> <li>For training, we first iteratively train D0 and G0 of Stage-I GAN for 600 epochs by fixing Stage-II GAN</li> <li>Then we iteratively train D and G of Stage-II GAN for another 600 epochs by fixing Stage-I GAN.</li> <li>All networks are trained using ADAM solver with batch size 64 and an initial learning rate of 0.0002. The learning rate is decayed to 1/2 of its previous value every 100 epochs.</li> </ul>"},{"location":"Stack%20GAN/#datasets-and-evaluation-metrics-cub","title":"Datasets and evaluation metrics CUB","text":"<ul> <li>Oxford-102</li> <li>MS COCO</li> <li>Evaluation metrics</li> <li>inception score</li> <li>I = exp(ExDKL(p(y|x) || p(y))),</li> <li>where x denotes one generated sample, and y is the label predicted by the Inception model</li> <li>he intuition behind this metric is that good models should generate diverse but meaningful images.</li> <li>Therefore, the KL divergence between the marginal distribution p(y) and the conditional distribution p(y|x) should be larg</li> </ul>"},{"location":"Stack%20GAN/#conclusions","title":"Conclusions","text":"<ul> <li>The proposed method decomposes the text-to-image synthesis to a novel sketch-refinement process.</li> <li></li> <li>Stage-I GAN sketches the object following basic color and shape constraints from given text descriptions. Stage-II GAN corrects the defects in Stage-I results and adds more details, yielding higher resolution images with better image quality</li> <li>Compared to existing text-to-image generative models, our method generates higher resolution images (e.g., 256\u21e5256) with more photo-realistic details and diversity. *</li> </ul>"},{"location":"Stack%20GAN/#backlinks","title":"Backlinks","text":"<ul> <li>Scalar Articles</li> <li>Stack GAN</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Stacking%20RNN/","title":"Stacking RNN","text":"<ul> <li>Deeper</li> <li>Each level -&gt; output is seq of features that is input at next set of Layers in the hierarchy</li> <li></li> </ul>"},{"location":"Staged%20Training/","title":"Staged Training","text":"<ul> <li>A tactic of training a model in a sequence of discrete stages. The goal can be either to speed up the training process, or to achieve better model quality.</li> </ul>"},{"location":"Standard%20Deviation/","title":"Standard Deviation","text":"<p>title: Standard Deviation</p> <p>tags: statistics</p>"},{"location":"Standard%20Deviation/#standard-deviation","title":"Standard Deviation","text":"<ul> <li>measure of dispersement</li> <li>how much your data is spread out around the mean</li> <li>Normal Distribution</li> <li> \\[std = \\sqrt{\\frac{\\Sigma(x-\\bar x)^{2}}{n-1}}\\] </li> </ul>"},{"location":"Stanford%20Dogs/","title":"Stanford Dogs","text":"<ul> <li>This dataset contains images of 120 breeds of dogs, with a total of 20,580 images.</li> </ul>"},{"location":"Stanford%20Dogs/#backlinks","title":"Backlinks","text":"<ul> <li>Fine Grained Datasets</li> <li>Stanford Dogs</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Static%20Friction/","title":"Static Friction","text":"<ul> <li> \\[F_{s} \\leq \\mu _{s}F_{N}\\] </li> <li>\\(F_{s}\\) is static friction</li> <li>\\(F_{N}\\) is normal force</li> <li>\\(\\mu _s\\) is coefficient of friction</li> </ul>"},{"location":"Static%20Graph%20Execution/","title":"Static Graph Execution","text":"<ul> <li>Computational Graph is first built then is executed</li> <li>less readable code and more difficult to debug</li> <li>better performance</li> <li>once compiled the graph architecture is static</li> </ul>"},{"location":"Stationarity/","title":"Stationarity","text":"<ul> <li>A property of data in a dataset, in which the data distribution stays constant across one or more dimensions. Most commonly, that dimension is time, meaning that data exhibiting stationarity doesn't change over time. For example, data that exhibits stationarity doesn't change from September to December.</li> </ul>"},{"location":"Statistical%20Word%20Segmentation/","title":"Statistical Word Segmentation","text":"<ul> <li>use mutual information between characters from a corpus</li> </ul>"},{"location":"Stem%20Cells/","title":"Stem Cells","text":"<ul> <li>Cells that have the potential to differentiate, to develop into many different specific cell types with specialized functions.</li> </ul>"},{"location":"Steve/","title":"Steve","text":"<ul> <li>Outer loop: Steve is a tutoring system that teaches hierarchical, multi-step procedures, such as how to start a large air compressor</li> <li>take Inner loop: Students steps by manipulating graphical widgets, such as clicking on a valve icon to open it or on a dipstick to check the oil level</li> <li>Steve can give immediate feedback</li> <li>Steve can also execute a step for the student. In fact, it can demonstrate the whole procedure for a student, explaining each step as it goes.</li> <li>Step analysis: Steve interprets the student's step by matching it to a set of anticipated steps. In particular, after each student step, Steve computes all possible correct next steps (usually there is just one).</li> <li>Notice that there is a one-to-one relationship between the step, the Learning Event and the Knowledge Component. This is a major contributor to the simplicity of Steve's analysis.</li> <li>Step generation: The student's most recent correct step is, by definition, part of the procedure being taught</li> <li>Steve uses immediate feedback to block incorrect steps, so Steve always knows where in the procedure the student is.</li> <li>When it needs to give hints on what to do next, it merely picks the next step. If there are multiple steps that could follow the student's most recent correct step, then Steve lists them and lets the student choose.</li> </ul>"},{"location":"Stochastic%20ensemble%20learning/","title":"Stoch Ensemble Learning","text":"<ul> <li>Stochastic algo repeatedly executed with random seeds</li> <li>Stronger the randomness -&gt; more members included -&gt; stronger reg</li> </ul>"},{"location":"Stochastic%20ensemble%20learning/#maybe-related-to","title":"Maybe related to","text":"<ul> <li>Ensemble Distillation</li> <li>Ensemble of Shape Functions</li> </ul>"},{"location":"Stratified%20Random%20Sampling/","title":"Stratified Random Sampling","text":"<ul> <li>Divides the population into relatively homogeneous groups (strata) and samples each stratum at random</li> </ul>"},{"location":"Stream%20Ribbons/","title":"Stream Ribbons","text":""},{"location":"Stream%20Surfaces/","title":"Stream Surfaces","text":""},{"location":"Streamline%20Stopping%20Criterion/","title":"Streamline Stopping Criterion","text":"<ul> <li>distance to neighboring streamline too small</li> <li>streamline leaves domain  </li> <li>after maximum number of integration steps</li> </ul>"},{"location":"Streamlines/","title":"Streamlines","text":"<ul> <li>Streamline Stopping Criterion</li> </ul>"},{"location":"Striatum/","title":"Striatum","text":"<ul> <li>A small group of subcortical structures, including the caudate nucleus, putamen, and nucleus accumbens, located in the midbrain. These regions are implicated in both movement and reward-related behaviors</li> </ul>"},{"location":"Strided%20Attention/","title":"Strided Attention","text":"<ul> <li>paper</li> <li>Sparse factorizations of the attention matrix</li> <li>Reduce to \\(O(n\\sqrt{n})\\)</li> <li>Recompute attention matrices to save memory</li> <li>Fast attention kernels</li> <li>Works nicely for images, music etc with a periodic structure</li> <li>Otherwise with the strided pattern , the spatial coordinates do not correlate with the positions the elements might be more relevant in the future</li> <li></li> </ul>"},{"location":"Strided/","title":"Strided","text":"<ul> <li>Normally S = 1</li> <li>S&gt;1 -&gt; Downsampling</li> <li>Dilated</li> <li>Spaces in the filter kernel</li> <li>D = 1 : normal Conv aka D-1 spaces</li> <li>Effective Filter size : \\(\\(\\hat F = F + (F-1)(D-1)\\)\\)</li> <li></li> </ul>"},{"location":"Strip%20Mining/","title":"Strip Mining","text":"<ul> <li>Generates code to allow vector operands whose size is less than or greater than size of vector registers</li> </ul>"},{"location":"Stroke/","title":"Stroke","text":"<ul> <li>A neurological event that occurs when the blood supply to the brain is blocked, depriving the tissue of oxygen, or when there is a bleed into the brain due to the rupturing of an artery.</li> </ul>"},{"location":"Stroop%20Task/","title":"Stroop Task","text":"<p>title: Stroop Task</p> <p>tags: brain cogneuro</p>"},{"location":"Stroop%20Task/#stroop-task","title":"Stroop Task","text":"<ul> <li>Verbal<ul> <li>The Stroop phenomenon demonstrates that it is difficult to name the ink color of a color word if there is a mismatch between ink color and word. For example, the word GREEN printed in red ink</li> </ul> </li> <li>Non Verbal<ul> <li>A series of white arrows pointing either left or right was displayed against a black background either on the left or right side of a centered fixation cross. Half of the stimuli were pointing in the same direction as their position on the screen</li> </ul> </li> </ul>"},{"location":"Structural%20Risk%20Minimization/","title":"Structural Risk Minimization","text":"<ul> <li>An algorithm that balances two goals<ul> <li>The desire to build the most predictive model (for example, lowest loss).</li> <li>The desire to keep the model as simple as possible (for example, strong regularization).</li> <li>or example, a function that minimizes loss+regularization on the training set is a structural risk minimization algorithm.</li> </ul> </li> </ul>"},{"location":"Structural%20Similarity%20Index/","title":"Structural Similarity Index","text":"<ul> <li>method for predicting the perceived quality of digital television and cinematic pictures, as well as other kinds of digital images and videos. SSIM is used for measuring the similarity between two images</li> <li> \\[\\hbox{SSIM}(x,y) = \\frac{(2\\mu_x\\mu_y + c_1)(2\\sigma_{xy} + c_2)}{(\\mu_x^2 + \\mu_y^2 + c_1)(\\sigma_x^2 + \\sigma_y^2 + c_2)}\\] </li> <li>\\(\\mu_x\\) the pixel sample mean of \\(x\\);</li> <li>\\(\\mu_y\\) the pixel sample mean of \\(y\\);</li> <li>\\(\\sigma_x^2\\) the variance of \\(x\\);</li> <li>\\(\\sigma_y^2\\) the variance of \\(y\\);</li> <li>\\(\\sigma_{xy}\\) the covariance of \\(x\\) and \\(y\\);</li> <li>\\(c_1 = (k_1L)^2\\), \\(c_2 = (k_2L)^2\\) two variables to stabilize the division with weak denominator;</li> <li>\\(L\\) the dynamic range of the pixel-values (typically this is \\(2^{\\#bits\\ per\\ pixel}-1\\));</li> <li>\\(k_1 = 0.01\\) and \\(k_2 = 0.03\\) by default.</li> </ul>"},{"location":"Structure%20Based%20Pruning/","title":"Structure Based Pruning","text":"<ul> <li>Regarding structural choices, some authors choose to prune individual parameters which produces a sparse network (lots of 0s). This might not be very ideal for storing efficiently.</li> <li>Some others consider methods where they group certain parameters and remove them as groups. This is more optimized.</li> </ul>"},{"location":"Structured%20Update/","title":"Structured Update","text":"<ul> <li>Directly learn an update from a restricted space that can be parametrized using a smaller number of variables.</li> <li>We train directly the updates of this structure</li> <li>Random mask. We restrict the update Hit to be a sparse matrix, following a pre-defined random sparsity pattern</li> </ul>"},{"location":"Style%20GAN/","title":"Style GAN","text":"<ul> <li>builds the picture layer after layer, where the layers get bigger and more accurate</li> <li>For example, the first layer is 4 by 4 pixels, the second 8 by 8, and so on</li> <li>every new layer can benefit from the less granular results of the previous ones</li> <li>better separate the generator and the discriminator, which ensures less dependence of the generator on the training set</li> <li>his allows one to, for example, reduce discrimination in the generated pictures</li> </ul>"},{"location":"Subgenual%20Cortex/","title":"Subgenual Cortex","text":"<ul> <li>The region in the back of the frontal lobes, found below the corpus callosum, which has been implicated in mood states.</li> </ul>"},{"location":"Subject%20relative/","title":"Subject Relative","text":"<ul> <li>The judge that ignored the doctor watched the movie about Colombian drug dealers</li> <li>Object relative The judge that the doctor ignored watched the movie about Colombian drug dealers.</li> <li>Subjects with higher working memory were significantly better at interpreting object relatives than subjects with lower working memory</li> </ul>"},{"location":"Subject-verb%20agreement/","title":"Subject-verb Agreement","text":"<ul> <li>John almost always reads the course papers before the lecture. I almost always forget to buy cat food.</li> <li>Relations with inflectional morphemes</li> <li>La femme est belle. L'homme est beau.</li> </ul>"},{"location":"Substantia%20Nigra/","title":"Substantia Nigra","text":"<ul> <li>This small region in the midbrain is part of the brain\u2019s reward system. In Parkinson\u2019s disease, the dopamine neurons in this region die off, leading to the disorder\u2019s movement-related and cognitive symptoms.</li> </ul>"},{"location":"Subthalamic%20Nucleus/","title":"Subthalamic Nucleus","text":"<ul> <li>A small brain structure, located in the basal ganglia, that plays an important role in coordinating movement. It is the most common target for neuromodulation techniques, like deep brain stimulation, to help diminish the symptoms of Parkinson\u2019s disease.</li> </ul>"},{"location":"Suffix/","title":"Suffix","text":"<ul> <li>follow the stem: eat / eats</li> </ul>"},{"location":"Sugar%20Factory%20Task/","title":"Sugar Factory Task","text":"<ul> <li>Berry and Broadbent ; Wallach</li> <li>Set no of workers per day</li> <li> \\[P_{t}=2W_{t}-P_{t-1}+ RandomFactor(-1/0/1)\\] <ul> <li>P: Production value</li> <li>W : No of workers</li> </ul> </li> </ul>"},{"location":"Sugar%20Factory%20Task/#backlinks","title":"Backlinks","text":"<ul> <li>Sugar Factory Task</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Sulcus/","title":"Sulcus","text":"<ul> <li>A shallower groove on the brain\u2019s cerebrum (deeper grooves are called fissures).</li> </ul>"},{"location":"Summit/","title":"Summit","text":"<ul> <li>combines two scalable tools: (1) activation aggregation discovers important neurons; (2) neuron-influenced aggregation identifies relationships among such neurons. An attribution graph that reveals and summarizes crucial neuron associations and substructures that contribute to a model's outcomes is created. Summit combines famous methods such as computing synthetic prototypes of features and showing examples from the dataset that maximize special neurons of different layers. Deeper in the graph, it is examined how the low-level features combine to create high-level features. Novel as well is that it exploits neural networks with activation atlases [63]</li> <li>This method uses feature inversion to visualize millions of activations from an image classification network to create an explorable activation atlas of features the network has learned. Their approach is able to reveal visual abstractions within a model and even high-level misunderstandings in a model that can be exploited. Activation atlases are a novel way to peer into convolutional vision networks and represents a global, hierarchical, and human-interpretable overview of concepts within the hidden layers.</li> </ul>"},{"location":"Super%20Resolution/","title":"Super Resolution","text":"<pre><code>def SuperResolutionNetNew(upscale_factor):\n    return nn.Sequential(\n        nn.Conv2d(1, 64, kernel_size=5, padding=2),\n        nn.ReLU(inplace=True),\n        nn.Conv2d(64, 64, kernel_size=3, padding=1),\n        nn.ReLU(inplace=True),\n        nn.Conv2d(64, 32, kernel_size=3, padding=1),\n        nn.ReLU(inplace=True),\n        nn.Conv2d(32, upscale_factor ** 2, kernel_size=3, padding=1),\n        Rearrange('b (h2 w2) h w -&gt; b (h h2) (w w2)', h2=upscale_factor, w2=upscale_factor),\n    )\n</code></pre>"},{"location":"SuperQuadrics/","title":"SuperQuadrics","text":""},{"location":"SuperScalar/","title":"SuperScalar","text":"<ul> <li>A superscalar CPU architecture implements Instruction level programming inside a single processor which allows faster CPU throughput at the same clock rate.</li> <li>A superscalar processor executes more than one instruction during a clock cycle</li> <li>Simultaneously dispatches multiple instructions to multiple redundant functional units built inside the processor.</li> </ul>"},{"location":"Superposition%20Catastrophe/","title":"Superposition Catastrophe","text":"<ul> <li>Bowers et al. (2014)</li> <li>Common claim of connnectionist models: they learn the best representations for a given task</li> <li>Learned representations are emergent, not stipulated</li> <li>If a PDP model learns localist codes when coding for multiple things at the same time, strongly suggests that the superposition problem pressures models to learn selective (e.g. localist) coding.</li> <li>Recurrent network</li> <li>Simple task, given vocabulary of 30 words</li> <li>Banding/selective responses do not appear with distributed letter coding when chance of ambiguity is null</li> <li>This means: when ambiguity/superposition catastrophe is very possible, hidden units learn selective responses</li> <li>Selective responses 'emerge' as a response to the potential for superposition catastrophe</li> <li>Recurrent networks trained to store multiple things at the same time over the same set of units learn highly selective (localist) representations</li> </ul>"},{"location":"Suppletion/","title":"Suppletion","text":"<ul> <li>word is completely replaced by something that has no connection at the surface label</li> <li>Go to went.</li> <li>Good to better</li> </ul>"},{"location":"Swichboard/","title":"Swichboard","text":""},{"location":"Swin%20Transformer/","title":"Swin Transformer","text":"<p>title: Swin Transformer</p> <p>tags: architecture</p>"},{"location":"Swin%20Transformer/#swin-transformer","title":"Swin Transformer","text":"<ul> <li>Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows<ul> <li>Vision Transformer</li> <li>general-purpose backbone for computer vision</li> <li>hierarchical feature representation</li> <li>linear computational complexity with respect to input image size</li> <li>shifted window based Self Attention</li> <li>address the challenges in adapting Transformer from language to vision</li> <li>limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection</li> <li>flexibility to model at various scales</li> <li>linear computational complexity with respect to image size</li> <li>ImageNet</li> <li>COCO</li> <li>ADE20K</li> <li>The hierarchical design and the shifted window approach also prove beneficial for all Perception Architectures.</li> <li>Ratio of 1:1:3:1</li> </ul> </li> </ul>"},{"location":"Swish/","title":"Swish","text":"<ul> <li> \\[x\\cdot sigmoid(x)\\] </li> <li>While Swish reportedly improves model performance (Ramachandran et al., 2017), it still does not allow you to avoid Vanishingexploding gradients</li> <li>Even though the vanishing gradients problem is much less severe in case of Swish, only inputs of \\(x &gt;= 2\\) result in gradients of 1 and (sometimes) higher. In any other case, the gradient will still cause the chain to get smaller with increasing layers.</li> <li>Move to Lisht</li> <li>non monotonic</li> <li>First, it is bounded below. Swish therefore benefits from sparsity similar to ReLU. Very negative weights are simply zeroed out.</li> <li>Second, it is unbounded above. This means that for very large values, the outputs do not saturate to the maximum value (i.e., to 1 for all the neurons). According to the authors of the Swish paper, this is what set ReLU apart from the more traditional activation functions.</li> <li>Third, separating Swish from ReLU, the fact that it is a smooth curve means that its output landscape will be smooth. This provides benefits when optimizing the model in terms of convergence towards the minimum loss.</li> <li>Fourth, small negative values are zeroed out in ReLU (since f(x) = 0 for x &lt; 0). However, those negative values may still be relevant for capturing patterns underlying the data, whereas large negative values may be zeroed out (for reasons of sparsity, as we saw above). The smoothness property and the values of f(x) &lt; 0 for x \u2248 0 yield this benefit. This is a clear win over ReLU.</li> <li></li> </ul>"},{"location":"Symbolic%20learning%20model/","title":"Symbolic Learning Model","text":"<ul> <li>Verb tokens are used instead of verb types.</li> <li>No sharp discontinuities in the supply of regular and irregular verb tokens in 'parental speech'</li> <li>Verb tokens sampled randomly with replacement according to the Francis-Kucera frequency estimates for English verbs</li> </ul>"},{"location":"Symbolic%20models/","title":"Symbolic Models","text":"<ul> <li>Code letters separately from order</li> <li>This type of model does make transposition errors \u25ee Can because B is a 'thing' on its own, separate from order</li> <li>Botvinick, M. M., &amp; Plaut, D. C. (2006). Short-term memory for serial order: a recurrent neural network model. Psychological review, 113(2), 201.</li> <li>Model could later correctly reproduce novel sequences</li> <li>But all letters had occurred in each position</li> </ul>"},{"location":"Symmetries%20Node%20Link/","title":"Symmetries Node Link","text":""},{"location":"Sympathetic/","title":"Sympathetic","text":"<ul> <li>Mobilizes body into action</li> </ul>"},{"location":"Synaptic%20Pruning/","title":"Synaptic Pruning","text":"<ul> <li>A process by which specialized cells called microglia eliminate unnecessary synapses as part of normal and healthy brain development.</li> </ul>"},{"location":"Synaptic%20Transmission/","title":"Synaptic Transmission","text":"<ul> <li>The process of nerve-to-nerve communication in the central nervous system, whereby one neuron sends a chemical signal across the synaptic cleft to another neuron.</li> </ul>"},{"location":"Syntactic%20Ambiguity/","title":"Syntactic Ambiguity","text":"<ul> <li>Flying plane is very dangerous</li> <li>The man saw a boy with binoculars</li> <li>The teacher wears sunglasses</li> <li>Becase students are bright</li> </ul>"},{"location":"Syntactic%20Analysis/","title":"Syntactic Analysis","text":"<ul> <li>concerned with the construction of sentences.</li> <li>Syntactic structure indicates how the words are related to each other</li> <li>Syntax tree is assigned by a grammer and a Lexicon</li> <li>Context Free Grammar</li> </ul>"},{"location":"Syntactic%20Bootstrapping/","title":"Syntactic Bootstrapping","text":"<ul> <li>\"John wugged Mary yesterday\" vs \"John wugged Marry\"</li> </ul>"},{"location":"Syntax%20First%20models/","title":"Syntax First Models","text":"<ul> <li>Syntax-first models (e.g., Ferreira &amp; Clifton, 1986; Frazier &amp; Clifton, 1996) have traditionally proposed that, at a point of syntactic ambiguity, syntactic heuristics alone select a single structure to pursue</li> <li>recovery from a misanalysis is achieved via a separate reanalysis mechanism that uses semantic and contextual information</li> <li>propose that only one representation is active at any given time and that nonsyntactic information only influences interpretation at a later reanalysis stage.</li> </ul>"},{"location":"S%C3%B8rensen-Dice%20Index/","title":"S\u00f8rensen-Dice Index","text":"<ul> <li>very similar to Jaccard index</li> <li>Although they are calculated similarly the S\u00f8rensen-Dice index is a bit more intuitive because it can be seen as the percentage of overlap between two sets, which is a value between 0 and 1</li> <li>overstate the importance of sets with little to no ground truth positive sets</li> <li>As a result, it could dominate the average score taken over multiple sets</li> <li>It weights each item inversely proportionally to the size of the relevant set rather than treating them equally.</li> <li>$\\(D(x,y) = \\frac{2|x\\cap y|}{|x|+|y|}\\)</li> <li></li> </ul>"},{"location":"TIghtly%20coupled/","title":"TIghtly Coupled","text":"<ul> <li>UMA</li> <li>NUMA</li> </ul>"},{"location":"TIme%20Series/","title":"Time Series Prediction","text":"<ul> <li>Task:<ul> <li>discrete time, equidistant timesteps</li> <li>Approximate teacher signal</li> <li>Input signal : \\(\\(u(t)_{t \\in \\mathcal{T}}\\)\\)</li> <li>Desired output : \\(\\((y(t))_{t \\in \\mathcal{T}} = u(t+h))_{t \\in \\mathcal{T}}\\)\\)</li> <li>Dynamical system state : \\(\\(x(t) \\in \\mathbb{R}^n\\)\\)</li> <li>Temporal evolution governed by<ul> <li>State update map<ul> <li>Governs how \\(\\(x(t)\\)\\) develops over time</li> <li> \\[x(t+1) = f(x(t) , u(t+1))\\] </li> </ul> </li> <li>Observation function<ul> <li>What output can be observed when in state \\(\\(x(t)\\)\\)</li> <li> \\[y(t) = g(x(t))\\] </li> </ul> </li> </ul> </li> </ul> </li> <li>Short range<ul> <li>PDE , ODE</li> </ul> </li> <li>Long range<ul> <li>HMM</li> <li>Recurrent</li> </ul> </li> </ul>"},{"location":"TO%20LOOK%20AT/","title":"To Look at","text":"<ul> <li>http://karpathy.github.io/2019/04/25/recipe/</li> <li>https://arxiv.org/abs/1311.2901</li> <li>http://torch.ch/blog/2015/09/07/spatial_transformers.html</li> <li>http://dpmd.ai/Ithaca-blog</li> <li>https://notesonai.com/Layer+Normalization</li> </ul>"},{"location":"TPU%20Node/","title":"TPU Node","text":"<ul> <li>A TPU resource on Google Cloud Platform with a specific TPU type.</li> </ul>"},{"location":"TPU%20Pod/","title":"TPU Pod","text":"<ul> <li>A specific configuration of TPU devices in a Google data center.</li> </ul>"},{"location":"TPU%20Slice/","title":"TPU Slice","text":"<ul> <li>A TPU slice is a fractional portion of the TPU devices in a TPU Pod</li> </ul>"},{"location":"TREEQN/","title":"TREEQN","text":"<pre><code>def transition(zl):\n  # -- [batch_size x num_actions x hidden_dimension]\n  return zl.unsqueeze(1) + F.tanh(torch.einsum(\"bk,aki-&gt;bai\", [zl, W]) + b)\n</code></pre>"},{"location":"TREPAN/","title":"TREPAN","text":"<ul> <li>or DeepRED</li> <li>TREPAN is an algorithm for extracting comprehensible, symbolic representations from trained neural networks</li> <li>The authors demonstrated that TREPAN is able to produce Decision Trees that are accurate and comprehensible and maintain a high level of fidelity to the networks from which they were extracted.</li> <li>According to the authors of DeepRED, their method is the first attempt to extract rules and make a DNN's decision more transparent.</li> </ul>"},{"location":"TSDF/","title":"TSDF","text":"<ul> <li>Truncated Signed Distance Function</li> </ul>"},{"location":"Tacotron/","title":"Tacotron","text":"<ul> <li>CBHG<ul> <li>[[Conv](Gated Recurrent Unit (GRU|Gated Recurrent Unit (GRU|Conv]].md)</li> </ul> </li> </ul> <pre><code>class CBHG_Old(nn.Module):\n    \"\"\"CBHG module: a [recurrent](./Recurrent.md) neural network composed of:\n        - 1-d convolution banks\n        - Highway networks + residual connections\n        - Bidirectional gated [recurrent](./Recurrent.md) units\n    \"\"\"\n\n    def __init__(self, in_dim, K=16, projections=[128, 128]):\n        super(CBHG, self).__init__()\n        self.in_dim = in_dim\n        self.relu = nn.ReLU()\n        self.conv1d_banks = nn.ModuleList(\n            [BatchNormConv1d(in_dim, in_dim, kernel_size=k, stride=1,\n                             padding=k // 2, activation=self.relu)\n             for k in range(1, K + 1)])\n        self.max_pool1d = nn.MaxPool1d(kernel_size=2, stride=1, padding=1)\n\n        in_sizes = [K * in_dim] + projections[:-1]\n        activations = [self.relu] * (len(projections) - 1) + [None]\n        self.conv1d_projections = nn.ModuleList(\n            [BatchNormConv1d(in_size, out_size, kernel_size=3, stride=1,\n                             padding=1, activation=ac)\n             for (in_size, out_size, ac) in zip(\n                 in_sizes, projections, activations)])\n\n        self.pre_highway = nn.Linear(projections[-1], in_dim, bias=False)\n        self.highways = nn.ModuleList(\n            [Highway(in_dim, in_dim) for _ in range(4)])\n\n        self.gru = nn.GRU(\n            in_dim, in_dim, 1, batch_first=True, bidirectional=True)\n\ndef forward_new(self, inputs, input_lengths=None):\n    x = rearrange(inputs, 'b t c -&gt; b c t')\n    _, _, T = x.shape\n    # Concat conv1d bank outputs\n    x = rearrange([conv1d(x)[:, :, :T] for conv1d in self.conv1d_banks], \n                 'bank b c t -&gt; b (bank c) t', c=self.in_dim)\n    x = self.max_pool1d(x)[:, :, :T]\n\n    for conv1d in self.conv1d_projections:\n        x = conv1d(x)\n    x = rearrange(x, 'b c t -&gt; b t c')\n    if x.size(-1) != self.in_dim:\n        x = self.pre_highway(x)\n\n    # Residual connection\n    x += inputs\n    for highway in self.highways:\n        x = highway(x)\n\n    # (B, T_in, in_dim*2)\n    outputs, _ = self.gru(self.highways(x))\n\n    return outputs\n</code></pre>"},{"location":"Taking%20on%20semantic%20commitments%2C%20II%20collective%20versus%20distributive%20readings/","title":"Taking on Semantic Commitments, II Collective Versus Distributive Readings","text":"<ul> <li>Lyn Frazier, Jeremy M. Pacht, Keith Raynerb</li> </ul> <ul> <li>Minimal Semantic Commitment</li> <li>Given ambiguous representations, the MSC hypothesis predicts that the processor will commit to one interpretation</li> <li>In an experiment designed to evaluate these hypotheses with respect to the representation of distributivity, participants' eye movements were recorded as they read sentences containing distributive or collective predicates that were either disambiguated by a preceding adverb or left locally ambiguous by delaying the disambiguating adverb until the end of the predicate</li> <li>The results suggested that a semantic commitment is made in locally indeterminate cases as evidenced by a significant interaction of ambiguity and distributivity in first pass times, total times, and regressions</li> <li>Hence we argue that the distributive/collective distinction is treated as a matter of ambiguity rather than as one of vagueness</li> <li>In the absence of evidence for a distributive reading, the processor commits itself to a collective reading sometime during the processing of the predicate (before the disambiguation in our late disambiguation examples)</li> <li>By the MSC hypothesis, this will predict that a premature decision, one made early on the basis of little information, should not occur during immediate processing</li> <li>The point is that no commitment will be made in the absence of supporting evidence</li> <li>By contrast, given the MSC hypothesis, the notion that a string is grammatically ambiguous predicts that the processor encounters a choice point on initial analysis, adopting one representation rather than another</li> <li>In our experiment we only examined collective/distributive subjects in single clause sentences</li> <li>The ambiguity hypothesis predicts an interaction</li> <li>the vagueness hypothesis doesn't</li> <li>T</li> <li>In order to test for possible pragmatic biases in the predicate, participants were asked to rate the 'naturalness' of the locally ambiguous collective or distributive predicates contained in the experimental sentences</li> <li>when the actual conjoined NP subject in the experimental materials was replaced by a pronominal subject</li> <li>Thirty-two students native English speakers</li> <li>Each version of the questionnaire contained eight sentences in which the locally ambiguous predicate was subsequently disambiguated towards a distributive interpretation using each and eight sentences in which the predicate was subsequently disambiguated towards a collective interpretation using together</li> <li>7-point scale</li> <li>They were told that there were no right or wrong answers and that we were only interested in their opinions of the naturalness of the sentences</li> <li>another rating study was conducted using truncated versions of the locally ambiguous items such that each item ended after the word each or together</li> <li>examined first pass reading times, total reading times, and the pattern of regressions for different regions of the target sentences</li> <li>Sixty undergraduate students bite bar</li> </ul>"},{"location":"Taking%20on%20semantic%20commitments%2C%20II%20collective%20versus%20distributive%20readings/#experiment","title":"Experiment","text":"<ul> <li>dealt with where readers look during reading and that they should read each sentence for comprehension</li> <li>Sixteen experimental sentences were embedded in 107 filler sentences</li> <li>Each sentence appeared in one of four versions, as in (7) above: in two versions (a and c), the predicate received a distributive interpretation, and in the other two (b and d) the predicate received a collective interpretation</li> <li>region</li> <li>he first region consisted of the words preceding the predicate (a conjoined NP)</li> <li>the second region consisted of the predicate itself</li> <li>the third region comprised the next three words, or next two words if the third word was the last word in the sentence</li> <li>and the fourth region was the remainder of the sentence</li> <li>The fourth region was created solely so that results from the third region would be free of sentence wrapup effects</li> <li>In the first region, none of the effects were significant</li> <li>In the second region, there was an effect of ambiguity, wherein the locally ambiguous versions were read faster than unambiguous versions, possibly due to a preference for adverbs to appear in verb-phrase final position</li> <li>he raw reading times analyses and analyses of residuals were consistent in</li> <li>indicating that the interaction did not approach significance</li> <li>In the third region, the different versions of any given sentence were once again identical</li> <li>Here, locally ambiguous versions were read somewhat slower overall than unambiguous versions</li> <li>However, this difference failed to approach significance in the subjects analysis (F1(1,59) = 1</li> <li>91, P \u0000 0</li> <li>16) and was only marginally significant by items</li> <li>Crucially, each of these marginally significant main effects was qualified by a highly robust interaction between ambiguity and predicate type, suggesting that distributive predicates were read more slowly than collective predicates in the locally ambiguous versions</li> <li>Pairwise comparisons confirmed that while distributives were read more slowly in the ambiguous versions (F1(1,59) = 10</li> <li>84, P \u0000 0</li> <li>01</li> <li>F2(1,14) =20</li> <li>48, P \u0000 0</li> <li>001), there was no reliable predicate effect in the unambiguous versions</li> <li>The results of this analysis for first pass times revealed striking differences between ambiguous and unambiguous forms</li> <li>In the residuals analyses, distributives (each) were read slower than collectives (together) in both ambiguous and unambiguous forms</li> <li>However, the effect was much larger in ambiguous forms than unambiguous forms (223 ms vs</li> <li>43 ms), and while in ambiguous forms the effect was significan</li> <li>Distributives were significantly faster than collectives in unambiguous forms (means: 844 ms vs</li> <li>925 ms</li> <li>F1(1,59) = 4</li> <li>72, P \u0000 0</li> <li>05, F2 (1,14) = 10</li> <li>39, P \u0000 0</li> <li>01), but significantly slower than collectives in ambiguous forms</li> <li>n the third region, where the different versions were again identical, there was a main effect of ambiguity, with ambiguous versions being read slower overall than unambiguous versions</li> <li>There was also a robust predicate effect (F1(1,59) = 9</li> <li>53, P \u0000 0</li> <li>01</li> <li>F2(1,14) = 10</li> <li>88, P \u0000 0</li> <li>01), which indicated that distributives were read more slowly overall</li> <li>In the second region, there were no significant differences across conditions in the percentages of trials on which regressions occurred out of the region (Fs \u0000 1)</li> <li>In the third region, the mean percentage of trials on which regressions occurred out of the region was 19%, 8%, 4% and 4% for the ambiguous-distributive, ambiguouscollective, unambiguous-distributive, and unambiguous-collective conditions, respectively</li> <li>There were significantly more trials involving regressions out of the third region in the ambiguous than the unambiguous versions</li> <li>The predictions of the vagueness hypothesis were clearly disconfirmed</li> <li>Given the MSC hypothesis, the vagueness hypothesis predicts no interaction between ambiguity and sentence form: in both the ambiguous distributive (7a) and the unambiguous distributive (7c) the processor should postulate a distributive operator when each is encountered</li> <li>ounter to this prediction, the ambiguous distributive form was substantially more difficult to understand than the other sentence forms</li> <li>This may be seen in the significant interaction of ambiguity and sentence form in first pass and total times in region three, as well as in the regressions out of region three and regressions into region two</li> <li>Readers clearly exhibited a preference for the collective reading of the ambiguous portion of the sentences in our experiment</li> <li>These results make it difficult to maintain the assumptions needed to salvage the vagueness hypothesis</li> <li>Instead, given the MSC hypothesis, they support the assumption that the correct grammatical account of collective/distributive differences treats the distinction as one of ambiguity rather than as one of vagueness, at least in cases like those tested, where subject-predicate relations are involved</li> <li>We turn now to alternative interpretations of our results</li> <li>The question is whether the results can be attributed directly to the complexity of the distributive reading</li> <li>We think not</li> <li>It may be true that distributive readings, even unambiguous ones, are slightly more complex than collective readings</li> <li>This suggests that readers may not simply add information to the current representation of these locally ambiguous forms when each is encountered</li> <li>Similarly the results are difficult to reconcile with a parallel processing view unless the processor has computed both a collective and a distributive representation and then abandoned the distributive representation before each is encountered</li> </ul>"},{"location":"Tanh/","title":"Tanh","text":"<ul> <li> \\[\\frac{e^x-e^{-x}}{e^x+e^{-x}}\\] </li> <li>RNN : Hidden</li> <li>Xavier/Glorot init</li> <li></li> </ul>"},{"location":"Tanh/#backlinks","title":"Backlinks","text":"<ul> <li>DCGAN</li> <li>Output passed through Tanh to return it to [-1,1]</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Task%20%28endeffector%29%20Space%20Vs%20Joint%20Space/","title":"Task (endeffector) Space Vs. Joint Space","text":""},{"location":"Task%20%28endeffector%29%20Space%20Vs%20Joint%20Space/#task-endeffector-space-vs-joint-space","title":"Task (endeffector) Space Vs. Joint Space","text":""},{"location":"Tau%20Protein/","title":"Tau Protein","text":"<ul> <li>A type of protein abundantly found in neurons. When this protein is not adequately cleared from the brain, it can form tangles that are a key pathology of several neurodegenerative disorders including frontotemporal degeneration, CTE, and Alzheimer\u2019s disease.</li> </ul>"},{"location":"Teach%20Lock/","title":"Teach Lock","text":"<ul> <li>While the Teach Lock is set, the mode of operation is tied to the Teach Mode and the machines cannot be played back using either START to \u201cTEACH\u201d before beginning to teach.</li> </ul>"},{"location":"Teacher%20Forcing/","title":"Teacher Forcing","text":"<ul> <li>from</li> <li>Technique where the target word (ground truth word) is passed as the next input to the decoder instead of its last prediction.</li> <li>common technique to train\u00a0Basic RNN Architectures\u00a0or\u00a0Transformer<ul> <li>used in imageCaptioning\u00a0, Machine Translation</li> <li>but also in Time Series forecasting</li> </ul> </li> <li>intuition<ul> <li>math exam with dependent questions, e.g. a) depends on b), b) on c) and so on</li> <li>if a) is wrong, all subsequent questions are also wrong</li> <li>teacher forcing: after answering question a), the teacher compares it to the correct solution and grades it and then gives us the correct answer for a) to continue with</li> </ul> </li> <li>for example in sequence generation with\u00a0RNN\u00a0the situation is similar<ul> <li>each prediction depends on the last one, thus when one is wrong all subsequent will be wrong as well</li> </ul> </li> <li>no memorization can happen<ul> <li>the network can not look into the future</li> <li>ground truth is only fed as last \\(y_{t-1}\\) prediction not as the current \\(y_{t}\\)</li> </ul> </li> <li>loss does not need to be updated at each timestep, only needs to have a list with the true predictions of the model from which then the loss is calculated</li> <li>pros<ul> <li>training converges faster, because early predictions are very bad</li> </ul> </li> <li>cons<ul> <li>no ground truth label during inference, thus no teacher forcing</li> <li>discrepancy between training and inference scores<ul> <li>can lead to poor model performance and instability</li> <li>known as\u00a0Exposure Bias</li> </ul> </li> </ul> </li> </ul>"},{"location":"Teacher%20Student%20Architecture/","title":"Teacher Student Architecture","text":"<ul> <li>The gap is further reduced by residual learning, i.e., the assistant structure is used to learn the residual error (Gao et al., 2021).</li> </ul>"},{"location":"Telomere/","title":"Telomere","text":"<ul> <li>The protective cap found at the end of a chromosome. Research studies suggest these caps may be shortened in neurodegenerative diseases.</li> </ul>"},{"location":"Temporal%20Conv/","title":"Temporal Conv","text":"<ul> <li>FCN + Causal 1D Conv + Residual</li> <li>Outperforms Basic RNN Architectures such as Long Short Term Memory (LSTM)).md) and Gated Recurrent Unit (GRU)).md)</li> </ul>"},{"location":"Temporal%20lobe/","title":"Temporal Lobe","text":"<ul> <li>Understanding language (Wernicke Area)</li> <li>Memory</li> <li>Hearing</li> <li>Sequencing and organization</li> <li>Long-term memory is processed in the hippocampus of the temporal lobe and is activated when you want to memorize something for a longer time.</li> </ul>"},{"location":"TemporalLearning/","title":"Temporal Learning","text":"<ul> <li>Recurrent</li> <li>Online Learning</li> <li>Causal Systems</li> </ul>"},{"location":"Tensor%20Processing%20Unit/","title":"Tensor Processing Unit","text":"<ul> <li>An application-specific integrated circuit (ASIC) that optimizes the performance of machine learning workloads.</li> </ul>"},{"location":"Test-time%20Augmentation/","title":"Test-time Augmentation","text":"<ul> <li>augmenting data at test-time as well</li> <li>This can be seen as analogous to ensemble learning techniques in the data space.</li> <li>By taking a test image and augmenting it in the same way as the training images, a more robust prediction can be derived.</li> <li>restrict the speed of the model</li> <li>promising practice for applications such as medical image diagnosis</li> <li>Radosavovic et al. denote test-time augmentation as data distillation to describe the use of ensembled predictions to get a better representation of the image.</li> <li>They also found better uncertainty estimation when using test-time augmentation, reducing highly confident but incorrect predictions.</li> <li>Matsunaga et al. also demonstrate the effectiveness of test-time augmentation on skin lesion classification, using geometric transformations such as rotation, translation, scaling, and Flipping.</li> <li>A robust classifier is thus defined as having a low variance in predictions across augmentations</li> <li>In their experiments searching for augmentations with Reinforcement Learning, Minh et al. measure robustness by distorting test images with a 50% probability and contrasting the accuracy on un-augmented data with the augmented data.</li> <li>Some classification models lie on the fence in terms of their necessity for speed. This suggests promise in developing methods that incrementally upgrade the confidence of prediction. This could be done by first outputting a prediction with little or no testtime augmentation and then incrementally adding test-time augmentations to increase the confidence of the prediction.</li> <li>However, it is difficult to aggregate predictions on geometrically transformed images in object detection and semantic segmentation. Curriculum learning</li> <li>strategy for selecting training data that beats random selection</li> <li>best to initially train with the original data only and then finish training with the original and augmented data, although there is no clear consensus</li> </ul>"},{"location":"Text%20Normalization/","title":"Text Normalization","text":"<ul> <li>Merging di\ufb00erent written forms of a token into a canonical normalized form</li> </ul>"},{"location":"Text%20Preprocessing/","title":"Text Preprocessing","text":"<ul> <li>Document Triage</li> <li>Text Segmentation</li> </ul>"},{"location":"Text%20Segmentation/","title":"Text Segmentation","text":"<ul> <li>Word Segmentation</li> <li>Text Normalization</li> <li>Sentence Segmentation</li> <li>Character-set dependence</li> <li>Language dependence</li> <li>Corpus dependence</li> <li>Application dependence</li> </ul>"},{"location":"Textless%20Speech%20Emotion%20Conversion/","title":"Textless Speech Emotion Conversion","text":"<ul> <li>Textless Speech Emotion Conversion Using Discrete and Decomposed Representations</li> <li>Speech emotion conversion</li> <li>modifying the perceived emotion of a speech utterance while preserving the lexical content and speaker identity</li> <li>spoken language translation task</li> <li>decomposition of the speech signal into discrete learned representations, consisting of phonetic-content units, prosodic features, speaker, and emotion</li> <li>modify the speech content by translating the phonetic-content units to a target emotion, and then predict the prosodic features based on these units</li> <li>speech waveform is generated by feeding the predicted representations into a neural vocoder</li> <li>beyond spectral and parametric changes of the signal</li> <li>model non-verbal vocalizations, such as laughter insertion, yawning removal, etc</li> </ul>"},{"location":"Thalamus/","title":"Thalamus","text":"<ul> <li>Serves as a relay station for almost all information that comes and goes to the cortex.</li> <li>It plays a role in pain sensation, attention, alertness and memory.</li> </ul>"},{"location":"The%20Behavior%20of%20Tutoring%20Systems/","title":"The Behavior of Tutoring Systems","text":"<ul> <li>Kurt VanLehn</li> </ul>"},{"location":"The%20Behavior%20of%20Tutoring%20Systems/#intro","title":"Intro","text":"<ul> <li>Tutoring systems are described as having two loops.</li> <li>The outer loop executes once for each task, where a task usually consists of solving a complex, multi-step problem</li> <li>The inner loop executes once for each step taken by the student in the solution of a task. The inner loop can give feedback and hints on each step. The inner loop can also assess the student's evolving competence and update a student model, which is used by the outer loop to select a next task that is appropriate for the student.</li> <li>A task usually takes several minutes to an hour or so. Most tutors assume that the student is working alone on a task, but some (e.g., Zachary et al., 1999) have the student work as one member of a multi-student team.</li> <li>The tasks and the tutor's user interface are usually designed so that completing a task requires multiple steps, where a step is a user interface action that the student takes in order to achieve a task.</li> <li>Let us use knowledge for the information possessed by students that determines their behavior on task. Let us also assume that knowledge can be decomposed, and let us use the term Knowledge Component for the units into which it is decomposed.</li> <li>Knowledge Component</li> <li>Learning Event</li> <li>Modeling Transfer</li> <li>Predicting Student learning Curve</li> <li>Tutor</li> </ul>"},{"location":"The%20Behavior%20of%20Tutoring%20Systems/#the-outer-loop","title":"THE OUTER LOOP","text":"<ul> <li>The main responsibility of the outer loop is to decide which task the student should do next. Its other responsibilities, such as presenting the task to the student, are more mundane and will not be mentioned again. The main design issues are (1) selecting a task intelligently and (2) obtaining a rich set of tasks to select from.</li> </ul>"},{"location":"The%20Behavior%20of%20Tutoring%20Systems/#task-selection","title":"Task Selection","text":"<ul> <li>The outer loop displays a menu and lets the student select the next task.</li> <li>The outer loop assigns tasks in a fixed sequence</li> <li>Mastery learning</li> <li>Macroadaptation</li> <li>In addition to selecting a task, some tutoring systems also select a mode for the task. For instance, Steve can either (1) demonstrate how to do the task by taking each step itself and explaining it, or (2) hint each step before the student attempts it, or (3) let the student try to solve the task without such unsolicited hints.</li> <li>In addition to providing some kind of mechanism for selecting tasks, the designer must provide a set of tasks for the outer loop to select from. It is often surprising how many problems are necessary in order to field a tutoring system. For a 13 week semester at 10 problems per week, the tutoring system needs 130 problems at minimum</li> <li>Ideally, the tutor can generate its own tasks when given a specification of the desired characteristics.</li> <li>For instance, given a specification of a Knowledge Component to be taught, the SQL- Tutor (Martin &amp; Mitrovic, 2002) generates a database query, written in SQL, that involves the Knowledge Component. A human author then writes text for a problem that has this database query as its solution. Using this technique, a human author was able to create 200 problems in about 3 hours\u2014 enough for a 6 week instructional module in an SQL course. Such programs are called problem generators.</li> </ul>"},{"location":"The%20Behavior%20of%20Tutoring%20Systems/#the-inner-loop","title":"THE INNER LOOP","text":"<ul> <li>Whereas the outer loop is about tasks, the inner loop is about steps within a task</li> </ul>"},{"location":"The%20Behavior%20of%20Tutoring%20Systems/#minimal-feedback","title":"MINIMAL FEEDBACK","text":"<ul> <li>Minimal feedback usually indicates whether a step is correct or incorrect, although other categories are sometimes used as well.</li> </ul>"},{"location":"The%20Behavior%20of%20Tutoring%20Systems/#categories-of-minimal-feedback","title":"Categories of Minimal Feedback","text":"<ul> <li>Although most tutoring systems use just two categories, correct and incorrect, it is possible to use others.</li> <li>Suppose a step is not part of an ideal solution to the problem, but it might be part of a non-ideal solution. The instructors might wish to consider this as a third category for minimal feedback\u2014correct but non-optimal</li> <li>For instance, when students solve physics problems on Andes, they can enter an equation that is true of the given physical situation but is not necessary for solving the problem.</li> <li>If instructors sometimes disagree on what makes for the best solution to the problem, it might be wise to subcategorize non-optimal. For instance, because some Sherlock instructors emphasize reducing time and others emphasize reducing costs, Sherlock could subcategorize non-optimal steps as wastes time or wastes parts. In short, the categories used for minimal feedback should reflect the pedagogical policies of the instructors.</li> <li>If a step cannot be classified as into one of the minimal feedback categories (correct, incorrect, non-optimal, etc.), then it lands in an unrecognizable classification.</li> <li>There are basically just three ways to treat such unrecognizable steps. (1) Some tutoring systems, such as the Algebra Cognitive Tutor, assume that they can recognize all correct steps, so they treat unrecognizable steps as incorrect. (2) Other tutoring systems, such as the SQL-Tutor, assume that students will sometimes produce novel, correct solutions, so they treat unrecognized steps as correct. (3) Lastly, the tutoring system could simply tell the student that the step is unrecognizable. In this case, unrecognizable is yet another category for minimal feedback. When to give minimal feedback</li> <li>Immediate feedback: Andes, Algebra Cognitive Tutor, Steve and AutoTutor give feedback immediately after each student step. They vary considerably in how the feedback is presented.</li> <li>Delayed feedback: Only if the step violates a safety regulation will Sherlock give immediate feedback as the student solves a problem. However, after the problem is solved, the student can request a reviewing mode where minimal feedback is given. In particular, as Sherlock replays the student's solution step-by-step, it indicates which steps did not contribute any diagnostic information.</li> <li>Demand feedback: The SQL-Tutor gives feedback only when the student clicks on the Submit button. Because the student can continue working on their solution after receiving such feedback, this does not count as delayed feedback.</li> <li>The feedback policy can be a function of the student's competence. For instance, if the student is nearing mastery, then the feedback is delayed whereas a less competent student would be given the same task with immediate feedback</li> <li>This policy has been observed in human tutoring, and is one instance of fading the scaffolding, (Collins et al., 1989), because feedback is a kind of scaffolding (pedagogically useful help).</li> </ul>"},{"location":"The%20Behavior%20of%20Tutoring%20Systems/#next-step-hints","title":"NEXT-STEP HINTS","text":"<ul> <li> <ol> <li>When should the tutor give a hint about the next step? Should it wait for the student to ask? Should it give unsolicited hints when it detects guessing or floundering? 2. What step should the tutor suggest? For instance, if there are multiple paths to a solution, and the student appears to be following a long complex one, should the tutor suggest starting over? 3. How can the tutor give hints that maximize learning, keep frustration under control, and allow the student to finish the problem?</li> </ol> </li> <li>The common wisdom is that a tutor should give a hint on the next step only if the student really needs it. If the student can enter a correct step by thinking hard enough, then the system should refuse to give a hint even if the student asks for one</li> <li>On the other hand, if the student is likely to waste time and get frustrated by trying in vain to enter a correct step, or if the student is making repeated guesses instead of trying to figure out the next step, then the tutoring system should probably give a hint even if the student doesn't ask for one.</li> <li>DT Tutor</li> <li>Help Abuse</li> <li>Help Refusal</li> </ul>"},{"location":"The%20Behavior%20of%20Tutoring%20Systems/#which-step-to-hint","title":"Which Step to Hint","text":"<ul> <li>The step must be correct.</li> <li>The hinted step should not have been done already by the student.</li> <li>Instructors' preferences should be honored.</li> <li>If the student has a plan for solving the problem or even for just the next step, then the tutor should hint the step that the student is trying to complete.</li> <li>A somewhat more complex case occurs when the student has just made an incorrect step and received minimal negative feedback.</li> <li>In AI, this is called the plan recognition problem</li> </ul>"},{"location":"The%20Behavior%20of%20Tutoring%20Systems/#how-to-hint-the-next-step","title":"How to Hint the Next Step","text":"<ul> <li>Perhaps the most common policy is to construct a short sequence of hints for the next step.</li> <li>The first hints are weak\u2014they divulge little information so that students are encouraged to do most of the thinking themselves. Later hints are stronger.</li> <li>If the step requires only one Knowledge Component, then a standard hint sequence is Point, Teach and Bottom-out (Hume, Michael, Rovick, &amp; Evens, 1996)</li> <li>Pointing hints mention problem conditions that should remind the student of the Knowledge Component's relevance.</li> <li>Teaching hints describe the Knowledge Component briefly and show how to apply it.</li> <li>Bottom-out hints tell the student the step</li> </ul>"},{"location":"The%20Behavior%20of%20Tutoring%20Systems/#error-specific-feedback","title":"ERROR-SPECIFIC FEEDBACK","text":"<ul> <li>The service consists of analyzing an incorrect step in order to determine the incorrect Learning Event(s) that led to it, then giving instruction that is aimed at preventing that incorrect Learning Event(s) from occurring again</li> <li>There are many techniques for diagnosing errors. Most require that authors observe student errors, figure out what is causing a common error, write an appropriate error type for it, and implement some way to recognize such errors.</li> <li>FOIL: First, Outer, Inner and Last</li> </ul>"},{"location":"The%20Behavior%20of%20Tutoring%20Systems/#how-to-give-error-specific-feedback","title":"How to Give Error-specific Feedback","text":"<ul> <li>The main purpose of error-specific feedback is to get the student to change their knowledge so that they will not make this mistake again</li> <li>Correcting one's knowledge is sometimes compared to debugging a piece of software (Ohlsson, 1996). A programmer must first find evidence that the bug exists, then find out what the bug is, and then fix the bug. When tutees are not working with a tutor, they must do the whole self-debugging process themselves. They must first notice that a step is incorrect, then find the flaw in their reasoning and knowledge, then figure out what the correct learning events and knowledge components should be</li> <li>Many tutors present feedback as a sequence of hints that are loosely associated with the stages of self-debugging. When the student enters an incorrect step, the tutor begins the sequence by simply giving minimal feedback. This is like providing the student with evidence of a knowledge bug but giving no further help toward identifying the bug or correcting it.</li> </ul>"},{"location":"The%20Behavior%20of%20Tutoring%20Systems/#when-to-give-error-specific-feedback","title":"When to Give Error-specific Feedback","text":"<ul> <li>Students sometimes make careless errors, called slips in psychology (Norman, 1981), but fail to notice that they have made them, which can cause them to waste time looking for deep, potential misconceptions.</li> <li>To ameliorate into slips and potential this, Andes divides error misunderstandings. types</li> <li>Slips are often made by the experts, including the instructors.</li> <li>A potential misunderstanding is an error type that could be due to many factors, including incorrect beliefs, but it is almost never seen in expert's work</li> <li>When a student enters an incorrect step that is classified as a slip, then Andes gives the error specific remediation immediately, e.g., You forgot to include a unit on a number. If the error is classified as a potential misunderstanding, then Andes merely turns the incorrect step red, and lets the student ask for an error-specific hint if they want one.</li> <li>Some student steps contain two or more errors. In order to keep the communication simple, the tutoring system should probably respond to only one of them. Students typically fix that error only, so the tutoring system can then mention the second error</li> </ul>"},{"location":"The%20Behavior%20of%20Tutoring%20Systems/#what-kind-of-assessment-is-required","title":"What Kind of Assessment is Required?","text":"<ul> <li>A fundamental issue is the grain-size or granularity of the assessment.</li> <li>The granularity of an assessment refers to the degree of aggregation over the task domain's knowledge. An assessment is fine-grained if it provides, for instance, a probability of mastery for each Knowledge Component in the task domain. An assessment is coarse-grained if it provides a single number that indicates the student's overall competence.</li> <li>Assessments are decision aids, and as a general heuristic, the bigger the decision, the coarser the required assessment. If a decision affects a whole semester, e.g., whether the student needs to retake a</li> <li>required course, then the assessment should cover the whole semester and lead ultimately to a yes-no decision, so a single number per student is appropriate.</li> <li>If a decision affects only, say, whether the tutor starts at the beginning of a hint sequence or in the middle, then only a small part of a fine-grained assessment is relevant</li> <li>The general idea is that if the decision is small, in that it affects only a small amount of instructional time, then only a small amount of the domain's knowledge can be accessed during that time and thus the relevant decision aid is the student's competence on just that knowledge.</li> <li>Tutors make many small decisions, so they are the main customers for fine-grained assessments.</li> <li>Coarse-grained assessment</li> <li>Fine Grained assesment</li> </ul>"},{"location":"The%20Behavior%20of%20Tutoring%20Systems/#issues-with-assesment","title":"Issues with Assesment","text":"<ul> <li>There are many other issues involved with assessment. Here are a few:</li> <li>There is a big difference between little evidence and mixed evidence. If one merely takes the frequency of success relative to opportunities, then 0.5 can mean both 1 success and 1 failure (little evidence) or 20 successes and 20 failures (mixed evidence).</li> <li>Students should be learning, so their knowledge should be changing. When should old evidence be ignored?</li> <li>Should help given by the tutoring system be counted as evidence of learning or of lack of knowledge?</li> <li>Are all learning events equally difficult? If not, then item response theory (IRT) can be used so that success on easy learning events provides less evidence of competence than success on difficult learning events.</li> <li>If the tutoring system includes error-specific feedback based on error types, should it somehow include the frequency of occurrence of the error types in its assessments?</li> <li>Prior probabilities of mastery are not independent. If a student has not mastered a Knowledge Component that is taught early in the course, then it is less likely that the student has mastered.</li> <li>Knowledge components that appear later.</li> </ul>"},{"location":"The%20Behavior%20of%20Tutoring%20Systems/#examples-of-tutoring-systems","title":"Examples of Tutoring Systems","text":"<ul> <li>Steve</li> <li>Algebra Cognitive Tutor</li> <li>Andes</li> <li>Sherlock</li> <li>AutoTutor</li> <li>SQL-Tutor</li> </ul>"},{"location":"The%20Differentiation%20Condition/","title":"The Differentiation Condition","text":"<ul> <li>A sentence containing a quantified phrase headed by each can only be true of event structures which are totally distributive. Each individual object in the restrictor set of the quantified phrase must be associated with its own subevent, in which the predicate applies to that object, and which can be differentiated in some way from the other subevents.</li> </ul>"},{"location":"The%20Effect%20of%20Three%20Consecutive%20Context%20Sentences%20on%20EFL%20Vocabulary-Learning/","title":"The Effect of Three Consecutive Context Sentences on EFL Vocabulary-Learning","text":"<ul> <li>Sasan Baleghizadeh and Mohammad Naseh Nasrollahy Shahry</li> </ul> <ul> <li>effect of three consecutive context sentences instead of one</li> <li>Thirty-three Iranian EFL learners were asked to learn 20 challenging English words in two conditions</li> <li>The results of both immediate and delayed post-tests revealed a positive role for context sentences in vocabulary learning</li> <li>It is proposed that successful vocabulary learning through context sentences could be attributed to the mixed effects of both context and frequency of occurrence.</li> </ul>"},{"location":"The%20Effect%20of%20Three%20Consecutive%20Context%20Sentences%20on%20EFL%20Vocabulary-Learning/#results","title":"Results","text":"<ul> <li>seven students knew at least one of the target words, so their scores on the posttests were excluded</li> <li>data were analyzed by using a paired sample t-test because it was a within-subjects comparison and the scores on both learning conditions were related</li> <li>the participants' performance on words that had appeared in context sentences plus their L1 equivalents was significantly better than their performance on words paired only with their L1 equivalent</li> <li>Discussion</li> <li>Learners exposed to context sentences did better in terms retaining words, and they were also able to compose more correct sentences with them</li> <li>The learners who were exposed to context sentences had three sentences on which to draw as models, and it is plausible that part of their better sentence-making scores could be accounted for by their exposure to these sentences.</li> <li>frequency and context have an important place in vocabulary-learnin</li> <li>lthough learning new words through context-free activities such as working on word pairs might be a powerful tool to enhance one's breadth of vocabulary knowledge, this study provides strong evidence that adding a minimum of three contextually appropriate sentences to L1 glosses results in a significant improvement in vocabulary-learning</li> <li>need to furnish learners with more sample sentences when it comes to presenting vocabulary</li> <li>authors of textbooks seem to have a propensity for presenting isolated words either in designated boxes or in the context of a passage, which essentially provides only one context for the given word</li> <li>It appears that students would be in a better position to learn and retain new words if they were provided with repeated contexts through exposure to more sample sentences</li> <li>In other words, is it the elaborative nature of the context, or is it the frequency of occurrence that promotes better vocabulary learning? Future research is warranted to unravel this issue.</li> </ul>"},{"location":"The%20Effect%20of%20Three%20Consecutive%20Context%20Sentences%20on%20EFL%20Vocabulary-Learning/#backlinks","title":"Backlinks","text":"<ul> <li>Final Paper User Models</li> <li>The Effect of Three Consecutive Context Sentences on EFL Vocabulary-Learning</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"The%20Repulsive%20Potential/","title":"The Repulsive Potential","text":""},{"location":"The%20Repulsive%20Potential/#the-repulsive-potential","title":"The Repulsive Potential","text":"<ul> <li>The robot is pushed away from obstacles and attracted to the goal</li> <li> \\[U_{rep}(q)=\\frac{\\eta}{D(q,Q)}\\] </li> <li></li> </ul>"},{"location":"The%20Repulsive%20Potential/#backlinks","title":"Backlinks","text":"<ul> <li>The Repulsive Potential</li> <li>The Repulsive Potential</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"The%20Reward%20Experiment/","title":"The Reward Experiment","text":"<ul> <li>\u201cWhy aren\u2019t we always prepared?\u201d  <ul> <li>Is it a matter of motivation?  </li> <li>Reward bocks: flat(ter) preparation curves?</li> </ul> </li> <li>Reward blocks: more memory effects<ul> <li>Cognition Hazard Rates</li> </ul> </li> <li>Reward blocks: less memory effects<ul> <li>Cognitive fMTP</li> </ul> </li> <li></li> <li>Fanning<ul> <li>smaller if reward</li> <li>less effects of previous trial</li> </ul> </li> </ul>"},{"location":"The%20Reward%20Experiment/#findings","title":"Findings","text":"<ul> <li> <p>The effects of reward and brightness on preparation</p> <ul> <li>Additive effects : neither affects the slope</li> <li>even when motivated, preparation depends on FP</li> <li> <p>Stimulus visibility does not interact with preparation</p> <ul> <li>Maybe not a visual attention process</li> </ul> </li> <li> <p></p> </li> <li>Reward has more effects when targets are highly visible</li> <li>Interaction : Reward * Brightness</li> <li></li> <li>Effects on accuracy</li> <li>Reward blocks\u2192 higher accuracy  </li> <li>High visibility \u2192 higher accuracy  </li> <li>More errors with longer foreperiod\u2026.  <ul> <li>\u2026But only when rewarded/motivated  </li> <li>\u2026And mainly when visibility low</li> </ul> </li> <li></li> <li>Effects on memory from past trials</li> <li>Past trial effects \\(\\(fp \\ast fp(n-1)\\)\\)</li> <li>This effect (the fanning) is essentially constant across target visibility and reward  </li> <li>If anything: a little bit larger when there is no reward.</li> <li></li> <li>Effects on memory</li> <li>Up to n-5</li> <li>When there is reward, there is slightly \u2018faster forgetting\u2019</li> <li>Faster forgetting when rewarded slightly</li> <li></li> </ul> </li> </ul>"},{"location":"The%20Reward%20Experiment/#backlinks","title":"Backlinks","text":"<ul> <li>CogMod Final Paper</li> <li> <p>The Reward Experiment</p> </li> <li> <p></p> </li> <li>02:24 The Reward Experiment</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"The%20Self%20Organization%20of%20Explicit%20Attitudes/","title":"The Self Organization of Explicit Attitudes","text":"<ul> <li>Michael T. Wojnowicz, Melissa J. Ferguson, Rick Dale, and Michael J. Spivey</li> </ul>"},{"location":"The%20Self%20Organization%20of%20Explicit%20Attitudes/#abstract","title":"ABSTRACT","text":"<ul> <li>How do minds produce explicit attitudes over several hundred milliseconds?</li> <li>implicit biases beyond cognitive control and subjective awareness, yet mental processing may culminate in an explicit attitude that feels personally endorsed and corroborates voluntary intentions</li> <li>self-reported explicit attitudes derive from a continuous, temporally dynamic process, whereby multiple simultaneously conflicting sources of information selforganize into a meaningful mental representation</li> <li>While our participants reported their explicit (like vs. dislike) attitudes toward White versus Black people by moving a cursor to a ''like'' or ''dislike'' response box, we recorded streaming xand y-coordinates from their hand-movement trajectories</li> <li>participants' hand-movement paths exhibited greater curvature toward the ''dislike'' response when they reported positive explicit attitudes toward Black people than when they reported positive explicit attitudes toward White people</li> <li>these trajectories were characterized by movement disorder and competitive velocity profiles that were predicted under the assumption that the deliberate attitudes emerged from continuous interactions between multiple simultaneously conflicting constraints.</li> <li>For example, an implicit attitude toward a stimulus can be unintentionally activated by the mere presence of that stimulus.</li> <li>Given that many people demonstrate spontaneous initial biases toward traditionally stigmatized groups, how do they overcome these biases to explicitly report positive attitudes toward the same groups?</li> <li>coexistence of multiple attitudes and an emphasis on the temporal dynamics of how they influence evaluative responses</li> <li>Rather than selecting among the specific theories, we invoked the encompassing theoretical framework of self-organization to guide an exploration of those temporal dynamics, and made specific predictions for what should result from multiple attitudes interacting over time.</li> <li>In early moments of processing, distributed representations are partially consistent with multiple interpretations because of their proximity to multiple neural population codes.</li> <li>However, a continuous accrual of information causes the distributed pattern to dynamically ''sharpen'' into a confident (selected) interpretation, forcing other, partially activated, competing alternative representations, decisions, or actions to gradually die out.</li> <li>The latter attitude will eventually activate other subsystems, such as language and memory, thus making the attitude seem explicit</li> <li>What makes the first attitude implicit is not necessarily that it was generated in a different subsystem, but simply that it did not hold sway long enough to activate those language and memory subsystems.</li> <li>Mental processing generically involves recurrent processing loops (or cyclic feedback) between higher-order integrative regions and lower-level informational sources (Lamme &amp; Roelfsema, 2000; O'Reilly, 1998; Spivey, 2007)</li> <li>These higher-order integrative regions enforce representational competition, in which increasing the activation of one particular interpretation inhibits alternatives.</li> <li>The unfolding cognitive dynamics may be revealed in continuous motor output</li> <li>Because mental processing is recurrent, motor representations begin specifying movement parameters probabilistically, rather than waiting for a perfectly completed cognitive command</li> <li>If the phrase ''Black people'' evokes elevated dynamic competition between simultaneously active ''like'' and ''dislike'' representations, movement trajectories for ''Black people'' should exhibit evidence of nonlinear dynamics in their velocity profiles, as well as increased spatial disorder in the curviness of the trajectories.</li> </ul>"},{"location":"The%20Self%20Organization%20of%20Explicit%20Attitudes/#experiment-1","title":"EXPERIMENT 1","text":""},{"location":"The%20Self%20Organization%20of%20Explicit%20Attitudes/#method","title":"Method","text":"<ul> <li>Streaming xand y-coordinates of mouse-cursor movements were recorded from 68 Cornell University undergraduates (43 female and 25 male) as they performed a simple explicit-attitude task.</li> <li>2 s for participants to view the evaluative response options</li> <li>Participants then clicked on a small box at the bottom of the screen to reveal a stimulus word or phrase and dragged the mouse toward their selected evaluative response to that stimulus</li> <li>Responses to the two stimulus repetitions were averaged together to yield a single measurement for each participant for all statistical analyses.</li> </ul>"},{"location":"The%20Self%20Organization%20of%20Explicit%20Attitudes/#results-and-discussion","title":"Results and Discussion","text":"<ul> <li>Compared with the trajectories for ''White people,'' the trajectories for ''Black people'' curved significantly more toward the ''dislike'' response option observed differential motor curvatures could have been generated by a stage-based sequence of decisional commands, rather than by continuous motor attraction to the ''dislike'' response.</li> <li>If motor execution required the complete prespecification of a unique target destination, rather than tracking of motor trajectory parameters that continuously evolved midflight, then a mean trajectory could look differentially curved because of the effect of averaging in replanned trajectories</li> <li>To accommodate the empirical mean trajectory, which initially moved upward rather than actually toward ''dislike,'' such an account would need to predict a bimodal distribution of curvatures that included some trajectories that were very curved and others that were not curved.</li> <li>However, the distribution of trajectory curvatures shows no evidence of bimodality The standard cutoff for inferring bimodality in a distribution is b &gt; 0.55</li> <li>Neither the ''Black people'' nor the ''White people'' trajectories had distributions that met this cutoff, and in fact, the ''Black people'' trajectories formed a distribution of movement curvature that was closer to normal (b 5 0.24, skewness 5 0.613, kurtosis 5 2.57) than the ''White people'' trajectories (b 5 0.301, skewness 5 0.98, kurtosis 5 3.44).</li> <li>Velocity profiles were constructed by analyzing the temporal derivatives of motion toward the ''like'' response box along the x-coordinate.</li> <li>Our velocity predictions came from Usher and McClelland's (2003) differential equations for modeling the dynamics of competition between mental representations</li> <li>where, in this case, x1 and x2 represent the activations of the mental representations for ''like'' and ''dislike,'' dx1 and dx2 represent the change in the activation of the two mental representations in a time step of size dt, I1 and I2 represent excitatory input to the representations from informational sources, bf1 and bf2 represent the inhibitory input from each mental representation to the other (lateral inhibition), and fi (where i 5 1 or 2) is equal to xi if xi is greater than zero.</li> <li>differential equations for competition dynamics, a strong evaluative competitor (dislike, x2) sends intensified and prolonged lateral inhibition (bf2) to the ''like'' evaluation (x1)</li> <li>Thus, strong competition alters the velocity profile of the movement toward the evaluative attractor (dx1/dt), reducing velocity toward the attractor early on in processing</li> <li>as the more active alternative begins to win the competition, this lateral inhibition is gradually lifted, thus increasing velocity later in processing to produce greater acceleration</li> <li>Moreover, this particular dynamic pattern (reduced early velocity and greater later acceleration) should lead to greater peak velocity, if jerk is minimized as the system achieves equivalent integral under the curve (where the integral represents net change in activation or location)</li> <li>The spatial-disorder analysis investigated the regularity of change in x-coordinate location over time</li> <li>To investigate whether the ''Black people'' trajectories had more wiggles, blips, and other irregularities than the ''White people'' trajectories, we analyzed x-coordinate location over time, but only after the trajectory began moving in the positive x direction</li> <li>The ''Black people'' trajectories had significantly greater deviation from the sigmoidal fit</li> <li>indicated disorderly variation around the x dimension in those trajectories.</li> </ul>"},{"location":"The%20Self%20Organization%20of%20Explicit%20Attitudes/#experiment-2","title":"EXPERIMENT 2","text":"<ul> <li>our claim is that multiple, partially active mental representations compete for the privilege of driving evaluative responses, imposing a set of response options that are not particularly competitive should change the motor dynamics</li> </ul>"},{"location":"The%20Self%20Organization%20of%20Explicit%20Attitudes/#method_1","title":"Method","text":"<ul> <li>Sixty-six Cornell University undergraduates (40 female and 26 male) were asked to classify words (e.g., ''ice cream,'' ''sunshine,'' ''boron'') as something they liked (''like'') or as the name of a chemical element (''chemical'')</li> <li>We analyzed data only from the 63 participants who consistently chose the ''like'' response for both ''Black people'' and ''White people'' on both repetitions of these trials, and who reported in a poststudy questionnaire that they were not forced into selecting ''like'' by the paradigm.</li> </ul>"},{"location":"The%20Self%20Organization%20of%20Explicit%20Attitudes/#results","title":"Results","text":"<ul> <li>According to statistical analyses on maximum deviation and distance traveled, the ''Black people'' and ''White people'' trajectories no longer differed in their curvature toward the competing respons</li> <li>Thus, the results of Experiment 1 are not attributable merely to responses to ''Black people'' involving a longer latency to settle on a positive evaluation</li> <li>thereby drifting for longer in empty regions of movement space before curving</li> <li>toward the ''like'' response box</li> <li>Rather, the ''dislike'' response option in Experiment 1 was actively pulling movement trajectories toward it, in a way that the ''chemical'' response option in Experiment 2 did not.</li> </ul>"},{"location":"The%20Self%20Organization%20of%20Explicit%20Attitudes/#experiment-3","title":"EXPERIMENT 3","text":"<ul> <li>Experiment 1 may have diverged because of subtle confounds that do not refer to people at all.</li> </ul>"},{"location":"The%20Self%20Organization%20of%20Explicit%20Attitudes/#method_2","title":"Method","text":"<ul> <li>Seventy-one Cornell University undergraduates (37 female and 34 male) were asked to classify stimuli as something they liked (''like'') or disliked (''dislike'')</li> <li>The crucial stimuli in this experiment were ''African Americans'' and ''Caucasians. Results</li> <li>The trajectories for ''African Americans'' curved significantly more toward the ''dislike'' response than the trajectories for ''Caucasians,</li> <li>The motor trajectories evolved over time in accordance with the competitive velocity predictions, as reported in Experiment</li> <li>The ''African Americans'' trajectories, compared with the ''Caucasians'' trajectories, had significantly greater maximum xcoordinate acceleration</li> <li>Moreover, as we found for ''Black people'' trajectories in Experiment 1, the ''African Americans'' trajectories exhibited greater spatial disorder than the ''Caucasians'' trajectories, even after moving toward the ''like'' response, as indicated by significantly greater mean deviation from the sigmoidal fi</li> </ul>"},{"location":"The%20Self%20Organization%20of%20Explicit%20Attitudes/#general-discussion","title":"GENERAL DISCUSSION","text":"<ul> <li>People's hand-movement trajectories for explicitly evaluating ''Black people'' and ''White people'' were distinct as measured by three properties of movement dynamics: shape, time, and order.</li> <li>explicit attitudes evolve through continuous temporal dynamics during real-time mental processing, with graded motor curvature revealing the influence of tendencies toward dislike</li> <li>evidence for cleanly separated (i.e., discrete, rather than continuous) explicit decisions, in which an initial response was executed solely toward the ''dislike'' response box and then a corrective response was executed midflight toward the ''like'' response box.</li> <li>Rather, the results suggest that a dynamic competition process may be what allows a single explicit attitude choice to emerge from multiple, potentially conflicting evaluative influences (e.g., Busemeyer &amp; Townsend, 1993; Usher &amp; McClelland, 2003)</li> <li>the mind may host a continuously evolving blend of (implicit) evaluative decisions from which the eventual (explicit) behavioral choice emerges.</li> </ul>"},{"location":"The%20Self%20Organization%20of%20Explicit%20Attitudes/#pictures","title":"Pictures","text":""},{"location":"The%20Unreliability%20of%20Saliency%20Methods/","title":"The Unreliability of Saliency Methods","text":"<ul> <li>Pieter-Jan Kindermans, Sara Hooker, Julius Adebayo, Maximilian Alber, Kristof T. Sch\u00fctt, Sven D\u00e4hne, Dumitru Erhan, Been Kim</li> </ul>"},{"location":"The%20Unreliability%20of%20Saliency%20Methods/#backlinks","title":"Backlinks","text":"<ul> <li>Vision Explainibility</li> <li>The Unreliability of Saliency Methods</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Theta%20Waves/","title":"Theta Waves","text":"<ul> <li>4-9 Hz theta</li> <li>Memory/Decision</li> <li></li> </ul>"},{"location":"Threaded%20Cognition/","title":"Threaded Cognition","text":"<ul> <li>Aka no real \"switch\" between one task to next but bottlenecks</li> <li>Cognition is Threaded but overlaps in the sense that different areas can not be accessed if it uses same areas as the other task at the same time</li> <li>If no expertise, consult Declarative memory all the time. Until it's not required anymore if in long term store.</li> </ul>"},{"location":"Threaded%20Cognition/#backlinks","title":"Backlinks","text":"<ul> <li>Cognitive Multitasking</li> <li>Threaded Cognition</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Thrombosis/","title":"Thrombosis","text":"<ul> <li>A blood clot that forms inside a blood vessel restricting blood flow</li> </ul>"},{"location":"Through-beam/","title":"Through-beam","text":"<ul> <li>An object detection system used within a robot's imaging sensor system. A finely focused beam of light is mounted at one end and a detector at the other. When the beam of light is broken, an object is sensed.</li> </ul>"},{"location":"Time%20Dependant%20Vector%20Field/","title":"Time Dependant Vector Field","text":"<ul> <li>Lagrangian Coherent Structure</li> </ul>"},{"location":"Time%20Measuring%20Function/","title":"Time Measuring Function","text":"<ul> <li>Time measuring function measures the execution time for the specified section in the job or the signal output time of the specified signal.</li> </ul>"},{"location":"Time%20space%20duality/","title":"Time Space Duality","text":"<ul> <li>Array : Instruction operates on multiple data elements at the same time</li> <li>Vector : Instruction operates on multiple data elements in consecutive time steps</li> </ul>"},{"location":"Time/","title":"Time","text":"<ul> <li> \\[\\Delta t = t_{f}-t_{i}\\] </li> </ul>"},{"location":"TinyBERT/","title":"TinyBERT","text":"<ul> <li>TinyBERT: Distilling BERT for Natural Language Understanding</li> <li>novel Transformer distillation method to accelerate inference and reduce model size while maintaining accuracy</li> <li>specially designed for knowledge distillation (KD) of the Transformer-based models</li> <li>plenty of knowledge encoded in a large teacher BERT can be effectively transferred to a small student Tiny-BERT</li> <li>GLUE</li> </ul>"},{"location":"TinyBERT/#backlinks","title":"Backlinks","text":"<ul> <li>AutoDistill</li> <li>higher than BERT_BASE, DistillBERT, TinyBERT, NAS-BERT, and MobileBERT</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Token%20Embedding/","title":"Token Embedding","text":"<ul> <li>WordPiece Tokenizer</li> </ul>"},{"location":"Tokenizer/","title":"Tokenizer","text":"<ul> <li>Tokenizer expands the contraction to recover the essential grammatical features of the pronoun and the Verb.</li> <li>Space-delimited languages</li> <li>White space delimited tokens may not be the valid token</li> <li>Chinese and Thai<ul> <li>Words are written in succession with no indication of word boundaries</li> </ul> </li> <li>Word Structure</li> <li>Punctuation</li> </ul>"},{"location":"Top%20Down%20Parsing/","title":"Top Down Parsing","text":""},{"location":"Tourette%E2%80%99s%20Syndrome/","title":"Tourette\u2019s Syndrome","text":"<ul> <li>A neurological disorder, beginning in childhood, characterized by repetitive, involuntary movements or vocalizations, called tics.</li> </ul>"},{"location":"Tower/","title":"Tower","text":"<ul> <li>A component of a deep neural network that is itself a deep neural network without an output layer. Typically, each tower reads from an independent data source. Towers are independent until their output is combined in a final layer.</li> </ul>"},{"location":"Tracking%20the%20Continuity%20of%20Language%20Comprehension%20Computer%20Mouse%20Trajectories%20Suggest%20Parallel%20Syntactic%20Processing/","title":"Tracking the Continuity of Language Comprehension Computer Mouse Trajectories Suggest Parallel Syntactic Processing","text":"<ul> <li>Thomas A. Farmera, Sarah A. Cargilla, Nicholas C. Hindya, Rick Daleb, Michael J. Spiveya</li> </ul>"},{"location":"Tracking%20the%20Continuity%20of%20Language%20Comprehension%20Computer%20Mouse%20Trajectories%20Suggest%20Parallel%20Syntactic%20Processing/#abstract","title":"Abstract","text":"<ul> <li>Although several theories of online syntactic processing assume the parallel activation of multiple syntactic representations, evidence supporting simultaneous activation has been inconclusive</li> <li>continuous and non-ballistic properties of computer mouse movements are exploited</li> <li>procure evidence regarding parallel versus serial processing</li> <li>Participants heard structurally ambiguous sentences while viewing scenes with properties either supporting or not supporting the difficult modifier interpretation</li> <li>The curvatures of the elicited trajectories revealed both an effect of visual context and graded competition between simultaneously active syntactic representations</li> </ul>"},{"location":"Tracking%20the%20Continuity%20of%20Language%20Comprehension%20Computer%20Mouse%20Trajectories%20Suggest%20Parallel%20Syntactic%20Processing/#introduction","title":"Introduction","text":"<ul> <li>Sentences such as, \"The adolescent hurried through the door tripped,\" are difficult to process because, at least temporarily, multiple possible structural representations exist</li> <li>garden-path effect</li> <li>Syntax First models</li> <li>Multiple constraint-based theories</li> <li>what feel like garden-path effects are due to the incorrect syntactic alternative winning much of the competition during the early portion of the sentence, and then nonconforming information from the latter portion of the sentence inducing a laborious reversal of that activation pattern</li> <li>As a result, one can expect that some garden-path events may be very mild, some moderate, and some extreme such that a wide variety of sentence readings should all belong to one population of events with a relatively continuous distribution.</li> <li>Unrestricted Race Model</li> <li>When ambiguous sentences like 1a are heard in the presence of visual scenes where only one possible referent is present (an apple already on a towel), along with an incorrect destination (an empty towel), and a correct destination (a box), as in the top portion of Fig. 1, about 50% of the time participants fixate the incorrect destination after hearing the first PP.</li> <li>After the second disambiguating PP is heard, eye movements tend to be redirected to the correct referent and then to the correct destination</li> <li>This garden-path effect can, however, be modulated by contextual information contained within the visual scene</li> <li>it seems that when two possible referents are present, an expectation is created that they will be discriminated amongst, thus forcing a modifier interpretation of the</li> <li>ambiguous PP</li> <li>The attenuation of looks to the incorrect destination by the presence of two possible referents, then, is evidence for an early influence of non-syntactic (even nonlinguistic) information on the parsing process and is problematic for traditional syntax-first accounts discussed earlier.</li> <li>However, because saccadic eye movements are generally ballistic, they either send the eyes to fixate an object associated with a garden-path interpretation or they do not.</li> <li>The evidence from this paradigm, therefore, is also consistent with the Unrestricted Race model, where the various constraints are combined immediately, but on any given trial only one syntactic representation is initially pursued</li> <li>across experimental trials, distributions of eye-movement patterns are almost always bimodal because the fixations are coded as binomial</li> <li>There are saccades to locations on the display corresponding to either one of the possible representations, but almost never to a blank region in between those two potential targets</li> <li>In the following experiment, we examined the dynamics of hand movement in the same sentence comprehension scenario with the goal of determining whether the non-ballistic, continuous nature of computer mouse trajectories can serve to tease apart these two remaining theoretical accounts.</li> </ul>"},{"location":"Tracking%20the%20Continuity%20of%20Language%20Comprehension%20Computer%20Mouse%20Trajectories%20Suggest%20Parallel%20Syntactic%20Processing/#experiment","title":"Experiment","text":"<ul> <li>computer mouse movements can serve as an informative indicator of the cognitive processes underlying spoken-word recognition (Spivey, Grosjean, &amp; Knoblich, 2005)</li> <li>In addition, whereas self-paced reading affords 2 to 3 data points (button presses) per second, and eye-movement data allow for approximately 3 to 4 data points (saccades) per second, \"mouse tracking\" yields somewhere between 30 and 60 data points per second, depending on the sampling rate of the software used.</li> <li>The context and garden-path effects reported in the visual world paradigm are highly replicable when tracking eye movements</li> <li>As such, recording mouse movements in the visual world paradigm can serve as a strong test case by which to evaluate the efficacy of the mouse-tracking procedure for the study of language processing in real time.</li> </ul>"},{"location":"Tracking%20the%20Continuity%20of%20Language%20Comprehension%20Computer%20Mouse%20Trajectories%20Suggest%20Parallel%20Syntactic%20Processing/#expected-prediction","title":"Expected Prediction","text":"<ul> <li>1) Averaged trajectories recorded in response to ambiguous sentences in the onereferent context should show significantly more curvature toward the incorrect destination than the averaged trajectories elicited by unambiguous sentences\u2014a pattern corresponding to the garden-path effect. 2)</li> <li>The curvature of averaged trajectories in the two referent condition should not differ statistically between ambiguous and unambiguous sentences, thus demonstrating an influence of referential context on the garden-path effect.</li> <li>The second purpose of this study, then, was to exploit the continuity of the mousemovement trajectories to discriminate between these two remaining theoretical accounts</li> <li>a measure of curvature magnitude was used to determine the amount of spatial attraction toward the incorrect destination that was exhibited by the ambiguousand unambiguous-sentence trajectories in the one-referent context.</li> <li>If only one representation were active at any one time, as the unrestricted race account predicts, then the trial-by-trial distribution of trajectory curvatures in the ambiguous-sentence condition should be either (a) bimodal\u2014comprised of highly curved garden-path movements and noncurved, correct-interpretation movements, or (b) uniformly in the more extreme curved range, indicating that almost every trial exhibited a garden-path effect</li> </ul>"},{"location":"Tracking%20the%20Continuity%20of%20Language%20Comprehension%20Computer%20Mouse%20Trajectories%20Suggest%20Parallel%20Syntactic%20Processing/#method","title":"Method","text":""},{"location":"Tracking%20the%20Continuity%20of%20Language%20Comprehension%20Computer%20Mouse%20Trajectories%20Suggest%20Parallel%20Syntactic%20Processing/#participants","title":"Participants","text":"<ul> <li>Forty right-handed, native English-speaking undergraduates from Cornell University participated in the study for extra credit in psychology courses</li> <li>only right-handed individuals to avoid variability associated with subtle kinematic differences in leftward and rightward movement of the left versus the right arms.</li> </ul>"},{"location":"Tracking%20the%20Continuity%20of%20Language%20Comprehension%20Computer%20Mouse%20Trajectories%20Suggest%20Parallel%20Syntactic%20Processing/#materials-and-procedures","title":"Materials and Procedures","text":"<ul> <li>Sixteen experimental items 102 filler sentences</li> <li>The unambiguous version (1b) of each of the 16 experimental items was recorded first, and then the \"that\" was removed to produce the ambiguous (1a) sentence condition</li> <li>Each visual context corresponding to the 16 experimental items was varied to produce a oneand two-referent condition</li> <li>The one-referent visual context (illustrated in Fig. 1, top) contained the target referent (an apple on a towel), an incorrect destination (a second towel), the correct destination (a box), and a distracter object (a flower). In the two-referent context, all items were the same except that the distracter object was replaced with a second possible referent (such as an apple on a napkin). Twenty-four filler scenes, designed to accompany filler sentences, were also constructed.</li> <li>In critical trials for both the oneand two-referent conditions, the target referent always appeared in the top left corner of the screen, the incorrect destination always appeared in the top right corner of the screen, and the correct destination</li> <li>was always located at the bottom right portion of the screen.</li> <li>Given that the scene layout was held constant across all items in each experimental condition, a left-to-right movement was always necessary</li> <li>Although there could exist a systematic bias toward specific locations in the display when moving rightward, this was viewed as unproblematic given that the bias would be held constant across both the ambiguous and unambiguous sentences, which were directly compared in all statistical analyses, for each context.</li> <li>In each scene, participants saw four to six color images, depending on how many objects were needed for the scene</li> </ul>"},{"location":"Tracking%20the%20Continuity%20of%20Language%20Comprehension%20Computer%20Mouse%20Trajectories%20Suggest%20Parallel%20Syntactic%20Processing/#results","title":"Results","text":""},{"location":"Tracking%20the%20Continuity%20of%20Language%20Comprehension%20Computer%20Mouse%20Trajectories%20Suggest%20Parallel%20Syntactic%20Processing/#data-screening-and-coding","title":"Data Screening and Coding","text":"<ul> <li>A trajectory was considered valid and submitted to further analysis if it was initiated at the top left quadrant of the display and terminated in the bottom right quadrant, indicating that the correct referent had been picked up and then placed at the correct destination. This screening procedure resulted in 27 deleted trials, accounting for less than 5% of all experimental trials.</li> <li>To make sure that trajectories in one condition were not initiated (or that objects were not grabbed) at a systematically different region of the display than in the other conditions, we conducted two 2 (Context) \u00d7 2 (Ambiguity) ANOVAs on the x and y coordinates, separately.</li> <li>There was no significant main effect or interaction for either the x or the y coordinates (all ps were nonsignificant) indicating that, across conditions, the trajectories were initiated at approximately the same location of the display</li> <li>Subsequently, all analyzable trajectories were \"time normalized\" to 101 timesteps by a procedure described in Spivey et al. (2005) and Dale et al. (2007).</li> </ul>"},{"location":"Tracking%20the%20Continuity%20of%20Language%20Comprehension%20Computer%20Mouse%20Trajectories%20Suggest%20Parallel%20Syntactic%20Processing/#context-and-garden-path-effects","title":"Context and Garden-path Effects","text":"<ul> <li>The mean trajectories from ambiguous and unambiguous sentences in the onereferent context, illustrated in Fig. 1 (top), demonstrate that the average ambiguoussentence trajectory was more curved toward the incorrect destination than the average trajectory elicited by the unambiguous sentences</li> <li>Thus, in the presence of the garden-path effect, it seems clear that there exists more spatial attraction toward the incorrect destination for the ambiguous sentences.</li> <li>In addition, in line with the time-normalized analyses presented above, none of the</li> <li>eight time bins in the two-referent context showed the ambiguousand unambiguous-sentence trajectories significantly diverging for either the x or the y coordinates.</li> </ul>"},{"location":"Tracking%20the%20Continuity%20of%20Language%20Comprehension%20Computer%20Mouse%20Trajectories%20Suggest%20Parallel%20Syntactic%20Processing/#serial-versus-parallel-activation","title":"Serial Versus Parallel Activation","text":"<ul> <li>garden-path trials and some non-garden-path trials, the majority of the trajectories elicited in this condition fell somewhere in between those two extremes, forming a single population of non-, somewhat-, and highly curved responses.</li> <li>To determine whether any bimodality is present in the distribution of responses, we computed the area under the curve on a trial-by-trial basis</li> <li>The b value for each distribution is less than .555, indicating no presence of bimodality within the distributions.</li> <li>Notably, with regard to the distribution of responses in the one-referent, ambiguous-sentence condition, b &lt; .555 indicates that the graded spatial attraction effects elicited in this condition came not from two different types of trials but from a single population of trials.</li> <li>Finally, one might argue that bimodality was not detected (thus, b &lt; .555) in the crucial one-referent, ambiguous-sentence condition due to a lack of statistical power resulting from the relatively small number of trials in the garden-path distribution.</li> <li>To address this concern, we created an artificial distribution with a sample size almost identical to our crucial gardenpath distribution by randomly sampling 50% of the trials from the one-referent, ambiguoussentence condition (where garden pathing was observed) and 50% of the trials from the onereferent, unambiguoussentence condition.</li> <li>By examining the distributional properties of the area-under-the-curve values produced by the garden-path and non-garden-path trials together, we can thus determine whether the bimodality statistic (b) we used to assess the bimodality of the garden-path distribution (above) is capable of detecting bimodality in a case where the response distribution should clearly be bimodal</li> </ul>"},{"location":"Tracking%20the%20Continuity%20of%20Language%20Comprehension%20Computer%20Mouse%20Trajectories%20Suggest%20Parallel%20Syntactic%20Processing/#general-discussion","title":"General Discussion","text":"<ul> <li>In the one-referent context, participants' mouse movements in response to the ambiguous sentences curved significantly closer to the top right of the screen (toward the incorrect destination) than in response to unambiguous sentences.</li> <li>Thus, it would seem that when only one referent was present, the incorrect destination (e.g., the towel) was partially considered relevant, until disambiguating information was processed\u2014a trend corresponding to the garden-path effect associated with this condition.</li> <li>The fact that most mouse trajectories began while the speech file was still being heard suggests that the effect of visual context modulating the garden path took place during early moments of processing the linguistic input, not during a second stage of syntactic reanalysis.</li> <li>In addition, by capitalizing on the continuous, non-linear, and non-ballistic properties of trajectories produced by computer mouse movements, mouse tracking has the potential to answer questions that have been difficult to answer with more traditional methodologies.</li> <li>What does distinguish between these two accounts is the gradiency observed in the curvature of the trajectories in the garden-path condition</li> <li>If the Unrestricted Race model posits that only one syntactic representation is pursued at any one time, then it must predict that mouse movements in a gardenpath condition should initially move either in the direction of the correct destination or in the direction of the incorrect destination (producing either a bimodal distribution or an all-curved distribution)</li> <li>In contrast, because the constraintbased account posits simultaneous graded activation of multiple syntactic alternatives, it predicts that mouse movements can move in directions that are dynamically weighted combinations of the two competing destinations (producing a unimodal distribution of moderate curvatures).</li> <li>Fig. 4 shows that although approximately 5% of the trajectories moved all the way to the incorrect destination before changing direction, the vast majority of the trajectories responsible for the mean curvature were unmistakably graded in their degree of spatial attraction toward the incorrect destination.</li> <li>The lack of bimodality in the distribution of trial-by-trial trajectory curvatures suggests that the garden-path effect so frequently associated with this manipulation is not an all-or-none phenomenon\u2014that is, the activation of one structural representation does not forbid simultaneous activation of other possible representations</li> <li>Through a large-scale survey of children's computer use, for example, Calvert, Rideout, Woolard, Barr, and Strouse (2005) found that the mean age at which a child was able to point and click a computer mouse was 3.5 years, and that the mean age of the onset of autonomous computer use was 3.7 years</li> <li>we believe mouse tracking can serve as \"the poor man's eye tracker,\" providing detailed indices of cognitive processing to laboratories that cannot afford expensive eye-tracking equipment.</li> </ul>"},{"location":"Tracking%20the%20Continuity%20of%20Language%20Comprehension%20Computer%20Mouse%20Trajectories%20Suggest%20Parallel%20Syntactic%20Processing/#pictures","title":"Pictures","text":""},{"location":"Tractability/","title":"Tractability","text":"<ul> <li>Let X be the input and Z be the latent representation of X. Every generative model makes the assumption that it's tractable to compute the probability P(Z | X).</li> </ul>"},{"location":"Training-serving%20Skew/","title":"Training-serving Skew","text":"<ul> <li>The difference between a model's performance during training and that same model's performance during serving.</li> </ul>"},{"location":"Trajectory%20Planning/","title":"Trajectory Planning","text":""},{"location":"Trajectory%20Planning/#trajectory-planning","title":"Trajectory Planning","text":"<ul> <li>Scheduled motion to follow, including time information</li> <li>After finding a path, the trajectory definition is completed by the choice of a timing law</li> </ul>"},{"location":"Trajectory%20Planning/#backlinks","title":"Backlinks","text":"<ul> <li>Trajectory Planning</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Trajectory/","title":"Trajectory","text":"<ul> <li>In reinforcement learning, a sequence of tuples that represent a sequence of state transitions of the agent, where each tuple corresponds to the state, action, reward, and next state for a given state transition.</li> </ul>"},{"location":"Transcranial%20Electrical%20Stimulation%20%28tDCS%20and%20tACS%29/","title":"Transcranial Electrical Stimulation (tDCS and tACS)","text":"<ul> <li>A non-invasive procedure that applies electrical stimulation to the scalp to increase or decrease neural signaling. The two main types are direct current stimulation (tDCS) and alternating current stimulation (tACS). They are used for therapeutic purposes as well as to study cognitive processing.</li> </ul>"},{"location":"Transcranial%20Magnetic%20Stimulation%20%28TMS%29/","title":"Transcranial Magnetic Stimulation (TMS)","text":"<ul> <li>A non-invasive procedure that uses the energy from a strong magnet to stimulate changes in neural processing from above the scalp. It is used as a treatment for depression as well as a research method to investigate cognitive processes.</li> </ul>"},{"location":"Transducer/","title":"Transducer","text":"<ul> <li>A device that converts energy from one form to another. Generally, a device that converts an input signal into an output signal of a different form. It can also be thought of as a device which converts static signals detected in the environment (such as pressure) into an electrical signal that is sent to a robot's control system.</li> </ul>"},{"location":"Transfer%20Function/","title":"Transfer Function","text":"<ul> <li>Map a scalar to color and opacity</li> <li>Determines which features of the data are visible / highlighted</li> <li>Can be stored inside a color lookup table (LUT)</li> <li></li> <li></li> <li>Opacity Correction</li> </ul>"},{"location":"Transfer%20Learning/","title":"Transfer Learning","text":"<ul> <li>Transfer learning involves extrapolating a reward function for a new environment based on reward functions from many similar environments that might then transfer in a wrong way</li> </ul>"},{"location":"Transfer%20Learning/#refs","title":"Refs","text":"<ul> <li>openai</li> </ul>"},{"location":"Transferred%20compact%20convolutional%20filters/","title":"Transferred Compact Convolutional Filters","text":"<ul> <li>These methods remove inessential parameters by transferring or compressing the convolutional filters (Zhai et al., 2016).</li> </ul>"},{"location":"Transformer%20Physics/","title":"Transformer Physics","text":"<ul> <li>ratio of the voltages across the coils of a transformer = the ratio of the turns on the coils</li> <li> \\[\\frac{V_{1}}{V_{2}}= \\frac{N_{1}}{N_{2}}\\] </li> </ul>"},{"location":"Transformer-XL/","title":"Transformer-XL","text":"<ul> <li>Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context</li> <li>Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling</li> <li>learning dependency beyond a fixed length without disrupting temporal coherence</li> <li>segment-level recurrence mechanism and a novel positional encoding scheme</li> <li>resolves the context fragmentation problem</li> <li>enwiki8</li> <li>WikiText</li> <li>One Billion Word</li> <li>Penn Treebank</li> </ul>"},{"location":"Transformer/","title":"Transformer","text":"<ul> <li>Encoder Decoder</li> <li>Auto regressive : decoder outputs fed back as inputs to decoder</li> <li>Decoder can access not only the hidden step of the last time step from the encoder, but all the hidden states from the encoder</li> <li>During decoding, consider pairwise relationshop between decoder state and all the returned states from the encoder<ul> <li>Some words relevant, others are not</li> </ul> </li> <li>Transform all hidden states from the encoder into context vectors, that shows how the decoding step is relevant to the input sequences</li> <li>Attention</li> <li>Basic Transformer</li> </ul>"},{"location":"Transformer/#nice-little-blogs","title":"Nice Little Blogs","text":"<ul> <li>lillog</li> </ul>"},{"location":"Transitional%20probabilities/","title":"Transitional Probabilities","text":"<ul> <li> \\[\\text{Prob of }Y|X = \\frac{\\text{freq of} XY}{\\text{freq of }X}\\] </li> <li>Longer listening times for non-words indicates recognition of words</li> <li>Transitional probabilities can be high within words, and low at word boundaries as in Aslin et al</li> <li>But then you can also manipulate the frequency at which each word occurs, and in doing so, also the frequency of the syllables</li> <li>Graf Estes et al</li> <li>Transition frequencies can be made high because two words occur very often next to each other</li> </ul>"},{"location":"Transitive%20verb/","title":"Transitive Verb","text":"<ul> <li>a verb with a direct noun object</li> <li>I cooked a duck belonging to her</li> </ul>"},{"location":"Translational%20Invariance/","title":"Translational Invariance","text":"<ul> <li>In an image classification problem, an algorithm's ability to successfully classify images even when the position of objects within the image changes. For example, the algorithm can still identify a dog, whether it is in the center of the frame or at the left end of the frame.</li> </ul>"},{"location":"Transposed%20Conv/","title":"Transposed Conv","text":"<ul> <li>use a transformation going in the opposite direction of a normal convolution, i.e., from something that has the shape of the output of some convolution to something that has the shape of its input while maintaining a connectivity pattern that is compatible with said convolution.</li> <li>Upsampling</li> <li>Input i, kernel k, padding p, stride s , \\(\\(o = (i-1) \\times s +k -2p\\)\\)</li> <li>Steps<ul> <li>Calculate new Param's z and p'</li> <li>Between each row and columns of the input, insert z number of zeros.\u00a0This increases the size of the input to\u00a0\\((2*i -1) \\times (2*i -1)\\)</li> <li>Pad the modified image with p' no of zeros</li> <li>Standard conv with stride of 1</li> <li></li> </ul> </li> <li></li> </ul>"},{"location":"Transposed%20Conv/#backlinks","title":"Backlinks","text":"<ul> <li>DCGAN</li> <li>Transposed Conv , Batch Normalization and Relu</li> <li>Batch Normalization AFTER Transposed Conv is super important as it helps with flow of gradients</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Trapezoidal%20Trajectory/","title":"Trapezoidal Trajectory","text":""},{"location":"Trapezoidal%20Trajectory/#trapezoidal-trajectory","title":"Trapezoidal Trajectory","text":""},{"location":"Trapezoidal%20Trajectory/#backlinks","title":"Backlinks","text":"<ul> <li>Trapezoidal Trajectory</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Treemap/","title":"Treemap","text":""},{"location":"Trees/","title":"Trees","text":"<ul> <li>Decision Trees</li> <li></li> <li>Node Link Diagram</li> </ul>"},{"location":"Triplet%20Loss/","title":"Triplet Loss","text":"<ul> <li>Given an achor, pull similar closer and push dissimilar away</li> <li>Face recog FaceNet</li> <li>Anchor, positive sample are neigbors while neg isnt</li> <li></li> <li>For each triplet, this condition must hold $\\(||f(x^a) - f(x^p)||^2 + \\alpha \\gt f(x^a) - f(x^n)||^2\\)</li> <li>\\(\\alpha\\) is a margin b/w positive and neg</li> <li>Loss to minimize $\\(L(\\theta) = \\Sigma_i^n||f(x^a) - f(x^p)||^2 + f(x^a) - f(x^n)||^2 + \\alpha\\)</li> <li>Harmonic Triplet Loss</li> </ul>"},{"location":"Tuning%20Model%20Flexibility/","title":"Tuning Model Flexibility","text":"<ul> <li>Class Size</li> <li>Regularization</li> <li>Ridge Regression</li> <li>Adding noise</li> <li>Cross Validation</li> </ul>"},{"location":"Tutor/","title":"Tutor","text":"<ul> <li>However, a tutoring system does not have to replace a teacher or run an after-school remediation session</li> <li>Its role in the student's learning ecology can be anything\u2014a smart piece of paper; an encouraging coach; a stimulating peer, etc. The technology is quite neutral</li> <li>For instance, tutoring systems have been built that do not teach their task domain at all, but instead encourage students to discover its principles (e.g., Shute &amp; Glaser, 1990).</li> </ul>"},{"location":"Two-photon%20Microscopy/","title":"Two-photon Microscopy","text":"<ul> <li>An advanced microscopy technique that uses fluorescent markers to look at living tissue approximately one millimeter below the skin\u2019s surface.</li> </ul>"},{"location":"Types%20of%20Words/","title":"Types of Words","text":"<ul> <li>Content words</li> <li>Function words</li> </ul>"},{"location":"Types%20of%20uncertainty/","title":"Types of Uncertainty","text":"<ul> <li>Aleatoric</li> <li>Epistemic</li> <li>Predictive Uncertainty</li> </ul>"},{"location":"ULMFit/","title":"ULMFit","text":"<ul> <li>English Wikipedia -&gt; [IMDB%20-%3E%20%5B%5BIMDB)] Classifier</li> </ul>"},{"location":"UMA/","title":"UMA","text":"<ul> <li>Uniform memory access</li> <li>SMP</li> <li>Identical processors</li> <li>Identical processors</li> <li>Equal access and access times to memory</li> <li>Cache Coherence</li> </ul>"},{"location":"Ultrasound/","title":"Ultrasound","text":"<ul> <li>Imaging produced by high-frequency sound waves, usually used to view internal organs</li> </ul>"},{"location":"Unawareness/","title":"Unawareness","text":"<ul> <li>A situation in which sensitive attributes are present, but not included in the training data. Because sensitive attributes are often correlated with other attributes of one\u2019s data, a model trained with unawareness about a sensitive attribute could still have disparate impact with respect to that attribute, or violate other fairness constraints.</li> </ul>"},{"location":"Unbiased%20Look%20at%20Dataset%20Bias/","title":"Unbiased Look at Dataset Bias","text":"<ul> <li>Unbiased Look at Dataset Bias</li> <li>Alexei A. Efros Antonio Torralba</li> </ul>"},{"location":"Unbiased%20Look%20at%20Dataset%20Bias/#abstract","title":"Abstract","text":"<ul> <li>some datasets, that started out as data capture efforts aimed at representing the visual world, have become closed worlds unto themselves</li> <li>despite the best efforts of their creators, the datasets appear to have a strong buildin bias</li> <li>Of course, much of the bias can be accounted for by the divergent goals of the different datasets: some captured more urban scenes, others more rural landscapes; some collected professional photographs, others the amateur snapshots from the Internet; some focused on entire scenes, others on single objects, etc</li> <li>Caltech has a strong preference for side views, while ImageNet is into racing cars; PASCAL have cars at noncanonical view-points; SUNS and LabelMe cars appear to be similar, except LabelMe cars are often occluded by small objects, etc</li> <li>Name That Dataset</li> </ul>"},{"location":"Unbiased%20Look%20at%20Dataset%20Bias/#measuring-dataset-bias","title":"Measuring Dataset Bias","text":"<ul> <li> <p>settle for a few standard checks, a diagnostic of dataset health if you will.</p> </li> <li> <p>Cross-dataset generalization</p> </li> <li> <p>Selection bias</p> </li> <li> <p>Capture bias</p> </li> <li> <p>Label bias</p> </li> <li> <p>Negative Set Bias</p> </li> </ul>"},{"location":"Unbiased%20Look%20at%20Dataset%20Bias/#measuring-datasets-value","title":"Measuring Dataset's Value","text":"<ul> <li>two basic ways of improving the performance</li> <li>The first solution is to improve the features, the object representation and the learning algorithm for the detector.</li> <li>The second solution is to simply enlarge the amount of data available for training.</li> <li>So, what is the value of current datasets when used to train algorithms that will be deployed in the real world? The answer that emerges can be summarized as: \"better than nothing, but not by much\".</li> </ul>"},{"location":"Unbiased%20Look%20at%20Dataset%20Bias/#discussion","title":"Discussion","text":"<ul> <li>that the reason is not that datasets are bad, but that our object representations and recognition algorithms are terrible and end up over-learning aspects of the visual data that relates to the dataset and not to the ultimate visual task.</li> <li>In fact, a human learns about vision by living in a reduced environment with many potential local biases and yet the visual system is robust enough to overcome this.</li> <li>Should we care about the quality of our datasets? If the goal is to reduce computer vision to a set of feature vectors that can be used in some machine learning algorithm, then maybe not. But if the goal is to build algorithms that can understand the visual world, then, having the right datasets will be crucial.</li> </ul>"},{"location":"Unbiased%20Look%20at%20Dataset%20Bias/#images","title":"Images","text":""},{"location":"Uncertainity%20in%20classification/","title":"Uncertainty Classification","text":"<ul> <li>Distributions</li> <li>Use Softmax or Sigmoid</li> </ul>"},{"location":"Uncertainity%20in%20regression/","title":"Reg Uncertainty","text":"<ul> <li>LinearRegression</li> <li>Confidence intervals</li> <li>Prob that output belongs to this interval</li> <li>\\(f(x) \\in [a,b]\\)</li> <li>Mean and Variance</li> <li>\\(f(x) \\pm \\sigma\\)</li> <li>\\(f(x) \\in [f(x) - \\sigma,f(x) + \\sigma]\\)</li> </ul>"},{"location":"Uncertainty/","title":"Uncertainty","text":""},{"location":"Uncertainty/#tags-anchor","title":"tags: anchor","text":"<ul> <li>content/Aleatoric.md</li> <li>content/Entropy.md</li> <li>content/Epistemic.md</li> <li>content/Heteroscedatic.md</li> <li>content/Homoscedatic.md</li> <li>content/Inceptionism.md</li> <li>content/Interpretability vs Neuroscience.md</li> <li>content/LIME.md</li> <li>content/Predictive Uncertainty.md</li> <li>content/SHAP.md</li> <li>content/Types of uncertainty.md</li> <li>content/Uncertainity in regression.md</li> </ul>"},{"location":"Undue%20Inducement/","title":"Undue Inducement","text":"<ul> <li>When the value of something received in a clinical trial is so large that the study participant may agree to take risks that are not in their best interests.</li> </ul>"},{"location":"Unet%20Grasping/","title":"Unet Grasping","text":"<ul> <li>Y. Li, L. Schomaker, and H. Kasaei. \"Learning to Grasp 3D Objects using Deep Residual U-Nets.\" RO-MAN 2020.</li> <li>Formulate object grasping as a part segmentation problem</li> <li>Detect graspable shape primitives  </li> <li>The gripper is free to approach objects from arbitrary directions.</li> <li></li> <li></li> </ul>"},{"location":"Unet/","title":"Unet","text":"<ul> <li>Skip Connection</li> </ul>"},{"location":"Unicode%2050/","title":"Unicode 5.0","text":"<ul> <li>UNICODE Consortium 2006</li> <li>100,000 distinct characters</li> <li>75 supported scripts</li> <li>UTF-8 Variable Length Character Encoding<ul> <li>1-4 bytes for each character (max )6</li> <li>ASCII requires 1 byte</li> <li>Alphabetic Systems require 2 bytes</li> <li>Chinese \u2013Japanese-Korean \u2013 3 bytes (sometimes 4 bytes)</li> </ul> </li> </ul>"},{"location":"Uniform%20Distribution/","title":"Uniform Distribution","text":"<ul> <li>If \\(I = [a_{1}, b_{1}]\\times ..\\times[a_{n}, b_{n}]\\) is n dim interval in \\(\\mathbb{R}^{n}\\)</li> <li>PDF \\(\\(p(x) = \\begin{cases}\\frac{1}{(b_{1}-a_{1})\\cdot\u2026\\cdot(b_{n}, a_{n})} &amp; \\text{if } x \\in I\\\\[2ex]0&amp;\\text{if x }\\notin I \\end{cases}\\)\\)</li> <li>No need to learn, no shape that can be specified</li> </ul>"},{"location":"Uniform%20Sampling/","title":"Uniform Sampling","text":"<ul> <li>Random Sampler</li> <li>Uniform Distribution</li> <li>If need to sample from another distribution with a PDF f(x). Can use a uniform Sampler on the distribution [0,1] to indirectly sample from it<ul> <li>Coordinate transform</li> <li>CDF</li> <li>Get a Sampler for by \\(\\(X_{i} = \\varphi^{-1}\\circ U_{i}\\)\\)</li> <li></li> </ul> </li> </ul>"},{"location":"Unique%20Character%20Set/","title":"Unique Character Set","text":"<ul> <li>helps in identifying the language</li> <li>Greek or Hebrew</li> </ul>"},{"location":"Universal%20Approximation%20Theorem/","title":"Universal Approximation Theorem","text":"<ul> <li>What this means that given an x and a y, the NN can identify a mapping between them. \"Approximately\".</li> <li>This is required when we have non linearly separable data.</li> <li>So we take a non linear function, for example the Sigmoid. \\(\\(\\frac{1}{1 + e^{ - \\left( w^{T}x + b \\right)}}\\)\\).</li> <li>Then we have to combine multiple such neurons in a way such that we can accurately model our problem. The end result is a complex function and the existing weights are distributed across many Layers.</li> <li> <p>The Universal approximation theorem states that     &gt; a feed forward network with a single hidden layer containing a finite number of neurons can approximate continuous functions on compact subsets of \\(\\mathbb{R}\\) , under mild assumptions on the activation function.</p> </li> <li> <p>a feed forward network : take an input, apply a function, get an output, repeat</p> </li> <li>a single hidden layer : yes you can use more, but theoretically\u2026</li> <li>finite number of neurons: you can do it without needing an infinite computer</li> <li>approximate continuous functions: continuous functions are anything which dont have breaks/holes in between. This just says that it is possible to approximate the mapping which we talked about \\(\\mathbb{R}\\) is just the set of all real numbers</li> <li>All this boils down to the fact that a neural network can approximate any complex relation given an input and an output.</li> <li></li> <li></li> </ul>"},{"location":"Universal%20Approximation%20Theorem/#refs","title":"Refs","text":"<ul> <li>mm</li> </ul>"},{"location":"Universal%20Quantifiers/","title":"Universal Quantifiers","text":"<ul> <li>All the boys are building a snowman</li> <li>Each boy is building a snowman</li> <li>Two universally quantified sentences that involve/exhaust the totality of boys</li> <li>Collective Interpretation</li> <li>Distributive Interpretation</li> <li>Distributive Interpretation (2)</li> <li>Cumulative Interpretation</li> </ul>"},{"location":"Unrestricted%20Race%20Model/","title":"Unrestricted Race Model","text":"<ul> <li>The Unrestricted Race model (Traxler, Pickering, &amp; Clifton, 1998; van Gompel, Pickering, Pearson, &amp; Liversedge, 2005; van Gompel, Pickering, &amp; Traxler, 2001) follows in the footsteps of constraint-based models in proposing simultaneous integration of multiple constraints from statistical, semantic, and contextual sources</li> <li>However, rather than ambiguity resolution being based on a temporally dynamic competition process, the Unrestricted Race model posits an instantaneous probabilistic selection among the weighted alternatives of an ambiguity.</li> <li>much like the syntax-first models, it must hypothesize a separate reanalysis mechanism that is responsible for garden-path effects when the initial selected alternative turns out to be syntactically or semantically inappropriate.</li> <li>the Unrestricted Race model predicts that sentences with garden-paths and sentences without garden-paths are two separate populations of events</li> <li>In other words, in conditions where mean performance is expected to exhibit a garden-path effect, there should exist one of two possible patterns: (a) a bimodal distribution of some substantial gardenpath responses and some non-gardenpath responses, or (b) practically all trials exhibiting substantial garden-path effect</li> </ul>"},{"location":"Unsupervised%20Data%20Generation/","title":"Unsupervised Data Generation","text":"<ul> <li>training data creation procedure named Unsupervised Data Generation (UDG), which leverages few-shot prompts to synthesize high-quality training data without real human annotations</li> </ul>"},{"location":"Unsupervised%20Learning/","title":"Unsupervised Learning","text":"<ul> <li>Discover useful things from raw data</li> <li>Representation/Embedding Learning</li> <li>If labels , train network and take intermediate Layers</li> <li>Clustering</li> <li>PCA</li> <li>Feature Learning</li> <li>Hidden Models</li> <li>Generative Models</li> <li>Anomaly Detection</li> <li>Auto Encoders</li> </ul>"},{"location":"Unsupervised%20Learning/#_1","title":"\u2026","text":""},{"location":"Uplift%20Modeling/","title":"Uplift Modeling","text":"<ul> <li>A modeling technique, commonly used in marketing, that models the \"causal effect\" (also known as the \"incremental impact\") of a \"treatment\" on an \"individual.\" Here are two examples</li> <li>Doctors might use uplift modeling to predict the mortality decrease (causal effect) of a medical procedure (treatment) depending on the age and medical history of a patient (individual).</li> <li>Marketers might use uplift modeling to predict the increase in probability of a purchase (causal effect) due to an advertisement (treatment) on a person (individual).</li> </ul>"},{"location":"Upweighting/","title":"Upweighting","text":"<ul> <li>Applying a weight to the downsampled class equal to the factor by which you downsampled.</li> </ul>"},{"location":"Useful%20Codes/","title":"Useful Codes","text":"<ul> <li>Parallel Runner</li> </ul>"},{"location":"Utilitarian%20ethics/","title":"Utilitarian Ethics","text":"<ul> <li>resulting decisions often aim to produce the best aggregate consequences</li> </ul>"},{"location":"Utilitarian%20ethics/#backlinks","title":"Backlinks","text":"<ul> <li>Building Ethics into Artificial Intelligence</li> <li>Utilitarian ethics</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"VAE/","title":"Variational Autoencoder","text":"<ul> <li>Some control over distribution of learned features</li> <li>Eg: Decoder as a generative model</li> <li>Constraint loss function and a given Probability \\(\\mathcal{D}\\)<ul> <li>Eg: By Loss func KL Divergence and prob distribution $\\(L(X) = n^{-1}\\Sigma_i||x_i - D(E(\\tilde x))||^2 + \\lambda \\cdot KL(f_i, d)\\)</li> <li>Use 2D unit distribution. 0 mean, unit variance</li> <li>Latent vector : \\(\\(f=\\mu + \\epsilon e^{2log\\sigma}\\)\\)</li> </ul> </li> <li>$\\(L(X) = n^{-1}\\Sigma_i||x_i - D(E(\\tilde x))||^2 + \\frac{1}{2n}\\Sigma_i(e^{log\\sigma(x_i)} + \\mu(x_i)^2 -1 -log(\\sigma (x_i))\\)</li> <li>Encoder predicts mean and std \\(\\(E(x_i) = (\\mu(x_i) , log \\sigma(x_i))\\)\\)</li> </ul>"},{"location":"VGGFace2%203/","title":"VGGFace2","text":"<ul> <li>A dataset for recognising faces across pose and age.</li> <li>The VGGFace2 dataset is made of around 3.31 million images divided into 9131 classes, each representing a different person identity.</li> </ul>"},{"location":"VGGish/","title":"VGGish","text":"<ul> <li>CNN Architectures for Large-Scale Audio Classification</li> <li>applying various state-of-the-art image networks with CNN architectures to audio and show that they are capable of excellent results on audio classification</li> <li>examine fully connected deep neural networks such as AlexNet, VGG, InceptionNet, and ResNet</li> <li>The input audio is divided into non-overlapping 960 ms frames which are decomposed by applying the Fourier transform, resulting in a spectrogram</li> <li>spectrogram is integrated into 64 mel-spaced frequency bins, and the magnitude of each bin is log-transformed</li> <li>gives log-mel spectrogram patches that are passed on as input to all classifiers</li> <li>Acoustic Event Detection</li> <li>train a classifier on embeddings learned from the video-level task on AudioSet</li> <li>model for AED with embeddings learned from these classifiers does much better than raw features on the Audio Set AED classification task</li> <li>derivatives of image classification networks do well on the audio classification task</li> <li>increasing the number of labels they train on provides some improved performance over subsets of labels</li> <li>performance of models improves as they increase training set size,</li> <li>model using embeddings learned from the video-level task do much better than a baseline on the AudioSet classification task</li> </ul>"},{"location":"VICReg/","title":"VICReg","text":"<ul> <li>VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning</li> <li>Self Supervised</li> <li>based on maximizing the agreement between Embedding vectors from different views of the same image</li> <li>rivial solution is obtained when the encoder outputs constant vectors</li> <li>Mode Collapse is often avoided through implicit biases</li> <li>explicitly avoids the collapse problem with a simple regularization term on the variance of the embeddings along each dimension individually</li> <li>triple objective: learning invariance to different views with a invariance term, avoiding collapse of the representations with a variance preservation term, and maximizing the information content of the representation with a covariance regularization term</li> <li>Bias Vs Variance</li> <li>combines the variance term with a decorrelation mechanism based on redundancy reduction and covariance regularization</li> <li>does not require the embedding branches to be identical or even similar</li> </ul>"},{"location":"VL-BEIT/","title":"VL-BEIT","text":"<ul> <li>VL-BEIT: Generative Vision-Language Pretraining</li> <li>vision-language foundation model</li> <li>simple and effective approach to pretraining a bidirectional multimodal Transformer encoder for both vision-language and vision tasks learned by generative pretraining</li> <li>conducts masked prediction on both monomodal and multimodal data with a shared Transformer</li> <li>solely employs generative pretraining tasks, including masked language modeling on texts, masked image modeling on images, and masked vision-language modeling on image-text pairs</li> <li>learned from scratch with one unified pretraining task, one shared backbone, and one-stage training which renders it conceptually simple and empirically effective</li> <li>transferable visual features</li> </ul>"},{"location":"VQAv2%203/","title":"VQAv2","text":""},{"location":"VTAB/","title":"VTAB","text":"<p>title: VTAB</p> <p>tags: dataset</p>"},{"location":"VTAB/#vtab","title":"VTAB","text":""},{"location":"Vacuum%20Cup%20Hand/","title":"Vacuum Cup Hand","text":"<ul> <li>An end-effector for a robot arm which is used to grasp light to moderate weight objects, using suction, for manipulation. Such objects may include glass, plastic; etc. Commonly used because of its virtues of reduced object slide slipping while within the grasp of the vacuum cup.</li> </ul>"},{"location":"Vagus%20Nerve/","title":"Vagus Nerve","text":"<ul> <li>One of the twelve pairs of cranial nerves in the human body, the vagus nerve connects the brain stem to the body, transmitting information from the brain to the major organs and other tissues.</li> </ul>"},{"location":"Vanishingexploding%20gradients/","title":"Vanishing/exploding #gradients","text":"<ul> <li>Ill conditioning - Something like a pit of despair</li> <li> \\[\\nabla_xE = W^T(g'(a)\\circ \\nabla_y E)\\] <ul> <li> \\[g' = 1-g^2\\] </li> </ul> </li> <li>Active neurons saturate -&gt; prevent error #Backprop</li> <li> \\[g(a) \\approx 1 \\rightarrow \\nabla_xE \\approx 0\\] </li> <li>W is initialized with random values &lt;&lt; 1 -&gt; gradient decays exponentially in each layer (max eigenvalue of weight matrix)</li> <li>Solutions<ul> <li>Regularization , Optimizers , Architectures</li> </ul> </li> </ul>"},{"location":"Vanishingexploding%20gradients/#_1","title":"\u2026","text":""},{"location":"VarGrad/","title":"VarGrad","text":""},{"location":"VarGrad/#backlinks","title":"Backlinks","text":"<ul> <li>Vision Explainibility</li> <li>VarGrad</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Variable%20Importances/","title":"Variable Importances","text":"<ul> <li>A set of scores that indicates the relative importance of each feature to the model.</li> </ul>"},{"location":"Variation%20in%20Dissimilarity%20Variation%20in%20Dissimilarity/","title":"Variation in Dissimilarity Variation in Dissimilarity","text":"<ul> <li>is the variance of the NISSIM metric over the adversarial set over different levels of attack eps for a model, this shows the distribution of the attack on the model when different levels of attack are performed</li> <li>Ideally, we would want the distribution to be stable for different levels of attack</li> <li> \\[m_{h}= \\frac{1}{eps}\\Sigma(NISSIM_{eps})\\] </li> <li> \\[VID = \\sqrt{\\frac{\\Sigma(NISSIM_{eps}-m_{h})^{2}}{eps}}\\] </li> </ul>"},{"location":"VariationalRecurrent%20Dropout/","title":"Variational/Recurrent Dropout","text":"<ul> <li>Basic RNN Architectures</li> <li>Only on the non Recurrent parts such as inputs and outputs</li> <li>In recorrent parts, use the same dropout mask for all time steps</li> <li>Same dropout mask for each time step</li> <li></li> </ul>"},{"location":"VariationalRecurrent%20Dropout/#_1","title":"\u2026","text":""},{"location":"Vector%20Assembly%20level/","title":"Vector Assembly Level","text":""},{"location":"Vector%20Chaining/","title":"Vector Chaining","text":"<ul> <li>Equivalent to data forwarding in vector processors</li> <li>Results of one pipeline are fed into operand registers of another pipeline</li> </ul>"},{"location":"Vector%20Functional%20Units/","title":"Vector Functional Units","text":"<ul> <li>Fully pipelined, new operation every cycle</li> <li>Performs arithmetic and logic operations</li> <li>Typically 4-8 different units</li> </ul>"},{"location":"Vector%20Load%20Store%20Units/","title":"Vector Load Store Units","text":"<ul> <li>Moves vectors between memory and registers</li> </ul>"},{"location":"Vector%20Processor/","title":"Vector Processor","text":"<ul> <li>Memory to Memory Architecture</li> <li>Register to Register Architecture</li> <li>Vector Register</li> <li>Scalar Register</li> <li>Vector Functional Units</li> <li>Vector Load Store Units</li> <li>Strip Mining</li> <li>Vector Chaining</li> <li>Scatter and Gather</li> <li>Pipes</li> <li>Vector Assembly level</li> </ul>"},{"location":"Vector%20Quantization/","title":"Vector Quantization","text":"<ul> <li>Partitioned into k cells whose center of Gravity vectors are indexed</li> <li>Indices used as symbolic encodings</li> <li>Discretization</li> </ul>"},{"location":"Vector%20Register/","title":"Vector Register","text":"<ul> <li>Typically 8-32 vector registers with 64 -128 64-bit elements</li> <li>Each contains a vector of double-precision numbers</li> <li>Register size determines the maximum vector length</li> <li>Each includes at least 2 read and 1 write ports</li> </ul>"},{"location":"Vectorization/","title":"Vectorization","text":"<ul> <li>Hardware primitives</li> <li>Prioritize those are contiguous in memory</li> </ul>"},{"location":"Velocity/","title":"Velocity","text":"<ul> <li>Displacement wrt time</li> <li> \\[v_{avg}= \\frac{\\Delta{x}}{\\Delta t}\\] </li> </ul>"},{"location":"Verb/","title":"Verb","text":"<ul> <li>Actions, relationships</li> <li>Transitive verb</li> <li>DiTransitive verb</li> <li>Action Transitive verb</li> </ul>"},{"location":"Vertebral%20Arteries/","title":"Vertebral Arteries","text":"<ul> <li>The major arteries of the neck, which merge to form the basilar artery.</li> </ul>"},{"location":"Vestibular%20System/","title":"Vestibular System","text":"<ul> <li>Regions in the body and brain that help support balance in movement. Many people with hearing loss experience some degree of balance difficulties, since the vestibular (or balance) system and the auditory (or hearing) systems are so closely related.</li> </ul>"},{"location":"Vgg/","title":"Vgg","text":"<ul> <li>Very Deep Convolutional Networks for Large-Scale Image Recognition</li> <li>Deeper Alex Net</li> <li>Object detection and Image captioning</li> <li>5x5 -&gt; two 3x3</li> <li>No of filters increase according to depth</li> <li>No of filters : increase by power of two</li> <li>Filter size : Odd numbers</li> <li>SGD + LR Schedule</li> <li>three non-linear activations (instead of one), which makes the function more discriminative</li> </ul>"},{"location":"ViLT/","title":"ViLT","text":"<ul> <li>ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision</li> <li>Vision-and-Language Transformer</li> <li>seeks to improve performance on various joint vision-and-language downstream tasks</li> <li>Current approaches to VLP heavily rely on image feature extraction processes using convolutional visual embedding networks (e.g., Faster R-CNN and ResNets), which involve region supervision (e.g., object detection) and the convolutional architecture (e.g., ResNet)</li> <li>This is problematic in terms of both efficiency/speed, in that extracting input features requires much more computation than the multimodal interaction steps; and expressive power, as it is upper bounded to the expressive power of the visual embedder and its predefined visual vocabulary.</li> <li>minimal VLP model, which is monolithic in that the processing of visual inputs is drastically simplified to just the same convolution-free manner that they process textual inputs</li> <li>removing the need for object detectors</li> <li>avoiding heavyweight image encoders by directly embedding low-level pixel data with a single-layer projection and achieves similar results with reduced complexity,</li> <li>Self-supervision is accomplished using (i) Image Text Matching (ITM) loss and (ii) Masked Language Model (MLM) loss</li> <li>ITM Loss</li> <li>For text, ViLT simply reuses Masked Language Model - (MLM), used in BERT.</li> <li>MSCOCO</li> <li>Visual Genome</li> <li>SBU Captions</li> <li>Google Conceptual Captions</li> <li>VQAv2</li> <li>NLVR2</li> <li>Flickr30K</li> <li>ViLT is over 10x faster than previous VLP models, yet with competitive or better downstream task performance</li> <li>VLP needs to focus more on the multi-modality interactions aspect inside the transformer module rather than engaging in an arms race that merely powers up unimodal embedders</li> </ul>"},{"location":"Viewpoint%20Feature%20Histogram/","title":"Viewpoint Feature Histogram","text":""},{"location":"Viewpoint%20Feature%20Histogram/#viewpoint-feature-histogram","title":"Viewpoint Feature Histogram","text":"<ul> <li>VFH produces a histogram that encodes the geometry of the object and its viewpoint.</li> <li>For every pair of a point and the center of mass, a reference frame is constructed and the three angular variations are computed (a, q, f) and also d which represents distance between (point, center of mass)</li> <li>Another statistical feature is computed between the central viewpoint direction and the normal estimated at each point.</li> <li>The quality of VFH description depends on the quality of the surface normal estimation at each point.</li> <li></li> </ul>"},{"location":"Viewpoint%20Feature%20Histogram/#backlinks","title":"Backlinks","text":"<ul> <li>Viewpoint Feature Histogram</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Virtue%20ethics/","title":"Virtue Ethics","text":"<ul> <li>an agent is ethical if and only if it acts and thinks according to some moral values</li> <li>Agents with virtue ethics should exhibit an inner drive to be perceived favourably by others</li> </ul>"},{"location":"Virtue%20ethics/#backlinks","title":"Backlinks","text":"<ul> <li>Building Ethics into Artificial Intelligence</li> <li>Virtue ethics</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Vision%20Explainibility/","title":"Vision Explainibility","text":""},{"location":"Vision%20Explainibility/#links-useful","title":"Links Useful","text":"<ul> <li>Captum Algos Comparison</li> </ul>"},{"location":"Vision%20Explainibility/#flow","title":"Flow","text":"<ul> <li>DeconvNet (2013)</li> <li>Deep Inside Convolutional Networks (2014)</li> <li>Guided BackProp (2015) Aka All Conv net<ul> <li>Building up on Deep Inside Convolutional Networks and DeconvNet</li> </ul> </li> <li>Salience Map<ul> <li>Not class discriminative</li> <li>Noise</li> <li>Not appealing</li> </ul> </li> <li>CAM<ul> <li>less noisy</li> <li>not class discriminative</li> <li>Worked only a restricted set of CNN templates</li> </ul> </li> <li>Grad-CAM<ul> <li>class discriminative</li> <li>not high res</li> <li>Works for any arbitrary CNN</li> </ul> </li> <li>Occlusion Map<ul> <li>Same as the next but not very fast</li> </ul> </li> <li>Guided GradCAM</li> <li>DeepLIFT</li> <li>Noise Tunnel</li> <li>Smooth-Grad</li> <li>SmoothGrad Square</li> <li>VarGrad</li> <li>Integrated Gradients</li> <li>Proxy Attention</li> <li>Conductance</li> </ul>"},{"location":"Vision%20Explainibility/#disadvantages","title":"Disadvantages","text":"<ul> <li>The Unreliability of Saliency Methods</li> <li>Interpretation of Neural networks is fragile</li> <li>Fine grained data</li> </ul> <pre><code>\ngraph TD;\nE2[DeconvNet] --&gt; E1\nE3[Deep_Inside_Convolutional_Networks] --&gt; E1\nE1[Guided_BackProp]\n\nB1[CAM] --&gt; B2[GradCAM] --&gt; B3[GradCAM++]\nB2 --&gt; B4[Guided_GradCAM]\n\nE4[Network In Network] --&gt; B1\n\n\nE1 --&gt; B4\n\nE2 --&gt; A[Salience_Map]\nE3 --&gt; A\nE1 --&gt; A\n\nC0[Integrated Gradients] --&gt; C1\nE1 --&gt; C1\nC1[Smooth-Grad] --&gt; C4\nC2[SmoothGrad Square] --&gt; C4\nC3[VarGrad] --&gt; C4\nC0 --&gt; C5[Conductance]\nC4[Noise Tunnel] --&gt; P\nC0 --&gt; P\nC4 --&gt; P\nA --&gt; P\nB2 --&gt; P\n\nsubgraph proposed\nP([Proxy Attention])\nend\n\nU1[The Unreliability of Saliency Methods] --Changes break saliency--&gt; A\nU2[Interpretation of Neural networks is fragile] --Adversarial Attacks--&gt; A\nclass A internal-link\nclass B internal-link\nclass B1 internal-link\nclass B2 internal-link\nclass B3 internal-link\nclass B4 internal-link\nclass C internal-link\nclass C0 internal-link\nclass C1 internal-link\nclass C2 internal-link\nclass C3 internal-link\nclass C4 internal-link\nclass C5 internal-link\nclass D internal-link\nclass E internal-link\nclass F internal-link\nclass E1 internal-link\nclass E2 internal-link\nclass E3 internal-link\nclass E4 internal-link\nclass U1 internal-link\nclass U2 internal-link\nclass P internal-link\n</code></pre>"},{"location":"Vision%20Explainibility/#backlinks","title":"Backlinks","text":"<ul> <li> <p>12:01 Vision Explainibility</p> </li> <li> <p></p> </li> <li>11:05 Bunch of things today. First I have a thesis presentation Vision Explainibility, then an article on Masked Language Modeling and then Cogmod</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Vision%20Transformer/","title":"Vision Transformer","text":"<p>title: Vision Transformer</p> <p>tags: architecture</p>"},{"location":"Vision%20Transformer/#vision-transformer","title":"Vision Transformer","text":"<ul> <li>paper</li> <li></li> <li>Transformer applied directly to sequences/patches of images</li> <li>Lower computational resources</li> <li>ImageNet , CIFAR, VTAB</li> <li>Do Vision Transformers See Like Convolutional Neural Networks?</li> <li>analyzes the internal representation structure of ViTs and Conv on image classification benchmarks</li> <li>striking differences in the features and internal structures between the two architectures</li> <li>ViT having more uniform representations across all layers</li> <li>early aggregation of global information</li> <li>spatial localization</li> <li>discovering ViTs successfully preserve input spatial information with CLS tokens</li> <li>finding larger ViT models develop significantly stronger intermediate representations through larger pretraining datasets</li> <li>MLP-Mixer</li> </ul>"},{"location":"Visual%20Associative/","title":"Visual Associative","text":"<ul> <li>Can this variable allow us to spontaneously group items in a group? [i.e., 1 group from all]</li> </ul>"},{"location":"Visual%20Associative/#backlinks","title":"Backlinks","text":"<ul> <li>Characteristics of Visual Variables</li> <li>Visual Associative</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Visual%20Commonsense%20Reasoning/","title":"Visual Commonsense Reasoning","text":"<ul> <li>Given a challenging question about an image, a machine must answer correctly and then provide a rationale justifying its answer</li> <li>From Recognition to Cognition: Visual Commonsense Reasoning</li> <li>290k multiple choice QA problems derived from 110k movie scenes</li> <li>Adversarial Learning</li> </ul>"},{"location":"Visual%20Context%20Augmentation/","title":"Visual Context Augmentation","text":"<ul> <li>learns to place object instances at an image location depending on the surrounding context.</li> <li>A neural network is trained for this purpose.</li> <li>The training data is pre- pared to generate a context image with the masked- out object inside it.</li> <li>From an image, 200 context sub-images are generated surrounding the blacked- out bounding box. The neural network learns to predict the category (object or background) in masked pixels.</li> <li>The object instances are placed inside the selected boxes to generate a new train- ing image</li> </ul>"},{"location":"Visual%20Cortex/","title":"Visual Cortex","text":"<ul> <li>The area of the cerebrum that is specialized for vision. It lies primarily in the occipital lobe at the rear of the brain and is connected to the eyes by the optic nerves.</li> </ul>"},{"location":"Visual%20Cortex/#backlinks","title":"Backlinks","text":"<ul> <li>ACT-R</li> <li>Visual Cortex</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Visual%20Encoding/","title":"Visual Encoding","text":"<ul> <li>Characteristics of Visual Variables</li> </ul>"},{"location":"Visual%20Genome/","title":"Visual Genome","text":""},{"location":"Visual%20Implicit%20Learning/","title":"Visual Implicit Learning","text":"<ul> <li>Does ability to learn implicit dependencies correlate with ability to correctly predict the next word in speech?</li> <li>Participants saw a sequence of colors light up on screen</li> <li>They then had to reproduce it by clicking on the same sequence</li> <li>Auditory Only Sentence Perception</li> <li>25 highly predictable and 25 zero-predictability sentences</li> <li>acoustically degraded by processing them with a sinewave vocoder to 6 channels</li> <li>Ability to pick up/learn statistical dependencies seems almost to be a new cognitive function</li> <li>Shows a potential connection between sequence learning and language modelling ability</li> <li>Learning simple dependencies is not so difficult it seems. But natural language has many dependencies at a distance</li> </ul>"},{"location":"Visual%20Length/","title":"Visual Length","text":"<ul> <li>Across how many changes in this variable are distinctions possible? [i.e., how many can I see?]</li> </ul>"},{"location":"Visual%20Length/#backlinks","title":"Backlinks","text":"<ul> <li>Characteristics of Visual Variables</li> <li>Visual Length</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Visual%20Ordered/","title":"Visual Ordered","text":"<ul> <li>Can this variable allow us to spontaneously perceive an order [i.e., what is smaller and what is bigger?]</li> </ul>"},{"location":"Visual%20Ordered/#backlinks","title":"Backlinks","text":"<ul> <li>Characteristics of Visual Variables</li> <li>Visual Ordered</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Visual%20Quantitative/","title":"Visual Quantitative","text":"<ul> <li>Can the difference between two marks in this variable be interpreted numerically? [i.e., corresponds to 5?]</li> </ul>"},{"location":"Visual%20Quantitative/#backlinks","title":"Backlinks","text":"<ul> <li>Characteristics of Visual Variables</li> <li>Visual Quantitative</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Visual%20Selective/","title":"Visual Selective","text":"<ul> <li>Can this variable allow us to spontaneously differentiate/isolate items from groups? [i.e., 1 item from all]</li> </ul>"},{"location":"Visual%20Selective/#backlinks","title":"Backlinks","text":"<ul> <li>Characteristics of Visual Variables</li> <li>Visual Selective</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Visual%20Servo%20System/","title":"Visual Servo System","text":"<ul> <li>The task in visual servoing is to use visual information to control the robot\u2019s end-effector relative to a target object</li> <li></li> <li>Chaumette, Franc\u0327ois, and Seth Hutchinson. \"Visual servo control. II. Advanced approaches [Tutorial].\" IEEE Robotics &amp; Automation Magazine 14.1 (2007): 109-118.</li> <li>Morrison, Douglas, Peter Corke, and Ju\u0308rgen Leitner. \"Closing the loop for robotic grasping: A real-time, generative grasp synthesis approach.\" RSS (2018).</li> <li></li> <li></li> <li></li> </ul>"},{"location":"Visualization%20Of%20Layers/","title":"Visualization Of Layers","text":"<ul> <li>Tanh</li> <li>\\(tanh(Wx+b)\\)</li> <li> <ol> <li>A linear transformation by the \u201cweight\u201d matrix\u00a0\\(W\\)</li> </ol> </li> <li> <ol> <li>A translation by the vector\u00a0\\(b\\)</li> </ol> </li> <li> <ol> <li>Point-wise application of tanh.</li> </ol> </li> </ul>"},{"location":"Volterra%20expansion/","title":"Volterra","text":"<ul> <li>Add higher order polynomials</li> <li>Adding too much leads to combinatorial explosion -&gt; Pruning scheme</li> <li>Adding all polynomials of degree 2<ul> <li>(\\(d + d(d+1) /2\\)\\) input components<ul> <li> \\[{u_1, u_2, \u2026,u_d} \\cup {u_iu_j | 1 \\leq i \\leq j \\leq d}\\] </li> </ul> </li> </ul> </li> </ul>"},{"location":"Volume%20Rendering%20Equation/","title":"Volume Rendering Equation","text":"<ul> <li>Light-emitting particles fill volume</li> <li>Emission-absorption mode</li> <li>Based on a physical model for radiation</li> <li>Interaction of light with matter at the macroscopic scale, neglecting Diffraction, Interference, Wave-character, Polarization, etc.</li> <li>\\(\\kappa\\) is fraction of absorbed light</li> <li>\\(g\\) is fraction of emitted light</li> <li> \\[\\frac{dL}{ds} = g(s) - \\kappa(s)L(s)\\] </li> <li>aka Emission -Absorption</li> <li> \\[T(s_{1}, s_{2}) = e^{-\\int_{s_{1}}^{s_{2}}\\kappa(s')ds'}\\] </li> <li> \\[L_{1}^{n}= g(n)+T(n)L_{1}^{n-1}\\] </li> </ul>"},{"location":"Volume%20Visualization/","title":"Volume Visualization","text":"<ul> <li>Orthogonal Slicing</li> <li>Oblique Slicing</li> <li>Isosurface</li> <li>Volumetric Illumination</li> </ul>"},{"location":"Volumetric%20Grasping%20Network/","title":"Volumetric Grasping Network","text":"<ul> <li>M. Breyer, et al., \"Volumetric Grasping Network: Real-time 6 DOF Grasp Detection in Clutter\", ICLR 2020</li> <li>Learning to grasp 3D objects by constructing a full model of the scene</li> <li>TSDF</li> <li></li> </ul>"},{"location":"Volumetric%20Illumination/","title":"Volumetric Illumination","text":"<ul> <li>Phong Lighting</li> <li>Finite Differences</li> <li>Shading</li> <li>Raycasting</li> </ul>"},{"location":"Von%20Neumann%20Architecture/","title":"Von Neumann Architecture","text":"<p>-</p>"},{"location":"Voronoi%20Cell/","title":"Voronoi Cell","text":"<ul> <li>Partition a plane into n convex polygons -&gt; each containing one generating point and every point is closer to its generating point than others</li> <li></li> </ul>"},{"location":"Voronoi%20Cell/#refs","title":"Refs","text":"<ul> <li>wolf</li> </ul>"},{"location":"Voxel%20Projection/","title":"Voxel Projection","text":"<ul> <li>Volume = field of 3D interpolation kernels  </li> <li>One kernel at each grid voxel  </li> <li>Each kernel leaves a 2D footprint on screen</li> <li>Weighted footprints accumulate into image</li> <li></li> </ul>"},{"location":"WMT14/","title":"WMT14","text":""},{"location":"WOMBO%20Dream/","title":"WOMBO Dream","text":"<ul> <li>The app creates generative art from a descriptive text in various pre-determined styles.</li> <li>The app uses two machine learning technologies that combine a neural network to generate images and an algorithm that interprets text descriptions.</li> <li>Both algorithms learn from each iteration, meaning that every request generates a unique outcome.</li> </ul>"},{"location":"Wall%20Street%20Journal%20task/","title":"Wall Street Journal Task","text":""},{"location":"WaveGlow/","title":"WaveGlow","text":"<ul> <li>WaveGlow: a Flow-based Generative Network for Speech Synthesis</li> <li>flow-based network capable of generating high quality speech from mel-spectrograms</li> <li>combines insights from Glow and WaveNet in order to provide fast, efficient and high-quality audio synthesis, without the need for auto-regression</li> <li>mplemented using only a single network, trained using only a single cost function: maximizing the likelihood of the training data, which makes the training procedure simple and stable</li> <li>more than 500 kHz on an NVIDIA V100 GPU</li> <li>Mean Opinion Scores</li> </ul>"},{"location":"Weak%20Relation%20Bias/","title":"Weak Relation Bias","text":"<ul> <li>relationship between the neural units is weak, meaning that they\u2019re somewhat independent of each other. The choice of including a fully connected layer in the net can represent this kind of relationship</li> <li></li> </ul>"},{"location":"WebGPT/","title":"WebGPT","text":"<ul> <li>WebGPT: Browser-assisted Question-answering with Human Feedback</li> <li>fine-tuned version of GPT-3 to more accurately answer open-ended questions using a text-based web browser.</li> <li>submits search queries, follows links, and scrolls up and down web pages</li> <li>trained to cite its sources</li> <li>By setting up the task so that it can be performed by humans, they are able to train models on the task using imitation learning</li> <li>models must collect references while browsing in support of their answers</li> <li>ELI5</li> <li>dataset of questions asked by Reddit users</li> <li>fine-tuning GPT-3 using behavior cloning, and then performing rejection sampling against a reward model trained to predict human preferences</li> </ul>"},{"location":"Weight/","title":"Weight","text":"<ul> <li>mass x gravitational field\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 strength</li> <li> \\[w = mg\\] </li> </ul>"},{"location":"Weighted%20Alternating%20Least%20Squares/","title":"Weighted Alternating Least Squares","text":"<ul> <li>An algorithm for minimizing the objective function during matrix factorization in recommendation systems, which allows a downweighting of the missing examples. WALS minimizes the weighted squared error between the original matrix and the reconstruction by alternating between fixing the row factorization and column factorization.</li> </ul>"},{"location":"Wernicke%20Area/","title":"Wernicke Area","text":"<ul> <li>Damage to this area causes Wernicke's aphasia.</li> <li>The individual may speak in long sentences that have no meaning, add unnecessary words, and even create new words.</li> <li>They can make speech sounds, however they have difficulty understanding speech and are therefore unaware of their mistakes.</li> </ul>"},{"location":"Wh-dependencies/","title":"Wh-dependencies","text":"<ul> <li>[What type of network] do you plan to build [t]?</li> <li>Nobody knows [what] the brain is doing [t].</li> </ul>"},{"location":"Wickelphones/","title":"Wickelphones","text":"<ul> <li>Pro: capture just enough info about the context that determines irregular past-tense verb forms ense verbs, e.g.sing -sang, ring -rang</li> <li>1 wickelphone = 1 input unit and 1 output unit, connection matrix of 42875 * 42875!!</li> <li>Instead, reduce wickelphones to wickelfeatures (1210), where each wickelphone becomes 16 features</li> <li>Verbs with same stem and past tense</li> <li>English has many verbs where the past and stem are the same:<ul> <li>put, fit, spread</li> </ul> </li> <li>Usually verbs ending with -t or -d are likely have no-change</li> <li>Model also started to let the past be the same as the stem</li> <li>Verbs with vowel change</li> <li>Model made two errors older children have been shown to make: 1. stem + ed, e.g. comed, singed 2. past-form + ed, e.g. camed, sanged</li> <li>Does not explain differences in response times between irregular and regular verbs</li> <li>Models production only, not comprehension</li> <li>You can't reverse the model like you can reverse a rule</li> <li>Model can't generalize</li> <li>Computational models (Rule-based, Connectionist and MBL) all use adult vocabulary as input to simulate children's learning</li> <li>Children don't show vocabulary burst between State 2 and 3 (needed to produce U-shaped learning with Connectionist Model)</li> <li>Thousands of exposures</li> <li>No distinction between tokens and types, each verb simply included once</li> </ul>"},{"location":"Wide%20Deep%20Recommender/","title":"Wide Deep Recommender","text":"<ul> <li>Wide &amp; Deep Learning for Recommender Systems</li> <li>Generalized linear models with nonlinear feature transformations are widely used for large-scale regression and classification problems with sparse inputs.</li> <li>Memorization of feature interactions through a wide set of cross-product feature transformations are effective and interpretable, while generalization requires more feature engineering effort</li> <li>However, memorization and generalization are both important for recommender systems.</li> <li>With less feature engineering, deep neural networks can generalize better to unseen feature combinations through low-dimensional dense embeddings learned for the sparse features</li> <li>However, deep neural networks with embeddings can over-generalize and recommend less relevant items when the user-item interactions are sparse and high-rank.</li> <li>jointly trained wide linear models and deep neural networks \u2013 to combine the benefits of memorization and generalization for recommender systems</li> <li>Wide linear models can effectively memorize sparse feature interactions using cross-product feature transformations, while deep neural networks can generalize to previously unseen feature interactions through low dimensional embeddings</li> <li>In other words, the fusion of wide and deep models combines the strengths of memorization and generalization, and provides us with better recommendation systems</li> <li>The two models are trained jointly with the same loss function.</li> <li>Google Play Store</li> </ul>"},{"location":"Window%20Based%20Regression/","title":"Window Based Regression","text":"<ul> <li>TIme Series</li> <li>input window : \\(\\(u(t-d+1), u(t-d+2), \u2026. , u(t-1) , u(t)\\)\\)</li> <li>Require regression function (\\(f:(\\mathbb{R}^k)^d \\rightarrow \\mathbb{R}^m\\)\\)<ul> <li>\\(\\(k \\times d\\)\\) dim matrix</li> <li>Flatten into \\(\\(d \\cdot k\\)\\) vector and apply Quadratic Loss</li> </ul> </li> </ul>"},{"location":"Window%20Based%20Regression/#non-linearity","title":"Non Linearity","text":"<ul> <li>Add fixed nonlinear transforms to input arguments : eg polynomials</li> <li>Volterra expansion</li> </ul>"},{"location":"Wisdom%20of%20the%20Crowd/","title":"Wisdom of the Crowd","text":"<ul> <li>The idea that averaging the opinions or estimates of a large group of people (\"the crowd\") often produces surprisingly good results.</li> </ul>"},{"location":"Word%20Blending/","title":"Word Blending","text":"<ul> <li>Parts of two different words are combined</li> <li>Breakfast+lunch : brunch</li> <li>Smoke+fog:smog</li> </ul>"},{"location":"Word%20Clipping/","title":"Word Clipping","text":"<ul> <li>Longer words are shortened</li> <li>Doctor, laboratory, refrigerator</li> </ul>"},{"location":"Word%20Compounding/","title":"Word Compounding","text":"<ul> <li>Words formed by combining two or more words</li> <li>Adj +Adj= Adj bitter + sweet : bitter-sweet</li> <li>N + N = N rain+bow rain-bow</li> </ul>"},{"location":"Word%20Segmentation/","title":"Word Segmentation","text":"<ul> <li>breaks up the sequence of characters in a text by locating the word boundaries</li> <li>Maximum Matching Algorithm</li> <li>Forward Backward Matching</li> <li>Statistical Word Segmentation</li> <li>Lexical Word Segmentation</li> <li>Hybrid Word Segmentation</li> </ul>"},{"location":"Word%20Structure/","title":"Word Structure","text":"<ul> <li>Isolating words</li> <li>Agglutinating words</li> <li>Inflectional words</li> <li>Polysynthetic words</li> </ul>"},{"location":"Word%20Vectors/","title":"Word Vectors","text":"<ul> <li>Essentially word Embedding</li> <li>Text processing</li> <li>We can represent ideas/sentences/documents as vectors to feed into any kind of model</li> <li>Useful because vectors can be easily compared to find similarity<ul> <li>eg : Cosine Similarity</li> </ul> </li> <li>Higher dimensions make it challenging</li> <li></li> <li>Vectors that are metrically close to each other</li> <li>GloVE</li> <li></li> </ul>"},{"location":"Word2Vec/","title":"Word2Vec","text":"<ul> <li>Efficient Estimation of Word Representations in Vector Space</li> <li>Duality of vector representations of words derived by various models on a collection of syntactic and semantic language tasks involving word similarity</li> <li>possible to train high quality word vectors using very simple model architectures</li> <li>Skip Gram or CBOW</li> </ul>"},{"location":"Word2Vec/#training","title":"Training","text":"<ul> <li>Ask it to predict a vector with probabilities</li> <li>Find error vector</li> <li>Update params</li> <li>to generate high-quality embeddings using a high-performance model, we can switch the model\u2019s task from predicting a neighboring word And switch it to a model that takes the input and output word, and outputs a score indicating if they\u2019re neighbors or not (0 for \u201cnot neighbors\u201d, 1 for \u201cneighbors\u201d).</li> <li>This simple switch changes the model we need from a neural network, to a logistic regression model \u2013 thus it becomes much simpler and much faster to calculate. + Negative Sampling</li> <li>Embedding and Context matrices randomly initialized</li> <li></li> <li></li> </ul>"},{"location":"Word2Vec/#hyperparams","title":"Hyperparams","text":""},{"location":"Word2Vec/#window-size","title":"Window Size","text":"<ul> <li>smaller window sizes (2-15) lead to embeddings where high similarity scores between two embeddings indicates that the words are interchangeable (notice that antonyms are often interchangable if we\u2019re only looking at their surrounding words \u2013 e.g. good and bad often appear in similar contexts).</li> <li>Larger window sizes (15-50, or even more) lead to embeddings where similarity is more indicative of relatedness of the words</li> <li>default is 5 (two words - word - two words)</li> </ul>"},{"location":"Word2Vec/#no-of-negative-samples","title":"No of Negative Samples","text":"<ul> <li>The original paper prescribes 5-20 as being a good number of negative samples. It also states that 2-5 seems to be enough when you have a large enough dataset. The Gensim default is 5 negative samples.</li> </ul>"},{"location":"Word2Vec/#backlinks","title":"Backlinks","text":"<ul> <li>Scalar Articles</li> <li> <p>Word2Vec with gensim</p> </li> <li> <p></p> </li> <li>Word2Vec</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Words-and-Rules/","title":"Words-and-Rules","text":"<ul> <li>Look up past-tense         - No past-tense stored? Generate form<ul> <li>Why symbolic? It uses the abstract 'verb'</li> <li>Rules that refer to these categories are used to guide processing</li> <li>past tense</li> <li>Irregular forms stored in associative memory (declarative memory)</li> <li>Symbolic rules produce past tense forms (procedural memory)</li> <li>look-up is quicker than rule application</li> <li>rule application takes more time but is always done 'on-the-fly'</li> <li>Time 1: Every form is memorized (irregular and regular)</li> <li>Time 2: child notices pattern: verb root+ed = past</li> <li>Child creates a rule</li> <li>Child applies rule to all forms: overgeneralization</li> <li>Time 3: Child realizes that there are irregular and regular forms<ul> <li>creates a dual system: irregular forms are retrieved from memory, regular forms are created by a rule</li> <li>new and novel verbs will get regular endings in past tense unless exposed to irregular past</li> <li>the fact that children seem to learn a rule = language must be symbolic</li> </ul> </li> <li>Traditional U-shaped learning predicts children won't be able to create past-tense forms for novel verbs when they are in the initial stage. This isn't consistent with data</li> <li>Overregularization is not common</li> <li>Cannot account for the presence of two past forms<ul> <li>e.g. dream/dreamed-dreamt, light/lit-lighted</li> </ul> </li> <li>In production experiments<ul> <li>Irregulars produced faster</li> <li>Frequent irregulars are produced faster than infrequent irregulars (Prasada et al., 1990; Albright &amp; Hayes 2003)</li> <li>No difference between frequent and infrequent regulars</li> </ul> </li> <li>Maybe because experiments always present the root form? (1) This is a girl who knows how to dance. She did the same<ul> <li>Root presentation might mask differences due to frequency in regulars</li> </ul> </li> </ul> </li> <li>Challenges of Words-and-rules</li> </ul>"},{"location":"Work%20Envelope/","title":"Work Envelope","text":"<ul> <li>The set of all points which a manipulator can reach without intrusion. Sometimes the shape of the work space, and the position of the manipulator itself can restrict the work envelope.</li> </ul>"},{"location":"Wrist/","title":"Wrist","text":"<ul> <li>A set of rotary joints between the arm and the robot end-effector that allow the end-effector to be oriented to the work-piece. In most cases the wrist can have degrees of freedom which enable it to grasp an object with roll, pitch, and yaw orientation.</li> </ul>"},{"location":"X%20Vectors/","title":"X Vectors","text":"<ul> <li>X-Vectors: Robust DNN Embeddings for Speaker Recognition</li> <li>data augmentation to improve performance of deep neural network (DNN) embeddings for speaker recognition</li> <li>trained to discriminate between speakers, maps variable-length utterances to fixed-dimensional embeddings called x-vectors</li> <li>prior studies have found that embeddings leverage large-scale training datasets better than i-vectors, it can be challenging to collect substantial quantities of labeled data for training</li> <li>use data augmentation, consisting of added noise and reverberation, as an inexpensive method to multiply the amount of training data and improve robustness</li> <li>Their data augmentation strategy employs additive noises and reverberation</li> <li>Reverberation involves convolving room impulse responses (RIR) with audio</li> <li>simulated RIRs described by Ko et al.</li> <li>reverberation itself is performed with the multicondition training tools in the Kaldi ASpIRE recipe</li> <li>For additive noise, they use the MUSAN dataset,</li> <li>PLDA classifier is used in the x-vector framework to make the final decision, similar to i-vector systems</li> <li>x-vectors are compared with i-vector baselines on Speakers in the Wild and NIST SRE 2016 Cantonese where they achieve superior performance on the evaluation datasets</li> </ul>"},{"location":"XLM-R/","title":"XLM-R","text":"<ul> <li>Unsupervised Cross-lingual Representation Learning at Scale</li> <li>pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks</li> <li>Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data</li> <li>significantly outperforms multilingual BERT</li> <li>low-resource languages</li> <li>positive transfer and capacity dilution</li> <li>performance of high and low resource languages at scale</li> <li>possibility of multilingual modeling without sacrificing per-language performance</li> </ul>"},{"location":"XLNet/","title":"XLNet","text":"<ul> <li>XLNet: Generalized Autoregressive Pretraining for Language Understanding</li> <li>modeling bidirectional contexts</li> <li>denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling</li> <li>However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy</li> <li>generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order (thereby proposing a new objective called Permutation Language Modeling), and (2) overcomes the limitations of BERT thanks to its autoregressive formulation</li> <li>uses a permutation language modeling objective to combine the advantages of autoregressive and autoencoder methods</li> </ul>"},{"location":"XLSR/","title":"XLSR","text":""},{"location":"Xavier%20Initialization/","title":"Xavier Initialization","text":"<ul> <li> \\[\\mathrm{a=\\sqrt{\\frac{6}{\\left(\\mathrm{d}\\mathrm{_{\\mathrm{in}}^{\\mathrm{ }}}+\\mathrm{d}_{\\mathrm{out}} \\right)}}}\\] </li> <li>Random values drawn uniformly from \\([-a,a]\\)</li> <li>For Batch Normalization Layers, \\(\\gamma =1\\) and \\(\\beta=0\\)</li> <li>For Tanh based activating neural nets</li> </ul>"},{"location":"Xception/","title":"Xception","text":"<ul> <li>Only use Depthwise Separable convs + Inception modules</li> <li>Cross channel and spatial correlations can be decoupled completely</li> </ul>"},{"location":"YOLO/","title":"YOLO","text":"<ul> <li>You Only Look Once: Unified, Real-Time Object Detection</li> <li>LinearRegression problem</li> <li>predicts bounding boxes and class probabilities directly from full images in one evaluation</li> <li>loss function that directly corresponds to detection performance and the entire model is trained jointly</li> <li>Picasso Dataset, People Art Dataset</li> </ul>"},{"location":"Yaw/","title":"Yaw","text":"<ul> <li>Rotation of the end-effector in a horizontal plane around the end of the manipulator arm. Side to side motion at an axis.</li> </ul>"},{"location":"You%20can%20play%2020%20questions%20with%20nature%20and%20win/","title":"You Can Play 20 Questions with Nature and Win","text":"<ul> <li>You can play 20 questions with nature and win: Categorical versus coordinate spatial relations as a case study</li> <li>Stephen M. Kosslyn</li> </ul>"},{"location":"You%20can%20play%2020%20questions%20with%20nature%20and%20win/#intro","title":"Intro","text":"<ul> <li>Alan Newell famously asserted that \"You can't play 20 questions with nature and win\" (Newell, A. (1973)</li> <li>focused on the futility of studying binary distinctions</li> <li>However, the distinction between categorical and coordinate spatial relations representations has turned out to be fruitful</li> <li>First, from the outset this distinction was cast within the context of a theory of a more general processing system; second, it was formulated from the perspective of multiple levels of analysis within a processing system, and thereby bridges characteristics of information processing with characteristics of the brain.</li> <li>In the game of 20 questions, one player thinks of an object or situation, and the others attempt to guess it by asking a series of binary questions: is it living? is it an animal? is it domesticated?</li> <li>Each question reduces the search space, and eventually a questioner can pounce on just the right answer</li> <li>Newell argued that this game is a bad model for how science should be conducted.</li> <li>decried the tendency of psychologists to formulate and test binary distinctions\u2014 such as those between episodic versus semantic memory, serial versus parallel search, and gradual versus all-or-none learning</li> <li>more often than not such distinctions are illusory, and after an enormous amount of research ultimately all we know is that nature resists clear-cut binary divisions.</li> <li>We should consider how to fit the available data together into a single coherent story.</li> <li>And in his view, the best way to do this is to attempt to build computer simulation models that mimic human performance.</li> </ul>"},{"location":"You%20can%20play%2020%20questions%20with%20nature%20and%20win/#drawing-distinctions-within-processing-systems","title":"Drawing Distinctions Within Processing Systems","text":"<ul> <li>fundamental problem with most (if not all) of the binary distinctions that Newell railed against</li> <li>distinctions were formulated independently of concerns about how the putative representations or processes would operate within the context of a more general processing system</li> </ul>"},{"location":"You%20can%20play%2020%20questions%20with%20nature%20and%20win/#divide-and-conquer","title":"Divide-and-conquer","text":"<ul> <li>complex tasks never are accomplished by a single process, all in one swoop</li> <li>most tasks are treated as if they are combinations of simpler sub-tasks, each of which is grappled with by a separate aspect of the overall processing system.</li> <li>brain has clearly divided processing of object properties, such as shape and color, from processing of spatial properties, such as location</li> <li>Location is registered by a system that processes spatial properties\u2014the socalled \"dorsal system\", which runs from the occipital lobe to posterior parietal cortex.</li> <li>Thus, the two problems (recognizing objects in different locations and being able to specify location) have contradictory requirements\u2014and it is rather elegant that the brain deals with each in a separate system.</li> </ul>"},{"location":"You%20can%20play%2020%20questions%20with%20nature%20and%20win/#weak-modularity","title":"Weak Modularity","text":"<ul> <li>The brain has numerous specialized systems</li> <li>But these systems are not \"modules\" of the sort proposed by Fodor (1983).</li> <li>Fodor's modules are independent, in the sense that the workings of one cannot affect the inner workings of another</li> <li>However, given the nature of the neuroanatomy of the brain, we are better off conceptualizing processing in terms of neural networks\u2014 which may share some cortex and some types of processing.</li> <li>Moreover, we should expect \"leakage\" between these systems. Aspects of a theory of high-level vision</li> <li>Kosslyn &amp; Koenig, 1992)</li> </ul>"},{"location":"You%20can%20play%2020%20questions%20with%20nature%20and%20win/#aspects-of-high-level-vision","title":"Aspects of High Level Vision","text":""},{"location":"You%20can%20play%2020%20questions%20with%20nature%20and%20win/#visual-buffer","title":"Visual Buffer","text":"<ul> <li>visual input during perception is organized in a series of brain areas in the occipital lobe, which I have grouped into a single function structure called the visual buffer</li> <li>These areas are topographically organized, such that the pattern of activation over the surface of the cortex (roughly) preserves the pattern of activation on the retina.</li> <li>Most of the connections among neurons in these areas are short and inhibitory</li> <li>The output from the visual buffer is a representation of edges and regions of an object</li> </ul>"},{"location":"You%20can%20play%2020%20questions%20with%20nature%20and%20win/#object-properties-processing-system","title":"Object Properties Processing System","text":"<ul> <li>Output from the visual buffer flows into the ventral system, where it is compared to stored visual memories</li> <li>If a match is found, the object (or part of an object) is recognized. Spatial properties processing system</li> <li>Output from the visual buffer also flows into the dorsal system, where location and other spatial properties are computed.</li> </ul>"},{"location":"You%20can%20play%2020%20questions%20with%20nature%20and%20win/#long-term-associative-memory","title":"Long-term Associative Memory","text":"<ul> <li>The outputs from the object properties processing and spatial properties processing systems converge on long-term associative memories</li> <li>Such memories specify the spatial relations among objects or parts of objects. A problem in vision and a possible solution</li> <li>The distinction between the ventral and dorsal systems makes sense from the perspective of the two principles briefly outlined earlier, divide-and-conquer and weak modularity</li> <li>How can the visual system identify objects when they can project an almost infinite number of images?</li> <li>no new parts are added to the image when the object is contorted in its many and varied ways, although some parts may be occluded</li> <li>Thus, if a sufficient number of individual parts can be recognized, this is a strong indication that a specific object is present</li> <li>the spatial relations between parts remain constant if they are described in a relatively abstract way</li> </ul>"},{"location":"You%20can%20play%2020%20questions%20with%20nature%20and%20win/#categorical-spatial-relation","title":"Categorical Spatial Relation","text":"<ul> <li>A category is an equivalence class; for instance, if you hold one hand next to the other, the first will remain left or right of the second no matter how high, low, or far away it is from the other hand. Once assigned to the category, the spatial relations are treated as equivalent, with any differences (e.g. between a bent versus outstretched arm) ignored.</li> <li>However, the dorsal system cannot compute only categorical spatial relations representations.</li> <li>Such representations are useless for another key role of the dorsal system, namely reaching and navigation</li> <li>Knowing that a table is \"in front of\" you (a categorical spatial relation) will not help you walk around it, or pull your chair up to it</li> <li>In these cases you need precise metric information, and you need such</li> <li>information relative to your body, a part of your body, or relative to another object that serves as an \"origin\"</li> </ul>"},{"location":"You%20can%20play%2020%20questions%20with%20nature%20and%20win/#coordinate-spatial-relation","title":"Coordinate Spatial Relation","text":"<ul> <li>categorical spatial relations representations typically can be captured by a word or two, and the left cerebral hemisphere is better than the right at such processing</li> <li>coordinate spatial relations representations are essential for navigation, and the right cerebral hemisphere is better than the left at such processing</li> <li>In short, here is an example of a situation where 20 questions seems to be working</li> <li>At the first cut, we divided the entire system into two coarsely defined subsystems, distinguishing between the object-properties-processing ventral system and the spatial-properties-processing dorsal system</li> <li>At the second cut, we focused on the dorsal system, and now divided it into two more finely characterized subsystems, which compute categorical versus coordinate spatial relations representations.</li> </ul>"},{"location":"You%20can%20play%2020%20questions%20with%20nature%20and%20win/#levels-of-analysis","title":"Levels of Analysis","text":"<ul> <li>based largely on that of Marr (1982), but adapted in various ways to be more appropriate for cognitive processing rather than vision per se</li> <li>a fundamental characteristic of a theory of a processing system is that it begins with an analysis of the task to be accomplished</li> <li>The theory of the computation can be conceptualized as specifying a black box, which takes a specific input and produces a specific output; this output in turn is used as input to yet other processes.</li> <li>According to Marr, whereas a theory of the computation describes what is computed, a theory of the algorithm specifies how it is computed.</li> <li>An algorithm consists of a step-by-step procedure that guarantees that a certain output will be produced on the basis of a certain input.</li> <li>Finally, algorithms are implemented in hardware (on a computer) or \"wetware\" (in a brain)</li> <li>The level of the implementation specifies how an algorithm is physically realized</li> <li>This observation seems particularly relevant to the encoding and use of spatial relations representations (e.g. Baciu et al., 1999; Kosslyn et al., 1998).</li> <li>Interdependence among levels</li> <li>Marr sometimes wrote as if a theory at one level of analysis could be formulated with only weak links to theories at the other levels.</li> <li>However, computations rely on algorithms, and those algorithms have to operate in</li> <li>a brain that does some things well and other things not so well</li> <li>In addition, as evolution progressed, older parts of the brain often were relatively preserved\u2014new areas were added, but the old ones rarely were redesigned from scratch.</li> <li>Thus, the newer portions had to work with the older ones, which may not have been optimal for the final product (cf. Allman, 1999).</li> <li>characteristics at each of the levels of analysis affect theorizing at the other levels\u2014 and hence a powerful approach to theorizing about cognition requires that all three levels of analysis be considered at the same time.</li> <li>At the level of the algorithm, conceptualizing processing within the context of the larger system played a central role; the fact that object properties and spatial properties are processed separately provided a key constraint on the theory of what is computed and how such computation proceeds</li> <li>the idea that the two cerebral hemispheres would differ for the two kinds of processing not only helps to specify the nature of the representations and processes, but also offers one way to test the hypothesis.</li> <li>Leveraging multi-level theories</li> </ul>"},{"location":"You%20can%20play%2020%20questions%20with%20nature%20and%20win/#why-is-it-important-that-scientists-be-able-to-play-20-questions-with-nature-and-win","title":"Why is it Important That Scientists Be Able to Play 20 Questions with Nature and Win?","text":"<ul> <li>One reason is simple: cognitive processing is extraordinarily complex, and we must find ways to gain traction in studying it.</li> <li>I argue that multi-level theories, which bridge from information processing to the brain, should play a special role in playing the science game of 20 questions.</li> <li>First, they lead researchers to collect different sorts of data.</li> <li>when theorizing on the basis of such varied types of data, there are more constraints on the theory.</li> <li>Moreover, multi-level theories must respect qualitatively different sorts of constraints simultaneously</li> <li>Perhaps paradoxically, the more constraints that are available the easier it is to theorize, even though it is more difficult to fit all the constraints together within a common framework</li> <li>Newell was troubled not simply by the failure of most binary distinctions to lead to fruitful research, but also by the lack of accumulation of such results.</li> <li>He had the sense that research was not accumulating to paint a coherent overall picture, but instead isolated fragments of knowledge were being collected.</li> <li>The brain is, after all, a single organ.</li> </ul>"},{"location":"You%20can%20play%2020%20questions%20with%20nature%20and%20win/#backlinks","title":"Backlinks","text":"<ul> <li>You can play 20 questions with nature and win</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Z%20Normalization/","title":"Z Normalization","text":"<ul> <li>Centering and rescaling the data so that a zero mean and unit variance is obtained.</li> <li>Only compute the parameters \u03bck,\u03c3k on the training set!</li> <li>For image data, normalization is not done per pixel but computed over all the pixels</li> </ul>"},{"location":"Z-Space%20Entanglement/","title":"Z-Space Entanglement","text":"<ul> <li>Another challenge with controllable generation is referred to as entanglement in Z-space.</li> <li>When the z-space is entangled, this means movement in different directions has an effect on multiple features in the output simultaneously.</li> <li>Even if these features aren't correlated, an entangled z-space results in a single feature change modifying more than one feature in the output.</li> <li>Entanglement happens commonly if the number of dimensions in the z-space isn't large enough</li> </ul>"},{"location":"Z-Space%20Entanglement/#backlinks","title":"Backlinks","text":"<ul> <li>Conditional GAN</li> <li>Z-Space Entanglement</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"Zeiler%20Fergus/","title":"Zeiler Fergus","text":"<ul> <li>multiple interleaved layers of Conv, non-linear Activation Functions, local response normalizations, and max Pooling</li> </ul>"},{"location":"Zero%20Label%20Language%20Learning/","title":"Zero Label Language Learning","text":"<ul> <li>Towards Zero-Label Language Learning</li> <li>Unsupervised Data Generation</li> <li>SuperGLUE</li> <li>Treat LMs as few-shot generators (rather than few-shot learners)</li> <li>Create prompts with  pair(s) <li>Ask the model to generate more for the same label</li> <li>The emphasis is on the labelled data generation (rather than inference)</li> <li>The new idea is about generating more data and going with conventional route</li> <li>This paper confirms all the above by introducing UDG using LMs, even for complex higher-order tasks and empirically shows classical fine-tuning with more data works better.</li>"},{"location":"aphasia/","title":"Aphasia","text":"<ul> <li>Disturbance of language affecting speech production, comprehension, reading or writing, due to brain injury \u2013 most commonly from stroke or trauma.</li> <li>The type of aphasia depends on the brain area damaged</li> </ul>"},{"location":"augment/","title":"Augment","text":""},{"location":"augment/#tags-anchor","title":"tags: anchor","text":"<ul> <li>content/A survey on Image Data Augmentation for Deep Learning.md</li> <li>content/Adversarial Spatial Dropout for Occlusion.md</li> <li>content/Alleviating Class Imbalance with Data Augmentation.md</li> <li>content/Attentive CutMix.md</li> <li>content/AttributeMix.md</li> <li>content/AugMix.md</li> <li>content/Augmented Random Search.md</li> <li>content/AutoAugment.md</li> <li>content/Co-Mixup.md</li> <li>content/Color Space Transformations.md</li> <li>content/CowMask.md</li> <li>content/Cropping.md</li> <li>content/Cut and Delete.md</li> <li>content/Cut and Mix.md</li> <li>content/Cut, Paste and Learn.md</li> <li>content/CutMix.md</li> <li>content/Cutout.md</li> <li>content/Data Augmentation with Curriculum Learning.md</li> <li>content/Feature Space Augmentation.md</li> <li>content/Flipping.md</li> <li>content/GAN\u2010based Data Augmentation.md</li> <li>content/Geometric Transformations.md</li> <li>content/GridMask.md</li> <li>content/Hide and Seek.md</li> <li>content/Image Mixing and Deletion.md</li> <li>content/Intra-Class Part Swapping.md</li> <li>content/KeepAugment.md</li> <li>content/Kernel Filters.md</li> <li>content/Manifold MixUp.md</li> <li>content/Meta Learning Data Augmentations.md</li> <li>content/Mixed Example.md</li> <li>content/Neural Augmentation.md</li> <li>content/Noise Injection.md</li> <li>content/Puzzle Mix.md</li> <li>content/RICAP.md</li> <li>content/RandAugment.md</li> <li>content/Random Erasing.md</li> <li>content/ReMix.md</li> <li>content/ResizeMix.md</li> <li>content/SMOTE.md</li> <li>content/SaliencyMix.md</li> <li>content/Sample Pairing.md</li> <li>content/Smart Augmentation.md</li> <li>content/SmoothMix.md</li> <li>content/SnapMix.md</li> <li>content/SpecAugment.md</li> <li>content/Test-time Augmentation.md</li> <li>content/Visual Context Augmentation.md</li> </ul>"},{"location":"blind%20ethical%20judgement/","title":"Blind Ethical Judgement","text":"<ul> <li>the given agent's state and knowledge are unknown</li> </ul>"},{"location":"blind%20ethical%20judgement/#backlinks","title":"Backlinks","text":"<ul> <li>Building Ethics into Artificial Intelligence</li> <li>blind ethical judgement</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"brain/","title":"Brain","text":""},{"location":"brain/#tags-anchor","title":"tags: anchor","text":"<ul> <li>content/Action Potential.md</li> <li>content/Adrenal Glands.md</li> <li>content/Adrenaline.md</li> <li>content/Allele.md</li> <li>content/Alpha Waves.md</li> <li>content/Alzheimer\u2019s Disease.md</li> <li>content/Amino Acid.md</li> <li>content/Amygdala.md</li> <li>content/Amyloid Plaque.md</li> <li>content/Amyloid-beta (A\u03b2) Protein.md</li> <li>content/Amyotrophic Lateral Sclerosis (ALS).md</li> <li>content/Angiography.md</li> <li>content/Apoptosis.md</li> <li>content/Astrocyte.md</li> <li>content/Axon Terminal.md</li> <li>content/Axon.md</li> <li>content/BOLD.md</li> <li>content/Basal Ganglia.md</li> <li>content/Basilar Artery.md</li> <li>content/Belmont Principles.md</li> <li>content/Belmont Report.md</li> <li>content/Beneficence.md</li> <li>content/Beta Waves.md</li> <li>content/Biological Neuron.md</li> <li>content/Biomarkers.md</li> <li>content/Blood-brain Barrier.md</li> <li>content/Brain Areas.md</li> <li>content/Brain Cortex.md</li> <li>content/Brain Organoid.md</li> <li>content/Brain Oscillations.md</li> <li>content/Brain-derived Neurotrophic Factor (BDNF).md</li> <li>content/BrainWave Coherence.md</li> <li>content/BrainWave CrossFrequency Coupling.md</li> <li>content/BrainWave Synchronization.md</li> <li>content/Brainstem.md</li> <li>content/Brocas Area.md</li> <li>content/CRISPR (clustered Regularly-interspaced Short Palindromic repeats).md</li> <li>content/Central Sulcus.md</li> <li>content/Cerebellar Artery.md</li> <li>content/Cerebellum.md</li> <li>content/Cerebral Palsy.md</li> <li>content/Cerebrospinal Fluid (CSF).md</li> <li>content/Cerebrum.md</li> <li>content/Chimera.md</li> <li>content/Chronic Encephalopathy Syndrome (CES).md</li> <li>content/Chronic Traumatic Encephalopathy (CTE).md</li> <li>content/Cochlea.md</li> <li>content/Concussion.md</li> <li>content/Cone.md</li> <li>content/Corpus callosum.md</li> <li>content/Cortical Homunculus.md</li> <li>content/Cortisol.md</li> <li>content/Deep Brain Stimulation.md</li> <li>content/Delta Waves.md</li> <li>content/Dendrites.md</li> <li>content/Digital Phenotyping.md</li> <li>content/Down Syndrome.md</li> <li>content/EEG Cap.md</li> <li>content/EEG.md</li> <li>content/Electroconvulsive Therapy (ECT).md</li> <li>content/Epigenetics.md</li> <li>content/Epilepsy.md</li> <li>content/Eugenics.md</li> <li>content/Frontal Operculum.md</li> <li>content/Frontal lobe.md</li> <li>content/Functional Connectivity.md</li> <li>content/Gamma Waves.md</li> <li>content/Gamma-aminobutyric Acid (GABA).md</li> <li>content/Gene Expression.md</li> <li>content/Glia.md</li> <li>content/Glioblastoma.md</li> <li>content/Glioma.md</li> <li>content/Glucose.md</li> <li>content/Glymphatic System.md</li> <li>content/Granger Causallity.md</li> <li>content/Gyrus.md</li> <li>content/Huntington\u2019s Disease.md</li> <li>content/Hypothalamus.md</li> <li>content/Implicit Bias.md</li> <li>content/In Silico.md</li> <li>content/In Vitro.md</li> <li>content/In Vivo.md</li> <li>content/Induced Pluripotent Stem Cell (iPSC).md</li> <li>content/Insula.md</li> <li>content/Ion Channel.md</li> <li>content/Ketamine.md</li> <li>content/Lesion.md</li> <li>content/Limbic system.md</li> <li>content/Long Term Potentiation (LTP).md</li> <li>content/MRI.md</li> <li>content/Mesolimbic Pathway.md</li> <li>content/Microbiota.md</li> <li>content/Microglia.md</li> <li>content/Multiple Sclerosis.md</li> <li>content/Myelin.md</li> <li>content/Neural Chimera.md</li> <li>content/Neural Induction.md</li> <li>content/Neuroaesthetics.md</li> <li>content/Neurogenesis.md</li> <li>content/Neuroplasticity.md</li> <li>content/Nootropics.md</li> <li>content/Nucleotide Sequence.md</li> <li>content/Nucleotide.md</li> <li>content/Nucleus Accumbens.md</li> <li>content/Occipital lobe.md</li> <li>content/Optogenetics.md</li> <li>content/Organoid.md</li> <li>content/Oxytocin.md</li> <li>content/Parietal lobe.md</li> <li>content/Parkinson\u2019s Disease.md</li> <li>content/Pharmacotherapy.md</li> <li>content/Phenotype.md</li> <li>content/Pineal gland.md</li> <li>content/Pituitary gland.md</li> <li>content/Pluripotency.md</li> <li>content/Positron Emission Tomography (PET).md</li> <li>content/Postsynaptic Cell.md</li> <li>content/Presynaptic Cell.md</li> <li>content/Prion.md</li> <li>content/Protein Folding.md</li> <li>content/Psychosis.md</li> <li>content/Rapid Eye Movement (REM) Sleep.md</li> <li>content/Recessive.md</li> <li>content/Reuptake.md</li> <li>content/Rod.md</li> <li>content/Serotonin.md</li> <li>content/Somatosensory Cortex.md</li> <li>content/Sono-stimulation.md</li> <li>content/Sonogenetics.md</li> <li>content/Spectrogram.md</li> <li>content/Stem Cells.md</li> <li>content/Striatum.md</li> <li>content/Stroke.md</li> <li>content/Subgenual Cortex.md</li> <li>content/Substantia Nigra.md</li> <li>content/Subthalamic Nucleus.md</li> <li>content/Sulcus.md</li> <li>content/Synaptic Pruning.md</li> <li>content/Synaptic Transmission.md</li> <li>content/Tau Protein.md</li> <li>content/Telomere.md</li> <li>content/Temporal lobe.md</li> <li>content/Thalamus.md</li> <li>content/Theta Waves.md</li> <li>content/Tourette\u2019s Syndrome.md</li> <li>content/Transcranial Electrical Stimulation (tDCS and tACS).md</li> <li>content/Transcranial Magnetic Stimulation (TMS).md</li> <li>content/Two-photon Microscopy.md</li> <li>content/Undue Inducement.md</li> <li>content/Vagus Nerve.md</li> <li>content/Vertebral Arteries.md</li> <li>content/Vestibular System.md</li> <li>content/Visual Cortex.md</li> <li>content/Wernicke Area.md</li> <li>content/aphasia.md</li> <li>content/fMRI.md</li> </ul>"},{"location":"cogitivemodel/","title":"Cogitivemodel","text":""},{"location":"cogitivemodel/#tags-anchor","title":"tags: anchor","text":""},{"location":"cogneuro/","title":"Cogneuro","text":""},{"location":"cogneuro/#tags-anchor","title":"tags: anchor","text":"<ul> <li>content/Attentions and salience.md</li> <li>content/BOLD.md</li> <li>content/Berkeley et al.md</li> <li>content/Connectome.md</li> <li>content/Cross-situational learning.md</li> <li>content/Deductive Approaches.md</li> <li>content/Distributive units.md</li> <li>content/First order generalization.md</li> <li>content/Inductive Learning.md</li> <li>content/Interpretability vs Neuroscience.md</li> <li>content/Localist units.md</li> <li>content/Milin et al..md</li> <li>content/Mirman et al..md</li> <li>content/Motor Memories.md</li> <li>content/Overhypotheses.md</li> <li>content/Propose-but-verify.md</li> <li>content/Rescorla-Wagner Algorithm.md</li> <li>content/Rescorla-Wagner Blocking.md</li> <li>content/Second order generalization.md</li> <li>content/Single unit recording.md</li> <li>content/Superposition Catastrophe.md</li> <li>content/Symbolic models.md</li> <li>content/Transitional probabilities.md</li> <li>content/conditioning.md</li> <li>content/fMRI.md</li> <li>content/memory trace.md</li> </ul>"},{"location":"conditioning/","title":"Conditioning","text":"<ul> <li>Unconditioned Stimulus (US) (=food)</li> <li>Another feature or cue (bell), becomes Conditioned Stimulus (CS)</li> <li>A response that indicates association (salvation)</li> <li>The unconditioned stimulus is what we want to predict, our outcome</li> <li>Conditioned Stimuli or cues are the features we are learning to use to predict the outcome (US)</li> <li>Current Vector V t ij changes with each learning instance</li> <li> \\[V_{ij}^{t+1}= V_{ij}^{t}+ \\Delta V_{ij}^{t}\\] </li> <li> \\[\\Delta V_{ij}^{t}=(1-\\text{connection weight}_{j}^{t})\\] </li> <li>\u25b6 V = association strength \u25b6 i = current input (cue) \u25b6 j = current output (outcome) \u25b6 \u2206 V : Change in association strength</li> <li>Pavlov discovered evidence of positive associations</li> <li>Associations increase in strength whenever a feature and a cue occur together</li> </ul>"},{"location":"cross-layer%20parameter%20sharing/","title":"Cross-layer Parameter Sharing","text":"<ul> <li>between encoder segments, layer parameters are shared for every similar subsegment.</li> <li>This means that e.g. with 12 encoder segments:<ul> <li>The multi-head self-attention subsegments share parameters (i.e. weights) across all twelve layers.</li> <li>The same is true for the feedforward segments.</li> </ul> </li> <li>The consequence of this change is that the number of parameters is reduced significantly, simply because they are shared.</li> <li>the stabilization of the neural network due to parameter sharing. In other words, beyond simply reducing the computational cost involved with training, the paper suggests that sharing parameters can also improve the training process.</li> </ul>"},{"location":"dGSLM/","title":"dGSLM","text":"<ul> <li>Generative Spoken Dialogue Language Modeling</li> <li>dGSLM</li> <li>first \u201ctextless\u201d model able to generate audio samples of naturalistic spoken dialogues</li> <li>unsupervised spoken unit discovery coupled with a dual-tower transformer architecture with cross-attention trained on 2000 hours of two-channel raw conversational audio Fisher Spanish-English without any text or labels</li> <li>generate speech, laughter and other paralinguistic signals in the two channels simultaneously and reproduces naturalistic turn taking</li> </ul>"},{"location":"data2vec/","title":"data2vec","text":"<ul> <li>data2vec: a General Framework for Self-supervised Learning in Speech, Vision and Language</li> <li>closer to general self-supervised learning</li> <li>framework that uses the same learning method for either speech, NLP or computer vision</li> <li>predict latent representations of the full input data based on a masked view of the input in a self distillation setup using a standard Transformer architecture</li> <li>Instead of predicting modality-specific targets such as words, visual tokens or units of human speech which are local in nature, data2vec predicts contextualized latent representations that contain information from the entire inpu</li> <li>Today\u2019s self-supervised learning research almost typically focuses on a single modality</li> <li>As a result, researchers specializing in one modality often adopt a totally different strategy than those specializing in another.</li> <li>For each modality, algorithms anticipate distinct units: pixels or visual tokens for images, words for the text, and learned sound inventories for voice</li> <li>teaching models to anticipate their own representations of the incoming data, regardless of mode</li> <li>Instead of predicting visual tokens, phrases, or sounds, a single algorithm may work with completely different sorts of input by focusing on these representations \u2014 the layers of a neural network</li> <li>robust normalization of the features for the job that would be trustworthy in different modalities to directly predict representations.</li> <li>The method starts by computing target representations from an image, a piece of text, or a voice utterance using a teacher network</li> <li>After that, a portion of the input was masked and repeated with a student network, which predicts the teacher\u2019s latent representations</li> <li>Even though it only has a partial view of the data, the student model must predict accurate input data</li> <li>The instructor network is identical to the student network, except with somewhat out-of-date weights.</li> <li>ImageNet</li> <li>surpassed wav2vec 2.0 and HuBERT</li> <li>GLUE</li> <li>Method:</li> <li>data2vec is trained by predicting the model representations of the full input data given a partial view of the input</li> <li>They first encode a masked version of the training sample (model in student mode) and then construct training targets by encoding the unmasked version of the input sample with the same model but when parameterized as an exponentially moving average of the model weights (model in teacher mode)</li> <li>The target representations encode all of the information in the training sample and the learning task is for the student to predict these representations given a partial view of the input.</li> <li>Modality encoding:</li> <li>The model architecture used is the standard Transformer architecture with a modality-specific encoding of the input data borrowed from prior work:</li> <li>For computer vision, they have used the ViT-strategy of encoding an image as a sequence of patches, each spanning 16x16 pixels, input to a linear transformation.</li> <li>Speech data is encoded using a multi-layer 1-D convolutional neural network that maps 16 kHz waveform to 50 Hz representations.</li> <li>Text is pre-processed to obtain sub-word units, which are then embedded in distributional space via learned embedding vectors.</li> </ul>"},{"location":"dataset/","title":"Dataset","text":""},{"location":"dataset/#tags-anchor","title":"tags: anchor","text":"<ul> <li>content/AudioSet classification.md</li> <li>content/BUCC.md</li> <li>content/Benchmark LLM.md</li> <li>content/Billion Word.md</li> <li>content/BooksCorpus.md</li> <li>content/Broden.md</li> <li>content/CIFAR.md</li> <li>content/COCO.md</li> <li>content/CUB-200-2011.md</li> <li>content/Cityscapes.md</li> <li>content/CommonCrawl.md</li> <li>content/DrawBench.md</li> <li>content/English Wikipedia.md</li> <li>content/Europarl-ST.md</li> <li>content/FGVC Aircraft.md</li> <li>content/FGVCx.md</li> <li>content/Fashion MNIST.md</li> <li>content/Fine grained datasets.md</li> <li>content/Fisher Spanish-English.md</li> <li>content/Flickr30K.md</li> <li>content/GLUE.md</li> <li>content/GTA5.md</li> <li>content/Google Conceptual Captions.md</li> <li>content/Google voice search task.md</li> <li>content/IDRiD.md</li> <li>content/ILSVRC.md</li> <li>content/IMDB.md</li> <li>content/ISIC 2018.md</li> <li>content/Kvasir Dataset.md</li> <li>content/Labeled Faces in the Wild.md</li> <li>content/LibriSpeech.md</li> <li>content/MILANNOTATIONS.md</li> <li>content/MIT1003.md</li> <li>content/MIT300.md</li> <li>content/MLDoc.md</li> <li>content/MMLU.md</li> <li>content/MNIST.md</li> <li>content/MSCOCO.md</li> <li>content/MUSAN.md</li> <li>content/NIST 2008 Speaker Recognition Evaluation dataset.md</li> <li>content/NIST SRE 2016 Cantonese.md</li> <li>content/NLVR2.md</li> <li>content/OSIE.md</li> <li>content/PASCAL VOC.md</li> <li>content/PASCAL-S.md</li> <li>content/People Art Dataset.md</li> <li>content/Picasso Dataset.md</li> <li>content/PlantCLEF.md</li> <li>content/RACE.md</li> <li>content/SBU Captions.md</li> <li>content/SQuAD.md</li> <li>content/SYNTHIA.md</li> <li>content/Salicon dataset.md</li> <li>content/Shapes Dataset.md</li> <li>content/Speakers in the Wild.md</li> <li>content/Stanford Dogs.md</li> <li>content/Swichboard.md</li> <li>content/VGGFace2.md</li> <li>content/VQAv2.md</li> <li>content/Visual Commonsense Reasoning.md</li> <li>content/Visual Genome.md</li> <li>content/WMT14.md</li> <li>content/Wall Street Journal task.md</li> <li>content/XLSR.md</li> <li>content/iNaturalist.md</li> </ul>"},{"location":"fMRI/","title":"fMRI","text":"<ul> <li>Unlike MRI. Studies measures brain activity by detecting changes associated with blood flow</li> <li>3mm</li> <li>BOLD</li> </ul>"},{"location":"fastai/","title":"Fastai","text":"<ul> <li>Fastai Blocks</li> <li>Fastai Interpretation</li> <li>Fastai Deployment</li> <li>Fastai Tricks</li> </ul>"},{"location":"flow/","title":"Flow","text":""},{"location":"flow/#tags-anchor","title":"tags: anchor","text":"<ul> <li>content/notes/Vision Explainibility.md</li> </ul>"},{"location":"fully%20informed%20ethical%20judgement/","title":"Fully Informed Ethical Judgement","text":"<ul> <li>with complete information about a given agent's state and knowledge</li> <li>no quantitative measure of how far a behaviour is from rightfulness or goodness</li> </ul>"},{"location":"fully%20informed%20ethical%20judgement/#backlinks","title":"Backlinks","text":"<ul> <li>Building Ethics into Artificial Intelligence</li> <li>fully informed ethical judgement</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"gensim/","title":"Gensim","text":"<ul> <li>library</li> </ul>"},{"location":"gensim/#w2vec-code","title":"w2vec Code","text":"<pre><code>nltk==3.6.1  \nnode2vec==0.4.3  \npandas==1.2.4  \nmatplotlib==3.3.4  \ngensim==4.0.1  \nscikit-learn=0.24.1\n</code></pre> <pre><code>import nltk  \nnltk.download('stopwords')  \nnltk.download('punkt')\n\nimport pandas as pd\n\nimport nltk\n\nimport string\n\nimport matplotlib.pyplot as plt\n\nfrom nltk.corpus import stopwords\n\nfrom nltk import word_tokenize\n\nfrom gensim.models import Word2Vec as w2v\n\nfrom sklearn.decomposition import PCA\n\n# constants\n\nPATH = 'data/shakespeare.txt'\n\nsw = stopwords.words('english')\n\nplt.style.use('ggplot')\n\n# nltk.download('punkt')\n\n# nltk.download('stopwords')\n\n# import data\n\nlines = []\n\nwith open(PATH, 'r') as f:\n\nfor l in f:\n    lines.append(l)\n\n# remove new lines\nlines = [line.rstrip('\\n') for line in lines]\n\n# make all characters lower\nlines = [line.lower() for line in lines]\n\n# remove punctuations from each line\nlines = [line.translate(str.maketrans('', '', string.punctuation)) for line in lines]\n\n# tokenize\nlines = [word_tokenize(line) for line in lines]\n\ndef remove_stopwords(lines, sw = sw):\n    '''\n    The purpose of this function is to remove stopwords from a given array of \n    lines.\n\n    params:\n        lines (Array / List) : The list of lines you want to remove the stopwords from\n        sw (Set) : The set of stopwords you want to remove\n\n    example:\n        lines = remove_stopwords(lines = lines, sw = sw)\n    '''\n\n    res = []\n    for line in lines:\n        original = line\n        line = [w for w in line if w not in sw]\n        if len(line) &lt; 1:\n            line = original\n        res.append(line)\n    return res\n\nfiltered_lines = remove_stopwords(lines = lines, sw = sw)\n\nw = w2v(\n    filtered_lines,\n    min_count=3,  \n    sg = 1,       # 1 for skip gram, 0 for cbow\n    window=7      \n)       \n\nprint(w.wv.most_similar('thou'))\n\nemb_df = (\n    pd.DataFrame(\n        [w.wv.get_vector(str(n)) for n in w.wv.key_to_index],\n        index = w.wv.key_to_index\n    )\n)\nprint(emb_df.shape)\nemb_df.head()\n\npca = PCA(n_components=2, random_state=7)\npca_mdl = pca.fit_transform(emb_df)\n\nemb_df_PCA = (\n    pd.DataFrame(\n        pca_mdl,\n        columns=['x','y'],\n        index = emb_df.index\n    )\n)\n\nplt.clf()\nfig = plt.figure(figsize=(6,4))\n\nplt.scatter(\n    x = emb_df_PCA['x'],\n    y = emb_df_PCA['y'],\n    s = 0.4,\n    color = 'maroon',\n    alpha = 0.5\n)\n\nplt.xlabel('PCA-1')\nplt.ylabel('PCA-2')\nplt.title('PCA Visualization')\nplt.plot()\n</code></pre>"},{"location":"gensim/#backlinks","title":"Backlinks","text":"<ul> <li>Scalar Articles</li> <li> <p>Word2Vec with gensim</p> </li> <li> <p></p> </li> <li>gensim</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"handwritingRecognition/","title":"handwritingRecognition","text":"<ul> <li>https://arxiv.org/pdf/1912.10205.pdf<ul> <li>https://github.com/Canjie-Luo/Text-Image-Augmentation</li> <li></li> <li></li> </ul> </li> <li>https://github.com/FactoDeepLearning/VerticalAttentionOCR<ul> <li>https://arxiv.org/pdf/2012.03868v2.pdf</li> <li>segmentation free</li> <li></li> <li></li> </ul> </li> </ul>"},{"location":"i-Code/","title":"i-Code","text":"<ul> <li>i-Code: an Integrative and Composable Multimodal Learning Framework</li> <li>Human intelligence is multimodal; they integrate visual, linguistic, and acoustic signals to maintain a holistic worldview</li> <li>Most current pretraining methods, however, are limited to one or two modalities.</li> <li>jointly learns representations for vision, language and speech into a unified, shared and general-purpose vector representation</li> <li>data from each modality are first given to pretrained single-modality encoder</li> <li>The encoder outputs are then integrated with a multimodal fusion network, which uses novel attention mechanisms and other architectural innovations to effectively combine information from the different modalities</li> <li>new objectives including (i) masked modality modeling and (ii) cross-modality contrastive learning</li> <li>pretraining on dual-modality datasets can also yield competitive or even better performance than pretraining on videos, the data resource that previous three-modality models were restricted to</li> <li>dynamically process single, dual, and triple-modality data during training and inference, flexibly projecting different combinations of modalities into a single representation space</li> <li>GLUE</li> <li>merge-attention layers and (b) co-attention layers</li> <li>fusion architecture</li> <li>mechanisms that merge and cross the attention scores of different modalities, namely merge-attention (based on self-attention) and co-attention (based on self- and cross-attention) respectively</li> </ul>"},{"location":"iNaturalist/","title":"iNaturalist","text":"<ul> <li>This dataset contains images of thousands of different species of plants and animals, with a total of over 8 million images.</li> </ul>"},{"location":"iNaturalist/#backlinks","title":"Backlinks","text":"<ul> <li>Fine Grained Datasets</li> <li>iNaturalist</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"imageCaptioning/","title":"Image Captioning","text":""},{"location":"inter-sentence%20coherence%20loss/","title":"Inter-sentence Coherence Loss","text":"<ul> <li>inter-sentence coherence loss called sentence-order prediction (SOP) is used.</li> <li>The key problem with this loss is that it merges topic prediction and coherence prediction into one task.</li> <li>Intuitively, we can argue that topic prediction is much easier than coherence prediction. The consequence is that when the model discovers this, it can focus entirely on this subtask, and forget about the coherence prediction task; actually taking the path of least resistance. The authors actually demonstrate that this is happening with the NSP task, replacing it within their work with a sentence-order prediction or SOP task.</li> </ul>"},{"location":"language/","title":"Language","text":""},{"location":"language/#tags-anchor","title":"tags: anchor","text":"<ul> <li>content/2 X 2 Study.md</li> <li>content/2 byte character set.md</li> <li>content/8 bit character set.md</li> <li>content/A matter of ambiguity? Using eye movements to examine collective vs. distributive interpretations of plural sets.md</li> <li>content/ANOVA.md</li> <li>content/ASCII.md</li> <li>content/Action Transitive verb.md</li> <li>content/Adjective.md</li> <li>content/Adverb.md</li> <li>content/Agglutinating words.md</li> <li>content/Allomorph.md</li> <li>content/Application dependence.md</li> <li>content/Approximately Compositional Semantic Parsing.md</li> <li>content/Attentions and salience.md</li> <li>content/Berkeley et al.md</li> <li>content/Bottom Up Parsing.md</li> <li>content/Bound morpheme.md</li> <li>content/Cardinality Principle.md</li> <li>content/Case Grammar.md</li> <li>content/Challenges of Words-and-rules.md</li> <li>content/Character-set dependence.md</li> <li>content/Circumfix.md</li> <li>content/Collective Interpretation.md</li> <li>content/Collectivity, Distributivity, and the Interpretation of Plural Numerical Expressions in Child and Adult Language.md</li> <li>content/Conceptual Parsing.md</li> <li>content/Connectionism.md</li> <li>content/Connectionist Networks.md</li> <li>content/Connectives.md</li> <li>content/Content Morpheme.md</li> <li>content/Content words.md</li> <li>content/Context Free Grammar.md</li> <li>content/Corpus dependence.md</li> <li>content/Cross-situational learning.md</li> <li>content/Cumulative Interpretation.md</li> <li>content/Deductive Approaches.md</li> <li>content/Derivational Morphology.md</li> <li>content/Determiners.md</li> <li>content/DiTransitive verb.md</li> <li>content/Distributive Interpretation (2).md</li> <li>content/Distributive Interpretation.md</li> <li>content/Distributive units.md</li> <li>content/Document Triage.md</li> <li>content/Elements of sets.md</li> <li>content/Elman 1990.md</li> <li>content/Emergentism.md</li> <li>content/Entities involving in actions.md</li> <li>content/Evidence For Distributivity Effects in Comprehension.md</li> <li>content/Extra-position.md</li> <li>content/Final Paper Language Modeling.md</li> <li>content/First order generalization.md</li> <li>content/Fixed Factors.md</li> <li>content/Forward Backward Matching.md</li> <li>content/Free morpheme.md</li> <li>content/Function words.md</li> <li>content/Functional Morpheme.md</li> <li>content/Hybrid Word Segmentation.md</li> <li>content/Inductive Learning.md</li> <li>content/Infix.md</li> <li>content/Inflectional Morphology.md</li> <li>content/Inflectional words.md</li> <li>content/Isolating words.md</li> <li>content/Language Identification.md</li> <li>content/Language dependence.md</li> <li>content/Latent Dirchlet Allocation.md</li> <li>content/Lemmatization.md</li> <li>content/Lexical Ambiguity.md</li> <li>content/Lexical Disambiguation.md</li> <li>content/Lexical Word Segmentation.md</li> <li>content/Lexically Collective.md</li> <li>content/Lexically Distributive.md</li> <li>content/Lexicon.md</li> <li>content/Linguistic details.md</li> <li>content/Localist units.md</li> <li>content/Maximum Matching Algorithm.md</li> <li>content/Memory-based learning.md</li> <li>content/Milin et al..md</li> <li>content/Minimal Semantic Commitment.md</li> <li>content/Mirman et al..md</li> <li>content/Misyak et al 2010.md</li> <li>content/Mixed Effect Models.md</li> <li>content/Morpheme Generation.md</li> <li>content/Morpheme Segmentation.md</li> <li>content/Morpheme.md</li> <li>content/Morphology Affix.md</li> <li>content/Morphology Stem.md</li> <li>content/Morphology.md</li> <li>content/Morphotactic.md</li> <li>content/Multiple constraint-based theories.md</li> <li>content/Names of individuals.md</li> <li>content/Nativists.md</li> <li>content/Non-adjacent dependencies.md</li> <li>content/Numerically Quantified Expressions.md</li> <li>content/Object-relative clauses.md</li> <li>content/Ostension.md</li> <li>content/Ostensive Information.md</li> <li>content/Overhypotheses.md</li> <li>content/Parts of action.md</li> <li>content/Parts of entities.md</li> <li>content/Phonetics.md</li> <li>content/Phonology.md</li> <li>content/Picky Puppet Method.md</li> <li>content/Polysynthetic words.md</li> <li>content/Pragmatics.md</li> <li>content/Predicate.md</li> <li>content/Prefix.md</li> <li>content/Prepositions.md</li> <li>content/Propose-but-verify.md</li> <li>content/Psycholinguistics.md</li> <li>content/Punctuation.md</li> <li>content/Quantifier spreading children misled by ostensive cues.md</li> <li>content/Quantifiers.md</li> <li>content/Random Factors.md</li> <li>content/Relevance Account.md</li> <li>content/Relevance Theory.md</li> <li>content/Rescorla-Wagner Algorithm.md</li> <li>content/Rescorla-Wagner Blocking.md</li> <li>content/Saffran, Aslin and Newport.md</li> <li>content/Salient Object Strategy.md</li> <li>content/Second order generalization.md</li> <li>content/Semantic Analysis.md</li> <li>content/Semantic Grammar.md</li> <li>content/Semantic Markers.md</li> <li>content/Semantics influences form.md</li> <li>content/Sentence Segmentation.md</li> <li>content/Sentence level processing.md</li> <li>content/Shared Character Set.md</li> <li>content/Shortcuts to Quantifier Interpretation in Children and Adults.md</li> <li>content/Single unit recording.md</li> <li>content/Statistical Word Segmentation.md</li> <li>content/Subject relative.md</li> <li>content/Subject-verb agreement.md</li> <li>content/Suffix.md</li> <li>content/Superposition Catastrophe.md</li> <li>content/Suppletion.md</li> <li>content/Symbolic learning model.md</li> <li>content/Symbolic models.md</li> <li>content/Syntactic Ambiguity.md</li> <li>content/Syntactic Analysis.md</li> <li>content/Syntactic Bootstrapping.md</li> <li>content/Syntax First models.md</li> <li>content/Taking on semantic commitments, II collective versus distributive readings.md</li> <li>content/Text Normalization.md</li> <li>content/Text Preprocessing.md</li> <li>content/Text Segmentation.md</li> <li>content/The Differentiation Condition.md</li> <li>content/The Self Organization of Explicit Attitudes.md</li> <li>content/Tokenizer.md</li> <li>content/Top Down Parsing.md</li> <li>content/Tracking the Continuity of Language Comprehension Computer Mouse Trajectories Suggest Parallel Syntactic Processing.md</li> <li>content/Transitional probabilities.md</li> <li>content/Transitive verb.md</li> <li>content/Types of Words.md</li> <li>content/Unicode 5.0.md</li> <li>content/Unique Character Set.md</li> <li>content/Universal Quantifiers.md</li> <li>content/Unrestricted Race Model.md</li> <li>content/Verb.md</li> <li>content/Visual Implicit Learning.md</li> <li>content/Wh-dependencies.md</li> <li>content/Wickelphones.md</li> <li>content/Word Blending.md</li> <li>content/Word Clipping.md</li> <li>content/Word Compounding.md</li> <li>content/Word Segmentation.md</li> <li>content/Word Structure.md</li> <li>content/Words-and-Rules.md</li> <li>content/conditioning.md</li> <li>content/past tense.md</li> </ul>"},{"location":"loss/","title":"Loss","text":""},{"location":"loss/#tags-anchor","title":"tags: anchor","text":"<ul> <li>content/0-1 Loss.md</li> <li>content/AUC-Borji.md</li> <li>content/AUC-Judd.md</li> <li>content/Absolute Error.md</li> <li>content/Adversarial Loss.md</li> <li>content/Akaike Information Criterion.md</li> <li>content/Attention Alignment.md</li> <li>content/BCE with Logits.md</li> <li>content/BLEU.md</li> <li>content/Bayesian Information Criterion.md</li> <li>content/Binary Cross Entropy.md</li> <li>content/CTC.md</li> <li>content/Confusion Matrix.md</li> <li>content/Contrastive Loss.md</li> <li>content/Cosine Similarity.md</li> <li>content/Cross Entropy.md</li> <li>content/Cycle Consistency Loss.md</li> <li>content/Dice Score.md</li> <li>content/Focal Loss.md</li> <li>content/GE2E.md</li> <li>content/Hinge Loss.md</li> <li>content/Huber.md</li> <li>content/ITM Loss.md</li> <li>content/Identity Loss.md</li> <li>content/Intra cluster variance.md</li> <li>content/KL Divergence.md</li> <li>content/Log Likelihood Loss.md</li> <li>content/LogCosh.md</li> <li>content/MAPE.md</li> <li>content/MSE.md</li> <li>content/MSLE.md</li> <li>content/Mallows Cp Statistic.md</li> <li>content/Margin Ranking.md</li> <li>content/Max Margin Loss.md</li> <li>content/Negative Log Likelihood.md</li> <li>content/PatchGAN.md</li> <li>content/Perplexity.md</li> <li>content/Poisson Loss.md</li> <li>content/Precision Recall Curve.md</li> <li>content/Precision.md</li> <li>content/Quadratic Loss.md</li> <li>content/RAHP.md</li> <li>content/ROC Curve.md</li> <li>content/Recall.md</li> <li>content/SDR.md</li> <li>content/SSR.md</li> <li>content/Sensitivity.md</li> <li>content/Shuffled-AUC.md</li> <li>content/Sparse Dictionary Learning Loss.md</li> <li>content/Specificity.md</li> <li>content/Squared Error.md</li> <li>content/Squared Hinge.md</li> <li>content/Triplet Loss.md</li> <li>content/inter-sentence coherence loss.md</li> </ul>"},{"location":"mastersthesis/","title":"Mastersthesis","text":""},{"location":"mastersthesis/#tags-anchor","title":"tags: anchor","text":"<ul> <li>content/Beware of Inmates Running the Asylum.md</li> <li>content/CAM.md</li> <li>content/Classifying a specific image region using convolutional nets with an ROI mask as input.md</li> <li>content/Conductance.md</li> <li>content/Contributions of Shape, Texture, and Color in Visual Recognition Abstract.md</li> <li>content/DeepLIFT.md</li> <li>content/Embedding Human Knowledge into Deep Neural Network via Attention Map.md</li> <li>content/Generalizing Adversarial Explanations with Grad-CAM.md</li> <li>content/Influence of image classification accuracy on saliency map estimation.md</li> <li>content/Integrated Gradients.md</li> <li>content/Interpretation of Neural networks is fragile.md</li> <li>content/Layerwise Conservation Principle.md</li> <li>content/Noise Tunnel.md</li> <li>content/On the overlap between Grad-CAM saliency maps and explainable visual features in skin cancer images.md</li> <li>content/Real Time Image Saliency for Black Box Classifiers.md</li> <li>content/Salience Map.md</li> <li>content/SmoothGrad Square.md</li> <li>content/The Unreliability of Saliency Methods.md</li> <li>content/VarGrad.md</li> <li>content/Vision Explainibility.md</li> <li>content/journals/2022-10-05.md</li> <li>content/journals/2022-10-17.md</li> </ul>"},{"location":"medical/","title":"Medical","text":""},{"location":"medical/#tags-anchor","title":"tags: anchor","text":"<ul> <li>content/Acute.md</li> <li>content/Afib.md</li> <li>content/Angina.md</li> <li>content/Appendectomy.md</li> <li>content/Benign.md</li> <li>content/Biopsy.md</li> <li>content/Blood Culture.md</li> <li>content/Blood Lancet.md</li> <li>content/Blood Swab.md</li> <li>content/C-section.md</li> <li>content/Chronic.md</li> <li>content/Coronary Bypass.md</li> <li>content/Defibrillator.md</li> <li>content/Dialyser.md</li> <li>content/Dialysis.md</li> <li>content/Edema.md</li> <li>content/Embolism.md</li> <li>content/Endoscope.md</li> <li>content/Foley.md</li> <li>content/Forceps.md</li> <li>content/Fracture.md</li> <li>content/Hypertension.md</li> <li>content/Hypodermic Needle.md</li> <li>content/Hypotension.md</li> <li>content/Hysterectomy.md</li> <li>content/Intravenous.md</li> <li>content/Intubation.md</li> <li>content/Lead Test.md</li> <li>content/Lumbar Puncture or Spinal Tap.md</li> <li>content/Malignant.md</li> <li>content/Mastectomy.md</li> <li>content/Myocardial Infarction.md</li> <li>content/Nebulizer.md</li> <li>content/Occult Blood Screen.md</li> <li>content/Ophthalmoscope.md</li> <li>content/Otoscope or Auriscope.md</li> <li>content/Pulse Oximeter.md</li> <li>content/Reflex Hammer.md</li> <li>content/Remission.md</li> <li>content/Sepsis.md</li> <li>content/Speculum.md</li> <li>content/Spirometer.md</li> <li>content/Thrombosis.md</li> <li>content/Ultrasound.md</li> </ul>"},{"location":"memory%20trace/","title":"Memory Trace","text":"<ul> <li>the researchers were able to identify specific neurons in the brain\u2019s motor cortex \u2014 an area responsible for controlling movements \u2014 that were activated during the learning process.</li> <li>The researchers tagged these potential engram cells with a fluorescent marker so they could see if they also played a role in recalling the memory later on</li> </ul>"},{"location":"pGLSM/","title":"pGLSM","text":"<ul> <li>Text-Free Prosody-Aware Generative Spoken Language Modeling</li> <li>similar to how GPT-2 can generate coherent paragraphs</li> <li>builds upon </li> <li>addresses the generative aspects of speech pre-training</li> <li>replacing text with discovered phone-like units for language modeling and shows the ability to generate meaningful novel sentences</li> <li>the units used in GSLM discard most of the prosodic information</li> <li>fails to leverage prosody for better comprehension, and does not generate expressive speech</li> <li>prosody-aware generative spoken language model (pGSLM)</li> <li>multi-stream transformer language model (MS-TLM) of speech, represented as discovered unit and prosodic feature streams, and an adapted HiFi-GAN model converting MS-TLM outputs to waveform</li> </ul>"},{"location":"partially%20informed%20ethical%20judgement/","title":"Partially Informed Ethical Judgement","text":"<ul> <li>with some information about a given agent's state and knowledge</li> </ul>"},{"location":"partially%20informed%20ethical%20judgement/#backlinks","title":"Backlinks","text":"<ul> <li>Building Ethics into Artificial Intelligence</li> <li>partially informed ethical judgement</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"past%20tense/","title":"Past Tense","text":"<ul> <li>When a speaker needs to say the past tense form of a verb if the verb has an irregular form, choose that if not, add -ed</li> </ul>"},{"location":"plan%20recognition%20problem/","title":"Plan Recognition Problem","text":"<ul> <li>It is likely that the student wants a hint on how to do that step correctly. If the tutor can determine which correct step corresponds to the incorrect step entered by the student, then it can safely hint that correct step. On the other hand, if the tutor cannot determine which correct step corresponds to the student's step, the tutor may ask the student what step they were trying to enter. Andes does this for unrecognizable equations that the student wants help on.</li> <li>If none of these simple cases apply, then the tutoring system has to somehow figure out which of the possible next steps is most likely to be part of the solution that the student is trying to pursue.</li> <li>(Russell &amp; Norvig, 2003)</li> <li>The general idea is to match the sequence of steps leading up to the most recent student step against step sequences that are either generated by a planner or pulled from a plan library. Once the tutor has determined the plan that the student seems to be following, then it can easily determine what the next step in that plan is.</li> </ul>"},{"location":"psychology/","title":"Psychology","text":""},{"location":"psychology/#tags-anchor","title":"tags: anchor","text":"<ul> <li>content/Attentions and salience.md</li> <li>content/Berkeley et al.md</li> <li>content/Challenges of Words-and-rules.md</li> <li>content/Connectionism.md</li> <li>content/Connectionist Networks.md</li> <li>content/Cross-situational learning.md</li> <li>content/Deductive Approaches.md</li> <li>content/Distributive units.md</li> <li>content/Elman 1990.md</li> <li>content/Emergentism.md</li> <li>content/Extra-position.md</li> <li>content/First order generalization.md</li> <li>content/Inductive Learning.md</li> <li>content/Localist units.md</li> <li>content/Memory-based learning.md</li> <li>content/Milin et al..md</li> <li>content/Mirman et al..md</li> <li>content/Misyak et al 2010.md</li> <li>content/Nativists.md</li> <li>content/Non-adjacent dependencies.md</li> <li>content/Object-relative clauses.md</li> <li>content/Overhypotheses.md</li> <li>content/Propose-but-verify.md</li> <li>content/Rescorla-Wagner Algorithm.md</li> <li>content/Rescorla-Wagner Blocking.md</li> <li>content/Saffran, Aslin and Newport.md</li> <li>content/Second order generalization.md</li> <li>content/Semantics influences form.md</li> <li>content/Single unit recording.md</li> <li>content/Subject relative.md</li> <li>content/Subject-verb agreement.md</li> <li>content/Superposition Catastrophe.md</li> <li>content/Symbolic learning model.md</li> <li>content/Symbolic models.md</li> <li>content/Transitional probabilities.md</li> <li>content/Visual Implicit Learning.md</li> <li>content/Wh-dependencies.md</li> <li>content/Wickelphones.md</li> <li>content/Words-and-Rules.md</li> <li>content/conditioning.md</li> </ul>"},{"location":"random%20dump/","title":"Random Dump","text":""},{"location":"regularize/","title":"Regularize","text":""},{"location":"regularize/#tags-anchor","title":"tags: anchor","text":"<ul> <li>content/AdaIn.md</li> <li>content/Adam.md</li> <li>content/Batch Normalization.md</li> <li>content/DeepNorm.md</li> <li>content/Dropout.md</li> <li>content/Effects of Regularization.md</li> <li>content/Fine Tuning Based Pruning.md</li> <li>content/Global Gradient Magnitude Based Pruning.md</li> <li>content/Global Magnitude Based Pruning.md</li> <li>content/He Initialization.md</li> <li>content/Label Smoothing.md</li> <li>content/Layer Normalization.md</li> <li>content/Layerwise Gradient Magnitude Based Pruning.md</li> <li>content/Layerwise Magnitude Based Pruning.md</li> <li>content/LeCun Init.md</li> <li>content/Leaky Relu.md</li> <li>content/Learning Rate Range Test.md</li> <li>content/Lp Regularization.md</li> <li>content/Mixup.md</li> <li>content/Modality Dropout.md</li> <li>content/No bias decay.md</li> <li>content/Optimizers.md</li> <li>content/Orthogonal Initialization.md</li> <li>content/Pruning.md</li> <li>content/Random Pruning.md</li> <li>content/Regularization Term.md</li> <li>content/Regularization.md</li> <li>content/Scheduling.md</li> <li>content/Scoring Pruning Approaches.md</li> <li>content/Structure Based Pruning.md</li> <li>content/Tuning Model Flexibility.md</li> <li>content/VariationalRecurrent Dropout.md</li> <li>content/Xavier Initialization.md</li> </ul>"},{"location":"robotics/","title":"Robotics","text":""},{"location":"robotics/#tags-anchor","title":"tags: anchor","text":"<ul> <li>content/Action Component.md</li> <li>content/Active Compliant Robot.md</li> <li>content/Actuator.md</li> <li>content/Affordance Detection Task Specific.md</li> <li>content/Anthropomorphic.md</li> <li>content/Articulated Manipulator.md</li> <li>content/Articulation.md</li> <li>content/Assembly Robot.md</li> <li>content/Average Number of Stored Instances per Category.md</li> <li>content/Bag of Words robotics.md</li> <li>content/Base Link.md</li> <li>content/Burn-in.md</li> <li>content/Carousel.md</li> <li>content/Centrifugal Force.md</li> <li>content/Circular Motion Type.md</li> <li>content/Clamp.md</li> <li>content/Clamping.md</li> <li>content/Compliant Robot.md</li> <li>content/Contact Sensor.md</li> <li>content/Continuous Path.md</li> <li>content/Cyclo Drive.md</li> <li>content/Cylindrical Topology.md</li> <li>content/Degrees of Freedom.md</li> <li>content/Direct-drive.md</li> <li>content/Drop Delivery.md</li> <li>content/Dual-memory Approach.md</li> <li>content/Enabling Device.md</li> <li>content/End-effector.md</li> <li>content/Endpoint.md</li> <li>content/Ensemble of Shape Functions.md</li> <li>content/Eye-to-hand System.md</li> <li>content/Familar Object Grasping Object Viiew recog.md</li> <li>content/Fine-grained Object Recognition.md</li> <li>content/Forgetting.md</li> <li>content/Forward Kinematic Solution.md</li> <li>content/Forward Kinematics.md</li> <li>content/GGCNN.md</li> <li>content/GRConvNet.md</li> <li>content/Gantry Robot.md</li> <li>content/Global Classification Accuracy.md</li> <li>content/Grasp Point Detection.md</li> <li>content/Gravity Loading.md</li> <li>content/Gripper.md</li> <li>content/Hand Guiding.md</li> <li>content/Harmonic Drive.md</li> <li>content/Harness.md</li> <li>content/Heightmaps Kinesthetic.md</li> <li>content/Humanoid Vision Engine.md</li> <li>content/Inductive Sensor.md</li> <li>content/Instance-based Learning.md</li> <li>content/Instruction Cycle.md</li> <li>content/Inverse Kinematics.md</li> <li>content/Inverse Reinforcement Learning.md</li> <li>content/Iterative Closest Point.md</li> <li>content/Joint Interpolated Motion.md</li> <li>content/Joint Motion Type.md</li> <li>content/Joint Space.md</li> <li>content/Joint Velocity.md</li> <li>content/Kalman Filter.md</li> <li>content/Kinesthetic Teaching.md</li> <li>content/Ladle Gripper.md</li> <li>content/Learning Component.md</li> <li>content/Learning to Detect Grasp Affordance.md</li> <li>content/Linear Interpolated Motion.md</li> <li>content/Load Cycle Time.md</li> <li>content/Local Descriptor.md</li> <li>content/Local Reference Frame.md</li> <li>content/Local-LDA Object Representation.md</li> <li>content/MVCNN.md</li> <li>content/MVGrasp.md</li> <li>content/Magnetic Detectors.md</li> <li>content/Manipulator.md</li> <li>content/Material Processing Robot.md</li> <li>content/Mirror Shift Function.md</li> <li>content/Mode Switch.md</li> <li>content/Neural Radiance Field.md</li> <li>content/Occlusion.md</li> <li>content/Opportunistic Learning.md</li> <li>content/Optical Encoder.md</li> <li>content/Optical Proximity Sensors.md</li> <li>content/OrthographicNet.md</li> <li>content/Palletizing.md</li> <li>content/Parallel Shift Function.md</li> <li>content/Particle Filter.md</li> <li>content/Pendant Teaching.md</li> <li>content/Perception Component.md</li> <li>content/Perceptual Messages.md</li> <li>content/Phases of Simulated User Experiments.md</li> <li>content/Pick and Place Cycle.md</li> <li>content/Pinch Points.md</li> <li>content/Point-to-Point.md</li> <li>content/Polynomial Trajectories.md</li> <li>content/Power and Force Limiting (PFL).md</li> <li>content/Presence-sensing Safeguarding Device.md</li> <li>content/Prismatic Joint.md</li> <li>content/Programmable Logical Controller (PLC).md</li> <li>content/Proximity Sensor.md</li> <li>content/Pulse Coordinates.md</li> <li>content/Quadratic Potential Field.md</li> <li>content/Quasi-static Clamping.md</li> <li>content/RANSAC.md</li> <li>content/Reasoning Component.md</li> <li>content/Revolute Joint.md</li> <li>content/Risk Mitigation.md</li> <li>content/Robot Range Limit Monitoring.md</li> <li>content/Robotic Joints.md</li> <li>content/Roll.md</li> <li>content/Rotary Joint.md</li> <li>content/Rotary Vector Drive (RV).md</li> <li>content/Safeguard.md</li> <li>content/Safety Integrity Level.md</li> <li>content/Safety Logic Circuit.md</li> <li>content/Semantic Data.md</li> <li>content/Sense-Plan-Act Model.md</li> <li>content/Sensory Feedback.md</li> <li>content/Servo Control.md</li> <li>content/Servo Motor.md</li> <li>content/Servo Pack.md</li> <li>content/Servo-controlled Robot.md</li> <li>content/Servo-system.md</li> <li>content/Shock Detection Function.md</li> <li>content/Singularity.md</li> <li>content/Softlimit Setting Function.md</li> <li>content/Spline Motion Type.md</li> <li>content/Spline.md</li> <li>content/TSDF.md</li> <li>content/Task (endeffector) Space Vs. Joint Space.md</li> <li>content/Teach Lock.md</li> <li>content/The Repulsive Potential.md</li> <li>content/Through-beam.md</li> <li>content/Time Measuring Function.md</li> <li>content/Trajectory Planning.md</li> <li>content/Transducer.md</li> <li>content/Trapezoidal Trajectory.md</li> <li>content/Unet Grasping.md</li> <li>content/Vacuum Cup Hand.md</li> <li>content/Viewpoint Feature Histogram.md</li> <li>content/Visual Servo System.md</li> <li>content/Volumetric Grasping Network.md</li> <li>content/Work Envelope.md</li> <li>content/Wrist.md</li> <li>content/Yaw.md</li> </ul>"},{"location":"sacred%20values/","title":"Sacred Values","text":"<ul> <li>morally forbids the commitment of certain actions regardless of consequences</li> </ul>"},{"location":"sacred%20values/#backlinks","title":"Backlinks","text":"<ul> <li>Building Ethics into Artificial Intelligence</li> <li>Such rules often involve protected values (a.k.a. sacred values)</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"swap-dominance/","title":"Swap-dominance","text":"<ul> <li>when ranking alternatives to form a model of ethical preferences</li> <li>When new decisions need to be made, the summarized model is used to compute a collective decision that results in the best possible outcome</li> </ul>"},{"location":"swap-dominance/#backlinks","title":"Backlinks","text":"<ul> <li>Building Ethics into Artificial Intelligence</li> <li>swap-dominance</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"textless-lib/","title":"Textless-lib","text":"<ul> <li>textless-lib: a Library for Textless Spoken Language Processing</li> <li>Textless spoken language processing research aims to extend the applicability of standard NLP toolset onto spoken language and languages with few or no textual resources</li> <li>PyTorch</li> <li>speaker probing, (ii) speech resynthesis and compression, and (iii) speech continuation.</li> </ul>"},{"location":"todo/","title":"ToDO","text":"<ul> <li>graphs for every loss function and it\u2019s derivative</li> <li>reflow gradient descent document</li> <li>https://github.com/kennethleungty/MLOps-Specialization-Notes</li> </ul>"},{"location":"treecoverSegmentation/","title":"Tree Cover Segmentation","text":"<ul> <li>treecover segmentation PointNet++<ul> <li>Data collected from above</li> <li>Normalization : height, xy</li> <li>Rotation</li> <li>Jiggling ??</li> <li>Labeling<ul> <li> Segmentation algorithm. Canopy hide model</li> <li>Weighted loss + Focal Loss</li> </ul> </li> </ul> </li> </ul>"},{"location":"treecoverSegmentation/#2d-methods","title":"2d Methods","text":"<ul> <li>Watershed + Unet</li> <li></li> <li><ul> <li>\\(\\Theta\\) is just clippingpng]]</li> <li>The sqrt makes it a little smoother</li> </ul> </li> </ul>"},{"location":"treecoverSegmentation/#ref","title":"Ref","text":"<ul> <li>Max Freudenberg - Gottingen Uni Germany</li> <li>Adrian Stroker - Gottingen Uni Germany</li> </ul>"},{"location":"trolley%20scenario/","title":"Trolley Scenario","text":"<ul> <li>making a participant actively cause harm to an innocent bystander by pushing him on to the train track in order to save the lives of five people</li> </ul>"},{"location":"trolley%20scenario/#backlinks","title":"Backlinks","text":"<ul> <li>Building Ethics into Artificial Intelligence</li> <li>trolley scenario</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"usermodel/","title":"Usermodel","text":""},{"location":"usermodel/#tags-anchor","title":"tags: anchor","text":"<ul> <li>content/Algebra Cognitive Tutor.md</li> <li>content/Andes.md</li> <li>content/AutoTutor.md</li> <li>content/Coarse-grained assessment.md</li> <li>content/Collaborative Recommender.md</li> <li>content/Content Based Recommender.md</li> <li>content/DT Tutor.md</li> <li>content/Declarative memory.md</li> <li>content/Effects of Contextual Cues on Inferring and Remembering Meanings of New Word.md</li> <li>content/Extensions to SlimStampen.md</li> <li>content/Eye Tracking.md</li> <li>content/Filter Bubble Problem.md</li> <li>content/Final Paper User Models.md</li> <li>content/Fine Grained assesment.md</li> <li>content/GOMS.md</li> <li>content/Game Based Learning.md</li> <li>content/Gamification.md</li> <li>content/Gaze position.md</li> <li>content/Grey sheep problem.md</li> <li>content/Group Modeling Approach.md</li> <li>content/Help Abuse.md</li> <li>content/Help Refusal.md</li> <li>content/IRT.md</li> <li>content/Ideas for Fact Learning.md</li> <li>content/Individual Modeling.md</li> <li>content/Knowledge Component.md</li> <li>content/Latent Semantic Analysis.md</li> <li>content/Learning Event.md</li> <li>content/Learning L2 German Vocabulary Through Reading.md</li> <li>content/Macroadaptation.md</li> <li>content/Mastery learning.md</li> <li>content/Modeling Driver Behavior with Cognitive Architecture.md</li> <li>content/Modeling Transfer.md</li> <li>content/Predicting Student learning Curve.md</li> <li>content/Pupil Dilation.md</li> <li>content/Ramp up problem.md</li> <li>content/Rational inference.md</li> <li>content/Recommender System.md</li> <li>content/SQL-Tutor.md</li> <li>content/Satisficing Heuristic.md</li> <li>content/Second Language Vocabulary Learning , The role of context  versus translation.md</li> <li>content/Serious Games.md</li> <li>content/Sherlock.md</li> <li>content/SlimStampen.md</li> <li>content/Steve.md</li> <li>content/The Behavior of Tutoring Systems.md</li> <li>content/The Effect of Three Consecutive Context Sentences on EFL Vocabulary-Learning.md</li> <li>content/Tutor.md</li> <li>content/plan recognition problem.md</li> </ul>"},{"location":"visualization/","title":"Visualization","text":""},{"location":"visualization/#tags-anchor","title":"tags: anchor","text":"<ul> <li>content/1D piecewise linear interpolation.md</li> <li>content/Area Minimization.md</li> <li>content/Asymptotic Decider.md</li> <li>content/Average Filter.md</li> <li>content/Back To Front Raycasting.md</li> <li>content/Barycentric Interpolation.md</li> <li>content/Bend Minimization.md</li> <li>content/Bilinear Interpolation.md</li> <li>content/Change Blindness.md</li> <li>content/Characteristics of Visual Variables.md</li> <li>content/Classification Ray Casting.md</li> <li>content/Clutter In Visualisation.md</li> <li>content/Color Compositing.md</li> <li>content/Color Spaces.md</li> <li>content/ColorMap.md</li> <li>content/Complex Geometry.md</li> <li>content/Contour.md</li> <li>content/Conv Based Noise Reduction.md</li> <li>content/Countouring with Transparency.md</li> <li>content/Critical Points.md</li> <li>content/Cross Minimization.md</li> <li>content/Cross angle Maximization.md</li> <li>content/Cuboids.md</li> <li>content/Curl And Vorticity.md</li> <li>content/Cylinders.md</li> <li>content/Data Structures.md</li> <li>content/Diffusion Tensor.md</li> <li>content/Divergence.md</li> <li>content/Divide Oriented.md</li> <li>content/Early Ray Termination.md</li> <li>content/Eigenvector.md</li> <li>content/Ellipsoids.md</li> <li>content/Entourage Plot.md</li> <li>content/Euler Integration.md</li> <li>content/Eulerian Grid.md</li> <li>content/Filtering.md</li> <li>content/Finite Differences.md</li> <li>content/First order integration.md</li> <li>content/Force Directed Graph Layout.md</li> <li>content/Fractional Anisotropy.md</li> <li>content/Front to Back Raycasting.md</li> <li>content/Gaussian Filter.md</li> <li>content/Gestalt Laws.md</li> <li>content/Glyphs.md</li> <li>content/Graphs.md</li> <li>content/Grids.md</li> <li>content/H3 View.md</li> <li>content/Height Plots.md</li> <li>content/Helmholtz Theorem.md</li> <li>content/Hierarchial Refinement.md</li> <li>content/Hierarchical Edge Bundling.md</li> <li>content/High pass filter.md</li> <li>content/HyperStreamlines.md</li> <li>content/ICA Noise Removal.md</li> <li>content/Inattentional Blindness.md</li> <li>content/Inceptionism.md</li> <li>content/Indirect Volume Visualization.md</li> <li>content/Information Visualization.md</li> <li>content/Integral Lines.md</li> <li>content/Interpolation.md</li> <li>content/Intuitive Color spaces.md</li> <li>content/Isoline.md</li> <li>content/Isosurface.md</li> <li>content/Lagrangian Coherent Structure.md</li> <li>content/Lagrangian Grid.md</li> <li>content/Laplacian Grid Smoothing.md</li> <li>content/Length Optimization.md</li> <li>content/Line Integral Convolution.md</li> <li>content/Mapping to Geometry.md</li> <li>content/Marching Cubes.md</li> <li>content/Marching Squares.md</li> <li>content/Marching Tetrahedra.md</li> <li>content/Mean Diffusivity.md</li> <li>content/Median Filter.md</li> <li>content/Mesh Smoothing.md</li> <li>content/Mesh refinement.md</li> <li>content/Midpoint Decider.md</li> <li>content/Midpoint Method.md</li> <li>content/Node Link Diagram.md</li> <li>content/Noise Suppression.md</li> <li>content/Notch filter.md</li> <li>content/Oblique Slicing.md</li> <li>content/Opacity Correction.md</li> <li>content/Orthogonal Slicing.md</li> <li>content/Parallel Coordinate Plots.md</li> <li>content/Particle Visualization.md</li> <li>content/Pathlines.md</li> <li>content/Perception.md</li> <li>content/Perceptually Uniform.md</li> <li>content/Phong Lighting.md</li> <li>content/Post Classification.md</li> <li>content/Postattentive Amnesia.md</li> <li>content/Pre Classification.md</li> <li>content/Pre Integrated Volume Rendering.md</li> <li>content/Radial Plot.md</li> <li>content/Raycasting.md</li> <li>content/Region Growing.md</li> <li>content/Runge Kutta.md</li> <li>content/Sampling Ray Casting.md</li> <li>content/Scalar Color Coding.md</li> <li>content/Shading.md</li> <li>content/Shepard Interpolation.md</li> <li>content/Slice Based Volume Rendering.md</li> <li>content/Spectrogram.md</li> <li>content/Stream Ribbons.md</li> <li>content/Stream Surfaces.md</li> <li>content/Streamline Stopping Criterion.md</li> <li>content/Streamlines.md</li> <li>content/SuperQuadrics.md</li> <li>content/Symmetries Node Link.md</li> <li>content/Time Dependant Vector Field.md</li> <li>content/Transfer Function.md</li> <li>content/Treemap.md</li> <li>content/Visual Associative.md</li> <li>content/Visual Encoding.md</li> <li>content/Visual Length.md</li> <li>content/Visual Ordered.md</li> <li>content/Visual Quantitative.md</li> <li>content/Visual Selective.md</li> <li>content/Visualization Of Layers.md</li> <li>content/Volume Rendering Equation.md</li> <li>content/Volume Visualization.md</li> <li>content/Volumetric Illumination.md</li> <li>content/Voxel Projection.md</li> </ul>"},{"location":"visualization/#backlinks","title":"Backlinks","text":"<ul> <li>Conductance</li> <li> <p>The visualization is done by aggregating the conductance along the color channel and scaling the pixels in the actual image by the conductance values.</p> </li> <li> <p>Demographic Parity</p> </li> <li> <p>Contrast with equalized odds and equality of opportunity, which permit classification results in aggregate to depend on sensitive attributes, but do not permit classification results for certain specified ground-truth labels to depend on sensitive attributes. See \"Attacking discrimination with smarter machine learning\" for a visualization exploring the tradeoffs when optimizing for demographic parity.</p> </li> <li> <p>GradCAM</p> </li> <li>Gradient-weighted Class Activation Mapping (Grad-CAM) is an improvement over Class Activation Mapping (CAM) that provides a more detailed and accurate visualization of the regions of an image that are important for a given classification.</li> <li> <p>In summary, Grad-CAM is an improvement over CAM because it provides a more detailed and accurate visualization of the regions of an image that are important for a given classification by using gradients of the class scores with respect to the feature maps, providing additional information about the contribution of different regions of the image to the final classification decision.</p> </li> <li> <p>Guided [Grad-CAM](./Grad-CAM.md)</p> </li> <li>The main difference between Grad-CAM and Guided Grad-CAM is that while Grad-CAM focuses on finding the regions of an image that are most important for a given classification, Guided Grad-CAM also takes into account the positive gradients of the guided backpropagation algorithm, in order to provide a more fine-grained visualization of the internal representations of the network. This can make Guided Grad-CAM more effective for understanding how the model is making its decisions, and for identifying the specific features of an image that the model is using for a given classification.</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"wave2vec/","title":"wave2vec","text":"<ul> <li>wav2vec: Unsupervised Pre-training for Speech Recognition</li> <li>Reducing the need for manually annotated data is important for developing systems that understand non-English languages, particularly those with limited existing training sets of transcribed speech</li> <li>first application of unsupervised pre-training to speech recognition using a fully convolutional model that learns representations of raw, unlabeled audio</li> <li>trained on large amounts of unlabeled audio data and the resulting representations are then used to improve acoustic model training</li> <li>pre-train a simple multi-layer convolutional neural network optimized via a noise contrastive binary classification task</li> <li>learn the difference between original speech examples and modified versions, often repeating this task hundreds of times for each second of audio, and predicting the correct audio milliseconds into the future</li> <li>beats traditional ASR systems that rely solely on transcribed audio</li> <li>experiments on WSJ reduce WER of a strong character-based log-mel filterbank baseline</li> <li>more data for pre-training improves performance and that this approach not only improves resource-poor setups, but also settings where all WSJ training data is used</li> </ul>"},{"location":"articles/AI%20For%20startups/","title":"AI For Startups","text":"<p>AI For startup #ai</p>"},{"location":"articles/AI%20For%20startups/#hitchhikers-guide-to-ai-for-startups","title":"Hitchhikers Guide to AI for Startups","text":"<p>Are you part of a startup, or want to start one? Want to have AI superpowers but don't know where to start? Want to make sure you don't sink your boat before it floats? Make sure you hire the right people and avoid blowing your budget out of the water. This article is for you.</p> <p>A helpful guide on what to focus on, resources you need, and punching common myths in the face. Sounds interesting?</p> <p>Read on.</p> <p>(This is a long article. Skip to what you need. But if you are just starting, I would recommend reading the whole thing. Take your time. Take notes. Drop me an email with your questions. msubhaditya@gmail.com)</p> <p>Okay, now, DONT PANIC.</p>"},{"location":"articles/AI%20For%20startups/#a-definition","title":"A Definition","text":"<p>Before we move on, let us first define what we mean by AI. Of course, there are quite a few definitions but for our purposes, we can define it as \u201cany means of injecting a form of semi-automated intelligence that either performs a task previously impossible for computers or does a task as good as/better/faster than a human. An AI is used to learn how to perform a task, and can be thought of as a more advanced software.\u201d</p> <p>Note for AI experts: The term AI is used in the article as a general word, interchangeably with deep learning, neural networks, etc. Yes, technically this is inaccurate. But this is an article for people with not much in the way of experience.</p>"},{"location":"articles/AI%20For%20startups/#some-useful-terms-you-might-need","title":"Some Useful Terms You Might Need","text":"<ul> <li>AI model/architecture: The brain that makes up an AI</li> <li>GPU: A specialized computing unit that accelerates intensive computation, like training an AI</li> <li>Neural Networks: Inspired by the brain, what makes \"AI\" tick</li> <li>Latency: How long does your model take to give results</li> <li>SOTA: State of the Art. What is the best model for the task, right now?</li> <li>Training: The process where the AI learns how to perform a task</li> <li>Inference: Using a trained AI to predict some results</li> <li>Cluster: A group of computers used to parallelly perform a task</li> </ul>"},{"location":"articles/AI%20For%20startups/#a-little-index","title":"A Little Index","text":"<p>This article is divided into three major sections.</p> <ul> <li>If you want to dive into this field for the first time, or find ways to inject some AI into your companies, the first section is for you.</li> <li>If you already have an AI startup, and are looking for ways to improve your infrastructure so you can grow, the second section is for you.</li> <li>The third section talks about some of the pitfalls that one might face when they first dive into this space. Take it as a word of caution.</li> </ul>"},{"location":"articles/AI%20For%20startups/#section-1-the-beginning-stages-or-how-should-i-care-about-ai","title":"Section 1 - The Beginning Stages Or How Should I Care About \u201cAI\u201d?","text":""},{"location":"articles/AI%20For%20startups/#cutting-through-the-hype","title":"Cutting Through the Hype","text":"<ul> <li>Unrealistic expectations At the end of the day, AI is not a magic mushroom. It cannot solve everything you want, and neither can it evolve and take over the planet. Yet. Can you use it for your needs? Undoubtedly. So what can it do? It can categorize images, translate text, understand what it hears, recognize tumors, and anything your creativity allows. The key is to set realistic expectations. Think of it this way, if you can get a team of experts to do the task, then it might be possible to have an AI do it (Terms and conditions apply). Ask an AI consultant, or if you cannot afford one, look at this website [insert paperswithcode] for a list of tasks that are possible. If nothing else, it can give an idea of what can be done. Sometimes it might be possible for your idea to work in the long run, but it might just take more time and resources than you imagined. Only an expert would be able to tell you if it is a feasible plan. Expecting your developers to come up with something impossible is great, but only if you can afford it. For example, instead of trying to make an \"AI that will take humans to Mars\", break it down into smaller tasks - \"AI that recognizes space debris\" + \"AI that would identify system faults\" + \"AI that might predict what a type of rock found was\" + \u2026 etc</li> <li>Data requirements It is a myth that to \"train\" an AI, you need massive amounts of data. Yes, technically you do. But recently, there has been a lot of research conducted on \"transfer learning\", which is a technology that allows you to start with an AI trained on large amounts of data, and fine-tune it to your specific use case. This is very helpful, especially if you are working on tasks similar to those that exist. For instance, if you want to train an AI to recognize different types of cars, this might not need a massive amount of data, because similar \"recognition\" tasks exist. But if you want to classify a hundred types of new tumors, that might require a little more data.</li> <li>Extreme requirements The Tech giants want everyone to think that we need extreme computing power for our AI needs. But in reality, most companies can start with minimal requirements. Even if you cannot afford huge computing clusters or lots of computers, you can use online services to run the code. There are quite a few such services that provide \"GPUs\" (special computing units that accelerate running compute-intensive code). They charge you a fee by the hour, which is often quite cheaper, even at scale. As long as you have skilled workers, decent computers, and funds to support it. A word of warning, AI is still an experimental field. If your staff does not deliver, it would serve to try and understand if the task handed to them is realistic, or needs more resources. An AI expert/consultant would be helpful here.</li> </ul>"},{"location":"articles/AI%20For%20startups/#so-you-want-to-be-an-ai-startup","title":"So You Want to Be an AI Startup?","text":"<ul> <li>Is AI needed? The rush to say \"We use AI in our products to do XYZ\" is very intense. Do not fall into peer pressure. It serves to first understand and identify where you need AI. Does a specific part of the infrastructure need to be revamped? Is there something you want to do that just normal programming cannot handle? Have you tried other options first? AI is expensive in the long run. Can you afford the staff? Pay for the time taken to research? Test some ideas, you might be able to hire a freelancer to make some mockups before diving in headfirst.</li> <li>AI products vs AI as part of your existing infrastructure You might not need to fully use AI, maybe only some parts of your idea need it. If you are a new startup, please note that just AI will not be the end of all. Even if your idea is a pure AI product, there are quite a few software engineering components in it as well. It serves to understand the pipeline before starting, just so you do not fall into common traps and end up wasting time and money.</li> <li>Domain knowledge Does your product revolve around a specific domain? Hire/Consult an expert. You will suffer badly otherwise. Yes, an AI model will give you results. But you will have no way of knowing how well they do. Just having numbers is not enough. In return, this will help you make even better software.</li> <li>Survey Understanding your customer base is even more important here. The better you understand the requirements, the better you can make use of AI. The more you can flesh out your ideas, the more specific you can get with your software.</li> <li>Competition Who is your competition? Do you have the time to revamp your software? Everyone wants AI these days. Some companies can spend millions on it, while others can't. If you are in a field that has cutthroat technology developments, just starting up might be harder for you if you cannot afford it. If you are a new startup, try to focus on fields that need solutions but do not have larger corporations working on them. Domain expertise is great here.</li> <li>Using products from Google/Microsoft/Amazon You will find every big company these days offering AI support. Should you use them? In my opinion, these are useful if you are planning on performing a very common task. If you have a domain-specific idea, making your own is probably the better bet. But do not hesitate to make use of the resources that they offer. Google Colab, Amazon AWS, and Microsoft Azure are great services. Using them, especially at the start, is a good idea. They could be a cheaper way of testing out your ideas.</li> </ul> <p>Remember, AI is a tool. Not a complete product.</p>"},{"location":"articles/AI%20For%20startups/#section-2-the-mid-stages-or-how-can-i-make-sure-i-make-my-best-ai","title":"Section 2 - The Mid Stages ,Or, How Can I Make Sure I Make My Best \u201cAI\u201d?","text":""},{"location":"articles/AI%20For%20startups/#focus","title":"Focus","text":"<p>Being an AI startup, you have a lot of things to look out for. Some of the main focus points to keep in mind are as follows.</p> <ul> <li>Clear goals of what you want to achieve With any endeavor, knowing exactly what you want is imperative. More so in fields where testing takes up quite a lot of time and money. The better and more fine-grained your explanation, the better your results.</li> <li>Clear expectations on what is possible within a time frame I repeat, AI is a research field. Just because Google can make a huge product in a matter of months, does not guarantee you can too. An expert can help you set more realistic goals. You can, of course, try it out on your own, but only if you have the time and the risk appetite for the same.</li> <li>Easy of use Make sure your service is easy to use. Using AI will suddenly show you how many knobs and switches you need to control. Do not overwhelm your poor users. Specific research in understanding what your customer wants is essential.</li> <li>Multiple large platforms Scaling up is probably not a major concern for you right now. If it is though, make sure you can afford it. If you can't, see if you can outsource it, or use an online service. Perhaps also consider other ways of optimizing your workflow.</li> <li>Quick, rough tests Before you work on a final product, try out some small ideas. Hire freelancers if you do not have full-time staff on hand. Try out different models with any data you have on hand. Making sure you have a good baseline will save you quite some headache later on.</li> <li>Validation testing By this point, you might have heard me say \"test your ideas\" quite a lot. But again, test your models. Use new data, use crappy data. Does it still work? If not, keep working.</li> </ul>"},{"location":"articles/AI%20For%20startups/#people","title":"People","text":"<p>Aside from being able to build AI models, you also need people who would be able to support the infrastructure. Make sure you have domain experts you can call on. Also find people who would be able to deploy the model onto a chosen service, and can maintain them.</p> <p>Of course, the usual requirements of making an interface, servers, etc still stay based on the type of project you have in mind.</p> <p>Budget and requirements play a key role here and are specific to your idea and scale.</p>"},{"location":"articles/AI%20For%20startups/#section-3-a-word-of-caution-how-can-i-make-sure-my-boat-does-not-sink","title":"Section 3 - A Word of Caution How Can I Make Sure My Boat Does Not Sink?","text":""},{"location":"articles/AI%20For%20startups/#capacity","title":"Capacity","text":"<p>Oh? So you made an AI model? Congratulations. Now you need to get your users on board. Regardless of if you have an app, a website, or a device, make sure you can estimate how many users you will have. AI models tend to take more resources than classical computer software.</p> <p>Identify how long your model takes to get results. Will your users be fine with that? Do you need more resources to ensure faster outputs? Will they be able to reliably access your service?</p> <p>Test, test, test.</p> <p>There are quite a few optimizations possible. It is beyond the scope of this article, but you can drop me an email if you want to know more.</p>"},{"location":"articles/AI%20For%20startups/#model-drift","title":"Model Drift","text":"<p>Over time, how good your results are will drift. This might be because the data your model learned differs from the model it is getting now. For example, in a fashion-related product, the trend of clothing changes over time. Using an old model that has not seen new data will not be great.</p> <p>Periodically checking for a spike in errors, and retraining the model on new data is essential. Make sure your employees know how to do that.</p>"},{"location":"articles/AI%20For%20startups/#bias-and-ethical-concerns","title":"Bias and Ethical Concerns","text":"<p>If you teach a kid that colored people are evil, that's what they will learn. Similarly, an AI model can learn mistaken assumptions. The larger the data, the more such assumptions are automatically made. Identifying these would help you understand if your product would have unintended consequences.</p> <p>The more variation you can provide to your model, the better in the long run.</p> <p>Make sure your data is inclusive. Especially if it contains hints of the biases of age, gender, ethnicity, etc.</p> <p>Test for these biases specifically.</p> <p>Make sure that you can explain if your model goes wrong and does something stupid.</p> <p>There is a great course on how to do this Ethics for AI by Rachel Thomas from fast.ai.</p>"},{"location":"articles/AI%20For%20startups/#fin","title":"Fin","text":"<p>So, will you use AI, or will AI use you</p> <p>Feel free to reach out or leave your comments. Email: msubhaditya@gmail.com</p>"},{"location":"articles/Browser%20Extensions%20for%20Research/","title":"Browser Extensions for Research","text":""},{"location":"articles/Browser%20Extensions%20for%20Research/#11-free-chromefirefox-extensions-that-make-research-easier","title":"11 Free Chrome/Firefox Extensions That Make Research Easier.","text":"<p>While researching anything, we tend to heavily rely on our browser. To make this process more efficient, quite a few \u201cplugins\u201d have been created over the years. Every major browser has hundreds, if not thousands of such plugins/extensions/add-ons. Like any infinite list of things, it\u2019s often overwhelming to find helpful ones.</p> <p>For my work, I read a lot of articles and skim through many web pages, blogs, you name it. In the process, I need to store this information somewhere. Either to write a research paper, or a blog like this one, or just for my knowledge. The following plugins have made my life a lot easier and so I thought I would share them with you.</p> <p>If any of these plugins make no sense to you, ignore them. You probably don\u2019t need them as of now. But you might later on!</p> <p>Disclaimer - I am not sponsored by or affiliated with any of the programs I mention. They are shared purely because I find them useful.</p>"},{"location":"articles/Browser%20Extensions%20for%20Research/#a-note-about-browsers","title":"A Note About Browsers","text":"<p>I know I only mention Chrome and Firefox. Anywhere I mention Chrome, you can safely assume that Microsoft Edge, Brave, Opera, and Vivaldi also work. The same links should work directly!</p>"},{"location":"articles/Browser%20Extensions%20for%20Research/#collecting-information","title":"Collecting Information","text":"<p>I like taking notes, and my tool of choice here is the Markdown (another format like txt) notes app Obsidian. Of course, you can use anything you want. There are endless note applications, so everyone has their preference. These two plugins have been invaluable in collecting information across pages.</p> <ul> <li>Roam Highlighter This little plugin is absolutely beautiful. You can call it with a shortcut and highlight across the whole page! It strips out useless formatting and converts them to Markdown. If you use Roam/Obsidian/any other markdown editor, it even auto-converts the links and converts the highlighted text to markdown.<ul> <li>Chrome , Firefox</li> </ul> </li> <li>MarkDownload Sometimes you want large amounts of text from a webpage you come across. This plugin gets all the text/links/images etc from the page you are on and converts it into a format you can easily copy and paste from. Extremely handy isn\u2019t it?<ul> <li>Chrome , Firefox</li> </ul> </li> </ul>"},{"location":"articles/Browser%20Extensions%20for%20Research/#writing-research-papersarticles","title":"Writing Research Papers/Articles","text":"<p>A large chunk of my work involves writing technical articles and research papers. It\u2019s a lot of work, and I need shortcuts to help me out in the process. These plugin-ins have saved me a lot of headaches in the long run.</p> <ul> <li>TamperMonkey: Bibtex copy This plugin is a bit of a special case. Instead of providing specific functionality, it enables users to write their scripts to modify any webpage in real-time. You can do anything from Getting direct links, endlessly loading Google search results to modifying Twitter. While writing papers, citing them is a huge headache. Since I write my reports in LaTEX, I need the \u201cBibTex\u201d version from Google Scholar which is a pain. This little plugin automatically does that with a single click. You can find out how to install it here.<ul> <li>Chrome , Firefox</li> </ul> </li> <li>SLext If you write your documents in LaTEX, chances are you use Overleaf to do so. It is a website that makes it extremely easy to write any kind of professional document using LaTEX. But it does have limitations, and the biggest bummer for me is the lack of tabs. This plugin adds just that, and it saves me so much trouble.<ul> <li>Chrome , Firefox</li> </ul> </li> <li>Sci-Hub scholar I firmly believe that research should not be paywalled. (Even this article is available for free on my blog without the fancy stuff). If you are affiliated with a university or company, you might have access to as many papers as you want. But as an independent researcher? Well. Happy crying. This plugin adds links to the website Sci-Hub directly in Google Scholar which lets you access a lot of paywalled research directly. (I would link it but I don\u2019t want to get demonetized).<ul> <li>Chrome , Firefox</li> </ul> </li> <li>Unpaywall Similar to the previous one, this also lets you access paywalled articles by finding other un-paywalled versions of them from elsewhere on the web.<ul> <li>Chrome , Firefox</li> </ul> </li> <li>Zotero/Mendeley Connector For anyone who reads a lot of research papers, managing them is probably the biggest headache. Mendeley and Zotero are probably the most popular library managers. While browsing the web, you might want to directly save any research paper you like to your computer. This plugin lets you do that with a single click. Both the programs have this installed by default, you just need to enable them from the application itself - Zotero, Mendeley.</li> </ul>"},{"location":"articles/Browser%20Extensions%20for%20Research/#medium","title":"Medium","text":"<p>I admit most of you probably don\u2019t write articles yourself. But if you do, I find these two plugins useful for extending what is possible here on Medium.</p> <ul> <li>Code medium Adding proper code snippets is honestly such a pain on Medium. Maybe in time, this will be fixed, but for now, this plugin lets you directly create gists on Github. It looks pretty and just works. I write technical articles and trust me, it has saved me hours.<ul> <li>Chrome , Firefox</li> </ul> </li> <li>TOC Medium If you have noticed, not every article has a clickable table of contexts like this one. If you were to do it manually, you might need to look at the HTML source code, and do a lot of drama. Instead, this plugin just lets you add the TOC just as you would any other element. It does not auto-update though, so I would recommend leaving it for after you are done with your article. It is also sadly not available for Firefox.<ul> <li>Chrome , Firefox(Not there)</li> </ul> </li> </ul>"},{"location":"articles/Browser%20Extensions%20for%20Research/#honorable-mentions","title":"Honorable Mentions","text":"<p>Some of these are not exactly extensions, but I think they deserve to be mentioned anyway.</p> <ul> <li>My Text Cleanup Script Although many of the applications I mentioned above let you copy the text in chunks, they don\u2019t offer any advanced formatting options. (Eg: cleaning up Wikipedia links, making lists, formatting paragraphs, etc.) Now, there are a million ways to do this. But my favorite is this script that I wrote a while back. It uses Python and lets you do whatever you want to the text in your clipboard. After processing is complete, it pastes it back to your clipboard. This is not for everyone. But if you are interested, I\u2019ll be happy to explain how it works. Just ask!<ul> <li>Github</li> </ul> </li> <li>Text Workflow This application is not free. (Sorry!) I have a Mac, and this app lets you do whatever my script above does but with a UI. If you want something free, use the previously mentioned script. I\u2019m sure there are other alternatives for Linux and Windows. I either use vim or my script. I have not used Windows in a long time and so I can\u2019t recommend anything.<ul> <li>TextWorkflow</li> </ul> </li> <li>Adblocker An adblocker is essential to save your sanity. (Although turning it off on websites you want to support is good!) These are my favorites.<ul> <li>Chrome , Firefox</li> </ul> </li> <li>I don\u2019t care about cookies As the name suggests, this auto accepts only essential cookies from the prompt. It saves you that extra click and removes the banner that covers the entire bottom of web pages.<ul> <li>Chrome , Firefox</li> </ul> </li> </ul>"},{"location":"articles/Browser%20Extensions%20for%20Research/#fin","title":"Fin","text":"<p>This article is in the hopes that it will help someone out. Maybe have the help that I did not. I do not know who it will reach. But to whoever it does, best of luck :)</p> <p>Like these/Want more? Buy me a coffee! Kofi</p> <p>Want articles on something specific? Just ask!</p> <p>You can always contact me on LinkedIn, or drop me an Email</p>"},{"location":"articles/Commisions%20Overview/","title":"Commisions Overview","text":""},{"location":"articles/Commisions%20Overview/#commission-faq","title":"Commission FAQ","text":""},{"location":"articles/Commisions%20Overview/#name","title":"Name","text":"<p>Full form article about anything related to AI/Neural networks/Technical writing.</p>"},{"location":"articles/Commisions%20Overview/#description","title":"Description","text":"<p>Want an article on something in AI? Go ahead and ask me for it.</p> <p>If there are specific questions you want answered, you can request them as well.</p> <p>Topics you can request for (my expertise) - AI, Computer Vision, Deep learning, Neural Networks, Shell scripting, Python, Scientific Writing</p> <p>My previous blogs :</p> <p>https://msubhaditya.medium.com</p> <p>https://subhadityamukherjee.github.io/AI-knowledge-base/</p> <p>I\u2019d suggest you contact me first before you pay.</p> <p>You can either drop me an email here : msubhaditya@gmail.com</p> <p>Text me on LinkedIn : https://www.linkedin.com/in/subhaditya-mukherjee-a36883100/</p> <p>Word count isn't really my thing but it would be a medium sized article - around 1k-1.5k words.</p> <p>Please read the FAQs!</p>"},{"location":"articles/Commisions%20Overview/#instructions-and-faq","title":"Instructions and FAQ","text":"<p>The more specific your request, the better you will get an answer to them.</p> <p>Please Please drop your name and email. It would make it so much easier to contact you.</p> <p>FAQ:</p> <ul> <li>Will you do my homework?<ul> <li>Probably not. I take no responsibility for the grades you get.</li> </ul> </li> <li>How long will it take you to write my article?<ul> <li>That depends on your topic. Around 3 days is a good estimate. Although I would be able to give you a better estimate once I see the topic.</li> </ul> </li> <li>Can I distribute this article freely?<ul> <li>Generally yes. You must link to my page and have my name on it though. If you are going to monetise it, well. I can\u2019t really stop you but then I will monetise it too.</li> </ul> </li> <li>Will it be public?<ul> <li>That is your choice. Generally, yes. But if you want it on your website/company website, then just inform me before. As long as you credit me properly wherever you post it, I\u2019m okay with that.</li> </ul> </li> <li>Will you make a whole framework for me?<ul> <li>Nope. You will get demo code. Full code etc will be a lot more time intensive so the rate will be decided based on the request.</li> </ul> </li> <li>How many rewrites do I get?<ul> <li>Minor changes : two times.</li> <li>Major changes : once.</li> <li>Delays suck for both of us. So if you want something specific, it\u2019s better you tell me before I start.</li> </ul> </li> <li>Delays<ul> <li>I am not GPT-3, and sometimes I do face delays in getting work done. If I do though, I will keep you informed.</li> </ul> </li> <li>Refunds<ul> <li>Oops, you didn\u2019t like what you got. Apologies. If you can convince me that it was a badly written article, even after 2 minor and 1 major change, I will send you half your money back. After all, it did take a lot of my time. I don\u2019t want to encourage scams, so I hope you understand.</li> </ul> </li> </ul>"},{"location":"articles/Commisions%20Overview/#thank-you","title":"Thank You!","text":"<p>I hope we can both enjoy this article.</p> <p>If you have any further questions, don't hesitate to drop me a message or email at msubhaditya@gmail.com</p>"},{"location":"articles/Commisions%20Overview/#linkedin-post","title":"LinkedIn Post","text":"<p>TL;DR AI Article Commissions - Open!</p> <p>https://ko-fi.com/subhadityamukherjee/commissions</p> <p>I'm going to take a deep breath, and tell you that I'm now open to article commissions. Want an article on something in AI? Go ahead and ask me for it.</p> <p>If there are specific questions you want answered, you can request them as well.</p> <p>Topics you can request for (my expertise) - AI, Computer Vision, Deep learning, Neural Networks, Shell scripting, Python, Scientific Writing</p> <p>More info on the page linked here. This is not a free article. Although, if you do want one but can't afford it, you can still drop me a message. I can't guarantee it though.</p> <p>PS: This took a lot more than one deep breath. Let's see how it goes.</p>"},{"location":"articles/Commisions%20Overview/#commission-computervision-python-neuralnetworks-ai-deeplearning-shellscripting-article-request-scientificresearch","title":"commission #computervision #python #neuralnetworks #ai #deeplearning #shellscripting #article #request #scientificresearch","text":""},{"location":"articles/Commisions%20Overview/#prices","title":"Prices","text":"<ul> <li>Minimum. Short article. 500 ish words : 30 euro</li> <li>Medium sized article. More in depth. Around 1k-1.5k words. : 50 euro</li> <li>Longer article. Fully in-depth. Around 2k-2.5k words. : 100 euro</li> <li>Along with working code. : 200 euro</li> </ul>"},{"location":"articles/Easier%20Deep%20Learning%20Research/","title":"Easier Deep Learning Research","text":""},{"location":"articles/Easier%20Deep%20Learning%20Research/#starting-deep-learning-research-part-1-a-start-using-fastai","title":"Starting Deep Learning Research (PART 1): A Start Using FastAI","text":"<p>{{TOC}}</p> <p>So, you took your first steps into Deep Learning. Maybe you read a few articles, did a course or two, and watched a bunch of videos. Or maybe you just heard so much about it that you wanted to learn more.</p> <p>Welcome.</p> <p>This is a beautiful world. It is also very overwhelming. There is so much to learn and understand. But we need to start somewhere.</p> <p>This is your ticket. Enjoy the ride.</p> <p>Note: This will be very long-winded as it is meant for complete beginners. It might seem very scary at first. But don\u2019t panic. The hardest part is getting started. Hold on. Come back to it. It will take time. Slow down. Read through it. It will save you a lot of pain later on.</p>"},{"location":"articles/Easier%20Deep%20Learning%20Research/#entry-requirements","title":"Entry Requirements","text":"<ul> <li>Go to this link for the code and follow along kaggle_notebook. Once that opens up, click the button next to \u201cAccelerator\u201d and choose GPU. Accept the terms.</li> <li>You have learned some Python. If not, go to YouTube and learn as much as you can first. Do as many examples and problems as you deem enough to understand. Come back.</li> <li>You know what a GPU is and if you have one.</li> <li>You have a computer and an internet connection.<ul> <li>If you have a powerful computer, you can set this up locally. fastai<ul> <li>You will need an editor. I would recommend VSCode.</li> <li>If you do not, that\u2019s alright. You can still follow along.</li> </ul> </li> <li>Watch a small tutorial on how to use a Jupyter Notebook here.</li> </ul> </li> </ul>"},{"location":"articles/Easier%20Deep%20Learning%20Research/#what-we-want-to-do","title":"What We Want to Do","text":"<p>Today we will be teaching our little \u201cAI\u201d how to categorize different fruits. To do that, we need to give examples - aka the \u201cdataset\u201d. You can grab it from Kaggle.</p> <p>We will show our little \u201cAI\u201d quite a lot of images and tell it \u201chey, this is an Apple, this is not one\u201d and so on, in the hope that it will learn.</p> <p>The exact algorithms are not important at this stage. I believe it is first good to be able to see where to use it in practice and play around with it before diving deep.</p> <p>In the end, we want our \u201cAI\u201d to accurately classify the fruits that it has learned about.</p>"},{"location":"articles/Easier%20Deep%20Learning%20Research/#libraries-we-will-use","title":"Libraries We Will Use","text":"<ul> <li>Major Framework : Pytorch. Deep learning is quite a vast field, which means that code quickly starts to become complicated. There is a lot of boilerplate code that needs to be written every time you want to make something. Writing the same 200 lines of code every time is not efficient, and leads to bugs and weird errors. To avoid that, we use a framework that handles most of the hard work for us so we can focus on innovation and application. There are quite a few of these frameworks, PyTorch and Tensorflow/Keras are the two major ones in Python. \\     I prefer Pytorch. There are many reasons for that. Instead of me adding to this blog about them, read this little article by the creator of the next library we will use, link</li> <li>fastai FastAI was one of my first introductions to research grade Deep Learning. It is a wrapper around Pytorch that does live up to its name. Pytorch, being a general framework, focuses on the core components of Deep Learning. FastAI was made to provide a lot of ease-of-use functions that Pytorch did not. But because it\u2019s just a wrapper, any functions that Pytorch offers can be easily accessed.</li> </ul>"},{"location":"articles/Easier%20Deep%20Learning%20Research/#grab-the-data-and-set-everything-up","title":"Grab the Data and Set Everything Up","text":"<ul> <li>Make an account on Kaggle. This is not promoted or anything. It is probably one of the biggest hubs for AI code, and you will make an account sooner or later. Better to do it now.</li> <li>If you are using this link, click the 3 dots, and then click \u201cCopy and Edit\u201d. That will set up the data and the code for you.</li> <li>If you are working on your machine, you can download the notebook by clicking the 3 dots, and then clicking \u201cDownload code\u201d. Open this up in VSCode. The data can be found from Kaggle.</li> </ul>"},{"location":"articles/Easier%20Deep%20Learning%20Research/#code-imports","title":"Code Imports","text":"<p>We will need to first import the libraries we will use.</p> <pre><code>import os\nfrom fastai.vision.all import *\nfrom fastai.vision.widgets import *\n\n# Set the base directory where Kaggle saves its data. Change this if you are on your machine.\nroot_dir = \"../input/fruits/fruits-360_dataset/fruits-360/Training\"\n</code></pre>"},{"location":"articles/Easier%20Deep%20Learning%20Research/#load-data","title":"Load Data","text":"<p>Fastai provides quite a few convenient functions to load data. Now slow down. What do we have? Think about it for a second.</p> <p>We have an image and a label right? And we need to \u201cmap\u201d the image to the label. If this does not make sense. Think about it some more. This is important.</p> <p>Okay.</p> <p>Now, what does the file path for an image look like?</p> <p>Something like : \"../input/fruits/fruits-360_dataset/fruits-360/Test/Cantaloupe 1/r_140_100.jpg\u201d.</p> <p>What will the label be? \u201cCantaloupe 1\u201d right? This is the \u201cparent\u201d of the path, aka the parent folder.</p> <p>This fully depends on how the data is structured. More on that later.</p> <pre><code>def get_parent_name(x): \n    return x.parent.name\n</code></pre> <p>Okay, now we need to tell the system the kind of data we are giving it. In this case, it is Image -&gt; Category (Label). Then we can find all the image files in the folders, and use the \u201cget_parent_name\u201d function to assign labels to all our data.</p> <p>When we learn something in school, we are tested on it with questions we have never seen right? This helps us understand if we comprehended the topic or not. Similarly, we split the data into Train and Test sets.</p> <p>We also resize the images to a single size (for convenience).</p> <p>Since we want our little \u201cAI\u201d to recognize objects from any angle, lighting condition, etc, we provide some \u201caugmentation\u201d to the data. Things like randomly changing the brightness, rotating the image, etc. The objective is to provide variation, that further improves how well our network learns.</p> <p>We can then pass these instructions are create a \u201cData-loader\u201d. The definition is the name itself.</p> <pre><code>path = Path(root_dir) # base path\nfields = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_items=get_image_files,\n    get_y=get_parent_name,\n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    item_tfms=RandomResizedCrop(64, min_scale=0.5),\n    batch_tfms=aug_transforms(),\n)\ndls = fields.dataloaders(path)\n</code></pre> <p>\u200c</p> <p>Okay. Does this work? Let us get a sample (or a batch as it is called in this field.)</p> <pre><code>dls.show_batch()\n</code></pre> <p>How many categories of fruits do we have?</p> <pre><code>dls.c # no of categories\n</code></pre>"},{"location":"articles/Easier%20Deep%20Learning%20Research/#teaching-our-ai-part-1","title":"Teaching Our AI: Part 1","text":"<p>We have the data. Now we need the \u201cAI\u201d.</p> <p>You must be wondering why the word has been in quotes the whole time. It is because what we are training is not an \u201cAI\u201d, but just a component of it, called a \u201cNeural Network\u201d.</p> <pre><code>learn = vision_learner(dls,\n                resnet18, #architecture\n                loss_func=LabelSmoothingCrossEntropy(), #loss function/objective\n                opt_func=partial(OptimWrapper, opt=torch.optim.AdamW), # Optimizer\n                metrics=[accuracy, error_rate],\n                cbs=[MixUp]).to_fp16() #callbacks, mixed precision\n</code></pre> <p>The only things you need to know about it for now are :</p> <ul> <li>There are many types of networks, each better at something else. These are called \u201carchitectures\u201d. ResNet18 is one of them.</li> <li>You can train a network to learn a specific type of mapping. Be it an image -&gt; text, audio -&gt; labels , image -&gt; image etc.</li> <li>This training takes time and is mathematically pretty hard. Which is why this field is still in research. Further explanations about all of this will be given in later articles. (And others that I have written about in the past, linked at the end.)</li> <li>In addition to the architectures, we have algorithms called \u201cOptimisers\u201d that enable smoother learning. This will not make any sense right now. But all in due time.</li> <li>A \u201closs function\u201d is fancy speak that is a way of seeing how well our network is doing while it\u2019s learning. A mini exam if you will, in the sense that the network tries to \u201coptimise\u201d for this exam. The better it gets, the better your final results.</li> <li>Metrics give us somewhat precise values. Such as the grade in an exam. The \u201cAccuracy\u201d. Etc.</li> <li>The other weird things that you see here are:<ul> <li>cbs: [MixUp]. And to_fp16(). The first is to make sure that the network trains better, like the augmentations we spoke about. The second is an interesting paradigm that lets us train our models faster and with lower memory. More on that here. These are advanced topics. More on that later.</li> </ul> </li> </ul>"},{"location":"articles/Easier%20Deep%20Learning%20Research/#teaching-our-ai-part-2","title":"Teaching Our AI : Part 2","text":"<p>Let the Neural network.. drumroll.. learn!</p> <pre><code>learn.fine_tune(1, wd=0.5)\nlearn.export(\"model.pkl\") # Save the model\n</code></pre> <p>Fine-tune? Huh? 1? Huh? Don\u2019t worry too much about that right now. It is called Transfer Learning. And we are training the model for 1 epoch. (One run over all the data.). More epochs generally lead to better results.</p> <p>AND WE ARE DONE!! We have a working model already. This can recognize fruits. Congratulations!!!!</p> <p>(Too many words and terms. How will I ever learn them all? I should just give up now. Breathe. One thing at a time. Go through this. Come back. Go through bits you do not understand. More intermediate articles will follow. But in the meanwhile, the links are excellent resources on getting started.)</p>"},{"location":"articles/Easier%20Deep%20Learning%20Research/#how-well-did-we-do","title":"How Well Did We Do?","text":"<pre><code>from fastai.interpret import *\nfrom fastai.vision.widgets import *\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_top_losses(5, nrows=1)\n</code></pre> <p>We can use this to see what our model gets confused about. This will change as you train it more.</p> <pre><code>interp.most_confused()\n</code></pre>"},{"location":"articles/Easier%20Deep%20Learning%20Research/#using-the-model","title":"Using The Model","text":"<p>Okay, the last little bit here. We now need to use this trained model on images the network has never seen. (They were in a different folder. Also called the Validation set.)</p> <p>Fine, It\u2019s an exam. Best of luck little model. I hope you do well.</p> <pre><code>predictions_path = \"../input/fruits/fruits-360_dataset/fruits-360/Test\"\n\ndef predict_batch(self, item, rm_type_tfms=None, with_input=False): # this bit is slightly complicated. ignore it for now\n    dl = self.dls.test_dl(item, rm_type_tfms=rm_type_tfms, num_workers=15)\n    ret = self.get_preds(dl=dl, with_input=False, with_decoded=True)\n    return ret\nimport random\npredictions_path = Path(predictions_path)\nLearner.predict_batch = predict_batch\n\n# This is important\nlearn = load_learner(\"model.pkl\")\n\ntst_files = get_image_files(predictions_path) #same as before\ntst_files = tst_files.shuffle() \n</code></pre> <p>Now running the predictions.</p> <pre><code>preds = learn.predict_batch(tst_files)\nclasses = learn.dls.vocab # the original categories\npreds_mapped = list(map(lambda x: classes[int(x)], preds[2])) #just saving them out\n</code></pre> <p>So did it work? tst_files are the images we gave it. Preds is the output. Would you look at that?? It got most of them right :)</p>"},{"location":"articles/Easier%20Deep%20Learning%20Research/#fin","title":"Fin","text":"<p>What\u2019s next? More articles. In the meanwhile, you can look at this little link with resources I have collected over the years.</p> <p>You have a long way to go. But I do hope this was a good start. I know you didn\u2019t read the whole thing. Maybe you didn\u2019t make it till here either. I get that. I also did that when I was starting.</p> <p>This article is in the hopes that it will help someone out. Maybe have the help that I did not. I do not know who it will reach. But to whoever it does, best of luck :)</p> <p>You can contact me on LinkedIn, drop me an Email</p>"},{"location":"articles/GradCAM%20Notes/","title":"Grad-CAM Notes","text":""},{"location":"articles/GradCAM%20Notes/#grad-cam","title":"GRAD-Cam","text":"<p>Note: This is a slightly advanced article. If you are not comfortable with training neural networks, this is probably not for you yet. Start here instead.</p>"},{"location":"articles/GradCAM%20Notes/#intro","title":"Intro","text":"<p>So you want to train a Neural Network to classify images. Woah. That\u2019s awesome! How well did it do? Did you get a good score? Oh? You want to do better? I hear you.</p> <p>What if you could see what the network sees to make the choice? That would help understand how to make it perform better right? Read on!</p> <p>A few years ago, a paper titled \u201cGrad-CAM:</p> <p>Visual Explanations from Deep Networks via Gradient-based Localization\u201d by Selvaraju et al. talked about how we could visually see the activation maps of a trained CNN by looking at the gradients in the final layer. This post will show you how to use that for your own needs.</p> <p>Note: We will be using PyTorch and the fast.ai library. But the concepts stay the same, so you should be able to use it in any other library.</p>"},{"location":"articles/GradCAM%20Notes/#the-objective-and-data","title":"The Objective and Data","text":"<p>Before we can go to the code, what exactly are we trying to achieve? In short, we want to first train a network such as the \u201cresnet34\u201d architecture on any kind of data. In today\u2019s example, I will be using the Fish Dataset. This dataset has 9 types of sea creatures.</p> <p>You can of course, use any other image based classification data that you want.</p> <p>Knowing how the Dataset is structured is essential, so let\u2019s see that.</p> <p>Once the training is completed, we want to be able to give the trained network any image for inference and then have it spit out a visual heat map of exactly what it saw in the image. The brighter the color, the more the network focused on that particular spot to make it\u2019s decision.</p> <p>Have a look at the below example.</p> <p>In this image, we can see a yellow border around the fish. (Apologies if it is a disturbing image. Feel free to use any other dataset.) This shows that the network has identified the boundary of the fish and probably tried to use that information to decide what kind of fish it is.</p> <p>Examining these, we can see if the network is looking at the wrong thing, and find ways to show it what we want it to see.</p>"},{"location":"articles/GradCAM%20Notes/#code","title":"Code","text":""},{"location":"articles/GradCAM%20Notes/#training","title":"Training","text":"<p>Okay, let\u2019s not waste any more time and delve right into the code. If you are not familiar with fastai, you can look at this tiny blog for reference. The complete code can be found here on github.</p> <p>First, we import fastai as the deep learning library, matplotlib for plots, and IPython.display.Image to display the image inline.</p> <p>https://gist.github.com/SubhadityaMukherjee/grad_imports.py</p> <p>Training a classifier in fastai is just a few lines. We first decide where the dataset is.</p> <p>Then we create a DataBlock. (Think of it as a constructor for a dataloader). This DataBlock is given the following information</p> <ul> <li>Type of task : Image -&gt; Label</li> <li>What to do : get the images from the directory</li> <li>How to label the images : use the folder where the images are. For example, if the file name is \u201c/media/hdd/Datasets/Fish_Dataset/Fish_Dataset/Shrimp/Shrimp/00001.png\u201d , then it\u2019s parent is \u201cShrimp\u201d.</li> <li>How to split the data: Randomly with an 80/20 train/test split.</li> <li>Transforms : Basic vision transforms, Crop and resize to 224x224 px.</li> </ul> <p>Once we have that, we can pass it to the main trainer class - the \u201cvision_learner\u201d. To it, we pass in the network we want to use (You can use any other as well), and the metrics we care about.</p> <p>The to_fp16() enables Mixed Precision Training that would further increase the training speed and decrease the memory consumption.</p> <p>Awesome! Now let\u2019s train the network. We are using a pre-trained resnet34 and performing transfer learning, so we use fine_tune. If you want to train from scratch, you can use \u201clearn.fit(1)\u201d instead. I trained it for a single epoch as a demo.</p> <p>https://gist.github.com/SubhadityaMukherjee/grad_train.py</p>"},{"location":"articles/GradCAM%20Notes/#hooks","title":"Hooks","text":"<p>I mentioned previously that we would be looking at the gradients of the trained network. But how do we access them?</p> <p>In PyTorch, we can modify the components of the training loop using the concept of \u201cHooks\u201d. As the name suggests, it involves inserting a hook in the training loop and execute arbitrary code. Using that, we need to save the gradients during training.</p> <p>PyTorch has functions for the same called \u201cregister_forward_hook\u201d for the forward pass, and \u201cregister_backward_hook\u201d for the backward pass. We can take this information and write the following classes as a wrapper around our training loop.</p>"},{"location":"articles/GradCAM%20Notes/#plotting-activations","title":"Plotting Activations","text":"<p>Now for the intense bit. Let\u2019s pick a random image from the dataset. The original image is slightly disturbing so I blurred it a bit.</p> <p>Okay, we need to now pass the image through the model. The FastAI syntax for this kind of patching is slightly complicated. But let us walk through it.</p> <p>We first create the test data loader with the new image that we picked. We can then convert that into a Tensor. Once we pass the image to the network, it performs a full forward and backward pass on that image through every layer. Now for every layer in the model, we can get the computed gradients that we stored away using the Hook class.</p> <p>Selvaraju et al. defined the activation map as the weighted combination of the forward activation maps. Which is what we perform.</p> <p>The rest of the code is just matplotlib convenience functions to plot the nice grid heatmap that you see.</p> <p>The only weird line of code you might see is \u201ccam_map.detach().cpu()\u201d. This is done because we cannot plot a Tensor on the GPU, so we first detach it from the computational graph, then bring it back to the CPU to plot it.</p> <p>Well, yay! You made it. Try it with your own network and/or data. Just a word of warning, the 0 in line \u201cwith HookBwd(learn.model[0][layer]) as hookg\u201d, needs to be modified based on the network architecture. If you get errors, try with a 1 and so forth.</p> <p>https://gist.github.com/SubhadityaMukherjee/grad_plot.py</p>"},{"location":"articles/GradCAM%20Notes/#whats-next","title":"What\u2019s Next?","text":"<p>Firstly, good job on making it so far! Pat yourself on the back or go grab something nice to eat.</p> <p>Then look at what the network sees. Does it make sense? Is the model looking at something weird? Train for a few more epochs, rinse and repeat.</p> <p>Try it for different images. You might find examples that make no sense. Sometimes it might be because of random augmentation, other times it might be because of model bias or the data itself being not okay. You will find ways to improve on it eventually. If you can, look for examples that the model gets wrong, and apply Grad-CAM on those.</p>"},{"location":"articles/GradCAM%20Notes/#fin","title":"Fin","text":"<p>Woah. That was long.</p> <p>What\u2019s next? More articles. In the meanwhile, you can look at this little link with resources I have collected over the years.</p> <p>You have a long way to go. But I do hope this was a good start. I know you didn\u2019t read the whole thing. Maybe you didn\u2019t make it till here either. I get that. I also did that when I was starting. This article is in the hopes that it will help someone out. Maybe have the help that I did not. I do not know who it will reach. But to whoever it does, best of luck :)</p> <p>You can contact me on LinkedIn, drop me an Email</p>"},{"location":"articles/Latex%20Tutorial/","title":"Latex Tutorial","text":"<p>At some point in your study or career, you will be required to write a report or an article, perhaps even a research paper. You start writing your content down in Word or your favourite text editor. After a little while, some issues crop up. Maybe you don\u2019t like how it looks, or you want to change the format to a two column layout. Maybe you moved an image and the entire document went crazy. Maybe you wrote a long article, but have to change your citations manually. Or heaven forbid, you need to create a table of contents.</p> <p>Ugh, you think. I don\u2019t have time for this. Just let me work on the content. If only there was a better way.</p> <p>Sounds familiar? Read on.</p>"},{"location":"articles/Latex%20Tutorial/#latex-vs-word","title":"LaTEX Vs Word","text":"<p>At the heart of everything, what you are struggling with is the issue of change. If you had a small article, with barely any changes or styling, Word is great. But for anything more than 5 pages? Ouch.</p> <p>LaTeX is a complete document preparation system, with the added advantage of a different \u201clanguage\u201d that makes your life a lot easier. It sounds and looks very strange at first. But once you get the hang of it, it will change the way you write content. Think of it as a more advanced template that is infinitely customisable. All you do is set things up once. Then you can focus on your content.</p> <p>Oh no! Something changed? That\u2019s okay. Add a few lines to your document and you are good to go.</p> <p>I do all my reports, articles, homework, projects everything using it and it has saved me days of effort.</p> <p>It looks professional right off the bat.</p> <p>(Note: You do NOT need to be a programmer to use this. Just go with it. And google to your heart\u2019s content.)</p>"},{"location":"articles/Latex%20Tutorial/#intro-to-overleaf","title":"Intro to Overleaf","text":"<p>So how do you use this magic mushroom? Well, luckily we have Overleaf. This website allows you to write anything you want, provides a lot of templates, live collaboration, and so much more. Mostly for free as well.</p> <p>Just open the site and make an account. If your institution provides premium access, use your official email ID to register. (You don\u2019t really need it for most use cases.)</p> <p>(Note: This is not a sponsored post. I just use it everyday and want to make sure it helps more people.)</p> <p>Everything I show can be viewed at this link.</p>"},{"location":"articles/Latex%20Tutorial/#choosing-a-template","title":"Choosing A Template","text":"<p>Okay great, now it\u2019s time to start working on a project. I will demonstrate with a small article : \u201cComputer Vision in Pytorch - A Primer\u201d.</p> <ul> <li>Click \u201cNew project\u201d</li> <li>Based on what kind of work you are writing this article for, pick a section. For this article, I picked Academic Journal.</li> <li>Look around and find the look you are going for. Click on it, and then click \u201cUse as template\u201d. If you are a University, look for an official template, most of them have one.</li> <li>I picked this - IEEE template Perfect! You are halfway there.</li> </ul>"},{"location":"articles/Latex%20Tutorial/#initial-setup","title":"Initial Setup","text":"<p>Before we start, let\u2019s just get used to the interface.</p> <p>On the left you can see a sidebar with all your files and folders.</p> <p>To it\u2019s right, there is the main editor window. If you look closely, the text seems a little strange? Don\u2019t worry, more on that later.</p> <p>After that you will see a preview of your article. Every time you hit save or \u201crecompile\u201d, this preview will update. You can latex export this as a PDF.</p> <ul> <li>In the sidebar, I like to make a folder for images by click the little icon that looks like a folder.</li> <li>Now, copy paste the first few lines until the line before \u201cbegin document\u201d. Think of these as extra functionality. For example: highlighting your links, structuring your document etc.</li> <li>Make a new document titled \u201cmain.tex\u201d using the file icon.</li> <li>Paste the contents there.</li> </ul> <p>Great job! Now we can get to writing everything.</p>"},{"location":"articles/Latex%20Tutorial/#understanding-the-components","title":"Understanding The Components","text":"<p>Now the following might seem slightly too complicated. And you will probably feel like it\u2019s not worth the effort. But, trust the process okay?</p> <p>So far, you should have something like this.</p> <p>```tex,latex_report_modules,latex_report_modules \\documentclass[conference]{IEEEtran} \\IEEEoverridecommandlockouts % The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out. \\usepackage{cite} \\usepackage{amsmath,amssymb,amsfonts} \\usepackage{algorithmic} \\usepackage{graphicx} \\usepackage{textcomp} \\usepackage{xcolor}</p> <pre><code>\nNow we define a basic template.\n\n```tex,latex_report_template,latex_report_template\n\\begin{document}\n\n\\title{Computer Vision in Pytorch - A Primer\\\\\n\\thanks{I thank Overleaf for this template}\n}\n\n\\author{\\IEEEauthorblockN{1\\textsuperscript{st} Subhaditya Mukherjee}\n\\IEEEauthorblockA{\\textit{Faculty of Science and Engineering } \\\\\n\\textit{University Of Groningen}\\\\\nGroningen, Netherlands \\\\\nmsubhaditya@gmail.com}\n}\n\\maketitle\n\n\\begin{abstract}\n\\end{abstract}\n\n\\begin{IEEEkeywords}\n\n\\end{IEEEkeywords}\n\n\\section*{}\n\\subsection*{}\n\n\\begin{thebibliography}{00}\n\n\\end{thebibliography}\n\n\\end{document}\n\n</code></pre> <p>You will only need to do this once to get a feel for what\u2019s happening. It\u2019s scary I am sure. But hold on. Keep trying. You\u2019ve got this!</p> <p>Look at the following code snippets twice. Do you see a pattern?</p>"},{"location":"articles/Latex%20Tutorial/#title-abstract-keywords","title":"Title, Abstract, Keywords","text":"<p>We now need a title. So we put it in this line. If you notice the little {}, that is required by LaTeX to know where to start and end something.</p> <p>```tex,latex_report_title, latex_report_parts \\title{Computer Vision in Pytorch - A Primer}</p> <pre><code>\nAre you required to write an abstract or a summary of sorts?\n\n```tex,latex_report_abstract, latex_report_parts\n\\begin{abstract}\nThis paper is a short introduction to Pytorch, a deep learning framework. Special focus will be given to applications of Computer Vision. This is a demo paper, and has no particular significance.\n\\end{abstract}\n</code></pre> <p>Sometimes, a template will have keywords. You can just enter the ones you think are relevant.</p>"},{"location":"articles/Latex%20Tutorial/#authors","title":"Authors","text":"<p>Every template you copy, will have a section for the author. Just fill it in the way you want.</p>"},{"location":"articles/Latex%20Tutorial/#sections-and-subsections","title":"Sections and Subsections","text":"<p>Now for the actual content. Here we use the commands \u201csection\u201d, \u201csubsection\u201d, \u201csubsubsection\u201d. You do not need to bother about giving them numbers. LaTeX will take care of it.</p> <p>Something like this is a good start.</p> <p>```tex,latex_report_section, latex_report_parts \\section{What is Computer Vision?} A study of the techniques used to extract meaning from image or video related data. The applications are endless, starting from face recognition, to self driving cars. \\subsection{Computer Vision in the field of Deep Learning} Deep learning has revolutionised the field of Computer Vision by giving it superpowers. The ability to learn from billions of images come as a huge leap forward in the field. \\subsubsection*{A note} Classicial CV is still very relevant today.</p> <pre><code>\n### Figures\n\nDoesn\u2019t seem too hard does it? Let\u2019s add some figures. Woah. What is happening here??\n\nWe are defining how we want our figure to look! We tell the system where the image is, how want it to look - centered, fit inside the line, have a caption, and a label. Just change the text to what is relevant to you.\n\nThink about it once. Makes sense right?\n\n```tex,latex_report_fgure, latex_report_parts\n\\begin{figure}[h]\n\\includegraphics[width=\\linewidth]{figures/2560px-PyTorch_logo_black.svg.png}\n\\centering\n\\caption{Representation in the Simulation}\n\\label{fig:colors}\n\\end{figure}\n</code></pre> <p>If you want to change the size, replace \u201c\\linewidth\u201d with something like \u201c0.2\\linewidth\u201d which makes the figure 0.2 times the size of the line.</p>"},{"location":"articles/Latex%20Tutorial/#formatting-text","title":"Formatting Text","text":"<p>Okay how about formatting such as bold italics and the like? Super simple. Look at these lines.</p> <p>```tex,latex_report_form, latex_report_parts \\section{Formatting} This \\textbf{will be bold}, then \\textit{italic}, and also \\textcolor{red}{red}. </p> <p>To add a line break, simply add \\ this will be a new line</p> <pre><code>\nSee? That was not too bad was it? Now we have colours as well!\n\n### Code\n\nIf you are a programmer, or need to have any bits of code, this is how you can do it.\n\nTo the section where the packages were imported, add the following line.\n\n```tex,latex_report_code,latex_report_code\n\\usepackage{listings}\n</code></pre> <p>Now wherever you want to add code, just use it like this. Change the language to what you need of course. Viola! Syntax highlighting!</p> <p>```tex,latex_report_code_2, latex_report_parts \\begin{lstlisting}[language=Python] import numpy as np print(np.random.rand(10)) \\end{lstlisting}</p> <pre><code>\n### Equations\n\nHave you ever had to add equations in Word? I feel sorry for you. LaTeX lets you do it in a breeze.\n\n```tex,latex_report_equations, latex_report_parts\n\\section{Equations}\nWe can have three types of these - An inline equation : $2x+3 = 10$, or a proper block , $$2 \\sin(x)+10 = 100$$ or a long form one such as this.\n\\begin{equation}\nE[g^{2}]_{t}= 0.9E[g^{2}]_{t-1}+ 0.1g^{2}_{t}\\\\\n\\theta_{t+1}= \\theta_{t}- \\frac{\\eta}{\\sqrt{E[g^{2}])_{t}+\\epsilon}}g_{t}\n\\end{equation}\n</code></pre> <p>It would be impossible to explain the intricacies of using these, but as you can see, it almost feels like writing the equation down as it is. And it looks gorgeous as well.</p> <p>For more information, refer to this nice page.</p>"},{"location":"articles/Latex%20Tutorial/#cover-page","title":"Cover Page","text":"<p>Hopefully you picked a template that already had one. But if you did not, add this before \u201c\\begin{document}\u201d. Replace the text however you feel like.</p> <p>```tex,latex_report_cover, latex_report_parts \\begin{titlepage}    \\begin{center}        \\vspace*{1cm}</p> <pre><code>   \\textbf{Thesis Title}\n\n   \\vspace{0.5cm}\n    Thesis Subtitle\n\n   \\vspace{1.5cm}\n\n   \\textbf{Author Name}\n\n   \\vfill\n\n   A thesis presented for the degree of\\\\\n   Doctor of Philosophy\n\n   \\vspace{0.8cm}\n\n   \\includegraphics[width=0.4\\textwidth]{university}\n\n   Department Name\\\\\n   University Name\\\\\n   Country\\\\\n   Date\n</code></pre> <p>\\end{center} \\end{titlepage}</p> <pre><code>\n### Table Of Contents\n\nAdding a TOC is even easier. It also updates automatically. Just add.\n\n```tex,latex_report_toc, latex_report_parts\n\\tableofcontents\n</code></pre>"},{"location":"articles/Latex%20Tutorial/#appendix","title":"Appendix","text":"<p>Want a list of images and/or tables you have used throughout your document? With page numbers.</p> <p>```tex,latex_report_appendix, latex_report_parts %\\newpage %add this if you want it to be on a separate page \\begin{appendix}   \\listoffigures   \\listoftables \\end{appendix}</p> <pre><code>\n### Tables\n\nTables are a bit complicated in LaTeX, but there is an easier way. Just open this website - [Table generator](https://www.tablesgenerator.com).\n\nThis is a very user friendly UI, so just add whatever you want. And click generate.\n\nCopy paste that into your Overleaf editor. Done!\n\nNotice the auto numbering? Cool right?\n\n### Citations\n\nCitations are one of the most powerful features of working in LaTeX. The best part? Only the ones you cited will show up in your bibliography! You do not need to worry if you missed any, or forgot to remove any. All you need to do is paste all your citations in a bib file.\n\n- Create a file called references.bib\n- Paste all your references in \u201cBibTex\u201d format in the file. Google scholar or any reference manager you use will have that option.\n- Cite them like this\n```tex,latex_report_cite, latex_report_parts\n\\section{Citation Example}\nTwo interesting libraries are Kornia \\cite{riba2020kornia} and fast.ai \\cite{howard2020fastai}. If you want it inline then : \\citep{howard2020fastai}.\n</code></pre>"},{"location":"articles/Latex%20Tutorial/#bibliography","title":"Bibliography","text":"<p>Once you have added all your citations, you would need to have a Bibliography. The file \u201creferences.bib\u201d you created? Remember the name. Right before \u201c\\end{document}\u201d you can add these lines. (Change the first one to what you need. Your template mostly will define it already.)</p> <p>```tex,latex_report_bib, latex_report_parts \\bibliographystyle{IEEEtran} \\bibliography{references}</p> <p>\\end{document} ```</p> <p>Looking good!</p>"},{"location":"articles/Latex%20Tutorial/#comments","title":"Comments","text":"<p>Want to make comments that you or a fellow author can refer to in the future? Just select any bit of text in the editor, you will get a pop up for adding a comment.</p>"},{"location":"articles/Latex%20Tutorial/#exporting","title":"Exporting","text":"<p>Congratulations! Looks like you made it to the end. Now how do you export your awesome document?</p> <p>See the little button that has a downward arrow? Click that. You can also click the \u201cMenu\u201d button and find more options there.</p>"},{"location":"articles/Latex%20Tutorial/#collaboration","title":"Collaboration","text":"<p>Want to work with colleagues/teammates? Just hit Share and send them the link.</p>"},{"location":"articles/Latex%20Tutorial/#general-principles-and-how-to-get-help","title":"General Principles and How To Get Help","text":"<p>That\u2019s about it for the basics. Feel free to come back to this document for your reference. You will face problems. Just remember the following things.</p> <ul> <li>Google is your best friend.</li> <li>This website is a good place to start looking for what\u2019s possible.</li> <li>It will take a few tries. But it is definitely worth it.</li> <li>If you can say what you want in English, you mostly know the commands already.</li> <li>There are a lot of commands. You do NOT need to remember them. You will passively pick them up. Overleaf also has autocomplete which helps.</li> <li>Give it some time, it will change your life.</li> </ul>"},{"location":"articles/Latex%20Tutorial/#fin","title":"Fin","text":"<p>This article is in the hopes that it will help someone out. Maybe have the help that I did not. I do not know who it will reach. But to whoever it does, best of luck :)</p> <p>You can contact me on LinkedIn, drop me an Email</p> <p>Like my work? Buy me a coffee :)</p>"},{"location":"articles/Notes%20on%20Machine%20System%20Design/","title":"Notes on Machine Learning Interviews","text":""},{"location":"articles/Notes%20on%20Machine%20System%20Design/#notes-on-machine-learning-system-design-for-interviews-by-chip-huyen-ai","title":"Notes on \u201cMachine Learning System Design\u201d (for Interviews) by Chip Huyen #ai","text":"<p>These are my notes on this article by Chip Huyen.</p> <p>The article that these notes are based on, talks about some factors that are involved in designing machine learning systems, and what to watch out for in interviews on the same. I wanted to write a summary of what I read and add my take on it for future reference.</p> <p>As a note, I found Chip\u2019s book Designing Machine Learning Systems an excellent resource for anyone starting or willing to improve their skills in this field. I would recommend a read. (This is not sponsored by any means.)</p>"},{"location":"articles/Notes%20on%20Machine%20System%20Design/#interviews","title":"Interviews","text":"<ul> <li>The major issue with Machine Learning interviews seems to be the lack of a standard criterion by which to judge a candidate. This makes a lot of sense considering how varied the requirements of each project are.</li> <li>Interviewers generally look for what they are familiar with and often this means that an ideal candidate would be someone who thinks along similar lines. This is especially interesting in ML because there are an infinite number of ways to approach a problem.</li> <li>It would be helpful for the candidate to understand what kind of answers the company would be looking for based on their previous work.</li> </ul>"},{"location":"articles/Notes%20on%20Machine%20System%20Design/#compute-requirements","title":"Compute Requirements","text":"<ul> <li>Contrary to the vast amount of research in the ML space on improving models and focusing on metrics, in production, users might barely notice a tiny improvement in accuracy. In return, this would further increase the complexity of the system and thus its latency.</li> <li>It is important to first understand exactly what you wish to achieve and what you need to optimize for.</li> <li>You cannot do everything.</li> </ul>"},{"location":"articles/Notes%20on%20Machine%20System%20Design/#setting-up-the-project","title":"Setting Up The Project","text":"<ul> <li>At the end of the day, an ML system also requires skills from Software Engineering. A complete system is not just a model but a lot of moving parts. Thus there are quite a few tradeoffs to consider.</li> <li>An initial thought process<ul> <li>Pick the top 2 goals that your solution needs</li> <li>What does the user interaction look like?<ul> <li>Do you care about personalized results?</li> <li>Does latency matter?</li> <li>How much of your system relies on ML?</li> <li>What kind of devices are you looking at for deployment?</li> <li>Does privacy matter?</li> </ul> </li> <li>Data<ul> <li>Do you have the data?</li> <li>Is it usable? Is it clean?</li> <li>Where is it stored? How much of it is stored?</li> </ul> </li> <li>What metrics are useful in this context? (Domain expertise would be very helpful here)</li> <li>What resources do you have/can acquire? People, time, users, etc</li> </ul> </li> </ul>"},{"location":"articles/Notes%20on%20Machine%20System%20Design/#baselines","title":"Baselines","text":"<ul> <li>These systems can get complex pretty fast. So start with the simplest possible algorithm. You might not even need a very complex Deep learning system. And it might not be feasible at the start.</li> <li>To evaluate if it is worth shifting to a more complex implementation, looking at baselines is essential.<ul> <li>Random Baseline: How well would a random guess do?</li> <li>How well does a human do on this task? (If that is feasible)</li> <li>What minimum results do you need for a functioning solution?</li> </ul> </li> </ul>"},{"location":"articles/Notes%20on%20Machine%20System%20Design/#debugging-models","title":"Debugging Models","text":"<p>This is one endless minefield.</p> <p>In my experience, this list by Andrej Karpathy is a pretty comprehensive guide. I do not have anything to add to it so I will skip this section.</p> <p>As a recommendation though, I would suggest considering the use of a framework such as fast.ai. These are built with a lot of issues in mind and take care of a lot of them.</p>"},{"location":"articles/Notes%20on%20Machine%20System%20Design/#model-scaling","title":"Model Scaling","text":"<ul> <li>Most large-scale systems these days use Parallel and Distributed computing. Multiple GPUs/TPUs etc.</li> <li>If your data does not fit into memory, there are many ways of getting it - Gradient checkpointing, Mixed Precision, and Parallelism are some of them. (Future blogs will go a lot more in-depth. Putting it here will make it pretty huge.)</li> </ul>"},{"location":"articles/Notes%20on%20Machine%20System%20Design/#inference","title":"Inference","text":"<ul> <li>There are a lot of steps and tradeoffs involved in inference.<ul> <li>Where will you run your model? How long does it take? Can you say with certainty why a particular result is obtained? Can you retrain it?</li> <li>Before deployment, it is advisable to turn off all the modifications one by one and test how well the system performs without them.</li> <li>Check for biases in the results. (For example: Does your model prefer colored people?)</li> <li>Does your data remain constant? Or keep changing?</li> </ul> </li> </ul>"},{"location":"articles/Notes%20on%20Machine%20System%20Design/#fin","title":"Fin","text":"<p>The list of things to consider to make an efficient and thought-out model is endless. I do not think it is possible to take everything into account in the real world, but the more you think these through and follow these guidelines, the better and more stable your implementation will be.</p> <p>Of course, this is more of a short primer rather than a comprehensive guide. Future articles will cover more details.</p> <p>This article is in the hopes that it will help someone out. Maybe have the help that I did not. I do not know who it will reach. But to whoever it does, best of luck :)</p> <p>You can contact me on LinkedIn, drop me an Email</p>"},{"location":"articles/Notes%20on%20a%20Nervous%20Planet-%20take%20control/","title":"Notes on a Nervous Planet- Take Control","text":""},{"location":"articles/Notes%20on%20a%20Nervous%20Planet-%20take%20control/#chapterwise-summary-30-ways-to-take-back-control-notes-on-a-nervous-planet-by-matt-haig","title":"Chapterwise Summary + 30+ Ways to Take Back Control : Notes on a Nervous Planet by Matt Haig","text":"<p>Notes on a nervous planet is a book that brings a lot of important points about the collective overwhelm that we as a \u201cmodern\u201d society face. Social media, lack of sleep, odd working hours, loneliness. Neither of these were aspects that we were really prepared to deal with. But before treating a disease, we must know what the symptoms are and why they showed up in the first place. (At the end of the article, there is a long is list of what you can do to avoid overwhelm.)</p> <p>This is a summary and my thoughts (chapter wise, mostly) on the book. I try to cover what stuck out to me the most and in turn, hope that it helps someone out. If you like the summary, you will like the book more.</p> <p>Go support the author here.</p> <p>(Note: This is not sponsored by the author and is a personal opinion that just reflects my own views. Since this is an interpretation, the author might have thought something different in a few places.)</p>"},{"location":"articles/Notes%20on%20a%20Nervous%20Planet-%20take%20control/#a-stressed-out-mind-in-a-stressed-out-world","title":"A Stressed-out Mind in a Stressed-out World","text":"<p>As society progresses, we face an overwhelming increase in everything from groceries to people to jobs and somewhere down the line it takes a huge toll on our mental health. Humans weren\u2019t meant to deal with this constant barrage of information.</p> <p>If you stop and look around you, you can almost feel this underlying quit panic. The rush of everyday life, the endless calling out for attention by companies, all of these things wear you down little by little.</p> <p>Before, if you wanted to find out about what was happening in the world, you would look at the news and be presented with a limited selection of it. These days however, the more attention grabbing headlines are what you see. There is always something horrible happening and this triggers the part of our brains that make us feel unsafe.</p> <p>You might think that social media is different, but it still does the same thing doesn\u2019t it? It keeps triggering the fear and panic that makes you feel FOMO (fear of missing out), or that someone else\u2019s life is better. After even a few minutes, you might have noticed the guilt and sadness you feel but you can\u2019t stop.</p> <p>All of these, and the added loneliness it brings, makes us feel like we are in a 24x7 catastrophe.</p>"},{"location":"articles/Notes%20on%20a%20Nervous%20Planet-%20take%20control/#the-big-picture","title":"The Big Picture","text":"<p>Some people blame this on time and modernisation. Which is true, but at the end of the day the main reason for this insanity is just\u2026 consumerism and capitalism. Companies need to grow, and get the \u201cbig bucks\u201d and what better way to do that but constantly keep expanding.</p> <p>But every expansion has a price, and in this case it\u2019s the sanity of society and the slow extortion of our planet. In time, this mentality has seeped into every aspect of society.</p> <p>Our politicians glorify the \u201chard working families\u201d and our companies glorify endless work hours. To what end?</p> <p>These goalposts are endless. And the constant comparison with everyone else that social media brings just feeds the fire.</p> <p>It\u2019s come to the point that nobody is really ever satisfied. You spend years to work on a degree, and social media convinces you that traveling the world would have been a better idea. You get a great job, a new car, move to a bigger house etc, and your device shows you pictures of even more.</p> <p>The fuel is dissatisfaction and the fire is the need to buy. The need to keep creating and producing.</p> <p>Doesn\u2019t this remind you of a factory line?</p>"},{"location":"articles/Notes%20on%20a%20Nervous%20Planet-%20take%20control/#a-feeling-is-not-your-face","title":"A Feeling is Not Your Face","text":"<p>Social media has added a lot to the constant insecurity that we face, and beauty standards have reached an almost ridiculous peak. It\u2019s not just a couple of magazines anymore. We are constantly surrounded by unrealistic body standards. Regardless of your gender, you are shown endless options and made to subconsciously identify with either the \u201cbarbie\u201d, \u201csuperhuman\u201d or some other stereotype based on what you see online. Neither of these are achievable.</p> <p>If you go to a beach, you might think that the people around you are so concerned with how you look with your shirt off. But in all honesty, most people are thinking the same way. Most people are too concerned with their own appearance to care about yours.</p> <p>And then the beach itself does not care. It\u2019s nature after all. Regardless of what your body looks like, you\u2019re a part of it. It bears no judgement.</p> <p>Accepting yourself the way you are, and realising that what you see online in unreasonable, goes a pretty long way.</p>"},{"location":"articles/Notes%20on%20a%20Nervous%20Planet-%20take%20control/#notes-on-time","title":"Notes on Time","text":"<p>Cavemen didn\u2019t particularly care if they were 5 minutes late to a hunting meeting. Time was not quantified to this precision. At some point the sun and the moon guided time. There was day and night, maybe afternoon. Some times were good for the hunt, the others were good for rest and community.</p> <p>As technology marches on, the concept of time becomes ever refined. Now not only are you always \u201clate\u201d, you know exactly how badly you failed within a nanosecond of accuracy. Hello guilt!</p> <p>Deadlines came to exist, and the constant need to check your watch or phone.</p> <p>Society made time an enemy. \u201cFinish everything before the timer runs out\u201d. Suddenly, you could never achieve enough in the time that you had. Because the more divisions we make, the more we are expected to do to fill them up.</p> <p>We stopped listening to our bodies and became a slave to a ticking clock.</p> <p>It\u2019s time we started going back to a simpler time.</p>"},{"location":"articles/Notes%20on%20a%20Nervous%20Planet-%20take%20control/#life-overload","title":"Life Overload","text":"<p>It\u2019s not just your sink that gets filled with dishes, your poor brain is struggling to keep up with all this work and information.</p> <p>There is an unnecessary excess in everything and thus your brain needs to constantly make decisions. In turn, everyone around you faces the same thing. If you look it from the outside, it\u2019s almost as if we are all feeding that collective frenzy.</p> <p>The world is heading for a collective breakdown, and in our heart of hearts, we know that we are not just a part of it, we are feeding the fire too.</p> <p>It\u2019s not just life, but an overload of it.</p>"},{"location":"articles/Notes%20on%20a%20Nervous%20Planet-%20take%20control/#internet-anxieties","title":"Internet Anxieties","text":"<p>Ah the internet. Humanities craziest information. A way to \u201cforever\u201d and almost instantly look up anything. Any think you wanted to know, feel, see or find, all up for grabs. The biggest fuel we have added to the frenzy. But the internet is not always a bad thing is it? So many people have learnt skills, built families, found similar people, started a business and found partners through it.</p> <p>But they have also been pushed opinions that they never knew of. They have been fed with a constant diet of hate, choice, games of power, games of \u201cratings\u201d and a desire for pretence.</p> <p>On the internet, nobody knows if you are a human or a dog. And it\u2019s so much easier to bully someone if you know you are a continent away and can\u2019t be held accountable.</p> <p>These exacerbate the collective overload. Because now not only do people have the power to play either the victim or the bully, they can see millions of the these stories playing out everyday.</p> <p>A slowly spreading poison doesn\u2019t kill very fast, but withers you away in time.</p> <p>The only way to go about it is empathy. You cannot avoid the internet but you can choose what you want to actively look for.</p> <p>Simplify, and leave the fighting to the real world.</p>"},{"location":"articles/Notes%20on%20a%20Nervous%20Planet-%20take%20control/#shock-of-the-news","title":"Shock of the News","text":"<p>Fear does sell though, and the news profits on that. Wars and death have always been common, but these days? You almost feel like the violence of the world has increased to a ridiculous point. You feel unsafe everywhere you go, 24x7.</p> <p>A few decades ago, you would get your news twice a day maybe. Now you get it every second, from every possible angle, all across the world. There\u2019s more than 7 billion people and the stories are endless. You don\u2019t hear about one robbery, you hear about them from 10 countries, 300 people and commented on by thousands more. Feeding the flame of anxiety again.</p> <p>Remember, fear sells.</p> <p>Try to limit your exposure to it. If something truly important happens, you will get to know.</p>"},{"location":"articles/Notes%20on%20a%20Nervous%20Planet-%20take%20control/#a-small-section-on-sleep-priorities","title":"A Small Section on Sleep Priorities","text":"<p>The part that really stuck out to me here was this \u201cSleep is the enemy of consumerism\u201d. Who\u2019s the biggest competition of Netflix? It\u2019s SLEEP. You can\u2019t watch a show if you\u2019re asleep. Neither can you buy the next \u201cgreatest phone\u201d.</p> <p>Likewise, you cannot produce these things. The endless factory \u201cneeds\u201d to grow. Working hours keep getting later and later, working on weekends is a normal thing, working 16 hours is a badge of pride, we all know these don\u2019t we?</p> <p>And the lesser we sleep, the more health issues we have. Catch my drift? (Hint: Healthcare is a product too you know?)</p> <p>Apart from healthcare, the constant panic and fear leads us to the other large businesses - Drugs and Weapons.</p> <p>Might as well get those good hours of sleep in right?</p> <p>Routines do help. And a little light movement, a warm shower, turning off the devices etc do help quite a bit.</p>"},{"location":"articles/Notes%20on%20a%20Nervous%20Planet-%20take%20control/#phone-fears","title":"Phone Fears","text":"<p>I think we all have heard a lot about how our devices are affecting our lives negatively. That being the case, the more important aspect of that is understanding how to tackle it. A selection is as follows.</p> <p>Notifications do suck. Turn them off except for the most important ones. Spend some time away from your devices, especially before sleep. Multitasking is an illusion.</p> <p>Social media breeds unnecessary uncertainty. You are always left \u201cwondering\u201d. What is my friend doing? Did my favourite artist upload a new comic? Are there any new TikTok\u2019s on the thing I\u2019m currently obsessing about? The answer of course, is yes.</p> <p>The problem is, we cannot have everything can we?</p> <p>Accepting that uncertainty is a part of life helps drastically.</p>"},{"location":"articles/Notes%20on%20a%20Nervous%20Planet-%20take%20control/#the-detective-of-despair","title":"The Detective of Despair","text":"<p>All the effects so far seem almost too general? The best way to see how something affects you is to look at what your body is feeling. Listen to what it says. Is it telling you to stop watching people making food and go eat dinner? Is it saying that hey, I\u2019m so tired, can you stop looking at people waking up at 6 am and taking cold showers?</p> <p>Listen.</p> <p>Breathe.</p>"},{"location":"articles/Notes%20on%20a%20Nervous%20Planet-%20take%20control/#the-thinking-body","title":"The Thinking Body","text":"<p>One rather interesting point the author makes is that of a \u201cunit\u201d to how much mental toll something takes : a \u201cpsychogram\u201d. I find it a very interesting notion.</p> <p>Some examples the author gave are (negative and lower is better):</p> <ul> <li>The sun appearing unexpectedly from behind a cloud : -57pg</li> <li>Dancing : -1350pg</li> <li>Arriving home after a terrible train journey : -398pg</li> <li>Watching the news : 222pg</li> <li>A worrying symptom you have googled : 672pg Trying to at least vaguely quantify how much we can deal with in a day before snapping can help us set better boundaries and listen to our body more. We all start off with a limited amount of energy, and some things drain more than the others. Of course, the actual quantification of it is not very useful. It\u2019s the idea that makes it interesting to me. A reminder that we have limited resources. Like a Health Bar in a game, ours keeps going down too. We just can\u2019t see it.</li> </ul>"},{"location":"articles/Notes%20on%20a%20Nervous%20Planet-%20take%20control/#the-end-of-reality","title":"The End of Reality","text":"<p>The more the virtual creeps in, the more choices are presented to us. Instead of going to your local store to buy a new phone, you look it up online first. Instead of being presented with one shampoo, you have 40 in a store, and 4000 online.</p> <p>All these choices can lead your brain to want to not compute and choose. So it turns part of itself off. It hides in \u201cDerealisation\u201d. Reality seems a bit wonky. The virtual world feels more like home, an uncertain, panic filled home, but home nonetheless.</p> <p>We must take steps to protect ourselves from this frenzy before it drowns us.</p>"},{"location":"articles/Notes%20on%20a%20Nervous%20Planet-%20take%20control/#wanting","title":"Wanting","text":"<p>This section ties in a lot the \u201cbeauty\u201d one that came before. It serves as a reminder to be kinder to yourself, consider that age and time are a natural function of life itself and realise that \u201ctoo much\u201d of anything is not always a good thing.</p> <p>More is not a solution. The solution is acceptance and being grateful for what you have.</p> <p>We can never stop wanting, but if we can draw the line between what we truly \u201cwant\u201d and what we are made to believe that we do by our consumerist economy, then we can get some control back.</p>"},{"location":"articles/Notes%20on%20a%20Nervous%20Planet-%20take%20control/#two-lists-about-work","title":"Two Lists About Work","text":"<p>As a society, we like work. We base our worth on it sometimes, which is not great though. The cultural obsession we have with \u201chard working families\u201d is not always a good thing though. Statistics show that the \u201chardest workers\u201d aren\u2019t always the happiest or the wealthiest. Does that say something?</p> <p>Choosing to have less stuff to do vs choosing to constantly be \u201cbusy\u201d is a lot healthier.</p> <p>Never forget that deadlines are a product of consumerism. Imperfection is a part of humanity itself, it\u2019s a feature, not a bug.</p> <p>One quote that I really liked in the book :</p> <p>\u2018One of the symptoms of an approaching nervous breakdown is</p> <p>the belief that one\u2019s work is terribly important.\u2019 - Bertrand Russell</p>"},{"location":"articles/Notes%20on%20a%20Nervous%20Planet-%20take%20control/#shaping-the-future","title":"Shaping the Future","text":"<p>Of course, change is hard. Most of these changes are easier said than done. And many of them are not even fully achievable by you yourself. Remembering that the space around you matters quite a bit, is also very important.</p> <p>We are a part of nature, and the vast openness and freedom that it brings is in our blood. Having more open space, parks and clean workspaces makes our days a little better.</p> <p>When we can\u2019t escape into the woods, sometimes a good fiction book can serve that purpose.</p> <p>Progress is not something that happens overnight, it takes effort and time. But the peace is worth it.</p>"},{"location":"articles/Notes%20on%20a%20Nervous%20Planet-%20take%20control/#the-song-of-you","title":"The Song of You","text":"<p>Another important reminder here, the sky is always there for you. Wherever you may be, whoever you may be.</p> <p>We are not separate from nature, we are nature.</p> <p>Your inside world is important too. Tend to the garden that is you, sing the songs that are your very being.</p>"},{"location":"articles/Notes%20on%20a%20Nervous%20Planet-%20take%20control/#everything-you-are-is-enough","title":"Everything You Are is Enough","text":"<p>The key takeaway really, is acceptance. Accepting that you cannot be everything, do everything, go everywhere, have everything. And even if you did, it would not really fill the void. What would is accepting the present moment, prioritising living over just existing, taking back the power and freedom that society so desperately wants to take away from us. Accepting that our failures are a part of life. Accepting that after a point, putting more work into something only has diminishing returns.</p> <p>Accepting that, people aren\u2019t forever. If you want to show your love, what are you waiting for?</p>"},{"location":"articles/Notes%20on%20a%20Nervous%20Planet-%20take%20control/#a-list","title":"A List","text":"<p>That ends my summary and thoughts about the book.</p> <p>The author gives quite a few helpful tips dispersed among the chapters, and I thought I would list my favourites in one place. (Some are verbatim and I take no credit for them.)</p> <ol> <li>Happiness is felt heading out, not in</li> <li>Happiness is about what we already have</li> <li>Maybe the point of life is to embrace life\u2019s beautiful uncertainty.</li> <li>Products that make us ashamed of our age, don\u2019t actually help us not age</li> <li>The beach does not care what you look like</li> <li>Young people are more worried about age than young</li> <li>Acceptance &gt; Denial in the long run</li> <li>Feeling something doesn\u2019t mean that it\u2019s the absolute truth. It\u2019s just a feeling</li> <li>The empty joy of likes is\u2026 empty</li> <li>Posting about experiences instead of having experiences is not great</li> <li>Don\u2019t type your symptoms into Google</li> <li>What is real on the Internet, isn\u2019t always true</li> <li>Social media abstinence is good for you</li> <li>You cannot understand someone through Intsagram</li> <li>Ratings are not worth the judgement</li> <li>Don\u2019t be steered towards being a caricature of yourself by the internet</li> <li>Algorithms eat empathy</li> <li>Limit the number of times you get the news</li> <li>The world is not as violent as it feels</li> <li>Bad news doesn\u2019t mean good news doesn\u2019t happen</li> <li>Sleep is the enemy of consumerism</li> <li>The imperfections of the real world fill the void that the perfections of the virtual do not</li> <li>Being lonely sometimes is not bad</li> <li>You don\u2019t always have to be available</li> <li>Uncertainty is not going to go away</li> <li>Be your own friend</li> <li>Don\u2019t grab life by the throat.</li> <li>Too many choices trigger your fight or flight response.</li> <li>You cannot be everything</li> <li>Work isn\u2019t the point of life. It\u2019s the point of capitalism. You are replaceable to your work, not your family.</li> <li>Aim to have less stuff to do.</li> <li>Nature is always there for you. So are animals</li> </ol>"},{"location":"articles/Notes%20on%20a%20Nervous%20Planet-%20take%20control/#fin","title":"Fin","text":"<p>And again, if you liked my summary, you will like the original book more. You wouldn\u2019t read Wikipedia and feel like you watched a movie would you? Support the author!</p> <p>This article is in the hopes that it will help someone out. Maybe have the help that I did not. I do not know who it will reach. But to whoever it does, best of luck :)</p> <p>Like these/Want more? Buy me a coffee! Kofi</p> <p>Want articles on something specific? Just ask!</p> <p>You can always contact me on LinkedIn, or drop me an Email</p>"},{"location":"articles/Obsidian%20Plugins/","title":"Obsidian Plugins","text":""},{"location":"articles/Obsidian%20Plugins/#my-favourite-obsidian-plugins-for-research-notes-2-bonus-tips","title":"My Favourite Obsidian Plugins for Research Notes + 2 Bonus Tips","text":"<p>Obsidian is my favourite program for taking notes. Be it for research, general things I learn, summaries from papers, lecture notes and the like. Out of the box, it does so many things really well.</p> <p>But, its real power lies in the vast number of plugins it has. Most of these are user created, and you can even make your own (or hack one together)! In this sea of functionality, these are the top few that I use. Grouped by the type of task for easier lookup.</p> <p>(Disclaimer : I am not sponsored by either Obsidian or any of the authors of the plugins mentioned here. These are personal preferences.)</p>"},{"location":"articles/Obsidian%20Plugins/#the-use-case","title":"The Use Case","text":"<p>I am a student, researcher and programmer. I take lecture notes, read a lot of research papers, articles and books. These come down to a lot of information. Of course, there\u2019s no way I can remember all of these bits of fragmented information.</p> <p>Therefore, drumroll\u2026, I use Obsidian to help me put these bits of information in a place I can easily access. Since I use this almost everyday, I want taking notes to be as painless and efficient as possible.</p> <p>These plugins are a huge help in doing exactly that. (Ordered by the type of task)</p>"},{"location":"articles/Obsidian%20Plugins/#how-to-install-these-plugins","title":"How to Install These Plugins?","text":"<p>This is a simple step. Simple open Obsidian Settings, Scroll down a bit and select \u201cCommunity plugins\u201d. Disable Restrictive mode, and then browse to your hearts content!</p>"},{"location":"articles/Obsidian%20Plugins/#writing","title":"Writing","text":"<p>Writing notes is the major objective here. So how do we make it extra painless? Plugins of course!</p> <ul> <li>Dynamic Table of Contents : Many times, I take notes for a long form text. Sometimes these notes end up pretty huge, and it becomes slightly harder to find something. What about adding a Table Of Contents to the start? Sounds great, but what if we update the note? In comes this plugin, with an automatically updating TOC.</li> <li>Tags : Super simple, also built in. Adding \u201c#topic1, #topic2 etc\u201d to a file to make it easier to search and organise.</li> <li>Frontmatter Tag Suggest : Tags are great, but who remembers which ones they used before? Nobody. This plugin autocompletes tags based on ones you have used in previous notes. You can create new ones the normal way of course.</li> <li>Note Refactor : Made a huge note with a lot of headings? Why not split them into individual topics and maintain links to them? This makes it easier for you to have one major idea per note. Here I have a bunch of test headings, you can see how after applying them they become new notes that link to the current file.</li> <li>Paste URL into selection : The name says it all doesn\u2019t it?</li> <li>Templater : Another plugin I use daily. I like starting my notes with \u201cdate_created\u201d, \u201cdate_modified\u201d, \u201ctags\u201d, \u201ctitle\u201d and insert the file name as the header. Since I do this for every single note, why not automate it? This plugin lets you create blocks of dynamic text to be inserted with a keyboard shortcut. I use \u201cCmd+Shift+I\u201d (\u201cControl+Shift+I\u201d for Windows)</li> <li>Typewriter Scroll : Zen Mode is a way of life. This lets me focus on what I am writing by automatically scrolling the page, and dimming the rest of the text apart from the line I am currently writing. I do disable it while reading though.</li> <li>Command Palette : This one is pretty obvious, but this built in plugin is just a text search. You can quickly open files with a (Cmd/Control + O) that brings up a searchable menu, or use (Cmd/Control + P) to bring up a searchable list of quick actions.</li> <li>Vim Mode : This little option is not for everyone honestly. If you have never heard of Vim, just skip this point. I use vim as my default text editor for everything else. And I can\u2019t live without its keybindings. This just lets me use the vim keys for everything.</li> </ul>"},{"location":"articles/Obsidian%20Plugins/#research","title":"Research","text":"<p>For research (AI research in my case), we have three main objectives :</p> <ul> <li>Merge important information from a large number of sources.</li> <li>Find links between ideas that you did not see.</li> <li>Maintain a daily log as something of a lab notebook. There are 5 plugins that fulfil these criteria pretty decently.</li> <li>Daily Notes : This is a Core plugin and comes with Obsidian. Essentially it\u2019s a journal. You can add whatever you want to it and it is created every day. I use it to keep a time stamped log of what I did that day. It is also useful if you just want to dump a bunch of information but don\u2019t want to format and organise it just yet.</li> <li>TimeStamper : In my daily notes, I like having timestamps (eg - 9:30 : I did xyz). This plugin lets me set a custom format and a keyboard shortcut. I have set it to \u201cCmd+T\u201d (for Mac or Control+T for Windows)</li> <li>Backlinks : A real game changer and another built in plugin. This shows you every file that either is linked in the current file, or refers to the current one. Identifying links between concepts, and finding more of them is absolutely invaluable in research.</li> <li>Quick LaTEX for Obsidian : LaTEX is probably the easiest way of writing professional looking math-y stuff, be it equations or formulae or anything similar. This plugin has a lot of options for autocomplete, formatting, and makes my job almost ridiculously easy. Here\u2019s how it looks. (Just typing a random equation)</li> </ul>"},{"location":"articles/Obsidian%20Plugins/#organising","title":"Organising","text":"<p>What do you do once you have a lot of files, you organise them of course! Now Obsidian by default makes it pretty easy to do this. But these plugins make organising less of a chore and much more of a fun thing to do.</p> <ul> <li>Local Images : To make my notes more informative, I sometimes paste images. Now many times these are links from some website, which makes it a little risky, because what if the website stops working? This plugin automatically downloads image links in your notes and saves them locally. (It also links to the correct downloaded file.)</li> <li>Graph view : Oh the gift and curse of a pretty graph. I sometimes use this to navigate between my links either to or from a file. It also gives me a very useful overview of what I have. I generally use the \u201cLocal graph\u201d that shows me a graph for the current note, rather than the \u201cGlobal\u201d full one which shows me everything. (It\u2019s pretty, but unhelpful)</li> <li>Linter : Maybe I have a bunch of empty lines, empty list items, my headers are not in sentence case, my text is not formatted, my paragraphs are weird. Or anything like that. I am lazy, so I use the Linter plugin to automatically perform a bunch of processing and clean up my files.</li> <li>Tag Wrangler : Have a lot of tags? View/Edit/Change them across every file that uses them in one place. Also useful for finding files that match a few criteria.</li> <li>File Cleaner : Remove empty files, unreferenced images etc. Keeping your \u201cDigital Garden\u201d pruned and bug free.</li> <li>Obsidian Link Converter : Because I host my Obsidian Vault on a personal website, sometimes the links that Obsidian uses don\u2019t work, this plugin lets me mass convert them to a format that does.</li> </ul>"},{"location":"articles/Obsidian%20Plugins/#bonus-tips","title":"Bonus tips!!","text":"<ul> <li>Make sure every file has a single major idea. If you have too many, use the \u201cNote Refactor\u201d to put them in their own files. This will make it extremely easy to refer to the \u201cIdeas\u201d in the text somewhere else instead of linking to the whole text.</li> <li>Want pages that consolidate all the notes that have a particular tag together and save them automatically to a single file? Say you want a file that has links to all the notes that have the tag \u201c#apple\u201d. Here is a little script that I wrote which does just that.</li> </ul>"},{"location":"articles/Obsidian%20Plugins/#fin","title":"Fin","text":"<p>This article is in the hopes that it will help someone out. Maybe have the help that I did not. I do not know who it will reach. But to whoever it does, best of luck :)</p> <p>Like these/Want more? Buy me a coffee! Kofi</p> <p>Want articles on something specific? Just ask!</p> <p>You can always contact me on LinkedIn, or drop me an Email</p>"},{"location":"articles/Simplify/","title":"Simplify","text":"<p>This book felt more like a primer into minimalism. It did have some good quotes and introduced somewhat of a less monastic approach. The author talked about how their family adopted the bits of minimalism that worked for their unique needs and applied them to every part of their lives.</p> <p>Some tips that I remember from the book</p> <ul> <li>Have a little box next to the \u201cclutter spots\u201d at home. That would show that the nest time you put things away and let them look less messy</li> <li>Every item has a home. If it doesn\u2019t, things tend to get lost</li> <li>Gifting season doesn\u2019t have to be stressful if you tell people what you want instead of blindly getting things</li> <li>Family can be taught how to keep things in their place. Be it toys or books or anything</li> <li>This book was written a while ago but everything about digital minimalism still holds. Social media sucks the life out of you</li> <li>Have the things that are either useful or make you feel good.</li> <li>When you die, someone will have to handle all your goods. You\u2019ll just doom them instead of being helpful. I guess that doesn\u2019t apply if they are poor and they could use the money, but still.</li> <li>Craigslist or similar services help a lot in decluttering</li> <li>Of course, giving it to thrift stores works too This reminds me of a few more tips</li> <li>Everytime you leave a room, see if there is something that doesn\u2019t belong</li> <li>Turn hangers one way, and only turn them the other way when you use the clothes. In effect you will see how many of them don\u2019t get worn at all. I think that\u2019s a pretty good idea.</li> <li>Similarly, keep things in a \u201cto-give\u201d pile. But wait for a month or two. If you don\u2019t remember what was there or actually havent bothered using it yet.. then its time to give it away</li> <li>Simplification should go to every part of your life. Be it digital or physical</li> <li>Maybe using the default apps or just clean applications without the clutter serves a good purpose in that regard.</li> <li>I guess it all boils down to the Pareto principle. \u201cThe big rocks\u201d. Only 20% of the things actually have 80% of the effect. That really does apply to every aspect of our lives doesn\u2019t it? Its almost weird if you think about it.</li> <li>There were also some somewhat vague tips on how to make sure the clutter does not come back. Most of it went in actually convincing yourself that you dont really need all those things. And trying it and seeing just how much difference it actually makes.</li> </ul> <p>All in all it was a small, nice introduction to the topic. It didn\u2019t really teach me anything new per se but some reminders of some ideas were needed. I will put them into practise. Subconsciously at least.</p>"},{"location":"journals/2022-06-14/","title":"2022 06 14","text":"<ul> <li>Started using logseq</li> <li>Basically doing this aman</li> <li>CenterNet</li> <li>16:45<ul> <li>RandAugment</li> </ul> </li> <li>16:50<ul> <li>RegNet</li> <li>LATER EfficientNet</li> </ul> </li> <li>LATER 16:53 Vision Transformer<ul> <li>Using obsidian to paste images didnt work out very well :/</li> <li>I really need to learn about transformers</li> </ul> </li> <li>17:16 DeiT</li> <li>17:25 Neural Radiance Field</li> <li>17:29 I discovered this awesome plugin called Roam-highlighter. I can just highlight and then it makes it into a proper copy paste thing. Its so cool.</li> <li>17:30 BYOL<ul> <li>Okay saving the images is really annying. Maybe I should find a fix for it at some point</li> </ul> </li> <li>17:38 SimCLR</li> <li>17:47</li> <li>NCE</li> <li>MoCO</li> </ul>"},{"location":"journals/2022-06-15/","title":"2022 06 15","text":"<ul> <li>Well I couldnt do anything more yesterday</li> <li>Today I need to focus on CCN, and also continue what I was doing</li> <li>The papers<ul> <li>11:34 Vision Transformer</li> <li>11:36 BEiT</li> <li>11:41 Swin Transformer</li> <li>11:48<ul> <li>CvT</li> <li>RepVGG</li> <li>Self Supervised Vision Transformers</li> </ul> </li> <li>11:54 ConvNeXt</li> <li>12:06 I just realized that I dont see a way to add backlinks here in this app. Should I go back to obsidian?</li> <li>12:12 Well im back in obsidian. It is arguably quite a bit better lol</li> <li>12:30 It took a while to fix all the things to make it back to the obisidan format. I guess that was useful because of this daily note thing</li> </ul> </li> <li>12:50 I need to do CCN. Lets see if I can find related stuff to what I wrote<ul> <li>12:54 watching the video she uploaded on it<ul> <li>Background - what do we know now about it. Theres a gap</li> <li>Main aims - outline what we will do. Characterize everything. Compare things. More difficult and easier versions</li> <li>Methodology - Just logic. Link between questions and answers</li> <li>Hypothesized results - where the field arries. What will it do?</li> <li>Sell the work</li> <li>2 - 4 pages</li> </ul> </li> <li>01:16 I really want to work but I should eat something. Need to call, go shopping and like also make dinner/cookies etc. Idk really.<ul> <li>Im also going to have to get the logs of that dudes experiments.</li> <li>And that stupid confuson matrix. It will work today though. It just has to.</li> <li>Okay I guess ill just download the logs. go make some lunch. Im conflicted between calling grandma and watching videos. Calling her would be an obligation. idk. Maybe ill call her later today when Im making cookies. that sounds like a nice time. Okay decided. I might add mum to it as well.</li> </ul> </li> </ul> </li> <li>03:29 So after eating a nice meal, I did collect a lot of papers on the topic. I will probably go grab stuff from jumbo as well After I organize things<ul> <li>Found this nice backup utility called Restic. Its so fast :o</li> <li>Also im getting a lot better at art now. Thanks to our fren Photoshop and quite a few YT tutorials</li> </ul> </li> </ul>"},{"location":"journals/2022-06-16/","title":"2022 06 16","text":"<ul> <li>12:03 Today I am starting with reading some books and working more on the CCN thing. Im at the library for a change. The chairs are pretty comfy</li> <li>12:03 First up is Grokking Deep learning by Andrew Trask</li> </ul>"},{"location":"journals/2022-06-17/","title":"DL","text":"<ul> <li>10:30 Well I woke up around here. Suddenly got some ideas about coding and stuff</li> <li>11:59 I finally understand how to use equinox. Im just looking at their codebase. It is actually.. not that hard? But to get to transformers, I need to build everything from scratch. To understand it. The only thing I need to understand apart from it is how to get the data loaded nicely. But I feel like it is within my reach now. Equinox really is nice. It is pretty normal and not bogged down by a million things.<ul> <li>https://d2l.ai/ is one of the best resources I have found so far</li> <li>Once these things are created, I think I will be in the position to actually explore research papers. But of course, it would take time to get there. I think it is definitely worth it though.</li> <li>But now I will eat something and focus on getting CCN together. I barely have like 5 days to finish it. I need to go through some papers and figure out what I want with them. But it will be okay I think.</li> </ul> </li> <li>12:13 Okay this is distracting as hell. I should really eat and work on CCN. Enough of this for now.</li> <li>12:58 Well I finally got down. Time to start work.</li> </ul>"},{"location":"journals/2022-06-17/#ccn","title":"CCN","text":"<ul> <li>12:59 CCN. Here we go</li> <li>Background - what do we know now about it. Theres a gap<ul> <li>Main aims - outline what we will do. Characterize everything. Compare things. More difficult and easier versions<ul> <li>a</li> </ul> </li> <li>Methodology - Just logic. Link between questions and answers<ul> <li>a</li> </ul> </li> <li>Hypothesized results - where the field arries. What will it do?<ul> <li>a</li> </ul> </li> <li>Sell the work  <ul> <li>a</li> </ul> </li> </ul> </li> <li>01:21 Papers and what they talk about<ul> <li>A RESEARCH ON SOCIAL MEDIA ADDICTION AND DOPAMINE DRIVEN FEEDBACK<ul> <li>Gaming addiction</li> <li>A nice blurb about Dopamine</li> <li>fMRI that shows that regions are activated that lead to production of dopamine</li> <li>Somewhat of an addiction</li> <li>Social media being an ass</li> </ul> </li> <li>01:33 Oh found this paper which talks about what I want. And another one they cited for a similar thing. I might be on to something here</li> <li>01:40 iNTERNET ADDICTION AND FUNCTIONAL BRAIN NETWORKS: TASK-RELATED FMRI STUDY<ul> <li>This one. This one answers a lot of questions</li> <li>Task related fMRI</li> <li>looking at specific brain regions, and their BOLD response</li> <li>Default mode network changes</li> <li>Only males were looked at. Consider more people? Different walks of life?</li> <li>Maybe in the short/longer term?</li> <li>The kind of changes were only observed in terms of people who answered that they were addicted to the internet</li> <li>Should I look at the more short term effects of it? Say math problems or something? Maybe something that requires a bit more thinking.</li> <li>02:04 How should I take the people? With a quick survery like these people? Choosing different samples?<ul> <li>Two groups. One with content similar to the task and another with ones that are different</li> <li>People of say different ages, but having too much variation is only possible if there are enough samples of each age. So instead maybe only focus on the \"addicted\" label?</li> <li>Functional correlates of Addiction in the Default mode network and Inhibitory Control Network</li> <li>I am still not sure of the task. That is the major question. I do want to test the short term effects of social media. So basically I need some sort of task. What was this Stroop Task ?<ul> <li>Okay this does not seem like something that might be very useful</li> <li>02:35 Adding accuracy and speed into the metrics is an interesting point</li> </ul> </li> <li>hypothesized that BOLD signal changes in the regions of the Default mode network and Inhibitory Control Network are correlated with IA scores.</li> <li>PIUQ</li> <li>A baseline session would be nice</li> <li>Incongruent stimuli induced BOLD signal changes were found to be negatively correlated with PIUQ</li> <li>Moreover, we found significant deactivations in these areas during incongruent stimuli for both verbal Stroop and non-verbal Stroop-like tasks. Brain areas in the cerebral cortex with constantly decreasing activity during demanding tasks are considered being part of the DMN</li> <li>Authors claim that this may serve as neural background of the uncontrolled Internet use and they also suggest that IA may share similar neurobiological abnormalities than other addictions.</li> </ul> </li> </ul> </li> <li>02:36 Li et al. - 2015 - Brain structures and functional connectivity assoc<ul> <li>To ensure signal equilibrium and participants\u2019 adaptation to their immediate environment, the first 10 volumes of the functional images were excluded. The remaining 232 scans images were preprocessed including slice timing, head motion correction, and spatial normalization to a standard template.</li> <li>To the best of our knowledge, this is the first study to simultaneously investigate structural and functional brain alterations in a large sample of healthy young adults with an IA tendency.</li> </ul> </li> </ul> </li> <li>02:38 So far<ul> <li>reasons<ul> <li>(Long term) and short term effects of this in two groups who identify as addicted/non addicted and changes in scores in a short term setting</li> <li>Threshold of difference in cognition (if enough samples can be obtained)</li> </ul> </li> <li>Need<ul> <li>Kind of pages to show. Since it is verbal and non verbal tasks. Maybe more of meme pages? Or color schemes<ul> <li>Expected -&gt; Related pages might lead to better performance, Unrelated might lead to degradation<ul> <li>In people who are addicted, the baseline performance is lower</li> <li>In a short term setting, how much do these (two) tasks lead to a degredation in performance</li> </ul> </li> </ul> </li> </ul> </li> <li>How<ul> <li>Measuring fMRI BOLD response in relation to the regions of the Default mode network and Inhibitory Control Network</li> <li>Pick people with the PIUQ survey first</li> </ul> </li> <li>Issues<ul> <li>Too simple task?<ul> <li>Maybe combine multiple Stroops in one task? Say two colors on a page instead of one?</li> </ul> </li> </ul> </li> </ul> </li> <li>02:46 Yeah, I think I have the structure of the paper in hand now. Just need to flesh it out and see if any other points come up that might lead to questions<ul> <li>Maybe looking at a few more papers to find out what other types of work have been done.</li> <li>But I can go take a break now and call parents and eat something more</li> </ul> </li> </ul>"},{"location":"journals/2022-06-19/","title":"Main Day","text":"<ul> <li>12:41 Okay so here we are again. yesterday was a nice break. Today I want to finish a first draft of CCN. It is almost 1. But maybe in an hour I should have a skeleton of the draft ready.<ul> <li>2-4 pages</li> <li>Title<ul> <li>Short term distraction on the internet and functional brain networks : A task-related FMRI study</li> <li>Short term distraction on the internet and it's effects on functional brain networks</li> <li>Effects of intermittent social media distraction on functional brain networks<ul> <li>Yeah I am leaning towards this one here</li> </ul> </li> </ul> </li> <li>Participants<ul> <li>An ideal number of participants would be atleast 10 for each of the two groups. (One with pages of a similar nature to the task, and another without)</li> <li>!!!!! Okay maybe send them an email asking for how long this actually takes</li> <li>For every group, the participants are divided into age brackets 18-20, 21-39, 40-50+. The brackets were chosen based on the common scale of teen, adult, middle age. Since the objective is to see a general trend and not specifically focus on age groups, the author believes this to be a decent criterion.</li> <li>The participants would be chosen based on the PIUQ survey and broadly classified into categories that are addicted to the internet and those that are not.</li> <li>01:14 The participants are further divided into two groups. Each group of 10, would have atleast 2 from each age bracket.</li> <li>Further funding would enable a more comprehensive study with more people from each age bracket.</li> </ul> </li> <li>Data Collected (Renamed at 01:24 )<ul> <li>From the task software, the accuracy achieved, along with the time to finish the task would be collected. From the fMRI data, the BOLD responses at the regions of the DMN and ICN would be collected as a timeseries. These would further be chunked into data from before and after the distraction was induced. Combining these sources of information for every individual, it would be possible to find that patterns that we are looking for.</li> <li>Once the data is obtained, standard preprocessing steps would first be applied to the fMRI data (denoising, removing faulty data, removing outliers, normalizing). The task data will also be checked for outliers.</li> </ul> </li> <li>Analysis Performed<ul> <li>The analysis would be performed for every individual. It would later split into the age brackets and internet addiction categories (since this is just a few simple data aggregations). A similar analysis would be done for both groups with the similar and dissimilar distractions to their task.</li> <li>The activations of the DMN and ICN from before and after the distraction would be compared. Care would be taken to ignore the time that the participant would require to get used to their environment. Special focus would be given to the performance in the moments right after the distraction. It would also be interesting to note, if the performance increases a span of time after they start doing the task again.</li> </ul> </li> <li>01:36 Hypothesised results<ul> <li>Altered ICN shows that they can't stop being distracted, and keep coming back. Altered DMN can be linked to performance</li> <li>01:57 It is expected, that distraction lowers performance in the short term as the brain gets engaged to a task and is not able to quickly switch between the two. This could be contrasted between how related the distraction is to the task at hand.</li> <li>03:31 A slightly lower baseline score is also expected for the participants that are more addicted to the internet than the others.</li> <li>01:48</li> <li>In these tasks, a lowered activation of the DMN might be correlated with the performance over time. The initial span right after the distraction is predicted to have a lower DMN activation, corresponding to lower accuracy. This is also predicted to get better (increase activation) over time.</li> <li>If the distraction is related to the task at hand, it is expected that the participants would show a slight drop in performance that recovers in time. While, if it was not related to the task, a much larger and more consistent drop in performance is expected.</li> <li>Altered activations of the ICN are related to not being able to stop coming back to the distraction. Therefore this is also expected to vary in both tasks, but more significantly in the different task.</li> <li>The author also expects that the different age brackets would show similar results, with a different response as the age increases.</li> </ul> </li> <li>01:53 The task<ul> <li>Distraction of 5 minutes.</li> <li>A visual Stroop task where the participants are shown a color and the corresponding word is shown. The correct answer should be typed into a box. (Spelling errors are not accounted for as of this study but could be a future research point.)</li> <li>After half of the total number of questions, a distraction of 5 minutes is given. One group of participants is given a curated feed of images and comments of random media (focus on different colors in each image). The other group is given a feed of images that have a specific color scheme (blue houses, green cars etc) and their related comments.</li> </ul> </li> <li>01:51 Future analysis<ul> <li>If the initial task is succesful, further funding would enable future research in the following ways.</li> <li>It would be possible to explore the age difference in more depth, which would lead to more interesting results. This could be very useful in providing different levels of changes in schools and organizations. For example, requiring a certain amount of phone free time before an exam.</li> <li>A more long term study would be possible, where there are multiple times of distraction of longer times.</li> </ul> </li> <li>02:03 Background<ul> <li>I think I will eat first and come back to this after I am done</li> <li>03:19 In recent times, our devices have begun to consume most of our time and energy. Be it scrolling through Instagram for hours, or checking our notifications every few minutes, this is undoubtedly very harmful to us in the long run.</li> <li>A lot of the research has been focused on the negative impact of this constant barrage of information. Dieter et al. showed that people who were more addicted to communication based applications, exhibited an impairment in inhibitory responses and had a harder time being calm, and emotionally focused. In attempting to measure if social media led to changes in the perception of time, Turel et al. found that the groups of participants that were at risk to being addicted, showed a distortion in time estimation. Another interesting study was done by Zhan et al. where they looked at fMRI data when users of the Chinese social media site, Sina Weibo were shown positive or negative messages. They found that reposts of negative messages affected the functional connectivity of certian parts of the brain more than positive reposts. H\u00fcseyin et al. looked at the effects of long term social media addiction and talked about how the brain's dopamine feedback loop is responsible for being addicted to online games. This further leads to users coming back for more, which in the long term leads to major mental health issues. Doing an fMRI scan, they identified similar regions of activations as those with substance addictions. Li. et al performed a very interesting study on healthy adolescents with internet addictions. They found concerning changes in the functional connectivity in many such brains. The authors do state that to the best of their knowledge, it was one of the first to perform a study of this scale in this domain. Continuing this thought, Gergely et al. investigated the links between functional brain networks and being addicted to the internet. They used a task related fMRI to collect data. A stroop type task was assigned to the participants and the fMRI BOLD response in the Default Mode Network (DMN) and the Inhibitory Control Network (ICN) were observed. They chose participants using the Problematic Internet Usage Questionnaire (PIUQ) and found a negative correlation between it and the BOLD signals. They talked about how these responses might lead to the users coming back to the internet, thus perpetuating the cycle.</li> <li>03:47 Although many such studies have been done on the long term effects of these distractions, not many were performed on the more short term effects of the same. These would be extremely useful in identifying just how much these devices affect us in our day to day activities. For instance, would texting people or scrolling Instagram right before an exam, negatively affect performance? Or would it only have a major effect right at the start. These questions are very relevant to our condition in these years. This study aims to answer some of them and hopefully lead to more extensive research in the same.</li> </ul> </li> </ul> </li> <li>04:12 Well, I am almost done with this. Just need to reread it once, add an abstract, some more references. And then we are golden. Also spellcheck. I think I need a break though. Idk. Maybe I will go get some chocolate and a small walk before I come back and work on this again.</li> <li>07:44 Uh. That was a long break. went on a walk. Napped. Called parents. And idk wasted time I guess. I need to do a few things in order<ul> <li>spellcheck</li> <li>Add references to unhand sentences</li> <li>finish that abstract</li> <li>I guess I can leave the rereading for tomorrow</li> </ul> </li> </ul>"},{"location":"journals/2022-06-19/#dump-task-list","title":"Dump Task List","text":"<ul> <li>Remove the weird art reference thing that we started yesterday I guess. Or not. Im not sure. Maybe i will keep that as a notes folder. Hmm. If i delete the papers from there (which is a good idea tbh), this is actually feasible</li> <li>vim how to use without breaking fingers lol</li> </ul>"},{"location":"journals/2022-06-20/","title":"2022 06 20","text":"<ul> <li>04:07 Just finished the whole CCn thing. Couldnt upload it though, looks like she has not enabled the link yet.</li> <li>Im probably going to do some semi-mindless paper copying rn for an hour or so and then maybe nap for a while</li> <li>04:13<ul> <li>MILAN</li> <li>Robust RegNet</li> </ul> </li> <li>04:23</li> <li>BlockNeRF</li> <li>VICReg</li> <li>Masked Autoencoders</li> <li>04:30 Wow, formatting these take quite a while.</li> <li>04:36 Regularization</li> <li>04:40 Instant NeRF</li> <li>04:44 Pix2Seq</li> <li>MobileOne</li> <li>04:47 Imagen</li> <li>05:29 Found another backlinking thing. Oops. that took a while. I should take a break</li> <li>05:39 Some more</li> <li>ELMO</li> <li>Neural Probabilistic Model</li> <li>Speech Recognition</li> <li>Word2Vec</li> <li>GloVE</li> <li>Seq2Seq</li> <li>Phrase Representation Learning</li> <li>Bahdanau Attention</li> <li>Attention NMT</li> <li>Google NMT</li> <li>05:46 Okay I am fully tired of this now. Ill just format the previous ones and stop this for today. Ill probably come back to RNNs and transformeers later tonight</li> <li>05:57 finally done</li> </ul>"},{"location":"journals/2022-06-21/","title":"2022 06 21","text":"<ul> <li>11:32 Okay I have been here for an hour or so. I have been saving things to add to the KB from every course that has been given and is relevant. I found a script that would extract pdf highlights. (Already had one for the web). And now I wrote another to format it a little so it's easier.<ul> <li>I can now very easily just go through the merged pdfs, highlight what I need. And then find a way to add it to the database. Pretty neat.</li> <li>Although now I should focus on important stuff</li> <li>I also found a very nice seat here. I really like it.</li> </ul> </li> <li>Emergentism</li> <li>Connectionism</li> <li>Nativists</li> <li>Elman 1990</li> <li>Connectionist Networks</li> <li>Backprop</li> <li>Bias nodes</li> <li>Words-and-Rules</li> <li>11:37 that was fast and nice.</li> <li>Okay maybe till 12 I will do language modeling. Then will move on. This also has RNN's in it so that is definitely a bonus</li> <li>11:54 Some more from the CSL stuff. then will move on to the code of RNNs and the lot</li> <li>Wickelphones</li> <li>Semantics influences form</li> <li>Symbolic learning model</li> <li>Memory-based learning</li> <li>Non-adjacent dependencies</li> <li>Visual Implicit Learning</li> <li>Misyak et al 2010</li> <li>Saffran, Aslin and Newport</li> <li>12:46 I FINALLY UNDERSTAND THE RNN, GRU AND LSTM DIAGRAMS. Stupid weird things. Its not even hard -.-. Good ol code saving my ass</li> <li>05:06 Oops its been a while. Ive been around quite a bit in the meanwhile. Had food. Went to get the bike fixed which did not work. Now I am here in the forum. Working on the models again. Until I can I guess</li> </ul>"},{"location":"journals/2022-06-23/","title":"CSL","text":"<ul> <li>Transitional probabilities</li> <li>Mirman et al.</li> <li>Localist units</li> <li>Distributive units</li> <li>Berkeley et al</li> <li>Superposition Catastrophe</li> <li>Symbolic models</li> <li>conditioning</li> <li>Rescorla-Wagner Algorithm</li> <li>Attentions and salience</li> <li>Milin et al.</li> <li>Deductive Approaches</li> <li>Inductive Learning</li> <li>Overhypotheses</li> </ul>"},{"location":"journals/2022-06-23/#hwr","title":"HWR","text":"<ul> <li>Hit list</li> <li>Monk</li> </ul>"},{"location":"journals/2022-06-24/","title":"2022 06 24","text":"<ul> <li>12:16 I have spent the past 1.5 hours cleaning up the notes, adding some tags, linking some stuff<ul> <li>New shortcut for a random file is control+shift+b</li> <li>I mostly have two major tasks today (maybe 3)<ul> <li>Seeing if I have comments on the CCN paper and fixing those(not sure if i can today)</li> <li>Learning some more about transformers and preparing some more for the talk</li> <li>Working on the paper that I have thrown to the corner</li> </ul> </li> <li>I also want to focus on drawing a little more today. There are some things I want to try.<ul> <li>I want to first pick any game. Look at their wiki for descriptions of the background. And design my own with that. Free prompts but not vague enough to get me annoyed. I think that is a good way to start. The question is. What game? Maybe some hack and slash types?</li> </ul> </li> </ul> </li> </ul>"},{"location":"journals/2022-06-25/","title":"2022 06 25","text":"<ul> <li>10:31 Well I acheived nothing today except going out. I dont feel like doing anything rn either lol. Have to prepare for that workshop and also finish CCN comments. But i dont feel like it. So ill just do some mindless work. Expand the KB or something. Idk. Dont have much brain space left rn honestly. Im very tired.</li> <li>10:39 ADVENT</li> <li>10:48 I barely understand whats happening. But thats fine. Its a KB right. Im pasting the links to the original article as well. Just in case I need it but didnt copy it correctly now. Atleast itll be easy to find</li> <li>10:49 Isotropic Architectures</li> <li>11:15 A really nice blog archive link</li> <li>11:16 ALBERT</li> <li>11:47 Lisht</li> <li>12:03 I could do more but I really feel myself being slow and weird. I can barely keep my eyes open. Ill just go sleep. Atleast I added some stuff to my KB and found some useful articles.</li> </ul>"},{"location":"journals/2022-06-26/","title":"2022 06 26","text":"<ul> <li>11:17 Okay Im here at the library. Today I want to focus on finishing the CCN paper (there were only a few comments). After that the rest of the time I want to work on tomorrows workshop. I feel like itll be fine if i write it down<ul> <li>As a start, I want to continue some KB work that I started yesterday from this blog . Its no longer a website for some reason, so Id rather get the materials. Some of the pages are genuinly awesome</li> </ul> </li> <li>11:18 DALL-E</li> <li>11:21 There are some common transforms I keep having to make while copying from these blogs. Ill make a quick script to help me make them instantly</li> <li>12:40 Forgot to mention, I re-added most of the Optimizers and have been looking at comments in the paper<ul> <li>Its not too bad. I fixed some of the minor ones. Now working on the bigger ones. Ill probably be done in an hour or so.</li> <li>Then maybe ill take a small break and come back to work on the workshop. Lets see how it goes.</li> <li>I do feel kinda anxious rn.</li> <li>Ah the poor website is still very very broken.</li> <li>12:54 Maybe this change fixed the links. Fingers crossed. One sed command to convert wikilinks to markdown links</li> </ul> </li> </ul> <pre><code>      find . -type f -exec sed -ri \"s,\\[\\[([^\\[\\|]([^\\[\\](%5B%5E%5C%5B),g\" {} \\;\n</code></pre> <ul> <li>01:25 I should take a break</li> <li>01:40 Im back. Going to work on the paper again.</li> <li>02:55 Okay done. I finished the paper and Plag scan and uploaded it. Im officially done with uni work for now yay! Okay now I will work some more on the KB, then focus on getting the workshop stuff ready to go.</li> <li>03:19 Added some more stuff on activation functions and random network stuff</li> </ul>"},{"location":"journals/2022-06-27/","title":"2022 06 27","text":"<ul> <li>RoBERTa</li> <li>BART</li> <li>DistillBERT</li> <li>Transformer-XL</li> <li>XLNet</li> <li>Adaptive Input Representation</li> <li>Interpreting Attention</li> <li>Grad-CAM</li> <li>LASER</li> <li>GPT3</li> <li>Longformer</li> <li>Big Bird</li> <li>CheckList</li> <li>Neural Text Degeneration</li> <li>ELECTRA</li> <li>TinyBERT</li> <li>BinaryBERT</li> <li>Zero Label Language Learning</li> <li>RETRO</li> <li>WebGPT</li> <li>Curriculum Learning</li> <li>Hallucination Text Generation</li> <li>FLASH</li> <li>Chain of Thought</li> <li>PaLM</li> <li>Few Shot Order Sensitivity</li> <li>Big-Bench</li> <li>Chinchilla</li> <li>10:10 Starting the </li> </ul>"},{"location":"journals/2022-07-04/","title":"2022 07 04","text":"<ul> <li>08:05 Been a while. Just want to add some stuff to the DB. Some timepass tbh. Maybe for 30 35 mins</li> <li>08:26 I guess it would be nice to upgrade my little website formatter script with the filenames as tags. :/ Idk whats up with me and making this database. I feel like its the most important thing ive done so far in this domain. Idek why. But what to do. its just something I have to do I guess.<ul> <li>Okay lets see if I can make this script better.</li> </ul> </li> <li>08:35 Yeah idk about that. It seems like all im doing is redifining what Obsidian does already. Maybe I should just let it do its thing and focus on simple formatting only</li> <li>Benchmark LLM</li> <li>OPT</li> <li>Diffusion LM</li> <li>DeepPERF</li> <li>CTC</li> <li>Joint Factor Analysis</li> <li>Speech Recognition</li> <li>Speaker Verification</li> <li>Listen Attend Spell</li> <li>VGGish</li> <li>08:47 I guess this whole thing counts as procrastinating? I dunno honestly but well. I do think its not tooo bad</li> <li>08:50 Maybe I should write one bit to automate the file creation.. that is taking the most time right now honestly</li> <li>X Vectors</li> <li>WaveGlow</li> <li>11:05 More now</li> <li>11:19</li> <li>Tacotron 2</li> <li>wave2vec</li> <li>SpecAugment</li> <li>Conformer</li> <li>wave2vec 2</li> <li>HiFI-GAN Denoising</li> <li>HiFI-GAN Synthesis</li> <li>Speech Emotion Recognition</li> <li>XLM-R</li> <li>GE2E</li> <li>Generative Spoken Language Modeling</li> <li>pGLSM</li> <li>Speech Resynthesis</li> <li>S2ST</li> <li>Textless Speech Emotion Conversion</li> <li>dGSLM</li> <li>textless-lib</li> <li>11:35 More?</li> </ul>"},{"location":"journals/2022-07-07/","title":"2022 07 07","text":"<ul> <li>08:08</li> <li>LIME</li> <li>SHAP</li> <li>08:59</li> <li>Contrastive Predictive Coding</li> <li>Data aug for spoken language</li> <li>CLIP</li> <li>DALL-E</li> <li>ViLT</li> <li>MLIM</li> <li>DeepNet</li> </ul>"},{"location":"journals/2022-07-11/","title":"2022 07 11","text":"<ul> <li>02:53 Just spending time here. Will add more stuff to the KB as I go along.<ul> <li>I majorly want to work on segmentation using that Vision transformer today</li> <li>Add add things to the KB</li> </ul> </li> <li>03:09 Some more. Gods theres so many of them</li> <li>data2vec</li> <li>DALL-E 2</li> <li>AutoDistill</li> <li>Gato</li> <li>Scene based text to image generation</li> <li>i-Code</li> <li>VL-BEIT</li> <li>FLAVA</li> <li>Flamingo</li> <li>Collaborative Topic Regression</li> <li>Wide Deep Recommender</li> <li>DeepFM</li> <li>DLRM</li> <li>03:15 I am somewhat tired of mindless copy paste. Ill look at some more in depth articles for now</li> <li>Learning Rate Range Test</li> <li>Style GAN</li> <li>06:13 I found a nice blog of quite a few things colah<ul> <li>Connectome</li> <li>This is really interesting :o<ul> <li>Interpretability vs Neuroscience<ul> <li>06:17 HOLY. I found something REALLY INTERESTING. ../Neural Circuits</li> </ul> </li> <li>In fact, I think it ties in to my thesis ideas as well :O What are the odds??</li> <li>Okay wait. After Karpathy and Jeremy Howard, this researcher is another crazy nut. Maybe I have a new hero?? Christopher Olah is his name. All bow.</li> </ul> </li> </ul> </li> <li>06:51 Interpretability vs Neuroscience</li> <li>I know this should be an AI knowledge base but I think ill just use it as a general one. Who knows where connections come from right?</li> <li>Micromarriage</li> <li>Research Intimacy</li> <li>Research Debt This is so true. I guess thats a huge problem..</li> <li>This person is literally doing what I want to look into. Guess it will directly influence my thesis</li> <li>Inceptionism</li> <li>08:19 Okay I should stop for today</li> </ul>"},{"location":"journals/2022-07-17/","title":"2022 07 17","text":"<ul> <li>\u201cAnother study, out of Yale, looked at the part of the brain known as the default mode network (DMN), which is active when we\u2019re lost in thought\u2014ruminating about the past, projecting into the future, obsessing about ourselves. The researchers found meditators were not only deactivating this region while they were practicing, but also when they were not meditating.\u201d</li> <li>\u201che had invented something potentially revolutionary: a real-time neuro-feedback mechanism that allows meditators to see when they\u2019re shutting down the Default Mode Network (DMN) of their brains, the so-called \u201cselfing regions\u201d that are active during most of our waking, mindless hours. From inside the narrow tube of the scanner (which I was too claustrophobic to get inside of, by the way), the meditator can see, via a mirror, a small computer monitor. When the DMN is deactivated, the screen goes blue.\u201d</li> </ul>"},{"location":"journals/2022-07-25/","title":"2022 07 25","text":"<ul> <li>10:03 Been a while. Today the point is to work on the fastai course :)</li> <li>10:40 Cups</li> <li></li> <li>11:03  This is such a cool visualization of the training process. Dahm</li> <li>I missed this guy. Good ol Jeremy Howard</li> <li></li> <li>11:07</li> <li>11:56 I had written all of these out in this daily journal but now I think I will move it to it's own page</li> <li>fastai</li> <li>Gradio</li> <li>Pytorch Tricks</li> <li>05: 03 Finally finished the art classifier. I also ended up doing quite lot of other work</li> </ul>"},{"location":"journals/2022-07-26/","title":"2022 07 26","text":"<ul> <li>09: 55 Day 2 of the fastai course. lets see what today brings us.</li> <li>ULMFit</li> <li>11: 00 I think Ive had enough for now. The NLP bits still seem not very interesting to me, although I might look into it after a little while. I do want to focus on implementation now.</li> <li>Maybe I will read the fastai blog for interesting bits or just code. Probably code.</li> </ul>"},{"location":"journals/2022-07-27/","title":"2022 07 27","text":"<ul> <li>12: 55 Today I want to implement as much of the ConvNeXt as I can</li> </ul>"},{"location":"journals/2022-07-28/","title":"2022 07 28","text":"<ul> <li>05: 50 A new research on how art was affected by factors in the past and how well. AI is not doing anything new.</li> <li>AI Art and How Machines Have Expanded Human Creativity #Roam-Highlights<ul> <li>AI artists embrace the interplay between accident and control and use AI to find a balance between the two while developing novel concepts and visuals.</li> <li>The origins of the concept of artificial intelligence can be traced to classical philosophers who made their mark in the quest for describing the process of human thinking as a symbolic system</li> <li>However, the first proto-computers and the idea of programmable, creative machines only came around in the 19th century.</li> <li>Nearly lost to history, in the 1840s Ada Lovelace merged her creative and analytical ambitions and cemented herself as the first computer programmer and creative coder</li> <li>In the late 1950s, visual artists started engaging with the new technological concepts and began experimenting with computer graphics</li> <li>In the early days of computer art, artists like Manfred Mohr and Vera Moln\u00e1 explored aesthetic perspectives led by science and created compelling artifacts that were believed to be derived from the subjectivity of the artistic process.</li> <li>One of the first artistic applications of GANs was Alexander Mordvintsev\u2019s DeepDream algorithm</li> <li>In 2015, Mordvintsev, a researcher at Google, found a way to plumb the hidden depths of a neural network and study how machines learn visual concepts.</li> <li>The growing accessibility of working with open source repositories and training datasets allowed artists to generate works from the get-go</li> <li>One of the first auctions of AI art was arranged in 2016 in the Gray Area in San Francisco, where AI artists like Memo Akten and Mike Tyka exhibited images generated by Google\u2019s initial Deep Dream algorithm</li> <li>The auction of Portrait of Edmond Belamy was the first widely covered sale of an AI artwork; however, many data scientists and artists had already created and sold AI art before the auction.</li> <li>Inceptionism</li> <li>WOMBO Dream</li> <li>GauGAN2</li> </ul> </li> </ul>"},{"location":"journals/2022-07-29/","title":"2022 07 29","text":"<ul> <li>02: 49 Just trying out different datasets and seeing how well we can perform on each</li> <li>Kvasir Dataset</li> </ul>"},{"location":"journals/2022-08-02/","title":"2022 08 02","text":"<ul> <li>06: 30 Got a mac. Will do most of my research on this from today.</li> <li>08: 06 Memory formation</li> <li>Motor Memories</li> </ul>"},{"location":"journals/2022-08-03/","title":"2022 08 03","text":"<ul> <li>11: 03 Will add a folder for papers as its kinda hard to put papers into one folder with their titles.</li> </ul>"},{"location":"journals/2022-08-10/","title":"2022 08 10","text":"<ul> <li>10: 27 I feel out of touch with DL. Ill try to read a paper today and make notes on it. Maybe I should do some more fun reading. Rn its pretty overwhelming and boring.</li> <li>10: 36 Should I start blogging again?</li> <li>Im not sure. I feel like it might be good for me.. But it seems a bit overwhelming again. I also want to go through this KB and add whatever I want to specific topics. Anything I do not understand. Maybe I can flag them with a #blog</li> <li>hmm. After I go through this current paper, I will do that.</li> <li>I realized I can write 'microblogs'</li> <li>Just short things, what, why, how. Will take the pressure off me and also help me remember things better. Maybe I can just have a little folder of articles. Hmm.</li> <li>11:13 I read the distillation paper. Ill get back to writing notes on it soon. For now Ill go and tend to my digital garden of knowledge.</li> <li>02:01 Ah Im very satisfied with the improvements to the KB. The links finally work. I also wrote a tag page generator.</li> <li>05:19 Added quite some content, cleaned up a bit.<ul> <li>I know this KB will be useful someday. I just dont know why I invest so much time in it. Maybe its just procrastinating or maybe its the only thing I can do rn.</li> </ul> </li> <li>05:46 Added a lot of stuff in robotics from motoman</li> <li>07:06 Im fond of these glossary lists. Especially in fields I dont know. I think itll be useful while reading articles. Added some in brain stuff from dana</li> </ul>"},{"location":"journals/2022-08-12/","title":"2022 08 12","text":"<ul> <li>02:07 Another day, another existential crisis. Im not sure what I can do today or what I have the energy for. Maybe I want to write an article. But about what?</li> </ul>"},{"location":"journals/2022-08-14/","title":"2022 08 14","text":"<ul> <li>07:01 Just doing a crash course on Neuroscience. Dont have much brain power left so just doing what I feel like. here</li> <li>07:48 Now Im adding physics equations. I dunno why. But I feel like learning random stuff and remembering things from school. I cant remember everything but just putting things here seems like a nice thing.</li> <li>LINK to phys</li> </ul>"},{"location":"journals/2022-09-02/","title":"2022 09 02","text":"<ul> <li>12:07 TA meeting<ul> <li>Learn about brightspace</li> <li>1st - 2 weeks</li> <li>starts from 15th</li> <li>look at the metrics once</li> <li>https://rugcognitiverobotics.github.io</li> <li>Send an email to HR for work permit</li> <li>Send picture somewhere</li> </ul> </li> </ul>"},{"location":"journals/2022-09-06/","title":"2022 09 06","text":"<ul> <li>12:39 I wrote a blog post on another blog post by Chip Huyen https://msubhaditya.medium.com/notes-machine-learning-system-design-aeba04190856</li> <li>Prediction assumption</li> </ul> <ul> <li>01:02 I wanted to also start working on my thesis. Just atleast have a rough sketch of what I want to achieve with it so I can talk to my supervisor about it.<ul> <li>I also realized that I can keep producing blogs. Just write \"comments\" on what I read, and add my own take to it. But more importantly, I have to not post everything I write or paint in one go. Maybe I can post twice a week or something. Depending on how much work load I have and what I feel like then.</li> <li>Not sure how much engagement it can even get but then I have no pressure to write but I can just explore and talk about what I am working on.</li> <li>I was watching an interview of Kenji Lopez Alt. and he said something like, he makes content for himself. And there are people who end up finding it useful. But because he does what he finds interesting and can contribute to (ADHD life), he ends up not feeling the pressure. But just putting himself out there in his most authentic self. Another video by Adam Duff also was saying something like that. The highest performers, and the people who are really in any field doing their best, are there because they found out what they can do. Authentically. They are not doing it for anyone else. The best work.. was for themsleves.</li> <li>That reminds me a lot of the Dao. Anyway. I should get back to doing something or planning out what to do.</li> </ul> </li> <li>01:23 Putting this here so I can find it later - Trouble Shooting Neural Networks</li> <li>01:29 Gradient Checkpointing also seems like a very very interesting thing. I should check it out at some point.</li> <li>02:03 Okay I should really take a break and eat something. I collected some papers looking for my thesis too. Maybe Ill take some time and flesh it out properly in time.</li> <li>I did get quite some work done this morning.<ul> <li>Did my bills</li> <li>Sent emails about the course I wanted to drop out from</li> <li>Sent emails about my work permit</li> <li>Wrote a pretty decent article</li> <li>Read some articles about MLOps</li> <li>Collected quite a few papers around my thesis</li> <li>Planned out the assignment dates for this block</li> </ul> </li> <li>02:05 Maybe I will take a walk, eat something and then sit and work a little more. My head is starting to wear out for now though, so I might just go home instead. Lets see what I feel like doing.</li> </ul>"},{"location":"journals/2022-09-10/","title":"2022 09 10","text":"<ul> <li>12:22 I spent a while thinking of my thesis and came up with a proper training workflow. I think?</li> <li>01:58 Okay sent an email to the prof with the details. Honestly? Im kinda proud of myself. I actually planned something. Dahm xD<ul> <li>I just want to start ASAP because idk why. Im probably already way ahead of a lot of people just because I have a proper plan. Plus making a framework for the experiments will probably only take me a week or two. Maybe less if I sit and do it properly.</li> <li>TF should I do for a whole year? Maybe I can do some more TA positions or freelance jobs. Dunno.</li> </ul> </li> <li>Hmm. Ill work on the knowledge base again for a bit after a small break.</li> </ul>"},{"location":"journals/2022-09-12/","title":"2022 09 12","text":"<ul> <li>03:02 User models</li> <li>Extensions to SlimStampen</li> <li>Ideas for Fact Learning</li> <li>03:19 Gamification</li> <li>03:26 Game Based Learning</li> <li>03:47 Eye Tracking</li> </ul>"},{"location":"journals/2022-09-13/","title":"2022 09 13","text":"<ul> <li>09:25 Language Modeling</li> </ul>"},{"location":"journals/2022-09-14/","title":"2022 09 14","text":"<ul> <li>09:08 Pytorch syntax</li> <li>Pytorch Tricks</li> <li>10:42 Context learning for user models</li> <li>12:02 Sci Vis<ul> <li>Parallel Coordinate Plots</li> </ul> </li> </ul>"},{"location":"journals/2022-09-20/","title":"2022 09 20","text":"<ul> <li>09:17 Language modeling<ul> <li>fillers in experiments - planning, similar to each other but not too much</li> <li>bundle it with something</li> <li>Processing Sentences with distributive and collective readings<ul> <li>processor : all processes in brain</li> <li>Grammatically forced choice</li> <li>Vaguenes</li> </ul> </li> </ul> </li> <li>12:27 Language modeling assignment 1<ul> <li>The Factors Experiment 1 investigated transitive sentences with numerically quantified subject and object DPs. The numerical quantifiers were always the numbers \"three\" in the subject and \"two\" in the object. Additionally, sometimes the sentences included the distributional quantifier \"each\":</li> <li>Three girls pushed two wagons.</li> <li>Three girls each pushed two wagons. This means that we have one factor, call it \"Sentence\" that has two levels: we have version of the item without \"each\", and versions of the item with \"each\". These two types of sentences were then presented either with a picture where the three actors collectively act on the objects, or pictures where they each act separately on their own set of objects. This is then the second factor investigated, which we cann call \"Picture\". The factor \"Picture\" thus has two levels as well: Distributive or Collective. This gives us a 2x2 design, resulting in 4 Conditions:</li> </ul> </li> </ul>"},{"location":"journals/2022-09-20/#results","title":"Results","text":"<ul> <li>No Each + Distributive : 0.4722222</li> <li>No Each + Collective : 0.9768519</li> <li>Each + Distributive : 0.9631336</li> <li>Each + Collective : 0.1898148</li> </ul>"},{"location":"journals/2022-09-20/#what-we-know","title":"What We Know","text":"<ul> <li>From age 7, \u201ceach\u201d is gradually no longer acceptable with collective readings in active sentences, but this persists much longer (until after 9!) with passive sentences</li> <li>However, adults and children from age 5 don\u2019t like distributive readings with \u201call\u201d in passive</li> <li>Children correctly allow a collective interpretation with all, but not for sentences with each</li> <li>By 6, children consistently associate a distributive interpretation with each.</li> <li>Children gradually prefer collective readings with \u201call\u201d.</li> <li>Passive strengthens the collective preference, with children adult like at 7!</li> </ul>"},{"location":"journals/2022-09-22/","title":"2022 09 22","text":"<p>Experiment 1: Working with Categorical Data</p> <p>Experiment 1 collected judgments about whether or not the picture matched the sentences presented to participants. Officially it's a picture verification task, but frequently these types of studies are also called Truth-Value Judgment Tasks.\u00a0</p> <p>You will understand the experiment and the analysis best if you have done the experiment yourself. You can do the experiment at the following link:</p> <p>Truth Value Judgement Task</p> <p>Tips on understanding the experiment</p> <p>In order to understand the motivation of the research and the context in which it was conducted, you need to know something about the previous work that has been done.</p> <p>This work investigated how compatible sentences with numerically quantified subjects and objects are with distributive and collective situations, with and without the presence of the distributive quantifier \"each\". Classic works that are relevant to the experiment are\u00a0 Brooks and Braine (1996), Musolino (2009) and Syrett and Musolino (2013).\u00a0</p> <p>What to submit:</p> <p>The goal is to eventually be able to easily analyze and write up experimental results for a topic you know something about.\u00a0</p> <p>The format we will use is the CogSci conference submission template:</p> <p>Choose full paper template. There is both a latex and a word template.\u00a0</p> <p>CogSci Templates Location</p> <p>Lab 1 submissions</p> <p>For Lab1, You should fill in the title, author and try to write an abstract that summarizes the research questions, the method and the results. You can leave out the Introduction, Background, Discussion, and Conclusions/Directions for Future Research. You will need a reference list but it will be quite short because for the Method and Results section, there are only a few likely references related to the statistical analysis used.\u00a0</p> <p>Your main focus will be on the Method and Results sections.\u00a0</p> <p>What goes in the methods section?</p> <p>The methods section gives a kind of recipe for how to recreate the experiment. A key requirement of the documentation of scientific research is that the method has to be documented in enough detail that it can be replicated. Note however, that you do NOT need to included details that are no relevant to the results, or that other people will not have. For example, if you did an experiment and your computer kept breaking down, so you had to throw out some participants, then you would not include the details in the results. That's because it's irrelevant and it's also unlikely that other people will have similar problems. Instead, you can simply say that x number of participants could not complete the experiment due to technical difficulties and therefore their data was not analyzed.\u00a0</p> <p>In the Methods section you will need to explain (1) what the experimental design was, (2) what the fixed factors were\u00a0 (3) the method used and therefore the type of data collected, (4) the procedure, (5) the participants and their biographical features, etc.\u00a0</p> <p>In linguistic experiments it's standard to include example sentences, describing their form, and examples of the pictures, also describing their form. People say \"A picture's worth a thousand words\" and I would say \"An example sentence is worth 500\".\u00a0</p> <p>Lab1_Sentence_Examples.zip</p> <p>The attached file contains all the images and sentences used in the lab. It's all in PHP, so you will to decipher it a bit if you want to use it. You are not expected to do so, but it can help you if you really want to understand it (or you can just take a pretty picture for your report!).</p> <ul> <li>The file sentences_distributivity_new.php contains arrays of all the possible items, the sentence and the image associated with it. The files inputN.php decide which of these items will be shown to a user in group N (identified in the lab data with ListNum). The order of items is shuffled.</li> </ul> <p>An initial motivation?</p> <p>You can begin the methods section with a very brief introduction of what type of research question you wanted to answer, continuing with \"and for this reason we decided to test sentences like X\" or something like that, but a methods section can also be quite bare bones, just the details of what was done. If you do add some sort of motivation, keep it very brief.\u00a0</p> <p>Predictions at the end of the\u00a0Method\u00a0section?</p> <p>Sometimes it makes sense, and makes it clear for the reader, if the method section ends with just a brief statement clarifying what results are expected for the proposed set-up, given previous experiments. On the other hand, many people like to keep the method very simple and will not include this.\u00a0</p> <p>Personally, I like a clean method section. Instead, the background section should lead up to a motivation and a prediction at the end just before the Method section begins. But if you look at the papers associated with the course, you will find both types of Method sections.\u00a0</p> <p>The Results Section</p> <p>Here you should present the results of the experiment. You should first present data about if all participants finished the experiment and what the dataset is that you are then analyzing. After that, present the descriptive statistics, referring ideally to a diagram or figure. Descriptive statistics includes information about the mean acceptance rates, the standard deviation or other variation measurement and sometimes other things.</p> <p>After the descriptive statistics, present the mixed effect model analysis. There are several examples both in the file and also in many of the papers. I strongly suggest you try to find several examples and compare them.\u00a0</p> <p>Your results should include an informative figure (at least one). Please take care to formulate an informative caption for the Figure. Many people first glance through a paper looking at the figures. You want to make sure they can interpret what they see easily.\u00a0</p> <p>Reporting the Mixed Model Results:</p> <p>Here's an example from a recent paper of mine:</p> <p>We analyzed the children's responses using logistic mixed-effect models (Baayen et al. 2008) and step-wise deletion procedure. AIC values were compared to determine which model fit the data best, with a complex model being preferred over a simpler model only if its AIC value is two ore more points lower. The fixed effects were Quantifier (all vs. only), Picture Type (Exhaustive\u00a0 vs. Non-exhaustive) and the random effects of Participant, Item and Age (in days, normalized). We also tested both fixed effects as random slopes with all random factors (Jaeger 2008). The maximal model that explained significantly more variance retained the fixed factors Quantifier and Picture Type and the random factors Participant and Item only, with a significant interaction. The final model is presented in Appendix A, Table A.1.</p> <p>Pairwise comparisons (Tukey) showed two significant interactions at the p&lt;0.001 level: the \"all\" (al)-Exhaustive picture condition (24% \"Yes\" responses) had signfiicantly more \"Yes\" responses compared to the \"only\"(alleen)-Exhaustive picture condition (14% \"yes\" responses). The \"all\"(al)-Non-Exhaustive picture condition (14% \"yes\" responses) also had significantly fewer \"Yes\" responses compared to the \"only\" (alleen)-Non-exhaustive picture condition (26% \"yes\" responses). No other comparisons were significant.</p> <p>The appendix</p> <p>In the main results section you do not include the actual table output from the best model. But you should include this, and also any output from Post-hoc comparison tests, as tables in the appendix. Here again, watch your names. Below is an example of how you can present the model in the appendix.\u00a0</p> <p></p> <p>A word about names</p> <p>In referring to factors you need to use consistent, clear names throughout the paper you write. Choose names that make it easy for the reader to follow your thoughts. Scientists are busy people. If you present results with names like \"Factor 1\" or with uninterpretable abbreviations such as \"cll\" for \"Collective Pictures\", they might stop reading. They won't go reread previous text to figure out what Factor 1 was. Your task as an author is to make the results as easy to follow as possible.\u00a0</p> <p>The same goes for Figures. You need to make sure Figures are clearly labelled with interpretable names (and fonts that fit with the size of the rest of the text, please not super tiny!) It may be that you have trouble with the program you use for the figures to get the labels the way you like them. This has happened to me as well. My favorite fix for that is MS Paint. I simply write over too small text with something that readable.\u00a0</p> <p>Conclusions or commentary in the Results section?</p> <p>In presenting the results, and at the end of the results section you may want to add some commentary that helps the reader follow your thinking. Technically, this commentary is a kind of discussion of the results, but a very brief one. That's ok and can really help make a paper more readable. The kind of thing I mean is you may say something like \"Participants accepted X at a significantly higher rate than Y in all conditions\" and then you add \", consistent with previous research\". Or \"as expected\". Just be careful that you don't get into a longer discussion.</p>"},{"location":"journals/2022-09-22/#add","title":"Add","text":""},{"location":"journals/2022-09-22/#intro","title":"Intro","text":"<ul> <li>concepts, collative, distributive</li> <li>research q. will use models to check significance</li> <li>key ideas for exp. eg: sentences match pictures</li> <li>super summary</li> </ul>"},{"location":"journals/2022-09-22/#methods","title":"Methods","text":"<ul> <li>data about participants<ul> <li>outliers</li> </ul> </li> <li>models</li> </ul>"},{"location":"journals/2022-09-22/#results","title":"Results","text":"<ul> <li>significances and what they mean</li> </ul> <p>Pr(&gt;|z|)</p> \\[Pr(\\gt|z|)\\]"},{"location":"journals/2022-09-26/","title":"2022-09-26","text":"<ul> <li>https://ieee-dataport.org/open-access/annotated-image-dataset-household-objects-robofeihome-team<ul> <li>Another dataset household object</li> </ul> </li> <li>https://www.cs.cmu.edu/~ehsiao/datasets.html<ul> <li>Household objects dataset : CMU</li> </ul> </li> </ul> <p>Information -&gt; Schedule Lectures</p> <p>From this week, Tuesday afternoon labs are shifted to thursday morning</p>"},{"location":"journals/2022-09-26/#mastersthesis","title":"mastersthesis","text":""},{"location":"journals/2022-09-27/","title":"2022-09-27","text":""},{"location":"journals/2022-09-27/#lm-lab-2","title":"LM Lab 2","text":"<ul> <li>Unfortunately, the program we used to create the second experiment, IBEX farms, has shutdown. So it's currently not possible to do the experiment yourself.</li> <li>For lmer output what you need to report are the Estimate (\u03b2)\u00a0, Standard Error (SE), and p-value. You can also add the t-value. So somethin like:\u00a0 'The model showed a significant difference between X and Y\u00a0(\u03b2 = -1.58,\u00a0SE\u00a0= 0.36,\u00a0t= 4.3,\u00a0p &lt; 0.001)'.\u00a0</li> <li>Sometimes, depending on the journal where you are publishing your paper, you may be asked to include the Confidence Interval (CI), especially when you present the results on a Table.\u00a0In case you are curious and want to check the confidence interval, you can run this:\u00a0lsmeans(your model, \"Fixed factor (e.g.Ambiguity)\"). </li> <li>For this assignment, you only need report like the example I have given above.</li> <li>Write the lab report as if you are writing a conference article.</li> <li>Include your name in both in the file name and in the file itself!</li> <li>Re-read Bodo\u00a0Winter, it will be easier now that you need to do lmer\u00a0models</li> <li>check the beginning of the datafile to make sure they don't include fake tests that Jennifer did. The first four subjects may be fake. Make sure you remove them.</li> <li>Outlier Tutorial</li> </ul>"},{"location":"journals/2022-09-28/","title":"LM Lab 2","text":"<ul> <li>Predicate</li> <li>09:31 Adding to the KB, course materials and stuff. Will do this for about an hour or two. Maybe more if my head allows</li> </ul>"},{"location":"journals/2022-09-29/","title":"2022-09-29","text":"<ul> <li>Unfortunately, the program we used to create the second experiment, IBEX farms, has shutdown. So it's currently not possible to do the experiment yourself.</li> <li>For lmer output what you need to report are the Estimate (\u03b2)\u00a0, Standard Error (SE), and p-value. You can also add the t-value. So somethin like:\u00a0 'The model showed a significant difference between X and Y\u00a0(\u03b2 = -1.58,\u00a0SE\u00a0= 0.36,\u00a0t= 4.3,\u00a0p &lt; 0.001)'.\u00a0</li> <li>Sometimes, depending on the journal where you are publishing your paper, you may be asked to include the Confidence Interval (CI), especially when you present the results on a Table.\u00a0In case you are curious and want to check the confidence interval, you can run this:\u00a0lsmeans(your model, \"Fixed factor (e.g.Ambiguity)\"). </li> </ul>"},{"location":"journals/2022-10-03/","title":"2022 10 03","text":"<ul> <li>02:07 Gou, J., Yu, B., Maybank, S. J., &amp; Tao, D. (2021). Knowledge distillation: A survey.\u00a0International Journal of Computer Vision,\u00a0129(6), 1789-1819.</li> <li>Knowledge Distillation</li> <li>03:06 : User models<ul> <li>Models are simplifications of complex systems</li> <li>Filter Bubble Problem</li> <li>Recommender System</li> <li>Humans - Rational inference vs Satisficing Heuristic</li> <li>Declarative memory</li> <li>GOMS</li> </ul> </li> </ul>"},{"location":"journals/2022-10-04/","title":"2022 10 04","text":"<ul> <li>Syntactic Bootstrapping</li> <li>Picky Puppet Method</li> <li>Collectivity, Distributivity, and the Interpretation of Plural Numerical Expressions in Child and Adult Language</li> <li>Evidence For Distributivity Effects in Comprehension</li> <li>Knowledge Distillation</li> </ul>"},{"location":"journals/2022-10-04/#lab-3-language-modeling","title":"Lab 3 Language Modeling","text":"<ul> <li>Since it is an unpublished work,\u00a0the zip file contains an abstract and a presentation of the study done by Boylan\u00a0et al\u00a0(2012), which we replicated in the Mouse tracking study.</li> </ul>"},{"location":"journals/2022-10-05/","title":"2022 10 05","text":"<ul> <li>12:30 Reading some papers today</li> <li>Classifying a specific image region using convolutional nets with an ROI mask as input</li> <li>Embedding Human Knowledge into Deep Neural Network via Attention Map</li> <li>Generalizing Adversarial Explanations with Grad-CAM</li> </ul>"},{"location":"journals/2022-10-06/","title":"2022 10 06","text":"<ul> <li>10:29 Reading some more papers today</li> <li>On the overlap between Grad-CAM saliency maps and explainable visual features in skin cancer images</li> <li>12:15 Wrote another article</li> <li>01:01 Influence of image classification accuracy on saliency map estimation</li> <li>01:44 Wow that was a lot. My brain is so fried</li> </ul>"},{"location":"journals/2022-10-07/","title":"2022 10 07","text":"<ul> <li>12:17 Data Aug today. I almost found something similar to my thesis idea. But still different thankfully. I should really write and send that asap and do more tests.</li> <li>Image Mixing and Deletion</li> </ul>"},{"location":"journals/2022-10-09/","title":"2022 10 09","text":"<ul> <li>11:08 Have to read the paper for LM, and finish this one Image Mixing and Deletion</li> <li>12:22 Okay finished Image Mixing and Deletion</li> <li>03:58 Shortcuts to Quantifier Interpretation in Children and Adults<ul> <li>Wow that was so boring.</li> </ul> </li> </ul>"},{"location":"journals/2022-10-10/","title":"2022 10 10","text":"<ul> <li>10:58 Presentation on Shortcuts to Quantifier Interpretation in Children and Adults</li> </ul>"},{"location":"journals/2022-10-10/#initial-presentation","title":"Initial Presentation","text":"<ul> <li>Eliminate interpretive confound -&gt; locative scenes<ul> <li>locative pictures with animals and other entities shown in containers of various sorts (e.g., bananas in baskets, bears in beds)</li> <li>systematically varied both the syntactic position of the universal quantifier and whether the subject of the sentence referred to the containers or the entities in them</li> </ul> </li> </ul>"},{"location":"journals/2022-10-10/#participants","title":"Participants","text":"<ul> <li>Different from the exp 1</li> <li>Twelve 7-year-olds (M = 7;6, range = 7;1\u20137;10), twelve 8-year-olds (M = 8;6, range = 8;0\u20138;11), and twelve 9-year-olds (M = 9;5, range = 9;0\u20139;10)</li> </ul>"},{"location":"journals/2022-10-10/#materials","title":"Materials","text":"<ul> <li>Focus on locative</li> <li>27 pairs of pictures depicting various entities arranged in containers (e.g., alligators in bathtubs, turtles in tanks, apples in bowls)</li> <li>The pictures showed distributive arrangements with the entities and containers in partial, one-to-one correspondence with each other.</li> </ul>"},{"location":"journals/2022-10-10/#analysis","title":"Analysis","text":"<ul> <li>The main effect of quantifier and all of the interactions involving quantifiers were not significant.</li> <li>Across syntactic constructions, 7-year-olds preferred the picture with the extra animals or objects as opposed to the picture with the extra containers.</li> <li>n contrast, the majority of 9-year-olds correctly varied their picture selections in accordance with the varying syntactic constructions and performed above chance as a group for all sentence types.</li> <li>Experiment 2 replicated one of the main findings of Experiment 1: Only 9-yearolds as a group consistently identified the domain of the universal quantifier and selected the appropriate picture at above-chance levels for distributive events in which sets of objects were in partial, one-to-one correspondence.</li> <li>The observed bias to prefer locative scenes in which all of the containers were filled (the so-called garage-centered bias) has been observed many times; see Drozd (2001) for a review</li> <li>These results are difficult to reconcile with Kang's</li> </ul>"},{"location":"journals/2022-10-10/#general-discussion","title":"General Discussion","text":"<ul> <li>Across all participants, no errors were made on filler sentences indicating that the participants were generally compliant with the task instructions</li> <li>Performance on the task was not at ceiling, with adults making errors on an average of 21% of the trials</li> <li>Our findings that both children and adults make errors in quantifier interpretation are more readily explained by the underspecification account of Sanford and Sturt (2002).</li> <li>The results demonstrate that many school-age children and adults had considerable difficulty in restricting the domain of a universal quantifier, especially when two sets of entities were in partial, one-to-one correspondence. This result contrasts most dramatically with the near perfect performance of preschool children in Crain et al. (1996)</li> <li>This suggests that the problem does not reside in the child's syntax, given the similarities in sentence structures used across studies, but in</li> <li>stead has to do with the difficulty of selecting the appropriate set of entities and avoiding distraction by salient objects.</li> <li>Taken together, the experiments suggest that it was the collective scenes as opposed to the use of all that improved children's performance in Experiment 1. Collective scenes were easier presumably because the group depiction aided the child in isolating one set of entities relative to the other</li> <li>We suspect that both children and adults make errors in comprehension because they engage in shallow processing that causes inaccurate mapping between syntactic and semantic representations.</li> <li>Shallow processing also provides a straightforward explanation of the errors made by adults in Experiment 3. Their high error rates suggest that adult listeners often do not tax their limited information-processing capacities by conducting exhaustive syntactic analyses of sentences but rather make use of simpler strategies in generating reasonable guesses</li> <li>01:29 Backup</li> </ul>"},{"location":"journals/2022-10-10/#final-presentation","title":"Final Presentation","text":""},{"location":"journals/2022-10-10/#exp-2","title":"Exp 2","text":"<ul> <li>Objective : Eliminate interpretive confound for locative scenes</li> <li>Why : Kang(2001) -&gt; better performance when universal quantifier modifies the subject</li> <li>How :<ul> <li>Entities shown in containers (Eg: Animals in bathtubs) (#27)</li> <li>Distributive arrangement, partial, 1-1 correspondence</li> </ul> </li> <li>Participants : 12 each of ( 7, 8, 9) y/o</li> </ul>"},{"location":"journals/2022-10-10/#results","title":"Results","text":"<ul> <li>Both children and adults make errors in quantifier interpretation</li> <li>Only 9 y/o were consistent in identifying the quantifier properly</li> <li>7 y/o preferred pictures with extra animals/objects rather than containers</li> <li>Better performance :<ul> <li>Children : If sentence has quantifier modifying the containers</li> <li>Adults : Distributed between the others</li> </ul> </li> <li>Bias to prefer locative scenes with all filled containers (Drozd (2001)) :<ul> <li>Children : Yes</li> <li>Adults : No bias</li> </ul> </li> </ul>"},{"location":"journals/2022-10-10/#discussion","title":"Discussion","text":"<ul> <li>Replace Truth Value Judgement Task : allows rejection</li> <li>Contrasts with :<ul> <li>Crain et al. (1996) : near perfect performance of preschool children</li> <li>Kang et al. (2001) : better performance when universal quantifier modifies the subject</li> </ul> </li> <li>Problem : not in syntax but in difficulty of selecting the appropriate entity while avoiding distraction by other objects<ul> <li>To solve this problem : Instead of taxing limited processing capacities =&gt; engage in \u201cshallow processing\u201d=&gt; Use simpler strategies aka Shortcutsc</li> </ul> </li> <li>01:34</li> </ul>"},{"location":"journals/2022-10-10/#a-survey-on-image-data-augmentation-for-deep-learning","title":"A survey on Image Data Augmentation for Deep Learning","text":""},{"location":"journals/2022-10-12/","title":"2022 10 12","text":"<ul> <li>Modeling Driver Behavior with Cognitive Architecture</li> <li>The Behavior of Tutoring Systems<ul> <li>This will be super useful for the user models final paper</li> </ul> </li> <li>*04:10 Analysis of Explainers of Black Box Deep Neural Networks for Computer Vision A Survey</li> <li>04:48 A matter of ambiguity? Using eye movements to examine collective vs. distributive interpretations of plural sets</li> </ul>"},{"location":"journals/2022-10-14/","title":"2022 10 14","text":"<ul> <li>SlimStampen</li> <li>Effects of Contextual Cues on Inferring and Remembering Meanings of New Word</li> <li>The Self Organization of Explicit Attitudes</li> </ul>"},{"location":"journals/2022-10-14/#for-user-models-paper","title":"For User Models Paper","text":"<ul> <li>SlimStampen</li> <li>Effects of Contextual Cues on Inferring and Remembering Meanings of New Word</li> <li>The Behavior of Tutoring Systems</li> </ul>"},{"location":"journals/2022-10-14/#for-lm","title":"For LM","text":"<ul> <li>The Self Organization of Explicit Attitudes</li> <li>A matter of ambiguity? Using eye movements to examine collective vs. distributive interpretations of plural sets</li> </ul>"},{"location":"journals/2022-10-15/","title":"2022 10 15","text":""},{"location":"journals/2022-10-15/#for-lm","title":"For LM","text":"<ul> <li>Taking on semantic commitments, II collective versus distributive readings</li> <li>A matter of ambiguity? Using eye movements to examine collective vs. distributive interpretations of plural sets</li> <li>The Self Organization of Explicit Attitudes</li> <li>Tracking the Continuity of Language Comprehension Computer Mouse Trajectories Suggest Parallel Syntactic Processing</li> </ul>"},{"location":"journals/2022-10-16/","title":"2022 10 16","text":""},{"location":"journals/2022-10-16/#lm-lab-3","title":"LM Lab 3","text":""},{"location":"journals/2022-10-16/#background","title":"Background","text":"<ul> <li>Fraiser</li> <li>Minimal Semantic Commitment (MSC) hypothesis</li> <li>Given ambiguous representations, the MSC hypothesis predicts that the processor will commit to one interpretation</li> <li>In an experiment designed to evaluate these hypotheses with respect to the representation of distributivity, participants' eye movements were recorded as they read sentences containing distributive or collective predicates that were either disambiguated by a preceding adverb or left locally ambiguous by delaying the disambiguating adverb until the end of the predicate</li> <li>The results suggested that a semantic commitment is made in locally indeterminate cases as evidenced by a significant interaction of ambiguity and distributivity in first pass times, total times, and regressions</li> <li>Hence we argue that the distributive/collective distinction is treated as a matter of ambiguity rather than as one of vagueness</li> <li>In the absence of evidence for a distributive reading, the processor commits itself to a collective reading sometime during the processing of the predicate (before the disambiguation in our late disambiguation examples)</li> <li>By the MSC hypothesis, this will predict that a premature decision, one made early on the basis of little information, should not occur during immediate processing</li> <li>The point is that no commitment will be made in the absence of supporting evidence</li> <li>) By contrast, given the MSC hypothesis, the notion that a string is grammatically ambiguous predicts that the processor encounters a choice point on initial analysis, adopting one representation rather than another</li> <li>In our experiment we only examined collective/distributive subjects in single clause sentences</li> <li>The ambiguity hypothesis predicts an interaction</li> <li>the vagueness hypothesis doesn't</li> <li>T</li> <li>In order to test for possible pragmatic biases in the predicate, participants were asked to rate the 'naturalness' of the locally ambiguous collective or distributive predicates contained in the experimental sentences</li> <li>when the actual conjoined NP subject in the experimental materials was replaced by a pronominal subject</li> <li>Thirtytwo students native English speakers</li> <li>Each version of the questionnaire contained eight sentences in which the locally ambiguous predicate was subsequently disambiguated towards a distributive interpretation using each and eight sentences in which the predicate was subsequently disambiguated towards a collective interpretation using together</li> <li>7point scale</li> <li>They were told that there were no right or wrong answers and that we were only interested in their opinions of the naturalness of the sentences</li> <li>another rating study was conducted using truncated versions of the locally ambiguous items such that each item ended after the word each or together</li> <li>examined first pass reading times, total reading times, and the pattern of regressions for different regions of the target sentences</li> <li>Sixty undergraduate students bite bar</li> <li> </li> <li>dealt with where readers look during reading and that they should read each sentence for comprehension</li> <li>Sixteen experimental sentences were embedded in 107 filler sentences</li> <li>Each sentence appeared in one of four versions, as in (7) above: in two versions (a and c), the predicate received a distributive interpretation, and in the other two (b and d) the predicate received a collective interpretation</li> <li>region</li> <li>he first region consisted of the words preceding the predicate (a conjoined NP)</li> <li>the second region consisted of the predicate itself</li> <li>the third region comprised the next three words, or next two words if the third word was the last word in the sentence</li> <li>and the fourth region was the remainder of the sentence</li> <li>The fourth region was created solely so that results from the third region would be free of sentence wrapup effects</li> <li>In the first region, none of the effects were significant</li> <li>In the second region, there was an effect of ambiguity, wherein the locally ambiguous versions were read faster than unambiguous versions, possibly due to a preference for adverbs to appear in verbphrase final position</li> <li>he raw reading times analyses and analyses of residuals were consistent in</li> <li>indicating that the interaction did not approach significance</li> <li>In the third region, the different versions of any given sentence were once again identical</li> <li>Here, locally ambiguous versions were read somewhat slower overall than unambiguous versions</li> <li>However, this difference failed to approach significance in the subjects analysis (F1(1,59) = 1</li> <li>91, P \ufffd 0</li> <li>16) and was only marginally significant by items</li> <li>Crucially, each of these marginally significant main effects was qualified by a highly robust interaction between ambiguity and predicate type, suggesting that distributive predicates were read more slowly than collective predicates in the locally ambiguous versions</li> <li>Pairwise comparisons confirmed that while distributives were read more slowly in the ambiguous versions (F1(1,59) = 10</li> <li>84, P \ufffd 0</li> <li>01</li> <li>F2(1,14) =20</li> <li>48, P \ufffd 0</li> <li>001), there was no reliable predicate effect in the unambiguous versions</li> <li>The results of this analysis for first pass times revealed striking differences between ambiguous and unambiguous forms</li> <li>In the residuals analyses, distributives (each) were read slower than collectives (together) in both ambiguous and unambiguous forms</li> <li>However, the effect was much larger in ambiguous forms than unambiguous forms (223 ms vs</li> <li>43 ms), and while in ambiguous forms the effect was significan</li> <li>Distributives were significantly faster than collectives in unambiguous forms (means: 844 ms vs</li> <li>925 ms</li> <li>F1(1,59) = 4</li> <li>72, P \ufffd 0</li> <li>05, F2 (1,14) = 10</li> <li>39, P \ufffd 0</li> <li>01), but significantly slower than collectives in ambiguous forms</li> <li>n the third region, where the different versions were again identical, there was a main effect of ambiguity, with ambiguous versions being read slower overall than unambiguous versions</li> <li>There was also a robust predicate effect (F1(1,59) = 9</li> <li>53, P \ufffd 0</li> <li>01</li> <li>F2(1,14) = 10</li> <li>88, P \ufffd 0</li> <li>01), which indicated that distributives were read more slowly overall</li> <li>In the second region, there were no significant differences across conditions in the percentages of trials on which regressions occurred out of the region (Fs \ufffd 1)</li> <li>In the third region, the mean percentage of trials on which regressions occurred out of the region was 19-, 8-, 4- and 4- for the ambiguousdistributive, ambiguouscollective, unambiguousdistributive, and unambiguouscollective conditions, respectively</li> <li>There were significantly more trials involving regressions out of the third region in the ambiguous than the unambiguous versions</li> <li>The predictions of the vagueness hypothesis were clearly disconfirmed</li> <li>Given the MSC hypothesis, the vagueness hypothesis predicts no interaction between ambiguity and sentence form: in both the ambiguous distributive (7a) and the unambiguous distributive (7c) the processor should postulate a distributive operator when each is encountered</li> <li>ounter to this prediction, the ambiguous distributive form was substantially more difficult to understand than the other sentence forms</li> <li>This may be seen in the significant interaction of ambiguity and sentence form in first pass and total times in region three, as well as in the regressions out of region three and regressions into region two</li> <li>Readers clearly exhibited a preference for the collective reading of the ambiguous portion of the sentences in our experiment</li> <li>These results make it difficult to maintain the assumptions needed to salvage the vagueness hypothesis</li> <li>Instead, given the MSC hypothesis, they support the assumption that the correct grammatical account of collective/distributive differences treats the distinction as one of ambiguity rather than as one of vagueness, at least in cases like those tested, where subjectpredicate relations are involved</li> <li>We turn now to alternative interpretations of our results</li> <li>The question is whether the results can be attributed directly to the complexity of the distributive reading</li> <li>We think not</li> <li>It may be true that distributive readings, even unambiguous ones, are slightly more complex than collective readings</li> <li>This suggests that readers may not simply add information to the current representation of these locally ambiguous forms when each is encountered</li> <li>Similarly the results are difficult to reconcile with a parallel processing view unless the processor has computed both a collective and a distributive representation and then abandoned the distributive representation before each is encountered</li> </ul>"},{"location":"journals/2022-10-16/#experiment","title":"Experiment","text":""},{"location":"journals/2022-10-16/#boylan","title":"Boylan","text":"<ul> <li>Frazier et al. (1999) used eyetracking reading times to compare processing loads of sentences that were explicitly distributive (involving the adverb each), explicitly collective (involving the adverb together), and locally indeterminate at the predicate.</li> <li>Having found evidence for increased processing load associated with distributive sentences, they concluded that the processor initially pursues a collective reading, and thus the distributive / collective distinction was one of ambiguity and not vagueness.</li> <li>However, an increased processing load for the distributive reading might be expected regardless of whether the underlying representation is vague or ambiguous</li> <li>eye movements of listeners were recorded to investigate the representation of collective vs. distributive interpretations of plural subjects in light of the Minimal Semantic Commitment (MSC) hypothesis</li> <li>Rather than relying on processing times to infer representational commitments, we employed the visual world paradigm to track which representations subjects considered over the course of hearing a sentence</li> <li>In a targeted analysis of disambiguator effects in each time window, we found significant differences between together and each sentences and the null and each sentences after the predicate onset but not before</li> <li>This provides evidence that the listener has committed to the collective interpretation in the absence of disambiguating information.</li> <li>This is consistent with a theory that treats the collective / distributive distinction as ambiguous rather than vague.</li> <li>The results also indicate that this processing commitment is essentially immediate; i.e., as soon as listeners begin hearing the ambiguous predicate, they show a preference for the collective interpretation.</li> <li>The Self Organization of Explicit Attitudes</li> <li>implicit biases beyond cognitive control and subjective awareness, yet mental processing may culminate in an explicit attitude that feels personally endorsed and corroborates voluntary intentions</li> <li>While our participants reported their explicit (like vs. dislike) attitudes toward White versus Black people by moving a cursor to a ''like'' or ''dislike'' response box, we recorded streaming xand ycoordinates from their handmovement trajectories</li> <li>participants' handmovement paths exhibited greater curvature toward the ''dislike'' response when they reported positive explicit attitudes toward Black people than when they reported positive explicit attitudes toward White people</li> <li>these trajectories were characterized by movement disorder and competitive velocity profiles that were predicted under the assumption that the deliberate attitudes emerged from continuous interactions between multiple simultaneously conflicting constraints.</li> <li>2 experiments</li> <li>For example, an implicit attitude toward a stimulus can be unintentionally activated by the mere presence of that stimulus.</li> <li>Given that many people demonstrate spontaneous initial biases toward traditionally stigmatized groups, how do they overcome these biases to explicitly report positive attitudes toward the same groups?</li> <li>In early moments of processing, distributed representations are partially consistent with multiple interpretations because of their proximity to multiple neural population codes.</li> <li>However, a continuous accrual of information causes the distributed pattern to dynamically ''sharpen'' into a confident (selected) interpretation, forcing other, partially activated, competing alternative representations, decisions, or actions to gradually die out.</li> <li>The latter attitude will eventually activate other subsystems, such as language and memory, thus making the attitude seem explicit</li> <li>What makes the first attitude implicit is not necessarily that it was generated in a different subsystem, but simply that it did not hold sway long enough to activate those language and memory subsystems.</li> <li>The unfolding cognitive dynamics may be revealed in continuous motor output</li> <li>Because mental processing is recurrent, motor representations begin specifying movement parameters probabilistically, rather than waiting for a perfectly completed cognitive command</li> <li>If the phrase ''Black people'' evokes elevated dynamic competition between simultaneously active ''like'' and ''dislike'' representations, movement trajectories for ''Black people'' should exhibit evidence of nonlinear dynamics in their velocity profiles, as well as increased spatial disorder in the curviness of the trajectories.</li> <li>Streaming xand ycoordinates of mousecursor movements were recorded from 68 Cornell University undergraduates (43 female and 25 male) as they performed a simple explicitattitude task.</li> <li>2 s for participants to view the evaluative response options</li> <li>Participants then clicked on a small box at the bottom of the screen to reveal a stimulus word or phrase and dragged the mouse toward their selected evaluative response to that stimulus</li> <li>Responses to the two stimulus repetitions were averaged together to yield a single measurement for each participant for all statistical analyses.</li> <li>Compared with the trajectories for ''White people,'' the trajectories for ''Black people'' curved significantly more toward the ''dislike'' response option observed differential motor curvatures could have been generated by a stagebased sequence of decisional commands, rather than by continuous motor attraction to the ''dislike'' response.</li> <li>To accommodate the empirical mean trajectory, which initially moved upward rather than actually toward ''dislike,'' such an account would need to predict a bimodal distribution of curvatures that included some trajectories that were very curved and others that were not curved.</li> <li>However, the distribution of trajectory curvatures shows no evidence of bimodality The standard cutoff for inferring bimodality in a distribution is b &gt; 0.55</li> <li>as the more active alternative begins to win the competition, this lateral inhibition is gradually lifted, thus increasing velocity later in processing to produce greater acceleration</li> <li>Moreover, this particular dynamic pattern (reduced early velocity and greater later acceleration) should lead to greater peak velocity, if jerk is minimized as the system achieves equivalent integral under the curve (where the integral represents net change in activation or location)</li> <li>The ''Black people'' trajectories had significantly greater deviation from the sigmoidal fit</li> <li>indicated disorderly variation around the x dimension in those trajectories.</li> <li>our claim is that multiple, partially active mental representations compete for the privilege of driving evaluative responses, imposing a set of response options that are not particularly competitive should change the motor dynamics</li> <li>Seventyone Cornell University undergraduates (37 female and 34 male) were asked to classify stimuli as something they liked (''like'') or disliked (''dislike'')</li> <li>The crucial stimuli in this experiment were ''African Americans'' and ''Caucasians. Results</li> <li>The trajectories for ''African Americans'' curved significantly more toward the ''dislike'' response than the trajectories for ''Caucasians,</li> <li>The motor trajectories evolved over time in accordance with the competitive velocity predictions, as reported in Experiment</li> <li>The ''African Americans'' trajectories, compared with the ''Caucasians'' trajectories, had significantly greater maximum xcoordinate acceleration</li> <li>Moreover, as we found for ''Black people'' trajectories in Experiment 1, the ''African Americans'' trajectories exhibited greater spatial disorder than the ''Caucasians'' trajectories, even after moving toward the ''like'' response, as indicated by significantly greater mean deviation from the sigmoidal</li> <li>Rather, the results suggest that a dynamic competition process may be what allows a single explicit attitude choice to emerge from multiple, potentially conflicting evaluative influences (e.g., Busemeyer &amp; Townsend, 1993; Usher &amp; McClelland, 2003)</li> <li>the mind may host a continuously evolving blend of (implicit) evaluative decisions from which the eventual (explicit) behavioral choice emerges.</li> </ul>"},{"location":"journals/2022-10-16/#tracking-the-continuity-of-language-comprehension-computer-mouse-trajectories-suggest-parallel-syntactic-processing-farmer","title":"Tracking the Continuity of Language Comprehension Computer Mouse Trajectories Suggest Parallel Syntactic Processing - FARMER","text":"<ul> <li>Although several theories of online syntactic processing assume the parallel activation of multiple syntactic representations, evidence supporting simultaneous activation has been inconclusive</li> <li>continuous and nonballistic properties of computer mouse movements are exploited</li> <li>procure evidence regarding parallel versus serial processing</li> <li>Participants heard structurally ambiguous sentences while viewing scenes with properties either supporting or not supporting the difficult modifier interpretation</li> <li>The curvatures of the elicited trajectories revealed both an effect of visual context and graded competition between simultaneously active syntactic representations</li> <li>Sentences such as, \"The adolescent hurried through the door tripped,\" are difficult to process because, at least temporarily, multiple possible structural representations exist</li> <li>gardenpath effect</li> <li>When ambiguous sentences like 1a are heard in the presence of visual scenes where only one possible referent is present (an apple already on a towel), along with an incorrect destination (an empty towel), and a correct destination (a box), as in the top portion of Fig. 1, about 50- of the time participants fixate the incorrect destination after hearing the first PP.</li> <li>After the second disambiguating PP is heard, eye movements tend to be redirected to the correct referent and then to the correct destination</li> <li>This gardenpath effect can, however, be modulated by contextual information contained within the visual scene</li> <li>it seems that when two possible referents are present, an expectation is created that they will be discriminated amongst, thus forcing a modifier interpretation of the ambiguous PP</li> <li>However, because saccadic eye movements are generally ballistic, they either send the eyes to fixate an object associated with a gardenpath interpretation or they do not.</li> <li>The evidence from this paradigm, therefore, is also consistent with the Unrestricted Race model, where the various constraints are combined immediately, but on any given trial only one syntactic representation is initially pursued</li> <li>In the following experiment, we examined the dynamics of hand movement in the same sentence comprehension scenario with the goal of determining whether the nonballistic, continuous nature of computer mouse trajectories can serve to tease apart these two remaining theoretical accounts.</li> <li>In addition, whereas selfpaced reading affords 2 to 3 data points (button presses) per second, and eyemovement data allow for approximately 3 to 4 data points (saccades) per second, \"mouse tracking\" yields somewhere between 30 and 60 data points per second, depending on the sampling rate of the software used.</li> <li>The context and gardenpath effects reported in the visual world paradigm are highly replicable when tracking eye movements</li> <li> </li> <li>1) Averaged trajectories recorded in response to ambiguous sentences in the onereferent context should show significantly more curvature toward the incorrect destination than the averaged trajectories elicited by unambiguous sentences\u2014a pattern corresponding to the gardenpath effect. 2)</li> <li>The curvature of averaged trajectories in the two referent condition should not differ statistically between ambiguous and unambiguous sentences, thus demonstrating an influence of referential context on the gardenpath effect.</li> <li>The second purpose of this study, then, was to exploit the continuity of the mousemovement trajectories to discriminate between these two remaining theoretical accounts</li> <li>If only one representation were active at any one time, as the unrestricted race account predicts, then the trialbytrial distribution of trajectory curvatures in the ambiguoussentence condition should be either (a) bimodal\u2014comprised of highly curved gardenpath movements and noncurved, correctinterpretation movements, or (b) uniformly in the more extreme curved range, indicating that almost every trial exhibited a gardenpath effect</li> <li>Forty righthanded, native Englishspeaking undergraduates from Cornell University participated in the study for extra credit in psychology courses</li> <li>only righthanded individuals to avoid variability associated with subtle kinematic differences in leftward and rightward movement of the left versus the right arms.</li> <li>Sixteen experimental items 102 filler sentences</li> <li>There was no significant main effect or interaction for either the x or the y coordinates (all ps were nonsignificant) indicating that, across conditions, the trajectories were initiated at approximately the same location of the display</li> <li>To determine whether any bimodality is present in the distribution of responses, we computed the area under the curve on a trialbytrial basis</li> <li>Thus, in the presence of the gardenpath effect, it seems clear that there exists more spatial attraction toward the incorrect destination for the ambiguous sentences.</li> <li>The b value for each distribution is less than .555, indicating no presence of bimodality within the distributions.</li> <li>Thus, it would seem that when only one referent was present, the incorrect destination (e.g., the towel) was partially considered relevant, until disambiguating information was processed\u2014a trend corresponding to the gardenpath effect associated with this condition.</li> <li>The fact that most mouse trajectories began while the speech file was still being heard suggests that the effect of visual context modulating the garden path took place during early moments of processing the linguistic input, not during a second stage of syntactic reanalysis.</li> <li>The lack of bimodality in the distribution of trialbytrial trajectory curvatures suggests that the gardenpath effect so frequently associated with this manipulation is not an allornone phenomenon\u2014that is, the activation of one structural representation does not forbid simultaneous activation of other possible representations</li> <li>Unrestricted Race Model</li> <li>The Unrestricted Race model (Traxler, Pickering, &amp; Clifton, 1998; van Gompel, Pickering, Pearson, &amp; Liversedge, 2005; van Gompel, Pickering, &amp; Traxler, 2001) follows in the footsteps of constraintbased models in proposing simultaneous integration of multiple constraints from statistical, semantic, and contextual sources</li> <li>However, rather than ambiguity resolution being based on a temporally dynamic competition process, the Unrestricted Race model posits an instantaneous probabilistic selection among the weighted alternatives of an ambiguity.</li> <li>much like the syntaxfirst models, it must hypothesize a separate reanalysis mechanism that is responsible for gardenpath effects when the initial selected alternative turns out to be syntactically or semantically inappropriate.</li> <li>the Unrestricted Race model predicts that sentences with gardenpaths and sentences without gardenpaths are two separate populations of events</li> <li>In other words, in conditions where mean performance is expected to exhibit a gardenpath effect, there should exist one of two possible patterns: (a) a bimodal distribution of some substantial gardenpath responses and some nongardenpath responses, or (b) practically all trials exhibiting substantial gardenpath effect</li> </ul>"},{"location":"journals/2022-10-16/#expected-prediction","title":"Expected Prediction","text":""},{"location":"journals/2022-10-16/#self-org-of-explicit-woj","title":"Self Org of Explicit Woj","text":""},{"location":"journals/2022-10-16/#abstract","title":"Abstract","text":"<ul> <li>Mouse movements are not ballistic</li> <li>Mousetracking can show us curved movement trajectories can gives us more information about what causes a delay in processing</li> <li>Mouse movements have an advantage in identifying decisions at a very high resolution</li> <li>However, it cannot tell us information about what causes a slow- down in processing</li> <li>Garden Path Sentences</li> <li>Eye-tracking experiment investigating preferences for collective vs.</li> <li>distributive interpretations</li> <li>Response to Frazier et al. (1999)'s results</li> <li>Argue that Frazier et al.'s results reflect processing cost but not necessarily representational commitments.</li> <li>A second study (recall Frazier et al. ) that the collective-distributive distinction is a matter of ambiguity and not vagueness.</li> <li>2x2 study (Picture: Collective vs. Distributive) and Sentence: (With marker (\"each\" or \"together\") or without)</li> <li>Understand what type of data you get out of a mouse tracking experiment with two</li> <li>conditions</li> <li>Use linear mixed effect models to analyze experimental results with a continuous</li> <li>dependent variable \u2013 do lmer's for Reaction Time (RT) data, Maximum Deviation (MD) data and Area under the Curve (AUC) and determine whether or not there was a significant difference between conditions.</li> <li>It was predicted that in the experimental trials, mouse movement trajectories would show evidence of competition between the categories.</li> <li>This competition would be revealed by an analysis of mouse movement trajectory divergence</li> <li>For the analysis of our results we are going to focus on three measures:</li> <li>A. Reaction Time (RT) from Start-click to response click</li> <li>B. Maximum Deviation (MD) of the response curve from an idealized response curve</li> <li>C. Area under the curve (AUC) created by the response curve extending to the idealized response curve</li> </ul>"},{"location":"journals/2022-10-16/#results","title":"Results","text":"<ul> <li>Task 2</li> <li>1 person over 3 : REMOVED</li> <li>mean RTs</li> <li>pirateplot</li> <li>Null is higher. Without marker &gt; higher RTs. (Ambiguous)</li> <li>Collective are lower</li> <li>1 outlier</li> <li>Task 4</li> <li>Bes model : RT_log ~ ambiguity + picture + (1 | subject)</li> <li>QQnorm is yeeted. So remove outliers over 2.5 stddev</li> <li>7 outliers removed</li> <li>0.02302632 - removed</li> <li>AIC : 201.0 &gt; 289.2</li> <li>Fixed effect</li> <li>Adding in Max dev and AUC got us a better model : AIC : 312.66</li> <li>RT_log ~ ambiguity + picture + sum_AUC * sum_MD + (1 | subject)</li> <li>Moderate + core between picture and ambiguity</li> </ul>"},{"location":"journals/2022-10-16/#extras","title":"EXTRAS","text":"<ul> <li>1) Do you expect the commitment towards one of the interpretations (collective or distributive) in the Unambiguous condition?</li> <li>No.</li> <li>2) Do you expect the commitment towards one of the interpretations (collective or distributive) in the Ambiguous condition?</li> <li>Yes. Towards collective.</li> <li>4) Is it correct to remove Incorrect responses for the analysis for this experiment?</li> <li>Since we had a small sample, we will tolerate up to 3 errors per subject.</li> <li>5) Do you expect the responses for one condition to be faster than responses for the other? Which one?</li> <li>The results also indicate that this processing commitment is essentially immediate; i.e., as soon as listeners begin hearing the ambiguous predicate, they show a preference for the collective interpretation.</li> <li> </li> <li>The RT/MD/AUC for NullDis have low values (low in the sense \u2013 smaller AUC, lower MD and faster RT)?</li> <li>NO/NO</li> <li>The RT/MD/AUC for NullColl have high values (high in the sense \u2013 Larger AUC, higher MD and slower RT?</li> <li>NO/NO</li> <li>The RT/MD/AUC for MarkDis have low values</li> <li>YES/YES</li> <li>The RT/MD/AUC for MarkColl have low values</li> <li>YES/YES</li> </ul>"},{"location":"journals/2022-10-16/#what-comparisons-make-the-most-sense-given-your-predictions","title":"What comparisons make the most sense given your predictions?","text":""},{"location":"journals/2022-10-17/","title":"2022 10 17","text":""},{"location":"journals/2022-10-17/#thesis-first-report","title":"Thesis First Report","text":""},{"location":"journals/2022-10-17/#intro","title":"Intro","text":"<ul> <li>Objectives and related stuff</li> <li>Couple of paras</li> </ul>"},{"location":"journals/2022-10-17/#theoretical-framework","title":"Theoretical Framework","text":"<ul> <li>Literature review basically</li> <li>Kinda small ish</li> </ul>"},{"location":"journals/2022-10-17/#research-question","title":"Research Question","text":"<ul> <li>Around 3</li> </ul>"},{"location":"journals/2022-10-17/#methods","title":"Methods","text":"<ul> <li>Dataset</li> <li>Task specific</li> <li>Models</li> <li>Evaluation</li> </ul>"},{"location":"journals/2022-10-17/#scientific-relevance-for-artificial-intelligence","title":"Scientific Relevance for Artificial Intelligence","text":"<ul> <li>Around 1-3 ish</li> <li>Talk about field</li> <li>Automation bla bla</li> <li>Benefit whom</li> </ul>"},{"location":"journals/2022-10-17/#planning","title":"Planning","text":"<ul> <li>Georgios<ul> <li>WP1 Bridge gaps in efficient annotation generation and vision-and-language learning methods by studying literature.</li> <li>WP2 Build OCID* by annotating the OCID dataset.</li> <li>WP3 Generate OCID-like synthetic data in PyBullet.</li> <li>WP4 Evaluate Sim2Real Transfer of baseline model in OCID*</li> <li>WP5 Establish benchmarks for OCID*</li> <li>WP6 Evaluate implemented models in other domains</li> <li>WP7 Develop a visual grounding agent in ROS and perform experiments in a simulated/real HRI scenario</li> <li>WP8 Write thesis and prepare presentation</li> </ul> </li> <li>Tommaso<ul> <li>WP1 Literature study and project overview.</li> <li>WP2 Design model for poses estimation.</li> <li>WP3 Test and tuning poses estimation model.</li> <li>WP4 Baseline of the multi-view architecture.</li> <li>WP5 Test and optimization of the multi-view architecture.</li> <li>WP6 Setup of the architecture to perform in simulated environment.</li> <li>WP7 Test and evaluate performance in simulated environment.</li> <li>WP8 Setup for testing with real robot.</li> <li>WP9 Test and evaluate performance on real robot.</li> <li>WP10 Write report.</li> <li>WP11 Prepare final presentation.</li> </ul> </li> <li>Hari<ul> <li>WP1 Begin Literature Study, Topic and Project Overview.</li> <li>WP2 Continue Literature Study, Project Proposal.</li> <li>WP3 Detailed Literature Analysis, Theoretical Design: System/Architecture, Algorithm.</li> <li>WP4 Simulation Environment Setup, Train and Test Environments, Reward Design.</li> <li>WP 5 Build architectures</li> <li>WP6 Simulation Training, Experimentation, Hyper Parameter Optimization.</li> <li>WP7 Simulation to Real Robot Transfer and Experimentation.</li> <li>WP8 Analyze the Results.</li> <li>WP9 Write Thesis and Prepare for Final Demo/Presentation.</li> </ul> </li> </ul>"},{"location":"journals/2022-10-17/#rubrics","title":"Rubrics","text":""},{"location":"journals/2022-10-19/","title":"2022 10 19","text":"<ul> <li>09:32 Feedback for lab 1</li> <li>Submission Feedback Overall Feedback Subhaditya Mukherjee Graded by Malina Title and abstract:  2/4 Title: Not informative enough. Picture-Verification is not the actual name of the task, \u201ctruth-value judgement\u201d is. You need to mention quantifiers since they are the focus of the study. It\u2019s not common to name the statistical analysis method in the title unless the paper is more about statistics than language modeling. Abstract: You need a brief motivation for the study, should contain the results of the study on 8 y/o, otherwise it\u2019s pointless to compare your results to theirs. Describe your actual results. Method: 3/5 Participants: Non-native speakers are not outliers unless you actually look at their data.  Experimental design: The explanation of fixed factors should also go in this section. You need to explain your dependent variable (the acceptability) and the independent variables (fixed + random factors) better, focusing on what they mean, how the levels/values were chosen, and how they help you answer the research question.  Control conditions are usually discussed in the Materials and Stimuli section, along with examples of pictures and sentences used. Data is usually discussed in the results directly if you conducted your own experiment (this is also the case for you since the data is for the course only). If the data is publicly available, you can discuss it in the method. Procedure: You need a section that focuses on the task itself / what your participants had to do. You can explain in the method why you used mixed effects models and how you select the best model, but the model itself has to be discussed in the Results. Results:4/5 Descriptive statistics: - You don't need the table if you have the RDI plot, which contains the same info + extra info about the distribution. If you want a table, maybe add some distribution statistics like the standard deviation, etc. Your explanation of the RDI plot is too vague and general. The distribution of no-each distributive condition actually suggests that you have a sizeable subpopulation that accepts distributive readings of sentences without each, but there is also a population which doesn't. People seem to be quite divided on this, not many people are in the middle (which would indicate that most people randomly accept distributive sentences without each, without having a clear preference).  Model: No need to explain what * and + mean for models, if you mention the optimizer cite it in the text correctly \\cite{...}. Model selection is not very clear: did you incrementally add or remove factors? Which structure did you fit first? The random factors or the fixed factors? How did you compare models with very similar AIC scores? You don\u2019t need to report the exact AIC score. Inferential statistics: explain what the model predicts briefly. How does the interaction between picture and sentence influence acceptability? What does it mean for the sentence to be a slope for ID? Good post-hoc test and explanation! You need sth like this for the model also! Appendix: 1/2 The table with the best model coefficients should go here, specify the model formula in the caption.  Even though the R output for models combines variable names and values, your table should be more readable, so spell them out properly. You don\u2019t need to include all models tried. Edited well: 1/2 When you explain a figure/table give the number. You can use \\label{img1} for an image and then reference it in the text as Figure ???  and it will automatically fill in the number of the figure.  The model formula is written twice. Many of the plot labels are too small to read, and the post hoc plot is too small to understand. Clarity: 1/2 - some explanations are not clear Total: 13/20</li> </ul>"},{"location":"journals/2022-10-23/","title":"2022 10 23","text":"<ul> <li>LM paper things</li> <li>Final Paper Language Modeling</li> <li>04:56 SLAK</li> </ul>"},{"location":"journals/2022-10-24/","title":"2022 10 24","text":"<ul> <li>05:14 Final Paper Language Modeling</li> </ul>"},{"location":"journals/2022-10-25/","title":"2022 10 25","text":"<ul> <li>11:32 Writing the Obsidian Article</li> <li>12:08 Saving screenshots</li> <li>03:09 Final Paper User Models</li> <li>04:18 Final Paper Language Modeling</li> </ul>"},{"location":"journals/2022-10-26/","title":"2022 10 26","text":"<ul> <li>Inverse Reinforcement Learning</li> </ul>"},{"location":"journals/2022-11-03/","title":"2022 11 03","text":"<ul> <li>11:27 Dont have too many urgent things today, so just doing some timepass for a while and will probably update the KB, read a book or something, work on the report idk</li> <li>12:00 Language modeling</li> <li>The Differentiation Condition</li> <li>Lexically Distributive</li> <li>Lexically Collective</li> <li>Universal Quantifiers</li> <li>Psycholinguistics</li> <li>2 X 2 Study</li> <li>Random Factors</li> <li>ANOVA</li> <li>Fixed Factors</li> <li>Mixed Effect Models</li> <li>Numerically Quantified Expressions</li> <li>Cardinality Principle</li> <li>12:24 Robotics</li> <li>Occlusion</li> <li>RANSAC</li> <li>Iterative Closest Point</li> <li>Kalman Filter</li> <li>Particle Filter</li> <li>Ensemble of Shape Functions</li> <li>Viewpoint Feature Histogram</li> <li>Local Descriptor</li> <li>Bag of Words robotics</li> <li>Local-LDA Object Representation</li> <li>MVCNN</li> <li>OrthographicNet</li> <li>Local Reference Frame</li> <li>Fine-grained Object Recognition</li> <li>Opportunistic Learning</li> <li>Instance-based Learning</li> <li>Global Classification Accuracy</li> <li>Average Number of Stored Instances per Category</li> <li>Phases of Simulated User Experiments</li> <li>Eye-to-hand System</li> <li>Sense-Plan-Act Model</li> <li>The Repulsive Potential</li> <li>Trajectory Planning</li> <li>Quadratic Potential Field</li> <li>The Repulsive Potential</li> <li>[[Task (endeffector) Space Vs. Joint Space]]</li> <li>Joint Velocity</li> <li>Trapezoidal Trajectory</li> <li>Polynomial Trajectories</li> <li>Semantic Data</li> <li>Perceptual Messages</li> <li>Perception Component</li> <li>Reasoning Component</li> <li>Learning Component</li> <li>Action Component</li> <li>Dual-memory Approach</li> <li>Forgetting</li> </ul>"},{"location":"journals/2022-11-04/","title":"2022 11 04","text":"<ul> <li>Contributions of Shape, Texture, and Color in Visual Recognition Abstract</li> </ul>"},{"location":"journals/2022-11-06/","title":"2022 11 06","text":"<ul> <li>01:48 I also decided to make a tag for articles that I write so I can have them in the same place.</li> <li>../articles/Notes on a Nervous Planet- take control</li> </ul>"},{"location":"journals/2022-11-14/","title":"2022 11 14","text":"<ul> <li>01:01 Cognitive Modeling<ul> <li>Memory, TIme perception , Motivation</li> <li> Anderson et al</li> <li>ACT-R</li> <li> Alan Newell - twenty questiosn with nature</li> <li>Criteria<ul> <li>Functional - behavior appropriate to the task</li> <li>Behavorial - same way as humans</li> <li>Neurophysiological - faithful to brain</li> </ul> </li> <li>Course - Declarative memory , [[Time perception]]</li> <li>Sugar Factory Task</li> </ul> </li> </ul>"},{"location":"journals/2022-11-17/","title":"2022-11-17","text":"<ul> <li>11:49 Starting with cognitive modeling. Will later go to understanding CAM and stuff and maybe fixing the website. We will see.</li> <li>You can play 20 questions with nature and win</li> </ul>"},{"location":"journals/2022-11-21/","title":"2022-11-21","text":"<ul> <li>01:25 Done with a bunch of stuff. Exp for cognitive modeling, emailed for a second supervisor and filled a form for some content writing work.</li> <li>Now have to grade final assignments for Cogrob<ul> <li>Identify gaps in literature</li> <li>Proper survey paper</li> <li>Ordered properly</li> </ul> </li> </ul>"},{"location":"journals/2022-11-24/","title":"2022 11 24","text":"<ul> <li>01:26Network Dissection Quantifying Interpretability of Deep Visual Representions</li> </ul>"},{"location":"journals/2022-11-25/","title":"2022 11 25","text":"<ul> <li>01:12 Did some more grading</li> <li>01:12 </li> </ul>"},{"location":"journals/2022-11-28/","title":"2022 11 28","text":"<ul> <li>Real Time Image Saliency for Black Box Classifiers</li> <li>Dikes and Rivers</li> <li>Cognitive Preparation</li> </ul>"},{"location":"journals/2022-11-28/#week-3-lab-cogmod","title":"Week 3 Lab Cogmod","text":"<ul> <li>Foreperiod in ms</li> <li>1/RT is normally distributed</li> <li>Effects : FP, reward, target visibility</li> <li>3 FP x 2 re x 2 tv</li> <li>lmer : \\(\\(1/RT \\sim fp*reward*tv\\)\\)</li> <li>Effect of FP<ul> <li>Faster if FP is longer</li> </ul> </li> <li>Effect of reward</li> <li>Faster if TV is easy to see</li> <li>Interaction of TV : FP<ul> <li>Preparation related to TV</li> </ul> </li> <li>If longer to prepare : more errors (longer FP)<ul> <li>RT might not always be a benefit</li> </ul> </li> <li>If low visibility + reward + prepared : bad performance<ul> <li>not a lot of effect if no reward</li> </ul> </li> <li>Effect of memory<ul> <li>Effects of duration of prev trials</li> <li>Always memory search?</li> </ul> </li> <li>Reward<ul> <li>Not searching back memory for too many trials</li> </ul> </li> </ul>"},{"location":"journals/2022-11-28/#question","title":"Question","text":"<ul> <li>This assignment is a generalization of the previous one, for which you will need declarative memory in addition to time perception.</li> <li>You will model data from an experiment by Kruine et al. (in prep), which has a design very similar to that of Los et al., (2021) (See literature in the course overview, and see the slides of this week). One pattern of results from this experiment is very clear and is an effect that has been repeated throughout the literature.</li> <li>Participants performed \u00b1896 trials of a task that was very similar to the experiment you have done: after a warning stimulus, there was a foreperiod, after which participants had to generate a response to a target on the left or on the right. The foreperiod varied 'uniformly', and was equally likely to be either 400ms, 800ms, 1200ms or 1600ms. Of particular interest here is to what extent RT on the current trial, is affected by the FP on the current trial, as well as on the previous trial. The results are shown in the following graph, where the x-axis shows the current foreperiod, and there are graphs with different colors for each of the previous foreperiods. The exact numbers are in the attached file.</li> <li>![[Pasted image 20221129165746.png]]</li> <li>(Note that the labels in this graph are off by 5ms. This has to do with technical details involving the implementation of the experiment and the refresh rate of the monitors. You can ignore this discrepancy, and assume FPs of 400, 800, 1200 &amp; 1600).</li> <li>You can see in this graph, for example, that a foreperiod of 400 ms that follows a foreperiod of 1600 ms produces the longest RT.</li> <li>Your task is to model these data. You can use the same idea as for the Grosjean data, except that you will need to use ACT-R's declarative memory to store experiences on previous trials throughout the experiment. I also attach the best fit I could manage, which can certainly still be improved.</li> <li>![[Pasted image 20221129165754.png]]</li> <li>Here are some tips:</li> <li>Between the response and the S1 (the warning stimulus) there was a randomly jittered inter-trial interval of 1.0 - 1.5s</li> <li>At the end of each trial I - 02:04 stored an estimate of the foreperiod in memory as a number of pulses that the model perceived (so no merging of similar experiences)</li> <li>At the start of a trial, I retrieved the chunk with the highest activation to determine the model's estimate for the foreperiod. I used regular retrieval, so no partial matching or blending</li> <li>I tweaked the activation noise to optimize the fit</li> <li>This model's fit can probably be improved, by making different assumptions about preparation or different assumption how we handle stored experiences. This will be a nice topic for discussion in the rest of course, as the same thing will come up when you are modeling the data from our own experiment.</li> <li>Submit</li> <li>Write your code in a Jupyter notebook and submit both the notebook (.ipynb file) and a PDF version of the notebook. Start the notebook with a few paragraphs in which you describe how your model works, and how it can explain the data. Furthermore, where necessary, use comments to explain your code. Make sure to include your name and student number in the file name and at the top of the notebook!</li> <li>Assessment</li> <li>This assignment is worth 2 points.</li> </ul>"},{"location":"journals/2022-11-28/#tips","title":"Tips","text":"<ul> <li>Yeah I think we just lower the 410 to something that better matches the graphs in the assignment</li> <li>Here it\u2019s important to be aware of the difference between response time and reaction time.We\u2019re looking at response times here. We\u2019re not looking at a simple reaction to a stimulus, but we expect our participant to process a stimulus, make a decision and then press the right button. This means that the response time can be very dependent on the stimulus and the processing required.Point is: maybe this stimulus is faster to classify than the one for Grosjean, so it\u2019s maximum response time might be quicker</li> <li>So we can still assume that the difference between ready and not ready adds up to 0.05 seconds, but the 0.41 seconds is something to tweak to best fit the data? Thanks for your reply</li> </ul>"},{"location":"journals/2022-11-28/#backlinks","title":"Backlinks","text":"<ul> <li>2022-11-28</li> </ul> <p>Backlinks last generated 2023-01-28 13:11:06</p>"},{"location":"journals/2022-12-01/","title":"2022 12 01","text":"<ul> <li>09:22 Working on the Cogmod lab</li> <li>2022-11-28</li> <li>09:55 Content writing</li> </ul>"},{"location":"journals/2022-12-03/","title":"2022 12 03","text":"<ul> <li>05:16 So the best thing is I realized I have a Mac now and can use iCloud to use obsidian on the iPad.<ul> <li>Im currently writing this note on the iPad. It's not perfect but it gets the job done :)</li> <li>The only caveat is that I cant get github to work I guess. Although I could manually push the files every once in a while.</li> </ul> </li> <li>05:17 Anyway Beware of Inmates Running the Asylum</li> <li>05:24 Lets see if this shows up on the Ipad</li> <li>Aaand it does!!!!!!!!</li> </ul>"},{"location":"journals/2022-12-05/","title":"2022 12 05","text":"<ul> <li>01:13 Cognition Hazard Rates</li> <li>02:24 The Reward Experiment</li> <li>02:37 CogMod Final Paper</li> </ul>"},{"location":"journals/2022-12-06/","title":"2022 12 06","text":"<ul> <li>12:01 Vision Explainibility</li> <li>No of pixels removed vs metric</li> <li>replacement for attention using gradient based</li> <li>gradient - likes</li> <li>tabular</li> <li>04:33 Word2vec with gensim</li> </ul>"},{"location":"journals/2022-12-07/","title":"2022 12 07","text":"<ul> <li>10:22 Just working on getting content for blogs I need to write</li> <li>Building the word2vec model using gensim<ul> <li>Word2Vec</li> <li>gensim</li> <li>medium article</li> <li>radimrehurek blog</li> <li>illustrated word2vec</li> <li>tensorflow tuts</li> </ul> </li> <li>Masked Language Modeling in BERT</li> </ul>"},{"location":"journals/2022-12-09/","title":"2022 12 09","text":"<ul> <li>11:05 Bunch of things today. First I have a thesis presentation Vision Explainibility, then an article on Masked Language Modeling and then Cogmod</li> </ul>"},{"location":"journals/2022-12-11/","title":"2022 12 11","text":"<ul> <li>12:57 Have to write an article about CycleGAN</li> </ul>"},{"location":"journals/2022-12-12/","title":"2022 12 12","text":"<ul> <li>01:07 Cognitive Preparation</li> </ul>"},{"location":"journals/2022-12-13/","title":"2022 12 13","text":"<p>tags: temp date created: Tuesday, December 13th 2022, 10:53:22 am date modified: Tuesday, December 13th 2022, 4:54:10 pm</p> <ul> <li>04:54 I have some more articles to write. But I do have quite a bit of time to do them. For now Im waiting for them to announce my flight so I just wanted to organize the things that I need</li> <li>I decided to put all my articles for scalar in Scalar Articles</li> </ul>"},{"location":"journals/2022-12-15/","title":"2022 12 15","text":"<ul> <li>DCGAN</li> </ul>"},{"location":"journals/2022-12-18/","title":"2022 12 18","text":"<ul> <li>02:04 Conditional GAN</li> </ul>"},{"location":"journals/2022-12-21/","title":"2022-12-21","text":"<ul> <li>02:47 man these articles are all the same arent they. Scalar Articles</li> </ul>"},{"location":"journals/2023-01-09/","title":"2023 01 09","text":"<ul> <li>one model doing multiple things vs combine many models modeling different effects</li> </ul>"},{"location":"journals/2023-01-13/","title":"2023 01 13","text":"<ul> <li>Fine grained datasets</li> </ul>"},{"location":"journals/2023-01-18/","title":"2023-01-18","text":"<ul> <li>Building Ethics into Artificial Intelligence</li> </ul>"},{"location":"journals/2023-01-27/","title":"2023-01-27","text":"<ul> <li>10:51 Attempting to figure out what papers to read for the ../CogMod Final Paper</li> </ul>"},{"location":"journals/2023-01-28/","title":"2023 01 28","text":""},{"location":"journals/2023-01-28/#-","title":"---","text":""},{"location":"read_later/Deep%20Neural%20Nets%2033%20years%20ago%20and%2033%20years%20from%20now/","title":"Deep Neural Nets 33 years ago and 33 years from now","text":"<p>The Yann LeCun et al. (1989) paper Backpropagation Applied to Handwritten Zip Code Recognition is I believe of some historical significance because it is, to my knowledge, the earliest real-world application of a neural net trained end-to-end with backpropagation. Except for the tiny dataset (7291 16x16 grayscale images of digits) and the tiny neural network used (only 1,000 neurons), this paper reads remarkably modern today, 33 years later - it lays out a dataset, describes the neural net architecture, loss function, optimization, and reports the experimental classification error rates over training and test sets. It\u2019s all very recognizable and type checks as a modern deep learning paper, except it is from 33 years ago. So I set out to reproduce the paper 1) for fun, but 2) to use the exercise as a case study on the nature of progress in deep learning.</p> <p></p> <p>Implementation. I tried to follow the paper as close as possible and re-implemented everything in PyTorch in this karpathy/lecun1989-repro github repo. The original network was implemented in Lisp using the Bottou and LeCun 1988 backpropagation simulator SN (later named Lush). The paper is in french so I can\u2019t super read it, but from the syntax it looks like you can specify neural nets using higher-level API similar to what you\u2019d do in something like PyTorch today. As a quick note on software design, modern libraries have adopted a design that splits into 3 components: 1) a fast (C/CUDA) general Tensor library that implements basic mathematical operations over multi-dimensional tensors, and 2) an autograd engine that tracks the forward compute graph and can generate operations for the backward pass, and 3) a scriptable (Python) deep-learning-aware, high-level API of common deep learning operations, layers, architectures, optimizers, loss functions, etc.</p> <p>Training. During the course of training we have to make 23 passes over the training set of 7291 examples, for a total of 167,693 presentations of (example, label) to the neural network. The original network trained for 3 days on a SUN-4/260 workstation. I ran my implementation on my MacBook Air (M1) CPU, which crunched through it in about 90 seconds (~3000X naive speedup). My conda is setup to use the native arm64 builds, rather than Rosetta emulation. The speedup may have been more dramatic if PyTorch had support for the full capability of the M1 (including the GPU and the NPU), but this seems to still be in development. I also tried naively running the code on an A100 GPU, but the training was actually slower, most likely because the network is so tiny (4 layer convnet with up to 12 channels, total of 9760 params, 64K MACs, 1K activations), and the SGD uses only a single example at a time. That said, if one really wanted to crush this problem with modern hardware (A100) and software infrastructure (CUDA, PyTorch), we\u2019d need to trade per-example SGD for full-batch training to maximize GPU utilization and most likely achieve another ~100X speedup of training latency.</p> <p>Reproducing 1989 performance. The original paper reports the following results:</p> <pre><code>eval: split train. loss 2.5e-3. error 0.14%. misses: 10\neval: split test . loss 1.8e-2. error 5.00%. misses: 102\n</code></pre> <p>While my training script repro.py in its current form prints at the end of the 23rd pass:</p> <pre><code>eval: split train. loss 4.073383e-03. error 0.62%. misses: 45\neval: split test . loss 2.838382e-02. error 4.09%. misses: 82\n</code></pre> <p>So I am reproducing the numbers roughly, but not exactly. Sadly, an exact reproduction is most likely not possible because the original dataset has, I believe, been lost to time. Instead, I had to simulate it using the larger MNIST dataset (hah never thought I\u2019d say that) by taking its 28x28 digits, scaling them down to 16x16 pixels with bilinear interpolation, and randomly without replacement drawing the correct number of training and test set examples from it. But I am sure there are other culprits at play. For example, the paper is a bit too abstract in its description of the weight initialization scheme, and I suspect that there are some formatting errors in the pdf file that, for example, erase dots \u201c.\u201d, making \u201c2.5\u201d look like like \u201c2 5\u201d, and potentially (I think?) erasing square roots. E.g. we\u2019re told that the weight init is drawn from uniform \u201c2 4 / F\u201d where F is the fan-in, but I am guessing this surely (?) means \u201c2.4 / sqrt(F)\u201d, where the sqrt helps preserve the standard deviation of outputs. The specific sparse connectivity structure between the H1 and H2 layers of the net are also brushed over, the paper just says it is \u201cchosen according to a scheme that will not be discussed here\u201d, so I had to make some some sensible guesses here with an overlapping block sparse structure. The paper also claims to use tanh non-linearity, but I am worried this may have actually been the \u201cnormalized tanh\u201d that maps ntanh(1) = 1, and potentially with an added scaled-down skip connection, which was trendy at the time to ensure there is at least a bit of gradient in the flat tails of the tanh. Lastly, the paper uses a \u201cspecial version of Newton\u2019s algorithm that uses a positive, diagonal approximation of Hessian\u201d, but I only used SGD because it is significantly simpler and, according to the paper, \u201cthis algorithm is not believed to bring a tremendous increase in learning speed\u201d.</p> <p>Cheating with time travel. Around this point came my favorite part. We are living here 33 years in the future and deep learning is a highly active area of research. How much can we improve on the original result using our modern understanding and 33 years of R&amp;D? My original result was:</p> <pre><code>eval: split train. loss 4.073383e-03. error 0.62%. misses: 45\neval: split test . loss 2.838382e-02. error 4.09%. misses: 82\n</code></pre> <p>The first thing I was a bit sketched out about is that we are doing simple classification into 10 categories, but at the time this was modeled as a mean squared error (MSE) regression into targets -1 (for negative class) or +1 (for positive class), with output neurons that also had the tanh non-linearity. So I deleted the tanh on output layers to get class logits and swapped in the standard (multiclass) cross entropy loss function. This change dramatically improved the training error, completely overfitting the training set:</p> <pre><code>eval: split train. loss 9.536698e-06. error 0.00%. misses: 0\neval: split test . loss 9.536698e-06. error 4.38%. misses: 87\n</code></pre> <p>I suspect one has to be much more careful with weight initialization details if your output layer has the (saturating) tanh non-linearity and an MSE error on top of it. Next, in my experience a very finely-tuned SGD can work very well, but the modern Adam optimizer (learning rate of 3e-4, of course :)) is almost always a strong baseline and needs little to no tuning. So to improve my confidence that optimization was not holding back performance, I switched to AdamW with LR 3e-4, and decay it down to 1e-4 over the course of training, giving:</p> <pre><code>eval: split train. loss 0.000000e+00. error 0.00%. misses: 0\neval: split test . loss 0.000000e+00. error 3.59%. misses: 72\n</code></pre> <p>This gave a slightly improved result on top of SGD, except we also have to remember that a little bit of weight decay came in for the ride as well via the default parameters, which helps fight the overfitting situation. As we are still heavily overfitting, next I introduced a simple data augmentation strategy where I shift the input images by up to 1 pixel horizontally or vertically. However, because this simulates an increase in the size of the dataset, I also had to increase the number of passes from 23 to 60 (I verified that just naively increasing passes in original setting did not substantially improve results):</p> <pre><code>eval: split train. loss 8.780676e-04. error 1.70%. misses: 123\neval: split test . loss 8.780676e-04. error 2.19%. misses: 43\n</code></pre> <p>As can be seen in the test error, that helped quite a bit! Data augmentation is a fairly simple and very standard concept used to fight overfitting, but I didn\u2019t see it mentioned in the 1989 paper, perhaps it was a more recent innovation (?). Since we are still overfitting a bit, I reached for another modern tool in the toolbox, Dropout. I added a weak dropout of 0.25 just before the layer with the largest number of parameters (H3). Because dropout sets activations to zero, it doesn\u2019t make as much sense to use it with tanh that has an active range of [-1,1], so I swapped all non-linearities to the much simpler ReLU activation function as well. Because dropout introduces even more noise during training, we also have to train longer, bumping up to 80 passes, but giving:</p> <pre><code>eval: split train. loss 2.601336e-03. error 1.47%. misses: 106\neval: split test . loss 2.601336e-03. error 1.59%. misses: 32\n</code></pre> <p>Which brings us down to only 32 / 2007 mistakes on the test set! I verified that just swapping tanh -&gt; relu in the original network did not give substantial gains, so most of the improvement here is coming from the addition of dropout. In summary, if I time traveled to 1989 I\u2019d be able to cut the rate of errors by about 60%, taking us from ~80 to ~30 mistakes, and an overall error rate of ~1.5% on the test set. This gain did not come completely free because we also almost 4X\u2019d the training time, which would have increased the 1989 training time from 3 days to almost 12. But the inference latency would not have been impacted. The remaining errors are here:</p> <p></p> <p>Going further. However, after swapping MSE -&gt; Softmax, SGD -&gt; AdamW, adding data augmentation, dropout, and swapping tanh -&gt; relu I\u2019ve started to taper out on the low hanging fruit of ideas. I tried a few more things (e.g. weight normalization), but did not get substantially better results. I also tried to miniaturize a Visual Transformer (ViT)) into a \u201cmicro-ViT\u201d that roughly matches the number of parameters and flops, but couldn\u2019t match the performance of a convnet. Of course, many other innovations have been made in the last 33 years, but many of them (e.g. residual connections, layer/batch normalizations) only become relevant in much larger models, and mostly help stabilize large-scale optimization. Further gains at this point would likely have to come from scaling up the size of the network, but this would bloat the test-time inference latency.</p> <p>Cheating with data. Another approach to improving the performance would have been to scale up the dataset, though this would come at a dollar cost of labeling. Our original reproduction baseline, again for reference, was:</p> <pre><code>eval: split train. loss 4.073383e-03. error 0.62%. misses: 45\neval: split test . loss 2.838382e-02. error 4.09%. misses: 82\n</code></pre> <p>Using the fact that we have all of MNIST available to us, we can simply try scaling up the training set by ~7X (7,291 to 50,000 examples). Leaving the baseline training running for 100 passes already shows some improvement from the added data alone:</p> <pre><code>eval: split train. loss 1.305315e-02. error 2.03%. misses: 60\neval: split test . loss 1.943992e-02. error 2.74%. misses: 54\n</code></pre> <p>But further combining this with the innovations of modern knowledge (described in the previous section) gives the best performance yet:</p> <pre><code>eval: split train. loss 3.238392e-04. error 1.07%. misses: 31\neval: split test . loss 3.238392e-04. error 1.25%. misses: 24\n</code></pre> <p>In summary, simply scaling up the dataset in 1989 would have been an effective way to drive up the performance of the system, at no cost to inference latency.</p> <p>Reflections. Let\u2019s summarize what we\u2019ve learned as a 2022 time traveler examining state of the art 1989 deep learning tech:</p> <ul> <li>First of all, not much has changed in 33 years on the macro level. We\u2019re still setting up differentiable neural net architectures made of layers of neurons and optimizing them end-to-end with backpropagation and stochastic gradient descent. Everything reads remarkably familiar, except it is smaller.</li> <li>The dataset is a baby by today\u2019s standards: The training set is just 7291 16x16 greyscale images. Today\u2019s vision datasets typically contain a few hundred million high-resolution color images from the web (e.g. Google has JFT-300M, OpenAI CLIP was trained on a 400M), but grow to as large as a small few billion. This is approx. ~1000X pixel information per image (3843843/(16*16)) times 100,000X the number of images (1e9/1e4), for a rough 100,000,000X more pixel data at the input.</li> <li>The neural net is also a baby: This 1989 net has approx. 9760 params, 64K MACs, and 1K activations. Modern (vision) neural nets are on the scale of small few billion parameters (1,000,000X) and O(~1e12) MACs (~10,000,000X). Natural language models can reach into trillions of parameters.</li> <li>A state of the art classifier that took 3 days to train on a workstation now trains in 90 seconds on my fanless laptop (3,000X naive speedup), and further ~100X gains are very likely possible by switching to full-batch optimization and utilizing a GPU.</li> <li>I was, in fact, able to tune the model, augmentation, loss function, and the optimization based on modern R&amp;D innovations to cut down the error rate by 60%, while keeping the dataset and the test-time latency of the model unchanged.</li> <li>Modest gains were attainable just by scaling up the dataset alone.</li> <li>Further significant gains would likely have to come from a larger model, which would require more compute, and additional R&amp;D to help stabilize the training at increasing scales. In particular, if I was transported to 1989, I would have ultimately become upper-bounded in my ability to further improve the system without a bigger computer.</li> </ul> <p>Suppose that the lessons of this exercise remain invariant in time. What does that imply about deep learning of 2022? What would a time traveler from 2055 think about the performance of current networks?</p> <ul> <li>2055 neural nets are basically the same as 2022 neural nets on the macro level, except bigger.</li> <li>Our datasets and models today look like a joke. Both are somewhere around 10,000,000X larger.</li> <li>One can train 2022 state of the art models in ~1 minute by training naively on their personal computing device as a weekend fun project.</li> <li>Today\u2019s models are not optimally formulated, and just changing some of the details of the model, loss function, augmentation or the optimizer we can about halve the error.</li> <li>Our datasets are too small, and modest gains would come from scaling up the dataset alone.</li> <li>Further gains are actually not possible without expanding the computing infrastructure and investing into some R&amp;D on effectively training models on that scale.</li> </ul> <p>But the most important trend I want to comment on is that the whole setting of training a neural network from scratch on some target task (like digit recognition) is quickly becoming outdated due to finetuning, especially with the emergence of foundation models like GPT. These foundation models are trained by only a few institutions with substantial computing resources, and most applications are achieved via lightweight finetuning of part of the network, prompt engineering, or an optional step of data or model distillation into smaller, special-purpose inference networks. I think we should expect this trend to be very much alive, and indeed, intensify. In its most extreme extrapolation, you will not want to train any neural networks at all. In 2055, you will ask a 10,000,000X-sized neural net megabrain to perform some task by speaking (or thinking) to it in English. And if you ask nicely enough, it will oblige. Yes you could train a neural net too\u2026 but why would you?</p>"}]}