---
title: Additive Attention

tags: architecture 
---

# Additive Attention
- [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf)
- Uses a one layer feedforward network to calculate [[Attention Alignment]]
- Oh, basically it is the same as [Bahdanau Attention](Bahdanau%20Attention.md)


## Backlinks

> - [Attention](Attention.md)
>   - [[Additive Attention]]

_Backlinks last generated 2022-06-21 17:08:56_
