---
title: Attention Based Distillation

tags: knowledgedistillation 
date modified: Monday, October 10th 2022, 2:02:09 pm
date created: Tuesday, October 4th 2022, 11:51:17 am
---

# [Attention](Attention.md) Based Distillation
- That is to say, knowledge about feature embedding is transferred using [attention](Attention.md) map functions. Unlike the [attention](Attention.md) maps, a different attentive knowledge distillation method was proposed by Song et al. (2018). An [attention](Attention.md) mechanism is used to assign different confidence rules (Song et al., 2018).

