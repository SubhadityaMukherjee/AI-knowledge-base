---
tags: temp
title: Linear Learning Rate Scaling
date modified: Monday, October 10th 2022, 2:02:23 pm
date created: Tuesday, July 26th 2022, 8:33:15 pm
---

# Linear Learning Rate Scaling
- If [[Initialization|He [Initialization]] ]] is used, 0.1 is a good learning rate for batch size 256 and for a larger b, $0.1\times\frac{\mathrm{b}}{256}$ is okay

## Backlinks
> - [Learning Rate [[Scheduling]]](Learning Rate Scheduling.md)
>   - [[Linear Learning Rate Scaling]]

_Backlinks last generated 2022-10-04 13:01:19_
