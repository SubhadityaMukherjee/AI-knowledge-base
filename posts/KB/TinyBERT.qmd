---
title: TinyBERT

categories: ['architecture']
date modified: Monday, October 10th 2022, 2:02:15 pm
date created: Tuesday, July 26th 2022, 8:33:15 pm
---

# TinyBERT
- [TinyBERT: Distilling BERT for Natural Language Understanding](https://arxiv.org/abs/1909.10351)
- novel Transformer distillation method to accelerate inference and reduce model size while maintaining accuracy
- specially designed for [knowledge distillation](Knowledge%20Distillation.qmd) (KD) of the Transformer-based models
- plenty of knowledge encoded in a large teacher BERT can be effectively transferred to a small student Tiny-BERT
- [GLUE](GLUE.qmd)



