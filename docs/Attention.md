---
title: Attention
tags: architecture
---

# Attention
- Model can decide where to look in the input

- [[Additive Attention]]
- [[Dot Product Attention]]
- [[Location Aware Attention]]
- [[Relative Multi Head Self Attention]]

- [[Soft Attention]]

- [[Scaled Dot Product Attention]]

- [[Encoder Decoder Attention]]

- [[Self Attention]]

- [[Multi Head Attention]]


