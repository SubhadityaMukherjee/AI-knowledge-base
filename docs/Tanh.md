---
title: Tanh
tags: architecture 
date modified: Monday, October 10th 2022, 2:02:15 pm
date created: Tuesday, July 26th 2022, 8:33:15 pm
---

# Tanh
- $$\frac{e^x-e^{-x}}{e^x+e^{-x}}$$
- RNN : Hidden
- Xavier/Glorot init
- ![[assets/Pasted image 20220626151651.jpg]]

## Backlinks
> - [[[visualization|Visualization]] Of Layers](Visualization Of Layers.md)
>   - [[Tanh]]
>
> - [Xavier [[Initialization]]](Xavier Initialization.md)
>   - For [[Tanh]] based activating neural nets
>
> - [Squared Hinge](Squared Hinge.md)
>   - [[Tanh]] for last layer
>
> - [Lisht](Lisht.md)
>   - his activation function simply uses the [[Tanh]] function and scales it linearly, as follows
>
> - [](ActivationFunctions.md)
>   - [[SELU]] > [[Elu]] > [[Leaky Relu]] > [[Relu]] > [[Tanh]] > [[Sigmoid]]
>
> - [Recurrent](Recurrent.md)
>   - Activation usually [[Sigmoid]] or [[Tanh]]

_Backlinks last generated 2022-10-04 13:01:19_
