---
title: BERT
tags: architecture
---

# BERT
- Bidirectional Encoder rep from transformers
- Uses [Token Embedding](Token%20Embedding.md)
- [[Self supervised]]
- Masked language modeling, next sentence prediction
- ![im](assets/Pasted%20Image%2020220307183916.png)
- [CLS] : start of classification task, [SEP] between sentences, [MASK] : masked token














## Backlinks

> - [GELU](GELU.md)
>   - Used in [[GPT3]], [[Transformer]], [[Vision Transformer]], [[BERT]]
>    
> - [BEiT](BEiT.md)
>   - proposed method is critical to make [[BERT]] like pre-training (i.e., auto-encoding with masked input) work well for image Transformers

_Backlinks last generated 2022-06-24 12:00:32_
