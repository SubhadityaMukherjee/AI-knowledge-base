---
toc: true
title: DistillBERT

tags: ['architecture']
date modified: Monday, October 10th 2022, 2:02:29 pm
date created: Tuesday, July 26th 2022, 8:33:15 pm
---

# DistillBERT
- [DistilBERT, a Distilled Version of BERT: Smaller, Faster, Cheaper and Lighter](https://arxiv.org/abs/1910.01108)
- Huggingface
- general-purpose pre-trained version of BERT
- 40% smaller, 60% faster, cheaper to pre-train, and retains 97% of the language understanding capabilities
- [Knowledge Distillation](Knowledge%20Distillation.md) during the pre-training phase
- triple loss combining language modeling, distillation and cosine-distance losses



