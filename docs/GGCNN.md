---
title: GGCNN

tags: robotics architecture 
date modified: Wednesday, September 28th 2022, 9:40:52 pm
date created: Thursday, July 28th 2022, 5:59:06 pm
---

# GGCNN
- Learning an object agnostic function to grasp objects
- Grasping using uni-modal data (depth image).  
- Generate pixel-wise grasp configuration for the given input.  
- The gripper approaches the target object in top-down manner. Uses a shallow network, and an eye-in-hand camera configuration.
- ![](assets/Pasted%20image%2020220928213943.png)
- Morrison, Douglas, Peter Corke, and JuÌˆrgen Leitner. "Closing the loop for robotic grasping: A real-time, generative grasp synthesis approach." RSS (2018).

## Backlinks

> - [Grasp Point Detection](Grasp Point Detection.md)
>   - [[GGCNN]], [[GRConvNet]], [[MVGrasp]], [[Unet Grasping]], [[Learning to Detect Grasp Affordance]], [[Volumetric Grasping Network]] , [[Affordance Detection Task Specific]]

_Backlinks last generated 2022-10-03 15:37:32_
