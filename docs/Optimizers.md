---
title: Optimization
tags: regularize 
date modified: Monday, October 10th 2022, 2:02:20 pm
date created: Tuesday, July 26th 2022, 8:33:15 pm
---

# Optimization
- [[Gradient Descent gradients]]
- [[Adagrad]]
- [[Rmsprop]]
- [[Adam]]
- [[Learning Rate Decay tricks]]
- [[Early Stopping tricks]]

## â€¦

## Backlinks
> - [](DeepLearning.md)
>   - [[Optimizers]]
>
> - [Non [[Relational Inductive Bias]]](Non Relational Inductive Bias.md)
>   - [[Optimizers]]
>
> - [Vanishing/exploding #gradients](Vanishingexploding gradients.md)
>   - [[Regularization]] , [[Optimizers]] , [[Architectures]]
>
> - [Bias Vs Variance](Bias Variance Dilemma.md)
>   - Tune on [[Emperical Risk]] instead using [[Optimizers]]

_Backlinks last generated 2022-10-04 13:01:19_
