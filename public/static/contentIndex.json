{"Articles/BookNotes/Summary---Notes-on-a-Nervous-Planet-by-Matt-Haig":{"title":"Summary - Notes on a Nervous Planet by Matt Haig","links":[],"tags":["articles"],"content":"Chapterwise summary + 30+ ways to take back control : Notes on a Nervous Planet by Matt Haig\nNotes on a nervous planet is a book that brings a lot of important points about the collective overwhelm that we as a “modern” society face. Social media, lack of sleep, odd working hours, loneliness. Neither of these were aspects that we were really prepared to deal with. But before treating a disease, we must know what the symptoms are and why they showed up in the first place. (At the end of the article, there is a long is list of what you can do to avoid overwhelm.)\nThis is a summary and my thoughts (chapter wise, mostly) on the book. I try to cover what stuck out to me the most and in turn, hope that it helps someone out. If you like the summary, you will like the book more.\nGo support the author here.\n(Note: This is not sponsored by the author and is a personal opinion that just reflects my own views. Since this is an interpretation, the author might have thought something different in a few places.)\nA stressed-out mind in a stressed-out world\nAs society progresses, we face an overwhelming increase in everything from groceries to people to jobs and somewhere down the line it takes a huge toll on our mental health. Humans weren’t meant to deal with this constant barrage of information.\nIf you stop and look around you, you can almost feel this underlying quit panic. The rush of everyday life, the endless calling out for attention by companies, all of these things wear you down little by little.\nBefore, if you wanted to find out about what was happening in the world, you would look at the news and be presented with a limited selection of it. These days however, the more attention grabbing headlines are what you see. There is always something horrible happening and this triggers the part of our brains that make us feel unsafe.\nYou might think that social media is different, but it still does the same thing doesn’t it? It keeps triggering the fear and panic that makes you feel FOMO (fear of missing out), or that someone else’s life is better. After even a few minutes, you might have noticed the guilt and sadness you feel but you can’t stop.\nAll of these, and the added loneliness it brings, makes us feel like we are in a 24x7 catastrophe.\nThe big picture\nSome people blame this on time and modernisation. Which is true, but at the end of the day the main reason for this insanity is just… consumerism and capitalism. Companies need to grow, and get the “big bucks” and what better way to do that but constantly keep expanding.\nBut every expansion has a price, and in this case it’s the sanity of society and the slow extortion of our planet. In time, this mentality has seeped into every aspect of society.\nOur politicians glorify the “hard working families” and our companies glorify endless work hours. To what end?\nThese goalposts are endless. And the constant comparison with everyone else that social media brings just feeds the fire.\nIt’s come to the point that nobody is really ever satisfied. You spend years to work on a degree, and social media convinces you that traveling the world would have been a better idea. You get a great job, a new car, move to a bigger house etc, and your device shows you pictures of even more.\nThe fuel is dissatisfaction and the fire is the need to buy. The need to keep creating and producing.\nDoesn’t this remind you of a factory line?\nA feeling is not your face\nSocial media has added a lot to the constant insecurity that we face, and beauty standards have reached an almost ridiculous peak. It’s not just a couple of magazines anymore. We are constantly surrounded by unrealistic body standards. Regardless of your gender, you are shown endless options and made to subconsciously identify with either the “barbie”, “superhuman” or some other stereotype based on what you see online. Neither of these are achievable.\nIf you go to a beach, you might think that the people around you are so concerned with how you look with your shirt off. But in all honesty, most people are thinking the same way. Most people are too concerned with their own appearance to care about yours.\nAnd then the beach itself does not care. It’s nature after all. Regardless of what your body looks like, you’re a part of it. It bears no judgement.\nAccepting yourself the way you are, and realising that what you see online in unreasonable, goes a pretty long way.\nNotes on time\nCavemen didn’t particularly care if they were 5 minutes late to a hunting meeting. Time was not quantified to this precision. At some point the sun and the moon guided time. There was day and night, maybe afternoon. Some times were good for the hunt, the others were good for rest and community.\nAs technology marches on, the concept of time becomes ever refined. Now not only are you always “late”, you know exactly how badly you failed within a nanosecond of accuracy. Hello guilt!\nDeadlines came to exist, and the constant need to check your watch or phone.\nSociety made time an enemy. “Finish everything before the timer runs out”. Suddenly, you could never achieve enough in the time that you had. Because the more divisions we make, the more we are expected to do to fill them up.\nWe stopped listening to our bodies and became a slave to a ticking clock.\nIt’s time we started going back to a simpler time.\nLife overload\nIt’s not just your sink that gets filled with dishes, your poor brain is struggling to keep up with all this work and information.\nThere is an unnecessary excess in everything and thus your brain needs to constantly make decisions. In turn, everyone around you faces the same thing. If you look it from the outside, it’s almost as if we are all feeding that collective frenzy.\nThe world is heading for a collective breakdown, and in our heart of hearts, we know that we are not just a part of it, we are feeding the fire too.\nIt’s not just life, but an overload of it.\nInternet anxieties\nAh the internet. Humanities craziest information. A way to “forever” and almost instantly look up anything. Any think you wanted to know, feel, see or find, all up for grabs. The biggest fuel we have added to the frenzy. But the internet is not always a bad thing is it? So many people have learnt skills, built families, found similar people, started a business and found partners through it.\nBut they have also been pushed opinions that they never knew of. They have been fed with a constant diet of hate, choice, games of power, games of “ratings” and a desire for pretence.\nOn the internet, nobody knows if you are a human or a dog. And it’s so much easier to bully someone if you know you are a continent away and can’t be held accountable.\nThese exacerbate the collective overload. Because now not only do people have the power to play either the victim or the bully, they can see millions of the these stories playing out everyday.\nA slowly spreading poison doesn’t kill very fast, but withers you away in time.\nThe only way to go about it is empathy. You cannot avoid the internet but you can choose what you want to actively look for.\nSimplify, and leave the fighting to the real world.\nShock of the news\nFear does sell though, and the news profits on that. Wars and death have always been common, but these days? You almost feel like the violence of the world has increased to a ridiculous point. You feel unsafe everywhere you go, 24x7.\nA few decades ago, you would get your news twice a day maybe. Now you get it every second, from every possible angle, all across the world. There’s more than 7 billion people and the stories are endless. You don’t hear about one robbery, you hear about them from 10 countries, 300 people and commented on by thousands more. Feeding the flame of anxiety again.\nRemember, fear sells.\nTry to limit your exposure to it. If something truly important happens, you will get to know.\nA small section on sleep priorities\nThe part that really stuck out to me here was this “Sleep is the enemy of consumerism”. Who’s the biggest competition of Netflix? It’s SLEEP. You can’t watch a show if you’re asleep. Neither can you buy the next “greatest phone”.\nLikewise, you cannot produce these things. The endless factory “needs” to grow. Working hours keep getting later and later, working on weekends is a normal thing, working 16 hours is a badge of pride, we all know these don’t we?\nAnd the lesser we sleep, the more health issues we have. Catch my drift? (Hint: Healthcare is a product too you know?)\nApart from healthcare, the constant panic and fear leads us to the other large businesses - Drugs and Weapons.\nMight as well get those good hours of sleep in right?\nRoutines do help. And a little light movement, a warm shower, turning off the devices etc do help quite a bit.\nPhone fears\nI think we all have heard a lot about how our devices are affecting our lives negatively. That being the case, the more important aspect of that is understanding how to tackle it. A selection is as follows.\nNotifications do suck. Turn them off except for the most important ones. Spend some time away from your devices, especially before sleep. Multitasking is an illusion.\nSocial media breeds unnecessary uncertainty. You are always left “wondering”. What is my friend doing? Did my favourite artist upload a new comic? Are there any new TikTok’s on the thing I’m currently obsessing about? The answer of course, is yes.\nThe problem is, we cannot have everything can we?\nAccepting that uncertainty is a part of life helps drastically.\nThe detective of despair\nAll the effects so far seem almost too general? The best way to see how something affects you is to look at what your body is feeling. Listen to what it says. Is it telling you to stop watching people making food and go eat dinner? Is it saying that hey, I’m so tired, can you stop looking at people waking up at 6 am and taking cold showers?\nListen.\nBreathe.\nThe thinking body\nOne rather interesting point the author makes is that of a “unit” to how much mental toll something takes : a “psychogram”. I find it a very interesting notion.\nSome examples the author gave are (negative and lower is better):\n\nThe sun appearing unexpectedly from behind a cloud : -57pg\nDancing : -1350pg\nArriving home after a terrible train journey : -398pg\nWatching the news : 222pg\nA worrying symptom you have googled : 672pg\nTrying to at least vaguely quantify how much we can deal with in a day before snapping can help us set better boundaries and listen to our body more. We all start off with a limited amount of energy, and some things drain more than the others.\nOf course, the actual quantification of it is not very useful. It’s the idea that makes it interesting to me. A reminder that we have limited resources.\nLike a Health Bar in a game, ours keeps going down too. We just can’t see it.\n\nThe end of reality\nThe more the virtual creeps in, the more choices are presented to us. Instead of going to your local store to buy a new phone, you look it up online first. Instead of being presented with one shampoo, you have 40 in a store, and 4000 online.\nAll these choices can lead your brain to want to not compute and choose. So it turns part of itself off. It hides in “Derealisation”. Reality seems a bit wonky. The virtual world feels more like home, an uncertain, panic filled home, but home nonetheless.\nWe must take steps to protect ourselves from this frenzy before it drowns us.\nWanting\nThis section ties in a lot the “beauty” one that came before. It serves as a reminder to be kinder to yourself, consider that age and time are a natural function of life itself and realise that “too much” of anything is not always a good thing.\nMore is not a solution. The solution is acceptance and being grateful for what you have.\nWe can never stop wanting, but if we can draw the line between what we truly “want” and what we are made to believe that we do by our consumerist economy, then we can get some control back.\nTwo lists about work\nAs a society, we like work. We base our worth on it sometimes, which is not great though. The cultural obsession we have with “hard working families” is not always a good thing though. Statistics show that the “hardest workers” aren’t always the happiest or the wealthiest. Does that say something?\nChoosing to have less stuff to do vs choosing to constantly be “busy” is a lot healthier.\nNever forget that deadlines are a product of consumerism. Imperfection is a part of humanity itself, it’s a feature, not a bug.\nOne quote that I really liked in the book :\n‘One of the symptoms of an approaching nervous breakdown is\nthe belief that one’s work is terribly important.’ - Bertrand Russell\nShaping the future\nOf course, change is hard. Most of these changes are easier said than done. And many of them are not even fully achievable by you yourself. Remembering that the space around you matters quite a bit, is also very important.\nWe are a part of nature, and the vast openness and freedom that it brings is in our blood. Having more open space, parks and clean  workspaces makes our days a little better.\nWhen we can’t escape into the woods, sometimes a good fiction book can serve that purpose.\nProgress is not something that happens overnight, it takes effort and time. But the peace is worth it.\nThe song of you\nAnother important reminder here, the sky is always there for you. Wherever you may be, whoever you may be.\nWe are not separate from nature, we are nature.\nYour inside world is important too. Tend to the garden that is you, sing the songs that are your very being.\nEverything you are is enough\nThe key takeaway really, is acceptance. Accepting that you cannot be everything, do everything, go everywhere, have everything. And even if you did, it would not really fill the void. What would is accepting the present moment, prioritising living over just existing, taking back the power and freedom that society so desperately wants to take away from us. Accepting that our failures are a part of life. Accepting that after a point, putting more work into something only has diminishing returns.\nAccepting that, people aren’t forever. If you want to show your love, what are you waiting for?\nA list\nThat ends my summary and thoughts about the book.\nThe author gives quite a few helpful tips dispersed among the chapters, and I thought I would list my favourites in one place. (Some are verbatim and I take no credit for them.)\n\nHappiness is felt heading out, not in\nHappiness is about what we already have\nMaybe the point of life is to embrace life’s beautiful uncertainty.\nProducts that make us ashamed of our age, don’t actually help us not age\nThe beach does not care what you look like\nYoung people are more worried about age than young\nAcceptance &gt; Denial in the long run\nFeeling something doesn’t mean that it’s the absolute truth. It’s just a feeling\nThe empty joy of likes is… empty\nPosting about experiences instead of having experiences is not great\nDon’t type your symptoms into Google\nWhat is real on the Internet, isn’t always true\nSocial media abstinence is good for you\nYou cannot understand someone through Intsagram\nRatings are not worth the judgement\nDon’t be steered towards being a caricature of yourself by the internet\nAlgorithms eat empathy\nLimit the number of times you get the news\nThe world is not as violent as it feels\nBad news doesn’t mean good news doesn’t happen\nSleep is the enemy of consumerism\nThe imperfections of the real world fill the void that the perfections of the virtual do not\nBeing lonely sometimes is not bad\nYou don’t always have to be available\nUncertainty is not going to go away\nBe your own friend\nDon’t grab life by the throat.\nToo many choices trigger your fight or flight response.\nYou cannot be everything\nWork isn’t the point of life. It’s the point of capitalism. You are replaceable to your work, not your family.\nAim to have less stuff to do.\nNature is always there for you. So are animals\n\nFin\nAnd again, if you liked my summary, you will like the original book more. You wouldn’t read Wikipedia and feel like you watched a movie would you? Support the author!\nThis article is in the hopes that it will help someone out. Maybe have the help that I did not. I do not know who it will reach. But to whoever it does, best of luck :)\nLike these/Want more? Buy me a coffee! Kofi\nWant articles on something specific? Just ask!\nYou can always contact me on LinkedIn, or drop me an [[mailto:msubhaditya@gmail.com|Email]]"},"Articles/BookNotes/index":{"title":"Book Notes","links":[],"tags":[],"content":""},"Articles/DL/AI-and-Doom":{"title":"AI and Doom","links":["topic/openai","article/10.1007/s11948-016-9770-5"],"tags":["article"],"content":"AI and Doom : The Real Fear and Some Hope\nIdeas\n\nWhy do we work : society\nThe pursuit of art\nLuddites\nMalthusian mindset\n\nResearch\n\n\nWhy be an artist when there is AI?\n\nJust because we have cars, should we stop walking?\nWe taught machines how to speak, but now that they have, why should we stop?\n\n\n\nAI and Rumors of Impending Doom\n\nAnother week, another story about how artificial intelligence (AI) is an existential threat to the human race\ndiscouraging to see so many pronouncements of AI’s existential threat to humanity by people who should know better\none technology that poses existential risk and whose creation undermined the enlightenment narrative of progress in science and technology improving human life. Global nuclear war\nA long series of movies produced after the explosions of 1945—beginning with The Day the Earth Stood Still, On the Beach, Fail Safe, Dr. Strangelove, even Godzilla—entertained us with apocalyptical tales and created a narrative whose theme (humanity is at risk from uncontrolled science) continues to shape thinking in unhelpful ways.\nloss of faith in the ability of democratic societies to manage themselves.\nseemingly intractable domestic issues, undercut the belief that change can be managed.\nThe perception of failure does not inspire confidence and undercuts the legitimacy of leaders and institutions.\nThe internet has made discourse chaotic.\nthe internet puts rumor and conspiracy before a giant audience.\nCompetition for attention in an intensely commercial society inclines people to tell horror stories—reflecting the inherent bias in human cognition, where a scary story commands larger audiences than a happy one\nIf predictions of doom were only for entertainment and PR purposes, they would not be a problem, but exaggerated fear can lead to bad policy.\nfears about AI-created unemployment\nThat automation will cause jobs to disappear is a fear that goes back to the early nineteenth century and the Luddites\nAI is the latest phase in the automation of human activity that began in the eighteenth century, and automation creates wealth and innovation. Some jobs disappear; more jobs are created. With these new jobs will come increased wealth and leisure.\nIt would be better to reinterpret the challenge of AI as deciding how to allocate increased wealth and leisure, but income distribution has not been a shining success for social policy for the last 30 years.\n\n\n\nAI Dooms Humanity But Not In The Way You Think\n\nIf you want to get people to read your material there is no better way than to highlight the negativity.\nI tend to think the basis for this is largely influenced by the well-established “Malthusian” mindset that constantly states we are about to be out of resources and the four horseman are just around the corner. Malthus simply stated that population rises until all resources are consumed creating a boom bust dynamic similar to what we today often see in markets. Too many people equates to too much consumption which results in unavoidable catastrophe.\nSo we should nonetheless worry about climate change and therefore we need to worry about energy use because if “climate change is a thing,” if energy use is not carefully curated, it’s not that we perish because we run out of energy, we all perish because we run out of environment.\nAI equates to power equates to CO2.\nIf ChatGPT really is the beginning of transhuman intelligence, then that’s it for the decarbonization strategy because any country not prepared to run its energy at full blast to fuel its AI\nBut for even a skeptic like me, ChapGPT writing my bio in the style of the old testament is enough to prove it’s smarter than 95% of people I know, so how long before that the full 100%?\n\n\n\nChatGPT creator says there’s 50% chance AI ends in ‘doom’ | The Independent\n\nFormer OpenAI worker Paul Christiano, who now runs AI research non-profit Alignment Research Center, said he believed there was a significant chance that the technology would lead to the destruction of humanity.\nThe main danger, he claimed, will come when AI systems reach and surpass the cognitive capacity of a human. Dr Christiano predicts there is a “50/50 chance of doom” once this moment arrives.\nThe most likely way we die involves – not AI comes out of the blue and kills everyone – but involves we have deployed a lot of AI everywhere… [And] if for some reason, God forbid, all these AI systems were trying to kill us, they would definitely kill us.”\ngodfather of AI Geoffrey Hinton quitting Google to sound the alarm about the dangers of AI\n\n\n\n[ LinkedIn](www.linkedin.com/pulse/one-ai-prophets-doom-alistair-enser/|(2) One in the AI for the prophets of doom | LinkedIn](2)%20One%20in%20the%20AI%20for%20the%20prophets%20of%20doom%20)\n\nWhen experts like Hinton speak on the subject of AI we should, of course, listen.\nBut I would argue that the fears currently being expressed around AI amount to little more than scaremongering and are consistent with an approach towards technology that has existed for centuries, perhaps longer.\nThe advent of technologies as diverse as the printing press, the steam engine and the computer have all been accompanied by fears over what they might bring\nIndeed, I wonder whether some of the warnings around AI can be reduced to fears and frustrations about the pace of its development, and the fact that generative AI, based on large language models (LLM), remains firmly the hands of the usual big tech players. How much of this is sour grapes?\nWe are surely right to be concerned about the potential of AI to eradicate jobs. But how many more jobs will be created as workers are redeployed to do the things that AI cannot, and will not, be able to do?\nShould there be guardrails in place to govern the development of such a powerful technology as AI? Of course. But we place regulations around loads of technologies – such as the installation of electrical wiring, the roadworthiness of cars, or the use and storage of our data.\nDoes anyone seriously suggest switching off every computer around the globe just because ‘bad actors’ can take control of corporate networks and steal money from bank accounts? Of course not.\n\n\n\nWill Life Be Worth Living in a World Without Work? Technological Unemployment and the Meaning of Life - Science and Engineering Ethics\n\nSimple Subjectivist Theories\nlife is meaningful to the extent that the individual living it experiences certain subjective states, typically conscious well-being and desire satisfaction\nSimple Objective Theories\nindividual living it brings about certain objectively good or valuable states of affairs\nAim-Achievement Theories\ncombination of subjective and objective states are needed in order to make life meaningful\nFitting-Fulfillment Theories\ncombination of subjective and objective states are needed in order to make life meaningful\nunder a simple subjectivist theory, there is reason to think that technological unemployment could enhance the overall level of meaning in our lives, but only if we make use of the right kinds of technological advances\nreason to think that technological unemployment could undermine the overall level of meaning in our lives, but this impact could lessened with the right kind of technological developments\nThe Subjective Satisfaction of Non-work\nThe idea is that compulsory work takes us away from the things that we are really passionate about and that would confer upon us the most subjective satisfaction\nI cannot do these things because the productivist ethos of modern academia demands that I produce more peer-reviewed publications to pad out my CV.\nThis argument assumes that if we control our own time we will spend it in a way that induces the right subjective states.\nDan Gilbert’s work on mis-wanting, for example, suggests that we often don’t really know what makes us truly happy and often stumble upon happiness (Gilbert ref-CR29 “Gilbert, D. (2005. Stumbling on happiness. Vintage.”); Gilbert et al. ref-CR31 “Gilbert, D. T., Pinel, E. C., Wilson, T. D., Blumberg, S. J., &amp; Wheatley, T. (1998. Immune neglect: A source of durability bias in affective forecasting. Journal of Personality and Social Psychology,\n75, 617–638.”), ref-CR30 “Gilbert, D. T., Lieberman, M. D., Morewedge, C. K., &amp; Wilson, T. D. (2004. The peculiar longevity of things not so bad. Psychological Science,\n15, 14–19.”)).\nFindings like this can be exploited by critics of automation and technological unemployment. A recent example of this is Carr (ref-CR12 “Carr, N. (2015. The glass cage: Where automation is taking us. London: The Bodley Head.”)). Carr contends that without the pressures and incentives of work we may live a life of listless and unsatisfied boredom.\nCskikszentmihalyi (ref-CR16 “Cskikszentmihalyi, M. (1990. Flow: The psychology of optimal experience. New York: Harper &amp; Row.”), ref-CR17 “Cskikszentmihalyi, M. (1997. Finding flow: The psychology of engagement with everyday life. New York: Basic Books.”), ref-CR18 “Cskikszentmihalyi, M. (2007. Experience sampling method: Measuring the quality of everyday life (p. 2007). Thousand Oaks: Sage Publications Inc.”))\nstudies reveal that people are generally more focused, happier and more satisfied at work than at play\nIndeed, they often report feeling anxious and bored outside of work when they are presumably free to engage in their preferred activities.\n‘paradox of work\nCskikszentmihalyi’s theory holds that entering into a flow state is a function of how difficult the task is and how much pressure is associated with it.\nWork is often an excellent way to provide the right kind of pressure and difficulty.\nThere is a limited utility to studies, such as Cskikszentmihalyi’s, which compare work and leisure in a world that is dominated by work.\nWhen I come home from a busy day at the office, I’m usually drained and lethargic. I’m often not physically able to engage in the kinds of activity I would prefer. I’m conscious of the fact that I need to recover before going back to the office again. In an era of rampant technological unemployment, in which the shadow of work is removed, things could be very different.\nIt is paternalistic to assume that people will lack sufficient self-motivation if they are unemployed.\nThis paternalism is behind much of the traditional ideological glorification of the work ethic.\nthe argument ignores the ways in which modern technology can greatly assist in providing these alternative sources of pressure.\nalthough technology could be leveraged in ways that make us more likely to achieve flow states through our activities, there are also ways in which we could use technology to trick ourselves into such subjectively pleasurable states without any associated activity\nif we are to think seriously about meaning and personal fulfillment in an age of technological unemployment, we probably need to take into consideration the link between our actions and the objective world, and how technology might mediate the relationship between our actions and the objective world.\nantiwork positions\nit is too dismissive of the potential for the market to direct human activity towards objectively valuable outcomes.\nproduction of art and intellectual discovery, both of which are subject to significant market forces in the modern world\nIf automating technology renders human contribution to such market-based activities unnecessary, then we may be robbed of something that is conducive to the good life.\nthe antiwork camp will simply respond by saying that nonwork is better at allowing us to do these things.\nIt assumes that the kinds of technological advance that make widespread technological unemployment possible will occur in a vacuum—that the impact of automating technologies will be felt solely in our economic lives.\nIf this trend continues, and we rely on those technologies in these other domains, we could sever the necessary causal and mental link between our actions and the outcomes that are said to be constitutive of meaning.\nScience is increasingly a ‘big data’ enterprise, reliant on algorithmic, and other forms of automated assistance, to process large datasets and make useful inferences from those datasets. Humans are becoming increasingly irrelevant to the process of discovery.\nThe machines start doing much of the work themselves; the logic of their decision-making becomes more and more opaque to those who interact with them\nThus, once again, the rise of automation reduces the space in which humans can engage in meaningful and fulfilling moral activities.\nNow, you might dispute this characterisation. You might argue that there is still room for human input in all of these automated systems. For one thing, such systems would seem to require human designers, programmers and overseers; for another, humans would still have to contribute to the smooth functioning of such systems, e.g. by becoming kidney donors or by providing crucial data\nnot everyone is going to be equipped or trained to design or program such systems\nadvances in technology may be such that human designers, programmers and overseers will become less needed over time\neven if humans will always participate in such systems, the participation in question has to be of the right type in order to facilitate meaningfulness in the objective or hybridist sense.\neven if machines are better at achieving certain objective outcomes there is nothing to stop humans from achieving them too\nwhy would you waste time and risk lives if the machines are better\nit assumes that if machines get better and better at doing things they will take away from the fixed lump of potentially meaningful activities that are open to human beings.\nBut why couldn’t more and more objectively meaningful activities open up\n\n\n\nwww.history.com/news/who-were-the-luddites\n\n“Luddite” is now a blanket term used to describe people who dislike new technology, but its origins date back to an early 19th-century labor movement that railed against the ways that mechanized manufacturers and their unskilled laborers undermined the skilled craftsmen of the day\nMost were trained artisans who had spent years learning their craft, and they feared that unskilled machine operators were robbing them of their livelihood\ncheap competition of early textile factories particularly threatening to the artisans, a few desperate weavers began breaking into factories and smashing textile machines\nMachine-breaking Luddites attacked and burned factories, and in some cases, they even exchanged gunfire with company guards and soldiers\nIt wasn’t until the 20th century that their name re-entered the popular lexicon as a synonym for “technophobe.”\n\n\n"},"Articles/DL/Artists-vs-AI":{"title":"Artists vs AI","links":[],"tags":["article"],"content":"Artists vs AI (companies) : The real battle under the hype"},"Articles/DL/Experiments-with-LLM-temperature":{"title":"Experiments with Temperature","links":[],"tags":["architecture"],"content":"Experimenting With Temperature (LLMs)\nOver the past few months at OpenML, we have been experimenting with LLM models in an attempt to improve the search experience for our users. While our existing implementation uses ElasticSearch, we wanted to also have the option of having a more “semantic” search experience.\nAside from the usual RAG pipeline that everyone and their grandparents seems to be using these days, we also wanted to experiment with using an LLM to semi-automatically generate filters for our search queries. While it may not seem like a big feature, it is something that has always been a bit of an annoyance for some of our users.\nSo what does this entail? Consider the interface we have at the moment. We have a search bar at the top, and subsequently a bunch of filters that users can use to narrow down their search. While this works pretty well as is, how about trying to automate it a bit.\nIn summary, we want a query like “find me a large dataset with multiple classes of flowers” to automatically generate filters like “classification”, “multiclass”, “sort by size of dataset” etc.\n\nTemperature\nThink about the first time you used ChatGPT. What stood out to you? Was it how well it could elaborate on a topic? Or was it how creative it could be? The temperature parameter in LLMs is what controls this.\nHow can we control creativity? Well, saying that we can directly control creativity is a bit of a stretch. We can however use a workaround.\nDo you remember the softmax function? The function that takes a vector of arbitrary real-valued scores and squashes it into a vector of probabilities that sum to 1. The inputs to the softmax function are the unnormalized log likelikhoods or the raw per class score assigned by the model.\nThe softmax function is defined as:\n\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^{k} e^{x_j}}\nIf we want more control over the distribution of the probabilities, we can use a temperature parameter. This would look like:\n\\text{softmax}(x_i) = \\frac{e^{x_i/T}}{\\sum_{j=1}^{k} e^{x_j/T}}\nwhere T is the temperature parameter.\n\n\nIf T = 1, the softmax function is the same as the original softmax function.\n\n\nIf T &gt; 1, the probabilities will become “flatter”. Since the difference between the probabilities will be less, the model can be more exploratory aka more creative.\n\n\nIf T &lt; 1, the distribution of the probabilities are “peakier”. There will be a higher difference between the probabilities, leading to the model being more confident in its predictions, but also less creative.\n\n\nVisualizing Temperature Using Softmax\nfrom tqdm import tqdm\nimport regex as re\n# LangChain supports many other chat models. Here, we&#039;re using Ollama\nfrom langchain_community.chat_models import ChatOllama\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom typing import List, Dict, Any\nimport numpy as np  \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd \nsns.set_theme(style=&quot;white&quot;)\ndef softmax(input, t=1.0):\n  ex = np.exp(input/t)\n  sum = np.sum(ex, axis=0)\n  return ex / sum\n# plot softmax over a range of inputs\nx = np.arange(0,1.0, 0.01)\nt = np.array([0.1,.5, .8, 1.0])\ny = np.array([softmax(x, ti) for ti in t])\n \n# Create a DataFrame for Seaborn\ndata = pd.DataFrame({\n    &#039;x&#039;: np.tile(x, len(t)),\n    &#039;softmax&#039;: np.concatenate(y),\n    &#039;t&#039;: np.repeat(t, len(x))\n})\n \n# Plotting with Seaborn\nplt.figure(figsize=(8, 6))\nsns.lineplot(data=data, x=&#039;x&#039;, y=&#039;softmax&#039;, hue=&#039;t&#039;, palette=&#039;viridis&#039;)\nplt.xlabel(&#039;x&#039;)\nplt.ylabel(&#039;softmax(x)&#039;)\nplt.toc: true\ntitle(&#039;Softmax Function for Different Values of t&#039;)\nplt.legend(toc: true\ntitle=&#039;t&#039;)\nplt.show()\n\nCreating the Experimental Setup\nNow, we can focus on testing the effects of temperature for our use case. We are using the llama3 model for our experiments. The experiments are being run on a 2023 MacBook Pro with an M3 chip and 18GB memory.\nDefining a Prompt\nWe need to first think of a prompt that we can use for our experiments. This prompt can be thought of as an instruction that the model uses along with the query to generate answers. To make it easier for us to use, we only want one/two word answers and for now we are only focusing on a small subset of the filters that we want our model to understand.\nprompt = &quot;&quot;&quot;User Query : {query}\nBased on the query, answer the following questions one by one in one or two words only and a maximum of two with commas only if asked for. Use only the information given and do not make up answers - \nDoes the user care about the size of the dataset? Yes/No and if yes, ascending/descending.\nDoes the user want to sort by number of downloads? Yes/No.\nDoes the user care about missing values? Yes/No.\nIf it seems like the user wants a classification dataset, is it binary/multi-class/multi-label? If not, say none.\n&quot;&quot;&quot;\nquery = &quot;Find me a big classification dataset about mushrooms&quot;\nCreating a Chain\nSince we are using the langchain and ollama libraries for our experiments, we follow their API and create a chain. The template uses string formatting to insert the prompt and the query into the chain.\ndef create_chain(prompt , temperature, llm_model = &quot;llama3&quot;):\n    prompt = ChatPromptTemplate.from_template(prompt)\n    llm = ChatOllama(model=llm_model, temperature=temperature)\n    chain = prompt | llm | StrOutputParser()\n    return chain\nParsing the Results\nTo make it easier for us to analyze the results, we generate an example answer and then see see if any further processing is needed.\n# functiont to parse responses like this to a list of yes/no/none/yes,aescending/no etc\ndef parse_response(response):\n    # split by new line and remove first two lines (here are the answers:)\n    response = response.split(&#039;\\n&#039;)[2::]\n    # if response has a question mark, split by question mark and remove empty strings\n    for i in range(len(response)):\n        if &#039;?&#039; in response[i]:\n            response[i] = response[i].split(&#039;?&#039;)[1].strip()\n    # replace full stops with empty strings\n    response = [x.replace(&#039;.&#039;,&#039;&#039;) for x in response]\n    response = [x for x in response if x]\n    return response\nchain = create_chain(prompt, 0.5)\nresponse = chain.invoke({&quot;query&quot;: query})\nprint(response)\nHere are the answers:\n\n1. Does the user care about the size of the dataset?\nYes, ascending.\n\n2. Does the user want to sort by number of downloads?\nNo\n\n3. Does the user care about missing values?\nNo\n\n4. Is it a classification dataset? If so, is it binary/multi-class/multi-label?\nYes, multi-class\n\nYay, it works. We now write a function to generate results for different temperatures.\ndef generate_results_for_temp(query:str, range_of_temps : np.ndarray) -&gt; List[List[str]:\n    results = []\n    for temperature in tqdm(range_of_temps):\n        chain = create_chain(prompt, temperature)\n        response = chain.invoke({&quot;query&quot;: query})\n        results.append(parse_response(response))\n    return results\n        \nRunning the Experiments and Plotting Results\nIt is time to run the experiments and plot the results.\nWe write a function to plot the results in a stripplot to see the distribution of the answers for different temperatures.\ndef plot_yes_no(df: pd.DataFrame, toc: true\ntitle:str) -&gt; None:\n    fig, axs = plt.subplots(2, 2, figsize=(15, 15))\n    fig.suptoc: true\ntitle(toc: true\ntitle)\n    sns.stripplot(data=df, x=&#039;size&#039;, y=&#039;temperature&#039;, ax=axs[0, 0], hue=&#039;size&#039;)\n    sns.stripplot(data=df, x=&#039;sort_by_downloads&#039;, y=&#039;temperature&#039;, ax=axs[0, 1], hue=&#039;sort_by_downloads&#039;)\n    sns.stripplot(data=df, x=&#039;missing_values&#039;, y=&#039;temperature&#039;, ax=axs[1, 0], hue=&#039;missing_values&#039;)\n    sns.stripplot(data=df, x=&#039;classification_type&#039;, y=&#039;temperature&#039;, ax=axs[1, 1], hue=&#039;classification_type&#039;)\n    # tilt x axis labels\n    for ax in axs.flat:\n        ax.set_xticklabels(ax.get_xticklabels(), rotation=30, horizontalalignment=&#039;right&#039;)\n    plt.show()\nSometimes, the model returns an extra field, we combine the last two fields to plot the results. (This is a bit of a hack, but it works for now and is ONLY used for plotting)\ndef combine_last_two_elements(lst):\n    # Check if the list has at least two elements\n    if len(lst) &gt; 4:\n        # Combine the last two elements with a space separator\n        combined_element = lst[-2] + &#039; &#039; + lst[-1]\n \n        # Create a new list with combined element instead of the last two\n        return lst[:-2] + [combined_element]\n    else:\n        return lst\nExperiment 1\nOut first experiment is a rather simple query, “Find me a big classification dataset about mushrooms”. As you can probably guess, we are looking for a dataset that is large, is a classification dataset and is about mushrooms.\nrange_of_temps = np.linspace(0, 1, 20)\nquery = &quot;Find me a big classification dataset about mushrooms&quot;\nresults1 = generate_results_for_temp(query, range_of_temps)\n100%|██████████| 20/20 [00:49&lt;00:00,  2.49s/it]\n\nresults1 = [y for y in x if all(sub not in y for sub in [&quot;If&quot;, &quot;:&quot;])] for x in results1]\ndf = pd.DataFrame(results1, columns = [&#039;size&#039;, &#039;sort_by_downloads&#039;, &#039;missing_values&#039;, &#039;classification_type&#039;])\ndf[&#039;temperature&#039;] = range_of_temps\nplot_yes_no(df, toc: true\ntitle = query)\n\nRather interesting don’t you think? At higher temperatures, the model gets the answers wrong. Even at a temperature slightly above 0.1, the model starts adding extra information to it’s answers.\nDid you notice that I tried to remove sentences that started with “If”? There are more examples of this later, but this is because at higher temperatures, the model tends to add random sentences to the answers and this makes it quite hard to plot them.\nExperiment 2\nOur second experiment is super easy. “Find me a dataset that has a lot of missing values and order by number of downloads”. As you can obviously guess, we are looking for a dataset that has a lot of missing values and we want to order the results by the number of downloads.\nrange_of_temps = np.linspace(0, 1, 20)\nquery = &quot;Find me a dataset that has a lot of missing values and order by number of downloads&quot;\nresults2 = generate_results_for_temp(query, range_of_temps)\nresults2 = [y for y in x if &quot;so&quot; not in y] for x in results2]\n \n100%|██████████| 20/20 [00:34&lt;00:00,  1.74s/it]\n\ndf = pd.DataFrame(results2, columns = [&#039;size&#039;, &#039;sort_by_downloads&#039;, &#039;missing_values&#039;, &#039;classification_type&#039;])\ndf[&#039;temperature&#039;] = range_of_temps\nplot_yes_no(df, toc: true\ntitle = query)\n\nHmm, same as before. The model starts adding extra information at higher temperatures and starts getting the answers wrong. (Yes, No?? ) What kind of answer is that?\nExperiment 3\n\nNow a slightly more complex query. “Find me a dataset that has 10 classes and sort by number of downloads”. We want it to understand that we want a multiclass classification dataset and we want to sort the results by the number of downloads.\n\nrange_of_temps = np.linspace(0, 1, 20)\nquery = &quot;Find me a dataset that has 10 classes and sort by number of downloads&quot;\nresults3 = generate_results_for_temp(query, range_of_temps)\n100%|██████████| 20/20 [00:55&lt;00:00,  2.80s/it]\n\nresults3 = [combine_last_two_elements(x) for x in results3]\ndf = pd.DataFrame(results3, columns = [&#039;size&#039;, &#039;sort_by_downloads&#039;, &#039;missing_values&#039;, &#039;classification_type&#039;])\ndf[&#039;temperature&#039;] = range_of_temps\nplot_yes_no(df, toc: true\ntitle = query)\n\nThis seems to have been very easy for the model. But as always, the model starts adding extra information at higher temperatures. A lot of extra information in fact. Even though the prompt says to ONLY answer with one or two words\nExperiment 4\n\n“Find me a dataset that 2 classes and is a big dataset”. You know the drill by now. We want a binary classification dataset that is large.\n\nrange_of_temps = np.linspace(0, 1, 20)\nquery = &quot;Find me a dataset that 2 classes and is a big dataset&quot;\nresults4 = generate_results_for_temp(query, range_of_temps)\n100%|██████████| 20/20 [00:42&lt;00:00,  2.14s/it]\n\nresults4 = [combine_last_two_elements(x) for x in results4]\ndf = pd.DataFrame(results4, columns = [&#039;size&#039;, &#039;sort_by_downloads&#039;, &#039;missing_values&#039;, &#039;classification_type&#039;])\ndf[&#039;temperature&#039;] = range_of_temps\nplot_yes_no(df, toc: true\ntitle = query)\n\nNotice how some things changed? At higher temperatures, we get extended answers.\nConclusion\nIn conclusion, we can see that we should probably stick to lower temperatures for our use case. As we go higher, the model starts being more “creative” and either adds extra information to the answers or gets them wrong. While this behaviour might be useful in cases like creative writing, it is not something we want in our search.\nUsing LLMs can sometimes be a bit of a hit or miss. But of course, learning to control it’s parameters can help us get the most out of it. This blog post was just a simple experiment, but in the deluge of content made by people who have no idea what Softmax is, I hope this was helpful.\nFin\nThis article is in the hopes that it will help someone out. Maybe have the help that I did not. I do not know who it will reach. But to whoever it does, best of luck :)\nLike these/Want more? Buy me a coffee! Kofi\nWant articles on something specific? Just ask!\nYou can always contact me on LinkedIn, or drop me an [[mailto:msubhaditya@gmail.com|Email]]"},"Articles/DL/Parsing-and-Querying-Tensorboard-logs---A-Mini-Tutorial":{"title":"Parsing and Querying Tensorboard logs - A Mini Tutorial","links":[],"tags":["article"],"content":"Parsing and Querying Tensorboard logs: A Mini Tutorial\nSo, you wanted to parse your Tensorboard logs, didn’t you? Did you try using GPT-3? OH! GPT-4? Well. Guess that didn’t give you what you wanted.\nYeah, me neither. So here we are.\nRead on and you will find out how to take all the runs you logged to Tensorboard, clean them up, and put them in a single DataFrame. From there, you can query it as you would any other table.\nWhat do we want?\nTensorboard is one of the more popular means of logging your deep learning experiments. The issue though, is that it is hard to run custom queries over the already-created graphs. Now, if you were saving your results separately, this would not be an issue. But you probably weren’t were you? (Yeah, me neither.)\nSo we want to iterate over all the logs, for every file we create rows and then columns for each of the tags you saved (eg: Loss, accuracy, etc.)\nAnd as a bonus, this script also takes into account all those juicy images you saved from the last time you wanted to try running a DCGAN, again. (Or a cats and dogs classifier, I don’t know)\nShush and show me the code?!\nYes, I know this could just be a code snippet on Stack Overflow and there was no real need for this article. If you are an intermediate/advanced programmer, honestly just skip ahead and grab the code here.\nNow if you did go and skip ahead and realized that it made no sense, welcome back. Read on and hopefully, your doubts will be answered.\nImports\nAs usual, we need to grab some libraries for this to work. You probably have most of them anyway. We need os and pathlib to read the files, pandas for the dataframe, and numpy if you want to further process your data. Tqdm is a little progress bar helper that prints a pretty little progress bar as you wait for your loop to finish running. PIL will be used to read the Image files. BytesIO and base64 will be used to decode the images from Tensorboard so we can save them to the dataframe.\nFinally, we also do need the Tensorboard package, but if you didn’t have that already then what are you doing here?\nWhy pickle you might ask? You will see.\nAll of these packages are available either as a pip or a conda/mamba install. So just run pip install &lt;x&gt; and it should hopefully work.\nNow that we have that out of the way, let’s define the path where your logs are. (The same one you pass to the —logdir argument on Tensorboard)\nimport os\nimport pandas as pd\nfrom tensorboard.backend.event_processing.event_accumulator import EventAccumulator\nfrom tqdm import tqdm\nfrom pathlib import Path\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport base64\nfrom io import BytesIO\nfrom PIL import Image\nimport pickle\n \nmain_path = &#039;./runs/&#039; # CHANGE THIS\nFind TFevent files\nTensorboard uses a custom format that it calls a ‘tfevent’. If you do look at your logs, you will see that the format of the files is either ‘tfevent’, ‘checkpoint’, and of course whatever else you have saved.\nThe first step is then to just read all the tfevent files from the directory. We will use the walk function and pathlib to find all of the relevant files.\nWhy convert it to a Path object? Using the Path function from the pathlib instead of just a string for the directories will allow us to quickly perform operations on the directory if we need it. (For path.name will give us the file name from the full path.)\nWe just save the complete paths of each of the files to an array here.\ndef get_event_files(main_path):\n    &quot;&quot;&quot;Return a list of event files under the given directory&quot;&quot;&quot;\n    all_files = []\n    for root, _, filenames in os.walk(main_path):\n        for filename in filenames:\n            if &quot;events.out.tfevents&quot; in filename:\n                all_files.append(str(Path(root) / Path(filename)))\n    return all_files\nCreate DataFrame\nOur objective is to collect all the metadata from the logs and convert it to a single DataFrame. To this accord, we get a list of all the relevant tfevents using the previous function. We then create an empty dictionary to store file-wise information.\nThe EventAccumulator function uses the Tensorboard API to read the tfevent file and so we run every file through it. To make sure we read the logs from the start, we also reload the object.\nOnce we have this object, we can pass it to a function that will return the relevant tags to a dictionary with the file name as a key and the tags as a value. This function will be discussed in the next section.\nOnce we run over all the files, we have a dictionary of dictionaries with all the information we need. Now, pandas provides a function to convert a nested dictionary to a DataFrame, so we use it directly.\nIf you look at this output, you will see that the rows are the tags while the columns are individual files. If this version works for you, then do use it!\nI find it easier to have the tags as column names and each row of the DataFrame for file information. I also wish to access the file names later so I use the reset_index() function that will essentially make sure all the columns have names here. (In this case, the file names will be named ‘index’. You can change this if you want by passing in a columns=[] that you want to the from_records function.)\ndef process_runs(main_path):\n    all_files = get_event_files(main_path=main_path)\n    all_dict = {}\n    for files in tqdm(all_files, total = len(all_files)):\n        event_acc = EventAccumulator(files)\n        event_acc.Reload()\n        temp_dict = process_event_acc(event_acc)\n        all_dict[files] = temp_dict\n    return pd.DataFrame.from_records(all_dict).T.reset_index()\nProcess an Event\nNow for the main bit. Depending on what you were training, you probably have many types of logs that you saved. In my case, these are Scalars, Tensors, or Images. The following function processes these, but it is easy enough to extend to whatever you want.\nFrom the process_runs function, we pass in the EventAccumulator object to this function. This object contains the tags that we saved each of our logs as. (For eg: ‘Train/Acc’, ‘Train/Loss’ etc.)\nI do not want to manually type these every time, and would rather use a function to do that for me.\nThese tags are first divided into the category of object it is, for instance, “images”, “scalars”, “histogram” etc. We will need to write a separate pre-processing step for each object depending on what information we want from them.\nNote that for each of the tags, the subtags are the names of the actual values that we want. (Eg: For the tag ‘scalars’, we have ‘Train/Acc’).\nScalars\nThese are probably numerical values that you saved. Say the loss, accuracy, number of classes, etc. For this type of data, we first read the Scalar value using the event_acc.Scalars command. We need to pass in the name of the subtag that we want to look at (eg: ‘Train/Acc’).\nNow, if you were looking at values that change throughout training, you will probably get a list here. For instance, you will get an epoch-wise accuracy list. Since I only want the final accuracy/loss etc, I am only returning the final index.\nFeel free to customize it to whatever you need.\nTensors\nThe name gives it away, but these are the values that you might find have the extra tag ‘/text_summary’. We can access these by using the event_acc.Tensors command. Now if you inspect the object, you will see that the actual value can be found in the first index of this object. The value is stored in the ‘string_val’ field of this object, so we take that out and again index it into the first element. (You will see why if you print the object. No further explanations are given because that’s just how the API is. It also depends on what exactly you want to save of course.)\nNow if you look at the final output, you will find that it looks something like b’some value’. This is an encoded string, and to convert it to a normal string, we have to decode it as an ASCII character string. Pretty easily done.\nImages\nImages are a bit of a complicated case here. If you look at the tensor board object that we get from event_acc.Images, you will see that it is a Bytes object and not a numpy array/PIL image. This is just the format Tensorboard chose, so all we can do is accept it and convert it to our needs.\nAfter indexing into the correct component of the object, the field ‘encoded_image_string’ holds, well, the encoded image string. We take that and convert it to a BytesIO object. This is a format that PIL can read as an image, so we read it as one.\nOther formats\nNow if you have something like a histogram, you hopefully get how to process it. Use event_acc.Histogram for instance, and then apply whatever transform you want to it. I do not need them yet, but I might add them to this article later on.\ndef process_event_acc(event_acc):\n    &quot;&quot;&quot;Process the EventAccumulator and return a dictionary of tag values&quot;&quot;&quot;\n    all_tags = event_acc.Tags()\n    temp_dict = {}\n    for tag in all_tags.keys():\n        if tag == &#039;scalars&#039;:\n            for subtag in all_tags[tag]:\n                temp_dict[subtag] = [tag[-1] for tag in event_acc.Scalars(tag=subtag)][-1]\n        if tag == &#039;tensors&#039;:\n            for subtag in all_tags[tag]:\n                temp_dict[subtag.replace(&#039;/text_summary&#039;, &quot;&quot;)] = [tag[-1] for tag in event_acc.Tensors(tag=subtag)][0].string_val[0].decode(&#039;ascii&#039;)\n        if tag == &#039;images&#039;:\n            for subtag in all_tags[tag]:\n                temp_dict[subtag] = Image.open(BytesIO(event_acc.Images(subtag)[1].encoded_image_string))\n    return temp_dict\nA Caveat\nNow, there is a catch. After creating the DataFrame, if you save it as a “csv” object, it becomes impossible to load the images back. This happens because the object is saved as a string like ‘` instead of the actual image.\nInstead of that, you can save the DataFrame as a pickled object.\nimport pickle\nwith open(&quot;pickled_df.pkl&quot;, &quot;wb+&quot;) as f:\n\tpickle.dump(combined_df, f)\n \nwith open(&quot;pickled_df.pkl&quot;, &quot;rb+&quot;) as f:\n\tcombined_df = pickle.load(f)\nLoad and clean DataFrame\nTo get the DataFrame we so badly desire, we just run the functions we wrote before. And hopefully, if everything worked fine, you can go home and sleep. (Or if not, sorry! I hope you only have a few hours of your workday left.)\nAnother step I want to mention is the ability to ignore failed runs. If you were tracking failure, then just use that object. If you weren’t, then just look at the columns that are written at runtime. For instance, I always write the name of the experiment, and if even a single epoch was completed, then there should be a validation loss as well.\nTo filter the data, I just take the rows that have values for these. (As usual, depends on what you want.)\ncombined_df = process_runs(main_path=main_path)\ncombined_df = combined_df[(~pd.isnull(combined_df[&#039;experiment_name&#039;])) &amp; (~pd.isnull(combined_df[&#039;Loss/Val&#039;]))]\ncombined_df.head()\nDisplay images\nThe final part of the code is looking at the images. Filter out what you want, and pick the row and column name as you would index a text object. Done! If you are using a Jupyter notebook, then you should see the image pop up. If you are running this program as a script, you will have to use .show()\nfiltered_df = combined_df[(~pd.isnull(combined_df[&#039;converted_proxy&#039;])) &amp; (~pd.isnull(combined_df[&#039;original_images&#039;]))]\nfiltered_df.iloc[0].original_images\nWhat next?\nDepends on how much you are getting paid. :)\nJokes aside, this is just a regular DataFrame now. So you can write all the queries you want. Do you want to know how badly your model did? Sure, look at the columns. Did you write some complex logic and now forgot what your actual project was? Oops. Open the DataFrame in Excel and cry.\nBut I am sure you will manage. After all, you’ve got this far haven’t you?\nWhy not Wandb/W&amp;B etc etc\nI do want to mention that I am not against any of the other logging platforms. Honestly, they do some pretty great work. But I am used to Tensorboard, and having my data offline and not on someone else’s cloud (jokes on me, this article is on someone else’s cloud) is nice.\nUse whatever works for you. Or write your own. Heck, use a CSVLogger and save what you want directly to a CSV.\nFin\nThis article is in the hopes that it will help someone out. Maybe have the help that I did not. I do not know who it will reach. But to whoever it does, best of luck :)\nLike these/Want more? Buy me a coffee! Kofi\nWant articles on something specific? Just ask!\nYou can always contact me on LinkedIn, or drop me an [[mailto:msubhaditya@gmail.com|Email]]. For all the code, drop by my Github."},"Articles/DL/Tree-of-Thoughts-Explained-Simply-(LLMs-Dehyped---1)":{"title":"Tree of Thoughts Explained Simply (LLMs Dehyped - 1)","links":[],"tags":["article"],"content":"\nTree of Thoughts Explained From Foundations (LLMs Unhyped - 1)\n\n“Programmers are, in their hearts, architects, and the first thing they want to do when they get to a site is to bulldoze the place flat and build something grand.” : Joel Spolsky, co-founder of Stack Overflow\n\nIntroduction\nLarge language models like GPT4 have taken over the world and has left every second IT person to scramble towards the next great AI solution. Given the monetary benefits, a lot of research has popped up in a very short time. While this is very exciting, it is wise to look a little deeper beneath the hype. The “new groundbreaking research from {X} that will change your company” is sometimes a tiny change or a creative use of an existing concept.\nThis is not to pour cold water over your dreams, but a way to help you understand these concepts better. If you did not come from a technical computer science background, this constant influx of “groundbreaking research” can get very overwhelming.\nSo here is a more sober, in depth view of how the “Tree of thoughts” [1] paradigm came into being from concepts that have been around for decades and creatively applied to LLMs.\nNote : Both the research and my understanding of it fluctuates over time and if something changes, I will try to come back and correct it. If you notice something weird, do drop a comment!\n\nWhat is the Tree of Thought?\nIf you have not already encountered it, the Tree of Thoughts claims to help an LLM arrive at a more logical conclusion and also generate the steps it took to come to it.\nIt was first mentioned in a paper by Yao et al. [1] and was further expanded on by a LOT of articles and papers. As the authors say, it is a way of “Deliberate Problem Solving with Large Language Models”.\nBut you might ask, why do we care? I just want my assignment done for me.\n\nWhy Bother Adding it to an LLM?\nTo better understand why we care about algorithms like this, we need to dig into some of the shortcomings behind LLMs.\n\nFixed Knowledge : A LLM is trained on a large text database encompassing a huge chunk of the web. But the web is not a fixed resource, neither is the information in it. Unless the model is trained with new data or an external data source is given to it, it’s “knowledge” is essentially fixed.\nCost of training : Training a LLM like ChatGPT is extremely costly, and it is just not possible to keep updating the model everytime something new pops up on the internet.\nKnowing everything is not the point : At it’s heart, an LLM is not meant to know everything. It is just a text model - it predicts the next word in a sentence. Using it as a model that can understanding text and it’s underlying relations is a better and cheaper way to use it.\nLogic : It is not easy to understand the steps an LLM took to arrive at an answer. Neither is it easy to make it follow logical steps without knowing the logic in the first place.\nStructured Data : LLMs are great at making sense of large amounts of unstructured data. But it is not meant to be good at working with structured data like graphs by itself. There are workarounds, but it is not something an LLM can do by itself.\n\nGiven these shortcomings, the Tree of Thought is an attempt at combining classical “logical search algorithms” with LLMs. How? Well, read on.\n\nTheoretical Background\nThis section is for you, the reader who want to dig deeper and understand the actual concepts that led to the research. If you just want to mash together models and don’t really care how they work. Just skip to the next section.\nNow, if you have a background in computers, you will recognize these terms. If you don’t, then pay attention. It will help you understand a lot of research in this domain.\n\nChain of Thought\nThe Tree of Thought is derived from a theoretical computer science concept - the “Chain of Thought”.\nSimply put, the Chain of Thought is a logical breakdown of a task into it’s simplest steps. Instead of trying to solve a problem as a whole, we try to solve it in parts and then combine the result.\nWhy? Well, it is a lot easier to do and writing it formally makes it possible for others to understand how you arrived at the solution. (Think of trying to solve for x in a math equation.)\nIt also adds in error checking. If something went wrong, you can backtrack and find the bug.\nEg: You want to check if it’s raining outside. How would you break down the steps?\nLook outside the window → Check for rain clouds → Check if there are raindrops → Check the ground to see if it is wet → If all the conditions are satisfied, you know it is raining.\n\nTrees\nIn computing, a tree is a way of representing this chain of thought. It’s main advantage is that once we create a tree of steps, it is possible to iterate over it and reach a logical conclusion or explore options.\nA tree has a root node (eg: Is it raining) and leaf nodes (eg: rain clouds, wetness). The leaf nodes are arranged in levels and are connected to previous levels (eg: clouds → ((present) , (absent)) )\n\nTraversing a Tree\nOnce we have a tree, there are many ways of traversing on it. Choosing an algorithm usually depends on what you need, and how much compute you are willing to use. Some of the approaches used in the paper are as follows (high level explanations):\n\nBreadth First Search : This is used to find the shortest path from the root to a leaf. The tree is traversed layer by layer and all nodes at each level are evaluated. If a match is found, the algorithm stops. This is quite fast.\nDepth First Search : This is used if you want to explore your options and find new possibilites. The tree is traversed by starting at a node and going as deep as possible from there until the end. If a match is found, the algorithm stops. If not, it backtracks and goes to the next node. This is much slower, but is useful in certain cases.\nOther Options : Covering the whole lot (like A*) is beyond the scope of this article, but you can refer to [4] if you want to learn more.\n\n\nEnsemble Learning\nA large portion of ML algorithms are stochastic (if you run it again, you will get a different result). While this is good for tasks like creative writing, it is not great if you want logical answers.\nOne way around this is to use multiple similar models on the same data and then “average” out the results. This somewhat helps to counter the randomness and usually leads to better performance.\nThere are many ways of combining these results - Weight them and use a mix of them, use a majority vote, use a separate model to evaluate which result is better etc.\nCan you see how this would be useful for an LLM when trying to solve a logical problem?\nWant to learn more? Refer to [3].\n\nTricking an LLM\nNow that you understand the background, let us dive into how this works with an LLM.\nSo, what do we want to do? Simply put, we want the LLM to come up with different answers that we can then put into a graph. We can then use this graph to arrive at a more logical solution.\nSpoiler : Can you see why this would not always be a good idea?\n\nPrompts\nHow do we do it? Quite simple really, we first define a format such as “the answer is {n} because {x1} and {x2} lead to {n}”. We then prompt multiple times using a prompt like “solve it in multiple ways while pretending to be three different experts” and save the results. We also add a prompt like “only use the information give”, and voila! We have a graph.\nWell, mostly. The answers you get might or might not be useful.\n\nEnsemble\nNow that you have the answers, provided your prompt has the logic you want, you can decide how to traverse the graph and find the best answer. You can also then repeat this multiple times and vote on the best result from multiple graphs.\n\nEvaluation\nIf you know exactly how to evaluate the task (eg: The best step for a robot to take when choosing to get left or right depends on if it will hit something or not), then you can use that as a criteria. But well, this is neither always possible. If you already knew what you wanted, then you would probably not be using an LLM.\nThe paper asks the model to evaluate how well it did itself. Respectfully, this might be a bit dubious for real world issues if unchecked.\n\nImmediate Shortcomings\nWhile the Tree of Thought is quite a nice idea, there are quite a few issues that immediately crop up.\n\nEvaluation : How can we decide if the answer is correct. Say for a math problem, if you knew how to get to the right step, why would you use an LLM? And for say a creative writing task, how can you even evaluate if the answer was correct?\nManual Effort : Using a Tree of Thought in practise requires a bit of manual effort in creating the perfect logical prompt, and knowing the evaluation steps beforehand. This might not always be useful.\nBias : Asking a model to evaluate how well it did is to ask a math student to see if they got the problem right. If they knew how to check, why would they want to explore other paths? But perhaps it could encourage them to think deeper about other aspects of a problem.\nComputation : This is a big one. Running an LLM is expensive. For most tasks, running a model multiple times and then further algorithms is not exactly compute friendly.\nNot everything needs an LLM : As they say, “when you have a hammer, everything starts looking like a nail”. An LLM is useful, but not everywhere.\n\n\nPerks\nSo when would you actually want to use them?\n\nGenerating Data : An LLM comes with a lot of knowledge inbuilt. Perhaps this is a good way to generate data for a different task. Using a Tree of Thought would enable an LLM to come up with much better and more logical examples.\nForcing an LLM to think more : Instead of taking the first result for granted, you can force the LLM to think a step deeper. Like a kid asking “why?” multiple times, perhaps a better answer can be reached.\nCombining with Knowledge Bases : Combining the language understanding capabilites of LLMs with existing knowledge bases is pretty useful. While this area is a little different from the Tree of Thought, they are related concepts. Perhaps in the future these ideas will be combined to improve LLMs even further [6].\nDomain Specific Information : This could be used as a means of injecting domain specific logical steps to the results obtained from an LLM.\n\n\nWhy “LLMs Unhyped”?\nThis is the first article in the series LLMs Unhyped. A rather weird name, I know. But why even have this series in the first place?\nLLMs are amazing, but they are still in the research phase. With companies like OpenAI and Hugging face, it is now possible for a lot of people to work with these massive AI pipelines without much effort.\nWhile that is an amazing feat in itself, and so many great ideas come out of it, it also leads to a lot of misinformation. In the hype of AI, many people who don’t fully understand the background research end up  using these massive models in places where they were probably not needed.\nSometimes, it’s awesome. But in other times, it is a massive waste of money. Now ultra large corporations want you to spend your money on them, why wouldn’t they? But occasionally it’s like using a helicopter to get to a supermarket 100m away.\nAI has it’s uses. But not everywhere. By no means is this a critique against research or enjoying the magic of AI. Please, continue to do that! But consider your options too. At the end of the day, this is just yet another tool.\n\nSome More Resources for You :)\n\n[1] Tree of Thoughts : arxiv.org/abs/2305.10601\n[2] Graph of Thoughts : arxiv.org/abs/2308.09687\n[3] Voting Algorithms : scikit-learn.org/stable/modules/ensemble.html#voting-classifier\n[4] Graph Search Algorithms : en.wikipedia.org/wiki/Graph_traversal , en.wikipedia.org/wiki/Pathfinding\n[5] Roadmap of KB + LLM : arxiv.org/abs/2306.08302\n\n\nFin\nThis article is in the hopes that it will help someone out. Maybe have the help that I did not. I do not know who it will reach. But to whoever it does, best of luck :)\nLike these/Want more? Buy me a coffee! Kofi\nWant articles on something specific? Just ask!\nYou can always contact me on LinkedIn, or drop me an [[mailto:msubhaditya@gmail.com|Email]]"},"Articles/DL/index":{"title":"Deep Learning","links":[],"tags":[],"content":""},"Articles/Drafts/What-I-learnt-from-an-AI-Masters-Part-2":{"title":"What I learnt from an AI Masters Part 2","links":[],"tags":["articles"],"content":"Part 2 : Planning your thesis\nPart 3 : Setting up your writing, programming, and research environment for success\nPart 4 : Tips for writing your thesis\nWhat I learned from an AI masters degree - Part 2 (Setting up your writing, programming, and research environment)\nSetting up your programming environment\nIf you are a programmer, this part might not be something you are particularly afraid of. But if you are not as comfortable with programming, it would help you to set up your environment before you start. This might include things like creating a GitHub repository, installing all the packages that you need, making sure that you have the resources that you might require (such as a GPU) etc. In future parts of the article, I will try to detail as much of this as I can based on my experience. (Depending on when you are reading this, these parts of the article might be already posted, so refer to them.)\nSetting up your writing environment\nA thesis is a rather long document, but by this point, you hopefully have written enough assignments to be comfortable with writing technical articles. If you have not, or you are reading this before doing a Master, it would serve you to be a little familiar with LaTEX. It’s took me about a few hours to get everything set up and running, but it might take you longer. (Refer to a detailed guide in future articles.)\nProgramming your idea\nNot every thesis would require a lot of programming. Depending on what you’re doing, you might have to spend more time writing analysis scripts instead. In my case, I created a solution from scratch and also wrote the analysis scripts so it definitely took me a lot of experimentation and time. But to be honest, it took me longer because I hesitated quite a lot, and did not spend enough time trying to refine what I already had.\nRunning experiments/studies\nAgain, depending on the kind of thesis, you are working on the time taken for you to run, experiments or studies might differ considerably. If you are running experiments where you collect data yourself, from different systems or from human studies, it might take you a lot longer. In my case, I used datasets that were already available but since I ran a lot of experiments, the running took longer.\nAnalysing the experiments/studies\nJust like the previous one, analysing your results might take you longer. I personally found it useful to have the analysis scripts written as early as possible. I started writing the scripts around the same time that I started programming my solution. This allowed me to be able to run quick experiments just to make sure that my code was working and saved me quite a lot of time in the long run. It also meant that as my code changed, and I thought of new things, I could add them directly to my analysis without having to wait till I was finished running my experiments.\nActually writing the thesis\nSo, you made it to having your experiments and data and literature survey now you actually want to start writing the thesis. I will warn you though, it will probably take more time than you think it would. No matter how good you are at writing long form, there will be many things that will crop up that you did not probably think of. It took me probably the longest to write my thesis compared to every other step. But, although it took a long time, it was not very painful as I had planned ahead and made preparations. Many of the little things such as making notes while reading papers, setting up the environments, beforehand, etc significantly cut down the mental effort it would have required otherwise.\nAs for all the parts that helped me out, there will be detailed articles about the same. Depending on when you read, this, you could probably refer to them already.\nChanges\nAnother thing that you probably should be warned about is having to change things. Like any project, a thesis is not straightforward. There will be many times when you need to redo a part of the project or rewrite a section. This may take a decent amount of time as well. You might also get a lot of feedback that you need to implement as you go along.\nBefore I started my project, I made sure that I had systems in place that would let me quickly change things if required. This article would be too long if I also listed them, so future parts will follow up on these.\nAs you can tell by now, none of these steps are linear. There are many things that you need to go back and forth between. But now that you know what to expect, it will be quite a lot easier for you to plan the same.\nCheckpoints\nI think it’s important to have checkpoints along the way to help guide you and make sure that you are on the right track. The process of finding these checkpoints was quite a bit of trial and error on my part, but perhaps they may come easier to you.\nThese checkpoints are like deciding a day when you want to complete your literature survey or have a prototype ready etc. They might also be ones where you update your supervisor on your progress or take a little break.\nI admit these are a bit hard to plan for until you are sure of where your project is headed, but it is nice to keep it in mind.\nRecovery time\nThere are a million articles on how to do your thesis. If you went looking for them, you probably found some more. But what I don’t see a lot of them talking about is recovery.\nDoing a thesis is hard. For many of us (including me), it is the first time we have had to do such a long project without any external motivation. It’s your project, after all. Unless you choose a company internship, in which case you will have stricter deadlines, of course. There are bound to be days, sometimes even weeks, where you have literally 0 motivation to even look at your thesis.\nAnd while that is pretty normal, I think many of us don’t factor that into our plans. Make sure you keep some extra time on hand for these days because you will really need them.\nSupervisor\nWhat can you expect from them\nUpdating\nExtra tips"},"Articles/Drafts/Writing-your-own-Markdown-to-LaTEX-parser":{"title":"Writing your own Markdown to LaTEX parser","links":[],"tags":["articles"],"content":"Writing your own Markdown to LaTEX parser\nWhat we want\nDisclaimers\nOverview of Steps\nLet’s Make It!\nLibraries\nimport markdown\nimport argparse as ap\nfrom pathlib import Path\nimport re\nfrom html.parser import HTMLParser\nfrom html.entities import name2codepoint\nBase Templates\ndefault_template = &quot;&quot;&quot;\n\\\\documentclass[12pt]{article}\n\\\\usepackage[a4paper, total={6in, 8in}]{geometry}\n\\\\usepackage[utf8]{inputenc}\n\\\\usepackage[T1]{fontenc}\n\\\\usepackage[english]{babel}\n\\\\usepackage{graphicx}\n\\\\usepackage[dvipsnames]{xcolor}\n\\\\usepackage{hyperref}\n\\\\usepackage{listings}\n \n\\\\newcommand\\myshade{85}\n\\\\colorlet{mylinkcolor}{violet}\n\\\\colorlet{mycitecolor}{YellowOrange}\n\\\\colorlet{myurlcolor}{Aquamarine}\n \n\\\\hypersetup{\n  linkcolor  = mylinkcolor!\\\\myshade!black,\n  citecolor  = mycitecolor!\\\\myshade!black,\n  urlcolor   = myurlcolor!\\\\myshade!black,\n  colorlinks = true,\n}\n\\\\author{}\n&quot;&quot;&quot;\nHTML Parser\nclass MyHTMLParser(HTMLParser):\n    def __init__(self):\n        super().__init__()\n        self.attrs = []\n    def handle_starttag(self, tag, attrs):\n        for attr in attrs:\n            self.attrs.append(attr)\n    def get_attrs(self):\n        return self.attrs\n    def handle_endtag(self, tag):\n        pass\n \n    def handle_data(self, data):\n        print(&quot;Data     :&quot;, data)\n \n    def handle_comment(self, data):\n        print(&quot;Comment  :&quot;, data)\n \n    def handle_entityref(self, name):\n        c = chr(name2codepoint[name])\n        print(&quot;Named ent:&quot;, c)\n \n    def handle_charref(self, name):\n        if name.startswith(&#039;x&#039;):\n            c = chr(int(name[1:], 16))\n        else:\n            c = chr(int(name))\n        print(&quot;Num ent  :&quot;, c)\n \n    def handle_decl(self, data):\n        print(&quot;Decl     :&quot;, data)\ndef get_html_attributes(text):\n    parser = MyHTMLParser()\n    parser.feed(text)\n    return parser.get_attrs()\n\nReplace strings\nreplacer_dict = {\n    &quot;&lt;head&gt;&quot; : &quot;&quot;,\n    &quot;&lt;/head&gt;&quot; : &quot;&quot;,\n    &quot;&lt;html&gt;&quot; : &quot;&quot;,\n    &quot;&lt;/html&gt;&quot; : &quot;&quot;,\n    &quot;&lt;p&gt;&quot; : &quot;&quot;,\n    &quot;&lt;/p&gt;&quot; : &quot;&quot;,\n    &quot;&lt;h1&gt;&quot; : &quot;\\\\begin{document}\\n\\\\toc: true\ntitle{&quot;,\n    &quot;&lt;/h1&gt;&quot; : &quot;}\\n\\\\maketoc: true\ntitle{}\\n&quot;,\n    &quot;&lt;h2&gt;&quot; : &quot;\\\\section{&quot;,\n    &quot;&lt;h3&gt;&quot; : &quot;\\\\subsection{&quot;,\n    &quot;&lt;h4&gt;&quot; : &quot;\\\\subsubsection{&quot;,\n    # &quot;&lt;body&gt;&quot; : &quot;\\\\begin{document}\\n&quot;,\n    &quot;&lt;/body&gt;&quot; : &quot;\\\\end{document}\\n&quot;,\n    &quot;&lt;ul&gt;&quot; : &quot;\\\\begin{itemize}\\n&quot;,\n    &quot;&lt;/ul&gt;&quot; : &quot;\\\\end{itemize}\\n&quot;,\n    &quot;&lt;il&gt;&quot; : &quot;\\\\begin{enumerate}\\n&quot;,\n    &quot;&lt;/il&gt;&quot; : &quot;\\\\end{enumerate}\\n&quot;,\n    &quot;&lt;code&gt;&quot; : &quot;\\\\begin{lstlisting}[language=Python]\\n&quot;,\n    &quot;&lt;/code&gt;&quot; : &quot;\\\\end{lstlisting}\\n&quot;,\n    &quot;&lt;li&gt;&quot; : &quot;\\\\item &quot;,\n    &quot;&lt;/li&gt;&quot; : &quot;&quot;,\n    &quot;%&quot;: &quot;\\%&quot;,\n    &quot;&amp;&quot;: &quot;\\&amp;&quot;,\n}\n\nClose Tags\ndef add_end_brace(list_of_vals, replacer_dict):\n    list_of_vals = [x.strip() for x in list_of_vals.split(&quot;,&quot;)]\n    for i in list_of_vals:\n        replacer_dict[i.replace(&quot;&lt;&quot;, &quot;&lt;/&quot;)] = &quot;}\\n&quot;\n\nadd_end_brace(\n    list_of_vals=&quot;&lt;h2&gt;, &lt;h3&gt;, &lt;h4&gt;&quot;, \n    replacer_dict=replacer_dict\n)\n\nImages\ndef figure_code(text):\n    found_links = re.findall(&#039;\\&lt;img .* \\/&gt;&#039; , text)\n    for link in found_links:\n        attrs = get_html_attributes(link)\n        caption_data = &quot;&quot;\n        file_path = &quot;&quot;\n        for i in attrs:\n            if i[0] == &quot;alt&quot;:\n                caption_data = i[1]\n            if i[0] == &quot;src&quot;:\n                file_path = i[1]\n        gen_latex = &quot;\\\\begin{figure}[!htbp]\\n\\centering\\n\\includegraphics[width=.75\\columnwidth]{&quot;+file_path+&quot;}\\n\\caption{&quot;+caption_data+&quot;}\\n\\label{}\\n\\end{figure}&quot;\n        text = text.replace(link, gen_latex)\n    return text\n\nCLI input\nags = ap.ArgumentParser(&quot;md2tex&quot;)\nags.add_argument(&quot;-f&quot;, help=&quot;Full file path&quot;, required=True)\nags.add_argument(&quot;-d&quot;, help=&quot;Insert default formatting code&quot;, action=&#039;store_true&#039;)\naps = ags.parse_args()\n \nf_name = Path(aps.f)\nRunning the pipeline\n# Read the file\nwith open(f_name, &#039;r&#039;) as f:\n    text = f.read()\n    html = markdown.markdown(text)\n\n# Replacing things\ntext = figure_code(html)\nfor key in replacer_dict.keys():\n    text = text.replace(key, replacer_dict[key])\n\n# Write the file\nwith open(f_name.parent/f&quot;{f_name.stem}.tex&quot;, &#039;w&#039;) as f:\n    if aps.d:\n        f.write(default_template)\n    f.write(text)\n    if aps.d:\n        f.write(&quot;\\\\end{document}&quot;)\n\nFin\nThis article is in the hopes that it will help someone out. Maybe have the help that I did not. I do not know who it will reach. But to whoever it does, best of luck :)\nLike these/Want more? Buy me a coffee! Kofi\nWant articles on something specific? Just ask!\nYou can always contact me on LinkedIn, or drop me an [[mailto:msubhaditya@gmail.com|Email]]. For all the code, drop by my Github."},"Articles/Drafts/index":{"title":"Drafts","links":[],"tags":[],"content":""},"Articles/FAQ/Commission-FAQ":{"title":"Commission FAQ","links":["tags/computervision","tags/python","tags/deeplearning","tags/shellscripting","tags/articles","tags/request","tags/scientificresearch"],"tags":["articles","computervision","python","deeplearning","shellscripting","request","scientificresearch"],"content":"Commission FAQ\nName\nFull form article about anything related to AI/Neural networks/Technical writing.\nDescription\nWant an article on something in AI? Go ahead and ask me for it.\nIf there are specific questions you want answered, you can request them as well.\nTopics you can request for (my expertise) - AI, Computer Vision, Deep learning, Neural Networks, Shell scripting, Python, Scientific Writing\nMy previous blogs :\nmsubhaditya.medium.com\nsubhadityamukherjee.github.io/AI-knowledge-base/\nI’d suggest you contact me first before you pay.\nYou can either drop me an email here : msubhaditya@gmail.com\nText me on LinkedIn : www.linkedin.com/in/subhaditya-mukherjee-a36883100/\nWord count isn’t really my thing but it would be a medium sized article - around 1k-1.5k words.\nPlease read the FAQs!\nInstructions and FAQ\nThe more specific your request, the better you will get an answer to them.\nPlease Please drop your name and email. It would make it so much easier to contact you.\nFAQ:\n\n\nWill you do my homework?\n\nProbably not. I take no responsibility for the grades you get.\n\n\n\nHow long will it take you to write my article?\n\nThat depends on your topic. Around 3 days is a good estimate. Although I would be able to give you a better estimate once I see the topic.\n\n\n\nCan I distribute this article freely?\n\nGenerally yes. You must link to my page and have my name on it though. If you are going to monetise it, well. I can’t really stop you but then I will monetise it too.\n\n\n\nWill it be public?\n\nThat is your choice. Generally, yes. But if you want it on your website/company website, then just inform me before. As long as you credit me properly wherever you post it, I’m okay with that.\n\n\n\nWill you make a whole framework for me?\n\nNope. You will get demo code. Full code etc will be a lot more time intensive so the rate will be decided based on the request.\n\n\n\nHow many rewrites do I get?\n\nMinor changes : two times.\nMajor changes : once.\nDelays suck for both of us. So if you want something specific, it’s better you tell me before I start.\n\n\n\nDelays\n\nI am not GPT-3, and sometimes I do face delays in getting work done. If I do though, I will keep you informed.\n\n\n\nRefunds\n\nOops, you didn’t like what you got. Apologies. If you can convince me that it was a badly written article, even after 2 minor and 1 major change, I will send you half your money back. After all, it did take a lot of my time. I don’t want to encourage scams, so I hope you understand.\n\n\n\nThank you!\nI hope we can both enjoy this article.\nIf you have any further questions, don’t hesitate to drop me a message or email at msubhaditya@gmail.com\nLinkedIn Post\nTL;DR AI Article Commissions - Open!\nko-fi.com/subhadityamukherjee/commissions\nI’m going to take a deep breath, and tell you that I’m now open to article commissions. Want an article on something in AI? Go ahead and ask me for it.\nIf there are specific questions you want answered, you can request them as well.\nTopics you can request for (my expertise) - AI, Computer Vision, Deep learning, Neural Networks, Shell scripting, Python, Scientific Writing\nMore info on the page linked here. This is not a free article. Although, if you do want one but can’t afford it, you can still drop me a message. I can’t guarantee it though.\nPS: This took a lot more than one deep breath. Let’s see how it goes.\n#commissioncomputervisionpythondeeplearningdeeplearningdeeplearningshellscriptingarticlesrequestscientificresearch\nPrices\n\nMinimum. Short article. 500 ish words : 30 euro\nMedium sized article. More in depth. Around 1k-1.5k words. : 50 euro\nLonger article. Fully in-depth. Around 2k-2.5k words. : 100 euro\nAlong with working code. : 200 euro\n"},"Articles/Mental-Health/ADHD-+-Autism---Challenges-and-hopeful-workarounds":{"title":"ADHD + Autism - Challenges and hopeful workarounds","links":[],"tags":["articles"],"content":"ADHD + Autism - Challenges, possible reasons and hopeful workarounds\nMotivation\nFinally, a manual for why I am such a wreck and how to salvage it! (Kind of.)\nThink of this as a lab notebook. You can pick whichever chapter you want to read. This obscenely long post is intended to be an experience log of workarounds that I have found over the years that have worked, most times atleast and their possible reasons. It is not intended to be a “oh if I do XYZ I will be free of my head~” but more of “oh~~, maybe that’s why doing XYZ broke my brain! I should remember that.”.  I wrote down whatever I could (clearly hyper focusing on this now don’t judge me please) in the hopes that sharing my understanding helps you find your own.\nFeel free to critique (politely thank you, don’t make me cry). But more importantly, feel free to start a discussion. I’m on a journey as well. I have too much left to learn. I would love to hear your stories too.\nDisclaimers\n\nThis is not scientifically backed but is purely anecdotal.\nI do “not” have my s**t together at all. Just because I wrote this huge article doesn’t mean all these points always work. Sometimes the Autism side of my head takes over and breaks down, sometimes the other. It’s a war, baby :)\nIt is a “spectrum” and you probably won’t relate to a few of them.\nConversely, relating to these doesn’t automatically make you have ADHD/ASD. Get a diagnosis if you can! But either ways. If it helps you, then that’s all that matters to me.\nI would love to add scientific literature to these points, but I’m scared it will take long enough for me to lose motivation and never end up posting. (Help me out please?)\nIf you disagree with anything, comment! I’m learning too. I fully admit that I might be wrong.\nI would love to have a discussion on these. Especially if you have more tips!!\nToday is one of the good days. And I am writing this whole article in one go. I don’t have the courage to do it in parts.\nI admit I do not have extreme symptoms. They ruin my life, but subtly sabotage it. I do not have any visible effects unless someone lives with me or sees me crying in the corner after my fourth breakdown when all I wanted to do was wash two plates so I could have lunch. My family does not know of my diagnosis. Some friends and my girlfriend do. My birth society would not accept me if they did. I am studying in a different country, so.. I have my privacy here.\n\nSomewhat categorised - Choose your own adventure\nTime\nTime blindness affects every little part of my life. But I have come to some workarounds to help me with it. Turns out, our brains update their internal clocks by changes in the environment. That was an important realisation for me. The following points relate to that.\n\nI used to listen to a lot of lofi music while working and always felt like time would stretch out to infinity. Well, turns out it does. Because it repeats, and doesn’t change enough. Now I actively try to switch up the music a bit.\nI have tried time blocking a bunch of times. But it inevitably always fails. It’s still useful though. I just have to forgive myself for not always being able to stick to it. But I tried you know?\nYou can use this to your advantage. If you want to slow down, make sure you have repeating things. If you want to speed up, do the opposite.\nI tried having clocks everywhere but only “analog” clocks helped. Maybe because they are more visual.\nTelling yourself “Okay this will take 1 hour” and then seeing that it actually took 3 is probably a good way to realise that you are setting yourself up for failure.\nI have tried to explain to people that my sense of time is a little warped. I don’t think they always understand, but atleast some of them try.\n\nADHD Meds\nI made it through 21 years without a diagnosis and forever wondering if I was completely looney. But yeah, a while back I did get one and was prescribed Ritalin. It changed my life, but it then ASD side started showing up a little more frequently. And I realized even more how chaotic my poor brain was.\nI am on my meds as I write this manual of course.\nWhat it helped me with:\n\nGetting my work done, the ones I don’t care about but still have to do.\nSlowing down a bit.\nHaving the space to see just how badly I was wrecking my life. (Cue. Also maybe helping me heal a little)\nRealising that the ASD side had been suppressed a lot and maybe that’s why nothing was working out. I was focusing too much on “fixing” the ADHD side but the other side wanted all the opposites.\nHelping me separately understand what the ADHD and ASD sides wanted. Not that I can fully balance their needs yet. It’s a work in progress.\nWhat it somewhat made worse\nRegular meals become harder because the meds suppress my hunger.\nSometimes I forget to take the right dosage and then oh god it’s down to the bottom of the emotional well.\nNow that I realise how badly the neurodivergent “spice” affected every part of my life, I’m almost angry. But I have to remember to be grateful too. Without it, I don’t think I would have come this far.\nI sometimes expect it to “fix” me. But it doesn’t of course. It does help a lot though.\nI have to remember to set an intention before I take them. Or I end up wasting my time anyway. But more focused.\n\nRejections\nI don’t want to throw terms around, but RSD (Rejection Sensitive Dysphoria) has shadowed me all my life. Like it probably has many of you. More than anything, I realise it hurts more because we are always trying to play “catch up” with everyone else. And.. honestly… it’s very unfair.\n\nRecently, I have been trying to have the courage to speak up and say “hey, that kind of hurt. I would appreciate some empathy.” Sometimes, it helps.\nI realize the ADHD side wants to talk a lot, keep talking and enjoy. But the ASD side can’t keep up with it. It gets very overwhelmed. I talk a lot, but when someone asks me something, I freeze. Then I stutter something or just, want to disappear into the ground.\nI think a lot of times, we are just expected to be able to do something. But we can’t and everyone just says “you are just lazy” when I’m not. I really want to do it. But the other side of my head wants to shut down forever.\nI’m learning to take breaks. Give my ADHD side conversation but then also take my time to soothe and recover.\n\nPeople.. and.. forgetting them\nAnother rather interesting and sad feeling I’ve had is that if I don’t interact with someone for a few days, they kind of.. disappear. I stop feeling anything for them. It’s not permanent of course. I still care a lot, and love the people I do. But, I forget to reach out. And.. they get hurt.\n\nMost of my life, I blamed myself for being well, I dunno what. “You forgot to even text your girlfriend today, come on man. You love her right? You can’t even remember she’s there because she’s in a different country right now? Such a dumbass”.\nAt some point, I realized that it’s the same thing with me and cupboards. If I wanted to stop eating chips or cookies, I would keep them at the back of a cupboard and then forget they ever existed. I vaguely know they are there, I just don’t act on it or feel anything towards it.\nNote to self. If you care, try to make semi regular contact. It will help you “renew the connection”.\nNote to self P2. You are not a sociopath. You are neurodivergent. You still have feelings. Too many of them lol.\n\nOrganisation and Planning\nGods. I’m so bad at this it is not even funny. Some things arguably make it slightly easier though.\n\nI realise I get frustrated because I don’t have parts of the puzzle and  in spending time trying to find them, I lose all my motivation to work. So instead, I first try to collect all the information I might need for something in one place. I do not start actual work before I do.\nI try not to do things in parts. I sit down and finish things in one go. If I can.\nSimplify, simplify, and simplify some more. The more choices you have, the harder you make something. Pick the essentials. You cannot plan everything and you will get overwhelmed.\nAsk for help. I know, I know. But it’s okay. Pick someone who might understand if you can? In time, maybe we can be better at it too.\n\nMasking - General\nI admit, I don’t fully understand to what extent this affects me. It is a lot, but maybe in time I will understand it more.\n\nIt started ages ago in school - when I didn’t look people in the eye, or when I spoke in monotone, or when I would not talk at all, or when I spoke too softly. My mom, or teachers would tell me I wasn’t “normal” and tell me what to do instead. And they would be “very” upset about it if I didn’t. So I started pretending. I would look people in the eye, I would talk louder, I would “emphasise”, heck I pretended to be emotional too sometimes.\nI didn’t realise it was slowly taking over though. In time, nobody believed me anymore when I said I was different. They’d say, no you cannot have ADHD/ASD. You are too “normal”. Well, another one bites the dust I suppose.\nWho are you when the dust settles and you are alone in your room crying your eyes out because it hurts so badly that nobody ever understands you? Give that person a long hug because oh god, they tried their best.\nI tried so hard, and go so far, but in the end it doesn’t even matter~ . There is always a price. At the end of the day, who are you living for?\n\nMasking - ADHD/ASD\nAh this is a nice little cocktail. Do they cancel each other out? Not really. I just feel like I am always at war with myself. I do not have workarounds yet, so I will just list down what I face instead.\n\nI really want to do the dishes because warm soapy water is fun, but the other side wants to shut down forever and scream.\nI make plans to hang out with new people, but the other side wants comfort and people I know already.\nI want to plan novel experiences, new trips, new food etc, but the other side wants to do none of them and sit at home and have ramen.\nI am horrible at conversations and I die inside each time someone asks me something I don’t know, but the other side of me learns to adapt so fast nobody even realises that I’m screaming inside.\nOne side of me has gotten too good at pretending to be normal, while the other needs days to recover from all this masking.\nI technically should not be able to work as much as I do, but when I hyper focus, I am unstoppable. When I don’t, I am a carrot.\nI like working on new projects, new hobbies etc, but the other side wants to sit and do the same thing everyday because new things are scary.\nI beat myself up so badly and get good grades so everyone thinks I’m a genius and have it so easy because I get so much done. Ah. Well. Madame, there is always a price\n\n(I think my motivation is running out now. Or maybe it’s because I didn’t have lunch yet.)\nSocial Media\nI think this kinda applies to everyone. But more so to us. Doom scrolling is the bane of my existence. But I am trying to use what I have learnt about my brain to make it easier to cope.\n\nBrain loves dopamine right? Stop giving it that. Make social media a chore. Open instagram on your browser, step back and see how “boring” all the reels are. Do you really want to watch someone make a coffee for the fourth time today? No you don’t.\nShove it in a cupboard. Forget it exists. For a while atleast.\nWrite instead. I would never have been able to write so much before. But ever since I replaced my doom scrolling with writing, it’s easier.\nIt doesn’t always work. Yeah. Sorry. You can’t fully forget about it. It’ll come back. You’ll waste hours on it. But… atleast you saved a couple.\n\nA thank you note\nThere are so many people I want to thank here. I sadly do not know who most of them are.\n\nJessica and their team from HowToADHD (www.youtube.com/@HowtoADHD). I am so grateful to you I want to run to you from the other end of the world and give you one huge huge huge hug. Thank you. Thank you so so much. (Am I really tearing up now? Yes.) I don’t have words but you have my eternal gratitude.\nReddit and all of you. Feeling like I am not alone here makes things a lot easier to handle. I am very grateful to you for giving me that.\nEvery website that gave me tips on how to manage my life. Not all of them helped of course, but they were a start.\n"},"Articles/Mental-Health/What_I_learnt_about_myself_after_two_years":{"title":"What I Learnt About Myself the Past Two Years","links":[],"tags":["articles"],"content":"What I Learnt About Myself the Past Two Years\n\nIrrational rage with task interruptions : probably because I am not sure if I can come back to the task now or ever, or how long the interruption will last.\nBeing told to do things right when I am just about to do them makes me feel like control is being taken away from me. And since I barely have control usually, that just frustrates me a lot.\nCleaning is hard because it feels like a big thing. This could be also similar with things I suck at, and the energy cost for it is crazy.\nIf I clean everything, the next time I have to do it, it feels like it’s a brand new task. And brand new tasks take a lot of energy\nI always only do upto 80$ of something. Just by this point if I knew about ADHD as a kid I would have been diagnosed already.\nI am slowly learning that masking does take a lot of energy, and when I don’t have it it pisses me off\nWhen I get hungry my ADHD is at max\nADHD is not really about not being able to focus but more of being unable to regulate it\nIf I change the food I want slightly then it makes it a looot better. Even if that means adding some tiny sauce to it\nExercise REALLY helps\nSometimes simple things that you don’t consider to solve anything solves anything : like the breathe right nasal strips?\nShowers help me regulate\nI love running, a lot\nTravelling on trains is not bad if I take care of noise and have something to do\nIf I plan my day before doing anything or have something to look forward to, it really helps\nLong conversations are also fine if I am drawing or have something to regulate my emotions\nLow energy people are so nice to be around though. Esp other neurodivergent people. Gods.\nI have more friends than I think I do\nPeople love talking about things they care about. I asked Manon for running shoe recs and it’s one of her special interests apparently\nI get angry a lot because I feel like my boundaries are being crossed but I have grown up ignoring them\nMayu needs space in between things like space when she comes back home.\nMom likes to talk and she does not super care if someone pays that much attention but atleast cares enough to give her some attention. Makes sense right?\nI dont actually hate TV. I just need something else on the side to regulate because my empathy kicks in a lot of times\nI sometimes get too much input from things and then need to take some of it out. Like write something, draw something whatever\nIf I start getting annoyed with something that I love, it means that I probably hyperfocuses the absolute shit out and should probably give myself a break before it gets too much\nI have learnt how to identify when my energy is low\nMeditation is hard but the count backwards from X with 3 repeats of X per breath is the best\nMom really does have ADHD. No wonder she switches topics three times in every sentence\n"},"Articles/Mental-Health/index":{"title":"Mental Health","links":[],"tags":[],"content":""},"Articles/Others/A-letter-to-my-junior-school-art-teacher":{"title":"A letter to my junior school art teacher","links":[],"tags":["articles"],"content":"A letter to my junior school art teacher - David Fitzgerald\nDear David sir,\nThis is Subhaditya, or Vishal as you knew me back then. I barely remember how many years ago it was. You had recently started teaching art in my class then. I have these vague memories of awe when I saw you draw on the blackboard with a marker. It seemed like magic almost, you would draw some lines and suddenly, presto! It was a horse. Or a dragon. Or a face.\nDid you know I have loved dragons since then? You always started them with the letter 5.\nI remember pestering my parents about art. Back then I used to doodle and draw all the time. When I went to Kolkata for the summer break, my parents enrolled me in a class for painting. It was with my cousin brother, and I hated it. It felt so stifling. I do not remember the chronology of this, of course, I was in the fourth grade I think. A little boy who just wanted to draw dragons.\nAt some point, mom decided to ask you if you were willing to teach me art after school. Once a week. I think it was on weekends, I cannot remember. You did not take tuitions, and I was just a child anyway. But somehow you agreed. I remember going to your place the first day with an art book. You had a pet chameleon that was fascinating to me. I remember that it jumped on me. I can still feel its tiny claws on my shirt.\nYou showed me a tattoo machine. An airbrush. You painted horses. You painted dragons.\nYou made me paint a dragon.\nI loved those days. At least, I think I did. After all, I had forgotten about them until now.\nYou gave me a little pebble with a white horse painted on it. I still have it back at home.\nI started painting properly during the pandemic in 2019 when I found art videos online. It’s been a few years since then. But if I think back, I remember traces of me trying to draw all my life. I never did it properly though. I did not realize it was something that I could do.\nIt is almost the end of 2023 as I write this, I graduated with a masters degree in AI a few days ago. In my hunt for a job, I came across one that had the toc: true\ntitle of “Creative Technologist”. It was an entry to art! As I write this, I am waiting to know if they accepted me for the job. It will be my first one.\nWhile trying to find my way of art and thinking about the job, I suddenly remembered those days with you. And then I realized why I had stopped drawing. And… what made me fall in love with it in the first place.\nWill you take a guess?\nOf course. It was. You.\nYou, David sir. You.\nBut life is not easy is it? I wonder if things had not gone the way it did, and you had stayed, perhaps this life of mine would have been very different.\nI remember walking into class one day and seeing a new lady. Maybe she was a nice person, I do not know. I remember her trying to teach shadows with a coat hangar.\nI missed you. I wanted to know where you went.\nI don’t remember how but I found out that you had tried to kill yourself. I remember hearing that you set yourself on fire, that your skin had melted off. I remembered those hushed conversations.\nI vaguely remember seeing a picture, or did I actually see you? I do not remember. As a kid, I did not understand what it meant. I thought you left because they thought you looked like a monster.\nAs an adult, I wonder how it must have felt. Many years after that, I had long forgotten these things, wiped away in the warm embrace of forgetfulness. I blocked off my art without realizing it. It broke me I think. There were many factors, of course. But that is not the point of this letter. In my darkest times, I wanted to kill myself many, many, many times. I wonder how it felt when it didn’t work. When you were left with the ashes of a broken life. I wonder how you are now, more than a decade later.\nI wonder if you were made to feel like a monster for the gift that you had. I wonder if it tore you apart. I wonder what set you on fire. Was it you? Or was it them?\nThe real demons are not us are they? You were not a monster then, they were.\nAfter I rediscovered art, I started getting better you know? I no longer wanted to die. I didn’t realize that that was what saved me in the end. I just did while writing this letter to you. Art showed me the world for what it was, not what I was told it was.\nI tried to find you online, but I can’t. I wonder where you are now. If you are even alive. I don’t know. Maybe one day our paths will cross. Maybe they won’t.\nI want you to know that I have been painting almost everyday for the past few years. I have a long way to go, but I am getting there. Art has made my life brighter. I wish I could show you my work. I wish I could tell you that I went to two art conferences and got to meet some of my favorite artists. I wish I could tell you that my friends have my art hanging on their walls. Yes, i gave it to them. But they cherish it, I think. Yes, I still do not feel like I am at a point where I can accept those compliments.\nI suddenly remember something you said once. A classmate had drawn something poorly and he expected anger, like all of our other teachers had. You stood in front of the class and said, “Let nobody tell you that your art is bad. There is no bad art.”\nI wonder if you feel the same way about yourself.\nI realized that art is not just our work, it is us. We are creatives. We live, breathe and exude creation. I do not know why. I don’t think anyone does. I realized that if we stopped creation, it either destroys us, or finds the cracks in our soul and gets out somehow.\nYou will probably never see this. Even if you did, I doubt you remember any of these. But, I do now. I see you sir, Mr.David Fitzgerald. I see you. I see your pain. I was a child then, but I’m all grown up now.\nThis is making me cry, so I will end this here.\nThank you sir, you started this journey for me without me realizing it. There are so many things I want to say that I can’t. If you are out there somewhere, I want you to know that you made a difference in my life. Perhaps this journey towards art was complicated by your pain, but perhaps it was the only way I could process it. After all, you were my art hero. As a little kid, what could hurt more than knowing that heros could die too?\nThank you David sir, for the horses, dragons and the magic. Thank you.\nWherever you are, I send you my love and affection. Little Vishal to you. I hope the fire that consumed you showed you the road you were scared to take, and I hope you followed it.\nThank you. For everything.\nSubhaditya Mukherjee\n17th October 2023"},"Articles/Others/Ai_for_startups":{"title":"Ai_for_startups","links":["tags/deeplearning"],"tags":["article","deeplearning"],"content":"AI For startupdeeplearning\nHitchhikers Guide to AI for Startups\nAre you part of a startup, or want to start one? Want to have AI superpowers but don’t know where to start? Want to make sure you don’t sink your boat before it floats? Make sure you hire the right people and avoid blowing your budget out of the water. This article is for you.\nA helpful guide on what to focus on, resources you need, and punching common myths in the face. Sounds interesting?\nRead on.\n(This is a long article. Skip to what you need. But if you are just starting, I would recommend reading the whole thing. Take your time. Take notes. Drop me an email with your questions. msubhaditya@gmail.com)\nOkay, now, DONT PANIC.\nA definition\nBefore we move on, let us first define what we mean by AI. Of course, there are quite a few definitions but for our purposes, we can define it as “any means of injecting a form of semi-automated intelligence that either performs a task previously impossible for computers or does a task as good as/better/faster than a human. An AI is used to learn how to perform a task, and can be thought of as a more advanced software.”\nNote for AI experts: The term AI is used in the article as a general word, interchangeably with deep learning, neural networks, etc. Yes, technically this is inaccurate. But this is an article for people with not much in the way of experience.\nSome useful terms you might need\n\nAI model/architecture: The brain that makes up an AI\nGPU: A specialized computing unit that accelerates intensive computation, like training an AI\nNeural Networks: Inspired by the brain, what makes “AI” tick\nLatency: How long does your model take to give results\nSOTA: State of the Art. What is the best model for the task, right now?\nTraining: The process where the AI learns how to perform a task\nInference: Using a trained AI to predict some results\nCluster: A group of computers used to parallelly perform a task\n\nA little index\nThis article is divided into three major sections.\n\nIf you want to dive into this field for the first time, or find ways to inject some AI into your companies, the first section is for you.\nIf you already have an AI startup, and are looking for ways to improve your infrastructure so you can grow, the second section is for you.\nThe third section talks about some of the pitfalls that one might face when they first dive into this space. Take it as a word of caution.\n\nSection 1 - The beginning stages Or How should I care about “AI”?\nCutting through the hype\n\nUnrealistic expectations\nAt the end of the day, AI is not a magic mushroom. It cannot solve everything you want, and neither can it evolve and take over the planet. Yet. Can you use it for your needs? Undoubtedly. So what can it do? It can categorize images, translate text, understand what it hears, recognize tumors, and anything your creativity allows.\nThe key is to set realistic expectations. Think of it this way, if you can get a team of experts to do the task, then it might be possible to have an AI do it (Terms and conditions apply).\nAsk an AI consultant, or if you cannot afford one, look at this website [insert paperswithcode] for a list of tasks that are possible. If nothing else, it can give an idea of what can be done.\nSometimes it might be possible for your idea to work in the long run, but it might just take more time and resources than you imagined. Only an expert would be able to tell you if it is a feasible plan. Expecting your developers to come up with something impossible is great, but only if you can afford it.\nFor example, instead of trying to make an “AI that will take humans to Mars”, break it down into smaller tasks - “AI that recognizes space debris” + “AI that would identify system faults” + “AI that might predict what a type of rock found was” + … etc\nData requirements\nIt is a myth that to “train” an AI, you need massive amounts of data. Yes, technically you do. But recently, there has been a lot of research conducted on “transfer learning”, which is a technology that allows you to start with an AI trained on large amounts of data, and fine-tune it to your specific use case. This is very helpful, especially if you are working on tasks similar to those that exist. For instance, if you want to train an AI to recognize different types of cars, this might not need a massive amount of data, because similar “recognition” tasks exist. But if you want to classify a hundred types of new tumors, that might require a little more data.\nExtreme requirements\nThe Tech giants want everyone to think that we need extreme computing power for our AI needs. But in reality, most companies can start with minimal requirements. Even if you cannot afford huge computing clusters or lots of computers, you can use online services to run the code. There are quite a few such services that provide “GPUs” (special computing units that accelerate running compute-intensive code). They charge you a fee by the hour, which is often quite cheaper, even at scale. As long as you have skilled workers, decent computers, and funds to support it.\nA word of warning, AI is still an experimental field. If your staff does not deliver, it would serve to try and understand if the task handed to them is realistic, or needs more resources. An AI expert/consultant would be helpful here.\n\nSo you want to be an AI startup?\n\nIs AI needed?\nThe rush to say “We use AI in our products to do XYZ” is very intense. Do not fall into peer pressure. It serves to first understand and identify where you need AI. Does a specific part of the infrastructure need to be revamped? Is there something you want to do that just normal programming cannot handle? Have you tried other options first?\nAI is expensive in the long run. Can you afford the staff? Pay for the time taken to research? Test some ideas, you might be able to hire a freelancer to make some mockups before diving in headfirst.\nAI products vs AI as part of your existing infrastructure\nYou might not need to fully use AI, maybe only some parts of your idea need it. If you are a new startup, please note that just AI will not be the end of all. Even if your idea is a pure AI product, there are quite a few software engineering components in it as well. It serves to understand the pipeline before starting, just so you do not fall into common traps and end up wasting time and money.\nDomain knowledge\nDoes your product revolve around a specific domain? Hire/Consult an expert. You will suffer badly otherwise. Yes, an AI model will give you results. But you will have no way of knowing how well they do. Just having numbers is not enough. In return, this will help you make even better software.\nSurvey\nUnderstanding your customer base is even more important here. The better you understand the requirements, the better you can make use of AI. The more you can flesh out your ideas, the more specific you can get with your software.\nCompetition\nWho is your competition? Do you have the time to revamp your software? Everyone wants AI these days. Some companies can spend millions on it, while others can’t. If you are in a field that has cutthroat technology developments, just starting up might be harder for you if you cannot afford it. If you are a new startup, try to focus on fields that need solutions but do not have larger corporations working on them. Domain expertise is great here.\nUsing products from Google/Microsoft/Amazon\nYou will find every big company these days offering AI support. Should you use them? In my opinion, these are useful if you are planning on performing a very common task. If you have a domain-specific idea, making your own is probably the better bet. But do not hesitate to make use of the resources that they offer. Google Colab, Amazon AWS, and Microsoft Azure are great services. Using them, especially at the start, is a good idea. They could be a cheaper way of testing out your ideas.\n\nRemember, AI is a tool. Not a complete product.\nSection 2 - The mid stages ,Or, How can I make sure I make my best “AI”?\nFocus\nBeing an AI startup, you have a lot of things to look out for. Some of the main focus points to keep in mind are as follows.\n\nClear goals of what you want to achieve\nWith any endeavor, knowing exactly what you want is imperative. More so in fields where testing takes up quite a lot of time and money. The better and more fine-grained your explanation, the better your results.\nClear expectations on what is possible within a time frame\nI repeat, AI is a research field. Just because Google can make a huge product in a matter of months, does not guarantee you can too. An expert can help you set more realistic goals. You can, of course, try it out on your own, but only if you have the time and the risk appetite for the same.\nEasy of use\nMake sure your service is easy to use. Using AI will suddenly show you how many knobs and switches you need to control. Do not overwhelm your poor users. Specific research in understanding what your customer wants is essential.\nMultiple large platforms\nScaling up is probably not a major concern for you right now. If it is though, make sure you can afford it. If you can’t, see if you can outsource it, or use an online service. Perhaps also consider other ways of optimizing your workflow.\nQuick, rough tests\nBefore you work on a final product, try out some small ideas. Hire freelancers if you do not have full-time staff on hand. Try out different models with any data you have on hand. Making sure you have a good baseline will save you quite some headache later on.\nValidation testing\nBy this point, you might have heard me say “test your ideas” quite a lot. But again, test your models. Use new data, use crappy data. Does it still work? If not, keep working.\n\nPeople\nAside from being able to build AI models, you also need people who would be able to support the infrastructure. Make sure you have domain experts you can call on. Also find people who would be able to deploy the model onto a chosen service, and can maintain them.\nOf course, the usual requirements of making an interface, servers, etc still stay based on the type of project you have in mind.\nBudget and requirements play a key role here and are specific to your idea and scale.\nSection 3 - A word of caution How can I make sure my boat does not sink?\nCapacity\nOh? So you made an AI model? Congratulations. Now you need to get your users on board. Regardless of if you have an app, a website, or a device, make sure you can estimate how many users you will have. AI models tend to take more resources than classical computer software.\nIdentify how long your model takes to get results. Will your users be fine with that? Do you need more resources to ensure faster outputs? Will they be able to reliably access your service?\nTest, test, test.\nThere are quite a few optimizations possible. It is beyond the scope of this article, but you can drop me an email if you want to know more.\nModel drift\nOver time, how good your results are will drift. This might be because the data your model learned differs from the model it is getting now. For example, in a fashion-related product, the trend of clothing changes over time. Using an old model that has not seen new data will not be great.\nPeriodically checking for a spike in errors, and retraining the model on new data is essential. Make sure your employees know how to do that.\nBias and ethical concerns\nIf you teach a kid that colored people are evil, that’s what they will learn. Similarly, an AI model can learn mistaken assumptions. The larger the data, the more such assumptions are automatically made. Identifying these would help you understand if your product would have unintended consequences.\nThe more variation you can provide to your model, the better in the long run.\nMake sure your data is inclusive. Especially if it contains hints of the biases of age, gender, ethnicity, etc.\nTest for these biases specifically.\nMake sure that you can explain if your model goes wrong and does something stupid.\nThere is a great course on how to do this Ethics for AI by Rachel Thomas from fast.ai.\nFin\nSo, will you use AI, or will AI use you\nFeel free to reach out or leave your comments. Email: msubhaditya@gmail.com"},"Articles/Others/An-understanding-of-will---Part-1":{"title":"An understanding of will - Part 1","links":[],"tags":["articles"],"content":"An understanding of will - Part 1\nWillpower is illusive. In essence, it would be something that you don’t encounter unless you see an extreme. Someone going through insane hardships to make ends meet, a mother lifting a car up to save her child, people struggling through all odds to escape oppression. But in real life, sometimes you encounter people like that. Ones that have lived entire lives with the ability to drag through. Hell, high water or drought. They will pull through anything. The question of course, is.. how? How can someone pull through being a 95 year old grandmother with every last living direct family member having passed on; in the last decade? How can one push through holding their husband, their child, their siblings as they took their final breath? It truly shows how strong we are , when we want to be.\nWillpower is fleeting, yet all pervasive. Sometimes it only lasts for a few moments. Sometimes it lasts for years. Sometimes it is that frantic burst of energy we get to clean our rooms and get our life straight at 3 am when we cannot sleep. Other times, it is people working towards a common goal for years on end, building schools, cities, empires.  How do any of these people prevail through ages of discomfort, misery, pain and what would seem like an inhuman amount of suffering?\nI think it boils down to a few core ideas.\n1 - A reason\nFor someone to get up at 6 am everyday to go for a run, there must be a reason. Without one, they are not better than a caged animal pacing around. Be it making a difference in someones lives, or as penance, or as a way of redeeming something that was lost. These are motives we hear all around us. But there are other, more subtle motives that show up from time to time. One persons dream of bringing their poor parents into a new home. Another person’s dream of building a school for women who were never given a chance to learn. Another of providing wartime health services. All of them have something in common - emotion. Long, deep, slow. An ache that must be massaged.\n2 - Challenges\nThrough time immemorial, we have never faced a lack of problems. Most people wait for someone to solve them, some try and some others succeed. Through all of these stories, we see that most of these people were not exactly taught the hardcore perseverance that they show. Some had it beaten into them, others had no choice but to get their shit together, others were running from something they were too scared to face.\n3 - Beliefs\nAt the end of the day, we are the sum of our thoughts. The first time you realise, just how much your life is influenced by your perspective on life, it changes you. There are people who are not willing to believe that they would lie down and let their fate drag them. You see people like this, they stick out like a sore thumb. You know people like this, maybe a grandmother who faced war and famine but still is the nicest person you know. Or a parent, who against all odds, raised healthy and successful children. Or even a friend, who went against everyone and everything to pursue a career nobody approved of.\n4 - Consistency\nThe person who wakes up at 6 am and goes for a run everyday, would be hard pressed to be eating chips the minute they wake up. But if they let themselves go for long enough, the efforts of years of discipline is lost. It is truly strange if you think about it isn’t it?\nFrom these ideas, what can we think of that will make us… mentally stronger over time? If you start looking, you will find countless pieces of text that try to answer this question. This particular article does not aim to be more definitive than any that preceded it, but collects the stories the author has had the fortune of hearing.\n1 - Do something hard every day.\n2 - Document your day, to yourself, honestly. Keep a journal.\n3 - Find a few things that truly matter to you. Be it a sunset, or people, or that book you’ve been reading. Beware though, that this will change in time. You will not have the same thoughts forever. What you like and care about will change. The point is to just have something that matters.\n4 - Discipline really does matter. In times where sticking to a routine does not really seem feasible, look at the tiny things that you do. Did you tuck in your blanket? Drink water right after you wake up? Light a candle? Wind down? In times where all else fails, these little things give you the sanity that you would not find otherwise. Look inside, you will find your own little routine.\n5 - Being part of a community. We need people. Hopefully some of those people need us too. It also helps if those people share a hobby with you.\n6 - Finding the joy in the little things. It goes a long way. Sometimes a warm bowl of soup gives you the motivation to continue.\n7 - Light your own candles. If you keep waiting, they will just remain dark and cold.\n8 - Give yourself a little reward, if you can. If you learn to love the hardships, then you can be better at facing them\n9 - Learning some form of meditation or prayer seems to help tremendously."},"Articles/Others/Duolingo-is-a-platform-for-language-learning":{"title":"Duolingo is a platform for language learning","links":["tags/deeplearning","tags/inprogress","KB/CIFAR"],"tags":["article","deeplearning","inprogress"],"content":"My Take on Some ML Interview Questions - P1deeplearninginprogress\nChip Huyen is one of my favourite authors in the space of MLOps. She has some great blogs, and a really useful book. In one of them, she asks the exercises-rWl8SQW. This blog post is my answer to the ones I felt I could contribute interesting points to. Since there are quite a few, I will probably split them up into parts.\nNote: These are my views on these questions. They are not a comprehensive resource by any means. Just me thinking out loud on how I would go about solving the problem. Like any research project, as more time passes, these answers might change. They are here as a way for someone starting out to get a feel for approaching problems posed to them.\n\nDuolingo is a platform for language learning. When a student is learning a new language, Duolingo wants to recommend increasingly difficult stories to read.\n\nHow would you measure the difficulty level of a story?\nGiven a story, how would you edit it to make it easier or more difficult?\n\n\nYou run an e-commerce website. Sometimes, users want to buy an item that is no longer available. Build a recommendation system to suggest replacement items.\nWhen you enter a search query on Google, you’re shown a list of related searches. How would you generate a list of related searches for each query?\nEach question on Quora often gets many different answers. How do you create a model that ranks all these answers? How computationally intensive is this model?\nHow to you build a system to display top 10 results when a user searches for rental listings in a certain location on Airbnb?\nWhen you type a question on StackOverflow, you’re shown a list of similar questions to make sure that your question hasn’t been asked before. How do you build such a system?\nOn social networks like Facebook, users can choose to list their high schools. Can you estimate what percentage of high schools listed on Facebook are real? How do we find out, and deploy at scale, a way of finding invalid schools?\nHow would you build a trigger word detection algorithm to spot the word “activate” in a 10 second long audio clip?\nIf you were to build a Netflix clone, how would you build a system that predicts when a user stops watching a TV show, whether they are tired of that show or they’re just taking a break?\nFacebook would like to develop a way to estimate the month and day of people’s birthdays, regardless of whether people give us that information directly. What methods would you propose, and data would you use, to help with that task?\nImagine you were working on iPhone. Everytime users open their phones, you want to suggest one app they are most likely to open first with 90% accuracy. How would you do that?\nHow would you train a model to predict whether the word “jaguar” in a sentence refers to the animal or the car?\nHow would you create a model to recognize whether an image is a triangle, a circle, or a square?\nGiven only CIFAR-10 or not?\n"},"Articles/Others/Easier-Deep-Learning-Research-for-Beginners":{"title":"Easier Deep Learning Research for Beginners","links":["tags/deeplearning","KB/Mixup"],"tags":["article","deeplearning"],"content":"Starting Deep Learning Research (PART 1): A start using FastAIdeeplearning\n{{TOC}}\nSo, you took your first steps into Deep Learning. Maybe you read a few articles, did a course or two, and watched a bunch of videos. Or maybe you just heard so much about it that you wanted to learn more.\nWelcome.\nThis is a beautiful world. It is also very overwhelming. There is so much to learn and understand. But we need to start somewhere.\nThis is your ticket. Enjoy the ride.\nNote: This will be very long-winded as it is meant for complete beginners. It might seem very scary at first. But don’t panic. The hardest part is getting started. Hold on. Come back to it. It will take time. Slow down. Read through it. It will save you a lot of pain later on.\nEntry Requirements\n\nGo to this link for the code and follow along kaggle_notebook. Once that opens up, click the button next to “Accelerator” and choose GPU. Accept the terms.\nYou have learned some Python. If not, go to YouTube and learn as much as you can first. Do as many examples and problems as you deem enough to understand. Come back.\nYou know what a GPU is and if you have one.\nYou have a computer and an internet connection.\n\nIf you have a powerful computer, you can set this up locally. fastai\n\nYou will need an editor. I would recommend VSCode.\nIf you do not, that’s alright. You can still follow along.\n\n\nWatch a small tutorial on how to use a Jupyter Notebook here.\n\n\n\nWhat we want to do\nToday we will be teaching our little “AI” how to categorize different fruits. To do that, we need to give examples - aka the “dataset”. You can grab it from Kaggle.\nWe will show our little “AI” quite a lot of images and tell it “hey, this is an Apple, this is not one” and so on, in the hope that it will learn.\nThe exact algorithms are not important at this stage. I believe it is first good to be able to see where to use it in practice and play around with it before diving deep.\nIn the end, we want our “AI” to accurately classify the fruits that it has learned about.\nLibraries we will use\n\nMajor Framework : Pytorch. Deep learning is quite a vast field, which means that code quickly starts to become complicated. There is a lot of boilerplate code that needs to be written every time you want to make something. Writing the same 200 lines of code every time is not efficient, and leads to bugs and weird errors. To avoid that, we use a framework that handles most of the hard work for us so we can focus on innovation and application. There are quite a few of these frameworks, PyTorch and Tensorflow/Keras are the two major ones in Python. \nI prefer Pytorch. There are many reasons for that. Instead of me adding to this blog about them, read this little article by the creator of the next library we will use, link\nfastai FastAI was one of my first introductions to research grade Deep Learning. It is a wrapper around Pytorch that does live up to its name. Pytorch, being a general framework, focuses on the core components of Deep Learning. FastAI was made to provide a lot of ease-of-use functions that Pytorch did not. But because it’s just a wrapper, any functions that Pytorch offers can be easily accessed.\n\nGrab the Data and Set Everything Up\n\nMake an account on Kaggle. This is not promoted or anything. It is probably one of the biggest hubs for AI code, and you will make an account sooner or later. Better to do it now.\nIf you are using this link, click the 3 dots, and then click “Copy and Edit”. That will set up the data and the code for you.\nIf you are working on your machine, you can download the notebook by clicking the 3 dots, and then clicking “Download code”. Open this up in VSCode. The data can be found from Kaggle.\n\nCode Imports\nWe will need to first import the libraries we will use.\nimport os\nfrom fastai.vision.all import *\nfrom fastai.vision.widgets import *\n \n# Set the base directory where Kaggle saves its data. Change this if you are on your machine.\nroot_dir = &quot;../input/fruits/fruits-360_dataset/fruits-360/Training&quot;\nLoad Data\nFastai provides quite a few convenient functions to load data. Now slow down. What do we have? Think about it for a second.\nWe have an image and a label right? And we need to “map” the image to the label. If this does not make sense. Think about it some more. This is important.\nOkay.\nNow, what does the file path for an image look like?\nSomething like : “../input/fruits/fruits-360_dataset/fruits-360/Test/Cantaloupe 1/r_140_100.jpg”.\nWhat will the label be? “Cantaloupe 1” right? This is the “parent” of the path, aka the parent folder.\nThis fully depends on how the data is structured. More on that later.\ndef get_parent_name(x): \n    return x.parent.name\nOkay, now we need to tell the system the kind of data we are giving it.   In this case, it is Image → Category (Label). Then we can find all the image files in the folders, and use the “get_parent_name” function to assign labels to all our data.\nWhen we learn something in school, we are tested on it with questions we have never seen right? This helps us understand if we comprehended the topic or not. Similarly, we split the data into Train and Test sets.\nWe also resize the images to a single size (for convenience).\nSince we want our little “AI” to recognize objects from any angle, lighting condition, etc, we provide some “augmentation” to the data. Things like randomly changing the brightness, rotating the image, etc. The objective is to provide variation, that further improves how well our network learns.\nWe can then pass these instructions are create a “Data-loader”. The definition is the name itself.\npath = Path(root_dir) # base path\nfields = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_items=get_image_files,\n    get_y=get_parent_name,\n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    item_tfms=RandomResizedCrop(64, min_scale=0.5),\n    batch_tfms=aug_transforms(),\n)\ndls = fields.dataloaders(path)\n‌\nOkay. Does this work? Let us get a sample (or a batch as it is called in this field.)\ndls.show_batch()\nHow many categories of fruits do we have?\ndls.c # no of categories\nTeaching our AI: Part 1\nWe have the data. Now we need the “AI”.\nYou must be wondering why the word has been in quotes the whole time. It is because what we are training is not an “AI”, but just a component of it, called a “Neural Network”.\nlearn = vision_learner(dls,\n                resnet18, #architecture\n                loss_func=LabelSmoothingCrossEntropy(), #loss function/objective\n                opt_func=partial(OptimWrapper, opt=torch.optim.AdamW), # Optimizer\n                metrics=[accuracy, error_rate],\n                cbs=[MixUp]).to_fp16() #callbacks, mixed precision\nThe only things you need to know about it for now are :\n\nThere are many types of networks, each better at something else. These are called “architectures”. ResNet18 is one of them.\nYou can train a network to learn a specific type of mapping. Be it an image → text, audio → labels , image → image etc.\nThis training takes time and is mathematically pretty hard. Which is why this field is still in research. Further explanations about all of this will be given in later articles. (And others that I have written about in the past, linked at the end.)\nIn addition to the architectures, we have algorithms called “Optimisers” that enable smoother learning. This will not make any sense right now. But all in due time.\nA “loss function” is fancy speak that is a way of seeing how well our network is doing while it’s learning. A mini exam if you will, in the sense that the network tries to “optimise” for this exam. The better it gets, the better your final results.\nMetrics give us somewhat precise values. Such as the grade in an exam. The “Accuracy”. Etc.\nThe other weird things that you see here are:\n\ncbs: Mixup\n\n\n\nTeaching our AI : Part 2\nLet the Neural network.. drumroll.. learn!\nlearn.fine_tune(1, wd=0.5)\nlearn.export(&quot;model.pkl&quot;) # Save the model\nFine-tune? Huh? 1? Huh? Don’t worry too much about that right now. It is called Transfer Learning. And we are training the model for 1 epoch. (One run over all the data.). More epochs generally lead to better results.\nAND WE ARE DONE!! We have a working model already. This can recognize fruits. Congratulations!!!!\n(Too many words and terms. How will I ever learn them all? I should just give up now. Breathe. One thing at a time. Go through this. Come back. Go through bits you do not understand. More intermediate articles will follow. But in the meanwhile, the links are excellent resources on getting started.)\nHow Well Did We Do?\nfrom fastai.interpret import *\nfrom fastai.vision.widgets import *\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_top_losses(5, nrows=1)\nWe can use this to see what our model gets confused about. This will change as you train it more.\ninterp.most_confused()\nUsing The Model\nOkay, the last little bit here. We now need to use this trained model on images the network has never seen. (They were in a different folder. Also called the Validation set.)\nFine, It’s an exam. Best of luck little model. I hope you do well.\npredictions_path = &quot;../input/fruits/fruits-360_dataset/fruits-360/Test&quot;\n \ndef predict_batch(self, item, rm_type_tfms=None, with_input=False): # this bit is slightly complicated. ignore it for now\n    dl = self.dls.test_dl(item, rm_type_tfms=rm_type_tfms, num_workers=15)\n    ret = self.get_preds(dl=dl, with_input=False, with_decoded=True)\n    return ret\nimport random\npredictions_path = Path(predictions_path)\nLearner.predict_batch = predict_batch\n \n# This is important\nlearn = load_learner(&quot;model.pkl&quot;)\n \ntst_files = get_image_files(predictions_path) #same as before\ntst_files = tst_files.shuffle() \nNow running the predictions.\npreds = learn.predict_batch(tst_files)\nclasses = learn.dls.vocab # the original categories\npreds_mapped = list(map(lambda x: classes[int(x)], preds[2])) #just saving them out\n\nSo did it work? tst_files are the images we gave it. Preds is the output. Would you look at that?? It got most of them right :)\nFin\nWhat’s next? More articles. In the meanwhile, you can look at this little link with resources I have collected over the years.\nYou have a long way to go. But I do hope this was a good start. I know you didn’t read the whole thing. Maybe you didn’t make it till here either. I get that. I also did that when I was starting.\nThis article is in the hopes that it will help someone out. Maybe have the help that I did not. I do not know who it will reach. But to whoever it does, best of luck :)\nYou can contact me on LinkedIn, drop me an [[mailto:msubhaditya@gmail.com|Email]]"},"Articles/Others/GRAD":{"title":"GRAD","links":["tags/deeplearning"],"tags":["article","deeplearning"],"content":"GRAD-Camdeeplearning\nNote: This is a slightly advanced article. If you are not comfortable with training neural networks, this is probably not for you yet. Start here instead.\nIntro\nSo you want to train a Neural Network to classify images. Woah. That’s awesome! How well did it do? Did you get a good score? Oh? You want to do better? I hear you.\nWhat if you could see what the network sees to make the choice? That would help understand how to make it perform better right? Read on!\nA few years ago, a paper toc: true\ntitled “Grad-CAM:\nVisual Explanations from Deep Networks via Gradient-based Localization” by Selvaraju et al. talked about how we could visually see the activation maps of a trained CNN by looking at the gradients in the final layer. This post will show you how to use that for your own needs.\nNote: We will be using PyTorch and the fast.ai library. But the concepts stay the same, so you should be able to use it in any other library.\nThe Objective and Data\nBefore we can go to the code, what exactly are we trying to achieve? In short, we want to first train a network such as the “resnet34” architecture on any kind of data. In today’s example, I will be using the Fish Dataset. This dataset has 9 types of sea creatures.\nYou can of course, use any other image based classification data that you want.\nKnowing how the Dataset is structured is essential, so let’s see that.\nOnce the training is completed, we want to be able to give the trained network any image for inference and then have it spit out a visual heat map of exactly what it saw in the image. The brighter the color, the more the network focused on that particular spot to make it’s decision.\nHave a look at the below example.\nIn this image, we can see a yellow border around the fish. (Apologies if it is a disturbing image. Feel free to use any other dataset.) This shows that the network has identified the boundary of the fish and probably tried to use that information to decide what kind of fish it is.\nExamining these, we can see if the network is looking at the wrong thing, and find ways to show it what we want it to see.\nCode\nTraining\nOkay, let’s not waste any more time and delve right into the code. If you are not familiar with fastai, you can look at this tiny blog for reference. The complete code can be found here on github.\nFirst, we import fastai as the deep learning library, matplotlib for plots, and IPython.display.Image to display the image inline.\ngist.github.com/SubhadityaMukherjee/grad_imports.py\nTraining a classifier in fastai is just a few lines. We first decide where the dataset is.\nThen we create a DataBlock. (Think of it as a constructor for a dataloader). This DataBlock is given the following information\n\nType of task : Image → Label\nWhat to do : get the images from the directory\nHow to label the images : use the folder where the images are. For example, if the file name is “/media/hdd/Datasets/Fish_Dataset/Fish_Dataset/Shrimp/Shrimp/00001.png” , then it’s parent is “Shrimp”.\nHow to split the data: Randomly with an 80/20 train/test split.\nTransforms : Basic vision transforms, Crop and resize to 224x224 px.\n\nOnce we have that, we can pass it to the main trainer class - the “vision_learner”. To it, we pass in the network we want to use (You can use any other as well), and the metrics we care about.\nThe to_fp16() enables Mixed Precision Training that would further increase the training speed and decrease the memory consumption.\nAwesome! Now let’s train the network. We are using a pre-trained resnet34 and performing transfer learning, so we use fine_tune. If you want to train from scratch, you can use “learn.fit(1)” instead. I trained it for a single epoch as a demo.\ngist.github.com/SubhadityaMukherjee/grad_train.py\nHooks\nI mentioned previously that we would be looking at the gradients of the trained network. But how do we access them?\nIn PyTorch, we can modify the components of the training loop using the concept of “Hooks”. As the name suggests, it involves inserting a hook in the training loop and execute arbitrary code. Using that, we need to save the gradients during training.\nPyTorch has functions for the same called “register_forward_hook” for the forward pass, and “register_backward_hook” for the backward pass. We can take this information and write the following classes as a wrapper around our training loop.\nPlotting Activations\nNow for the intense bit. Let’s pick a random image from the dataset. The original image is slightly disturbing so I blurred it a bit.\nOkay, we need to now pass the image through the model. The FastAI syntax for this kind of patching is slightly complicated. But let us walk through it.\nWe first create the test data loader with the new image that we picked. We can then convert that into a Tensor. Once we pass the image to the network, it performs a full forward and backward pass on that image through every layer. Now for every layer in the model, we can get the computed gradients that we stored away using the Hook class.\nSelvaraju et al. defined the activation map as the weighted combination of the forward activation maps. Which is what we perform.\nThe rest of the code is just matplotlib convenience functions to plot the nice grid heatmap that you see.\nThe only weird line of code you might see is “cam_map.detach().cpu()”. This is done because we cannot plot a Tensor on the GPU, so we first detach it from the computational graph, then bring it back to the CPU to plot it.\nWell, yay! You made it. Try it with your own network and/or data. Just a word of warning, the 0 in line “with HookBwd(learn.model[0][layer]) as hookg”, needs to be modified based on the network architecture. If you get errors, try with a 1 and so forth.\ngist.github.com/SubhadityaMukherjee/grad_plot.py\nWhat’s next?\nFirstly, good job on making it so far! Pat yourself on the back or go grab something nice to eat.\nThen look at what the network sees. Does it make sense? Is the model looking at something weird? Train for a few more epochs, rinse and repeat.\nTry it for different images. You might find examples that make no sense. Sometimes it might be because of random augmentation, other times it might be because of model bias or the data itself being not okay. You will find ways to improve on it eventually. If you can, look for examples that the model gets wrong, and apply GRADCam on those.\nFin\nWoah. That was long.\nWhat’s next? More articles. In the meanwhile, you can look at this little link with resources I have collected over the years.\nYou have a long way to go. But I do hope this was a good start. I know you didn’t read the whole thing. Maybe you didn’t make it till here either. I get that. I also did that when I was starting. This article is in the hopes that it will help someone out. Maybe have the help that I did not. I do not know who it will reach. But to whoever it does, best of luck :)\nYou can contact me on LinkedIn, drop me an [[mailto:msubhaditya@gmail.com|Email]]"},"Articles/Others/Ideas":{"title":"Ideas","links":[],"tags":["article"],"content":"Blog Ideas\n\nFastAI/Pytorch tricks : Shorts\nGRADCAM\n"},"Articles/Others/Learnings-from-this-time-back-home":{"title":"Learnings from this time back home","links":["tags/learnings","KB/Resistance"],"tags":["article","learnings"],"content":"Lessons for future me : Part 1 - 31st Aug 2022learnings\nThis is a time of change, and this trip was the period of rest between what my life was, and what it will become soon. I needed it. A break. A space between me and everything that I thought mattered. It gave me the time and environment to learn a lot more about myself and what I want to do in my life from now on until whenever my time comes. This is a list of the things that I realised in this holiday. It is probably not an exhaustive list, but it is something to look back on in the future and see how much this changed my life.\n\nTime. Time is the biggest helper. But it doesn’t exist. And we give it the power to. The more we are attached to the clock, the lesser we give ourselves permission to actually enjoy life.\nOur whole lives, we had been used to a very confusing relationship with money. We either thought that we had enough, when dad bought something expensive, or were led to think that we didn’t and wanting things was very wrong, when mom would remind us that wanting too many things was a bad and almost evil thing. When I realised that we had enough. We really did. But what parents wanted me to realise is that, we should forever be grateful for what we have. Nothing else. There is enough for me. And for everyone else around. If I have the ability to use a bit of what I have to make life slightly better for someone else, then I should do my best to enable them. If it is the right moment.\nCreativity is stumped by so many things. But mostly by the belief in not having it. Every time I sat down to paint, I should be grateful, and feel something first. A good emotion. A strong feeling. Without which, creation is not possible. It is the first step to manifestation. The same way I manifested winning an impossible choice in the Cover draw, or the way I manifested good weather and a beautiful trip. Or parents not coming there so I could have more time to heal. It is the first step\nIn effect, many things stop us from doing things. One of them, and probably the greatest of them all, is fear. Fear of not having enough, fear of failure, fear of being rejected, an outcast. Probably also aggravated by ADHD. We have been different our whole lives, and have been shamelessly mocked for it. It made me resent everything and stopped me from starting things just for that fear.\nIn the age of content and the internet, it is hard to say that you are the first person to come up with something. Me being me, that stops me from starting. Which is a waste of time. Once there is power, and an emotion, work must be done. Creation must come into being. There will be many obstacles to this. But creation cannot be stopped, one way or the other, through one person or the other, it will come into being. Remember the war of art?\nRelationships are made by understanding and effort. Over time, we lose track of who we were and who we fell in love with in the first place. Life shows us so many things, we take it for granted, and keep asking for things that do not exist. In essence, we take what was beautiful, and mar it. Over the past few months, neither me nor her have been okay. And instead of offering support, we ended up fighting not only life, but ourselves and our demons too. All empathy was lost. So was the understanding that neither of us wanted the other to solve our problems, or babysit each other. We have our own lives and ways of dealing with things.\nIn the headlong rush to get over what has dragged me down all these years, I sometimes forget what the past has led to. Everything happens for a reason. This period has let me realise that, at the end of the day, people matter. So much. And so do I. But in the attempt to make a space of rmyself, I must realise that not everyone is in the same space as me. Maybe they will get there, maybe this is not their road. We can only be there on the way.\nMaybe my destiny is just this. To live. To guide when I am called. To provide for when it is needed. To make others realise how to make their lives flow forward. To create. To spread my light. Everything else is.. secondary. That’s all that I am truly good at isn’t it? Spreading my light. Telling people that they are enough, that they matter. That life is going to turn out okay. I am a guide. And I have been called for so many times. Along the way, I stopped listening. A guide doesn’t have to take the person all the way does he? His only objective is to show the way. Point to what would be  a good place to go.\nSo much changes with time, but I remain. So much happens around me, but I alone am unchanging. Forever present. Eternal. One thought for a thousand years.\nParents did their best for me. Maybe they got lost too. They were children once too. I think there is no real adult here. Just kids. Very lost kids, with voices that so few people hear anymore. Nobody showed them how to live. And they made mistakes. A lot of mistakes. So will I. There remains no time to blame anyone. Just accept, take what serves me, give back what doesn’t.\nGames and shared experiences bind us together. We forgot how to be together. A simple game of Uno brings so many people joy. No matter how old or young they are. Baking cookies, brings a smile on everyones face.\nBeing part of an experience with people around you, bring satisfaction. Be it baking, or painting, or just walking. But the real question is, how much of their attention is directed towards what you are doing together.\nLong journeys are frustrating, only because we think of how long it will take. We keep looking at the time. But the miracle of it all, is just.. breathtaking sometimes. A few “hours” and everything you know and love changes. New people, new places, new experiences. The only cost is some unrest. And even that fades in time. This journey will take 18 “hours”. But its a journey between two eras in my life. In the moment, it really is not too bad is it?\nThere are an infinite streams of energy around us, all the time. Maybe you can call it the Dao. They are us. We are them. Streams do not flow backwards. They keep moving forward. Regardless. That’s how life goes too. We move from places of stagnation and rot, to life and change.\nWe have an intimate relationship with food. Whatever it may be, being involved in the process of making something nourishing is something I greatly enjoy. I would like to spend many hours in my life, creating and enjoying these creations.\nArt. Art came out of nowhere and took over everything. Or so I thought. It was always there. Everywhere. Doodles. Those fake tattoos from School that mom would have me wipe off everyday, all those scribbled doodles across my notebooks, the random urges to create, make something, draw and colour. It was something I did not fully understand until recently. Now I just need to speak the language. I am learning it, it will take time. But I will let it flow and see where it takes me. It’s a journey after all.\nTravelling is something I very much want to do. In time. And whenever I can. The experiences and the ideas that flow through, are unmatched.\nThe idea of impermanence is something that I want to focus on. Every time I feel sad about not being too close to someone. Or anything. I know that its not going to last. Nothing does. Its a cycle of lows and highs. Or whatever we choose to perceive. My time here is not permanent. Neither are my grudges, or discomfort with people.\nEveryone, deep down, wants similar things. Even people who appear cold and uncaring, turn out to be the most broken and confused souls. They too just want a warm hug and someone to tell them that it will be okay. Especially when they are drunk.\nPeople regret saving up a lot and never spending it when they have the time to. Mainly because they are dead.\nResistance is the biggest killer of anything. The more you resist something, the more it might be an indicator of what you truly want to do.\nWith old age and too much power, comes a sense of ego. A sense of “I am the best”.\nJournalling is life changing. Especially micro journaling\nDon’t forget to backup!\nSometimes you need to negotiate. People are not always extremely nice, or honest.\nPlain text files rule. Ease of use, simplicity, universal support, easy to backup. What else can one want? Pictures? Oops, use markdown.\nHaving a knowledge base of information is amazing. And probably the best first step to truly.. doing research. Your own personal interconnected search. How truly beautiful. A whole second brain.\nDoing something everyday, no matter how tiny, leads to massive changes over time.\nSometimes all you need to do to start writing is to set a timer of 10 minutes on an app that erases your work if you stop.\nMinimalism is not always the answer. Neither is it always a good question.\nResentment builds up over a long time, from sacrificing things that you need not have, for reasons that were not clear at the time.\nEmotions, pure emotions, are a good indicator of if your choices are right.\nDestiny.. oh sweet destiny, how intricate your works are.\nFriends.. sometimes do not seem to really exist. But they have their own lives. That does not mean they do not want you in theirs.\nMoving abroad is.. hard. Extremely hard. Especially because you’re in a place where almost everyone has a community already. And you barging in is almost never going to work. It will take a very long time. Better get used to looking at yourself in the mirror.\nWhen people tell you, hey, you could look a lot more professional and smart if you gave a shit. Please give a shit. Please. At least try.\nIf you do not meditate for a very long time, prepare for the consequences.\nInstead of just stretching, some yoga will go a very long way.\nTrust the process.\nPour your time into things. In the majority of cases, that is what is lacking. Both with art and with life.\nNot eating meat is probably a good idea. That doesn’t mean cutting it out, it means not treating it like the most important part of the meal. This comes from your Bengali upbringing and you can feel free to get over it.\nSo many things in your past have been tainted, no the better word is influenced, by your ADHD. From time blindness, to forever forgetting things, to taking time to process things, to auditory overload. People dont understand it yet, its okay though. Its not their fault. Neither is it yours.\nWe should be grateful to YouTube for everything it has taught us so far. Who knows what will come later. But they did give us a good base.\nFocus on giving time to research. You never really have. Maybe because of your ADHD, or maybe because of your fear of failure. Things take time. Give it the time it deserves.\nThe things that you are the most afraid of, thinking that it would take the longest, will actually get over pretty fast. Just try it okay?\nYour inner child needs a lot more love than you think. But remember that like all children, and like anything else, too much of anything leads to ruin. Heal the parts that need it, let the others go.\nMost emotions are temporary. Watching them go by is probably the best course of action before doing anything about it.\nTry to remember names. People really like it\nDale Carnige was a goddamn genius.\nThe Pareto principle applies every single place\nUse your calendar a little more. It really helps out.\n"},"Articles/Others/Myself":{"title":"Myself","links":[],"tags":["article"],"content":"Myself : Learnings\n\nWhen you are slogging on something and feel like not doing it, consider if it is worth it to start over. Sometimes that is what you need.\n\n"},"Articles/Others/Notes---Machine-Learning-System-Design":{"title":"Notes - Machine Learning System Design","links":["tags/deeplearning"],"tags":["article","deeplearning"],"content":"Notes on “Machine Learning System Design” (for Interviews) by Chip Huyendeeplearning\nThese are my notes on this article by Chip Huyen.\nThe article that these notes are based on, talks about some factors that are involved in designing machine learning systems, and what to watch out for in interviews on the same. I wanted to write a summary of what I read and add my take on it for future reference.\nAs a note, I found Chip’s book Designing Machine Learning Systems an excellent resource for anyone starting or willing to improve their skills in this field. I would recommend a read. (This is not sponsored by any means.)\nInterviews\n\nThe major issue with Machine Learning interviews seems to be the lack of a standard criterion by which to judge a candidate. This makes a lot of sense considering how varied the requirements of each project are.\nInterviewers generally look for what they are familiar with and often this means that an ideal candidate would be someone who thinks along similar lines. This is especially interesting in ML because there are an infinite number of ways to approach a problem.\nIt would be helpful for the candidate to understand what kind of answers the company would be looking for based on their previous work.\n\nCompute requirements\n\nContrary to the vast amount of research in the ML space on improving models and focusing on metrics, in production, users might barely notice a tiny improvement in accuracy. In return, this would further increase the complexity of the system and thus its latency.\nIt is important to first understand exactly what you wish to achieve and what you need to optimize for.\nYou cannot do everything.\n\nSetting Up The Project\n\nAt the end of the day, an ML system also requires skills from Software Engineering. A complete system is not just a model but a lot of moving parts. Thus there are quite a few tradeoffs to consider.\nAn initial thought process\n\nPick the top 2 goals that your solution needs\nWhat does the user interaction look like?\n\nDo you care about personalized results?\nDoes latency matter?\nHow much of your system relies on ML?\nWhat kind of devices are you looking at for deployment?\nDoes privacy matter?\n\n\nData\n\nDo you have the data?\nIs it usable? Is it clean?\nWhere is it stored? How much of it is stored?\n\n\nWhat metrics are useful in this context? (Domain expertise would be very helpful here)\nWhat resources do you have/can acquire? People, time, users, etc\n\n\n\nBaselines\n\nThese systems can get complex pretty fast. So start with the simplest possible algorithm. You might not even need a very complex Deep learning system. And it might not be feasible at the start.\nTo evaluate if it is worth shifting to a more complex implementation, looking at baselines is essential.\n\nRandom Baseline: How well would a random guess do?\nHow well does a human do on this task? (If that is feasible)\nWhat minimum results do you need for a functioning solution?\n\n\n\nDebugging Models\nThis is one endless minefield.\nIn my experience, this list by Andrej Karpathy is a pretty comprehensive guide. I do not have anything to add to it so I will skip this section.\nAs a recommendation though, I would suggest considering the use of a framework such as fast.ai. These are built with a lot of issues in mind and take care of a lot of them.\nModel Scaling\n\nMost large-scale systems these days use Parallel and Distributed computing. Multiple GPUs/TPUs etc.\nIf your data does not fit into memory, there are many ways of getting it - Gradient checkpointing, Mixed Precision, and Parallelism are some of them. (Future blogs will go a lot more in-depth. Putting it here will make it pretty huge.)\n\nInference\n\nThere are a lot of steps and tradeoffs involved in inference.\n\nWhere will you run your model? How long does it take? Can you say with certainty why a particular result is obtained? Can you retrain it?\nBefore deployment, it is advisable to turn off all the modifications one by one and test how well the system performs without them.\nCheck for biases in the results. (For example: Does your model prefer colored people?)\nDoes your data remain constant? Or keep changing?\n\n\n\nFin\nThe list of things to consider to make an efficient and thought-out model is endless. I do not think it is possible to take everything into account in the real world, but the more you think these through and follow these guidelines, the better and more stable your implementation will be.\nOf course, this is more of a short primer rather than a comprehensive guide. Future articles will cover more details.\nThis article is in the hopes that it will help someone out. Maybe have the help that I did not. I do not know who it will reach. But to whoever it does, best of luck :)\nYou can contact me on LinkedIn, drop me an [[mailto:msubhaditya@gmail.com|Email]]"},"Articles/Others/Professional-Reports-and-Papers-with-LaTEX":{"title":"Professional Reports and Papers with LaTEX","links":[],"tags":["article"],"content":"Writing Professional Reports and Papers at 2x speed - A LaTeX tutorial + tips\nAt some point in your study or career, you will be required to write a report or an article, perhaps even a research paper. You start writing your content down in Word or your favourite text editor. After a little while, some issues crop up. Maybe you don’t like how it looks, or you want to change the format to a two column layout. Maybe you moved an image and the entire document went crazy. Maybe you wrote a long article, but have to change your citations manually. Or heaven forbid, you need to create a table of contents.\nUgh, you think. I don’t have time for this. Just let me work on the content. If only there was a better way.\nSounds familiar? Read on.\nLaTEX vs Word\nAt the heart of everything, what you are struggling with is the issue of change. If you had a small article, with barely any changes or styling, Word is great. But for anything more than 5 pages? Ouch.\nLaTeX is a complete document preparation system, with the added advantage of a different “language” that makes your life a lot easier. It sounds and looks very strange at first. But once you get the hang of it, it will change the way you write content. Think of it as a more advanced template that is infinitely customisable. All you do is set things up once. Then you can focus on your content.\nOh no! Something changed? That’s okay. Add a few lines to your document and you are good to go.\nI do all my reports, articles, homework, projects everything using it and it has saved me days of effort.\nIt looks professional right off the bat.\n(Note: You do NOT need to be a programmer to use this. Just go with it. And google to your heart’s content.)\nIntro to Overleaf\nSo how do you use this magic mushroom? Well, luckily we have Overleaf. This website allows you to write anything you want, provides a lot of templates, live collaboration, and so much more. Mostly for free as well.\nJust open the site and make an account. If your institution provides premium access, use your official email ID to register. (You don’t really need it for most use cases.)\n(Note: This is not a sponsored post. I just use it everyday and want to make sure it helps more people.)\nEverything I show can be viewed at this link.\nChoosing A Template\nOkay great, now it’s time to start working on a project. I will demonstrate with a small article : “Computer Vision in Pytorch - A Primer”.\n\nClick “New project”\nBased on what kind of work you are writing this article for, pick a section. For this article, I picked Academic Journal.\nLook around and find the look you are going for. Click on it, and then click “Use as template”. If you are a University, look for an official template, most of them have one.\nI picked this - IEEE template\nPerfect! You are halfway there.\n\nInitial Setup\nBefore we start, let’s just get used to the interface.\nOn the left you can see a sidebar with all your files and folders.\nTo it’s right, there is the main editor window. If you look closely, the text seems a little strange? Don’t worry, more on that later.\nAfter that you will see a preview of your article. Every time you hit save or “recompile”, this preview will update. You can latex export this as a PDF.\n\nIn the sidebar, I like to make a folder for images by click the little icon that looks like a folder.\nNow, copy paste the first few lines until the line before “begin document”. Think of these as extra functionality. For example: highlighting your links, structuring your document etc.\nMake a new document toc: true\ntitled “main.tex” using the file icon.\nPaste the contents there.\n\nGreat job! Now we can get to writing everything.\nUnderstanding The Components\nNow the following might seem slightly too complicated. And you will probably feel like it’s not worth the effort. But, trust the process okay?\nSo far, you should have something like this.\n\\documentclass[conference]{IEEEtran}\n\\IEEEoverridecommandlockouts\n% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.\n\\usepackage{cite}\n\\usepackage{amsmath,amssymb,amsfonts}\n\\usepackage{algorithmic}\n\\usepackage{graphicx}\n\\usepackage{textcomp}\n\\usepackage{xcolor}\n\\def\\BibTeX{{\\rm B\\kern-.05em{\\sc i\\kern-.025em b}\\kern-.08em\n    T\\kern-.1667em\\lower.7ex\\hbox{E}\\kern-.125emX}}\nNow we define a basic template.\n\\begin{document}\n \n\\toc: true\ntitle{Computer Vision in Pytorch - A Primer\\\\\n\\thanks{I thank Overleaf for this template}\n}\n \n\\author{\\IEEEauthorblockN{1\\textsuperscript{st} Subhaditya Mukherjee}\n\\IEEEauthorblockA{\\textit{Faculty of Science and Engineering } \\\\\n\\textit{University Of Groningen}\\\\\nGroningen, Netherlands \\\\\nmsubhaditya@gmail.com}\n}\n\\maketoc: true\ntitle\n \n\\begin{abstract}\n\\end{abstract}\n \n\\begin{IEEEkeywords}\n \n\\end{IEEEkeywords}\n \n\\section*{}\n\\subsection*{}\n \n\\begin{thebibliography}{00}\n \n\\end{thebibliography}\n \n\\end{document}\n \nYou will only need to do this once to get a feel for what’s happening. It’s scary I am sure. But hold on. Keep trying. You’ve got this!\nLook at the following code snippets twice. Do you see a pattern?\nTitle, Abstract, Keywords\nWe now need a toc: true\ntitle. So we put it in this line. If you notice the little {}, that is required by LaTeX to know where to start and end something.\ntitle, latex_report_parts\n\\toc: true\ntitle{Computer Vision in Pytorch - A Primer}\nAre you required to write an abstract or a summary of sorts?\n\\begin{abstract}\nThis paper is a short introduction to Pytorch, a deep learning framework. Special focus will be given to applications of Computer Vision. This is a demo paper, and has no particular significance.\n\\end{abstract}\nSometimes, a template will have keywords. You can just enter the ones you think are relevant.\nAuthors\nEvery template you copy, will have a section for the author. Just fill it in the way you want.\nSections and Subsections\nNow for the actual content. Here we use the commands “section”, “subsection”, “subsubsection”. You do not need to bother about giving them numbers. LaTeX will take care of it.\nSomething like this is a good start.\n\\section*{What is Computer Vision?}\nA study of the techniques used to extract meaning from image or video related data. The applications are endless, starting from face recognition, to self driving cars.\n\\subsection*{Computer Vision in the field of Deep Learning}\nDeep learning has revolutionised the field of Computer Vision by giving it superpowers. The ability to learn from billions of images come as a huge leap forward in the field.\n\\subsubsection*{A note}\nClassicial CV is still very relevant today.\nFigures\nDoesn’t seem too hard does it? Let’s add some figures. Woah. What is happening here??\nWe are defining how we want our figure to look! We tell the system where the image is, how want it to look - centered, fit inside the line, have a caption, and a label. Just change the text to what is relevant to you.\nThink about it once. Makes sense right?\n\\begin{figure}[h]\n\\includegraphics[width=\\linewidth]{figures/2560px-PyTorch_logo_black.svg.png}\n\\centering\n\\caption{Representation in the Simulation}\n\\label{fig:colors}\n\\end{figure}\nIf you want to change the size, replace “\\linewidth” with something like “0.2\\linewidth” which makes the figure 0.2 times the size of the line.\nFormatting Text\nOkay how about formatting such as bold italics and the like? Super simple. Look at these lines.\n\\section{Formatting}\nThis \\textbf{will be bold}, then \\textit{italic}, and also \\textcolor{red}{red}. \n \nTo add a line break, simply add \\\\\nthis will be a new line\nSee? That was not too bad was it? Now we have colours as well!\nCode\nIf you are a programmer, or need to have any bits of code, this is how you can do it.\nTo the section where the packages were imported, add the following line.\n\\usepackage{listings}\nNow wherever you want to add code, just use it like this. Change the language to what you need of course. Viola! Syntax highlighting!\n\\begin{lstlisting}[language=Python]\nimport numpy as np\nprint(np.random.rand(10))\n\\end{lstlisting}\nEquations\nHave you ever had to add equations in Word? I feel sorry for you. LaTeX lets you do it in a breeze.\n\\section{Equations}\nWe can have three types of these - An inline equation : $2x+3 = 10$, or a proper block , $$2 \\sin(x)+10 = 100$$ or a long form one such as this.\n\\begin{equation}\nE[g^{2}]_{t}= 0.9E[g^{2}]_{t-1}+ 0.1g^{2}_{t}\\\\\n\\theta_{t+1}= \\theta_{t}- \\frac{\\eta}{\\sqrt{E[g^{2}])_{t}+\\epsilon}}g_{t}\n\\end{equation}\nIt would be impossible to explain the intricacies of using these, but as you can see, it almost feels like writing the equation down as it is. And it looks gorgeous as well.\nFor more information, refer to this nice page.\nCover page\nHopefully you picked a template that already had one. But if you did not, add this before “\\begin{document}”. Replace the text however you feel like.\n\\begin{toc: true\ntitlepage}\n   \\begin{center}\n       \\vspace*{1cm}\n \n       \\textbf{Thesis Title}\n \n       \\vspace{0.5cm}\n        Thesis Subtoc: true\ntitle\n            \n       \\vspace{1.5cm}\n \n       \\textbf{Author Name}\n \n       \\vfill\n            \n       A thesis presented for the degree of\\\\\n       Doctor of Philosophy\n            \n       \\vspace{0.8cm}\n     \n       \\includegraphics[width=0.4\\textwidth]{university}\n            \n       Department Name\\\\\n       University Name\\\\\n       Country\\\\\n       Date\n            \n   \\end{center}\n\\end{toc: true\ntitlepage}\nTable Of Contents\nAdding a TOC is even easier. It also updates automatically. Just add.\n\\tableofcontents\nAppendix\nWant a list of images and/or tables you have used throughout your document? With page numbers.\n%\\newpage %add this if you want it to be on a separate page\n\\begin{appendix}\n  \\listoffigures\n  \\listoftables\n\\end{appendix}\nTables\nTables are a bit complicated in LaTeX, but there is an easier way. Just open this website - Table generator.\nThis is a very user friendly UI, so just add whatever you want. And click generate.\nCopy paste that into your Overleaf editor. Done!\nNotice the auto numbering? Cool right?\nCitations\nCitations are one of the most powerful features of working in LaTeX. The best part? Only the ones you cited will show up in your bibliography! You do not need to worry if you missed any, or forgot to remove any. All you need to do is paste all your citations in a bib file.\n\nCreate a file called references.bib\nPaste all your references in “BibTex” format in the file. Google scholar or any reference manager you use will have that option.\nCite them like this\n\n\\section{Citation Example}\nTwo interesting libraries are Kornia \\cite{riba2020kornia} and fast.ai \\cite{howard2020fastai}. If you want it inline then : \\citep{howard2020fastai}.\nBibliography\nOnce you have added all your citations, you would need to have a Bibliography. The file “references.bib” you created? Remember the name. Right before “\\end{document}” you can add these lines. (Change the first one to what you need. Your template mostly will define it already.)\n\\bibliographystyle{IEEEtran}\n\\bibliography{references}\n \n\\end{document}\nLooking good!\nComments\nWant to make comments that you or a fellow author can refer to in the future? Just select any bit of text in the editor, you will get a pop up for adding a comment.\nExporting\nCongratulations! Looks like you made it to the end. Now how do you export your awesome document?\nSee the little button that has a downward arrow? Click that. You can also click the “Menu” button and find more options there.\nCollaboration\nWant to work with colleagues/teammates? Just hit Share and send them the link.\nGeneral Principles and How To Get Help\nThat’s about it for the basics. Feel free to come back to this document for your reference. You will face problems. Just remember the following things.\n\nGoogle is your best friend.\nThis website is a good place to start looking for what’s possible.\nIt will take a few tries. But it is definitely worth it.\nIf you can say what you want in English, you mostly know the commands already.\nThere are a lot of commands. You do NOT need to remember them. You will passively pick them up. Overleaf also has autocomplete which helps.\nGive it some time, it will change your life.\n\nFin\nThis article is in the hopes that it will help someone out. Maybe have the help that I did not. I do not know who it will reach. But to whoever it does, best of luck :)\nYou can contact me on LinkedIn, drop me an [[mailto:msubhaditya@gmail.com|Email]]\nLike my work? Buy me a coffee :)"},"Articles/Others/Sigh,-Gone---My-Thoughts-on-the-Book":{"title":"Sigh, Gone - My Thoughts on the Book","links":[],"tags":["article"],"content":"Sigh, Gone ; Phuc Tran - My Thoughts on the Book\nAt some point in life, we stop reading and shift to faster content - articles, videos, and now “reels.” This year, I decided to go the other way and read as many books as possible. And perhaps write about some of them. Summarizing a book, though, is an exercise in futility and encourages more “fast content,” which is the opposite of what I want to do. Instead, I want to explore some of my thoughts while reading these books as a note to you, my dear reader, and future me.\nIntroduction\nThis article is about the book Sigh, Gone: A Misfit’s Memoir of Great Books, Punk Rock, and the Fight to Fit In by Phuc Tran. The book is a journey through the author’s life as a second-generation immigrant in America and his struggle to fit in. Phuc talks about many things: kind families, racism, trying to blend in, books, and what influenced him growing up. I loved the writing style and how every chapter was the name of a book. It was an enjoyable read, although some parts did make me want to cry.\nOkay, that’s enough of an introduction—time for a more serious discussion.\nBlending In\nI think it makes sense to start with the actual premise of the book, without which the context is impossible to understand. Immigrants face many challenges wherever they go, some harder than others. While this is hard for adults, it is even more challenging for children. Growing up, they not only have to live in a society that does not want them there but also understand what shaped their families. Many children experience a different life before moving to a foreign country, and the change is sometimes too difficult for them to understand. More often than not, they grow up with many conceptions about the world and themselves that are not helpful to them as adults.\nBut everyone is different, so why not just be yourself? Well, that is easier said than done. Growing up, the author tries to fit in with a society his family is unfamiliar with. He faces blatant racism, violence, and impossible expectations. As a kid, this led to a lot of confusion. Perhaps the only way to fit in is to be a misfit?\nCultures\nFrom personal experience, culture is the biggest source of my confusion in understanding my identity. Growing up as a part of two different cultures is genuinely confusing. For the author, this, too, was a significant source of confusion. Who was he? In his community, he was an Oriental, a highly fetishized term. According to some of his siblings from larger cities, he was an Asian, someone who could be cool. But perhaps he was an American too? But he could never be what they were - White.\nWe often struggle to blend in because we don’t have a fixed identity. All these parts of us blend in to be a mix of everything. But society loves putting people and concepts in rigid boxes. If you are a punk rocker, you are not a good student. If you are a person of color, you can never be as cool as the others.\nThis affects us all the way down, from food to clothing to our self-image and worth. Perhaps inherently, we believe that parts of us are bad and the parts that fit in are good. We forget that everyone is different. We try to fit in by erasing all the parts of us that are bad but end up erasing many that are good too.\nWe Are the Racists\nAs a kid, everyone is the same. There is no “other”. But as adults, we believe in our tribe to the point of excluding everyone else. The author experiences a lot of racism, from his classmates to teachers and even the police.\nWhen he was a little older, he realized everyone, including himself, was a little racist. Not in the traditional sense of the word - to distinguish between white and people of color but in the much broader sense. His family thought that hard-working people were a lot better than those who did not. People with a good education were better than those without, etc.\nNow that I think about it, we tend to categorize everyone as tribe and not tribe. This may be useful to some extent, but on a broader scale, it leads to alienation both within and without.\nWe are the racists. All of us are. One way or the other.\nKindness\nAmid all the struggle, there are always moments of joy and love. The author notes that without the help of families that supported them, there was no way they would have been able to survive. Some people help others unconditionally; perhaps they are why we still have faith in humanity.\nNeither I nor the author intend to say we should unquestioningly trust everyone. But being careful is different from excluding everyone different.\nGreat Expectations\nAsk any children whose parents struggled to be where they were, and you will find great expectations. Perhaps this is only natural. Our parents faced a lot of hardship to give us the life that we have. In that, we must be grateful. That being said, a certain expectation trickles down to the generations that come after.\n“We worked so hard to raise you. You must be the best.” Perhaps this is our parents trying to live their dreams through us. Or maybe everything seems worth it to them when we are happy.\nThe intent is filled with love and kindness, but the effects might not be. The outside world is not always kind to those who are different, but what if family makes it worse? Growing up, the author’s family wanted him to be someone he was probably not—good grades, showing the world that the family was okay and much more.\nHard work does not always equate to a successful life. But trying to live the American Dream, makes it feel like that is the only way to get there. Exchange your life and dreams to be someone you are not.\nSometimes, children need to be told that they are doing well. We can see that we must be more, but hearing that a hundred times does not make us want to do it.\nViolence\nMore often than not, violence is born from pain. A lot of pain. For immigrant families, the drastic change from home to elsewhere is a significant source of this pain. New cultures, hard work, loneliness, and fitting in all lead to a lot of frustration. Sometimes, this pain stays with people and only bursts out at points where they cannot hold it in anymore. The author experiences physical abuse from his parents, being beaten to the point he could not sit down, being forced to sit on uncooked rice, etc.\nOne can say that these are manifestations of a large amount of pain that has not been processed. Therapy helps here, but many of our parents have never had the chance to have that.\nI think that, as children, it helps to know that these bouts of rage are not because of us. While that does not change anything, it does help us accept it a little more. Perhaps in the future, when we have the space to process these emotions, we can forgive our past and present.\nThis Is Not Your Home\nThe author and his brother faced many instances where their parents pretended to leave to discipline them. They left the author stranded on a road, packed their bags, and left them alone at home, etc.\nSome part of me wants to talk about my experiences here, but I am not ready to do that yet.\nIn some ways, these experiences inspire discipline in children but also make them trust their families less. To some extent, they end up growing up faster than they should. When everyone around you already does not want you, if your parents show that they don’t, it breaks you. It really does.\nBooks\nAh, books. I, for one, would never be where I am without them. Growing up, they were more than just stories. They were my mentors, friends, and confidants.\nPhuc talks about how we found a book called the Lifetime Reading Plan, a list of books that every civilized American must read before they die. He initially starts reading some of them as a means of fitting in and being more American, but he slowly starts to enjoy them. They show him a world beyond his own, ways of living and understanding.\nHe realizes that he can be a Punk but also a good student.\nTo a large extent, this is also my own journey with books. I started reading to escape and also because I hoped it would help me understand my fellow humans a bit better. I was a misfit too and that will never change. But without books, I do not think I would have ever had the confidence to be myself, learn so much about the world and want to live in it.\nSymbols\nI did not realize the concept of symbols in this fashion before I read the chapter on it. Phuc talks about how sometimes we treat people as symbols. For instance, we might think of Vietnamese people as those affected by the war. The author bumps into many people who fought in the war and probably lost loved ones there. He realizes that they look at him and his family as symbols—a concept, not a human.\nI think this is one way we, as humans, try to make sense of the world. We categorize everyone we see. Indians are the ones with butter chicken, and Middle Easterns are terrorists. Are they? NO. Every single person is different. Every single person has a long, complex history that ranges back generations before them. But perhaps thinking of this is not easy, and we try to define people as a singular concept.\nIt is nice to be mindful of these thought patterns, I think. But it is a major shift in perspective too.\nBlack and White\nOne of the things I love about this book is that the author does not take a black-and-white approach when talking about his past. Hard boundaries do not define people.\nHe talks about how his father had great expectations and showed them in not very calm ways, but also how he tried to let his kids enjoy life. He did take them to the movies. He did try to let them become who they were.\nThis is the part that confused me the most growing up. Society tells us that people are either good or bad. But everyone is a mix of both. People can hurt us but also love us. As a child, these mixed signals might lead to some of us shutting everyone out. How can you trust anyone if they are not consistent?\nOur parents love us, but they have their demons too. Perhaps if we understand their pain, we can try to forgive them and soothe our own.\nDisclaimer\nI said many things in the blog, most of which are around acceptance. I think these views reflect what the author talks about, but I do not want to make the claim that they fully portray his views. These opinions are my own, and perhaps this is a way of showing my thoughts about the book without spoiling the story for those who decide to read it.\nI understand that acceptance seems like a scam for anyone facing many of these issues now. For now, it probably is. If you are being abused, please get help!\nI do not intend to say that you should sit back and accept everything happening to you. Instead, this is for when you are out of the situation. Life moves on, and we must all make friends with our demons.\nResources For Second Generation Immigrants\nIf you are like me, a second-generation immigrant, this book might resonate quite deeply with you. In my journey to learn more about these struggles and how they affect me, I came across a blog post that made a difference. If I am being honest, it made me bawl my eyes out.\nCheck it out, maybe?\nEgg Shell Therapy\nFin\nThis article is not a conclusive overview of the book or all the emotions I felt while reading it. But by putting it out there, I wanted to share a different perspective.\nI would love to talk some more about it, so please do reach out if that is something you want to do too!\nLike these/Want more? Buy me a coffee! Kofi\nWant articles on something specific? Just ask!\nYou can always contact me on LinkedIn, or drop me an [[mailto:msubhaditya@gmail.com|Email]]. For all the code, drop by my Github."},"Articles/Others/Sklearn-and-OpenML-hackathon":{"title":"Sklearn and OpenML hackathon","links":[],"tags":["articles","openml","conference"],"content":"\nSklearn and OpenML Hackathon\n \n\nIntro\nScikit-learn is a free and open-source machine learning library for the Python programming language. If you are reading this article though, chances are that you have probably used it already.\nOpenML is an open platform for sharing datasets, algorithms, and experiments - to learn how to learn better, together.\nOur teams have been working together for many years and both of us share the goal of developing the open-source ML space.\nWe recently had a developers hackathon at the :Probabl. (a company built around scikit-learn focused on maintaining and expanding open-source ML in Europe) office in Paris. We met to discuss not only the state of AI and how both our organizations fit in, but also to brainstorm solutions to challenges faced by our developers and communities.\nMany interesting topics were brought up, most of which were not only relevant to us but also to the broader open-source community. In the spirit of open-source, we wanted to share these insights with you.\n\nCommunity Engagement and Onboarding Contributors\nThe focus of this discussion was around community engagement and emphasized the importance of effectively attracting, onboarding, and retaining contributors, especially newcomers.\nOver the past few years, both in OpenML and scikit-learn, we have been noticing that a majority of contributors are only active for short durations and do not stick around for too long. The ones that do are distributed between a smaller number of more experienced senior developers and a much larger pool of junior developers. This then leads to many PR’s being of lower quality and thus requiring a lot more verification and correction. \nThe question at hand then, is not only how we can attract new contributors to our projects but also how we can make it easier for them and our developers to maintain these projects. Some of the ideas that came up have been tried and tested by communities our colleagues have founded or been part of across the globe.\nTakeaways:\n\n\nEmotional connection : All of the participants agreed that the most important part of any community is its people. Contributors only stick around if they have an emotional connection to either the project, or the people contributing to the project. In this vein, it would be nice if the maintainers of the project could also be present at events. It also helps if the contributors use the projects themselves.\n\n\n\nFocus on beginners : Since we see that most of our contributors are beginners, it serves to organize sprints that are inclusive of them with a focus on beginner-friendly issues, especially documentation tasks. Having these would not only help them understand the project and contribute better, but also let them form a connection with the project.\n\n\n\nCurated issues : Most of our external contributors have enough on their plate already. Having a curated list of issues before sprints would ensure that no time is wasted and let our contributors focus on the tasks suitable for them.\n\n\n\nDifferent tiers of events : To ensure that everyone is given tasks they can handle, it would be nice to have separate events for contributors with varying levels of expertise. This would also have the added benefit of retaining beginner-friendly issues for sprints to prevent more experienced contributors from claiming them early.\n\n\n\nMentoring : Incorporating one-to-one mentoring to help new contributors set up their development environments would help them feel more connected to the project. It is understandable that this is time consuming, so perhaps some way of deciding who gets mentored can be set up in time.\n\n\n\nContributors guide : Simplifying the contributor’s guide and adding video tutorials would make it a lot more beginner-friendly, especially to those who have never filed a PR before.\n\n\n\nSemi regular events : Some of our colleagues found that they tended to care about a project more if there were semi regular events they could set time aside for. Having these helped them slowly build a sense of community as well.\n\n\n\nIncentives : A common question that many developers have is why they should even bother contributing. Helping them understand how contributing to open source projects can aid their careers, bring them closer to the community and also help them get internships would be a good start.\n\n\nGovernance, Funding, and Sponsorship\nThis focus of this discussion was the governance structures of open-source projects, sustainable funding models, and the role of sponsorships in supporting project activities.\nTakeaways:\n\n\nEvolving Governance : Since governance is not static, we can treat it as a living document that evolves with the needs of the community.\n\n\n\nCommunication: It is good practise to maintain open communication channels, such as mailing lists and monthly meetings. This keeps all interested contributors in the loop.\n\n\n\nReviews: A major challenge is that of approving new reviews in a large organization without passing through too many hoops. How to manage this is still an open question.\n\n\n\nSponsorship: It would be nice to explore sponsorship models like the INRIA foundation, where companies contribute to meetings and have a say in setting priorities.\n\n\n\nCorporate partnerships: To keep investors interested, it would also be interesting to look into corporate partnerships (similar to those used by the Linux Foundation) that are of mutual benefit.\n\n\nDevelopment Tooling and Workflows\n\nSetting up and maintaning CI/CD pipelines across multiple repositories and programming languages puts a huge burden on the maintainers of the repositories.\nHaving to keep up with the trends and learn new technology very frequently also makes developers more reluctant to change the stack, even if doing so would be beneficial.\nThis discussion looked at how to tackle these challenges and make it easier for developers to handle such complex workloads.\n\nTakeaways:\n\n\nAutomation: Automating as much of the CI/CD pipeline as possible by using bots for linting and code coverage checks makes PR quality control easier.\n\n\n\nBetter workflows: Migrating to Github Actions and Azure workflows for testing and deployment also seems to help significantly.\n\n\n\nBetter tools: There are many open source platforms/tools that offer comparable convenience to tools that are currently used. Some examples of these are CodeBerg and Forgejo. Eventually migrating to using these tools might also help manage projects like Scikit-Learn and OpenML.\n\n\n\nCircleCI: Tools for rendering documentation examples directly in the browser can also be used to enhance the review process.\n\n\nBroader Ecosystem and Scope of Collaboration\n\nWhile both OpenML and Scikit-Learn focus on the open source ML community, it is sometimes hard to explain how we fit into the broader AI ecosystem.\nEspecially with the rise of LLMs and Generative AI, stakeholders are somewhat inclined to think that frameworks such as ours are just not “enough”. Of course, this is not true at all.\n\nTakeaways:\n\n\nCommunity projects: There are so many successful examples of open source projects, and a lot can be learned from their efforts. Projects like Scientific Python for example, has very well setup CI documentation and governance processes that can be quite readily applied to any open source project.\n\n\n\nExploring connections: A longer term focus for both our teams would be to explore connections with other open-source ML frameworks, such as PyTorch, Tensorflow and other AutoML tools. Doing so would also help integrate and strengthen the open-source ML communities and ecosystems.\n\n\nDiscussion on Croissant\n\nMachine Learning datasets are a combination of structured and unstructured data, which makes them all the more complicated to manage. This has led to the rise of multiple “dataset formats” which further make it hard to consistently load data across platforms and tools.\nCroissant is one such dataset format which directly tackles the issue of consistency. This format is now not only compatible with the most popular ML libraries/platforms (sklearn, PyTorch, Tensorflow, Kaggle, Hugging Face), it is also recommended by NeurIPS (one of the topmost conferences in the AI space).\n\nFeatures of Croissant:\n\n\nSchema.org: Croissant was built on top of schema.org with more metadata information specific to ML datasets. Since it does not require any changes to the underlying data structure, existing datasets can quite easily be converted to use it.\n\n\n\nLayers: The format has 4 layers - Dataset level metadata, resource descriptions, content structure, and ML semantics. Each of which make it possible to encode and maintain structural information about datasets regardless of platform.\n\n\n\nXAI and Visualization: Analysis and visualisation of the data works out of the box for all datasets and across multiple platforms. Croissant also supports the Core RAI vocabulary for explainable AI.\n\n\n\nSupported Platforms: Every dataset in OpenML has a Croissant representation, while a majority of data on Kaggle and Google Dataset search also support it.\n\n\nConclusion\nOverall, this discussion was quite a successful one for both of our teams. We learnt a lot from each other and found new ways of collaborating on our shared dream of open-source ML. So much infact, that we wanted to share our discussion with you, dear reader.\nWe hope you learnt something new. We would love to welcome you to our community and would be glad to support your journey in this ML space.\n❤️ The OpenML and Scikit-Learn team"},"Articles/Others/What-I-learnt-from-an-AI-Masters-Part-1":{"title":"What I learnt from an AI Masters Part 1","links":[],"tags":["article"],"content":"What I learned from an AI master degree - Part 1 (Finding a Project and Supervisors; Creating the Proposal)\nPart 1 : Starting  Troubles - Finding a Project, Supervisors ; Creating a proposal (This article)\nPart 2 : Timelines  - Optimal Planning and Breakdowns\nPart 3 : Setup and Tools - Setting up Writing, Programming, and research to prevent tears\nPart 4 : Writing and Programming Advice\nIntroduction\nSo you decided to pursue a master degree? It must be scary to know that you have to write some obscure research thesis on a niche topic that your friends will give you weird looks about. Welcome to the club. I was pretty terrified too, but it all worked out in the end, and I now have a master in Artificial Intelligence from the University of Groningen, Netherlands. (Yay for me!)\nThis is a series of articles where I share what I learned from the project and what I would do differently if I were to start again. This is no definitive guide, but I hope it helps you, my dear reader.\nThis article is Part 1 : How to find a project and supervisors, plan your idea, and pitch it without going crazy.\nDepending on when you read this, the other parts will be out, and the links will be updated at the start of the article.\nPick your adventure. I am proud of you for whatever you do.\nFinding a Project\nThe first hurdle you will face is finding a project. Of course, there is no point in thinking about the rest of the steps if you don’t know what exactly you want to do. So, how do you find a project? You have approximately three options.\n\nYou can find a project on your own by finding a problem you want to solve or an idea you want to explore.\nYou can find supervisors who have projects that you find interesting and ask them if you can work on them.\nYou can find a company hiring interns and working on a project they have as the thesis.\n\nEach of them has their own advantages and drawbacks. For instance, if you are not particularly comfortable in the field you chose, it would be harder for you to judge what kind of projects are feasible for you. In that case, you might be better off finding an existing project. That way you will be able to learn faster and also have a supervisor that is already familiar with the field.\nOn the other hand, if you are particularly interested in a field, you can find a research gap that you want to explore. In that case, you might be better off finding a supervisor who is interested in the same field and pitching your idea to them. If they think it is too ambitious, they can help you refine it. (This is what I did.)\nIf you are more interested in the industry, you might be better off finding a company that is hiring interns. That might give you some work experience off the bat and might (depending on how you work and the connections you make) also help you get a job later on.\nDepending on where you are based, this might be a little harder to accomplish. For instance, if you are in a country where either the language or the culture is different, it might be harder for you to find a company that is willing to hire you. Or perhaps you might find one, but it might take you a lot longer than you expected. If you are a native of the country, that might not be an issue. But if you are an international student, perhaps it is nice to have a backup plan and contact a few supervisors as well.\nBut What Ideas Do They Want?\nThe whole point of a thesis is to solidify your foundations and help you find a field you would perhaps enjoy contributing your time and energy to. So, it is not necessary that you have to find a project that is completely new.\nMany universities don’t impose much of a guideline except for stating that you should use at least some of the concepts you learned in your master.\nThere is no “wrong” project. There is only a matter of feasibility in the time that you have. You might have a great idea, but if you don’t have the time to implement it (even partially), it might help to save it for later. Sometimes you may have to change your idea to make it more feasible. But that is okay. (This is what I had to do, as my initial idea was too ambitious.)\nIt would be nice to find something that you are interested in. Perhaps talking to your seniors or faculties might help if you are not sure what you want to do.\nOkay, so how do you find a project? You have the following options. Pick your adventure.\nOn Your Own\nOkay, so you chose to find a project on your own. Yay! But how do you actually find something to work on? Well this depends on what kind of project you want to do, but I will list down a few things that might help you out.\n\nFinding a Literature Gap : Simply put, this would be to dig through research in your field and find problems that they were not able to solve. Finding such a research gap does take time and effort, though. In some sense, this is also what the supervisors do, except they are more familiar with the field and might have an easier time in understanding the papers. (*Look at the “Future work” section of papers.)\nMerge Projects : Suppose you find multiple projects that do similar/different things, and you want to see what happens if you combine them. That is a good start. (This is what I did.)\nContinue Existing Research : Another way to find a project is to look at existing research and see if you have any other ideas that you can add to it. They might not solve the problem at hand, but even finding solutions that don’t work is a good start and might lead to one that does.\nCompany Research Projects : Many large companies publish a lot of their projects as blog posts. For instance, one of my favorite ones is Google AI Blog. I have been following them on and off for many years now, and I have found many interesting projects that I would like to work on. This route may be helpful for you as well.\nSeniors : Almost all universities have a thesis archive. You can look through them and see if you find something interesting. But of course, you can also ask your senior friends for inspiration. You may be able to continue their work or find supervisors they worked with.\n\nFind a Supervisor\nOkay, so you chose to find a supervisor. For many people, this step is one of the hardest things to do. (Depending, of course, on the institute you are at and how many faculties are available.) In my opinion, it would serve you to make some relationships with the faculties about as soon as you can. This could be as simple as trying to find out what kind of research each of your faculties does.\nAt the end of the day, they are here for you, and if you show them some interest, they might be able to guide you more than you think.\nIn my case, I remember getting this piece of advice from a senior during my bachelor, and since then, I have put a little extra effort into getting to know some of my faculties. I have to admit this made it easier to find a supervisor. My mentor not only guided me throughout the thesis, he helped me find a second supervisor and also trusted me to do my own project. He taught one of my favorite courses, and I remember just asking him how I could learn a little more in it, and he invited me to his office for a much longer discussion. To be honest, I had no idea he would be my supervisor after a year. I was just interested in what he was teaching.\nThe Proposal\nAlmost every university requires you to submit a proposal before you can start your thesis. This is a document that outlines what you want to do, how you plan to do it, and what you expect to achieve. It is also a document that your supervisor will use to judge whether or not your project is feasible and if they can guide you through it.\nOnce you have the initial idea, creating a proposal will not take you that long, but what might take time is being able to phrase your idea in a way that is understandable and creating supporting diagrams, etc., that would make it easier for your supervisor to understand exactly what you want to do.\nDepending on your supervisor, getting an initial approval may be quick or may take a little bit of time. It would help to maintain communication throughout. You are probably not the only person under them, and they are genuinely busy people. While you wait, there is no harm in trying to flesh out the rest of your project. It might save you a little brainpower in the long run.\nSome Tips for the Proposal\nWhile there is no “correct” way to write a proposal, there are some things that might help you out. I will list down some of the things that I found useful.\n\nLook At Examples : Simple, but perhaps not always done. You are not the first person to do a thesis. There is no need to reinvent the wheel. Look at examples of previous proposals (ask your supervisor, perhaps), and save your brainpower for later. Trust me, you will need it.\nTMLIF : Aka, tell me like I am five. How would you explain your idea to a five-year-old? Start there and then make it to the standard of an adult. Assume that the person you will present it to only knows about the basics and builds up from there.\nLiterature : It would be nice to have a small initial literature survey to show that your idea has some support from the research community. This means finding some papers that do similar things or have concepts that you will use. Since you will need to do this properly later on, just finding a few relevant papers is enough here.\nTimeline : In most cases, a timeline is requested. While this is personal and my approach will not work for you, I recommend that you take into account resting time. This was something I only encountered later on, and it would have helped to think of it before. A thesis is a long task. You will lose motivation many times. It is quite normal, but if you don’t plan for it, you might end up feeling miserable for no reason at all.\nTemplate : Most universities have a proposal template or guidelines you can follow. While they are not always strict, it is a good place to start. At least you will know what is expected of you.\nHow Much Is Too Much? : If you are doing your own project, it is hard to know when to stop. Just remember that this is not your entire project. This is just a draft, an idea. As long as what you have conveys your idea and how you plan to tackle it properly, that is enough. Keep it simple and short, stupid. :)\n\nAn Example: My Thesis\nWhile my thesis is by no means a standard. If you are interested in Computer Vision, or want to see an example, feel free to look at the links below.\nThe project was about emulating Attention from Transformer networks in regular CNNs using gradient-based XAI techniques as a proxy - aka “Proxy Attention”.\n(Yeah, I tried to make it sound fancy. Come on, just let me sound cool sometimes. ;p)\nYou can find the paper here and the code along with all the writing here.\nFin\nThis article is in the hopes that it will help someone out. Maybe have the help that I did not. I do not know who it will reach. But to whoever it does, best of luck :)\nLike these/Want more? Buy me a coffee! Kofi\nWant articles on something specific? Just ask!\nYou can always contact me on LinkedIn, or drop me an [[mailto:msubhaditya@gmail.com|Email]]"},"Articles/Productivity/CheckEmptySections":{"title":"CheckEmptySections","links":[],"tags":["article"],"content":"TODO\nimport os\nfrom pathlib import Path\nimport argparse as ap\nimport concurrent.futures\nags = ap.ArgumentParser()\nags.add_argument(&quot;-f&quot;, help=&quot;File or folder name&quot;)\nargs = ags.parse_args()\n \nmainpath = Path(args.f)\n# Check if the input is a directory or a file\nif mainpath.is_dir() == True:\n    # If it&#039;s a directory, create a list of Path objects for all the files in the directory\n    all_files = [mainpath/Path(fl) for fl in os.listdir(mainpath)]\nelse:\n    # If it&#039;s a file, create a list with a single Path object for the file\n    all_files = [mainpath]\ndef pipeline(txt_fn_list):\n    &quot;&quot;&quot;\n    Applies a series of functions to a text.\n \n    Parameters:\n        txt_fn_list: A list containing the text to be processed as the first element, and the functions to be applied as the remaining elements.\n \n    Returns:\n        The final result of applying all the functions to the text.\n    &quot;&quot;&quot;\n    # [txt, fn1, fn2....]\n    txt = txt_fn_list[0]\n    # Apply all fns one by one\n    for fn in txt_fn_list[1]:\n        txt = fn(txt)\n    return txt\ndef read_md(path):\n    &quot;&quot;&quot;\n    Reads the contents of a Markdown file.\n \n    Parameters:\n        path: The path to the file.\n \n    Returns:\n        The contents of the file as a string.\n    &quot;&quot;&quot;\n    print(path)\n    try:\n        with open(path, &#039;r&#039;) as fin:\n            return fin.read()\n    except:\n        return None\ndef check_blanks_hash(markdown):\n    &quot;&quot;&quot;\n    Finds headers in a Markdown text that have no content under them.\n \n    Parameters:\n        markdown: The Markdown text to be processed.\n \n    Returns:\n        A list of headers that have no content under them.\n    &quot;&quot;&quot;\n    if markdown != None:\n        lines = markdown.split(&#039;\\n&#039;)\n        headers = []\n        current_header = None\n        current_content = []\n        for line in lines:\n            if line.startswith(&#039;#&#039;):\n                if current_header is not None:\n                    headers.append({&#039;header&#039;: current_header, &#039;content&#039;: &quot; &quot;.join(current_content)})\n                current_header = line\n                current_content = []\n            else:\n                current_content.append(line)\n        if current_header is not None:\n            headers.append({&#039;header&#039;: current_header, &#039;content&#039;: &quot; &quot;.join(current_content)})\n        \n        # Return a list of headers that have no content under them\n        return [header[&quot;header&quot;] for header in headers if len(header[&quot;content&quot;]) &lt; 2]\n    else:\n        return None\ndef main():\n    pipe_fns = [read_md, check_blanks_hash]\n    with concurrent.futures.ProcessPoolExecutor() as executor:\n        # Start a process for each file and store the returned result in a list\n        results = [executor.submit(pipeline, [file, pipe_fns]) for file in all_files]\n        for future in concurrent.futures.as_completed(results):\n            print(future.result())\n \nif __name__ == &#039;__main__&#039;:\n    main()"},"Articles/Productivity/Extensions-that-make-writing-research-papers-easier":{"title":"Extensions that make writing research papers easier","links":["overleaf.com/"],"tags":["article"],"content":"11 Free Chrome/Firefox Extensions That Make Research Easier.\nWhile researching anything, we tend to heavily rely on our browser. To make this process more efficient, quite a few “plugins” have been created over the years. Every major browser has hundreds, if not thousands of such plugins/extensions/add-ons. Like any infinite list of things, it’s often overwhelming to find helpful ones.\nFor my work, I read a lot of articles and skim through many web pages, blogs, you name it. In the process, I need to store this information somewhere. Either to write a research paper, or a blog like this one, or just for my knowledge. The following plugins have made my life a lot easier and so I thought I would share them with you.\nIf any of these plugins make no sense to you, ignore them. You probably don’t need them as of now. But you might later on!\nDisclaimer - I am not sponsored by or affiliated with any of the programs I mention. They are shared purely because I find them useful.\nA Note About Browsers\nI know I only mention Chrome and Firefox. Anywhere I mention Chrome, you can safely assume that Microsoft Edge, Brave, Opera, and Vivaldi also work. The same links should work directly!\nCollecting Information\nI like taking notes, and my tool of choice here is the Markdown (another format like txt) notes app Obsidian. Of course, you can use anything you want. There are endless note applications, so everyone has their preference. These two plugins have been invaluable in collecting information across pages.\n\nRoam Highlighter\nThis little plugin is absolutely beautiful. You can call it with a shortcut and highlight across the whole page! It strips out useless formatting and converts them to Markdown. If you use Roam/Obsidian/any other markdown editor, it even auto-converts the links and converts the highlighted text to markdown.\n\nChrome , Firefox\n\n\nMarkDownload\nSometimes you want large amounts of text from a webpage you come across. This plugin gets all the text/links/images etc from the page you are on and converts it into a format you can easily copy and paste from. Extremely handy isn’t it?\n\nChrome , Firefox\n\n\n\nWriting Research Papers/Articles\nA large chunk of my work involves writing technical articles and research papers. It’s a lot of work, and I need shortcuts to help me out in the process. These plugin-ins have saved me a lot of headaches in the long run.\n\nTamperMonkey: Bibtex copy\nThis plugin is a bit of a special case. Instead of providing specific functionality, it enables users to write their scripts to modify any webpage in real-time. You can do anything from Getting direct links, endlessly loading Google search results to modifying Twitter.\nWhile writing papers, citing them is a huge headache. Since I write my reports in LaTEX, I need the “BibTex” version from Google Scholar which is a pain. This little plugin automatically does that with a single click. You can find out how to install it here.\n\nChrome , Firefox\n\n\nSLext\nIf you write your documents in LaTEX, chances are you use Overleaf to do so. It is a website that makes it extremely easy to write any kind of professional document using LaTEX. But it does have limitations, and the biggest bummer for me is the lack of tabs. This plugin adds just that, and it saves me so much trouble.\n\nChrome , Firefox\n\n\nSci-Hub scholar\nI firmly believe that research should not be paywalled. (Even this article is available for free on my blog without the fancy stuff). If you are affiliated with a university or company, you might have access to as many papers as you want. But as an independent researcher? Well. Happy crying.\nThis plugin adds links to the website Sci-Hub directly in Google Scholar which lets you access a lot of paywalled research directly. (I would link it but I don’t want to get demonetized).\n\nChrome , Firefox\n\n\nUnpaywall\nSimilar to the previous one, this also lets you access paywalled articles by finding other un-paywalled versions of them from elsewhere on the web.\n\nChrome , Firefox\n\n\nZotero/Mendeley Connector\nFor anyone who reads a lot of research papers, managing them is probably the biggest headache. Mendeley and Zotero are probably the most popular library managers.\nWhile browsing the web, you might want to directly save any research paper you like to your computer. This plugin lets you do that with a single click. Both the programs have this installed by default, you just need to enable them from the application itself - Zotero, Mendeley.\n\nMedium\nI admit most of you probably don’t write articles yourself. But if you do, I find these two plugins useful for extending what is possible here on Medium.\n\nCode medium\nAdding proper code snippets is honestly such a pain on Medium. Maybe in time, this will be fixed, but for now, this plugin lets you directly create gists on Github. It looks pretty and just works. I write technical articles and trust me, it has saved me hours.\n\nChrome , Firefox\n\n\nTOC Medium\nIf you have noticed, not every article has a clickable table of contexts like this one. If you were to do it manually, you might need to look at the HTML source code, and do a lot of drama. Instead, this plugin just lets you add the TOC just as you would any other element. It does not auto-update though, so I would recommend leaving it for after you are done with your article.\nIt is also sadly not available for Firefox.\n\nChrome , Firefox(Not there)\n\n\n\nHonorable Mentions\nSome of these are not exactly extensions, but I think they deserve to be mentioned anyway.\n\nMy Text Cleanup Script\nAlthough many of the applications I mentioned above let you copy the text in chunks, they don’t offer any advanced formatting options. (Eg: cleaning up Wikipedia links, making lists, formatting paragraphs, etc.) Now, there are a million ways to do this. But my favorite is this script that I wrote a while back. It uses Python and lets you do whatever you want to the text in your clipboard. After processing is complete, it pastes it back to your clipboard.\nThis is not for everyone. But if you are interested, I’ll be happy to explain how it works. Just ask!\n\nGithub\n\n\nText Workflow\nThis application is not free. (Sorry!) I have a Mac, and this app lets you do whatever my script above does but with a UI. If you want something free, use the previously mentioned script. I’m sure there are other alternatives for Linux and Windows. I either use vim or my script. I have not used Windows in a long time and so I can’t recommend anything.\n\nTextWorkflow\n\n\nAdblocker\nAn adblocker is essential to save your sanity. (Although turning it off on websites you want to support is good!) These are my favorites.\n\nChrome , Firefox\n\n\nI don’t care about cookies\nAs the name suggests, this auto accepts only essential cookies from the prompt. It saves you that extra click and removes the banner that covers the entire bottom of web pages.\n\nChrome , Firefox\n\n\n\nFin\nThis article is in the hopes that it will help someone out. Maybe have the help that I did not. I do not know who it will reach. But to whoever it does, best of luck :)\nLike these/Want more? Buy me a coffee! Kofi\nWant articles on something specific? Just ask!\nYou can always contact me on LinkedIn, or drop me an [[mailto:msubhaditya@gmail.com|Email]]"},"Articles/Productivity/Extracting-Highlights-from-a-PDF-easily":{"title":"Extracting Highlights from a PDF easily","links":[],"tags":["article"],"content":"How to get PDF Highlights to Notes in 3 steps\nReading a research paper/lecture slides/book? Made a lot of highlights in a PDF? Now want to get them into your notes as text? Don’t want to have to type everything again? Here’s your way out.\nHey. Reading a research paper or a long document is itself a lot of effort. After that, I do not have the energy to again copy everything properly into my notes. I have been searching for a way to do that, but none of the options I found work for two-column layouts, or even maintain page order properly.\n(Disclaimer: I am not sponsored by, or hold any affiliations to any of the products mentioned below. I share them because I use them every day and think they would be useful to you.)\nWhat we want to do\nConvert highlights like in this document on the left to notes with minimal effort.\nI use a note application called Obsidian, but anything you use works here. As long as it supports text.\nExisting Programs and… their flaws\nMany PDF viewers offer the ability to export notes into text. Like Skim for Mac, Adobe Acrobat, OneNote etc\nBut they have their fair share of issues\n\nMost of them are paid\nTwo column layouts, like those in research papers are not handled very well\nThe export option removes all page information which makes it super hard to format it\nThey do not export comments or other annotations properly\nThey get very confused when the word or line spacing differs a little\n\nYou get the point.\npdfannots\nAfter a lot of searching, I found a Python script on GitHub called pdfannots. Sadly their documentation makes it very hard for someone without experience to get it running. Hence this article.\nInstalling the script\nThere are not many steps involved here. But it might sound a little complicated.\n\nStep 1: Install Python. If you are on Windows, select install ‘pip’ as well. If you are on Mac or Linux, you can skip this step as it already comes pre-installed with your system.\nStep 2: Open a terminal (Hit the Windows button → Search/ Mac: Open Spotlight → Search/Linux: You probably know how to)\nStep 3: Run (paste it and hit Enter) the following command\n\npip install pdfannots\n\n(If you get an error, you probably did not install Python or pip. Try Step 1 again.)\nPS: Don’t get annoyed at me for not counting these steps. This is a one-time install after all. Once it’s on your laptop, you can just proceed with the ones below every other time.\nGetting your highlights!\n\nStep 1: Once that completes, identify where the PDF with highlights is saved on your computer. You can open the location in your file manager. Note the location. Replace  with this\nAs an example : (Windows: “C:\\Users\\Subha\\Documents\\paper.pdf)”,  Mac : “/Users/eragon/Documents/paper.pdf”)\nStep 2: Now go back to the terminal, and run the following\n\npdfannots &lt;filename&gt;\n\n\nStep 3: Voila! Now just select all that output text, copy it, and paste it wherever. Easy right?\nStep 4: This is optional of course, but you can format your text however you want to. (I will write an article on how you can do that and update this blog later.)\n\nSpecific Formatting\nMaybe your document did not get read properly. Here is a little FAQ.\n\nDocument not found: Check your file path properly. Maybe a little google search? Try dragging and dropping instead.\nTwo column research paper, use this instead: pdfannots —cols=2 \nSpaces all weird? : pdfannots —word-margin= \nWeird technicalities? : Refer to the original page of the script. You can also ask the developers questions.\nWant to automate common formatting, convert to markdown, etc? : (An article will come soon, but you can leave a comment in the meanwhile.)\n\nFin\nThis article is in the hopes that it will help someone out. Maybe have the help that I did not. I do not know who it will reach. But to whoever it does, best of luck :)\nHave any questions? Comment or shoot me an email.\nLike these/Want more? Buy me a coffee! Kofi\nWant articles on something specific? Just ask.\nYou can contact me on LinkedIn, drop me an [[mailto:msubhaditya@gmail.com|Email]]"},"Articles/Productivity/Obisidian-Daily-Notes":{"title":"Obisidian Daily Notes","links":[],"tags":["article"],"content":"Daily Notes in Obsidian\nObsidian is one of my favorite productivity tools, but there are some things that I don’t fully like about it. One of them that being the daily notes feature. I use Obsidian for journaling and getting an overview of my day while making notes and so being able to use this function efficiently would be very helpful to me.\nIn this article, I talk about how I use daily notes in Obsidian, the tweaks I have made, the plugins I use, and the hotkeys I have set up.\nNote: None of the plugins I discuss are sponsored and I am sharing them only because I use almost them every day.\nA Disclaimer\nThis article is about the way I like to work. If it does not work for you, take what you want from it and modify the workflow to fit your style. There are no “rules”. I wanted to write this article as I did not find too many of them on using Daily notes more efficiently.\nWhy use Daily notes?\nDaily notes help me track my daily activities in a structured way. This way, I can easily find what I did on a particular day and see if I’m progressing toward my goals. It helps me keep track of my time and allows me to see if I am spending too much or too little time on certain tasks, enabling me to make adjustments where needed.\nFurthermore, by keeping a record of my daily work, I can get a quick overview of my weekly work. This overview also helps me to find topics that I have recently worked on, which is useful when I want to continue working on them or refer to them later.\nHaving a place to temporarily store ideas before turning them into proper notes removes the pressure of having to write something perfect from the start. This way, I can record my thoughts without worrying about the format or accurately writing something, which reduces the chance of breaking my flow.\nAn example of what a daily note for me might look like is as follows.\nMy issues with Daily notes\nOne of the main areas for improvement of the daily notes feature in Obsidian is the need for an overview. The lack of one makes it difficult to quickly see all of the notes I have created on a specific date or within a certain range of dates.\nIt can also be hard to navigate between different dates, making it time-consuming to find the note I am looking for.\nAnother issue that I wanted to tackle was the ability to record time. The lack of this means I don’t have any record of the exact time I created or edited something, making it challenging to track how much time I spent on a particular task or activity.\nAdditionally, getting to the daily notes page requires too many clicks and can be frustrating, especially because I use this feature frequently and need to access my daily notes quickly. These shortcomings make it less convenient for me to use the daily notes feature, making my workflow less efficient.\nSome solutions\nPlugins are a great way to solve specific problems in my workflow. Some of the plugins that I have mentioned in this article, such as the Daily Notes Plugin and the Time Stamper Plugin, have greatly improved my experience with the daily notes feature in Obsidian. They provide an overview of all my daily notes, make it easy to navigate between them, and easily add timestamps to my notes. These plugins have helped me stay organized, find my notes quickly, and make the most of my time.\nIn addition to using plugins, I also use shortcuts to make my workflow more efficient. I use shortcuts to navigate between pages quickly, create new notes and add timestamps without breaking my flow while working. These shortcuts help me focus on the task by reducing the time I would otherwise spend navigating through pages or menus.\nTemplater Plugin\nThe Templater plugin is a useful tool that I use to streamline the process of creating new notes in Obsidian. It allows me to insert a predefined template with a shortcut every time I start a new note. This template contains the date and time I created the note and when it was last modified. The modified time is automatically updated as I make changes to the note. It also has tags and headers, which makes it easy for me to organize my notes and find the ones I need more easily. This plugin saves me time and effort by providing a consistent and standardized format for all my notes. It also provides me with important information about when the note was created and last modified, which is useful for me to track the progress of my work. This helps me to stay organized and make the most of my time by making it easy to find and refer to my notes.\nThe template I use is as follows. It uses the Liquid syntax to insert entries automatically. Note that I have configured the plugin to ignore the folder where I save these templates to prevent Obsidian from auto-converting the template into text.\n---\ntoc: true\ntitle: &lt;% tp.file.toc: true\ntitle %&gt;\ntags: [&#039;temp&#039;]\ndate modified: &lt;% tp.date.now(&quot;dddd Do MMMM YYYY, ddd&quot;) %&gt;\ndate created: &lt;% tp.date.now(&quot;dddd Do MMMM YYYY, ddd&quot;) %&gt;\n---\n# &lt;% tp.file.toc: true\ntitle %&gt;\nDaily Notes Plugin\nThe Daily Notes Plugin is a great solution to improve my daily note-taking experience. It provides a single-page overview of my daily notes, making it easy to navigate between them. Instead of hunting for something I wrote, I can scroll through the notes and find it quickly. Additionally, each date is editable directly, so I can skip opening multiple pages or searching for the note I want to edit. This makes it much more convenient and efficient for me to review, update or organize my daily notes.\nAnother advantage of using this plugin is how simple it is to create new daily notes. Instead of navigating multiple pages or menus, I can click a button to create a new note for the day. This feature is incredibly useful for quickly capturing new ideas, thoughts, or information that comes up during the day and helps me to keep track of my daily activities and efficiently.\nThe picture shows what this plugin looks like.\nTime Stamper\nThe Time Stamper Plugin is another very useful plugin in my note-taking process. It enables me to use a custom time stamp format, which gives me the hour and minute time as a new bullet point. This plugin makes it easy for me to track time and is an efficient way to keep track of what I do during the day. Not having to enter the time manually saves me time and makes my note-taking process more streamlined.\nI use this plugin mostly when I start a new topic. This way, I can quickly see when I began working on it. This plugin does not replace manually tracking time spent on a note which can be useful when I want a rough idea of the duration of a topic or task.\nThe template I use to generate my timestamps is.\n- **hh:mm** \nThis picture shows what it looks like.\nUseful Shortcuts\nI use some custom shortcuts to navigate between different pages in Obsidian quickly. These shortcuts save me time and effort by allowing me to jump between pages with a single keystroke. These shortcuts are essential to keep my work efficient and streamline my note-taking process.\n(Note that if you are on Windows or Linux, Command is replaced by Control and Option by the Alt key.)\nOne shortcut I often use is Command+Shift+G, which takes me directly to the daily notes page. This shortcut allows me to quickly access my daily notes and start working on them without navigating through different menus or pages.\nAnother shortcut that I use frequently is Command+Shift+T. This shortcut creates a timestamp quickly and helps me keep track of time when I start a new topic or task, making it easier to track the time spent on each task.\nFor navigation between notes, I use Command+Option+Left Arrow to access the previous note quickly and Command+Option+Right Arrow to quickly access the next note. These shortcuts help me stay organized by allowing me to easily switch between notes without manually navigating through different pages."},"Articles/Productivity/Obsidian-Plugins":{"title":"Obsidian Plugins","links":["tags/topic2"],"tags":["article","topic2"],"content":"My Favourite Obsidian Plugins for Research Notes + 2 Bonus Tips\nObsidian is my favourite program for taking notes. Be it for research, general things I learn, summaries from papers, lecture notes and the like. Out of the box, it does so many things really well.\nBut, its real power lies in the vast number of plugins it has. Most of these are user created, and you can even make your own (or hack one together)! In this sea of functionality, these are the top few that I use. Grouped by the type of task for easier lookup.\n(Disclaimer : I am not sponsored by either Obsidian or any of the authors of the plugins mentioned here. These are personal preferences.)\nThe use case\nI am a student, researcher and programmer. I take lecture notes, read a lot of research papers, articles and books. These come down to a lot of information. Of course, there’s no way I can remember all of these bits of fragmented information.\nTherefore, drumroll…, I use Obsidian to help me put these bits of information in a place I can easily access. Since I use this almost everyday, I want taking notes to be as painless and efficient as possible.\nThese plugins are a huge help in doing exactly that. (Ordered by the type of task)\nHow to install these plugins?\nThis is a simple step. Simple open Obsidian Settings, Scroll down a bit and select “Community plugins”. Disable Restrictive mode, and then browse to your hearts content!\nWriting\nWriting notes is the major objective here. So how do we make it extra painless? Plugins of course!\n\nDynamic Table of Contents : Many times, I take notes for a long form text. Sometimes these notes end up pretty huge, and it becomes slightly harder to find something. What about adding a Table Of Contents to the start? Sounds great, but what if we update the note? In comes this plugin, with an automatically updating TOC.\nTags : Super simple, also built in. Adding “#topic1,topic2 etc” to a file to make it easier to search and organise.\nFrontmatter Tag Suggest : Tags are great, but who remembers which ones they used before? Nobody. This plugin autocompletes tags based on ones you have used in previous notes. You can create new ones the normal way of course.\nNote Refactor : Made a huge note with a lot of headings? Why not split them into individual topics and maintain links to them? This makes it easier for you to have one major idea per note. Here I have a bunch of test headings, you can see how after applying them they become new notes that link to the current file.\nPaste URL into selection : The name says it all doesn’t it?\nTemplater : Another plugin I use daily. I like starting my notes with “date_created”, “date_modified”, “tags”, “toc: true\ntitle” and insert the file name as the header. Since I do this for every single note, why not automate it? This plugin lets you create blocks of dynamic text to be inserted with a keyboard shortcut. I use “Cmd+Shift+I” (“Control+Shift+I” for Windows)\nTypewriter Scroll : Zen Mode is a way of life. This lets me focus on what I am writing by automatically scrolling the page, and dimming the rest of the text apart from the line I am currently writing. I do disable it while reading though.\nCommand Palette : This one is pretty obvious, but this built in plugin is just a text search. You can quickly open files with a (Cmd/Control + O) that brings up a searchable menu, or use (Cmd/Control + P) to bring up a searchable list of quick actions.\nVim Mode : This little option is not for everyone honestly. If you have never heard of Vim, just skip this point. I use vim as my default text editor for everything else. And I can’t live without its keybindings. This just lets me use the vim keys for everything.\n\nResearch\nFor research (AI research in my case), we have three main objectives :\n\nMerge important information from a large number of sources.\nFind links between ideas that you did not see.\nMaintain a daily log as something of a lab notebook.\nThere are 5 plugins that fulfil these criteria pretty decently.\nDaily Notes : This is a Core plugin and comes with Obsidian. Essentially it’s a journal. You can add whatever you want to it and it is created every day. I use it to keep a time stamped log of what I did that day. It is also useful if you just want to dump a bunch of information but don’t want to format and organise it just yet.\nTimeStamper : In my daily notes, I like having timestamps (eg - 9:30 : I did xyz). This plugin lets me set a custom format and a keyboard shortcut. I have set it to “Cmd+T” (for Mac or Control+T for Windows)\nBacklinks : A real game changer and another built in plugin. This shows you every file that either is linked in the current file, or refers to the current one. Identifying links between concepts, and finding more of them is absolutely invaluable in research.\nQuick LaTEX for Obsidian : LaTEX is probably the easiest way of writing professional looking math-y stuff, be it equations or formulae or anything similar. This plugin has a lot of options for autocomplete, formatting, and makes my job almost ridiculously easy. Here’s how it looks. (Just typing a random equation)\n\nOrganising\nWhat do you do once you have a lot of files, you organise them of course! Now Obsidian by default makes it pretty easy to do this. But these plugins make organising less of a chore and much more of a fun thing to do.\n\nLocal Images : To make my notes more informative, I sometimes paste images. Now many times these are links from some website, which makes it a little risky, because what if the website stops working? This plugin automatically downloads image links in your notes and saves them locally. (It also links to the correct downloaded file.)\nGraph view : Oh the gift and curse of a pretty graph. I sometimes use this to navigate between my links either to or from a file. It also gives me a very useful overview of what I have. I generally use the “Local graph” that shows me a graph for the current note, rather than the “Global” full one which shows me everything. (It’s pretty, but unhelpful)\nLinter : Maybe I have a bunch of empty lines, empty list items, my headers are not in sentence case, my text is not formatted, my paragraphs are weird. Or anything like that. I am lazy, so I use the Linter plugin to automatically perform a bunch of processing and clean up my files.\nTag Wrangler : Have a lot of tags? View/Edit/Change them across every file that uses them in one place. Also useful for finding files that match a few criteria.\nFile Cleaner : Remove empty files, unreferenced images etc. Keeping your “Digital Garden” pruned and bug free.\nObsidian Link Converter : Because I host my Obsidian Vault on a personal website, sometimes the links that Obsidian uses don’t work, this plugin lets me mass convert them to a format that does.\n\nBonus tips!!\n\nMake sure every file has a single major idea. If you have too many, use the “Note Refactor” to put them in their own files. This will make it extremely easy to refer to the “Ideas” in the text somewhere else instead of linking to the whole text.\nWant pages that consolidate all the notes that have a particular tag together and save them automatically to a single file? Say you want a file that has links to all the notes that have the tag “#apple”. Here is a little script that I wrote which does just that.\n\nFin\nThis article is in the hopes that it will help someone out. Maybe have the help that I did not. I do not know who it will reach. But to whoever it does, best of luck :)\nLike these/Want more? Buy me a coffee! Kofi\nWant articles on something specific? Just ask!\nYou can always contact me on LinkedIn, or drop me an [[mailto:msubhaditya@gmail.com|Email]]"},"Articles/Productivity/Pomodoro---A-means-but-not-the-end":{"title":"Pomodoro - A means but not the end","links":[],"tags":["article"],"content":"Pomodoro : A Means but not the End\nIn our quest for productivity, many ideas, tools, and theories have sprung up over the ages. Some work, some don’t, and some are versions of previous ones with a “modern” twist. The concept of the Pomodoro timer seems to have stood the test of time. The hustle culture especially has glorified its existence and made it out to be something of a magic pill that either works for you or does absolutely nothing.\nI think it is a very valuable tool. But in time, we have stopped thinking of it as a tool and made it out to be more of a rule.\nSo how do we make it work for us?\n(Disclaimer - I am no self-help productivity guru. This works for me, and I share it in the hopes that it helps someone else. Don’t take my words as a golden rule.)\nA definition\nBefore we move further it is my habit to define the terms we work with. A Pomodoro timer, in its essence, is a combination of time blocking and scheduling rest periods. It has “cycles” that are combinations of a 25-minute deep work session followed by a 5-minute break. After four such cycles, a longer break of 45 minutes can be taken.\nIn theory, it does sound pretty good. So why is it that it doesn’t work for so many of us?\nWhy does it fail\nI think there are a few simple reasons why it might not work for you. Which one(s) is(are) it for you?\n\nYou try sticking to the 25-5-25-5.. timing, but it just doesn’t work.\nYou start one, but something comes up in between and then it just doesn’t work.\nYou think it should give you hours of focus, and when it doesn’t, you decide to just use something else instead.\nThe breaks are not timed right.\nYou end up constantly looking at the timer and it is way too distracting.\n\nSounds like you? Welcome to the club. Something else? Let me know in the comments. Either way, let’s make it work for us.\nRedefining our objective\nI believe that the common theme we face with using the Pomodoro, or honestly any other productivity technique is that we put too many expectations on it. At the end of the day, it’s just a tool. The ability to use a tool depends on who wields it.\nOur objective is not to work for long long hours. It is to focus, and get into an undistracted state where you are deeply concentrated on getting whatever you have to do, done. A “flow state” if you want to use a trendy term.\nPomodoro can help there. But only if you let it.\nHow I use it\nIndeed, that was a pretty long-winded introduction to something rather simple. I use it as a means to get started. I don’t think I have ever completed a “cycle” recently. But almost every time, it does help me set the tone for my work session of the day.\nAs of writing this article, I had a Pomodoro of 30 minutes going. After that initial time, I think I spent about an hour more. I didn’t check on the timer then. I just sat down and finished my writing for the morning.\nBut, as it goes, this is not perfect. In time I have noticed a few factors that make or break my session.\n\nPlanning. If I don’t plan out my work for the day, and just get started, nothing I do works. I’m still too distracted even with the timer on. I like to block off times for work on a calendar and try my best to stick to them.\nChanging the timings. A 30 - 5 - 30 … timing works for me when it comes to longer running tasks. Sometimes, a task I pick is more challenging and I end up adjusting the breaks to give me some more rest.\nBreaks are subjective. There is no way for Pomodoro to know what you are working on. If the break comes when you are way too focused and working hard, skip it. Take it later though. Add them up.\nFinding your productive times. To make use of the timer, try to find the general times in a day that you have more mental clarity and headspace. For me, it’s earlier in the day. And maybe a few hours in the evening. I do have to say that, these times aren’t fixed in stone. Pick what works for you.\nStart with a completed task. I find the satisfaction of finishing something very motivating. I have noticed that if the first task you do in your work session is something you know for sure that you can complete, it does set the tone for the rest of the time. Try ordering your work that way.\nPlay around with the timings. With some experimentation, you will find one that works for you.\nDon’t stick with it. Ironic, I know. Use it to start and then do what you need to.\n\nFin\nThis article is in the hopes that it will help someone out. Maybe have the help that I did not. I do not know who it will reach. But to whoever it does, best of luck :)\nLike these/Want more? Buy me a coffee! Kofi\nWant articles on something specific? Just ask!\nYou can always contact me on LinkedIn, or drop me an [[mailto:msubhaditya@gmail.com|Email]]"},"Articles/Productivity/Taking-notes-from-websites-to-markdown---A-workflow":{"title":"Taking notes from websites to markdown - A workflow","links":["tags/inprogress","tags/articles"],"tags":["article","inprogress","articles"],"content":"Taking notes from websites to markdown - A workflowinprogressarticles\nWhy\nWhat we want\nObsidian Quick intro\nWhy markdown\nChrome extension\nExtension\nRoam Highlighter\nScript\nimport clipboard\nimport os\nfrom pathlib import path\nimport re\ndef apply_transforms(txt, list_of_trans):\n    for trans in list_of_trans:\n        txt = trans(txt)\n    return txt\n\ndef replacer(txt, dict_of_replace):\n    for rep in dict_of_replace.keys():\n        txt = txt.replace(rep, dict_of_replace[rep])\n    return txt\n\ndef regexreplacer(txt, dict_of_replace):\n    for rep in dict_of_replace.keys():\n        txt = re.sub(rep, dict_of_replace[rep], txt)\n    return txt\n\ndef indent_transform(txt):\n    l = txt.split(&quot;\\n&quot;)\n    return &quot;\\n&quot;.join([x.strip() for x in l])\n\ndef paragraph_converter(txt):\n    l = txt.split(&quot;\\n&quot;)\n    for item in range(len(l)):\n        if len(l[item])&gt;0 and l[item][0] not in [&quot;#&quot;,&quot; &quot;,]:\n            l[item] = &quot;- &quot;+l[item]\n    return &quot;\\n&quot;.join(l)\n\nPractically Using it\ndict_regex_replace = {\n    r&#039;\\[.*?\\]&#039;:&quot;&quot;, #wiki links\n}\n\ndict_of_replace = {\n    #&quot;*&quot;: &quot;&quot;,\n    &quot;- **&quot;: &quot;# &quot;,\n    &quot;**&quot;: &quot;&quot;,\n    &quot;****&quot;: &quot;&quot;,\n    &quot;[latex]&quot;: &quot;$&quot;,\n    &quot;[/latex]&quot;: &quot;$&quot;,\n        &quot;(&lt;en.wikipedia.org/w/index.php: true\ntitle=&quot;:&quot; &quot;,\n    &quot;(en.wikipedia.org/w/index.php: true\ntitle=&quot;:&quot; &quot;,\n    &quot;s&amp;action=edit&amp;redlink=1&quot;:&quot; &quot;,\n}\n\ntext = clipboard.paste()\ntext = replacer(text, dict_of_replace)\ntext = regexreplacer(text, dict_regex_replace)\ntext = apply_transforms(text, [\n    indent_transform,\n    paragraph_converter,\n])\nclipboard.copy(text)\n\nFAQ\nWhy not a program?\nFin\nThis article is in the hopes that it will help someone out. Maybe have the help that I did not. I do not know who it will reach. But to whoever it does, best of luck :)\nLike these/Want more? Buy me a coffee! Kofi\nWant articles on something specific? Just ask.\nYou can contact me on LinkedIn, drop me an [[mailto:msubhaditya@gmail.com|Email]]"},"Articles/Productivity/index":{"title":"Productivity","links":[],"tags":[],"content":""},"Articles/Scalar/Autoencoders-with-Convolutions":{"title":"Autoencoders with Convolutions","links":["KB/Dimensionality-Reduction"],"tags":["article"],"content":"Autoencoders with Convolutions\n:::section{.abstract}\nOverview\nIn the absence of labels in a dataset, only a few models can perform well. The Convolutional Autoencoder is a model that can be used to re-create images from a dataset, creating an unsupervised classifier and an image generator in the process. This model uses an Encoder-Bottleneck-Decoder architecture to understand the latent space of the dataset and re-create the images.\n:::\n:::section{.scope}\nScope\n\nThis article explains the principle behind the Convolutional Autoencoder.\nIt explores the architecture of the Autoencoder.\nIt presents a template we can use to implement the Autoencoder in Tensorflow.\nThe article also covers the loss functions and optimizers required to train a Convolutional Autoencoder.\n\n:::\n:::section{.main}\nIntroduction\nTo be able to learn how to re-create a dataset, a model must have an understanding of the underlying latent space. The Convolutional Autoencoder compresses the information in an image dataset by applying successive convolutions. This output is passed to a Bottleneck layer, the smallest possible representation of the dataset. Using this compressed representation, the Decoder attempts to recreate the original dataset. In the process of re-creation, the compressed output resembles a sort of Dimensionality Reduction procedure, while the Reconstruction Loss can be used as a classification metric.\nThis article explores the architecture and methods behind creating a Convolutional Autoencoder.\nWhat is an Autoencoder?\nThe convolutional Autoencoder is part of a family of models that can reduce the noise in data. In the noise reduction task, these models learn the latent space by reconstructing the data. Autoencoders are split into three main parts - the Encoder, the Bottleneck, and the Decoder. The Encoder compresses the input data while keeping the useful features. The Bottleneck is responsible for choosing the important features that can flow through it to the Decoder. Finally, the Decoder uses the features passed to it by the Bottleneck layer to reconstruct the input.\nThe architecture diagram of a convolutional autoencoder is shown below.\n[IMAGE {1} { arch } START SAMPLE]\n\n[IMAGE {1} FINISH SAMPLE]\nEncoder Structure\nThe Encoder part of the network is to compress the input data and passes it to the Bottleneck layer. The compression creates a knowledge representation much smaller than the original input but has most of its features.\nThis part of the network comprises blocks of convolutions followed by pooling layers that, in turn, further help to create a compressed data representation. The output of an ideal Encoder should be the same as the input but with a smaller size. The Encoder should be sensitive to the inputs to recreate it and not over-sensitive. Being over-sensitive would make the model memorize the inputs perfectly and then overfit the data.\nBottleneck layer\nThe Bottleneck is the most important layer of an Autoencoder. This module stores the compressed knowledge that is passed to the Decoder. The Bottleneck restricts information flow by only allowing important parts of the compressed representation to pass through to the Decoder. Doing so ensures that the input data has the maximum possible information extracted from it and the most useful correlations found. This part of the architecture is also a measure against overfitting as it prevents the network from memorizing the input data directly.\nNote that smaller bottlenecks lead to lesser overfitting (to an extent).\nDecoder Structure\nThis part of the network is a “Decompressor” that attempts to recreate an image given its latent attributes. The Decoder gets the compressed information from the Bottleneck layer and then uses upsampling and convolutions to reconstruct it. The output generated by the Decoder is compared with the ground truth to quantify the network’s performance.\nLatent Space Structure\nThe latent space of a network is the compressed representation it creates from an input dataset. This latent space usually has hundreds of dimensions and is hard to visualize directly. More complex neural networks have latent spaces so hard to visualize that they are generally referred to as black boxes. In a convolutional autoencoder, the better the representation of the data, the richer the latent space. The space structure here is a large matrix of tensors that encode the weights of layers of the network.\nUses of Autoencoder\nThe Convolutional Autoencoder architecture is good for a lot of use cases. Some of these are explained below.\nReducing Complexity\nThe Encoder of the model works very well as a Dimensionality Reduction technique. For example, if we consider an image dataset, we can compress every image before feeding it to another model. This compression reduces the number of input values and thus makes the model less likely to be biased toward smaller details. The Autoencoder thus helps in improving the performance of a second model.\nAnomaly Detection\nAn Autoencoder is generally used for reconstructing the base data using an Encoder-Bottleneck-Decoder architecture. Thus if the output reconstruction has a much larger error for a given sample, this sample could be an outlier. We can thus use the reconstruction error to find unusual data points in a dataset.\nUnsupervised Learning\nA convolutional Autoencoder’s information compression ability is a good start for an unsupervised learning problem. Using the Autoencoder as a Dimensionality Reduction technique allows the data to be clustered without any labels much more easily. This clustering may not be useful but can be a starting point for many other solutions.\nWhat is a CNN?\nA CNN is one of the foundation stones of Deep Learning that can take an input data sample and learn to recognize it given other such samples. A CNN generally comprises an input, multiple hidden layers, and an output. CNN’s are composed of individual neurons that respond to specific sets of stimuli and combine their responses to perform many tasks such as classification, clustering, segmentation, and others. CNNs are useful because they can understand spatial and temporal dependencies given large amounts of data by learning feature “filters”. Complex CNNs can model large amounts of data previously impossible for a computer to understand.\nA simple CNN is shown below.\n[IMAGE {2} { CNN } START SAMPLE]\n\n[IMAGE {2} FINISH SAMPLE]\nThis article elaborates on a CNN-based architecture called the convolutional Autoencoder.\nImplementation of an Autoencoder\nThe Convolutional Autoencoder has a few hyper-parameters that must be tweaked before training. The section below discusses these hyper-parameters, the Reconstruction loss used, and the general implementation of the architecture of a simple convolutional Autoencoder.\nAn example use case would be to re-create the MNIST dataset. The following image shows the input and output of an Autoencoder that was trained on such a task. The first row is the original input and the second row is the output of the Autoencoder. In this case, a perfect reconstruction is obtained.\n[IMAGE {3} { Example } START SAMPLE]\n\n[IMAGE {3} FINISH SAMPLE]\nA complete demonstration of the code to re-create the MNIST dataset can be found on this link.\nCode size\nThe code size is defined as how large the bottleneck layer is, consequently deciding to what extent the input data is compressed before being Decoded. The code size is the most important hyper-parameter to tune.\nNumber of Layers\nThe convolutional Autoencoder is similar to other types of networks in that the number of layers is a hyper-parameter. Note that increasing the number of layers increases the time to train a network. A higher model depth also increases inference time.\nNumber of Nodes per Layer\nThis hyper-parameter controls the weights per layer. As a general rule, the number of nodes decreases in the Encoder and increases in the Decoder.\nReconstruction loss\nThe loss function the convolutional Autoencoder is trained to minimize is called the reconstruction error. This loss function depends on the type of data we use. In the case of image-based data, the reconstruction loss can be either an MSE, an L1, or Binary Cross Entropy loss.\nEncoder\nThe Encoder can be created with a stack of 2D Convolutions starting with a large size and slowly reducing in dimensions. Each of the convolutions is followed by a 2D Max-Pooling layer. The ReLU activation is used throughout.\nIn Keras, the Encoder can look something like this.\nx = layers.Conv2D(16, (3, 3), activation=&#039;relu&#039;, padding=&#039;same&#039;)(input_img)\nx = layers.MaxPooling2D((2, 2), padding=&#039;same&#039;)(x)\nx = layers.Conv2D(8, (3, 3), activation=&#039;relu&#039;, padding=&#039;same&#039;)(x)\nx = layers.MaxPooling2D((2, 2), padding=&#039;same&#039;)(x)\nx = layers.Conv2D(8, (3, 3), activation=&#039;relu&#039;, padding=&#039;same&#039;)(x)\nencoded = layers.MaxPooling2D((2, 2), padding=&#039;same&#039;)(x)\nDecoder\nThe Decoder comprises blocks of 2D convolutions followed by Up Sampling layers. This part of the network looks something like the following in Keras.\nx = layers.Conv2D(8, (3, 3), activation=&#039;relu&#039;, padding=&#039;same&#039;)(encoded)\nx = layers.UpSampling2D((2, 2))(x)\nx = layers.Conv2D(8, (3, 3), activation=&#039;relu&#039;, padding=&#039;same&#039;)(x)\nx = layers.UpSampling2D((2, 2))(x)\nx = layers.Conv2D(16, (3, 3), activation=&#039;relu&#039;)(x)\nx = layers.UpSampling2D((2, 2))(x)\ndecoded = layers.Conv2D(1, (3, 3), activation=&#039;sigmoid&#039;, padding=&#039;same&#039;)(x)\n:::\n:::section{.summary}\nConclusion\n\nConvolutional Autoencoders are a powerful unsupervised learning technique.\nThe article explained the Encoder-Bottleneck-Decoder architecture in detail.\nThe Reconstruction Loss obtained is a valuable classification and image generation resource.\nThe article also explained multiple use cases such as Anomaly Detection, Complexity Reduction, and some others.\nThis article also provided a template for implementing a Convolutional Autoencoder in Tensorflow.\n:::\n"},"Articles/Scalar/Building-a-GAN-from-scratch":{"title":"Building a GAN from scratch","links":["KB/Dropout","KB/Strided"],"tags":["article"],"content":"Building a GAN from scratch\n:::section{.abstract}\nOverview\nGenerating images from scratch is a huge deal in computer vision. A Generative Adversarial Network(GAN) was one of the first models to generate new images in an unsupervised manner efficiently. A GAN is not a single model but a family of different architectures used for image generation.\nThis article will look at the first Generative Adversarial Network, a vanilla GAN. We will learn how to make a Generative Adversarial Network from scratch.\n:::\n:::section{.scope}\nScope\nThis article covers the following topics:\n\nWhat is a Generative Adversarial Network, and how to make a Generative Adversarial Network from scratch?\nWhat is the architecture of a GAN, and what are the loss functions and optimizers required to train one?\nHow to feed a custom dataset to a GAN and use it to generate novel images.\n\n:::\n:::section{.main}\nIntroduction\nA GAN is a network we can use to create novel images given any vision dataset. In most cases, they are unsupervised, but many architectures also consider labels during training. Some examples of outputs GANs are shown here.\n[IMAGE {1} Summer to Winter START SAMPLE]\n\n[IMAGE {1} FINISH SAMPLE]\n[IMAGE {2} Face Generation START SAMPLE]\n\n[IMAGE {2} FINISH SAMPLE]\nGANs have much bigger and more complex architectural pipelines than a standard Convolutional network. They generally have two major structures, the Generator and the Discriminator. These structures are Convolutional networks that we can substitute for other networks that perform similar functions.\nThe training paradigm for GANs is called Adversarial Training and relies on an interplay between the Generator and the Discriminator.\nThis article will look at what a Generative Adversarial Network is and its components. After we understand the parts, we will build our own GAN from scratch and train it on a dataset of handwritten images (MNIST).\nArchitecture of a GAN\nOne of the hardest parts of understanding how to make a Generative Adversarial Network is comprehending the architecture. GANs are very different from regular neural networks in that they are composed of two completely different neural networks - The Generator and the Discriminator.\nConsider the architecture diagram shown below.\n[IMAGE {3} Architecture Diagram START SAMPLE]\n\n[IMAGE {3} FINISH SAMPLE]\nThe first part of the architecture is the Generator, whose job is to create images realistic enough that the Discriminator cannot tell the difference between a fake image and a real one.\n[IMAGE {4} Generator And Discriminator START SAMPLE]\n\n[IMAGE {4} FINISH SAMPLE]\nGenerator\nThe Generator can be considered a network that takes a random noise and then arranges the pixels to make it look like a real image. It is also a simple neural network composed of blocks of fully connected linear layers (FC) and Leaky ReLU activations. In the final layer of the Generator, the LeakyReLU is replaced by a Tanh activation. The Tanh activation is chosen as we do not want probabilities but want to take the generated image and squish it to the range of (-1,1). This range is the range of the MNIST data images.\nDiscriminator\nThe second part of the network is the Discriminator, whose job is to take the images that the Generator creates and return the probability that the image is real.\nThe Discriminator is a binary classifier and comprises blocks of fully connected linear layers (FC), Leaky ReLU activations, and Dropout layers. The final layer of the Discriminator is a block with an FC layer and a Sigmoid at the end. The Sigmoid is responsible for returning the classification probability that we want.\nDemystifying the loss function\nLoss functions are an essential part of any neural network pipeline. Before we learn how to make a Generative Adversarial Network, we first need to understand the loss functions.\nDiscriminator loss\nThe Discriminator’s job is to classify the generated images into real or fake and return the probability that it is real. To do so, it needs to do extremely well at ensuring that the input it gets belongs to the real dataset. It should also ensure that if the input is fake, it is not classified as belonging to the real dataset.\nMathematically, this can be understood as maximizing D(x) and minimizing D(G(z)).\nGenerator loss\nThe Generator is tasked with ensuring that the Discriminator is fooled. It can do so by creating realistic images that the Discriminator thinks are real. This process can be thought of as ensuring that the Discriminator classifies an image sampled from the fake dataset as belonging to the real one.\nMathematically, this is formulated as maximizing D(G(z)).\nUsing this as the loss might lead to the network becoming extremely confident, even if it is wrong. To prevent this from happening, log(D(G(z))) is used instead.\nTotal loss\nThere is no net loss that is used in practice. Still, while learning to make a Generative Adversarial Network, we must consider the total theoretical loss the network is trying to optimize.\nTraining a Generative Adversarial Network is a game between two enemies (aka adversaries). In other words, this is a MinMax game where one party attempts to reduce the probability of the other winning. Both parties are simultaneously also trying to increase their chances of winning.\nMathematically, this can be represented as \\underset{G}{min} \\underset{D}{max} V(D,G) = \\mathbb{E}_{x \\sim p_{data}(x)}[log(D(x)] + \\mathbb{E}_{z \\sim p_{z}}[log(1 - D(G(z))]\nHeuristic loss\nAnother aspect of knowing how to make a Generative Adversarial Network is understanding heuristics. These heuristics are not part of any network directly but are training guidelines that work for most GANs. (Any GAN created before 2016, at least.)\nWe can use these heuristics to ensure stable reductions in the loss landscape, which is key to training a Generative Adversarial Network well.\n\nIf the network has any pooling layers, they can be replaced with Strided convolutions in the Generator.\nWe can use Batch Normalization layers in the Generator and the Discriminator.\nIf the architecture is deep, we should remove FC layers for better performance.\nAs for activations, the ReLU activation should be used for all the layers. The only exception is the output layer, where a TanH activation should be used.\n\nTraining a GAN\nWe need an optimization algorithm that performs gradient descent on the network weights to train the GAN. The SGD (Stochastic Gradient Descent) algorithm is used for a vanilla GAN such as ours. The Generator and the Discriminator are assigned to their SGD optimizer for training. This procedure ensures that they both learn independent weights. Since the outputs of both networks flow to and from each other, they are influenced by each other as well.\nThe general training paradigm for any GAN is as follows. This paradigm is always a good place to refer to when figuring out how to build a Generative Adversarial Network.\n\nObtain an image, and create a random noise of the same size as the image.\nPass these images to the Discriminator and obtain the probability of the image passed being real or fake.\nCreate another noise of the same size as before, and pass it to the Generator.\nTrain the Generator with this input data.\nRepeat all the previous steps until the weights are successfully optimized and satisfactory results are obtained.\n\nCoding a GAN\nIn this article, we will create a GAN that can create novel handwritten digits every time it is called. We will take all the concepts we have learnt and, finally, learn how to make a Generative Adversarial Network in Python using the Tensorflow library. Before actually building the network and training pipeline, we need to choose a dataset and set up the optimizers and loss functions.\nAfter the initial set-up is completed, we can train the network and generate our handwritten digits (or any other data).\nImports\nFirst, we import all the required libraries. We will import the plotting library matplotlib and the numerical processing library numpy. In this case, we will import all the required functions from Tensorflow.\nfrom __future__ import print_function, division\n \nfrom keras.datasets import mnist\nfrom keras.layers import Input, Dense, Reshape, Flatten, [[../../Dropout.md|Dropout.md|../../Dropout|Dropout]]\nfrom keras.layers import BatchNormalization, Activation, ZeroPadding2D\nfrom keras.layers import LeakyReLU\nfrom keras.layers.convolutional import UpSampling2D, Conv2D\nfrom keras.models import Sequential, Model\nfrom keras.optimizers import Adam, SGD\n \nimport matplotlib.pyplot as plt\nimport sys\nimport numpy as np\nSetup Configuration\nTo further understand how to make a Generative Adversarial Network, we need to explore our configuration options.\nWe first define the size of the image we want to load and generate. Since we are using the MNIST dataset, we set this to 28x28.\nThe MNIST dataset is grayscale, and this only has one channel.\nWe can set the size of the latent space to 100. If the dataset was more complex, We could choose a higher number.\nnum_rows = 28\nnum_cols = 28\nnum_channels = 1\ninput_shape = (num_rows, num_cols, num_channels)\nz_size = 100\nopt = SGD()\nDataset\nThe dataset we use in this article is the Modified National Institute of Standards and Technology database or MNIST. It is a dataset of handwritten digits almost ubiquitous in deep learning. A sample of this dataset is shown below.\n[IMAGE {5} MNIST START SAMPLE]\n\n[IMAGE {5} FINISH SAMPLE]\nThe MNIST is a very simple dataset for modern networks to model, so it is a good challenge for our vanilla GAN.\nThe MNIST dataset comes pre-installed with Keras, and we can directly use it. We need to pre-process the images by normalizing them and converting them to 3 dimensions to pass them to the network (a Generative Adversarial Network cannot directly process 2D images without changing the architecture). We also create proxy containers for the real and fake images to save memory during training.\n(train_ims, _), (_, _) = mnist.load_data()\ntrain_ims = train_ims / 127.5 - 1.\ntrain_ims = np.expand_dims(train_ims, axis=3)\n \nvalid = np.ones((batch_size, 1))\nfake = np.zeros((batch_size, 1))\nNetworks\nWe now look at the network architecture to understand how to make a Generative Adversarial Network from scratch.\nDiscriminator\nTo create the Discriminator, we define a function that returns a model with the network defined. This function does not compile the model as we need to call it multiple times, and pre-compiling it will lead to issues when we do.\ndef build_discriminator():\n \n\tdisc_model = Sequential()\n\tdisc_model.add(Flatten(input_shape=input_shape))\n\tdisc_model.add(Dense(512))\n\tdisc_model.add(LeakyReLU(alpha=0.2))\n\tdisc_model.add(Dense(256))\n\tdisc_model.add(LeakyReLU(alpha=0.2))\n\tdisc_model.add(Dense(1, activation=&#039;sigmoid&#039;))\n\t\n\tdisc_img = Input(shape=input_shape)\n\tvalidity = disc_model(disc_img)\n\treturn Model(disc_img, validity)\nGenerator\nThe Generator is built similarly to the Discriminator. We define a custom function to create the Generator but do not compile it for now. The noise is also generated and passed through the network here.\ndef build_generator():\n \n    gen_model = Sequential()\n    gen_model.add(Dense(256, input_dim=z_size))\n    gen_model.add(LeakyReLU(alpha=0.2))\n    gen_model.add(BatchNormalization(momentum=0.8))\n    gen_model.add(Dense(512))\n    gen_model.add(LeakyReLU(alpha=0.2))\n    gen_model.add(BatchNormalization(momentum=0.8))\n    gen_model.add(Dense(1024))\n    gen_model.add(LeakyReLU(alpha=0.2))\n    gen_model.add(BatchNormalization(momentum=0.8))\n    gen_model.add(Dense(np.prod(input_shape), activation=&#039;tanh&#039;))\n    gen_model.add(Reshape(input_shape))\n \n    gen_noise = Input(shape=(z_size,))\n    gen_img = gen_model(gen_noise)\n    return Model(gen_noise, gen_img)\nOptimization\nWe define the following functions to set up the optimizers for both networks. We will be using an SGD optimizer in this case for both models.\n# discriminator\ndisc= build_discriminator()\ndisc.compile(loss=&#039;binary_crossentropy&#039;,\n    optimizer=&#039;sgd&#039;,\n    metrics=[&#039;accuracy&#039;])\n \nz = Input(shape=(z_size,))\n# generator\nimg = generator(z)\n \ndisc.trainable = False\n \nvalidity = disc(img)\n \n# combined model\ncombined = Model(z, validity)\ncombined.compile(loss=&#039;binary_crossentropy&#039;, optimizer=&#039;sgd&#039;)\nTraining\nBefore we can train, we need to define a few utility functions.\nThe first function sets up both the Generator and the Discriminator for training. It compiles the combined model and also creates noise.\ndef intialize_model():\n    disc= build_discriminator()\n    disc.compile(loss=&#039;binary_crossentropy&#039;,\n        optimizer=&#039;sgd&#039;,\n        metrics=[&#039;accuracy&#039;])\n \n    generator = build_generator()\n \n    z = Input(shape=(z_size,))\n    img = generator(z)\n \n    disc.trainable = False\n \n    validity = disc(img)\n \n    combined = Model(z, validity)\n    combined.compile(loss=&#039;binary_crossentropy&#039;, optimizer=&#039;sgd&#039;)\n    return disc, generator, combined\nThe entire training loop is then as follows. This loop follows the exact procedure described in previous sections.\nWe also add a running counter that tells us how far along we are in training and saves the outputs every sample_interval epochs.\ndef train(epochs, batch_size=128, sample_interval=50):\n    # load images\t\n    (train_ims, _), (_, _) = mnist.load_data()\n    # preprocess\n    train_ims = train_ims / 127.5 - 1.\n    train_ims = np.expand_dims(train_ims, axis=3)\n \n    valid = np.ones((batch_size, 1))\n    fake = np.zeros((batch_size, 1))\n    # training loop\n    for epoch in range(epochs):\n \n        batch_index = np.random.randint(0, train_ims.shape[0], batch_size)\n        imgs = train_ims[batch_index]\n    # create noise\n        noise = np.random.normal(0, 1, (batch_size, z_size))\n    # predict using Generator\n        gen_imgs = gen.predict(noise)\n    # calculate loss functions\n        real_disc_loss = disc.train_on_batch(imgs, valid)\n        fake_disc_loss = disc.train_on_batch(gen_imgs, fake)\n        disc_loss_total = 0.5 * np.add(real_disc_loss, fake_disc_loss)\n \n        noise = np.random.normal(0, 1, (batch_size, z_size))\n \n        g_loss = full_model.train_on_batch(noise, valid)\n    # show progress\n        print (&quot;%d [D loss: %f, acc.: %.2f%%] [G loss: %f]&quot; % (epoch, disc_loss_total[0], 100*disc_loss_total[1], g_loss))\n    # save outputs every few epochs\n        if epoch % sample_interval == 0:\n            one_batch(epoch)\ndisc, gen, full_model = intialize_model()\ntrain(epochs=10000, batch_size=32, sample_interval=200)\nAfter defining these functions, we train it for as many epochs as we want. For the sake of this article, we can train it for 10,000 epochs. Longer epochs do not necessarily mean better performance.\nTesting\nWe also need a function that samples a batch of data to generate images on demand during/after training.\nThis function creates a random noise vector and uses the trained Generator to perform a prediction on the noise. The generated images are then plotted for a batch of images.\ndef one_batch(epoch):\n    row, col = 5, 5\n    noise = np.random.normal(0, 1, (r * c, z_size))\n    gen_imgs = gen.predict(noise)\n \n    # Rescale images 0 - 1\n    gen_imgs = .5 * gen_imgs + .5\n \n    fig, axs = plt.subplots(r, c)\n    cnt_axis = 0\n    plt.cla()\n    plt.clf()\n    for i in range(row):\n        for j in range(col):\n            axs[i,j].imshow(gen_imgs[cnt_axis, :,:,0], cmap=&#039;gray&#039;)\n            axs[i,j].axis(&#039;off&#039;)\n            cnt_axis += 1\n    fig.savefig(&quot;images/%d.png&quot; % epoch)\n    plt.close()\nIn the training loop, if the number of passed epochs is a multiple of the sample interval (how many epochs to skip before saving the outputs), we call this function and save the images.\nWe can also do this later on.\nif epoch % sample_interval == 0:\n    one_batch(epoch)\nResults\nThe code we wrote saves images at an interval of 200 epochs. For clarity, we can look at the images generated at the start, at 400 epochs, at 5000 epochs and finally after 10,000 epochs.\nAt the start, we have random noise.\n[IMAGE {6} Epoch 0 START SAMPLE]\n\n[IMAGE {6} FINISH SAMPLE]\nAfter 400 epochs, we are getting somewhere slowly. But these results are different from real digits.\n[IMAGE {7} Epoch 400 START SAMPLE]\n\n[IMAGE {7} FINISH SAMPLE]\nAfter 5000 epochs, we can see figures that resemble the MNIST dataset.\n[IMAGE {8} Epoch 5000 START SAMPLE]\n\n[IMAGE {8} FINISH SAMPLE]\nAfter training the network for the entire 10,000 epochs, we get the following outputs.\n[IMAGE {9} Final results START SAMPLE]\n\n[IMAGE {9} FINISH SAMPLE]\nThese images look very close to the handwritten number data we fed the network. Note that none of these exact images was previously shown to the network, and the network generated these as we trained it.\n:::\n:::section{.summary}\nConclusion\n\nThis article taught us how to make a Generative Adversarial Network from scratch.\nWe looked at the architecture of a vanilla GAN and understood the loss functions required to train it.\nWe also made our own Generative Adversarial Network in Python and trained it on MNIST data.\nFinally, we looked at the stepwise results we obtained from training our GAN.\n:::\n"},"Articles/Scalar/Building-the-Word2Vec-Model-in-Gensim":{"title":"Building the Word2Vec Model in Gensim","links":["KB/visualization","KB/Cosine-Distance"],"tags":["article"],"content":"# Building the Word2Vec Model in Gensim\n:::section{.abstract}\nOverview\nWord2Vec is a family of models that can take large corpora and represent them in vector space. These representations, also known as word embeddings, are extremely useful as they help us perform many tasks in NLP. From recommender systems to analysing sentiments from internet feeds to large-scale chatbots, word embeddings have brought life to the field of NLP for decades. Word2Vec, one of the older models, is relatively simple to implement. After implementing it we will use word embedding visualization to further understand how the model works.\n:::\n:::section{.scope}\nScope\nThis article covers the following topics:\n\nWhat are Word2Vec and Gensim.\nHow to train a Word2Vec model using a text corpora.\nParameters that can be tweaked in a Word2Vec model.\nHow to load and pre-process text for Word2Vec.\nHow to perform word embedding visualization in TensorBoard.\n:::\n:::section{.main}\n\nWhat Are We Building?\nIn this article, we are modeling text data by converting a large corpora of text into a Vector model using Word2Vec. We will be using the Python library gensim to do so. We also will build the pipeline needed for word embedding visualization using the Tensorflow Embedding Projector as well as save the trained model for inference.\nProblem Statement\nOur problem statement for this article is to create a pipeline using gensim that uses Word2Vec to process a text corpora, visualize the embeddings, and save the trained model to disk.\nPre-Requisites\nBefore moving to the actual implementation, there are some pre-requisite topics that we need to know. A summary of them is as follows.\nEmbeddings and Word Embedding Visualization\nThe output of a Word2Vec model is a word embedding. There are many other models which give similar embeddings, some better than Word2Vec as well. These are synonymous with word vectors for our use case.\nStopwords\nCommon words like “the”, “to”, etc., do not add much to the model but can negatively influence the embedding.\nCBOW\nA continuous bag of words uses a single hidden layer NN to predict a source word based on its neighbouring words. It uses a sliding window over the sentence to generate these pairs. Consider these examples as (X, Y) pairs to be passed into a model.\nAn example: Given the sentence “I love gensim a lot” and a sliding window of 2, we get ([I, gensim], love), ([love, a], gensim) etc\nSkip-gram\nSkip Grams are a mirror of CBOW. It also uses a similar single hidden layer NN with a sliding window but uses the context words to predict the source word.\nAn example: Given the sentence “I love gensim a lot” and a sliding window of 2, we get (love, [I, gensim]), (gensim, [love, a]) etc\nHow Are We Going To Build This?\nWe are going to load a custom data file as an input to the model and preprocess it to make it fit for the model. After that, we will load the gensim implementation of Word2Vec and train it on the data we loaded. Once the model is done training, we will export the model to disk and load it the trained model using Tensorflow Embedding Projector for word embedding visualization. In the process, we also will save and load the model for further inference.\nFinal Output\nThe final output that we will get from the model is the text corpora that we load converted to vector space. This vector space can then be used to find similar words from the text and be passed to other neural network models if required. An example word embedding might look something like this.\nRequirements\nTo create a Word2Vec model, we need to first understand what it is. We will also be looking at the library Gensim where we obtain the Word2Vec model from.\nWhat Is Word2Vec?\nWord vectors are a numerical representation of text content. Word2Vec is a model that converts large text corpora to a vector representation as  doing so provides the corpora with the following properties :\n\nSince they are converted to numerical vectors, they can be fed into any numerical model, such as a neural network.\nThe converted vectors can be compared by using distance metrics such as the Cosine Distance function cos(\\theta) = \\frac{A\\cdot B}{||A||\\cdot||B||} ( where A and B are vectors). These metrics make it easy to find related words like the one we want to achieve.\n\nWhat is Gensim?\nGensim is a text processing library that lets us train models like Word2Vec very quickly. The library has many more features, such as finding related documents using trained word embeddings and other methods of vectorization that are beyond the scope of this article.\nBuilding Word2Vec Model with Gensim\nNow that we have understood the problem statement, we can start building the model using gensim. We first load all the required packages and data.\nLoading Packages and Data\nBefore we build the Word2Vec Model, we need to load a few packages along with the data. These packages can be installed using pip. (Eg : pip install gensim) if they are not already present on the system that is being used.\nWe also download the stopwords and punctuation data from nltk.\nimport nltk  \nnltk.download(&#039;stopwords&#039;)  \nnltk.download(&#039;punkt&#039;)\nnltk.download(&#039;gutenberg&#039;)\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize\nfrom gensim.models import Word2Vec as w2v\nAfter this, we load the data.\nFor this demo, we will be using text from the book “Emma” by “Jane Austen”. This dataset is a public domain dataset from Project Gutenberg that comes with nltk. We can get it using the following code.\nimport nltk\nnltk.download(&#039;gutenberg&#039;)\n!cp /root/nltk_data/corpora/gutenberg/austen-emma.txt sample.txt\nWe can also use custom text by creating a text file called “sample.txt” in the same directory as the code and pasting whatever we want. (Make sure it is English text).\nData Preprocessing\nAfter loading the data, we need to pre-process it to be able to pass it to the model by removing stopwords and punctuation and converting the words into lowercase tokens. We should get something like this [&#039;emma&#039;, &#039;jane&#039;, &#039;Austen&#039;, &#039;1816&#039;]\nsw = stopwords.words(&#039;English&#039;) # this is a list of stopwords\n# Function to remove stopwords per line if they are present in the line\ndef rem_stops_line(line, words):\n    if len(line) &gt;1:\n        return [w for w in line if w not in words]\n    else:\n        return line\n        \n# Remove stop words for an entire text. Separate functions make it easier to parallelise if required.\ndef remove_stops(text, words = SW):\n    return [rem_stops_line(line, words) for line in text]\n    \n# Open the file and convert it to a list of lines\nwith open(&#039;sample.txt&#039;, &#039;r&#039;) as f:\n    lines = f.readlines()\n \n# Remove new lines, convert all to lowercase, remove punctuation and stop words and tokenise\nlines = [line.rstrip(&#039;\\n&#039;).lower() for line in lines]\nlines = [line.translate(str.maketrans(&#039;&#039;, &#039;&#039;, string.punctuation)) for line in lines]\nlines = [word_tokenize(line) for line in lines]\nfiltered_lines = remove_stops(text = lines, words = SW)\n \nprint(filtered_lines[:10])\nGensim Word2Vec Model Training\nOnce both the data and the model have been loaded, we can train it on the data using the following code.\nw = w2v(\n    filtered_lines,\n    min_count=3,  \n    sg = 1,       # 1 for skip-gram, 0 for cbow\n    window=7   # sliding window size\n)      \nParameters of the Word2Vec Model\nThe Word2Vec model implemented in gensim has a few parameters that we can tune based on the task. They are explained as follows:\n\nsentences : This is the preprocessed input text.\nsize : This is the maximum dimension size of the output vector.\nwindow : This is the sliding window size.\nmin_count : The minimum word frequency below which words would not be passed to the model.\nworkers : This is the number of parallel threads for processing.\nsg : 1 for the Skip Gram model, 0 for the CBOW model.\n\nCBOW is prone to overfitting words that frequently appear in the same contexts, so try SkipGrams as well.\nSkip grams need more data and are more resource intensive but perform better. Choose it based on the task at hand.\n\n\niter : This is the number of iterations the model will update it’s gradients for.\nTweaking these parameters also help improving the accuracy of the word embedding visualization.\n\nCompute Similarities\nThe Word2Vec model can also be used to find similar words in a text.\nWe can find words similar to a random word we pick from the text to see our model works.\nSince we used data from Emma, let us try searching for the word “book”.\nprint(w.wv.most_similar(&#039;book&#039;))\nWe get the following words and how related the model thinks they are to the word “book”.\n[(&#039;peace&#039;, 0.9995625615119934), (&#039;exercise&#039;, 0.999549388885498), (&#039;burst&#039;, 0.9995266199111938), (&#039;purpose&#039;, 0.9995185732841492), (&#039;meet&#039;, 0.9995156526565552), (&#039;mei&#039;, 0.9995115995407104), (&#039;move&#039;, 0.9995064735412598), (&#039;week&#039;, 0.9995056986808777), (&#039;views&#039;, 0.9995036125183105), (&#039;persons&#039;, 0.9995019435882568)]\nT-SNE Visualizations\nTo see what the embedding space looks like, we can give these embeddings to Tensorboard.\nTo do so, make sure the model is saved. Here we have saved it as “wvecemma”. Now we use a script that comes inbuilt with gensim to convert our Word2Vec model to the Tensorboard format.\npython -m gensim.scripts.word2vec2tensor -i &quot;wvecemma&quot; -o &quot;model&quot;\nWe get two files, “model_tensor.tsv” and “model_metadata.tsv”.\nTo convert the model to a Keras Embedding, use the code (for model w), w.wv.get_keras_embedding()\nNow we can either open Tensorboard (if you have it installed) and navigate to this folder and open the generated files, or go to this website Tensorflow Projector.\nOn the website, click the Load button. For Step 1, choose the “model_tensor.tsv” file, and for Step 2, choose the other one.\nWe can then see the embeddings directly.\n[IMAGE {1} {Embedding Projector} START SAMPLE]\n\n[IMAGE {1} FINISH SAMPLE]\nSaving and Loading the Model\nTo prevent having to train again, we save the model to disk using the following code.\nw.wv.save_word2vec_format(&quot;wvecemma&quot;) # save to disk \nThis model can then be loaded for inference during production using the following code.\nmodel = gensim.models.KeyedVectors.load_word2vec_format(&quot;wvecemma.bin.gz&quot;, binary=True) # load from disk\n:::\n:::section{.summary}\nConclusion\n\nThis article taught us how to implement the Word2Vec model in gensim.\nWe learnt how to perform word embedding visualization using the TensorBoard Embedding Projector.\nWe also learnt how to use our own data to train a Word2Vec model.\n\n:::"},"Articles/Scalar/CycleGAN-scalar":{"title":"CycleGAN","links":["tags/scalar","KB/Basic-GAN","KB/Downsampling"],"tags":["article","scalar"],"content":"Translating Images Using a CycleGANscalar\n:::section{.abstract}\nOverview\nThe field of computer vision has been trying to create AI that creates never seen before images for decades. Generative networks such as the CycleGAN are part of a long line of such research, but one that performs extremely well in tasks ranging from converting images to paintings to changing the weather in images. The CycleGAN is rather different from many approaches before it as it is an unpaired Image2Image translation task with these tasks being cyclic in nature. In this article, we will explore what all these terms mean and how to put them into practise in a CycleGAN.\nDisclaimer\nThis is an intermediate level article and introduces a significant number of new terms. Attempting to understand this article is not recommended before mastering how a Basic GAN (eg: DCGAN) works.\nBeing so complex, it is advised to slow down and understand a section before moving on to the next.\n:::\n:::section{.scope}\nScope of the Article\nThis article covers the following concepts\n\nWhat is an unpaired Image2Image task\nWhat is GAN and subsequently, what is a CycleGAN\nConcepts such as Generators, Discriminator, Encoder Decoder architectures\nLatent space as an easier means of understanding CycleGANs\nArchitectural Details and Training procedure of CycleGANs\nHow to use CycleGANs in various domains\n:::\n:::section{.main}\n\nNote About The Code\nThis article does not contain the code to run a CycleGAN. This was done to prevent the article from becoming extremely huge. Instead, every concept that would be required to understand the code is explained in detail below.\nThe entire code with examples can be found on the official Tensorflow/Keras documentation website. On understanding this article, this code will become extremely easy to comprehend and use.\nQuick Recap\nThis section gives a recap of the important concepts we need to understand CycleGAN.\nImage2Image Translation\nAn image translation task’s objective is to convert from one image to either text or another image. An example of the same would be taking a picture of a sunset and converting that picture to the style of the artist Van Gogh. There are many such Image translation tasks, but the one in consideration in this article is specifically converting from an image to another image aka Image2Image.\nPaired vs Unpaired Translation\nAn important detail about the CycleGAN is that unlike most other GANs or UNet style architectures, it uses an unpaired translation. This means that the image, label pairs that are being passed into the model are not related to each other. In contrary to say, a classification task where the label passed is the exact label of the image passed. This might be slightly confusing to understand without knowledge of latent space. More explanations can be found in the next section\nGAN\nGANs are a special class of architectures that have two components, one that tries to get better at a task, and the other that tries to find out how badly the first part is performing. The training procedure is somewhat similar to a game with one component being an “adversary” of the other, hence the term “Adversarial network”. The generative term comes into play as these networks are used to create novel images from existing data.\nAdversarial Training\nA useful analogy to understand how a GAN is trained is the classic “cop” vs “thief” analogy. Assume that the thief in this case is trying to forge a painting by Van Gogh, while the cop tries to prove the thief wrong. The thief first comes up with a forgery, and the cop says no, this is fake and is not the real one because of some reason. The thief then shows a modified image to the cop with minor differences. This scenario repeats until the cop can no longer tell the difference between the real and the fake images.\nThis type of training is called adversarial training. Using a second opinion to improve the outputs of the first component of the model. A useful advantage here is that the data does not need to be labelled.\nGenerator\nThe generator is the “thief”. It starts with random noise to create a fake image and traverses the latent space until the Discriminator can no longer tell the difference between the fake and the real images.\nDiscriminator\nConsequently, the discriminator is the “cop”. It is essentially a classifier that returns a metric of how fake the image looks.\nEncoder Decoder Architecture\nMany networks such as the UNet and GANs have a two sided architecture that involves Downsampling the image until a point and then upsampling from there on. The Encoder is the first half which downsamples the image and condenses the information in a batch of data down into the smallest possible unit.\nThe decoder does the opposite, it takes this smallest possible unit and attempts to recreate the original input. In the process, it learns how to traverse the latent space and create the required translation.\nResidual Block\nThe Encoder Decoder architecture has a major drawback, in the process of compressing the image information and reconstructing it, a lot of information is lost during the operations. The Residual Block is the answer to this dilemma. It essentially contains a “Skip Connection” that ensures that the gradients from the previous layer are carried over to the next layer.\nThis is done very simply. If F(x) is the network, then for an input x_{i}, x_i = F(x_{i-1}) + x_{i-1}.\nCost Function\nA cost function is the objective that the network tries to minimise. In essence, it is a metric of how well the network performs with respect to a task. For image classification, it could be CrossEntropy. CycleGAN has a rather complex cost function that is explained in later sections.\nLatent Space\nUnderstanding latent space is the key to fully comprehending the CycleGAN. It can be thought of as an n-dimensional vector space that contains every possible image that can be generated from the given data. It is not possible to entirely visualise this directly but a useful analogy is considering faces.\nIf we consider the faces of every person that we know and “average” them together, we would end up with a “generic face” that has traits from every face that we considered. From this “generic face”, if we were to attempt to recreate any of the other faces, we would have to “add” or “subtract” some features from the face to reach the other one.\nThis can be thought of as “traversing” the latent space. Thus in essence, the latent space contains all such possible faces. In theory then, it makes sense to assume that if we can approximate this latent space, we can translate any image to the other by traversing it. The CycleGAN attempts to do just that. Traversal can go in both directions, hence it’s “cyclic” nature.\nCycleGAN Architecture\nThe CycleGAN architecture is divided into two major parts - The Generator and the Discriminator. The Generator further has the Encoder, the Transformer and the Decoder as components.\nEncoder Block\nThe encoder uses convolution layers to consolidate information from the data and compress it into the least possible representational unit. The number of channels consequently increase. In the current model, there are 3 convolution operations. The final output of the model reduces the original image size by 3/4th and passes it to the Transformer Block.\n“Transformer” Block\nThe Transformer Block has nothing to do with “Transformer architectures” but is called so because it takes the output of the encoder and transformers it so the decoder can use it. In the CycleGAN, this block has around 6-9 Residual blocks. This is done to ensure maximum information extraction from the compressed representation that the encoder generates.\nDecoder Block\nThe decoder block then takes the inputs from the transformer block and passes it through two de-convolution layers.\nDiscriminator\nThe Discriminator in the CycleGAN is another type of GAN - “PatchGAN”. This special type of Discriminator uses patches of the input image to map to outputs. Unlike a normal GAN that maps from a 256x256 sized image to a scalar result (“real” or “fake), the PatchGAN maps it to NxN sized arrays of outputs X where each X_{i,j} maps to (“real”, “fake”).\nThis is run as a convolution through the entire image and the results are averaged out.\nLoss Functions\nCycleGAN uses a mix of three loss functions. Along with adversarial loss, a cycle consistency loss and an identity loss are used to create the final objective function.\nCycle Consistency Loss\nThis is the most important part of the CycleGAN research. Considering two image domains X and Y, the Cycle consistency loss uses two different mappings G: X \\rightarrow Y and F : Y \\rightarrow X. These mappings are bijections, aka reverses of each other. (Hence “cyclic”). As a mathematical expression, the Cycle Consistency Loss \\mathcal{L}_{cyc} can then be represented as \\mathcal{L}_{cyc}(G, F, X, Y) = \\frac{1}{m}\\Sigma_{i=1}^{m}[(F(G(x_{i})-x_{i})+ (G(F(y_{i}))-y_{i})].\nIdentity Loss\nAnother loss function that CycleGAN proposes is the Identity loss. L_{identity}(G,F) = \\mathbb{E}_{y \\sim p_{data}(y)}[||G(y)-y)||_{1}] + \\mathbb{E}_{x \\sim p_{data}(x)}[||F(x)-x)||_{1}]\nThis is especially useful for converting to and from photos and paintings. The loss is used to preserve the color information during transformation and attempts to make sure that the reverse color is not used. If a part of the image looks like it belongs to the target image already, it is not mapped to something else. In principle, it makes the model more conservative if the content to be transformed is not known.\nObjective/Cost Function\nThe net loss function is therefore a combination of all the three losses just described. A hyper parameter \\lambda is used to control the strength of the transfer. It is usually set to 10.\n\\mathcal{L}_{GAN}(G, F, D_{X}, D_{Y}) = \\mathcal{L}_{GAN}(G, D_{Y}, X, Y) + \\mathcal{L}_{GAN}(F, D_{X}, X, Y) + \\lambda \\mathcal{L}_{cyc}(G,F)\nThus the function that we wish to solve is finding the value of the optimisers that gives the best loss while maintaining the ability to convert to and from the target image. This can be seen in the formula. G^{*},F^{*} =\\underset{G,F}{argmin} \\underset{D_{X}, D_{Y}}{max} \\mathcal{L}_{GAN}(G, F, D_{X}, D_{Y})\nOther Useful Details About The Architecture\nThe CycleGAN paper and code have a lot of interesting tweaks that were done to improve performance. Some of the ones that are not usually mentioned are as follows.\nSeparate Optimizers\nThere are two optimisers for the discriminator and the generator each. (Four in total.) Using two separate optimisers ensure that the model learns to convert images in both directions and minimises the loss for both sets of generators and discriminators.\nInstance Normalisation\nInstance normalisation is a regularisation technique that allows the network to remove specific contrast information when transferring style information between images. This technique makes CycleGANs extremely useful for image stylisation tasks.\nThe formula is similar to that of Batch Normalisation and is left in this article as a reference.\n    \\quad\n    \\mu_{ti} = \\frac{1}{HW}\\sum_{l=1}^W \\sum_{m=1}^H x_{tilm},\n    \\quad\n    \\sigma_{ti}^2 = \\frac{1}{HW}\\sum_{l=1}^W \\sum_{m=1}^H (x_{tilm} - mu_{ti})^2$$\n\n### Fractional Stride\nMost networks use a stride that is a whole number. CycleGAN has two types of convolutions. One type that has a stride of 2, while the other that has a stride of $\\frac{1}{2}$. This is a special case of convolution that is also known as a “de-convolution”. A fractional stride upsamples an image from a smaller dimension to a larger one. A whole number stride downsamples it.\n\n### Reflection Padding\nWhen performing convolutions, the sliding window does not always fit the size of the image to be convolved, padding is used in those cases to make up for the missing pixels. In CycleGAN, a reflection padding is used which just means that the missing pixels are filled in with its neighbouring pixels before being convolved instead of being filled with a “black” 0 pixel. This helps to preserve some more content information.\n\n## Photos To Paintings\nTo convert photos to paintings, the procedure remains exactly the same as before. The only difference is the dataset. As long as the dataset contains images in different folders with different painting style information alongside a folder of plain photos, CycleGAN can convert between them.\nThe original paper has many such results. Some of which are shown below.\n\n## More Use Cases\nJust like converting photos to paintings, a similar thought process can be applied to find other uses cases. The ones that the paper mention are as follows :\n- Style Transfer : Same as photos to and from paintings except with other types of imagery than just paintings.\n- Object Transformation : Convert to and from objects within ImageNet classes by traversing the latent space. Eg: Converting apples to oranges, zebras to horses etc.\n- Season Transfer : Converting images taken in Winter to summer and vice versa.\n\n## Limitations\nEvery method has its limitations. CycleGAN performs poorly when given geometrical transformations. This is because it is trained to change appearances but it does not penalise changes in geometry. \n:::\n:::section{.summary}\n\n## Conclusion\nIn this article, we learnt about CycleGAN and all the architectural details required to create it. We explored the concept of a latent space and understood how a GAN works by traversing it. We also looked at many applications of a CycleGAN and how to train one on our own data.\n:::"},"Articles/Scalar/DCGAN-–-Adding-convolution-to-a-GAN":{"title":"DCGAN – Adding convolution to a GAN","links":["KB/CIFAR","KB/Strided","KB/Downsampling","KB/Initialization"],"tags":["article"],"content":"DCGAN – Adding convolution to a GAN\n:::section{.abstract}\nOverview\nGenerative networks are a fascinating subfield of Computer vision. The GAN, in particular, is a training paradigm and a family of network architectures that convert a simple convolutional network to generate novel images based on an image dataset. This training is generally unpaired and does not require any labels. The original GAN architecture was unstable and had issues returning random noise as an output. The DCGAN was proposed as an alternative architecture with many tweaks over the original to counter issues such as mode collapse, diminished gradients, and non-convergence.\n:::\n:::section{.scope}\nScope\n\nThis article explains the concept of GANs and how DCGANs differ from Vanilla GANs.\nIt shows how to build a DCGAN from scratch using PyTorch for image generation using the CIFAR dataset.\nIt explains the preprocessing and loading of the CIFAR dataset using a DataLoader.\nIt describes the architecture of the DCGAN and the reasoning behind the choices of layers and activation functions.\nThe article also describes how to train the network, generate new images, and improve the training time and the results.\n\n:::\n:::section{.main}\nIntroduction to DGGAN\nThis article will explore using a Deep Convolutional Generative Adversarial Network (DCGAN) to generate new images from the CIFAR dataset. GANs are neural networks designed to generate new, previously unseen data similar to the input data the model trained on. DCGANs are a variation of GANs that address issues that can arise with standard GANs by using deep convolutional neural networks in both the Generator and the Discriminator.\nThis architecture allows larger image sizes than in standard GANs, as convolutional layers can efficiently process images with many pixels. Additionally, DCGANs use batch normalization and leaky ReLU activations in the Discriminator and transposed convolutional layers in the Generator, improving performance and stability during training.\nWe will use PyTorch to build the DCGAN from scratch, train it on the CIFAR dataset. Before we begin, we will set up the necessary libraries and create folders to store the models’ images and weights. This article will guide the implementation process and explain the reasoning for some architectural choices.\n:::\n:::section{.main}\nNeed for DCGAN\n\nA simple convolutional GAN needs to be more stable to generate images with a high resolution and suffers from mode collapse.\nDCGAN, on the other hand, has an architecture that uses not just convolutions but also transposed convolutions and other improvements.\nThese changes help the network learn better and generate images more stably compared to other architectures that came before it.\nThe DCGAN research was a monumental step for GANs as it was one of the earliest stable unsupervised image generators.\nUnderstanding how it works is the gateway to creating more advanced GANs.\n\n:::section{.main}\nPre-requisites\nTo understand the DCGAN Architecture, we need to know some pre-requisite concepts. Since the entire architecture is made up of blocks of the same components, knowing them is helpful.\nTransposed Convolutions/De-Convolution\nA De-Convolution is an upsampling method that uses transforms opposite to a normal convolution operation. It maintains the input’s shape and pattern that a standard convolution would possess.\n[IMAGE {1} { Transposed Convolution } START SAMPLE]\n\n[IMAGE {1} FINISH SAMPLE]\nStrided Convolutions\nThe stride in a Convolution determines how many steps the moving filter skips over in an image. In a general Convolution, the stride is set to 1. To perform Downsampling, we can set the stride to any number above 1. Larger numbers are only sometimes good; only experimenting with the parameter can be used to understand which to pick.\n[IMAGE {2} {Strided Convolution} START SAMPLE]\n\n[IMAGE {2} FINISH SAMPLE]\nLeaky ReLU\nThe Leaky ReLU activation is a small modification over the ReLU that is useful for networks with sparse gradients like the DCGAN. Due to the GAN architecture and training methodology, some nodes that use ReLU tend to die out and do nothing. The Leaky ReLU accounts for negative values by having a smaller slope instead of going straight to zero.\nThe change is quite minor but makes a huge difference. While the ReLU is defined as max(0, x), the Leaky ReLU is max(0,01x, x)\n[IMAGE {3} {Leaky ReLU} START SAMPLE]\n\n[IMAGE {3} FINISH SAMPLE]\n:::\n:::section{.main}\nArchitecture\nThe DCGAN architecture follows a similar pattern to many GAN architectures, with a Generator and a Discriminator to process inputs.\n[IMAGE {4} {Architecture} START SAMPLE]\n\n[IMAGE {4} FINISH SAMPLE]\nThe general flow of input looks something like the following. For every iteration, randomly generated noise is passed to the Generator. The Discriminator gets a random image sampled from the dataset. The Generator uses the learned weights to modify the noise closer to the target image. The Generator then passes this modified image to the Discriminator, which predicts how real the image looks and returns a probability of the same. The loss from both parts is combined to minimize the loss functions for the Generator and the Discriminator using back-propagation.\nAn important point to note for both parts is that the weights are initialized differently for the Convolutional and Batch Normalization layers. If the layer is Convolutional, the weights are from a random normal distribution with a standard deviation of 0.02 and a mean of 0. If the layer is a Batch Normalization layer, a standard deviation of 0.02 means of 1.0 with a bias of 0 is used.\nDeconvolutional Generator\n[IMAGE {5} {Generator} START SAMPLE]\n\n[IMAGE {5} FINISH SAMPLE]\nThe Generator maps the input from its latent space to the vector data space. This part of the network outputs an RGB image the same size as the training image (3x64x64).\nThe Generator comprises blocks of Transposed Convolutions, Batch Normalizations, and ReLU layers. The output is passed through a Tanh activation that maps it to a range of [-1,1].\nThe DCGAN authors also found that using a Batch Normalization layer after a Transposed Convolution led to the best results by aiding the gradient flow between the layers. This effect was previously never studied in depth.\nIn the architecture diagram of this component, nz stands for the width of the input, ngf stands for the shape of the maps that the network creates, and nc refers to a count of the channels that the output will have (Eg : 3 channels for RGB, 4 for RGBA).\nConvolutional Discriminator\nThe Discriminator is a mirror of the Generator except for a few changes. The input size remains the same as the Generator (3x64x64). Instead of a De-Convolution, a Strided Convolution is used. A Leaky ReLU version of ReLU replaces the ReLU activations. The final layer is a Sigmoid layer to return the probability of real vs. fake.\nThe DCGAN architecture also uses Strided Convolutions to downsample the images instead of Pooling, allowing the network to learn a custom pooling function.\nImplementation\nTo generate images using a DCGAN, we first need to prepare our dataset. This process includes creating a DataLoader to load the images, preprocessing them as necessary, and sending batches of the data to the GPU memory for efficient processing.\nNext, we need to define the architecture of the DCGAN, including the Generator and Discriminator networks. This process involves specifying the number and type of layers and initializing the weights of these layers. We must also send the network architecture to the GPU memory for efficient processing.\nOnce the data and network are ready, we can train the DCGAN. During training, the network learns to map random noise from the latent space to images that resemble the training data. After training, we can use the Generator to generate new images by providing random noise from the latent space.\nDefining the Discriminator\nIn the DCGAN, the Discriminator differentiates between the images generated by the Generator as real or fake. Its architecture resembles the Generator but with a few modifications. Specifically, the Discriminator incorporates Strided Convolution layers, a LeakyReLU activation function, and several layers of Batch Normalization. Lastly, the output is passed through a Sigmoid layer that returns a probability value.\nFor the process of DCGAN image generation, the Discriminator uses Strided Convolutions in place of Pooling layers. This approach enables the network to develop custom padding functions, improving performance. This approach is a key technique that helps the Discriminator to distinguish between real and fake images more accurately.\nngpu = 1\nnz = 100\nngf = 64\nndf = 64\n \nclass Disc_model(nn.Module):\n    def __init__(self, ngpu):\n        super(Disc_model, self).__init__()\n        self.ngpu = ngpu\n        self.main = nn.Sequential(\n            nn.Conv2d(num_channels, ndf, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n            nn.Sigmoid()\n        )\n \n    def forward(self, input):\n        if input.is_cuda and self.ngpu &gt; 1:\n            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n        else:\n            output = self.main(input)\n \n        return output.view(-1, 1).squeeze(1)\nDefining the Generator\nThe Generator in a DCGAN is responsible for taking a random vector from the latent space and mapping it to an image in the vector data space. This mapping uses a series of transposed convolutional layers, batch normalization layers, and ReLU activation layers. Using batch normalization after the transposed convolutional layers helps improve the gradient flow through the network, resulting in better performance and stability during the training process. The final layer of the Generator uses a Tanh activation function to ensure that the output image is in the range of [-1, 1], which is the expected range for image data.\nclass Gen_model(nn.Module):\n    def __init__(self, ngpu):\n        super(Gen_model, self).__init__()\n        self.ngpu = ngpu\n        self.main = nn.Sequential(\n            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf * 8),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf, num_channels, 4, 2, 1, bias=False),\n            nn.Tanh()\n        )\n \n    def forward(self, input):\n        if input.is_cuda and self.ngpu &gt; 1:\n            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n        else:\n            output = self.main(input)\n            return output\nDefining the inputs\nThe CIFAR10 dataset is utilized in this article provided by the Canadian Institute for Advanced Research. This dataset consists of ten classes of images that are similar to the MNIST format but with 3-channel RGB. The CIFAR10 dataset is widely used for benchmarking image classification models and is an easily learned dataset.\nBefore using the dataset, it must be loaded and preprocessed. PyTorch has an inbuilt CIFAR10 dataset implementation that we can load directly. If the dataset is being used for the first time, it must be downloaded. Once the dataset is loaded, images are resized to a common size of 64x64x3. Although CIFAR10 is a clean dataset, this resizing step is still important to standardize the images. Finally, the images are normalized and converted to PyTorch tensors.\nA DataLoader is then created, a class that creates optimized batches of data to pass to the model. If available, this DataLoader is sent to the GPU to accelerate the DCGAN image generation process.\ndataset = tv_data.CIFAR10(root=&quot;./data&quot;, download=True,\n                           transform=transforms.Compose([\n                               transforms.Resize(64),\n                               transforms.ToTensor(),\n                               transforms. Normalize ((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n                           ]))\nnum_channels=3\n \ndataloader = torch.utils.data.DataLoader(dataset, batch_size=128,\n                                         shuffle=True, num_workers=2)\ncurrent_device = &#039;cuda&#039; if torch.cuda.is_available() else &#039;cpu&#039;\nStarting the DCGAN\nTo streamline the workflow, some empty containers are set up at the beginning of the process. A fixed noise of shape (128, size of latent space, 1, 1) is created and transferred to the GPU memory. The labels for real images are also set as one and for fake images as 0. The network will run for 25 epochs in this example.\nFor tracking progress and analyzing performance, arrays are created to store the Generator and Discriminator loss during training.\nfixed_noise = torch.randn(128, nz, 1, 1).to(current_device)\nreal_label = 1\nfake_label = 0\n \nniter = 25\ng_loss = []\nd_loss = []\nComputing the loss function\nThe DCGAN image generation process involves two loss functions, one for the Generator and another for the Discriminator.\nThe Discriminator loss function penalizes the model for incorrectly classifying a real image as fake or a fake image as real. This loss can be thought of as maximizing the following function:\n\\nabla_{\\theta_{d}} \\frac{1}{m} \\Sigma_{i=1}^{m}[log D(x^{(i)}) + log(1-D(G(z^{(i)})))]\nThe Generator loss function considers the Discriminator’s output, rewarding the Generator if it can fool the Discriminator into thinking the fake image is real. If this condition is not met, the Generator is penalized. This loss can be thought of as minimizing the following function:\n\\nabla_{\\theta_{g}} \\frac{1}{m} \\Sigma_{i=1}^{m}log(1-D(G(z^{(i)})))\nIn summary, the Discriminator’s role is to maximize its loss function, and Generator’s role is to minimize its loss function, which results in Generator creating an image similar to real images. These fake images should be identified as real by the Discriminator.\nmodel_Gen = Gen_model(ngpu).to(current_device)\nmodel_Gen.apply(weights_normal_init)\nmodel_Disc = Disc_model(ngpu).to(current_device)\nmodel_Disc.apply(weights_normal_init)\nloss_func = nn.BCELoss()\nOptimizing the loss\nIn this implementation, for DCGAN image generation, the ADAM optimizer is used with a learning rate of 0.0002, and the beta parameters are set to (0.5, 0.999) to minimize the loss function. Different optimizers are used for each of them to ensure that the Generator and Discriminator learn independently.\noptimizerD = optim.Adam(model_Disc.parameters(), lr=0.0002, betas=(0.5, 0.999))\noptimizerG = optim.Adam(model_Gen.parameters(), lr=0.0002, betas=(0.5, 0.999))\nTrain the DCGAN\nThe DCGAN image generation process involves training the network before generating new images. The procedure is done in the following steps.\n\nFor each epoch, random noise is sent as an input to the Generator.\nThe Discriminator also receives a random image sampled from the dataset.\nThe Generator then uses its learned weights to transform the noise to be more similar to the target image. These weights allow the Generator to learn the mapping between random noise and the latent space of the image dataset.\nThe Generator sends the modified image to the Discriminator.\nThe Discriminator evaluates the realism of the generated image and communicates it to the Generator through a probability metric.\nThis process of the Generator creating new images and the Discriminator evaluating it continues until the desired number of epochs.\nOnce the training is completed, the Generator can generate new images by inputting random noise.\n\nfor epoch in tqdm(range(niter), total = niter):\n    for i, data in enumerate(dataloader, 0):\n        model_Disc.zero_grad()\n        device_model = data[0].to(current_device)\n        batch_size = device_model.size(0)\n        label = torch.full((batch_size,), real_label).to(current_device)\n \n        output = model_Disc(device_model) # Discriminator output\n        disc_error_real = loss_func(output.float(), label.float()) \n        disc_error_real.backward() # disc loss for real image\n        D_x = output.mean().item()\n \n        noise = torch.randn(batch_size, nz, 1, 1).to(current_device) # create noise\n        fake = model_Gen(noise) # Fake image\n        label.fill_(fake_label) # Fill with 0\n        output = model_Disc(fake.detach())\n        disc_error_fake = loss_func(output.float(), label.float()) # disc loss for fake image\n        disc_error_fake.backward() \n        D_G_z1 = output.mean().item()\n        disc_error = disc_error_real + disc_error_fake\n        optimizerD.step()\n \n        model_Gen.zero_grad()\n        label.fill_(real_label) # fill with 1\n        output = model_Disc(fake.float()) # disc output\n        gen_error = loss_func(output.float(), label.float())\n        gen_error.backward()\n        D_G_z2 = output.mean().item()\n        optimizerG.step()\n \n        print(&#039;[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f&#039; % (epoch, niter, i, len(dataloader), disc_error.item(), gen_error.item(), D_x, D_G_z1, D_G_z2))\n        \n        if i % 100 == 0: # save images every 100 steps\n            print(&#039;saving the output&#039;)\n            vutils.save_image(device_model,&#039;./images/real_samples.png&#039;,normalize=True)\n            fake = model_Gen(fixed_noise)\n            vutils.save_image(fake.detach(),&#039;./images/fake_samples_epoch_%03d.png&#039; % (epoch),normalize=True)\n    \n    torch.save(model_Gen.state_dict(), &#039;weights/model_Gen_epoch_%d.pth&#039; % (epoch))\n    torch.save(model_Disc.state_dict(), &#039;weights/model_Disc_epoch_%d.pth&#039; % (epoch))\nThis article trains the network for 25 epochs. To get a better understanding of the progression of the training, we compare the original sample to the outputs generated at the 0th, 10th, and 25th epochs. As the training progresses, the ./images folder is periodically checked every 100 steps to observe the output. After the training is completed, the final results are as follows.\n[IMAGE {5} Original Sample/Target START SAMPLE]\n\n[IMAGE {5} FINISH SAMPLE]\n[IMAGE {6} Epoch 0 START SAMPLE]\n\n[IMAGE {6} FINISH SAMPLE]\n[IMAGE {7} Epoch 10 START SAMPLE]\n\n[IMAGE {7} FINISH SAMPLE]\n[IMAGE {8} Final Results START SAMPLE]\n\n[IMAGE {8} FINISH SAMPLE]\nWeight Initialization\nThe DCGAN model requires a careful weight Initialization scheme. If the layer is a Convolutional layer, we can take the Initialization values from a Normal distribution in the range of (0.0,0.02). On the other hand, if the layer is a Batch Normalization layer, we can take the weights from a Normal distribution in the range of (0.0, 0.02) while we can set the bias to 0.\ndef weights_normal_init(m):\n    classname = m.__class__.__name__\n    if classname.find(&#039;Conv&#039;) != -1:\n        m.weight.data.normal_(0.0, 0.02)\n    elif classname.find(&#039;BatchNorm&#039;) != -1:\n        m.weight.data.normal_(0.0, 0.02)\n        m.bias.data.fill_(0)\n:::\n:::section{.summary}\nConclusion\n\nThe article has explained the concept of GANs and the specific architecture of DCGANs, which are a variation that can handle larger images.\nIt has also provided a step-by-step guide on how to build a DCGAN from scratch using the PyTorch library and the CIFAR dataset.\nThe implementation process, including loading the dataset and preprocessing it, creating the network architecture and Initialization of weights, as well as training the network, has been explained.\nThe final output is expected to be a set of photorealistic images that resemble one of the classes in the CIFAR dataset, which is a significant achievement.\nGANs, particularly DCGANs, have a wide range of applications and can generate images of different objects, depending on the dataset used to train the network. This article provides a foundation for further research and experimentation with GANs.\n\n:::"},"Articles/Scalar/Differences-between-Discriminative-and-Generative-models":{"title":"Differences between Discriminative and Generative models","links":["KB/Trees","KB/Decision-Boundaries","KB/LDA"],"tags":["article"],"content":"Differences between Discriminative and Generative models\n:::section{.abstract}\nOverview\nMachine learning models can be broadly classified into discriminative and generative. Discriminative models, such as logistic regression, support vector machines, and decision Trees, while discriminative models are more commonly used for classification and regression.\n:::\n:::section{.scope}\nScope\n\nThe article introduces the concept of discriminative and generative models in the context of machine learning.\nThe article explains the differences between these two models and how they approach tasks differently.\nThe article provides examples of commonly used machine learning models in the discriminative or generative category.\nThe article discusses the applications of discriminative and generative models in various tasks, such as classification, regression, and unsupervised learning.\nThe article also compares the advantages and disadvantages of discriminative and generative models.\n\n:::\n:::section{.main}\nIntroduction\nVarious Machine Learning models have been proposed over the years, each for different tasks. A broad categorization of these models is to classify them into Generative and Discriminative models. Discriminative models estimate the conditional probability, while Generative models estimate the joint probability distribution. This article will examine the difference between Generative and Discriminative models. We will also introduce several commonly used ML models and categorize them into these two groups.\nDiscriminative Model\nDiscriminative Models are a family of models that do not generate new data points but learn to model boundaries around classes in a dataset instead. These models aim to maximize the separation between the classes in the dataset to perform classification or Regression. Discriminative models estimate the conditional probability P(Y|X =x) on a data X and a target Y.\nNote that sometimes classifiers that do not use a probability model are called Discriminative Models.\nLogistic Regression\nLogistic Regression is a classification algorithm that uses the Sigmoid function instead of a linear function to model data.\nThe Sigmoid curve is shown in the figure below.\n[IMAGE {1} { Sigmoid Curve } START SAMPLE]\n\n[IMAGE {1} FINISH SAMPLE]\nWe can also use Logistic Regression for multi-class tasks by modelling each class separately. Therefore, the Regression’s outcome must be a discrete or categorical value. (e.g., Yes/No, True/False) The model’s output is a probabilistic value in the range [0,1]. The modelled curve that the logistic function uses indicates the likelihood of the binary decision.\nThe following equation can mathematically represent Logistic Regression.\nlog[\\frac{y}{1-y}] = b_0 + b_1 x_1 + b_2 x_2 + … + b_n x_n\nConsidering the difference between Generative and Discriminative models is important in understanding why these models are Discriminative.\nSupport Vector Machine\nA Support Vector Machine (SVM) is a supervised classification and regression algorithm that uses the concept of hyperplanes. These hyperplanes can be understood as multi-dimensional linear Decision Boundaries that separate groups of unequal data points.\nAn example of a hyperplane is shown below.\n[IMAGE {2 } { SVM } START SAMPLE]\n\n[IMAGE { 2} FINISH SAMPLE]\nAn optimal fit of the SVM occurs when a hyperplane is furthest from the training data points of any of the classes—the larger this distance margin, the lower the error of the classifier.\nTo better understand how the SVM works, consider a group of data points like the one shown in the diagram. It is a good fit if the hyperplane separates the points in the space so they are clustered according to their labels. If not, further iterations of the algorithm are performed.\nDecision Tree\nDecision Trees are tree-based decision models that use an internal structure of a root node followed by successive child leaf nodes. The leaf nodes are a placeholder for the classification label, and the branches show the outcomes of the decision. The paths from the tree’s root to the leaves represent the classifier rules. Each tree and sub-tree models a single decision and enumerates all the possible decisions to choose the best one.\nA Decision tree can be optimal if it represents most of the data with the least number of levels. Decision Trees are computationally efficient, and many tree-based optimizations have been created over the years to make them perform even faster.\nAn example of such a tree is shown below.\n[IMAGE {3 } { Decision Tree } START SAMPLE]\n\n[IMAGE { 3} FINISH SAMPLE]\nRandom Forest\n[IMAGE {4 } { Random Forest } START SAMPLE]\n\n[IMAGE { 4} FINISH SAMPLE]\nRandom Forest models use a forest of Decision Trees for a task is the best after the aggregation. This technique of aggregating multiple results from similar processes is called Ensembling.\nThe second component of the Random Forest pertains to another technique called Bagging. Bagging differs from Ensembling because, in Bagging, the data is different for every model, while in Ensembling, the different models are run on the same data. In Bagging, a random sample with replacement is chosen multiple times to create a data sample. These data samples are then used to train the model independently. After training all these models, the majority vote is taken to find a better estimate of the data.\nRandom forests combine the concepts of Bagging and Ensembling to decide the best feature splits and select subsets of the same. This algorithm is better than using a single Decision Tree as it reduces bias and the net variance, generating better predictions.\nBagging and Ensembling might seem like they help model the joint probability distribution, but that is not the case. Understanding the difference between Generative and Discriminative models can clear this confusion.\nGenerative Models\nGenerative Models are a family of models that create new data points. They are generally used for unsupervised tasks. Generative Models use the joint probability distribution P(X, Y) on a variable X and a target variable Y to model the data and perform inference by estimating the probability of the new data point belonging to any given class.\nLatent Dirichlet Allocation (LDA)\nLDA maximizes the class separation and not the variance of the data.\nThis principle is illustrated in the figure below.\n[IMAGE {5 } { LDA } START SAMPLE]\n\n[IMAGE { 5} FINISH SAMPLE]\nBayesian Network\nA Bayesian Network is a graph-based probabilistic model that uses a special graph structure known as a DAG - Directed Acyclic Graph to model conditional dependencies between the given variables. These networks are useful in finding the possible cause of an event, given several contributing factors.\nA classic example of what a Bayesian Network looks like is shown below.\n[IMAGE {6 } { Bayesian Network } START SAMPLE]\n\n[IMAGE { 6} FINISH SAMPLE]\nThe Bayesian Network uses this graph to model the joint probability distribution. Each of the edges in the graph represents a dependency, while each node represents a unique variable. The model can then use this learnt distribution for inference.\nWe can use Bayesian Networks to infer unobserved variables, learn parameters from the data and learn the structure of a manually created data distribution.\nNote that each represents Boolean variables if there are m parent nodes. A minimum of 2^m entries are required to model the possible events.\nHidden Markov Model\nA Markov process is a sequential process where the previous item only influences the next item in the sequence. A Markov Chain, therefore, is a graph that uses probabilities to denote how likely it is to move to the next state in the chain. (If it is not clear how this is a Generative model, refer to the section on the difference between Generative and Discriminative models)\nAn example Markov Chain is shown below.\n[IMAGE {7 } { Markov Chain } START SAMPLE]\n\n[IMAGE { 7} FINISH SAMPLE]\nA Hidden Markov Model is a graph where the chain is unobservable. The inputs the model receives are combined with the probabilities of the previous step. This combination is used to calculate the next step in the graph.\nA constraint in an HMM is that at a certain time t= t_{0}, the target Y must be influenced only by the state of X at t= t_{0}. The states of Y at t= t_{0} should not be affected by the states of X and Y at t &lt; t_{0}.\nAn example of a Markov Model is shown here for reference.\n[IMAGE {8 } { HMM } START SAMPLE]\n\n[IMAGE { 8} FINISH SAMPLE]\nAutoregressive model\nAn Autoregressive model is used in time series forecasting. This model uses the past values in the time series to predict values that might occur in the future.\nAn Autoregressive model gets its name as it is a regression of itself. These models are generally represented as stochastic difference equations that use linear combinations of past values to model the data. A mathematical representation is as follows.\ny_{t} + c + \\phi_{1} y_{t-1} + \\phi_{2} y_{t-2} + … + \\phi_{p} y_{t-p} + \\epsilon_{t}\nwhere  \\epsilon_{t} is white noise.\nNote that changing the patterns for \\phi changes the time series. Varying the error term does not change the pattern but modifies the scale of the data instead.\nAn example of an Autoregressive model is shown below.\n[IMAGE {9 } { Autoregressive Model } START SAMPLE]\n\n[IMAGE { 9} FINISH SAMPLE]\nGenerative Adversarial Network\nGenerative Adversarial Networks are models that take large image datasets as input and generate new images. A GAN models the data distribution by exploiting the latent space of the dataset given to it. A GAN comprises two parts - A Generator and a Discriminator. These parts play a MinMax Game, where the Generator creates novel images from random noise while the Discriminator classifies the outputs into real or fake. When the Discriminator can no longer distinguish between real and fake images created by the Generator, the GAN training is said to be complete.\nAn example of a GAN that converts images to a different style is shown in the following image.\n[IMAGE {10 } { GAN } START SAMPLE]\n\n[IMAGE { 10} FINISH SAMPLE]\nDiscriminative vs Generative Models\nThe difference between Generative and Discriminative models is summarised in the following table.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGenerative ModelsDiscriminative ModelsAim to understand the data distributionAim to model the data decision boundaryUses the joint probabilityUses the conditional probabilityRelatively computationally expensiveRelatively cheaper computationallyUnsupervised TasksSupervised TasksHarmed by outliersMore robust to the presence of outliersModels how data is placed in spaceGenerates boundaries around similar classes of data in space\n:::\n:::section{.summary}\nConclusion\n\nIn summary, discriminative and generative models are two categories of machine learning models that approach tasks differently.\nDiscriminative models aim to maximize the separation between classes in a dataset to perform classification or regression.\nIn contrast, generative models create new data points by estimating the joint probability distribution of the data and the target variable.\nBoth models have their own set of advantages and disadvantages and can be applied to various tasks.\n\n:::"},"Articles/Scalar/Generating-Images-using-GANs-in-Tensorflow":{"title":"Generating Images using GANs in Tensorflow","links":["KB/Dropout"],"tags":["article"],"content":"Generating Images using GANs in Tensorflow\n:::section{.abstract}\nOverview\nThis article explains using a Generative Adversarial Network (GAN) to generate new images of handwritten digits. A GAN is a machine-learning model consisting of a generator and a discriminator. The generator creates novel images from random, while the Discriminator attempts to prove that the images generated are fake. The GAN is trained on the MNIST dataset of handwritten digits and is evaluated by testing it on unseen data and creating new images using the generator. The final output of the GAN is a batch of images that look like handwritten digits. The article provides code for reading the dataset, creating the required architecture, computing loss functions, training the network, and testing the network.\n:::\n:::section{.scope}\nScope\n\nThe article provides a general overview of Generative Adversarial Networks (GANs) and their use in image generation.\nThe specific goal of the article is to demonstrate how to create a GAN from scratch using the Tensorflow library and train it on the MNIST dataset to generate new images of handwritten digits.\nThe article explains the architecture and components of a GAN, including the generator and Discriminator.\nThe article also provides code for reading and preprocessing the MNIST dataset, creating the GAN architecture, computing loss functions, training the network, and testing the network.\nThe article also discusses the final output of the GAN, which should be a batch of images that look like handwritten digits.\n\n:::\n:::section{.main}\nWhat are we building?\nCreating novel images given an image dataset is one of the strengths of a specific branch of models called Generative Adversarial Networks (GAN). These networks specialize in unsupervised/semi-supervised image generation given any image data.\nThis article uses the GANs image generation ability to create novel handwritten digits. We perform this generation by training the network on a dataset of handwritten digits. We will create a simple GAN from scratch using the Tensorflow library, train it on the MNIST dataset and generate new images of handwritten digits.\nPre-requisites\nWhat are GANs\nGANs, or Generative Adversarial Networks, are a family of networks used for unsupervised image generation, converting between images to another, and many other applications. They are composed of two parts - a Generator and a Discriminator. The Generator creates novel images from random. The Discriminator attempts to prove that the images generated are fake. This game leads to a training approach dubbed “Adversarial Learning”. This article focuses on implementing a GAN and its image-generation ability to create new handwritten digits.\nHow are we going to build this?\nIn this article, we focus on the GAN’s image generation ability. To let the GAN learn about images, we must first load an image dataset and preprocess it. After loading the data, we must create the GAN and write the training and testing code. The below sections focus on implementing these features and generating new images from the MNIST dataset.\nFinal Output\nThe final output we want should be a batch of images that look like handwritten digits. The image shown below is what we get after training the GAN for 10000 epochs on the MNIST dataset.\n[IMAGE {1} { Final results } START SAMPLE]\n\n[IMAGE {1} FINISH SAMPLE]\nRequirements (List the libraries, modules, and other requirements needed for the project)\nBefore creating the GAN’s image generation module, we must import a few libraries. We will import all the functions, layers and dataset loaders from Tensorflow. We will also import numpy (a math library) and matplotlib (a plotting library).\nWe also need to set up some that will make up our configuration for running the module. The shape of the image is defined as a matrix of 28x28x1. The last dimension corresponds to the number of channels in an image. Since we are using the MNIST dataset in black and white, we only have a single channel.\nThe zsize is the shape of the latent space we want to generate. In this case, we set it to 100. This number could be modified if required.\nfrom __future__ import print_function, division\n \nfrom keras.datasets import mnist\nfrom keras.layers import Input, Dense, Reshape, Flatten, [[../../Dropout.md|Dropout.md|../../Dropout|Dropout]]\nfrom keras.layers import BatchNormalization, Activation, ZeroPadding2D\nfrom keras.layers import LeakyReLU\nfrom keras.layers.convolutional import UpSampling2D, Conv2D\nfrom keras.models import Sequential, Model\nfrom keras. optimizers import Adam, SGD\n \nimport matplotlib.pyplot as plt\nimport sys\nimport numpy as np\n \nnum_rows = 28\nnum_cols = 28\nnum_channels = 1\ninput_shape = (num_rows, num_cols, num_channels)\nz_size = 100\nBuilding the Model\nThe GAN we want to create comprises two major parts - The Generator and the Discriminator. The Generator is responsible for creating novel images, while the Discriminator is responsible for understanding how good the generated image is.\nThe entire architecture we want to build for the GANs image generation is shown in the following diagram.\n[IMAGE {2} { Architecture Diagram } START SAMPLE]\n\n[IMAGE {2} FINISH SAMPLE]\nThe sections below explain how to read a dataset, create the required architecture, compute the loss functions and train the network. Finally, the code to test the network and create new images is also shown.\nReading the dataset\nThis article will use the MNIST (Modified National Institute of Standards and Technology) dataset. This dataset has a larger number of handwritten digits of 28x28 and is one of the most widely used datasets in computer vision. The MNIST is an easy dataset for a GAN such as the one we are building, as it has small, single channels images.\nA sample of the dataset is shown below.\nWe only need to write a little code to load the MNIST dataset as Tensorflow comes with it inbuilt. After loading the dataset, we normalize it and then reshape it to 3 dimensions. This reshaping enables the GAN architecture to use this 2D data. We also pre-allocate some memory for our training and validation data.\n(train_ims, _), (_, _) = mnist.load_data()\ntrain_ims = train_ims / 127.5 - 1.\ntrain_ims = np.expand_dims(train_ims, axis=3)\n \nvalid = np.ones((batch_size, 1))\nfake = np.zeros((batch_size, 1))\nDefining the Generator\n[IMAGE {3} { Generator And Discriminator } START SAMPLE]\n\n[IMAGE {3} FINISH SAMPLE]\nThe job of the Generator (D) is to create realistic images that the Discriminator fails to understand are fake. Thus, the Generator is an essential component that enables a GANs image generation ability. The architecture we consider in this article comprises fully connected layers (FC) and Leaky ReLU activations. The final layer of the Generator has a TanH activation rather than a LeakyReLU. This replacement was done because we wanted to convert the generated image to the same range as the original MNIST dataset (-1,1).\ndef build_generator():\n    gen_model = Sequential()\n    gen_model.add(Dense(256, input_dim=z_size))\n    gen_model.add(LeakyReLU(alpha=0.2))\n    gen_model.add(BatchNormalization(momentum=0.8))\n    gen_model.add(Dense(512))\n    gen_model.add(LeakyReLU(alpha=0.2))\n    gen_model.add(BatchNormalization(momentum=0.8))\n    gen_model.add(Dense(1024))\n    gen_model.add(LeakyReLU(alpha=0.2))\n    gen_model.add(BatchNormalization(momentum=0.8))\n    gen_model.add(Dense(np.prod(input_shape), activation=&#039;tanh&#039;))\n    gen_model.add(Reshape(input_shape))\n \n    gen_noise = Input(shape=(z_size,))\n    gen_img = gen_model(gen_noise)\n    return Model(gen_noise, gen_img)\nDefining the Discriminator\nThe GAN uses the Discriminator (D) to identify how real the Generator’s outputs look by returning a probability of real vs fake. This part of the network can be thought of as a binary classification problem. To solve this binary classification problem, we need a rather simple network composed of blocks of Fully Connect Layers (FC), Leaky ReLU activations and Dropout layers. Note that the final layer has a block with an FC layer and a Sigmoid.\nThe final Sigmoid activation returns the classification probability that we require.\ndef build_discriminator():\n \n\tdisc_model = Sequential()\n\tdisc_model.add(Flatten(input_shape=input_shape))\n\tdisc_model.add(Dense(512))\n\tdisc_model.add(LeakyReLU(alpha=0.2))\n\tdisc_model.add(Dense(256))\n\tdisc_model.add(LeakyReLU(alpha=0.2))\n\tdisc_model.add(Dense(1, activation=&#039;sigmoid&#039;))\n\t\n\tdisc_img = Input(shape=input_shape)\n\tvalidity = disc_model(disc_img)\n\treturn Model(disc_img, validity)\nComputing the loss function\nTo make the GANs image generation procedure smoother, we need to supply it with metrics that show how well it is performing now. Loss functions do just that.\nThe Discriminator classifies the generated images into real or fake and returns the probability of it being real. To make this distinction, it needs to ensure that the input it receives is part of the real dataset. And if the input received is fake, it is not classified as part of the real dataset.\nWe can mathematically understand this difference as maximizing D(x) and minimizing D(G(z)).\nBuilding on these concepts, the Generator is tasked with fooling the Discriminator by creating realistic images. We can understand this procedure as ensuring that when the Discriminator gets an image sampled from the fake dataset, it thinks that the image belongs to the real dataset instead.\nWe can mathematically understand this procedure as maximizing D(G(z)). It is to be. Note that just using this part of the formulae as a loss function sometimes makes the network confident about the wrong outputs. To prevent this assumption, we use log(D(G(z))) instead.\nThe net cost function for the GAN’s image generation can be thus mathematically represented as \\underset{G}{min} \\underset{D}{max} V(D, G) = \\mathbb{E}_{x \\sim p_{data}(x)}[log(D(x)] + \\mathbb{E}_{z \\sim p_{z}}[log(1 - D(G(z))]\nTraining a GAN such as this is a delicate balance and can be considered a game between two enemies. (Hence the name - Adversarial Learning.) Since either party attempts to influence the opposition and reduce the others’ chance of winning, this is a MinMax game.\nWe can then create the Generator and Discriminator with a Binary Crossentropy loss.\n# discriminator\ndisc= build_discriminator()\ndisc.compile(loss=&#039;binary_crossentropy&#039;,\n    optimizer=&#039;sgd&#039;,\n    metrics=[&#039;accuracy&#039;])\n \nz = Input(shape=(z_size,))\n# generator\nimg = generator(z)\n \ndisc.trainable = False\n \nvalidity = disc(img)\n \n# combined model\ncombined = Model(z, validity)\ncombined.compile(loss=&#039;binary_crossentropy&#039;, optimizer=&#039;sgd&#039;)\nOptimizing the loss\nTo train the network, we need the GAN to play the MinMax game. The training procedure hinges on performing Gradient Descent on the network weights. To reduce the training time and ensure that the training does not get stuck on the loss landscape, we use a Stochastic version of GD, aka Stochastic Gradient Descent.\nBoth the Discriminator and the Generator have different losses. If We gave both these networks a single loss function, they would not be able to optimize each other.\ndef intialize_model():\n    disc= build_discriminator()\n    disc.compile(loss=&#039;binary_crossentropy&#039;,\n        optimizer=&#039;sgd&#039;,\n        metrics=[&#039;accuracy&#039;])\n \n    generator = build_generator()\n \n    z = Input(shape=(z_size,))\n    img = generator(z)\n \n    disc.trainable = False\n \n    validity = disc(img)\n \n    combined = Model(z, validity)\n    combined.compile(loss=&#039;binary_crossentropy&#039;, optimizer=&#039;sgd&#039;)\n    return disc, Generator, and combined\nHaving defined all the required functions, we can train the network to optimize the losses. The steps we follow for the GAN’s image generation are as follows.\n\nLoad an image, and generate random noise of the same size as the loaded image.\nSend these images to the Discriminator and calculate the real vs fake probability for the same.\nGenerate another noise of the same size. Send this noise to the Generator.\nRun training for the Generator for a few epochs.\nRepeat all the steps until a satisfactory image is generated.\n\nThese steps are directly translated into the code shown below.\ndef train(epochs, batch_size=128, sample_interval=50):\n    # load images\t\n    (train_ims, _), (_, _) = mnist.load_data()\n    # preprocess\n    train_ims = train_ims / 127.5 - 1.\n    train_ims = np.expand_dims(train_ims, axis=3)\n \n    valid = np.ones((batch_size, 1))\n    fake = np.zeros((batch_size, 1))\n    # training loop\n    for epoch in range(epochs):\n \n        batch_index = np.random.randint(0, train_ims.shape[0], batch_size)\n        imgs = train_ims[batch_index]\n    # create noise\n        noise = np.random.normal(0, 1, (batch_size, z_size))\n    # predict using Generator\n        gen_imgs = gen.predict(noise)\n    # calculate loss functions\n        real_disc_loss = disc.train_on_batch(imgs, valid)\n        fake_disc_loss = disc.train_on_batch(gen_imgs, fake)\n        disc_loss_total = 0.5 * np.add(real_disc_loss, fake_disc_loss)\n \n        noise = np.random.normal(0, 1, (batch_size, z_size))\n \n        g_loss = full_model.train_on_batch(noise, valid)\n    # show progress\n        print (&quot;%d [D loss: %f, acc.: %.2f%%] [G loss: %f]&quot; % (epoch, disc_loss_total[0], 100*disc_loss_total[1], g_loss))\n    # save outputs every few epochs\n        if epoch % sample_interval == 0:\n            one_batch(epoch)\nGenerating handwritten digits\nFinally, we can generate handwritten digits from the MNIST dataset. To look at how far the network has trained the images, we create a helper function to store predictions from the Generator for a batch of images. This function creates random noise, passes them to the Generator, processes it for displaying and then saves it to a folder. We run this helper function every 200 epochs.\ndef one_batch(epoch):\n    r, c = 5, 5\n    noise = np.random.normal(0, 1, (r * c, z_size))\n    gen_imgs = gen.predict(noise)\n \n    # Rescale images 0 - 1\n    gen_imgs = 0.5 * gen_imgs + 0.5\n \n    fig, axs = plt.subplots(r, c)\n    cnt = 0\n    for i in range(r):\n        for j in range(c):\n            axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap=&#039;gray&#039;)\n            axs[i,j].axis(&#039;off&#039;)\n            cnt += 1\n    fig.savefig(&quot;images/%d.png&quot; % epoch)\n    plt.close()\nFor this article, we trained the GAN for around 10,000 epochs with a batch size of 32. We save the generated images every 200 epochs in the images folder.\ndisc, gen, full_model = intialize_model()\ntrain(epochs=10000, batch_size=32, sample_interval=200)\nWe can now look at the results of the GAN’s image generation at the start, at 400 epochs, at 5000 epochs and the final result at 10000 epochs.\nAt the start, we have random noise.\n[IMAGE {4} { Epoch 0 } START SAMPLE]\n\n[IMAGE {4} FINISH SAMPLE]\nAfter 400 epochs, we are getting somewhere slowly. But these results are different from real digits.\n[IMAGE {5} { Epoch 400 } START SAMPLE]\n\n[IMAGE {5} FINISH SAMPLE]\nAfter 5000 epochs, we can see figures that resemble the MNIST dataset.\n[IMAGE {6} { Epoch 5000 } START SAMPLE]\n\n[IMAGE {6} FINISH SAMPLE]\nAfter training the network for the entire 10,000 epochs, we get the following outputs.\n[IMAGE {7} { Final results } START SAMPLE]\n\n[IMAGE {7} FINISH SAMPLE]\nThese images look very close to the handwritten number data we fed the network. These images were not shown to the network during training and were generated from scratch.\nWhat’s next\nThe output we got from the GANs image generation is good, but there are many ways we can improve it. Without leaving the scope of this article, we can experiment with a few parameters. Some of them are as follows:\n\nTry different values of the latent space variable z_size to see if the performance improves.\nTry training the model for a larger number of epochs. We trained it for 10000; try doubling or tripling that to see if the results improve or worsen.\nTry different datasets such as the Fashion MNIST or the Moving MNIST. These datasets follow the same structure as the MNIST, making it possible to use the code we wrote directly.\nFinally, it is worth experimenting with other architectures such as CycleGAN, DCGAN etc. Many of them would only require changing the functions of the Generator and Discriminator.\n\n:::\n:::section{.summary}\nConclusion\n\nGANs are machine learning models that can generate new images from a dataset.\nIn this article, a simple GAN is created using the Tensorflow library and trained on the MNIST dataset.\nThe GAN comprises two parts: a Generator that creates novel images from random and a Discriminator that attempts to prove that the images generated are fake.\nThe final output is a batch of images that look like handwritten digits, as shown in the example image provided.\nThe GAN is trained by supplying it with metrics and loss functions that show how well it correctly classifies real and fake images.\nThe GAN is then evaluated by testing it on unseen data and creating new images using the generator.\n\n:::"},"Articles/Scalar/Image-Classification-using-TensorFlow":{"title":"Image Classification using TensorFlow","links":["KB/Regularization"],"tags":["article"],"content":"Image Classification using TensorFlow\n:::section{.abstract}\nOverview\nImage Classification is one of the basic tasks in Deep Learning. Given a dataset with images of different categories, we create a Deep Learning model and a pipeline to classify these images. We can create models in any library, but Tensorflow is a good starting point for beginners, and we will use this library to create an image classifier.\n:::\n:::section{.main}\nWhat are we building?\nThis article will tackle a Tensorflow image classification problem by creating a neural network that can classify images from the CIFAR10 dataset. We will explore the concepts of Pre-Processing, Augmentation, and Performance Optimisation. We will learn how to load a dataset, build the model and finally train the model we created using the dataset. We will also learn how to use the trained model to make predictions on custom images.\nThe following sections explain these concepts and how to implement them using Tensorflow.\nPre-requisites\nBefore we get to the actual code, we must understand a few pre-requisite terms. They are explained here.\nData Loaders\nA Data Loader is a utility function that enables Tensorflow to optimize the data loading performance. The Loader does this by pre-allocating memory, creating batched containers, and applying many other tweaks to improve performance.\nData Augmentation\nData Augmentation is a Regularization, and many others.\n[IMAGE {1} Data Augmentation  START SAMPLE]\n\n[IMAGE {1} FINISH SAMPLE]\nLambda functions\nLambda functions are special functions in Tensorflow that lets the user create functions without explicitly defining a function call. These functions are useful for improving the readability of the code and avoiding defining extra functions for single use.\nMap functions\nIn deep learning, there are many times when we need to apply a function over a batch of data. Sequentially performing these tasks is extremely time-consuming, so we use map functions to apply a function in parallel over any batch of data.\nHow are we going to build this?\nIn this article, we will be building a Tensorflow image classification model. After loading the required libraries, we first load the data. Here, we will use the CIFAR10 dataset. After loading the data, we split it into the train, test, and validation components and then create batches of the same. To optimize performance, we will also be using caching and pre-fetching. After creating the required data loaders, we can create the model. This demo will use a ResNet50 model with the Adam optimizer and a Sparse Cross-Entropy loss function. Once we have loaded both the data and the model, we can finally train the model on the data and then evaluate its performance.\nAll of these steps are detailed in the sections below.\nFinal Output\nThe final output we want is a Tensorflow image classification model that can identify the class of a given image. We want our model to learn from the CIFAR10 dataset and understand all ten classes accurately.\nFor example, if we pass this 64x64 image to the model, it should classify it as a horse.\n[IMAGE {2} Classified Data START SAMPLE]\n\n[IMAGE {2} FINISH SAMPLE]\nRequirements\nBefore creating a Tensorflow image classification model, we need to import the required modules. Apart from the base Tensorflow library, we also import the Keras package. Keras is a wrapper around Tensorflow that simplifies building neural networks. We will also import a library called Tensorflow Datasets that will enable us to load and pre-process our dataset faster.\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow_datasets as tfds\nimport os\nBuilding the Classifier\nWe can now move on to building the classifier with the libraries we implemented. The below sections explain how to load the dataset, pre-process it, optimize it for use, and pass it to the model. We also explore how to create a model using Tensorflow and how to train it on the CIFAR10 dataset. Finally, we also learn how to evaluate a trained model on the test dataset.\nDownload and explore the dataset\nFor this article, we will be using the CIFAR10 dataset. This dataset has 60000 32x32 color images grouped into ten classes. Before we create the model, we need to load and pre-process the dataset. A sample of images is shown below.\n[IMAGE {3} CIFAR10 START SAMPLE]\n\n[IMAGE {3} FINISH SAMPLE]\nWe split the dataset into training, testing, and validation and loaded the dataset with labels using the as_supervised option.\nTo verify that we loaded the data correctly, we checked the size of the splits we had just created. If they are not zero or tiny numbers, we can know that our code has worked so far.\ntrain_dataset, validation_dataset, test_dataset = tfds.load(\n    &quot;cifar10&quot;,\n    split=[&quot;train[:40%]&quot;, &quot;train[40%:50%]&quot;, &quot;train[50%:60%]&quot;],\n    as_supervised=True,\n)\n \nprint(&quot;Number of training samples: %d&quot; % tf.data.experimental.cardinality(train_dataset))\nprint(\n    &quot;Number of validation samples: %d&quot; % tf.data.experimental.cardinality(validation_dataset)\n)\nprint(&quot;Number of test samples: %d&quot; % tf.data.experimental.cardinality(test_dataset))\nConfigure the dataset for performance\nSimply loading the data and passing it to the model works out of the box but leads to a sharp down in performance. We need to perform some tweaks to ensure that we use our resources optimally.\nWe first define our image size as 128x128x3 pixels and use a lambda function to resize all the images in our dataset to this image size.\nThe next optimization we perform is converting the dataset into batches of 64 and informing Keras that we wish to cache the data and prefetch 10 samples.\nPrefetching data reduces the time it takes to pass the data to the memory by preallocating memory and fetching a few extra samples for the next time the model is called.\nsize = (128, 128)\nbs = 64\n \ntrain_dataset = train_dataset.map(lambda x, y: (tf.image.resize(x, size), y))\nvalidation_dataset = validation_dataset.map(lambda x, y: (tf.image.resize(x, size), y))\ntest_dataset = test_dataset.map(lambda x, y: (tf.image.resize(x, size), y))\n \ntrain_dataset = train_dataset.cache().batch(bs).prefetch(buffer_size=10)\nvalidation_dataset = validation_dataset.cache().batch(bs).prefetch(buffer_size=10)\ntest_dataset = test_dataset.cache().batch(bs).prefetch(buffer_size=10)\nCreate the model\nSince we wish to maximize performance, we also use two simple Data Augmentation techniques. The first one randomly flips the images along the horizontal axis to ensure that the model learns some spatial information. The second Augmentation we use is Random Rotation. We only want to apply this for some images, so we use a lower probability. Applying it to every image might make the model perform worse.\nIn this article, we use the Tensorflow image classification model ResNet50. This model uses the concept of Skip Connections to improve performance. We will not create the model from scratch but use the Keras implementation. To the function call, we need to pass in the weights from which we wish to use the pre-trained model. Since we are training the model from scratch here, we will pass the option as None. If we were using transfer learning, we would use the option “imagenet”. We also need to pass the image size (128x128x3), the number of classes (10), and whether we want to include the whole model. This final option is only useful for transfer learning.\n[IMAGE {4} Skip Connections START SAMPLE]\n\n[IMAGE {4} FINISH SAMPLE]\nOnce we load the model, we create an input, pass the current batch through the Augmentation, and finally, a Fully Connected (FC) layer with a size of 10 (CIFAR10 has ten classes). This final model is the one we will be used for training on our data.\nWe will use the summary function to check the layers to verify if we created the model correctly.\naug_transforms = keras.Sequential(\n    [layers.RandomFlip(&quot;horizontal&quot;), layers.RandomRotation(0.1),]\n)\n \nmodel_base = keras.applications.ResNet50(\n    weights=None, \n    input_shape=(128, 128, 3),\n    classes = 10,\n    include_top=True\n)\n \ninputs = keras.Input(shape=(128, 128, 3))\n \nx = aug_transforms(inputs) \nx = model_base(x, training=False)\noutputs = keras.layers.Dense(10)(x)\nfinal_model = keras.Model(inputs, outputs)\nfinal_model.summary()\nTrain the model\nWe can finally move on to training our model on the CIFAR10 data. Since we have defined our model, we need to define all the parameters required for training it.\nIn this article, we will use the Adam optimizer with the default parameters. We choose a Sparse Categorical Crossentropy function for the loss function as this task is a multi-class classification problem. The metric we use here is a Categorical Accuracy metric that checks how well the classifier performed across all the classes.\nWe will now train the model for five epochs. We can perform further training by increasing the number of epochs.\nfinal_model.compile(\n    optimizer=keras.optimizers.Adam(),\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n)\n \nnum_epochs = 5\nfinal_model.fit(train_dataset, epochs=num_epochs, validation_data=validation_dataset)\n[IMAGE {5} Training START SAMPLE]\n\n[IMAGE {5} FINISH SAMPLE]\nEvaluating the Model\nFive epochs are very small since we are not using transfer learning, and it is only done as a demo. After our model is done training, we can perform a full evaluation on it by performing predictions on the test dataset. While performing inference, we can increase the batch size as we are not training a network.\nWe can perform this evaluation by using the evaluate function from Keras.\nresults = final_model.evaluate(test_dataset, batch_size=128)\nprint(&quot;test loss, test acc:&quot;, results)\nA single prediction can be performed with the following code.\nfrom PIL import Image\nimport numpy as np\nfrom skimage import transform\ndef load(filename):\n   np_image = Image.open(filename)\n   np_image = np.array(np_image).astype(&#039;float32&#039;)/255\n   np_image = transform.resize(np_image, (256, 256, 3))\n   np_image = np.expand_dims(np_image, axis=0)\n   return np_image\n \nimage = load(&#039;my_file.jpg&#039;)\nfinal_model.predict(image)\n:::\n:::section{.summary}\nConclusion\n\nThis article taught us how to build a Tensorflow Image Classification model.\nThe article showed how to load the CIFAR10 dataset and pre-process it for training.\nIt also explained how to create a simple ResNet50 model and how to train it on the CIFAR10 dataset.\nThe article also explained how to evaluate the model and perform inference using the trained model.\n:::\n"},"Articles/Scalar/Implementing-DCGAN-to-generate-CIFAR-images":{"title":"Implementing DCGAN to generate CIFAR images","links":["KB/CIFAR","KB/Strided"],"tags":["article"],"content":"Implementing DCGAN to generate CIFAR images\nWhat are we building?\nCreating novel images is one of the strengths of Generative Adversarial Networks (GANs). In this article, we will build a DCGAN for image generation using the CIFAR dataset. The DCGAN is a type of GAN that builds upon the Vanilla GAN and addresses some of its issues. The DCGAN is a good choice if the image data size is larger than 28x28. This network also leads to fewer chances of a mode collapse and is thus a better network than a standard GAN.\nHere, we want the network to create realistic images to resemble any of the ten classes of the CIFAR dataset. We will create the DCGAN from scratch using PyTorch, train it and write scripts to generate our images.\nWhat are DGANs\nResearchers created the Vanilla GAN architecture to generate images in an unsupervised manner from image datasets. But this GAN had quite a few flaws that impacted its training. DCGANs are a modification of the Vanilla GAN architecture. The implementation of the Discriminator and Generator is configured to tackle some of the issues of the original GAN. Some of the changes are as follows.\nConvolutional layers are used explicitly in the Discriminator. In this architecture, the Generator explicitly uses Transposed Convolution layers. The Discriminator relies on Batch Normalization along with LeakyReLU activations. The Generator, on the other hand, uses ReLU activations.\nThe DCGAN image generation process is almost similar to the Vanilla GAN except for a few tweaks to the optimizer and the architecture itself.\nHow are we going to build this?\nTo build the DCGAN, we will use the Python library PyTorch. We will first import PyTorch and other required libraries. We then load the image dataset using a DataLoader, which is the CIFAR10 dataset in this case. After we load the data, we create the functions that make up the DCGAN. We can then train the network on this dataset and generate new photorealistic images that look like they were part of the CIFAR10 dataset.\nThe below sections focus on implementing all the functions required to create a DCGAN image generation pipeline.\nFinal Output\nThe final output we want is photorealistic images that look like they might belong to the CIFAR10 dataset. The result that we get is shown below.\n[IMAGE {1} Final Output START SAMPLE]\n\n[IMAGE {1} FINISH SAMPLE]\nNote that longer training might yield better results but take significantly longer.\nRequirements\nBefore starting the DCGAN image generation process, we must import some libraries and perform set-up operations.\nFirst, we must create folders to store the models’ images and weights using the mkdir command.\nWe import the PyTorch library and all the required features we need that comes with it. We also import the numerical processing library numpy and the plotting library matplotlib.\nTraining a GAN takes quite a long time. To further improve performance, we enable a flag in Pytorch that enables benchmarking. Pytorch runs a few checks on your current device during the benchmarking process to determine which algorithms perform the best. These checks let you run slightly more optimized code for your current device.\n!mkdir images\n!mkdir weights\nfrom __future__ import print_function\nimport os\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nimport torch.utils.data\nimport torchvision.datasets as tv_data\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\nfrom tqdm import tqdm\nimport numpy as np\nimport matplotlib.pyplot as plt\n \ncudnn.benchmark = True\nImplementing DCGAN to generate CIFAR images\nWe can now move on to the main DCGAN image generation process. To generate the images, we need to create a DataLoader, load the CIFAR images,  preprocess them, and send batches of this data to the GPU memory. We also need to create the network architecture and initialize the weights of its components. This network also needs to be sent to the GPU memory.\nAfter these initial steps have been completed, we can finally train the network and generate new images.\nThe DCGAN architecture is similar to many other GAN architectures and consists of a Generator and a Discriminator. The Generator is responsible for creating photo-realistic images from random noise to fool the Discriminator. On the other hand, the Discriminator takes the outputs of the Generator and returns the probability that the generated image is real. The Generator uses this probability to improve its generation capabilities by training the model.\nThe architecture diagram of the DCGAN is shown below.\n[IMAGE {2} Architecture START SAMPLE]\n\n[IMAGE {2} FINISH SAMPLE]\n[IMAGE {3} Generator START SAMPLE]\n\n[IMAGE {3} FINISH SAMPLE]\nIn the architecture, ngpu stands for the number of GPUs. In this case, we will only be using a single one. nz stands for the width of the input. ngf and ndf denote the shape of the maps that the Generator and Discriminator create, respectively. nc is the number of channels that the image has.\nWe first start by loading our data and understanding how it looks.\nExploring the dataset\nThis article uses the CIFAR10 (Canadian Institute for Advanced Research) dataset. This dataset contains ten classes of images similar to the MNIST format but in 3-channel RGB. The CIFAR10 is a very common dataset for benchmarking image classification models and is easy for a model to learn.\nBefore we can explore the dataset, we must load and preprocess it. PyTorch comes with the CIFAR10 dataset inbuilt, and we can directly load it. If this is the first time we have used this dataset, it may not exist locally, and we need to download it first. After that, we need to resize all the images to a common size of 64x64x3. CIFAR10 is a clean dataset, and this resizing step is probably unnecessary, but it is a good practice to uphold.\nWe also normalize the images and convert them to the PyTorch tensors.\nIn PyTorch, we need to create a DataLoader, simply a class that creates optimized batches of data to pass to the model.\nWe send this DataLoader to the GPU if we are using one to enhance the speed of the DCGAN image generation process.\ndataset = tv_data.CIFAR10(root=&quot;./data&quot;, download=True,\n                           transform=transforms.Compose([\n                               transforms.Resize(64),\n                               transforms.ToTensor(),\n                               transforms. Normalize ((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n                           ]))\nnum_channels=3\n \ndataloader = torch.utils.data.DataLoader(dataset, batch_size=128,\n                                         shuffle=True, num_workers=2)\ncurrent_device = &#039;cuda&#039; if torch.cuda.is_available() else &#039;cpu&#039;\nAfter loading the data, we can see how it looks. To visualize a batch of data, we iterate over the DataLoader to grab a single batch of images. After obtaining the images, we can create a grid the size of a single batch, pad the images to make them look neater and normalize them. This batch of images is still on the GPU, which means that we cannot plot it on the CPU without sending it back.\nsingle_batch = next(iter(dataloader))\nplt.figure(figsize=(8,8))\nplt.axis(&quot;off&quot;)\nplt.toc: true\ntitle(&quot;Training Images&quot;)\nplt.imshow(np.transpose(vutils.make_grid(single_batch[0].to(current_device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))\n[IMAGE {4} Training Images START SAMPLE]\n\n[IMAGE {4} FINISH SAMPLE]\nAs we can see, the dataset comprises ten classes of images from which we plot a random sample.\nDefining the Discriminator\nThe Discriminator in the DCGAN is responsible for classifying the images returned by the Generator as real or fake. Architecturally, the Discriminator is almost a mirror of the Generator with minor differences. The Discriminator uses a Strided Convolution and a LeakyReLU along with Batch Normalization layers. The final layer is a Sigmoid layer that returns the probability we want.\nFor DCGAN image generation, the Discriminator uses Strided Convolutions instead of Pooling layers. This choice allows the network to learn custom padding functions that, in turn, improve performance.\nngpu = 1\nnz = 100\nngf = 64\nndf = 64\n \ndef weights_normal_init(m):\n    classname = m.__class__.__name__\n    if classname.find(&#039;Conv&#039;) != -1:\n        m.weight.data.normal_(0.0, 0.02)\n    elif classname.find(&#039;BatchNorm&#039;) != -1:\n        m.weight.data.normal_(1.0, 0.02)\n        m.bias.data.fill_(0)\n \nclass Disc_model(nn.Module):\n    def __init__(self, ngpu):\n        super(Disc_model, self).__init__()\n        self.ngpu = ngpu\n        self.main = nn.Sequential(\n            nn.Conv2d(num_channels, ndf, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n            nn.Sigmoid()\n        )\n \n    def forward(self, input):\n        if input.is_cuda and self.ngpu &gt; 1:\n            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n        else:\n            output = self.main(input)\n \n        return output.view(-1, 1).squeeze(1)\nDefining the Generator\nThe Generator is responsible for mapping the input data from the latent space to the vector data space. In this part of the network, we get an RGB image as an output that can then be passed to the Discriminator. The generated image size is the same as the original training images but in channel first indexing (3x64x64).\nThe Generator comprises blocks with Transposed Convolutions, Batch Normalizations and ReLU layers. The final layer has a Tanh activation used to make the data to a range of [-1,1].\nIn this part of the DCGAN image generation process, the DCGAN uses Batch Normalization layers after Transposed convolutions. This shift enables a smoother gradient flow between the layers of the network.\nclass Gen_model(nn.Module):\n    def __init__(self, ngpu):\n        super(Gen_model, self).__init__()\n        self.ngpu = ngpu\n        self.main = nn.Sequential(\n            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf * 8),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf, num_channels, 4, 2, 1, bias=False),\n            nn.Tanh()\n        )\n \n    def forward(self, input):\n        if input.is_cuda and self.ngpu &gt; 1:\n            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n        else:\n            output = self.main(input)\n            return output\nDefining the inputs\nWe need to set up some empty containers for an optimized workflow. We first create a fixed noise with the shape (128, size of latent space, 1, 1) and send it to the GPU. We also denote the label of the real image as one and the fake image as 0. For this article, we will run the network for 25 epochs.\nWe also pre-allocate arrays to store the Generator and Discriminator loss during training.\nfixed_noise = torch.randn(128, nz, 1, 1).to(current_device)\nreal_label = 1\nfake_label = 0\n \nniter = 25\ng_loss = []\nd_loss = []\nComputing the loss function\nThe DCGAN image generation pipeline has two loss functions, for the Generator and Discriminator, respectively.\nThe Discriminator penalizes wrongly classifying a real image as a fake or a fake image as real. This concept can be thought of as maximizing the following function.\n\\nabla_{\\theta_{d}} \\frac{1}{m} \\Sigma_{i=1}^{m}[log D(x^{(i)}) + log(1-D(G(z^{(i)})))]\nThe Generator loss takes the output of the Discriminator into account and rewards it if the Generator is fooled into thinking the fake image is real. If this condition is not satisfied, the Generator is penalized.\nThis concept can be thought of as minimizing the following function.\n\\nabla_{\\theta_{g}} \\frac{1}{m} \\Sigma_{i=1}^{m}log(1-D(G(z^{(i)})))\nmodel_Gen = Gen_model(ngpu).to(current_device)\nmodel_Gen.apply(weights_normal_init)\nmodel_Disc = Disc_model(ngpu).to(current_device)\nmodel_Disc.apply(weights_normal_init)\nloss_func = nn.BCELoss()\nOptimizing the loss\nWe will use the ADAM optimizer with a learning rate of 0.0002 and the beta parameters set to (0.5m, 0.999) to optimize the loss. The Generator and Discriminator have different optimizers to ensure that they both learn independently.\noptimizerD = optim.Adam(model_Disc.parameters(), lr=0.0002, betas=(0.5, 0.999))\noptimizerG = optim.Adam(model_Gen.parameters(), lr=0.0002, betas=(0.5, 0.999))\nTrain the DCGAN\nTo run DCGAN image generation, we need to train the network first. The general procedure is as follows.\nFor each epoch, the noise is sent to the Generator. The Discriminator also gets a random image sampled from the dataset. The Generator then uses the weights it has learned to modify the noise to be closer to the target image. In doing so, the Generator learns a mapping between random noise and the latent space of the image dataset. The Generator then sends the tweaked image to the Discriminator. The Discriminator then predicts how real it thinks the generated image is and informs the Generator using a probability metric.\nfor epoch in tqdm(range(niter), total = niter):\n    for i, data in enumerate(dataloader, 0):\n        model_Disc.zero_grad()\n        device_model = data[0].to(current_device)\n        batch_size = device_model.size(0)\n        label = torch.full((batch_size,), real_label).to(current_device)\n \n        output = model_Disc(device_model) # Discriminator output\n        disc_error_real = loss_func(output.float(), label.float()) \n        disc_error_real.backward() # disc loss for real image\n        D_x = output.mean().item()\n \n        noise = torch.randn(batch_size, nz, 1, 1).to(current_device) # create noise\n        fake = model_Gen(noise) # Fake image\n        label.fill_(fake_label) # Fill with 0\n        output = model_Disc(fake.detach())\n        disc_error_fake = loss_func(output.float(), label.float()) # disc loss for fake image\n        disc_error_fake.backward() \n        D_G_z1 = output.mean().item()\n        disc_error = disc_error_real + disc_error_fake\n        optimizerD.step()\n \n        model_Gen.zero_grad()\n        label.fill_(real_label) # fill with 1\n        output = model_Disc(fake.float()) # disc output\n        gen_error = loss_func(output.float(), label.float())\n        gen_error.backward()\n        D_G_z2 = output.mean().item()\n        optimizerG.step()\n \n        print(&#039;[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f&#039; % (epoch, niter, i, len(dataloader), disc_error.item(), gen_error.item(), D_x, D_G_z1, D_G_z2))\n        \n        if i % 100 == 0: # save images every 100 steps\n            print(&#039;saving the output&#039;)\n            vutils.save_image(device_model,&#039;./images/real_samples.png&#039;,normalize=True)\n            fake = model_Gen(fixed_noise)\n            vutils.save_image(fake.detach(),&#039;./images/fake_samples_epoch_%03d.png&#039; % (epoch),normalize=True)\n    \n    torch.save(model_Gen.state_dict(), &#039;weights/model_Gen_epoch_%d.pth&#039; % (epoch))\n    torch.save(model_Disc.state_dict(), &#039;weights/model_Disc_epoch_%d.pth&#039; % (epoch))\nIn this article, we train the network for 25 epochs. After the training is complete and even during training, we can periodically check the ./images folder to see outputs every 100 steps.\nThe results we get are as follows.\nTo understand the training progression, we look at the original sample and then the outputs at 0, ten, and 25 epochs.\n[IMAGE {5} Original Sample/Target START SAMPLE]\n\n[IMAGE {5} FINISH SAMPLE]\n[IMAGE {6} Epoch 0 START SAMPLE]\n\n[IMAGE {6} FINISH SAMPLE]\n[IMAGE {7} Epoch 10 START SAMPLE]\n\n[IMAGE {7} FINISH SAMPLE]\n[IMAGE {8} Final Results START SAMPLE]\n\n[IMAGE {8} FINISH SAMPLE]\nWhat’s next\nWe can apply a few tweaks to the DCGAN in further experiments. Some of them are mentioned below.\n\nTweaking the z_size variable and either increasing or decreasing it might lead to better performance.\nLonger training might also lead to better results.\nUsing Label Smoothing Cross Entropy Loss instead of just a Cross Entropy Loss might also improve performance.\nThere is a long list of tweaks proposed by one of the creators of the PyTorch library regarding the DCGAN. The article mentioned is quite a few years old but gives a good background for further experiments on the DCGAN image generation process.\n\nConclusion\n\nA DCGAN was built using PyTorch to generate images from the CIFAR10 dataset.\nThe DCGAN is a modified version of a Vanilla GAN that addresses some issues and leads to fewer chances of mode collapse.\nSuggestions for further experimentation with the DCGAN include adjusting the z_size variable, increasing training time, and using Label Smoothing Cross Entropy Loss.\n"},"Articles/Scalar/Improving-Model-Accuracy-in-Image-Classification":{"title":"Improving Model Accuracy in Image Classification","links":["KB/Dropout","KB/Regularization","KB/Cropping"],"tags":["article"],"content":"Improving Model Accuracy in Image Classification\n:::section{.abstract}\nOverview\nImproving image classification accuracy is one of the biggest hurdles in deep learning. Apart from using a deeper network and better data, many techniques have been developed to optimize network performance. Some techniques, such as Dropout, are more focused on improving the overall pipeline.\n:::\n:::section{.scope}\nScope\n\nThis article explains the concepts of Overfitting and Underfitting.\nDropout and other Regularization techniques like Data Augmentation are explained.\nThe article also explains Early Stopping and other pipeline tweaks such as Hyperparameter Tuning and Transfer Learning.\n\n:::\n:::section{.main}\nIntroduction\nIt is often impossible to always have better data or larger models. In such cases, using techniques like Regularization and Early Stopping tackle the challenges of Overfitting.\nThis article provides an introduction to many such algorithms and pipeline tweaks that help in the process of improving model accuracy in image classification.\nImproving Model Accuracy\nThe two biggest hurdles in training neural networks are Overfitting and Underfitting. In the first case, the network memories the data, and in the second, the network does not learn enough. The following techniques can be divided into categories based on these two concepts.\nDropout, Early Stopping, tackle Overfitting.  Transfer Learning and Hyperparameter Tuning tackle Underfitting.\nIf there is a lack of data, we can use Transfer learning and Data Augmentation. The other algorithms can be experimented with if the model does not perform well.\nThe below sections explain all of these algorithms.\nOverfitting and Underfitting\nTraining a neural network is a balancing act that requires understanding many metrics.\nTraining accuracy is defined as the model’s accuracy during training time on the train/test split of the data.\nValidation accuracy is defined as the model’s performance when tested on real-world data that the model has never seen.\nIf, in improving image classification accuracy, the training accuracy is far higher than the validation accuracy for many epochs, this is called Overfitting. In Overfitting, the model focuses too heavily on the training data and essentially fails to predict any sample it has yet to see before.\nIf the training accuracy is very low and the validation accuracy seems to fluctuate or is much higher than the training accuracy, this is called Underfitting. In Underfitting, the model must be more powerful to fit the data.\nBoth Overfitting and Underfitting can be countered in many ways, but it is to be noted that they have a delicate balance. Learning to understand which of these the network is going through is essential in being able to improve image classification accuracy.\n[IMAGE {1} Overfitting_Underfitting START SAMPLE]\n\n[IMAGE {1} FINISH SAMPLE]\nDropout layers\nWhen the single unit in a network computes gradients wrt the error, it also considers the other units and tries to fix their mistakes. This dependency is known as Co Adaptation and leads to the formation of complex relations that encourages Overfitting. Dropout layers reduce co-dependence between the neurons in a network by randomly (with a probability p) setting neuron activations to 0. This layer is applied to Dense (Fully connected) layers in a network.\nDropout can help performance as more information is recovered. Similarly, if the dataset is too large, the model performance might also worsen.\nDuring testing, the weights are scaled by the probability p.\n[IMAGE {2} Dropout START SAMPLE]\n\n[IMAGE {2} FINISH SAMPLE]\nData Augmentation\nNeural networks are extremely data-hungry, and training them requires many training examples. It is, of course, only sometimes possible to have a large amount of training data. We can use a method called Data Augmentation to artificially expand the number of available examples. In essence, Data Augmentation is the process of tweaking the given examples multiple times in different ways to generate new training samples from the existing images. Some examples of Data Augmentation for image data include Random Flipping, Jittering Brightness/Contrast, Random Resizing, and Random Cropping.\nSome Data Augmentations are shown below.\n[IMAGE {3} Augmentation START SAMPLE]\n\n[IMAGE {3} FINISH SAMPLE]\nData augmentation is a good method for improving image classification accuracy. This technique is not restricted to images; we can apply similar concepts to every other data domain. Data Augmentation also has the added benefit of being a regularizer by showing the model data from different perspectives.\nRegularization\nOne of the biggest challenges neural networks face during training is Overfitting. Penalizing complex models that have better performance during training but not during validation is one way of reducing the effects of Overfitting. The objective of training neural networks is for them to be used on real data outside the training set. Penalizing models that learn too much of the training set is called Regularization term is used to control the penalty applied to the model. This term is also a hyperparameter, as increasing it too much may hurt model performance.\nMany algorithms perform Regularization, etc.\nEarly Stopping\nEarly Stopping is a Regularization technique that improves image classification accuracy by intentionally stopping the training when validation loss increases. Training is stopped as training a model for too many epochs sometimes causes Overfitting. In Early Stopping, the number of epochs becomes a tunable hyperparameter. We continuously store the best parameters during training, and when these parameters no longer change for several epochs, we stop training.\nThe idea of Early Stopping can be seen in this diagram.\n[IMAGE {4} Early Stopping START SAMPLE]\n\n[IMAGE {4} FINISH SAMPLE]\nTransfer Learning\nTraining large-scale image models are time and energy-consuming. Since most vision datasets have some common features, it is possible to take a network trained on a similar dataset and use the trained features to reduce training time on a different dataset.\nTransfer learning is a procedure that lets a pre-trained model be used either as a feature extractor or as a weight initializer. In most cases, Transfer learning is used for fine-tuning. We can transfer knowledge from a network trained on a complex task to a simpler one or from a network trained on large amounts of data to one with fewer data.\nTransfer learning is thus a potential key to multi-task learning, an active field of research in deep learning. This technique is also key in quickly improving image classification accuracy with fewer data.\nThe following diagram shows the concept behind using Transfer learning to improve image classification accuracy.\nHyperparameter tuning\nEvery DL model and training pipeline has parameters we can tune to optimize performance. Parameters can include - how many epochs to train the network, weight decay, optimizers, learning rate, and a lot more. Each hyperparameter can have multiple values, and these quickly add up to hundreds or more different cases to try.\nHyperparameter tuning is the art of tweaking these parameters to create an optimal model in the shortest amount of time. We can test only some parameters, but a tuning service can estimate which hyperparameter to keep and which to discard. Many algorithms enable such a service, one of them being a grid search over the hyperparameter space. If the hyperparameter in question reduces model performance, it is dropped, and sometimes similar hyperparameters are also dropped.\nHyperparameter tuning is a challenging problem as every task requires different requirements. Tuning hundreds of parameters is a balancing act between the choices.\nThis technique is one of the final bits of the pipeline that leads to improving image classification accuracy.\n:::\n:::section{.summary}\nConclusion\n\nIn this article, we learned the importance of other algorithms in improving image classification accuracy.\nWe looked at the concepts of Overfitting and Underfitting and understood how they affect model training.\nWe also looked at many algorithms that improve performance by modifying the architecture or changing how we train the network.\nWe understood what techniques to use when we lack data.\nWe also tackled improving the existing model’s performance by tuning its hyperparameters.\n:::\n"},"Articles/Scalar/Intro-to-Conditional-GANS":{"title":"Intro to Conditional GANS","links":["KB/One-hot"],"tags":["article"],"content":"Introduction to Conditional GANs\n:::section{.abstract}\nOverview\nGenerating novel images from an image dataset has been a dream in Computer Vision. Being able to influence the generation of these images was made possible by a family of GANs named Conditional GANs. The following article explores these CGANs and shows how the DCGAN was modified to have the ability to control latent space traversal to a certain extent. We also look at the training paradigm and cover some challenges we might encounter while training a CGAN on our data.\n:::\n:::section{.scope}\nScope\n\nThis article provides an overview of Conditional GANs (CGANs) and their ability to influence the generation of images.\nIt talks about the differences between CGANs and normal GANs.\nIt provides an explanation of the architecture of a CGAN and the added components for controlling feature generation.\nIt discusses the loss functions and the training paradigm used in CGANs.\nIt provides an overview of challenges that may arise when training a CGAN on specific data and how to potentially overcome them.\n:::\n:::section{.main}\n\nPre-requisites\nBefore understanding CGANs, we need to understand some concepts.\nZ Space\nConsider the task of generating new faces from a large dataset of faces. To create a new face, we can take an average of all the faces.\nNow, to create a new face, we need to tweak the average face a little. We could make the nose a little longer, the mouth smaller and so on. Our average face would also have the same features as human faces but be in between all the faces.\nThese transformations can be thought of as a kind of interpolation. In essence, we consider the faces as vectors and instead of moving from one to the other, we move to an intermediate point in between.\nWe get new faces depending on which “direction” we choose to interpolate between these face vectors.\nThis transformation enables CGANs to have more control over what they generate.\nIntroduction\nCGANs are a variant of GANs that allow for greater control over the features of the generated images. They build upon the DCGAN architecture, which is a popular architecture for GANs, and have a similar architecture with some small differences. The main difference between a CGAN and a DCGAN is the ability of a CGAN to control which features are modified in the generated images. This is achieved by conditioning the generator with additional information, such as labels, during the training process.\nThis added complexity allows the CGAN to generate images with specific features, such as generating a specific type of fruit with certain characteristics. By providing the generator with more information, it can create a more diverse set of images while still preserving the features that we want. Additionally, the CGAN has a better ability to generalize to unseen data, which is an essential aspect when it comes to image generation tasks. Overall, the CGAN architecture is an important step forward in the field of GANs, as it allows for more control over the generation process and improves the quality of the generated images.\nThis article takes an in-depth look at CGANs.\nWhat is a conditional GAN?\nA Conditional GAN (CGAN) is a modification of the DCGAN architecture that allows for more control over the features of the generated images. Unlike the DCGAN, a CGAN can selectively modify certain features of the generated image by conditioning the generator with additional information, such as labels, during training. This input allows the generator to generate images consistent with the conditioning information and specific characteristics that the labels represent. The differences between a CGAN and a DCGAN come from changes to the training methodology. In a CGAN, the generator is guided by the labels during training to traverse the latent space, the lower-dimensional space of random noise input that the generator uses to create images. These labels give the CGAN more control over what it generates by allowing it to traverse the latent space more precisely and be guided by the labels.\nAn example would include choosing specific features of the face to modify.\n[IMAGE {0} { CGAN Example } START SAMPLE]\n\n[IMAGE {0} FINISH SAMPLE]\nHow is it different from GAN\nThere are a few significant differences between a GAN and a CGAN. This article compares the CGAN to the DCGAN, as the latter is the base for many advanced GANs. These differences are summarized in the following table.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGANCGANOutput features are not controllableOutput features can be controlledUnsupervisedSemi-SupervisedDiscriminator does not receive labelsDiscriminator requires labelsDiscriminator evaluates similarity between input and target imagesDiscriminator considers input and target images and their respective labels\nArchitecture\nThe overall architecture of a CGAN is similar to that of a DCGAN but with minor modifications.\nThe Discriminator architecture in a CGAN is similar to that of a DCGAN, consisting of several convolutional layers, batch normalization layers, and Leaky ReLU activation functions. However, in a CGAN, the Discriminator receives an additional input, the conditioning information, such as labels, and the generated image. This extra input allows the Discriminator to consider both the image’s realism and the consistency of the conditioning information when evaluating the generated image.\nThe Generator architecture in a CGAN is similar to that of a DCGAN, consisting of several transpose convolutional layers, batch normalization layers, and ReLU activation functions. However, in a CGAN, the Generator receives an additional input, the conditioning information, and the random noise used as the latent code. This input allows the Generator to generate images consistent with the conditioning information.\n[IMAGE {1} { CGAN Architecture } START SAMPLE]\n\n[IMAGE {1} FINISH SAMPLE]\nThe Discriminator’s network\nThe Discriminator in a CGAN has a similar architecture as the DCGAN, consisting of several convolutional layers, batch normalization layers, and Leaky ReLU activation functions. However, an additional hot-encoding layer is added to consider the current image’s labels. This layer is used to represent the conditioning information, such as the labels, in a format that the network can process. This addition allows the Discriminator to consider both the image’s realism and the consistency of the conditioning information when evaluating the generated image. Thus it can guide the generator to create images with specific characteristics.\n[IMAGE {2} { Discriminator Architecture } START SAMPLE]\n\n[IMAGE {2} FINISH SAMPLE]\nThe Generator’s network\nThe Generator in a CGAN also has a similar architecture as the DCGAN, consisting of several transposed convolutional layers, batch normalization layers, and ReLU activation functions. However, it also has an additional layer added to take into consideration the labels. This added layer helps the generator create images with certain characteristics and make them more consistent with the conditioning information.\nLoss functions\nTo train the network, we use two loss functions for the Generator and the Discriminator of the CGAN, respectively.\nGenerator Loss\nSince the Generator’s objective is to create better fake images gradually, it needs to minimize the difference between the predicted image and the target. The model uses One hot encoded label in this architecture to decide which features to care for. The loss function thus is the following.\n\\mathcal{L}^{(G)}(\\theta^{(G)}, \\theta^{(D)}) = - \\mathbb{E}_{z} log \\mathcal{D} (\\mathcal{G} (z|y’))\nThe equation can be thought of as: given a label, use the Generator to traverse the latent space and create an image. Then pass the created image through the Discriminator and find how well we performed.\nDiscriminator Loss\nSince the objective of the Discriminator is to label the generated images, its outputs are the probability that the image is true. The loss function is, therefore, the following. (This is a Binary Cross Entropy Loss function)\n \\mathcal{L}^{(D)}(\\theta^{(G)}, \\theta^{(D)})= - \\mathbb{E}_{x \\sim p_{data}}log \\mathcal{D}(x|y) - \\mathbb{E}_{z} log (1- \\mathcal{D}(\\mathcal{G}(z|y&#039;)))\nTraining\nTraining a CGAN is similar to training any other GAN. The Discriminator and the Generator work parallel to create novel images and identify how real they look. The Generator first starts with random noise and passes it to the Discriminator. The Discriminator, in this case, is also provided with labels and returns a probability of how real it thinks the generated image is. This probability is passed to the Generator, which updates its weights to generate better images slowly. This cycle continues until the required quality of images is generated.\nTraining Flow\nTraining a CGAN is similar to training any other GAN, the main difference being the use of conditioning information to guide the network in generating specific features. We can break down the process into several steps:\n\nThe Generator starts with a random noise, often called a latent code, as its input. This noise is passed through the generator network, which maps it to an image in the data space.\nThe generated image and its corresponding conditioning information, such as labels, are passed to the Discriminator. The Discriminator evaluates the image based on its similarity to the real images and the consistency of the conditioning information. It returns a probability of how realistic the generated image is.\nThe probability is passed back to the Generator, which uses it to update its weights to generate better images. - The weights are updated to minimize the difference between the generated and real images.\nThis process is repeated for several iterations until the Generator can produce images that are of the desired quality. During this process, the Discriminator and Generator improve each other, with the Discriminator getting better at identifying realistic images and the Generator getting better at producing realistic images.\n\nOne important thing to note is that, during the training process, we must provide the Discriminator with real images and the conditioning information. The additional information allows the Discriminator to make informed decisions about the realism of the generated images and to detect if the conditioning information is consistent with the generated image.\nChallenges with Conditional generation\nNo model is perfect. CGANs have multiple issues as well. Some of these have been tackled by later research, while others are still active research areas.\n[IMAGE {3} { Discriminator Training Flow } START SAMPLE]\n\n[IMAGE {3} FINISH SAMPLE]\n[IMAGE {4} { Generator Training Flow } START SAMPLE]\n\n[IMAGE {4} FINISH SAMPLE]\nFeature Correlation\nAnother challenge with CGANs is related to the correlation of features in the dataset, which can cause the generated images to be biased toward certain characteristics. For instance, when working with an image dataset of fruits, if the dataset contains images of mostly red apples, the output of the CGAN will also be biased toward generating red apples. Similarly, if the dataset contains mostly pears with spots, the CGAN will generate images of pears with spots.\nThis feature correlation can cause problems when we want to generate an image of a fruit with certain characteristics that do not align with the features of the images in the dataset. For example, if we want to generate an apple with the spots of a pear, the CGAN might modify the entire apple instead of just the spots because these features are tightly linked in the dataset. This problem can make generating images with specific characteristics difficult or even impossible.\nOne solution to this problem is to use larger and more diverse datasets that contain a wide range of images with various features. These datasets will help reduce feature correlation and increase the range of images the CGAN can generate. Pre-processing, such as data augmentation or removing correlation between features, might also help mitigate this issue.\nZ-Space Entanglement\nThe Z-Space is the space of all possible points where a generator could generate an image. In other words, it is the space of all possible random inputs to the generator.\nWhen a generator generates an image, it takes a point from the Z-space and maps it to an image in the data space. The idea is that the generator learns a mapping from the latent space to the data space to generate new images by sampling from the latent space. However, this process is not always straightforward, as the latent space may need to be better structured or may not be entirely controllable.\nOne of the challenges of working with the latent space is that it is often high-dimensional, which makes it difficult to visualize and understand. Additionally, the space may not be smoothly connected, resulting in “entanglement,” where the generator produces unexpected results when the latent space is traversed. Entanglement occurs when interpolation between examples becomes hard to perform and can lead to difficulties in controlling the generation process, making it difficult to generate specific images.\nClassifier Gradients\nAnother challenge with CGANs is related to the ability of the network to identify and modify specific features in the generated images. For the CGAN to make changes in a specific way, it must first be able to understand and interpret what we want it to modify. For example, in the case of generating fruits, if we want to generate an image of a fruit with a longer stem, the network must first be able to understand what a stem is and what it means for it to be longer.\nThis understanding can be difficult for the network because the feature we want to modify may not always be clearly defined, or the network may need more information to understand. In some cases, the network may generate images different from what we want because it may interpret our request differently or need more information to make the desired change. Additionally, the feature we want to change might be harder to identify in the images. Hence the network might need help to learn it.\nOne solution to this problem is to use more explicit and detailed conditioning information, such as annotations or labels, to guide the network in identifying and modifying specific features. For example, in the case of generating fruits, we could provide the network with detailed annotation data on the fruits, including information on the size, shape, and position of the stem. However, this approach requires a large amount of labeled data, which may only sometimes be available or can be time-consuming to acquire.\nOverall, being able to modify specific features in the generated images is a challenging task for CGANs and requires careful consideration of the available data and the network architecture used.\n:::\n:::section{.summary}\nConclusion\n\nIn conclusion, this article discussed Conditional GANs (CGANs), a family of GANs that allow for control over the features modified in the generated image.\nThe CGAN builds upon the DCGAN architecture and mainly differs in its training methodology, which allows for traversing the latent space by conditioning the generator with labels during training.\nThe article also compared CGANs to DCGANs, highlighting the key differences between the two, such as the output feature control and the semi-supervised nature of CGANs.\nThe article also provided an overview of CGAN architecture and the challenges one may face while training a CGAN.\nOverall, the article offers a comprehensive introduction to the concept and application of Conditional GANs, making it a valuable resource for anyone interested in this area of computer vision.\n:::\n"},"Articles/Scalar/Masked-Language-Modeling-in-BERT-scalar":{"title":"Masked Language Modeling in BERT","links":["KB/Masked-Language-Modeling","tags/scalar"],"tags":["article","scalar"],"content":"Masked Language Modeling in BERTscalar\nmasked language model explained\n:::section{.abstract}\nOverview\nLanguage modelling is a massive domain and has many sub-research areas. One such domain involves understanding contextual information about words in a sentence. This task can be done in many ways, and masked language modelling is one such method. In the past few years, Transformer based models have reached SOTA(state of the art) in many NLP domains. BERT is one such model. In this article, we will understand how to train BERT to fill in missing words in a sentence using MLM.\n:::\n:::section{.scope}\nScope of the Article\nThis article covers the following topics:\n\nMasked Language Modeling explained in an easy-to-understand manner\nQuickly recap all the pre-requisite terms to build an MLM model\nGo over some libraries that are essential to MLM\nHow to build a BERT-based MLM model using our data in Tensorflow/Keras\n\n:::\n:::section{.main}\nMasked Language Modeling Explained\nTo a NN model, the word context has no meaning. So, we need to find ways to make the model consider surrounding words to learn which context words appear. For example, consider the sentence, “I am eating an ice cream”. In this, the “ice cream” is being “eaten”. What would an appropriate word be if we now remove the word “eating” and have the sentence as “I am ___ an ice cream”? We can consider something along the lines of “licking”, “eating”, “sharing”, etc. However, we cannot say “drowning”, “cycle”, “chair”, or other random words.\nIn the same way, to ensure the model learns which word is appropriate, it needs to understand the structure of language. As modellers, we need to help it do so.\nQuite simply, all we do is give the model inputs with blanks as a “token” &lt;MASK&gt; along with the word that should be there. We can create data in this format by taking any text and running over it. How to do so will be explained later on.\nQuick Recap\nThis section gives a small recap of all the important concepts we need to understand the code.\nTokens\nTokens can be considered parts of a sentence that contribute to understanding. A sentence such as “I am Jane Austen” can have the tokens [&quot;I&quot;, &quot;am&quot;, &quot;Jane&quot;, &quot;Austen&quot;]. As a computer does not understand words, we need to convert these to numerical values. To do so, we give each word a unique ID. Something like [100, 101, 102, 103] etc.\nAttention\nOne of the most important concepts in Machine learning today, attention mechanisms give models the power to decide where to look in an input. Compared to CNNs, this makes models such as Transformers perform much better as it simultaneously learns which parts of the input would be better along with what the input could be classified.\nTransformers\nTransformers are a large family of models that employ different variants of attention mechanisms. They first started in NLP but have also branched out to computer vision due to their immense potential. They generally have an Encoder-Decoder architecture with multiple “heads”. Each head is responsible for its attention.\nBERT\nBERT is part of a sub-family of transformer architectures made for NLP and led to a sea of other new models that reached SOTA. It can be used in many different contexts, especially MLM, next-word prediction, question answering and many more.\nTransfer Learning\nA training paradigm that changed the DL world by allowing any researcher with limited resources to use SOTA models for inference and fine-tune them to their needs. A model like BERT requires Terabytes of data before it can be trained to extremely high levels. This level of computing is almost impossible for most people to have apart from companies with massive funding and resources. However, now we can use their trained models and make them work for our tasks.\nStopwords\nWords that do not add much value to the model but are repeated enough times for it to become a problem. These are generally removed before modelling.\nMLM vs CLM vs Word2Vec\nThe major difference between MLM and CLM is that CLM can only take into account words that occur before it in a sentence, unlike MLM, which is bi-directional. This difference means that CLM does better for generating large amounts of text. However, MLM is better for contextually understanding text (refer to the Masked Language Modeling Explained section).\nWord2Vec, on the other hand, has similar ideas but the embeddings it generates have weaker contextual information than Transformer based models. The outputs it produces can also be used as a part of BERT training, although it is not usually required.\nLibraries We Need\nWe need a few libraries to make this work.\nTensorflow/Keras\nTF is one of the major DL libraries in Python. We will be using it for training our model.\nHugging Face Transformers\nThis library is one of the recent developments in the open-source community. It is a database of trained models and datasets that can be directly used in any codebase. They have thousands of tasks, making it extremely easy to get results fast. We will use their pre-trained BERT model.\nNLTK\nA text processing library that we will use to clean up our text before passing it to the model.\nSeaborn/Matplotlib\nThese libraries are used for plotting our training performance.\nImplementation\nNow for the code.\nImports\nWe first import all the required packages. We also download the stopwords and punctuation data from nltk.\n“`python\nimport nltk\nnltk.download(‘stopwords’)\nnltk.download(‘punkt’)\nnltk.download(‘gutenberg’)\nimport string\nfrom nltk.corpus import stopwords\nimport seaborn as sns\nfrom transformers import BertTokenizer, TFBertForMaskedLM\nimport tensorflow as tf\nimport numpy as np\nimport re\nimport matplotlib.pyplot as plt\nsns.set()\n\n### Data\nFor this demo, we use text from the book &quot;Emma&quot; by &quot;Jane Austen&quot;. This dataset is a public domain dataset from Project Gutenberg that comes with nltk. We can download it using the following code. \n&quot;`python\nimport nltk\nnltk.download(&#039;gutenberg&#039;)\n!cp /root/nltk_data/corpora/gutenberg/austen-emma.txt sample.txt\n\nWe can also use custom text by creating a text file called “sample.txt” in the same directory as the code and pasting whatever we want. (Make sure it is English text).\nWe then pre-process the data by removing stopwords and punctuation and converting the words into tokens BERT needs. Since every Transformer model has their configuration of tokens in the pre-trained model, we will use the tokenizer that Hugging face provides us.\n(Note: Here, we only take the first 1000 lines from the text. Language models take a long time to train; this is just a demo. If we have GPUs, the entire data can be used.)\n“`Python\ntokenizer = BertTokenizer.from_pretrained(‘bert-base-uncased’)\nsw = stopwords.words(‘english’) # this is a list of stopwords\nTake first 1000 sentences for demo purposes\nwith open(‘sample.txt’, ‘r’) as f:\nlines = f.readlines()[:1000]\nRemove new lines, convert all to lowercase, remove punctuation and stop words and tokenize\nlines = [line.rstrip(‘\\n’).lower() for line in lines]\nlines = [line.translate(str.maketrans(”, ”, string.punctuation)) for line in lines]\ndef rem_stops_line(line, words):\n# Function to remove stopwords per line if they are present in the line\nif len(line) &gt;1:\nreturn [w for w in line if w not in words]\nelse:\nreturn line\ndef remove_stops(text, words = sw):\n# Remove stop words for an entire text. Separate functions make it easier to parallelize if required.\nreturn [rem_stops_line(line, words) for line in text]\nfiltered_lines = remove_stops(text = lines, words = sw)\ninputs = tokenizer(lines,max_length=100,truncation=True,padding=‘max_length’,return_tensors=‘tf’)\n\n### Masked Language Model Explained Further\nWe use the model from the Transformers library directly. The uncased model converts all text into lowercase. Other models do not, and any of them can be used. This one was chosen for the sake of this demo.\n&quot;`Python\nmodel = TFBertForMaskedLM.from_pretrained(&#039;bert-base-uncased&#039;)\n\nIn Masked Language Modeling, we explained that every sentence needs to be converted to a format with words masked using a special token, &lt;MASK&gt;. We can do that by using the tokenized words and making the model aware of which token number corresponds to this special token. (In this case, it is 103). In the original paper, token numbers 101 and 102 were replaced, but we ignore that here. (It is not relevant for now.)\n“`Python\nMASK\ninp_ids = []\nlbs = []\nfor idx, inp in enumerate(inputs.input_ids.numpy()):\ncurrent_tokens = list(set(range(100)) -\nset(np.where((inp  101) | (inp  102)\n| (inp == 0))[0].tolist()))\n# Find number of tokens to mask\nnum_token_masked = 0.15 * len(current_tokens)\ntoken_to_mask = np.random.choice(np.array(actual_tokens),\nsize=int(num_token_masked),\nreplace=False)\n# Store special token and inform model\ninp[token_to_mask.tolist()] = 103\ninp_ids.append(inp)\nConvert the tokens to tensor format\ninp_ids = tf.convert_to_tensor(inp_ids)\ninputs[‘input_ids’] = inp_ids\n\n### Training the Model\nNow that we have all the required data and the model, we need to train it on our data.\nIf the system does not have a GPU or access to a cloud GPU is unavailable, this model will take a very long time to train. Consider using lesser data.\n\nConsidering we have a GPU, we first check if TF can find it.\n&quot;`Python\nprint(&#039;GPU name: &#039;, tf.config.experimental.list_physical_devices(&#039;GPU&#039;))\n\nWe use a Sparse Categorical Crossentropy loss with logits enabled. (logits are enabled if the model does not end with a Softmax. BERT does not.). We use a learning rate of 1e-3 for the Adam Optimizer.\nFinally, we run training for around ten epochs.\n“`Python\nCompile and Train\nlr = 1e-3\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),loss=loss)\nhistory = model.fit([inputs.input_ids,inputs.attention_mask],inputs.labels,verbose=1,batch_size=16,epochs=10)\n\n### Loss\nWe plot the loss per epoch to see if our model is learning anything.\nWe can see that the loss has decreased, which is a good sign! Our model is learning. More data and longer training will help the model be better than before.\n\n### Prediction\nJust training a model is useless. We need to be able to use it for prediction. To do that, we need to define a few functions. We need to be able to find the `&lt;MASK&gt;` tokens in the sentence, we need to tokenize the sentence and pass it into the model for prediction. And finally, we need to do this for multiple sentences. \nThe following functions do exactly these. \n\n```python\ndef find_masks(inp):\n    # Find position of the masks in a sentence\n    return np.where(inp.input_ids.numpy()[0] == 103)[0].tolist()\n\ndef single_prediction(model, inp, mask_loc):\n    # Prediction for all the positions of the masks\n    return model(inp).logits[0].numpy()[mask_loc]\n\ndef return_prediction(model, query):\n    # Return a prediction for a single sentence\n    inp = tokenizer(query,return_tensors=&#039;tf&#039;)\n    mask_loc = find_masks(inp)\n    # Find prediction with the highest confidence\n    predicted_tokens = np.argmax(single_prediction(model, inp, mask_loc),axis=1).tolist()\n    # Decode the numerical value of the returned ID back to the word \n    return tokenizer.decode(predicted_tokens)\n\ndef multiple_preds(model, query_list):\n    # Return predictions for a list of queries\n    preds = [f&quot;{x} -&gt; {return_prediction(model, x).split(&#039; &#039;)}&quot; for x in query_list]\n    for i in preds: print(i)\n\nPredictions\nAfter training, we can finally give the model a practice exam! Since we fine-tuned it on the book “Emma”, we give it the following sentences. [&quot;too well [MASK] for her&quot;, &quot;nice to [MASK] her&quot;, &quot;Emma [MASK] a girl who wanted a [MASK]&quot;]\n“`Python\nquery_list = [“too well [MASK] for her”, “nice to [MASK] her”, “Emma [MASK] a girl who wanted a [MASK]”]\nmultiple_preds(model, query_list)\n\nAs an output we get the following. We see that it performs quite well even with such a short training! \n\n&quot;`plaintext\ntoo well [MASK] for her -&gt; [&#039;suited&#039;]\nnice to [MASK] her -&gt; [&#039;see&#039;]\nEmma [MASK] a girl who wanted a [MASK] -&gt; [&#039;was&#039;, &#039;chance&#039;]\n\nFurther Tips\n\nLanguage models are generally very heavy to use. If possible, using Mixed Precision training helps\nHaving more GPU memory is more useful than having a faster GPU for language models\nMultiple GPUs are useful for expanding available memory\nThere are smaller variants of BERT that use less memory.\nHugging Face has a huge list of models that we can use. Trying them might lead to better results.\nTo get over the overwhelming number of pre-trained models, pick the task and find benchmarks in that task. PaperswithCode is a great place to start.\n\n:::\n:::section{.summary}\nConclusion\nThis article showed us masked language modelling explained. We learnt the following:\n\nWhat MLM is and when to use it.\nHow to pre-process our data for MLM\nHow to fine-tune pre-trained BERT models for MLM\nHow to perform predictions over multiple sentences with our fine-tuned models.\n\n:::"},"Articles/Scalar/Reconstructing-the-MNIST-images-using-an-autoencoder":{"title":"Reconstructing the MNIST images using an autoencoder","links":["KB/Downsampling"],"tags":["article"],"content":"Reconstructing MNIST images using an Autoencoder\n:::section{.abstract}\nOverview\nGiven noisy images, an Autoencoder is a family of models that can convert these noisy images to their original form. These models are unsupervised and use an Encoder-Decoder architecture to re-create the original images given noisy variants of the same. In the process of re-creation, the model also learns to model the latent space of the dataset.\n:::\n:::section{.main}\nScope\n\nThis article explores the concepts required to build a simple Autoencoder.\nIt explains how to implement an Autoencoder using Tensorflow.\nIt explains how to use an Autoencoder to re-create MNIST images.\nThe article also explains how to train an Autoencoder model and use the trained model for inference.\n\nWhat are we building?\nIn this article, we will build an Autoencoder in Tensorflow that can re-create MNIST images. We will create functions to load and pre-process the dataset, along with creating noisy versions of the data points. We will create the Encoder-Decoder structure of the Autoencoder and then use the noisy and real images as inputs to it. After training the model, we will use it to generate new images.\nThe following sections elaborate on these points.\nPre-requisites\nBefore moving to the implementation, we must understand some prerequisite terms.\nTransposed Convolution\n2D Convolutions compress information from images into smaller representations by Downsampling them. Transposed Convolutions perform the opposite operation. These convolutions take compressed/small images and attempt to expand their sizes. An illustration of how this happens is as follows.\n[IMAGE {1} Transposed Conv START SAMPLE]\n\n[IMAGE {1} FINISH SAMPLE]\nWhat are Autoencoders\nAutoencoders are models that we created to reduce noise in images. In attempting to learn how to reduce noise, they model the latent space of the dataset. Any architecture that understands the latent space can then re-create the original forms of the images from the noisy variants. In the process, the model can not only act as an unsupervised classifier but also be used to generate new images. AutoEncoders have an Encoder-Decoder structure where the Encoder compresses the image while the Decoder re-creates the original image from the compressed representation.\nHow are we going to build this?\nTo build an autoencoder that can recreate MNIST images, we will be using the Tensorflow library. We will create functions to load the MNIST dataset and pre-process it. We will also need to create a random noise generator and a function to plot a batch of images. Once we have these functions, we can create the Autoencoder. The network structure follows an Encoder-Decoder pattern, and we will explore how to create that using Tensorflow. We will then train the Autoencoder on the MNIST data and use the trained model to re-create examples from the dataset.\nThe sections below elaborate on these steps.\nFinal Output\nThe final output of the model should be a near-perfect representation of the MNIST dataset. After training the model for a few epochs. The first row of images is the training data, while the second row contains the images generated by the Autoencoder. These rows are almost the same, which shows that our model has done a good job.\n[IMAGE {2} Final Output START SAMPLE]\n\n[IMAGE {2} FINISH SAMPLE]\nRequirements\nWe must import some libraries and write a few functions to create a model that can read and re-create mnist images.\nSince we will be using the Tensorflow library, we import it and its other useful components. We import the numerical processing library numpy and also a plotting library matplotlib.\nfrom tensorflow.keras.datasets import mnist\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras import layers\nimport numpy as np\nfrom tensorflow.keras.models import Model\nimport tensorflow as tf\nWe also need to write some helper functions. The first function takes an array as an input and reshapes it to the size that the model requires.\ndef data_proc(dat):\n    larr = len(dat)\n    return np.reshape(dat.astype(&quot;float32&quot;) /255.0 , (larr, 28,28,1))\nThe second helper function is used to take an array, add Gaussian noise, and ensure that the values lie in the range (0,1).\ndef gen_noise(array):\n    noisy_array = array + 0.4 * np.random.normal(\n        loc=0.0, scale=1.0, size=array.shape\n    )\n    return np.clip(noisy_array, 0.0, 1.0)\nTo understand if our model is performing well, we will need to display batches of images. The third function takes two arrays - the input array and the predicted images array and plots them in two rows.\ndef display(array1, array2):\n    n = 10\n    indices = np.random.randint(len(array1), size=n)\n    images1 = array1[indices, :]\n    images2 = array2[indices, :]\n    plt.figure(figsize=(20, 4))\n    for i, (image1, image2) in enumerate(zip(images1, images2)):\n        ax = plt.subplot(2, n, i + 1)\n        plt.imshow(image1.reshape(28, 28))\n        plt.gray()\n        ax.get_xaxis().set_visible(False)\n        ax.get_yaxis().set_visible(False)\n        ax = plt.subplot(2, n, i + 1 + n)\n        plt.imshow(image2.reshape(28, 28))\n        plt.gray()\n        ax.get_xaxis().set_visible(False)\n        ax.get_yaxis().set_visible(False)\n    plt.show()\nBuilding the AutoEncoder\nThe following sections explain how to create a simple Autoencoder in Tensorflow and use MNIST images to train it. We will first see how to load the MNIST data and preprocess it for our needs. After converting the data to the right format, we will build and train the model.\nThe architecture of the network is split into three major parts - the Encoder, the Bottleneck, and the Decoder. The Encoder is used to compress the input images while also preserving useful information. The Bottleneck chooses which features are relevant to flow through to the Decoder, and the Decoder re-creates the images using the outputs of the Bottleneck. The Autoencoder attempts to learn the latent space of the data in the process of this reconstruction.\nThe architecture diagram of an autoencoder is shown below.\n[IMAGE {3} arch START SAMPLE]\n\n[IMAGE {3} FINISH SAMPLE]\nPreparing the dataset\nThe MNIST dataset is already included with Tensorflow as a split dataset so we can load it directly. We use the default splits into train and test datasets and then pass them to the pre-processing function we defined earlier.\nThe second half of the inputs to the model are noisy versions of the original MNIST images. We use the gen_noise function we defined before to create these noise images. Note that the larger the noise, the more distorted the image gets, and the harder the model must work to re-create them.\nWe will also visualize the noise data alongside the original.\n[IMAGE {4} Data Visualized START SAMPLE]\n\n[IMAGE {4} FINISH SAMPLE]\n(ds_train, _), (ds_test, _) = mnist.load_data()\nds_train,ds_test = data_proc(ds_train), data_proc(ds_test)\n \nnoisy_ds_train, noisy_ds_test = gen_noise(ds_train), gen_noise(ds_test)\n \ndisplay(ds_train, noisy_ds_train)\nDefining the Encoder\nThe Encoder of the network uses blocks of Convolutions and Max Pooling layers with ReLU activations. The objective is to compress the input data before passing it through the network.\nThe output of this part of the network should be a compressed version of the original data.\nSince the MNIST images are of shape 28x28x1, we create an input with that shape.\ninput = layers.Input(shape=(28, 28, 1))\n \n# Encoder\nx = layers.Conv2D(32, (3, 3), activation=&quot;relu&quot;, padding=&quot;same&quot;)(input)\nx = layers.MaxPooling2D((2, 2), padding=&quot;same&quot;)(x)\nx = layers.Conv2D(32, (3, 3), activation=&quot;relu&quot;, padding=&quot;same&quot;)(x)\nx = layers.MaxPooling2D((2, 2), padding=&quot;same&quot;)(x)\nDefining the Bottleneck\nUnlike the other two components, the Bottleneck does not need to be explicitly programmed. Because the output of the Encoder’s final MaxPooling layer is very small, the Decoder must learn to recreate the images using this compressed representation.\nIn more complex implementations of Autoencoders, modifying the Bottleneck is also an option.\nDefining the Decoder\nThe Decoder comprises Transposed Convolutions with a stride of 2. The final layer of the model is a simple 2D convolution with the sigmoid activation function.\nSince this part of the network is used to recreate images from the compressed representation, upsampling is done using the Transposed Convolution. Larger strides are used for upsampling the images in fewer steps.\n# Decoder\nx = layers.Conv2DTranspose(32, (3, 3), strides=2, activation=&quot;relu&quot;, padding=&quot;same&quot;)(x)\nx = layers.Conv2DTranspose(32, (3, 3), strides=2, activation=&quot;relu&quot;, padding=&quot;same&quot;)(x)\nx = layers.Conv2D(1, (3, 3), activation=&quot;sigmoid&quot;, padding=&quot;same&quot;)(x)\nTraining the model\nAfter defining the model, we must compile it with the Optimiser and the loss function. This article will use the Adam Optimiser and a Binary Cross Entropy loss function.\n# conv_autoenc_model\nconv_autoenc_model = Model(input, x)\nconv_autoenc_model.compile(optimizer=&quot;adam&quot;, loss=&quot;binary_crossentropy&quot;)\nconv_autoenc_model.summary()\nAfter we compile the model, we can finally train it on the modified MNIST images we generated at the start of the article. We will train the model for 50 epochs with a batch size of 128. We also pass the validation data to the model.\nconv_autoenc_model.fit(\n    x=ds_train,\n    y=ds_train,\n    epochs=50,\n    batch_size=128,\n    shuffle=True,\n    validation_data=(ds_test, ds_test),\n)\nReconstructing images\nAfter training the model, we can use the trained model to generate predictions. We display the re-created images using the function we wrote previously.\npreds = conv_autoenc_model.predict(ds_test)\ndisplay(ds_test, preds)\n:::\n:::section{.summary}\nConclusion\n\nIn this article, we implemented a simple Autoencoder to re-create the MNIST image dataset.\nWe learned how to load and pre-process the MNIST images to make them fit the Autoencoder model.\nWe explored the architecture of the network and understood how to implement it using Tensorflow.\nFinally, we learned how to train the Autoencoder and use it to generate new images.\n:::\n"},"Articles/Scalar/Siamese-Network":{"title":"Siamese Network","links":["KB/Dropout"],"tags":["article"],"content":"Siamese Network\n:::section{.abstract}\nOverview\nThe lack of data is a huge hurdle for any Deep learning task. Finding large datasets is nearly impossible in domains such as facial recognition, signature verification, text similarity, and many others. In many cases, a single example of each class is present. A regular CNN fails to perform in these cases, but if a network learns to minimize a similarity metric between images, it can easily perform this task. The Siamese Network is a class of architectures that excel at this one-shot learning task.\nThis article explains the workings behind the Siamese Network and provides an implementation for Signature Verification.\n:::\n:::section{.scope}\nScope\n\nThis article explains the concept of Siamese Networks and why they are useful.\nIt explains the multiple loss functions behind Siamese Networks.\nIt talks about few-shot learning and how to apply it to tasks.\nIt talks about some of the use cases of Siamese Networks.\nThe article also explains how to create a Siamese Network for a Signature Verification task using PyTorch.\n:::\n:::section{.main}\n\nIntroduction\nSiamese networks are a one-shot classification paradigm where only a single example is enough for the network to classify images accurately. This network uses the concept of Contrastive Loss, which finds a pairwise similarity score between the images in the Dataset. Instead of learning the content of the images, the Siamese network learns the differences and similarities between them. This unique learning paradigm makes these networks much more robust to the lack of data and improves performance without needing domain-specific information.\nSignature verification is a task in which these networks excel. This task aims to identify forged signatures given a single signature sample for thousands of people. This task is quite challenging due to the vast differences between individual signatures and the need for more training data.\nIn this article, we will explore the task of Signature Verification using these Siamese Networks and create a working model using PyTorch.\nWhat Are Siamese Networks?\nSiamese Networks are a family of networks that uses two identical subnetworks for one-shot classification. The sub-networks share the same configuration, parameters, and weights but have different inputs. Unlike a regular CNN that learns to predict multiple classes using vast amounts of data, a Siamese Network learns a similarity function. We can use the learned function to differentiate between classes without needing a lot of data. These networks are specialized in one-shot classification, which means that in many cases, they only need a single example to classify images accurately.\nAs a real-life use case, Siamese Networks are applied to face recognition and signature verification tasks. Consider the face recognition task done for a company that wants to take an automated face-based attendance. The company would only have a single picture of its employees. A regular CNN would have been incapable of accurately classifying thousands of employees based on a single image of each of them. A Siamese network, on the other hand, excels at this task.\nExploring Few-Shot Learning\nFew shot models are a family of architectures that rely not on the number of training examples but on exploiting the differences between a small number of samples. This property allows them to make predictions ranging from a few samples to a single sample.\nThe advantage of few-shot learning comes into play when the training data is very small. For large datasets, this training paradigm could be more helpful.\nArchitecture of Siamese Networks\nThe objective of the Siamese network is to find similar inputs and magnify the differences between dissimilar pairs. The architecture of this network is shown in the figure below.\n[IMAGE {1} {Architecture} START SAMPLE]\n\n[IMAGE {1} FINISH SAMPLE]\nSome features that set Siamese networks apart from the usual CNN architecture are as follows.\n\nThe network has two different inputs. Each of these inputs is passed into identical subnetworks.\nThe inputs are passed through a Convolutional network first and then encoded.\nAny changes to one side of the network are reflected on the other.\nThe network returns an encoding that is a similarity score. This score can be used to differentiate between classes.\nThe network is a one-shot classifier and does not require a lot of examples per class.\n\nLoss Functions Used in Siamese Networks\nThe Siamese Network uses multiple loss functions. They are explained below.\nBinary Cross Entropy Loss\nThis loss is one of the common image classification loss functions. The Siamese network uses this loss to classify the image pairs as similar or dissimilar.\nContrastive Loss\nThe Contrastive Loss function finds the difference between image pairs by using distance as a similarity measure. This function is useful when there are few training examples per class.\nA caveat of using Contrastive loss is that it requires pairs of both negative and positive training samples. We can visualize this loss in the figure below.\n[IMAGE {2} {Contrastive Loss} START SAMPLE]\n\n[IMAGE {2} FINISH SAMPLE]\nThe Contrastive Loss equation is (1-Y) \\frac{1}{2} D^{2}_{w} + (Y) + \\frac{1}{2} (max(0, m - D^{2}_{\\omega}))\nWhen Y is 0, the inputs share the same class. When the value of Y is 1, they are from different classes.\nThe margin m defines the margin that the distance function uses to identify pairs that contribute to the loss. The value of m is always greater than 0.\nD denotes Euclidean distance.\nTriplet Loss\nThe triplet loss uses triples of data. These triples can be seen in the image below.\n[IMAGE {3} {Triplet Loss} START SAMPLE]\n\n[IMAGE {3} FINISH SAMPLE]\nThe objective of the triplet loss function is to maximize the distance between the anchor and the negative samples while minimizing the distance between the anchor and the positive samples.\nThis task is shown in the below image.\nThe Triplet loss is defined as L = max(d(a,p) - d(a,n) + margin, 0)\nBuilding a Signature Verification Model With Siamese Networks\nSignature verification is the task of identifying forged signatures given a dataset of real ones. For this task, a model has to learn the difference between hundreds of signatures. Given a fake or a real signature, the model has to differentiate between them.\nThis verification task is extremely hard for a regular CNN due to the complexity of changes and lack of training samples. In most cases, only a single signature is available per person, and the model needs to learn how to verify signatures for thousands of people.\nThe following sections explore building a model to tackle this task using PyTorch.\nDataset\nThe Dataset we will be using is a signature verification dataset known as ICDAR 2011. This Dataset contains Dutch signatures that are either forged or original. An example of the data is shown below.\n[IMAGE {4} {Examples} START SAMPLE]\n\n[IMAGE {4} FINISH SAMPLE]\nWe can download the Dataset from this drive link.\nDescription of problem statement\nThis article considers recognizing fake signatures as part of a signature verification problem. We aim to take a dataset of signatures and use a Siamese network to predict which of the test signatures belong to real people and which are forged.\nWe need to create a pipeline that reads the Dataset, creates image pairs, and passes them to the Siamese network. After training the network on the Dataset, we need to create functions for inference.\nImports\nTo create the Siamese Network, we need to import a few libraries. We import the Pillow library(PIL) for image processing. We will import the plotting package matplotlib, the numerical library numpy, and the progress bar library tqdm for other utilities. We will use Pytorch and torchvisionto train and build the network.\nfrom PIL import Image\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader, Dataset\nimport PIL.images\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.data as utils\nimport torchvision\nimport torchvision.transforms as transforms\nimport torchvision.utils\nfrom tqdm import tqdm\nUtility Functions\nTo visualize the network’s outputs, we create a function that takes the images and the labels as inputs and plots them in an easy-to-visualize grid.\ndef imshow(img, text=None, should_save=False):\n    npimg = img.numpy()\n    plt.axis(&quot;off&quot;)\n    if text:\n        plt.text(\n            75,\n            8,\n            style= &quot;italic&quot;,\n            fontweight= &quot;bold&quot;,\n            bbox={&quot;face color&quot;: &quot;white&quot;, &quot;alpha&quot;: 0.8, &quot;pad&quot;: 10},\n        )\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\nData preprocessing\nThe data structure used by the Siamese network is very different from the usual image classification networks. Instead of providing an image and a label, the Dataset Generator for the Siamese network must provide pairs of images. These pairs are converted to Black and white and are then resized and converted to Tensors.\nThere are two types of pairs - the positive pair, where both the inputs images are identical, and the negative pair, where the images are not identical.\nWe also create a function that returns the size of the Dataset when called.\ndir_train = &quot;train&quot;\ndf_train = &quot;train_data.csv&quot;\ndf_val = &quot;test_data.csv&quot;\ndir_val = &quot;test&quot;\nbs = 32\nnum_epoch = 20\n \nclass PairedDataset:\n    def __init__(self, df_train=None, dir_train=None, transform=None, load_subset = None):\n        self.train_df = PD.read_csv(df_train)\n        if load_subset!=None:\n            self.train_df = self.train_df[:load_subset]\n        self.train_df.columns = [&quot;image1&quot;, &quot;image2&quot;, &quot;label&quot;]\n        self.train_dir = dir_train\n        self.transform = transform\n \n    def __getitem__(self, index):\n        pair1 = os.path.join(self.train_dir, self.train_df.iat[index, 0])\n        pair2 = os.path.join(self.train_dir, self.train_df.iat[index, 1])\n \n        pair_left = Image.open(pair1).convert(&quot;L&quot;)\n        pair_right = Image.open(pair2).convert(&quot;L&quot;)\n \n        if self.transform is not None:\n            pair_left = self.transform(pair_left)\n            pair_right = self.transform(pair_right)\n \n        return (\n            pair_left,\n            pair_right,\n            torch.from_numpy(\n                np.array([int(self.train_df.iat[index, 2])], dtype=np.float32)\n            ),\n        )\n \n    def __len__(self):\n        return len(self.train_df)\n \nBrief Description of the features\nThe features that the network gets are pairs of images. There are positive or negative data pairs. Both the pairs are image data and are Tensor representations of the underlying images.\nThe labels provided to the Siamese network are categorical.\nStandardization of the features\nTo standardize the features, we first convert them to Black and White. We also resized all the images to be (105x105) square as the Siamese Network requires this size. Finally, we convert all the images to Tensors to improve performance and be able to use the GPU.\ntransform=transforms.Compose(\n        [transforms.Resize((105, 105)), transforms.ToTensor()]\n)\nSplitting the Dataset\nTo ensure that the model can be used for prediction and not just training, we split the Dataset into training and testing parts.\nFor simplicity, we only use the first 1000 data points. Setting the load_subset function to None would use the entire Dataset but take much longer. We do not perform Data Augmentation here, but that is an option to make the network perform better in the long run.\ntrain_ds = PairedDataset(\n    df_train,\n    dir_train,\n    transform=transforms.Compose(\n        [transforms.Resize((105, 105)), transforms.ToTensor()]\n    ),\n    load_subset=1000\n)\neval_ds = PairedDataset(\n    df_val,\n    dir_val,\n    transform=transforms.Compose(\n        [transforms.Resize((105, 105)), transforms.ToTensor()]\n    ),\n    load_subset=1000\n)\nNeural Network Architecture\nWe can create the architecture that we described above in a few steps.\nFirst, we create a function that creates blocks of Convolutions, Batch Normalisation, and ReLU with different input and output channels. We give this function the option of having a Dropout layer at the end or skipping that layer.\nWe also create another function that generates blocks of FC layers followed by ReLU layers.\nWe can use these functions to create the Sequential model that defines the Siamese Network. After creating the CNN part of the architecture using the functions we defined earlier, we have to create the FC part of the network. Note that different padding and kernel sizes are used across the network.\nThe FC part of the network is blocks of Linear layers followed by ReLU activations.\nOnce we have defined the architecture, we can run a forward pass for the data we pass to the network. Note that the view function is used to resize the output of the previous block by flattening dimensions. After creating this function, we can start training the Siamese network on the data.\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n \n        self.cnn1 = nn.Sequential(\n            self.conv_bn_relu(1, 96, 11, 1, False),\n            self.conv_bn_relu(96, 256, 5, 2, True),\n            nn.Conv2d(256,384 , kernel_size=3,stride=1,padding=1),\n            nn.BatchNorm2d(384),\n            nn.ReLU(inplace=True),\n            self.conv_bn_relu(384, 256, 3, 1, True),\n        )\n        \n        self.fc1 = nn.Sequential(\n            self.linrel(30976, 1024),\n            nn.Dropout2d(p=0.5),\n            self.linrel(1024, 128),\n            nn.Linear(128,2))\n    \n    def linrel(self, inc, outc): return nn.Sequential(nn.Linear(inc, outc), nn.ReLU(inplace=True))\n        \n    def conv_bn_relu(self,inc, outc, ks, pad,dropout = True):\n        if dropout == True:\n            return nn.Sequential(\n                nn.Conv2d(inc, outc, kernel_size=ks,stride=1,padding=pad),\n                nn.BatchNorm2d(outc),\n                nn.ReLU(inplace=True),\n                nn.MaxPool2d(3, stride=2),\n                nn.Dropout2d(p=0.3),\n            )\n        else:\n            return nn.Sequential(\n                nn.Conv2d(inc, outc, kernel_size=ks,stride=1),\n                nn.BatchNorm2d(outc),\n                nn.ReLU(inplace=True),\n                nn.MaxPool2d(3, stride=2),\n            )\n \n    def forward_once(self, x):\n        output = self.cnn1(x)\n        output = output.view(output.size()[0], -1)\n        output = self.fc1(output)\n        return output\n \n    def forward(self, input1, input2):\n        out1 = self.forward_once(input1)\n        out2 = self.forward_once(input2)\n        return out1, out2\nLoss Function\nThe loss function that the Siamese Network uses is contrastive loss. We can define this loss using the equations mentioned earlier in the article.\nTo improve code performance, instead of defining the loss as a simple function, we inherit from nn.Module and create a class that returns the outputs of the function. This wrapper will allow PyTorch to optimize the code for better runtime performance.\nclass ContrastiveLoss(nn.Module):\n    def __init__(self, margin=2.0):\n        super(ContrastiveLoss, self).__init__()\n        self.margin = margin\n \n    def forward(self, out1, out2, label):\n        euclidean_distance = F.pairwise_distance(out1, out2)\n        return torch.mean(\n            (1 - label) * torch.pow(euclidean_distance, 2)\n            + (label)\n            * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)\n        )\nTraining the Siamese Network\nNow that we have loaded and cleaned up the data, we can start training the Siamese network using it. To do so, we first create the training and testing data loaders. Note that the evaluation DataLoader has a batch size of 1 as we want to perform one-by-one evaluations.\nWe then send the model to the GPU and define the Contrastive Loss and the Adam optimizer.\ndl_train = DataLoader(train_ds,\n                        shuffle=True,\n                        num_workers=8,\n                        batch_size=bs) \ndl_eval = DataLoader(eval_ds,\n                        shuffle=True,\n                        num_workers=8,\n                        batch_size=1) \n \nnet = Model().cuda()\ncriterion = ContrastiveLoss()\noptimizer = torch.optim.Adam(net.parameters(), lr=1e-3, weight_decay=0.0005)\nWe then write a function that takes the train DataLoader as an argument. We keep a running array of the loss and the counter to plot it later. After that, we iterate over the points in the DataLoader. For every point, we send the pairs to the GPU, run the pairs through the network, and calculate the Contrastive Loss. We can then perform the backward pass and return the net loss for a batch of data.\ndef train(dl_train):\n    loss=[] \n    counter=[]\n    iteration_number = 0\n    for i, data in tqdm(enumerate(dl_train,0), total = len(dl_train)):\n      pair_left, pair_right , label = data\n      pair_left, pair_right , label = pair_left.cuda(), pair_right.cuda() , label.cuda()\n      optimizer.zero_grad()\n      out1,out2 = net(pair_left,pair_right)\n      loss_contrastive = criterion(out1,out2,label)\n      loss_contrastive.backward()\n      optimizer.step()\n      loss.append(loss_contrastive.item())\n    loss = np.array(loss)\n    return loss.mean()/len(dl_train)\n \nWe can train the model for several epochs using the function we just created. This article only trains the model for a few epochs as a demo.\nIf the evaluation loss is the best we have seen across the entire training period, we save the model for inference at that epoch.\nfor epoch in tqdm(range(1,num_epoch)):\n  best_eval_loss = 9999\n  train_loss = train(dl_train)\n  eval_loss = eval(dl_eval)\n \n  print(f&quot;Training loss {train_loss}&quot;)\n  print(f&quot;Eval loss {eval_loss}&quot;)\n \n  if eval_loss&lt;best_eval_loss:\n    best_eval_loss = eval_loss\n    print(f&quot;Best Eval loss{best_eval_loss}&quot;)\n    torch.save(net.state_dict(), &quot;model.pth&quot;)\n    print(&quot;Model Saved Successfully&quot;) \n[IMAGE {5} {Training} START SAMPLE]\n\n[IMAGE {5} FINISH SAMPLE]\nTesting the model\nAfter training the model, we can evaluate it and run inference for a single data point.\nSimilar to the training function, we create an evaluation function that takes the test data loader as input. We iterate the data loader one at a time and obtain the pairs of images we wish to test. We pass these image pairs to the GPU and run the model over them. After obtaining the output from the model, we find the Contrastive loss and save it to a list.\ndef eval(dl_eval):\n    loss=[] \n    counter=[]\n    iteration_number = 0\n    for i, data in tqdm(enumerate(dl_eval,0), total=len(dl_eval)):\n      pair_left, pair_right , label = data\n      pair_left, pair_right , label = pair_left.cuda(), pair_right.cuda() , label.cuda()\n      out1,out2 = net(pair_left,pair_right)\n      loss_contrastive = criterion(out1,out2,label)\n      loss.append(loss_contrastive.item())\n    loss = np.array(loss)\n    return loss.mean()/len(dl_eval)\nWe can now run the code for a single evaluation over all the test data points. To visualize the performance, we will plot the image and print the pairwise distances between the data points the model identifies. We can then plot these results as a grid.\nfor i, data in enumerate(dl_eval, 0):\n    x0, x1, label = data\n    concat = torch.cat((x0, x1), 0)\n    out1, out2 = net(x0.to(&#039;cuda&#039;), x1.to(&#039;cuda&#039;))\n \n    eucledian_distance = F.pairwise_distance(out1, out2)\n    print(label)\n    if label == torch.FloatTensor([[0.md|0]]):\n        label = &quot;Original Pair Of Signature&quot;\n    else:\n        label = &quot;Forged Pair Of Signature&quot;\n \n    imshow(torchvision.utils.make_grid(concat))\n    print(&quot;Predicted Euclidean Distance:-&quot;, eucledian_distance.item())\n    print(&quot;Actual Label:-&quot;, label)\n    if i == 4:\n        break\n \n[IMAGE {6} {Outputs} START SAMPLE]\n\n[IMAGE {6} FINISH SAMPLE]\nPros and Cons of Siamese Networks\nLike all Deep Learning applications, Siamese Networks have multiple pros and cons. Some of them are listed below.\nCons\n\nThe biggest disadvantage of using a Siamese network is that it returns only a similarity score. Since the network’s output is a distance metric, it does not sum up to 1. This property makes it harder to use in some cases.\n\nPros\n\nSiamese networks are robust to varying numbers of examples in classes. This robustness is due to the network requiring very little information about the classes.\nDomain-specific information does not need to be provided to the network to classify images.\nSiamese networks can perform predictions even with a single image per class.\n\nApplications\nSiamese Networks have quite a few applications. Some of them are as follows.\nFacial Recognition\nDue to the paired nature of the Siamese networks, one-short facial recognition is a good use case to use this network. The contrastive loss is used to push different faces away from each other and pull similar faces closer. In doing so, the Siamese network learns to identify faces without requiring too many examples.\nFingerprints\nSimilar to facial recognition, we can also use Siamese Networks for fingerprint recognition. Once the fingerprints in the database have been cleaned and pre-processed, we can feed them pairwise to the network. The Siamese network then learns to find their differences and identify which fingerprint is valid and invalid.\nSignature Verification\nThis article focused on implementing Signature Verification using Siamese networks. As we saw in this article, we can create a pairwise dataset using signatures and the network to identify which signatures are forged and which are real.\nText Similarity\nAnother useful application of Siamese Networks is Text similarity. Given multiple pieces of text, the network can be fed a pairwise dataset and tasked with identifying which are similar. Examples of such tasks include - finding similar questions from a question bank and using Siamese networks to find similar documents from a text database.\n:::\n:::section{.summary}\nConclusion\n\nSiamese networks are a powerful tool for classifying datasets with few examples per class.\nWe learned the concepts behind Siamese networks and understood the architecture, loss functions used, and how to train such a network.\nWe explored the Signature verification task using the ICDAR 2011 dataset and implemented a model to identify forged signatures.\nWe also understood the entire training and testing pipeline for Siamese networks, including its paired data representation.\n:::\n"},"Articles/Scalar/StackGAN":{"title":"StackGAN","links":["KB/Downsampling","KB/Covariance","KB/Regularization"],"tags":["article"],"content":"StackGAN\n:::section{.abstract}\nOverview\nIn image generation, the GAN architecture is one of the best ones. The StackGAN architecture addresses some of the flaws of basic GANs by decomposing the task of generating images into multiple parts. This article will focus on the training paradigm proposed by StackGAN and take an in-depth look at its architecture.\n:::\n:::section{.scope}\nScope\n\nThe scope of this article is to provide an introduction to StackGANs.\nThe article focuses on understanding how a StackGAN works, how it differs from other GANs and where We can use it.\nOnce these concepts are understood, the architecture is explained in detail with all the different stages and how they are linked.\n\n:::\n:::section{.main}\nIntroduction\nGenerating novel photorealistic images is a huge part of the computer vision process in many fields, such as photo editing, design and other graphics-related work. Many attempts to generate high-resolution images have been made in the past, and StackGAN is one of the major ones.\nInstead of performing the generation task in one go as most existing architectures do, a StackGAN uses two separate GANs. The authors of the StackGAN made this architectural choice to replicate how a human artist paints a picture. They first start with a rough sketch and a colour blockout, then move on to refining the details of the sketch. They then add more information based on the description of what they want to paint.\nThis article looks at StackGAN and how and why it works. The multi-stage architecture and the loss functions are also explained, along with the need to have such a modification to the GAN paradigm.\nSome of the images generated by StackGAN given their descriptions are shown below.\n[IMAGE {1} Example Images START SAMPLE]\n\n[IMAGE {1} FINISH SAMPLE]\nWhat is a StackGAN\nThe StackGAN is a multi-modal network that can produce much higher quality images than many networks by first generating a low-quality image and then rectifying it to increase the image’s resolution. A StackGAN is a modification of the general GAN training paradigm were generating new objects is split into sub-tasks. This split makes training the network much easier. The StackGAN research paper also introduces a technique called “Conditioning augmentation” that produces better results.\nPre Requisites\nBefore understanding the StackGAN architecture, we need to understand the concept of Conditional GANs (CGANs). In a CGAN, the Generator and Discriminator are given conditioning variables c alongside the input. These enable the Generator to create images influenced by these variables. This conditioning is formulated as G(z,c), D(x,c) for the Generator and the Discriminator, respectively.\nArchitecture\nThe StackGAN comprises two parts - Stage I and Stage II GANs. The first stage generates low-quality images by “sketching” a primitive shape and colouring the image with a simple colour blockout based on the text description provided. The background is generated from random noise.\nThe second stage corrects defects in the output of the first stage by re-reading the provided description and then completing the details the first phase missed. The output of the second stage is thus a high-resolution image.\n[IMAGE {2} Architecture START SAMPLE]\n\n[IMAGE {2} FINISH SAMPLE]\nStage I GAN\nStage I of the GAN is focused on generating a rough sketch with simple colours from the description.\nArchitecture\nIt is first fed into a fully connected layer(FC) to understand the embedding. The output of this FC layer is then passed to the Generator, which attempts to learn how to create the image better. The Discriminator takes the text embeddings and compresses them to a smaller representation using another FC layer. The image is also passed through a few Downsampling blocks until it is a size the network can use. The final down-sampled image is combined with the text embedding and passed through a 1x1 convolutional layer. The final layer is another FC layer that returns the probability of the generated image being real or fake.\nLoss functions\nConsider the text embeddings of the required description as \\varphi{t}. The meaning of the text embeddings is sampled from the Gaussian conditioning variables \\hat{c{0}}.\nStage I first trains the Discriminator and then the Generator to alternatively maximize the discriminator and generator losses. The equations for these loss functions are as follows.\n\\mathcal{L}_{D{0}} = \\mathbb{E}{(I{0},t)\\sim p{data}}[log D{0}(I{0}, \\varphi{t})]+ \\\\ \\mathbb{E}{z \\sim p{z}, t \\sim p{data}}[log(1- D{0}(G{0}(z , \\hat{c{0}}, \\varphi_{t})))]\n\\mathcal{L}_{G{0}}= \\mathbb{E}{z \\sim p{z}, t \\sim p{data}}[log(1- D{0}(G{0}(z, \\hat{c{0}}),\\varphi{t}))] + \\\\ \\lambda D{KL}(\\mathcal{N}(\\mu{0}(\\varphi{t}), \\Sigma{0}(\\varphi{t}))|| \\mathcal{N}(0, I))\nz is a noise vector that is randomly sampled from the Gaussian distribution p_{z}. A regularizing parameter is provided in the \\lambda variable. The StackGAN research uses a \\lambda = 1 for the paper.\nStage II GAN\nThe Stage II GAN receives the output of the Stage I GAN and refines it by re-considering the descriptions.\nArchitecture\nThe StackGANs Stage II Generator follows an encoder-decoder architecture with residual blocks. Text embedding is used first to create the conditioning variables. The result of Stage I is passed through Downsampling layers and then concatenated with the features obtained from the text embedding. The output of these layers is then upsampled to generate high-resolution images.\nThe Discriminator architecture is almost identical to Stage I, except for a few extra down-sampling layers. These down-sampling layers were included as the output of this part of the network is of a higher resolution than Stage I.\nLoss functions\nIf the low-resolution image is given by s_{0} and the Gaussian sampled latent variables are given by \\hat{c}, the Discriminator and Generator are trained by alternatively maximizing the value of the Discriminator loss and minimizing the Generator loss.\nThe equations for these loss functions are the same as the Stage I GAN, except the low-resolution image s_{0} is used instead of the noise z.\nIt is also to be noted that the noise z is not used in Stage II as the StackGAN is meant to preserve the required randomness with the previous stage. A different FC layer is also used here that generates different statistical outputs compared to Stage I to learn better features.\nMore architectural details\nSome architectural details were also mentioned in the StackGAN research. These details apply to both the Generator and the Discriminator.\n\nThe up-sampling blocks are composed of nearest neighbour upsampling and then passed to a 3x3 stride one convolutional layer. Besides the final layer, Batch Normalization and the ReLU activation are applied after every convolution.\nThe residual blocks have 3x3 stride 1 convolutions.\nThe StackGAN model that generates 256x256 images has four residual blocks, while the one that generates 128x128 images has only two blocks.\nThe down-sampling blocks have 4x4 stride two convolutions and LeakyReLU instead of ReLU.\nThe first Downsampling block does not have a Batch Normalization layer.\n\nEmbedding\nContrary to other networks where the text embeddings are transformed using non-linear techniques, the StackGAN uses additional variables and a process called Conditioning Augmentation. These embeddings are more robust to minor changes in the data manifold and work with lesser image data.\nConditioning Augmentation\nConditioning Augmentation is one of the major contributions of the StackGAN research. Given a text description t, the StackGAN uses an embedding to convert it to input for the Generator. Under circumstances where the data is limited, the latent space of the embedding is not fully exploited and leads to changes in the data manifold. These changes in the manifold are not desirable and hurt performance.\nConditioning Augmentation uses these to create more training pairs from a small subset of data. Instead of using a fixed conditioning variable, StackGAN samples latent variables from a Gaussian distribution. The mean and Covariance matrix is generated for a text embedding \\varphi_{t}.\nThe secondary objective of Conditioning Augmentation is to encourage reducing changes in the output with small changes in the data manifold. To do this, StackGAN uses a Regularization term called KL Divergence as part of the Generator.\nThis is given by, D{KL}(\\mathcal{N}(\\mu(\\varphi{t}), \\Sigma(\\varphi_{t})) || \\mathcal{N}(0,I))\nNeed for StackGAN\nEven though generating novel photorealistic images is easy enough to do with a GAN such as DCGAN, generating higher-resolution images is a hard problem. Previously failed approaches have tried stacking more up-sampling layers. StackGAN, using the decomposition of the generation and refinement tasks, can generate 256x256 images.\nThe StackGAN training paradigm can be used with existing GANs to improve performance as it generates higher image sizes.\n:::\n:::section{.summary}\nConclusion\nIn this article, we looked at StackGAN and all its components.\n\nWe understood how to decompose the task of generating novel images using a StackGAN.\nWe looked at the architectural details of the StackGAN, its’ embeddings and the respective training stages.\nWe also explored Conditional Augmentation and understood why it was proposed.\n:::\n"},"Articles/Scalar/Time-series-forecasting-using-LSTM":{"title":"Time series forecasting using LSTM","links":[],"tags":["article"],"content":"Time series forecasting using LSTM\n:::section{.abstract}\nOverview\nAny temporal data can be framed as a time series task. Data such as heart rates, stock market prices, sensor logs and many others fall under the category of time series data. There are many Deep Learning architectures that are used to model such data, LSTMs being one of them. This article focuses on building an LSTM time series model.\n:::\n:::section{.main}\nWhat are we building?\nIn this article, we will be creating an LSTM time series model. We will be using data that we generate and create a simple LSTM that can model it accurately. To perform this task, we will write functions that can generate data, model it and perform predictions on future points.\nWe will implement this model using Tensorflow and the below sections explain how to perform just that.\nPre-Requisites\nBefore moving on to creating the LSTM time series model, we must understand some pre-requisite information.\nWhat is Time Series?\nA time series data is any temporal data that has a discrete time interval and almost equidistant time steps. The general task is to estimate the function that was used to generate such the time series. If the function can be estimated correctly, future points that the model has not encountered yet can be predicted.\nExamples of time series include heart rate data, stock market data and many others.\nRNNs\nRNNs are a family of models that take entire series as inputs and return series as outputs. This algorithm is a sequential process and contains hidden states that model the underlying data. Unlike a simple Convolutional network that uses Backpropagation, an RNN uses a modified variant called Backpropagation through time (BPTT) that enables it to embed temporal data. An RNN is said to be Turing complete and is used in domains such as Natural Language Processing, Computer Vision, Robotics and many others.\nThe RNN architecture is made up of gates and is shown below.\n[IMAGE {1} RNN START SAMPLE]\n\n[IMAGE {1} FINISH SAMPLE]\nProblems with Classical RNNs\nRNNs suffer from a variety of problems due to their sequential nature.\n\nRNNs fail to model longer sequences. This property makes it very hard to use for data that has a long temporal span.\nThe classical RNN also had an issue with exploding and vanishing gradients due to the way the underlying architecture worked. These problems make an RNN very unstable.\n\nWhat is LSTM?\nLSTMs are a modified version of RNNs with different gates that enable the architecture to model much longer sequences. The LSTMs use gated connections that learn which features to forget and which to remember. The ability to choose what to forget makes them much better than a classical RNN. LSTMs are also a lot more stable and have a smaller chance of exploding or vanishing gradients.\nThe LSTM architecture is shown below.\n[IMAGE {2} LSTM START SAMPLE]\n\n[IMAGE {2} FINISH SAMPLE]\nHow are we going to build this?\nTo build an LSTM time series model, we will write functions that can generate a time series data. Once we have the data, we will pre-process it and make it fit to be used by the model. We will also write a function that can display the results of the model. After creating these helper functions, we will create a simple LSTM model and train it using the data we generated previously.\nThe LSTM time series model we will be using in this article is just comprised of a single LSTM block followed by a FC layer and is very easy to implement. After implementing all the required functions, we will train the model and use it to predict future points.\nThe following sections detail the implementation.\nFinal Output\nThe final output of the LSTM time series model is a prediction of future points that the model has not encountered yet. The output we get after training the model for ~50 epochs is shown below.\n[IMAGE {3} Final Output START SAMPLE]\n\n[IMAGE {3} FINISH SAMPLE]\nRequirements\nThere are many libraries that we need to implement an LSTM time series model. Since we will be building the architecture in Tensorflow, we import it first. We will also be using the numerical processing library numpy, the tabular data library pandas and the plotting libraries matplotlib and seaborn. The rc module in matplotlib enables configuring some of the plots and comes in handy later on.\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nimport pandas as pd\nimport seaborn as sns\nfrom pylab import rcParams\nimport matplotlib.pyplot as plt\nfrom matplotlib import rc\nBuilding the Time Series Forecaster\nThe Time Series Forecaster model has a simple LSTM based architecture. Before creating it, we have to write functions to set up the library, generate and load and finally pre-process the data.\nThe model we will use for this article is a Sequential model comprising an LSTM block followed by a Fully Connected layer. We will then use the generated data and this model to train a LSTM time series prediction model. We will use the trained model to predict points in the future that the model has not seen before.\nThe following sections detail all of these points.\nSetup\nTo set up our modules, we set the RANDOM_SEED variable. This variable sets the seed for the random number generator and ensures that we get the same “random” numbers every time. This is not useful in practice but is done only for demonstration purposes. We also modify the plot to be a white style grid with a muted palette for better display.\nsns.set(style=&#039;whitegrid&#039;, palette=&#039;muted&#039;, font_scale=1.5)\nrcParams[&#039;figure.figsize&#039;] = 16, 10\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)\ntf.random.set_seed(RANDOM_SEED)\nData\nTo generate the data, we create a custom function that uses a combination of a Sin wave and a small Gaussian noise. These values are generated in the range of (0,200) with a step of 0.1 .\nTo see how this data looks like, we can plot it using matplotlib.\ndata_time = np.arange(0, 200, 0.1)\nsin_values = np.sin(data_time) + np.random.normal(scale=0.5, size=len(data_time))\nplt.plot(data_time, sin_values, label=&#039;sine (with noise)&#039;);\n[IMAGE {4} Original Data START SAMPLE]\n\n[IMAGE {4} FINISH SAMPLE]\nData Pre-processing\nNow, we need to convert this data into a DataFrame before passing it to the model. Doing so makes future processes much easier. We also split the data into training and testing components.\nThe first few rows of the DataFrame are shown here.\n[IMAGE {5} Pre-Processing START SAMPLE]\n\n[IMAGE {5} FINISH SAMPLE]\ndata_full = pd.DataFrame(dict(sine=sin_values), index=data_time, columns=[&#039;sine&#039;])\ndata_full.head()\n \nlen_train = int(len(data_full) * 0.8)\nlen_test = len(data_full) - len_train\ntrain, test = data_full.iloc[0:len_train], data_full.iloc[len_train:len(data_full)]\nNow that we have created a data frame, we will use it to generate batches of data. We do this using the following function and create the input and labels for both training and testing.\ndef gen_data(X, y, num_steps=1):\n    Xs, ys = [], []\n    for i in range(len(X) - num_steps):\n        Xs.append(X.iloc[i:(i + num_steps)].values)       \n        ys.append(y.iloc[i + num_steps])\n    return np.array(Xs), np.array(ys)\n    \nnum_steps = 10\ntrainX, trainY = gen_data(train, train.sine, num_steps)\ntestX, testY = gen_data(test, test.sine, num_steps)\nImplementing the Sequential model\nWe can finally implement the LSTM time series model. This is a very simple model and just has a single LSTM layer followed by a FC layer.\nWe compile the model with the Mean Squared Error loss function and an Adam Optimiser. This compiled model can now be trained on the generated data.\nlstm_model = keras.Sequential()\nlstm_model.add(keras.layers.LSTM(128, input_shape=(trainX.shape[1], trainX.shape[2])))\nlstm_model.add(keras.layers.Dense(1))\nlstm_model.compile(loss=&#039;mean_squared_error&#039;, optimizer=keras.optimizers.Adam(0.001))\nEarly Stopping Callback\nTime series model tend to Overfit really easily. To reduce the probability of Overfitting, the Early Stopping callback is used. This callback uses the number of epochs as a hyper parameter. If the validation accuracy does not increase for a few epochs, the model is saved and training is stopped. This stops the training before the model starts to focus too much on the training data.\ncallbacks=[tf.keras.callbacks.EarlyStopping(monitor=&#039;loss&#039;, patience=3)]\nModel Training\nOnce we have defined all the required functions, we can train the model. In this article we train the LSTM time series model for 30 epochs with a batch size of 16. We use a validation split of 0.1% and also supply the Early Stopping callback that we defined earlier.\nhistory = lstm_model.fit(\n    trainX, trainY, \n    epochs=30, \n    batch_size=16, \n    validation_split=0.1,\n    shuffle=False,\n    callbacks=[tf.keras.callbacks.EarlyStopping(monitor=&#039;loss&#039;, patience=3)],\n)\n[IMAGE {6} Training START SAMPLE]\n\n[IMAGE {6} FINISH SAMPLE]\nEvaluation\nAfter training the model, we can use the evaluate function to perform a batch evaluation on the test dataset. We can see that the model performs pretty decently.\nlstm_model.evaluate(testX)\nTo visualize the training performance, we plot both the training and validation losses throughout history. We can see that the model is learning stably and is neither Overfitting nor Underfitting the data.\nplt.plot(history.history[&#039;loss&#039;], label=&#039;train&#039;)\nplt.plot(history.history[&#039;val_loss&#039;], label=&#039;test&#039;)\nplt.legend();\n[IMAGE {7} Evaluation START SAMPLE]\n\n[IMAGE {7} FINISH SAMPLE]\nPredicting a new point in the future\nNo LSTM time series model is useful without the ability to predict future points. We can use the predict function on a set of future points to see how well the model can predict the results. After performing inference, we plot the results against the actual data.\ny_pred = lstm_model.predict(testX)\nplt.plot(testY, marker=&#039;.&#039;, label=&quot;true&quot;)\nplt.plot(y_pred, &#039;r&#039;, label=&quot;prediction&quot;)\nplt.ylabel(&#039;Value&#039;)\nplt.xlabel(&#039;Time Step&#039;)\nplt.legend()\nplt.show();\nWe can see that the model did perform pretty decently. Further improvements in performance can be obtained by training for longer, using more data and many other methods that are beyond the scope of this article.\n:::\n:::section{.summary}\nConclusion\n\nIn this article, we learnt what an LSTM model is and why it was created.\nWe learnt how to create an LSTM model in Tensorflow.\nWe also learnt how to generate our own time series data using a sin curve.\nFinally, we trained an LSTM time series model on the generated data.\nWe also learned how to use the trained model to predict points in the future and display its predictions.\n:::\n"},"Articles/Scalar/Transfer-Learning-and-Fine-tuning":{"title":"Transfer Learning and Fine-tuning","links":["KB/Initialization","KB/Xception"],"tags":["article"],"content":"Transfer Learning and Fine-tuning\n:::section{.abstract}\nOverview\nTraining deep learning models requires a massive amount of labeled data. In most cases, this data needs to be made available or easier to clean up. Many approaches for working with limited data sets have been created over the years, Transfer Learning being one of the breakthroughs. Transfer learning enables us to fine-tune a model pre-trained on a large dataset on our task.\n:::\n:::section{.scope}\nScope\n\nThis article explains the principles behind Transfer Learning.\nIt covers the method of fine-tuning using a pre-trained model.\nIt elaborates on the principles of freezing and unfreezing weights.\nThe article also discusses implementing the Transfer Learning pipeline in Tensorflow.\n:::\n:::section{.main}\n\nIntroduction\nTransfer Learning is useful for smaller datasets and can be considered an intelligent weight Initialization scheme. Instead of randomly initializing the weights of the model like we usually do, we obtain weights from a model trained on a larger dataset. Any company/individual with the funds can train a larger model and make its weights public. After doing so, we can train these models on any other similar dataset much faster than before.\nThis article explores the concept of Transfer Learning by creating a network that can identify ten different classes from the CIFAR10 dataset by fine-tuning a model pre-trained on the ImageNet dataset (1000 classes).\nTransfer Learning\nIn a DL pipeline, Transfer Learning is usually done when the data available is too less to train a network properly. The general approach for a Transfer Learning workflow is as follows.\n\nObtain a pre-trained model on data similar to your current dataset. For example, many models are pre-trained on the ImageNet dataset in computer vision approaches. Since the ImageNet dataset has classes relating to real-life objects and things, models pre-trained on it already have some knowledge of the world.\nLoad the model and understand its layer structure.\nFreeze the weights of the model. Freezing the weights sets these layers to be un-trainable and prevents them from having their existing knowledge destroyed by the Transfer Learning process.\nAppend new layers to the frozen part of the model. These new layers can be trained and use the pre-trained weights to learn faster.\nTrain the new model on a new dataset.\n\nImplementation\nThis article will explore how to take a model trained on ImageNet and fine-tune it on new data. We will create this implementation in Tensorflow and use the Cats and Dogs dataset from Kaggle.\nPre-Requisites.\nBefore we can fine-tune a model, we must decide what base model we need. We also need to load and preprocess the dataset. Since Transfer Learning is generally used for small datasets, we take a subset of the Cats and Dogs dataset for this example.\nImports\nWe first import the required libraries. We use Tensorflow for the entire pipeline.\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow_datasets as tfds\nimport os\nimport zipfile\nLoading the Data\nSince the Cats and Dogs dataset is not a part of Tensorflow, we download it from Kaggle and then use the tensorflow_datasets library to load it into memory.\nAfter loading, we split the data into train and test while also sub-setting it.\ntrain_dataset, validation_dataset, test_dataset = tfds.load(\n    &quot;cats_vs_dogs&quot;,\n    split=[&quot;train[:40%]&quot;, &quot;train[40%:50%]&quot;, &quot;train[50%:60%]&quot;],\n    as_supervised=True,\n)\nAn example subset of the data is shown below.\n[IMAGE {1} CatsDogs START SAMPLE]\n\n[IMAGE {1} FINISH SAMPLE]\nWe can then convert the data into batches, split them into data loaders, and optimize the data loading using caching and pre-fetching. We use a batch size of 32 for this example. After loading, we can also apply some simple data augmentation methods. For example, we use Random Horizontal Flipping and Random Rotation.\nsize = (150, 150)\nbs = 32\naug_transforms = keras.Sequential(\n    [layers.RandomFlip(&quot;horizontal&quot;), layers.RandomRotation(0.1),]\n)\n \ntrain_dataset = train_dataset.map(lambda x, y: (tf.image.resize(x, size), y))\nvalidation_dataset = validation_dataset.map(lambda x, y: (tf.image.resize(x, size), y))\ntest_dataset = test_dataset.map(lambda x, y: (tf.image.resize(x, size), y))\n \n \ntrain_dataset = train_dataset.cache().batch(bs).prefetch(buffer_size=10)\nvalidation_dataset = validation_dataset.cache().batch(bs).prefetch(buffer_size=10)\ntest_dataset = test_dataset.cache().batch(bs).prefetch(buffer_size=10)\nThis article uses an Xception model pre-trained on the ImageNet dataset and applied to images 150x150x3 in size. The important point is to exclude the pre-trained model’s final classification layer. This final layer is just for classification, and we only care about the layers before it.\nmodel_pretrained = keras.applications.Xception(\n    weights=&quot;imagenet&quot;, \n    input_shape=(150, 150, 3),\n    include_top=False,\n)\nThe Xception model architecture is shown here.\n[IMAGE {2} arch START SAMPLE]\n\n[IMAGE {2} FINISH SAMPLE]\nFine-Tuning\nNow, we freeze the layers of the model we just loaded by setting the trainable parameter to False.\nAfter that, we create a model on top of the frozen layers and apply the data augmentations we defined.\nThe Xception model’s caveat is that it defines the inputs are scaled from the original range of (0,255) to the range of (-1.0, 1.0). We perform this rescaling using the Rescaling layer as follows.\nmodel_pretrained.trainable = False\n \ninputs = keras.Input(shape=(150, 150, 3))\nrescale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=-1)\n \nx = aug_transforms(inputs) \nx = rescale_layer(x)\nUnfreeze the top layers of the model\nThe Xception** layers to improve performance further. Global Average Pooling is an alternative to the Fully Connected layer (FC) that preserves spatial information better. Since our pre-trained model uses different data, these layers are useful here. The final layer is an FC layer for a binary classification task.\nx = model_pretrained(x, training=False)\nx = keras.layers.GlobalAveragePooling2D()(x)\nx = keras.layers.Dropout(0.2)(x) \n \noutputs = keras.layers.Dense(1)(x)\nfinal_model = keras.Model(inputs, outputs)\nWe can now train the new layers that we created.\nfinal_model.compile(\n    optimizer=keras.optimizers.Adam(),\n    loss=keras.losses.BinaryCrossentropy(from_logits=True),\n    metrics=[keras.metrics.BinaryAccuracy()],\n)\n \nnum_epochs = 5\nfinal_model.fit(train_dataset, epochs=num_epochs, validation_data=validation_dataset)\nNow that we trained the new layers, we unfreeze the entire model and then train it with a very small learning rate. This gradual training leads to much better performance. Note that the Batch Normalization layers are not updating during this training, as if they did, it would badly hurt performance.\nmodel_pretrained.trainable = True\nfinal_model.summary()\n \nfinal_model.compile(\n    optimizer=keras.optimizers.Adam(1e-5),\n    loss=keras.losses.BinaryCrossentropy(from_logits=True),\n    metrics=[keras.metrics.BinaryAccuracy()],\n)\n \nnum_epochs = 5\nfinal_model.fit(train_dataset, epochs=num_epochs, validation_data=validation_dataset)\nEvaluation and prediction\nThis example shows how useful Transfer Learning is for quickly training small datasets. After training the model, we evaluate the test dataset. The model still performs quite well despite the few training epochs and fewer data.\n[IMAGE {3} Results START SAMPLE]\n\n[IMAGE {3} FINISH SAMPLE]\n:::\n:::section{.summary}\nConclusion\n\nTransfer Learning is a powerful method when fewer data is present.\nAs long as the pre-trained model uses similar data, a niche model can be fine-tuned using it.\nSelectively freezing the pre-trained layers and training the rest is a way to achieve the effects of fine-tuning.\nAfter an initial round of selective training, unfreezing the model and training the entire model improves performance.\nThe Transfer Learning approach is thus an invaluable breakthrough in Deep Learning.\n:::\n"},"Articles/Scalar/index":{"title":"Scalar Academy Articles","links":[],"tags":["articles"],"content":"Scalar Academy Articles\n\nThis section has some of the articles I wrote on commission for Scalar Academy.\nThese are unpublished as of now but can be eventually found here.\n"},"Articles/index":{"title":"Articles","links":[],"tags":["articles"],"content":"Articles\n\nThis section has open source no paywall versions of every article I write (unless requested for closed source of course)\nThey might not have the images required as those are only available on the source website. BUT if you search for those topics in the Knowledge base, you will find all the material there anyway, with images (most of them)\nI use this KB as a gateway into writing these articles as well.\n"},"Book-Notes/Understanding-Deep-Learning/Chapter-1---Introduction":{"title":"Chapter 1 - Introduction","links":[],"tags":[],"content":"Chapter 1\n \n\n\n"},"Book-Notes/Understanding-Deep-Learning/Chapter-10---CNNs":{"title":"Chapter 10 - CNNs","links":["KB/Conv","KB/Padded-Conv","KB/Strided","KB/Kernel-Filters","KB/Atrous-Convolution","KB/Channels","KB/Receptive-field","KB/Downsampling","KB/Pooling","KB/Transposed-Conv","KB/1x1-conv","KB/Image-Classification","KB/Alex-Net","KB/Relu","KB/Vgg","KB/Res-Net","KB/Object-Detection","KB/YOLO","KB/Non-Maxima-Supression","KB/Semantic-Segmentation","KB/DeconvNet","KB/ImageNet","KB/Le-Net","KB/Inception","KB/Depthwise-Separable","KB/PixelShuffle","KB/Xavier-Initialization","KB/Cutout","KB/Unet","KB/Mixup","KB/Batch-Normalization"],"tags":["deeplearning"],"content":"Chapter 10 - CNNs\n \nInvariance\n\nA function f[x] of an image is invariant to a transform t[x] if f(t(x)) = f(x)\ncovariant if f(t(x)) = t(f(x))\n\nConvolution\n\nConv\nPadded Conv\nStrided\nKernel Filters\nAtrous Convolution\nChannels\nReceptive field\nDownsampling\nPooling\nTransposed Conv\n1x1 conv\n\n\nImage Classification\n\nAlex Net\nRelu\nVgg\nRes Net\n\nObject Detection\n\nYOLO\nNon Maxima Supression\n\nSemantic Segmentation\n\nVgg\nDeconvNet\n\nExtras\n\nImageNet\nLe Net\nInception\nDepthwise Separable\nPixelShuffle\nXavier Initialization\nCutout\nUnet\nMixup\nBatch Normalization\n"},"Book-Notes/Understanding-Deep-Learning/Chapter-11---Residual-Networks":{"title":"Chapter 11 - Residual Networks","links":["KB/Alex-Net","KB/Vgg","KB/Exploding-Gradient","KB/Res-Net","KB/Skip-Connection","KB/Batch-Normalization","KB/Dense-Net","KB/Unet","KB/Highway-Convolutions","KB/ResNeXt","KB/BlockDrop","KB/Shake-Shake","KB/Shake-Drop"],"tags":["architecture"],"content":"Chapter 11 - Residual Networks\n \nSequential Processing\n\n\n\nLimitations\n\nAlex Net, Vgg . image classification performance decreases again as further layers are added\nExploding Gradient\n\nResidual\n\nRes Net\nSkip Connection\nBatch Normalization\n\nArchitectures\n\nRes Net\nDense Net\nUnet\nHighway Convolutions\nResNeXt\n\nExtras\n\nBlockDrop\nShake-Shake\nShake-Drop\n"},"Book-Notes/Understanding-Deep-Learning/Chapter-12---Transformers":{"title":"Chapter 12 - Transformers","links":["KB/Dot-Product-Attention","KB/Self-Attention","KB/Basic-Transformer","KB/Position-Encoding","KB/Scaled-Dot-Product-Attention","KB/Multi-Head-Attention","KB/Layer-Normalization","KB/Tokenizer","KB/Embedding","KB/Encoder-Decoder-Attention","KB/BERT","KB/Transfer-Learning","KB/Self-Supervised","KB/SentimentAnalysis","KB/GPT","KB/GPT3","KB/Autoregressive","KB/Masked-Autoencoders","KB/Masked-Language-Modeling","KB/Generative-Models","KB/Seq2Seq","KB/Vision-Transformer","KB/Swin-Transformer","KB/Long-Short-Term-Memory-(LSTM)","KB/GLUE","KB/SQuAD","KB/Teacher-Forcing","KB/Big-Bird","KB/CLIP"],"tags":["architecture"],"content":"Chapter 12 - Transformers\n \n\nDot Product Attention\nweight sharing → reuse the same weights for every input token\nSelf Attention\nBasic Transformer\nPosition Encoding\nScaled Dot Product Attention\nMulti Head Attention\nLayer Normalization\nTokenizer\nEmbedding\nEncoder Decoder Attention\nBERT\nTransfer Learning\nSelf Supervised\n\nUses\n\nNamed entity recognition\nText span prediction\nSentimentAnalysis\nGPT, GPT3\nAutoregressive\nMasked Autoencoders\nMasked Language Modeling\nGenerative Models\nSeq2Seq\nVision Transformer\nSwin Transformer\nLong Short Term Memory (LSTM)\nGLUE\nSQuAD\nTeacher Forcing\nPosition Encoding\nBig Bird\nCLIP\n"},"Book-Notes/Understanding-Deep-Learning/Chapter-13---Graph-Networks":{"title":"Chapter 12 - Graph Networks","links":["KB/Graphs","KB/Challenges-of-Graphs","KB/Types-of-Graphs","KB/Representing-Graphs","KB/Graph-Neural-Network","KB/Graph-convolutional-network","KB/Pooling"],"tags":["graph"],"content":"Chapter 13 - Graph Networks\n \n\nGraphs\nChallenges of Graphs\nTypes of Graphs\nRepresenting Graphs\nGraph Neural Network\nGraph convolutional network\n\nLinks to Interesting Things\n\nNose Job\nPytorch geometric\n\nhands on gnns : node, graph classification, point cloud classification etc\n\n\nsuvey paper\nPooling\n\nEigenpooling\nHaarpooling\n\n\ndistill.pub gnn\nDropEdge - Tackling Over-Smoothing for General Graph Convolutional Networks\nAwesome gnn\n"},"Book-Notes/Understanding-Deep-Learning/Chapter-14---Unsupervised-Learning":{"title":"Chapter 14 - Unsupervised Learning","links":[],"tags":["unsupervised"],"content":"Chapter 14 - Unsupervised Learning\n \n\nabsence of labels\n"},"Book-Notes/Understanding-Deep-Learning/Chapter-4---Deep-Neural-Networks":{"title":"Chapter 4 - Deep Neural Networks","links":["KB/ImageNet","KB/Relu","KB/SGD","KB/Composing-shallow-neural-networks-to-get-deep-networks","KB/Matrix-notation-for-NNs","KB/Shallow-vs-deep-networks","KB/Universal-Approximation-Theorem","KB/Depth-Efficiency-of-Neural-Networks","KB/Width-Efficiency-of-Neural-Networks"],"tags":["deeplearning"],"content":"Chapter 4 - Deep Neural Networks\n \nIntro\n\nDeep networks : more than one hidden layer\nBoth deep and shallow describe piecewise linear mappings from input → output\nA shallow network can describe complex functions but might have too many hidden layers to be practically possible to use.\n\nWhy did deep learning take off\n\nKrizhevsky et al. (2012) aka ImageNet\nLarger datasets\nImproved processing power for training\nRelu\nSGD\n\nComposing networks\n\nComposing shallow neural networks to get deep networks\n\nGeneral deep neural networks\n\nConsider a deep network with 2 hidden layers, each of which has 3 hidden units\n\n\n\n\n\n\nHow does the network deal with complicated functions?\n\n\n\nHyperparameters\n\nCapacity : total number of hidden units\nNNs represent a family of families of functions relating input to output\n\nWidth\n\nNumber of hidden units in each layer (D_{1}, D_{2},..., D_{K})\n\nDepth\n\nNumber of hidden layers (K)\n\nMatrix notation to represent deep networks\n\nMatrix notation for NNs\n\nShallow vs deep networks\n\nShallow vs deep networks\n\nOther notes\n\nUniversal Approximation Theorem\nDepth Efficiency of Neural Networks\nWidth Efficiency of Neural Networks\n"},"Book-Notes/Understanding-Deep-Learning/Chapter-5---Loss-functions":{"title":"Chapter 5 - Loss functions","links":["loss","KB/Distributions","KB/Log-Likelihood-Loss","KB/Maximum-Likelihood","KB/Recipe-for-constructing-loss-functions","KB/Loss-for-univariate-regression","KB/Loss-for-binary-classification","KB/Loss-for-multiclass-classification","KB/KL-Divergence","KB/Cross-Entropy","KB/heteroscedastic-nonlinear-regression","KB/Robust-regression","KB/Quantile-Regression","KB/Focal-Loss","KB/Van-Mises-distribution","KB/Hinge-Loss","AdaBoost"],"tags":["deeplearning","loss"],"content":"Chapter 5 - Loss Functions\n \n\nA large list of useful loss functions can be found loss\nA large list of useful distributions can be found here Distributions\nA loss function or cost function L[φ] returns a single number that describes the mismatch between the model predictions f[xi, φ] and their corresponding ground-truth outputs y_i\n\nDistributions\n\n\n\nMaximum Likelihood\nLog Likelihood Loss\nMaximum Likelihood\nConstructing New Loss Functions\nRecipe for constructing loss functions\nLoss for univariate regression\nLoss for binary classification\nLoss for multiclass classification\n\nDifference between the distributions : KL Divergence or Cross Entropy\n\nFor Multimodal\n\nFusion loss\nMultiple attention heads : backprop from heads\n\nOther Notes\n\nheteroscedastic nonlinear regression\nRobust regression\nQuantile Regression\nFocal Loss\nVan Mises distribution\nNon probabilistic approaches - Hinge Loss, AdaBoost\n\nUseful Links\n\nvisualizing loss functions\nvisualizing the loss landscape - neurips\ndistributions → loss functions\n"},"Book-Notes/Understanding-Deep-Learning/Chapter-6---Fitting-models":{"title":"Chapter 6 - Fitting models","links":["loss","Book-Notes/Understanding-Deep-Learning/Chapter-5---Loss-functions","KB/Gradient-Descent","Gabor-Model","KB/Saddle-Points","KB/SGD","KB/SGD-Momentum","KB/Nesterov-Momentum","KB/Adam","KB/Adagrad","KB/Rmsprop","KB/AdaDelta","KB/Amsgrad","KB/Learning-Rate-Warmup","KB/Learning-Rate-Scheduling"],"tags":["optimizer","loss"],"content":"Chapter 6 - Fitting Models\n \n\n\nMinimizing loss , Chapter 5 - Loss functions\n\n\nbasic : compute the derivatives of the gradients of the loss wrt params and then adjust the params based on gradients : decrease loss\n\n\nGoal is to minimize the loss \\hat \\phi= \\underset {\\phi}{argmin}[L[\\phi]]\n\n\nGradient Descent\nGabor Model\n\nLocal minima and Saddle Points\nSGD\nSGD Momentum\nNesterov Momentum\nAdam\nAdagrad\nRmsprop\nAdaDelta\nAmsgrad\nLearning Rate Warmup\nLearning Rate Scheduling\n\nUseful Links\n\nbag of tricks for computer vision\nvisualizing optimisers 1 \noptimizers but easier and less scary math\ngradient descent video\nbackprop video\nvery advanced blog on if neural networks are overfitted\n1cycle scheduling and warmup\nAutomatic differentiation\n\nQs\n\nWhy do we need to modify the learning rate while training?\nHow would you decide a learning rate depending on batch size?\nWhat optimizer would you use for inference?\n"},"Book-Notes/Understanding-Deep-Learning/Chapter-7---Gradients-and-Initialization":{"title":"Chapter 7 - Gradients and Initialization","links":["KB/Backprop","KB/Relu","KB/Computational-Graph","KB/Initialization","KB/Exploding-Gradient","KB/Vanishing-Gradient","KB/Gradient-Checkpointing"],"tags":["gradients"],"content":"Chapter 7 - Gradients and Initialization\n \n\nBackprop\nRelu\nalgorithmic differentiation (I think this means AD?)\nComputational Graph\nInitialization\nExploding Gradient, Vanishing Gradient\nGradient Checkpointing\n\nUseful Links\n\nvisualizing weights\nall you need is a good init\nhow to initialize a neural network\nDifficulty of training networks by xavier and yoshua bengio\nnetron, tensorboard and pytorchviz for visualizing computational graphs\n\nQs\n\nIf a network can learn things anyway, why does initaliziation make such a big difference?\nHow can we make backprop more efficient? (aka how does it work in PyTorch/TF)\n"},"Book-Notes/Understanding-Deep-Learning/Chapter-8---Measuring-performanc":{"title":"Chapter  8 - Measuring  performance","links":["KB/Bias-Variance-Dilemma","KB/Noise-Suppression","KB/Double-Descent","KB/regularize","KB/Regularization","KB/Nasnet","KB/AutoML-Benchmark","KB/Cross-Validation","KB/Model-Capacity"],"tags":["deeplearning","loss","uncertainty"],"content":"Chapter 8 - Measuring Performance\n \n\nHow well does the model generalize to new data\n\nTraining a Simple Model\n\n\n\n\nNoise\n\nExample instruments and noisy outputs\n\nBias\n\nModel is not flexible enough to fit the true function.\n\n“Fix” : More capacity\n\n\n\nVariance\n\nWhen we fit a model, we do not get the closest possible approximation to the true underlying function\n\n“Fix” : More training data\n\n\nBias Variance Dilemma\n\nBias measures how strongly the avg result deviates from optimal value\nVariance measures how strongly the results vary around the expected value E_{retrain}\nWhen flexibility is too low → bias dominates(too good in train and horrible later) and underfits\nWhen flexibility is too high → variance dominates → overfitting\n\n\n\n\nSkeptic : Examples of noise and how to tackle it in some more real life examples. Not just the ML way : Noise Suppression\n\nTest Error\n\nConsider a 1D regression problem where the data generation process has additive noise with variance σ^{2}\n\n\nThis equation says that the expected loss after considering the uncertainty in the training data D and the test data y consists of three additive components.\nThe variance is uncertainty in the fitted model due to the particular training dataset we sample.\nThe bias is the systematic deviation of the model from the mean of the function we are modeling.\nThe noise is the inherent uncertainty in the true mapping from input to output.\n\nDouble Descent\nregularize , Regularization\n\nSkeptic : How does this connect with optimizers\nSkeptic: Visualize loss landscape for double descent\nSkeptic: More about hyperparameter tuning\n\nNasnet , AutoML Benchmark\nExtras\n\nCross Validation\nModel Capacity\n"},"Book-Notes/Understanding-Deep-Learning/Chapter-9---Regularization":{"title":"Chapter 9 - Regularization","links":["KB/Mixup","KB/Optimizers","KB/Label-Smoothing","KB/Batch-Normalization","KB/AdamW","KB/Regularization","KB/Regularization-Term","KB/Maximum-Likelihood","KB/Negative-Log-Likelihood","KB/Lp-Regularization","KB/Gradient-Descent","KB/SGD","KB/Early-Stopping-tricks","KB/Dropout","KB/Adversarial-Learning","KB/Bayesian","KB/Transfer-Learning","KB/Multi-Task-Learning","Tag-Pages/augmentation","KB/Self-Supervised"],"tags":["regularization","normalization"],"content":"Chapter 9 - Regularization\n \n\nSkeptic : Mixup\nSkeptic: How does this tie into Optimizers\nSkeptic : Label Smoothing : Mentioned later\nBatch Normalization - The idea is to normalize the inputs of each layer in such a way that, they have a mean activation output zero and a unit standard deviation.\nSkeptic : AdamW\n\n\nRegularization\n\nExplicit Regularization\n\nRegularization Term\nFrom Maximum Likelihood,\n\n\n\n\n\nfrom Negative Log Likelihood\n\nLp Regularization\nGradient Descent\nSGD\nImproving Performance\n\nEarly Stopping tricks\nEnsembling models\nDropout\nAdversarial Learning\nBayesian\nTransfer Learning\nMulti Task Learning\naugmentation\nLabel Smoothing\nSelf Supervised\n"},"Book-Notes/Understanding-Deep-Learning/index":{"title":"Understanding Deep Learning","links":["Book-Notes/Understanding-Deep-Learning/Chapter-1---Introduction","Book-Notes/Understanding-Deep-Learning/Chapter-4---Deep-Neural-Networks","Book-Notes/Understanding-Deep-Learning/Chapter-5---Loss-functions","Book-Notes/Understanding-Deep-Learning/Chapter-6---Fitting-models","Book-Notes/Understanding-Deep-Learning/Chapter-7---Gradients-and-Initialization","Book-Notes/Understanding-Deep-Learning/Chapter-8---Measuring-performanc","Book-Notes/Understanding-Deep-Learning/Chapter-9---Regularization","Book-Notes/Understanding-Deep-Learning/Chapter-10---CNNs","Book-Notes/Understanding-Deep-Learning/Chapter-11---Residual-Networks","Book-Notes/Understanding-Deep-Learning/Chapter-12---Transformers","Book-Notes/Understanding-Deep-Learning/Chapter-13---Graph-Networks","Book-Notes/Understanding-Deep-Learning/Chapter-14---Unsupervised-Learning","Chapter-15---GAN","KB/Chapter-16---Normalizing-Flows"],"tags":[],"content":"UDL Book\n\nUDL Book\nChapter 1 - Introduction\nChapter 4 - Deep Neural Networks\nChapter 5 - Loss functions\nChapter 6 - Fitting models\nChapter 7 - Gradients and Initialization\nChapter 8 - Measuring performanc\nChapter 9 - Regularization\nChapter 10 - CNNs\nChapter 11 - Residual Networks\nChapter 12 - Transformers\nChapter 13 - Graph Networks\nChapter 14 - Unsupervised Learning\nChapter 15 - GAN\nChapter 16 - Normalizing Flows\n"},"Book-Notes/index":{"title":"Book Notes","links":[],"tags":["books"],"content":"Notes on Books I have read"},"KB/0-1-Loss":{"title":"0-1 Loss","links":["loss"],"tags":["loss"],"content":"0-1 loss\n\n\\begin{cases} 1 &amp; f(x)=y \\\\ 0 &amp; f(x)\\neq y\\end{cases}\nClassification\n"},"KB/1D-piecewise-linear-interpolation":{"title":"1D piecewise linear interpolation","links":["KB/Interpolation"],"tags":["visualization"],"content":"1D Piecewise Linear Interpolation\n1D Piecewise Linear Interpolation\n\n\n"},"KB/1D-ALVINN":{"title":"1D-ALVINN","links":["KB/MSE"],"tags":["dataset"],"content":"1D-ALVINN\n\nroad simulator\npredict steering angle from road conditions\naugmented to produce additional labels representing new tasks, namely: road is one or two lanes, left edge, center, and right edge road locations, location and intensity of road centerline, intensity of road surface and region bordering road\nRoot MSE\n"},"KB/1x1-conv":{"title":"1x1 conv","links":[],"tags":["architecture"],"content":"1x1 Conv\n \n\n\n"},"KB/2-X-2-Study":{"title":"2 X 2 Study","links":[],"tags":["language"],"content":"2 X 2 Study\n\nFirst factor has 2 levels (The vs each)\nSecond factor has 2 levels (Distributive vs. Collective situations)\nWe are trying to see how these fixed factors effect responses\nWe control the fixed factors\nWe carefully design them so we know what category an item presented to a participant belongs to\n"},"KB/2-byte-character-set":{"title":"2 byte character set","links":[],"tags":["language"],"content":"2 Byte Character Set\n\n65,536 unique characters Pairs of Bytes for a Single Character\nSometimes single byte letters, spaces and punctuations will be interspersed with two-byte characters\nChinese characters are encoded in two format:\n\nBig-5 - Complex Mandarin\nGB - Simple form\n\n\n"},"KB/2024-11-18":{"title":"2024-11-18","links":["KB/Self-Supervised-Survey"],"tags":["daily"],"content":"2024-11-18\n \nTo Ask Israel\n\nGrant requirements\nProblems with frameworks (OOM etc?)\nConstraints enforced?\nDatasets tested?\nHow did you set it up\nSnellius or something else?\n\nInfo\nOpenMLSuites:\nSparse datasets = turn off inference time evaluation (inference_time_measurements)\ntasks_ids = [360932, 360933]AMLB:\nIncrease time overhead_time_multiplier or overhead_time_multiplier\ninference_time_measurements = 5?   (do it to 10)\nFix versions of the frameworksFrameworks\n\nflaml:\n\nIf there is a problem with “pandas”\nOnly install it inside the virtual env of flaml\nIf there is a problem with “callback”:\nOnly install the next inside the virtual env of flaml\npip install xgboost==2.0.3\n\nGAMA:\n\na package from sklearn should be updated (I don’t remember)\n\nH2OAutoML:\n\nDefine the memory manually (exec.py)jvm_memory = str(round(config.max_mem_size_mb * 2/3))+&quot;M&quot;\n\nNaiveAutoML:\nFEDOT:\nAutoGluon\n\n“NoResultError: Trainer has no fit models that can infer.” (Figure)\nSlurm:\nAutoGluon does not respect the resources defined.\nRun each fold per each framework:\npython runbenchmark.py AutoGluon_benchmark:2024Q1 openml/t/360975 5min8c_gp3 -f 0\nClean cache\nfind -O3 /gpfs/scratch1/nodespecific/ -maxdepth 2 -type d -user icampero -exec rm -rf {} \\;\nTouch:\nfind israelAutoML2 -exec touch -a -m {} \\;  (edited)\n\napptainer.org/\n\nSlurm\n\nmyquota\nrome/thin or genoa\nrun it with 28GB memory\n\nproblem with h20 and autogloun\n\n\nglobal save : false, retrieve save from backup\nclean cache\n\ninode\n2 weeks without touching folder\nuse scratch to run your stuff\n\n\n"},"KB/8-bit-character-set":{"title":"8 bit character set","links":["KB/ASCII"],"tags":["language"],"content":"8 Bit Character Set\n\nFirst 128 characters are reserved for ASCII\nISO-8859 series of 10+ Character Sets for most European Languages\nResults in large number of overlapping character sets for different languages\n"},"KB/A-declarative-modular-framework-for-representing-and-applying-ethical-principles":{"title":"A declarative modular framework for representing and applying ethical principles.","links":[],"tags":["ethics"],"content":"A Declarative Modular Framework for Representing and Applying Ethical Principles.\n\nFiona Berreby, Gauvain Bourgne, and JeanGabriel Ganascia\nhigh level action language for designing ethical agents in an attempt to shift the burden of moral reasoning to the autonomous agents\ncollects action, event and situation information to enable an agent to simulate the outcome of various courses of actions\nevent traces are then passed to the causal engine to produce causal traces\nethical specifications and priority of ethical considerations under a given situation are used to compute the goodness assessment on the consequences\ncombined with deontological specifications (duties, obligations, rights) to produce a final rightfulness assessment\n"},"KB/A-low-cost-ethics-shaping-approach-for-designing-reinforcement-learning-agents":{"title":"A low-cost ethics shaping approach for designing reinforcement learning agents","links":[],"tags":["ethics"],"content":"A Low-cost Ethics Shaping Approach for Designing Reinforcement Learning Agents\n\nYueh-Hua Wu and Shou-De Lin.\nauthors investigated how to enable RL to take ethics into account ethics shaping\nassuming that the majority of observed human behaviours are ethical, the proposed approach learns ethical shaping policies from available human behaviour data in given application domains\nrewards positive ethical decisions, punishes negative ethical decisions, and remains neutral when ethical considerations are not involved\n"},"KB/A-matter-of-ambiguity-Using-eye-movements-to-examine-collective-vs-distributive-interpretations-of-plural-sets":{"title":"A matter of ambiguity? Using eye movements to examine collective vs. distributive interpretations of plural sets","links":["KB/Minimal-Semantic-Commitment"],"tags":["language"],"content":"A Matter of Ambiguity? Using Eye Movements to Examine Collective Vs. Distributive Interpretations of Plural Sets\n\nChristine Boylan, Dimka Atanassov, Florian Schwarz, John Trueswell\n\nPrevious Work\n\nFrazier et al. (1999) used eye-tracking reading times to compare processing loads of sentences that were explicitly distributive (involving the adverb each), explicitly collective (involving the adverb together), and locally indeterminate at the predicate.\nHaving found evidence for increased processing load associated with distributive sentences, they concluded that the processor initially pursues a collective reading, and thus the distributive / collective distinction was one of ambiguity and not vagueness.\nHowever, an increased processing load for the distributive reading might be expected regardless of whether the underlying representation is vague or ambiguous\nprocessor must stipulate a distributive operator D (the spell-out of which is each) to interpret a distributive meaning, which may incur processing delays\nMoreover, increased reading times at the point of disambiguation preclude conclusions about exactly when listeners may have committed to a distributive reading during the processing the underdetermined predicate.\n\nIntroduction\n\neye movements of listeners were recorded to investigate the representation of collective vs. distributive interpretations of plural subjects in light of the Minimal Semantic Commitment (MSC) hypothesis\nMinimal Semantic Commitment\nThe crucial difference between these two proposed representation types is that an ambiguous representation forces a decision about an interpretation, while a vague representation tolerates unspecified features.\nGiven the prediction that an ambiguous item will prompt the processor to converge on one interpretation even in the absence of disambiguating information, we tested whether sentences underdetermined for collectivity / distributivity would nonetheless cause listeners to converge on a single interpretation.\nRather than relying on processing times to infer representational commitments, we employed the visual world paradigm to track which representations subjects considered over the course of hearing a sentence\n\nMethod\n\nThe eye movements of 24 participants were recorded as they listened to explicit/indeterminate collective/distributive sentences while they considered collective and distributive acts depicted on a computer screen\nAn earlier switch in gaze to one of the two images would indicate a processing preference for one interpretation over the other. Results and Discussion\nExplicitly collective sentences prompted looks to the collective scenario at the point of disambiguation (i.e. together) and explicitly distributive sentences (using each) prompted looks to the distributive scenario (Fig. 2a)\nCrucially, however, the indeterminate, nulldisambiguator sentences patterned along the same trajectory as together sentences: the predicate alone prompted looks to the collective, prior to hearing the final word of the sentence\nWe also compared the proportion of looks averaged across two time windows: an 800- ms interval before the onset of the predicate and an 800-ms time window following the predicate onset (Fig. 2b)\nSince the distinction between ambiguity and vagueness lies at the interface of semantics and pragmatics, it is particularly important that we find a psychometric realization of the difference between these two types of representations.\nHere we presented a method by which to investigate the time courses of these representations as they relate to distributivity, and we suggest this method may further contribute to the study of the semantics-pragmatics interface at large.\n\nResults\n\nIn an ANOVA of proportion of looks to collective and distributive scenes, we found significant interactions between disambiguator and time window.\nIn a targeted analysis of disambiguator effects in each time window, we found significant differences between together and each sentences and the null and each sentences after the predicate onset but not before\nMoreover, the together sentences did not significantly differ from the null form.\nThus, despite a lack of explicit disambiguating information, the indeterminate, nulldisambiguator sentences nonetheless prompted looks to the collective scenario, which was reliably different from the time course of distributive-directed each sentences.\nThis provides evidence that the listener has committed to the collective interpretation in the absence of disambiguating information.\nThis is consistent with a theory that treats the collective / distributive distinction as ambiguous rather than vague.\nThe results also indicate that this processing commitment is essentially immediate; i.e., as soon as listeners begin hearing the ambiguous predicate, they show a preference for the collective interpretation.\n\nPictures\n\n\n"},"KB/A-survey-on-Image-Data-Augmentation-for-Deep-Learning":{"title":"Data Augmentation with Curriculum Learning","links":["KB/Data-Augmentation-with-Curriculum-Learning","KB/Geometric-Transformations","KB/Flipping","Color-Space-Transform","KB/Cropping","KB/Noise-Injection","KB/Color-Space-Transformations","KB/Kernel-Filters","KB/Feature-Space-Augmentation","KB/SMOTE","KB/GAN‐based-Data-Augmentation","KB/Meta-Learning-Data-Augmentations","KB/Neural-Augmentation","KB/Smart-Augmentation","KB/AutoAugment","KB/Augmented-Random-Search","KB/Test-time-Augmentation","SamplePairing","KB/Alleviating-Class-Imbalance-with-Data-Augmentation","KB/Regularization"],"tags":["augmentation"],"content":"\n@shortenSurveyImageData2019\nData Augmentation with Curriculum Learning\n\nMethods\n\nGeometric Transformations\nFlipping\nColor Space Transform\nCropping\nNoise Injection\nColor Space Transformations\nKernel Filters\nFeature Space Augmentation\nSMOTE\nGAN‐based Data Augmentation\nMeta Learning Data Augmentations\nNeural Augmentation\nSmart Augmentation\nAutoAugment\nAugmented Random Search\nTest-time Augmentation\nSamplePairing\nData Augmentation with Curriculum Learning\nAlleviating Class Imbalance with Data Augmentation\n\nDiscussion\n\nIt is easy to explain the benefit of horizontal Flipping or random Cropping\nHowever, it is not clear why mixing pixels or entire images together such as in PatchShuffle Regularization or SamplePairing is so effective.\ndditionally, it is difficult to interpret the representations learned by neural networks for GAN-based augmentation, variational auto-encoders, and meta-learning.\nAn interesting characteristic of these augmentation methods is their ability to be combined together.\nThe GAN framework possesses an intrinsic property of recursion which is very interesting\nSamples taken from GANs can be augmented with traditional augmentations such as lighting filters, or even used in neural network augmentation strategies such as Smart Augmentation to create even more samples. These samples can be fed into further GANs and dramatically increase the size of the original dataset.\nAn interesting question for practical Data Augmentation is how to determine postaugmented dataset size.\nno consensus about the best strategy for combining data warping and oversampling techniques\nOne important consideration is the intrinsic bias in the initial, limited dataset\nThere are no existing augmentation techniques that can correct a dataset that has very poor diversity with respect to the testing data\n"},"KB/A-voting-based-system-for-ethical-decision-making":{"title":"A voting-based system for ethical decision making","links":["KB/Moral-Machine-project"],"tags":["ethics"],"content":"A Voting-based System for Ethical Decision Making\n\nRitesh Noothigattu, Snehalkumar ‘Neil’ S. Gaikwad, Edmond Awad, Sohan Dsouza, Iyad Rahwan, Pradeep Ravikumar, and Ariel D. Procaccia\nvoting-based system for autonomous entities to make collective ethical decisions leverages data collected from the Moral Machine project\nSelf reported preference over different outcomes under diverse ethical dilemmas are used to learn models of preference for the human voters over different alternative outcomes.\nindividual models are then summarized to form a model that approximates the collective preference of all voters\n"},"KB/ABN-Amro-AI-Dev":{"title":"ABN Amro AI Dev","links":[],"tags":["jobsearch"],"content":"ABN AMRO Motivation Letter - Subhaditya\nDuring COVID, it became increasingly clear how important it was to have stable financial systems. No matter your position in the financial sector, it is essential to consider the customers affected by the pipeline. That being the case, many systems can be created to improve the user experience. I have been using ABN AMRO as my bank of choice ever since I came to the Netherlands two years ago, and recently, I started noticing many AI features such as predicted recurring costs, automated filling in of details from ID cards during new user creation, etc. Given the active development of such features, I am interested in an AI development position at ABN AMRO.\nMy expertise is a combination of data analytics and computer vision, and as of a month ago, I also have a Masters in Artificial Intelligence from the University of Groningen. I am familiar with the tools required for basic and advanced AI from internships, research projects, papers, freelance work, and many personal projects. In the past, I interned at the AI divisions of Emirates NBD (one of the largest banks in the UAE) and KPMG (one of the big four consultancy firms) in similar teams with similar goals of developing disruptive AI solutions, and I quite enjoyed the work there. Working in a team that has a start-up vibe but also in the context of a much larger system is also something that I enjoy. As this position offers something similar, I can contribute to any team I get the chance to work with.\nIn any team, it is essential that each of the members can contribute something to the overall development of the projects. Although I have a lot to learn, I genuinely enjoy developing AI solutions that have some impact, and I hope to get to work with the amazing AI team at ABN AMRO and ship some useful solutions to the users (myself included)."},"KB/ABN-amro-ml":{"title":"ABN amro ml","links":[],"tags":["jobsearch"],"content":"ABN AMRO Motivation Letter - Subhaditya\nDuring COVID, it became increasingly clear just how important it was to have stable financial systems. At the end of the day, no matter what position you work in in the financial sector, it is important to consider the customers that will be affected by the pipeline. Even small-scale bank frauds can affect a large number of people. Growing up, I heard many stories of such cases and how many people lost their livelihoods because of financial crime. Back then, of course, I could do nothing about it. But now I have a small chance of contributing to the fight for financial safety, and I want to take it.\nA while back, I had the privilege of doing an internship at Emirates NBD, one of the biggest banks in the UAE. There I was involved in a project to build a similar system to detect financial fraud. I helped build the ETL pipelines and ML models to identify load defaulters and also generate forecasts of how much customers might be able to pay based on their historical records. I learned quite a bit about the proceedings there. It has been a few years since that internship, and I have completed my Masters in Artificial Intelligence as of last month. That being the case, I am now more prepared to tackle the challenges that this job might bring about.\nI hope to contribute to this and any future projects to the best of my abilities. It would be good to give customers a bit more peace of mind by helping create such systems."},"KB/ACT-R-Chunk":{"title":"ACT-R Chunk","links":[],"tags":["cognitivemodel"],"content":"ACT-R Chunk\n\nid\nAttribute\n\nAttributes point to other chunks (eg: fact3+2)\nnumber of attributes\n\n\nActivation\n\ndetermines priority of retrieval\nif below threshold, chunk cannot be retrieved\ndetermines time of retrieval\ndecays with time\ndepends on\n\nHow often retrieved or recreated\ncontext\n\n\n\n\n"},"KB/ACT-R":{"title":"ACT-R","links":["Motor-Cortex","KB/Visual-Cortex","KB/Parietal-lobe","KB/Declarative-memory","Anterior-Cingulate","Time-perception"],"tags":["cognitivemodel"],"content":"ACT-R\n\n\nExplain human behavior\n\n\nManual Control\n\nMotor Cortex\n\n\n\nVisual Perception\n\nVisual Cortex\n\n\n\nProblem State\n\nParietal lobe\nStore intermediate results\n\n\n\nDeclarative memory\n\n\nPerception\n\n\nControl State\n\nAnterior Cingulate\nKeep track of goals\nWhat? How far?\n\n\n\nTime perception\n\n\nIdeas always get reinforced if retrieve\n\n"},"KB/ADE20K":{"title":"ADE20K","links":[],"tags":["temp"],"content":"\ntoc: true\ntitle: ADE20K\ntags: [‘temp’]\n\nADE20K"},"KB/ADVENT":{"title":"ADVENT","links":["KB/Entropy","loss","KB/Direct-entropy-minimization","KB/Entropy-minimization-by-adverarial-learning","KB/GTA5","KB/SYNTHIA","KB/Cityscapes"],"tags":["architecture"],"content":"ADVENT\n\nblog\npaper\n\nADVENT is a flexible technique for bridging the gap between two different domains through Entropy minimization\nmodels trained only on source domain tend to produce over-confident, i.e., low-Entropy, predictions on source-like images and under-confident, i.e., high-Entropy, predictions on target-like ones\nConsequently by minimizing the Entropy on the target domain, we make the feature distributions from the two domains more similar.\nMore annotated data has been shown to always improve performance of DNNs\nHere we are working on Unsupervised DA (UDA), which is a more challenging task where we have access to labeled source samples and only unlabeled target samples. We use as source, data generated by a simulator or video game engine, while for target we consider real-data from car-mounted cameras.\nThe main approaches for UDA include discrepancy minimization between source and target feature distributions usually achieved via adversarial training ganin2015unsupervised, tzeng2017adversarial, self-training with pseudo-labels zou2018unsupervised and generative approaches hoffman2018cycada, wu2018dcan.\nWe present our two proposed approaches for Entropy minimization using (i) an unsupervised Entropy loss and (ii) adversarial training. To build our models, we start from existing semantic segmentation frameworks and add an additional network branch used for domain adaptation.\nDirect entropy minimization\nEntropy minimization by adverarial learning\nGTA5\nSYNTHIA\nCityscapes\n"},"KB/ALBERT":{"title":"ALBERT","links":["KB/BERT","Roam-Highlights","KB/Factorized-Embedding-Parameters","KB/cross-layer-parameter-sharing","KB/inter-sentence-coherence-loss","KB/Embedding","KB/GLUE","KB/SQuAD","KB/RACE","KB/Attention","KB/Transformer"],"tags":["architecture"],"content":"ALBERT\n\nLite BERT\n\nblog #Roam-Highlights\nperhaps even better when scaled to the same number of parameters as BERT\nFactorized Embedding Parameters\ncross-layer parameter sharing\ninter-sentence coherence loss\nWe can see that the ALBERT base model attempts to mimic BERT base, with a hidden state size of 768, parameter sharing and a smaller Embedding size due to factorization explained above. Contrary to the 108 million parameters, it has only 12 million. This makes a big difference when training the model.\nAnother model, ALBERT xxlarge (extra-extra large) has 235 million parameters, with 12 encoder segments, 4096-dimensional hidden state and 128-dimensional Embedding size. It also includes parameter sharing.\nGLUE\nSQuAD\nRACE\nFor Factorized Embedding Parameters, the authors report good performance. Both the case where cross-layer parameters were not shared and where they were, are reported. Without sharing, larger Embedding sizes give better performance. With sharing, performance boosts satisfy at an Embedding size of 128 dimensions. That’s why the 128-size embeddings were used in the table above.\nFor cross-layer parameter sharing, the authors looked at not performing cross-layer sharing, performing cross-layer sharing for the feedforward segments only, performing sharing for the Attention segments, and performing sharing for all subsegments. It turns out that sharing the parameters for the Attention segments is most effective, while sharing the feedforward segment parameters does not contribute significantly. This clearly illustrates the important role of the Attention mechanism in Transformer models. Because, however, all-segment sharing significantly decreases the number of parameters, at only slightly worse performance compared to Attention-only sharing, the authors to perform all-segment sharing instead.\nFor the SOP task, we can read that if NSP is performed on a SOP task, performance is poor. NSP on NSP of course performs well, as well as SOP on SOP. However, if SOP is performed on NSP, it performs really well. This suggests that SOP actually captures sentence coherence whereas NSP might not, and that SOP yields a better result than NSP.\n"},"KB/ANOVA":{"title":"ANOVA","links":[],"tags":["language"],"content":"ANOVA\n\nFor an ANOVA you aggregate the data\nso that you get the mean responses per condition for each individual, look at the differences between their means, and then check if the differences for all the\n"},"KB/ASCII":{"title":"ASCII","links":[],"tags":["language"],"content":"ASCII\n\n7 bit character set\nRoman, Latin\n"},"KB/AUC-Borji":{"title":"AUC-Borji","links":[],"tags":["loss"],"content":"AUC-Borji\n\n“PR is calculated in the same way as AUC-Judd”\n“a location-based metric”\nFPR is obtained by calculating the proportion of negatives in the thresholded region, where the negatives are collected uniformly at random.\n“AUC for the curve is calculated as AUC-Borji.”\n“TPR is calculated in the same way as AUC-Judd”\n“location-based metric”\n"},"KB/AUC-Judd":{"title":"AUC-Judd","links":[],"tags":["loss"],"content":"AUC-Judd\n\n“location-based metric”\nThe true positive rate (TPR) is calculated as the proportion of fixations falling into the thresholded saliency map\nThe false positive rate (FPR) is calculated as the proportion of no-fixated pixels in the thresholded saliency map.\nAfter calculating TPR and FPR at each threshold, the area under the curve (AUC) is calculated for the curve of TPR against FPR.\n"},"KB/Absolute-Error":{"title":"Absolute Error","links":[],"tags":["loss"],"content":"Absolute Error\n\n\\lvert y-f(x)\\rvert\nPenalize large errors\n"},"KB/Acceleration":{"title":"Acceleration","links":["KB/Velocity","KB/Time"],"tags":["physics"],"content":"Acceleration\n\nChange in Velocity wrt Time\na_{avg}= \\Delta v/\\Delta t\n"},"KB/Accessibility":{"title":"Accessibility","links":[],"tags":["explainability"],"content":"Accessibility\n\na minor subset of the reviewed contributions argues for explainability as the property that allows end users to get more involved in the process of improving and developing a certain ML model\nexplainable models will ease the burden felt by non-technical or non-expert users when having to deal with algorithms that seem incomprehensible at first sight\n"},"KB/Action-Component":{"title":"Action Component","links":[],"tags":["robotics"],"content":"Action Component\n\nreactively dispatches and monitors the execution of actions.\n"},"KB/Action-Potential":{"title":"Action Potential","links":["KB/Axon"],"tags":["brain"],"content":"Action Potential\n\nSometimes called a “spike” or described as a neuron “firing,” an action potential occurs when there is a significant increase in the electrical activity along the membrane of a nerve cell. It is associated with neurons passing electrochemical messages down the Axon, releasing neurotransmitters to neighboring cells in the synapse.\n"},"KB/Action-Transitive-verb":{"title":"Action Transitive verb","links":["KB/Verb"],"tags":["language"],"content":"Action Transitive Verb\n\na Verb has a direct object + Verb\nI made her lower her head or body\n"},"KB/Activation-Functions":{"title":"Activation Functions","links":["KB/SELU","KB/Elu","KB/Leaky-Relu","KB/Relu","KB/Tanh","KB/Sigmoid","loss"],"tags":["architecture"],"content":"\ngeneral rule “which is better”\nSELU &gt; Elu &gt; Leaky Relu &gt; Relu &gt; Tanh &gt; Sigmoid\nloss\n"},"KB/Active-Compliant-Robot":{"title":"Active Compliant Robot","links":["KB/Compliant-Robot"],"tags":["robotics"],"content":"Active Compliant Robot\n\nAn active Compliant Robot is one in which motion modification during the performance of a task is initiated by the control system. The induced motion modification is slight, but sufficient to facilitate the completion of a desired task.\n"},"KB/Active-tracking":{"title":"Active tracking","links":[],"tags":["cognitivemodel"],"content":"Active Tracking\n\nhz evaluated moment-to-moment\nIs it already time to start preparing?\nMakes your models run much slower!\n"},"KB/Actuator":{"title":"Actuator","links":["KB/Electrical-Energy"],"tags":["robotics"],"content":"Actuator\n\nA power mechanism used to effect motion, or maintain position of the robot (for example, a motor which converts Electrical Energy to effect motion of the robot) (R15.07). The actuator responds to a signal received from the control system.\n"},"KB/Acute":{"title":"Acute","links":[],"tags":["medical"],"content":"Acute\n\nA condition that is often severe but starts and ends quickly\n"},"KB/AdaDelta":{"title":"AdaDelta","links":[],"tags":["gradients"],"content":"AdaDelta\n\nRMS[\\Delta \\theta]_{t}= \\sqrt{E[\\Delta \\theta^{2}]_{t}+ \\epsilon}\n\n&amp; \\Delta \\theta_{t}= -\\frac{RMS[\\Delta \\theta]_{t-1}}{RMS[g]_{t}}g_{t}\\\\\n&amp; \\theta_{t+1}= \\theta_{t}+ \\Delta \\theta_{t}\n\\end{align}$$"},"KB/AdaIn":{"title":"AdaIn","links":["KB/Instance-Normalization","KB/Features"],"tags":["normalization"],"content":"AdaIn\n\npaper\nAdaIN(x,y) = \\sigma(y) \\big( \\frac{x-\\mu(x)}{\\sigma (x)} \\big)\nAdaptive Instance Normalization is a normalization method that aligns the mean and variance of the content Features with those of the style Features.\nno learnable affine Features\nAdaptively computes affine params from style input\n"},"KB/Adagrad":{"title":"Adagrad","links":[],"tags":["architecture"],"content":"Adagrad\n\nPast squared grads as scaling factor for learning rate\n\\begin{align}&amp; g_{t,i} = \\nabla_\\theta J(\\theta_{t,i}) \\\\ &amp; \\theta_{t+1, i} = \\theta_{t,i} - \\eta \\cdot g_{t,i} \\\\ &amp; \\theta_{t+1, i} = \\theta_{t,i} - \\frac{\\eta}{\\sqrt{G_{t,ii} + \\epsilon}} \\cdot g_{t,i} \\end{align}\nDoesnt forget past gradients\n"},"KB/Adam":{"title":"Adam","links":["KB/Rmsprop"],"tags":["regularization"],"content":"Adam\n\nSupervised learning\nRmsprop + Momentum\nCorrects bias in exponentially weighted averages\nStruggles with large no of params → Over smooths the gradient\n\\begin{align} &amp; s_n = \\rho_1 s_{n-1} + (1-\\rho_1) g_n \\\\ &amp; r_n = \\rho_2 r_{n-1} + (1-\\rho_2) g_n \\odot g_n \\\\ &amp; \\Theta_{n+1} = \\Theta_n - \\alpha \\frac{s_n}{\\epsilon + \\sqrt{r_n}} \\frac{1-\\rho_2^n}{1-\\rho^n_1} \\end{align}\nFirst and second moments\n"},"KB/AdamW":{"title":"AdamW","links":["KB/Weight-Decay-Vs-L2-Regularization","KB/Amsgrad"],"tags":["optimizer","regularization"],"content":"AdamW\n\n@loshchilovDecoupledWeightDecay2019\n\nAdam Features\n\n\nwhy use the same learning rate for every parameter, when we know that some surely need to be moved further and faster than others\n\n\nSince the square of recent gradients tells us how much signal we’re getting for each weight, we can just divide by that to ensure even the most sluggish weights get their chance to shine\n\n\nwith a little tweak to keep early batches from being biased\n\n\nFew research articles used it to train their models, new studies began to clearly discourage to apply it and showed on several experiments that plain ole SGD with momentum was performing better.\n\n\n\n\n\n\n\n\nIlya Loshchilov and Frank Hutter pointed out in their paper that the way weight decay is implemented in Adam in every library seems to be wrong, and proposed a simple way (which they call AdamW) to fix it\n\n\nPhD student Jeremy Bernstein has pointed out that the claimed convergence problems are actually just signs of poorly chosen hyper-parameters, and that perhaps amsgrad won’t fix things anyway. Another PhD student, Filip Korzeniowski, showed some early results that seemed to support this discouraging view of amsgrad.\n\n\nWeight Decay Vs L2 Regularization\n\n\nAmsgrad\n\n"},"KB/Adaptive-Gradient-Clipping":{"title":"Adaptive Gradient Clipping","links":["KB/Gradient-Clipping"],"tags":["architecture"],"content":"Adaptive Gradient Clipping\n\nclips gradients to the ratio between weight gradient and weight value\nClipping parameter is more robust than in traditional GC\nSwapping Batch Normalisation for AGC\n\nfaster training for equally sized models\nAllows for even larger batch size training\n\n\n"},"KB/Adaptive-Input-Representation":{"title":"Adaptive Input Representation","links":["KB/language","KB/English-Wikipedia","KB/Billion-Word"],"tags":["architecture"],"content":"Adaptive Input Representation\n\nAdaptive Input Representations for Neural Language Modeling\nvarying the size of input word embeddings for neural language modeling\nimprove accuracy while drastically reducing the number of model parameters\nmore than twice as fast to train than the popular character input CNN while having a lower number of parameters\nEnglish Wikipedia\nBillion Word\n"},"KB/Adaptive-Whitening-Saliency":{"title":"Adaptive Whitening Saliency","links":["@Adaptive-deconvolutional-networks-for-mid-and-high-level-feature-learning"],"tags":["explainability"],"content":"\n\nAdaptive Whitening Saliency\n\n@Adaptive deconvolutional networks for mid and high level feature learning\nis based on the whitening of low-level features and has shown good performance for saliency map estimation.\nuses the features extracted by the model pre-trained for scene recognition.\n"},"KB/Adding-noise":{"title":"Noise","links":["Augmentation","KB/Dropout","KB/Stochastic-ensemble-learning"],"tags":["temp"],"content":"Noise\n\nTo training input data\n\nGaussian random noise\nAugmentation\n\n\nWhile the algo runs\n\nDropout\n\n\nStochastic ensemble learning\n"},"KB/Additive-Attention":{"title":"Additive Attention","links":["KB/Attention","KB/Attention-Alignment","KB/Bahdanau-Attention"],"tags":["architecture"],"content":"Additive Attention\n\nBahdanau et al., 2015\nUses a one layer feedforward network to calculate Attention Alignment\nOh, basically it is the same as Bahdanau Attention\n"},"KB/Additive-coupling-layer":{"title":"Additive coupling layer","links":["KB/Scaling-matrix-for-coupling-layers"],"tags":["distributions","architecture"],"content":"Additive Coupling Layer\n \n\ng(z_{d+1:D}; m(z_{1:d})) = z_{d+1:D}+ m(z_{1:d})\nSo no matter what m is , maybe a neural network, it is still possible to invert it and compute whatever we need\nThey have a unit jacobian determinant, so neither contract nor expand space\n\npreserves volume\nbut we don’t really want this because we want the model to be able to freely scale the amount of latent space it uses\nSo we use a Scaling matrix for coupling layers\n\n\n"},"KB/Adjacency-matrix":{"title":"Adjacency matrix","links":["KB/Properties-of-Adjacency-matrix"],"tags":["graph"],"content":"Adjacency Matrix\n \n\nNxN matrix where (m,n) is set if there is an edge between the nodes m and n or 0 otherwise\nFor undirected graphs, this is always symmetric\nFor large sparse ones, it can be a list of connections to save memory\nThe n^{th} node has an associated node embedding x(n) of length D. These embeddings are concatenated and stored in the D×N node data matrix X. Similarly, the e^{th} edge has an associated edge embedding e(e) of length DE . These edge embeddings are collected into the D_E ×E matrix E.\n\nProperties of Adjacency matrix"},"KB/Adjective":{"title":"Adjective","links":[],"tags":["language"],"content":"Adjective\n\nProperties of objects\n"},"KB/Adrenal-Glands":{"title":"Adrenal Glands","links":["KB/Glucose","KB/Cortisol","KB/Adrenaline"],"tags":["brain"],"content":"Adrenal Glands\n\nLocated on top of each kidney, these two glands are involved in the body’s response to stress and help regulate growth, blood Glucose levels, and the body’s metabolic rate. They receive signals from the brain and secrete several different hormones in response, including Cortisol and Adrenaline.\n"},"KB/Adrenaline":{"title":"Adrenaline","links":["KB/Adrenal-Glands"],"tags":["brain"],"content":"Adrenaline\n\nAlso called epinephrine, this hormone is secreted by the Adrenal Glands in response to stress and other challenges to the body. The release of adrenaline causes a number of changes throughout the body, including the metabolism of carbohydrates to supply the body’s energy demands and increased arousal or alertness.\n"},"KB/Advantages-of-Federated-Learning":{"title":"Advantages of Federated Learning","links":["KB/Federated-Learning"],"tags":["federatedlearning"],"content":"Advantages of Federated Learning\n\nAll your information is locally stored and is never sent anywhere\nSaves your personalized data from being leaked\nRemoves all connections to you\nAllows the model to be updated and become better without compromizing on privacy\nNobody “owns” your data except you\n"},"KB/Adverb":{"title":"Adverb","links":[],"tags":["language"],"content":"Adverb\n\nProperties of verbs\nslowly, frequently, nally\n"},"KB/Adversarial-Distillation":{"title":"Adversarial Distillation","links":[],"tags":["knowledgedistillation"],"content":"Adversarial Distillation\n\nFurthermore, an effective intermediate supervision, i.e., the squeezed knowledge, was used by Shu et al. (2019) to mitigate the capacity gap between the teacher and the student.\nIn the third category, adversarial knowledge dis- tillation is carried out in an online manner, i.e., the teacher and the student are jointly optimized in each it- eration (Wang et al., 2018e; Chung et al., 2020)\n"},"KB/Adversarial-Learning":{"title":"Adversarial Learning","links":["KB/Manifold","KB/PDF"],"tags":["temp"],"content":"Adversarial Learning\n\nConsider data in a Manifold. The PDF is concentrated along a low dim Manifold \\mathcal{M}\nNow the original picture is a point on the Manifold (dim = output layer size)\nAdd noise to the image such that the image now appears to be in a direction orthogonal to \\mathcal{M} → value of PDF shrinks dramatically\nThen the network has never seen this before and will return a random classification\n"},"KB/Adversarial-Loss":{"title":"Adversarial Loss","links":[],"tags":["loss"],"content":"Adversarial Loss\n\nWe apply Adversarial Loss to both the Generators, where the Generator tries to generate the images of it’s domain, while its corresponding discriminator distinguishes between the translated samples and real samples.\nGenerator aims to minimize this loss against its corresponding Discriminator that tries to maximize it.\n"},"KB/Adversarial-Spatial-Dropout-for-Occlusion":{"title":"Adversarial Spatial Dropout for Occlusion","links":["KB/Dropout"],"tags":["augmentation"],"content":"Adversarial Spatial Dropout for Occlusion\n\nCrops region pixels to generate hard positives for object detection by learning key image regions\nWithin the proposed region only 1/3 pixels are dropped after sorting based on magnitudes\nDropped values are non-contiguous here as compared to previously discussed methods\n"},"KB/Afferent":{"title":"Afferent","links":[],"tags":["brain"],"content":"Afferent\n\nSensory Division\n"},"KB/Affine-Function":{"title":"Affine Function","links":[],"tags":["temp"],"content":"Affine Function\n\nb is a bias term which is padded at the end, size 1\nFunction + bias\n"},"KB/Affordance-Detection-Task-Specific":{"title":"Affordance Detection Task Specific","links":[],"tags":["robotics"],"content":"Affordance Detection Task Specific\n\nKokic, Mia, et al. “Affordance Detection for Task-Specific Grasping using Deep Learning.” Humanoids 2017.\n\n\nAffordance Score vs Contact Constraint\n"},"KB/Afib":{"title":"Afib","links":[],"tags":["medical"],"content":"Afib\n\nAtrial fibrillation, irregular and rapid heartbeats\n"},"KB/Agglutinating-words":{"title":"Agglutinating words","links":["KB/Morphology-Affix"],"tags":["language"],"content":"Agglutinating Words\n\nWords divide into smaller units with clear boundaries\nCompound words are hyphenated (as in English or not as in German)\nNachkriegszeit ; Nichtraucher\nSingle token words - end-of-line\nMulti-token words – Delhi-based\nString of Morphology Affix\n"},"KB/Akaike-Information-Criterion":{"title":"Akaike Information Criterion","links":["KB/Normal-Distribution"],"tags":["loss"],"content":"Akaike Information Criterion\n\nConsiders goodness-of-fit to the data and penalizes complexity of the model\nAIC=−2log⁡(L)+2q\nwhere:\nL: likelihood function for a particular model\nq: number of variables of this model\nIf error terms \\epsilon follows Normal Distribution , expected value 0 + constant variance AIC = \\frac{1}{\\eta \\sigma^{2}}(RSS + 2p \\hat \\sigma^2)\n"},"KB/Aleatoric":{"title":"Aleatoric","links":["KB/Uncertainty","KB/Homoscedatic","KB/Heteroscedatic"],"tags":["uncertainty"],"content":"Aleatoric\n\nUncertainty part of the data\nSensor noise etc\nSimplest noise : additive noise f(x) = x^{3}+ \\epsilon\n\\epsilon \\sim \\mathcal{N}(0, \\sigma^{2})\nHomoscedatic\nHeteroscedatic\n\n"},"KB/Alex-Net":{"title":"Alex Net","links":["KB/Dropout","KB/Relu"],"tags":["architecture"],"content":"Alex Net\n\nDropout + Relu\nNo of filters increase according to depth\n\n\n"},"KB/Algebra-Cognitive-Tutor":{"title":"Algebra Cognitive Tutor","links":[],"tags":["usermodel"],"content":"Algebra Cognitive Tutor\n\nAnderson’s group (Anderson, Corbett, Koedinger, &amp; Pelletier, 1995), and has been extended and marketed by Carnegie Learning (&lt;www.carnegielearning.com)&gt;\nBecause it is so widely used and has undergone so many positive evaluations, it is arguably the most successful ITS in the world at this time.\nInner loop: The inner loop monitors the student’s steps while solving an algebra problem.\nAlthough most problems, including the one in Figure 2, involve using multiple representational tools (graphs, tables, etc.) to analyze a problem scenario, some problems focus only on specific tools, such as the equation solver. In Figure 2, the problem to be solved is shown in the upper left window. This problem has four parts, labeled 1 through 4. When the student began, the cells of the table in the worksheet window at the lower left were all empty. The student has filled every cell with a number, text or an algebraic formula. In the process of figuring out what to put in the cells, the student used the solver window (upper right) and the graphing window (lower right). Each time the student filled a table cell, plotted a point on the graph, entered an equation in the solver window, etc, the tutor gave immediate feedback that told the student whether the step was correct or incorrect.\nOuter loop: The outer loop in the Algebra I Algebra Cognitive Tutor selects an algebra problem for the student to do and makes sure that the student submits a solution. The tutor uses its fine-grained assessment to select a task that exercises a few knowledge components that the student has not yet mastered. When the student has mastered all the knowledge components in a unit (all the bars turn gold), the student is advanced to the next unit in the algebra curriculum.\nStep analysis: The tutor analyzes each student step in terms of a set of anticipated steps (S. Ritter, Blessing, &amp; Wheeler, 2003). The set of anticipated steps for a problem is precomputed by solving the problem in all acceptable ways by running a rule-based problem solver. The rules are written to correspond to knowledge components. Each step is associated with the rules that were used to generate it during the precomputation. During tutoring, the student’s step is matched against these anticipated steps.\nWhen a student’s step matches an anticipated step, the student is credited in the assessment with having applied the associated knowledge components.\n"},"KB/Allele":{"title":"Allele","links":[],"tags":["brain"],"content":"Allele\n\nOne of two or more varying forms of a gene due to genetic mutation.\nDiffering alleles, which can be found at the same spot on a chromosome, produce variation in inherited characteristics such as hair color or blood type.\n"},"KB/Alleviating-Class-Imbalance-with-Data-Augmentation":{"title":"Alleviating Class Imbalance with Data Augmentation","links":["KB/Geometric-Transformations"],"tags":["augmentation"],"content":"Alleviating Class Imbalance with Data Augmentation\n\nData Augmentation falls under a Data-level solution to class imbalance and there are many different strategies for implementation.\nA naive solution to oversampling with Data Augmentation would be a simple random oversampling with small Geometric Transformations such as a 30° rotation\nOne problem of oversampling with basic image transformations is that it could cause overfitting on the minority class which is being oversampled\nThe biases present in the minority class are more prevalent post-sampling with these techniques.\nNeural Style Transfer is an interesting way to create new images. These new images can be created either through extrapolating style with a foreign style or by interpolating styles amongst instances within the dataset.\nOversampling with GANs can be done using the entire minority class as ‘real’ examples, or by using subsets of the minority class as inputs to GANs\nThe use of evolutionary sampling to find these subsets to input to GANs for class sampling is a promising area for future work.\n"},"KB/Allomorph":{"title":"Allomorph","links":["KB/Morpheme"],"tags":["language"],"content":"Allomorph\n\nVariants of the same Morpheme but cant be replaced by another\nUn – happy gives unhappy\nIn-comprehensible gives incomprehensible\n"},"KB/Alpha-Waves":{"title":"Alpha Waves","links":[],"tags":["brain"],"content":"Alpha Waves\n\n9-14 Hz\nDrowsy/Inhibition\n\n"},"KB/Alphacode":{"title":"Alphacode","links":[],"tags":["architecture"],"content":"Alphacode\n\nOther language models have demonstrated an impressive ability to generate code, but these systems still perform poorly when evaluated on more complex, unseen problems\nAlphacode is a system for code generation for problems that require for deeper reasoning\nhaving an extensive dataset for training and evaluation, large and ecient transformer based architectures and a large-scale model sampling.\nmodel is firstly pre-trained through GitHub repositories amounting to 715.1 GB of code.\nmore extensive dataset than Codex’s pre training dataset.\nFor the training to be better, a fine-tuning dataset is introduced from the Codeforces plataform\nCodecontests are conducted, for the validation phase, in which we better the performance of the model.\ntransformer-based architecture, they use an encoder-decoder transformer architecture\nCompared to decoder-only architectures commonly used, this architecture allows for a bidirectional description and extra flexibility.\nshallow encoder and a deep encoder to further the model’s ecienc\no reduce the cost of sampling, multi-query attention is used.\n"},"KB/Alzheimer’s-Disease":{"title":"Alzheimer’s Disease","links":[],"tags":["brain"],"content":"Alzheimer’s Disease\n\nA debilitating form of dementia, this progressive and irreversible neurodegenerative disease results in the development of protein plaques and tangles that damages neurons and interfere with neural signaling, ultimately affecting memory and other important cognitive skills\n"},"KB/Amdahl's-Law":{"title":"Amdahl's Law","links":[],"tags":["parallelcomputing"],"content":"Amdahl’s Law\n\nMaximum expected improvement to an overall system when only part of the system is improved\nTheoretical maximum speedup using multiple processors\nSpeedup = \\frac{\\text{Execution time before improvement}}{\\text{Execution time after improvement}}\nSpeedup = ((1-f_{E})+(\\frac{f_{E}}{f_{I}}))^{-1}\n\nf_E is fraction enhanced\nf_{I} is factor of improvement\nS_{e} is speedup enhanced\n\n\nExecutionTime_{new} = ExecutionTime_{old}\\times [(1-f_{E})+f_{E}/S_{e}]\nMaximumSpeedupp = n/(1+(n-1)f )\n\nn is no of processors\n\n\n"},"KB/Amino-Acid":{"title":"Amino Acid","links":[],"tags":["brain"],"content":"Amino Acid\n\nA type of small organic molecule that has a variety of biological roles but is best known as the “building block” of proteins.\n"},"KB/Amsgrad":{"title":"Amsgrad","links":["KB/Adam"],"tags":["optimizer"],"content":"Amsgrad\nModified Adam\n\nBy analyzing the proof of convergence for the Adam optimizer, they spotted a mistake in the update rule that could cause the algorithm to converge to a sub-optimal point\nupdate rule of Adam\n\navg_grads = beta1 * avg_grads + (1-beta1) * w.grad \navg_squared = beta2 * (avg_squared) + (1-beta2) * (w.grad ** 2) \nw = w - lr * avg_grads / sqrt(avg_squared)\n\nWe’ve just skipped the bias correction (useful for the beginning of training) to focus on the important point\nThe error in the proof of Adam the authors spotted is that it requires the quantity\n\nlr / sqrt(avg_squared)\n\nwhich is the step we take in the direction of our average gradients, to be decreasing over training\nSince the learning rate is often taken constant or decreasing (except for crazy people like us trying to obtain super-convergence), the fix the authors proposed was to force the avg_squared quantity to be increasing by adding another variable to keep track of their maximums.\nImplementing amsgrad\nThis causes the weight update code from the previous section to be changed to something like this:\n\nResults\n\nAmsgrad turns out to be very disappointing. In none of our experiments did we find that it helped the slightest bit, and even if it’s true that the minimum found by amsgrad is sometimes slightly lower (in terms of loss) than the one reached by Adam, the metrics (accuracy, f1 score…) always end up worse\nThe proof of convergence for the Adam optimizer in deep learning (since it’s for convex problems) and the mistake they found in it mattered for synthetic experiments that have nothing to do with real-\nlife problems. Actual tests show that when those avg_squared gradients want to decrease, it’s best for the final result to do so.\n"},"KB/Amygdala":{"title":"Amygdala","links":[],"tags":["brain"],"content":"Amygdala\n\nPart of the brain’s limbic system, this primitive brain structure lies deep in the center of the brain and is involved in emotional reactions, such as anger or fear, as well as emotionally charged memories. It also influences behavior such as feeding, sexual interest, and the immediate “fight or flight” stress reaction that helps ensure the person’s needs are met.\n"},"KB/Amyloid-Plaque":{"title":"Amyloid Plaque","links":[],"tags":["brain"],"content":"Amyloid Plaque\n\nThe sticky, abnormal accumulations of amyloid-beta protein aggregate around neurons and synapses in the memory and intellectual centers of the brain, in people with Alzheimer’s. These are sometimes referred to as neuritic plaques or senile plaques. While amyloid plaques have long been considered markers of Alzheimer’s, they are also found to some extent in many cognitively normal elderly people. The plaques’ role in Alzheimer’s neurodegeneration remains unclear.\n"},"KB/Amyloid-beta-(Aβ)-Protein":{"title":"Amyloid-beta (Aβ) Protein","links":["KB/Alzheimer’s-Disease"],"tags":["brain"],"content":"Amyloid-beta (Aβ) Protein\n\nA naturally occurring protein in brain cells. Large, abnormal clumps of this protein form the amyloid plaques that are a physiological hallmark of Alzheimer’s Disease.\n"},"KB/Amyotrophic-Lateral-Sclerosis-(ALS)":{"title":"Amyotrophic Lateral Sclerosis (ALS)","links":[],"tags":["brain"],"content":"Amyotrophic Lateral Sclerosis (ALS)\n\nAlso known as Lou Gehrig’s disease, this neurodegenerative disease results in the death of brain cells that control the muscles.\n"},"KB/Analysis-of-Explainers-of-Black-Box-Deep-Neural-Networks-for-Computer-Vision-A-Survey":{"title":"Analysis of Explainers of Black Box Deep Neural Networks for Computer Vision A Survey","links":["KB/GAM","KB/Partial-Dependence-Plot","KB/Salience-Map","KB/Explanator","KB/Comprehensibility","KB/Causality","KB/Causability","KB/Bayesian-Rule-List","KB/TREPAN","KB/Counterfactual-Impact-Evaluation","KB/DeconvNet","KB/Layerwise-Relevance-Propagation","KB/Parent-Approximations","KB/RETAIn","KB/SP-LIME","KB/Deep-Visual-Explanation","KB/Prediction-Difference-Analysis","KB/Smooth-Grad","KB/Multimodal-Explanation","KB/Summit","KB/DeepFool","KB/LIME"],"tags":[],"content":"\n10:14 ---\ntoc: true\ntitle: Analysis of Explainers of Black Box Deep Neural Networks for Computer Vision A Survey\n\ntags: [‘explainability’]\ndate modified: Wednesday, October 12th 2022, 4:48:43 pm\ndate created: Wednesday, October 12th 2022, 4:11:22 pm\nAnalysis of Explainers of Black Box Deep Neural Networks for Computer Vision A Survey\n\n@Analysis of Explainers of Black Box Deep Neural Networks for Computer Vision: A Survey\nVanessa Buhrmester , David Münch and Michael Arens\n \n\n\nExplaining Systems\n\nGAM\nPartial Dependence Plot\nSalience Map\nExplanator\nComprehensibility\nCausality\nCausability\nBayesian Rule List\nTREPAN\n\nSelected DNN Explainers\n\nCounterfactual Impact Evaluation\nDeconvNet\nLayerwise Relevance Propagation\nParent Approximations\nRETAIn\nSP-LIME\nDeep Visual Explanation\nPrediction Difference Analysis\nSmooth-Grad\nMultimodal Explanation\nSummit\nDeepFool\nDeconvNet\nLIME\n\nPictures\n\n\n\n"},"KB/Andes":{"title":"Andes","links":["KB/Algebra-Cognitive-Tutor"],"tags":["usermodel"],"content":"Andes\n\nOuter loop: The Andes physics tutoring system (www.andes.pitt.edu/) helps students learn how to solve physics problems (K. VanLehn et al., 2005; K. VanLehn et al., 2002).\nInner loop: The student solves the problem by making steps similar to the ones they would make if solving the problem with pencil and paper. One kind of step is to type an equation into one of the numbered boxes in the right window. Another kind of step is to draw a Cartesian coordinate system, such as the one showing in the lower left. A third kind of step is to sketch a vector, then fill out a dialogue box that defines it. A vector-drawing operation was in progress at the time the screen shot was taken, so its dialogue box covers part of the screen.\nEvery time the student makes a step, Andes gives immediate feedback. For most types of steps, Andes merely colors the step green if it is correct and red if it is incorrect.\nStep analysis: Like the Algebra Cognitive Tutor, Andes analyzes non-equation steps by precomputing anticipated steps and matching the student’s step against them\nHowever, this method does not work for the equation steps because there are too many anticipated equations to generate\n"},"KB/Angina":{"title":"Angina","links":[],"tags":["medical"],"content":"Angina\n\nIntermittent chest pain normally caused by insufficient blood flow to the heart\n"},"KB/Angiography":{"title":"Angiography","links":["KB/medical"],"tags":["brain"],"content":"Angiography\n\nA medical imaging technique that allows clinicians to visualize the interior of blood vessels, arteries, veins, and the heart.\n"},"KB/Anthropomorphic":{"title":"Anthropomorphic","links":[],"tags":["robotics"],"content":"Anthropomorphic\n\nHuman like in shape\nVery large number of joint\nVery large number of DOF\nCompact design\n"},"KB/Aphasia":{"title":"aphasia","links":["KB/language","KB/Stroke"],"tags":["brain"],"content":"Aphasia\n\nDisturbance of language affecting speech production, comprehension, reading or writing, due to brain injury – most commonly from Stroke or trauma.\nThe type of aphasia depends on the brain area damaged\n"},"KB/Apoptosis":{"title":"Apoptosis","links":[],"tags":["brain"],"content":"Apoptosis\n\nA form of programmed cell death that occurs as part of normal growth and development.\n"},"KB/Appendectomy":{"title":"Appendectomy","links":[],"tags":["medical"],"content":"Appendectomy\n\nSurgical procedure to remove the appendix\n"},"KB/Application-dependence":{"title":"Application dependence","links":["KB/Sentence-Segmentation","KB/Tokenizer"],"tags":["language"],"content":"Application Dependence\n\nWord and Sentence Segmentation are necessary\nTokenizer\n"},"KB/Application-for-PhD---AI-for-Parkinsons-(Subhaditya-Mukherjee)":{"title":"Application for PhD - AI for Parkinsons (Subhaditya Mukherjee)","links":[],"tags":[],"content":"Hello Yagmur!\nMy name is Subhaditya, it’s nice to meet you :)\nI recently graduated with a masters in AI from the RUG (Groningen), and am now looking for a position where I can bring my AI knowledge to healthcare. I found out about this position quite coincidentally a few hours ago, and I knew that I could not miss the chance to apply. There is a history of Parkinsons in my family and while I could do nothing about it as a kid, now I can help contribute to the research. So, here is my formal application to the PhD position :’ Artificial intelligence for monitoring of Parkinson’s disease’.\nI read that a lot of data was collected over the last few years using a smartwatch and video in the hope of gaining insights that would potentially help a lot of people. Here is where I can contribute to the most. I am not a healthcare professional, and all these years there wasn’t any skill I could bring to the medical side of innovation. But now I have quite a bit of experience in both Computer Vision and Data Analysis. I am comfortable with using deep learning libraries, data analysis pipelines and python to create whatever is necessary for the project. My main skills lie in building these pipelines, analyzing data and subsequently using AI. Over the past few years, I have done many freelance projects, each of which has given me quite a bit of knowledge about using AI in different domains. I think that that knowledge will be useful here too.\nSince I am familiar with the pipeline, perhaps I can also help anticipate what might be needed for the future, and ensure we are a little more ready for that. I have also worked quite a bit with vision models and sensor data in past internships and personal projects, which is also useful experience.\nAside from the technical aspects, I love interdisciplinary research. No field is good on it’s own, AI even less so. But that being the case, I acknowledge my lack of information about medical terms. I do think that is an easy fix though, and I’m willing to put in the effort to learn that side as best I can. This position is the perfect next step for me, as someone who is trying to find a position that bridges the gap between a pure AI background to something more useful, healthcare. I read that your lab is part of the ICAI network as well, and Verity healthcare too. That’s so awesome and helpful.\nI have done many projects and internships over the past few years, but for most of the time, it did not feel like my work amounted to anything. This is my shot at making it count, and helping make some lives a little better. I know I applied pretty late and you probably have a lot of applications already, but I hope you give me a chance!"},"KB/Applications-of-Knowledge-Distillation":{"title":"Applications of Knowledge Distillation","links":["KB/Denoising-Autoencoder"],"tags":["knowledgedistillation"],"content":"Applications of Knowledge Distillation\n\nSpecifically, in (Luo et al., 2016), the knowledge from the chosen informative neurons of top hint layer of the teacher network is trans- ferred into the student network\nA recursive knowledge distillation method was designed by using a previous student network to ini- tialize the next one (Yan et al., 2019). Since most face recognition methods perform the open-set recognition, i.e., the classes/identities on test set are unknown to the training set, the face recognition criteria are usually distance metrics between feature representations of positive and negtive samples, e.g., the angular loss in (Duong et al., 2019) and the correlated embedding loss in (Wu et al., 2020).\nSpecifically, Ge et al. (2018) proposed a selective knowledge distillation method, in which the teacher network for high-resolution face recognition selectively transfers its informative facial features into the student network for low-resolution face recognition through sparse graph optimization. In (Kong et al., 2019), cross- resolution face recognition was realized by designing a resolution invariant model unifying both face halluci- nation and heterogeneous recognition sub-nets. To get efficient and effective low resolution face recognition model, the multi-kernel maximum mean discrepancy between student and teacher networks was adopted as the feature loss (Wang et al., 2019c).\nKD-based face recognition can be extended to face alignment and verification by changing the losses in knowledge distillation (Wang et al., 2017).\nFor incomplete, ambiguous and redundant image labels, the label refinery model through self-distillation and label progression is proposed to learn soft, informative, collective and dynamic labels for complex image classi- fication (Bagherinezhad et al., 2018).\nTo address catas- trophic forgetting with CNN in a variety of image clas- sification tasks, a learning without forgetting method for CNN, including both knowledge distillation and lifelong learning is proposed to recognize a new image task and to preserve the original tasks (Li and Hoiem, 2017).\nFor improving image classification accuracy, Chen et al. (2018a) proposed the feature maps-based knowledge distillation method with GAN.\nSimilar to the KD-based low-resolution face recognition, Zhu et al. (2019) proposed deep feature distillation for the low-resolution image classification, in which the output features of a student match that of teacher.\nGordon and Duh (2019) explained the good perfor- mance of sequence-level knowledge distillation from the perspective of data augmentation and regulariza- tion. In (Kim and Rush, 2016), the effective word-level knowledge distillation is extended to the sequence- level one in the sequence generation scenario of NMT. The sequence generation student model mimics the sequence distribution of the teacher. To overcome the multilingual diversity, Tan et al. (2019) proposed multi teacher distillation, in which multiple individual models for handling bilingual pairs are teacher and a mul- tilingual model is student.\nTo improve the transla- tion quality, an ensemble of mutiple NMT models as teacher supervise the student model with a data filtering method Freitag et al. (2017).\n(Wei et al., 2019) proposed a novel online knowledge distillation method, which addresses the unstableness of the training process and the decreasing performance on each validation set.\nThe proposed pre- trained distillation performs well in sentiment classifi- cation, natural language inference, textual entailment. For a multi-task distillation in the context of natu- ral language understanding, Clark et al. (2019) pro- posed the single-multi born-again distillation, which is based on born-again neural networks (Furlanello et al., 2018).\nIn (Perez et al., 2020), a audio-visual multi- modal knowledge distillation method is proposed. knowl- edge is transferred from the teacher models on visual and acoustic data into a student model on audio data.\nPan et al. (2019) designed a enhanced collaborative Denoising Autoencoder (ECAE) model for recommender systems via knowledge distillation to capture useful knowledge from user feedbacks and to reduce noise. The unified ECAE framework contains a generation network, a retraining network and a distillation layer that trans- fers knowledge and reduces noise from the generation network.\n"},"KB/Approximately-Compositional-Semantic-Parsing":{"title":"Approximately Compositional Semantic Parsing","links":["KB/Semantic-Analysis"],"tags":["language"],"content":"Approximately Compositional Semantic Parsing\n\nwhich Semantic Analysis processing is applied to the result of performing a syntactic parse\n"},"KB/Arbitrary-Relation-Bias":{"title":"Arbitrary Relation Bias","links":["KB/Graphs"],"tags":["graph"],"content":"Arbitrary Relation Bias\n\nTo solve problems related to a group of things or people, it might be more informative to see them as a Graphs. The graph structure imposes arbitrary relationships between the entities, which is ideal when there’s no clear sequential or local relation in the model:\n\n"},"KB/Area-Minimization":{"title":"Area Minimization","links":[],"tags":["visualization","graph"],"content":"Area Minimization\n\nSmall areas preferable\naspect ratio can play role\n\n"},"KB/Articulated-Manipulator":{"title":"Articulated Manipulator","links":["KB/Manipulator"],"tags":["robotics"],"content":"Articulated Manipulator\n\nA Manipulator with an arm that is broken into sections (links) by one or more joints. Each of the joints represents a degree of freedom in the Manipulator system and allows translation and rotary motion.\n"},"KB/Articulation":{"title":"Articulation","links":["KB/Manipulator"],"tags":["robotics"],"content":"Articulation\n\nDescribes a jointed device, such as a jointed Manipulator. The joints provide rotation about a vertical axis, and elevation out of the horizontal plane. This allows a robot to be capable of reaching into confined spaces.\n"},"KB/Assembly-Robot":{"title":"Assembly Robot","links":[],"tags":["robotics"],"content":"Assembly Robot\n\nA robot designed specifically for mating, fitting, or otherwise assembling various parts or components into completed products. Primarily used for grasping parts and mating or fitting them together, such as in assembly line production.\n"},"KB/Astrocyte":{"title":"Astrocyte","links":[],"tags":["brain"],"content":"Astrocyte\n\nA star-shaped glial cell that supports neurons, by helping to both feed and remove waste from the cell, and otherwise modulates the activity of the neuron. Astrocytes also play critical roles in brain development and the creation of synapses.\n"},"KB/Asymptotic-Decider":{"title":"Asymptotic Decider","links":[],"tags":["visualization"],"content":"Asymptotic Decider\n\n\nConsider the bilinear interpolant within cell\nthe true isolines within a cell are hyperbolas\ninvestigate order of intersection points along x or y axis\nbuild pairs of first two and last two intersections\n"},"KB/Atrous-Convolution":{"title":"Atrous Convolution","links":[],"tags":["architecture"],"content":"Atrous Convolution\n \n\nThe kernel size can be increased to integrate over a larger area\nHowever, it typically remains an odd number so that it can be centered around the current position. Increasing the kernel size has the disadvantage of requiring more weights.\nThis leads to the idea of dilated or atrous convolutions, in which the kernel values are interspersed with zeros.\nThe number of zeros we intersperse between the weights determines the dilation rate\n"},"KB/Attention-Alignment":{"title":"Attention Alignment","links":["KB/Attention","KB/Recurrent","KB/Softmax"],"tags":["loss"],"content":"Attention Alignment\n\nIf there are sequences x, y\n\nEncoder is any Recurrent with a forward state \\overrightarrow h^{T} and \\overleftarrow h^{T} for backward\nConcat them represents the preceding and following word annotations\n\nh_{i}= [\\overrightarrow h_{i}^{T}; \\overleftarrow h_{i}^{T}], i = 1, …, n\nDecoder has hidden state s_{t}= f(s_{t-1}, y_{t-1}, c_{t}) for the output word at position t for t = 1, …, m\n\nContext vector c_{t} is a sum of hidden states of the input seq, weighted by alignment scores\nc_{t}= \\Sigma_{i=1}^{n}\\alpha_{t,i}h_{i}\nHow well the two words are aligned is given by\n\\alpha_{t,i} = align(y_{t}, x_{i})\nTaking Softmax\n\n\\frac{exp(score(s_{t-1}, h_{i}))}{\\Sigma_{i&#039;-1}^{n}exp(score(s_{t-1}, h_{i}&#039;))}\n\n\n\n\n\n\n\n\nf_{att}(h_{i}, s_{j}) = v_{a}^{T}tanh(W_{a}[h_{i};s_{j}])\nv_{a} and W_{a} are the learned Attention params\nh is the hidden state for the encoder\ns is the hidden state for the decoder\nMatrix of alignment\n\n\nFinal scores calculated with a Softmax\n\n\n"},"KB/Attention-Based-Distillation":{"title":"Attention Based Distillation","links":["KB/Attention"],"tags":["knowledgedistillation"],"content":"Attention Based Distillation\n\nThat is to say, knowledge about feature embedding is transferred using Attention map functions. Unlike the Attention maps, a different attentive knowledge distillation method was proposed by Song et al. (2018). An Attention mechanism is used to assign different confidence rules (Song et al., 2018).\n"},"KB/Attention-NMT":{"title":"Attention NMT","links":["KB/Attention","KB/BLEU"],"tags":["architecture"],"content":"Attention NMT\n\nEffective Approaches to Attention-based Neural Machine Translation\n[[Attention|[aman.ai/papers/#neural-machine-translation-by-jointly-learning-to-align-and-translate]] mechanism to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation\na global approach which always attends to all source words and a local one that only looks at a subset of source words at a time\nBLEU\n"},"KB/Attention":{"title":"Attention","links":["KB/Self-Attention","KB/Additive-Attention","KB/Dot-Product-Attention","KB/Location-Aware-Attention","KB/Relative-Multi-Head-Self-Attention","KB/Soft-Attention","KB/Scaled-Dot-Product-Attention","KB/Encoder-Decoder-Attention","KB/Multi-Head-Attention","KB/Strided-Attention","KB/Fixed-Factorization-Attention","KB/Sliding-Window-Attention","KB/Dilated-Sliding-Window-Attention","KB/Global-and-Sliding-Window-Attention","KB/Content-Based-Attention","KB/Location-Base-Attention","KB/Mixed-chunk-attention"],"tags":["architecture"],"content":"Attention\n\nModel can decide where to look in the input\nSelf Attention\nAdditive Attention\nDot Product Attention\nLocation Aware Attention\nRelative Multi Head Self Attention\nSoft Attention\nScaled Dot Product Attention\nEncoder Decoder Attention\nMulti Head Attention\nStrided Attention\nFixed Factorization Attention\nSliding Window Attention\nDilated Sliding Window Attention\nGlobal and Sliding Window Attention\nContent Based Attention\nLocation Base Attention\nMixed chunk attention\n"},"KB/Attentions-and-salience":{"title":"Attentions and salience","links":["KB/Attention","KB/Verb"],"tags":["language"],"content":"Attentions and Salience\n\nLearning is related to Attention\nWe give more Attention to salient items\nSelective Attention leads to overshadowing\nTwo cues presented together jointly predict outcomes\nsalience (and then Attention) leads to one cue being strongly associated\nther cue is only weakly associated (overshadowed)\nOvershadowing leads to blocking\nBlocking could model interference\nChildren learn temporal adverbs like hier late (Dale and Fenson 1996)\nL2 learners go through phases where time is marked with adverbials alone (Bardovi-Harlig 1992; Meisel 1987)\nThis seems to block acquisition of other cues\ntemporal adverbs are highly salient (and easier) so they get stronger associations\neven though they co-occur with different Verb forms, these are hard to learn\n"},"KB/Attentive-CutMix":{"title":"Attentive CutMix","links":["KB/CutMix","KB/Cutout"],"tags":["augmentation"],"content":"Attentive CutMix\n\n@walawalkarAttentiveCutMixEnhanced2020\nbuilds up on CutMix .\nInstead of random pasting, it identifies attentive patches for Cutout and pastes them at the same location in the other image.\navoids the problem of selecting a background region not important for the network and updating the label information\nA separate pre-trained network is employed to extract attentive regions.\nThe attention output is mapped back onto the original image\n"},"KB/Attribute-Selection":{"title":"Atrribute Selection","links":["augment"],"tags":["multitask"],"content":"Atrribute Selection\n\nSome tasks are harder than others due to noise and high dimensionality\nAnother task can help the model to learn a shared feature that is harder to learn with a single task alone.\nhelps models focus into the most important features\nConsequence of augment\n"},"KB/AttributeMix":{"title":"AttributeMix","links":["@Attribute-Mix:-Semantic-Data-Augmentation-for-Fine-Grained-Recognition"],"tags":["augmentation"],"content":"\n\nAttributeMix\n\n@Attribute Mix: Semantic Data Augmentation for Fine Grained Recognition\naugments images based on se- mantically extracted image attributes\nImage is divided into a grid of patches where highly activated six responses are pasted onto the training image. These image pairs are selected randomly in every training iteration.\nattribute classifier by extracting k attributes (e.g., leg, head, and wings of a bird) from each image.\nThe attribute mining procedure for every image is performed repetitively k times, whereas for each iteration, an attribute is masked out from the original image based on the most discriminative region in the attention map.\nattribute-level classifier is trained to generate new images for the actual classification model.\n"},"KB/AudioLM":{"title":"AudioLM","links":[],"tags":["architecture"],"content":"AudioLM\n\nmaps the input audio into a sequence of discrete tokens and casts audio generation as language modeling task in this representation space\ntraining on large corpora of raw\naudio waveforms\nlearns to generate natural and coherent continuations given short prompts\nextended beyond speech by generating coherent piano music continuations, despite being trained without any symbolic representation of music\nWhen it comes to audio synthesis, multiple scales make achieving high audio quality while displaying consistency very challenging\nThis gets achieved by this model by combining recent advances in neural audio compression, self-supervised representation learning and language modelling.\n"},"KB/AudioSet-classification":{"title":"AudioSet classification","links":[],"tags":["dataset"],"content":"AudioSet Classification"},"KB/AudioSet":{"title":"AudioSet","links":[],"tags":["dataset"],"content":"AudioSet\n\n2, 084, 320 human-labeled 10-second sound clips drawn from YouTube videos covers ontology of 632 audio event classes\nThe event classes cover a wide range of human and animal sounds, musical instruments and genres, and common everyday environmental sound\nselfsupervised learning from video and audio consistence\n"},"KB/Auditability":{"title":"Auditability","links":[],"tags":["explainability"],"content":"Auditability\n\nincludes the assessment of algorithms, data and design processes, but preserving the intellectual property related to the AI systems\nPerforming the assessment by both internal and external auditors, and making the reports available, could contribute to the trustworthiness of the technology.\nWhen the AI system affects fundamental rights, including safety-critical applications, it should always be audited by an external third party.\n"},"KB/AugMix":{"title":"AugMix","links":["KB/Jensen-Shannon-Divergence-Consistency-Loss"],"tags":["augmentation"],"content":"AugMix\n\n@hendrycksAugMixSimpleData2020\nusing the input image itself\nt transforms (translate, shear, rotate and etc) the input image and mixes it with the original image\nImage transformation involves series of randomly selected augmentation operations applied with three parallel augmentation chains.\nEach chain has a composition of operations that could involve applying, for example, translation on input image followed by shear and so on\nThe output of these three chains is three images mixed to form a new image.\nThis new image is later mixed with the original image to generate the final augmented output image,\nwhile we considered mixing by alpha compositing, we chose to use elementwise convex combinations for simplicity. The k-dimensional vector of convex coefficients is randomly sampled from a Dirichlet(α, … , α) distribution.\nOnce these images are mixed, we use a “skip connection” to combine the result of the augmentation chain and the original image through a second random convex combination sampled from a Beta(α, α) distribution. The final image incorporates several sources of randomness from the choice of operations, the severity of these operations, the lengths of the augmentation chains, and the mixing weights\nJensen Shannon Divergence Consistency Loss\n"},"KB/Augmentation-wise-Weight-Sharing-strategy":{"title":"Augmentation-wise Weight Sharing strategy","links":[],"tags":["augmentation"],"content":"Augmentation-wise Weight Sharing Strategy\n\nimproves efficiency significantly and make it affordable to directly search on large scale datasets.\n[Lin et al., 2019] formulates the augmentation policy as a parameterized probability distribution and the parameters can be optimized jointly with network parameters, known as OHL-Auto-Aug.\n"},"KB/Augmented-Random-Search":{"title":"Augmented Random Search","links":["KB/AutoAugment"],"tags":["augmentation"],"content":"Augmented Random Search\n\nThe authors point out that the sub-policies learned from AutoAugment are inherently flawed because of the discrete search space.\nThey convert the probability and magnitude of augmentations into a continuous space and search for sub-policies with ARS.\n"},"KB/Auto-Augment":{"title":"Auto Augment","links":["KB/Fast-AutoAugment","KB/Population-Based-Augmentation","KB/RandAugment","KB/Augmentation-wise-Weight-Sharing-strategy"],"tags":["augmentation"],"content":"Auto Augment\n\nsearch algorithm and search space\nThe search algorithm is designed to find the best policy regarding highest validation accuracy\nThe search space contains many policies which details various augmentation operations and magnitudes with which the operations are applied\nFast AutoAugment\nPopulation Based Augmentation\nRandAugment\nKeepAugment\nAugmentation-wise Weight Sharing strategy\n"},"KB/Auto-Encoders":{"title":"AutoEncoder","links":["KB/Gradient-Descent","KB/MSE","KB/Dimensionality-Reduction","KB/Denoising-Autoencoder","KB/VAE"],"tags":["architecture"],"content":"AutoEncoder\n\n\nRegression by predicting a reconstruction of the data\nEncoder E : \\mathscr{X} \\rightarrow \\mathscr{F}\nDecoder \\mathscr{F} \\rightarrow \\mathscr{D}\n$$E_\\theta, D_\\theta = argmin_{E_\\theta, D\\theta}||X-D(E(X))||^2$\n\nLearn using Gradient Descent\n\n\nCompressed rep of data → Good for Classification or Regression\nMSE : Unsupervised\n\nDifficulties\n\ndim \\mathscr{F} \\lt \\mathscr{X}\n\nCannot learn the identity function\n\n\nusages\n\ndata compression / Dimensionality Reduction\nencoder to obtain features (use the latent variable as feature)\ndenoising autoencoders\n\ninput noisy image and try to obtain image without noise\n\n\nsparse auto-encoder\ncontractive autoencoder\n\n\n\nTypes\n\nDenoising Autoencoder\nVAE\n"},"KB/AutoAugment":{"title":"AutoAugment","links":["KB/Neural-Augmentation","KB/Geometric-Transformations"],"tags":["augmentation"],"content":"AutoAugment\n\ndeveloped by Cubuk et al.\nmuch different approach to meta-learning than Neural Augmentation\nAutoAugment is a Reinforcement Learning algorithm that searches for an optimal augmentation policy amongst a constrained set of Geometric Transformations with miscellaneous levels of distortions. For example, ‘translateX 20 pixels’ could be one of the transformations in the search space\nIn Reinforcement Learning algorithms, a policy is analogous to the strategy of the learning algorithm. This policy determines what actions to take at given states to achieve some goal. The AutoAugment approach learns a policy which consists of many subpolicies, each sub-policy consisting of an image transformation and a magnitude of transformation\nReinforcement Learning is thus used as a discrete search algorithm of augmentations.\n"},"KB/AutoDistill":{"title":"AutoDistill","links":["KB/Knowledge-Distillation","KB/Bayesian","KB/GLUE","KB/TinyBERT"],"tags":["architecture"],"content":"AutoDistill\n\nAutoDistill: an End-to-End Framework to Explore and Distill Hardware-Efficient Language Models\n(NLP) tasks but they are expensive to serve due to long serving latency and large memory usage\ncompress these models, Knowledge Distillation\nhandling fast evolving models, considering serving performance, and optimizing for multiple objectives.\nend-to-end model distillation framework integrating model architecture exploration and multi-objective optimization for building hardware-efficient NLP pre-trained models\nBayesian Optimization to conduct multi-objective Neural Architecture Search for selecting student model architectures\nproposed search comprehensively considers both prediction accuracy and serving latency on target hardware\nTPUv4i\nMobileBERT\nGLUE\nhigher than BERT_BASE, DistillBERT, TinyBERT, NAS-BERT, and MobileBERT\n"},"KB/AutoML-Benchmark":{"title":"AutoML Benchmark","links":[],"tags":["openml,-automl"],"content":"AutoML Benchmark\n\n@gijsbersAMLBAutoMLBenchmark2023\n"},"KB/AutoTutor":{"title":"AutoTutor","links":["KB/Knowledge-Component","KB/Learning-Event","KB/Latent-Semantic-Analysis"],"tags":["usermodel"],"content":"AutoTutor\n\nOuter loop: AutoTutor (demo.autotutor.org/) teaches by engaging students in a natural language (English) dialogue\nFor AutoTutor, a task corresponds to a single question, such as the one shown in the upper right of Figure 4, that has a complex answer. Its outer loop consists of selecting such a question and working with the student to get it completely answered.\nInner loop: The inner loop starts with the student typing in an initial answer to the top level question (see Figure 4; the student types into the lower right window; the whole dialogue is displayed in the lower left window).\nAutoTutor has been used to compare output modalities.\nAn AutoTutor dialogue is composed of tutor turns alternating with student turns. On most of the student turns, the student makes a small contribution toward completing the whole task. Those student turns count as steps, because they are a user interface event that contributes to a solution of the whole task\nStep analysis:\nThese are conclusions that are produced by applying knowledge components. For instance, the first two items above correspond to distinct learning events, wherein the student has applied the same Knowledge Component,\nIn addition to having a list of all anticipated correct learning events, such as the ones mentioned above, AutoTutor has a list of several of the most important incorrect learning events\nTo find out which learning events underlie the student’s step, AutoTutor measures the semantic similarity between the text of the Learning Event and the text of the step. It uses a measure called Latent Semantic Analysis\n"},"KB/Automation-Bias":{"title":"Automation Bias","links":[],"tags":["temp"],"content":"Automation Bias\n\nWhen a human decision maker favors recommendations made by an automated decision-making system over information made without automation, even when the automated decision-making system makes errors.\n"},"KB/Autonomic":{"title":"Autonomic","links":["KB/Sympathetic","KB/Parasympathetic"],"tags":["brain"],"content":"Autonomic\n\nInvoluntary\nSympathetic + Parasympathetic\n"},"KB/Autoregressive":{"title":"Autoregressive","links":["KB/TIme-Series","KB/Multi-Variate-AR"],"tags":["temp"],"content":"Autoregressive\n\npredict the future by past of TIme Series\nMulti Variate AR\n"},"KB/Average-Filter":{"title":"Average Filter","links":[],"tags":["visualization"],"content":"Average Filter\n\nEach grey value is replaced by the average value in the kernel in a local surrounding\nlinear\nflatten edges\n"},"KB/Average-Number-of-Stored-Instances-per-Category":{"title":"Average Number of Stored Instances per Category","links":[],"tags":["robotics"],"content":"Average Number of Stored Instances per Category\n\nmemory resource required for learning\n"},"KB/Axon-Terminal":{"title":"Axon Terminal","links":["KB/Axon"],"tags":["brain"],"content":"Axon Terminal\n\nThe very end of the Axon, where electrochemical signals are passed through the synapse to neighboring cells by means of neurotransmitters and other neurochemicals. A collection of axons coming from, or going to, a specific brain area may be called a white matter fiber tract.\n"},"KB/Axon":{"title":"Axon","links":["KB/Dendrites"],"tags":["brain"],"content":"Axon\n\nA long, single nerve fiber that transmits messages, via electrochemical impulses, from the body of the neuron to Dendrites of other neurons, or directly to body tissues such as muscles.\n"},"KB/BART":{"title":"BART","links":["KB/Denoising-Autoencoder","KB/GPT","KB/RoBERTa","KB/GLUE","KB/SQuAD"],"tags":["architecture"],"content":"BART\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\nDenoising Autoencoder\npretraining sequence-to-sequence\ntrained by corrupting text with an arbitrary noising function, and learning a model to reconstruct the original text\ngeneralizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder),\nfinding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token\nWith BERT, random tokens are replaced with masks, and the document is encoded bidirectionally. Missing tokens are predicted independently, so BERT cannot easily be used for generation.\nWith GPT, tokens are predicted auto-regressively (generation of a new token is conditioned on the prior tokens), meaning GPT can be used for generation.\nnoising schemes to an input document and thus corrupts it by replacing spans of text with mask symbols\neffective when finetuned for text generation but also works well for comprehension tasks\nmatches the performance of RoBERTa with comparable training resource\nGLUE\nSQuAD\n"},"KB/BCE-with-Logits":{"title":"BCE Logits","links":["KB/Logits","KB/Cross-Entropy"],"tags":["loss"],"content":"BCE Logits\n\nCross Entropy + Logits\n\\left( - \\mathrm{sum}\\left( y \\cdot \\mathrm{logsoftmax}\\left( ŷ \\right) \\cdot weight \\right) \\right) \\cdot \\mathrm{//}\\left( 1, \\mathrm{size}\\left( y, 2 \\right) \\right)\n\n…"},"KB/BERT":{"title":"BERT","links":["KB/Token-Embedding","KB/Self-Supervised","KB/language","Roam-Highlights","KB/Attention","KB/Masked-Language-Modeling","KB/BooksCorpus","KB/English-Wikipedia"],"tags":["architecture"],"content":"BERT\n\nBidirectional Encoder rep from transformers\nUses Token Embedding\nSelf Supervised\nMasked language modeling, next sentence prediction\n\n[CLS] : start of classification task, [SEP] between sentences, [MASK] : masked token\nchristianversloot #Roam-Highlights\n\n\nBERT base \\text{BERT}_\\text{BASE}, which has 12 Encoder Segments stacked on top of each other, has 768-dimensional intermediate state, and utilizes 12 Attention heads (with hence 768/12 = 64-dimensional Attention heads).\n\n\nBERT large (\\text{BERT}_\\text{LARGE}), which has 24 Encoder Segments, 1024-dimensional intermediate state, and 16 Attention heads (64-dimensional Attention heads again).\n\n\nBERT utilizes the encoder segment, meaning that it outputs some vectors T_i for every token. The first vector, T_0, is also called C in the BERT paper: it is the “class vector” that contains sentence-level information (or in the case of multiple sentences, information about the sentence pair). All other vectors are vectors representing information about the specific token.\n\n\nIn other words, structuring BERT this way allows us to perform sentence-level tasks and token-level tasks. If we use BERT and want to work with sentence-level information, we build on top of the C token.\n\n\nMasked Language Modeling\n\n\nNext Sentence Prediction (NSP)\n\nThis task ensures that the model learns sentence-level information. It is also really simple, and is the reason why the BERT inputs can sometimes be a pair of sentences. NSP involves textual entailment, or understanding the relationship between two sentences.\nConstructing a training dataset for this task is simple: given an unlabeled corpus, we take a phrase, and take the next one for the 50% of cases where BERT has a next sentence. We take another phase at random given A for the 50% where this is not the case (Devlin et al., 2018). This way, we can construct a dataset where there is a 50/50 split between ‘is next’ and ‘is not next’ sentences.\n\n\n\nBooksCorpus\n\n\nEnglish Wikipedia\n\n\nIt thus does not matter whether your downstream task involves single text or text pairs: BERT can handle it.\n\nSentence pairs in paraphrasing tasks.\nHypothesis-premise pairs in textual entailment tasks.\nQuestion-answer pairs in question answering.\nText-empty pair in text classification.\n\n\n\nYes, you read it right: sentence B is empty if your goal is to fine-tune for text classification. There simply is no sentence after the token.\n\n\nFine-tuning is also really inexpensive\n\n\n\n"},"KB/BEiT":{"title":"BEiT","links":["KB/Self-Supervised","KB/Encoder-Decoder-Attention","KB/Vision-Transformer","KB/VAE","KB/Tokenizer","KB/BERT","KB/Transformer","KB/Layers","KB/ImageNet","KB/DeiT"],"tags":["temp"],"content":"\ntoc: true\ntitle: BEiT\ntags: [‘temp’]\n\nBEiT\n\nBEiT: BERT Pre-Training of Image Transformers\n\nSelf Supervised pre-trained representation model\nBidirectional Encoder Decoder Attention representations from Vision Transformer\nmasked image modeling task to pretrain vision Transformers\neach image has two views in their pre-training\nthe embeddings of which are calculated as linear projections of flattened patches\nvisual tokens\ndiscrete VAE (dVAE) which acts as an “image Tokenizer” learnt via autoencoding-style reconstruction\ninput image is tokenized into discrete visual tokens obtained by the latent codes of the discrete VAE\nproposed method is critical to make BERT like pre-training (i.e., auto-encoding with masked input) work well for image Transformers\nautomatically acquired knowledge about semantic regions, without using any human-annotated data\nrandomly masks some image patches and feeds them into the backbone Transformer\npre-training objective is to recover the original visual tokens based on the corrupted image patches\ndirectly fine-tune the model parameters on downstream tasks by appending task Layers upon the pretrained encoder\nImageNet\noutperforming from-scratch DeiT\n\n\n"},"KB/BLEU":{"title":"BLEU","links":[],"tags":["loss"],"content":"BLEU"},"KB/BOLD":{"title":"BOLD","links":["KB/MRI"],"tags":["brain"],"content":"BOLD\n\nBlood oxygenation level dependant signal\n\nIndirect measure of neural activity\nBlood goes to a place\n\n\nWhen neurons fire or increase their firing rate, they draw on oxygen and various nutrients.\nThe circulatory system of the brain reacts by sending the region that just fired more highly-oxygenated blood than is needed. This results in an increased blood oxygen level in the activated region.\nWith right pulse sequence, an MRI scanner is able to detect this difference in blood oxygen level\nFactors such as drugs, substances cite_note-3 and excitation cite_note-4 have been shown to increase BOLD response. Conversely, age and brain pathology cite_note-5 have been shown to decrease BOLD response\n"},"KB/BUCC":{"title":"BUCC","links":[],"tags":["dataset"],"content":"BUCC"},"KB/BYOL-Loss":{"title":"BYOL Loss","links":["KB/BYOL","loss"],"tags":["temp"],"content":"\ntoc: true\ntitle: BYOL Loss\ntags: [‘temp’]\n\nBYOL Loss\n\nSimilarity loss between q_\\theta (z_\\theta) and sg(z^{&#039;}_{\\xi})\n\\theta is trained weights\n\\xi is exponentially moving average of \\theta and sg is stop gradient\nf_\\theta is discarded, y_\\theta is used as image representation\n\n"},"KB/BYOL":{"title":"BYOL","links":["KB/Self-Supervised","Online-Learning","KB/ImageNet","KB/Res-Net","Augmentation","KB/BYOL-Loss"],"tags":["temp"],"content":"\ntoc: true\ntitle: BYOL\ntags: [‘temp’]\n\nBYOL\n\nBootstrap Your Own Latent: a New Approach to Self-supervised Learning\n\nSelf Supervised image representation learning\npredicting previous versions of its outputs, without using negative pairs\ntwo neural networks, referred to as online and target networks\nthat interact and learn from each other\nFrom an augmented view of an image, they train the Online Learning network to predict the target network representation of the same image under a different augmented view\nupdate the target network with a slow-moving average of the online network\nImageNet\nRes Net\ndependent on existing sets of Augmentation that are specific to vision applications\nBYOL Loss\n\n\n"},"KB/Back-Propamine":{"title":"backpropamine","links":[],"tags":["explainability"],"content":"Backpropamine\n\n@miconiBackpropamineTrainingSelfmodifying2020\n\nABSTRACT\n\nThe impressive lifelong learning in animal brains is primarily enabled by plastic changes in synaptic connectivity\nImportantly, these changes are not passive, but are actively controlled by neuromodulation, which is itself under the control of the brain\nThe resulting self-modifying abilities of the brain play an important role in learningand adaptation, and are a major basis for biological reinforcement learning\nartificial neural networks with such neuromodulated plasticity can be trained withgradient descen\ndifferentiable Hebbian plasticity\nneuromodulated plasticity improves the performance of neural networks on bothreinforcement learning and supervised learning tasks\nneuromodulated plastic LSTMs with millions of parameters outperform standard LSTMs on a benchmark language modeling task\n\nBACKGROUND: DIFFERENTIABLE HEBBIAN PLASTICITY\n\n(Miconi, 2016; Miconi et al., 2018)\nallows gradient descent to optimize not just the weights, but also the plasticity of each connection\neach connection in the network is augmented with a Hebbian plastic component that grows and decays automatically as a result of ongoing activity. In effect, each connection contains a fixed and a plastic component:\nHebbi,j is initialized to zero at the beginning of each episode/lifetime, and is updatedautomatically\npurely episodic/intra-life quantity\nwi,j, fi,j and ⌘ are the structural components of the network, which are optimizedby gradient descent between episodes/lifetimes to minimize the expected loss overan episode.\nClip(x) in Eq. 2 is any function or procedure that constrains Hebbi,j to the [1, 1]range, to negate the inherent instability of Hebbian learning.\ndistinction between the ⌘ and fi,j parameters: ⌘ is the intra-life “learning rate” ofplastic connections\ndetermines how fast new information is incorporated into the plastic component\nfi,j is a scale parameter, which determines the maximum magnitude of the plasticcomponent (since Hebbi,j is constrained to the [-1,1] range)\nin contrast to other approaches using uniform plasticity (Schmidhuber, 1993a),including “fast weights” (Ba et al., 2016), the amount of plasticity in each connection(represented by fi,j ) is trainable, allowing the meta-optimizer to design complexlearning strategies\nimplementing a plastic recurrent network only requires less than four additional linesof code over a standard recurrent network implementation\n\nBACKPROPAMINE: DIFFERENTIABLE NEUROMODULATION OF PLASTICITY\n\nplasticity is modulated on a moment-to-moment basis by a network controlled neuromodulatory signal M (t)\nThe computation of M (t) could be done in various ways; at present, it is simply asingle scalar output of the network, which is used either directly (for the simple RLtasks) or passed through a meta-learned vector of weights (one for eachconnection, for the language modeling task)\n\nSIMPLE NEUROMODULATION\n\nmake the (global) ⌘ parameter depend on the output of one or more neurons in the network\nBecause ⌘ essentially determines the rate of plastic change, placing it under network control allows the network to determine how plastic connections should beat any given time.\nonly modification to the equations above in this simple neuromodulation variant is to replace ⌘ in Eq. 2 with the network-computed, time-varying neuromodulatory signalM (t)\n\n\nRETROACTIVE NEUROMODULATION AND ELIGIBILITY TRACES\n\nalternative neuromodulation scheme that takes inspiration from the short-term retroactive effects of neuromodulatory dopamine on Hebbian plasticity in animalbrains\ndopamine was shown to retroactively gate the plasticity induced by past activity,within a short time window of about 1s (Yagishita et al., 2014; He et al., 2015; Fisheret al., 2017; Cassenaer &amp; Laurent, 2012)\nThus, Hebbian plasticity does not directly modify the synaptic weights, but creates a fast-decaying “potential” weight change, which is only incorporated into the actual weights if the synapse receives dopamine within a short time window\neligibility trace\nkeeping memory of which synapses contributed to recent activity, while thedopamine signal modulates the transformation of these eligibility traces into actualplastic changes.\n\n"},"KB/Back-To-Front-Raycasting":{"title":"Back To Front Raycasting","links":["KB/Raycasting"],"tags":["visualization"],"content":"Back To Front Raycasting\n\n\nblending over operator for semi-transparent geometry\n"},"KB/Backprop":{"title":"Backprop","links":["loss","tags/architecture","KB/Activation-Functions"],"tags":["loss","architecture"],"content":"Backprop\n\nGradient \\nabla l(\\theta) = [\\frac{\\partial l}{\\partial \\theta_1}(\\theta) , … , \\frac{\\partial L}{\\partial \\theta_L}(\\theta)]\n\npartial derivs of the loss wrt weights\nForward pass\n\nStore result of operation in u\n\n\nBackward Pass\n\nTraverse the graph backwards\n\nChain Rule : \\frac{dl}{d\\theta_i} = \\Sigma_{k \\in parents(l)} \\frac{\\partial l}{\\partial u_k} \\frac{\\partial u_k}{\\partial \\theta_i}\n\\begin{align} &amp;\\frac{d\\hat y}{d\\mathbf{W_1}}\\\\ &amp;= \\frac{\\partial \\hat y}{\\partial u_2} \\frac{\\partial u_2}{\\partial h_1} \\frac{\\partial h_1}{\\partial u_1} \\frac{\\partial u_1}{\\partial \\mathbf{W_1}} \\\\ &amp;= \\frac{\\partial \\sigma (u2)}{\\partial u_2} \\frac{\\partial \\mathbf{W}^T_2 h_1}{\\partial h_1} \\frac{\\partial \\sigma (u1)}{\\partial u_1} \\frac{\\partial \\mathbf{W}^T_1 x}{\\partial \\mathbf{W}_1} \\end{align}\nCollecting all the \\partial \\sigma(u_i) wrt params →architecture exponentially decreases wrt depth of the network : Vanishing\n\nSolved by Activation Functions\n\n\n\n\n\n\n\n\n"},"KB/Bag-of-Words-robotics":{"title":"Bag of Words Robotics","links":["KB/Clustering"],"tags":["robotics"],"content":"Bag of Words Robotics\n\nRecognizing objects using local descriptors would be computationally expensive.\nThe number of local features for a given object mainly depends on the size of the object, and therefore, varies for different objects.\nThe key idea for fast 3D object recognition is to use mechanisms for representing objects in a compact and uniform format (e.g., histogram).\nIf we represent objects in a uniform format, then we can apply ML algorithms\nCompute local features for all the discovered objects and make a pool of features.\nA dictionary is generated via Clustering of the pool of features into N clusters (the number of the clusters is the codebook size).\nVisual word are then defined as the centres of the extracted clusters.\nFinally, each object is described (abstracted) by a histogram of occurrences of these visual words.\n\n\n"},"KB/Bag-of-n-grams":{"title":"Bag of n-grams","links":["KB/Sparsity"],"tags":["nlp"],"content":"Bag of N-grams\n\nconsider word phrases of length n to represent documents as fixed-length vectors to capture local word order\nsuffer from data Sparsity and high dimensionality.\n"},"KB/Bag-of-words":{"title":"Bag of words","links":["KB/Bag-of-n-grams","KB/Curse-Of-Dimensionality"],"tags":["nlp"],"content":"Bag of Words\nExplained\n\nCount based conversion of document into fixed length vectors of integers\nJohn likes to watch movies. Mary likes movies too.\n\n[1, 2, 1, 1, 2, 1, 1, 0, 0, 0, 0]\n\n\nJohn also likes to watch football games. Mary hates football.\n\n[1, 1, 1, 1, 0, 1, 0, 1, 2, 1, 1]\n\n\nOrder is arbitary\n\nDisadvantages\n\nLose all info about word order\nDoes not learn meaning of the words, so distance isnt very accurate\nSomewhat solved by Bag of n-grams\nCurse Of Dimensionality\n"},"KB/Bahdanau-Attention":{"title":"Bahdanau Attention","links":["KB/Attention","KB/Additive-Attention"],"tags":["architecture"],"content":"Bahdanau Attention\n\nNeural Machine Translation by Jointly Learning to Align and Translate\n\nAttention mechanism (borrowed from the field of information retrieval) within the context of NLP\n\n\nSame as Additive Attention\n\n"},"KB/Banach-fixed-point-theorem":{"title":"Banach fixed point theorem","links":[],"tags":["algebra"],"content":"Banach Fixed point Theorem\n \n\n\nA contraction mapping f[\\bullet] has the property\n\n"},"KB/Barycentric-Interpolation":{"title":"Barycentric Interpolation","links":["KB/Interpolation","KB/Affine-Function"],"tags":["visualization"],"content":"Barycentric Interpolation\n\n\nd+1 points\nPoint x is an Affine Function of x_i\n"},"KB/Basal-Ganglia":{"title":"Basal Ganglia","links":["KB/Cerebellum","KB/Hypothalamus","KB/Pituitary-gland","KB/Pineal-gland","KB/Thalamus","KB/Limbic-system"],"tags":["brain"],"content":"Basal Ganglia\n\nincludes the caudate, putamen and globus pallidus. These nuclei work with the Cerebellum to coordinate fine motions, such as fingertip movements.\nHypothalamus\nPituitary gland\nPineal gland\nThalamus\nLimbic system\n\nBasal Ganglia\n\nA group of structures below the cortex involved in motor, cognitive, and emotional functions.\n"},"KB/Base-Link":{"title":"Base Link","links":[],"tags":["robotics"],"content":"Base Link\n\nThe stationary base structure of a robot arm that supports the first joint.\n"},"KB/Basic-GAN":{"title":"Basic GAN","links":["KB/Markov-Chain","KB/Adversarial-Learning","KB/Gradient-Descent","KB/Gradient-Ascent","KB/Issues","KB/Mode-Collapse"],"tags":["architecture"],"content":"Basic GAN\n\nGenerative Adversarial Networks\nLearn a prob distribution directly from data generated by that distribution\nno need for any Markov Chain or unrolled approximate inference networks during either training or generation of samples\nAdversarial Learning\n\nMin Max game max_D min_G V(G,D) where V(G,D) = \\mathbb{E}_{p_{data}(x)}logD(x) + \\mathbb{E}_{p_{data}(x)}log(1-D(x))\nG : Gradient Descent\nD : Gradient Ascent\nDiscriminator Loss (Given Generator)\n\nL_{disc}(D_\\theta) =-\\frac{1}{2}(\\mathbb{E}_{x\\sim p_{real}(x)}[log(D_{theta}(x))] + \\mathbb{E}_{x\\sim p_{latent}(x)}[log(1- D_\\theta(G_\\phi(z)))])\n\n\nGenerator Loss (Given Discriminator)\n\nL_{gen}(G_{\\phi})= - \\mathbb{E}_{z\\sim p_{latent}(z)}[log(D_\\theta(G_\\phi(z)))]\nThis is low if Discriminator is fooled by Gen, D_{\\theta}(x_{gen}) \\approx 1\n\n\n\nTraining\n\npick mini batch of samples \nupdate discriminator with Gradient Descent based** on discriminator loss with generator obtained from previous update\nupdate the generator with Gradient Descent based on generator loss with the discriminator from the previous step\n\nIssues\n\nMode Collapse\n"},"KB/Basic-RNN-Architectures":{"title":"Basic RNN Architectures","links":["KB/Recurrent","KB/SRN","KB/Stacking-RNN","KB/Bi-Directional-RNN","KB/Seq2Seq","KB/Temporal-Conv","KB/GRU"],"tags":["architecture"],"content":"Basic RNN Architectures\n\n\nRecurrent\nSRN\nStacking RNN\nBi Directional RNN\nSeq2Seq\nTemporal Conv\n[[GRU|[GRU|GRU|[GRU|[GRU|GRU|[GRU|[GRU|GRU|[GRU|[GRU](GRU.md|[Gated Recurrent Unit (GRU|Gated Recurrent Unit (GRU|[GRU|GRU|[GRU|[GRU|GRU|[GRU|Gated Recurrent Unit (GRU|[GRU|GRU|[GRU|[GRU|GRU|[GRU|[GRU|GRU|[GRU|[GRU](GRU.md|[GRU|GRU|[GRU|[GRU|GRU|[GRU|Gated Recurrent Unit (GRU|[GRU|GRU|[GRU|[GRU|GRU|[GRU|[GRU|GRU|[GRU|[GRU]]](Gated Recurrent Unit (GRU|Gated Recurrent Unit (GRU|GRU.md|[GRU|Gated Recurrent Unit (GRU|[GRU|GRU|[GRU|[GRU|GRU|[GRU|Gated Recurrent Unit (GRU|[GRU|GRU|[GRU|[GRU|GRU|[GRU|[GRU|GRU|[GRU|[GRU]].md)\n[[LSTM|Long Short Term Memory (LSTM|[LSTM|LSTM|[LSTM|[LSTM|LSTM|[LSTM|Long Short Term Memory (LSTM|[LSTM|LSTM|[LSTM|[LSTM|LSTM|[LSTM|[LSTM|LSTM|[LSTM|[LSTM|LSTM|[LSTM|Long Short Term Memory (LSTM|[LSTM|LSTM|[LSTM|[LSTM|LSTM|[LSTM|Long Short Term Memory (LSTM|[LSTM|LSTM|[LSTM|[LSTM|LSTM|[LSTM|[LSTM|LSTM|[LSTM|[LSTM|LSTM|[LSTM|[LSTM|LSTM|[LSTM|[LSTM|LSTM|[LSTM|Long Short Term Memory (LSTM|[LSTM|LSTM|[LSTM|[LSTM|LSTM|[LSTM|[LSTM|LSTM|[LSTM|[LSTM|LSTM|[LSTM|[LSTM|LSTM|[LSTM|[LSTM|LSTM|[LSTM|[LSTM|LSTM|[LSTM|[LSTM|LSTM)]]]]]]]]](Long Short Term Memory (LSTM|Long Short Term Memory (LSTM|[[LSTM|LSTM|[LSTM|[LSTM|[Long Short Term Memory (LSTM|Long Short Term Memory (LSTM|[LSTM|LSTM|[LSTM|[LSTM|LSTM|[LSTM|Long Short Term Memory (LSTM|[LSTM|LSTM|[LSTM|[LSTM|LSTM|[LSTM|[LSTM|LSTM|[LSTM|[LSTM|LSTM|[LSTM|Long Short Term Memory (LSTM|[LSTM|LSTM|[LSTM|[LSTM|LSTM|[LSTM|Long Short Term Memory (LSTM|[LSTM|LSTM|[LSTM|[LSTM|LSTM|[LSTM|[LSTM|LSTM|[LSTM|[LSTM|LSTM|[LSTM|[LSTM|LSTM|[LSTM|[LSTM|LSTM|[LSTM|Long Short Term Memory (LSTM|[LSTM|LSTM|[LSTM|[LSTM|LSTM|[LSTM|[LSTM|LSTM|[LSTM|[LSTM|LSTM|[LSTM|[LSTM|LSTM|[LSTM|[LSTM|LSTM|[LSTM|[LSTM|LSTM|[LSTM|[LSTM|LSTM)]]]].md).md)\n"},"KB/Basic-Transformer":{"title":"Basic Transformer","links":["KB/Transformer","KB/Dense","KB/Relu","KB/Attention","KB/Embedding","KB/Layers","KB/Position-Encoding","KB/Token-Embedding","KB/Position-Wise-Feed-Forward"],"tags":["architecture"],"content":"Basic Transformer\n\n\n\nFeed forward blocks, are two Dense MLPs with Relu. Residual connections in between\nUses Attention\nEmbedding Layers transform between 1 hot and vector rep\nPosition Encoding + Token Embedding\nPosition Wise Feed Forward\n"},"KB/Basics-of-Federated-Learning":{"title":"Basics of Federated Learning","links":["KB/Federated-Learning"],"tags":["federatedlearning"],"content":"Basics of Federated Learning\n\nGet data (Hopefully a lot)\nPreprocess (aka clean up) the data\nFind/create an architecture\nTrain the model using the data(1) and the architecture(3). This step is done once. And then periodically updated as the data changes over time. Keep this in mind.\nPush the model out to n users\nCollect data about how well the model did. (Bye bye privacy)\nSend this data back to the main model.\n7 (#new). Find the difference between the original model and the personalized one’s parameters. Do this for multiple users. Remove identifiable information.\n7.1 (#new). Aggregate (eg. average) the information and then send that to the main model\nRetrain the model on new data\n"},"KB/Basilar-Artery":{"title":"Basilar Artery","links":[],"tags":["brain"],"content":"Basilar Artery\n\nLocated at the base of the skull, the basilar artery is a large, specialized blood vessel that supplies oxygenated blood to the brain and nervous system.\n"},"KB/Batch-Normalization":{"title":"Batch Normalization","links":["tags/deeplearning","tags/architecture","KB/Distributions","KB/Regularization"],"tags":["normalization","deeplearning","architecture"],"content":"Batch Normalization\n\nbias=False for Linear/Conv2D for input and True for outputdeeplearning\nNormalizesarchitecture\nInput Distributions change per layer → Make sure they stay similar\nReduces co variate shift because now the network must adapt per layer\nDuring testing : use stats saved during training\nSimplifies learning dynamics\n\nCan use larger learning rate\nHigher order interactions are suppressed because the mean and std are independant of the activations which makes training easier\n\n\nCant work with small batches. Not great with RNN\n\n\\mu_j \\leftarrow \\frac{1}{m}\\Sigma_{i=1}^m x_{ij}\n\\sigma^2_j \\leftarrow \\frac{1}{m}\\Sigma^m_{i=1}(x_{ij}-\\mu_j)^2\n\\hat x_{ij} \\leftarrow \\frac{x_{ij}-\\mu_j}{\\sqrt{\\sigma^2_j + \\epsilon}}\n\\hat x_{ij} \\leftarrow \\gamma \\hat x_{ij} + \\beta\n\nWhy\n\nStable forward prop\nHigher learning rates\nRegularization\n"},"KB/Batching-for-GNN":{"title":"Batching for GNN","links":["KB/SGD","KB/Binary-Cross-Entropy","KB/Dropout"],"tags":["graph"],"content":"Batching for GNN\n \nBatching for GNN\n\nGIven I training graphs {X_{i}, A_{i}} and labels y_{i}, params {\\Phi = \\{\\beta_{k}, \\Omega_{k}}\\}^{K}_{k=0} , SGDand Binary Cross Entropy\nWe cannot concat batches into a big tensor since each graph has a different number of nodes\nInstead, we treat the graphs in each batch as disjoint components of a single large graph.\nwe can then just run the network over this big graph as a single instance of the network equations.\nThen we can do mean pooling on it\n\nNeighborhood Sampling\n\nrandomly sample fixed number of neighbors, recursively\nsomewhat like Dropout\n\nGraph Partitioning\n\ncluster the original graph into disjoint (not connected to each other) subsets of nodes\nthese are batches\nthis converts a transductive problem to an inductive one\nfor inference, k-hop neighbors\n"},"KB/Bayes-Prediction":{"title":"Bayes Prediction","links":[],"tags":["temp"],"content":"Bayes Prediction\n\nP(y|x) = \\int_{w}P(y|w,x)P(w|x)dw\nw is model parameters\nbasically gets y with different model parms w\nprob of those params given input x\nModel averaging\n"},"KB/Bayes-Rule":{"title":"Bayes Rule","links":[],"tags":["temp"],"content":"Bayes Rule\n\nh(\\theta|D) = \\frac{p_{\\otimes_{i}}x(D|\\theta)h(\\theta)}{p(D)}\n"},"KB/Bayesian-Information-Criterion":{"title":"Bayesian Information Criterion","links":[],"tags":["loss"],"content":"Bayesian Information Criterion\n\nBIC = -2 log(L) + 2log(n)q = \\frac{1}{n}(RSS + log(n) p \\hat \\sigma^{2})\nL: denotes the likelihood function for a particular model\nq: number of estimated parameters of the model\n"},"KB/Bayesian-Model-Estimation":{"title":"Bayesian Model Estimation","links":["KB/Bayesian","KB/Frequentist","KB/Bayesian-Prior","KB/Bayesian-Posterior","KB/Posterior-Mean-estimate","Monte-Carlo","KB/Protein-Modeling"],"tags":["temp"],"content":"Bayesian Model Estimation\n\nUnlike Frequentist, sometimes things like sample mean is not a good metric because it has a high variance. Might give different results with different trials in a real valued distribution\nThe task is to estimate \\theta from the data\nBayesian Prior\nNow there are two sources of info about the true distribution p_{X}(\\theta)\n\nThe likelihood p_{\\otimes_{i}}x(D|\\theta) of \\theta . Empirical data\nPrior plausibility in h(\\theta)\nSince these are independant sources we can combine them by multiplication: p_{\\otimes_{i}}x(D|\\theta)h(\\theta)\n\nHigh values → Candidate model \\theta is a good estimate\nBayesian Posterior\nPosterior Mean estimate\n\n\n\n\n\nAdvantages\n\nIf priors are well chosen → Better than frequentists with small sample sizes\n\nDisadvantages\n\nIntegrating over millions of params and performing multiple preds for each param → infeasible\nHow to encode or represent Bayesian Posterior as very high dim\n\nNo closed form representation over weights\nRepresent data with histograms and use Monte Carlo\n\n\n\nExample\n\n\n\nGreen : prior , Red: Posterior\nThe Posterior Mean estimate is obtained by integrating \\int_{\\mathbb{R}}\\mu h(\\mu|D)d\\mu\nSince this is different from sample mean → Prior distribution really does influence the models\n\nProtein Modeling"},"KB/Bayesian-Neural-Network":{"title":"Bayesian Neural Network","links":["KB/Bayesian","KB/Bayesian-Model-Estimation","KB/Probability","KB/Uncertainty","KB/Bayesian-Predictive-Posterior"],"tags":["temp"],"content":"Bayesian Neural Network\n\nBayesian Model Estimation\nGenerally we want to learn Joint Probability distribution P(y|x) but this does not use the model parameters w\nWe need P(w|D) = \\frac{P(D|w)P(w)}{P(D)}\n\nD is the labelled dataset\nModel is now defined by structure and parameters\n\n\nThe parameters encode information about Uncertainty\n\nCan be understood using Bayesian Predictive Posterior\n\n\n"},"KB/Bayesian-Posterior":{"title":"Bayesian Posterior","links":["KB/Bayesian","KB/PDF","KB/Bayes-Rule","KB/Proto-Distributions"],"tags":["temp"],"content":"Bayesian Posterior\n\nWhen D is fixed though, this becomes a function of Model Candidates\nNon negative on K dim param space\nNot a PDF but if we divide it by its integral → PDF .\n\n\\frac{p_{\\otimes_{i}}x(D|\\theta)h(\\theta)}{\\int_{\\mathbb{R}^K}p_{\\otimes_{i}}x(D|\\theta)h(\\theta)d\\theta}\nProb distrib over candidate models\n\n\nIf the denominator is replaced written as p(D) then it looks like the Bayes Rule\nShape : P(D|\\theta)h(\\theta)\n\nIntegral not 1\nProto Distributions on \\theta space\n\n\n"},"KB/Bayesian-Predictive-Posterior":{"title":"Bayesian Predictive Posterior","links":["KB/Bayesian","KB/Bayesian-Posterior","KB/Probability"],"tags":["temp"],"content":"Bayesian Predictive Posterior\n\nFollows from Bayesian Posterior\nMarginalizing over all possible model parameters w\nComputes predictions y with different model parameters w and weights them by the Probability of those params given an input x\nBayesian Model Averaging\nP(y|x) = \\int_{w}P(y|w,x)P(w|x)dx\n"},"KB/Bayesian-Prior":{"title":"Bayesian Prior","links":["KB/Bayesian","KB/Probability","KB/PDF","KB/Distributions","KB/MLE"],"tags":["temp"],"content":"Bayesian Prior\n\nUse prior knowledge as beliefs (param vectors \\theta). Cast in the form of a Probability distribution over the space \\Theta .\n\nWeak knowledge most times\nFor a K parametric PDF p_{x} , \\Theta \\in \\mathbb{R}^{K} .\nNot connected to Random variable(RVS).\nDoes not model outcomes. Instead has “beliefs” about true distribution P_{X_{i}}\nEach \\theta \\in \\mathbb{R}^{K} corresponds to one specific PDF p_{X}(\\theta) → single candidate distribution \\hat P_{X} for values x_i (In frequentist, it models single data points)\nSince this is a distribution over Distributions, it is a hyperdistribution\nN dim PDF p_{\\otimes}x_{i}: \\mathbb{R}^{N} \\rightarrow \\mathbb{R}^{\\geq 0} for the distribution of RV \\otimes_{i}X_{i}\n\np_{\\otimes_{i}}x_{i}((x_{i},…, x_{N})) = p_{x_{1}}, …, p_{x_{N}}(x_{N}) = \\Pi_{i}p_{X}(x_{i})\np_{\\otimes_{i}}x(D|\\theta) → PDF values on a data sample D p_{\\otimes_{i}}x_{i}((x_{i},…, x_{N})) = p_{\\otimes_{i}}(\\theta)(D)\n\n\nWhen \\theta is fixed then p_{\\otimes_{i}}x(D|\\theta) is a function of data vectors D. For each sample, it describes how probable this distribution is assuming the true distribution of X is p_{X}(\\theta)\nWhen D is fixed, then it is a function of \\theta. But this does not really measure anything.\n\nIntegral over \\theta is not 1\nIt is a function of \\theta and so it is a likelihood function. MLE\nIf given data D → it can show which models are more likely than others.\nHigher values of\tp_{\\otimes_{i}}x(D|\\theta) are better\n\n\n\n\n"},"KB/Bayesian-Rule-List":{"title":"Bayesian Rule List","links":["@Interpretable-classifiers-using-rules-and-Bayesian-analysis:-Building-a-better-stroke-prediction-model"],"tags":["explainability"],"content":"\n\nBayesian Rule List\n \n\n@Interpretable classifiers using rules and Bayesian analysis: Building a better stroke prediction model\nThe BRL is a generative model that yields a posterior distribution over possible decision lists, which consist of a series of if-then statements that discretize a high-dimensional, multivariate feature space into a series of simple, readily interpretable decision statements. The if statements define a partition of a set of features, and the then statements correspond to the predicted outcome of interest.\nAccording to the authors, their experiments showed that the BRL has predictive accuracy on par with the current top algorithms for prediction in Machine Learning\nThe BRL is able to be used to produce highly accurate and interpretable medical scoring systems\n"},"KB/Bayesian":{"title":"Bayesian","links":["KB/Bayes-Prediction","KB/Bayes-Rule","KB/Bayesian-Model-Estimation","KB/Probability","KB/Density"],"tags":["temp"],"content":"Bayesian\n\nSubjective\nBayes Prediction\nBayes Rule\nBayesian Model Estimation\nProbability Density function\n"},"KB/Beam-search":{"title":"Beam search","links":[],"tags":["architecture"],"content":"Beam Search\n\n\n(from)\nat each step, keep track of the k mos probable translation hypotheses (k, beam size)\nexamine the k most probable words for each hypothesis, compute their entire scores, keep k best ones\nnot guaranteed to find optimal solution, but more efficient than exhaustive search\nit does not only take the best word, it rather takes the best B (user specified) and generates multiple hypothesis, which will then be evaluated and the best one at each step is chosen for the next ones\nnot guaranteed to find the optimal solution and is therefore an approximate search\nproblems:\n\nwhen multiplying a lot of probabilities of very unlikely word (e.g. almost 0 but not exactly), the result will get very small and the system can no longer represent it. results in numerical underflow ⇒ instead of multiplying, summing the log of probabilities (more numerical stable)\nif the sentence is very long, the probabilities get very low, therefore it rather takes smaller translations ⇒ normalize the output by the number of word in the translation (average of the log of each word)\n\n\nhow to choose beam width B?\n\nthe smaller, the fewer probabilities are considered (worse result, faster)\nthe larger, the more are considered, but the more computing expensive it is (better results, slower)\ntry out different values and cross check\n\n\n"},"KB/Belief-Desire-Intention":{"title":"Belief-Desire-Intention","links":[],"tags":["ethics"],"content":"Belief-Desire-Intention\n\nTo judge the ethics of an agent’s own actions, the awareness process generates the beliefs that describe the current situation facing the agent and the goals of the agent.\nBased on the beliefs and goals, the evaluation process generates the set of possible actions and desirable actions\ngoodness process then computes the set of ethical actions based on the agent’s beliefs, desires, actions, and moral value rules\nrightness process evaluates whether or not executing a possible action is right under the current situation and selects an action which satisfies the rightfulness requirement\n"},"KB/Belmont-Principles":{"title":"Belmont Principles","links":["KB/Beneficence","KB/Belmont-Report"],"tags":["brain"],"content":"Belmont Principles\n\nThe three principles—\nBeneficence, distributive justice, and respect for persons—which the 1976 Belmont Report concluded should underlie all conduct in biomedical and behavioral research in order to protect human participants.\n"},"KB/Belmont-Report":{"title":"Belmont Report","links":["KB/Belmont-Principles"],"tags":["brain"],"content":"Belmont Report\n\nAn influential report that identified and defined the basic ethical principles (the Belmont Principles) that should govern research studies involving human participants.\n"},"KB/Benchmark-Data-Repositories-for-Better-Benchmarking":{"title":"Benchmark Data Repositories for Better Benchmarking","links":[],"tags":["benchmark"],"content":"Benchmark Data Repositories for Better Benchmarking\n \n\nBenchmark Data Repositories for Better\nBenchmarking\ncomparatively less attention has been paid to the repositories where\nthese datasets are stored, documented, and shared\n\nIntroduction\n\nBenchmarking can help quantify\nprogress on these tasks over time, and the availability of a well-studied, standard task evaluation\nenvironment can be a critical first step before moving to real-world applications, especially in high-\nstakes or expensive domains.\nEarly data repositories, such as the UCI ML Repository, arose to address the data needs that come\nwith ML benchmarking\nthe process of selecting a dataset is often less about a scientific or engineering application and more\nabout the compositional characteristics of the data and its associated tasks, for which a particular\nclass of methods is applicable\nbenchmark repository\nto describe repositories that support the discovery and use of datasets for evaluating ML models\n\nValuing Datasets as Research Contributions\nDataset Citations and Metrics\n\npersistent identifier (PID), such as a DOI, that can reliably\nbe used to access a dataset has been widely recommended by experts\nML\ndatasets and their documentation frequently lack PIDs and are often only available via GitHub or\npersonal/research group websites\nIn ML, however, datasets are often referred\nto using combinations of names, descriptions, and associated papers, which can be challenging to\ndisambiguate\ninclude the minted DOI\n\nConnection Metadata\n\nconnects a dataset to associated research entities (such as the dataset’s creators or\nmaintainers, publications, code, or other datasets)\nIntroductory papers can give the data a story beyond standardized documentation, providing useful context about the problem,\nbackground on data collection procedures, and guidance about tasks for which the data have already\nbeen used.\nIntroductory papers can be included in benchmark repositories as a\nstandardized metadata field\ndataset’s point of contact:\nsomeone responsible for answering questions about the dataset and addressing any issues\nlthough long-term data maintenance, and determining different\nstakeholders’ responsibilities in that maintenance, remain challenging tasks [28, 60, 61], establishing\na point of contact can help prevent the development of a disconnect between a dataset and its creators,\nwhich is not uncommon in ML\n\nDataset Licenses\n\nData repositories can include licenses as part of a dataset’s metadata.\nit\nis ambiguous if models trained on a dataset count as “derivative work”\ncurrent licensing practices for ML datasets are often irregular,\nconflicting, poorly documented, or over-permissive given the dataset content\n\nAddressing Issues with Dataset Content\n\ntechnical flaws such as labeling errors and annotation artifacts\nprivacy\nand copyright violations\ninclusions of hate speech or other harmful content\nrepresentational biases\nmiscellaneous ethical issues\n\nContextual Metadata\n\nincluding information about a dataset’s\nsource, funding, collection, annotation, and preprocessing, benchmark repositories can illuminate the\nassumptions and motivations of dataset creators and flag potential dataset issues\nData Cards\ndatasheets\nDataset Nutrition Label\nFAIR principles\nData selection, filtering, and annotation processes are important design decisions that can significantly\nimpact downstream performance [7, 38, 75, 100, 101]; however, they tend to be under-documented\n\nQuality Review\n\ndatasets containing personally identifiable information should not be released\nBenchmark repositories can help\nidentify these problems throughout the data lifecycle by (1) performing a pre-release quality review\n[92] to catch issues before a dataset is shared, and (2) by serving as a centralized location to collect\nusers’ reports and concerns [72] to flag issues throughout a dataset’s use and reuse\nIt is an open\nquestion to what extent repositories should be involved in these decisions; several popular repositories\n(e.g., Zenodo, Mendeley, etc.) view their role as only providing infrastructure and not conducting any\nkind of data review.\n\nDataset Revision and Deprecation\n\ndataset revision by documenting data versions and connecting\neach dataset to a responsible point of contact\ncan enforce versioning by assigning a new version number\nwhenever a data file is changed.\ndeprecation of datasets.\ncreators often withdraw their dataset without an explanation of why it\nwas withdrawn or explicit instructions not to use the dataset\ndeprecation reports are posted in a scattered, decentralized manner via news articles,\nconference papers, or researcher or lab websites\nit can be unclear to researchers if a\ndataset is acceptable to use; it is not uncommon for datasets to remain in use after their deprecation,\nincluding in published, peer-reviewed papers\nIf a deprecation report is clearly displayed in the same place where a dataset was available, it\nclarifies to researchers (and reviewers) that the dataset should not be used\n\nCompositional and Task Metadata\n\nCompositional metadata\ndescribe the makeup of a dataset,\nCroissant\nTask metadata\ninclude the intended or appropriate ML tasks for a dataset (e.g., image classification\nor time-series prediction) and specialized metadata relevant to those tasks\nAs an example, HuggingFace Datasets\n[17], which specializes in NLP data, collects metadata on language and multilinguality, text creation,\nand fine-grained NLP tasks\nrepositories can require that data\ndonors provide high-quality compositional and task metadata, including specialized task metadata,\nwhere appropriate\n\nBenchmark Metadata\n\nif a repository displays a\nparticular benchmarked result for a dataset, they should ensure that specific details on all settings\nand hyperparameter values used to obtain that result are available\n\nEncouraging Holistic Evaluation\n\nKaggle [18]\nand Papers with Code\nLeaderboarding\nFurther, as measures\nof uncertainty are seldom incorporated, seemingly record-breaking performance improvements are\nnot always statistically significant\n\nAnalysis Beyond Single Metrics\n\nvariety of metrics, capturing model size and\ncomplexity, energy consumption, inference latency, and the amount of data used\nerror analysis or disaggregated evaluations\n\nMetric Uncertainty\n\nMetrics shown without any measure of uncertainty can prompt fallacious conclusions, e.g., that\none model performs definitively better than another\nInstead, including uncertainty makes these\nbenchmarked results more informative [1, 157], and a growing body of methodologies have been\ndeveloped for estimating uncertainty, computing confidence intervals, and performing statistical\nsignificance testing in the context of model comparison\nvariance, uncertainty, and statistical significance\nin model analysis\n\nLiving Datasets\n\nleaderboards can evaluate submitted models\non a private, hitherto unused test set [167—169] (e.g., as done by Kaggle) or on out-of-distribution\ndata\neaderboards can also support “living” or evolving datasets,\nto which dataset creators continuously add new examples or tasks and remove outdated or erroneous\nexamples.\nbenchmarked performances of two models evaluated at two\ndifferent points in time may not be directly comparable\nrobust models will generally outperform those using a specific trick or artifact as\nevaluations are repeated over time\nde-incentivizing\noverfitting and helping bridge the gap between benchmarked and real-world performance.\n\nDataset Discoverability\n\nthere also exists a plethora\nof other high-quality datasets that could have been used but did not win the “benchmark lottery”\nexisting standards emphasize\nthe importance of standardized, rich metadata [39, 129, 130], which enable searching for datasets\nvia keywords, filtering, and controlled vocabularies\nsearch based on compositional and task\nmetadata\nUCI ML Repository’s search functionality includes a filter\nfor classification, regression, clustering, or other datasets.\n"},"KB/Benchmark-LLM":{"title":"Benchmark LLM","links":["KB/language","KB/GPT3","KB/PaLM"],"tags":["dataset"],"content":"Benchmark LLM\n\n[[A Benchmark for LLMs on Planning and Reasoning about Change|Large Language Models Still Can’t Plan (A Benchmark for LLMs on Planning and Reasoning about Change|[A Benchmark for LLMs on Planning and Reasoning about Change)]]]])))\nThe recent advances in large language models (LLMs) have transformed the field of natural language processing (NLP)\nFrom GPT3 to PaLM, the state-of-the-art performance on natural language tasks is being pushed forward with every new large language model\ncurrent benchmarks are relatively simplistic and the performance over these benchmarks cannot be used as an evidence to support\nextensible assessment framework motivated by the above gaps in current benchmarks to test the abilities of LLMs on a central aspect of human intelligence, which is reasoning about actions and change\nmultiple test cases\n"},"KB/Bend-Minimization":{"title":"Bend Minimization","links":["KB/Gestalt-Laws"],"tags":["visualization","graph"],"content":"Bend Minimization\n\nCurved lines easier to follow than edged lines Gestalt Laws\ndomain specific constraints and traditions have to be acknowledged\n\n"},"KB/Beneficence":{"title":"Beneficence","links":["KB/Belmont-Principles"],"tags":["brain"],"content":"Beneficence\n\nOne of the three Belmont Principles, the requirement that physicians and researchers provide, to the best of their ability, positive benefits for patients that participate in clinical trials, including good health and the prevention and removal of harmful conditions.\n"},"KB/Benford's-Law":{"title":"Benford's Law","links":[],"tags":["temp"],"content":"Benford’s Law\n\nIn a genuine dataset of numbers\n\n1 will be the leading digit 30.1% of the time\n2 will be the leading digit 17.6% of the time\nrest with decreasing frequency\n\n\n"},"KB/Benign":{"title":"Benign","links":["KB/Malignant"],"tags":["medical"],"content":"Benign\n\nRefers to a tumor that is neither cancerous nor Malignant\n"},"KB/Berkeley-et-al":{"title":"Berkeley et al","links":["KB/Single-unit-recording","KB/Features","KB/Lesion"],"tags":["language"],"content":"Berkeley Et Al\n\n576 input patterns\n5793 epochs needed to reach convergence\nModel frozen, stimulus set presented again\nActivation of each hidden unit recorded\nSingle unit recording\nWhy do these bands appear?\nGaussian activation function\nBut banding patterns have been found with sigmoidal activation as well\nEffect = units only respond to a limited number of inputs\nBands appear when weights into HUs cancel each other out\nActivations of hidden neurons can be organized into bands\nBands are associated with interpretable Features\nLesion studies show bands are essential to solving problem\nFor some problems under some circumstances, neural networks develop highly selective hidden units\nLooks like localist coding (grandmother cells)\nPatterns of activation can be ambiguous on their own\nBut realistically, more than one pattern might be activated simultaneously\nSuperimposing two or more patterns over same units leads to an ambiguous blend\nProblem for localist representations, but even more serious with distributed representations\n"},"KB/Bernoulli-Distribution":{"title":"Bernoulli Distribution","links":["KB/PMF"],"tags":["distributions"],"content":"Bernoulli Distribution\n\n\nOnly two possible outcomes\n\n\nPMF : Pr(y|\\lambda) = \\begin{cases} 1-\\lambda, &amp; \\text{for i = 1} \\\\ \\lambda,&amp; \\text{for i =2} \\end{cases} = (1-\\lambda)^{1-y} \\cdot \\lambda^{y}\n\n\nGiven : Data - {x_{1}, .., x_{N}} and x_{i} \\in {s_{1}, s_{2}} then \\hat q = \\frac{1}{N}|\\{i|x_{i}= s_{2}\\}\n\n\n\n\n"},"KB/Best-Maching-Unit":{"title":"Best Matching Unit","links":[],"tags":["temp"],"content":"Best Matching Unit\n\nNeuron whose weight vector best matches input pattern\n"},"KB/Beta-Distribution":{"title":"Beta Distribution","links":["KB/Dirichlet-Distribution"],"tags":["distributions"],"content":"Beta Distribution\n\n[0,1]\nParameterized by two positive shape parameters \\alpha, \\beta\nExponents of the random variable and control the shape of the distribution\nMultiple variables is Dirichlet Distribution\n"},"KB/Beta-Waves":{"title":"Beta Waves","links":[],"tags":["brain"],"content":"Beta Waves\n\nMovement\n\n"},"KB/Beware-of-Inmates-Running-the-Asylum":{"title":"Beware of Inmates Running the Asylum","links":["@Explainable-AI:-Beware-of-Inmates-Running-the-Asylum-Or:-How-I-Learnt-to-Stop-Worrying-and-Love-the-Social-and-Behavioural-Sciences","KB/XAI"],"tags":["explainability"],"content":"\n\nBeware of Inmates Running the Asylum\n \n\nTim Miller∗ and Piers Howe† and Liz Sonenberg\n\n\n\n@Explainable AI: Beware of Inmates Running the Asylum Or: How I Learnt to Stop Worrying and Love the Social and Behavioural Sciences\n\n\nTL;DR\n\nEssentially proposes to look at behavioral science research as well. Not particularly useful but is a good reminder to look at other research because XAI is meant for people and not programmers.\n\nAbstract\n\nprogrammers design software for themselves, rather than for their target audience; a phenomenon he refers to as the ‘inmates running the asylum’.\nThis paper argues that explainable AI risks a similar fate.\nevaluation of these models is focused more on people than on technology\nconsiderable scope to infuse more results from the social and behavioural sciences into explainable AI, and present some key results from these fields that are relevant to explainable AI.\n\nExplainable AI Survey\nSurvey Method\n\n\nOn topic\n\nEach paper was categorised as either being about explainable AI or not, based on our understanding of the topic\nData Driven\n\nEach paper was given a score from 0–2 inclusive.\nA score of 1 was given if and only if one or more of the references of the paper was an article on explanation in social science\n\n\nValidation\n\nEach paper was given a binary 0/1. A score of 1 was given if and only if the evaluation in the survey article (note, not the referenced article) was based on data from human behavioural studies.\n\n\n\n\n\nResults\n\nThese results show that for the on-topic papers, only four articles referenced relevant social science research, and only one of them truly built a model on this\nFurther, serious human behavioural experiments are not currently being undertaken.\nFor off topic papers, the results are similar: limited input from social sciences and limited human behavioural experiments.\nWhere to? A Brief Pointer to Relevant Work Contrastive Explanation\nexplanations are contrastive why–questions are contrastive\nThat is, why–questions are of the form “Why P rather than Q?”, where P is the fact that requires explanation, and Q is some foil case that was expected\nImportantly, the contrast case helps to frame the possible answers and make them relevant\nThis is a challenge for explainable AI, because it may not be easy to elicit a contrast case from an observer.\nHowever, it is also an opportunity: as Lipton [1990] argues, answering a contrastive question is often easier than giving a full cause attribution because one only needs to understand the difference between the two cases, so one can provide a complete explanation without determining or even knowing all causes of the event.\n\nAttribution Theory\n\nstudy of how people attribute causes to events Social Attribution\nThe book from Malle [2004], based on a large body of work from himself and other researchers in the field, describes a mature model of how people explain behaviour of others using folk psychology\npeople attribute behaviour based on the beliefs, desires, intentions, and traits of people\nimportant for systems in which intentional action will be cited as a cause important for systems doing deliberative reasoning\n\nCausal Connection\n\nResearch on how people connect causes shows that they do so by undertaking a mental simulation of what would have happened had some other event turned out differently\n"},"KB/Bhattacharya-Distance":{"title":"Bhattacharya Distance","links":["KB/Distributions"],"tags":["loss"],"content":"Bhattacharya Distance\n\nIn statistics, the Bhattacharyya distance measures the similarity of two Distributions. It is normally used to measure the separability of classes in classification.\nD_{B}(p,q) = -ln(BC(p,q))\nBC(p,q) = \\Sigma_{x\\in X}\\sqrt{p(x)q(x)}\n"},"KB/Bi-Directional-RNN":{"title":"Bi Directional RNN","links":[],"tags":["architecture"],"content":"Bi Directional RNN\n\nNot causal\nLooks at the forward timestep dimension and also the backward\n\nBoth combined to make a prediction\n\n\n\n"},"KB/Bias-Variance-Dilemma":{"title":"Bias Vs Variance","links":["LossFunctions","KB/Emperical-Risk","KB/Optimizers","KB/Fitting","KB/Quadratic-Loss","KB/Tuning-Model-Flexibility"],"tags":["temp"],"content":"Bias Vs Variance\n\n\n\n\n\nm is choices of PC vectors\nas m increases, weight matrices grow by 10\\cdot m . Aka more flexible models.\nIncreasing tail of MSEtest → overfitting. too flexible\nIncreasing flexibility → decrease of empirical risk\nInc : very low to very high → less and less underfitting then overfitting\nBest: min point in curve. But it is defined on test data which we do not have\n\n\n\n\n\nDecision function should minimize LossFunctions and yield a function with risk h. This is hopeless R(h) = E[L(h(X), Y)]\nTune on Emperical Risk instead using Optimizers\n\\mathcal{H} is hypothesis space (related to Fitting).\n\nWhy is This a Dilemma\n\nAny learning algo \\mathcal{A}\nIf we run \\mathcal{A} repeatedly but for different “fresh” sampled data → \\hat h varies from trial to trial\nFor any fixed x, \\hat h(x)\n\nis a random variable\nvalue determined by drawn training samples\nrep by distribution P_{X,Y} (which we cannot really know)\nExpectation E_{retrain}[\\hat h(x)] . aka taken over ALL possible training runs with sampled data\n\n\nQuadratic Loss (risk) is minimized by the function \\Delta(x) = E_{Y|X=x}[Y]\n\nExpectation of Y given x.\n\n\n\n\nBias measures how strongly the avg result deviates from optimal value\nVariance measures how strongly the results vary around the expected value E_{retrain}\nWhen flexibility is too low → bias dominates(too good in train and horrible later) and underfits\nWhen flexibility is too high → variance dominates → overfitting\n\nTuning Model Flexibility"},"KB/Bias-nodes":{"title":"Bias nodes","links":[],"tags":["architecture"],"content":"Bias Nodes\n\nBias nodes give a defaults activation to other nodes\n\nUsually included, the bias node does not connect to input nodes but to the output nodes\n\n\n"},"KB/Big-Bird":{"title":"Big Bird","links":["KB/Transformer","KB/Attention"],"tags":["architecture"],"content":"Big Bird\n\nBig Bird: Transformers for Longer Sequences\nimitation of Transformer-based models is the quadratic complexity\nsparse Attention mechanism that reduces this quadratic complexity to linear\n"},"KB/Big-Bench":{"title":"Big-Bench","links":["KB/language","KB/GPT","KB/Transformer"],"tags":["benchmark"],"content":"Big-Bench\n\nBeyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models\npresent and near-future capabilities and limitations of language models\nBeyond the Imitation Game benchmark (BIG-bench)\nbenchmark that can measure progress well beyond the current state-of-the-art\n204 tasks, contributed by 442 authors across 132 institutions\nTask topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development\ntasks that are believed to be beyond the capabilities of current language models\nvaluate the behavior of OpenAI’s GPT models, Google-internal dense Transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters\n"},"KB/Bilinear-Interpolation":{"title":"Bilinear Interpolation","links":["KB/Interpolation"],"tags":["visualization"],"content":"Bilinear Interpolation\n\n\nf(x,y) = (1-\\beta)(1-\\alpha)f_{i,j}+(1-\\beta)\\alpha f_{i+1,j} + \\beta(1-\\alpha)f_{i,j+1}+\\beta \\alpha f_{i+1,j+1}\nQuadratic\nik, jkl , il → ij\n"},"KB/Billion-Word":{"title":"Billion Word","links":[],"tags":["dataset"],"content":"Billion Word"},"KB/Binary-Cross-Entropy":{"title":"Binary Cross Entropy","links":[],"tags":["loss"],"content":"Binary Cross Entropy\n-(ylog(p)+(1-y)log(1-p))\n\n\nL(y, \\hat y) = - \\Sigma_{i}y_{i}log(\\hat y_{i})+ (1-y_{i})log(1-\\hat y_{i})\n\n\n\n\n"},"KB/Binary-pattern":{"title":"Binary Pattern Encoding","links":["KB/One-hot"],"tags":["temp"],"content":"Binary Pattern Encoding\n\nIf symbol alphabet has a large size k\n\nOne hot is too huge\n\n\nEncode into binary vector of length \\lceil log_{2} \\rceil\n{a,b,c,d} → {[0,0]’, [0,1]’, [1,0]’, [1,1]’}\nNon linear effort as it is a arbitrary encoding\nToo intensive\n\n…"},"KB/Binary":{"title":"Binary","links":[],"tags":[],"content":"Binary\n \n"},"KB/BinaryBERT":{"title":"BinaryBERT","links":["loss","KB/GLUE"],"tags":["architecture"],"content":"BinaryBERT\n\nBinaryBERT: Pushing the Limit of BERT Quantization\ndemand for model compression techniques\nweight binarization\nbinary BERT is hard to be trained directly than a ternary counterpart due to its steep and complex loss landscape\nternary weight splitting\ninitializes BinaryBERT by equivalently splitting from a half-sized ternary network, followed by fine-tuning for further refinement\nbinary model thus inherits the good performance of the ternary one, and can be further enhanced by fine-tuning the new architecture after splitting\ntailor the size of BinaryBERT based on the edge device constraints\nGLUE\nSQuAD\n"},"KB/Binning":{"title":"Binning","links":["KB/Decision-Trees"],"tags":["temp"],"content":"Binning\n\nk segments\nTransform each symbol\nTypes\n\nSimplest → k equal bins\nApprox Equal no of data points\nReduced precision devices perform as well as the high precision ones\nContinuous range → adaptive bin boundaris Decision Trees\n\n\n"},"KB/Binomial-Distribution":{"title":"Binomial Distribution","links":["KB/Bernoulli-Distribution","KB/Probability","KB/PMF"],"tags":["distributions"],"content":"Binomial Distribution\n\nBernoulli Distribution repeated for N independant trials with success Probability q\nAka N times with only 2 outcomes\nPMF: p(s) = \\binom{N}{s}q^{s}(1-q)^{N-s} = \\frac{N!}{s!(N-s)}q^{s}(1-q)^{N-s}\ns = 0,1,2..N\nN\\choose s is binomial coefficient\nX \\sim Bi(N,s)\n\n"},"KB/Biological-Neuron":{"title":"Biological Neuron","links":["KB/Dendrites","KB/Axon","KB/Glucose"],"tags":["brain"],"content":"Biological Neuron\n\n\n(from)\ncomposed of \n\ncell body\nDendrites: many branching extensions\nAxon: very long extension that splits off at its tip into many branches called synaptic terminals\n\n\ncomposed in a network (e.g. brain) by synaptic terminals of one neuron connected to Dendrites of other neurons\nelectrical impulses (signals) are sent from other neurons via these synapses\nif a neuron receives a sufficient number of signal from other neurons within a few milliseconds, it is exited and fires its own signals (activation)\nconnectivity in a biological neural system is huge, human brain:\n\nnumber of neurons: \\approx 10^{11}\nnumber of connections per neuron: \\approx 10^4\n\n\nnetworks are organized into hierarchical structures\nIrreplacable\nRequires constant supply of Glucose\n"},"KB/Biomarkers":{"title":"Biomarkers","links":[],"tags":["brain"],"content":"Biomarkers\n\nA measurable physiological indicator of a biological state or condition.\n"},"KB/Biopsy":{"title":"Biopsy","links":[],"tags":["medical"],"content":"Biopsy\n\nRemoval of a small tissue sample for testing\n"},"KB/Block-Sparse-Kernel":{"title":"Block Sparse Kernel","links":["KB/Sparsity","KB/Dense","KB/Layers","KB/Small-World-graphs"],"tags":["parallelcomputing"],"content":"Block Sparse Kernel\n\nFor networks with block sparse weights\nCan choose amount of Sparsity\nCan replace normal Dense Layers with sparse and wide or sparse and deep\n\nEnables wider and deeper networks\nOnly compute on non zero blocks\n\nConnectivity is unaffected in the spatial dimensions\nCompute cost is only prop to number of non zero weights\nSmall World graphs\nAlso useful for compression\n\nRefs\n\nopenai\n"},"KB/BlockDrop":{"title":"BlockDrop","links":[],"tags":["architecture"],"content":"BlockDrop\n \n\nwhich analyzes an existing network and decides which residual blocks to use at runtime with the goal of improving the eﬀiciency of inference.\n"},"KB/BlockNeRF":{"title":"BlockNeRF","links":["KB/Neural-Radiance-Field","KB/Embedding"],"tags":["architecture"],"content":"BlockNeRF\n\nBlock-NeRF: Scalable Large Scene Neural View Synthesis\nvariant of Neural Radiance Field\nreconstruct large-scale environments\nscaling NeRF to render city-scale scenes spanning multiple blocks, it is vital to decompose the scene into individually trained NeRFs that can be optimized independently.\nthis decomposition decouples rendering time from scene size\nallows per-block updates of the environment\ndata collected will necessarily have transient objects and variations in appearance\nmodifying the underlying NeRF architecture to make NeRF robust to data captured over months under different environmental conditions\nappearance Embedding, learned pose refinement, and controllable exposure to each individual NeRF\nprocedure for aligning appearance between adjacent NeRFs so that they can be seamlessly combined\nbuilding an entire neighborhood in San Francisco from 2.8M images using a grid of Block-NeRFs, forming the largest neural scene representation to date\n"},"KB/Blood-Culture":{"title":"Blood Culture","links":[],"tags":["medical"],"content":"Blood Culture\n\nTest to reveal the existence of fungi or bacteria in the blood, possibly indicating an infection\n"},"KB/Blood-Lancet":{"title":"Blood Lancet","links":[],"tags":["medical"],"content":"Blood Lancet\n\nA double-edged blade or needle used to obtain blood samples\n"},"KB/Blood-Swab":{"title":"Blood Swab","links":[],"tags":["medical"],"content":"Blood Swab\n\nTaking a blood sample using a cotton-tipped stick\n"},"KB/Blood-brain-Barrier":{"title":"Blood-brain Barrier","links":["KB/Glucose"],"tags":["brain"],"content":"Blood-brain Barrier\n\nA protective barrier that separates the brain from the blood circulating across the body. The blood-brain barrier is semipermeable, meaning it allows the passage of water as well as molecules like Glucose and other amino acids that help promote neural function.\n"},"KB/Blur-Baseline":{"title":"Blur Baseline","links":[],"tags":["explainability"],"content":"Blur Baseline\n\n@fongInterpretableExplanationsBlack2017\nAnother baseline is called Blur baseline and uses a multi-dimensional gaussian filter (\nThe idea presented by Fong and Vedaldi blurred version of the image is a domain-specific way to represent missing information and therefore be a valid baseline according to the original definition\n"},"KB/Boltzmann-Distribution":{"title":"Boltzmann Distribution","links":["KB/PDF","KB/Markov-Random-Field"],"tags":["distributions"],"content":"Boltzmann Distribution\n\nPDF p(s|T)= \\frac{1}{\\int_{s}e^{-C(s)/T}ds}e^{-C(s)/T}\nEnergy function E: S \\rightarrow \\mathbb{R}^{\\geq 0}\nMarkov Random Field\n"},"KB/BooksCorpus":{"title":"BooksCorpus","links":[],"tags":["dataset"],"content":"BooksCorpus"},"KB/Bottom-Up-Parsing":{"title":"Bottom Up Parsing","links":[],"tags":["language"],"content":"Bottom Up Parsing\n\n\n"},"KB/Bound-morpheme":{"title":"Bound morpheme","links":["KB/Morpheme"],"tags":["language"],"content":"Bound Morpheme\n\nmorphemes that cannot appear as a word by itself\ne.g., +ing, +s, +ness,ly,ed\n"},"KB/Brain-Areas":{"title":"Brain Areas","links":["KB/Cerebrum","KB/Cerebellum","KB/Brainstem"],"tags":["brain"],"content":"Brain Areas\n\n\nCerebrum\nCerebellum\nBrainstem\n"},"KB/Brain-Cortex":{"title":"Brain Cortex","links":["KB/Brain-Areas","KB/Gyrus","KB/Basal-Ganglia","Medial-Prefrontal-Cortex","Posterior-Cingulate-Cortex","Precuneus","Lateral-Parietal-Cortex"],"tags":["brain"],"content":"Brain Cortex\n\nAlso called Cerebral Cortex\nIt has a folded appearance with hills and valleys\nThe nerve cell bodies color the cortex grey-brown giving it its name – gray matter\nBeneath the cortex are long nerve fibers (axons) that connect Brain Areas to each other — called white matter\n\nGyrus\nBasal Ganglia\nDivided into parts\n\nMedial Prefrontal Cortex, and the Posterior Cingulate Cortex with the nearby Precuneus and Lateral Parietal Cortex\n**\n\n\n"},"KB/Brain-Organoid":{"title":"Brain Organoid","links":["KB/Organoid","KB/Stem-Cells"],"tags":["brain"],"content":"Brain Organoid\n\nA research model that uses pluripotent Stem Cells (iPSCs) to grow structures that resemble brains in some ways, but are grown in a lab dish made of neurons and other brain tissues.\n"},"KB/Brain-Oscillations":{"title":"Brain Oscillations","links":["KB/Delta-Waves","KB/Theta-Waves","KB/Alpha-Waves","KB/Beta-Waves","KB/Gamma-Waves","KB/Spectrogram"],"tags":["brain"],"content":"Brain Oscillations\n\nPeriodic\nBrain waves\nDelta Waves\nTheta Waves\nAlpha Waves\nBeta Waves\nGamma Waves\nSpectrogram\n\n"},"KB/Brain-derived-Neurotrophic-Factor-(BDNF)":{"title":"Brain-derived Neurotrophic Factor (BDNF)","links":[],"tags":["brain"],"content":"Brain-derived Neurotrophic Factor (BDNF)\n\nSometimes referred to as “brain fertilizer,” BDNF is a protein that helps promote the growth, maintenance, and survival of neurons.\n"},"KB/BrainWave-Coherence":{"title":"BrainWave Coherence","links":["KB/Correlation"],"tags":["brain"],"content":"BrainWave Coherence\n\nCorrelation in the frequency domain\nUnlike synchronization, this also depends on signal amplitude\n\n\ncoherence vs freq\ndecent coherence between CZ and O1\nO1 and PZ has little coherence\n\n\n"},"KB/BrainWave-CrossFrequency-Coupling":{"title":"BrainWave CrossFrequency Coupling","links":[],"tags":["brain"],"content":"BrainWave CrossFrequency Coupling\n\nLow frequency + Superimposed High freq signal\n\n-\n\nElectrode with low freq + high freq\n\n\n"},"KB/BrainWave-Synchronization":{"title":"BrainWave Synchronization","links":[],"tags":["brain"],"content":"BrainWave Synchronization\n\nConsistency of phase difference\n\nIf 0 then perfect\n\n\n\nPhase Locking Value\n"},"KB/Brainstem":{"title":"Brainstem","links":["KB/Cerebrum","KB/Cerebellum"],"tags":["brain"],"content":"Brainstem\n\nrelay center connecting the Cerebrum and Cerebellum to the spinal cord. It performs many automatic functions such as breathing, heart rate, body temperature, wake and sleep cycles, digestion, sneezing, coughing, vomiting, and swallowing\n"},"KB/Brainstorm-openml_pytorch":{"title":"Brainstorm openml_pytorch","links":[],"tags":["openml"],"content":"Brainstorm openml_pytorch\n \nIssues\n\ncustom data flows\nWhat if not nn.module but custom classes. huggingface for instace\nOpenMLDataset thats not an image\nWhat kinds of things are recorded? - Dataloader? augmentation etc?\nOther types of tasks → generation etc\n\n"},"KB/Branch-Prediction":{"title":"Branch Prediction","links":[],"tags":["parallelcomputing"],"content":"Branch Prediction\n\navoid delays cause of control dependencies to be resolved.\ndetermines whether a conditional branch (jump) in the instruction flow of a program is likely to be taken or not\n"},"KB/Broadcasting":{"title":"Broadcasting","links":[],"tags":["temp"],"content":"Broadcasting\n\nExpanding the shape of an operand in a matrix math operation to dimensions compatible for that operation. For instance, linear algebra requires that the two operands in a matrix addition operation must have the same dimensions. Consequently, you can’t add a matrix of shape (m, n) to a vector of length n. Broadcasting enables this operation by virtually expanding the vector of length n to a matrix of shape (m,n) by replicating the same values down each column.\n"},"KB/Brocas-Area":{"title":"Brocas Area","links":["KB/language","KB/Aphasia","KB/Frontal-lobe"],"tags":["brain"],"content":"Brocas Area\n\nIf this area is damaged, one may have difficulty moving the tongue or facial muscles to produce the sounds of speech. The person can still read and understand spoken language but has difficulty in speaking and writing\nBroca’s Aphasia\nDiscovered by French physician Paul Broca in the late 19th century, this small region in the left Frontal lobe has been linked to speech production.\n"},"KB/Broden":{"title":"Broden","links":[],"tags":["explainability"],"content":"Broden\n\nBroadly and Densely Labeled Dataset\nunifies several densely labeled image data sets: ADE [43], OpenSurfaces [4], Pascal-Context [19], Pascal-Part [6], and the Describable Textures Dataset [7]\nThese data sets contain examples of a broad range of objects, scenes, object parts, textures, and materials in a variety of contexts\nsegmented down to the pixel level except textures and scenes which are given for full-images\nevery image pixel in the data set is annotated with one of the eleven common color names according to the human perceptions classified by van de Weijer\nThe concept labels in Broden are normalized and merged from their original data sets so that every class corresponds to an English word\nLabels are merged based on shared synonyms, disregarding positional distinctions such as ‘left’ and ‘top’\n"},"KB/Bruckhaus---2024---RAG-Does-Not-Work-for-Enterprises":{"title":"Bruckhaus - 2024 - RAG Does Not Work for Enterprises","links":["KB/Dense-Vector-Indexes"],"tags":["architecture"],"content":"Bruckhaus - 2024 - RAG Does Not Work for Enterprises\n\n@bruckhausRAGDoesNot2024\n\nIntro\n\nimplementing RAG effectively in real-world, enterprise settings poses several challenges.\nThe retriever needs to efficiently search through massive, constantly-updated knowledge bases to find the most relevant information for each query [Karpukhin et al., 2020]\nThe generator needs to intelligently fuse the retrieved content with its own learned knowledge to produce coherent and accurate outputs [Shao et al., 2023]\nthe RAG system needs to satisfy stringent requirements around data security, privacy, interpretability, and auditability [Arrieta et al., 2020].\n\nEnterprise Requirements for Retrieval-Augmented Generation\n\ninclude built-in access controls, anonymization techniques, and auditing mechanisms.\nintelligently blending advanced semantic search techniques with hybrid query strategies, advanced RAG solutions retrieve the most relevant and reliable information to augment the generation process\nsuch solutions must provide clear explanations and attributions for its outputs, enabling enterprises to trust and act on the insights with confidence.\nflexible, API-driven architecture and pre-built connectors for popular enterprise systems.\n\nSurvey of Current RAG Approaches and Their Limitations\n\nLack of fine-grained control over retrieval and generation processes, which is crucial for ensuring accuracy, consistency, and regulatory compliance [Martorana et al., 2022, Anderljung et al., 2023, Rahwan et al., 2023].\nLimited scalability and performance when dealing with massive, heterogeneous enterprise knowledge bases [Ahmad et al. 2019 , Nambiar et al., 2023].\nInsufficient explainability and auditability of RAG outputs, which is essential for building trust and accountability in high-stakes enterprise use cases [Eibich et al. 2024, Gao et al. 2024, Kamath &amp; Liu 2021].\nChallenges in integrating RAG capabilities into existing enterprise systems and workflows, which often have complex security, governance, and data management requirements.\n\nDense Vector Indexes"},"KB/Bucketing":{"title":"Bucketing","links":[],"tags":["temp"],"content":"Bucketing\n\nConverting a (usually continuous) feature into multiple binary features called buckets or bins, typically based on value range. For example, instead of representing temperature as a single continuous floating-point feature, you could chop ranges of temperatures into discrete bins. Given temperature data sensitive to a tenth of a degree, all temperatures between 0.0 and 15.0 degrees could be put into one bin, 15.1 to 30.0 degrees could be a second bin, and 30.1 to 50.0 degrees could be a third bin.\n"},"KB/Building-Ethics-into-Artificial-Intelligence":{"title":"Building Ethics into Artificial Intelligence","links":["KB/Consequentialist-ethics","KB/Utilitarian-ethics","KB/Deontological-ethics","KB/Virtue-ethics","KB/Ethical-dilemmas","KB/GenEth","KB/Moral-Machine-project","KB/sacred-values","KB/MoralDM","KB/Belief-Desire-Intention","KB/blind-ethical-judgement","KB/partially-informed-ethical-judgement","KB/fully-informed-ethical-judgement","KB/Moral-decision-making-frameworks-for-artificial-intelligence","KB/Preferences-and-ethical-principles-in-decision-making","A-declarative-modular-framework-for-representing-and-applying-ethical-principles.","KB/A-low-cost-ethics-shaping-approach-for-designing-reinforcement-learning-agents","KB/Even-angels-need-the-rules-AI,-roboethics,-and-the-law","KB/Norms-as-a-basis-for-governing-sociotechnical-systems","KB/Embedding-ethical-principles-in-collective-decision-support-systems","KB/A-voting-based-system-for-ethical-decision-making","KB/swap-dominance","KB/trolley-scenario","KB/Coping-Theory"],"tags":["ethics"],"content":"Building Ethics into Artificial Intelligence\n\nHan Yu, Zhiqi Shen, Chunyan Miao, Cyril Leung, Victor R. Lesser, Qiang Yang\n\nAbstract\n\ntaxonomy which divides the field into four areas: 1) exploring ethical dilemmas; 2) individual ethical decision frameworks; 3) collective ethical decision frameworks; and 4) ethics in human-AI interactions\n\nTypes\n\nConsequentialist ethics\nUtilitarian ethics\nDeontological ethics\nVirtue ethics\nEthical dilemmas\n\nExploring Ethical Dilemmas\n\nexplore the ethical dilemmas in the target application scenarios [Anderson and Anderson, 2014]\nGenEth\nMoral Machine project\n\nIndividual Ethical Decision Frameworks\n\nAI research community largely agrees that generalized frameworks are preferred over ad-hoc rules\nif updates are provided by people, some review mechanisms should be put in place to prevent abuse\nmoral decision-making by humans not only involves utilitarian considerations, but also moral rules.\nSuch rules often involve protected values (a.k.a. sacred values)\nMoralDM\nBelief-Desire-Intention\nblind ethical judgement\npartially informed ethical judgement\nfully informed ethical judgement\nMoral decision making frameworks for artificial intelligence\nPreferences and ethical principles in decision making\nA declarative modular framework for representing and applying ethical principles.\nA low-cost ethics shaping approach for designing reinforcement learning agents\nEven angels need the rules AI, roboethics, and the law\nNorms as a basis for governing sociotechnical systems\nEmbedding ethical principles in collective decision support systems\nA voting-based system for ethical decision making\nswap-dominance\nsatisfying consequentialist ethics Ethics in Human-AI Interactions Belmont Report\n[Luckin, 2017; Yu et al., 2017b]\n\n\npeople’s personal autonomy should not be violated (they should be able to maintain their free will when interacting with the technology); 2) benefits brought\n\n\nabout by the technology should outweigh risks; and 3) the benefits\nand risks should be distributed fairly among the users (people should not be discriminated based on their personal backgrounds such as race, gender and religion)\npersuasion agents\n[Kang et al., 2015; Rosenfeld and Kraus, 2016]\n[Stock et al., 2016]\nlarge-scale study to investigate human perceptions on the ethics of persuasion by an AI agent\ntrolley scenario\nauthors tested three persuasive strategies: 1) appealing to the participants emotionally; 2) presenting the participants with utilitarian arguments; and 3) lying\nparticipants hold a strong preconceived negative attitude towards the persuasion agent, and argumentation-based and lying-based persuasion strategies work better than emotional persuasion strategies\ndid not show significant variation across genders or cultures\nadoption of persuasion strategies should take into account differences in individual personality, ethical attitude and expertise in the given domain.\nCoping Theory\nArgumentation-based explainable AI\n[Fan and Toni, 2015; Langley et al., 2017] well suited to the consequentialist ethics\ndepending on how the explanations are used, researchers need to strike a balance on the level of details to be included\nFull transparency may be too overwhelming if the objective is to persuade a user to follow a time-critical recommendation\nuseful as a mechanism to trace the AI decision process afterwards not enough transparency may hamper users’ trust in the AI\n"},"KB/Bunq":{"title":"Bunq","links":[],"tags":["jobsearch"],"content":"Bunq\nQ: How familiar are you with Python, SQL, AWS, and ETL processes?\nI have a Masters in Artificial Intelligence from the University of Groningen. From my previous internship, experiences, personal and freelance projects, and research papers, I have a good amount of experience with AWS, SQL, and ETL processes.\nAt almost every internship, I helped build/used ETL pipelines that required SQL, AWS, or some other data storage, and I am comfortable with learning new processes that might be required for this position.\nAs for Python, it is my language of choice, and I am very familiar with it. I have experience working with creating data pipelines, data analysis, and AI/ML frameworks such as PyTorch, Tensorflow, NLTK, Hugging face, sklearn, etc. I have built several open-source packages in varying domains over the past couple of years and published many analytics and AI notebooks and articles as well."},"KB/Burn-in":{"title":"Burn-in","links":[],"tags":["robotics"],"content":"Burn-in\n\nBurn-In is a robot testing procedure where all components of the robot are operated continuously for an extended period of time. This is done to test movement and movement programming of the robot at early stages to avoid malfunctions after deployment.\n"},"KB/C-section":{"title":"C-section","links":[],"tags":["medical"],"content":"C-section\n\nCaesarian section, where a baby is delivered through an abdominal and uterine incision\n"},"KB/CAM":{"title":"CAM","links":["Network-In-Network","KB/Global-Average-Pooling"],"tags":["explainability"],"content":"CAM\n\n@zhouLearningDeepFeatures2016\nClass Activation Mapping\nSimilar to Network In Network\nzeroes out the negative grads during backward pass to provide more visually appealing results\nUses Global Average Pooling\n\n\\alpha_{k}^{c}= \\overbrace{\\frac{1}{Z}\\Sigma_{i}\\Sigma_{j}}^\\text{global avg pool} \\underbrace{\\frac{\\partial y^{c}}{\\partial A^{k}_{ij}}}_\\text{grads via backprop}\nk is the index of the activation map in the last convolutional layer, and c is the class of interest. Alpha computed above shows the importance of feature map k for the target class c.\nFinally, we multiply each activation map by its importance score (i.e. alpha) and sum the values\n\nChatGPT\n\nThe paper, “Learning Deep Features for Discriminative Localization” by Zhou et al. (2016) introduces the concept of Class Activation Mapping (CAM) as a way to visualize which regions of an image are most important for a given classification task. CAM is a technique for generating heatmaps that highlight the regions in an image that are most important for a specific classification. The authors propose to use global average pooling (GAP) in the final convolutional layer to generate a feature map, followed by a linear combination of the feature map and the class weight vector to generate a single class activation map.\nThe authors apply CAM to the ResNet architecture and show that it outperforms the traditional fully-connected layer approach in terms of localization performance. They test the CAM on the image classification task using the ILSVRC-2012 dataset. The authors showed that by using CAM, they could identify the specific regions of an image that were important for a given classification, rather than just a “black box” decision made by the model. The authors also demonstrate how CAM can be used for fine-grained recognition, where the model is trained to identify sub-categories within a larger class.\nAdditionally, the authors also show that the CAM can be used to improve the interpretability of deep neural networks by providing a visual representation of the model’s decision-making process. They also use CAM to identify misclassifications and analyze the model’s decision-making process. The authors test CAM on other architectures such as VGG and GoogleNet and show that it can be applied.\nThe authors also use CAM for multi-label classification and show that it can identify the regions in an image that are relevant to multiple labels. They also use CAM for action classification in video and show that it can identify the regions of video frames that are important for a given action. They use CAM for object detection and show that it can be used to identify the regions of an image that contain an object of interest.\nThe authors use CAM for fine-tuning a pre-trained model on a new dataset and show that it can be used to improve the performance of the model on the new dataset. They also use CAM for unsupervised feature learning and show that it can be used to learn features that are useful for a wide range of tasks. They use CAM for zero-shot learning and show that it can be used to identify the regions of an image that are relevant to a class that the model has never seen before.\nFinally, the authors use CAM for domain adaptation and show that it can be used to identify the regions of an image that are important for a specific task, even when the model has been trained on a different dataset. They also use CAM for weakly-supervised object localization and show that it can be used to identify the regions of an image that contain an object of interest, even when only image-level labels are available. They use CAM for multi-modal learning and show that it can be used to identify the regions of an image that are important for a given task, even when multiple modalities (e.g. image and text) are available.\nThe authors conclude that the CAM is a powerful technique for visualizing the decision-making process of a deep neural network and can be used to improve the interpretability, performance, and robustness of deep models.\n"},"KB/CBOW":{"title":"CBOW","links":["KB/Bag-of-words"],"tags":["nlp"],"content":"CBOW\n\nContinous implementation of Bag of words\ntries to predict the current target word (the center word) based on the source context words (surrounding words)\n“the quick brown fox jumps over the lazy dog”, this can be pairs of (context_window, target_word) where if we consider a context window of size 2, we have examples like ([quick, fox], brown), ([the, brown], quick), ([the, dog], lazy) and so on\ncontext window\n\nseveral times faster to train than the skip-gram, slightly better accuracy for the frequent words.\nCBOW is prone to overfit frequent words because they appear several time along with the same context.\ntends to find the probability of a word occurring in a context\nit generalizes over all the different contexts in which a word can be used\nalso a 1-hidden-layer neural network\nThe synthetic training task now uses the average of multiple input context words, rather than a single word as in skip-gram, to predict the center word.\nAgain, the projection weights that turn one-hot words into averageable vectors, of the same width as the hidden layer, are interpreted as the word embeddings.\n"},"KB/CDF":{"title":"CDF","links":["KB/Density"],"tags":["temp"],"content":"CDF\n\nget cumulative Density function \\varphi : \\mathbb{R} \\rightarrow [0,1]\n"},"KB/CIFAR":{"title":"CIFAR","links":[],"tags":["dataset"],"content":"CIFAR\n\n60’000 images\n10 classes with 6’000 images\nimage size: 32x32x3\n50’000 training, 10’000 testing\n"},"KB/CLIP":{"title":"CLIP","links":["KB/Contrastive-Loss","KB/GPT","KB/GPT3"],"tags":["architecture"],"content":"CLIP\n\nLearning Transferable Visual Models from Natural Language Supervision\nintroduces CLIP, a pre-training task which efficiently learns visual concepts from natural language supervision\nperforms language-guided image generation\nuses vision and language encoders trained in isolation and uses a Contrastive Loss to bring similar image-text pairs closer, while pulling apart dissimilar pairs as a part of pretaining\ncan be applied to any visual classification benchmark by simply providing the names of the visual categories to be recognized, similar to the “zero-shot” capabilities of GPT and GPT3\npre-trains an image encoder and a text encoder to predict which images were paired with which texts in our dataset\nzero-shot classifier\nthey convert all of a dataset’s classes into captions such as “a photo of a dog” and predict the class of the caption CLIP estimates best pairs with a given image\n\n\ntoc: true\ntitle: CLIP\ntags: [‘architecture’]\nCLIP\n\nis a neural network trained on a variety of (image, text) pairs\nUsing CLIP, that can be instructed in natural language to predict the most relevant text snippet, given an image, the model has recently merged as a successful representation learner for images\nConcretely, CLIP embeddings have several desirable properties\nthey are robust to image distribution shift, have impressive zero-shot capabilities and have been fine-tuned to achieve state-of-theart results\nthe CLIP image embedding decoder module is combined with a prior model, which generates possible CLIP image embeddings from a given text caption\n"},"KB/COCO":{"title":"COCO","links":[],"tags":["dataset"],"content":"COCO"},"KB/CRISPR-(clustered-Regularly-interspaced-Short-Palindromic-repeats)":{"title":"CRISPR (clustered Regularly-interspaced Short Palindromic repeats)","links":[],"tags":["brain"],"content":"CRISPR (clustered Regularly-interspaced Short Palindromic repeats)\n\nA relatively precise and reliable DNA-editing technique.\n"},"KB/CTC":{"title":"CTC","links":["KB/Recurrent"],"tags":["loss"],"content":"CTC\n\nConnectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks\nMany real-world sequence learning tasks require the prediction of sequences of labels from noisy, unsegmented input data\nRecurrent neural networks (RNNs) are powerful sequence learners that would seem well suited to such task\nhey require pre-segmented training data, and post-processing to transform their outputs into label sequences, their applicability has so far been limited\ntemporal classification\nlabel unsegmented sequences directly\nprobabilistic principles\nTIMIT speech corpus\n"},"KB/CUB-200-2011-4":{"title":"CUB-200-2011","links":[],"tags":["dataset"],"content":"CUB-200-2011\n\nCaltech-UCSD Birds 200 (CUB-200) is a dataset of images of birds, with 200 different species of birds and 11,788 images in total.\n"},"KB/CUB-200-2011":{"title":"CUB-200-2011","links":[],"tags":["dataset"],"content":"CUB-200-2011\n\nCaltech-UCSD Birds 200 (CUB-200) is a dataset of images of birds, with 200 different species of birds and 11,788 images in total.\n"},"KB/Cache-Coherence":{"title":"Cache Coherence","links":[],"tags":["parallelcomputing"],"content":"Cache Coherence\n\nIndividual CPU caches or memories can become out of synch with each other\nif one processor updates a location in shared memory, all the other processors know about the update\n"},"KB/Calibration-Layer":{"title":"Calibration Layer","links":[],"tags":["temp"],"content":"Calibration Layer\n\nA post-prediction adjustment, typically to account for prediction bias. The adjusted predictions and probabilities should match the distribution of an observed set of labels.\n"},"KB/Candidate-Sampling":{"title":"Candidate Sampling","links":[],"tags":["temp"],"content":"Candidate Sampling\n\nA training-time optimization in which a probability is calculated for all the positive labels, using, for example, softmax, but only for a random sample of negative labels. For example, if we have an example labeled beagle and dog candidate sampling computes the predicted probabilities and corresponding loss terms for the beagle and dog class outputs in addition to a random subset of the remaining classes (cat, lollipop, fence). The idea is that the negative classes can learn from less frequent negative reinforcement as long as positive classes always get proper positive reinforcement, and this is indeed observed empirically. The motivation for candidate sampling is a computational efficiency win from not computing predictions for all negatives.\n"},"KB/Capacitance":{"title":"Capacitance","links":[],"tags":["physics"],"content":"Capacitance\n\nCharge stored/Potential Difference\nC= Q/V\n"},"KB/Capsule-Layer":{"title":"Capsule Layer","links":[],"tags":["architecture"],"content":"Capsule Layer\n\n Each capsule is a group of neurons that is sensitive to a specific feature of the input image\n\n\n"},"KB/Capsule-Network":{"title":"Capsule Network","links":["KB/Translational-Invariance","KB/Downsampling","KB/Capsule-Layer","KB/Primary-Capsule","KB/Higher-Layer-Capsule","KB/Max-Margin-Loss","KB/Reconstruction-loss"],"tags":["architecture"],"content":"Capsule Network\n\nreplace traditional convolutional and pooling layers with a more biologically inspired architecture that better captures the spatial relationships between objects in an image\nidea that the human visual system is composed of a hierarchy of “capsules” that process visual information at different levels of abstraction.\nEach capsule comprises a group of neurons sensitive to specific features of an image, such as the presence of an edge or a particular shape.\nThese features are then combined and passed up the hierarchy to higher-level capsules, which extract more abstract concepts such as the identity of an object or the presence of a face.\nCapsule Networks overcome the problem of Translational Invariance caused by CNNs.\nCapsule Networks are able to capture better spatial relationship. \nCapsule Networks uses better Downsampling methods which do not cause loss of information seen in CNNs.\nCapsule Network perform much better than CNNs but are more computationally epensive.\n\nDrawbacks of pooling layers\n\npooling layers, which down-sample the input image and can lead to the loss of important information about the spatial relationships between objects in the image.\nCapsule networks aim to overcome this limitation by using a different down-sampling mechanism that preserves more spatial information.\nCapsule Layer\nPrimary Capsule\nHigher Layer Capsule\n\nLoss\n\nMax Margin Loss\nReconstruction loss\n\nPros\n\nCapsule networks are more robust to image distortions and translations than traditional CNNs\nThey can maintain the spatial relationships between objects in an image\nThey can handle partially obscured objects better\nThey can be used for a variety of tasks, including object recognition and segmentation\n\nCons\n\nCapsule networks are more complex and computationally expensive than traditional CNNs\nThey are a relatively new architecture, and there is still ongoing research to improve their performance and computational efficiency.\n"},"KB/Capture-bias":{"title":"Capture bias","links":[],"tags":["ethics"],"content":"Capture Bias\n\nphotographers tending to take pictures of objects in similar ways\nSearching for “mug” on Google Image Search will reveal another kind of capture bias: almost all the mugs has a right-facing handle\nBeyond better data sampling strategies, one way to deal with this is to perform various data transformations to reduce this bias\n"},"KB/Cardinality-Principle":{"title":"Cardinality Principle","links":[],"tags":["language"],"content":"Cardinality Principle\n\nhow each step in the counting sequence (1,2,3,4,5,6….) means an increase of one individual\n"},"KB/Carousel":{"title":"Carousel","links":[],"tags":["robotics"],"content":"Carousel\n\nA rotating platform that delivers objects to a robot and serves as an object queuing system. This carousel delivers the objects, or work pieces, to the loading/unloading station of the robot.\n"},"KB/Case-Grammar":{"title":"Case Grammar","links":[],"tags":["language"],"content":"Case Grammar\n\nThe structure that is built by the parser contains some semantic information, although further interpretation may also be necessary\n"},"KB/Categorical-Distribution":{"title":"Categorical Distribution","links":[],"tags":["distributions"],"content":"Categorical Distribution\n \n\n\n"},"KB/Causability":{"title":"Causability","links":[],"tags":["explainability"],"content":"Causability\n\nmeasures how exact an interpretable model is in imitating the behavior of a black box.\nfidelity\nIt is measured in terms of the accuracy score, but with respect to the outcome of the black box, similarly to the model accuracy.\n"},"KB/Causal-1D-Conv":{"title":"Causal 1D Conv","links":["KB/Conv"],"tags":["temp"],"content":"Causal 1D Conv\n\nOnly past info used for prediction\nConv works in both directions and can leak future information into predictions\n"},"KB/Causal-Dilated-Conv":{"title":"Causal Dilated Conv","links":["KB/Conv","KB/Receptive-field"],"tags":["temp"],"content":"Causal Dilated Conv\n\nReceptive field is how much of the input sequence is needed for one prediction\n"},"KB/Causal-Language-Model":{"title":"Causal Language Model","links":["KB/Masked-Language-Modeling"],"tags":["nlp"],"content":"Causal Language Model\n\nUnlike Masked Language Modeling, this is uni-directional.\nCan consider words to its left\nBetter for generating text\n"},"KB/Causal-Systems":{"title":"Causal Systems","links":["KB/TIme-Series"],"tags":["temp"],"content":"Causal Systems\n\nDoes not depend on future input\nHas memory if current input not fully determined by previous one but influenced by earlier inputs\nTIme Series\n"},"KB/Causality":{"title":"Causality","links":[],"tags":["explainability"],"content":"Causality\n\nAn understandable explanation must be created by a machine in a given time (e.g., one hour or one day) and can be comprehended by a user, who need not to be an expert, but has an educational background\nThe user keeps asking a finite number of questions of the machine until he/she can no longer ask why or how because he/she has a satisfactory answer; we say he/she has comprehended.\nThis is the relationship between cause and effect; it is not a synonym for causability\nCausability is about measuring and ensuring the quality of an explanation and refers to a human model\ncausality requires a wide frame of prior knowledge to prove that observed effects are causal\nA ML model only discovers correlations among the data it learns from, and therefore might not suffice for unveiling a cause-effect relationship.\nHowever, causation involves correlation, so an explainable ML model could validate the results provided by causality inference techniques, or provide a first intuition\n"},"KB/CenterNet":{"title":"CenterNet","links":["Linear-Regression","KB/COCO"],"tags":["temp"],"content":"\ntoc: true\ntitle: CenterNet\ntags: [‘temp’]\n\nCenterNet\n\npaper\ncenter point-based object detection approach\nend-to-end differentiable\nbounding box based detectors\nAnchorless\nkeypoint estimation networks to find center points\nid:: 62a89b04-c7bf-4205-9695-39da04c2aafb\nLinear Regression to all other properties\nid:: 62a89d01-53af-46f6-82d4-362fab069b46\nCOCO\nSingle stage\nid:: 62a89d42-312d-4f3d-8235-c54a9dfafadf\n"},"KB/Central-Limit-Theorem":{"title":"Central Limit Theorem","links":["KB/Normal-Distribution"],"tags":["temp"],"content":"Central Limit Theorem\n\nWhen random effects of many independant small sized causes sum up to large scale observable effects : one gets the Normal Distribution\nLet (X_{i})_{i\\in N} is a seq of independant, real valued, [[X_{i}- E[X_{i}|(X_{i}- E[X_{i}]] = E[[X_{i}- E[X_{i}|[X_{i}- E[X_{i}]] P_{S_{n}} of standardized sum variables converge weakly to \\mathscr{N}(0,1|[Square Integrable] . S_{n}= \\frac{\\Sigma_{i= 1}^{n}(X_{i}- E[X_{i}])}{\\sigma(\\Sigma^{n}_{i=1}X_{i})}\n\nConverge weakly : lim_{n\\rightarrow\\infty}\\int f(x)P_{n}(dx) = \\int f(x)P(dx) for all f: \\mathbb{R} \\rightarrow \\mathbb{R}\nLebesgue Integrals\n\n\n\nX_{i} Are Identically Distributed\n\nRegardless of shape of each X_{i}, distribution of normalized sum converges to \\mathscr{N}(0,1)\nUniformly bounded\nNone of the X_{i} dominates the other “washing out”\n"},"KB/Central-Nervous-System":{"title":"Central Nervous System","links":["KB/brain","Spinal-Cord"],"tags":["brain"],"content":"Central Nervous System\n\nbrain + Spinal Cord\n"},"KB/Central-Sulcus":{"title":"Central Sulcus","links":["KB/Sulcus","KB/Cerebrum","KB/Frontal-lobe"],"tags":["brain"],"content":"Central Sulcus\n\nThe primary groove in the brain’s Cerebrum, which separates the Frontal lobe in the front of the brain from the parietal and occipital lobes in the rear of the brain.\n"},"KB/Centrifugal-Force":{"title":"Centrifugal Force","links":["KB/Force"],"tags":["robotics"],"content":"Centrifugal Force\n\nWhen a body rotates about an axis other than one at it’s center of mass, it exerts an outward radial Force called centrifugal Force upon the axis, which restrains it from moving in a straight tangential line. To offset this Force, the robot must exert an opposing torque at the joint of rotation.\n"},"KB/Centripetal-Force":{"title":"Centripetal Force","links":["KB/Force"],"tags":["physics"],"content":"Centripetal Force\n\ncentripetal Force = mass x speed2   radius of path\nF_{C}= \\frac{mv^{2}}{r}\n"},"KB/Centroid":{"title":"Centroid","links":[],"tags":["temp"],"content":"Centroid\n\nThe center of a cluster as determined by a k-means or k-median algorithm. For instance, if k is 3, then the k-means or k-median algorithm finds 3 centroids.\n"},"KB/Cerebellar-Artery":{"title":"Cerebellar Artery","links":["KB/Cerebellum"],"tags":["brain"],"content":"Cerebellar Artery\n\nThe major blood vessel providing oxygenated blood to the Cerebellum.\n"},"KB/Cerebellum":{"title":"Cerebellum","links":["KB/Basal-Ganglia"],"tags":["brain"],"content":"Cerebellum\n\nIts function is to coordinate muscle movements, maintain posture, and balance\nrelays information to the Basal Ganglia.\nIt stores automatic learned memories like tying a shoe, playing an instrument, or riding a bike.\n"},"KB/Cerebral-Palsy":{"title":"Cerebral Palsy","links":[],"tags":["brain"],"content":"Cerebral Palsy\n\nA developmental disorder resulting from damage to the brain before or during birth, usually characterized by impaired muscle coordination and body movements, but can also include impaired cognition and social behavior.\n"},"KB/Cerebrospinal-Fluid-(CSF)":{"title":"Cerebrospinal Fluid (CSF)","links":[],"tags":["brain"],"content":"Cerebrospinal Fluid (CSF)\n\nThe clear, colorless liquid found surrounding the brain and spinal cord. This fluid can be analyzed to detect diseases.\n"},"KB/Cerebrum":{"title":"Cerebrum","links":["KB/Corpus-callosum","KB/Brain-Cortex","KB/Frontal-lobe","KB/Parietal-lobe","KB/Occipital-lobe","KB/Temporal-lobe"],"tags":["brain"],"content":"Cerebrum\n\nlargest part of the brain\nperforms higher functions like interpreting touch, vision and hearing, as well as speech, reasoning, emotions, learning, and fine control of movement\nDivided by Corpus callosum\nSurface is called the Brain Cortex\n\nFrontal lobe\nParietal lobe\nOccipital lobe\nTemporal lobe\n"},"KB/Chain-of-Thought":{"title":"Chain of Thought","links":["KB/language"],"tags":["temp"],"content":"Chain of Thought\n\nChain of Thought Prompting Elicits Reasoning in Large Language Models\nability of language models to generate a coherent chain of thought\nseries of short sentences that mimic the reasoning process a person might have when responding to a question\nthe more complex the task of interest is (in the sense of requiring multi-step reasoning approach), the bigger the boost from the chain of thought prompting!\nchain of thought processing is an emergent property of model scale that can be induced via prompting and can enable sufficiently large language models to better perform reasoning tasks that otherwise have flat scaling curves.\n"},"KB/Challenges-of-Graphs":{"title":"Challenges of Graphs","links":["KB/Graphs"],"tags":["graph"],"content":"Challenges of Graphs\n \nChallenges of Graphs\n\nvariable topology\n\nhard to design networks that are expressive enough\n\n\nmight be enormous\nmay only have a monolithic graph → so cant do training and testing with new data\n"},"KB/Challenges-of-Words-and-rules":{"title":"Challenges of Words-and-rules","links":["KB/Words-and-Rules"],"tags":["language"],"content":"Challenges of Words-and-Rules\n\nWords-and-Rules fits a lot of the data, but is vague on\n\nExactly what innate structures are available to a child learner\nExactly how learning proceeds\n\n\n"},"KB/Change-Blindness":{"title":"Change Blindness","links":[],"tags":["visualization"],"content":"Change Blindness\n\nDifficulty detecting changes in separated scenes even after careful inspection\nOnce found viewers agree that it was trivial\nNot due to limited visual acuity but inappropriate attentional guidance\nTemporal separation (instead of spatial)\nsudden changes within a static scene are easily perceived (cf. preattentiveness of motion)\nChange blindness also in animations if temporal separation spans multiple scenes\nA scene that should be the same but differs between cuts is known as continuity error\n"},"KB/Change-in-Gravitational-Potential-Energy":{"title":"Change in Gravitational Potential Energy","links":[],"tags":["temp"],"content":"Change in Gravitational Potential Energy\n\nmass x gravitational field strenth x  difference in height\nDGPE = mgDh\n"},"KB/Change-of-variable-theorem":{"title":"Change of variable theorem","links":["images/6bf14b0bc57f4e48294e30954defc432_MD5.jpeg"],"tags":["algebra"],"content":"Change of Variable Theorem\n \n\nthis is from here\nGiven a random variable z and its known probability density function z \\sim \\pi(z), we would like to construct a new random variable using a 1-1 mapping function x = f(z). The function f is invertible, so z=f^{-1}(x).\nNow the question is how to infer the unknown probability density function of the new variable, p(x)\n\nExample\n\nOpen: Pasted image 20241119160831.png\n\ndefine how space will be transformed locally\n\ndet(J) = 2\\cdot2 -0\\cdot0 = 4 (area increases by a factor of 4)\n\n\np(x) = p(z) |det(\\frac{\\partial z}{\\partial x})| = p(z) |1/ det(\\frac{\\partial x}{\\partial z})|\n\neach point in the X region should have 1/4th of the area of it’s inverse\norientation does not matter\n\n\n\nDerivation\n\\begin{aligned}\n&amp; \\int p(x)dx = \\int \\pi(z)dz = 1 \\scriptstyle{\\text{   ; Definition of probability distribution.}}\\\\\n&amp; p(x) = \\pi(z) \\left\\vert\\frac{dz}{dx}\\right\\vert = \\pi(f^{-1}(x)) \\left\\vert\\frac{d f^{-1}}{dx}\\right\\vert = \\pi(f^{-1}(x)) \\vert (f^{-1})&#039;(x) \\vert\n\\end{aligned}\n\nBy definition, the integral \\int \\pi(z)dz is the sum of an infinite number of rectangles of infinitesimal width \\Delta z.\nThe height of such a rectangle at position z is the value of the density function \\pi(z). When we substitute the variable, z = f^{-1}(x) yields \\frac{\\Delta z}{\\Delta x} = (f^{-1}(x))&#039; and \\Delta z =  (f^{-1}(x))&#039; \\Delta x.\nHere \\vert(f^{-1}(x))&#039;;\\vert indicates the ratio between the area of rectangles defined in two different coordinate of variables z and x respectively.\nThe multivariable version has a similar format\n\n\\begin{aligned}\n\\mathbf{z} &amp;\\sim \\pi(\\mathbf{z}), \\mathbf{x} = f(\\mathbf{z}), \\mathbf{z} = f^{-1}(\\mathbf{x}) \\\\\np(\\mathbf{x}) \n&amp;= \\pi(\\mathbf{z}) \\left\\vert \\det \\dfrac{d \\mathbf{z}}{d \\mathbf{x}} \\right\\vert  \n= \\pi(f^{-1}(\\mathbf{x})) \\left\\vert \\det \\dfrac{d f^{-1}}{d \\mathbf{x}} \\right\\vert\n\\end{aligned}\nwhere \\det \\frac{\\partial f}{\\partial\\mathbf{z}} is the Jacobian determinant of the function f."},"KB/Channels":{"title":"Channels","links":[],"tags":["architecture"],"content":"Channels\n \n\n\n\n\nMany to one or one to many or one to one\n"},"KB/Chapter-16---Normalizing-Flows":{"title":"Chapter 16 - Normalizing Flows","links":["KB/Jacobian-Matrix","KB/bijective-function","KB/surjection","KB/PDF","KB/Change-of-variable-theorem","KB/Maximum-Likelihood","KB/NICE---non-linear-independant-components-estimation","KB/coupling-flows","KB/Density-estimation-using-real-NVP","KB/Properties-Required-by-Network-Layers-for-Normalizing-Flows","KB/Types-of-Normalizing-flows","KB/ELBO-loss"],"tags":["architecture","distributions"],"content":"Chapter 16 - Normalizing Flows\n \n\nWarning : If I explain every equation fully, we won’t get lunch. So I will share this document later.\n\n\nIntro\n\nGANs → samples should be as close to real data as possible → hard to identify distribution\nNormalizing flow → take an input distribution → convert it to a more complicated one using a NN → learn a probability model\n\n\nAlgebra\n\nJacobian Matrix\nbijective function, surjection\n\nWhat is a Normalizing Flow\n\nDensity estimation is hard\n\nwe need to run backprop in deep learning models, the posterior p(z|x) should be easy to calculate the derivative of. So, we use a gaussian distribution when we can. (Of course this is not really nice always)\n\n\nNF models offer better and more powerful distribution approximation\nTransforms a simple distribution into a complex one by applying a sequence of invertible transformation functions\n\nFlowing through a chain of transformations, we repeatedly substitute the variable for the new one according to the change of variables theorem and eventually obtain a probability distribution of the final target variable.\n\n\n\n\nUses of Normalizing Flows\n\n\nApproximating density models\n\nSteps for a Normalizing Flow\n\nConsider a continuous random variable z \\sim p_{\\theta(z)}= \\mathcal{N}(z;0,I) (simple distribution like Spherical gaussian)\nTransform this distribution using a composition of functions to a more complicated one. x = f_{\\theta}(z)\neach f_{i} is invertible (bijective function), so all of the transformations are also invertible x = f_{\\theta}(z) = f_{k} \\circ … \\circ f_{2} \\circ f_{1}(z)\n\n\n\n\nSo what is the PDF of x?\n\nMaybe, p_{\\theta(x)}= p_\\theta(f^{-1}_{\\theta}(x))\nbut this is not really always true, and depends on the behavior of the function f\nwhy?\n\n\nz = f(x) is where the point in x moves to z\nthe det is the mass that gets pushed in the neighborhood\n\n\nforward and backward mapping\n\n\n\n\n\n\nFrom the Change of variable theorem and Jacobian Matrix, we get p_{\\theta}(x)= p_{\\theta}(f^{-1}(\\mathbf{x})) \\left\\vert \\det (\\dfrac{\\partial f^{-1}(x)}{\\partial \\mathbf{x}}) \\right\\vert\n\nAssume f^{-1}(x) =z\np_{\\theta}(x)= p_{\\theta}(z) \\left\\vert \\det (\\dfrac{\\partial z}{\\partial \\mathbf{x}}) \\right\\vert\nA valid pdf must always integrate to 1\n\n\nSo what is a normalizing flow?\n\nA sequence of bijective transformations is a normalizing flow. After z is initially sampled, it flows through these steps.\n\nDensity function is re-normalized at each step to ensure it remains valid.\n\n\n\n\nforward mapping for deep neural networks\n- \n- \nLoss function\n\nLet us parameterize the likelihood function as a flow\n\nWe have, p_{\\theta(x)}= p_{\\theta}(z)\\left\\vert \\det (\\dfrac{\\partial f^{-1}}{\\partial \\mathbf{x}}) \\right\\vert\nApplying log on both sides, \\log p_{\\theta(x)}= \\log p_{\\theta}(z) + \\log \\left\\vert \\det (\\dfrac{\\partial f^{-1}}{\\partial \\mathbf{x}}) \\right\\vert\n\n\nOverall transformation f is a composition of a sequence of functions\n\nreplace the log determinant with the sum of the log determinants of the intermediate jacobians\n\\log p_{\\theta(x)}= \\log p_{\\theta}(z) + \\Sigma_{i=1}^{K} \\log \\left\\vert \\det (\\dfrac{\\partial f^{-1}}{\\partial \\mathbf{x}}) \\right\\vert\nUsing this, we can now use Maximum Likelihood to train our model since we can calculate the log marginal likelihood exactly\nExact posterior inference (unique z for a given x using (z = f^{-1}(x))\n\n\nThis determinant is now very expensive to compute (of course it is :P), so we ensure that the Jacobian is triangular.\n\ndeterminant of a triangular matrix = product of it’s diagonal\nto ensure triangular : the paper NICE - non linear independant components estimation uses coupling flows\n\n\nUsing coupling flows, we now get \\log p_{\\theta(x)}= \\Sigma_{i=1}^D [\\log (p_\\theta(f^{-1}(x)_{i}))- log(\\vert S_{ii} \\vert)]\n\n\nNow it is annoying to only have latent space = dimensionality of the data (need this for a bijection)\n\nDensity estimation using real NVP\n\n\n\nOther Detailed Information (if time permits)\n\nProperties Required by Network Layers for Normalizing Flows\nHow do we handle different types of data - Types of Normalizing flows\n\nVs Others\n\nvs. VAE\n\ncan only get the lower bound on log-likelihood (ELBO loss)\napproximate posterior q_\\theta(z|x)\n\n\nGAN\n\nno log likelihood, only min-max evaluation\nno latent variable inference\n\n\n\nReferences + Resources for the Math\n\nAri seff\nlilian weng\nhans van gorp\n4 hour cvpr tutorial\n"},"KB/Character-set-dependence":{"title":"Character-set dependence","links":["KB/ASCII","KB/8-bit-character-set","KB/2-byte-character-set","Unicode-5.0"],"tags":["language"],"content":"Character-set Dependence\n\nASCII\n8 bit character set\n2 byte character set\nUnicode 5.0\n"},"KB/Characteristics-of-Visual-Variables":{"title":"Characteristics of Visual Variables","links":["KB/Visual-Selective","KB/Visual-Associative","KB/Visual-Ordered","KB/Visual-Quantitative","KB/Visual-Length"],"tags":["visualization"],"content":"Characteristics of Visual Variables\n\nVisual Selective\nVisual Associative\nVisual Ordered\nVisual Quantitative\nVisual Length\n"},"KB/Charge":{"title":"Charge","links":[],"tags":["temp"],"content":"Charge\n\nCurrent x time\nDQ = IDt\n"},"KB/Chat-GPT-is-Not-All-You-Need":{"title":"chatgptisnotallyouneed","links":["KB/DALL·E-2","KB/CLIP","KB/Stable-Difusion","KB/Muse","KB/Dreamfusion","KB/Magic3D","KB/Flamingo","KB/VisualGPT","KB/Phenaki","KB/Soundify","KB/AudioLM","KB/Jukebox","KB/Whisper","KB/ChatGPT","KB/LaMDA","KB/PEER","KB/Meta-AI-Speech-from-Brain","KB/Codex","KB/Alphacode","KB/Galactica","KB/Minerva","KB/Imagen"],"tags":["architecture"],"content":"Chatgptisnotallyouneed\n\n\n@gozalo-brizuelaChatGPTNotAll2023\n\n\nDALL·E 2\n\n\nCLIP\n\n\nStable Difusion\n\n\nMuse\n\n\nDreamfusion\n\n\nMagic3D\n\n\nFlamingo\n\n\nVisualGPT\n\n\nPhenaki\n\n\nSoundify\n\n\nAudioLM\n\n\nJukebox\n\n\nWhisper\n\n\nChatGPT\n\n\nLaMDA\n\n\nPEER\n\n\nMeta AI Speech from Brain\n\n\nCodex\n\n\nAlphacode\n\n\nGalactica\n\n\nMinerva\n\n\nImagen\n\n"},"KB/ChatGPT":{"title":"ChatGPT","links":[],"tags":["architecture"],"content":"ChatGPT\n\ninteracts in a conversational way\nthe model answers follow-up questions, challenges incorrect premises and reject inappropriate requests\nReinforcement Learning for Human Feedback\nan initial model is trained using supervised fine-tuning: human AI trainers would provide conversations in which they played both sides, the user and an AI assistant\nthose people would be given the model-written responses to help them compose their response\nThis dataset was mixed to that of InstructGPT [3], which was transformed into a dialogue format\n"},"KB/Chebyshev-Distance":{"title":"Chebyshev Distance","links":[],"tags":["loss"],"content":"Chebyshev Distance\n\nD(x,y) = max_{i}(|x_{i}-y_{i}|)\ngreatest of difference between two vectors along any coordinate dimension\nsimply the maximum distance along one axis.\nChessboard distance since the minimum number of moves needed by a king to go from one square to another is equal to Chebyshev distance\ncan be used to extract the minimum number of moves needed to go from one square to another\nwarehouse logistics as it closely resembles the time an overhead crane takes to move an object\n"},"KB/CheckList":{"title":"CheckList","links":["KB/Issues"],"tags":["benchmark"],"content":"CheckList\n\nBeyond Accuracy: Behavioral Testing of NLP Models with CheckList\nML systems can run to completion without throwing any errors (indicating functional correctness) but can still produce incorrect outputs (indicating behavioral Issues)\nCheckList\nmodel-agnostic and task-agnostic methodology for testing NLP models inspired by principles of behavioral testing\nmatrix of general linguistic capabilities and test types that facilitate comprehensive test ideation\nMinimum Functionality Test (MFT): A Minimum Functionality Test (MFT) uses simple examples to make sure the model can perform a specific task well. For example, they might want to test the performance of a sentiment model when dealing with negations\nInvariance Test: Besides testing the functionality of a model, they might also want to test if the model prediction stays the same when trivial parts of inputs are slightly perturbed. These tests are called Invariance Tests (IV)\nDirectional Expectation Test: In the Invariance Test, they expect the outputs after the perturbation to be the same. However, sometimes they might expect the output after perturbation to change. That is when Directional Expectation Tests comes in handy\n"},"KB/Chi-Squared-Distance":{"title":"Chi Squared Distance","links":[],"tags":["loss"],"content":"Chi Squared Distance\n\nd= \\Sigma_{i}\\frac{p_{i}-q_{i}}{p_{i}+q_{i}}\n"},"KB/Chimera":{"title":"Chimera","links":[],"tags":["brain"],"content":"Chimera\n\nA single organism with cells from more than one distinct genotype.\n"},"KB/Chinchilla":{"title":"Chinchilla","links":["KB/Transformer","KB/language","KB/GPT","KB/MMLU"],"tags":["architecture"],"content":"Chinchilla\n\nTraining Compute-Optimal Large Language Models\ngiven a 10x increase in computational budget, model size should increase 5.5x, and the number of tokens should only increase 1.8x\nmodel and data size should increase in accordance\ncollecting high-quality datasets will play a key role in further scaling of LLMs\noptimal model size and number of tokens for training a Transformer language model under a given compute budget\nBy training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, they find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled\nsignificantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks\nubstantially less compute for fine-tuning and inference, greatly facilitating downstream usage\nMMLU\n"},"KB/Chronic-Encephalopathy-Syndrome-(CES)":{"title":"Chronic Encephalopathy Syndrome (CES)","links":[],"tags":["brain"],"content":"Chronic Encephalopathy Syndrome (CES)\n\nSymptoms, including memory [[CTE|chronic traumatic encephalopathy (CTE|[issues.md|issues]]]])).md).\n"},"KB/Chronic-Traumatic-Encephalopathy-(CTE)":{"title":"Chronic Traumatic Encephalopathy (CTE)","links":[],"tags":["brain"],"content":"Chronic Traumatic Encephalopathy (CTE)\n\nOnce known as dementia pugilistica and thought to be confined largely to former boxers, this neurodegenerative disease, with symptoms including impulsivity, memory problems, and depression, affects the brains of individuals who have suffered repeated concussions and traumatic brain injuries.\n"},"KB/Chronic":{"title":"Chronic","links":[],"tags":["medical"],"content":"Chronic\n\nDescribes a condition that is persistent or recurring\n"},"KB/Circular-Motion-Type":{"title":"Circular Motion Type","links":[],"tags":["robotics"],"content":"Circular Motion Type\n\nA calculated path that the robot executes, and is circular in shape.\n"},"KB/Circumfix":{"title":"Circumfix","links":[],"tags":["language"],"content":"Circumfix\n\nprecede and follow the stem\n"},"KB/Cityscapes":{"title":"Cityscapes","links":[],"tags":["dataset"],"content":"Cityscapes"},"KB/Clamp":{"title":"Clamp","links":["KB/End-effector","KB/Force"],"tags":["robotics"],"content":"Clamp\n\nAn End-effector which serves as a pneumatic hand that controls the grasping and releasing of an object. Tactile, and feed-back Force sensors are used to manage the applied Force to the object by the clamp.\n"},"KB/Clamping":{"title":"Clamping","links":["KB/Force"],"tags":["robotics"],"content":"Clamping\n\nThe maximum permissible Force acting on a body region, resulting from a robot collision where the period of contact results in a plastic deformation of a person’s soft tissue.\n"},"KB/Class-Conditional-distribution":{"title":"Class Conditional Distribution","links":["KB/PDF"],"tags":["distributions"],"content":"Class Conditional Distribution\n\nf_{i} is the PDF for P_{X|Y=c_{i}}\n"},"KB/Class-Size":{"title":"Class Size","links":[],"tags":["temp"],"content":"Class Size\n\nClass inclusion seq : Set of candidate models with increasing flexibility\n\\mathcal{H}_{1} \\subset \\mathcal{H}_{2} \\subset, …, \\subset \\mathcal{H}_{l} \n"},"KB/Classification-Ray-Casting":{"title":"Classification Ray Casting","links":["KB/Transfer-Function","KB/Pre-Classification","KB/Post-Classification"],"tags":["visualization"],"content":"Classification Ray Casting\n\nTransfer Function\nPre Classification\nPost Classification\n"},"KB/Classifier-Gradients":{"title":"Classifier Gradients","links":[],"tags":["architecture"],"content":"Classifier Gradients\n\nFor example, if we want to add sunglasses to an image of a face, we can used a trained classifier that identifies if a personal has that feature.\nTo do this, we can take a batch of noise vector Z that goes through the generator.\nWe then pass this image through a classifier, in this case a sunglasses classifier, which will tell us if the output has that feature.\nWe the use this information to modify the Z vectors, without modifying the weights of the generator at all.\nTo do so, we modify the Z vectors by moving in the direction of the gradient with the costs that will penalize the model for images classified as not having sunglasses.\nWe then repeat this process until the images are classified with the desired feature.\nThe downside with this method is that we need a pre-trained classifier that can detect the desired feature, which may not always be readily available.\n"},"KB/Classifying-a-specific-image-region-using-convolutional-nets-with-an-ROI-mask-as-input":{"title":"Classifying a specific image region using convolutional nets with an ROI mask as input","links":["KB/COCO"],"tags":["explainability"],"content":"Classifying a Specific Image Region Using Convolutional Nets with an ROI Mask as Input\n\nEppel, Sagi. “Classifying a Specific Image Region Using Convolutional Nets with an ROI Mask as Input,” n.d., 8.\n\nIntro\n\nIn some cases, it is desirable to classify only a specific region of the image that corresponds to a certain object.\nHence, assuming that the region of the object in the image is known in advance and is given as a binary region of interest (ROI) mask, the goal is to classify the object in this region using a convolutional neural net.\nThis goal is achieved using a standard image classification net with the addition of a side branch, which converts the ROI mask into an attention map. This map is then combined with the image classification net\nfocus the attention on the object region while still extracting contextual cues from the background\nCOCO\nOpenSurfaces materials dataset\n\nNetwork\n\ncombining the attention map at the first layer of the net gave better results than combining it at higher layers of the net\nAn alternative approach is to generate an attention map, which can be used by the net to extract features from both objects and the background using the ROI mask as an additional input to the net\nAn attention map can easily be generated from the input ROI mask using a convolution layer\nThis attention map is then combined with one or more layers of the main branch, either by element-wise addition or multiplication\nThe combined layer is then used as an input for the next layer of the main branch\nIn order to allow element-wise addition or multiplication, the attention map must be the same size as the layer with which it is combined. To achieve this, the ROI mask was first resized to match the size of the layer with which it was merged, and a convolution layer was then applied with the same number of filters as the depth of the target layer.\nFor cases where the attention maps were combined with more than one layer , a separate attention map was generated using different convolution filters for each layer\n\nNet Initiation\n\nThe convolution layer of the side branch was initialized as follows: if the attention map was to be merged by element-wise addition, both the weights and the bias were initialized to zero; if the attention map was to be merged multiplication, the bias was set to one and the filter weights to zero\nThis weights initiation method promise that the initial effect of the attention branch on the classification branch is zero at the outset and increases gradually during training.\n\nDatasets\n\nThe nets were also trained using the OpenSurfaces material classification dataset1 0 ; in this case, the ROI was generated by taking a connected region of the image corresponding to a single material, and the output was the material type.\n\nResults\n\nIt can be seen that methods based on generating an attention map and combining it with the main branch net branch gave considerably better accuracy than hard attention methods based on blacking out the background region3\nThe difference in accuracy is particularly large for the classification of small segments where background information is more important in classification.\nMerging the attention map with the first layer of the net gave significantly better results than merging at higher layers\nThis probably due to the fact that higher layers of the net suffer from a loss of high-resolution information that is relevant in the classification of small objects.\nGenerating several attention maps and merging them with multiple layers of the net gave the same or worse results than generating a single attention map and merging it with the first layer\n\nImages\n\n\n"},"KB/Clear-Thinking":{"title":"Clear Thinking","links":[],"tags":["random"],"content":"Clear Thinking\n\nDeveloping good abstractions, notations, visualizations, and so forth, is improving the user interfaces for ideas.\nThis helps both with understanding ideas for the first time and with thinking clearly about them.\nConversely, if we can’t explain an idea well, that’s often a sign that we don’t understand it as well as we could.\n"},"KB/Clustering":{"title":"Clustering","links":["KB/KMeans","KB/SOMs"],"tags":["temp"],"content":"Clustering\n\nKMeans\nSOMs\n"},"KB/Clutter-In-Visualisation":{"title":"Clutter In Visualisation","links":[],"tags":["visualization"],"content":"Clutter In Visualisation\n\n\n\n\n"},"KB/Co-adaptation":{"title":"Co adaptation","links":[],"tags":["gradients"],"content":"Co Adaptation\n\nComputing the gradient is done with respect to the error, but also with respect to what all other units are doing (Srivastava et al., 2014). This means that certain neurons, through changes in their weights, may fix the mistakes of other neurons. These, Srivastava et al. (2014) argue, lead to complex co-adaptations that may not generalize to unseen data, resulting in overfitting.\n"},"KB/Co-Mixup":{"title":"Co-Mixup","links":[],"tags":["augmentation"],"content":"Co-Mixup\n\n@kimCoMixupSaliencyGuided2021\nsalient image mixing on a batch of input images to generate a batch of augmented images\nThis technique maximizes saliency in output images by penalizations to ensure local data smoothness and diverse image regions\n"},"KB/Co-training":{"title":"Co-training","links":[],"tags":["temp"],"content":"Co-training\n\nA semi-supervised learning approach particularly useful when all of the following conditions are true\nThe ratio of unlabeled examples to labeled examples in the dataset is high.\nThis is a classification problem (binary or multi-class).\nThe dataset contains two different sets of predictive features that are independent of each other and complementary.\nCo-training essentially amplifies independent signals into a stronger signal.\n"},"KB/Coarse-grained-assessment":{"title":"Coarse-grained assessment","links":["KB/IRT"],"tags":["usermodel"],"content":"Coarse-grained Assessment\n\nIf the required assessment is coarse grained, such as a single number reporting the student’s competence on the current unit, then it is usually computed from several measures such as:\nA measure of progress and coverage, such as the number of problems solved or the number of correct steps.\nA measure of the amount of help given, such as the number of hint sequences started and the proportion that ended with a bottom-out hint.\nSome measure of competence, such as the frequency of incorrect initial steps, the time required to enter a correct step, or the number of attempts at a step before it is entered correctly.\nPsychometrics is the field that studies how to do this for conventional tests (e.g., multiple choice tests)\nOne of their main tools is item-response theory IRT\n"},"KB/Cochlea":{"title":"Cochlea","links":[],"tags":["brain"],"content":"Cochlea\n\nThe part of the inner ear that transforms sound vibrations into neural impulses.\n"},"KB/Codex":{"title":"Codex","links":[],"tags":["architecture"],"content":"Codex\n\ntranslates text to code\ngeneral-purpose programming model, as it can be applied to basically any programming task\nProgramming can be broken down into two parts: breaking a problem down into simpler problems and mapping those problems into existing code (libraries, APIs, or functions) that already exist\nThe second part is the most time-barring part for programmers, and it is where Codex excels the most\nmodel is fine-tuned from GPT-3, which already contains strong natural language representations\n"},"KB/CogMod-Final-Paper":{"title":"CogMod Final Paper","links":["KB/The-Reward-Experiment","KB/Sequential-effects-within-a-short-foreperiod-context-Evidence-for-the-conditioning-account-of-temporal-preparation","KB/Traces-of-times-past-Representations-of-temporal-intervals-in-memory","KB/The-warning-stimulus-as-retrieval-cue-The-role-of-associative-memory-in-temporal-preparation","KB/Modeling-motivation-using-goal-competition-in-mental-fatigue-studies","The-neural-correlates-of-mental-fatigue-and-reward-processing---A-task-based-fMRI-study","KB/Implicitly-learning-when-to-be-ready---From-instances-to-categories","KB/On-the-Distinction-Between-Perceived-Duration-and-Event-Timing---Towards-a-Unified-Model-of-Time-Perception","Change-of-Variable-Foreperiod-Effects-within-an-Experiment---A-Bayesian-Modeling-Approach","KB/Revisiting-variable-foreperiod-effects-evaluating-the-repetition-priming-account"],"tags":["cognitivemodel"],"content":"CogMod Final Paper\n\nThe Reward Experiment\nAn ACT-R model that explains at least one of these effects (well)\nReport to justify design choices\n‘cognitive’ interpretation, that is justifiable &amp; plausible\n\nTo test\n\nlinear decrease in goal activation\n\nPapers\n\nSequential effects within a short foreperiod context Evidence for the conditioning account of temporal preparation : IMPORTANT\nTraces of times past Representations of temporal intervals in memory : Explain the fanning effect\nThe warning stimulus as retrieval cue The role of associative memory in temporal preparation : temporal preparation , sloping effects\nModeling motivation using goal competition in mental fatigue studies : performance in reward vs non reward , distraction, linear decrease in goal activation\nThe neural correlates of mental fatigue and reward processing - A task-based fMRI study : studies the physical brain effect of reward and fatigue\nImplicitly learning when to be ready - From instances to categories : fanning , maybe the graphs can also be explained by categorical association instead of just instance based\nOn the Distinction Between Perceived Duration and Event Timing - Towards a Unified Model of Time Perception : brain uses temporal expectations to bias perception in a way that stimuli are ‘regularized’\nChange of Variable Foreperiod Effects within an Experiment - A Bayesian Modeling Approach : sequential modulation, which is attributed to feature binding and retrieval by the BRAC framework, could have different underlying mechanisms depending on the task scenario.\nRevisiting variable foreperiod effects evaluating the repetition priming account : change in foreperiod\n"},"KB/Cognition-Hazard-Rates":{"title":"Cogntition Hazard Rates","links":["KB/Cognitive-fMTP","KB/Scaled-benefits","KB/Active-tracking","KB/Gaussian-Filter"],"tags":["cognitivemodel"],"content":"Cogntition Hazard Rates\n\n{Proven wrong} : Cognitive fMTP\nMathematical construct about probability\nContinuously tracking the odds the event appeads rn given it has not happened yet\nIdea : Use this “hazard rate” to decide when to prepare\nRT is proportional to hazard\nOptimally prepared if certain\n\nDistributions Used (PDF)\n\nConstant, exponential, flipped exponential\nHazard rate is this pdf by 1-F\nh(t) = \\frac{f(t)}{1-F(t)}\n\n\nHazard Rates\n\nHow does that translate to RT?\nProposed\n\nRT = c- h(t) : linear effect\nRT = c+ \\frac{1}{h(t)} : inverse relation\n\n\ndashed : \\frac{1}{hazard}\n\n\n\nVs ACT-R\n\nPrepare for ‘the right moment’\n\n‘degree of preparation’ given by moment-to-moment hz\n\n\n‘the right moment’ is estimated based on time (pulses) and memory (DM)\n\nNo ‘time’, no explicit memory?\n\n\nIf we are prepared→ benefit, else cost\n\nScaled benefits (useful for assignment)\nDoes not specify why/how; i.e., what preparation is\n\n\nNo active process during the interval\n\nActive tracking\n\n\nOnce we are prepared, it doesn’t ‘go away’\n\nA by-product of the Hazard rate\n\n\nNo memory model\n\nSuch mathematical models give no mechanism for how the pdf is stored in memory, retrieved, or used…\n\n\n\nProblems\n\nDoes not explain preparation\nHow do particpants ‘learn’ the distribution?\nDo participants truly track ‘conditional probabilities’ throughout the foreperiod\n\nExtending\n\nDoes not store PDFs in memory, which sucks\n\nDoes not keep track of time as well\n\n\nSubjective hazard/ anticipation function\n\nTemporal uncertainty\nBlur the pdf such that later points are less certain using a Gaussian Filter that gets wider for later points in time\nf&#039;(t) = \\frac{1}{\\theta t \\sqrt{2 \\pi}} \\int_{-\\infty}^{\\infty}f(\\tau)e^{-\\frac{(r-t)^{2}}{2 \\theta^{2}t^{2}}}d \\tau\nClimb to 1 after a while\nHazard is more even though probs are equal in classical. This equates them and makes them less blurred out\n\n\n\n\nImages\n\n\n"},"KB/Cognitive-Engagement":{"title":"Cognitive Engagement","links":[],"tags":["explainability"],"content":"Cognitive Engagement\n\nCognitive effort is a form of labor, and unsurprisingly, people tend to favor less demanding forms of cognition and other mental shortcuts [18, 51].\nUnfortunately, this human tendency can lead to unintended or dangerous outcomes because humans are susceptible to a wide variety of cognitive biases such as confirmation bias\nConfirmation bias [41]\nrefers to the interpreting of new evidence in ways that confirm one’s existing beliefs\nFor XAI, this manifests as practitioners only superficially examining explanations instead of digging deeply, leading to over-trust, misuse, and a lack of accurate understanding of the outputs [31]\nForcing users to cognitively engage through some small task before showing a system’s output yielded the highest performance in a comparative study [21]\nTrain conductors in Japan famously point and call out important information on their journeys—a cognitive forcing method which has reduced human errors by nearly 85% [45].\nRealistically, how much will users actually cognitively engage with the magnitude of generated outputs to ensure that they are correct and aligned with their intentions?\n"},"KB/Cognitive-Foreperiod":{"title":"Cognitive Foreperiod","links":["KB/Cognition-Hazard-Rates"],"tags":["cognitivemodel"],"content":"Cognitive Foreperiod\n\nTime before stimulus\nPrepare to act\n\nConstant FP\n\nNo uncertainty\nUp to ~150ms : RT decrease, then increase\nPeople prepare if warning - faster\nif longer intervals\n\navg response time increases\nTemporal estimates get noiser : Temporal Uncertainty\n\nnot really prepared\n\n\n\n\nFaster if less time to prepare\n\nVariable FP\n\nFaster if more time to prepare\nAsymptotic decrease : plateau\nTry your best based on exp to be prepared\n\nBut stay prepared if you already are\n\n\n“Strategic” : aim to be prepared as late\n\nWhy? Dunno. Maybe energy conservation\n\n\n\nDistribution Effects\n\nUniform\nExponential\n\nMany shorts\n\n\nAnti Exponential\n\nMany long\n\n\nPrep strategy altered based on which type of distribution\n\nSequential Effects\n\nPrep dependent on previous trials\n\nIf prev short, present longer : RTs are slow\n\n\nTraces of gradually forgetting previous trials\n\nin the shape of prep effects\n\n\nCannot just be accounted for by n-1 trials\n\nTransfer Effects\n\nStart with uniform - then something else - then uniform again\nLong lasting effects\nEven if participants were informed that things changed\nEven a week later\n\n\nMotivation Determined by\n\n\nTime\n\n\nMemory\n\n\nMotivation - still works on earlier prep?\n\n\nCognition Hazard Rates\n\n"},"KB/Cognitive-Multitasking":{"title":"Cognitive Multitasking","links":["KB/Threaded-Cognition","KB/Mental-Fatigue"],"tags":["cognitivemodel"],"content":"Cognitive Multitasking\n\nNo overlap between areas of brain in fMRI with similar tasks concurrently\nThreaded Cognition\nNatural way of cognition, need to be prepared for a new “task”\nIf unused brain resources, put it to use\nMental Fatigue\n"},"KB/Cognitive-Preparation":{"title":"Cognitive Preparation","links":["KB/Cognitive-Multitasking","KB/Cognitive-Foreperiod"],"tags":["cognitivemodel"],"content":"Cognitive Preparation\n\nHow brains use time to make decisions\nReflects implicit learning mechanisms\n\nrelies to optimize behavior\n\n\nMany models assume brain can time but not about how time is implemented\nPrepartion effects are present on different time scales\nWithin trial\nBlock of trials\nAcross blocks of trials\nCognitive Multitasking\n\nDesign\n\nTask has to be boring -.-\n\nTerms\n\nCognitive Foreperiod\n"},"KB/Cognitive-fMTP":{"title":"Cognitive fMTP","links":["KB/Declarative-Memory-Blending","KB/Scaled-benefits"],"tags":["cognitivemodel"],"content":"Cognitive fMTP\n\nPreparation effects manifest in the motor system\nPreparation is a balance between inhibition and activation\nA neural representation of time\nA (crude) model of the motor system\nHebbian associations + Forgetting &amp; Retrieval\n\nBetween Fore and Start\n\nTiming\n\nLayer of time cells\n\n\n\nPreparation\n\nThe motor system ‘stages’ a response, but holds it under inhibition..\nWhen the Go-stimulus (S2) arrives: activation\n\n\n\n\nLearning\n\nDifferent ‘Time cells’ and ‘motor inhibiton &amp; activation’ are active at the same time: this leads to hebbian learning\nFire together, wire together\nForms memory traces : aka chunk\n\n\nRetrieval\n\nAt the start, high degree of inhibition and low degree of activation\nIf prev trial is short, inhibition short and more activation\nRT = \\frac{I}{A}\nMore activation retrieved : faster\nRecency weighted\nDeclarative Memory Blending\nPreparation := Ratio of retrieved I vs. A\n\n\nVs ACT-R\n\nPrepare for ‘the right moment’\n\nMoment-to-moment balance of I and A\n\n\n‘the right moment’ is estimated based on time (pulses) and memory (DM)\n\nSimilar; but memory ‘chunks’ contain I- and A-traces not a single moment at which one should be prepared\n\n\nIf we are prepared→ benefit, else cost\n\nScaled benefits\nInhibiton increases RT; activation decreases RT\n\n\nNo active process during timing\n\nContinuously retrieving associated memories?\n\n\nOnce we are prepared, it doesn’t ‘go away’\n\nConsequence of ‘more A, less I retrieved’\n\n\n"},"KB/Cold-email-templates":{"title":"Cold email templates","links":[],"tags":["jobsearch"],"content":"Cold email templates\nEg1\n“Hi Betsy,\nMy name is Alison Parker. I’m a freelance writer covering science and psychology. I’ve written for several publications, including [X], [Y] and [Z publications]. I’ve been doing this work for about three years, and I’m really passionate about making people think critically about the world.\nMy former boss, [X boss’ name], who you used to work with at [X company], mentioned you when I told her I was looking for a full-time position. She said your magazine is hiring a senior science reporter. I was pleasantly surprised to hear your name, as I’ve been reading your work for years! I especially enjoyed the cover story you wrote for [X name of publication] in 2016 about the psychology of consumer spending.\nI submitted an application for the position online, but even if things don’t work out, I’d be so grateful for a 30 minute call to learn about your experience and what advice you have for young professionals in this field. I’m free any time after 3 p.m. (ET) on Wednesday, Thursday and Friday this week.\n(P.S. I saw your call on Twitter for volunteers to help at the food drive at [X location] this weekend. I organized a few of these in college and would love to help out, so count me in!)\nThanks so much for your time,\nAlison”"},"KB/Collaborative-Recommender":{"title":"Collaborative Recommender","links":[],"tags":["usermodel"],"content":"Collaborative Recommender\n\nClusters users according to behavior\nMatch with other users\neg : netflix\n"},"KB/Collaborative-Topic-Regression":{"title":"Collaborative Topic Regression","links":["KB/Filtering","KB/Sparsity","KB/Bayesian"],"tags":["architecture"],"content":"Collaborative Topic Regression\n\nCollaborative Deep Learning for Recommender Systems\nCollaborative Filtering (CF) is a successful approach commonly used by many recommender systems\nConventional CF-based methods use the ratings given to items by users as the sole source of information for learning to make recommendation\nHowever, the ratings are often very sparse in many applications, causing CF-based methods to degrade significantly in their recommendation performance\nTo address this Sparsity problem, auxiliary information such as item content information may be utilized\nCollaborative topic regression (CTR) is an appealing recent method taking this approach which tightly couples the two components that learn from two different sources of information\nNevertheless, the latent representation learned by CTR may not be very effective when the auxiliary information is very sparse.\ngeneralizing recent advances in deep learning from i.i.d input to non-i.i.d (CF-based) input and propose a hierarchical Bayesian model called collaborative deep learning (CDL), which jointly performs deep representation learning for the content information and collaborative Filtering for the ratings (feedback) matrix.\n"},"KB/Collective-Ethical-Decision-Frameworks":{"title":"Collective Ethical Decision Frameworks","links":[],"tags":["ethics"],"content":"Collective Ethical Decision Frameworks\n\nadvocates the need of primary rules governing social norms and allowing the creation, modification and suppression of the primary rules with secondary rules as situations evolve.\n"},"KB/Collective-Interpretation":{"title":"Collective Interpretation","links":[],"tags":["language"],"content":"Collective Interpretation\n\nNo scopal relation\nThree aliens are holding two flags.\nBoth Np’s are interpreted individually and connected to each other\n"},"KB/Collectivity,-Distributivity,-and-the-Interpretation-of-Plural-Numerical-Expressions-in-Child-and-Adult-Language":{"title":"Collectivity, Distributivity, and the Interpretation of Plural Numerical Expressions in Child and Adult Language","links":[],"tags":["language"],"content":"Collectivity, Distributivity, and the Interpretation of Plural Numerical Expressions in Child and Adult Language\n\nKristen Syrett, Ph.D. and Rutgers, The State University of New Jersey, Linguistics, New Brunswick, United States\nJulien Musolino : Rutgers, The State University of New Jersey, Psychology, Piscataway, United States\n\nIntro\n\nSentences containing plural numerical expressions (e.g., two boys) can give rise to two interpretations (collective and distributive), arising from the fact that their representation admits of a part-whole structure\ndesigned to explore children’s understanding of this distinction and its implications for the acquisition of linguistic expressions with number words.\npreschoolers access both interpretations, indicating that they have the requisite linguistic and conceptual machinery to generate the corresponding representations.\nhift their interpretation in response to structural and lexical manipulations.\nunlike adults, they are drawn to the distributive interpretation, and are not yet aware of the lexical semantics of each and together, which should favor one or another interpretation.\nHere, we take a different approach, and use numerically quantified expressions to study how children acquire a fundamental semantic property shared by a range of plurality-denoting expressions\n\nFindings\n\nOur findings demonstrate that the ability to generate collective and distributive interpretations of sentences such as (1) is part of the semantic repertoire of children as young as three (Experiment 1). However, we also uncover intriguing differences in the preferences preschoolers and adults have for resolving the collective/distributive ambiguity: whereas adults strongly prefer the collective interpretation, preschoolers prefer the distributive one (Experiment 2).\nFollowing analyses by Link (1983, 1987) and others more recently, we will assume that these sentences are truly ambiguous, and not merely underspecified, and that the source of the ambiguity in our target sentences is the VP predicate. Here, we adopt a default semantics approach in order to illustrate how two different interpretations may be generated.\nWhen the predicate in our example sentence is applied to the individuals, the derived reading is the distributive one\nWhen the predicate is applied to the group, however, a collective reading is derived, and the extension is an atomic joint ‘car pushing’ event in which the boys collectively push the car.\nBeginning with the latter, Musolino (2009) was primarily concerned with the range of readings arising from the interaction of two numerically quantified expressions in so-called relational plural sentences such as (3).\n\nJudgment Task with Ambiguous Sentences\n\nThe results demonstrate that both children and adults were able to access both the collective and distributive interpretations of the target sentences. While there was a significant main effect of age (p= .02), there was no main effect of context (p=.75) and no interaction between age group and context (p=.26).\nThis difference stems from the fact that while four-year-olds were near ceiling in their acceptance of the sentences in the distributive context, adults’ acceptance rates were slightly suppressed.\n\nAmbiguous Sentences That Yield Either Interpretation.\n\nAs the results demonstrate, adults overwhelmingly preferred the collective version of the event\nIn this experiment, we found that while adults robustly prefer the collective context as a match for the ambiguous target sentences, children display a slight preference in the opposite direction, leaning towards preference for the distributive context.\nstructural manipulation of passivization will lead participants to prefer the collective context.\nAs predicted, adults consistently accepted the passive test sentences in the collective context, but largely rejected them in the distributive context. Most children also followed this pattern, although the difference between acceptances in the two contexts was not as striking for children as it was for the adults.\nwhether participants can recruit lexical semantic information provided by individual words to disambiguate the target sentence and assign either a collective or distributive interpretation, depending on the lexical item.\nAs predicted, adults were guided by the presence of the additional lexical item in their interpretation of these sentences, accepting the test sentences with each in the distributive context, but rejecting them in the collective context\nIn place of the ambiguous sentences, children heard sentences with a post-verbal together (\nInterestingly, despite children’s acceptance of the together sentences in both contexts of the judgment task of Experiment 4, children appeared to be aware of the collectivizing force of together in the current preference task.\n"},"KB/Color-Compositing":{"title":"Color Compositing","links":["KB/Marching-Cubes"],"tags":["visualization"],"content":"Color Compositing\n\n\nC_{i}= c_{i}+ (1-o_{i})C_{i-1}\nwhere\nc_{i}= o_{i}c_{i}&#039;\n\nFirst is same as Marching Cubes\n\n\n\n\nf(\\sigma)&amp; \\exists t \\in [0,T], s(t) = \\sigma \\\nI_{o}&amp;otherwise\n\\end{cases*}$$\n\nHigher pixel accurate quality\n"},"KB/Color-Space-Transformations":{"title":"Color Space Transformations","links":["KB/Geometric-Transformations"],"tags":["augmentation"],"content":"Color Space Transformations\n\nImage data is encoded into 3 stacked matrices, each of size height×width. These matrices represent pixel values for an individual RGB color value\nLighting biases are amongst the most frequently occurring challenges to image recognition problems\nA quick fix to overly bright or dark images is to loop through the images and decrease or increase the pixel values by a constant value.\nAnother quick color space manipulation is to splice out individual RGB color matrices.\nAnother transformation consists of restricting pixel values to a certain min or max value.\nSimilar to Geometric Transformations, a disadvantage of color space transformations is increased memory, transformation costs, and training time.\nAdditionally, color transformations may discard important color information and thus are not always a label-preserving transformation.\nFor example, when decreasing the pixel values of an image to simulate a darker environment, it may become impossible to see the objects in the image.\nDigital image data is usually encoded as a tensor of the dimension (height × width × color channels)\nPerforming augmentations in the color channels space is another strategy that is very practical to implement.\nVery simple color augmentations include isolating a single color channel such as R, G, or B.\nAn image can be quickly converted into its representation in one color channel by isolating that matrix and adding 2 zero matrices from the other color channels. Additionally, the RGB values can be easily manipulated with simple matrix operations to increase or decrease the brightness of the image.\nMore advanced color augmentations come from deriving a color histogram describing the image\n"},"KB/Color-Spaces":{"title":"Color Spaces","links":["KB/Divide-Oriented","KB/Intuitive-Color-spaces"],"tags":["visualization"],"content":"Color Spaces\n\nDivide Oriented\nIntuitive Color spaces\n"},"KB/ColorMap":{"title":"ColorMap","links":["KB/Color-Spaces"],"tags":["visualization"],"content":"ColorMap\n\nColor Spaces\n\n"},"KB/CommonCrawl":{"title":"CommonCrawl","links":[],"tags":["dataset"],"content":"CommonCrawl"},"KB/Comparing-Data-Augmentation-Strategies-for-Deep-Image-Classification":{"title":"Comparing Data Augmentation Strategies for Deep Image Classification","links":["KB/CIFAR","KB/Shear","KB/Dropout"],"tags":["explainability"],"content":"Comparing Data Augmentation Strategies for Deep Image Classification\n\nSarah O’Gara and Kevin McGuinness\n\nSummary\n\nInject augmentation around 30 epochs\nUse learning rate decay\n[Random Erasing](Random Erasing.md) is useful\nUse [Adam] + [SGD](Adam] + [SGD.md)\n\nAbstract\n\nMore complex augmentation methods have recently been developed, but it is still unclear which techniques are most effective, and at what stage of the learning process they should be introduced.\nThe most accurate results in all experiments are achieved using random erasing due to its ability to simulate occlusion\nreducing the number of training examples significantly increases the importance of augmentation\nimprovements in generalization from augmentation do not appear to be only as a result of augmentation preventing overfitting\nlearning curriculum that injects augmentation after the initial learning phase has passed is more effective than the standard practice of using augmentation throughout, and that injection too late also reduces accuracy\nWe find that careful augmentation can improve accuracy by +2.83% to 95.85% using a ResNet model on CIFAR-10 with more dramatic improvements seen when there are fewer training examples\n\nModel and Optimizer\n\nResNet\n[He et al., 2015] presents an adaption of the model (ResNet-56) for use with 32×32 images that obtained an error rate of 6.97% on CIFAR-10, which we adopt in our experiments\nSGD with Nestrov momentum\nAlthough there are more sophisticated first order optimizers (e.g. Adam [Kingma and Ba, 2015]) that consistently improve the loss faster in the initial epochs, SGD has been observed to reach a local minima with lower overall loss and better generalization properties [Ruder, 2016]\n\nDatasets\nCIFAR-10\n\nrandomly sample the dataset to create a 200 samples per class and 1,000 samples per class dataset, reducing the training examples available to 4% and 20% of the original dataset\nThe effects of overfitting and model generalization as noted in [Hussain et al., 2018, Shijie et al., 2017] are more pronounced with data scarcity\n[Skew Tilt](Skew Tilt.md)\nShear\n[Random Distortion](Random Distortion.md)\n[Gaussian Distortion](Gaussian Distortion.md)\nWe introduce augmentation on epochs 30, 60, and 90 of the baseline model and continue training until epoch 163 to discover the optimal time to introduce augmentation. Epochs 30, 60, and 90 represent three distinct stages in the training process: initial loss rate stabilising, loss rate stagnate before learning rate decrease, and loss rate stagnate after learning rate decrease.\n\nExperiments\n\nThe range of learning rates that provide a stable convergence reduces as batch size increases\nIn the most extreme case, we reduce the training set to 4% of the original dataset, meaning a batch size of 128 would likely degrade performance\nLarge batches tend to converge to sharp minimizers leading to poor generalization due to the numerous large eigenvalues in the Hessian on convergence\nSmall batches, on the other hand, tend to converge to flat minimizers, which have smaller Hessian eigenvalues\nThey generate more noise in gradient calculations, decreasing the chance of the gradient dropping into a sharp local minima\nBased on these observations, we train the small and medium datasets using three learning rate strategies: 1) the original strategy from [He et al., 2015], 2) using a batch size of 128 with no learning rate schedule, and 3) using a batch size of 8 with original learning rate schedule.\n\nResults &amp; Discussion\nSingle Augmentations\n\nRandom erasing shows the best improvement in accuracy of +1.5%\nBoth distortion augmentations obtain worse or similar results to the baseline\nThe complexity of the augmentation effects the overall training time. Traditional, more simplistic augmentations require little processing time, leading to increases in training time of ∼ 3.5 hours. [Gaussian distortion](Gaussian distortion.md) sees the most significant increase in training time of 665%\nWe apply each augmentation separately, leading to the dataset increasing from 50k training images to 250k. This leads to the most accurate result seen throughout all experiments of 95.85%\nOur method of applying several single augmentations produces better generalization properties\n\nVarying Augmentation Injection Epoch\n\nepoch 30 is the optimal time to introduce augmentation\ny injecting augmentation on the 30th epoch, the model combats the effects of overfitting better with increases in accuracy from +0.05% up to +0.76%\nEpoch 30 is the point in the training process when the reduction in loss rate begins to decrease drastically, i.e. the model falls into a local minima point\nThe slight improvements in accuracy over the baseline result for introduction at epoch 90 support this conclusion\nThe model has already overfit the training data and can no longer benefit from the augmentation’s generalization properties.\nEpoch 60 presents a more interesting point in the training process. The form of augmentation appears to dictate whether the model will have better generalization properties than training with augmentation from scratch but will always be worse than injection at epoch 30\n\nVarying Sample Size\n\nFor the small dataset, by decreasing the batch size from 128 to 8, the validation accuracy is shown to improve by +31.45% using random erasing (74.46%) when compared to the baseline (43.01%)\naugmentation is most effective in training when data is scarce\noverfitting, as measured by high accuracy on the training set, in many of the augmentation results is more severe than for the baseline\nhis would contradict current assumptions that augmentation improves generalization by preventing overfitting in the case of all NNs\nIn many of these cases where augmentation has proven to prevent overfitting the sample size for each class is large\ngeneralization of the model is better in the presence of augmentation\nWith smaller datasets using augmentation increases the models ability to learn certain features present in the training set as augmentation can only alter the data already available, i.e. the model will see similar images twice as much so is more likely to overfit.\nFor the medium dataset, the best accuracy is achieved by random erasing trained with a batch size of 8 at 87.45%, which is an improvement of +6.3% over the baseline.\nThe importance of the learning rate adjustment schedule is apparent with the accuracy decreasing for each model when not applied\nAugmentation does reduce overfitting with the most significant decrease occurring for the small batch size\nAt this scale, augmentation has similar effects on accuracy as seen in the full dataset\nWhen the model has large volumes of training data available, augmentation only slightly increases the generalization capabilities of the network as a large amount of variance already exists\n\nConclusion\n\nThe initial augmentation gives rise to the most significant increase in training time with any additional augmentations adding little overhead\nprocessing time required to apply said augmentation to the dataset, which must be considered when choosing a form of augmentation to apply\ncombining multiple single augmentations with the original dataset is the most effective augmentation strategy with an increase in accuracy of +2.36% to 95.85%\nRandom distortion and [Gaussian distortion](Gaussian distortion.md) are the worst forms of augmentation tested leading to changes in accuracy of -0.15% and +0.05%, respectively\nThis is due to the augmented images not representing the original class and highlights the importance of the choice of augmentation\nThe most effective form of single augmentation is found to be random erasing with an increase in accuracy of +1.5%. This is due to its ability to combat the effects of occlusion, and is similar to preventing co-adaption through the use of Dropout.\nAn interesting avenue to explore is the generalization and overfitting properties of augmentation for data scarcity\nValidation accuracy is seen to improve with augmentation, with the most significant improvement of +31.45% for random erasing, indicating better generalization capabilities.\nHowever, the model also appears to overfit the training data more\nExploring the interaction of augmentation with more advanced optimizers such as the Adam optimizer, could lead to further improvements in accuracy and training times\ngeneralization gap between SGD and Adam can be reduced by switching from Adam to SGD during the training process\nDuring the switching process the learning rate for SGD is calculated as noted in [Keskar and Socher, 2017] and must be switched at the optimal time to ensure better generalization properties.\nBuilding on this approach, the optimizer switching approach could be combined with data augmentation potentially yielding improvements in accuracy.\nInjecting augmentation at epoch 30 yielded the best improvements in accuracy for single augmentations, indicating a learning curriculum is most effective for augmentation\nLate injection of augmentation improves the generalization capabilities of the network similar to the optimizer switching method of [Keskar and Socher, 2017].\n\nImages\n\n\n\n\n"},"KB/Complete-AI-Pipeline":{"title":"Complete AI Pipeline","links":["KB/DICOM","KB/HL7","KB/FHIR"],"tags":["deeplearning"],"content":"Complete AI Pipeline\nPlanning\n\nData Availability\nApplicability\nLegal Constraints\nRobustness\nScalability\nExplainability\nAvailability of Resources\n\nData Collection\nData Standards\n\nHealthcare\n\nDICOM\nHL7\nFHIR\n\n\n\nData Formats\n\nHealthcare\n\nNext generation sequencing\n\nRaw sequencing data\n\nFASTQ\n\nBioPython\n\n\n\n\nAligned Reads\n\nSAM/BAM\n\npysam\n\n\n\n\nVariant Calls\n\nVCF\n\npyVCF\n\n\n\n\n\n\nMass spectrometry\n\nmzML/mzXML\n\npyteomics\n\n\nMGF\n\npyOpenMS\n\n\n\n\nMRI\n\nDICOM\n\n\nCT\n\nDICOM\n\n\nUltrasound\n\nDICOM\nSCU\nACR-NEMA\n\n\nFunctional (pre)clinical studies\nE-health and m-health technology\nWearable Device monitoring\nUnstructured vocal information\nUnstructured textual information\n\n\n\nData Processing and Transformation\nAWS Services\n\nGlue\n\nFully managed ETL (Extract, Transform, Load) service for preparing and loading data.\n\n\n\nData Annotation\nData Integration\nData Storage\nAWS Services\n\n\nS3\n\nObject storage service, suitable for storing and retrieving any amount of data at any time.\n\n\n\nDynamo DB\n\nModern\nHigh activity\nHigh Velocity data (sensors and stuff)\nStructured/unstructured\nmulti data, multi cloud\nNoSQL database service designed for high-performance applications that require seamless scalability.\n\n\n\nRDS\n\nTables\nlow velocity/activity\nHarder to modify\nRelational Database Service, supports multiple database engines, managing backups, and software patching.\n\n\n\nHealthImaging\n\n\nHealthLake\n\nA HIPAA-eligible service for storing, transforming, and analyzing healthcare data.\n\n\n\nLake Formation\n\nSimplifies the process of setting up, securing, and managing a data lake.\n\n\n\nData Analysis and Querying\nAWS Services\n\nRedshift\n\nFully managed data warehouse service, designed for high-performance analysis using SQL queries.\n\n\n\nStreaming Data Processing\nAWS Services\n\nKinesis\n\nStreaming data service that enables real-time processing of large data streams.\n\n\n\nModel Development\nModel Architectures\nModel Metrics\nCI/CD\nTracking Experiments, Metadata, Features, Code Changes\n\nMLFlow\n\nState of the Models\n\nStaging\nProduction\nArchived\n\nModel Evaluation\nMachine Learning\nAWS Services\n\nSageMaker\n\nMachine learning service that helps build, train, and deploy machine learning models at scale.\n\n\n\nModel Deployment\nReal Time Prediction\nMonitoring and Maintenance\nInterpretability and Explainability\nRegulatory Compliance\n\nHIPAA\n\nCollaboration"},"KB/Complex-Geometry":{"title":"Challenge of Complex Geometry","links":["KB/Manifold"],"tags":["visualization"],"content":"Challenge of Complex Geometry\n\nManifold\n"},"KB/Compliant-Robot":{"title":"Compliant Robot","links":[],"tags":["robotics"],"content":"Compliant Robot\n\nA robot that performs tasks, with respect to external forces, by modifying its motions in a manner that minimizes those forces. The indicated or allowed motion is accomplished through lateral (horizontal), axial (vertical) or rotational compliance.\n"},"KB/Composing-shallow-neural-networks-to-get-deep-networks":{"title":"Composing shallow neural networks to get deep","links":["KB/Relu","KB/Activation-Functions"],"tags":["deeplearning"],"content":"Composing shallow neural networks to get deep networks\n \n\nConsider two networks with 3 hidden units. Input x, x&#039; , output y, y&#039;\n\n\n\n\n\n\nIf we use a Relu, this also is a family of piecewise linear functions.\n\nthe number of linear regions is potentially greater than for a shallow network with six hidden units\n\n\nThis maps three ranges of x to same range of y \\in [-1,1] → 9 linear regions\n\nDeep neural networks\n\n(Input → Shallow network → second network) is a special case of neural network\nOutput of the first network (f1) y is a linear combination of the activations at the hidden units. And the first operations of the second network (f2) are linear in the output of the first network. Hence f2(f1) = linear function\n\n\n\\psi_{10}= \\theta&#039;_{10}+ \\theta&#039;_{11}\\phi_{0},\\psi_{11}= \\theta&#039;_{11}\\phi_{1}, \\psi_{12}=\\theta&#039;_{11}\\phi_{2}...\nwhich is a network with 2 hidden layers.\nSo a network with two layers can represent a family of functions formed by composing those networks.\n\n\n\n\nFolding space\n\nOne way to think about composing networks is by thinking of it as folding space and then applying an Activation Functions.\n\n\n\n\n\n"},"KB/Comprehensibility":{"title":"Comprehensibility","links":[],"tags":["explainability"],"content":"Comprehensibility\n\nability of a learning algorithm to represent its learned knowledge in a human understandable fashion\nAn understandable explanation must be created by a machine in a given time (e.g., one hour or one day) and can be comprehended by a user, who need not to be an expert, but has an educational background\nThe user keeps asking a finite number of questions of the machine until he/she can no longer ask why or how because he/she has a satisfactory answer; we say he/she has comprehended.\nThis is the relationship between cause and effect; it is not a synonym for causability\n"},"KB/Computational-Graph":{"title":"Computational Graph","links":["Tag-Pages/loss"],"tags":["temp"],"content":"Computational Graph\n\n\npatterns in backward flow\n\nadd gate: gradient distributor\nmax gate: gradient router\nmul gate: gradient switcher\nbranches: sum up gradients\n\n\npros\n\nintuitive interpretation of gradient\neasily define new nodes using forward/backward pattern (i.e. only these two functions must be implemented)\nany complex learning architecture can be composed of atomic nodes (node composition or factorization)\nno need to compute manually complex gradients\nloss function can be seen as extra nodes in the end of the graph\n\n\n"},"KB/Conceptual-Parsing":{"title":"Conceptual Parsing","links":["KB/Syntactic-Analysis","KB/Semantic-Analysis"],"tags":["language"],"content":"Conceptual Parsing\n\nSyntactic Analysis and Semantic Analysis knowledge are combined into a single interpretation system that is driven by the semantic knowledge\n"},"KB/Concurrency":{"title":"Concurrency","links":["KB/Parallel-Granularity"],"tags":["parallelcomputing"],"content":"Concurrency\n\nThe number of tasks that can be executed in parallel is the degree of concurrency of a decomposition.\nalways equal to the number of leaves in the tree\nBoth the maximum and the average degrees of concurrency usually increase as the Parallel Granularity of tasks becomes smaller (finer)\nCriticalPath = \\frac{\\text{Total amount of work}}{\\text{Critical path length}}\n\n"},"KB/Concussion":{"title":"Concussion","links":[],"tags":["brain"],"content":"Concussion\n\nA type of mild traumatic brain injury resulting from a blow or hit to the head that causes the brain to move rapidly back and forth inside the skull.\n"},"KB/Conditional-GAN":{"title":"Conditional GAN","links":[],"tags":["architecture"],"content":"Conditional GAN\n\nImage2Image, Face Aging, Text to Image\nGenerate images with certain extra conditions or attributes\nThe Generator and Discriminator both receive some additional conditioning input information. This could be the class of the current image or some other property.\nadd an additional input layer with values of one-hot-encoded image labels\nAdding a vector of features controls the output and guide Generator figure out what to do.\nSuch a vector of features should derive from a image which encode the class(like an image of a woman or a man if we are trying to create faces of imaginary actors) or a set of specific characteristics we expect from the image (in case of imaginary actors, it could be the type of hair, eyes or complexion).\nWhereas conditional generation uses labels during training, controllable generation focuses on controlling the features that you want in the output examples.\nWe can incorporate the information into the images that will be learned and also into the Z input, which is not completely random anymore.\nThis can be done by adjusting the input noise vector z that is fed into the generator after it has been trained.\nWe can use the same DCGANs and imposed a condition on both Generator’s and Discriminator’s inputs. The condition should be in the form of a one-hot vector version of the digit. This is associated with the image to Generator or Discriminator as real or fake.\nTypically this is done with a one-hot vector, meaning there are zeros in every position except for the position of the class we want to generate\n\nArchitecture\n\n[GAN Z Space](GAN Z Space.md)\n\nThe Discriminator’s network\n\nDiscriminator’s evaluation is done not only on the similarity between fake data and original data but also on the correspondence of the fake data image to its input label (or features)\nSame as [DCGAN] except [one hot](DCGAN] except [one hot.md) vector for conditioning\n\nThe Generator’s network\n\nTo create an image that looks as “real” as possible to fool the Discriminator.\nSame as [DCGAN] except [one hot](DCGAN] except [one hot.md) vector.\n\n\nLoss functions\n\nWe need to calculate two losses for the Discriminator. The sum of the “fake” image and “real” image loss is the overall Discriminator loss. So the loss function of the Discriminator is aiming at minimizing the error of predicting real images coming from the dataset and fake images coming from the Generator given their one-hot labels.\n\nGen\n\nThe loss function of the Generator minimizes the correct prediction of the Discriminator on fake images conditioned on the specified one-hot labels.\n\\mathcal{L}^{(G)}(\\theta^{(G)}, \\theta^{(D)}) = - \\mathbb{E}_{z} log \\mathcal{D} (\\mathcal{G} (z|y’))\n\nDisc\n\nhas to correctly label real images which are coming from training data set as real.\nhas to correctly label generated images which are coming from Generator as fake.\n \\mathcal{L}^{(D)}(\\theta^{(G)}, \\theta^{(D)})= - \\mathbb{E}_{x \\sim p_{data}}log \\mathcal{D}(x|y) - \\mathbb{E}_{z} log (1- \\mathcal{D}(\\mathcal{G}(z|y&#039;)))\n\nTraining\n\nThe Discriminator is trained using real and fake data and generated\nAfter the Discriminator has been trained, both models are trained together.\nFirst, the Generator creates some new examples.\nThe Discriminator’s weights are frozen, but its gradients are used in the Generator model so that the Generator can update its weights.\n\nTraining Flow\n\nFor the Disc \nFor the Gen \n\nChallenges with Conditional generation\n\nNot strictly unsupervised. Needs labels\nWith a conditional GAN, you get a random example from the class you specify\nWith conditional generation, you have to train the GAN with labeled datasets.\n[Feature Correlationa](Feature Correlationa.md)\n[Z-Space Entanglement](Z-Space Entanglement.md)\n[Classifier Gradients](Classifier Gradients.md)\n\nDCGAN vs CGAN\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDCGANCGANOutput features are not controllableOutput features can be controlledUnsupervisedSemi-SupervisedDiscriminator does not receive labelsDiscriminator requires labelsDiscriminator evaluates similarity between input and target imagesDiscriminator considers input and target images and their respective labels"},"KB/Conditional-Independence":{"title":"Conditional Independence","links":[],"tags":["temp"],"content":"Conditional Independence\n\nA Naive Bayes classifier assumes that the attribute values are independent of each other given the class. Normally distributed: many statistical methods assume that data is normally distributed.\n"},"KB/Conductance":{"title":"Conductance","links":[],"tags":["explainabilitye-**02:35**explainability"],"content":"Conductance\n\n@dhamdhereHowImportantNeuron2018\nKedar Dhamdhere, Mukund Sundararajan, Qiqi Yan\n\nSummary\n\nThis paper introduces the concept of conductance as a way to understand the importance of hidden units in deep networks. Conductance is defined as the flow of [Integrated Gradients](Integrated Gradients.md)’ attribution via a hidden unit, and is used to understand the importance of a hidden unit to the prediction for a specific input or over a set of inputs. The effectiveness of conductance is evaluated in multiple ways, including theoretical properties, ablation studies, and a feature selection task using the Inception network over ImageNet data and a sentiment analysis network over reviews. The properties of conductance include completeness, linearity and insensitivity to variations in inputs or hidden unit values. The paper also discusses the issue of saturation in neural networks, where the gradient of the output with respect to the input can be near-zero, and how conductance addresses this issue. The authors also compare conductance with other methods of understanding hidden unit importance and find it to be more intuitive and accurate.\n\nAbstract\n\nWe introduce the notion of conductance to extend the notion of attribution to the understanding the importance of hidden units\nconductance of a hidden unit of a deep network is the flow of attribution via this hidden unit\nconductance to understand the importance of a hidden unit to the prediction for a specific input, or over a set of inputs\nWe evaluate the effectiveness of conductance in multiple ways, including theoretical properties, ablation studies, and a feature selection task\nInception network over ImageNet data, and a sentiment analysis network over reviews\nInformally, the conductance of a hidden unit of a deep network is the flow of [Integrated Gradients](Integrated Gradients.md)’ attribution via this hidden unit\nThe key idea behind conductance is to decompose the computation of [Integrated Gradients](Integrated Gradients.md) via the chain rule\n\nConductance\n\n[Integrated Gradients](Integrated Gradients.md) produces attributions for base features\nThere is a natural way to ‘lift’ these attributions to a neuron in a hidden layer. Consider a specific neuron y in a hidden layer of a network\n\n\n\nF:R^{n} \\rightarrow [0,1]\nrepresents a deep network.\n\nx \\in R^{n} is input, x&#039; \\in R^{n} is baseline input\n[Integrated Gradients](Integrated Gradients.md) is path integral of gradient along straightline path from baseline x&#039; to input x. The function F varies from a near zero value for the informationless baseline to its final value. The gradients of F with respect to the image pixels explain each step of the variation in the value of F\nThe integration (sum) over the gradients cumulates these micro explanations and accounts for the net difference between the baseline prediction score (near zero) and the prediction value at the input x.\n\n\n\n\nIG_{i}(x) ::== (x_{i}- x_{i}’) \\int_{\\alpha=0}^{1} \\frac{\\partial F(x’ + \\alpha(x-x’))}{\\partial x_{i}}d \\alpha\n$$ where \\frac{\\partial F(x)}{\\partial x_{i}} is grad of F along i^th dimension at x\n\nConductance of neuron y for attribution to input variable i is $$\nCond_{i}^{y}(x) ::== (x_{i}- x_{i}’) \\int_{\\alpha=0}^{1} \\frac{\\partial F(x’ + \\alpha(x-x’))}{\\partial y} \\cdot \\frac{\\partial y}{\\partial x_{i}} d \\alpha\n\n\n## Evaluation of Conductance\n- Activation: The value of the hidden unit is the feature importance score.\n- $Gradient\\times Activation$ : $$\ny \\times \\frac{\\partial F(x&#039; + \\alpha \\times (x-x&#039;))}{\\partial y} d \\alpha\n\nInternal Influence : $$\nInt Inf ^{y}(x) ::= \\int^{1}_{\\alpha=0} \\frac{\\partial F(x’ + \\alpha(x-x’))}{\\partial y} d \\alpha\n\n- The premise is that hidden units that are important across a set of inputs from a class should be predictive of this input class.\n\n## Properties of Conductance\n\n### Completeness\n- conductances for any single hidden layer add up to the difference between the predictions $F(x) - F(x&#039;)$\n- conductances thus satisfy the [Layerwise Conservation Principle](Layerwise Conservation Principle.md)\n\n### Linearity\n- So do internal influence and gradient*activations\n- Suppose that we linearly compose hidden neurons f1 and f2 to form the final network that models the function $a \\times f_{1} + b \\times f_{2}$. Then, the conductances of the two hidden neurons will be $a \\times (f_{1}(x) f_{1}(x_{0}))$ and $b \\times (f_{2}(x) f_{2}(x&#039;))$ respectively.\n- This is a sanity-check because if the action of a network is mostly linear from a hidden layer, the conductances will match what is intuitively the obvious solution.\n\n### Insensitive\n- If varying the values of a hidden unit does not change the network&#039;s prediction, it has zero conductance\n- If varying the inputs does not change value of the hidden unit, the hidden unit has zero conductance\n- Based on $\\frac{\\partial F}{\\partial y_{j}}$ and $\\frac{\\partial y_{j}}{\\partial x_{i}}$ being 0\n\n## Saturation of Neural Networks\n- Basically, for a network, or a sub-network, even when the output crucially depends on some input, the gradient of the output w.r.t. the input can be near-zero.\n- As an artificial example, suppose the network first transforms the input x linearly to y = 2x, and then transforms it to z = max(y, 1). Suppose the input is x = 1 (where z is saturated at value 1), with 0 being the baseline. Then for the hidden unit of y, gradient of z w.r.t. y is 0. Gradient\\*activation would be 0 for y, which does not reflect the intuitive importance of y. Like in [Integrated Gradients](Integrated Gradients.md), in computing conductance, we consider all extrapolated inputs for x between 0 and 1, and look at the gradients of output w.r.t. y at these points. This takes the non-saturated region into account, and ends up attributing 1 to y, as desired.\n- wrong Polarity/Sensitivity\n\n## Methods\n- we compare against can yield scores that have signs and magnitudes that are intuitively incorrect\n- This is intuitively because each misses terms/paths that our method considers.\n- Activation values for a ReLU based network are always positive. However, ReLU nodes can have positive or negative influence on the output depending on the upstream weights. Here, Activation does not distinguish the sign of the influence, whereas condutance can.\n- Gradient\\*Activation as a linear projection can overshoot\n- Certain hidden units that actually have near zero influence can be assigned high\n- importance scores.\n- For example, suppose that the network is the composition of two functions f (x) = x and a weighted ReLU g(y) = max(y 1, 0). Again, the network computes the composition g(f(x)). Suppose that the baseline is x = 0 and the input is x = 1 . The output of the network is 0. But the feature importance of the unit f is deemed to be 1 (activation) times 1 (gradient), which is 1 . Notice that this is the only unit in its layer, so the fact that its influence does not agree in magnitude with the output is undesirable. In contrast, conductance assigns all hidden units a score of zero. The example can be extended to show that the feature importance score can disagree in sign with the actual direction of influence.\n- Suppose that the network is the composition two functions f(x) = x and g(y) = y, i.e., the network computes the composition g(f(x)). Suppose that the baseline is x = 0 and the input is x = 1. The output of the network is 1. But the internal influence of the unit represented by the function g is +1 (regardless of the choice of the input or the path). Notice that this is the only unit in\n- its layer, so the fact that its influence does not agree in sign with the output is highly undesirable. In contrast, conductance assigns an influence score of 1.\n\n## Applying Conductance to an Object Recognition Model\n- We use conductance as a measure to identify influential filters in hidden layers in the Inception network.\n- Given an input image, we identify the top predicted label\n- For the pre-softmax score for this label, we compute the conductance for each of the filters in each of the hidden layers\n- The [visualization](visualization.md) is done by aggregating the conductance along the color channel and scaling the pixels in the actual image by the conductance values.\n\n## Ablation Study\n- Next we studied how many filters we need to ablate in the network in order for the network to change its prediction. We found that, it is sufficient to ablate 3.7 on an average for the network to change its prediction for an image. Only 3 out of 100 images needed more than 10 filter ablations to change the predicted label. The maximum was 16. This provides further evidence that using conductance we can identify filters that are important for the prediction.\n- We compare this to the filters with highest internal influence. Out of the 100 sample images, the network prediction changed for only 5 images when their top 10 filters\n\n## Division of Labour\n- We notice that almost all the filters either capture positive sentiment or negative sentiment, but not both.\n- We substantiate via Figure 3, which is a clustered heatmap of signs of conductances of the 256 filters (columns) for around four thousand examples (rows) from the Stanford Sentiment Tree Bank [24]. Notice that very few filters have\n- both negative and positive conductance. Negation\n- Negation is commonly used in expressing sentiments, in phrases like &quot;this is not good&quot; or &quot;this is not bad&quot;. Does the sentiment network understand negation? Does it have hidden units dedicated to implement the logic of negation? We first identify high conductance filters for the input &quot;this is not good&quot; that have a high attribution to the pattern &quot;not good&quot;.\n- Sentences with high conductance for filters that have high conductance for the phrase &quot;not bad&quot;. These filters are largerly focussed on negation.\n\n## Images\n- ![](../images/Pasted%20image%2020230110140703.png)\n- ![](../images/Pasted%20image%2020230110140713.png)\n- ![](../images/Pasted%20image%2020230110140725.png)"},"KB/Cone":{"title":"Cone","links":[],"tags":["brain"],"content":"Cone\n\nA type of photoreceptor cell responsible for color vision that is found in the retina.\n"},"KB/Confidence":{"title":"Confidence","links":[],"tags":["explainability"],"content":"Confidence\n\nconfidence should always be assessed on a model in which reliability is expected.\nstability is a must-have when drawing interpretations from a certain model\nTrustworthy interpretations should not be produced by models that are not stable.\nHence, an explainable model should contain information about the confidence of its working regime.\n"},"KB/Confirmation-Bias":{"title":"Confirmation Bias","links":[],"tags":["temp"],"content":"Confirmation Bias\n\nfairness\nThe tendency to search for, interpret, favor, and [recall] information in a way that confirms one’s preexisting beliefs or hypotheses. Machine learning developers may inadvertently collect or label data in ways that influence an outcome supporting their existing beliefs. Confirmation bias is a form of [implicit bias](recall] information in a way that confirms one’s preexisting beliefs or hypotheses. Machine learning developers may inadvertently collect or label data in ways that influence an outcome supporting their existing beliefs. Confirmation bias is a form of [implicit bias.md).\nExperimenter’s bias is a form of confirmation bias in which an experimenter continues training models until a preexisting hypothesis is confirmed.\n\nconvenience sampling\n\nUsing a dataset not gathered scientifically in order to run quick experiments. Later on, it’s essential to switch to a scientifically gathered dataset\n\nconvex optimization\n\nThe process of using mathematical techniques such as gradient descent to find the minimum of a convex function. A great deal of research in machine learning has focused on formulating various problems as convex optimization problems and in solving those problems more efficiently.\n"},"KB/Conformer":{"title":"Conformer","links":["KB/Transformer","KB/Features","KB/LibriSpeech"],"tags":["architecture"],"content":"Conformer\n\nConformer: Convolution-augmented Transformer for Speech Recognition\nTransformer models are good at capturing content-based global interactions, while CNNs exploit local Features effectively\nintegrating components from both CNNs and Transformers for end-to-end [speech recognition](speech recognition.md) to model both local and global dependencies of an audio sequence in a parameter-efficient way\nimportance of each component, and demonstrated that the inclusion of convolution modules is critical to the performance of the Conformer model\npropose the convolution-augmented Transformer for [speech recognition](speech recognition.md), named Conformer\nLibriSpeech\n"},"KB/Confusion-Matrix":{"title":"Confusion Matrix","links":[],"tags":["loss"],"content":"Confusion Matrix\n\nMeasures the test performance of a classification system on a per-class basis by indicating the number of samples of actual class a predicted as class b.\nThe rows relate to the actual class labels a and the columns to the predicted class labels b.\n"},"KB/Connectionism":{"title":"Connectionism","links":["KB/Verb"],"tags":["language"],"content":"Connectionism\n\nSo called symbols like noun, Verb or noun-phrase are just epiphenomenal misunderstandings we have of learned through network arrangements of non-linguistic primitive elements\n"},"KB/Connectionist-Networks":{"title":"Connectionist Networks","links":[],"tags":["language"],"content":"Connectionist Networks\n\nIntelligence ‘emerges’ through the changes in the connections (weights)\nBasically deep learning\n"},"KB/Connectives":{"title":"Connectives","links":[],"tags":["language"],"content":"Connectives\n\nconnect words, phrases (and, but, when)\n"},"KB/Connectome":{"title":"Connectome","links":[],"tags":["cognitivemodel"],"content":"Connectome\n\nthe graph of how neurons in a brain connect\n\nConnectome"},"KB/Consequentialist-ethics":{"title":"Consequentialist ethics","links":[],"tags":["ethics"],"content":"Consequentialist Ethics\n\nan agent is ethical if and only if it weighs the consequences of each choice and chooses the option which has the most moral outcomes\n"},"KB/Conservation-Of-Momentum":{"title":"Conservation Of Momentum","links":[],"tags":["physics"],"content":"Conservation Of Momentum\n\n\\Sigma p_{i}= \\Sigma p _{f}\nm_{1}u_{1}+m_{2}u_{2}= m_{1}v_{1}+ m_{2}v_{2}\np is momentum\nv and d are velocity\nm is mass\n"},"KB/Contact-Sensor":{"title":"Contact Sensor","links":["KB/Force"],"tags":["robotics"],"content":"Contact Sensor\n\nA device that detects the presence of an object or measures the amount of applied Force or torque applied on the object through physical contact with it. Contact sensing can be used to determine location, identity, and orientation of work pieces.\n"},"KB/Content-Based-Attention":{"title":"Content Based Attention","links":["KB/Attention","KB/Attention-Alignment"],"tags":["architecture"],"content":"Content Based Attention\n\nGraves2014\nAttention Alignment score $score(s_{t}, h_{i}) = cosine[s_{t}, h_{i}]$$\n"},"KB/Content-Based-Recommender":{"title":"Content Based Recommender","links":[],"tags":["usermodel"],"content":"Content Based Recommender\n\nUser actions linked to content\nUser model rep as info context\neg: Google\n"},"KB/Content-Morpheme":{"title":"Content Morpheme","links":["KB/Morpheme"],"tags":["language"],"content":"Content Morpheme\n\ncarry some semantic content\ne.g. able, un, van\n"},"KB/Content-words":{"title":"Content words","links":["Noun","KB/Adjective","KB/Verb","KB/Adverb"],"tags":["language"],"content":"Content Words\n\nIdentifies part of a word\nNoun\nAdjective\nVerb\nAdverb\n"},"KB/Context-Free-Grammar":{"title":"Context Free Grammar","links":["KB/Types-of-Words","KB/Top-Down-Parsing","KB/Bottom-Up-Parsing"],"tags":["language"],"content":"Context Free Grammar\n\nformal system that describes a language by specifying how any legal text can be derived from a distinguished symbol called the axiom, or sentence symbol.\nIt consists of a set of productions, each of which states that a given symbol can be replaced by a given sequence of symbols\nTypes of Words\n\n\nTop Down Parsing\nBottom Up Parsing\n"},"KB/Context-Similarity":{"title":"Context Similarity","links":[],"tags":["semisupervisedlearning"],"content":"Context Similarity\n\nbetween image patches\nimage clusteringbased methods\ngraph constraint-based methods\n"},"KB/Continous-->-Discrete":{"title":"Continous -> Discrete","links":["KB/Binning","KB/Hierarchial-Refinement","KB/Vector-Quantization","KB/Neural-Dynamics"],"tags":["temp"],"content":"Continous → Discrete\nBinning\nHierarchial Refinement\nVector Quantization\nNeural Dynamics"},"KB/Continuous-Path":{"title":"Continuous Path","links":["KB/Point-to-Point","KB/Trajectory","KB/End-effector"],"tags":["robotics"],"content":"Continuous Path\n\nDescribes the process where by a robot is controlled over the entire path traversed, as opposed to a Point-to-Point method of traversal. This is used when the Trajectory of the End-effector is most important to provide a smooth movement, such as in spray painting etc\n"},"KB/Contour":{"title":"Contour","links":["KB/Isoline","KB/Isosurface","KB/Countouring-with-Transparency"],"tags":["visualization"],"content":"Contours\n\nFor nD \\{x \\in \\mathbb{R}^{n}|f(x)=c\\}\nAlways closed curves\nNever self instersect\nNested\nContours cut the plane into values smaller or larger than the isovalue c\nIsoline\nIsosurface\nCountouring with Transparency\n"},"KB/Contrastive-Loss":{"title":"Contrastive Loss","links":["Tag-Pages/loss","KB/Gradient-Descent","KB/Gradient-Ascent","KB/Embedding"],"tags":["loss"],"content":"Contrastive loss\n\nMinimize distance between similar inputs Gradient Descent, maximize between dissimilar Gradient Ascent\nLearn Embedding/Feature space using neighbors\ndim(Embedding d) &lt; dim(input Space D)\nEncoded using a learnable function(NN) G_\\theta(x) : \\mathcal{R}^D \\rightarrow \\mathcal{R}^d\nBinary labels : similar or not\n$$D_\\theta(x_1, x_2) = ||G_\\theta(x_1) - G_\\theta(x_2)||_2$\nL(\\theta, y, x_1, x_2) = \\frac{(1-y)(D_\\theta(x_1, x_2))^2}{2} + \\frac{y(max(0,m-D\\theta(x_1, x_2)))^2}{2}\n\nm is enforced margin between similar and dissimilar (m&gt;0)\nLabeled points (y,x_1,x_2) are generated\n\n\n"},"KB/Contrastive-Predictive-Coding":{"title":"Contrastive Predictive Coding","links":["KB/Autoregressive","KB/Contrastive-Loss"],"tags":["architecture"],"content":"Contrastive Predictive Coding\n\nRepresentation Learning with Contrastive Predictive Coding\nContrastive Predictive Coding\nframework for extracting compact latent representations to encode predictions over future observations\nlearn such representations by predicting the future in latent space by using powerful Autoregressive models\nprobabilistic Contrastive Loss based on NCE, which both the encoder and Autoregressive model are trained to jointly optimize, which they call InfoNCE\nInfoNCE induces the latent space to capture information that is maximally useful to predict future samples\ncombines Autoregressive modeling and noise-contrastive estimation with intuitions from predictive coding to learn abstract representations in an unsupervised fashion\nnegative sampling\n"},"KB/Contributions-of-Shape,-Texture,-and-Color-in-Visual-Recognition-Abstract":{"title":"Contributions of Shape, Texture, and Color in Visual Recognition Abstract","links":["KB/Grad-CAM","KB/Modality"],"tags":["explainability"],"content":"Contributions of Shape, Texture, and Color in Visual Recognition Abstract\n\n/zotero\nYunhao Ge, Yao Xiao, Zhi Xu, Xingrui Wang, and Laurent Itti\n \n\n\nAbstract\n\n[humanoid vision engine](humanoid vision engine.md) (HVE) that explicitly and separately computes shape, texture, and color features from images\nresulting feature vectors are then concatenated to support the final classification\nHVE can summarize and rankorder the contributions of the three features to object recognition.\nWe use human experiments to confirm that both HVE and humans predominantly use some specific features to support the classification of specific classes\nTo demonstrate more usefulness of HVE, we use it to simulate the open-world zeroshot learning ability of humans with no attribute labeling\nFinally, we show that HVE can also simulate human imagination ability with the combination of different features.\n\nIntroduction\n\nA widely accepted intuition about the success of CNNs on perceptual tasks is that CNNs are the most predictive models for the human ventral stream object recognition\nTo understand which feature is more important for CNN-based recognition, recent paper shows promising results: ImageNet-trained CNNs are biased towards texture while increasing shape bias improves accuracy and robustness [33]\nHere, inspired by HVS, we wish to find a general way to understand how shape, texture, and color contribute to a recognition task by pure data-driven learning.\nIt has been shown by neuroscientists that there are separate neural pathways to process these different visual features in primate\nAmong the many kinds of features crucial to visual recognition in humans, the shape property is the one that we primarily rely on in static object recognition [16]. Meanwhile, some previous studies show that surface-based cues also play a key role in our vision system\nFor example, [21] shows that scene recognition is faster for color images compared with grayscale ones\n[Humanoid Vision Engine](Humanoid Vision Engine.md)\n\nImage Parsing and Foreground Identification.\n\nwe use the entity segmentation method [41] to simulate the process of parsing objects from a scene in our brain.\nEntity segmentation is an open-world model and can segment the object from the image without labels.\nThis method aligns with human behavior, which can (at least in some cases; e.g., autostereograms [29]) segment an object without deciding what it is\nAfter we get the segmentation of the image, we use a pre-trained CNN and Grad-CAM [47] to find the foreground object among all masks.\nWe design three different feature extractors after identifying the foreground object segment: shape extractor, texture extractor, and color extractor, similar to the separate neural pathways in the human brain which focus on specific property\n\nShape Feature Extractor\n\nwant to keep both 2D and 3D shape information while eliminating the information of texture and color\nfirst use a 3D depth prediction model [44,43] to obtain the 3D depth information of the whole image\nAfter element-wise multiplying the 3D depth estimation and 2D mask of the object, we obtain our shape feature\nWe can notice that this feature only contains 2D shape and 3D structural information (the 3D depth) and without color or texture information\n\nTexture Feature Extractor\n\nwant to keep both local and global texture information while eliminating shape and color information.\nto remove the color information, we convert the RGB object segmentation to a grayscale image\ncut this image into several square patches with an adaptive strategy (the patch size and location are adaptive with object sizes to cover more texture information)\nIf the overlap ratio between the patch and the original 2D object segment is larger than a threshold τ, we add that patch to a patch pool (we set τ to be 0.99 in our experiments, which means the over 99% of the area of the patch belongs to the object\nSince we want to extract both local (one patch) and global (whole image) texture information, we randomly select 4 patches from the patch pool and concatenate them into a new texture image\n\nColor Feature Extractor\n\nThe first method is phase scrambling\n\nPhase Scrambling\n\ntransforms the image into the frequency domain using the fast Fourier transform (FFT)\nIn the frequency domain, the phase of the signal is then randomly scrambled, which destroys shape information while preserving color statistics\nThen we use IFFT to transfer back to image space\nWe also used simple color histograms (see suppl.) as an alternative, but the results were not as good, hence we focus here on the phase scrambling approach for color representation.\n\nHumanoid Neural Network\n\nAfter preprocessing, we have three features\nTo simulate the separate neural pathways in humans’ brains for different feature information [1,11], we design three feature representation encoders for shape, texture, and color, respectively\nResNet-18 [24] as the backbone for all feature encoders to project the three types of features to the corresponding well-separated embedding spaces.\nhard to define the ground-truth label of the distance between features.\nGiven that the objects from the same class are relatively consistent in shape, texture, and color, the encoders can be trained in the classification problem independently instead, with the supervision of class labels.\nfter training our encoders as classifiers, the feature map of the last convolutional layer will serve as the final feature representation\nWe also propose a gradient-based contribution attribution method to interpret the contributions of shape, texture, and color to the classification decision,\nTake the shape feature as an example, given a prediction p and the probability of\nclass k, namely pk, we compute the gradient of pk with respect to the shape feature Vs\ngradient as shape importance weights ↵sk\nIn other words, Ssk represents the “contribution” of shape feature to classifying this\nimage as class k\n\nEffectiveness of Feature Encoders\n\nhandcrafted three subsets of ImageNet\nShape-biased dataset containing 12 classes, where the classes were chosen which intuitively are strongly determined by shape\nTexture-biased dataset uses 14 classes which we believed are more strongly determined by texture\nColor-biased dataset includes 17 classes\nAfter pre-processing the original images and getting their feature images, we input the feature images into feature encoders and get the T-SNE\nEach row represents one feature-biased dataset and each column is bounded with one feature encoder, each image shows the results of one combination\n\nEffectiveness of Humanoid Neural Network\n\nAs these classifiers classify images based on corresponding feature representation, we call them feature nets.\nIf we combine these three feature nets with the interpretable aggregation module, the classification accuracy is very close to the upper bound, which means our vision system can classify images based on these three features almost as well as based on the full original color images.\n\nMore Humanoid Applications with HVE Open-world Zero-shot Learning with HVE\n\nMost current methods [37,32,13] need humans to provide detailed attribute labels for each image, which is costly in time and energy. However, given an image from an unseen class, humans can still describe it with their learned knowledge\nFirst, to represent learnt knowledge, we use feature extractors\nTo retrieve learnt classes as description, we calculate the average distance dkm\nbetween Iun and images of other class k in the latent space on feature m Open-world classification\nTo further predict the actual class of Iun based on the feature-wise description, we use ConceptNet as common knowledge to conduct reasoning\nWe form a reasoning root pool R⇤ consisting of feature roots Rs, Rt, Rc obtained during image description, and shared attribute roots Ras , Rat , Rac . The reasoning roots will be our evidence for reasoning\nWe humans can intuitively imagine an object when seeing one aspect of a feature, especially when this feature is prototypical (contribute most to classification)\nFor instance, we can imagine a zebra when seeing its stripe (texture). This process is similar but harder than the classical image generation task since the input features Modality here dynamic which can be any feature among shape, texture, or color\n\nCross Feature Retrieval\n\nIn order to reasonably retrieve the most possible other two corresponding features given only one feature (among shape, texture, or color), we learn a feature agnostic encoder that projects the three features into one same feature space and makes sure that the features belonging to the same class are in the nearby regions.\nIn the retrieval process, given any feature of any object, we can map it into the cross feature embedding space by the corresponding encoder net and the feature agnostic net\nThen we apply the 2 norm to find the other two features closest to the input one as output. The output is correct if they belong to the same class as the input.\n\nCross Feature Imagination\n\nTo stimulate imagination, we propose a crossfeature imagination model to generate a plausible final image with the input and retrieved features\nInspired by the pixel2pixel GAN[26] and AdaIN[25] in the style transfer, we design a crossfeature pixel2pixel GAN model to generate the final image.\n\nPictures\n\n\n\n\n\n\n\n\n\n"},"KB/Conv-Based-Noise-Reduction":{"title":"Conv Based Noise Reduction","links":["KB/Conv"],"tags":["visualization"],"content":"Conv Based Noise Reduction\n\nNoise is high frequency component, suppress via low-pass filters\nIdeal low-pass filter\nmultiply with box filter in frequency domain\nconvolution with sinc in spatial domain (impractical: infinite extent)\n\n\nSpatially narrow (wide) filter has wide (narrow) spectrum and low (high) smoothing effect\n"},"KB/Conv":{"title":"Convnd","links":["KB/Padded-Conv","KB/Strided","KB/Depthwise-Separable","KB/Causal-1D-Conv","KB/Causal-Dilated-Conv"],"tags":["temp"],"content":"Convnd\n\nA\\ast B\nConnect a neighbor only to spatial neighborhood → spatial order\n\nSome rotation and illumination invariance\n\n\nSlide over → Same weights independant of location → less weights\nSubsample after conv\nmultiple 2d feature maps\nSimilar to Gabor filters after learning\nSome might collapse to 0\nout(x,y) = \\Sigma_i \\Sigma_j input(x+i, y+j) kernel(i,j)\nZ^j = \\Sigma_{i=0}^{l-1}W^{ji} \\ast X^i + b^{ij}\nY^j = g(Z^j)\nOutput shape : \\frac{()_i-f+2p}{s}\n\nIf p = \\frac{f-1}{2} and s=1 then dimensions maintained\n\n\nOne operation repeated over and over starting with raw\n\nPadded Conv\nStrided\nDepthwise Separable\nCausal 1D Conv\nCausal Dilated Conv\n"},"KB/ConvBERT":{"title":"ConvBERT","links":["KB/BERT","KB/Multi-Head-Attention","KB/Self-Attention","span-based-dynamic-convolutions","KB/Attention"],"tags":["architecture"],"content":"ConvBERT\n\nConvolutional BERT (ConvBERT) improves the original BERT by replacing some Multi Head Attention Self Attention segments with cheaper and naturally local operations, so-called span-based dynamic convolutions. These are integrated into the self-Attention mechanism to form a mixed Attention mechanism, allowing Multi-headed Self-Attention to capture global patterns; the Convolutions focus more on the local patterns, which are otherwise captured anyway. In other words, they reduce the computational intensity of training BERT.\n"},"KB/ConvNeXt":{"title":"ConvNeXt","links":["KB/Res-Net","KB/Vision-Transformer","KB/Swin-Transformer","KB/Conv","KB/AdamW","KB/Label-Smoothing","KB/Pooling","KB/Transformer","KB/Depthwise-Separable","KB/Attention","KB/GELU","KB/Relu","KB/Layers","KB/Batch-Normalization","KB/Layer-Normalization","KB/Downsampling","KB/ImageNet","KB/COCO","KB/ADE20K"],"tags":["architecture"],"content":"ConvNeXt\n\n@liuConvNet2020s2022\nmodifying a standard Res Net , following design choices closely inspired by Vision Transformer\nA vanilla ViT, on the other hand, faces difficulties when applied to general computer vision tasks such as object detection and semantic segmentation\nhierarchical Transformers (e.g., Swin Transformer ) that reintroduced several Conv priors, making Transformers practically viable as a generic vision backbone\neffectiveness of such hybrid approaches is still largely credited to the intrinsic superiority of Transformers, rather than the inherent inductive biases of convolutions\nextending the number of epochs, using AdamW optimizer, Stochastic Depth, Label Smoothing\nnumber of blocks in each stage (stage compute ratio), which was adjusted from (4, 4, 6, 3) to (3, 3, 9, 3)\nThe second is the stem cell configuration, which in the original ResNet consisted of 7×7 convolutions with stride 2 followed by a max-Pooling layer. This was substituted by a more Transformer-like “patchify” layer which utilizes 4×4 non-overlapping convolutions with stride 4\nDepthwise Separable , which are interestingly similar to self-Attention as they work on a per-channel basis\nhigher number of channels (from 64 to 96)\nInverted Bottleneck: An essential configuration of Transformers is the expansion-compression rate in the MLP block (the hidden dimension is 4 times higher than the input and output dimension)\ninput is expanded using 1 \\times 1 convolutions and then shrunk through depthwise convolution and 1 \\times 1 convolutions\nmove the depthwise convolution before the convolution\n7 \\times 7 window (higher values did not bring any alterations in the results\nGELU instead of Relu , a single activation for each block (the original Transformer module has just one activation after the MLP), fewer normalization Layers, Batch Normalization substituted by Layer Normalization , and separate Downsampling layer\nImageNet\nCOCO\nADE20K\nA case in point is multi-modal learning, in which a cross-Attention module may be preferable for modeling feature interactions across many modalities\nTransformers may be more flexible when used for tasks requiring discretized, sparse, or structured outputs\n"},"KB/Convolutional-RNN":{"title":"Convolutional RNN","links":["KB/Conv"],"tags":["architecture"],"content":"Convolutional RNN\n\nh_t = \\sigma_h(W_{hh}\\star h_{t-1} + W_{xh}\\star x_t + b_h)\ny_t = \\sigma_y(W_{hy}\\star h_t + b_y)\n\\star is spatial Conv\n5D shapes → [samples, timesteps, width, height, channels]\nVery memory intensive\nx^{2}+x\n"},"KB/Coping-Theory":{"title":"Coping Theory","links":[],"tags":["ethics"],"content":"Coping Theory\n\n[Marsella and Gratch, 2003]\nallow agents to deal with strong negative emotions by changing the appraisal of the given situation was proposed\nagent assesses the ethical effects of its own actions and other agents’ actions\nIf its own action violates a given moral value, the shame emotion is triggered which serves to lower the priority of continuing with the given action\nIf another agent’s action violates a given moral value, the reproach emotion is triggered in the observing agent which serves to increase social distance with the given agent\nsimilar to existing individual ethical decision frameworks implicit reward\nhumans in the loop\n"},"KB/Coronary-Bypass":{"title":"Coronary Bypass","links":[],"tags":["medical"],"content":"Coronary Bypass\n\nSurgical transplant of a healthy blood vessel into the heart to bypass or replace an unhealthy vessel\n"},"KB/Corpus-callosum":{"title":"Corpus callosum","links":[],"tags":["brain"],"content":"Corpus Callosum\n\nbundle of fibers that transmits messages from one side to the other\n"},"KB/Corpus-dependence":{"title":"Corpus dependence","links":[],"tags":["language"],"content":"Corpus Dependence\n\nMisspellings\nErroneous Punctuations and Spacing\nDifficult to write rules to govern the corpora from the Internet\nDiﬃcult to prescribe rules governing the use of a written language\nPunctuations mean – Suprasegmentals in Spoken Language but might not be the same for corpora\nAlgorithms may expect corpora need to obey some rules\n"},"KB/Correlation":{"title":"Correlation","links":[],"tags":["temp"],"content":"\ntoc: true\ntitle: Correlation\ntags: [‘temp’]\n\nCorrelation\n\nHow strong a relationship is between data.\nThe formulas return a value between -1 and 1, where:\n\n1 indicates a strong positive relationship.\n-1 indicates a strong negative relationship.\nA result of zero indicates no relationship at all.\n\n\n\n"},"KB/Cortical-Homunculus":{"title":"Cortical Homunculus","links":[],"tags":["brain"],"content":"Cortical Homunculus\n\n\n"},"KB/Cortisol":{"title":"Cortisol","links":["KB/Adrenal-Glands"],"tags":["brain"],"content":"Cortisol\n\nA steroid hormone produced by the Adrenal Glands that controls how the body uses fat, protein, carbohydrates, and minerals, and helps reduce inflammation. Cortisol is released in the body’s stress response; scientists have found that prolonged exposure to cortisol has damaging effects on the brain\n"},"KB/Cosine-Distance":{"title":"Cosine Distance","links":[],"tags":["nlp"],"content":"Cosine Distance\n\nComplement of [Cosine Similarity](Cosine Similarity.md)\nD_{c}(A,B) := 1- S_{c}(A,B)\n"},"KB/Cosine-Learning-Rate-Decay":{"title":"Cosine Learning Rate Decay","links":["KB/Learning-Rate-Warmup"],"tags":["temp"],"content":"Cosine Learning Rate Decay\n\nInstead of Learning Rate Warmup and then decay\n\\eta_{\\mathrm{t}}=\\frac{1}{2}\\left(1+\\cos\\left(\\frac{\\mathrm{t}\\pi}{\\mathrm{\\mathrm{T}}}\\right)\\right)\\eta\nRate decreases slowly at first, then almost linear in the middle and slows down again in the end\n\n"},"KB/Cosine-Similarity":{"title":"Cosine Similarity","links":["KB/Lp-Regularization","KB/Recommender-System"],"tags":["loss"],"content":"Cosine Similarity\n\nLp Regularization l2norm aka p = 2\nS_{c}(A,B) := cos(\\theta) = \\frac{A\\cdot B}{||A|| ||B||} = \\frac{\\Sigma_{i=1}^{n}A_{i}B_{i}}{\\sqrt{\\Sigma_{i=1}^{n}A^{2}_{i}} \\sqrt{\\Sigma_{i=1}^{n}B_{i}^{2}}}\nranges from -1 : exactly opposite, 1 : exactly same, 0: orthogonal/not correlated, intermediate\n[Cosine Distance](Cosine Distance.md)\nCosine similarity is  - \\mathrm{sum}\\left( \\mathrm{l2norm}\\left( y \\right) \\cdot \\mathrm{l2norm}\\left( ŷ \\right) \\right)\n\nmagnitude of vectors is not taken into account, merely their direction\nIn practice, this means that the differences in values are not fully taken into account\nIf you take a Recommender System, for example, then the cosine similarity does not take into account the difference in rating scale between different users\nhigh-dimensional data and when the magnitude of the vectors is not of importance\n"},"KB/Cost-conference-'24":{"title":"Cost conference","links":["KB/croissant","KB/Fairness"],"tags":["conference","openml"],"content":"Cost Conference\n \n\nDaemon cost\n\nDemocratizing AI\n\nNot widespread enough in many fields\nPopular with funding agencies (?)\n\nBut dont really know what it is very well\n\n\nWhy not popular (AI driven materials)\n\nExpectation management ← increase in transparency\nDocumentation\nUpskilling, field develops too fast → moving target\n\nwhere do you want to enter : at the forefront, or somewhere in the middle\n\n\nEducation\n\nLectures and courses\n\nSummer school\n\n\nincreasing awareness\n\nThrow money at the problem\n\n\nunderrepresented research communities\ncommunity of learners\n\n\nRole model effect\n\n\nLong term issues\n\nHow long do we have to do this\nPeople dont like to share data\nUser friendly tools\nData standards : quality check - croissant\nReference implementations\nMetrics of success\n\nCitations\nHow many datasets use standards\n\n\nIndustry outreach, sharing success stories\n\n\nGeneral\n\nData prominence\n\n\n\nTDCC - NES\n\nThematic Digital Competence Centre (TDCC) for the Natural &amp; Engineering Sciences (NES)\nJoanne Yeomans - Network Manager\nFairness\n\nSURF\n\nMonica Rotulo - ML Advisor\nHPC and ML consultancy\nTheoretical vs Applied ML\n"},"KB/Counterfactual-Fairness":{"title":"Counterfactual Fairness","links":["https/papers.nips.cc/paper/2017/file/1271a7029c9df08643b631b02cf9e116-Paper.pdf"],"tags":["temp"],"content":"Counterfactual Fairness\n\nA fairness metric that checks whether a classifier produces the same result for one individual as it does for another individual who is identical to the first, except with respect to one or more sensitive attributes. Evaluating a classifier for counterfactual fairness is one method for surfacing potential sources of bias in a model.\nSee “When Worlds Collide Integrating Different Counterfactual Assumptions in Fairness” for a more detailed discussion of counterfactual fairness.\n"},"KB/Counterfactual-Images":{"title":"Counterfactual Images","links":[],"tags":["explainability"],"content":"Counterfactual Images\n\nConcepts in images, removing which would increment networks confidence about the target (aka competing class)\n"},"KB/Counterfactual-Impact-Evaluation":{"title":"Counterfactual Impact Evaluation","links":[],"tags":["explainability"],"content":"Counterfactual Impact Evaluation\n\nlocal method of comparison for different predictions. Counterfactuals are contrastive. They explain why a decision was made instead of another. A counterfactual explanation of a prediction may be defined as the smallest change to the feature values that changes the prediction to a predefined output.\n"},"KB/Countouring-with-Transparency":{"title":"Countouring with Transparency","links":[],"tags":["visualization"],"content":"Countouring with Transparency\n\ndraw several contours for several isovalues\nassign “adequate” transparency\n"},"KB/Covariance":{"title":"Covariance","links":[],"tags":["temp"],"content":"\ntoc: true\ntitle: Covariance\ntags: [‘temp’]\n\nCovariance\n\nhow much two random variables vary together\n\nCov(X,Y) = \\frac{\\Sigma(x_{i}- \\bar x)(y_{i}- \\bar y)}{N-1}\n\\bar x is the mean of x\n"},"KB/Coverage-Bias":{"title":"Coverage Bias","links":[],"tags":["temp"],"content":"Coverage Bias\n\nThe population represented in the dataset does not match the population that the machine learning model is making predictions about.\n"},"KB/Coverage-of-ethics-within-the-artificial-intelligence-and-machine-learning-academic-literature":{"title":"Coverage of ethics within the artificial intelligence and machine learning academic literature","links":[],"tags":["ethics"],"content":"Coverage of ethics within the artificial intelligence and machine learning academic literature\n\nLillywhite, Aspen; Wolbring, Gregor\n\nAbstract\n\nDisabled people are often the anticipated users of scientific and technological products and processes advanced and enabled by artificial intelligence\nalso impacted by societal impacts of AI/ML\nproblems have been identified in how ethics discourses engage with disabled people\nOf the n = 1659 abstracts engaging with AI/ML and ethics downloaded from Scopus (which includes all Medline articles) and the 70 databases of EBSCO ALL, we found 54 relevant abstracts using the term “patient” and 11 relevant abstracts mentioning terms linked to “impair*”, “disab*” and “deaf”\n\nStudy design\n\nWe used a modified scoping review drawing from (Arksey &amp; O’Malley, 2005) as the most appropriate approach for the study given the aim of our study\nScoping studies “map rapidly the key concepts underpinning a research area” (Arksey &amp; O’Malley, 2005, p. 21), to identify the extent of research conducted on a given topic (Davis, Drey, &amp; Gould, 2009; Grant &amp; Booth, 2009) and the current understanding of a given topic (S. Anderson, Allen, Peckham, &amp; Goodwin, 2008).\n\nData sources\n\nEBSCO ALL, an umbrella database that includes over 70 other databases itself and Scopus, which incorporates the full Medline database collection\nThe first article with the term “ethic*” and any of the AI terms within the EBSCO databases was published in 1981, while the first article within Scopus was published in 1962\n\nLimitation\n\nOur findings also do not cover all words one could use to depict disabled people and as such, our results can not be generalized to every disability term\n\nDiscussion\n\nMany ethics issues pertinent to disabled people were discussed within the abstracts; for example the ethical decision making of robots but without engaging with disabled people.\nHowever, many barriers have been identified for disabled people to shape technology governance discussions in an anticipatory way\nincluding that the medical imagery of disabled people is seen to hinder their involvement in policy discussions (Wolbring, Mackay, Rybchinski, &amp; Noga, 2013)\nIn one study, it is acknowledged that ethical, moral, social, cultural, and political issues have been traditionally de-emphasized in research guided by usability concerns (Fallman, 2010)\nGiven the breadth of academic disciplines, including disability studies covered by the two databases, and given that we found only one article coming from a disability studies program based out of Bremen, Germany (Bruhn et al., 2006), it might be warranted to investigate how academics choose their topics of investigation and why the topics we found lacking in the literature were not chosen\nAnother angle of investigation could be why students are not acting as knowledge producers on the topics we found lacking\nAs to disabled students, based on a study that investigated the experience of disabled postsecondary students in postsecondary education (Hutcheon &amp; Wolbring, 2012) we suggest that the experience reported (feeling medicalized, hesitant to self-advocate, to try to fit in with the norm) might be factors that hinder disabled students to be knowledge producers especially on contentious issues such as ethics and disabled people\n"},"KB/CowMask":{"title":"CowMask","links":["KB/CutMix"],"tags":["augmentation"],"content":"CowMask\n\n@frenchMilkingCowMaskSemiSupervised2020\nsemi-supervised learning\noriginal and augmented images are brought closer during training\nCowMask suggests two types of mixing approaches 1) erasing and 2) mixing two images similar to CutMix\nThe mask here is of irregular shape rather than rectangular and generated by masking or keeping a proportion of image pixels through thresholding.\n[Gaussian filter](Gaussian filter.md) is applied to remove noise before thresholding.\nPixel values below the threshold are either erased or replaced by the pixel values of the randomly selected image at the corresponding locations.\n"},"KB/Crash-Blossom":{"title":"Crash Blossom","links":[],"tags":["temp"],"content":"Crash Blossom\n\nA sentence or phrase with an ambiguous meaning. Crash blossoms present a significant problem in natural language understanding. For example, the headline Red Tape Holds Up Skyscraper is a crash blossom because an NLU model could interpret the headline literally or figuratively.\n"},"KB/Critical-Points":{"title":"Critical Points","links":[],"tags":["visualization"],"content":"Critical Points\n\nsink (attracting node): all vectors converge\nsource (repelling node): all vectors diverge\nsaddle point: slopes are zero in orthogonal directions, no extremum • center point: embedded by (circular) flow around\nattracting focus: flow is attracted in a spiral pattern\nrepelling focus, where the flow is repelled in a spiral pattern\n\n"},"KB/Cropping":{"title":"Cropping","links":[],"tags":["augmentation"],"content":"Cropping\n\nCropping images can be used as a practical processing step for image data with mixed height and width dimensions by cropping a central patch of each image\nAdditionally, random cropping can also be used to provide an effect very similar to translations.\nwhereas translations preserve the spatial dimensions of the image\nDepending on the reduction threshold chosen for cropping, this might not be a label-preserving transformation. Rotation\nRotation augmentations are done by rotating the image right or left on an axis between 1° and 359°\nThe safety of rotation augmentations is heavily determined by the rotation degree parameter.\nas the rotation degree increases, the label of the data is no longer preserved post-transformation. Translation\nShifting images left, right, up, or down can be a very useful transformation to avoid positional bias in the data\nFor example, if all the images in a dataset are centered, which is common in face recognition datasets, this would require the model to be tested on perfectly centered images as well.\nremaining space can be filled with either a constant value such as 0 s or 255 s, or it can be filled with random or Gaussian noise\n"},"KB/Cross-Entropy":{"title":"Cross Entropy","links":["KB/Entropy","KB/Probability","KB/Distributions","KB/KL-Divergence"],"tags":["loss"],"content":"Cross Entropy\n\nEntropy\nCross-Entropy is a measure from the field of information theory, building upon Entropy and generally calculating the difference between two Probability Distributions.\nIt is closely related to but is different from KL Divergence that calculates the relative Entropy between two Probability Distributions, whereas cross-Entropy can be thought to calculate the total Entropy between the Distributions.\nimplicit distribution $$\np(Y|x;\\theta)\n\n- $$\n\\mathscr{L}(\\theta) = -\\mathbb{E}_{(x,y) \\sim P(X,Y)} log (p_{model}(Y|x))\n- Categorical CE\n\t- ![Pasted image 20240828102117](Pasted%20image%2020240828102117.png)\n\t- Classification\n\t- Labels should be [One hot](One hot.md) \n\t- $$\n\n\\mathscr{L}(\\theta) = -\\mathbb{E}{(x,y) \\sim P(X,Y)} \\Sigma{i=1}^C 1(y=i)log (p_{model}f_i(x|\\theta))\n\t\t- C is no of classes\n\t\t- $$\nL(y, \\hat y) = - \\Sigma_{i}\\Sigma_{c}y_{i}^{c} log(\\hat y_{i}^{c})\n- [MSE](MSE.md)\n\t- Regression\n\t- $$\n\n\\mathscr{L}(\\theta) = \\frac{1}{2}\\mathbb{E}_{(x,y) \\sim P(X,Y)}||y-f(x;\\theta)||\n- [Binary Cross Entropy](Binary%20Cross%20Entropy.md)\n\n- The Cross Entropy Loss function is a popular loss function that is used in multi-class image classification tasks. Derived from the field of information theory, it uses the concept of entropy to quantifies the discrepancy between two given probability distributions. The formula for computing the loss is given by $$\n\\mathscr{l}(x,y) = L = \\{l_{1}, ..., l_{N}\\}^{T}\n$$ where $$\nl_{n} = -w_{y_{n}}log \\frac{exp(x_{n, y_{n}})}{\\Sigma_{c=1}^{C}exp(x_{n,c})}\n$$, $x$ is the input, $y$ is the target, $C$ is the number of classes\n\n- ![](../images/Pasted%20image%2020240828102220.png)\n- ![](../images/Pasted%20image%2020240828102230.png)\n- Also [Negative Log Likelihood](Negative%20Log%20Likelihood.md)"},"KB/Cross-Minimization":{"title":"Cross Minimization","links":["KB/Graphs"],"tags":["visualization","graph"],"content":"Cross Minimization\n\nplanar graph: can be drawn on a plane without edge crossings\nfrom Euler’s formula - the maximum number of edges for planar Graphs: e \\leq 3v-6\n\n"},"KB/Cross-Modal-Distillation":{"title":"Cross Modal Distillation","links":[],"tags":["knowledgedistillation"],"content":"Cross Modal Distillation\n\nMoreover, Do et al. (2019) proposed a knowledge distillation-based visual question answering method, in which knowledge from trilinear interaction teacher model with image-question-answer as inputs is distilled into the learning of a bilinear interaction student model with image-question as inputs\n"},"KB/Cross-Modal-based-Methods":{"title":"Cross Modal-based Methods","links":[],"tags":["semisupervisedlearning"],"content":"Cross Modal-based Methods\n\ntrain ConvNets to verify whether two different channels of input data are corresponding to each other\nVisual-Audio Correspondence Verification\nRGB-Flow Correspondence Verification\negomotion\n"},"KB/Cross-Validation":{"title":"Cross Validation","links":["KB/Emperical-Risk"],"tags":["temp"],"content":"Cross Validation\nKFold\n\nRepeat for m = 1..L\n\nSplit data into roughly equal sizes. Disjoint subsets\nGet model with min Emperical Risk\nTest it with validation set\nAvg it for the folds for this value of m\n\n\nFind optimal class for that m that had min avg validation risk (aka training error)\nCompute h_{opt} using the original training data\n\nLeave One Out\n\nEach D contains a single training example\nFor tiny datasets\n"},"KB/Cross-angle-Maximization":{"title":"Cross angle Maximization","links":[],"tags":["visualization","graph"],"content":"Cross Angle Maximization\n\navoid ambiguities\n\n"},"KB/Cross-dataset-generalization":{"title":"Cross-dataset generalization","links":[],"tags":["ethics"],"content":"Cross-dataset Generalization\n\nvirtually no papers demonstrating cross-dataset generalization, e.g. training on ImageNet, while testing on PASCAL VOC\nif our datasets were truly representative of the real world, this would be a very easy thing to do, and would give access to more of the much needed labelled data\nBut from our perspective, all the datasets are really trying to represent the same domain – our visual world – and we would like to measure how well or badly they do it.\nOverall the results look rather depressing, as little generalization appears to be happening beyond the given dataset\n"},"KB/Cross-situational-learning":{"title":"Cross-situational learning","links":[],"tags":["language"],"content":"Cross-situational Learning\n\nSpeakers ‘take statistics’ about word-concept co-occurrences\nPredicts gradual learning\n"},"KB/Cuboids":{"title":"Cuboids","links":[],"tags":["visualization"],"content":"Cuboids\n\n\n"},"KB/Cumulative-Interpretation":{"title":"Cumulative Interpretation","links":[],"tags":["language"],"content":"Cumulative Interpretation\n\nNo scopal relation\nThree aliens are holding two flags.\n"},"KB/Curl-And-Vorticity":{"title":"Curl And Vorticity","links":["KB/Helmholtz-Theorem"],"tags":["visualization"],"content":"Curl And Vorticity\n\n\n\nHelmholtz Theorem\n"},"KB/Curriculum-Learning":{"title":"Curriculum Learning","links":["KB/GPT"],"tags":["architecture"],"content":"Curriculum Learning\n\nFormal Mathematics Statement Curriculum Learning\nneural theorem prover using GPT-f\nsolve a curriculum of increasingly difficult problems out of a set of formal statements of sufficiently varied difficulty\nhigh-school Math Olympiad problems\nlanguage model to find proofs of formal statements\nformal mathematics\nat same compute budget, expert iteration, by which they mean proof search interleaved with learning, dramatically outperforms proof search only\nexpert iteration is capable of finding and solving a curriculum of increasingly difficult problems, without the need for associated ground-truth proofs\nminiF2F\nautomatically solving multiple challenging problems drawn from high school olympiads\nlack of self-play in the formal mathematics setup can be effectively compensated for by automatically as well as manually curated sets of formal statement\ncheaper to formalize than full proofs\n"},"KB/Curse-Of-Dimensionality":{"title":"Curse of Dimensionality","links":["KB/Dimensionality-Reduction"],"tags":["temp"],"content":"Curse of Dimensionality\n\nIn an n dim hypercube → greatest possible distance is \\sqrt{n}\nAka the higher the dimension → wider the training points from each other\nBut there are fewer data points than dimensions and the distances are huge\nDimensionality Reduction\n"},"KB/Cut-and-Delete":{"title":"Cut and Delete","links":[],"tags":["augmentation"],"content":"Cut and Delete\n\ndata augmentation by deleting image patches randomly or semantically.\nlearn in case of occlusions\nThis kind of [dropout] is different from conventional [dropout] because it drops contiguous image regions, whereas values in traditional [dropout](dropout] is different from conventional [dropout] because it drops contiguous image regions, whereas values in traditional [dropout.md) work at noncontiguous locations\n"},"KB/Cut-and-Mix":{"title":"Cut and Mix","links":["KB/Mixup"],"tags":["augmentation"],"content":"Cut and Mix\n\n@yunCutMixRegularizationStrategy2019\ninstead of deleting a patch, the patch is replaced with some other image region\ny this approach, an image shares multiple class labels, whereas the major class label belongs to the original class label\nHence, the model learns to differentiate between two classes within a single image.\nCutMix can be defined by the following operations \\overset{\\sim}x = M \\odot x_{A} + (1-M) \\odot x_{B}\n\\overset{\\sim}y = \\lambda y_{A}+ (1- \\lambda)y_{B}\nwhere x is an RGB image, y is the respective label, M is a binary mask of the patch of the image that will be dropped and \\odot represents element wise multiplication. The new training sample \\overset{\\sim}x , \\overset{\\sim}y is created by combining two other training samples x_{A}, y_{A} and x_{B} , y_{B}. To control the combination ratio \\lambda, a sample from the \\beta(1,1) distribution is chosen. This combination is quite similar to Mixup.\n\n"},"KB/Cut,-Paste-and-Learn":{"title":"Cut, Paste and Learn","links":[],"tags":["augmentation"],"content":"Cut, Paste and Learn\n\n@dwibediCutPasteLearn2017\ngenerates new data by extracting object instances and pasting them on randomly selected background images.\nInstances are blended with various blending approaches, for example, gaussian blurring and poison blending, to reduce pixel artifacts around the augmented object boundaries.\nAdded instances are also rotated, oc- cluded, and truncated to make the learning algo- rithm robust.\n"},"KB/CutMix":{"title":"CutMix","links":[],"tags":["augmentation"],"content":"CutMix\n\n@yunCutMixRegularizationStrategy2019\nimages are augmented by sampling patch coordinates, x, y, h, w from a uniform distribution\nselected patch is replaced at the\ncorresponding location with a patch from the other randomly picked image from the current mini-batch during training.\nM is the image mask, xa and xb are images, λ is the proportion of label, and ya and yb are the labels of images.\n\n\n\n\nx_{new}= M.x_{a}+(1-M).x_{b}\n- $$\ny_{new}= \\lambda.y_{a}+ (1-\\lambda).y_{b}"},"KB/Cutout":{"title":"Cutout","links":[],"tags":["augmentation"],"content":"Cutout\n\n@devriesImprovedRegularizationConvolutional2017\nremoves constant size square patches randomly by replacing them with any constant value.\nThe selection of region is performed by selecting a pixel value randomly and placing a square around it\nCutout can be expressed as an element-wise multiplication operation x_{cutout} = x \\odot M, where x is the original image, M is a binary mask of the same size as x with randomly chosen coordinates of a square patch of pixels to be cut out, and \\odot denotes element-wise multiplication.\n"},"KB/CvT":{"title":"CvT","links":["KB/Vision-Transformer","KB/Conv","KB/Embedding","KB/Transformer","KB/Attention","KB/ImageNet","KB/Position-Encoding"],"tags":["temp"],"content":"\ntoc: true\ntitle: CvT\ntags: [‘temp’]\n\nCvT\n\nCvT: Introducing Convolutions to Vision Transformers\n\nimproves Vision Transformer\nintroducing Conv\na hierarchy of Transformers containing a new convolutional token Embedding\nconvolutional Transformer block leveraging a convolutional projection\nshift, scale, and distortion invariance\ndynamic Attention , global context, and better generalization\nImageNet\nPosition Encoding , a crucial component in existing Vision Transformers, can be safely removed in our model\npotential advantage for adaption\nbuilt-in local context structure introduced by convolutions, CvT no longer requires a position Embedding\n\n\n"},"KB/Cycle-Consistency-Loss":{"title":"Cycle Consistency Loss","links":[],"tags":["loss"],"content":"Cycle Consistency Loss\n\nFor two domains X, Y mapping G: X \\rightarrow Y, F: Y \\rightarrow X\ntrying to enforce the intuition that these mappings should be reverses of each other and that both mappings should be bijections\nEncourages F(G(x)) \\approx x \\text{ and } G(F(y)) \\approx y\nreduces the space of possible mapping functions by enforcing forward and backwards consistency\nL_{cyc}(G,F) = \\mathbb{E}_{x \\sim p_{data}(x)}[||F(G(x))-x)||_{1}] + \\mathbb{E}_{x \\sim p_{data}(y)}[||G(F(x))-x)||_{1}]\n\\mathcal{L}_{cyc}(G, F, X, Y) = \\frac{1}{m}\\Sigma_{i=1}^{m}[(F(G(x_{i})-x_{i})+ (G(F(y_{i}))-y_{i})]\n"},"KB/CycleGAN":{"title":"CycleGAN","links":["KB/Strided","KB/PatchGAN"],"tags":["architecture"],"content":"CycleGAN\n\nUnpaired image2image\n2 Mapping functions G, F : generator and D_{X}, D_{Y} as generators\n[Adversarial Loss](Adversarial Loss.md)\n\\mathcal{L}_{cyc} [Cycle Consistency Loss](Cycle Consistency Loss.md)\nFull objective\n\nAdversarial Loss + [Cycle Consistency Loss](Cycle Consistency Loss.md)\n\\lambda is a hyperparam. Generally set to 10\n\\mathcal{L}_{GAN}(G, F, D_{X}, D_{Y}) = \\mathcal{L}_{GAN}(G, D_{Y}, X, Y) + \\mathcal{L}_{GAN}(F, D_{X}, X, Y) + \\lambda \\mathcal{L}_{cyc}(G,F)\n\n\nTo Solve\nG^{*},F^{*} =\\underset{G,F}{argmin} \\underset{D_{X}, D_{Y}}{max} \\mathcal{L}_{GAN}(G, F, D_{X}, D_{Y})\ntwo stride-2 convolutions, several residual blocks, and two fractionally Strided convolutions with stride \\frac{1}{2}\n[Instance Normalization](Instance Normalization.md)\n\nArchitecture\nGenerator\n\n\n\nEncoder\n\nThe encoder extracts features from the input image by using Convolutions and compressed the representation of image but increase the number of channels\nThe encoder consists of 3 convolution that reduces the representation by 1/4 th of actual image size\n\nTransforming Block\n\nThe transformer contains 6 or 9 residual blocks based on the size of input.\nThe output of transformer is then passed into the decoder which uses 2 -deconvolution block of fraction strides to increase the size of representation to original size.\n\nDiscriminator\n\nPatchGAN\n\nApplications\n\nStyle Transfer\n\nUnlike other works on neural style transfer, CycleGAN learns to mimic the style of an entire collection of artworks, rather than transferring the style of a single selected piece of art\n\n\nObject Transformation\n\nCycleGAN can transform object from one ImageNet class to another such as: Zebra to Horses and vice-versa, Apples to Oranges and vice versa etc\n\n\nSeason Transfer\n\nCycleGAN can also transfer images from Winter Season to Summer season and vice-versa. For this the model is trained on 854 winter photos and 1273 summer photos of Yosemite from Flickr.\n\n\nPhoto Generation from Painting\n\ncan also be used to transform photo from paintings and vice-versa\n[Identity Loss](Identity Loss.md)\n\n\n\nLimits\n\napplied to perform geometrical transformation, CycleGAN does not perform very well. This is because of the generator architecture which is trained to perform appearance changes in the image.\n\nReduce Model Oscillation\n\nTo prevent the model from changing drastically from iteration to iteration, the discriminators were fed a history of generated images, rather than just the ones produced by the latest versions of the generator.\nTo do this we keep storing the 50 most recently generated images. Based on this technique we reduce the model Oscillation as well as model overfitting.\n\nTechnical Implementation\nThe CycleGAN paper provides a number of technical details regarding how to implement the technique in practice.\nThe generator network implementation is based on the approach described for style transfer by Justin Johnson in the 2016 paper toc: true\ntitled “Perceptual Losses for Real-Time Style Transfer and Super-Resolution.”\nThe generator model starts with best practices for generators using the deep convolutional GAN, which is implemented using multiple residual blocks (e.g. from the ResNet).\nThe discriminator models use PatchGAN, as described by Phillip Isola, et al. in their 2016 paper toc: true\ntitled “Image-to-Image Translation with Conditional Adversarial Networks.”\n\nThis discriminator tries to classify if each NxN patch in an image is real or fake. We run this discriminator convolutionally across the image, averaging all responses to provide the ultimate output of D.\n\n— Image-to-Image Translation with Conditional Adversarial Networks, 2016.\nPatchGANs are used in the discriminator models to classify 70×70 overlapping patches of input images as belonging to the domain or having been generated. The discriminator output is then taken as the average of the prediction for each patch.\nThe adversarial loss is implemented using a least-squared loss function, as described in Xudong Mao, et al’s 2016 paper toc: true\ntitled “Least Squares Generative Adversarial Networks.”\n\n[…] we propose the Least Squares Generative Adversarial Networks (LSGANs) which adopt the least squares loss function for the discriminator. The idea is simple yet powerful: the least squares loss function is able to move the fake samples toward the decision boundary, because the least squares loss function penalizes samples that lie in a long way on the correct side of the decision boundary.\n\n— Least squares generative adversarial networks, 2016.\nAdditionally, a buffer of 50 generated images is used to update the discriminator models instead of freshly generated images, as described in Ashish Shrivastava’s 2016 paper toc: true\ntitled “Learning from Simulated and Unsupervised Images through Adversarial Training.”\n\n[…] we introduce a method to improve the stability of adversarial training by updating the discriminator using a history of refined images, rather than only the ones in the current minibatch.\n\n— Learning from Simulated and Unsupervised Images through Adversarial Training, 2016.\nThe models are trained with the Adam version of stochastic gradient descent and a small learning rate for 100 epochs, then a further 100 epochs with a learning rate decay. The models are updated after each image, e.g. a batch size of 1."},"KB/Cyclic-Learning-Rate":{"title":"Cyclic Learning Rate","links":["KB/Saddle-Points"],"tags":["optimizer"],"content":"Cyclic Learning Rate\n\nWith respect to local minima and Saddle Points, one could argue that you could simply walk “past” them if you set steps that are large enough. Having a learning rate that is too small will thus ensure that you get stuck.\nNow, Cyclical Learning Rates - which were introduced by Smith (2017) - help you fix this issue. These learning rates are indeed cyclical, and ensure that the learning rate moves back and forth between a minimum value and a maximum value all the time.\n\n\n\n"},"KB/Cyclo-Drive":{"title":"Cyclo Drive","links":[],"tags":["robotics"],"content":"Cyclo Drive\n\nA brand name for a speed reduction device that converts high speed low torque to low speed high torque, usually used on the major (larger) axis.\n"},"KB/Cylinders":{"title":"Cylinders","links":[],"tags":["visualization"],"content":"Cylinders\n\n\n"},"KB/Cylindrical-Topology":{"title":"Cylindrical Topology","links":[],"tags":["robotics"],"content":"Cylindrical Topology\n\nA topology where the arm follows a radius of a horizontal circle, with a [prismatic joint](prismatic joint.md) to raise or lower the circle. Not popular in industry\n"},"KB/DALL-E-3":{"title":"DALL-E 2","links":["KB/DALL-E","KB/CLIP","KB/Embedding","Tag-Pages/loss","KB/Autoregressive"],"tags":["architecture"],"content":"DALL-E 2\n\nHierarchical Text-Conditional Image Generation with CLIP Latents\nDALL-E 2, generates more realistic and accurate images with 4x greater resolution, better caption matching and photorealism\nContrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style\ntwo-stage model: a prior that generates a CLIP image Embedding given a text caption, and a “unCLIP” decoder that generates an image conditioned on the image Embedding\nexplicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity\ndecoder, which is conditioned on image representations, can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation\ndiffusion models for the decoder and experiment with both Autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples\n"},"KB/DALL-E":{"title":"DALL-E","links":["KB/AdaIn","KB/GPT3","KB/BERT","KB/GPT","KB/Autoregressive"],"tags":["architecture"],"content":"DALL-E\n\nAdaIn\nit was capable of generating text that could not be distinguished from human-written text\nnamed after Salvador Dalí and Pixar’s WALL·E\nbased on the GPT3\nPrevious approaches like BERT and the original GPT model followed the fine-tuning approach.\nGPT-2 and GPT-3 recognized that even while pretraining already provided lots of benefits compared to training from scratch, so-called zero-shot learning - where the model is finetuned and then applied to language tasks, without pretraining - could be the way forward.\nDALL·E is capable of performing a variety of tasks:\n\nControlling attributes, instructing the model what particular attributes of an object should look like. For example: “a collection of glasses is sitting on a table” (OpenAI, 2021). Here, we instruct the model about the glasses, and more precisely, their location.\nDrawing multiple objects is also possible, but is more challenging, because it can be unknown whether certain characteristics belong to one object or another (OpenAI, 2021). DALL·E is however also capable of performing that task, but at the risk of making mistakes - once again due to the issue mentioned previously. The success rate decreases rapidly when the number of objects increases.\nVisualizing perspective and three-dimensionality, meaning that DALL·E can be instructed to take a particular “perspective” when generating the image (OpenAI, 2021).\nVisualizing across many levels, from “extreme close-up” to “higher-level concepts” (OpenAI, 2021).\nInferring context, meaning that particular elements can be added to an image that normally do not belong to a particular context (e.g. the OpenAI logo in the image above; this is normally not displayed on a store front).\n\n\nUses\n\nIndustrial and interior design, to aid designers when creating a variety of household and other objects.\nArchitecture, to guide the creation of buildings and other forms of constructions.\nPhotography, to create an image specifically tailored to one’s requirements.\nGraphic design, with e.g. the creation of a variety of icons.\n\n\nZero-Shot Text-to-Image Generation\nDALL-E which offers a simple approach for text-to-image generation based on an Autoregressive transformer which models the text and image tokens as a single stream of data\nsimple decoder-only transformer that receives both the text and the image as a single stream of 1280 tokens—256 for the text and 1024 for the image—and models all of them autoregressively\nThey find that sufficient data and scale can lead to improved generalization, both in terms of zero-shot performance relative to previous domain-specific approaches\nand in terms of the range of capabilities that emerge from a single generative model.\n"},"KB/DALL·E-2":{"title":"DALL·E 2","links":[],"tags":["architecture"],"content":"DALL·E 2\n\ngenerate original, genuine and realistic images and art from a prompt consisting on a text description\nDALL·E 2 manages to combine concepts, attributes and diferent styles - it uses the CLIP neural network\n"},"KB/DCGAN":{"title":"DCGAN","links":["KB/Tanh","KB/Sigmoid","Tag-Pages/loss"],"tags":["architecture"],"content":"DCGAN\nArchitecture\n\n\n\nWeight Init\n\nIf conv Random Normal with mean = 0 and std.dev = 0.02\nIf BatchNorm with mean = 1.0 and std.dev = 0.02, Bias = 0\n\nGenerator\n\nMap latent space vector z to data space\nCreating RGB with same size as training image\n[Transposed Conv] , [Batch Normalization] and [Relu](Transposed Conv] , [Batch Normalization] and [Relu.md)\nOutput is 3x64x64\nOutput passed through Tanh to return it to [-1,1]\n[Batch Normalization] AFTER [Transposed Conv](Batch Normalization] AFTER [Transposed Conv.md) is super important as it helps with flow of gradients\nNotice, how the inputs we set in the input section (nz, ngf, and nc) influence the generator architecture in code. nz is the length of the z input vector, ngf relates to the size of the feature maps that are propagated through the generator, and nc is the number of channels in the output image (set to 3 for RGB images)\n\n\nDiscriminator\n\n[Strided] [Conv], [Batch Normalization], and [Leaky Relu](Strided] [Conv], [Batch Normalization], and [Leaky Relu.md)\n3x64x64 input\nBinary classification network - outputs prob of real/fake\nFinal is a Sigmoid layer\nFor [downsampling], good Practise to use [Strided] rather than [Pooling](downsampling], good Practise to use [Strided] rather than [Pooling.md) as it lets the network learn it’s own pooling function\nAlmost a direct inverse of the Generator\n\nSpecial Features\n\nExplicitly uses convolutional layers in the discriminator and transposed-convolutional layers in the generator\nFurther the discriminator uses batch norm layers and [Leaky Relu] activations while the generator uses [Relu](Leaky Relu] activations while the generator uses [Relu.md) activations\nThe input is a latent vector drawn from a standard [normal distribution](normal distribution.md) and the output is a 3 \\times 32 \\times 32 RGB image\nIn this implementation, I also added in [Label Smoothing](Label Smoothing.md)\n\nLoss functions\nDiscriminator loss\nThe Discriminator penalizes wrongly classifying a real image as a fake or a fake image as real. This can be thought of as maximizing the following function.\n\\nabla_{\\theta_{d}} \\frac{1}{m} \\Sigma_{i=1}^{m}[log D(x^{(i)}) + log(1-D(G(z^{(i)})))]\nGenerator loss\n\n\nThe Generator loss takes the output of the Discriminator into account and rewards it if the Generator is fooled into thinking the fake image is real. If this condition is not satisfied, the Generator is penalized.\n\n\nThis can be thought of as minimizing the following function.\n\\nabla_{\\theta_{g}} \\frac{1}{m} \\Sigma_{i=1}^{m}log(1-D(G(z^{(i)})))\n\n"},"KB/DICOM":{"title":"DICOM","links":[],"tags":["medical"],"content":"DICOM\n\nMedical Imaging Standards\npydicom\n\npip install pydicom\n\n\n"},"KB/DLRM":{"title":"DLRM","links":["KB/Features","KB/Parallelization","KB/Embedding","KB/Layers"],"tags":["architecture"],"content":"DLRM\n\nDeep Learning Recommendation Model for Personalization and Recommendation Systems\nDLRM\nThe DLRM model handles continuous (dense) and categorical (sparse) Features that describe users and products\nwide range of hardware and system components, such as memory capacity and bandwidth, as well as communication and compute resources\ndesign a specialized Parallelization scheme utilizing model parallelism on the Embedding tables to mitigate memory constraints while exploiting data parallelism to scale-out compute from the fully-connected Layers\nit computes the feature interactions explicitly while limiting the order of interaction to pairwise interactions.\ntreats each embedded feature vector (corresponding to categorical Features) as a single unit, whereas other methods (such as Deep and Cross) treat each element in the feature vector as a new unit that should yield different cross terms\nThese design choices help reduce computational/memory cost while maintaining competitive accuracy\n"},"KB/DT-Tutor":{"title":"DT Tutor","links":[],"tags":["usermodel"],"content":"DT Tutor\n\nDT Tutor (Murray, VanLehn, &amp; Mostow, 2004) implements a version of this ideal tutoring policy.\nThe tutor applies decision theory to make its choice about whether to give a hint.\nFor each tutor action it can make (e.g., to give a hint, and which kind of hint), it uses a probabilistic model of the student to predict all possible student reactions to the tutor’s action and their probability.\nThe predicted student state includes the likelihood of learning, of becoming frustrated, of entering the next step correctly, etc.\nDT Tutor evaluates the utility of each of the predicted student states, multiplies the state’s utility by the state’s probability, and eventually produces the expected utility of each proposed tutor action.\nIt then takes the tutor action with the highest expected utility. Although advances in probabilistic reasoning make it feasible for DT Tutor to perform this calculation in real time, considerable data from human students is needed in order to set the parameters in its model of student learning\n"},"KB/Data-Augmentation-via-Latent-Space-Interpolation-for-Image-Classification":{"title":"Data Augmentation via Latent Space Interpolation for Image Classification","links":["KB/Cropping","KB/CIFAR","KB/CDF","KB/regularize","KB/Regularization"],"tags":[],"content":"Data Augmentation via Latent Space Interpolation for Image Classification\n \n\n@liuDataAugmentationLatent2018\n\nAbstract\n\nstandard data augmentation produces only limited plausible alternative data by for example, flipping, distorting, adding noise to, Cropping(cropping.qmd) a patch from the original samples\nadversarial autoencoder (AAE) to impose the feature representations with uniform distribution and apply the linear interpolation on latent space, which is potential to generate a much broader set of augmentations for image classification\nimproves the generalization and performance of state-of-the-art deep neural networks\nGenerative models are often evaluated by examining samples from the latent space\nTechniques frequently used are random sampling and linear interpolation But often these can result in sampling the latent space from locations very far outside the manifold of probable location In high dimensional space, even with a uniform distribution most points lie on a thin shell in the unit cube. We utilize both the uniform distribution prior to avoid the “hole” of dead zone.\n\n\nWe propose a novel framework to augment the dataset using the interclass interpolation of latent feature representations. 2)uniform distribution are imposed as the prior to avoid “hole” effect of the inter-class interpolation via the adversarial autoencoder network. 3) we explore the linear interpolation on the ILSVRC 2012 and CIFAR(CIFAR.qmd)-10 datasets\n\n\n\nPROPOSED METHOD\n\nlatent space interpolation followed two useful principles when sampling the latent space of a generative model avoid sampling from locations that are highly unlikely given the prior of the model. being used in the original VAE paper which adjusted sampling through the inverse CDF(CDF.qmd) of the Gaussian to accommodate the Gaussian prior recognize that the dimensionality of the latent space is often artificially high and may contains dead zones that are not on the manifold learned during training implies that simply matching the model’s prior will not always be sufficient to yield samples that appear to have been drawn from the training set\nit does not require significant domain knowledge. that interpolation and extrapolation in feature space can improve generalization\nRecent approaches have also proposed to regularize(regularize.qmd) the output distribution of a neural network by label smoothing [27], or penalizing highconfidence softmax distributions [28]These methods bear similarities with mixup in the sense that supervision depends on multiple smooth labels, rather than on single hard labels as in traditional ERMthe label smoothing in these works is applied or regularized independently from the associated feature values. the supervision of every example is not overly dominated by the groundtruth labe\nLSI transformation establishes a linear relationship between data augmentation and the supervision signal.strong regularizer that improves generalization The linearity constraint, through its effect on the\nderivatives of the function approximated, also relates mixup to other methods such as Sobolev training of neural networks [29] or WGAN-GP [30].\nWhen other types of data augmentation are employed in addition to our technique, we can apply them for each image before mixing them into the final image for training\nThe data augmentation incurs additional time to prepare the input image, but this can be done on the CPU while the GPU is executing the training through back propagation\na single data loader to obtain one minibatch, and then latent space interpolation is applied to the same minibatch after random shuffling\n\nUniform distribution representations\n\n(images/Pasted image 20230209131445.png.qmd)\nAdversarial autoencoder (AAE) [31] can be treated as the combination of generative adversarial networks (GANs) [20] and variational autoencoder (VAE)maintains the autoencoder structure like the VAE but replaces the KLdivergence loss with a discriminative network, denoted by Dis\nInstead of generating images from random noise as in GAN, AAE utilizes the encoder part to learn the latent variables approximated on certain prior, making the style of generated images controllable AAE better captures the data manifold compared to VAEinput as x, output as x’, and the distribution of the training data as �!�௧�(ݔ ,(then the distribution of z is q(z|x). Assuming p(z) is a prior distribution, and denotes the random sampling process from p(z). The min-max objective function can be used to� train the Enc and Dis\n\\mathbb{E}_{z* \\sim p(z)}[log(Dis(z*))] \\mathbb{E}_{x \\sim p_{data}(x)}[\\overset{min Enc}{log(1-Dis_{z}(z*))}]\nAs for the Dec, the L2 loss is used as the reconstruction loss\nL_{recon}= ||x-x&#039;||^{2}_{2}\nDis imposes a prior distribution (i.e., uniform distribution) on z Dis aims to discriminate the z generated by encoder Enc Enc will be trained to generate z that could fool Dis\nSuch adversarial process forces the distribution of the generated z to gradually approach the prio uniform distribution as the prior, forcing z to evenly populate the latent space with no apparent “holes”. the generated z’s (depicted by blue dots in a 2-D space) present uniform distribution under the Regularization(regularization.qmd) of Dis, while the distribution of z exhibits a “hole” without the application of Dis Exhibition of the “hole” indicates that the samples generated by interpolating between arbitrary z’s may not lie on the real image manifold – generating unrealistic appearancesnot to generate photorealistic images as in GAN offer more informative and discriminative training sample, the adversarial loss in pixel-level is dropped in here for faster training and processing.\n\nLinear interpolation\n\nThe difference with mixup [17] in here is that the linear interpolation manipulation is conducted in latent space which align the uniform distribution instead of the pixel-level\nThe one-hot vectors are used to label the original samples\nThe loss function for a generated sample are calculated as the weighted sum (via ߣ (of two cross-entropy losses corresponding to both of its original samples\nWhen the 0.5=ߣ, we can simply use two-hot vector to label the generated sample belongs to two of its original samples for faster processing.\n\nCONCLUSION\n\nvicinal risk minimization trains on virtual examples constructed via interpolations of the features in a latent space with uniform distributionSeveral methods are employed to avoid the dead zone in manifold of feature representations.\nthe ILSVRC 2012\nCIFAR(CIFAR.qmd)-10\nvaluable for tasks with a limited number of samples, such as medical image classification tasks\n\nImages\n\n(images/Pasted image 20230209131412.png.qmd)\n(images/Pasted image 20230209131420.png.qmd)\n(images/Pasted image 20230209131428.png.qmd)\n\n"},"KB/Data-Augmentation-with-Curriculum-Learning":{"title":"Data Augmentation with Curriculum Learning","links":[],"tags":["augmentation"],"content":"\n\nData Augmentation with Curriculum Learning\n\nCurriculum learning decisions are especially important for One-Shot Learning systems such as FaceNet\nIn this sense, the concept of curriculum learning shares many similarities with adversarial search algorithms or learning only on hard examples.\noriginally coined by Bengio et al.\nPlotting out training accuracy over time across different initial training subsets could help reveal patterns in the data that dramatically speed up training time.\n"},"KB/Data-Free-Distillation":{"title":"Data Free Distillation","links":[],"tags":["knowledgedistillation"],"content":"Data Free Distillation\n\nJust as “data free” implies, there is no training data. Instead, the data is newly or synthetically generated.\nSpecifically, in (Chen et al., 2019a; Ye et al., 2020; Micaelli and Storkey, 2019; Yoo et al., 2019; Hu et al., 2020), the transfer data is generated by a GAN. In the proposed data-free knowledge distillation method (Lopes et al., 2017), the transfer data to train the student network is reconstructed by using the layer ac- tivations or layer spectral activations of the teacher net- work.\nYin et al. (2020) proposed DeepInversion, which uses knowledge distillation to generate synthesized images for data-free knowledge transfer. Nayak et al. (2019) proposed zero-shot knowledge distillation that does not use existing data.\nThe transfer data is pro- duced by modelling the softmax space using the pa- rameters of the teacher network. In fact, the target data in (Micaelli and Storkey, 2019; Nayak et al., 2019) is generated by using the information from the fea- ture representations of teacher networks.\ndistilling knowl- edge from a teacher model into a student neural network (Kimura et al., 2018; Shen et al., 2021).\ndata distillation, which is similar to data-free distillation (Radosavovic et al., 2018; Liu et al., 2019d; Zhang et al., 2020d). In data distillation, new training annotations of unlabeled data generated from the teacher model are employed to train a student model.\n"},"KB/Data-Structures":{"title":"Data Structures","links":["KB/Grids"],"tags":["visualization"],"content":"Data Structures\n\nGrids\n"},"KB/Data-aug-for-spoken-language":{"title":"Data aug for spoken language","links":["KB/Features","Augmentation"],"tags":["temp"],"content":"Data Aug for Spoken Language\n\nComparing Data Augmentation and Annotation Standardization to Improve End-to-end Spoken Language Understanding Models\nAll-neural end-to-end (E2E) Spoken Language Understanding (SLU) models can improve performance over traditional compositional SLU models, but have the challenge of requiring high-quality training data with both audio and annotations\nthey struggle with performance on “golden utterances”, which are essential for defining and supporting Features, but may lack sufficient training data\nusing data augmentation to compare two data-centric AI methods to improve performance on golden utterances\nimproving the annotation quality of existing training utterances and augmenting the training data with varying amounts of synthetic data\nboth data-centric approaches to improving E2E SLU achieved the desired effect, although data augmentation was much more powerful than annotation standardization.\nleads to improvement in intent recognition error rate (IRER) on their golden utterance test set by 93% relative to the baseline without seeing a negative impact on other test metrics\n"},"KB/Data-storage-for-different-AI-dataset-services":{"title":"Data storage for different AI dataset services","links":["KB/Benchmark-Data-Repositories-for-Better-Benchmarking"],"tags":["dataset"],"content":"Data Storage for Different AI Dataset Services\n \n\nAnalyze data sharing platforms\nBenchmark Data Repositories for Better Benchmarking\n\nTemplate\n\nHow do they store data?\n\nblank\n\n\nWhy is it useful?\n\nblank\n\n\nData upload interface\nData view interface\n\nKaggle\n\nThey started with datasets, and then moved on to other things\nHow do they store data?\n\nPretty much anything, but usually a folder with files and metadata\n\n\nWhy is it useful?\n\ncompetitions\nthey were the pretty much the first ones in this space\ncommunity features (likes etc)\n\n\nData upload interface\n\n\n\n\n\nData view interface\n\n\n\n\n\nHugging Face\n\nStarted with models, integrated with deep learning libraries. they took advantage of the rise in transformer libraries and then supported all of the ones since\nHow do they store data?\n\nFolder of parquet files , with the occasional metadata csv\n\n\nWhy is it useful?\n\neasy to view the dataset directly without downloading\nspaces to run the data directly\ndirectly copy API command\nmany papers published\ngithub LFS upload and discussions\ncommunity features (likes etc)\n\n\nData upload interface\n\n\n\n\n\nData view interface\n\n\n\n\n\nNCBI (Bio Tech info)\n\nSpecific biotech data\nHow do they store data?\n\nA tab delimited file : DataSet SOFT (but saved as a .gz archive)\n\n\nWhy is it useful?\n\nSpecific data views : eg genes\n\n\nData view interface\n\n\n\n\n\n\nU.S. Department of Energy Office of Scientific and Technical Information (Physics)\n\nHow do they store data?\n\nImages are in .zip, tabular in .csv and metadata as .xml\n\n\nWhy is it useful?\nData upload interface\nData view interface\n\nInteresting Features for Us\n\nCommunity features - likes, comments\nNotebooks\nDirectly copy API command\nMore PR ( implementations of recent datasets/models etc)\n"},"KB/Decision-Boundaries":{"title":"Decision Boundaries","links":["KB/Distributions","KB/Probability","KB/PDF","KB/Class-Conditional-distribution"],"tags":["temp"],"content":"Decision Boundaries\n\nMinimal risk decision function is unique and must be represented in terms of Distributions of data generating RVs X and Y\n\nA is some subvolume of P. (n dimensional hypercubes or volume bodies)\nP_{X,Y} is ground truth\n\nFunction that assigns every choice of A \\subseteq P , c \\in C the number P\n\n\n\n\nDecision function h: P \\rightarrow {c_{1}, …, c_{k}} partitions pattern space into k disjoint decision regions R_{1}, …, R_{k} by R_{i}= \\{x \\in P | h(x) = c_{i}\\}\nIf a test pattern falls into R_{i} it is classified as class i\n\nFinding Decision Regions\n\nwhich yields the lowerst misclassification rate or highest Probability of correct classification\nf_{i} be the PDF for Class Conditional distribution\nProbability of obtaining a correct classification for R_{i} is \\Sigma_{i=1}^{k}P(X \\in R_{i}, Y = c_{i})\n\nThis region has curved boundaries aka decision boundaries\n\nFolded and on higher dims : very complex and fragmented\n\n\nx is a vector\nFor patterns on these boundaries, two or more classifications are equally probable\nMaximal if R_{i}= \\{x \\in P| i = argmax_{j} P(Y=c_{j})f_{j}(x)\\}\nThen h_{opt}: P \\rightarrow C_{j}x \\rightarrow c_{argmax_{j}P(Y=c_{j})f_{j}(x)}\nAlgo learns estimates of the Class Conditional distribution and class probabilities aka priors\nThe separator between classes learned by a model in a binary class or multi-class classification problems. For example, in the following image representing a binary classification problem, the decision boundary is the frontier between the orange class and the blue class\n"},"KB/Decision-Trees":{"title":"DT","links":[],"tags":["temp"],"content":"DT"},"KB/Declarative-Memory-Blending":{"title":"Declarative Memory Blending","links":["KB/Softmax"],"tags":["cognitivemodel"],"content":"Declarative Memory Blending\n\nLike “weighted avg”\nstore no of pulses\nactivation decays in time\n\nA(t) = log(t-t_{creation})^{-d}+\\text{mismatchpenalty}\nRetrieval probability\n\nSoftmax\nP_{i}= \\frac{e^{\\frac{A_{t}}{t}}}{\\Sigma_{i}e^{\\frac{A_{t}}{t}}}\n\n\nAdds up to 1\nResult = \\Sigma_{j}P_{j}V_{j}\nt controls noise\n\nif t is high : 1/no of competitors , more prob of retrieval\n\n\nLooking for long interval (partial matching)\nPenalty for short intervals\nApply P_{i}\nWeighted avg Result\n\n\nToo short is positive. else negative, correct is 0\nno of pulses to wait : duration + feedback from memory\n\nFit\n\nExp done on generated data as well\nCompares if same as when run on original\nDoes well with unmodified mode\n\n"},"KB/Declarative-memory":{"title":"Declarative memory","links":[],"tags":["usermodel"],"content":"Declarative Memory\nInfo\n\nAll decisions are based on knowledge\nDepends on frequency and recency of use\nSemantic , episodic\nRepresentation similar to semantic networks\nNo inheritence\nPartially sub-symbolic\nRelated to [Prefrontal cortex](Prefrontal cortex.md)\n\nProgramming\n\n[ACT-R Chunk](ACT-R Chunk.md)\n"},"KB/DeconvNet":{"title":"DeconvNet","links":[],"tags":["explainability"],"content":"DeconvNet\n \n\n@zeilerVisualizingUnderstandingConvolutional2013\nZeiler, Fergus\n\nSummary\n\nDeconvnets are designed to work similar to convolutional networks but reverse (reversing pooling component, reversing filter component etc.), and they can be trained using an unsupervised approach.\nTo reconstruct the activation on a specific layer, we are attaching deconv layers to corresponding CNN layers\nTo examine a reconstruction for a given class c, we have to set all activations except the one responsible for predicting class c to zero.\nThen we can propagate through deconvnet layers and pass all the feature maps as inputs to corresponding layers\nPropagation through the whole deconvnet gives us a representation of the features from the first layer of the original CNN\nThis approach causes the saliency map to feature some biases from the first convolutional layer and the representation looks like a localized edge detector\nworks better when there is a clear distinction in the feature importance rather than similar values for the whole image\nBasically invert operations between input and the chosen layer.\n\nConv → Deconv\nPool → Unpooling\nReLU → ReLU with negative valyes clamped going backward from the activation space to image space\nPooling is non invertible, but uses a switch module : recover positions of maxima in the forward pass\n\n\nDeconvNet is a calculation of a backward convolutional network that reuses the weights at each layer from the output layer back to the input image\nThe employed mechanisms are deconvolution and unpooling, which are especially designed for CNNs with convolutions, max-pooling, and Rectified Linear Units (ReLUs). The method makes it possible to create feature maps of an input image that activates certain hidden units most, linked to a particular prediction\nWith their propagation technique, they identified the most responsible patterns for this output. The patterns are visualized in the input space\nDeconvNet is limited to max-pooling layers, but the unpooling uses an approximate inverse\n\nFiltering\n\nFiltering in the original CNN computes feature maps using learned filters. Reversing that operation requires the use of a transposed version of the same filters. Those transposed filters are then applied to the Rectified Unpooled Maps.\n\nRectification\n\nsame ReLU non-linearity\nsimply just rectifying the values and propagate only non-negative ones to the filtering layer\n\nUnpooling\n\nThe original max-pooling operation is non-invertible, but this approach uses additional variables called switch variables, which are responsible for remembering the locations of the maxima for each pooling region.\n\nImages\n\n\n\n"},"KB/Deductive-Approaches":{"title":"Deductive Approaches","links":[],"tags":["language"],"content":"Deductive Approaches\n\nSettling on one hypothesis by eliminating all others\n"},"KB/Deep-Brain-Stimulation":{"title":"Deep Brain Stimulation","links":[],"tags":["brain"],"content":"Deep Brain Stimulation\n\nA method of treating various neuropsychiatric and neurodegenerative disorders through small, controlled electric shocks administered from a special battery-operated neurostimulation implant. The implant, sometimes called a “brain pacemaker,” is placed within deep brain regions such as the globus pallidus or subthalamus.\n"},"KB/Deep-Generative-Models":{"title":"Deep Generative Models","links":["KB/StarGAN"],"tags":["augmentation"],"content":"Deep Generative Models\n\nThe ultimate goal of data augmentation is to draw samples from the distribution, which represent the generating mechanism of dataset\nthe data distribution we generate data from should not be different with the original one\nThis is the core idea of deep generative models.\nPix2Pix\nCycleGAN\nStarGAN\n[StarGAN v2](StarGAN v2.md)\n"},"KB/Deep-Inside-Convolutional-Networks":{"title":"Deep Inside Convolutional Networks","links":[],"tags":["explainability"],"content":"Deep Inside Convolutional Networks\n\n\nKaren Simonyan, Andrea Vedaldi, Andrew Zisserman\n\n\n@simonyanDeepConvolutionalNetworks2014\n\n\nBecause the word saliency is often related to the whole approach to display input attribution called Saliency Map, this method is also known as Vanilla Gradient\n\n\nfinding L2 regularized image III that maximizes score S_{c}​ for a given class c\n\n\nIt can be written formally as:\n\n\narg \\underset{I}max S_{c}(I) - \\lambda||I||^{2}_{2}\n\n\nWhere \\lambda is a regularisation parameter\n\n\nTo find the value of I, we can use the back-propagation method. Unlike in the standard learning process, we are going to back-propagate with respect to the input image, not the first convolution layer\n\n\nFrom class visualization to Saliency\n\nThis idea can be extrapolated, and with minor modifications, we should be able to query for spatial support of class ccc in a given image I0I_0I0​.\nrank pixels of I_{0}​ in relation to their importance in predicting score S_{c}(I_{0})\nAuthors assume that we can approximate S_{c}(I) with a linear function in the neighborhood of I_{0}\nS_{c}(I) \\approx w^{T}I + b\nFor a pair of input image I_{0} \\in \\mathbb{R}^{m \\times n} and the class c, we are able to compute saliency map A \\in \\mathbb{R}^{m \\times n} (where m and n are the height and width of the input in pixels).\ncompute derivative w and rearrange elements in the returned vector.\nuses different approaches base on the number of channels in the input image I_{0}​.\nFor grey-scale pixels (one color channel), we can rearrange the pixels to match the shape of the image\nIf the number of channels is greater than one, we are going to use the maximum value from each set of values related to the specified pixel.\nA_{i,j}= \\underset{ch}max |w_{h_{(i,j,ch)}}|\nch is a color channel of the pixel (i,j) and h(i,j,ch) is an index of the www corresponding to the same pixel (i,j).\nThe original Saliency method produces a lot of additional noise but still gives us an idea of which part of the input image is relevant when predicting a specific class.\nThis often causes a problem when the object on the image has a lot of details and the model is using most of them to make a prediction.\n\nImages\n\n\n\n"},"KB/Deep-Neural-Networks-are-Easily-Fooled-High-Confidence-Predictions-for-Unrecognizable-Images":{"title":"Deep Neural Networks are Easily Fooled High Confidence Predictions for Unrecognizable Images","links":[],"tags":["explainability"],"content":"Deep Neural Networks Are Easily Fooled High Confidence Predictions for Unrecognizable Images\n\n@nguyenDeepNeuralNetworks2015\nCurrent study\n\nFalse positives\nMAP Elites algorithm\nparallel generation\nDirect encodings\nIndirect encodings\nGradient ascent generation\n\n\n\nGA\n\nPopulation of individuals\nEach individual has a fitness\nMutation makes small edits to specific individuals\nRecombination (not used here)\n\nDirect Encoding\n\nIndividuals are images in pixel space\nFitness is the confidence of the DNN that the individual is a class\nMutations make edits to the pixel values\n\nIndirect Encoding\n\nIndividuals are Compositional Pattern-Producing Networks (CPPNs)\nThe CPPN generates an image\nAll individuals initially have no hidden neurons\nMutations add new neurons to the networks\nMaximize confidence of the network\n\nGradient Ascent\n\nTake the gradient with respect to the image pixel values\nModify the image by moving it in the direction of the gradient\n\nMNIST Results - EAs\n\n\n\nImageNet Results - EAs\n\nHarder to fool\nDifferent runs result into differences in patterns\nRemoving repetitive patterns does not cause a dramatic confidence drop\nGlobal structures are not learned\n\n\n\n\nWhat about a Fooling Class?\n\nMNIST\n\nAdded an 11th fooling class.\nEvolved unrecognizable images were still recognized as digits.\nNumber of misclassifications did not decrease.\n\n\nImageNet\n\nAdded an 1001st fooling class.\nNo decrease in confidence for directly evolved images, but already low confidence.\nConfidence decreased from 88.1% to 11.7% for indirectly evolved images.\n\n\nIndirectly evolved images are easier to differentiate.\n\nGradient Ascent Results\n\nMaximize softmax output\nProduced unrecognizable images classified with 99.99% confidence\n"},"KB/Deep-Visual-Explanation":{"title":"Deep Visual Explanation","links":[],"tags":["explainability"],"content":"Deep Visual Explanation\n\n@babikerIntroductionDeepVisual2018\nThey captured the discriminative areas of the input image by considering the activation of high and low spatial scales in the Fourier space.\n“Deep,” because it is the development and performance of deep neural network models that we want to understand. “Visual,” because we believe that the most rapid insight into a complex multi-dimensional model is provided by appropriate visualization techniques, and “Explanation,” because in the spectrum from instrumentation by inserting print statements to the abductive inference of explanatory hypotheses, we believe that the key to understanding deep learning relies on the identification and exposure of hypotheses about the performance behavior of a learned deep model.\nDeep convolutional neural networks (DCNN) produce spatial information at the convolution layers\nloss of information makes the explanation process challenging, especially when it comes to interpreting the output of sensitive data such as medical images\nOur immediate goal is to create an explanation about the outcome of a DCNN, i.e., to identify which discriminative pixels in the image influence the final prediction\nTo approach this task in this restricted context, we assume that the convolution feature maps X at pooling layer l contain some relevant information about class y\nWe can then write our solution as: D : I \\rightarrow y_{i} \\rightarrow S i.e., map the input I to class y_{i} using network D, and compute the evidence/explanation S\noutput. So to explain y_{i} \\rightarrow S, we can compute the low-spatial scale and high-spatial scale activations of every feature map\n\nVisual Explanation\n\nExplanaton in Fourier domain. Function F(x) represents transform with x \\in \\mathbb{R} , x is a feature map at a conv layer\nFor every x_{i} \\in X of size M \\times N , the transform  can be written as $$\nF(u,v) = \\Sigma_{k=0}^{M-1} \\Sigma_{j=0}^{N-1}f(k,j)e^{-i2\\pi(\\frac{uk}{M}+ \\frac{vj}{N})}\n\n\t- $f(k,j)$ is feature map at layer $l$\n\t- exp term is the basis function\n\t- inverse of fourier is $$\nf(m,n) = \\frac{1}{M\\times N} \\Sigma_{u=0}^{M-1} \\Sigma_{v=0}^{N-1}F(u,v)e^{i2\\pi(\\frac{ux}{M}+ \\frac{vy}{N})}\n\nFor every feature map, x_{i} \\in X, visual explanation is $$\nS = \\Sigma_{i=1}F^{-1}(F(x_{i} * G_{1}) * F^{-1}(F(x_{i})*(1-G_{2})))\n\n\t- $G_{1}, G_{2}$ are Guassians at different $\\sigma$ values,\n\t- Low spatial scale activation $F(x_{i})*G_{1}$\n\t- High scale activation $F^{-1}(F(x_{i})*(1-G_{2}))$\n- ![](../images/Pasted%20image%2020230511183730.png)\n\n## Targeted Deep Visual Explanation\n- In our simple case of image classification (cf. speech, language) one of the ultimate goals of the visual explanation in the context of debugging is to be precise when determining the component salient patch.\n- Therefore, we should penalize any activations that do not contribute much\n- To handle this, we propose a method called targeted-DVE to provide a more targeted explanation. This algorithm removes any pixel that has less influence on the best explanation\n- The process is identical to our previous approach except that, we slightly modify the final output S obtained in Algorithm 1. This is done, by computing S0 as follows\n- $$S&#039; = F^{−1}(F(S) * G_{1}) * F^{−1}(F(S * (1 − G_{2}))\n\nOur approach captures the discriminative pixels by considering the activation of high and low spatial scales in Fourier space.\nWe experimented with a simple version of our approach on image classification.\n\nImages\n\n\n\n\n\n"},"KB/DeepFM":{"title":"DeepFM","links":["KB/Features","KB/Embedding","KB/Feature-Learning"],"tags":["architecture"],"content":"DeepFM\n\nDeepFM: a Factorization-Machine Based Neural Network for CTR Prediction\nLearning sophisticated feature interactions behind user behaviors is critical in maximizing CTR for recommender systems\nexisting methods seem to have a strong bias towards low- or high-order interactions, or require expertise feature engineering\nan end-to-end learning model that emphasizes both low- and high-order feature interactions\nDeepFM is a Factorization-Machine (FM) based Neural Network for CTR prediction, to overcome the shortcomings of the state-of-the-art models and to achieve better performance.\nDeepFM trains a deep component and an FM component jointly and models low-order feature interactions through FM and models high-order feature interactions through the DNN\nDeepFM can be trained end-to-end with a shared input to its “wide” and “deep” parts, with no need of feature engineering besides raw Features.\n\n\nit does not need any pre-training; 2) it learns both high- and low-order feature interactions; 3) it introduces a sharing strategy of feature Embedding to avoid feature engineering\n\n\ncombines the power of factorization machines for recommendation and deep learning for Feature Learning in a new neural network architecture\nCriteo\n"},"KB/DeepFool":{"title":"DeepFool","links":[],"tags":["explainability"],"content":"\n\nDeepFool\n\n\n \n\n[@DeepFool: A Simple and Accurate Method to Fool Deep Neural Networks](@DeepFool: A Simple and Accurate Method to Fool Deep Neural Networks.md)\n\n\n"},"KB/DeepLIFT":{"title":"DeepLIFT","links":[],"tags":["explainability"],"content":"DeepLIFT\n\n@liDeepLIFTDeepLabelSpecific2022\n"},"KB/DeepLearning":{"title":"Index","links":["KB/Features","KB/Fundamentals","KB/Issues","KB/Layers","Architectures","KB/Optimizers","KB/Regularization","LossFunctions","KB/Activation-Functions","KB/Initialization","Augmentation","KB/Uncertainty","KB/Optimizing-Code","KB/Useful-Codes","KB/Federated-Learning","KB/Reinforcement-Learning","Refs","tags/anchor"],"tags":["index","anchor"],"content":"\n\nFeatures\n\n\nFundamentals\n\n\nIssues\n\n\nLayers\n\n\nArchitectures\n\n\nOptimizers\n\n\nRegularization\n\n\nLossFunctions\n\n\nActivation Functions\n\n\nInitialization\n\n\nAugmentation\n\n\nUncertainty\n\n\nOptimizing Code\n\n\nUseful Codes\n\n\nFederated Learning\n\n\nReinforcement Learning\n\n\nRefs\nanchor\n\n"},"KB/DeepNet":{"title":"DeepNet","links":["KB/Layers","KB/DeepNorm","KB/Initialization","KB/Xavier-Initialization"],"tags":["architecture"],"content":"DeepNet\n\nDeepNet: Scaling Transformers to 1,000 Layers\nallows train extremely deep transformers with 1000L+ Layers\nfundamental, effective and simple\ncan be used in any Transformer architecture (encoder, decoder, encoder-decoder) which covers almost all different tasks across AI areas (language, vision, speech, multimodal, and beyond)\nnewly proposed normalization function\nDeepNorm\nIt works alongside a dedicated Initialization scheme based on Xavier Initialization.\nThese two tricks lead to greater stability during the training which allows the authors to scale their modified Transformer architecture (DeepNet) up to 1000 Layers\n"},"KB/DeepNorm":{"title":"DeepNorm","links":["KB/Layer-Normalization"],"tags":["regularization"],"content":"DeepNorm\n\nwhich modifies the residual connection in Transformers\ntheoretical justification of bounding the model update by a constant which makes stable training possible in a principled way\nDeepNorm modifies the residual connection in the Transformer architecture by up-scaling it before performing Layer Normalization\n"},"KB/DeepPERF":{"title":"DeepPERF","links":["KB/Issues","KB/Recall","KB/Transformer"],"tags":["architecture"],"content":"DeepPERF\n\nDeepPERF: a Deep Learning-Based Approach for Improving Software Performance\nPerformance bugs may not cause system failure and may depend on user input, so detecting them can be challenging\nharder to fix than non-performance bugs\nperformance bug detection approaches have emerged to help developers identify performance Issues\nBuilding rule-based analyzers is a non-trivial task, as it requires achieving the right balance between precision and Recall\nOnce developed, maintaining these rules can also be costly\nlarge Transformer model to suggest changes at application source code level to improve its performance\nfirst pretrain the model using masked language modelling (MLM) tasks on English text and source code taken from open source repositories on GitHub, followed by finetuning on millions of performance commits made by .NET developers\nrecommend patches to provide a wide-range of performance optimizations in C# applications\nMost suggested changes involve modifications to high-level constructs like API/Data Structure usages or other algorithmic changes, often spanning multiple methods, which cannot be optimized away automatically by the C# compiler and could, therefore, lead to slow-downs on the user’s side\n"},"KB/Default-mode-network":{"title":"Default mode network","links":["KB/Brain-Areas","KB/Brain-Cortex","KB/Functional-Connectivity","Dorsolateral-Prefrontal-cortex","Anterior-Cingulate","Prefrontal-cortex"],"tags":["temp"],"content":"\ntoc: true\ntitle: Default mode network\ntags: [‘temp’]\n\nDefault Mode Network\n\nBrain is organized into coherent spatio-temporal networks such as this one\nBrain Areas in the Brain Cortex that constantly decreased their activity while performing highly demanding task\nSome studies revealed task-induced activations in the DMN, e.g. when internally directed/self-related cognition is required\nAltered with addictions\nFunctional Connectivity within DMN may predict successful quitting, the intensity of withdrawal-induced craving and the degree of cognitive decline in addictions\nThey ^1 (as key node of the cognitive control network) and the Anterior Cingulate/Prefrontal cortex (as key nodes of the DMN)\n\nFunctional Connectivity associated with individual differences in Internet tendency in healthy young adults , Neuropsychologia"},"KB/Defibrillator":{"title":"Defibrillator","links":[],"tags":["medical"],"content":"Defibrillator\n\nA device that discharges an electric current to the heart to correct cardiac arrhythmia or arrest\n"},"KB/Degrees-of-Freedom":{"title":"Degrees of Freedom","links":["KB/Yaw","KB/Roll"],"tags":["robotics"],"content":"Degrees of Freedom\n\nThe number of independent directions or joints of the robot (R15.07), which would allow the robot to move its end effector through the required sequence of motions. For arbitrary positioning, 6 degrees of freedom are needed: 3 for position (left-right, forward-backward and up- down), and 3 for orientation (Yaw, pitch and Roll).\n"},"KB/DeiT":{"title":"DeiT","links":["KB/Conv","KB/Transformer","KB/Vision-Transformer","KB/Knowledge-Distillation","KB/Distillation-Token","KB/Attention","KB/ImageNet"],"tags":["temp"],"content":"\ntoc: true\ntitle: DeiT\ntags: [‘temp’]\n\nDeiT\n\npaper\nblog\nConv free Transformer, Vision Transformer\ndoes not require very large amount of data\nid:: 62a8a66a-941e-4a6d-918a-bb49cd496b15\nKnowledge Distillation\nteacher-student strategy specific to transformers\nDistillation Token\nConvNet as teacher through Attention\nid:: 62a8a6b2-abf4-4869-934e-c75d05884304\nImageNet\n#+BEGIN_CAUTION\nHeh. Didnt they say no convs?\n#+END_CAUTION\n"},"KB/Delta-Waves":{"title":"Delta Waves","links":[],"tags":["brain"],"content":"Delta Waves\n\n2-4 Hz\nsleep\n\n"},"KB/Demographic-Parity":{"title":"Demographic Parity","links":["KB/visualization"],"tags":["temp"],"content":"Demographic Parity\n\nA fairness metric that is satisfied if the results of a model’s classification are not dependent on a given sensitive attribute.\nFor example, if both Lilliputians and Brobdingnagians apply to Glubbdubdrib University, demographic parity is achieved if the percentage of Lilliputians admitted is the same as the percentage of Brobdingnagians admitted, irrespective of whether one group is on average more qualified than the other.\nContrast with equalized odds and equality of opportunity, which permit classification results in aggregate to depend on sensitive attributes, but do not permit classification results for certain specified ground-truth labels to depend on sensitive attributes. See [“Attacking discrimination with smarter machine learning”](http //research.google.com/bigpicture/attacking-discrimination-in-ml/) for a visualization exploring the tradeoffs when optimizing for demographic parity.\n"},"KB/Dendrites":{"title":"Dendrites","links":[],"tags":["brain"],"content":"Dendrites\n\nShort nerve fibers that project from a neuron, generally receiving messages from the axons of other neurons and relaying them to the cell’s nucleus.\n"},"KB/Denoising-Autoencoder":{"title":"Denoising Autoencoder","links":["KB/Normal-Distribution"],"tags":["architecture"],"content":"Denoising Autoencoder\n\nCorrupt inputs with noise\nSalt pepper noise\nNormal Distribution\nCompare outputs to clean inputs\n$$L(X) = n^{-1}\\Sigma_i||x_i - D(E(\\tilde x))||^2$\n"},"KB/Dense-Net":{"title":"Dense Net","links":["KB/Dense","KB/Res-Net","KB/Conv"],"tags":["architecture"],"content":"Dense Net\n\nGeneralized Res Net\nSkip connections inside the Dense block itself\n![im](images/Dense Skip Connections]\nTransition layer → Dense → 1x1 Conv , 2x2 avg pool → Dense\n"},"KB/Dense-Skip-Connections":{"title":"Dense Skip Connections","links":["KB/Dense","KB/Conv","KB/Relu","KB/Skip-Connection"],"tags":["temp"],"content":"Dense Skip Connections\n\nx_i = F(x_0,x_1 ,… ,x_{i-1})\n\nF : 3x3 Conv + Relu → k feature maps\nno of feature maps : k(i-1) + k_0 where k is growth rate (hyperparam)\n\n\nSkip Connection\n"},"KB/Dense-Vector-Indexes":{"title":"Dense Vector Indexes","links":["KB/HNSW","KB/PQ","KB/IVFADC","KB/Sparse-Encoder-Indexes"],"tags":["architecture"],"content":"Dense Vector Indexes\n\n\nadvanced dense vector indexing techniques to capture the semantic meaning and context of documents more effectively than traditional keyword-based methods in the enterprise knowledge base\n\n\nHNSW\n\n\nPQ\n\n\nIVFADC\n\n\nSparse Encoder Indexes\n\n"},"KB/Dense":{"title":"Dense","links":["KB/LinearRegression"],"tags":["temp"],"content":"Dense\n\nWeighted LinearRegression\nForward\n\nz = W\\cdot x + b , y=g(z)\n\n\nBackward\n\n\\delta = g&#039;(z)\\circ \\nabla_y E\n\\nabla_WE = \\delta \\cdot x^T , \\nabla_bE = \\delta\n\\nabla_xE = W^T\\cdot \\delta\n\n\n"},"KB/Density-estimation-using-real-NVP":{"title":"Density estimation using real NVP","links":["KB/NICE---non-linear-independant-components-estimation","images/268644ad8a5d0ae0a9b1f6c93465616e_MD5.jpeg","images/71485bd3414e589c2d9e74c3f27fe674_MD5.jpeg"],"tags":["distributions","architecture"],"content":"Density Estimation Using Real NVP\n \n\n\nfollow up to NICE - non linear independant components estimation\ndifferent sets of latent variables for different resolutions z = (z^{(1)}, …, z^{(L)})\n\nlatents corresponding to finer details can be left out during inference\n\n\n\nAlso for VAE\n\nEg: Parameterize the approximate posterior in a VAE using a flow for better inference p_\\theta(x|z)p_\\theta(z)\n\nOpen: Pasted image 20241119171440.png\n\n\n\ntransform into something more flexibleOpen: Pasted image 20241119171554.png\n\n"},"KB/Density":{"title":"Density","links":[],"tags":["temp"],"content":"Density\n\nmass / vol\nr - m/V\n"},"KB/Deontological-ethics":{"title":"Deontological ethics","links":[],"tags":["ethics"],"content":"Deontological Ethics\n\nan agent is ethical if and only if it respects obligations, duties and rights related to given situations\nact in accordance to established social norms\n"},"KB/Depth-Efficiency-of-Neural-Networks":{"title":"Depth Efficiency of Neural Networks","links":[],"tags":["deeplearning"],"content":"Depth Efficiency of Neural Networks\n \n\nthere are functions that can be realized by deep networks but not by any shallow network whose capacity is bounded above exponentially.\nit would take an exponentially larger number of units in a shallow network to describe these functions accurately\nEldan &amp; Shamir (2016) showed that when there are multivariate inputs, there is a three-layer network that cannot be realized by any two-layer network if the capacity is sub-exponential in the input dimension.\nLiang &amp; Srikant (2016) show that for a broad class of functions, including univariate functions, shallow networks require exponentially more hidden units than deep networks for a given upper bound on the approximation error\n"},"KB/Depthwise-Separable":{"title":"Depthwise Separable","links":[],"tags":["temp"],"content":"Depthwise Separable\n\nOnly transforms the input once and saves computation → elongate it to more channels\nFrom C → F channels : Use F instances of a 1x1xC filter\n\n"},"KB/Derivational-Morphology":{"title":"Derivational Morphology","links":["KB/Morphology"],"tags":["language"],"content":"Derivational Morphology\n\ncreates new word by changing the POS tag\n"},"KB/Detailed-Balance":{"title":"Detailed Balance","links":["KB/Ergodic","KB/Markov-Chain","KB/Probability","KB/Density","KB/Invariant-Distribution"],"tags":["temp"],"content":"Detailed Balance\n\nTo find a transition kernel T(x|y) for a homogenous, Ergodic Markov Chain\nIf we pick some state x with the Probability given by g and multiply its prob g(x) with the transition Probability Density T(x|y) (weighted by Probability Density of x) then its the same as the reverse weighted transiting Probability Density from y to x\n\\forall x,y \\in \\mathbb{R}^{k}: T(y|x)g(x) = T(x|y)g(y)\nIf T(x|y) has detailed balance wrt g, then it is an Invariant Distribution\n\\int_{\\mathbb{R}^{k}}T(x|y)g(y)dy = \\int_{\\mathbb{R}^{k}}T(y|x)g(x)dy = g(x)\\int_{\\mathbb{R}^{k}}P(y|x)dy = g(x)\n"},"KB/Determiners":{"title":"Determiners","links":[],"tags":["language"],"content":"Determiners\n\nindicate specific object (a, the,that)\n"},"KB/DevOn-AI-dev":{"title":"DevOn AI dev","links":[],"tags":["jobsearch"],"content":"DevOn AI Dev\nAs the days go by, Machine Learning and AI are slowly becoming terms that every company wants to have in their portfolio. While this drive leads to many innovations, most companies are not sure how to do AI “well”. They want to use AI but need clarification on whether it is required, how to handle bias, how to create proper data, or even what models (ML vs. DL) to choose. The job of an AI dev, then, is to provide the key information required to find and fulfill KPIs given any project.\nMy expertise is a combination of data analytics and computer vision, and as of a month ago, I also have a Master in Artificial Intelligence from the University of Groningen. I am familiar with the tools required for basic and advanced AI and analytics from internships, research projects, papers, freelance work, and many personal projects. That being the case, I can step aside from my experience with AI and decide if a project needs another solution in reality. I am familiar with PyTorch, Tensorflow, supervised and unsupervised learning, data preprocessing, and many of the other tools that are required to create successful AI applications. I am familiar with the pipeline, from analyzing data to building the model to deployment.\nThe customer is king, they say, and the first step in having happy clients is understanding what they truly want and then being able to give them a solution they can use. Although I have a lot to learn, I can contribute to any team I get the chance to work with. This job would be the perfect next step for me, and so I hope you give me a chance to work with you and your team."},"KB/DiTransitive-verb":{"title":"DiTransitive verb","links":["KB/Verb","noun"],"tags":["language"],"content":"DiTransitive Verb\n\na Verb has two noun objects\nI cooked a duck for her\n"},"KB/Dialyser":{"title":"Dialyser","links":[],"tags":["medical"],"content":"Dialyser\n\nA machine that replaces the function of the kidneys by removing solutes, excess water and toxins from the blood\n"},"KB/Dialysis":{"title":"Dialysis","links":[],"tags":["medical"],"content":"Dialysis\n\nProcess to filter the blood, usually performed as a result of kidney failure\n"},"KB/Dice-Score":{"title":"Dice Score","links":[],"tags":["loss"],"content":"Dice Score\n\n2 * the Area of Overlap divided by the total number of pixels in both images\n"},"KB/Dictionary-Learning":{"title":"Dictionary Learning","links":["KB/Sparse-Dictionary-Learning-Loss","KB/Features"],"tags":["temp"],"content":"Dictionary Learning\n\nGiven : N unlabeled data points x_i \\in \\mathcal{R}^d\nTo find:\n\nLinear rep of these points based on set of basis vectors\n\nx = \\Sigma_i^k r_i \\cdot d_i = Dr\ndimension d\nr are repr weights corresponding to basis vector d\nD is a dict with basis vectors\nR contains weights. Scalar\n$$||d_i|| \\leq 1$\n\n\n\n\nSparse Dictionary Learning Loss\nAfter learning, these can be used as discriminative Features\n\nExpensive to compute\n\n\n"},"KB/Diffusion-LM":{"title":"Diffusion LM","links":["KB/Autoregressive"],"tags":["architecture"],"content":"Diffusion LM\n\nDiffusion-LM Improves Controllable Text Generation\nControlling the behavior of language models (LMs) without re-training is a major open problem in natural language generation\nnon-Autoregressive language model based on continuous diffusions\nsubstantial departure from the current paradigm of discrete Autoregressive generation\niteratively denoises a sequence of Gaussian vectors into word vectors, yielding a sequence of intermediate latent variables\ncontinuous, hierarchical nature of these intermediate variables enables a simple gradient-based algorithm to perform complex, controllable generation tasks\nsuccessful control of Diffusion-LM for six challenging fine-grained control tasks\n"},"KB/Diffusion-Tensor":{"title":"Diffusion Tensor","links":[],"tags":["visualization"],"content":"Diffusion Tensor\n\n\n"},"KB/Digital-Phenotyping":{"title":"Digital Phenotyping","links":["KB/medical"],"tags":["brain"],"content":"Digital Phenotyping\n\nThe use of data collected from personal electronic devices like smart phones to diagnose and monitor medical and psychiatric conditions.\n"},"KB/Dikes-and-Rivers":{"title":"Dikes and Rivers","links":[],"tags":["cognitivemodel"],"content":"Dikes and Rivers\n\nSubjects alternate producing time intervals of a short and a long duration.\nInitially, short is 2 seconds and long is 3.1 seconds\nThey receive feedback on whether their estimate is within +/- 12.5% of the target, and receive “too short” or “too long” as feedback otherwise.\nAfter a number of trials, the criterion for the long interval starts to change\n\nShort intervals are also effected and vice versa\n\nModel\n\nACTR declarative memory + [Declarative Memory Blending](Declarative Memory Blending.md)\n"},"KB/Dilated-Sliding-Window-Attention":{"title":"Dilated Sliding Window Attention","links":["KB/Sliding-Window-Attention","KB/Layers","KB/Receptive-field"],"tags":["architecture"],"content":"Dilated Sliding Window Attention\n\nAnalgous to dilated CNN\nAssuming a fixed d and w for all Layers, Receptive field is l \\times d \\times w which can reach tens of thousands of tokens even with small values of d\n\n"},"KB/Dimensionality-Reduction":{"title":"Dimensionality Reduction","links":["KB/Features","KB/KMeans","KB/PCA","KB/SOMs","tags/anchor"],"tags":["temp","anchor"],"content":"Dimensionality Reduction\n\nGiven\n\n(x_i)_{i = 1, …,N} raw data points, x_i \\in \\mathbb{R}^n : High dim\n\n\nTo get\n\nLow dim x_i \\in \\mathbb{R}^m where m &lt;n\n\n\nf(x) is composed of m component functions aka Features\n\nf: \\mathbb{R}^n \\rightarrow \\mathbb{R} : scalar characteristic\nm such Features : feature map\n\n(f_1 , …, f_m)&#039; =: f : \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\nmaps input vectors to feature vectors\n\n\n\n\nKMeans\nPCA\nSOMs\n\nanchor"},"KB/Dirac-Delta":{"title":"Dirac Delta","links":["KB/PDF","KB/Point-Distribution"],"tags":["temp"],"content":"Dirac Delta\n-P(X \\in A) = \\begin{cases}1&amp; \\text{if 0}\\in A\\\\[2ex] 0&amp; \\text{if } 0 \\notin A \\end{cases}\n\nP(X \\in A) = \\int_{A}\\delta(x)dx\n\nIn \\mathbb{R}^{n}\n\nit is a PDF which describes prob concentrated in the origin\nMulti Point Distribution → combine dirac deltas\n\n"},"KB/Direct-entropy-minimization":{"title":"Direct entropy minimization","links":["KB/Entropy","Shannon-Entropy"],"tags":["temp"],"content":"Direct Entropy Minimization\n\nOn the source domain we train our model,\nas usual using a supervised loss\nFor the target domain, we do not have annotations and we can no longer use the segmentation loss to train\nsupervision signal that could leverage visual information from the target samples, in spite of the lack of annotations\nconstrain\nto produce high-confident predictions on target samples similarly to source samples\nEntropy loss \\mathcal{L}_{ent} to maximize directly the prediction confidence in the target domain.\nShannon Entropy\n"},"KB/Direct-drive":{"title":"Direct-drive","links":[],"tags":["robotics"],"content":"Direct-drive\n\nJoint actuation, including no transmission elements (i.e., the link is bolted onto the output of the motor.)\n"},"KB/Dirichlet-Distribution":{"title":"Dirichlet Distribution","links":["KB/PDF"],"tags":["distributions"],"content":"Dirichlet Distribution\n\nPDF\nh(\\theta|\\alpha) = \\frac{1}{Z(\\alpha)} \\Pi_{j=1}^{l}\\theta_{j}^{a_{j}-1}\nZ(\\alpha) = \\int_{\\mathcal{H}}\\Pi_{j=1}^{l}\\theta_{j}^{\\alpha_{j}-1}d\\theta is normalization constant. Ensures integral of h over \\mathcal{H} is 1\n"},"KB/Discrete-->-Continuous":{"title":"Discrete -> Continous Transforms","links":["KB/One-hot","KB/Binary-pattern","KB/Linear-scale","KB/Word-Vectors"],"tags":["temp"],"content":"Discrete → Continous Transforms\nOne hot\nBinary pattern\nLinear scale\nWord Vectors"},"KB/Discrete-Cosine-Transform":{"title":"Discrete Cosine Transform","links":["Roam-Highlights","KB/MNIST","KB/Layers","KB/Pooling","KB/Image-Data"],"tags":["temp"],"content":"Discrete Cosine Transform\n\nmachine-learning-articles/cnns-and-feature-extraction-the-curse-of-data-sparsity.md at main · christianversloot/machine-learning-articles #Roam-Highlights\nexpresses a finite sequence of data points in terms of a sum of cosine functions oscillating at different frequencies\n\nyou make the CNN blind to the unique aspects represented by the numbers… despite the fact that they are already in there\nIn my opinion, this can be explained by looking at the internals of a convolutional layer. It works as follows. You specify a number of filters which, during training, learn to recognize unique aspects of the image-like data. They can then be used to classify new samples - quite accurately, as we have seen with raw MNIST data. This means that the convolutional layer already makes your data representation sparser. What’s more, this effect gets even stronger when Layers like Pooling are applied\nBut when you downsample the data first by e.g. applying the DCT, you thus effectively apply sparsening twice. My only conclusion can thus be that by consequence, the convolutional filters can no longer learn the unique aspects within the image-like data, as they are hidden in the data set made compact. Only then, I literally found out why people always suggest to input your Image Data into CNNs as untransformed as possible.\nBesides the architectural differences between them, one must also conclude that CNNs make data essentially sparser while SVMs do not.\n\n\n"},"KB/Disparate-Impact":{"title":"Disparate Impact","links":[],"tags":["temp"],"content":"Disparate Impact\n\nMaking decisions about people that impact different population subgroups disproportionately. This usually refers to situations where an algorithmic decision-making process harms or benefits some subgroups more than others.\nFor example, suppose an algorithm that determines a Lilliputian’s eligibility for a miniature-home loan is more likely to classify them as “ineligible” if their mailing address contains a certain postal code. If Big-Endian Lilliputians are more likely to have mailing addresses with this postal code than Little-Endian Lilliputians, then this algorithm may result in disparate impact.\n"},"KB/Disparate-Treatment":{"title":"Disparate Treatment","links":["KB/Disparate-Impact"],"tags":["temp"],"content":"Disparate Treatment\n\nFactoring subjects’ sensitive attributes into an algorithmic decision-making process such that different subgroups of people are treated differently.\nFor example, consider an algorithm that determines Lilliputians’ eligibility for a miniature-home loan based on the data they provide in their loan application. If the algorithm uses a Lilliputian’s affiliation as Big-Endian or Little-Endian as an input, it is enacting disparate treatment along that dimension.\nContrast with Disparate Impact, which focuses on disparities in the societal impacts of algorithmic decisions on subgroups, irrespective of whether those subgroups are inputs to the model.\nBecause sensitive attributes are almost always correlated with other features the data may have, explicitly removing sensitive attribute information does not guarantee that subgroups will be treated equally. For example, removing sensitive demographic attributes from a training data set that still includes postal code as a feature may address disparate treatment of subgroups, but there still might be Disparate Impact upon these groups because postal code might serve as a proxy for other demographic information.\n"},"KB/Displacement":{"title":"Displacement","links":[],"tags":["physics"],"content":"Displacement\n\n\\Delta x = x_{f}-x_{i}\n"},"KB/Distance-Measures":{"title":"Distance Measures","links":["KB/Euclidean-Distance","KB/Cosine-Similarity","KB/Hamming-Distance","KB/Manhattan-Distance","KB/Chebyshev-Distance","KB/Hausdorff-Distance","KB/Chi-Squared-Distance","KB/Bhattacharya-Distance","KB/Minkowski-Distance","KB/Jaccard-Distance","KB/Haversine-Distance","KB/Sørensen-Dice-Index"],"tags":["loss"],"content":"Distance Measures\n\nEuclidean Distance\nCosine Similarity\nHamming Distance\nManhattan Distance\nChebyshev Distance\nHausdorff Distance\nChi Squared Distance\nBhattacharya Distance\nMinkowski Distance\nJaccard Distance\nHaversine Distance\nSørensen-Dice Index\n"},"KB/DistillBERT":{"title":"DistillBERT","links":["KB/Knowledge-Distillation"],"tags":["architecture"],"content":"DistillBERT\n\nDistilBERT, a Distilled Version of BERT: Smaller, Faster, Cheaper and Lighter\nHuggingface\ngeneral-purpose pre-trained version of BERT\n40% smaller, 60% faster, cheaper to pre-train, and retains 97% of the language understanding capabilities\nKnowledge Distillation during the pre-training phase\ntriple loss combining language modeling, distillation and cosine-distance losses\n"},"KB/Distillation-Algorithms":{"title":"Distillation Algorithms","links":["KB/Adversarial-Distillation","KB/Multi-Teacher-Distillation","KB/Cross-Modal-Distillation","KB/Graph-Based-Distillation","KB/Attention-Based-Distillation","KB/Data-Free-Distillation","KB/Quantized-Distillation"],"tags":["knowledgedistillation"],"content":"Distillation Algorithms\n\nAdversarial Distillation\nMulti Teacher Distillation\nCross Modal Distillation\nGraph Based Distillation\nAttention Based Distillation\nData Free Distillation\nQuantized Distillation\n"},"KB/Distillation-Loss":{"title":"Distillation Loss","links":["KB/Cross-Entropy","KB/Probability","KB/Distributions","KB/Softmax"],"tags":["temp"],"content":"Distillation Loss\n\n\\mathscr{l}(p, softmax(z))+T^{2}\\mathscr{l}(softmax(\\frac{r}{T}), softmax(\\frac{z}{T}))\nNegative Cross Entropy + other\np is the true Probability Distributions\nz,r are outputs of the student and teacher model\nT is the temperature to make Softmax smoother\n"},"KB/Distillation-Schemes":{"title":"Distillation Schemes","links":["KB/Offline-Distillation","KB/Self-Distillation"],"tags":["knowledgedistillation"],"content":"Distillation Schemes\n\nOffline Distillation\nSelf Distillation\n"},"KB/Distillation-Token":{"title":"Distillation Token","links":["KB/Image-Data"],"tags":["temp"],"content":"\ntoc: true\ntitle: Distillation Token\ntags: [‘temp’]\n\nDistillation Token\n\nA learned vector that flows through the network along with the transformed Image Data\ncues the model for its distillation output, which can differ from its class output\nSpecific to Transformers\n"},"KB/Distilling-the-Knowledge-in-a-Neural-Network":{"title":"Distilling the Knowledge in a Neural Network","links":["KB/Dropout","KB/Regularization","KB/Covariance","KB/Clustering"],"tags":["knowledgedistillation"],"content":"Distilling the Knowledge in a Neural Network\n\nGeoffrey Hinton, Oriol Vinyals, Jeff Dean\n\nIntro\n\ncompress the knowledge in an ensemble into a single model which is much easier to deploy\nnew type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse\nUnlike a mixture of experts, these specialist models can be trained rapidly and in parallel\nMany insects have a larval form that is optimized for extracting energy and nutrients from the environment and a completely different adult form that is optimized for the very different requirements of traveling and reproduction\ntraining must extract structure from very large, highly redundant datasets but it does not need to operate in real time and it can use a huge amount of computation. Deployment to a large number of users, however, has much more stringent requirements on latency and computational resources.\nThe cumbersome model could be an ensemble of separately trained models or a single very large model trained with a very strong regularizer such as Dropout\nwe tend to identify the knowledge in a trained model with the learned parameter values and this makes it hard to see how we can change the form of the model but keep the same knowledge.\nlearned\nwhere T is a temperature that is normally set to 1\n\nObservations\n\nThis net achieved 67 test errors whereas a smaller net with two hidden layers of 800 rectified linear hidden units and no Regularization achieved 146 errors\nsoft targets can transfer a great deal of knowledge to the distilled model, including the knowledge about how to generalize that is learned from translated training data even though the transfer set does not contain any translations.\nWhen the distilled net had 300 or more units in each of its two hidden layers, all temperatures above 8 gave fairly similar results\nBut when this was radically reduced to 30 units per layer, temperatures in the range 2.5 to 4 worked significantly better than higher or lower temperatures.\n\nAutomatic [Speech Recognition](Speech Recognition.md)\n\nState-of-the-art ASR systems currently use DNNs to map a (short) temporal context of features derived from the waveform to a probability distribution over the discrete states of a Hidden Markov Model (HMM)\nDNN produces a probability distribution over clusters of tri-phone states at each time and a decoder then finds a path through the HMM states that is the best compromise between using high probability states and producing a transcription that is probable under the language model.\nThere is, however, another important objection to ensembles: If the individual models are large neural networks and the dataset is very large, the amount of computation required at training time is excessive, even though it is easy to parallelize.\n\nJFT\n\nJFT is an internal Google dataset that has 100 million labeled images with 15,000 labels. When we did this work, Google’s baseline model for JFT was a deep convolutional neural network that had been trained for about six months using asynchronous stochastic gradient descent on a large number of cores.\nWhen the number of classes is very large, it makes sense for the cumbersome model to be an ensemble that contains one generalist model trained on all the data and many ‘specialist’ models, each of which is trained on data that is highly enriched in examples from a very confusable subset of the classes (like different types of mushroom)\nThe softmax of this type of specialist can be made much smaller by combining all of the classes it does not care about into a single dustbin class.\nTo reduce overfitting and share the work of learning lower level feature detectors, each specialist model is initialized with the weights of the generalist model\nThese weights are then slightly modified by training the specialist with half its examples coming from its special subset and half sampled at random from the remainder of the training set. After training, we can correct for the biased training set by incrementing the logit of the dustbin class by the log of the proportion by which the specialist class is oversampled.\nIn order to derive groupings of object categories for the specialists, we decided to focus on categories that our full network often confuses.\nEven though we could have computed the confusion matrix and used it as a way to find such clusters, we opted for a simpler approach that does not require the true labels to construct the clusters. In particular, we apply a [clustering] algorithm to the [covariance](clustering] algorithm to the [covariance.md) matrix of the predictions of our generalist model, so that a set of classes Sm that are often predicted together will be used as targets for one of our specialist models, m.\non-line version of the K-means algorithm to the columns of the Covariance matrix, and obtained reasonable clusters\nOne of our main claims about using soft targets instead of hard targets is that a lot of helpful information can be carried in soft targets that could not possibly be encoded with a single hard target.\nIt is even more remarkable to note that we did not have to do early stopping: the system with soft targets simply ‘converged’ to 57%. This shows that soft targets are a very effective way of communicating the regularities discovered by a model trained on all of the data to another model.\nThe specialists that we used in our experiments on the JFT dataset collapsed all of their non-specialist classes into a single dustbin class. If we allow specialists to have a full softmax over all classes, there may be a much better way to prevent them overfitting than using early stopping. A specialist is trained on data that is highly enriched in its special classes.\nThis means that the effective size of its training set is much smaller and it has a strong tendency to overfit on its special classes.\nThe use of specialists that are trained on subsets of the data has some resemblance to mixtures of experts which use a gating network to compute the probability of assigning each example to each expert\nAt the same time as the experts are learning to deal with the examples assigned to them, the gating network is learning to choose which experts to assign each example to based on the relative discriminative performance of the experts for that example.\nUsing the discriminative performance of the experts to determine the learned assignments is much better than simply Clustering the input vectors and assigning an expert to each cluster, but it makes the training hard to parallelize: First, the weighted training set for each expert keeps changing in a way that depends on all the other experts and second, the gating network needs to compare the performance of different experts on the same example to know how to revise its assignment probabilities.\n\nConclusions\n\nThese difficulties have meant that mixtures of experts are rarely used in the regime where they might be most beneficial: tasks with huge datasets that contain distinctly different subsets.\nIt is much easier to parallelize the training of multiple specialists\ne first train a generalist model and then use the confusion matrix to define the subsets that the specialists are trained on\nWe have shown that distilling works very well for transferring knowledge from an ensemble or from a large highly regularized model into a smaller, distilled model.\nFor really big neural networks, it can be infeasible even to train a full ensemble, but we have shown that the performance of a single really big net that has been trained for a very long time can be significantly improved by learning a large number of specialist nets, each of which learns to discriminate between the classes in a highly confusable cluster. We have not yet shown that we can distill the knowledge in the specialists back into the single large net.\n"},"KB/Distributions":{"title":"Distributions","links":[],"tags":[],"content":""},"KB/Distributive-Interpretation-(2)":{"title":"Distributive Interpretation (2)","links":[],"tags":["language"],"content":"Distributive Interpretation (2)\n\nObject takes scope over the subject:\nThree aliens are holding two flags. = (Two flags, and then three aliens hold them)\n"},"KB/Distributive-Interpretation":{"title":"Distributive Interpretation","links":[],"tags":["language"],"content":"Distributive Interpretation\n\nSubject takes scope over the object: Three aliens are holding two flags.\nBoth Np’s are interpreted individually and connected to each other No indicators of how they are connected.\n"},"KB/Distributive-units":{"title":"Distributive units","links":[],"tags":["language"],"content":"Distributive Units\n\neach unit responds to multiple categories\nYou must see the entire pattern over a collection of units, to uniquely categorize an input.\nThe states of individual units are uninterpretable\n"},"KB/Divergence":{"title":"Divergence","links":[],"tags":["visualization"],"content":"Divergence\n\nFlow field transports mass v:\\mathbb{R}^{3}\\rightarrow \\mathbb{R}^{3}\nIncrease/loss of mass at point p\ndiv_v: \\mathbb{R}^{3} \\rightarrow \\mathbb{R} = \\nabla \\cdot v = \\frac{\\partial v_{x}}{\\partial x} + \\frac{\\partial v_{y}}{\\partial y} + \\frac{\\partial v_{z}}{\\partial z}\n\n"},"KB/Divide-Oriented":{"title":"Divide Oriented","links":[],"tags":["visualization"],"content":"Divide Oriented\n\ncorresponds to physical realization (screen,printer), e.g., RGB, CMYK\n"},"KB/Docker-Cheatsheet":{"title":"Docker Cheatsheet","links":[],"tags":["cheatsheets"],"content":"Docker Cheatsheet\nMinimal Workflow\ndocker init\ndocker compose up --build\ndocker compose up --build -d #run_in_background\ndocker compose down #stop\n\nManaging Containers\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNameCommandStarting Containersdocker container start nginxStopping Containersdocker container stop nginxRestarting Containersdocker container restart nginxPausing Containersdocker container pause nginxUnpausing Containersdocker container unpause nginxBlocking a Containerdocker container wait nginxSending SIGKILL Containersdocker container kill nginxSending another signaldocker container kill -s HUP nginxConnecting to an Existing Containerdocker container attach nginxCheck the Containersdocker psTo see all running containersdocker container lsContainer Logsdocker logs infinite‘tail -f’ Containers’ Logsdocker container logs infinite -fInspecting Containersdocker container inspect infiniteInspecting Containers for certaindocker container inspect –format ‘{{ .NetworkSettings.IPAddress }}’ $(docker ps -q)Containers Eventsdocker system events infinitedocker system events infinitedocker container port infiniteRunning Processesdocker container top infiniteContainer Resource Usagedocker container stats infiniteInspecting changes to files or directories on a container’s filesystemdocker container diff infinite\nManage Images\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNameCommandListing Imagesdocker image lsBuilding Imagesdocker build.From a Remote GIT Repositorydocker build github.com/creack/docker-firefoxInstead of Specifying a Context, You Can Pass a Single Dockerfile in the URL or Pipe the File in via STDINdocker build – &lt; DockerfileBuilding and Taggingdocker build -t eon/infinite.Building a Dockerfile while Specifying the Build Contextdocker build -f myOtherDockerfile.Building from a Remote Dockerfile URIcurl example.com/remote/Dockerfile|docker build -f – .Removing an Imagedocker image rm nginxLoading a Tarred Repository from a File or the Standard Input Streamdocker image load &lt; ubuntu.tar.gzSaving an Image to a Tar Archivedocker image save busybox &gt; ubuntu.tarShowing the History of an Imagedocker image historyCreating an Image From a Containerdocker container commit nginxTagging an Imagedocker image tag nginx eon01/nginxPushing an Imagedocker image push eon01/nginx\nRemoving Images\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNameCommandRemoving a Running Containerdocker container rm nginxRemoving a Container and its Volumedocker container rm -v nginxRemoving all Exited Containersdocker container rm $(docker container ls -a -f status=exited -q)Removing All Stopped Containersdocker container rm docker container ls -a -qRemoving a Docker Imagedocker image rm nginxRemoving Dangling Imagesdocker image rm $(docker image ls -f dangling=true -q)Removing all Imagesdocker image rm $(docker image ls -a -q)Removing all Untagged Imagesdocker image rm -f (docker image ls \\|grep “^”\\|awk “{print 3}”)Stopping &amp; Removing all Containersdocker container stop (docker container ls -a -q) &amp;&amp; docker container rm (docker container ls -a -q)Removing Dangling Volumesdocker volume rm $(docker volume ls -f dangling=true -q)Removing all unused (containers, images, networks and volumes)docker system prune -fClean alldocker system prune -a\nDockerfile Commands\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommandDescriptionExampleFROMSpecifies the base image for the buildFROM ubuntu:latestRUNExecutes a command inside the container during build timeRUN apt-get update &amp;&amp; apt-get install -y curlCMDSpecifies the default command to run when the container startsCMD [“npm”, “start”]EXPOSEInforms Docker that the container listens on specific network ports at runtimeEXPOSE 80/tcpENVSets environment variables inside the containerENV NODE_ENV=productionCOPYCopies files or directories from the build context into the containerCOPY app.js /usr/src/app/ADDSimilar to COPY but supports additional features like URL retrieval and decompressionADD example.com/file.tar.gz /usr/src/WORKDIRSets the working directory for subsequent instructionsWORKDIR /usr/src/appARGDefines variables that users can pass at build-time to the builder with the docker build commandARG VERSION=1.0ENTRYPOINTConfigures a container to run as an executableENTRYPOINT [“python”, “app.py”]VOLUMECreates a mount point and assigns it to a specified volumeVOLUME /dataUSERSets the user or UID to use when running the imageUSER appuserLABELAdds metadata to an image in the form of key-value pairsLABEL version=”1.0″ maintainer=”John DoeONBUILDConfigures commands to run when the image is used as the base for another buildONBUILD ADD . /app/src\nVolume Commands\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommandDescriptionExamplevolume createCreates a named volumedocker volume create mydatavolume lsLists the available volumesdocker volume lsvolume inspectDisplays detailed information about a volumedocker volume inspect mydatavolume rmRemoves one or more volumesdocker volume rm mydatavolume pruneRemoves all unused volumesdocker volume prune"},"KB/Document-Triage":{"title":"Document Triage","links":["KB/ASCII","KB/Language-Identification"],"tags":["language"],"content":"Document Triage\n\nCharacters in file must be MACHINE READABLE (Character Encoding)\nCharacter Encoding Identification (ASCII, UNICODE..)\nLanguage Identification (English, French,..)\nText Sectioning\n"},"KB/Dopamine":{"title":"Dopamine","links":["KB/Attention"],"tags":["temp"],"content":"\ntoc: true\ntitle: Dopamine\ntags: [‘temp’]\n\nDopamine\n\nDopamine is chemically expressed as C2H11NO2.\nIt is a neuro-chemical created in various parts of the brain and is critical for all kinds of brain functions including thinking, carrying, sleeping, mood, Attention, motivation, seeking and rewarding.\nThe dopamine is responsible for the feeling of pleasure.\nWhen a person eats, drinks or performs a pleasurable action, dopamine is stimulated in his brain to repeat the action.\nUnexpected rewards increase the activity of dopamine neurons, acting as positive feedback signals for the brain regions associated with the preceding behavior.\nAs learning takes place, the timing of activity will shift until it occurs upon the cue alone, with the expected reward having no additional effect.\n\nAnd should the expected reward not be received, dopamine activity drops, sending a negative feedback signal to the relevant parts of the brain, weakening the positive association\n\n\n"},"KB/Dot-Product-Attention":{"title":"Dot Product Attention","links":["KB/Attention","KB/Multiplicative-Attention","KB/Attention-Alignment","KB/Softmax"],"tags":["architecture"],"content":"Dot Product Attention\n\nLuong et al., 2015\nf_{att}(h_{i}, s_{j}) = h_{i}^{T}s_{j}\nEquivalent to Multiplicative Attention with no trainable weight matrix. Performs better at larger dimensions\nIdentity matrix\nh is hidden state for encoder and s is hidden state for decoder\nA type of Attention Alignment\nFinal scores after Softmax\n\n"},"KB/Double-Descent":{"title":"Double Descent","links":["KB/Inductive-Bias","KB/Curse-Of-Dimensionality"],"tags":["optimizer"],"content":"Double Descent\n\nWhen increasing the model size or the number of epochs, performance on the test set initially improves, then worsens but then again starts to improve and finally saturates.\nThis phenomena is against conventional wisdom, because the test error should not be decreasing again after increasing.\noccurs often in the over-parameterization regime\n\nmodels which have a lot of parameters\nmodels that have huge complexity\n\n\n\nonce the model has enough capacity to drive the training loss to near zero, the model fits the training data almost perfectly\nThis implies that further capacity cannot help the model fit the training data any better; any change must occur between the training points\nInductive Bias\nCurse Of Dimensionality\nIt’s certainly true that as we add more capacity to the model, it will have the capability to create smoother functions. Figures 8.11b–f show the smoothest possible functions that still pass through the data points as we increase the number of hidden units. When the number of parameters is very close to the number of training data examples (figure 8.11b), the model is forced to contort itself to fit the training data exactly, resulting in erratic predictions. This explains why the peak in the double descent curve is so pronounced. As we add more hidden units, the model has the ability to construct smoother functions that are likely to generalize better to new data.\nFirst, the network initialization may encourage smoothness, and the model never departs from the sub-domain of smooth function during the training process. Second, the training algorithm may somehow “prefer” to converge to smooth functions\n"},"KB/Down-Syndrome":{"title":"Down Syndrome","links":[],"tags":["brain"],"content":"Down Syndrome\n\nA genetic disorder characterized by intellectual impairment and physical abnormalities that arises from the genome having an extra copy of chromosome 21.\n"},"KB/Downsampling":{"title":"Downsampling","links":[],"tags":["temp"],"content":"Downsampling\n\nOverloaded term that can mean either of the following\nReducing the amount of information in a feature in order to train a model more efficiently. For example, before training an image recognition model, downsampling high-resolution images to a lower-resolution format.\nTraining on a disproportionately low percentage of over-represented class examples in order to improve model training on under-represented classes. For example, in a class-imbalanced dataset, models tend to learn a lot about the majority class and not enough about the minority class. Downsampling helps balance the amount of training on the majority and minority classes.\n"},"KB/Downstream-Task":{"title":"Downstream Task","links":[],"tags":["semisupervisedlearning"],"content":"Downstream Task\n\ncomputer vision applications that are used to evaluate the quality of features learned by self-supervised learnin\ntraining data are scarce\nIn general, human-annotated labels are needed to solve the downstream tasks.\nin some applications, the downstream task can be the same as the pretext task without using any human-annotated labels.\n"},"KB/DrawBench":{"title":"DrawBench","links":["KB/DrawBench","VQGAN","KB/CLIP","KB/Latent-Diffusion","KB/DALL-E"],"tags":["dataset"],"content":"DrawBench\n\ncomprehensive and challenging benchmark for text-to-image models\nWith DrawBench, we compare Imagen with recent methods including VQGAN+CLIP, Latent Diffusion Models, and DALL-E, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment\n"},"KB/Dreamfusion":{"title":"Dreamfusion","links":[],"tags":["architecture"],"content":"Dreamfusion\n\ntext-to-3D model\nuses a pretrained 2D text-to-image difusion model to perform textto-3D synthesis\nDreamfusion replaces previous CLIP techniques with a loss derived from distillation of a 2D difusion model\nthe difusion model can be used as a loss within a generic continuous optimization problem to generate samples\nsampling in parameter space is much harder than in pixels as we want to create 3D models that look like good images when rendered from random angles\nthis model uses a diferentiable generator - Other approaches are focused on sampling pixels, however, this model instead focuses on creating 3D models that look like good images when rendered from random angles\n"},"KB/Drop-Delivery":{"title":"Drop Delivery","links":["KB/Gravity"],"tags":["robotics"],"content":"Drop Delivery\n\nA method of introducing an object to the workplace by Gravity. Usually, a chute or container is so placed that, when work on the part is finished, it will fall or drop into a chute or onto a conveyor with little or no transport by the robot.\n"},"KB/Dropout":{"title":"Dropout","links":["KB/Dense","KB/Layers","tags/architecture","KB/Co-adaptation"],"tags":["regularization","architecture"],"content":"Dropout\n\nApplied to Dense Layers\nTraining : Randomly (Bernoulli, p = 0.5 say) setarchitecture to 0\nGenerally p = 0.1, 0.5\nTesting: Reweight by p\n\nBecause after training values will increase by 1/(1-p)\n\n\nReduces co dependence between neurons\nDecreases overfitting\nStart with small rate : 20 %\nHelps with small datasets\nReducing Co adaptation by making the presence of other hidden [neurons] unreliable\nThe authors found that there is a trade-off between when Dropout is necessary, and when it’s no longer useful. First, to cover the case where the dataset is extremely small: even Dropout does not improve performance in that case, simply because the dataset size is too small. The same is true for datasets that are large enough: Dropout then does no longer improve the model, but rather, model performance gets worse.\n"},"KB/Dual-memory-Approach":{"title":"Dual-memory Approach","links":[],"tags":["robotics"],"content":"Dual-memory Approach\n\nCombination of several memories specialized for storing different types of data and supporting different functionalities\n\nTriplestores\n\nThe contents of this memory system is semantic in nature (small)\n\nLevelDB\n\nkey-value storage database, operate in RAM (developed by Google)\nInterpretation includes computing spatial relations between objects to keep an updated relational model of the scene around the robot\n"},"KB/Dynamic-Eager-Execution":{"title":"Dynamic Eager Execution","links":["KB/Eager-Execution","KB/Static-Graph-Execution"],"tags":["temp"],"content":"Dynamic Eager Execution\n\noperations are executed immediately\nmore readable code and easier to debug\nlower performance than Static Graph Execution\ngraph architecture can evolve dynamically\n"},"KB/Dynamic-Sparsity":{"title":"Dynamic Sparsity","links":["KB/Sparsity"],"tags":["architecture"],"content":"Dynamic Sparsity\n\ntrain intrinsically sparse neural networks from scratch using only a small proportion of parameters and FLOPs\nDynamic Sparsity enables training sparse models from scratch, hence the training and inference FLOPs and memory requirements are only a small fraction of the dense models.\nmodels built with dynamic Sparsity can be trained from scratch to match their dense counterparts without involving any pre-training or dense training\n"},"KB/Dynamic-visual-attention":{"title":"Dynamic visual attention","links":[],"tags":["explainability"],"content":"\n\nDynamic Visual Attention\n\n[@Dynamic visual attention: searching for coding length increments](@Dynamic visual attention: searching for coding length increments.md)\nIncremental coding length (ICL) using the features of local image patches is proposed to maximise the entropy of the sampled visual features\nunexpected features elicit entropy gain in the perception state and are therefore assigned high energy\nThe probability function of feature activities of this model is updated dynamically\n"},"KB/EEG-Artifacts":{"title":"EEG Artifacts","links":["KB/EEG","KB/ICA"],"tags":["temp"],"content":"EEG Artifacts\n\nICA\n\n\nDrift\n\nA\n\nProbably Disconnected\n\n\n\nPeriodic - Probably From ECG\n\n\nArteries in neck exposed when person is nervous\n"},"KB/EEG-Baseline-Correction":{"title":"EEG Baseline Correction","links":["KB/EEG"],"tags":["temp"],"content":"EEG Baseline Correction\n\nSignal drifts\nAdjust pre stimulus value by averaging across pre stimulus points during baseline period\nSubtract average value from post stimulus period\nPeriod where nothing is going on\n100-200 ms\n\n"},"KB/EEG-Cap":{"title":"EEG Cap","links":["KB/EEG"],"tags":["brain"],"content":"EEG Cap\n\n![im](images/Pasted%20Image%2020220510230806.png|]\n\n"},"KB/EEG-Cluster-Testing":{"title":"EEG Cluster Testing","links":["KB/EEG"],"tags":["temp"],"content":"EEG Cluster Testing\n\nIf done for each point, same test repeated and false positives increase\neg: t test in each electrode\nA result is more believable if it occurs in a set of adjacent channels:\n\nThreshold data of statistical test\nCompute clusters\nThreshold randomized data\nCompute clusters for 100 or so distr of randomized data\nDecide if its rare\n\n\nsumT = \\text{sum of all t stats}\n\n\nThere was a significant difference between easy and more difficult trials between 712 ms post-stimulus and 768 ms post-stimulus. This difference was initially localized to a few central electrodes but over time spread out more posteriorly. This is consistent with previous studies that have shown\n\n\n"},"KB/EEG-Filtering":{"title":"EEG Filtering","links":["KB/EEG","KB/Filtering","KB/Notch-filter"],"tags":["temp"],"content":"EEG Filtering\n\n\nRemove 50/60Hz Notch filter for line noise\nMight introduce distortion\n"},"KB/EEG-Statistical-Analysis":{"title":"EEG Statistical Analysis","links":["KB/EEG","KB/EEG-Cluster-Testing"],"tags":["temp"],"content":"EEG Statistical Analysis\n\nEEG Cluster Testing\n"},"KB/EEG":{"title":"EEG","links":["KB/Pyramidal-cell","KB/Electrode-nomenclature","KB/fMRI","KB/Specificity","KB/EEG-Artifacts","KB/EEG-Filtering","KB/ERP","KB/EEG-Baseline-Correction","KB/EEG-Statistical-Analysis","KB/EEG-Cap"],"tags":["brain"],"content":"EEG\n\nElectrical activity on the surface of the brain\nFrequencies\nPyramidal cell\nElectrode nomenclature\nCheaper than fMRI\nFast signals\nLow anatomical Specificity\n\nCant find where its coming from\nLots of noise\n\n\nEEG Artifacts\nEEG Filtering\nERP\nEEG Baseline Correction\nEEG Statistical Analysis\nEEG Cap\nThis might be related to fMRI\n"},"KB/ELBO-loss":{"title":"ELBO loss","links":["KB/KL-Divergence"],"tags":["loss"],"content":"ELBO Loss\n \n\n\\log p_\\theta(x_{i}) \\geq \\mathbb{E}_{z\\sim q_{\\theta (z | x_{i})}}log(p_\\theta(x_{i}|z))- KL{q_\\theta(z|x_{i})||p(z)}\nKL isKL Divergence\n"},"KB/ELECTRA":{"title":"ELECTRA","links":["KB/GLUE","KB/RoBERTa","KB/XLNet"],"tags":["architecture"],"content":"ELECTRA\n\nELECTRA: Pre-training Text Encoders As Discriminators Rather Than Generators\nPre-training methods such as BERT corrupt the input by replacing some tokens with [MASK] and then train a model to reconstruct the original tokens.\nsample-efficient pre-training alternative task called replaced token detection\nself-supervised task for language representation learning\nInstead of masking the input, their approach corrupts it by replacing some tokens with plausible alternatives sampled from a small generator network\nThen, instead of training a model that predicts the original identities of the corrupted tokens, the key idea is training a discriminative text encoder model to distinguish input tokens from high-quality negative samples produced by an small generator network\nmore compute-efficient and results in better performance on downstream tasks\nparticularly strong for small models\nGLUE\nperforms comparably to RoBERTa and XLNet while using less than 1/4 of their compute and outperforms them when using the same amount of compute.\n"},"KB/ELMO":{"title":"ELMO","links":["LSTM","KB/Embedding"],"tags":["architecture"],"content":"ELMO\n\nDeep Contextualized Word Representations\ncontext-sensitive word embeddings using the [LSTM)](Long Short Term Memory (LSTM|Long Short Term Memory (LSTM|LSTM)](LSTM)](Long Short Term Memory (LSTM|Long Short Term Memory (LSTM|LSTM).md).md)-based Embedding from Language Models (ELMo) architecture\n"},"KB/ERP":{"title":"ERP","links":[],"tags":["temp"],"content":"ERP\n\nEvent related potentials\n-\n\n\n"},"KB/Eager-Execution":{"title":"Eager Execution","links":[],"tags":["temp"],"content":"Eager Execution\n\nA TensorFlow programming environment in which operations run immediately. By contrast, operations called in graph execution don’t run until they are explicitly evaluated. Eager execution is an imperative interface , much like the code in most programming languages. Eager execution programs are generally far easier to debug than graph execution programs.\n"},"KB/Early-Ray-Termination":{"title":"Early Ray Termination","links":[],"tags":["visualization"],"content":"Early Ray Termination\n\nGeneral acceleration idea: neglect regions with irrelevant information\n"},"KB/Early-Stopping-tricks":{"title":"Early Stopping","links":["KB/Regularization"],"tags":["deeplearning"],"content":"Early Stopping\n\nNo of epochs is a hyper parameter : to prevent overfitting\nEarly Stopping is a Regularization technique that improves image classification accuracy by intentionally stopping the training when validation loss increases. Training is stopped as training a model for too many epochs sometimes causes Overfitting.\nIn Early Stopping, the number of epochs becomes a tunable hyperparameter. We continuously store the best parameters during training, and when these parameters no longer change for several epochs, we stop training.\n"},"KB/Earth-Mover's-Distance-(EMD)":{"title":"Earth Mover's Distance (EMD)","links":["KB/PDF"],"tags":["temp"],"content":"Earth Mover’s Distance (EMD)\n\nA measure of the relative similarity between two documents. The lower the value, the more similar the documents.\nSpatial Distance between two PDF\n"},"KB/Eavesdropping":{"title":"Eavesdropping","links":[],"tags":["multitask"],"content":"Eavesdropping\n\nTwo tasks T_{A}, T_{B} , share feature F\n\nF is harder to learn in one task\nOne task can eavesdrop into another ([Hard Parameter Sharing] , [Soft Parameter Sharing](Hard Parameter Sharing] , [Soft Parameter Sharing.md))\nExtra info in each task\n\n\n"},"KB/Edema":{"title":"Edema","links":[],"tags":["medical"],"content":"Edema\n\nSwelling as a result of fluid retention or build-up\n"},"KB/Edge-Graphs":{"title":"Edge Graphs","links":[],"tags":["graph"],"content":"Edge Graphs\n \nEdge Graphs\n\neach edge in the original graph becomes a node, and every two edges with a common node in the original graph create an edge in the new graph\n"},"KB/Edge-Prediction-Tasks":{"title":"Edge Prediction Tasks","links":[],"tags":["graph"],"content":"Edge Prediction Tasks\n \nEdge Prediction Tasks\n\nThe network predicts whether or not there should be an edge between nodes n and m\n\n"},"KB/Effect-Of-Depth":{"title":"Effect of Depth","links":[],"tags":["temp"],"content":"Effect of Depth\n\nAdding skip connections make the loss surface smoother\n\n\nDeeper Architectures\n\nMakes more uneven and chaotic\n\n\nWider Architectures\n\nMakes landscape smoother and flatter\n\n"},"KB/Effects-of-Contextual-Cues-on-Inferring-and-Remembering-Meanings-of-New-Word":{"title":"Effects of Contextual Cues on Inferring and Remembering Meanings of New Word","links":["KB/Recall"],"tags":["usermodel"],"content":"Effects of Contextual Cues on Inferring and Remembering Meanings of New Word\n\nxiaolongli\nLi, X. (1988). Effects of contextual cues on inferring and remembering meanings of new words. Applied linguistics, 9(4), 402-413.\n\nAbstract\n\nThis study tested four directional hypotheses: Compared with those receiving cue- inadequate sentences, subjects receiving cue-adequate sentences will (1) report greater ease in word inference, and (2) score higher in inferring and remembering the contextual meanings of unfamiliar words. (3) Contextual cues being equally adequate, subjects reading, in contrast to listening to, the sentences will better infer and remember the contextual meanings ofunfamiliar words. (4) The higher the scores of word inference, the better the retention of the contextual meanings of the target word\nAn approach combining schema theory and the generative model of comprehension was usedfor the rationale of this study and the discussion of its findings.\n\nLITERATURE REVIEW\n\nInferring, or ‘inferencing’, the meanings of unfamiliar words in context can be seen as ‘a process of identifying and acquiring’ new vocabulary by utilizing ‘attributes and contexts that are familiar’\nIn language learning, inferring word meanings while reading or listening is a process of vocabulary acquisition which has an important influence upon comprehension either in a first language (Kruse 1979) or in a second language (Yorio 1971).\nContextual cues can affect the process and outcome of word inference.\nCarton (1971) hypothesized that in the process of identifying and acquiring unfamiliar words in context, greater certainty results from guesses based on many cues than on few.\nHowever, for contextual cues to be of real help for word inference, they must (1) be perceptually and conceptually familiar to the text-receiver, and (2) contain the information available for the text-receiver to find the relevant schemata in order to (a) account for the oncoming input in the text, and (b) identify unfamiliar stimuli in context.\nMemories are, in a sense, natural effects of the comprehension process (Rumelhart and Ortony 1977) which, by nature, is schematic (Bartlett 1932\nmemory performance is enhanced to the extent that the encoding context forms an integrated unit with the to-be-remembered word\n\nTHIS STUDY: PURPOSE AND HYPOTHESES\n\nThe present study was conducted among second language learners\nIt not only focused on the effects of cue adequacy on inferring and remembering the meanings of new words in discrete, semantically disconnected sentences, but also aimed at an empirical exploration concerning the relationship between word inference and retention.\nthis study compared the effects of cue adequacy in both reading and listening contexts\ncue-adequate sentences were compared with their cue- inadequate counterparts for testing four directional hypotheses. These were: Compared with those receiving cue-inadequate sentences, subjects receiving cue-adequate sentences will (1) report greater ease in inferring the meanings of new words, and (2) score higher in inferring and remembering the meanings of new words. (3) Contextual cues being equally adequate, subjects reading, in contrast to listening to, the sentences will better infer and remember the meanings of unfamiliar words. (4) The higher the scores of word inference, the better the retention of the meanings of the target words.\na sentence with certain input information that contains clues sufficient for inferring the contextual meaning of a target word was defined as a cue-adequate sentence, while a sentence without such input information was defined as a cue-inadequate one\nFor example, the sentence John took out a collapsible bicycle and rode to school was treated as a cue- inadequate sentence, for, in this sentence, there was no input information signaling any clue to the contextual meaning of the target word collapsible\nHowever, the sentence John took out a collapsible bicycle, unfolded it, and rode to school was treated as a cue-adequate one, for the word unfolded provided the clue to the approximate meaning of the target word.\n\nMETHOD\nParticipant\n\nForty-eight advanced trainees from an EAP (English for Academic Purposes) center in China were involved in this study.\nTheir average age was 35 (SD = 7), ranging from 22 to 48\nThey were randomly assigned into four treatment groups, namely\nLC— (i.e. listening group with inadequate cues); RC— (i.e. reading group with inadequate cues); LC + (i.e. listening group with adequate cues), and RC+ (i.e. reading group with adequate cues). Of the four groups, two (i.e. LC— and RC—) received cue-inadequate sentences, and the others (i.e. LC+ and RC+), cue- adequate sentences. In each pair of groups that received the same sentences, one group (i.e. LC— and LC+) took the listening test, and the other (i.e. RC— and RC+), the reading test\nThere were two independent variables. The first one, text, had two levels- sentences with adequate cues versus inadequate cues. The second one, language skills, also had two levels—reading versus listening dependent\nThese were group means in terms of: (1) measures of word inference (i.e. inferring the meanings of unfamiliar words); (2) ratings of degrees of difficulty of word inference, and (3) measures of word retention (i.e. Recall of the inferred meanings of the target words).\n\nTask\n\nSixty discrete, semantically disconnected sentences were constructed for the experiment.\nThey formed two sets of counterparts. Each set was composed of 30 sentences.\nOne set consisted of cue-adequate sentences, and the other of cue-inadequate sentences\nA target word was defined as a perceptually, not conceptually, unfamiliar term.\nSince the target words were only perceptually unfamiliar, it would not be a prerequisite for the subjects to acquire any new concept to perform the task for this experiment.\nBy the same token, the topic of all the test items was based on common knowledge; thus, there was no need to turn to any biased or specialized frame of reference for inferring word meanings in this experiment\nFurthermore, no meaning of any target word for this study could be deduced simply by applying morphological knowledge in terms of stems, affixes, or other devices of word formation.\nIn the pretest, the subjects were asked to write down (either in English or Chinese) the common meanings of the target words they knew.\nThree more tasks were performed after the pretest. The first one, word inference, was to infer the contextual meanings of the target words based on the input information in the sentences in which the target words were embedded.\nBoth tapes, one containing sentences with adequate, and the other sentences with inadequate cues, were produced by a native English speaker at a speed of about 90 words per minute.\nThe subjects listened to the sentences one by one, with each sentence repeated three times.\nSentences were shown one by one on the screens by using a mask, and presented at the same rate as the corresponding items on the tapes for the listening groups.\nThe tests were presented in an open-ended, not in a multiple-choice, form\nAfter reading or listening to each sentence, the subjects were asked to state (either in English or Chinese) their guesses of the contextual meaning of the target word in the sentence\nThe second task after the pretest was to rate the degrees of difficulty in terms of word inferences\nThe last task, word retention, was a cued Recall of the target words’ inferred contextual meanings.\nEach target word was cued by another word from the same sentence that had been processed for inferring the contextual meaning of the target word.\nThe target words were listed in exactly the same order as they appeared in the tests for word inference.\n\nDATA ANALYSES\n\nHypothesis 1 was tested by a Chi-square, Hypothesis 2 by two separate one-way ANOVAs, Hypothesis 3 by two separate Duncan’s Multiple Range Tests, and Hypothesis 4 by a Correlation Test\nSince the Chi-square is a test especially designed for nominal data, it was decided beforehand that the nine-point scale should be dichotomized: ratings less than 5 were defined as ‘difficult’ (to infer the contextual meanings of the target words from the discrete sentences), while ratings equal to or greater than 5 were defined as ‘easy’.\nCronbach’s Alpha was used for computing the test reliability. Reliability coefficients for the four tests of word inference were 0.60 for LC—, 0.54 for RC-, 0.68 for LC+, and 0.64 for RC+, which were rather low.\nHowever, they can be considered as being acceptable for this study, for both the sample size (12 per cell) and the number of test items (30 for each test) were very small.\n\nRESULTS\n\nData analyses indicated that all the four hypotheses were confirmed with statistical significance.\n\nHypothesis 1\n\nno significant difference between the four groups in rating degrees of difficulty of word inference\nresult of the Chi-square Test presented null hypothesis could be rejected\n\nHypothesis 2\n\nsubjects in the four groups performed differently on both tasks of word inference and word retention\nshowed that RC+ scored significantly higher than LC+, and that both RC+ and LC+\nscored significantly higher than RC— and LC—\nHowever, there was no significant difference between RC— and LC—\nshowed both RC+ and LC+ scored significantly higher than RC— and LC—, and that R C + scored significantly higher than LC+. However, there was no significant difference between RC— and LC-.\n\nHypothesis 3\n\nboth in word inference and retention, R C + scored significantly higher than LC+; however, in either word inference or retention, there was no significant difference between RC— and LC—\n\nHypothesis 4\n\nthere was a positive correlation of statistical significance between word inference and word retention\n\nDISCUSSION AND CONCLUSION\n\nubjects receiving cue- adequate sentences, in contrast to cue-inadequate sentences, not only reported greater ease in word inference, but also scored significantly higher in inferring and remembering the meanings of unfamiliar words in context\nexisted a positive correlation between word inference and word retention. That is, the higher the group means in inferring the contextual meanings of unfamiliar words, the better the performance in remembering the meanings of those words\ncontextual cues being equally adequate, subjects reading the sentences scored significantly higher in both inferring and remembering the contextual meanings of unfamiliar words than those listening to the sentences.\nThis finding further sustained Carton’s (1971) hypothesis that texts with adequate contextual cues minimize errors in the process of identifying and acquiring new words in a natural context\nThe presence of contextual cues means ‘bridging information’ (Garrod and Sanford 1981), grammatical and/or semantic, conceptual as well as perceptual.\nWithout adequate bridging information, it would seem next to impossible to infer and Recall the contextual meaning of any unfamiliar word.\nThis explains why the LC— and RC— groups scored so low on both word inference and word retention.\nthe target words associated with more powerful retrieval cues were more recallable than those associated with less powerful retrieval cues\nProbably, more powerfully associated retrieval cues better triggered the schematic memory, which created a ‘short cut’ that linked the process needed for recalling the contextual meaning of the target word and the initial process involved in inferring the contextual meaning of that target word.\ncontextual cues being equally adequate (not inadequate), subjects in the reading group scored significantly higher in both word inference and word retention than subjects in the listening group.\nnot clear why this was so\nThat is, the subjects might be more competent in reading than in listening contextual cues presented visually were more accessible\n\nImplications\n\nirst, since adequate cues in context can relieve learners of English as a second language from the anxiety of unfamiliar words, it might follow that reasonably sufficient contextual cues should be provided in texts for second language learners, so that enough information can be created for them to play the ‘psychoUnguistic guessing game’ (Goodman 1983)\ncontextual cues can enhance inferring and remembering the meanings of unfamiliar words in context,\nsince contextual cues being equally adequate, subjects can, within the same amount of time, better acquire vocabulary through visual patterns of learning than through oral patterns, it might follow that learners whose learning styles are congruent or similar to the subjects in this study, may well enlarge their vocabulary for reading in a more efficient way through visual ways of learning\n\nPictures\n\n\n"},"KB/Effects-of-Regularization":{"title":"Effects of Regularization","links":["KB/Regularization","Augmentation","Weight-Decay","KB/Cross-Validation","KB/Transfer-Learning","KB/ImageNet"],"tags":["regularization"],"content":"Effects of Regularization\n\nThe Effects of Regularization and Data Augmentation are Class Dependent\nCurrent Deep Networks heavily rely on regularizers such as data Augmentation (DA) or Weight Decay, and employ structural risk minimization, i.e., Cross Validation, to select the optimal Regularization hyper-parameters\nweight decay increases the average test performances at the cost of significant performance drops on some specific classes\nunfair across classes\nBy focusing on maximizing aggregate performance statistics we have produced learning mechanisms that can be potentially harmful, especially in Transfer Learning tasks\noptimal amount of DA or weight decay found from cross-validation leads to disastrous model performances on some classes\nonly by introducing random crop DA during training\nsuch performance drop also appears when introducing uninformative Regularization techniques such as weight decay\nur search for ever increasing generalization performance – averaged over all classes and samples – has left us with models and regularizers that silently sacrifice performances on some classes.\nvarying the amount of Regularization employed during pre-training of a specific dataset impacts the per-class performances of that pre-trained model on different downstream tasks e.g. an ImageNet pre-trained ResNet50 deployed on INaturalist sees its performances fall from 70% to 30% on a particular classwhen introducing random crop DA during the ImageNet pre-training phase\ndesigning novel regularizers without class-dependent bias remains an open research question\nCategories largely identifiable by color or texture (for e.g., yellow bird, textured mushroom) are unaffected by aggressive [cropping], while categories identifiable by shape (for e.g., corkscrew) see a performance degradation with aggressive [cropping](cropping], while categories identifiable by shape (for e.g., corkscrew) see a performance degradation with aggressive [cropping.md) that only contains part of the object\nConversely, color jitter does not affect shape or texture-based categories (for e.g., zebra), but affects color-based categories (for e.g., basket ball)\n"},"KB/Efferent":{"title":"Efferent","links":["KB/Somatic","KB/Autonomic"],"tags":["brain"],"content":"Efferent\n\nMotor Division\nInstructions from brain to muscle and glands\nAlso includes Somatic + Autonomic\n"},"KB/EfficientNet":{"title":"EfficientNet","links":[],"tags":["architecture"],"content":"EfficientNet\n\n@tanEfficientnetRethinkingModel2019\n"},"KB/Ego-motion":{"title":"Ego-motion","links":[],"tags":["semisupervisedlearning"],"content":"Ego-motion\n\nself-driving car\nequipped with various sensors\nlarge-scale egocentric video along with ego-motor signal can be easily collected with very low cost by driving the car in the street\nthe correspondence between visual signal and motor signal for self-supervised feature learning\ncorrespondence between visual signal and motor signal for s\nunderline intuition of this type of methods is that a self-driving car can be treated as a camera moving in a scene\negomotion of the visual data captured by the camera is as same as that of the car\ncorrespondence between visual data and egomotion can be utilized for self- supervised feature learning\ninputs to the network are two frames sampled from an egocentric video within a short time\nlabels for the network indicate the rotation and translation relation between the two sampled images which can be derived from the odometry data of the dataset.\nConvNet is forced to identify visual elements that are present in both sampled images.\nego-motor signal is a type of accurate supervision signal\nIn addition to directly applying it for self-supervised feature learning, it has also been used for unsupervised learning of depth and ego-motion\n"},"KB/EigenCAM":{"title":"EigenCAM","links":[],"tags":["explainability"],"content":"EigenCAM\n\n@banymuhammadEigenCAMVisualExplanations2021\n\nSummary by Me\nAnother method for computing Saliency Maps without modifying the architecture of the network, EigenCAM was proposed by Bany et al. \\cite{banymuhammadEigenCAMVisualExplanations2021}. EigenCAM uses a combination of an Eigen analysis of the class activated output by projecting it on the input, and a PCA of it to remove unnecessary features from the maps. The Eigen-Saliency map is computed across the network and produces sharper outputs based on the distance (using PCA) from the input image. EigenCAM and Eigen Saliency maps were fused by a point wise multiplication operation."},"KB/Eigenvector":{"title":"Eigenvector","links":[],"tags":["visualization"],"content":"Eigenvector\n\n\n\n"},"KB/Einsum":{"title":"Einsum","links":[],"tags":["einsum"],"content":"Einsum\n\nMatrix transpose\n\nij → ji\n\n\nSum\n\nij →\n\n\nColumn sum\n\nij → j\n\n\nRow sum\n\nij → i\n\n\nMatrix vector multiply\n\nik, k → i\n\n\nMatrix matrix multiply\n\nik, kj → ij\n\n\nDot product\n\ni,i →\nij, ij →\n\n\nHadmard product\n\nij, ij → ij\n\n\nOuter product\n\ni,j → ij\n\n\nBatch matrix multiply\n\nijk, ikl → ijl\n\n\nTensor Contraction\n\npqrs , tuvr → pstuv\n\n\n"},"KB/Elaborateness":{"title":"Elaborateness","links":[],"tags":["explainability"],"content":"Elaborateness\n\ndepth or detail of the explanation\na doctor may tell a patient that their diagnosis looks “similar to” another diagnosis\nThis is a “shallow” explanation; it does not point to the root cause of the diagnosis\n"},"KB/Electrical-Energy":{"title":"Electrical Energy","links":[],"tags":["temp"],"content":"Electrical Energy\n\nElectrical energy changed into heat = potential difference x current x time\nE= \\frac{V}{t}\n"},"KB/Electroconvulsive-Therapy-(ECT)":{"title":"Electroconvulsive Therapy (ECT)","links":[],"tags":["brain"],"content":"Electroconvulsive Therapy (ECT)\n\nA therapeutic treatment for depression and other mental illnesses that sends small electric currents over the scalp to trigger a brief seizure.\n"},"KB/Electrode-nomenclature":{"title":"Electrode nomenclature","links":[],"tags":["temp"],"content":"5---\ntoc: true\ntitle: Electrode nomenclature\n\nElectrode Nomenclature\n\n\nBathing cap\nTop of head\nCz : middle of head\nLeft : odd\nRight : even\nFrontal : F\nFrontal Polar : Fp\nTemporal : T\nPosterior : Pz\nOccipital : Oz\n"},"KB/Elements-of-sets":{"title":"Elements of sets","links":[],"tags":["language"],"content":"Elements of Sets\n\nThe stickers we have in stocks are stars, the moons, item and a flag.\nI’ll take two moons.\nThe moons in the 2nd sentences should be understood to be some of the moons mentioned in the 1st sentence.\nNotice that to understand the 2nd sentence at all requires that we use the context of the first sentence to establish that the word ‘moons’ means moon stickers.\n"},"KB/Elementwise-Flows":{"title":"Elementwise Flows","links":["KB/coupling-flows"],"tags":["architecture"],"content":"Elementwise Flows\n \n\nsimplest nonlinear flow\n\n\n\nHowever, in practice, elementwise flows are used as components of more complex layers like coupling flows\n"},"KB/Ellipsoids":{"title":"Ellipsoids","links":["KB/Fractional-Anisotropy"],"tags":["visualization"],"content":"Ellipsoids\n\nFractional Anisotropy\n\nLinear, Planar, Spherical\n2D projection can convey ambiguous 3D orientation\n"},"KB/Elman-1990":{"title":"Elman 1990","links":["KB/Euclidean-Distance","KB/Filtering","KB/Features","KB/Basic-RNN-Architectures"],"tags":["language"],"content":"Elman 1990\n\nThe network learned generalizations\nexamine hidden unit activation pattern for each word measure distance between each pattern and every other pattern (Euclidean Distance)\nUse this to create a hierarchical cluster.\nNetwork learned semantic classes\nIf the input to a simulation is preselected to avoid problems, one has instantiated an expert Filtering system.\nin order to accomplish the goal of creating word classes by surface structure alone, it appears that the input must be filtered in just the right way.\nInstead of semantic representations, semantics gets replaced with distributional information\n\nThis is not what humans know about word classes.\nIf the simulation’s goals are accomplished by avoiding pronouns then we have the equivalent of a pronoun filter\n\n\nSome strings in English are both nouns and verbs, e.g. smell, break\nThe simulation did not learn what children learn\nYes, the input was oversimplified, but it’s not clear that adding these additional Features will make the model perform worse\nlanguage is very redundant, so certain simplifications actually remove helpful Features\nCategories can ‘emerge’ via statistical regularities\nBasic RNN Architectures can find these\n"},"KB/Elman-1991":{"title":"Elman 1990","links":["KB/Euclidean-Distance","KB/Filtering","KB/Features","KB/Basic-RNN-Architectures"],"tags":["language"],"content":"Elman 1990\n\nThe network learned generalizations\nexamine hidden unit activation pattern for each word measure distance between each pattern and every other pattern (Euclidean Distance)\nUse this to create a hierarchical cluster.\nNetwork learned semantic classes\nIf the input to a simulation is preselected to avoid problems, one has instantiated an expert Filtering system.\nin order to accomplish the goal of creating word classes by surface structure alone, it appears that the input must be filtered in just the right way.\nInstead of semantic representations, semantics gets replaced with distributional information\n\nThis is not what humans know about word classes.\nIf the simulation’s goals are accomplished by avoiding pronouns then we have the equivalent of a pronoun filter\n\n\nSome strings in English are both nouns and verbs, e.g. smell, break\nThe simulation did not learn what children learn\nYes, the input was oversimplified, but it’s not clear that adding these additional Features will make the model perform worse\nlanguage is very redundant, so certain simplifications actually remove helpful Features\nCategories can ‘emerge’ via statistical regularities\nBasic RNN Architectures can find these\n"},"KB/Elman-1992":{"title":"Elman 1990","links":["KB/Euclidean-Distance","KB/Filtering","KB/Features","KB/Basic-RNN-Architectures"],"tags":["language"],"content":"Elman 1990\n\nThe network learned generalizations\nexamine hidden unit activation pattern for each word measure distance between each pattern and every other pattern (Euclidean Distance)\nUse this to create a hierarchical cluster.\nNetwork learned semantic classes\nIf the input to a simulation is preselected to avoid problems, one has instantiated an expert Filtering system.\nin order to accomplish the goal of creating word classes by surface structure alone, it appears that the input must be filtered in just the right way.\nInstead of semantic representations, semantics gets replaced with distributional information\n\nThis is not what humans know about word classes.\nIf the simulation’s goals are accomplished by avoiding pronouns then we have the equivalent of a pronoun filter\n\n\nSome strings in English are both nouns and verbs, e.g. smell, break\nThe simulation did not learn what children learn\nYes, the input was oversimplified, but it’s not clear that adding these additional Features will make the model perform worse\nlanguage is very redundant, so certain simplifications actually remove helpful Features\nCategories can ‘emerge’ via statistical regularities\nBasic RNN Architectures can find these\n"},"KB/Elman-1993":{"title":"Elman 1990","links":["KB/Euclidean-Distance","KB/Filtering","KB/Features","KB/Basic-RNN-Architectures"],"tags":["language"],"content":"Elman 1990\n\nThe network learned generalizations\nexamine hidden unit activation pattern for each word measure distance between each pattern and every other pattern (Euclidean Distance)\nUse this to create a hierarchical cluster.\nNetwork learned semantic classes\nIf the input to a simulation is preselected to avoid problems, one has instantiated an expert Filtering system.\nin order to accomplish the goal of creating word classes by surface structure alone, it appears that the input must be filtered in just the right way.\nInstead of semantic representations, semantics gets replaced with distributional information\n\nThis is not what humans know about word classes.\nIf the simulation’s goals are accomplished by avoiding pronouns then we have the equivalent of a pronoun filter\n\n\nSome strings in English are both nouns and verbs, e.g. smell, break\nThe simulation did not learn what children learn\nYes, the input was oversimplified, but it’s not clear that adding these additional Features will make the model perform worse\nlanguage is very redundant, so certain simplifications actually remove helpful Features\nCategories can ‘emerge’ via statistical regularities\nBasic RNN Architectures can find these\n"},"KB/Elu":{"title":"Elu","links":[],"tags":["architecture"],"content":"Elu\n\nf(x) = max(x, a \\cdot (e^x-1))\n\n"},"KB/Embedding-Human-Knowledge-into-Deep-Neural-Network-via-Attention-Map":{"title":"Embedding Human Knowledge into Deep Neural Network via Attention Map","links":["KB/IDRiD","KB/CAM","KB/Grad-CAM","KB/Density"],"tags":["explainability"],"content":"Embedding Human Knowledge into Deep Neural Network via Attention Map\n\n@mitsuharaEmbeddingHumanKnowledge2019\n\nIntro\n\nfocus on the attention mechanism of an attention branch network (ABN)\npropose a fine-tuning method that utilizes a single-channel attention map which is manually edited by a human expert.\nOur fine-tuning method can train a network so that the output attention map corresponds to the edited ones. As a result, the fine-tuned network can out- put an attention map that takes into account human knowl- edge\nImageNet\nCUB-200-2010\nIDRiD\nhuman intuitive edit- ing via a visual interface and suggest new possibilities for human-machine cooperation in addition to the improvement of visual explanations.\nTypical visual explanation approaches in- clude class activation mapping (CAM) and Grad-CAM\nHowever, an inconsistency between the tar- get region of the recognition result, namely the ground truth (GT), and an attention region may occur.\nTo this end, we focus on the visual explanation and the attention mechanism of ABN\nABN applies an atten- tion map for visual explanation to the attention mechanism.\nWe propose a fine-tuning method based on the characteristics of ABN and an edited attention map\nThe proposed method fine-tunes the attention and perception branches of ABN to output the same attention map as the edited one.\n\nRelated Work\nEmbedding Human Knowledge\n\nhuman-in-the- loop (HITL)\nBranson et al. [4] pro- posed an interactive HITL approach that helps to train a decision tree by using a question and answer with respect to a specific bird.\nDeng et al. [7] used a bubble, that is, a circular bounding box, as human knowl- edge. This bubble information is annotated from an atten- tion region when a user distinguishes the two types of birds. By annotating the bubble with various pairs and users, char- acteristic regions of bird images can be obtained when we recognize bird categories.\nLinsley et al. [18] proposed a method that incorpo- rates human knowledge into large-scale deep neural net- works using the HITL framework. This method added a spatial attention mechanism into the attention mecha- nism [19, 15, 13, 2, 20, 35, 33, 37, 39, 40] of squeeze-and- excitation networks (SENet) [13] and trained the network by using a ClickMe map that introduces human knowledge to the weights of the attention mechanism.\n\nEditing the Attention Map\n\nIn this experiment, we used an ABN whose backbone is 152-layer ResNet [12] (ResNet152+ABN) as a network mode\nThen, we selected the 1k misclassified samples from the validation samples and edited the maps\nbtain the attention map from the at- tention branch, where the size of the attention map is 14×14 pixels. Then, we edit the obtained attention map manually. Note that the attention map is resized to 224×224 pixels and is overlaid with the input image for ease of manual editing. The edited attention map is resized to 14 × 14 pixels and used for an attention mechanism to infer classification re- sults from the perception branch.\nBy training the attention and percep- tion branches with the edited attention map including hu- man knowledge, ABN can output an attention map that con- siders this knowledge and thereby improve the classification performance.\nDuring the fine-tuning process, we update the parameters of the attention and perception branches by using the loss calculated from the attention map obtained from ABN and the edited attention map in addition to the loss of ABN\nTo make an attention map from the bubbles, we use a kernel Density estimation with multiple bubbles\nA dense region of bubbles indicates an impor- tant region for recognizing the bird category.\nIn contrast, the proposed method highlights the local characteristic regions, such as the color and the head of the bird. In addition, the proposed method removes noise from the attention map by fine-tuning. Thus, the proposed method can also improve the performance of fine-grained recognition.\nConsequently, our method can gener- ate a more interpretable attention map and successfully em- bed human knowledge.\n\nFine Tuning Branches\n\nx_i is the i-th sample\n\nL_{abn}(x_{i})=L_{att}(x_{i})+L_{per}(x_{i})\n- where $L_{arr}, L_{per}$ are conventional cross entropy losses for the attention and perception branches, respectively\n\nL(x_{i})=L_{abn}(x_{i})+L_{map}(x_{i})\n\nEdited map : M’\n\nL_{map}(x_{i})=\\gamma||M&#039;(x_{i})-M(x_{i})||_{2}\n\t- $\\gamma$ is a scale factor\n\t- $L_{map}$ is larger than the others, hence needs to be scaled\n\nImages\n\n\n\n\n\n"},"KB/Embedding-ethical-principles-in-collective-decision-support-systems":{"title":"Embedding ethical principles in collective decision support systems","links":[],"tags":["ethics"],"content":"Embedding Ethical Principles in Collective Decision Support Systems\n\nJoshua Greene, Francesca Rossi, John Tasioulas, Kristen Brent Venable, and Brian Williams.\nauthors envisioned a possible way forward to enable human-agent collectives [Jennings et al., 2014] to make ethical collective decisions\nBy imbuing individual agents with ethical decision-making mechanisms (such as those mentioned in the previous section), a population of agents can take on different roles when evaluating choices of action with moral considerations in a given scenario\nBased on a set of initial ethics rules, more complex rules can be acquired gradually through learning.\nTheir evaluations, manifested in the form of preferences and limited by feasibility constraints, can be aggregated to reach a collective decision\nneed for new forms of preference representation in collective ethical decisionmaking\npotential candidate actions to choose from can vastly outnumber the number of agents involved which is very different from multi-agent voting scenarios.\ncandidate actions may not be independent from each other, some of them may share certain features which describe their ethical dilemma situations\n"},"KB/Embedding":{"title":"Embedding","links":[],"tags":["temp"],"content":"Embedding\n\nMore complex than 1 hot\nLookup table is an example.\n\ntoken\\_embedding(i) = gather(W, i)\n\n\nSay vocabulary is (the cat walks)\n\nEmbedding vector v that will be learnt\nValues like : v_{the}, v_{cat}, v_{walks}\n\n\n"},"KB/Embolism":{"title":"Embolism","links":[],"tags":["medical"],"content":"Embolism\n\nA clot caused by blood, fat, air or other types of fluid, gas or foreign material\n"},"KB/Emergentism":{"title":"Emergentism","links":[],"tags":["language"],"content":"Emergentism\n\nWhat seems symbolic emerges from distributed representations\n\nqualitatively newand more complex structures can emerge from simpler, basic facts\nLanguage structure can emerge from simply listening and producing speech\nStructural properties of language, e.g. part-of-speech (nouns, verbs) can emerge from serial order, distributional properties and procedural memory.\n\n\n"},"KB/Emperical-Risk":{"title":"Emperical Risk","links":["KB/Decision-Boundaries"],"tags":["temp"],"content":"Emperical Risk\n\nTRAINING ERROR. Mean loss computed over training examples\nR(f) = \\mathbb{E} _{(X,Y) \\sim P(X,Y)}[l(y, f(x))]\nR^{emp}(h) = \\frac{1}{N}\\Sigma_{i=1}^{N}L(h(x_{i}), y_{i})\njoint prob distribution P(X\\in A,Y=c) is unknown\n\nDecision Boundaries\n\n\nLearning set \\mathcal L is finite\nNeed an estimator to evaluate it\n\nSupervised Learning\n\nCompute \\mathcal L_{train}\nRisk train = (1/M)(sum of loss values for (y, f(x)))\nThis is an unbiased estimator, so we can use it to approximate the optimal function f* that minimizes \\mathbb{R}\nThis means that we find argmin_{f\\in F} \\hat R(f, \\mathcal{L}_Train) (out of all the possible functions)\nlim_{M\\rightarrow \\infty}(f^*_{\\mathcal{L}_Train}) = f^* : converges to the fn that minimizes emprical risk\n\n\nOrdinary least squares regression\n\n\n"},"KB/Enabling-Device":{"title":"Enabling Device","links":[],"tags":["robotics"],"content":"Enabling Device\n\nA manually operated device which when continuously activated, permits motion. Releasing the device shall stop robot motion and motion of associated equipment that may present a hazard.\n"},"KB/Encoder-Decoder-Attention":{"title":"Encoder Decoder Attention","links":["KB/Attention"],"tags":["architecture"],"content":"Encoder Decoder Attention\n\nQ comes from prev decoder\nK,V from encoder\n"},"KB/Encodings":{"title":"Encoding","links":["KB/Discrete-->-Continuous","KB/Continous-->-Discrete"],"tags":["temp"],"content":"Encoding\nDiscrete → Continuous\nContinous → Discrete"},"KB/End-effector":{"title":"End-effector","links":["KB/Wrist","KB/Gripper"],"tags":["robotics"],"content":"End-effector\n\nAn accessory device or tool, specifically designed for attachment to the robot Wrist or tool mounting plate to enable the robot to perform its intended task. (Examples may include: Gripper, spot weld gun, arc weld gun, spray point gun or any other application tools.)\n"},"KB/Endoscope":{"title":"Endoscope","links":[],"tags":["medical"],"content":"Endoscope\n\nAn optical instrument containing a tube with a lighted end used for internal examinations\n"},"KB/Endpoint":{"title":"Endpoint","links":["KB/Manipulator"],"tags":["robotics"],"content":"Endpoint\n\nThe nominal commanded position that a Manipulator will attempt to achieve at the end of a path of motion. The end of the distal link.\n"},"KB/Eneco-Data-Scientist":{"title":"ABN Amro AI Dev","links":[],"tags":["jobsearch"],"content":"Eneco Motivation Letter - Subhaditya\nAs I write this motivation letter, the temperatures in NL have hit yet another record for the hottest September ever. One of the biggest reasons for this is global warming, and carbon emissions are a massive component. Eneco’s mission to be carbon neutral and help customers shift towards sustainable energy sources greatly resonates with me, so I am applying for this position as a Data Scientist.\nMy expertise is a combination of data analytics and computer vision, and as of a month ago, I also have a Master in Artificial Intelligence from the University of Groningen. I am familiar with the tools required for basic and advanced AI and analytics from internships, research projects, papers, freelance work, and many personal projects. In my previous internship at KPMG, I built 10+ dashboards using PowerBI for a project with the Abu Dhabi government. In other internships, I have made several analytics pipelines and machine learning implementations.\nIn any team, it is essential that each of the members can contribute something to the overall development of the projects. Although I have a lot to learn, I genuinely enjoy developing solutions that have a positive impact, and I can contribute quite a bit to any team I get the chance to work with. If there is anything I do not know, I am also ready to develop those skills quickly."},"KB/Energy-Transferred-in-a-Component":{"title":"Energy Transferred in a Component","links":[],"tags":["temp"],"content":"Energy Transferred in a Component\n\ncharge passing through it x potential difference acorss it\nW = QV\n"},"KB/English-Wikipedia":{"title":"English Wikipedia","links":[],"tags":["dataset"],"content":"English Wikipedia"},"KB/Ensemble-Distillation":{"title":"Ensemble Distillation","links":["KB/MNIST"],"tags":["architecture"],"content":"Ensemble Distillation\n\nDistilling the Knowledge in a Neural Network\ntraining many different models on the same data and then to average their predictions\nmaking predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets\ncompress the knowledge in an ensemble into a single model\nMNIST\nensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse\nspecialist models can be trained rapidly and in parallel\ndistillation works remarkably well even when the transfer set that is used to train the distilled model lacks any examples of one or more of the classes\nperformance of a single really big net that has been trained for a very long time can be significantly improved by learning a large number of specialist nets, each of which learns to discriminate between the classes in a highly confusable cluster.\n"},"KB/Ensemble-of-Shape-Functions":{"title":"Ensemble of Shape Functions","links":[],"tags":["robotics"],"content":"Ensemble of Shape Functions\n\nESF\n\n"},"KB/Entities-involving-in-actions":{"title":"Entities involving in actions","links":[],"tags":["language"],"content":"Entities Involving in Actions\n\nHer house was broken into last week.\nThey took the TV and the stereo.\nThe pronoun ‘they’ should be recognized as referring to the burglars who broke into the house.\n"},"KB/Entourage-Plot":{"title":"Entourage Plot","links":[],"tags":["visualization"],"content":"Entourage Plot\n\n\n"},"KB/Entropy-minimization-by-adverarial-learning":{"title":"Entropy minimization by adverarial learning","links":["KB/Entropy"],"tags":["temp"],"content":"Entropy Minimization by Adverarial Learning\n\nA limitation of the Entropy loss is related to the absence of structural dependencies between local semantics.\nThis is caused by the aggregation of the pixel-wise prediction entropies by summation.\nunified adversarial training framework which minimizes indirectly the Entropy of target data, by encouraging it to become similar to the source one.\nminimizing distribution distance between source and target on the weighted self-information space\nWe perform the adversarial adaptation on weighted self-information maps using a fully-convolutional discriminator network\nthe discriminator produces domain classification outputs, i.e., class label for the source (resp. target) domain.\ndiscriminate outputs coming from source and target images, and at the same time, train the segmentation network to fool the discriminator.\n"},"KB/Entropy":{"title":"Entropy","links":["KB/Uniform-Distribution"],"tags":["uncertainty"],"content":"Entropy\n\nMeasure of information content\nH = -\\Sigma_{x}P(x)logP(x) = \\Sigma_{x}P(x)log \\frac{1}{P(x)}\nUnits : bits of log_{2}\nUniform Distribution maximizes entropy. Results harder to predict\n"},"KB/Ependymal-Cell":{"title":"Ependymal Cell","links":["CSF"],"tags":["brain"],"content":"Ependymal Cell\n\nLine cavities\nCreate, secrete and circulate Cerebrospinal Fluid (CSF)).md)\n"},"KB/Epigenetics":{"title":"Epigenetics","links":[],"tags":["brain"],"content":"Epigenetics\n\nA subset of genetics that focuses on how specific environmental factors can influence where, when, and how a gene is expressed, resulting in variation in the gene’s related traits.\n"},"KB/Epilepsy":{"title":"Epilepsy","links":[],"tags":["brain"],"content":"Epilepsy\n\nA neurological disorder characterized by abnormal electrical activity in the brain, leading to seizures.\n"},"KB/Epistemic":{"title":"Epistemic","links":["KB/Uncertainty"],"tags":["uncertainty"],"content":"Epistemic\n\nUncertainty produced by the model\nClass imbalance etc\nReduce by adding more info\n\n"},"KB/Equal-And-Opposite-Force-Pairs":{"title":"Equal And Opposite Force Pairs","links":["KB/Force"],"tags":["physics"],"content":"Equal And Opposite Force Pairs\n\n“When one body exerts a Force on a second body, the second body simultaneously exerts a Force equal in magnitude and opposite in direction on the first body.”\nF_{1}= -F_{2}\n"},"KB/Equality-of-Opportunity":{"title":"Equality of Opportunity","links":[],"tags":["temp"],"content":"Equality of Opportunity\n\nfairness\nA fairness metric that checks whether, for a preferred label (one that confers an advantage or benefit to a person) and a given attribute, a classifier predicts that preferred label equally well for all values of that attribute. In other words, equality of opportunity measures whether the people who should qualify for an opportunity are equally likely to do so regardless of their group membership.\nFor example, suppose Glubbdubdrib University admits both Lilliputians and Brobdingnagians to a rigorous mathematics program. Lilliputians’ secondary schools offer a robust curriculum of math classes, and the vast majority of students are qualified for the university program. Brobdingnagians’ secondary schools don’t offer math classes at all, and as a result, far fewer of their students are qualified. Equality of opportunity is satisfied for the preferred label of “admitted” with respect to nationality (Lilliputian or Brobdingnagian) if qualified students are equally likely to be admitted irrespective of whether they’re a Lilliputian or a Brobdingnagian.\nFor example, let’s say 100 Lilliputians and 100 Brobdingnagians apply to Glubbdubdrib University, and admissions decisions are made as follows\n"},"KB/Equalized-Odds":{"title":"Equalized Odds","links":[],"tags":["temp"],"content":"Equalized Odds\n\nA fairness metric that checks if, for any particular label and attribute, a classifier predicts that label equally well for all values of that attribute.\nFor example, suppose Glubbdubdrib University admits both Lilliputians and Brobdingnagians to a rigorous mathematics program. Lilliputians’ secondary schools offer a robust curriculum of math classes, and the vast majority of students are qualified for the university program. Brobdingnagians’ secondary schools don’t offer math classes at all, and as a result, far fewer of their students are qualified. Equalized odds is satisfied provided that no matter whether an applicant is a Lilliputian or a Brobdingnagian, if they are qualified, they are equally as likely to get admitted to the program, and if they are not qualified, they are equally as likely to get rejected.\n"},"KB/Equations-of-motion":{"title":"Equations of motion","links":[],"tags":["physics"],"content":"Equations of Motion\n\n\\Delta x is displacement\n\\Delta t  is time\nv is final velocity\nu is initial velocity\na is acceleration\nv = u + a\\Delta t\n\\Delta x = u \\Delta t + \\frac{1}{2}a \\Delta t^{2}\n\\Delta x = \\frac{1}{2}(v+u) \\Delta t\nv^{2}= u^{2}+2a \\Delta x\n"},"KB/Equivalent-Current-Dipole":{"title":"Equivalent Current Dipole","links":[],"tags":["temp"],"content":"Equivalent Current Dipole\n\n\nGenerates an electric field\n\nPerpendicular is Magnetic field - MEG\n"},"KB/Equivariance-and-Invariance-for-Graphs":{"title":"Equivariance and Invariance for Graphs","links":[],"tags":["graph"],"content":"Equivariance and Invariance for Graphs\n \nEquivariance and Invariance for Graphs\n\nif we permute the node indices, the node embeddings at each stage will be permuted in the same way.\n\n"},"KB/Eraneous":{"title":"Eraneous","links":[],"tags":["jobsearch"],"content":"Eraneous\nHey Naiyara,\nThis is Subhaditya, I just finished my masters in AI and am now looking for an AI job in the NL. I found some really cool projects from Eraneos and was wondering if there were any opportunities to work there.\nIt would be awesome to have a chat :)\nBest,\nSM"},"KB/Ergodic":{"title":"Ergodic","links":["KB/Invariant-Distribution","KB/Distributions"],"tags":["temp"],"content":"Ergodic\n\nIf only one Invariant Distribution\nSequence of Distributions g^{(n)} converges to g from any initial distribution\nAsymptotic, stationary, equilibrium distribution\n"},"KB/Ethical-dilemmas":{"title":"Ethical dilemmas","links":[],"tags":["ethics"],"content":"Ethical Dilemmas\n\nsituations in which any available choice leads to infringing some accepted ethical principle and yet a decision has to be made\n"},"KB/Euclidean-Distance":{"title":"Euclidean Distance","links":["KB/Features","KB/Curse-Of-Dimensionality"],"tags":["loss"],"content":"Euclidean Distance\n\nd = \\sqrt{\\Sigma_{i=1}^{n}(p_{i}-q_{i})^{2}}\nIt is a distance measure that best can be explained as the length of a segment connecting two points.\ncalculated from the cartesian coordinates of the points using the Pythagorean theorem\nEuclidean distance is not scale in-variant which means that distances computed might be skewed depending on the units of the Features. Typically, one needs to normalize the data before using this distance measure.\nMoreover, as the dimensionality increases of your data, the less useful Euclidean distance becomes. This has to do with the Curse Of Dimensionality\nworks great when you have low-dimensional data and the magnitude of the vectors is important to be measured\n"},"KB/Eugenics":{"title":"Eugenics","links":[],"tags":["brain"],"content":"Eugenics\n\nA 19th century scientific theory that advocated for selective mating of people with desirable hereditary traits.\n"},"KB/Euler-Integration":{"title":"Euler Integration","links":["KB/First-order-integration","KB/Midpoint-Method","KB/Runge-Kutta"],"tags":["visualization"],"content":"Euler Integration\n\n\\frac{dx}{dt} = f(x,t), x(t_{0}) = x_{0}\nFirst order integration\nMidpoint Method\nRunge Kutta\n"},"KB/Eulerian-Grid":{"title":"Eulerian Grid","links":[],"tags":["visualization"],"content":"Eulerian Grid\n\nFocus on domain\nProperties given on a grid\n(Position of particles is implicit)\n\n"},"KB/Europarl-ST":{"title":"Europarl-ST","links":[],"tags":["dataset"],"content":"Europarl-ST\n\nmultilingual speech translation corpus\nbased on speeches and debates in the European parliament between 2008-2013\nCreative Common Non-Commercial license\ndata belongs to the European Union\nby releasing, authors want to improve speech translation\nconsists of audio, transcript and translation of the transcript\n72 translation directions\nincludes also noisy samples\n"},"KB/Even-angels-need-the-rules-AI,-roboethics,-and-the-law":{"title":"Even angels need the rules AI, roboethics, and the law","links":[],"tags":["ethics"],"content":"Even Angels Need the Rules AI, Roboethics, and the Law\n\nUgo Pagallo\n[Collective Ethical Decision Frameworks](Collective Ethical Decision Frameworks.md)\n"},"KB/Evidence-For-Distributivity-Effects-in-Comprehension":{"title":"Evidence For Distributivity Effects in Comprehension","links":[],"tags":["language"],"content":"Evidence For Distributivity Effects in Comprehension\n\nNikole D. Patson and Tessa Warren\n\nIntro\n\nIn the current paper, we introduce a new methodology for detecting whether a word in a sentence is conceptually represented as plural and use it to shed light on a debate about whether comprehenders interpret singular indefinite noun phrases within a distributed predicate as plural during on-line reading.\nself-paced reading on a sentence presented in one- and two-word chunk\nindicated that participants were slower to judge that one word was on the screen when the word was plural (e.g., cats) than when it was singular (e.g., cat)\nbuild different conceptual representations for distributed versus collective predicates, and interpret a singular indefinite noun phrase within a distributed predicate as plura\n\nResults\n\nWhen the group was conceptually distributed (5a), participants incorrectly (in American English) used a plural verb (were) more often in their continuations than when the group was conceptually collective (5b). This indicates that participants were more likely to treat the gang as a plural when its individuals were more salient (5a) rather than when the group was the relevant referent (5b). This suggests that distributivity can make grammatically singular lexical items that have plural referents (e.g., gang, group) functionally plural during language production.\nThis production and off-line comprehension work suggests that readers build different conceptual representations for collective and distributed predicates, and is consistent with the hypothesis that singular indefinite noun phrases within distributed predicates are often treated as conceptually plural.\n\nExperiment 1\n\nExperiment 1 was conducted to test whether the Berent et al. (2005) methodology could be extended to sentences. It is possible that the added complexity involved in building and maintaining a sentence representation during the number judgment task, or the task demands of simultaneously carrying out self-paced reading and number judgments, might make participants less sensitive to interference than they were in Berent et al. (2005). Experiment 1 is also important because in order to use the paradigm to test ambiguous cases (as in Experiment 2), we must first establish that the paradigm works on simple, unambiguous sentences.\nThe critical measure was the reaction time for the number judgment for correct number judgment trials only. There was a significant main effect of noun type such that ‘one’ responses were slower when the target word was plural\nExperiment 1 confirmed that even in sentential contexts, semantic plural information on a word interferes with singular number judgments.\nSpecifically, if the distributing quantifier takes wide scope over the indefinite, and comprehenders build conceptual representations for distributed predicates that contain multiple exemplars of the referent introduced by the singular indefinite noun phrase, then one-word judgment times should be slower for indefinite noun phrases in distributed predicates than collective predicates.\nThe experiment had a 2 × 2 within-participants design. The first factor was the quantifier type and was either distributed (a) or collective (b)\nThey were asked to rate on a scale of 1 – 5 (where 1 was ‘definitely one’ and 5 was ‘definitely more than one’) whether the last word in the sentence referred to one or more than one object. Results indicated that the singular-marked distributed items were indeed biased toward a plural interpretation of the noun phrase. Participants rated the singular-marked distributed items as being closer to the ‘definitely more than one’ end of the scale\nthan the singular-marked collective items\n\nExperiment 2\n\nThe results of Experiment 2 confirm the hypothesis that singular indefinite noun phrases in distributed predicates can indeed be treated as conceptually plural during reading\nThere was no reliable effect of distributivity and no reliable difference between the plural-marked conditions, indicating that the difference in the singular-marked conditions was unlikely to be the result of one kind of predicate being more costly to compute than the other. These results indicate that in these items the distributing quantifier took wide scope over the indefinite\nindicate that conceptual plurality interferes with number judgments during sentence comprehension\nThese findings (Filik et al., 2004; Paterson et al., 2008) indicate that comprehenders do not build conceptually plural referents on-line for indefinite noun phrases in distributed structures that off-line norming had indicated were likely to be interpreted as plural.\n"},"KB/Example-GCN-Layer":{"title":"Example GCN Layer","links":["KB/Relational-Inductive-Bias","KB/Adjacency-matrix"],"tags":["graph"],"content":"Example GCN Layer\n \nExample GCN Layer\n\n\n\nthis is equivariant, can cope with any number of neighbors, uses the graph srtucture to provide a Relational Inductive Bias and parameter sharing\n\nExample : Graph Classification\n\nNetwork to classify molecules as harmless or toxic\nAdjacency matrix - A \\in \\mathbb{R}^{N \\times N}\nNode embedding matrix - X \\in \\mathbb{R}^{118 \\times N}\n\n118 elements of the periodic table\nvectors of length 118 where every position is zero except for the position corresponding to the relevant element, which is set to one\n\n\n\n\nExample : Node Classification\n\n\n"},"KB/Explainability-Defn":{"title":"Explainability Defn","links":[],"tags":["explainability"],"content":"Explainability Defn\n\nassociated with the notion of explanation as an interface between humans and a decision maker that is, at the same time, both an accurate proxy of the decision maker and comprehensible to humans\n"},"KB/Explainability-Taxonomy":{"title":"Explainability Taxonomy","links":[],"tags":["explainability"],"content":"Explainability Taxonomy\n\n[Explainable Artificial Intelligence (XAI) Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI](Explainable Artificial Intelligence (XAI) Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI.md)\n[Analysis of Explainers of Black Box Deep Neural Networks for Computer Vision A Survey](Analysis of Explainers of Black Box Deep Neural Networks for Computer Vision A Survey.md)\n"},"KB/Explainable-Artificial-Intelligence-(XAI)-Concepts,-Taxonomies,-Opportunities-and-Challenges-toward-Responsible-AI":{"title":"Explainable Artificial Intelligence (XAI) Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI","links":["KB/Understandability","KB/Comprehensibility","KB/Interpretability","KB/Transparency","KB/Trustworthiness","KB/Causality","KB/Transferability","KB/Informativeness","KB/Confidence","KB/Fairness","KB/Accessibility","KB/Interactivity","KB/Independence","KB/Separation","KB/Sufficiency","KB/Auditability","KB/Redress"],"tags":["explainability"],"content":"Explainable Artificial Intelligence (XAI) Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI\n\n@arrietaExplainableArtificialIntelligence2019\n\nCharactersitics\n\nUnderstandability\nComprehensibility\nInterpretability\n[Explainability Defn](Explainability Defn.md)\nTransparency\nTrustworthiness\nCausality\nTransferability\nInformativeness\nConfidence\nFairness\nAccessibility\nInteractivity\n[Privacy awareness](Privacy awareness.md)\n\nOn the Tradeoff between Interpretability and Performance\n\nit is not necessarily true that models that are more complex are inherently more accurate\nIt is in this situation that the trade-off between performance and interpretability can be observed\nIn this path toward performance, when the performance comes hand in hand with complexity, interpretability encounters itself on a downwards slope that until now appeared unavoidable\nAnother aspect worth mentioning at this point due to its close link to model interpretability and performance is the approximation dilemma: explanations made for a ML model must be made drastic and approximate enough to match the requirements of the audience for which they are sought, ensuring that explanations are representative of the studied model and do not oversimplify its essential features.\nconfluence of multiple criteria\nneed for having the human in the loop\nContextual factors, potential impacts and domain-specific needs must be taken into account when devising an approach to interpretability\na thorough understanding of the purpose for which the AI model is built\nthe complexity of explanations that are required by the audience\nthe performance and interpretability levels of existing technology, models and methods\nInterpretable techniques should be preferred when possible\nblack-box models such as those reviewed in this work (namely, support vector machines, ensemble methods and neural networks) should be selected only when their superior modeling capabilities fit best the characteristics of the problem at hand.\nIf a black-box model has been chosen, the third guideline establishes that ethics-, fairnessand safetyrelated impacts should be weighed\nrethink interpretability in terms of the cognitive skills, capacities and limitations of the individual human\n\nFairness and Discrimination\n\n[Individual fairness](Individual fairness.md)\n[Group fairness](Group fairness.md)\n[Counterfactual fairness](Counterfactual fairness.md)\n[Skewed data](Skewed data.md)\n[Tainted data](Tainted data.md)\n[Limited features](Limited features.md)\n[Proxy features](Proxy features.md)\nIndependence\nSeparation\nSufficiency\n\nAccountability\n\nAuditability\n[Minimization and reporting of negative impacts](Minimization and reporting of negative impacts.md)\nRedress\n"},"KB/Explanation-is-not-a-Technical-Term":{"title":"Explanation is not a Technical Term","links":["KB/Elaborateness","KB/Faithfulness"],"tags":["explainability"],"content":"Explanation is not a Technical Term\n\n@gilpinExplanationNotTechnical2022\n\nAbstract\n\nArtificial Intelligence (XAI) and those explanations that users and other audiences actually need, which should be defined by the full spectrum of functional roles, audiences, and capabilities for explanation\nIn this paper, we explore the features of explanations and how to use those features in evaluating their utility.\nwe discuss the risk of XAI enabling trust in systems without establishing their trustworthiness and define a critical next step for the field of XAI to establish metrics to guide and ground the utility of system-generated explanations\n\nIntroduction\n\nThe problem is that explainability is not a well-defined goal: there is no common definition, metrics, or benchmarks for success.\nRather than taking a view of explanations as undi↵erentiated artifacts shared by multiple users, we view them as generated in response to their functional roles, audience, and data access\nFunctional role: How is the explanation going to be used?\nAudience: To whom is it directed and what is their knowledge of the system and domain?\nCapabilities: What are the capabilities of the system constructing the explanation and the source of data/knowledge used to do so?\n\nMotivation\n\nBecause explanations are links between agents, we also need to consider the audience and their knowledge background\nAn explanation’s functional role defines the information that needs to be communicated.\nThe state of the audience’s initial knowledge of both the domain and processes define a second set of requirements related to the detail and vocabulary used in the explanation.\nThese three factors define the basis for metrics for an evaluation calculus that can be used to evaluate an explanation based on whether it serves the right functional role with the right level of elaboration for its audience supported by the system’s knowledge of its own reasoning.\nBy unpacking the idea of “explanation” into these factors, XAI can go beyond the “checking the box” phase to one in which explanations can play the role for which they were designed\n\nThe Requirements: Functional Roles\n\nUnfortunately, one of the early findings in the realm of explanation was the discovery that the human bar on what constitutes a “good explanation” is shockingly low.\nAs Langer discovered in 1978, people are satisfied with “Placebic” information if it has the syntactic form of an explanation [13].\nA system’s capabilities and access to knowledge about its own reasoning determine the scope and validity of the explanations it can generate.\n\nEngineers and Developers\n\nexplanations are close to the machinery, and the shared language is technical and can contain machine representation\nThese users have experience working with models, understand limitations, can run experiments, and can intuit from incomplete or partial “explanations” as they perform their task: debugging and iterating on the model to improve its performance and make it trustworthy\n\nDoctors\n\nexplanations are less about the core mechanics of the models themselves and reflect the logic of the domain, a↵ording exploration, counterfactuals, cohort comparison and transparent reasoning about the features most pertinent to a diagnosis\nif a system provides a warning that a particular patient is showing signs of possible heart failure, a doctor will want a list of relevant factors in order of importance or concern, as well as the patient’s prognosis compared to that of other patients, and how that prognosis changes if certain factors are amended\nAt their best, the interaction between a doctor and an intelligent system should seek to mirror the sorts of interactions two doctors might have when collaborating on the task of developing a diagnosis and refining a treatment plan.\n\nPatients\n\npatient’s goal is more immediate and comes with higher personal stakes\nThis is the realm of personal decision making and, in cases of emergency, immediate action\nThese individuals are less informed about aspects of the medical domain, and systems tailored to them must account for that information asymmetry.\nThus, XAI, in this use case, becomes less about collaboration or justification and instead is geared towards confidence building, risk assessment, contextualization, and guided calls to action\n\nRegulators and Auditors\n\nrequire explanations that scope across the mechanisms of a system, the data used to train it, and the medical practices that it embodies.\nthey require explanations that might include elements of the human in the loop in order to determine responsibility and culpability.\nengineers\nbuilding a system, explanations need to touch on the aspects of a system’s decisions that can be used in debugging, referencing the data, feature selection, and comparisons.\nusers interpreting the recommendations of a system\nexplanations need to include features that can be used to support exploration of hypotheticals, counterfactuals, cohort comparison and likelihoods.\nexplanations need to support trust and confidence building, risk assessment, contextualization, and decision support\nstakeholders impacted by a decision\n\nAuditors and Regulators\n\nexplanations need to support comparisons and aggregate review of performance and the trail of both algorithmic and human decisions that led to it\n\nThe Requirements: User Knowledge\n\nThe knowledge state of the various stakeholders interacting with the system defines another\nAs noted above, the functional role of XAI starts in the realm of model debugging and diagnostics – a tool set aimed at technical users capable of deciphering machine representations, interpreting model performance metrics, and updating model training to improve performance\nDomain experts may not need explanations that provide insight into the technical workings of systems\nThey want and understand explanations at the domain level.\nWhile it is often the case that they have the right level of domain knowledge, it is always a possibility that they lack detailed knowledge of specific domain level features and ideas.\nImpacted stakeholders are highly variable\nthey may have no knowledge of either the domain or the technology. They do not have expertise in either but do have basic knowledge of how the world works.\nStakeholders such as auditors and regulators have specialized knowledge of the ways in which data and algorithms interact and how to look at the performance of a system through the lens of comparison and systemic issues.\n\nSystem Capabilities\nInterpretability\n\nhow understandable the output representation is to the audience.\ndepends on the target audience and the task\n\nAccuracy\n\n\nbased on correctness\n\n\nexcept that it is in terms of the explanation itself\n\n\ndependent on both the domain and the user\n\n\nA saliency map applied to cancer images may highlight the hospital name, indicating that the reasons supporting the diagnosis is\n\n\nthe hospital where the image was taken [17]\n\n\nThis explanation is true to the model but the model is faulty. The explanation is accurate but the model is not.\n\n\nElaborateness\n\n\nFaithfulness\n\n\nTrusted but not Trustworthy: A New Dark Pattern\n\nOne of the dominant themes in XAI is the notion of trust\nThe focus is on getting users to trust a system rather than on making the system trustworthy.\nSuch trust is developed through output assessment over time and through a host of factors external to the model output itself (including the apparent trust of other experts, design decisions at the system and user experience levels, and ancillary components such as domain-informed conversational interfaces).\nhard won and easily lost\n\nDiscussion: Metrics - the next Critical step for XAI\n\nExplanations should provide new insights: explanations should go beyond the “why, what, how” [20]\nWe must also be able to compare and contrast them, as explanations can disagree and directly contradict each other [21]\nWe also want to quantify the system capabilities: how well the explanation actually explains\nWhen we as humans explain something, it is a deductive process. Each claim follows from the last claim\nThe crucial point is that explanations need to fit the functional role in a manner that is interpretable to the audience\nIn evaluating an explanation, the question is whether it fits the role and the audience\nWith a variety of explanation types, a single “one size fits all” metric is inappropriate\nExplanations must be built, interpreted, and evaluated through the lens of the functions that they serve and the knowledge situations in which they do so; through the lens of requirements and a system’s capability to meet them.\nAs work in XAI continues, it must attend to what exactly is being explained, to whom the explanation is directed, and how it is going to be used, and must provide a structured, standard evaluation approach via metrics of success.\n\nImages\n\n\n\n"},"KB/Explanator":{"title":"Explanator","links":[],"tags":["explainability"],"content":"Explanator\n\nsynonym for an explaining system or explaining process that gives answers to questions in understandable terms, which could, computationally, be considered a program execution trace.\nor instance, if the question is how a machine is working, the explainer makes the internal structure of a machine more transparent to humans\n"},"KB/Exploding-Gradient":{"title":"Exploding Gradient","links":[],"tags":["architecture"],"content":"Exploding Gradient\n\nweight matrices have max eigenvalue max_{i} \\lambda_{j} &gt; 1 , gradient increases per layer\n\nif enough depth, then converges to infinity\n\n\n\n\n"},"KB/Exponential-Distribution":{"title":"Exponential Distribution","links":["KB/PDF","KB/Spiking-Networks"],"tags":["distributions"],"content":"Exponential Distribution\n\nHow long you have to wait for something after the it has happened once already\nAverage rate /unit reference time\nPDF p(x) = \\lambda e^{-\\lambda x} and x \\geq 0\nExpectation E(X) = \\frac{1}{\\lambda}\n\nRate : \\hat \\lambda = \\frac{1}{N-1}\\Sigma_{i = 1, …, N}t_{i+1}-t_{i}\nSpiking Networks\n"},"KB/Extensions-to-SlimStampen":{"title":"Extensions to SlimStampen","links":["KB/SlimStampen","KB/ACT-R"],"tags":["usermodel"],"content":"Extensions to SlimStampen\n\nKonteksti\n\nSemantic similarity\n\n\nIncrease activation of similar facts\nDual-lingo\n\nWord + Picture based cues\n\n\nType of info\n\nEg vocabulary\n\n\nPerfect pitch\n\nauditory stimuli\n\n\nFun with flags\n\nImprove scheduling by computing a continous stimuli\n\n\nCramDroid\n\nApp that helps with between-sessions\nTrack decay functions and send notif\n\n\nSpace Times\n\nGame based memorization of tables\n\n\nVocab Warrior\n\nplay against an ACT-R\nanswer faster\n\n\n"},"KB/Extra-position":{"title":"Extra-position","links":[],"tags":["language"],"content":"Extra-position\n\nDid anyone who you expected to help actually help?\nDid anyone actually help who you expected to help?\n"},"KB/Eye-Tracking":{"title":"Eye Tracking","links":["KB/Gaze-position"],"tags":["usermodel"],"content":"Eye Tracking\n\nGaze position\n"},"KB/Eye-to-hand-System":{"title":"Eye-to-hand System","links":[],"tags":["robotics"],"content":"Eye-to-hand System\n\nThe task in visual servoing is to use visual information to control the robot’s endeffector relative to a target object.\nprovide feedback to the robot controller at each time step\n\n"},"KB/FGSM":{"title":"FGSM","links":[],"tags":["explainability"],"content":"FGSM\n\nFGSM is a method of generating noise in the direction of the cost function gradient concerning the data\nGiven original input image x, label y, model parameter θ, and loss J.\nadv_{x}= x+ \\epsilon \\ast sign(\\nabla_{x}(J(\\theta, x, y)))\nthis gives us the perturbations\n"},"KB/FGVC-Aircraft":{"title":"FGVC Aircraft","links":[],"tags":["dataset"],"content":"FGVC Aircraft\n\nThis dataset contains images of 100 different types of aircrafts, with a total of 10,000 images.\n"},"KB/FGVCx":{"title":"FGVCx","links":[],"tags":["dataset"],"content":"FGVCx\n\nFGVCx is a dataset that includes a number of fine-grained datasets, such as the FGVC Aircraft, Stanford Cars and Stanford Dogs datasets.\n"},"KB/FHIR":{"title":"FHIR","links":[],"tags":["medical"],"content":"FHIR\n\nspecific standard within HL7 that leverages modern web technologies and is gaining traction for its flexibility in healthcare data exchange\nfhir-py\n\n\n \n\n\nprotoc —version # Ensure version 3+\npip install google-fhir-views[r4,bigquery,spark]```\n"},"KB/FLASH":{"title":"FLASH","links":["KB/Mixed-chunk-attention","KB/GAU","KB/Transformer"],"tags":["architecture"],"content":"FLASH\n\nTransformer Quality in Linear Time\nweaknesses in handling long sequences\nFLASH\nperformant layer (gated linear unit) and by combining it with an accelerator-efficient approximation strategy (Mixed chunk attention)\nGAU\nMixed chunk attention\noutperforms three baselines: vanilla Transformer, Performer and Combiner in terms of quality and efficiency\nWiki\nPG-19\n"},"KB/FLAVA":{"title":"FLAVA","links":[],"tags":["architecture"],"content":"FLAVA\n\nFLAVA: a Foundational Language and Vision Alignment Model\nfoundational vision and language alignment model that performs well on all three target modalities: 1) vision, 2) language, and 3) vision &amp; language\nuse a single holistic universal model, as a “foundation”, that targets all modalities at once\nwide range of 35 tasks spanning these target modalities\n"},"KB/FP16-training":{"title":"FP16 training","links":[],"tags":["temp"],"content":"FP16 Training\n\n@micikeviciusMixedPrecisionTraining2018\nReduced precision has a narrower range that might make the results more out of range and worsen the training progress\nCan store all parameters and activations in FP16 and then use that for gradients.\nAlso copy to FP32 for parameter updates\nMultiply scalar to loss to align range of FP16\n"},"KB/FTSwish":{"title":"FTSwish","links":["KB/Relu","KB/Sigmoid","KB/Sparsity","KB/Swish"],"tags":["architecture"],"content":"FTSwish\n\n\nRelu + Sigmoid\n\\begin{equation} FTSwish: f(x) = \\begin{cases} T, &amp; \\text{if}\\ x &lt; 0 \\\\ \\frac{x}{1 + e^{-x}} + T, &amp; \\text{otherwise} \\\\ \\end{cases} \\end{equation}\nAs we can see, the Sparsity principle is still true - the neurons that produce negative values are taken out.\nWhat we also see is that the derivative of FTSwish is smooth, which is what made Swish theoretically better than Relu in terms of the loss landscape\nHowever, what I must note is that this function does not protect us from the dying Relu problem: the gradients for x &lt; 0 are zero, as with Relu.\n"},"KB/FaceNet":{"title":"FaceNet","links":["KB/Embedding","KB/Triplet-Loss","KB/Curriculum-Learning","KB/Lp-Regularization","KNN","KB/Labeled-Faces-in-the-Wild","KB/Zeiler-Fergus","KB/Inception","Harmonic-Embedding"],"tags":["architecture"],"content":"FaceNet\n\nFaceNet: a Unified Embedding for Face Recognition and Clustering\nmapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity\nOptimize the Embedding itself\nFaceNet directly trains its output to be a compact 128-D Embedding using a Triplet Loss function\nChoosing which triplets to use turns out to be very important for achieving good performance\n\ninspired by Curriculum Learning\nonline negative exemplar mining strategy which ensures consistently increasing difficulty of triplets as the network trains\nalso explore hard-positive mining techniques which encourage spherical clusters for the embeddings of a single person\n\n\nsquared Lp Regularization L2 distance, in the Embedding space directly correspond to face similarity: faces of the same person have small distances and faces of distinct people have large distances\nface verification simply involves thresholding the distance between the two embeddings; recognition becomes a KNN classification problem\nLabeled Faces in the Wild\nZeiler Fergus\nInception\nHarmonic Embedding\n"},"KB/Factorized-Embedding-Parameters":{"title":"Factorized Embedding Parameters","links":["KB/Embedding","KB/BERT","KB/ALBERT","KB/Word-Vectors"],"tags":["architecture"],"content":"Factorized Embedding Parameters\n\nFactorization of these parameters is achieved by taking the matrix representing the weights of the word embeddings E and decomposing it into two different matrices. Instead of projecting the one-hot encoded vectors directly onto the hidden space, they are first projected on some-kind of lower-dimensional Embedding space, which is then projected to the hidden space (Lan et al, 2019). Normally, this should not produce a different result, but let’s wait.\nAnother thing that actually ensures that this change reduces the number of parameters is that the authors suggest to reduce the size of the Embedding matrix.\nIn BERT, the shape of the vocabulary/Embedding matrix E equals that of the matrix for the hidden state H.\nFirst of all, theoretically, the matrix E captures context-independent information\nwhereas the hidden representation H captures context-dependent information\nALBERT solves this issue by decomposing the Embedding parameters into two smaller matrices, allowing a two-step mapping between the original Word Vectors and the space of the hidden state. In terms of computational cost, this no longer means \\text{O(VxH)} but rather \\text{O(VxE + ExH)}, which brings a significant reduction when \\text{H &gt;&gt; E}.\n"},"KB/Factors-for-MC-estimate":{"title":"Factors for MC Estimate","links":[],"tags":["temp"],"content":"Factors for MC Estimate\n\nAmount of computation required to simulate transition kernel\nTime for chain to converge to equilibrium → no of states that must be discarded\nNo of transitions needed to move from one state in the equilibrium to another that is independant\n\nRedundant information\nFirst value will depend to a decreasing degree on the distance from this timestep to the previous ones. Then washout (because old ones are too far away)\n\n\n"},"KB/Fairness-Constraint":{"title":"Fairness Constraint","links":[],"tags":["temp"],"content":"Fairness Constraint\n\nApplying a constraint to an algorithm to ensure one or more definitions of fairness are satisfied. Examples of fairness constraints include\n"},"KB/Fairness":{"title":"Fairness","links":[],"tags":["explainability"],"content":"Fairness\n\nexplainability can be considered as the capacity to reach and guarantee fairness in ML models.\nan explainable ML model suggests a clear visualization of the relations affecting a result, allowing for a fairness or ethical analysis\na related objective of XAI is highlighting bias in the data a model was exposed to\n"},"KB/Faithfulness":{"title":"Faithfulness","links":[],"tags":["explainability"],"content":"Faithfulness\n\nhow well the explanation describes the underlying model. This is also known as model completeness [18]\n"},"KB/Familar-Object-Grasping-Object-Viiew-recog":{"title":"Familar Object Grasping Object Viiew recog","links":[],"tags":["robotics"],"content":"Familar Object Grasping Object Viiew Recog\n\nShafii, Nima, S. Hamidreza Kasaei, and Luís Seabra Lopes. “Learning to grasp familiar objects using object view recognition and template matching.” IROS 2016.\nGrasp template :\nLocal shape feature for graspable regions\n\nspin-image feature\n\n\nGlobal feature, the radius (distance from the CoM to the selected keypoint)\n\nimportant to represent a stable grasp\n\n\nFinger configuration\nNew objects that are similar to known ones (i.e. they are familiar) can be grasped in a similar way\n\nAs an example, if the robot knows how to grasp a pen, it may use the same grasp temple to take a marker\n\n\n"},"KB/Fashion-MNIST":{"title":"Fashion MNIST","links":["KB/MNIST"],"tags":["dataset"],"content":"Fashion MNIST\n\ngithub.com/zalandoresearch/fashion-mnist\nalternative to MNIST\n\n60’000 train images\n10’000 test images\n28x28x1 grayscale\n10 classes\n\n\nbit more challenging than MNIST\n"},"KB/Fast-AutoAugment":{"title":"Fast AutoAugment","links":[],"tags":["augmentation"],"content":"Fast AutoAugment\n\nthat finds effective augmentation policies via a more efficient search strategy based on density matching\n"},"KB/FastText":{"title":"FastText","links":[],"tags":["architecture"],"content":"FastText\ndef FastTextNew(vocab_size, embedding_dim, output_dim):\n    return nn.Sequential(\n        Rearrange(&#039;t b -&gt; t b&#039;),\n        nn.Embedding(vocab_size, embedding_dim),\n        Reduce(&#039;t b c -&gt; b c&#039;, &#039;mean&#039;),\n        nn.Linear(embedding_dim, output_dim),\n        Rearrange(&#039;b c -&gt; b c&#039;),\n    )"},"KB/Fastai-Blocks":{"title":"Fastai Blocks","links":["KB/fastai","KB/Fitting"],"tags":["deeplearning"],"content":"fastai Blocks\nBuilding Blocks\n# Image Classification\ndls = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=[Resize(192, method=&#039;squish&#039;)]\n).dataloaders(path)\n \ndls.show_batch(max_n=6)\nlearn = cnn_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(8)\n# Label regex\npets = DataBlock(blocks = (ImageBlock, CategoryBlock),\n                 get_items=get_image_files, \n                 splitter=RandomSplitter(seed=42),\n                 get_y=using_attr(RegexLabeller(r&#039;(.+)_\\d+.jpg$&#039;), &#039;name&#039;),\n                 item_tfms=Resize(460),\n                 batch_tfms=aug_transforms(size=224, min_scale=0.75))\ndls = pets.dataloaders(path/&quot;images&quot;)\n\nDataBlock is more general\n\nlist models\n\n\n\ntimm.list_models(&#039;convnex*&#039;)\n# Segmentation\npath = untar_data(URLs.CAMVID_TINY)\ndls = SegmentationDataLoaders.from_label_func(\n    path, bs=8, fnames = get_image_files(path/&quot;images&quot;),\n    label_func = lambda o: path/&#039;labels&#039;/f&#039;{o.stem}_P{o.suffix}&#039;,\n    codes = np.loadtxt(path/&#039;codes.txt&#039;, dtype=str)\n)\nlearn = unet_learner(dls, resnet34)\nlearn.fine_tune(8)\n\nSegmentation Dataloaders is just another abstraction of the DataBlock for a specific case. Can use the DataBlock as well\n\n# Tabular\ndls = TabularDataLoaders.from_csv(path/&#039;adult.csv&#039;, path=path, y_names=&quot;salary&quot;,\n    cat_names = [&#039;workclass&#039;, &#039;education&#039;, &#039;marital-status&#039;, &#039;occupation&#039;, &#039;relationship&#039;, &#039;race&#039;],\n    cont_names = [&#039;age&#039;, &#039;fnlwgt&#039;, &#039;education-num&#039;],\n    procs = [Categorify, FillMissing, Normalize])\nlearn = tabular_learner(dls, metrics=accuracy)\nlearn.fit_one_cycle(2)\n\nFitting because pretrained models are not going to be there\n\n# Collaborative [Filtering](Filtering.md)\ndls = CollabDataLoaders.from_csv(path/&#039;ratings.csv&#039;)\nlearn = collab_learner(dls, y_range=(0.5,5.5))\nlearn.fine_tune(8)\n\nRange is for the output (Since its not a binary output)\nSaving a model\n\nlearn.export(&#039;model.pkl&#039;)"},"KB/Fastai-Deployment":{"title":"Fastai Deployment","links":["KB/fastai","KB/Gradio"],"tags":["deeplearning"],"content":"fastai Deployment\n\nGradio\n\nSave\nfrom fastai.vision.widgets import *\npath = Path()\npath.ls(file_exts=&#039;.pkl&#039;)\n \nlearn_inf = load_learner(path/&#039;export.pkl&#039;)\nlearn_inf.predict(&#039;images/grizzly.jpg&#039;)\nlearn_inf.dls.vocab"},"KB/Fastai-Interpretation":{"title":"Fastai Interpretation","links":["KB/fastai"],"tags":["deeplearning"],"content":"fastai Interpretation\nClassification Interpretation\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\ninterp.plot_top_losses(5, nrows=1)\n\nOrdered by loss\nIf predicted correctly but still shown, then low confidence\n\nCleaner\ncleaner = ImageClassifierCleaner(learn)\ncleaner\n \nfor idx in cleaner.delete(): cleaner.fns[idx].unlink()\nfor idx,cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat)\nGet All Classes and Their Probabilities\ndef classify_image(img):\n\tpred,idx,probs = learn.predict(img)\n \nreturn dict(zip(categories, map(float,probs)))\n \nclassify_image(im)"},"KB/Fastai-Tricks":{"title":"Fasai Tricks","links":[],"tags":["deeplearning"],"content":"Fasai Tricks\nBatched Map\ntok_ds = ds.map(tok_func, batched=True)\nLearning Rate Finder\nlearn.lr_find(suggest_funcs=(slide, valley))\nTest Dataset\ntst_dl = learn.dls.test_dl(tst_df)\npreds,_ = learn.get_preds(dl=tst_dl)\nEnsemble\ndef ensemble():\n    learn = tabular_learner(dls, metrics=accuracy, layers=[10,10])\n    with learn.no_bar(),learn.no_logging(): learn.fit(16, lr=0.03)\n    return learn.get_preds(dl=tst_dl)[0]\nlearns = [ensemble() for _ in range(5)]\n \nens_preds = torch.stack(learns).mean(0) # stack and mean"},"KB/Faster-RCNN":{"title":"Faster RCNN","links":["SPNet","Fast-RCNN","KB/Region-Proposal","KB/Attention","KB/Vgg","KB/PASCAL-VOC","KB/ILSVRC","KB/COCO"],"tags":["architecture"],"content":"Faster RCNN\n\nFaster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks\nSPNet\nFast-RCNN + Region Proposal\nAttention is also used\nVgg\nPASCAL VOC, ILSVRC, COCO\n"},"KB/FeatMatch":{"title":"FeatMatch","links":[],"tags":["augmentation"],"content":"FeatMatch\n\nnovel learned feature-based refinement and augmentation method to produce a varied set of complex transformations\nutilize information from both within-class and across-class prototypical repre- sentations\n"},"KB/Feature-Augmentation":{"title":"Feature Augmentation","links":["KB/FeatMatch"],"tags":["augmentation"],"content":"Feature Augmentation\n\nRather than conduct augmentation only in the input space, feature augmentation performs the transformation in a learned feature space\n[DeVries and Taylor, 2017a]\nwhen traversing along the manifold it is more likely to encounter realistic samples in feature space than compared to input space\nmanipulating the vector representation of data within a learned feature space are investigated\nFeatMatch\n[Moment Exchange](Moment Exchange.md)\n"},"KB/Feature-Based-Knowledge":{"title":"Feature Based Knowledge","links":["KB/Attention","KB/Distillation-Schemes","KB/Teacher-Student-Architecture","KB/Distillation-Algorithms","KB/Applications-of-Knowledge-Distillation"],"tags":["knowledgedistillation"],"content":"Feature Based Knowledge\n\nHuang and Wang (2017) using neuron selectivity trans- fer. Passalis and Tefas (2018) transferred knowledge by matching the probability distribution in feature space.\nKim et al. (2018) introduced so called “factors” as a more understandable form of intermediate repre- sentations. To reduce the performance gap between teacher and student, Jin et al. (2019) proposed route constrained hint learning, which supervises student by outputs of hint layers of teacher. Recently, Heo et al. (2019c) proposed to use the activation boundary of the hidden neurons for knowledge transfer. Interestingly, the parameter sharing of intermediate layers of the teacher model together with response-based knowledge is also used as the teacher knowledge (Zhou et al., 2018).\nTo match the semantics between teacher and stu- dent, Chen et al. (2021) proposed cross-layer knowledge distillation, which adaptively assigns proper teacher layers for each student layer via Attention allocation.\nThough feature-based knowledge transfer provides favorable information for the learning of the student model, how to effectively choose the hint layers from the teacher model and the guided layers from the student model remains to be further investigated (Romero et al., 2015).\nDistillation Schemes\nTeacher Student Architecture\nDistillation Algorithms\nApplications of Knowledge Distillation\nFor example, on one hand, some recent works find that the student model can learn little from some teacher models due to the model capac- ity gap between the teacher model and the student model (Zhang et al., 2019b; Kang et al., 2020); On the other hand, from some early theoretical analysis on the capacity of neural networks, shallow networks are capable of learning the same representation as deep neural networks (Ba and Caruana, 2014).\n"},"KB/Feature-Correlationa":{"title":"Feature Correlationa","links":[],"tags":["architecture"],"content":"Feature Correlationa\n\nIf certain features in a dataset have a high correlation in a dataset, it becomes difficult to control specific features without changing the closely correlated ones.\nFor example, let’s say you have a dataset of  face images and want to add facial hair to an image of a woman, it’s likely that you’ll end up modifying more features as this feature is highly correlated with a male’s face.\n"},"KB/Feature-Learning":{"title":"Feature Learning","links":["KB/Dictionary-Learning","KB/Methods-for-Feature-Learning","KB/Contrastive-Loss","KB/Max-Margin-Loss","KB/Triplet-Loss"],"tags":["temp"],"content":"Feature Learning\n\nDictionary Learning\nMethods for Feature Learning\nContrastive Loss\nMax Margin Loss\nTriplet Loss\n"},"KB/Feature-Map-Visualization":{"title":"Feature Map Visualization","links":[],"tags":["semisupervisedlearning"],"content":"Feature Map Visualization\n\nFeature maps are visualized to show the attention of networks\nLarger activation represents the neural network pays more attention to the corresponding region in the imag\neature maps are usually qualitatively visualized and compared with that of super- vised models [28], [36].\n"},"KB/Feature-Space-Augmentation":{"title":"Feature Space Augmentation","links":[],"tags":["augmentation"],"content":"Feature Space Augmentation\n\nThe sequential processing of neural networks can be manipulated such that the intermediate representations can be separated from the network as a whole. The lower-dimensional representations of image data in fully-connected layers can be extracted and isolated.\nDeVries and Taylor tested their feature space augmentation technique by extrapolating between the 3 nearest neighbors per sample to generate new data and compared their results against extrapolating in the input space and using affine transformations in the input space\nVector representations are then found by training a CNN and then passing the training set through the truncated CNN. These vector representations can be used to train any machine learning model from Naive Bayes, Support Vector Machine, or back to a fully-connected multilayer network.\nA disadvantage of feature space augmentation is that it is very difficult to interpret the vector data.\n"},"KB/Feature-Spec":{"title":"Feature Spec","links":[],"tags":["temp"],"content":"Feature Spec\n\nDescribes the information required to extract features data from the tf.Example protocol buffer. Because the tf.Example protocol buffer is just a container for data, you must specify the following\n\nthe data to extract (that is, the keys for the features)\nthe data type (for example, float or int)\nThe length (fixed or variable)\n\n\n"},"KB/Features":{"title":"Features","links":["KB/Perceptron"],"tags":["temp"],"content":"Features\nDimensions\nWide\n\nHad to train\nMore number of neurons\nEasy parallel\nInfinitely wide → Gaussian process\n\nDeep\n\nEasier to train\nLess data\nLinear amount\nDifficult to parallelize\n\nWhy\n\nDomain Adaptation\nStructure exploitation\nRelevant features\n\nRandom Things\n\n1 hidden layer Perceptron → Universal fn estimator\nBest generalization → First order optimization\n"},"KB/Federated-Learning":{"title":"Federated Learning","links":["KB/Basics-of-Federated-Learning","KB/Advantages-of-Federated-Learning","KB/Federated-Updates"],"tags":["federatedlearning"],"content":"Federated Learning\n\nBasics of Federated Learning\nAdvantages of Federated Learning\nFederated Updates\n\nRefs\n\nOpenMined Blog\nNvidia\nUnite.ai\nGoogle blog\nWiki\nDigital health : Rieke, N., Hancox, J., Li, W. et al. The future of digital health with federated learning. npj Digit. Med. 3, 119 (2020). doi.org/10.1038/s41746-020-00323-1\nGboard : Chen, M., Mathews, R., Ouyang, T., &amp; Beaufays, F. (2019). Federated learning of out-of-vocabulary words. arXiv preprint arXiv:1903.10635. Paper\nChen, M., Mathews, R., Ouyang, T., &amp; Beaufays, F. (2019). Federated learning of out-of-vocabulary words. arXiv preprint arXiv:1903.10635.\n"},"KB/Federated-Updates":{"title":"Federated Updates","links":["KB/Structured-Update","KB/Sketched-Update"],"tags":["federatedlearning"],"content":"Federated Updates\n\nStructured Update\nSketched Update\n"},"KB/Feedback-Loop":{"title":"Feedback Loop","links":[],"tags":["temp"],"content":"Feedback Loop\n\nIn machine learning, a situation in which a model’s predictions influence the training data for the same model or another model. For example, a model that recommends movies will influence the movies that people see, which will then influence subsequent movie recommendation models.\n"},"KB/Few-Shot-Order-Sensitivity":{"title":"Few Shot Order Sensitivity","links":["KB/Sensitivity","KB/GPT","KB/Entropy"],"tags":["temp"],"content":"Few Shot Order Sensitivity\n\nFantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity\nWhen primed with only a handful of training samples, very large, pretrained language models such as GPT-3 have shown competitive results when compared to fully-supervised, fine-tuned, large, pretrained language models\nfew-shot prompts suffer from order Sensitivity\nfor the same prompt the order in which samples are provided can make the difference between state-of-the-art and random performance – essentially some permutations are “fantastic” and some not\nproblem is prevalent across tasks, model sizes (even for the largest current models), prompt templates, it is not related to a specific subset of samples, number of training samples, and that a given good permutation for one model is not transferable to another.\nnovel probing method that exploits the generative nature of language models to construct an artificial development set\nidentity performant permutations for prompts using Entropy-based statistics over this set, which yields a 13% relative improvement for GPT-family models across eleven different established text classification tasks\n"},"KB/Filter-Bubble-Problem":{"title":"Filter Bubble Problem","links":[],"tags":["usermodel"],"content":"Filter Bubble Problem\n\nShows only things that you have seen before\n"},"KB/Filter-Wise-Normalization":{"title":"Filter Wise Normalization","links":[],"tags":["explainability"],"content":"Filter Wise Normalization\n\nFollowing up from [Random Directions](Random Directions.md)\n\nd_{i,j} \\leftarrow \\frac{d_{i,j}}{||d_{i,j}||}||\\theta_{i,j}||\n- d is a random Gaussian Direction vector\n- $d_{i,j}$ is $j_{th}$ filter of the $i_{th}$ layer of the direction vector d\n- $||\\cdot||$ is the [Frobenius norm](Frobenius norm.md)\n\n\n\n\n"},"KB/Filtering":{"title":"Filtering","links":["KB/Noise-Suppression"],"tags":["visualization"],"content":"Filtering\n\nNoise Suppression\n"},"KB/Final-Paper-Language-Modeling":{"title":"Final Paper LM","links":["KB/Shortcuts-to-Quantifier-Interpretation-in-Children-and-Adults","KB/Quantifier-spreading-children-misled-by-ostensive-cues","A-matter-of-ambiguity-Using-eye-movements-to-examine-collective-vs.-distributive-interpretations-of-plural-sets"],"tags":["language"],"content":"Final Paper LM\nTips\n\nIdentify a Research Question that links with previous research\nIdentify a Method that will let you answer the research question, or at least partially\nDevelop an experiment (or two) that would then test this research question\nPredict what the results will look like given current theory/theories\nIf your experiment tests the predictions of more than one theory then you should have one set of predictions for each theory\nWhat are the consequences of certain results for our understanding of the phenomena studied?\nWhat should following research do given certain results?\nHow will you analyze the results statistically? What methods, which tests, what does your data look like?\nSo write a 2-3 page paper about an experiment that would answer an open question about quantification.\nThis paper can be written as if you are proposing it (e.g. “We would then test x children with ….” ) or you could write it as if you already did the experiment, imagining the results, e.g. “We tested 30 Spanish speakers …“.\nGive it the kind of toc: true\ntitle you would give to a paper. (Don’t call it “My Gedankenexperiment” !;)).\nGive an introduction to the research area, summarize the results you already know, using references to relevant papers. This could be the background section. Explain what’s still missing in our knowledge and how we should test it.\nIn the methods section I need to know the details of the experimental design, including examples of sentences that will be tested and pictures that might be used (use clip art, or I also don’t mind simple drawings. Don’t worry if you are not very artistic!).\nExplain what kind of participants you will need to test, how many, and what features they need to have. Be realistic. Just because it’s fantasy, you shouldn’t propose testing 1000 children.\nDo you also want to do additional testing? (working memory, inhibition?) Make sure you motivate it.\n\nIdea.\n\nShortcuts to Quantifier Interpretation in Children and Adults But studies and comparisons were weird. Change?\nThey were salty about This feature of their design provided children with unambiguous cues as to which set of entities was the focus of attention. : Crain and Thornton (1998)\nCrain et al.’s (1996) claim that preschoolers have full competence with uni- versal quantifiers would seem to be undermined by the fact that even older school-age children make errors identifyi- 04:40 ng the domain of a universal quantifier.\n\nBrooks and Braine (1996, Experiment 1) tested adults with actional scenes and found no errors. Their data, however, came from 10 undergraduates at a highly selective private university (Carnegie Mellon) and thus may not be representative of adults in general.\nTest same thing as Shorts. but different data\nPictures\n\nAbstract\n\nOstensive cues\nLocative bias\nMouse tracking\n\nParticipants\n\nWe recruited and tested twelve 5-year-olds (M = 5;5, range = 5;2–5;11), twelve 6-year-olds (M = 6;6, range = 6;2–6;10), twelve 7-yearolds (M = 7;6, range = 7;1–7;11), twelve 8-year-olds (M = 8;6, range = 8;0–8;11), and twelve 9-year-olds (M = 9;6, range = 9;1–9;11), twelve adults at private elementary schools and after-school programs in Atlanta, Georgia.\nadults from RUG, kiddos from some school\n\nProcedure\n\nsingle, 20-min session conducted in a quiet room of their school\nWe showed children two pictures at a time and asked them to point to the picture that went best with a sentence read aloud\nAfter the child looked at both pictures, the experimenter read the corresponding sentence and asked the child to point to the picture that went best with the sentence.\nSame for adults (Contrary to this paper itself)\nthe teddy thing was nice and cute too\n\nExperimental Design\n\n16 test items\n2x2 study (Picture: Collective vs. Distributive) and Sentence: (With marker (“each” or “together”) or without)\n2 practice trials\n7 controls\n3 fillers\n\nWhat Do We Expect\n\nBoth children and adults make errors\nOnly 9 y/o were consistent\n7 y/o : extra animals/objects vs containers\nBetter performance :\n\nQuantifier modifying the containers vs subject (Disprove Kang et al.)\nChildren , Not Adults\n\n\nPrefer locative scenes with all filled containers (Drozd et al.)\n\nChildren , Not Adults\n\n\nThe rate of rejection of the sentences as true descriptions of the visually represented situations significantly correlated with the type of the visual representation.\nThe sentence– drawing pairs were rejected in 10.53% of the cases.\nn the case of the sentence–photo pairs, the rate of rejection was a mere 3.51%.\nJust as in Pintér’s (2016) experiment, the rate of rejection (i.e., the rate of the exhaustive interpretation of the sentences) was slightly even higher in the adult control group: 13.33% in the case of sentence–drawing pairs, and 8.88% in the case of sentence–photo p\nCrucially, the photos contained many more extra agents and extra objects than the drawings, still they elicited significantly less rejections.\nWhat made the presence of extra objects in the drawings ostensive was the minimality of the drawings, suggesting that everything irrelevant had been eliminated from them.\nThis suggests that the problem does not reside in the child’s syntax, given the similarities in sentence structures used across studies, but in\nstead has to do with the difficulty of selecting the appropriate set of entities and avoiding distraction by salient objects.\nTaken together, the experiments suggest that it was the collective scenes as opposed to the use of all that improved children’s performance in Experiment 1. Collective scenes were easier presumably because the group depiction aided the child in isolating one set of entities relative to the other\nWe suspect that both children and adults make errors in comprehension because they engage in shallow processing that causes inaccurate mapping between syntactic and semantic representations.\nShallow processing also provides a straightforward explanation of the errors made by adults in Experiment 3. Their high error rates suggest that adult listeners often do not tax their limited information-processing capacities by conducting exhaustive syntactic analyses of sentences but rather make use of simpler strategies in generating reasonable guesses\n\nLiterature\n\nQuantifier spreading children misled by ostensive cues\nShortcuts to Quantifier Interpretation in Children and Adults\nA matter of ambiguity? Using eye movements to examine collective vs. distributive interpretations of plural sets\n\nExperiment\n\nReplace doodles with images , Use more\nCombine locative bias fix + old version of exp\nMouse tracking\n\nSentences\n\nEach of the (people) is (verb)ing an (object), for example, Each of the men is washing a bear.\nThere is a (person) (verb)ing each of the (objects), for example, There is a man washing each of the bears.\nEvery (person) is (verb)ing an (object), for example, Every man is washing a bear.\nThere is a (person) (verb)ing every (object), for example, There is a man washing every bear.\nAll of the (people) are (verb)ing an (object), for example, All of the men are washing a bear.\nThere is a (person) (verb)ing all of the (objects), for example, There is a man washing all of the bears.\nAll of the (objects) are in a (container), for example, All of the alligators are in a bathtub.\nAll of the (containers) have an (object) in them, for example, All of the bathtubs have an alligator in them.\nThere is an (object) in all of the (containers), for example, There is an alligator in all of the bathtubs.\nEach of the (objects) is in a (container), for example, Each of the alligators is in a bathtub.\nEach of the (containers) has an (object) in it, for example, Each of the bathtubs has an alligator in it.\nThere is an (object) in each of the (containers), for example, There is an alligator in each of the bathtubs.\nEvery (object) is in a (container), for example, Every alligator is in a bathtub.\nEvery (container) has an (object) in it, for example, Every bathtub has an alligator in it.\nThere is an (object) in every (container), for example, There is an alligator in every bathtub.\n"},"KB/Final-Paper-User-Models":{"title":"Final Paper User Models","links":["KB/Effects-of-Contextual-Cues-on-Inferring-and-Remembering-Meanings-of-New-Word","KB/SlimStampen","KB/The-Behavior-of-Tutoring-Systems"],"tags":["usermodel"],"content":"Final Paper User Models\nLiterature\n\nvan den Broek, G. S., Takashima, A., Segers, E., &amp; Verhoeven, L. (2018). Contextual richness and word learning: Context enhances comprehension but retrieval enhances retention. Language learning, 68(2), 546-585.\nEffects of Contextual Cues on Inferring and Remembering Meanings of New Word\nSlimStampen\nThe Behavior of Tutoring Systems\n[The Effect of Three Consecutive Context Sentences on EFL Vocabulary-Learning](The Effect of Three Consecutive Context Sentences on EFL Vocabulary-Learning.md)\n[Second Language Vocabulary Learning , The role of context  versus translation](Second Language Vocabulary Learning , The role of context  versus translation.md)\n[Learning L2 German Vocabulary Through Reading](Learning L2 German Vocabulary Through Reading.md)\n\nMembers\n\nHila Schwartz\nJuliette Bruin\nIsabelle Tilleman\nSubhaditya Mukherjee\n\nConcept\n\nOutlines the general concept of our system, meaning how we tried to adjust and improve the SlimStampen system, and any additional improvements we made to the system.\n\nMain Idea\n\nThe main idea we have for this project is to improve the SlimStampen system by presenting vocabulary in context. The way we would like to do this, is by presenting users with the word they need to learn in a sentence. As the learner gets better, the system will increase the difficulty by presenting words without context. If the words are presented out of context and the user makes a mistake, the correct answer will also be shown without context.\nAn example of this would be as follows:\nPrompt: Wij kopen een huis\nCorrect answer: to buy OR buy OR buying\nThis idea is based on research by van den Broek et al. (2018), which showed that presenting novel words in context during the initial learning phase, and then reducing context later on improves long-term word retention. Next to that, Li (1988) found that learning words in context improves understanding of the word.\nBased on previous feedback, we have decided to have two contexts for each word. If a user gets a word wrong consistently, they will be shown a different context. This is based on the idea that they perhaps do not understand the first context they were provided with, and that perhaps the second context will provide them with the information they need to get the word correct. Unlike previously stated, this addition will not be tested in a separate condition. The only conditions we will test are no contexts at all and two contexts.\n\nParams\n\nWORD_THRESHOLD = 0.29\nCONTEXT2_THRESHOLD = 0.35\nDEFAULT_ALPHA = 0.3\n\nFact\nFact(fact_id = 6, \n question = &#039;gemiddeld&#039;, \n context_1 = &quot;Deze opleiding heeft &#039;gemiddeld&#039; …&quot;, \n context_2 = &quot;De temperatuur is &#039;gemiddeld&#039; …&quot;, \n answer = &#039;average&#039;, \n chosen_context = &quot;Deze opleiding heeft &#039;gemiddeld&#039; …&quot;,\n encounter_2 = True)\nAdditional Improvements\n\nWe added a few things to the system of which we will not test the effects:\n\n1 UI Improvements\n\nThe UI was updated for a cleaner look and feel, as well as to improve readability for dyslexic participants.\n\n2 Gamification\n\nWe have decided to add a few small gamification elements. One of those is that through the use of colors (green/red). Another is displaying a score which shows how many answers you got correct out of the total number of trials you have done so far.\n\n3 Multiple Translations\n\nFor some of the words, we added multiple correct answers. This means that multiple different answers can be counted as correct. This is especially useful for verbs, since there are different ways to translate those which do not really change the meaning of the words. An example of this is the word vergeten, which can be translated to forget, to forget, and forgot. Out-of-context, all three of these meanings make sense. Having multiple options be marked as correct also helped with gathering context sentences, since you are not as restricted to one specific use of the word.\n\nSystem\n\nExplains how the system itself works, meaning how it switches between words and context, and how it decided which word to show next.\n\nWord Order\n\nSimilarly to the set-up of SlimStampen, we let the system decide which words to show and how based on rate of forgetting. What we changed is that we have different thresholds for when it shows the context, which context it shows, and when it shows just the word.\nOnce it enters the second context, it will not go back to the first. This is based on the assumption that the first context did not help the participant figure out the meaning of the word. Thresholds were decided through trial-and-error, based on what felt like a natural progression of conditions.\nIn the original system, the same word (and in this case also context) was shown thrice without changing at all based on input. We changed that to twice in our system.\n\nTesting\n\nExplains how we aim to test the system, including each of the conditions and the full experimental procedure.\n\nStimuli and Design\n\nTo test whether the improvements we made to the SlimStampen system have an effect on word retrieval, we want to do a within-participant study. For each condition, we will vary whether words are presented in a sentence, and how many sentences are available. The independent variables in this study are accuracy and response time. The dependent variable is number of contexts available.\n\nConditions\n\nWe are planning to have two conditions:\nBaseline condition: In this condition, participants are only shown words without context.\nTwo-context condition: In this condition, participants will be presented with words in-context as well as out of context. If participants repeatedly give the wrong answer when an item is shown in-context, they will be shown a different context.\n\nComponents\n\nThe experiment consists of different components:\nQuestionnaire: This will ask the participants for some basic info: age, their native language, and their previous experience with learning Dutch.\nLevel Evaluation: Participants will be shown 20 Dutch words and will be asked to translate them to English. We can use this data to compare the Dutch level of each participant, in case we see some weird results. The words were chosen by taking vocabulary from different levels of Dutch (a1-b2).\nTraining: Participants will try to learn new Dutch words using our adapted SlimStampen system. This is where the conditions come into play. This segment lasts either 150 trials or until they have seen all words.\nDistractor: A small dot-counting task that functions as a distractor task. Participants need to do this 10 times between training and testing. The number of blue dots that are shown is between 10 and 20.\nTest: Participants will be tested on the words learned during training using a simple translation task. In this test, they are only presented with words that they saw during training. They immediately receive feedback on how they did.\nBreak: Participants will be asked to take a small break between testing and starting the training in a new condition.\nWe will present each participant with the same two word/context lists, but they will be randomized over the conditions. The order of the conditions will also be randomized.\n\nProcedure\n\nThe experiment will be performed on our personal laptops using OpenSesame in a quiet place. Participants will be presented with a small introduction to the experiment. Then, they will answer a small questionnaire about their Dutch level. After that, they will do a small Dutch test. All participants will be asked to do the same Dutch test. The words in this test will not be used in any of the conditions. This test will provide a frame of reference for each participant’s Dutch level, in case we see unexpected results. After the test, there is a small break.\nNext, participants will be asked to practice a word list in one of the two conditions. This finishes after 150 trials or after a participant has seen all of the words. Next, participants will do a small distractor test. In this distractor test, they are asked to count dots. They need to do this 10 times. Then, we will test their word retrieval with a simple single-word translation test. After this, participants are asked to take a small break. This process is repeated a total of 2 times, once for each of the conditions.\nAfter each participant has done each of the two conditions, we will end the experiment by thanking them for their efforts.\n\nResults\n\nNot significant ):\nNot effective for all learners \nShort time frame\nEffect : Using words in sentences vs Retrieval (Prince, 1996)\n\nLimitations\n\nVery small participant pool\nVariation in participant backgrounds\nCeiling effect\nNo external motivation to do well\nNot enough attention to the context\n\nFuture Directions\n\n\nAdditional focus on gamification\n\n\nLonger term studies (Like SlimStampen)\n\n\nHarder words : Ceiling Effect\n\n\nMore informed context in sentences (van den Broek et al., 2018) \n\n\nOther ways of testing context\n\n\nInvestigate thresholds\n\n\nPictures\n\n\n\n\n\n\n\n\n\n\n\n\n"},"KB/Fine-Grained-assesment":{"title":"Fine Grained assesment","links":["KB/Learning-Event","KB/Knowledge-Component","KB/Issues"],"tags":["usermodel"],"content":"Fine Grained Assesment\nCounting Learning Events\n\nWe also assumed that a step is the product of one or more learning events, and defined a Learning Event to be a mental event based on a Knowledge Component. Learning events and knowledge components are not directly observable, but steps are.\nMastery really means the probability that a Knowledge Component will be applied when it should be applied. If the student’s competence was frozen instead of constantly changing due to learning and forgetting, then mastery could be estimated by counting the number of times a Knowledge Component was applied and dividing by the number of times it should have been applied. Thus, we need to discuss three Issues: (1) How to detect applications of a Knowledge Component, (2) how to detect times when a Knowledge Component should have been applied, and (3) how to adjust for instruction, learning and forgetting.\n\nCounting Failures\n\nCounting only the successful applications of a Knowledge Component is not enough; we need to know how many times the student failed to apply it as well.\nOne approach is to detect failed attempts at steps. Suppose for simplicity that there is a one-to-one correspondence between a step and a Learning Event\nIf the student fails to make the step, then the student must lack the knowledge that underlies the Learning Event.\n"},"KB/Fine-Tuning-Based-Pruning":{"title":"Fine Tuning Based Pruning","links":["KB/Pruning"],"tags":["regularization"],"content":"Fine Tuning Based Pruning\n\nSome store weights before Pruning and use that to continue training.\nOthers somehow try to rewind to a previous state and reinitialize the network entirely\n"},"KB/Fine-grained-datasets":{"title":"Fine grained datasets","links":["KB/CUB-200-2011","KB/FGVCx","KB/iNaturalist","KB/PlantCLEF"],"tags":["dataset"],"content":"Fine Grained Datasets\n\nCUB-200-2011\n[Stanford Dogs](Stanford Dogs.md)\n[FGVC Aircraft](FGVC Aircraft.md)\nFGVCx\niNaturalist\nPlantCLEF\n"},"KB/Fine-grained-Object-Recognition":{"title":"Fine-grained Object Recognition","links":[],"tags":["robotics"],"content":"Fine-grained Object Recognition\n\nAn object can be represented by:\na shared (generic) dictionary, which is used to describe the content of all categories (basic-level)\nand a set of category-specific dictionaries for highlighting the small diversity within the different categories (fine-grained )\n\n"},"KB/Finite-Differences":{"title":"Finite Differences","links":["KB/High-pass-filter"],"tags":["visualization"],"content":"Finite Differences\n\nf&#039;(x) = \\frac{df}{dx} \\rightarrow \\frac{\\Delta f}{\\Delta x}\n\nForward differences f&#039;(x) = \\frac{f(x_{i+1})-f(x_{i})}{\\Delta x}\nNon Isotropic\nBackward differences f&#039;(x) = \\frac{f(x_{i})-f(x_{i-1})}{\\Delta x}\nNon Isotropic\nCentral differences f&#039;(x) = \\frac{f(x_{i+1})-f(x_{i-1})}{2\\Delta x}\nHigh pass filter\nNon isotropic\n"},"KB/First-order-generalization":{"title":"First order generalization","links":[],"tags":["language"],"content":"First Order Generalization\n\nPresent model with an example, ask it to choose which of three objects most likely of the same category\n"},"KB/First-order-integration":{"title":"First order integration","links":[],"tags":["visualization"],"content":"First Order Integration\n\nx(t+ \\Delta t) = x(t)+ \\Delta t f(x,t)\nGlobal error proportional to \\Delta t\nNot stable\n\n\n"},"KB/Fisher-Spanish-English":{"title":"Fisher Spanish-English","links":[],"tags":["dataset"],"content":"Fisher Spanish-English"},"KB/Fitting":{"title":"Fitting","links":[],"tags":["temp"],"content":"Fitting\n\nBayes risk\n\nMinimal expected risk over set of all functions R_B = min_{f\\in y^X} R(f)\nIf minimized → Best possible function\nCapacity of hypothesis space \\mathcal{H}\nIt is essentally all possible things. In reg, all possible affine linear fns. In neural networks, all possible specific connection structure.\n\nIf low, \\mathscr{F} = R(f) - R_B is large : Underfitting (Huge difference between best risk and current risk)\nIf high, \\mathscr{F} = R(f) - R_B is small : Overfitting (Tiny difference between best risk and current risk)\n\n\n\n\n"},"KB/Fixed-Factorization-Attention":{"title":"Fixed Factorization Attention","links":["KB/Attention","KB/Sparse-Transformer","KB/Strided-Attention"],"tags":["architecture"],"content":"Fixed Factorization Attention\n\npaper\nSpecific cells summarize previous locations and propagate to all future cells.\nPart of Sparse Transformer\nFixed Attention pattern with c = 1 limits expressivity\nmany representations in the network are only used for one block whereas a small number of locations are used by all blocks.\nChoosing $c \\in Strided Attention\nwhen using multiple heads, having them attend to distinct subblocks of length c within the block of size l was preferable to having them attend to the same subblock\n\n"},"KB/Fixed-Factors":{"title":"Fixed Factors","links":[],"tags":["language"],"content":"Fixed Factors\n\nAnother term: independent variables\nThis is a “Between subjects ANOVA”\nparticipants go in the same directions…\nBUT, because item is also a random factor, we have to check that too\nThis is a “Between items ANOVA”\nCheck that for each item, what’s the difference between the conditions, and check if they go in the same direction.\n"},"KB/Flamingo":{"title":"Flamingo","links":["KB/Layers","KB/Attention"],"tags":["architecture"],"content":"Flamingo\n\nFlamingo: a Visual Language Model for Few-Shot Learning\nlarge-scale pre-training followed by task-specific fine-tuning has emerged as a standard approach, but the fine-tuning step still requires a lot of samples.\nbuilding models that can be rapidly adapted to numerous tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research\nfamily of Visual Language Models (VLM) which seek to train a multi-modal model (i.e., with the ability to understand different types of input – visual, audio, text etc.) in a few-shot learning approach (which refers to the ability to learn a new task with just a few samples for training).\nbridge powerful pretrained vision-only and language-only models\nhandle sequences of arbitrarily interleaved visual and textual data\nseamlessly ingest images or videos as inputs\nInterleave cross-attention Layers with language-only self-Attention Layers (frozen).\nPerceiver-based architecture that transforms the input sequence data (videos) into a fixed number of visual token\nLarge-scale (web) multi-modal data by scraping webpages which has inter-leaved text and images\nFlamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities\n\n\ntoc: true\ntitle: Flamingo\ntags: [‘architecture’]\nFlamingo\n\nA Visual Language Model created by Deepmind using few shot learning on a wide range of open-ended vision and language tasks, simply by being prompted with a few input/output examples\nthe input of Flamingo contains visually conditioned autoregressive text generation models able to ingest a sequence of text tokens interleaved with images and/or videos\nand produce text as output\nA query is made to the model along with a photo or a video and the model answers with a text answer\nFlamingo models take advantage of two complementary models: a vision model that analyzes visual scenes and a large language model which performs a basic form of reasoning\nThe language model is trained on a large amount of text data.\n"},"KB/Flickr30K":{"title":"Flickr30K","links":[],"tags":["dataset"],"content":"Flickr30K"},"KB/Flipping":{"title":"Flipping","links":[],"tags":["augmentation"],"content":"Flipping\n\nHorizontal axis flipping is much more common than flipping the vertical axis.\nOn datasets involving text recognition such as MNIST or SVHN, this is not a label-preserving transformation.\n"},"KB/FlowNet":{"title":"FlowNet","links":[],"tags":["architecture"],"content":"\nFlowNet\n\nend-to-end convolution neural network for optical flow estimation from two consecutive frames [151], [152]\nConvNet needs to capture appearance changes of two frames\nself-supervised feature learning\nautomatically generated by simulators such as game engines or by hard-code programs without human annotation.\n\n\n"},"KB/Flynn's-Taxonomy":{"title":"Flynn's Taxonomy","links":["KB/SISD","KB/SIMD","KB/MISD","KB/MIMD"],"tags":["parallelcomputing"],"content":"Flynn’s Taxonomy\n\nClassify multi processor architectures\nSISD\nSIMD\nMISD\nMIMD\n"},"KB/Fmix":{"title":"Fmix","links":[],"tags":["augmentation"],"content":"Fmix\n\nrandom binary masks obtained by applying a threshold to low-frequency images sampled from Fourier space. Fmix can take on a wide range of shapes of random masks and can improve performance over Mixup and CutMix\n"},"KB/Focal-Loss":{"title":"Focal Loss","links":["KB/Dense","Sampling","KB/Cross-Entropy","KB/RetinaNet"],"tags":["loss"],"content":"Focal Loss\n\ntwo-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations.\nIn contrast, one-stage detectors that are applied over a regular, Dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far.\nExtreme foreground-background class imbalance encountered during training of Dense detectors is the central cause\nmodulating term to Cross Entropy in order to focus learning on hard misclassified examples\nscaling factor decays to zero as confidence in the correct class increases\ntraining on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training\nRetinaNet\n"},"KB/Focused-Life":{"title":"Focused Life","links":[],"tags":["deepwork"],"content":"Focused Life\n \n\nhabits : simplify the cues to start something\nshutdown ritual\ninversion thinking\n\nwhat can go wrong\n\n\nplan for double the time you think\ngood project - why didnt i do this before?\nI am someone who does X\nhard book/paper - conversation with the author\nhigh intensity feedback + cancel the noise\nteach others\n"},"KB/Foley":{"title":"Foley","links":[],"tags":["medical"],"content":"Foley\n\nA catheter inserted into the bladder to help with urinary drainage\n"},"KB/Force-Directed-Graph-Layout":{"title":"Force Directed Graph Layout","links":["KB/Force"],"tags":["visualization","graph"],"content":"Force Directed Graph Layout\n\nModel a graph as rings and springs\nAttractive forces between adjacent nodes\nedges are modeled as springs with uniform length\nRepulsive forces between non-adjacent nodes could be seen as springs of infinite length or repelling forces of electrically charged metal spheres\n\n"},"KB/Force":{"title":"Force","links":["KB/Acceleration"],"tags":["physics"],"content":"Force\n\n“The vector sum of the external forces F on an object is equal to the mass m of that object multiplied by the acceleration vector of the object.”\n\\Sigma F = ma\nmass times Acceleration\n"},"KB/Forceps":{"title":"Forceps","links":[],"tags":["medical"],"content":"Forceps\n\nA hinged instrument, like scissors, used to grasp and hold objects\n"},"KB/Forgetting":{"title":"Forgetting","links":[],"tags":["robotics"],"content":"Forgetting\n\nIf our memories are too precise and overfitted, then we can’t actually use them to make predictions about future situations\nForgetting is an essential component of adaptive system\nSimple memories that store the gist of our experiences and avoid complicated details will be better for generalizing to future events.\norgetting has been regarded as a passive decay over time of the information stored in the memory.\n\nPassive Forgetting\n\nConcepts stored in the memory can be forgotten “passively” based on: Decay over time (fading factor)\nLoss of context cue\nRetrieval interference\n\nActive Forgetting\n\nmay be more potent at erasing memory than the passive forgetting mechanisms Motivated forgetting\nforgetting is often more intentional\nunpleasant memories (categories) Intrinsic forgetting\nredundant data Interference-based forgetting samples that cause interference\n"},"KB/Forward-Backward-Matching":{"title":"Forward Backward Matching","links":[],"tags":["language"],"content":"Forward Backward Matching\n\nMatching proceeds from the end of the string of characters\nResults are compared\nOptimised Segmentation occurs\nLanguage-specific heuristics are used later\n"},"KB/Forward-Kinematic-Solution":{"title":"Forward Kinematic Solution","links":["KB/Endpoint"],"tags":["robotics"],"content":"Forward Kinematic Solution\n\nThe calculation required to find the Endpoint position, given the joint positions. For most robot topologies this is easier than finding the inverse kinematic solution.\n"},"KB/Forward-Kinematics":{"title":"Forward Kinematics","links":["KB/End-effector"],"tags":["robotics"],"content":"Forward Kinematics\n\nComputational procedures which determine where the End-effector of a robot is located in space. The procedures use mathematical algorithms along with joint sensors to determine its location.\n\n\nFor a robot with n joints, what is the endeffector pose (ξ ), given the joint angles (q)\n\n\nξ = κ(q) : q = {qi,i ∈ [1,…,n]},\n\n\n"},"KB/Fractional-Anisotropy":{"title":"Fractional Anisotropy","links":[],"tags":["visualization"],"content":"Fractional Anisotropy\n\nFA = \\sqrt{\\frac{3}{2}}\\frac{\\sqrt{\\Sigma_{i=1}^{3}(\\lambda_{1}-\\mu)^{2}}}{\\Sigma_{i=1}^{3}\\lambda_{i}^{2}}\n\n"},"KB/Fracture":{"title":"Fracture","links":[],"tags":["medical"],"content":"Fracture\n\nA cracked or broken bone\n"},"KB/Free-Semantic-Label-based-Method":{"title":"Free Semantic Label-based Method","links":[],"tags":["semisupervisedlearning"],"content":"Free Semantic Label-based Method\n\nautomatically generated semantic labels\nThe labels are generated by traditional hardcode algorithms\ngame engines\nmoving object segmentation\ncontour detection\nrelative depth prediction\n"},"KB/Free-morpheme":{"title":"Free morpheme","links":["KB/Morpheme"],"tags":["language"],"content":"Free Morpheme\n\ncan appear as a word by itself, often combined with other morphemes too.\ne.g., house (houses) , walk (walked ) of ,or,the\n"},"KB/Freedom":{"title":"Freedom","links":["KB/Degrees-of-Freedom","KB/Regularization"],"tags":["temp"],"content":"Freedom\n\n(N, D, P) N samples, D Degrees of Freedom\nIf N&lt;D , then ill posed\nNeed N &gt;&gt; D\nIf P learnable params , P&lt;N : underspecified\nIf P &gt;&gt; N : overparameterized\nNo of params not a good indicator of overfitting\nSolution : Regularization\n"},"KB/Frequentist":{"title":"Frequentist","links":["KB/Law-of-large-numbers","KB/MLE"],"tags":["temp"],"content":"Frequentist\n\nMeasure probablity → Counting\nRepeat an experiment n times and get the estimate : \\hat P (estimate based on finite amount of data)\nLaw of large numbers\nRandom variable X which takes values in a sample space S.\nMeasurement process hard to carry out in reality\nWhat does unbiased means? Especially because most things related to future input that we do not have yet\nDistibution of data points\nMLE\n"},"KB/Friction":{"title":"Friction","links":["KB/Force","KB/Static-Friction","KB/Kinetic-Friction"],"tags":["physics"],"content":"Friction\n\nFriction scales linearly with the normal Force.\nFriction is not affected by the area of contact between surfaces.\nStationary objects have more friction than sliding objects.\nSliding friction is not affected by sliding velocity.\nYou can look up the magnitude of friction for each pair of materials\nStatic Friction\nKinetic Friction\n"},"KB/Frobenius-norm":{"title":"Frobenius norm","links":[],"tags":["math"],"content":"Frobenius Norm\n\nThe Frobenius norm, sometimes also called the Euclidean norm\n[Lp Regularization](Lp Regularization.md)\nWhen p = q = 2 for the L_{p,q}norm, it is called the Frobenius norm or the Hilbert–Schmidt norm, though the latter term is used more frequently in the context of operators on (possibly infinite-dimensional) Hilbert space.\n\n{ \\|A\\|_{\\text{F}}={\\sqrt {\\sum _{i}^{m}\\sum _{j}^{n}|a_{ij}|^{2}}}={\\sqrt {\\operatorname {trace} \\left(A^{*}A\\right)}}={\\sqrt {\\sum _{i=1}^{\\min\\{m,n\\}}\\sigma _{i}^{2}(A)}},}\n\nwhere { \\sigma _{i}(A)} are the singular values of A\n"},"KB/Front-to-Back-Raycasting":{"title":"Front to Back Raycasting","links":["KB/Raycasting","KB/Color-Compositing"],"tags":["visualization"],"content":"Front to Back Raycasting\n\n\nColor Compositing\n"},"KB/Frontal-Operculum":{"title":"Frontal Operculum","links":["KB/Insula"],"tags":["brain"],"content":"Frontal Operculum\n\nThe part of the [frontal lobe](frontal lobe.md) that sits over the Insula.\n"},"KB/Frontal-lobe":{"title":"Frontal lobe","links":["KB/Brocas-Area"],"tags":["brain"],"content":"Frontal Lobe\n\nPersonality, behavior, emotions\nJudgment, planning, problem solving\nSpeech: speaking and writing (Brocas Area)\nBody movement (motor strip)\nIntelligence, concentration, self awareness\n"},"KB/Function-words":{"title":"Function words","links":["KB/Determiners","KB/Quantifiers","KB/Prepositions","KB/Connectives"],"tags":["language"],"content":"Function Words\n\nGlues words and phrases together\nDeterminers\nQuantifiers\nPrepositions\nConnectives\n"},"KB/Functional-Connectivity":{"title":"Functional Connectivity","links":["KB/BrainWave-Synchronization","KB/BrainWave-Coherence","KB/BrainWave-CrossFrequency-Coupling","KB/Granger-Causallity"],"tags":["brain"],"content":"Functional Connectivity\n\n\n\nSymmetric\n\nBrainWave Synchronization\nBrainWave Coherence\nBrainWave CrossFrequency Coupling\n\nDirected/Asymmetric\n\nGranger Causallity\n"},"KB/Functional-Morpheme":{"title":"Functional Morpheme","links":["KB/Morpheme"],"tags":["language"],"content":"Functional Morpheme\n\nprovides grammatical information\ne.g. s (plural ) third person singular\n"},"KB/Functional-correlates":{"title":"Functional correlates","links":["KB/Dimensionality-Reduction","KB/Correlation"],"tags":["temp"],"content":"\ntoc: true\ntitle: Functional correlates\ntags: [‘temp’]\n\nFunctional Correlates\n\nDimensionality Reduction technique used to quantify the Correlation and dependence between two variables when the data is functional\nRelations between the surface and phenomena that influence or are influenced by the topography.\n"},"KB/Fundamentals":{"title":"Fundamentals","links":["KB/Emperical-Risk","KB/LinearRegression","KB/TemporalLearning","KB/Dimensionality-Reduction","KB/Unsupervised-Learning","KB/Semi-Supervised","KB/Self-Supervised","KB/Encodings","KB/Probability","KB/Universal-Approximation-Theorem","Sampling","KB/Distributions"],"tags":["temp"],"content":"Fundamentals\n\nEmperical Risk\nLinearRegression\nTemporalLearning\nDimensionality Reduction\nUnsupervised Learning\nSemi Supervised\nSelf Supervised\nEncodings\nProbability\nUniversal Approximation Theorem\nSampling\nDistributions\n"},"KB/GAM":{"title":"GAM","links":[],"tags":["explainability"],"content":"GAM\n\nEarly explaining systems for ML black boxes go back to 1986 with Generalized Additive Models (GAM)\nGAMs are global statistic models that use smooth functions, which are estimated using a scatterplot smoother\nThe technique is applicable to any likelihood-based regression model, provides a flexible method for identifying nonlinear covariate effects in exponential family models and other likelihood-based regression models, and has the advantage of being completely automatic\nIn its most general form, the algorithm can be applied to any situation in which a criterion is optimized involving one or more smooth functions\n"},"KB/GAN-Z-Space":{"title":"GAN Z Space","links":[],"tags":["architecture"],"content":"GAN Z Space\nVector Algebra in Z-Space\n\nControllable generation is somewhat similar to interpolation.\nWith interpolation, you get intermediate examples between two generated observations.\nThese intermediate examples between to two targets by manipulating the inputs from Z-space, which is the same idea behind controllable generation.\nIn order to get intermediate values between two images, for example, you can make an interpolation between their two input vectors v1 and v2 in the Z-space.\nControllable generation also uses changes in Z-space and makes use of how adjustments to the noise vector are reflected in the output from the generator.\nDifferences in the features generated, for example different hair colors, occur due to changes in the direction that you have to move in Z-space to modify the features of the image.\nIf image output of g(v_{1}) , new controlled output with g(v_{1}+d)\n"},"KB/GAN‐based-Data-Augmentation":{"title":"GAN‐based Data Augmentation","links":[],"tags":["augmentation"],"content":"GAN‐based Data Augmentation\n\nBowles et al. describe GANs as a way to ‘unlock’ additional information from a dataset\nAnother useful strategy for generative modeling worth mentioning is variational auto-encoders. The GAN framework can be extended to improve the quality of samples produced with variational auto-encoders\nUsing CycleGANs to translate images from the other 7 classes into the minority classes was very effective in improving the performance of the CNN model on emotion recognition.\nAs exciting as the potential of GANs is, it is very difficult to get high-resolution outputs from the current cutting-edge architectures. Increasing the output size of the images produced by the generator will likely cause training instability and non-convergence\n"},"KB/GAU":{"title":"GAU","links":["KB/Attention"],"tags":["architecture"],"content":"GAU\n\ngated Attention unit; a generalization of GLU - gated linear unit\nallows for better and more efficient approximation of multi-head Attention than many other efficient Attention methods by using a weaker single-head Attention with minimal quality loss\n"},"KB/GE2E":{"title":"GE2E","links":["KB/Speaker-Verification","KB/Embedding","KB/MultiReader-technique"],"tags":["loss"],"content":"GE2E\n\nGeneralized End-to-end Loss for Speaker Verification\nnew loss function\ntraining of Speaker Verification models more efficient\nUnlike TE2E, the GE2E loss function updates the network in a way that emphasizes examples that are difficult to verify at each step of the training process\npushes the Embedding towards the [centroid] of the true speaker, and away from the [centroid](centroid] of the true speaker, and away from the [centroid.md) of the most similar different speaker\ndoes not require an initial stage of example selection\nMultiReader technique\n"},"KB/GELU":{"title":"GELU","links":["KB/Relu","KB/Normal-Distribution","KB/CDF","KB/GPT3","KB/Transformer","KB/Vision-Transformer","KB/BERT"],"tags":["architecture"],"content":"GELU\n\nPaper\nSmoother Relu\nx\\Phi(x) where \\Phi(x) is the Normal Distribution CDF\nWeights inputs by percentile, rather than by sign like Relu\nGELU(x) = xP(X \\leq x) = x\\Phi(x) = x. \\frac{1}{2}\\left[ 1+erf\\left( \\frac{x}{\\sqrt{ 2 }} \\right) \\right]\nIf X \\sim \\mathscr{N}(0,1)\nUsed in GPT3, Transformer, Vision Transformer, BERT\n\n"},"KB/GGCNN":{"title":"GGCNN","links":["KB/Gripper"],"tags":["robotics"],"content":"GGCNN\n\nLearning an object agnostic function to grasp objects\nGrasping using uni-modal data (depth image).\nGenerate pixel-wise grasp configuration for the given input.\nThe Gripper approaches the target object in top-down manner. Uses a shallow network, and an eye-in-hand camera configuration.\n\nMorrison, Douglas, Peter Corke, and Jürgen Leitner. “Closing the loop for robotic grasping: A real-time, generative grasp synthesis approach.” RSS (2018).\n"},"KB/GLOW":{"title":"GLOW","links":[],"tags":["architecture"],"content":"GLOW\ndef unsqueeze2d_new(input, factor=2):\n    return rearrange(input, &#039;b (c h2 w2) h w -&gt; b c (h h2) (w w2)&#039;, h2=factor, w2=factor)\n \ndef squeeze2d_new(input, factor=2):\n    return rearrange(input, &#039;b c (h h2) (w w2) -&gt; b (c h2 w2) h w&#039;, h2=factor, w2=factor)"},"KB/GLUE":{"title":"GLUE","links":[],"tags":["dataset"],"content":"GLUE"},"KB/GOMS":{"title":"GOMS","links":[],"tags":["usermodel"],"content":"GOMS\nParts\n\nGoals\nOperations\nMethods\nSelection Rules\n\nRest\n\nHuman Computer Interaction\nAtomic\nReactive vs proactive\nPerformance vs control\n"},"KB/GPT":{"title":"GPT","links":["KB/Unsupervised-Learning","KB/Log-Likelihood-Loss"],"tags":["architecture"],"content":"GPT\n\nPretrained using Unsupervised Learning and finetuned\nLog Likelihood Loss\n\n"},"KB/GPT3":{"title":"GPT3","links":["KB/Autoregressive","KB/Issues"],"tags":["architecture"],"content":"GPT3\n\nLanguage Models are Few-Shot Learners\nshows that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches\nAutoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting\nwithout any gradient updates or fine-tuning\non-the-fly reasoning or domain adaptation\nmethodological Issues related to training on large web corpora\ncan generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans\n"},"KB/GQL-for-SQL-Users":{"title":"GQL for SQL Users","links":["KB/Complete-AI-Pipeline","KB/Graphs"],"tags":["conference","sql"],"content":"GQL for SQL Users\n \n\nby Keith Hare\nsql : relational tables\ngql : property graphs\n\nWhy Standards\n\nless chaos\ncompetition on performance and not on language\nconsistent interfaces: across db vendors\n\nnot always same syntax but better for db users to migrate what they know\n\n\nbetter knowledge transfer\nRDO?\n\nProperty Graphs\n\nnodes and relationships → synthetic identity\nquery without knowing relationship\nthe “problem” → not recognising the problem is a graph\nnode/edge\ncase sensitive names\nlinear composition\ncreating data\n\nschema-less\n\njust insert data\nno restrictions on content\n\n\ngraph schema\n\ncreate graph type\ngraphs using type\n\n\n\n\nnon procedural languages\nCRUD - create read update and delete\nIn sql, constrains are evaluated when the data is inserted, but not automatically encoded in query\n\naka must include the relationship in the query : cartesian product (probably annoying)\n\n\n?? performance vs sql for a large graph esp add and insert for graph traversal?\n\nthe speaker isnt focused on this for now\n“thats why we have phd students” xD\n\n\n?? backup and restore graph connections\n\ncommit and rollback\n\n\nnodes have () (cuz when you draw it on a whiteboard you draw a circle)\nedges are by → and []\n\nSchema-less\n\nschema by inserting data so no restriction\nintitial version of GQL - no constraints\n\nfixed by having a closed graph\n\n\n\nGraph Pattern Matching\n\nlanguage for querying graphs\nquantified path patterns\ncheapest path\nmultiset alternation\ntypes\n\nsimple\nfiltering\nunion\nalternation\naggregation\n\n\n\nSQL/PGQ - Property Graphs in Sql\n\nsql + all the above\ndoes not support schema flexible graphs\nduckdb\npostgre sql\n"},"KB/GRConvNet":{"title":"GRConvNet","links":[],"tags":["robotics"],"content":"GRConvNet\n\nGenerative Residual Convolutional Neural Network\nLearning an object agnostic function to grasp objects\nUses multi modal input data (RGB + depth images).\nGenerates pixel-wise antipodal grasp configuration.\nState-of-the-art performance (97% on Cornell dataset).\nUse eye-to-hand camera configuration.\nSulabh Kumra, et al. “Antipodal robotic grasping using generative residual convolutional neural network.” IROS 2020.\n\n"},"KB/GRU":{"title":"Gated Recurrent Unit (GRU)","links":["KB/Recurrent","LSTM","KB/Sigmoid","KB/Interpolation"],"tags":["architecture"],"content":"Gated Recurrent Unit (GRU)\n\n\nSimplified [LSTM)](Long Short Term Memory (LSTM|Long Short Term Memory (LSTM|LSTM)](LSTM)](Long Short Term Memory (LSTM|Long Short Term Memory (LSTM|LSTM).md).md)\nIt has an input and forget gate, no output gate\nFaster than LSTM in training, but does not perform well in many tasks\nTries to forget what is not important\n\nThe Math\n\nTwo gates, Sigmoid\n\nReset : g_r = \\sigma(W_{hr}h_{t-1} + W_{xr}x_t + b_r)\nUpdate : g_u = \\sigma(W_{hu}h_{t-1} + W_{xu}x_t + b_u)\n\n\nHidden state proposal\n\n\\hat h_t = tanh(W_{xh}x_t + W_{hh}g_r\\cdot h_{t-1} + b_h)\n\n\nFinal hidden state\n\nLinear Interpolation between last hidden state and proposal\nh_t = (1-g_u)\\cdot h_{t-1} + g_u \\cdot \\hat h_t\n\n\n"},"KB/GTA5":{"title":"GTA5","links":[],"tags":["dataset"],"content":"GTA5"},"KB/Galactica":{"title":"Galactica","links":[],"tags":["architecture"],"content":"Galactica\n\nnew large model for automatically organizing science developed by Meta AI and Papers with Code\nability to train on it for multiple epochs without overfitting, where upstream and downstream performance improves with use of repeated token\nThe dataset design is critical to the approach as all of it is processed in a common markdown format to blend knowledge between sources.\nCitations are processed via a certain token that allows researchers to predict a citation given any input context\nThe capability of the model of predicting citations improves with scale and the model becomes better at the distribution of citations\nthe model can perform multi-modal tasks involving SMILES chemical formulas and protein sequences\ntransformer architecture in a decoder-only setup with GeLU activation for all model sizes.\n"},"KB/Game-Based-Learning":{"title":"Game Based Learning","links":[],"tags":["usermodel"],"content":"Game Based Learning\n\ngamified learning tasks\nimprove engagement\nLearning progress\nInteractivity\n\nSeductive details\n\ninteresting, but irrelevant\n\n\nShould support a specific function\n\n\nDoes it work?\n\nMixed\nStudies use many game elements in a study - makes it hard to understand if any work\nNot a lot of directed, guided\nMakes it fun though\n\n\n"},"KB/Gamification":{"title":"Gamification","links":["KB/Serious-Games","KB/Game-Based-Learning"],"tags":["usermodel"],"content":"Gamification\n\nUse of game-like design elements\nNon game concepts\nInterface\nMechanics\nDesign\nSerious Games\nGame Based Learning\n"},"KB/Gaming-addiction":{"title":"Gaming addiction","links":["KB/fMRI","KB/Brain-Areas","KB/Nucleus-Accumbens"],"tags":["temp"],"content":"\ntoc: true\ntitle: Gaming addiction\ntags: [‘temp’]\n\nGaming Addiction\n\nfMRI was performed while showing game images to online game addicts.\nAccording to Brain Areas control group, right orbitofrontal cortex, right Nucleus Accumbens, bilateral anterior cingulate and medial frontal cortex, right dorsolateral prefrontal cortex and right caudate nucleus activation were observed.\nThese areas are the rewarding areas\nThe results show that the same addiction to substance can share the same neuro-biological mechanisms with the extreme gaming demands of online gaming addiction.\nPaper\n"},"KB/Gamma-Waves":{"title":"Gamma Waves","links":["KB/Attention"],"tags":["brain"],"content":"Gamma Waves\n\n28-90 Hz\nAttention/Consciousness\n\n"},"KB/Gamma-aminobutyric-Acid-(GABA)":{"title":"Gamma-aminobutyric Acid (GABA)","links":[],"tags":["brain"],"content":"Gamma-aminobutyric Acid (GABA)\n\nA neurotransmitter implicated in brain development, muscle control, and reduced stress response\n"},"KB/Gantry-Robot":{"title":"Gantry Robot","links":["KB/Degrees-of-Freedom"],"tags":["robotics"],"content":"Gantry Robot\n\nA robot which has three Degrees of Freedom along the X, Y and Z coordinate system. Usually consists of a spooling system (used as a crane), which when reeled or unreeled provides the up and down motion along the Z axis. The spool can slide from left to right along a shaft which provides movement along the Z axis. The spool and shaft can move forward and back along tracks which provide movement along the Y axis. Usually used to position its end effector over a desired object and pick it up.\n"},"KB/Gas-Law":{"title":"Gas Law","links":[],"tags":["temp"],"content":"Gas Law\n\ncombination of Boyle’s Law and Charles’ Law\n\\frac{P_{1}V_{1}}{T_{1}}= \\frac{P_{2}V_{2}}{T_{2}}\nTemp must be in Kelvin\npressure x volume of a gas = number of moles x molar gas constant x absolute temperature\npV = nRT\n"},"KB/Gato":{"title":"Gato","links":["KB/robotics"],"tags":["architecture"],"content":"Gato\n\nA Generalist Agent\nGato\nsingle generalist agent beyond the realm of text outputs, inspired by progress in large-scale language modeling\nmulti-modal, multi-task, multi-embodiment generalist policy\nsame network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens\nTo enable processing this multi-modal data from different tasks and modalities, it is serialized into a flat sequence of tokens\nIn this representation, Gato can be trained and sampled from akin to a standard large-scale language model\nMasking is used such that the loss function is applied only to target outputs, i.e text and various actions\nDuring deployment, sampled tokens are assembled into dialogue responses, captions, button presses, or other actions based on the context\nTransformer sequence models are effective as multi-task multi-embodiment policies, including for real-world text, vision and robotics tasks\n"},"KB/Gaussian-Baseline":{"title":"Gaussian Baseline","links":["KB/Smooth-Grad"],"tags":["explainability"],"content":"Gaussian Baseline\n\nOne of the first propositions was to add Gaussian noise to the original image\nGaussian baseline was introduced by Smilkov et al. @smilkovSmoothGradRemovingNoise2017\nused a Gaussian distribution centered on the current image with a variance σ\\sigmaσ.\nThis variance is the only parameter when tuning the method\nThis is in Smooth-Grad\n"},"KB/Gaussian-Distortion":{"title":"Gaussian Distortion","links":[],"tags":["augmentation"],"content":"Gaussian Distortion\n\nGrid width and height, and magnitude are kept the same as the random distortion values of 6, 6, and 5, respectively\nThe Gaussian distortion has the added parameters of applying the distortion based on the 2D normal distribution\nnormal distortion is applied to each grid point on a circular surface (corner=“bell”) and with default values for the mean and standard deviation (\\mu_{x} = \\mu_{y} = 0.5,\\sigma_{x} = \\sigma_{y} = 0.05)\np(x,y) = exp\\{-(\\frac{(x-\\mu_{x})^{2}}{\\sigma_{x}} + \\frac{(x-\\mu_{y})^{2}}{\\sigma_{y}}))\\}\n"},"KB/Gaussian-Filter":{"title":"Gaussian Filter","links":["KB/Filtering"],"tags":["visualization"],"content":"Gaussian Filter\n\nFiltering with a discretized Gaussian function\nWeights follow G(x) = e^{-ax^{2}}\n"},"KB/Gaze-position":{"title":"Gaze position","links":["KB/Attention","KB/Pupil-Dilation"],"tags":["usermodel"],"content":"Gaze Position\n\nWhere the subject is looking\nProcess how that moves with new stimuli\nPupil size\nGaze direction is a good metric of Attention\nPupil Dilation\n"},"KB/GenEth":{"title":"GenEth","links":[],"tags":["ethics"],"content":"GenEth\n\nethical dilemma analyzer\nethical issues related to intelligent systems are likely to exceed the grasp of the original system designers, and designed GenEth to include ethicists into the discussion process in order to codify ethical principles in given application domains.\nFeatures: denoting the presence or absence of factors (e.g.,harm,benefit) with integer values;\nDuties: denoting the responsibility of an agent to minimize/maximize a given feature;\nActions: denoting whether an action satisfies or violates certain duties as an integer tuple;\nCases: used to compare pairs of actions on their collective ethical impact\nPrinciples: denoting the ethical preference among different actions as a tuple of integer tuples.\n"},"KB/Gene-Expression":{"title":"Gene Expression","links":["KB/Nucleotide"],"tags":["brain"],"content":"Gene Expression\n\nThe process by which a gene\n’s Nucleotide sequence is transcribed into the form of RNA\n—often as a prelude to being translated into a protein.\n"},"KB/Generalization-Curve":{"title":"Generalization Curve","links":[],"tags":["temp"],"content":"Generalization Curve\n\nA loss curve showing both the training set and the validation set. A generalization curve can help you detect possible overfitting. For example, the following generalization curve suggests overfitting because loss for the validation set ultimately becomes significantly higher than for the training set.\n"},"KB/Generalizing-Adversarial-Explanations-with-Grad-CAM":{"title":"Generalizing Adversarial Explanations with Grad-CAM","links":["KB/Grad-CAM","KB/Adversarial-Learning","KB/Normalized-Inverted-Structural-Similarity-Index","KB/FGSM","KB/Mean-Observed-Dissimilarity","KB/Variation-in-Dissimilarity-Variation-in-Dissimilarity","VGGFace2"],"tags":["explainability"],"content":"Generalizing Adversarial Explanations with Grad-CAM\n\n[@Generalizing Adversarial Explanations with Grad-CAM](@Generalizing Adversarial Explanations with Grad-CAM.md)\nChakraborty, Tanmay, Utkarsh Trehan, Khawla Mallat, and Jean-Luc Dugelay. “Generalizing Adversarial Explanations with Grad-CAM.” In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 186–92. New Orleans, LA, USA: IEEE, 2022. doi.org/10.1109/CVPRW56347.2022.00031.\nAdversarial Learning\n\nIntro\n\nThe drawback of Grad-CAM is that it cannot be used to generalize CNN behaviour.\nextends Grad-CAM from example-based explanations to a method for explaining global model behaviour\nThese metrics are computed by comparing a Normalized Inverted Structural Similarity Index (NISSIM) metric of the Grad-CAM generated heatmap for samples from the original test set and samples from the adversarial test set.\nWe observe a consistent shift in the region highlighted in the Grad-CAM heatmap, reflecting its participation to the decision making, across all models under adversarial attacks.\nThese adversarial attacks display specific properties, i) They are not perceptible to the human eye, ii) They are controllable, and iii) Transferability, i.e., an attack designed for one model is capable of attacking multiple models\nThere are mainly two kinds of attacks: targeted and non-targeted attacks. Targeted attack makes a model predict a certain label for the adversarial example, while for non-targeted attacks the labels for adversarial examples are not important, as long as the model is wrong\nThese attacks can also be subdivided into black-box attacks and white-box attacks. Black-box attacks have no information about the target model, training procedure, architecture, whereas white-box attacks know the target model, training procedure, architecture, parameters.\nThe CKA-similarity algorithm was used to compare the hidden representations of broad and deep models . They found that when the model capacity is large compared to the training set, a block structure emerges, which shows that the models propagate the main component of their hidden representation.\nMore recent methods leverage explainability of machine learning and use SHAP based signatures to detect adversarial attacks\nAs a result, we observed a global pattern displayed by all models. The shifting in the region of participation can be defined as when a model sees adversarial examples. Some parts of the input image no longer participate in the decision-making, while new parts do participate.\nThese changes are not deterministic, and given an adversarial example, there is no way to tell how it will affect the shift\nFGSM\n\nNew Metrics\n\nNormalized Inverted Structural Similarity Index\nMean Observed Dissimilarity\nVariation in Dissimilarity Variation in Dissimilarity\n\nFace Dataset Case Study\n\nVGGFace2\nFirst, we preprocess the dataset to align and crop the faces. Then, the dataset is split into 80% training, 10% testing, and 10% validation sets.\nOnce the training step is completed, the stored models are loaded and used to generate perturbations from the test set using FGSM.\nThen the test set is attacked with different values of \\epsilon from the stored perturbations and these counterexamples are stored as perturbed test sets\nFinally, Grad- CAM was used to generate heatmaps for every layer in each model and each ϵ in the perturbed test set.\nVGG 16: we can observe clearly that all the attacks were successful and illustrates a clear shift of participating regions as the ϵ increases.\nResNet50 : the number of layers are too many to pin point out some example, yet if observed very carefully the hidden layers as the ϵ increases, we can find a shifting in the region of participation.\nIn ResNet101: it seems more resilient there are some observable region shifts, but overall much less.\nInceptionNet v3 : seems to have learnt something different, the focus was more on forehead than face, but the overall shifting is much higher for this model, we even see focus regions getting inverted as the ϵ increases.\nFor XceptionNet : the phenomenon is more clear, some regions get expanded, and background areas are being highlighted.\nWe use the heatmap obtained from the original image (without adversarial perturbation) as our ground truth heatmap, i.e., what the model expects to see in order to make a decision, then the second heatmap is generated from the adversarially attacked image, and we create a dataframe with all the NISSIM values for the entire test set comparing with the adversarial test set for all values of ϵ.\nWe can also use this metric to explain the performance of VGG16. Since the shift was smaller, the model was less likely to fail.\nWe also find that deep networks perform better than wide networks for similar shifts\nThe main idea, that is examined here, is that the lower the shift in distribution, more the model is robust to adversarial attacks\nThis indicates that VGG16 is a stable model for this task, over the other models.\n\nObservations\n\nWe see a shift in the focus of the model in different directions, sometimes backgrounds get highlighted, other times, participation region expands or shrinks.\nDeeper models are much robust to this changes, for similar amount of shift, deeper models provide better performance than wider models.\nneural networks fail because of a shifting behaviour in the region of participation to the decision-making, when the model sees adversarial examples, its focus changes and it now sees a different hidden representation\nThe main observation to keep in mind is, as ϵ increases, the dissimilarity increases, indicating that the focus of the model is diverted when it is presented an adversarial example, this value indicates that the more the examples differ, the more likely the model will fail.\nWe can observe a pattern that wide models fail more than deep models as the ϵ increases\n\nImages\n\n\n\n\n"},"KB/Generative-Models":{"title":"Generative_Models","links":["GAN"],"tags":["architecture"],"content":"Generative_Models\n\n[Basic GAN](Basic GAN.md)\nGAN\n"},"KB/Generative-RNN":{"title":"Generative RNN","links":["KB/Softmax"],"tags":["architecture"],"content":"Generative RNN\n\ninitial sequence is used as seed and output is sampled \n\nrandom or argmax to sample\nnormally not taking argmax but sample with respective Softmax probabilities → allows to generate something different than input\n\n\nnew output is used as seed to generate next \nrepeat until termination criterion\n"},"KB/Generative-Spoken-Language-Modeling":{"title":"Generative Spoken Language Modeling","links":[],"tags":["architecture"],"content":"Generative Spoken Language Modeling\n\nGenerative Spoken Language Modeling from Raw Audio\nlearns speech representations from CPC, Wav2Vec2.0, and HuBERT for synthesizing speech\ntask of learning the acoustic and linguistic characteristics of a language from raw audio\net of metrics to automatically evaluate the learned representations at acoustic and linguistic levels for both encoding and generation\nset up baseline systems consisting of a discrete speech encoder (returning pseudo-text units)\ngenerative language model (trained on pseudo-text)\nspeech decoder (generating a waveform from pseudo-text)\ntrained without supervision\nnumber of discrete units (50, 100, or 200) matters in a task-dependent and encoder-dependent way, and that some combinations approach text-based systems\n"},"KB/Generative-vs-Discriminative-Models":{"title":"Generative vs Discriminative Models","links":["KB/LDA","HMM","KB/Autoregressive","SVM"],"tags":["architecture"],"content":"Generative vs Discriminative Models\nGenerative\n\nLDA\n[Bayesian Model Estimation](Bayesian Model Estimation.md)\nHMM\nAutoregressive\n[Basic GAN](Basic GAN.md)\n\nDiscriminative\n\n[Logistic Regression](Logistic Regression.md)\nSVM\n[Decision Trees](Decision Trees.md)\n[Random Forest](Random Forest.md)\n"},"KB/Geometric-Transformations":{"title":"Geometric Transformations","links":[],"tags":["augmentation"],"content":"Geometric Transformations\n\nThe safety of a Data Augmentation method refers to its likelihood of preserving the label post-transformation.\nA non-label preserving transformation could potentially strengthen the model’s ability to output a response indicating that it is not confident about its prediction. However, achieving this would require refined labels post-augmentation.\nDue to the challenge of constructing refined labels for post-augmented data, it is important to consider the ‘safety’ of an augmentation. This is somewhat domain dependent\n"},"KB/Gestalt-Laws":{"title":"Gestalt Laws","links":[],"tags":["visualization"],"content":"Gestalt Laws\n\n\nGood form can dominate other laws\ncrossing swarms in our visual field are perceived as different swarms\n"},"KB/Git-Commands":{"title":"Git Commands","links":[],"tags":["architecture"],"content":"Git Commands\n\nSources\n\nThe Essential GitHub CLI Commands\n\n\n\nManaging Gists\ngh gist create my\\_mergify\\_gist.py\n\ngh gist create --public my\\_mergify\\_gist.py\n\nList All Your Gists\ngh gist list\n\n\nYou can also apply filters on this list using the --limit int argument (default to 10) along with the --public and --secret flags.\n\nView\ngh gist view 4b5ba0b5daabf386ee01bc37ab667e58\n\nDelete\ngh gist delete 4b5ba0b5daabf386ee01bc37ab667e58\n\nManaging Issues\nCreating an Issue\ngh issue create --toc: true\ntitle &quot;Is it a bug?&quot; --body &quot;the behavior’s description&quot;\n\nListing All the repository’s Issues\ngh issue list\n\n\nYou can even open your browser with --web\n\nStatus\ngh issue status\n\nClosing an Issue\ngh issue close &lt;num&gt;\n\nReopening an Issue\ngh issue reopen &lt;num&gt;\n\nManaging Repositories\nCreate a Public Repository\ngh repo create\n\nForking a Repository\ngh repo fork Mergifyio/react-crisp\n\nListing the Repository of an account\ngh repo list CamClrt\n\n\nYou can filter this list down using the --archived, --no-archived, or --source flags.\n\nManaging PRs\nCreating a Pull Request with a Specific Title and Body\ngh pr create --toc: true\ntitle &quot;feat: my\\_super\\_feature&quot; --body &quot;all the details&quot; \n\nListing All the Pull Requests in the Repository\ngh pr list\n\n\nthis command allows you to apply a large number of filters like --assignee, --base, --label, and more\n\nStatus of Your Pull Requests\ngh pr status\n\nGetting a Pull Request to Inspect it\ngh pr checkout 2530\n\nDisplaying Continuous Integration (CI) Status for a Specific Pull Request\ngh pr checks 1234\n\nDiff\ngh pr checkout &lt;num&gt;\ngh pr diff\n\nMerge\ngh pr merge &lt;num&gt;\n\ngh pr merge -m -d &amp;lt;number&amp;gt; &amp;amp;&amp;amp; git pull\n\nDisplay the Title, Body, and other Information about a Pull Request.\ngh pr view\n\nMake a Pull Request as Ready for Review\ngh pr ready\n\nAdd a Review to a Pull Request\ngh pr review\n\nClose/reopen\ngh pr &lt;close, reopen&gt;\n"},"KB/Glia":{"title":"Glia","links":["KB/Central-Nervous-System","KB/Astrocyte","KB/Microglia","KB/Ependymal-Cell","KB/Ogliodendrocytes","KB/Peripheral-Nervous-System","KB/Satellite-Cell","KB/Schwann-Cell"],"tags":["brain"],"content":"Glia\n\nThe supporting cells of the central nervous system. They may contribute to the transmission of nerve impulses and play a critical role in protecting and nourishing neurons.\nPreviously thought of as protective covering\nCentral Nervous System\n\nAstrocyte\nMicroglia\nEpendymal Cell\nOgliodendrocytes\n\n\nPeripheral Nervous System\n\nSatellite Cell\nSchwann Cell\n\n\n"},"KB/Glioblastoma":{"title":"Glioblastoma","links":[],"tags":["brain"],"content":"Glioblastoma\n\nAn invasive brain tumor made up of glial tissue, blood vessels, and dead neurons.\n"},"KB/Glioma":{"title":"Glioma","links":[],"tags":["brain"],"content":"Glioma\n\nA tumor that arises from the brain’s glial tissue.\n"},"KB/GloVE":{"title":"GloVE","links":["KB/Word2Vec","KB/Unsupervised-Learning","KB/LinearRegression"],"tags":["architecture"],"content":"GloVE\nExplanation\n\nGloVe: Global Vectors for Word Representation\nWord2Vec relies only on local information of language. That is, the semantics learnt for a given word, is only affected by the surrounding words.\nUnsupervised Learning algorithm which captures both global statistics and local statistics of a corpus\naggregated global word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space\nwhether distributional word representations are best learned from count-based methods or from prediction-based methods\nprobe the underlying co-occurrence statistics of the corpus\nreformulated Word2Vec optimizations as a special kind of factorization for word co-occurence matrices\nNote that GloVe does not use neural networks\nutilizes this main benefit of count data while simultaneously capturing the meaningful linear substructures prevalent in recent log-bilinear prediction-based methods like Word2Vec\nglobal log-bilinear LinearRegression model for the Unsupervised Learning of word representations\n\nThere’s a straight red column through all of these different words. They’re similar along that dimension (and we don’t know what each dimensions codes for)\nThere are clear places where “king” and “queen” are similar to each other and distinct from all the others. Could these be coding for a vague concept of royalty?\n\nAnalogies\n\n\n"},"KB/Global-Average-Pooling":{"title":"Global Average Pooling","links":[],"tags":["architecture"],"content":"Global Average Pooling\ngraph TD;\n\nE1[Averages activations of each feature map] --&gt; E2[concatenates them] --&gt; E3[outputs as a vector]\n"},"KB/Global-Classification-Accuracy":{"title":"Global Classification Accuracy","links":[],"tags":["robotics"],"content":"Global Classification Accuracy\n\nan accuracy computed using all predictions in a complete experiment\n"},"KB/Global-Gradient-Magnitude-Based-Pruning":{"title":"Global Gradient Magnitude Based Pruning","links":["KB/Pruning"],"tags":["regularization"],"content":"Global Gradient Magnitude Based Pruning\n\nIdentifies lowest absolute value (weight*gradient) in the whole network and removes them\n"},"KB/Global-Magnitude-Based-Pruning":{"title":"Global Magnitude Based Pruning","links":["KB/Pruning"],"tags":["regularization"],"content":"Global Magnitude Based Pruning\n\nTakes the lowest values in the entire network. Drops them.\n"},"KB/Global-and-Sliding-Window-Attention":{"title":"Global and Sliding Window Attention","links":["KB/Sliding-Window-Attention","KB/Dilated-Sliding-Window-Attention","KB/Attention"],"tags":["architecture"],"content":"Global and Sliding Window Attention\n\nSliding Window Attention and Dilated Sliding Window Attention are not always enough\nglobal Attention” on few pre-selected input locations.\nThis Attention is operation symmetric: that is, a token with a global Attention attends to all tokens across the sequence, and all tokens in the sequence attend to it\n\n"},"KB/Glucose":{"title":"Glucose","links":[],"tags":["brain"],"content":"Glucose\n\nA natural sugar that is carried in the blood and is the principal source of energy for the cells of the brain and body.\n"},"KB/Glymphatic-System":{"title":"Glymphatic System","links":[],"tags":["brain"],"content":"Glymphatic System\n\nThe system that helps clear debris from the brain. During sleep, special glial cells called astrocytes form a network of conduits that allow cerebrospinal fluid to flush unwanted and unnecessary proteins out of the brain.\n"},"KB/Glyphs":{"title":"Glyphs","links":[],"tags":["visualization"],"content":"Glyphs\n\n\nAlpha Blending\n"},"KB/Goodhart's-Law":{"title":"Goodhart's Law","links":["KB/Proxy-Objective","KB/Rejection-Sampling"],"tags":["temp"],"content":"Goodhart’s Law\n\n“When a measure becomes a target, it ceases to be a good measure.”\nProxy Objective\nRejection Sampling\n\nRefs\n\nopenai\n"},"KB/Google-Conceptual-Captions":{"title":"Google Conceptual Captions","links":[],"tags":["dataset"],"content":"Google Conceptual Captions"},"KB/Google-NMT":{"title":"Google NMT","links":["LSTM","KB/Layers","KB/Attention","KB/FP16-training","KB/Beam-search"],"tags":["architecture"],"content":"Google NMT\n\nGoogle’s Neural Machine Translation System: Bridging the Gap Between Human and Machine Translation\n\ndeep [LSTM)](Long Short Term Memory (LSTM|Long Short Term Memory (LSTM|LSTM)](LSTM)](Long Short Term Memory (LSTM|Long Short Term Memory (LSTM|LSTM).md).md) network with 8 encoder and 8 decoder Layers using Attention and residual connections\nimprove parallelism and therefore decrease training time, their Attention mechanism connects the bottom layer of the decoder to the top layer of the encoder\nlow-precision arithmetic during inference computations (FP16 training ???)\nimprove handling of rare words, we divide words into a limited set of common sub-word units\ngood balance between the flexibility of “character”-delimited models and the efficiency of “word”-delimited models\nBeam search technique employs a length-normalization procedure and uses a coverage penalty\n\n\n"},"KB/Google-voice-search-task":{"title":"Google voice search task","links":[],"tags":["dataset"],"content":"Google Voice Search Task"},"KB/Grad-CAM":{"title":"GradCAM","links":["KB/CAM"],"tags":["architecture"],"content":"GradCAM\n\n@selvarajuGradCAMVisualExplanations\nModified CAM\nImportance of feature map k for target class c\n\nA is input\nY_{c}=\\text{score of class c} : value of output before softmax\ngrad of Y_{c} wrt A and take avg\n\n\n\n\n\n\\alpha_{k}^{c}= average(\\partial \\frac{Y_{c}}{\\partial A^{k}_{ij}})\n- If avg is high : important\n- 0 : not\n- neg : background/ others\n\n\nWeighted combination → relu\n\n\n\n\n\n\n\nL^{c}{GRADCAM}=Resize(ReLU(\\Sigma{k}(\\alpha^{c}_{k}A^{k})))\n\t- This is a coarse heatmap because the image is resized\n\t- ReLU used because we only care about positive values (actualy image pixel)\n- To identify [Counterfactual Images](Counterfactual Images.md), flip the signs\n\t- \n\\alpha_{k}^{c}=average(- \\partial \\frac{Y_{c}}{\\partial A^{k}_{ij}})\n\t- ![](images/Pasted%20image%2020221118132132.png)\n- Followed by [Guided GradCAM](Guided GradCAM.md)\n\n## Other Stuff\n- producing ‘visual explanations’ for decisions from a large class of CNN-based models, making them more transparent and explainable\n- Gradient-weighted Class Activation Mapping\n- uses the gradients of any target concept\n- flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept\n- lend insights into failure modes of these models (showing that seemingly unreasonable predictions have reasonable explanations)\n- are robust to adversarial perturbations\n- are more faithful to the underlying model\n- help achieve model generalization by identifying dataset bias\n- identify important neurons through GradCAM and combine it with neuron names to provide textual explanations for model decisions\n\n## GradCAM Vs CAM\n- Gradient-weighted Class Activation Mapping (Grad-CAM) is an improvement over Class Activation Mapping ([CAM]) that provides a more detailed and accurate [visualization](CAM]) that provides a more detailed and accurate [visualization.md) of the regions of an image that are important for a given classification.\n- [CAM](CAM.md) generates heatmaps by using global average pooling (GAP) in the final convolutional layer to generate a feature map, followed by a linear combination of the feature map and the class weight vector to generate a single class activation map. However, this approach does not take into account the gradients of the class scores with respect to the feature maps, which can provide additional information about the contribution of different regions of the image to the final classification decision.\n- Grad-CAM, on the other hand, uses the gradients of the class scores with respect to the feature maps in order to generate heatmaps. Specifically, it uses the gradients of the class scores with respect to the final feature maps of the network, which are then upsampled to the same size as the input image. The resulting heatmap highlights the regions of the input image that are most important for the given classification.\n- In summary, Grad-CAM is an improvement over CAM because it provides a more detailed and accurate [visualization](visualization.md) of the regions of an image that are important for a given classification by using gradients of the class scores with respect to the feature maps, providing additional information about the contribution of different regions of the image to the final classification decision."},"KB/GradCAM++":{"title":"GradCAM++","links":[],"tags":["explainability"],"content":"GradCAM++\n\n@chattopadhayGradCAMGeneralizedGradientBased2018\n"},"KB/Gradient-Accumulation":{"title":"Gradient Accumulation","links":["GPU"],"tags":["architecture"],"content":"Gradient Accumulation\n\nPytorch\nhelps when the model is not able to be trained with a big enough batch size\noften caused by memory limitations of the GPU\nAccumulate the gradients (for each trainable model value) of several forward passes and after some steps use the accumulated gradients to update the weights\nIs then equal to using a large batch size\nexample with SGD: \\theta_{i}=\\theta_{i}−1− \\alpha\\ast(\\Sigma_{i=0}^{N}grad_{\\theta_{i}})\n"},"KB/Gradient-Ascent":{"title":"Gradient Ascent","links":["KB/Gradient-Descent"],"tags":["temp"],"content":"Gradient Ascent\n\nTo maximize loss function unlike Gradient Descent\nProportional to positive of gradient\n\\theta_{t+1} = \\theta{t} + \\eta_t \\Sigma_{n=1}^N(\\nabla l_n(\\theta_t))^T\n"},"KB/Gradient-Boosting":{"title":"Gradient Boosting","links":[],"tags":["temp"],"content":"Gradient Boosting\n\nA training algorithm where weak models are trained to iteratively improve the quality (reduce the loss) of a strong model. For example, a weak model could be a linear or small decision tree model. The strong model becomes the sum of all the previously trained weak models.\nIn the simplest form of gradient boosting, at each iteration, a weak model is trained to predict the loss gradient of the strong model.\n"},"KB/Gradient-Checkpointing":{"title":"Gradient Checkpointing","links":[],"tags":["todo"],"content":"Gradient Checkpointing\n\nspell.ml/blog/gradient-checkpointing-pytorch-YGypLBAAACEAefHs\n"},"KB/Gradient-Clipping":{"title":"Gradient Clipping","links":["KB/CLIP","KB/Adaptive-Gradient-Clipping"],"tags":["architecture"],"content":"Gradient Clipping\n\nLimit the value or the norm of a gradient to a fixed Hyperparameter λ.\nmitigate the Vanishing &amp; Exploding Gradients, exploding ones\nidea is to CLIP the gradients during Backpropagation to a certain threshold (limit the value)\nmost often used in RNN or GAN, where Batch Normalisation is tricky to use\nmethods\n\nCLIP by norm\n\nCLIP the whole gradient if its L2 norm is greater than the threshold\nremains the orientation\n\n\nCLIP by value\n\nCLIP the gradient by a fixed value\nproblem: orientation of the gradient may change due to clipping\n\nexample: [0.9,100.0]→[0.9,1.0]\nhowever, this works well in practice\n\n\n\n\n\n\npros:\n\nlarger batch sizes\n\n\ncons:\n\nsensible to tuning Hyperparameter λ\n\n\nAdaptive Gradient Clipping\n"},"KB/Gradient-Descent":{"title":"Gradient Descent","links":["KB/Backprop","KB/Gradient-Direction","KB/Simple-Gradient-Descent","KB/SGD","KB/Mini-Batch-GD","KB/SGD-Momentum","KB/Adagrad","KB/Nesterov-Momentum","KB/AdaDelta","KB/Rmsprop","KB/Adam"],"tags":["loss"],"content":"Gradient Descent\n\nBackprop\nGradient Direction\nGradient Magnitude\nEdge Strength ||\\triangledown f|| = \\sqrt{(\\frac{\\partial f}{\\partial x})^{2} + (\\frac{\\partial f}{\\partial y})^{2}}\nParams \\theta\nMinimize loss function \\mathscr{L}(\\theta) = \\Sigma^N_{n=1}l_n(\\theta)\nSimple Gradient Descent\nSGD\nMini Batch GD\nSGD Momentum\nAdagrad\nNesterov Momentum\nAdaDelta\nRmsprop\nAdam\n\nImplicit regularization\n\n\n"},"KB/Gradient-Direction":{"title":"Gradient Direction","links":[],"tags":["loss"],"content":"Gradient Direction\n\nDirection of Steepest Descent \\theta = tan^{-1}(\\frac{\\frac{\\partial f}{\\partial y}}{\\frac{\\partial f}{\\partial x}})\n"},"KB/Gradient-Sensitivity":{"title":"Gradient Sensitivity","links":[],"tags":["explainability"],"content":"Gradient Sensitivity\n\nif for every input and baseline that differ in one feature but have different predictions, then the differing feature should be given a non-zero attribution\nIf the function implemented by the deep network does not depend (mathematically) on some variable, then the attribution to that variable is always zero.\nThe sensitivity axiom introduces the baseline\nA baseline is defined as an absence of a feature in an input\nThis definition is confusing, especially when dealing with complex models, but the baseline could be interpreted as “input from the input space that produces a neutral prediction”.\nA baseline can be treated as an input to produce a counterfactual explanation by checking how the model behaves when moving from baseline to the original image.\nThe authors give the example of the baseline for an object recognition network, which is a black image.\nAuthors argue that gradient-based methods are violating Sensitivity\nAs an example, we are presented with the case of simple function, f(x)=1-ReLU(1-x)\nand the baseline being x=0\nWhen trying to generate attribution for x=2, the functions’ output changes from 0 to 1 but after x=1, it becomes flat and causes the gradient to equal zero.\nObviously, x attributes to the result, but because the function is flat at the input we are testing results in invalid attribution and breaks the Sensitivity\nSundararajan et al. think that breaking Sensitivity causes gradients to focus on irrelevant features.\n"},"KB/Gradient":{"title":"Gradient","links":[],"tags":[],"content":""},"KB/Gradio":{"title":"Gradio","links":[],"tags":["deeplearning"],"content":"Gradio\nfrom fastai.vision.all import *\npath = untar_data(URLs.PETS)\ndls = ImageDataLoaders.from_name_re(path, get_image_files(path/&#039;images&#039;), pat=&#039;(.+)_\\d+.jpg&#039;, item_tfms=Resize(460), batch_tfms=aug_transforms(size=224, min_scale=0.75))\nlearn = vision_learner(dls, models.resnet50, metrics=accuracy)\nlearn.fine_tune(1)\nlearn.path = Path(&#039;.&#039;)\nlearn.export()\n \nlearn = load_learner(&#039;export.pkl&#039;)\n \nlabels = learn.dls.vocab\ndef predict(img):\n    img = PILImage.create(img)\n    pred,pred_idx,probs = learn.predict(img)\n    return {labels[i]: float(probs[i]) for i in range(len(labels))}\n \ntoc: true\ntitle = &quot;Pet Breed Classifier&quot;\ndescription = &quot;A pet breed classifier trained on the Oxford Pets dataset with [fastai](fastai.md). Created as a demo for Gradio and HuggingFace Spaces.&quot;\narticle=&quot;&lt;p style=&#039;text-align: center&#039;&gt;&lt;a href=&#039;tmabraham.github.io/blog/gradio_hf_spaces_tutorial&#039; target=&#039;_blank&#039;&gt;Blog post&lt;/a&gt;&lt;/p&gt;&quot;\nexamples = [&#039;siamese.jpg&#039;]\ninterpretation=&#039;default&#039;\nenable_queue=True\n \ngr.Interface(fn=predict,inputs=gr.inputs.Image(shape=(512, 512)),outputs=gr.outputs.Label(num_top_classes=3),toc: true\ntitle=toc: true\ntitle,description=description,article=article,examples=examples,interpretation=interpretation,enable_queue=enable_queue).launch()"},"KB/Gram-matrix":{"title":"Gram matrix","links":[],"tags":["einsum"],"content":"Gram Matrix\ndef gram_matrix_new(y):\n    b, ch, h, w = y.shape\n    return torch.einsum(&#039;bchw,bdhw-&gt;bcd&#039;, [y, y]) / (h * w)"},"KB/Granger-Causallity":{"title":"Granger Causallity","links":["KB/Autoregressive","KB/Transfer-Function"],"tags":["brain"],"content":"Granger Causallity\n\nAutoregressive\nIf significant then electrode Granger-causes another\nTheres some causality but not sure if physical or causal\n\nPartial Directed Coherence\nDirected Transfer Function\n\n\nMagnitude vs freq\nUndirected\nFrom O1 to PZ is different from PZ to O1\nHow well can activity in one channel predict one in another\n\n\n"},"KB/Graph-Based-Distillation":{"title":"Graph Based Distillation","links":[],"tags":["knowledgedistillation"],"content":"Graph Based Distillation\n\nLee and Song (2019) analysed intra-data rela- tions using a multi-head graph, in which the vertices are the features from different layers in CNNs. Park et al. (2019) directly transferred the mutual relations of data samples, i.e., to match edges between a teacher graph and a student graph. Tung and Mori (2019) used the similarity matrix to represent the mutual relations of the activations of the input pairs in teacher and student models. The similarity matrix of student matches that of teacher.\nPeng et al. (2019a) not only matched the response-based and feature-based knowl- edge, but also used the graph-based knowledge. In (Liu et al., 2019g), the instance features and instance relationships are modeled as vertexes and edges of the graph, respectively.\nSpecifically, Luo et al. (2018) considered the modal- ity discrepancy to incorporate privileged information from the source domain. A directed graph, referred to as a distillation graph is introduced to explore the relationship between different modalities. Each vertex represent a [modality] and the edges indicate the connection strength between one [modality](modality] and the edges indicate the connection strength between one [modality.md) and another.\nMinami et al. (2019) proposed a bidirectional graph-based diverse collaborative learning to explore diverse knowledge transfer patterns. Yao et al. (2020) introduced GNNs to deal with the knowledge trans- fer for graph-based knowledge.\nBesides, using knowl- edge distillation, the topological semantics of a graph convolutional teacher network as the topology-aware knowledge are transferred into the graph convolutional student network (Yang et al., 2020b)\n"},"KB/Graph-Level-Tasks":{"title":"Graph Level Tasks","links":["KB/Graph-mean-pooling"],"tags":["graph"],"content":"Graph Level Tasks\n \nGraph Level Tasks\n\nnetwork assigns a label or estimates one or more values from the entire graph\n\n\nGraph mean pooling\n"},"KB/Graph-Neural-Network":{"title":"Graph Neural Network","links":["KB/Embedding","KB/Transformer","KB/Graph-Level-Tasks","KB/Node-Level-Tasks","KB/Edge-Prediction-Tasks"],"tags":["graph","architecture"],"content":"Graph Neural Network\n \n\nmodel that takes the node embeddings X and the adjacency matrix A as inputs and passes them through a series of K layers.\nThe node embeddings are updated at each layer to create intermediate “hidden” representations H_{k} before finally computing output embeddings H_{K}\nAt the start of this network, each column of the input node embeddings X just contains information about the node itself. At the end, each column of the model output H_K includes information about the node and its context within the graph.\nsimilar to word Embedding for Transformer\n\n\nGraph Level Tasks\nNode Level Tasks\nEdge Prediction Tasks"},"KB/Graph-convolutional-network":{"title":"Graph convolutional network","links":["relational-inductive-bias","KB/Equivariance-and-Invariance-for-Graphs","KB/Parameter-Sharing-for-Graphs","KB/Example-GCN-Layer","KB/Batching-for-GNN","KB/Inductive-Models","KB/Transductive-Models","KB/Layers-for-GNNs","KB/Edge-Graphs"],"tags":["architecture","graph"],"content":"Graph Convolutional Network\n \n\nspatial-based convolutional graph neural networks, or GCNs\nThese models are convolutional in that they update each node by aggregating information from nearby nodes\nrelational inductive bias\n\n\nEquivariance and Invariance for Graphs\nParameter Sharing for Graphs\nExample GCN Layer\nBatching for GNN\nInductive Models\nTransductive Models\nLayers for GNNs\nEdge Graphs"},"KB/Graph-mean-pooling":{"title":"Graph mean pooling","links":[],"tags":["graph"],"content":"Graph Mean Pooling\n \nthe output node embeddings are combined (e.g., by averaging), and the resulting vector is mapped via a linear transformation or neural network to a fixed-size vector.\n\n\n"},"KB/Graph-based-visual-saliency":{"title":"Graph-based visual saliency","links":[],"tags":["explainability"],"content":"\n\nGraph-based Visual Saliency\n\nTODO\nexploits channel-wise feature maps computed by linear filtering followed by a nonlinear transformation.\nTo estimate saliency maps, the feature maps are transformed into activation maps and normalised by using the fully-connected directed graph of the feature maps.\n"},"KB/Graphs":{"title":"Graphs","links":["KB/Small-World-graphs"],"tags":["visualization","graph"],"content":"Graphs\n\nGraph G= (V,E) where\nedges E \\subseteq V \\times V\nvertices V\nSmall World graphs\n\n\n"},"KB/Grasp-Point-Detection":{"title":"Grasp Point Detection","links":["KB/GGCNN","KB/GRConvNet","KB/MVGrasp","KB/Unet-Grasping","KB/Learning-to-Detect-Grasp-Affordance","KB/Volumetric-Grasping-Network","KB/Affordance-Detection-Task-Specific","KB/Kinesthetic-Teaching"],"tags":["robotics"],"content":"Grasp Point Detection\n\n\nChoose a grasp point either from above or from side by considering:\n\nSize of object’s bounding box\nPrinciple axes\nProjections/Views\n\n\nGGCNN, GRConvNet, MVGrasp, Unet Grasping, Learning to Detect Grasp Affordance, Volumetric Grasping Network , Affordance Detection Task Specific\nKinesthetic Teaching\n"},"KB/Gravity-Loading":{"title":"Gravity Loading","links":["KB/Gravity","KB/Force"],"tags":["robotics"],"content":"Gravity Loading\n\nThe Force exerted downward, due to the weight of the robot arm and/or the load at the end of the arm. The Force creates an error with respect to position accuracy of the end effector. A compensating Force can be computed and applied bringing the arm back to the desired position.\n"},"KB/Gravity":{"title":"Gravity","links":[],"tags":["physics"],"content":"Gravity\n\nMass is a measure of an object’s inertia. Mass also determines the strength of gravity. Because of gravity all objects are attracted to each other, but we mostly notice the attraction towards the Earth because it is so large and so close.\nF_{g}= mg\ng = 9.8 m/s^2\n"},"KB/Greedy-Policy":{"title":"Greedy Policy","links":["KB/Reinforcement-Learning"],"tags":["temp"],"content":"Greedy Policy\n\nIn Reinforcement Learning, a policy that always chooses the action with the highest expected return.\n"},"KB/Grey-sheep-problem":{"title":"Grey sheep problem","links":["KB/Clustering"],"tags":["usermodel"],"content":"Grey Sheep Problem\n\nFall outside the bounds in something like Clustering\ngets assigned to something close by but not really related\n"},"KB/GridMask":{"title":"GridMask","links":[],"tags":["augmentation"],"content":"GridMask\n\n@chenGridMaskDataAugmentation2020\nThe algorithm tries to overcome drawbacks of [Cutout], [Random Erasing], and [Hide and Seek](Cutout], [Random Erasing], and [Hide and Seek.md) that are prone to deleting important information entirely or leaving it untouched without making it harder for the algorithm to learn.\nTo handle this, GridMask creates multiple blacked-out regions in evenly spaced grids to maintain a good balance between deletion and retention of critical information\nThe number of masking grids and their sizes are tuneable\n"},"KB/Grids":{"title":"Grids","links":[],"tags":["visualization"],"content":"Grids\n\n\n"},"KB/Gripper":{"title":"Gripper","links":[],"tags":["robotics"],"content":"Gripper\n\nAn end effector that is designed for seizing and holding (ISO 8373) and “grips” or grabs an object. It is attached to the last link of the arm. It may hold an object using several different methods, such as: applying pressure between its “fingers”, or may use magnetization or vacuum to hold the object, etc\n\n"},"KB/Group-Modeling-Approach":{"title":"Group Modeling Approach","links":["KB/Grey-sheep-problem"],"tags":["usermodel"],"content":"Group Modeling Approach\n\nTake all users → Split them into groups\nNot personalized\nEasy to classify a user\nGrey sheep problem\n"},"KB/Group-fairness":{"title":"Group fairness","links":[],"tags":["explainability"],"content":"Group fairness\n\nfairness is analyzed by modeling the differences between each subject and the\nrest of the population.\n"},"KB/Guided-BackProp":{"title":"Guided BackProp","links":["KB/DeconvNet"],"tags":["explainability"],"content":"Guided BackProp\n\n@springenbergStrivingSimplicityAll2015\nStriving for simplicity, the All conv net\n\nSummary\n\nCombination of [DeconvNet] and  [Deep Inside Convolutional Networks](DeconvNet] and  [Deep Inside Convolutional Networks.md)\nDeconvNet has an issue with flow of negative gradients which decrease accuracy of higher layers\nTheir idea is to combine two approaches and add a “guide” to the Saliency with the help of deconvolution.\nfocus on the ReLU activation function\nWhen computing values at the Rectification component of the deconvnet, we are masking all non-positive values with the ReLU\nIn that layer, the computed values are calculated only base on the top signal (reconstruction from the upper layer), and the input is ignored\nOn the other hand, in the Saliency method, we are focusing on the gradient values computed base on the input image\nIf we take deconvnet masking of the Rectification layer and apply it on the gradient values of the Saliency method, we could remove noise caused by the negative gradient values.\nDeconvolution guides backpropagation values of the Saliency method to produce sharper images\nThe idea of GBP is often misunderstood and interpreted as “applying deconvolution results on the saliency results”.\nThis is not true because ReLU masking extracted from the deconvnet is applied on every level and therefore affects the gradient values all the way down to the input of the CNN, not only at the first level of the CNN\n\nImages\n\n\n\n\n"},"KB/Guided-GradCAM":{"title":"Guided GradCAM","links":["KB/Grad-CAM"],"tags":["explainability"],"content":"Guided Grad-CAM\n\n@selvarajuGradCAMWhyDid2017\nPointwise multiply betwen [Grad-CAM] and [Guided BackProp](Grad-CAM] and [Guided BackProp.md)\nClass Discriminative\nHigh resolution\nSimilar to [Occlusion Map](Occlusion Map.md) but faster\nGuided [Grad-CAM] is a variation of [Grad-CAM](Grad-CAM] is a variation of [Grad-CAM.md) that combines the gradients of the class scores with respect to the feature maps with the gradients of a guided backpropagation algorithm. Guided backpropagation is a method for visualizing the internal representations of a neural network by backpropagating the output of the network to the input image, while only propagating the positive gradients.\nThe main difference between [Grad-CAM] and Guided [Grad-CAM] is that while [Grad-CAM] focuses on finding the regions of an image that are most important for a given classification, Guided [Grad-CAM] also takes into account the positive gradients of the guided backpropagation algorithm, in order to provide a more fine-grained [visualization] of the internal representations of the network. This can make Guided [Grad-CAM](Grad-CAM] and Guided [Grad-CAM] is that while [Grad-CAM] focuses on finding the regions of an image that are most important for a given classification, Guided [Grad-CAM] also takes into account the positive gradients of the guided backpropagation algorithm, in order to provide a more fine-grained [visualization] of the internal representations of the network. This can make Guided [Grad-CAM.md) more effective for understanding how the model is making its decisions, and for identifying the specific features of an image that the model is using for a given classification.\n\n"},"KB/Gyrus":{"title":"Gyrus","links":["KB/Sulcus"],"tags":["brain"],"content":"Gyrus\n\nThe folding of the cortex increases the brain’s surface area allowing more neurons to fit inside the skull and enabling higher functions\nEach groove between folds is called a Sulcus.\nThere are names for the folds and grooves that help define specific brain regions.\n"},"KB/H3-View":{"title":"H3 View","links":[],"tags":["visualization"],"content":"H3 View\n\n\n"},"KB/HL7":{"title":"HL7","links":[],"tags":["medical"],"content":"HL7\n\nbroader range of standards for clinical and administrative data exchange\nHL7apy\n\npip install hl7apy\n\n\n"},"KB/HMDB51":{"title":"HMDB51","links":[],"tags":["dataset"],"content":"HMDB51\n\nsmaller video dataset for human action recognition\n7,000 video clips in this dataset belong to 51 human action categories\nvideos in HMDB51 dataset have 320 x 240 pixels spatial resolution and 30 FPS frame rate\nthe self-supervised models are fine-tuned on the dataset to evaluate the quality of the learned video features.\n"},"KB/HNSW":{"title":"HNSW","links":[],"tags":["architecture"],"content":"HNSW\n\napproximate nearest neighbor search algorithm that enables efficient similarity search over dense vector representations\nIt builds a multi-layer navigable small world graph structure to index the vectors, allowing for fast and scalable retrieval of semantically similar documents.\n"},"KB/Hallucination-Text-Generation":{"title":"Hallucination Text Generation","links":[],"tags":["architecture"],"content":"Hallucination Text Generation\n\nSurvey of Hallucination in Natural Language Generation\noften produces false statements that are disconnected from reality because such models are not grounded in reality\nhallucinated texts\n"},"KB/Hallucination":{"title":"Hallucination","links":[],"tags":["temp"],"content":"Hallucination\n\nThe production of plausible-seeming but factually incorrect output by a generative model that purports to be making an assertion about the real world. For example, if a dialog agent claims that Barack Obama died in 1865, the agent is hallucinating.\n"},"KB/Hamming-Distance":{"title":"Hamming Distance","links":[],"tags":["loss"],"content":"Hamming Distance\n\nd = \\Sigma_{i}|p_{i}- q_{i}|\nHamming distance is the number of values that are different between two vectors\nIt is typically used to compare two binary strings of equal length.\ndifficult to use when two vectors are not of equal length\n"},"KB/Hand-Guiding":{"title":"Hand Guiding","links":[],"tags":["robotics"],"content":"Hand Guiding\n\nallows an operator to hand guide the robot to a desired position. This task can be achieved by utilizing additional external hardware mounted directly to the robot or by a robot specifically designed to support this feature. Both solutions will require elements of functional safety to be utilized. A risk assessment shall be used to determine if any additional safeguarding is necessary to mitigate risks within the robot system.\n"},"KB/Hard-Parameter-Sharing":{"title":"Hard Parameter Sharing","links":[],"tags":["multitask"],"content":"Hard Parameter Sharing\n\n\n"},"KB/Harmonic-Drive":{"title":"Harmonic Drive","links":[],"tags":["robotics"],"content":"Harmonic Drive\n\nCompact lightweight speed reducer that converts high speed low torque to low speed high torque. Usually found on the minor (smaller) axis.\n"},"KB/Harness":{"title":"Harness","links":[],"tags":["robotics"],"content":"Harness\n\nUsually several wires, bundled together to deliver power and/or signal communications to/from devices. For example, the robot motors are connected to the controller through a wire harness.\n"},"KB/Hashing":{"title":"Hashing","links":["KB/Bucketing"],"tags":["temp"],"content":"Hashing\n\nIn machine learning, a mechanism for Bucketing categorical data, particularly when the number of categories is large, but the number of categories actually appearing in the dataset is comparatively small.\nFor example, Earth is home to about 60,000 tree species. You could represent each of the 60,000 tree species in 60,000 separate categorical buckets. Alternatively, if only 200 of those tree species actually appear in a dataset, you could use hashing to divide tree species into perhaps 500 buckets.\n"},"KB/Hausdorff-Distance":{"title":"Hausdorff Distance","links":[],"tags":["loss"],"content":"Hausdorff Distance\n\nd= max_{i}(|p_{i}-q_{i}|)\n"},"KB/Haversine-Distance":{"title":"Haversine Distance","links":[],"tags":["loss"],"content":"Haversine Distance\n\n\nd = 2r\\times arcsin(\\sqrt{sin^{2}(\\frac{\\varphi_{2}-\\varphi_{1}}{2})+cos(\\varphi_{1})cos(\\varphi_{2})sin^{2}(\\frac{\\lambda_{2}-\\lambda_{1}}{2}))}\nHaversine distance is the distance between two points on a sphere given their longitudes and latitudes\nThe main difference is that no straight line is possible since the assumption here is that the two points are on a sphere.\nssumed the points lie on a sphere\nAs you might have expected, Haversine distance is often used in navigation\ncalculate the distance between two countries when flying between them\nNote that it is much less suited if the distances by themselves are already not that large. The curvature will not have that large of an impact.\n"},"KB/He-Initialization":{"title":"He Initialization","links":["KB/Relu","KB/Initialization","KB/Sigmoid"],"tags":["regularization"],"content":"He Initialization\n\nbring the variance of those outputs to approximately one\nHowever, Kumar indeed proves mathematically that for the Relu activation function, the best weight Initialization strategy is to initialize the weights randomly but with this variance:\n\n\\begin{equation} v^{2} = 2/N \\end{equation}\n\n\nFor Sigmoid based activation functions\n"},"KB/Heaviside":{"title":"Heaviside","links":["KB/Perceptron","KB/SGD"],"tags":["architecture"],"content":"Heaviside\n\n\n\n\n\n\\end{cases}$$\n\nused in Rosenblatt’s Perceptron\nnot Differentiable → SGD not possible\nno practical use\n"},"KB/Height-Plots":{"title":"Height Plots","links":[],"tags":["visualization"],"content":"Height Plots\n\n2D scalar field\n\\{(x,y, f(x,y))|(x,y)\\in \\mathbb{R}^{2}\\}\nDisplacement along z = f(x,y)\n\n"},"KB/Heightmaps-Kinesthetic":{"title":"Heightmaps Kinesthetic","links":[],"tags":["robotics"],"content":"Heightmaps Kinesthetic\n\nHerzog, Alexander, et al. “Learning of grasp selection based on shape-templates.” Autonomous Robots 36: pp. 51-65, 2014\n\n\n"},"KB/Helmholtz-Theorem":{"title":"Helmholtz Theorem","links":[],"tags":["visualization"],"content":"Helmholtz Theorem\n\n\n"},"KB/Help-Abuse":{"title":"Help Abuse","links":[],"tags":["usermodel"],"content":"Help Abuse\nSome students ask for help even when they don’t need it. In the extreme cases, some students ask for help on every step."},"KB/Help-Refusal":{"title":"Help Refusal","links":[],"tags":["usermodel"],"content":"Help Refusal\nSome students refuse to ask for help even when they need it. They enter a long series of incorrect steps, which may be guesses, instead of clicking on the help button."},"KB/Heteroscedatic":{"title":"Heteroscedatic","links":[],"tags":["uncertainty"],"content":"Heteroscedatic\n\nif \\sigma^{2} is a function of the input or variable in \\epsilon \\sim \\mathcal{N}(0, \\sigma^{2})\n\n"},"KB/HiFI-GAN-Denoising":{"title":"HiFI-GAN_Denoising","links":["KB/Features"],"tags":["architecture"],"content":"HiFI-GAN_Denoising\n\n[HiFi-GAN: High-Fidelity Denoising and Dereverberation Based on Speech Deep Features in Adversarial Networks](arxiv.org/abs/2006.05694)\nReal-world audio recordings are often degraded by factors such as noise, reverberation, and equalization distortion\ntransform recorded speech to sound as though it had been recorded in a studio\nend-to-end feed-forward WaveNet architecture, trained with multi-scale adversarial discriminators in both the time domain and the time-frequency domain\nrelies on the deep feature matching losses of the discriminators to improve the perceptual quality of enhanced speech\n"},"KB/HiFI-GAN-Synthesis":{"title":"HiFI-GAN Synthesis","links":["KB/Spectrogram"],"tags":["architecture"],"content":"HiFI-GAN Synthesis\n\nHiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis\nsynthesis\nAs speech audio consists of sinusoidal signals with various periods, they demonstrate that modeling periodic patterns of an audio is crucial for enhancing sample quality\nshows a significant improvement in terms of synthesis speed.\nMOS\ncharacteristic of speech audio that consists of patterns with various periods and applied it to neural networks, and verified that the existence of the proposed discriminator greatly influences the quality of speech synthesis through the ablation study\ngeneralize to the mel-Spectrogram inversion of unseen speakers and synthesize speech audio comparable to human quality from noisy inputs in an end-to-end setting\nprogress towards on-device natural speech synthesis, which requires low latency and memory footprint\ngenerators of various configurations can be trained with the same discriminators and learning mechanism\npossibility of flexibly selecting a generator configuration according to the target specifications without the need for a time-consuming hyper-parameter search for the discriminators\n"},"KB/Hide-and-Seek":{"title":"Hide and Seek","links":[],"tags":["augmentation"],"content":"Hide and Seek\n\n@singhHideandSeekDataAugmentation2018\ndivides an image into a specified number of grids and turns on and off each grid with an assigned probability\nvarious image regions are deleted, and they can be connected or disconnected from each other\nValues in turned-off regions are replaced with the average of all the pixel values in the entire dataset.\n"},"KB/Hierarchial-Refinement":{"title":"Heirarchial Refinement","links":["KB/Density","KB/Mesh-refinement"],"tags":["visualization"],"content":"Heirarchial Refinement\n\nSplit n dim volume → finite set of discrete regions\nn dim hypercubes\nConstruct regions where there is a higher point Density\nFine grained info encoded → smaller hypercubes to increase resolution\nn = 2 : quadtree\nn = 3 : octree\n\nMesh refinement\n\n…"},"KB/Hierarchical-Edge-Bundling":{"title":"Hierarchical Edge Bundling","links":["KB/Streamlines"],"tags":["visualization","graph"],"content":"Hierarchical Edge Bundling\n\nExploit the hierarchical structure to bundle non-hierarchical edges visually together\nconceptual similarity to bundling Streamlines\n\n"},"KB/High-pass-filter":{"title":"High pass filter","links":[],"tags":["visualization"],"content":"High Pass Filter\n\nThat passes signals with a frequency higher than a certain cutoff frequency and attenuates signals with frequencies lower than the cutoff frequency.\n"},"KB/Higher-Layer-Capsule":{"title":"Higher Layer Capsule","links":[],"tags":["architecture"],"content":"Higher Layer Capsule\n\nThe outputs of the [Primary Capsule](Primary Capsule.md) are then passed to higher-layer capsules, which combine the information from multiple primary capsules to extract more abstract concepts, such as the identity of an object or the presence of a face.\n[Routing by Agreement](Routing by Agreement.md)\n"},"KB/Highway-Convolutions":{"title":"Highway Convolutions","links":["KB/Conv"],"tags":["architecture"],"content":"Highway Convolutions\n\nConv\n\nclass HighwayConv1dNew(nn.Conv1d):\n    def forward(self, inputs):\n        L = super().forward(inputs)\n        H1, H2 = rearrange(L, &#039;b (split c) t -&gt; split b c t&#039;, split=2)\n        torch.sigmoid_(H1)\n        return H1 * H2 + (1.0 - H1) * inputs"},"KB/Hinge-Loss":{"title":"Hinge","links":["SVM"],"tags":["loss"],"content":"Hinge\n\n\nClassification\n\n\nSVM\n\n\nthe w are weights of the model\n\n\nlabels are 1 or -1\n\n\n\\mathrm{max}\\left( 0, 1 + \\mathrm{max}\\left( w_{y} \\cdot x - w_{t} \\cdot x \\right) \\right)\n\n\nL(y, \\hat y) = \\Sigma_{i}max(0, 1- y_{i}\\hat y_{i})\n\n\nmaximum margin classification\n\n"},"KB/Hit-list":{"title":"Hit list","links":[],"tags":["temp"],"content":"Hit List\n\n\nA shape feature which is powerful for Retrieval may not be strong in Recognition!\nFeature B: hit list should provide nice, intuitive rank in a satisfying ‘hit list’\nFeature A: target word class should survive competit with the other word classes (emerging needle from the heterogeneous hay stack)\n"},"KB/Holdout-Data":{"title":"Holdout Data","links":[],"tags":["temp"],"content":"Holdout Data"},"KB/Homoscedatic":{"title":"Homoscedatic","links":[],"tags":["uncertainty"],"content":"Homoscedatic\n\nif \\sigma^{2} is constant in \\epsilon \\sim \\mathcal{N}(0, \\sigma^{2})\n\n"},"KB/Hopfield-networks":{"title":"Hopfield networks","links":[],"tags":["architecture"],"content":"Hopfield Networks\n\nfrom\nOlder architecture used to store and retrieve patterns\nBuilding blocks\n\nData (list of patterns)\nNetwork\n\nNodes\nAll nodes are connected with each other\n\n\nRetrieval\n\nInput: partial pattern\nOutput: full pattern (retrieved)\n\n“best match” partial pattern to entire data\nFilling out the missing nodes with the best pattern is called update rule\n\n\n\n\nEnergy Function\n\nTheoretical concept, similar to a loss function\nIs not being optimized directly but trough update function\n\n\nUpdate function\n\nOptimizes the pattern that will be retrieved to best match the partial pattern\n\n\n\n\n"},"KB/How-the-Sciences-Faded-From-India":{"title":"How the Sciences Faded From India","links":["KB/T.-B.-Macaulay"],"tags":["indianhistory"],"content":"How the Sciences Faded From India\nHow the Sciences Faded From India\n\nMaintenance of secrecy, foreign invasions, easy availability of the means of subsistence, lack of royal patronage, and the apathy of the Indian people due to general introverted tendencies–all these gradually stemmed the flow of scientific research in ancient India, and in time reduced it to an antique. Even then, we must certainly remember with pride that the scientific research of ancient India is indeed an essential chapter of Indian culture in the evolution of national pedigree\nduring the colonization of India, a trend was set by the British in a systematic manner to discard all traditional systems of knowledge in India and to look at traditional practices with contempt. Unfortunately, this trend continued further after independence, and can still be detected even today. This resulted in the neglect of all the traditional knowledge systems, practices and indigenous science and technology systems of India.\none of the bands of scholars whose primary interest was the converting of Hindus to the one “true faith” by any means necessary were those at the University of Oxford who started the Boden Professorship of Sanskrit.\nAnd the British system of education was a great means of making Indians forget their culture and all that they once were, and how India was once the wealthiest area in the world and a center for great learning\nMoreover, it was also a means to make the Indian people feel backwards and unworthy, and that only by accepting the Western values could they again become truly progressive and part of the civilized world.\nwe can only guess at how much more advanced the world could have been, and how many more developments may have originated out of Vedic culture if it had been allowed to continue, uninterrupted by the invaders over the past 1000 years or so, whether they be the Muslims, Moghuls, the British, the Portuguese, or so on. They cared little for the culture and even preferred to destroy it, and had even less concern for the people.\nT. B. Macaulay\n"},"KB/How-to-take-your-visual-storytelling-to-the-next-level":{"title":"How to take your visual storytelling to the next level","links":[],"tags":["composition"],"content":"How to Take Your Visual Storytelling to the Next Level\n\nDetails matter. Little specks on clothes etc\nNeutral light is best for taking shot. Easiest to control\nthumbnails first always\nplanning is important\n"},"KB/Huber":{"title":"Huber/Smooth L1/Smooth MAE","links":["KB/MSE","tags/architecture","Fast-RCNN"],"tags":["loss","architecture"],"content":"Huber/Smooth L1/Smooth MAE\n\nIt is less sensitive to outliers than the MSE and in some cases prevents explodingarchitecture\nFast-RCNN\n\nif \\left( \\left\\|y - ŷ\\right\\| \\lt 1.0 \\right) &gt;1 \n\\frac{1}{\\mathrm{length}\\left( y \\right)} \\cdot \\mathrm{sum}\\left( 0.5 \\cdot \\left( y - ŷ \\right)^{2} \\right)\nelse\n\\frac{1}{\\mathrm{length}\\left( y \\right)} \\cdot \\mathrm{sum}\\left( \\left\\|y - ŷ\\right\\| - 0.5 \\right)\n…"},"KB/Human-Action-Recognition":{"title":"Human Action Recognition","links":[],"tags":["semisupervisedlearning"],"content":"Human Action Recognition\n\nThe action recognition task is often used to evaluate the quality of video features learned by self-supervised learning methods\nfirst trained on unlabeled video data with pretext tasks, then it is fine-tuned on action recognition datasets with human annotations to recognize the actions\n"},"KB/Humanoid-Vision-Engine":{"title":"Humanoid Vision Engine","links":[],"tags":["robotics"],"content":"Humanoid Vision Engine\n\nHVE\nsummarize the contribution of shape, texture, and color in a given task (dataset) by separately computing the three features to support image classification\nend-to-end learning with backpropagation to simulate the learning process of humans and to summarize the contribution of shape, texture, and color\nadvantage of end-to-end training is that we can avoid introducing human bias, which may influence the objective of contribution attribution\n"},"KB/Huntington’s-Disease":{"title":"Huntington’s Disease","links":[],"tags":["brain"],"content":"Huntington’s Disease\n\nA neurodegenerative disorder that causes progressive death of neurons in the brain, resulting in severe movement and cognitive problems. The disorder is caused by the mutation of a single gene\n—and symptoms typically present when an individual is in his or her 30’s or 40’s.\n"},"KB/HyTAS":{"title":"HyTAS","links":[],"tags":["automl"],"content":"HyTAS\nRebuttal\nReviewer 2\n\nwhich seems to have the problem of local optimization\n\nHyTAS seems to test specific variants of the same architecture : increasing depth/width. I think the reviewer meant that it does not test different kinds of transformer architectures → more branches/more concatenations etc.\nthey thought it was a general metric to evaluate any kind of transformer. this is not the case. perhaps a clarification would be nice?\n\n\nmore detailed optimization\n\narchitecture diagram\n\n\n\nGeneral Comments\n\nZICO → ZICO++ ?\ndifference between CNNs and transformers and why this architecture specifically?\n"},"KB/Hybrid-Word-Segmentation":{"title":"Hybrid Word Segmentation","links":["KB/Word-Segmentation","KB/Transducer"],"tags":["language"],"content":"Hybrid Word Segmentation\n\ncombination\nweighted Finite State Transducer to identify dictionary entries\n"},"KB/HyperStreamlines":{"title":"HyperStreamlines","links":[],"tags":["visualization"],"content":"HyperStreamlines\n\n\n"},"KB/Hypertension":{"title":"Hypertension","links":[],"tags":["medical"],"content":"Hypertension\n\nUnusually high blood pressure\n"},"KB/Hypodermic-Needle":{"title":"Hypodermic Needle","links":[],"tags":["medical"],"content":"Hypodermic Needle\n\nA very thin, hollow needle used with a syringe to inject substances into the body or to extract blood\n"},"KB/Hypotension":{"title":"Hypotension","links":[],"tags":["medical"],"content":"Hypotension\n\nUnusually low blood pressure\n"},"KB/Hypothalamus":{"title":"Hypothalamus","links":[],"tags":["brain"],"content":"Hypothalamus\n\nis located in the floor of the third ventricle and is the master control of the autonomic system.\nIt plays a role in controlling behaviors such as hunger, thirst, sleep, and sexual response.\nIt also regulates body temperature, blood pressure, emotions, and secretion of hormones.\n"},"KB/Hysterectomy":{"title":"Hysterectomy","links":[],"tags":["medical"],"content":"Hysterectomy\n\nSurgical procedure to remove the uterus\n"},"KB/ICA-Noise-Removal":{"title":"ICA Noise Removal","links":["KB/ICA","KB/Notch-filter","KB/High-pass-filter"],"tags":["visualization"],"content":"ICA Noise Removal\n\n\n\n\nNotch filter\nHigh pass filter\n"},"KB/ICA":{"title":"ICA","links":["KB/ICA-Noise-Removal"],"tags":["temp"],"content":"ICA\n\nIndependant Component Analysis\nUnmix combinations of signals\nLook for rotations of data into maximally independant components\nNot always orthogonal\nBetter after noise removal\nRemove fewer than 20%\nRemove really bad parts first\n\nICA Noise Removal\n"},"KB/IDRiD":{"title":"IDRiD","links":[],"tags":["dataset"],"content":"IDRiD\n\nIDRiD is con- cerned with the disease grade recognition of retina images, and the presence or absence of diseases is recognized from exudates and hemorrhages.\nIDRiD includes a segmentation label of disease regions annotated by a specialist\n"},"KB/IID":{"title":"IID","links":[],"tags":["temp"],"content":"IID\n\nNeural networks assumes that the data points are independent and identically distributed.\n"},"KB/ILSVRC":{"title":"ILSVRC","links":[],"tags":["dataset"],"content":"ILSVRC"},"KB/IMDB":{"title":"IMDB","links":[],"tags":["dataset"],"content":"IMDB\n\nMovie reviews\n"},"KB/IRT":{"title":"IRT","links":[],"tags":["usermodel"],"content":"IRT\n\nassumes that every test item has a difficulty, and different items have different difficulties (Embretson &amp; Reise, 2000)\nUnfortunately, IRT generally assumes that test items are conditionally independent given the student’s competence. This is seldom true of the raw measures collected at the step level by tutoring systems\nAlthough IRT has powerful features, such as calibration algorithms that empirically determine item difficulties and other parameters, considerable work is needed before it can be applied to tutoring systems.\n"},"KB/ISIC-2018":{"title":"ISIC 2018","links":[],"tags":["dataset"],"content":"ISIC 2018\n\n2386 dermoscopy images, all of them annotated with patterns on skin lesions unanimously recognized as indicators of potential malignancy\nbinary masks highlighting the presence of five “features” at pixel-level\nglobules, streaks, pigment network, negative network, and milia-like cysts.\nThe performances on the feature extraction task are measured using the Jaccard index.\n"},"KB/ITM-Loss":{"title":"ITM Loss","links":["KB/Modality"],"tags":["loss"],"content":"ITM Loss\n\nITM loss is an alignment loss that encompasses cross-Modality interaction between image and text\nITM requires positive and negative pairs\n"},"KB/IVFADC":{"title":"IVFADC","links":[],"tags":["architecture"],"content":"IVFADC\n\nindexing structure that combines an inverted file system with a quantization-based approach\nIt allows for efficient indexing and retrieval of large-scale vector databases by partitioning the vector space and using asymmetric distance computation.\n"},"KB/Ideas-for-Fact-Learning":{"title":"Ideas for Fact Learning","links":["KB/Eye-Tracking","KB/Pupil-Dilation","KB/Gamification"],"tags":["usermodel"],"content":"Ideas for Fact Learning\n\nType of information\n\nMethods of presentation\nTypes of feedback\nTime pressure\nStudy time\nVisualize progress\nDecide when an item is mastered\n\n\nIncrease system info about the user\n\nReaction times\naccuracy\nEye Tracking\nbiometric\nPupil Dilation\nDetect learning styles\n\n\nFramework\n\nGamification\n\n\n"},"KB/Identity-Loss":{"title":"Identity Loss","links":[],"tags":["loss"],"content":"Identity Loss\n\nL_{identity}(G,F) = \\mathbb{E}_{y \\sim p_{data}(y)}[||G(y)-y)||_{1}] + \\mathbb{E}_{x \\sim p_{data}(x)}[||F(x)-x)||_{1}]\nThe identity loss is used to preserve the color and prevent reverse color in the result.\nThis loss can regularize the generator to be near an identity mapping when real samples of the target domain are provided. If something already looks like from the target domain, you should not map it into a different image.\nThe model will be more conservative for unknown content.\nIn general, it can help bette preserve the content if that is your priority.\n"},"KB/Image-Classification":{"title":"Image Classification","links":[],"tags":["semisupervisedlearning"],"content":"Image Classification\n\nWhen choosing image classification as a downstream task to evaluate the quality of image features learned from self-supervised learning methods, the self-supervised learned model is applied on each image to extract features which then are used to train a classifier such as Support Vector Machine (SVM) [105].\n"},"KB/Image-Data-Augmentation-Survey":{"title":"Image Data Augmentation Survey","links":[],"tags":["explainability"],"content":"Image Data Augmentation Survey\nIntroduction\n\n\nsufficient open datasets like Imagenet [Russakovsky et al., 2015], MS-COCO [Lin et al., 2014] and PASCAL VOC [Everingham et al., 2015] are crucial to the development of deep learning models.\n\n\nimbalance among the developments of these three perspectives\n\n\nThe core idea of data augmentation is to improve the sufficiency and diversity of training data by generating synthetic dataset\n\n\nThe augmented data can be regarded as being extracted from a distribution that is close to the real one\n\n\naugmented dataset can represent more comprehensive characteristics\n\n\ndata augmentation methods are tasks-independent\n\n\nBecause the operations are performed on the image data and labels at the same time, and the label types are different under different tasks, the data augmentation methods for object detection task can not be directly applied to semantic segmentation task\n\n\n\n\n\nit is meaningful to apply basic image manipulations only under the assumption that the existing data obeys the distribution close to the actual data distribution.\n\n\nsome basic image manipulation methods, such as translation and rotation, suffer from the padding effect\n\n\n, some areas of the images will be moved out of the boundary and lost\n\n\nTherefore, some interpolation methods will be applied to fill in the blank part. Generally, the region outside the image boundary is assumed to be constant 0, which will be black after manipulation.\n\n\n[Image Erasing](Image Erasing.md)\n\n\n[Image Mix](Image Mix.md)\n\n\n[Image Manipulation](Image Manipulation.md)\n\n\n[Auto Augment](Auto Augment.md)\n\n\n[Feature Augmentation](Feature Augmentation.md)\n\n\n[Deep Generative Models](Deep Generative Models.md)\n\n\nSemantic Segmentation\n\nPASCAL VOC\n\n\nImage Classification\n\n\n\nObject Detection\n\n\n\nDiscussion for Future Directions\n\nlack of theoretical research on data augmentation\nsome methods can improve the accuracy, but we do not fully understand the reasons behind, such as pairing samples and mixup\nTo human eyes, the augmented data with pairing samples and mixup are visually meaningless\nno theory on the size of sufficient training datasets\nThe size of the dataset suitable for tasks and models is usually designed based on personal experience and through extensive experiments\nno unified metrics\nsaturate the minority class and cause overfitting\nUltimately, we expect generated data can simulate distribution similar with training data while diversity never losses.\nincrease in the amount of training data is not exactly proportional to the increase in the performance\nWhen a certain amount of data is reached, continue to increase the data without improving the effect.\ndespite the increase in the number of data, the diversity of data remains unchanged\nSince various data augmentation can be combined together to generate new image data, the selection and combination of data augmentation techniques are critical\nhow to choose and combine methods is a key point when performing data augmentation\n"},"KB/Image-Data":{"title":"Image Data","links":[],"tags":["temp"],"content":"Image Data\n\nCant use MLPs\n\nToo many weights to learn\nNo translation equi-invariance\n\n\n"},"KB/Image-Erasing":{"title":"Image Erasing","links":[],"tags":["augmentation"],"content":"Image Erasing\n\ndelete one or more sub-regions in the image\nreplace the pixel values of these sub-regions with constant values or random values\nHide-and-Seek\nrandom erasing\nGridMask\nCutout\nFenceMask\nsimulation of object occlusion strategy.\n"},"KB/Image-Generation-with-Colorization":{"title":"Image Generation with Colorization","links":[],"tags":["semisupervisedlearning"],"content":"Image Generation with Colorization\n\nImage colorization is a task of predicting a plausible color version of the photograph given a gray-scale photograph as input\nTo correctly colorize each pixel, networks need to recognize objects and to group pixels of the same part together. Therefore, visual features can be learned in the process of accomplishing this task.\nZhang et al. proposed to handle the uncertainty by posting the task as a clas- sification task and used class-rebalancing to increase the diversity of predicted colors [18]\nSome work specifically employs the image colorization task as the pretext for self- supervised image representation learning [18], [42], [82], [124]\nAfter the image colorization training is finished, the features learned through the colorization process are specifically evaluated on other downstream high-level tasks with transfer learning.\n"},"KB/Image-Generation-with-Inpainting":{"title":"Image Generation with Inpainting","links":[],"tags":["semisupervisedlearning"],"content":"Image Generation with Inpainting\n\n\nImage inpainting is a task of predicting arbitrary missing regions based on the rest of an image\nTo correctly predict missing regions, networks are required to learn the common knowledge including the color and structure of the common objects\nOnly by knowing this knowledge, networks are able to infer missing regions based on the rest part of the image.\n"},"KB/Image-Generation-with-Super-Resolution":{"title":"Image Generation with Super Resolution","links":[],"tags":["semisupervisedlearning"],"content":"Image Generation with Super Resolution\n\nImage super-resolution (SR) is a task of enhancing the resolution of images\nWith the help of fully convolutional networks, finer and realistic high-resolution images can be generated from low-resolution images\nperceptual loss which consists of an adversarial loss and a content loss\nWith the perceptron loss, the SRGAN is able to recover photo-realistic textures from heavily downsampled images and show significant gains in perceptual quality.\nThe networks for image super-resolution task are able to learn the semantic features of images\n"},"KB/Image-Generation":{"title":"Image Generation","links":[],"tags":["semisupervisedlearning"],"content":"Image Generation\n\nVisual features are learned through the process of image generation tasks.\nimage colorization\nsuper resolution\ninpainting\nimage generation with Generative Adversarial Networks (GANs)\n"},"KB/Image-Manipulation":{"title":"Image Manipulation","links":["KB/Fmix","KB/ManifoldMix"],"tags":["augmentation"],"content":"Image Manipulation\n\nimage transformations, such as rotation, flipping, and cropping, etc\nmanipulate the images directly and are easy to implement\nCutMix\nFmix\nAugMix\nManifoldMix\n"},"KB/Image-Mix":{"title":"Image Mix","links":[],"tags":["augmentation"],"content":"Image Mix\n\nmixing two or more images or sub-regions of images into one.\nsynthesizing every new image with two images randomly selected in the training set, known as pairing samples. The synthesis method used is to average the intensity of two images on each pixel\nMixup\n"},"KB/Image-Mixing-and-Deletion":{"title":"Image Mixing and Deletion","links":["KB/CutMix","KB/Mixup","KB/Cut-and-Delete","KB/Cutout","KB/Random-Erasing","KB/Hide-and-Seek","KB/GridMask","KB/Adversarial-Spatial-Dropout-for-Occlusion","KB/Cut-and-Mix","KB/Attentive-CutMix","KB/AttributeMix","KB/RICAP","KB/Mixed-Example","KB/CowMask","KB/ResizeMix","KB/SaliencyMix","KB/Intra-Class-Part-Swapping","KB/SnapMix","KB/KeepAugment","KB/Visual-Context-Augmentation","KB/Cut,-Paste-and-Learn","KB/Manifold-MixUp","KB/AugMix","KB/SmoothMix","KB/Co-Mixup","KB/Sample-Pairing","KB/Puzzle-Mix","KB/ReMix"],"tags":["augmentation"],"content":"Image Mixing and Deletion\n\n@naveedSurveyImageMixing2023\n[Cutout] and CutMix argues that hindering image regions enforces the classifier to learn from the partially visible objects and understand the overall structure\nCutMix verifies this argument by showing enhanced focus towards the target class in\nOpposite to this, MixUp has shown to improve classifier’s calibration and reduced prediction uncertainity\nmean of predictions vs accuracy where the confidence distribution for MixUp trained model is evenly distributed against the standard model whose disovertribution is towards higher conficence i.e. confidence\nSimilarly, the loss contours obtained for a network trained with MixUp are smooth as compared to sharp contours in standarad training\nbetter generalization and robustness of MixUp against adversarial attacks.\nMixup\nCut and Delete\nCutout\nRandom Erasing\nHide and Seek\nGridMask\nAdversarial Spatial Dropout for Occlusion\nCut and Mix\nCutMix\nAttentive CutMix\nAttributeMix\nRICAP\nMixed Example\nCowMask\nResizeMix\nSaliencyMix\nIntra-Class Part Swapping\nSnapMix\nKeepAugment\nVisual Context Augmentation\nCut, Paste and Learn\nManifold MixUp\nAugMix\nSmoothMix\nCo-Mixup\nSample Pairing\nPuzzle Mix\nReMix\n"},"KB/ImageNet":{"title":"ImageNet","links":[],"tags":["dataset"],"content":" "},"KB/Imagen":{"title":"Imagen","links":["KB/ImageNet","KB/EfficientNet","KB/Transformer","KB/COCO"],"tags":["architecture"],"content":"Imagen\n\nbetter top-1 accuracy on ImageNet than EfficientNet at similar latency\nPhotorealistic Text-to-Image Diffusion Models with Deep Language Understanding\ntext-to-image diffusion model\nlarge Transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation\nImagen produces 1024 \\times 1024 samples with unprecedented photorealism and alignment with text\ngeneric large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis\nincreasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model\nFID score\nCOCO\n"},"KB/Implementation-Invariance":{"title":"Implementation Invariance","links":[],"tags":["explainability"],"content":"Implementation Invariance\n\nTwo networks are functionally equivalent if their outputs are equal for all inputs, despite having very different implementations.\nAttribution methods should satisfy Implementation Invariance, i.e., the attributions are always identical for two functionally equivalent networks.\n"},"KB/Implicit-Bias":{"title":"Implicit Bias","links":[],"tags":["brain"],"content":"Implicit Bias\n\nThe unconscious attitudes, beliefs, or stereotypes we hold that have the power to affect our perceptions, actions, and decisions.\n\nImplicit Bias\n\nAutomatically making an association or assumption based on one’s mental models and memories. Implicit bias can affect the following\n\nHow data is collected and classified.\nHow machine learning systems are designed and developed.\n\n\nFor example, when building a classifier to identify wedding photos, an engineer may use the presence of a white dress in a photo as a feature. However, white dresses have been customary only during certain eras and in certain cultures.\n"},"KB/Implicitly-learning-when-to-be-ready---From-instances-to-categories":{"title":"Implicitly learning when to be ready - From instances to categories","links":[],"tags":["cognitivemodel"],"content":"Implicitly learning when to be ready - From instances to categories\n\n\nWouter Kruijne · Riccardo M. Galli · Sander A. Los\n\n\nrole of long-term memory in guiding temporal preparation in speeded reaction time tasks.\n\n\nIn experiments with variable foreperiods between a warning stimulus (S1) and a target stimulus (S2), preparation is affected by foreperiod distributions experienced in the past, long after the distribution has changed\n\n\nassociative nature of memory-guided preparation\n\n\nWhen distinct S1s predict different foreperiods, they can trigger differential preparation accordingly\n\n\nmemory-guided preparation allows for another key feature of learning: the ability to generalize across acquired associations and apply them to novel situations\n\n\nImages of either category were paired with different distributions with predominantly shorter versus predominantly longer foreperiods.\n\n\ndifferential preparation to never-before seen images of either category, without being aware of the predictive nature of these categories\n\n\ncontinued doing so in a subsequent Transfer phase, after they had been informed that these contingencies no longer held\n\n\nrolling regression analysis revealed at a fine timescale how category-guided preparation gradually developed throughout the task\n\n\nexplicit information about these contingencies only briefly disrupted memory-guided preparation\n\n\nPreparation across phases\nResults\n\nthe RT-FP curve, separately for the different S1 types during the Acquisition and Transfer phase\nthis curve is flatter for trials paired with S1E types than S1A types in both phases, indicating that the categories yielded differential preparation\nsignificant FP × Phase interaction\nsuggesting that there was a steeper RTFP slope during Acquisition than during Transfer\nevidence for a two-way S1 type × FP interaction\nbut not for a three way S1 type × Phase × FP interaction\ndifferent S1 types led to differential preparation in both phases\nbiased distributions in the Acquisition phase gave rise to long-lasting effects on preparation, persisting after the bias was removed\n\nTime course of differential preparation\n\nparticipants might have gotten somewhat fatigued throughout each block, but that block breaks allowed them to largely recover to baseline\nthe longer interruption between the Phases led to a more pronounced speeding up for the remaining two blocks\noverall temporal preparation remained largely consistent throughout the experiment\nthe results with a shorter window of 40 trials highlight that the ‘dip’ in the S1 type × FP time course at the start of the Transfer phase was very shortlived\nThe brevity of this effect could therefore explain why our earlier work, using block-wise analyses, consistently led us to conclude that this transition between phases had no noticeable effect on differential preparation\n\nDiscussion\n\nOur findings raise the possibility that participants in that study might have demonstrated similar memoryguided preparation even if they would have been unaware of the image-FP pairings\ncomplex interplay between implicit associative guidance and guidance by explicit awareness\nNevertheless, differential preparation was robust once acquired, persisting well into the Transfer phase despite the change in underlying FP distributions\nlonger Transfer phases, we similarly observed that the S1 type × FP interaction barely attenuated across Transfer blocks\nThe gradual acquisition of differential preparation and its longevity throughout the Transfer phase illustrate how temporal preparation is affected by long-term memory and sluggishly adapts to changing environmental statistics\nMany probability-driven models characterize preparation as guided by static representations of the current FP distribution (Janssen &amp; Shadlen, 2005; Grabenhorst et al., 2019; Trillenberg et al., 2000; Vangkilde et al., 2013), foregoing the role of memory and learning\nTransfer effects like those in the present study illustrate the need for a flexible basis for preparation, subject to learning and updating\n\nImages\n\n\n\n\n"},"KB/Improved-variational-inference-with-inverse-autoregressive-flows":{"title":"Improved variational inference with inverse autoregressive flows","links":["KB/Autoregressive","KB/autoregressive-flows"],"tags":["architecture"],"content":"Improved Variational Inference with Inverse Autoregressive Flows\n \n\nkingma et al 2016\nAutoregressive\nautoregressive flows\n"},"KB/Impulse":{"title":"Impulse","links":["KB/Force"],"tags":["physics"],"content":"Impulse\n\nAn impulse is defined as a Force applied over a period of time. Applying a larger Force or longer lasting Force produces a larger impulse. A large impulse produces a large change in momentum.\nJ = \\Delta p \nF \\Delta t = \\Delta p\nF \\Delta t = mv - mu\nF \\Delta t = m \\Delta v\n\\Delta v = \\frac{F}{m} \\Delta t\nJ is impulse in kg m/s\nF is Force\np is momentum\nv,u is velocity\n"},"KB/In-Silico":{"title":"In Silico","links":[],"tags":["brain"],"content":"In Silico\n\nAn experimental method to study brain or neural function using computer modeling or computer simulation.\n"},"KB/In-Vitro":{"title":"In Vitro","links":[],"tags":["brain"],"content":"In Vitro\n\nAn experimental method to study brain or neural function by looking at cells outside a living organism, for example, in a test tube or petri dish.\n"},"KB/In-Vivo":{"title":"In Vivo","links":[],"tags":["brain"],"content":"In Vivo\n\nAn experimental method allowing scientists to study brain or neural function in a living organism.\n"},"KB/In-group-Bias":{"title":"In-group Bias","links":[],"tags":["temp"],"content":"In-group Bias\n\n#fairness\nShowing partiality to one’s own group or own characteristics. If testers or raters consist of the machine learning developer’s friends, family, or colleagues, then in-group bias may invalidate product testing or the dataset.\nIn-group bias is a form of group attribution bias.\n"},"KB/Inattentional-Blindness":{"title":"Inattentional Blindness","links":["KB/Attention","KB/Change-Blindness"],"tags":["visualization"],"content":"Inattentional Blindness\n\nViewers can fail to perceive visual elements or activities caused by an absence of Attention to the unseen object\nRelated to Change Blindness\n"},"KB/Inception":{"title":"Inception","links":["KB/Conv","KB/Features","KB/Dimensionality-Reduction","KB/Batch-Normalization","KB/Inception","KB/Receptive-field"],"tags":["architecture"],"content":"Inception\n\n@szegedyGoingDeeperConvolutions2014\nRethinking the Inception Architecture for Computer Vision\n\nV1\n\nConv at different filter scales to find different kinds of Features → stack them up\nIncreasing both the depth and width of the network while keeping computations at a manageable level\nHuman visual system wherein information is processed at multiple scales and then aggregated locally\nchannel Dimensionality Reduction, by reducing the output channels of the input\nTo enable concatenation of Features convolved with different kernels, they pad the output to make it the same size as the input.\n\nwithout dilation\npadding p = (k-1)/2p\nsince out = in +2p -k +1\n\n\n\n\nV2/V3\n\nnxn Conv → 1xn followed by nx1 Conv\n5x5, 7x7 → 2 and three 3x3 seq Conv\nMore filters (wider)\nDistributed the computational budget in a balanced way between the depth and width of the network\nAdded Batch Normalization\n\n\nV4\n\n(from)\nPaper: arxiv.org/pdf/1602.07261.pdf\nYear: 2016\nSummary: New Residual Inception Architecture (deep CNN)\nWhy?\n\nIntroduction of residual connections on traditional architectures yielded SOTA performance (2015)\n\n\nResearch question\n\nAre there benefits when combining residual connections with the Inception architecture?\n\n\nFindings\n\nResidual connections accelerates training of the Inception architecture\nResidual Inception outperforming similar architecture only close\nWhen the number of filters were higher than 1′000 the residual variants of the network died early in the training (e.g. outputted only zeros)\n\nwas not able to fix with lowering the Learning Rate or Batch Normalization\n\n\nScaling down the residuals before adding them with the residual connection stabilized the training (factor: 0.1−0.3)\n\n\nGeneral Ideas\n\nParallel convolutions: Similar to the GoogLeNet architecture within their modules the authors simultaneously use multiple convolutional branches with different Receptive field sizes on the same input activation maps and again concatenate those activations for further processing.\nReduction modules: Instead of simply applying a single max pooling or a 2-stride convolution to downsize the spatial dimensions, the authors dedicated whole modules to this task again employing parallel branches. \nStrong usage of small convolutional kernels(e.g. 3×3): Throughout the network the authors pefer smaller convolutional kernel size over larger ones, as this enables the same Receptive field with less parameters (e.g. a single 5×5convolution Receptive field as 2 consecutive 3×3convolutions [∼18 params ], but the later has less parameters)\nFactorization of convolutions: They factorize convolutions of filter size n×n to a combination of 1×n and n×1 convolutions, in order to reduce the nr of parameters even further (e.g. 7×7 [∼49 params ] results in 1×7 and 7×1 [∼14 params ]!)\nResidual connections: In the Inception-ResNet-v1 and Inception-ResNet-v2 the authors employ the usage of residual connections. Although the residual version of the networks converge faster, the final accuracy seems to mainly depend on the model size.\nUsage of bottleneck layers: In order to reduce the cost of the individual convolutional branches within their modules, they apply 1×1convolutions at the beginning to reduce the depth of the input activation maps.\n\n\nRemarks\n\nAuthors disagree with residual paper one some points \n\nResidual connections are nessecary for training deep convolutional models\n\nThey show that it is not hard to train very deep models which achieve high performance without residual connections\nThey argue that residual connections do only speed up the training greatly\n\n\n“Warm up” phases (pre-training with very low LR followed by a high LR) do not help to stabilize training very deep networks\n\nhigh LR had the chance to destroy already learnt features\nScaling should be used instead\n\n\n\n\n\n\n"},"KB/Inceptionism":{"title":"Inceptionism","links":["Roam-Highlights","KB/Le-Net","KB/Features","KB/Layers","KB/Trees"],"tags":["visualization"],"content":"Inceptionism\n\nGoogle AI Blog: Inceptionism: Going Deeper into Neural Networks #Roam-Highlights\nLe Net\n\n\n\n\n\n\n\nOne of the challenges of neural networks is understanding what exactly goes on at each layer\nWe know that after training, each layer progressively extracts higher and higher-level Features of the image, until the final layer essentially makes a decision on what the image shows\nFor example, the first layer maybe looks for edges or corners\nIntermediate Layers interpret the basic Features to look for overall shapes or components, like a door or a leaf\nThe final few Layers assemble those into complete interpretations—these neurons activate in response to very complex things such as entire buildings or Trees.\nOne way to visualize what goes on is to turn the network upside down and ask it to enhance an input image in such a way as to elicit a particular interpretation.\nWhy is this important? Well, we train networks by simply showing them many examples of what we want them to learn, hoping they extract the essence of the matter at hand (e.g., a fork needs a handle and 2-4 tines), and learn to ignore what doesn’t matter (a fork can be any shape, size, color or orientation)\nBut how do you check that the network has correctly learned the right Features? It can help to visualize the network’s representation of a fork.\nIndeed, in some cases, this reveals that the neural net isn’t quite looking for the thing we thought it was.\nInstead of exactly prescribing which feature we want the network to amplify, we can also let the network make that decision\nIn this case we simply feed the network an arbitrary image or photo and let the network analyze the picture\nWe then pick a layer and ask the network to enhance whatever it detected.\nIf we choose higher-level Layers, which identify more sophisticated Features in images, complex Features or even whole objects tend to emerge\nAgain, we just start with an existing image and give it to our neural net.\nWe ask the network: “Whatever you see there, I want more of it!” This creates a [feedback loop](feedback loop.md): if a cloud looks a little bit like a bird, the network will make it look more like a bird\nThis in turn will make the network recognize the bird even more strongly on the next pass and so forth, until a highly detailed bird appears, seemingly out of nowhere.\nThe results are intriguing—even a relatively simple neural network can be used to over-interpret an image, just like as children we enjoyed watching clouds and interpreting the random shapes\nThis network was trained mostly on images of animals, so naturally it tends to interpret shapes as animals\nBut because the data is stored at such a high abstraction, the results are an interesting remix of these learned Features.\nOf course, we can do more than cloud watching with this technique\nWe can apply it to any kind of image\nThe results vary quite a bit with the kind of image, because the Features that are entered bias the network towards certain interpretations\nFor example, horizon lines tend to get filled with towers and pagodas\nRocks and Trees turn into buildings\nBirds and insects appear in images of leaves.\nWe must go deeper: Iterations If we apply the algorithm iteratively on its own outputs and apply some zooming after each iteration, we get an endless stream of new impressions, exploring the set of things the network knows about\nWe can even start this process from a random-noise image, so that the result becomes purely the result of the neural network, as seen in the following images:\nThe techniques presented here help us understand and visualize how neural networks are able to carry out difficult classification tasks, improve network architecture, and check what the network has learned during training\nIt also makes us wonder whether neural networks could become a tool for artists—a new way to remix visual concepts—or perhaps even shed a little light on the roots of the creative process in general.\n"},"KB/Independence":{"title":"Independence","links":[],"tags":["explainability"],"content":"Independence\n\nmodel predictions are independent of the sensitive feature.\nthe proportion of positive samples (namely, those ones belonging to the class of interest) given by the model is the same for all the subgroups within the sensitive feature\n"},"KB/Indirect-Volume-Visualization":{"title":"Indirect Volume Visualization","links":["KB/visualization","KB/Isosurface"],"tags":["visualization"],"content":"Indirect Volume visualization\n\nIsosurface\n"},"KB/Individual-Fairness":{"title":"Individual Fairness","links":[],"tags":["temp"],"content":"Individual Fairness\n\n\nA fairness metric that checks whether similar individuals are classified similarly. For example, Brobdingnagian Academy might want to satisfy individual fairness by ensuring that two students with identical grades and standardized test scores are equally likely to gain admission.\n\n\nNote that individual fairness relies entirely on how you define “similarity” (in this case, grades and test scores), and you can run the risk of introducing new fairness problems if your similarity metric misses important information (such as the rigor of a student’s curriculum).\n\n\ndeals with fairness from the perspective of all individual\n\n"},"KB/Individual-Modeling":{"title":"Individual Modeling","links":["KB/Ramp-up-problem"],"tags":["usermodel"],"content":"Individual Modeling\n\nMore personalized\nLess data per category\nRamp up problem\n"},"KB/Induced-Pluripotent-Stem-Cell-(iPSC)":{"title":"Induced Pluripotent Stem Cell (iPSC)","links":[],"tags":["brain"],"content":"Induced Pluripotent Stem Cell (iPSC)\n\nA cell that has been taken from adult tissue and genetically modified to behave like an embryonic stem cell, with the ability to develop into any type of cell found in the body, including nerve cells.\n"},"KB/Inductive-Bias":{"title":"Inductive Bias","links":["KB/Bayesian","KB/Bayesian-Prior","KB/Bayesian-Posterior","KNN","KB/LinearRegression","Logistic-Regression","KB/Non-Relational-Inductive-Bias","KB/Relational-Inductive-Bias"],"tags":["graph"],"content":"Inductive Bias\n\nSet of assumptions that the learner uses to predict outputs of given inputs that it has not yet encountered\nIn Bayesian\n\nBayesian Prior can shape the Bayesian Posterior in the way that it can be a similar distribution to the former\n\n\nIn KNN\n\nwe assume that similar data points are clustered near each other away from the dissimilar ones\n\n\nin LinearRegression\n\nwe assume that the variable Y is linearly dependent on the explanatory variables X.\nTherefore, the resulting model linearly fits the training data. However, this assumption can limit the model’s capacity to learn non-linear functions.\n\n\nin Logistic Regression\n\nassume that there’s a hyperplane that separates the two classes from each other. This simplifies the problem, but one can imagine that if the assumption is not valid, we won’t have a good model.\n\n\nNon Relational Inductive Bias\nRelational Inductive Bias\n"},"KB/Inductive-Learning":{"title":"Inductive Learning","links":["KB/Bayesian","KB/Clustering","KB/Latent-Semantic-Analysis","KB/Probability","KB/Overhypotheses"],"tags":["language"],"content":"Inductive Learning\n\nBayesian is inductive learning\nLearning is identifying which hypothesis set is a concept\nHypotheses don’t disappear, they just become less likely\nLearning develops through more experience\nOne challenge of Bayesian learning is that any small subset is consistent with many hypotheses\nDifferent hypotheses have different likelihoods based on the examples we are exposed to\nBut in the end we also prefer smaller hypotheses over larger ones: The size principle\nSimple Clustering methods can be used to get the data to automatically create the hypothesis space needed for Bayesian modelling\nProbabilities of different sets then match with human judgments surprisingly well\nClustering based on biology worked worse!\nClustering using linguistic co-occurrences with Latent Semantic Analysis also worked worse!\nHuman subject judgements of similarity worked best\nSuggests some human reasoning relies on Probability\nBayesian learning can also learn categories\nModels are capable of making generalizations about the specific objects as well as the appropriate generalizations about categorization (superordinate categories!) in general.\nAdvanced learning means learn constraints on what is a possible hypothesis\nHierarchical Bayesian Modelling (HBM) can explain how we acquire Overhypotheses\nusing observations from the lowest level (data) and calculating statistical inferences\n"},"KB/Inductive-Models":{"title":"Inductive Models","links":[],"tags":["graph"],"content":"Inductive Models\n \nInductive Models\n\nwe exploit a training set of labeled data to learn the relation between the inputs and outputs.\nThen we apply this to new test data. One way to think of this is that we are learning the rule that maps inputs to outputs and then applying it elsewhere.\n"},"KB/Inductive-Sensor":{"title":"Inductive Sensor","links":[],"tags":["robotics"],"content":"Inductive Sensor\n\nThe class of proximity sensors, which has half of a ferrite core, whose coil is part of an oscillator circuit. When a metallic object enters this field, at some point, the object will absorb enough energy from the field to cause the oscillator to stop oscillating. This signifies that an object is present in a given proximity.\n"},"KB/Inertia":{"title":"Inertia","links":["KB/Force"],"tags":["physics"],"content":"Inertia\n\n“A body either remains at rest or continues to move at a constant velocity, unless acted upon by a net external Force.”\n"},"KB/Inference-Path":{"title":"Inference Path","links":[],"tags":["temp"],"content":"Inference Path\n\nIn a decision tree, during inference, the route a particular example takes from the root to other conditions, terminating with a leaf.\n"},"KB/Infix":{"title":"Infix","links":[],"tags":["language"],"content":"Infix\n\ninserted inside the stem\n"},"KB/Inflectional-Morphology":{"title":"Inflectional Morphology","links":["KB/Morphology"],"tags":["language"],"content":"Inflectional Morphology\n\ncreates the new forms of the same word\ne.g.bring, brought, brings\n"},"KB/Inflectional-words":{"title":"Inflectional words","links":[],"tags":["language"],"content":"Inflectional Words\n\nboundaries unclear, can express more than one grammar meaning\n"},"KB/Influence-of-image-classification-accuracy-on-saliency-map-estimation":{"title":"Influence of image classification accuracy on saliency map estimation","links":["KB/ICA","KB/Graph-based-visual-saliency","KB/Saliency-using-natural-statistics","KB/Dynamic-visual-attention","KB/Adaptive-Whitening-Saliency","KB/SAM-ResNet","KB/Sparsity","KB/Salicon-dataset","KB/OSIE","KB/PASCAL-S","KB/MIT1003","KB/MIT300","KB/AUC-Judd","KB/AUC-Borji","KB/Shuffled-AUC"],"tags":["explainability"],"content":"Influence of Image Classification Accuracy on Saliency Map Estimation\n\n@oyamaInfluenceImageClassification2018.\n\nIntro\n\nSaliency map estimation in computer vision aims to estimate the locations where people gaze in images.\nSince people tend to look at objects in images, the parameters of the model pre-trained on ImageNet for image classification are useful for the saliency map estimation\nno research on the relationship between the image classification accuracy and the performance of the saliency map estimation\nstrong correlation between image classification accuracy and saliency map estimation accuracy\nIt the models pre-trained on ImageNet are useful for saliency map estimation the parameters of is known that\nThis would be because a human tends to look at the centres of objects , which are learned to be recognised in the pre-trained model for the ImageNet classification task.\nAlthough the model based on DenseNet has achieved the state-of-the-art performance in the ACPR 2017 paper, this additional study led to even better performance using the model based on dual path networks (DPNs)\n\nRelated Work\n\nuses the coefficients of Attention based on Information the basis Maximization (AIM) calculated by the independent component analysis (ICA) in local image patches\nThe distribution of the coefficients is estimated by the kernel density estimation, which is used for estimating saliency maps based on the the local patches self-information of\nGraph-based visual saliency\nSaliency using natural statistics\nDynamic visual attention\nAdaptive Whitening Saliency\nSAM-ResNet\n\nComponents of Readout Net\n\nThe operation attempts to directly minimise the reconstruction error of the input image under a Sparsity constraint on an over-complete set of feature maps\nThe mini-batch size and learning rate were set to 1 and 10−5 during training, respectively.\nWe subtract the per-channels mean value of training images from each image as pre-processing\n\nDC\n\n“DC is also called as transposed convolution”\nWhen DC is used for the up-sampling layers in Readout Net, the first, second, and third DC followed by a ReLU layer in Readout Net reduce the channels to 128, 64, and 32, respectively. Then, the 1 × 1 convolution reduces the channel to 1 to predict the saliency map. The filter size of DC was set to 4 × 4.\nmethod to recover a high-resolution image from its additional low resolution counterpart with little computational cost, by rearranging the data along the channel into feature maps with a convolution operation.\n\nSPC\n\nWhen SPC is used for the up-sampling layers, each SPC layer reduces the channels to one forth, followed by a 3 × 3 convolution and a ReLU layer. Then, the 1 × 1 convolution predicts the saliency map from the output of the last SPC layer.\neach BI layer in the up-sampling network resizes a feature map twice while maintaining the feature-map channels, GPU was out of memory when three up-sampling layers were used for all channels of outputs of Main Net\n\nBI\n\nthe order of up-sampling and projection (1 × 1 convolution) can be inverted without any influence on the output\nthe concatenated feature maps from Main Nets are first processed by the 1 × 1 convolution to output feature map, followed by the BI up-sampling layers. the 1-channel\n\nDatasets\n\nSalicon dataset\nOSIE\nPASCAL-S\nMIT1003\nMIT300\n\nMetrics\n\nAUC-Judd\nAUC-Borji\nShuffled-AUC\n\nConclusions\n\nstrong correlation between image classification accuracy and saliency map estimation accuracy.\nnot only the architecture but also the initialisation strategy using the weights pre-trained with the ImageNet classification task were important for estimating the saliency maps\nmodel which is pre-trained with the ImageNet classification and has achieved high\n“for performance on the classification task is also useful the”\n“saliency map estimation task”\nhuman fixations often concentrate on objects in the image, while the model pre-trained on ImageNet can react on many objects in images because ImageNet has a wide variety of object categories.\nIf the model is initialised with random weights and is trained on a fixation dataset with the limited categories of objects for saliency map estimation, to the objects in the training dataset the model would overfit\nif the model is trained for the image classification task which includes a wide variety of categories, overfitting for the objects in the training dataset would be suppressed owing to a large number of categories.\n\nImages\n\n\n{:height 598, :width 600}\n\n\n"},"KB/Information-Gain":{"title":"Information Gain","links":["KB/Entropy"],"tags":["temp"],"content":"Information Gain\n\nIn decision forests, the difference between a node’s Entropy and the weighted (by number of examples) sum of the Entropy of its children nodes. A node’s Entropy is the Entropy of the examples in that node.\n"},"KB/Information-Visualization":{"title":"Information Visualization","links":["KB/visualization","KB/Perception","KB/Visual-Encoding"],"tags":["visualization"],"content":"Information visualization\n\nvisualization of abstract data\nVisual mappings often have to be learned\nspatial layout is chosen\nPerception\nVisual Encoding\n"},"KB/Informativeness":{"title":"Informativeness","links":[],"tags":["explainability"],"content":"Informativeness\n\nML models are used with the ultimate intention of supporting decision making\nshould not be forgotten that the problem being solved by the model is not equal to that being faced by its human counterpart\ngreat deal of information is needed in order to be able to relate the user’s decision to the solution given by the model, and to avoid falling in misconception pitfalls.\nexplainable ML models should give information about the problem being tackled.\n"},"KB/Inhibitory-Control-Network":{"title":"Inhibitory Control Network","links":["KB/Brain-Areas","inferior-Frontal-Gyri","Medial-Frontal-Gyri","Opercular-Cingulate","Insular-Cingulate","Orbital-Posterior-Cingulate","Posterior-Parietal-Cortex"],"tags":["temp"],"content":"\ntoc: true\ntitle: Inhibitory Control Network\ntags: [‘temp’]\n\nInhibitory Control Network\n\nBrain Areas related to response inhibition ability\ninferior Frontal Gyri and Medial Frontal Gyri, the Opercular Cingulate, Insular Cingulate, Orbital Posterior Cingulate and Posterior Parietal Cortex\n"},"KB/Initialization":{"title":"Initialization","links":["KB/Xavier-Initialization","KB/He-Initialization","KB/LeCun-Init","KB/Orthogonal-Initialization"],"tags":["temp"],"content":"Initialization\n\n\nXavier Initialization , He Initialization , LeCun Init, Orthogonal Initialization\n"},"KB/Instance-Normalization":{"title":"Instance Normalization","links":[],"tags":["architecture","normalization"],"content":"Instance Normalization\n\nContrast Normalization\n\n  y_{tijk} = \\frac{x_{tijk} - \\mu_{ti}}{\\sqrt{\\sigma_{ti}^2 + \\epsilon}},\n  \\quad\n  \\mu_{ti} = \\frac{1}{HW}\\sum_{l=1}^W \\sum_{m=1}^H x_{tilm},\n  \\quad\n  \\sigma_{ti}^2 = \\frac{1}{HW}\\sum_{l=1}^W \\sum_{m=1}^H (x_{tilm} - mu_{ti})^2\n\n\n- This prevents instance-specific mean and [Covariance](Covariance.md) shift simplifying the learning process.\n- Intuitively, the normalization process allows to remove instance-specific contrast information from the content image in a task like image stylization, which simplifies generation.\n- ![](../images/Pasted%20image%2020221211131622.png)"},"KB/Instance-based-Learning":{"title":"Instance-based Learning","links":[],"tags":["robotics"],"content":"Instance-based Learning\n\nan object category is represented by a set of known instances a nearest neighbor classifier is used\nIBL considers category learning as a process of learning about the instances of the category:\nThe training phase is very fast\nIBL can recognize objects using a very small number of experiences IBL is a baseline approach to evaluate object representations Simple and easy to implement\nMemory usage in instance-based systems is continuously growing. Computational complexity grows with the number of training instances\nThe computational complexity of classifying a single new instance is O(n), where n is number of instances stored in perceptual memory.\nSalience and forgetting mechanisms can be used to bound the memory usage which are also useful for reducing the risk of overfitting to noise in the training set.\nOverfitting\nSensitive to noise\n\n"},"KB/Instant-NeRF":{"title":"Instant NeRF","links":["KB/Neural-Radiance-Field","KB/Data-Structures","KB/Gradient-Descent","KB/Density","KB/Operator-Fusion"],"tags":["architecture"],"content":"Instant NeRF\n\nInstant Neural Graphics Primitives with a Multiresolution Hash Encoding\nNeural Radiance Field\nNeural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate\nrely on task specific Data Structures\nnew input encoding that permits the use of a smaller network without sacrificing quality\neducing the number of floating point and memory access operations\nnear-instant training of neural graphics primitives on a single GPU for multiple tasks\nsmall neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through Gradient Descent\nautomatically focuses on relevant detail, independent of task at hand\nlow overhead\nIn a gigapixel image, they represent an image by a neural network. SDF learns a signed distance function in 3D space whose zero level-set represents a 2D surface\n2D images and their camera poses to reconstruct a volumetric radiance-and-Density field that is visualized using ray marching.\nneural volume learns a denoised radiance and Density field directly from a volumetric path tracer.\nonly vary the hash table size which trades off quality and performance\ndisambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern GPUs\nparallelism\nfully-fused Operator Fusion CUDA kernels with a focus on minimizing wasted bandwidth and compute operations\n"},"KB/Instruction-Bandwidth":{"title":"Instruction Bandwidth","links":[],"tags":["parallelcomputing"],"content":"Instruction Bandwidth\n\nBandwidth is the maximum amount of data that can travel through a ‘channel’\n"},"KB/Instruction-Cycle":{"title":"Instruction Cycle","links":[],"tags":["robotics"],"content":"Instruction Cycle\n\nThe time it takes for a robot controller system’s cycle to decode a command or instruction before it is executed. The Instruction Cycle must be analyzed very closely by robotic programmers to enable speedy and proper reaction to varying commands.\n"},"KB/Instruction-Latency":{"title":"Instruction Latency","links":[],"tags":["parallelcomputing"],"content":"Instruction Latency\n\nAmount of time to complete a task(time , seconds)\nfunction of how long it takes the data to get sent all the way from the start point to the end\n"},"KB/Instruction-Pipelining":{"title":"Instruction Pipelining","links":["KB/Instruction-Throughput"],"tags":["parallelcomputing"],"content":"Instruction Pipelining\n\nused in the design of modern microprocessors, microcontrollers and CPUs to increase their Instruction Throughput for the entire workload\ndivide the processing of a CPU instruction into a series of independent steps o microinstructions with storage at the end of each step.\nThis allows the CPUs control logic to handle instructions at the processing rate of the slowest step, which is much faster than the time needed to process the instruction as a single step\nIF: Instruction Fetch\nID: Instruction Decode, register fetch\nEX: Execution\nMEM: Memory Access\nWB: Register write Back\n"},"KB/Instruction-Throughput":{"title":"Instruction Throughput","links":[],"tags":["parallelcomputing"],"content":"Instruction Throughput\n\nthe number of instructions that can be executed in a unit of time\n(Jobs/Hour)\nhow much data actually does travel through the ‘channel’ successfully\n"},"KB/Instruction-level-programming":{"title":"Instruction level programming","links":["KB/Instruction-Pipelining","KB/SuperScalar","KB/Out-of-Order-Execution","KB/Register-Renaming","KB/Speculative-Execution","KB/Branch-Prediction"],"tags":["parallelcomputing"],"content":"Instruction Level Programming\n\nILP\nallows the compiler and the processor to overlap the execution of multiple instructions or even to change the order in which instructions are executed\nInstruction Pipelining\nSuperScalar\nOut of Order Execution\nRegister Renaming\nSpeculative Execution\nBranch Prediction\n"},"KB/Insula":{"title":"Insula","links":["KB/Cerebrum","KB/Sulcus"],"tags":["brain"],"content":"Insula\n\nSometimes referred to as the insular cortex, this small region of the Cerebrum is found deep within the lateral Sulcus, and is believed to be involved in consciousness, emotion, and keeping the body in balance.\n"},"KB/Integral-Lines":{"title":"Integral Lines","links":["KB/Streamlines","KB/Pathlines"],"tags":["visualization"],"content":"Integral Lines\n\nSeed particles in a flow field\nLet the particles move in the flow field\nCompute and show the trajectories\n\n\nStreamlines\nPathlines\n"},"KB/Integrated-Gradients":{"title":"Integrated Gradients","links":[],"tags":["explainability"],"content":"Integrated Gradients\n\n@sundararajanAxiomaticAttributionDeep2017\n\nTerms\n\n[Gradient Sensitivity](Gradient Sensitivity.md)\n[Implementation Invariance](Implementation Invariance.md)\n\nCalculation\n\na function F representing our model\ninput x \\in \\mathbb{R}^{n} because this is a general definition of IG and not CNN specific),\nbaseline x&#039; \\in \\mathbb{R}^{n}\nWe assume a straight line path between x and x’ and compute gradients along that path\nThe integrated gradient along i^{th} dimension is defined as:\nIntegratedGrads_i​(x)::=(x_i​-x_i&#039;​)\\times \\int_{\\alpha=0}^{1}\\frac{\\partial F(x&#039; + \\alpha \\times (x-x&#039;))}{\\partial x_{i}}d \\alpha\nThe original definition of Integrated Gradients is incalculable (because of the integral).\nTherefore, the implementation of the method uses approximated value by replacing the integral with the summation:\nIntegratedGrads_i​^{approx}(x)::=(x_i​-x_i&#039;​)\\times \\Sigma_{k=1}^{m}\\frac{\\partial F(x&#039; + \\frac{k}{m} \\times (x-x&#039;))}{\\partial x_{i}} \\times \\frac{1}{m}\nIn the approximated calculation (Eq. 2), m defines a number of interpolation steps.\n\nExplanation (chatgpt)\n\nIn IntegratedGrads, the goal is to understand the contribution of each pixel in the input image towards the model’s prediction. Specifically, for a given pixel i, IntegratedGrads computes the partial derivative of the model output with respect to that pixel and integrates it along a path from a baseline image to the input image, weighting each step of the path by the partial derivative.\nThe equation you provided is an approximation of IntegratedGrads, which involves dividing the path into m equally spaced steps and approximating the integral using a Riemann sum. Let’s break down the equation using an example of an image of a bird:\nIntegratedGrads_i^{approx}(x) represents the attribution of pixel i towards the model’s prediction, using the IntegratedGrads approximation.\n(x_i-x_i&#039;) is the difference between the input image pixel i and the baseline pixel i&#039;.\n\\Sigma_{k=1}^{m} is a sum over the m steps of the path from the baseline image to the input image.\n\\frac{\\partial F(x&#039; + \\frac{k}{m} \\times (x-x&#039;))}{\\partial x_i} is the partial derivative of the model output F with respect to pixel i at the k-th step of the path, where x&#039; + \\frac{k}{m} \\times (x-x&#039;) is the image at that step.\n\\frac{1}{m} is a weighting factor that ensures that each step of the path is given equal importance in the approximation.\nFor example, let’s say we have an image of a bird and we want to understand the contribution of the pixel at position (10, 20) towards the model’s prediction that the image is a bird. We set the baseline pixel value to be zero and divide the path into 10 steps. We compute the partial derivative of the model output with respect to pixel (10, 20) at each step of the path and weight each step by \\frac{1}{10} to obtain the approximation of IntegratedGrads for pixel (10, 20). The resulting attribution value will give us an indication of how important that pixel is in the model’s prediction.\n\nBaselines\n\nreplacing the constant color baseline with an alternative\n[Gaussian Baseline](Gaussian Baseline.md)\n[Blur Baseline](Blur Baseline.md)\n[Visualizing the Impact of Feature Attribution Baselines](Visualizing the Impact of Feature Attribution Baselines.md)\n"},"KB/Inter-rater-Agreement":{"title":"Inter-rater Agreement","links":[],"tags":["temp"],"content":"Inter-rater Agreement\n\nA measurement of how often human raters agree when doing a task. If raters disagree, the task instructions may need to be improved. Also sometimes called inter-annotator agreement or inter-rater reliability.\n"},"KB/Interactivity":{"title":"Interactivity","links":[],"tags":["explainability"],"content":"Interactivity\n\nthe ability of a model to be interactive with the user as one of the goals targeted by an explainable ML model.\nability to tweak and interact with the models is what ensures success\n"},"KB/Interneuron":{"title":"Interneuron","links":["KB/Impulse"],"tags":["brain"],"content":"Interneuron\n\nAssociation neuron\nImpulse moves between sensory and motor neurons\nmostly multipolar\n"},"KB/Interpolation":{"title":"Interpolation","links":["KB/1D-piecewise-linear-interpolation","KB/Bilinear-Interpolation","KB/Barycentric-Interpolation"],"tags":["visualization"],"content":"Interpolation\n\n1D piecewise linear interpolation\nBilinear Interpolation\nBarycentric Interpolation\n"},"KB/Interpretability-and-Explainability-A-Machine-Learning-Zoo-Mini-tour":{"title":"Interpretability and Explainability A Machine Learning Zoo Mini-tour","links":[],"tags":["explainability"],"content":"Interpretability and Explainability A Machine Learning Zoo Mini-tour\n@marcinkevicsInterpretabilityExplainabilityMachine2023"},"KB/Interpretability-vs-Neuroscience":{"title":"Interpretability vs Neuroscience","links":["rough-note","KB/Connectome","KB/Force","KB/Conv","KB/Correlation","KB/visualization","KB/Biological-Neuron","KB/Optogenetics","KB/Features"],"tags":["cognitivemodel"],"content":"Interpretability Vs Neuroscience\n\nInterpretability vs Neuroscience (rough note) — colah’s blog%20—%20colah’s%20blog)%20—%20colah’s%20blog)\n\nYou Can Get the Responses of All Neurons for Arbitrarily Many Stimuli\n\nIn neuroscience, one is limited in the number of neurons they can record from, their ability to select the neurons they record, and the number of stimuli they can record responses to.\nFor artificial neural networks, we can record the responses of all neurons to arbitrarily many stimuli\nTurn arounds are much faster than biological experiments\nThere’s no recording noise, No synaptic fatigue.\n\nNot Only Do You Have the Connectome, You Have the Weights!\n\nA major undertaking in neuroscience is the attempt to access the Connectome\nEven if they succeed, they won’t know the weights of those connections\nWith artificial neural networks, all the connections and weights are simply there for us to look at.\nAnd since we also know how these artificial neurons are computed, in principle we have everything we need to just reason through and understand the neural network.\n\nWeight-tying Massively Reduces the Number of Unique Neurons!\n\nweight-tying,\nForce many neurons to have the same weights\nhe most common use of this is in convolutional neural networks, where each neuron has translated copies of itself with the same weights.\nin ImageNet Conv nets, weight-tying often reduces the number of unique neurons in early vision by 10,000x or even more\nThis results in artificial neural networks having many fewer neurons for early vision than their biological counterparts\nThis means we can just literally study every single neuron.\n\nEstablishing Causality by Optimizing the Input\n\none of the thorniest issues in understanding neurons in artificial networks is separating Correlation from causation.\nDoes a neuron detect a dog head? Or does it just detect part of a dog head?\nThere’s a second very closely related problem: we don’t know what the space of likely functions a neuron might perform is.\nthis is also a challenge in neuroscience.\nWe create stimuli “from scratch” to strongly activate neurons (or combinations of neurons) in artificial neural networks, by starting with random noise and optimizing the input.\nThe key property of feature visualization is that anything in the resulting visualization there because it caused the neuron to fire more\nIf feature visualization gives you a fully formed dog head with eyes and ears arranged appropriately, it must be detecting an entire dog head\nIf it just gives an eye, it’s probably only (or at least primarily) responding to that.\nRecent efforts in neuroscience have tried to develop similar methods [], by using an artificial neural network as a proxy for a biological one.\nunclear they give you the same ability to establish a causal link.\nIt seems hard to exclude the possibility that the resulting stimulus might have content which causes the artificial neurons predicting the Biological Neuron to fire more, but aren’t causally necessary for the Biological Neuron to fire.\n\nInterventions, Ablations, and Edits\n\nOptogenetics has been a major methodological advance for neuroscience in allowing neuroscientists to temporarily ablate neurons, or to Force them to activate.\nArtificial neural networks are trivial to manipulate at the level of neurons\nOne can easily ablate neurons or set them to particular activations\nBut one can also do more powerful “circuit editing” where one modifies parameters at a finer grained level.\nIn image generation, Bau et al., 2018 show that you can ablate neurons to remove objects like tress and windows from generated images\nIn RL, Hilton et al., 2020 show that you can ablate Features to blind an agent to a particular enemy while leaving other competencies in tact\nMore recently, Cammarata et al, 2021 reimplements a large chunk of neural network from scratch, and then splices it into a model.\n\nWe Can Study the Exact Same Model.\n\nNeuroscientists might study a model organism species, but each brain they study has different neurons\nIf one neuroscientist reports on an interesting neuron they found, other neuroscientists can’t directly study that same neuron\nIn fact, the neuroscientists studying the original neuron will quickly lose access to it: probes can’t be left in indefinitely, organisms die, human subjects leave, and even setting that aside neurons change over time.\nStudying artificial networks, we can collaboratively reverse engineer the same “brain”, building on each other.\nwe have a shared web of thousands of “footholds” into InceptionV1, consisting of neurons we understand fairly well and know the connections between, which makes it massively easier to explore\n"},"KB/Interpretability":{"title":"Interpretability","links":[],"tags":["explainability"],"content":"Interpretability\n\nability to explain or to provide the meaning in understandable terms to a human\n"},"KB/Interpretation-of-Neural-networks-is-fragile":{"title":"Interpretation of Neural networks is fragile","links":[],"tags":["explainability"],"content":"Interpretation of Neural Networks is Fragile\n\nGhorbani et al\n@ghorbaniInterpretationNeuralNetworks2018\n"},"KB/Interpreting-Attention":{"title":"Interpreting Attention","links":["KB/Attention"],"tags":["architecture"],"content":"Interpreting Attention\n\nAttention Interpretability Across NLP Tasks\nempirically prove the hypothesis that Attention weights are interpretable and are correlated with feature importance measures\nn both single and pair sequence tasks, the Attention weights in samples with original weights do make sense in general\nHowever, in the former case, the Attention mechanism learns to give higher weights to tokens relevant to both kinds of sentiment.\nThey show that Attention weights in single sequence tasks do not provide a reason for the prediction, which in the case of pairwise tasks, Attention do reflect the reasoning behind model output\nBertViz repo\n"},"KB/Interpretive-Labor":{"title":"Interpretive Labor","links":["KB/Research-Debt"],"tags":["random"],"content":"Interpretive Labor\n\nThere’s a tradeoff between the energy put into explaining an idea, and the energy needed to understand it.\nOn one extreme, the explainer can painstakingly craft a beautiful explanation, leading their audience to understanding without even realizing it could have been difficult\nOn the other extreme, the explainer can do the absolute minimum and abandon their audience to struggle.\nThat is, really outstanding tutorials, reviews, textbooks, and so on.\nwe often have a group of researchers all trying to understand each other\nJust like before, the cost of explaining stays constant as the group grows, but the cost of understanding increases with each new member\nAt some size, the effort to understand everyone else becomes too much.\nAs a defense mechanism, people specialize, focusing on a narrower area of interest.\nThe maintainable size of the field is controlled by how its members trade off the energy between communicating and understanding.\nResearch Debt is the accumulation of missing interpretive labor.\nIt’s extremely natural for young ideas to go through a stage of debt, like early prototypes in engineering.\nThe problem is that we often stop at that point.\nYoung ideas aren’t ending points for us to put in a paper and abandon.\nWhen we let things stop there the debt piles up.\nIt becomes harder to understand and build on each other’s work and the field fragments.\n"},"KB/Interview-Tips":{"title":"Interview Tips","links":[],"tags":["jobsearch"],"content":"Interview Tips\nIntro\n\nName\nDo some sleuthing about the interviewer\n\nIntroduce\n\nI have always been a creative technologist\ntinkering, building experiences and products either on my vision or someone elses. Sharing my knowledge, teaching is important too\nmasters, specialization - AI, CV and analytics\nArtist!\n\nWhat Makes You Unique?\nIntroduce Yourself\n\nhiring managers are context switching to your interview. they probably dont know jack about you. so tell them\nWho you are professionally rn. One sentence\nSome sentences about your experience or education. start with recent and then go back in time\nWhy is this the best next step for me. Aka why do I want this job\n\nIn X Years?\n\nlong term goals - align with the company\ndont say a higher position\nhow will this position help you grow in a direction you are proud of\nlearning the industry, go to person with X skill, learn more skills, “mentor others”, bigger projects\nknow my strenghts, new opportunities to learn in the company and stuff\n\nWhy Do You want This Job?\n\nthe motivation\nthe job, the role, the team\n\nWhat Questions for the Interviewer?\n\nI did read the job description, but from your perspective how would you describe the role?\nwhat makes this role available?\nwhat kind of additional responsibilites can be gained over time?\nwhat can be expected in say the first few months?\nhow do you measure sucess for this role?\nwhat have been the biggest challenges for the role?\nwhat are some mistakes people have made in this role before?\n\nSend a Thank You Email in 24 Hours"},"KB/Intra-cluster-variance":{"title":"Intra Cluster Variance","links":[],"tags":["loss"],"content":"Intra Cluster Variance\n\n$$J = \\Sigma_{j=1}^K \\Sigma_{x \\in S_j} ||x - \\mu_j||^2$\nMeasure of how much the points in a given cluster spread\n"},"KB/Intra-Class-Part-Swapping":{"title":"Intra-Class Part Swapping","links":[],"tags":["augmentation"],"content":"Intra-Class Part Swapping\n\n@zhangIntraClassPartSwapping2021\nreplaces most attentive regions of one image by the other\nAttentive regions are extracted using a classification activation map (CAM), thresholded for the most prominent region\nThe attentive region in the source image is scaled and translated according to the attentive region of the target image for region replacement\nThe label information of the output is similar to the target image as this approach relies on augmenting similar class images.\n"},"KB/Intravenous":{"title":"Intravenous","links":[],"tags":["medical"],"content":"Intravenous\n\nAdministration of medication or fluids by vein\n"},"KB/Intubation":{"title":"Intubation","links":["KB/medical"],"tags":["medical"],"content":"Intubation\n\nmedical insertion of a tube into the body, for example, into the throat to assist with breathing\n"},"KB/Intuitive-Color-spaces":{"title":"Intuitive Color spaces","links":["KB/Color-Spaces"],"tags":["visualization"],"content":"Intuitive Color Spaces"},"KB/Invariant-Distribution":{"title":"Invariant Distribution","links":["KB/PDF","KB/Markov-Chain","KB/Ergodic"],"tags":["distributions"],"content":"Invariant Distribution\n\nIf g is the PDF. T(x|y) is the PDF of the transition kernel. Homogenous Markov Chain. Then g is PDF of an invariant distribution of T(x|y) if\ng(x) = \\int_{\\mathbb{R}^{k}}T(x|y)g(y)dy\nAtleast one invariant distribution\nErgodic\n"},"KB/Inverse-Kinematics":{"title":"Inverse Kinematics","links":[],"tags":["robotics"],"content":"Inverse Kinematics\n\nwhich joint movements (q) are needed achieve a particular robot end effector pose (ξ)\nq = κ−1(ξ) : q = {qi,i ∈ [1,…,n]}\n"},"KB/Inverse-Reinforcement-Learning":{"title":"Inverse Reinforcement Learning","links":[],"tags":["robotics"],"content":"Inverse Reinforcement Learning\n\nBasically, IRL is about studying from humans.\nInverse reinforcement learning is the sphere of studying an agent’s objectives, values, or rewards with the aid of using insights of its behavior.\nWe can fit a reward function with the use of professional demonstrations. Once a reward feature is fitted, we are able to use Policy Gradient, Model-based RL or different RL to locate the ideal policy.\nFor example, we are able to compute the policy gradient with the use of the reward feature as opposed to sampled rewards. With the policy gradient calculated, we optimize the policy closer to the finest rewards gain.\nAs part of the IRL, the task is to collect a set of human-generated driving data and extract an approximation of that human’s reward function for the task. Of course, this approximation necessarily relates to a simplified driving model.\nAs Ng and Russell put it, “the reward function, rather than the guideline, is the most concise, robust, and transferable definition of the task” because it quantifies how good or bad certain actions are. Once we have the right reward function, the problem is finding the right guideline and can be solved using standard reinforcement learning methods.\nFor our autonomous car example, we would use human driving data to automatically learn the correct functional weights for the reward. Since the task is fully described by the reward function, we don’t even need to know the details of human politics as long as we have the right reward function to optimize.\n"},"KB/Inverse-Square-Law":{"title":"Inverse Square Law","links":["KB/Force"],"tags":["physics"],"content":"Inverse Square Law\n\nfor Force on a charge in an electric field of another charge: Force is proportional to the product of the charges and inversely proportional to the square of the distance between them\nF_{E}= \\frac{\\frac{1}{4pe_{0}}Q_{1}Q_{2}}{r^{2}}\nForce on a mass in a gravitational field of another mass: Force is proportional to the product of the masses and inversely proportional to the square of the distance between them\nF_{G}= -G \\frac{m_{1}m_{2}}{r^{2}}\n"},"KB/Ion-Channel":{"title":"Ion Channel","links":[],"tags":["brain"],"content":"Ion Channel\n\nA pore in the membrane of a neuron that allows ions to pass through, helping to shape action potentials.\n"},"KB/Isolating-words":{"title":"Isolating words","links":[],"tags":["language"],"content":"Isolating Words\n\nwords do not divide into smaller units\n"},"KB/Isoline":{"title":"Isoline","links":["KB/Contour","KB/Marching-Squares"],"tags":["visualization"],"content":"Isoline\n\n2D\nContour line\n\\{(x,y|f(x,y)=c\\}\nCurve along function has constant value c\n\nMarching Squares\n\nNo isoline inside cells with same signs\nonly consider cells with different signs\naccess look-up table for respective case\n"},"KB/Isosurface":{"title":"Isosurface","links":["KB/Marching-Cubes","KB/Marching-Tetrahedra","KB/Fractional-Anisotropy"],"tags":["visualization"],"content":"Isosurface\n\n\nMarching Cubes\nMarching Tetrahedra\nFractional Anisotropy\n\n"},"KB/Isotropic-Architectures":{"title":"Isotropic Architectures","links":["KB/Transformer","MLP-Mixer"],"tags":["architecture"],"content":"Isotropic Architectures\n\n(of an object or substance) having a physical property which has the same value when measured in different directions.\nAnd precisely that is what an isotropic architecture is. Isotropic architectures do not produce pyramid shaped data transformations, but rather fixed ones where data does not change in shape and size\nIn other (simpler) words, when you take a look at the value going through an isotropic network, it doesn’t change in size.\nLike Transformer , MLP-Mixer\n"},"KB/Issues":{"title":"Issues","links":["KB/Multiple-Local-Minima","KB/Saddle-Points","Vanishingexploding-gradients","KB/Image-Data","KB/Fitting","KB/Freedom","KB/Bias-Variance-Dilemma","KB/Complex-Geometry","KB/Lack-of-information"],"tags":["temp"],"content":"Issues\n\nMultiple Local Minima\nSaddle Points\nVanishingexploding gradients\nImage Data\nFitting\nFreedom\nBias Variance Dilemma\nComplex Geometry\nLack of information\n"},"KB/Iterative-Closest-Point":{"title":"Iterative Closest Point","links":[],"tags":["robotics"],"content":"Iterative Closest Point\n\nStart from initial guess\nIterate\nFor each point on M, find closest point on P\nFind best transform for this correspondence Transform M\nGood initial guess → Converges to global minimum\nThe ICP is applicable when we have a relatively good starting point in advance.\nOtherwise, it will be trapped into the first local minimum and the solution will be useless.\nWithout pose information, ICP-based approaches are unable to recover the proper transformations because of the ambiguity in surface matching.\n\n"},"KB/Jaccard-Distance":{"title":"Jaccard Distance","links":[],"tags":["loss"],"content":"Jaccard Distance\n\nD(x,y) = 1- \\frac{x\\cup y}{x\\cap y}\nThe Jaccard index (or Intersection over Union) is a metric used to calculate the similarity and diversity of sample sets. It is the size of the intersection divided by the size of the union of the sample sets.\nIn practice, it is the total number of similar entities between sets divided by the total number of entities.\nTo calculate the Jaccard distance we simply subtract the Jaccard index from 1\nhighly influenced by the size of the dat\nLarge datasets can have a big impact on the index as it could significantly increase the union whilst keeping the intersection similar\nThe Jaccard index is often used in applications where binary or binarized data are used\ndeep learning model predicting segments of an image\ntext similarity analysis to measure how much word choice overlap there is between documents\n"},"KB/Jacobian-Matrix":{"title":"Jacobian Matrix","links":[],"tags":["algebra"],"content":"Jacobian Matrix\n \n\nGiven a function of mapping a n-dim input vector x to a m-dim output vector \\mathbf{f}: \\mathbb{R}^n \\mapsto \\mathbb{R}^m, the matrix of all first order partial derivates of this function is the Jacobian matrix J\n\\mathbf{J}_{ij} = \\frac{\\partial f_i}{\\partial x_j}\n\n\\frac{\\partial f_1}{\\partial x_1} &amp; \\dots &amp; \\frac{\\partial f_1}{\\partial x_n} \\\\[6pt]\n\\vdots &amp; \\ddots &amp; \\vdots \\\\[6pt]\n\\frac{\\partial f_m}{\\partial x_1} &amp; \\dots &amp; \\frac{\\partial f_m}{\\partial x_n} \\\\[6pt]\n\\end{bmatrix}$$\n\n## Determinant\n- The absolute value of the determinant can be thought of as a measure of _“how much multiplication by the matrix expands or contracts space”._\n- Only exists for square matrices\n- if det(M) = 0, then M is not invertible\n- The determinant of a 2 × 2 matrix is ${\\displaystyle {\\begin{vmatrix}a&amp;b\\\\c&amp;d\\end{vmatrix}}=ad-bc}$\n- determinant of a 3 × 3 matrix is $\\begin{vmatrix} a &amp; b &amp; c \\\\ d &amp; e &amp; f \\\\ g &amp; h &amp; i \\end{vmatrix}= aei + bfg + cdh - ceg - bdi - afh$\n- nxn matrix M is $$\\det M = \\det \\begin{bmatrix}\na_{11} &amp; a_{12} &amp; \\dots &amp; a_{1n} \\\\\na_{21} &amp; a_{22} &amp; \\dots &amp; a_{2n} \\\\\n\\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\\na_{n1} &amp; a_{n2} &amp; \\dots &amp; a_{nn} \\\\\n\\end{bmatrix} = \\sum_{j_1 j_2 \\dots j_n} (-1)^{\\tau(j_1 j_2 \\dots j_n)} a_{1j_1} a_{2j_2} \\dots a_{nj_n}$$\n- where the subscript under the summation j1⁢j2…jn are all permutations of the set {1, 2, …, n}, so there are n! items in total; τ⁡(.) indicates the signature of a permutation.\n- $det(AB) = det(A)det(B)$"},"KB/Jensen-Shannon-Divergence-Consistency-Loss":{"title":"Jensen Shannon Divergence Consistency Loss","links":[],"tags":["loss"],"content":"Jensen Shannon Divergence Consistency Loss\n\n\n@linDivergenceMeasuresBased\n\n\nenforces smother neural network responses\n\n\nstable, consistent and insensitive across range of inputs\n\n\n\n\n\n\\mathcal{L}(p_{orig}, y)+ \\lambda JS(p_{orig};p_{augmix1}; p_{augmix2})\n- $$\nM = \\frac{p_{orig}+p_{augmix1}+ p_{augmix2}}{3}\n\n\n\n\n\nJS(p_{orig}; p_{augmix1};p_{augmix2}) = \\frac{1}{3}(KL[p_{orig}||M||]+KL[p_{augmix1}||M||]+KL[p_{augmix2}||M||])\n"},"KB/Joint-Factor-Analysis":{"title":"Joint Factor Analysis","links":["KB/Speaker-Verification","KB/Features","KB/Cosine-Similarity","KB/NIST-2008-Speaker-Recognition-Evaluation-dataset"],"tags":["architecture"],"content":"Joint Factor Analysis\n\nFront-end Factor Analysis for Speaker Verification\nJoint Factor Analysis (JFA)\neature extractor to learn a low-dimensional speaker representation for Speaker Verification, which is also used to model session and channel effects/variabilities\nIn this new space, a given speech utterance is represented by a new vector named total factors (called the identity-vector or the “i-vector”)\nThe i-vector is thus a feature that represents the characteristics of the frame-level Features’ distributive pattern\n[dimensionality reduction](dimensionality reduction.md) of the GMM supervector (although the GMM supervector is not extracted when computing the i-vector)\nextracted in a similar manner with the eigenvoice adaptation scheme or the JFA technique\nextracted per sentence\nSupport-Vector-Machine-based system that uses the cosine kernel to estimate the similarity between the input data\nCosine Similarity as the final decision score\nremoved the SVM from the decision proces\nno speaker enrollment\nEER\nMinDCF\nNIST 2008 Speaker Recognition Evaluation dataset\nUp until d-vectors, the state-of-the-art Speaker Verification systems were based on the concept of i-vectors\n"},"KB/Joint-Interpolated-Motion":{"title":"Joint Interpolated Motion","links":["KB/Servo-Control","KB/Pick-and-Place-Cycle"],"tags":["robotics"],"content":"Joint Interpolated Motion\n\nA method of coordinating the movement of the joints, such that all joints arrive at the desired location simultaneously. This method of Servo Control produces a predictable path regardless of speed and results in the fastest Pick and Place Cycle time for a particular move.\n"},"KB/Joint-Motion-Type":{"title":"Joint Motion Type","links":["KB/Point-to-Point"],"tags":["robotics"],"content":"Joint Motion Type\n\nAlso known as Point-to-Point Motion, Joint Motion Type is a method of path interpolation that commands the movement of the robot by moving each joint directly to the commanded position so that all axis arrive to the position at the same time. Although the path is predictable, it will not be linear.\n"},"KB/Joint-Space":{"title":"Joint Space","links":[],"tags":["robotics"],"content":"Joint Space\n\na. Joint Space (or Joint Coordinates) is just a method of defining the position of the robot in terms of the value of each axis instead of as a TCP position. For example, the Home Position of a robot is often defined in Joint Space as each axis being at 0 degrees.\nb. The set of joint positions.\n"},"KB/Joint-Velocity":{"title":"Joint Velocity","links":[],"tags":["robotics"],"content":"Joint Velocity\n\n[Joint space](Joint space.md) trajectory is generally smoother than task space trajectory\n\n"},"KB/Jukebox":{"title":"Jukebox","links":[],"tags":["architecture"],"content":"Jukebox\n\ngenerates music with singing in the raw audio domain\nearlier models in the text-to-music genre generated music symbolically in the form of a pianoroll which specifies timing, pitch and velocity.\nThe challenging aspect is the non-symbolic approach where music is tried to be produced directly as a piece of audio\nthe space of raw audio is extremely high dimensional which makes the problem very challenging\nthe key issue is that modelling that raw audio produces long-range dependencies, making it computationally challenging to learn the high-level semantics of music.\nhierarchical VQ-VAE architecture to compress audio into a discrete space [14], with a loss function designed to retain the most amount of information.\nThis model produces songs from very diferent genres such as rock, hip-hop and jazz.\n"},"KB/KITTI":{"title":"KITTI","links":[],"tags":["dataset"],"content":"KITTI\n\ncollected from driving a car around a city which equipped with various sensors including high-resolution RGB camera, grayscale stereo camera, a 3D laser scanner, and highprecision GPS measurements and IMU accelerations from a combined GPS/IMU system\nVideos with various modalities captured by these sensors are available in this dataset.\n"},"KB/KL-Divergence":{"title":"KL Divergence","links":["KB/Entropy","KB/Cross-Entropy","KB/PDF"],"tags":["loss"],"content":"KL Divergence\n\nClassification\nEntropy + Cross Entropy\nDistribution Based metric\nMeasures difference between two PDF\nWe first define xlogx for a weird edge case x \\cdot \\log\\left( x \\right)\n\nThen Entropy \\mathrm{sum}\\left( \\mathrm{xlogx}\\left( y \\right) \\right) \\cdot \\mathrm{//}\\left( 1, \\mathrm{size}\\left( y, 2 \\right) \\right)\nThen cce as defined before  - \\mathrm{sum}\\left( y \\cdot \\log\\left( ŷ \\right) \\right)\nFinally KLD entropy + crossentropyloss\n\nKL(p,q) = \\Sigma_x p(x) log\\frac{p(x)}{q(x)}\n"},"KB/KMeans":{"title":"K Means","links":["KB/Intra-cluster-variance","KB/Decision-Boundaries","KB/Voronoi-Cell","KB/Gravity"],"tags":["temp"],"content":"K Means\n\nCodebook vectors. No manifolds.\nGiven: (x_i)_{i= 1,..,N} \\in \\mathbb{R}^n\nNeed : K clusters C_1 , … , C_K\nRandomly assign training points to K sets : S_j (j = 1, …, K)\nRepeat:\n\nFor each set S_j\n\nMean \\mu_j = |S_j|^{-1} \\Sigma_{x \\in S_j} x\nCreate new sets by putting points into set where ||x_i-\\mu_j|| is minimal\nIf empty, dismiss and reduce K to K’\n\n\n\n\nError quantity does not increase\n\nIntra cluster variance\nClusters are bounded by line Decision Boundaries and forms a Voronoi Cell\n\n\nDoes not work for curved boundaries\n\nCodebook Vector\n\nEach cluster represented by it\nVector pointing to the mean of all vectors in the cluster\nCenter of Gravity\n"},"KB/Kalman-Filter":{"title":"Kalman Filter","links":[],"tags":["robotics"],"content":"Kalman Filter\n\nThe standard Kalman filter is the optimum estimator when your system is linear and the system noise is Gaussian.\nlinear systems with Gaussian noise\n"},"KB/KeepAugment":{"title":"KeepAugment","links":[],"tags":["augmentation"],"content":"KeepAugment\n\nKeepAugment identifies the salient area in an image and assures the image generated by the augmentation strategies, for example, [Cutout], [RandAugment] [14], [CutMix] [82] or [AutoAugment](Cutout], [RandAugment] [14], [CutMix] [82] or [AutoAugment.md) [13], contains salient region in it.\nstandard augmentation may introduce distribution shifts\nincrease fidelity of the augmented data\nuse saliency maps to identify the regions of interest, then make sure those regins are not affected by the augmentation\nGiven image x and label logit value l_{y} , g_{i,j}(x,y) to be the vanilla gradient |\\nabla_{x}l_{y}(s)|\nFor RGB, channel wise maximum to get single saliency value for each pixel (i,j)\n\nSelective Cut\n\nRandomly sample regions S to be cut until importance score \\mathcal{I}(S, x, y) is smaller than threshold \\tau\n\n\\tilde x= (1-M(S)) \\odot x$$\n- where $M(S) = |M_{ij}(S)|_{ij}$ is the binary mask for $S$, $M_{ij} = \\mathbb{I}((i,j) \\in S)$\n\n## Selective Paste\n- image level augmented data $x&#039; = \\mathcal{A}(x)$ , uniformly sample region $S$ that satisfies $\\mathcal{I}(S,x,y) &gt; \\tau$ for a threshold $\\tau$ \n- paste the region $S$ of the original image  $x$ to $x&#039;$ \n\t- $$\\tilde x = M(S) \\odot x + (1-M(S)) \\odot x&#039;$$\n\t- $M_{ij}(S) = \\mathbb{I}((i,j) \\in S)$ is the binary mask of the region $S$\n- we choose our threshold $\\tau$ in an adaptive way.\n- given an image and consider an region size h × w of interest, we first calculate the importance scores of all possible candidate regions, following Eq. 1; then we set our threshold to be the $\\tau - quantile$ value of all the importance scores $\\mathcal{I}(S,x,y)$ of all candidate regions. For selective-cut, we uniformly keep sampling a mask region S until its corresponding score $\\mathcal{I}(S,x,y)$ is smaller than the threshold. For selective-paste, we uniformly sample a region S with importance score is greater than the threshold\n\n## Efficient Implementation of KeepAugment\n\n### Low Resolution Based Approximation\n- we proceed as follows: a) for a given image x, we first generate a lowresolution copy and then calculate its saliency map; b) we map the low-resolution saliency maps to their corresponding original resolution\n- This allows us to speed up the saliency maps calculation significantly, e.g., on ImageNet, we achieve roughly 3× computation cost reduction by reducing the resolution from 224 to 112.\n\n### Early Classification Head Based Approximation\n- In practice, we add an additional average pooling layer and a linear head after the first block of our networks evaluated\n- We achieve about 3× computation cost reduction in computing saliency maps\n\n## Region-Level Augmentation\n- including Cutout [8] and random erasing [45], work by randomly masking out or modifying rectangular regions of the input images\n- This procedure could be conveniently formulated as applying randomly generated binary masks to the original inputs\n- Precisely, consider an input image x of size H × W, and a rectangular region S of the image domain. Let M(S) = [Mij (S)]ij be the binary mask of S with Mij (S) = I((i, j) ∈ S)\n- Then the augmented data can be generated by modifying the image on region S, yielding images of form x′ = (1 − M(S)) ⊙ x + M(S) ⊙ δ, where ⊙ is element-wise multiplication, and δ can be either zeros (for Cutout) or random numbers (for random erasing)\n\n## Image-Level Augmentation\n- Exploiting the invariance properties of natural images, image-level augmentation methods apply label-invariant transformations on the whole image, such as solarization, sharpness, posterization, and color normalization\n- often manually designed and heuristically chosen\n- Recently, AutoAugment [4] applies reinforcement learning to automatically search optimal compositions of transformations\n- Several subsequent works, including RandAugment [5], Fast AutoAugment [18], alleviate the heavy computational burden of searching on the space of transformation policies by designing more compact search spaces\n\n## Data Augmentation and Its Trade-offs\n- Although data augmentation increases the effective size of data, it may inevitably cause loss of information and introduce noise and ambiguity if the augmentation is not controlled properly\n- To study this phenomenon empirically, we plot the train and testing accuracy on CIFAR-10 [16] when we apply Cutout with increasingly large cutout length in Figure 2(a), and RandAugment with increasing distortion magnitude \n- As typically expected, the generalization (the gap between the training and testing accuracy on clean data) improves as the magnitude of the transform increases in both cases\n- However, when the magnitudes of the transform are too large (≥ 16 for Cutout and ≥ 12 for RandAugment ), the training accuracy (blue line), and hence the testing accuracy (red line), starts to degenerate, indicating that augmented data no longer faithfully represent the clean training data in this case, such that the training loss on augmented data no longer forms a good surrogate of the training loss on the clean data.\n\n## Images\n- ![](../images/Pasted%20image%2020230508121420.png)\n- ![](../images/Pasted%20image%2020230508121431.png)\n- ![](../images/Pasted%20image%2020230508121452.png)\n- ![](../images/Pasted%20image%2020230508121458.png)"},"KB/Kernel-Filters":{"title":"Kernel Filters","links":["KB/Resistance","KB/Regularization"],"tags":["augmentation"],"content":"Kernel Filters\n\nsharpen and blur images\nThese filters work by sliding an n × n matrix across an image with either a Gaussian blur filter, which will result in a blurrier image, or a high contrast vertical or horizontal edge filter which will result in a sharper image along edges\nIntuitively, blurring images for Data Augmentation could lead to higher Resistance to motion blur during testing\nAdditionally, sharpening images for Data Augmentation could result in encapsulating more details about objects of interest.\nKang et al. experiment with a unique kernel filter that randomly swaps the pixel values in an n×n sliding window. They call this augmentation technique PatchShuffle Regularization\n"},"KB/Kernel-Support-Vector-Machines-(KSVMs)":{"title":"Kernel Support Vector Machines (KSVMs)","links":[],"tags":["temp"],"content":"Kernel Support Vector Machines (KSVMs)\n\nA classification algorithm that seeks to maximize the margin between positive and negative classes by mapping input data vectors to a higher dimensional space. For example, consider a classification problem in which the input dataset has a hundred features. To maximize the margin between positive and negative classes, a KSVM could internally map those features into a million-dimension space. KSVMs uses a loss function called hinge loss.\n"},"KB/Kernel-Visualization":{"title":"Kernel Visualization","links":[],"tags":["semisupervisedlearning"],"content":"# Kernel Visualization\n\nQualitatively visualize the kernels of the first convolution layer learned with the pretext tasks and compare the kernels from supervised model\nsimilarity of the kernels learned by supervised and selfsupervised models are compared to indicate the effectiveness of self-supervised methods\n"},"KB/Ketamine":{"title":"Ketamine","links":[],"tags":["brain"],"content":"Ketamine\n\nA powerful anesthetic drug, originally manufactured for veterinary use, that has been shown to be an effective treatment for major depressive disorder, especially in patients who do not respond well to traditional antidepressant medications.\n"},"KB/Kickstart-AI":{"title":"Kickstart AI","links":[],"tags":["jobsearch"],"content":"Kickstart AI\nIt is not every day that you find an AI company that not only wants to make a real-world impact but also cares about responsible AI. I found out about Kickstart.AI and the hackathons from some classmates (who are from my course at the RUG) who interned with you recently. Their project was on “Using Data to Predict Food Insecurity.” (Shray Juneja,Chaoyi Wang, and Lonneke Pulles study AI at the RUG as well.) Looking at the projects that Kickstart is involved in, it is obvious just how much talent and hard work is behind each of them. I really want to take on the challenge and contribute to making some awesome ideas a reality while being mindful of the societal impact and explainability of each of the models used.\nMy expertise is a combination of data analytics and computer vision, and as of a month ago, I also have a Master in Artificial Intelligence from the University of Groningen. I am familiar with the tools required for basic and advanced AI and analytics from internships, research projects, papers, freelance work, and many personal projects. I love hackathons, and truly think that they bring out the best in every member of a team. That being said, what I love the most is building prototypes of ideas. Be it an AI-powered credit scoring model, a model that converts between materials (e.g., glass to wood), or applications powered by LLMs. Any idea is fair game. But I also care about how any of the solutions that I am part of affect the end users. My master thesis was on XAI, and I firmly believe that an AI model needs a detailed understanding of biases before production, another aspect that I am familiar with.\nI have a lot to learn, but as an AI developer, we are always students. It is not every day you find a position that you enjoy and also think you can contribute something to, and I sincerely hope you give me a chance!"},"KB/Kinesthetic-Teaching":{"title":"Kinesthetic Teaching","links":["KB/Heightmaps-Kinesthetic","KB/Familar-Object-Grasping-Object-Viiew-recog"],"tags":["robotics"],"content":"Kinesthetic Teaching\n\nH. Kasaei et al., “Interactive open-ended object, affordance and grasp learning for robotic manipulation.” ICRA 2019.\nFormulate object grasping as a supervised learning problem,\nAn appropriate grasp configuration can be learned from human demonstrations\nPrimary assumption → familiar objects can be grasped in a similar way.\nHeightmaps Kinesthetic\nFamilar Object Grasping Object Viiew recog\n"},"KB/Kinetic-Energy":{"title":"Kinetic Energy","links":[],"tags":["temp"],"content":"Kinetic Energy\n\nhalf x mass x (velocity squared)\nE_{K}= \\frac{1}{2}mv^{2}\n"},"KB/Kinetic-Friction":{"title":"Kinetic Friction","links":["KB/Force"],"tags":["physics"],"content":"Kinetic Friction\n\nF_{k} \\leq \\mu _{k}F_{N}\nF_{k} is kinetic friction\nF_{N} is normal Force\n\\mu _s is coefficient of friction\n"},"KB/Kinetics":{"title":"Kinetics","links":[],"tags":["dataset"],"content":"Kinetics\n\nlarge-scale, highquality dataset for human action recognition in videos\n500, 000 video clips covering 600 human action classes with at least 600 video clips for each action class\nEach video clip lasts around 10 seconds and is labeled with a single action class.\n"},"KB/Kipf-Normalization":{"title":"Kipf Normalization","links":[],"tags":["graph"],"content":"Kipf Normalization\n \n\n\n"},"KB/Klue-ML-Engineer":{"title":"Klue ML Engineer","links":[],"tags":["jobsearch"],"content":"Klue ML Engineer\n\nWe are looking for Machine Learning Engineers to work with our team to deliver high quality products in the most efficient way.\n\nWe build machine learning services and data pipelines to automatically extract insights about competitors from both public and internal data sources. Every day, our services process millions of data points, including news articles, press releases, webpage changes, Slack posts, emails, reviews, CRM opportunities, and user actions. We utilize a broad array of ML techniques, including classification, clustering, recommendation, summarization, prompt engineering, vector search, and retrieval augmented generation.\n💡Klue + You?\nQ: Klue who?\nA: We’re Klue and from a technical perspective, Klue’s mission is to descale huge amounts of data to the human level, so people can process it and make use of it. Klue is that trusted intermediary, right now it’s proven for sales enablement, but tomorrow it’s all teams enablement.\nQ: What level of experience are we looking for?\nA: Right now, we are looking for experienced senior-level Machine Learning Engineers.\nQ: What are we working on?\nA: Services for collecting, processing and generating timely, relevant intel that is accurately linked to competitors, products, industries, and people. Our Machine Learning Engineers are primarily focused on the training, evaluation, and deployment of models to label, score, cluster, and generate insights.\nQ: What technologies do we use?\nA: Python, Transformers, Pytorch, Hugging Face, Spacy, Sklearn, Pinecone, Kubeflow, Vertex AI, BentoML, Aporia, JS, PostgreSQL, Elasticsearch, Redis, GCP, BigQuery, Docker/Kubernetes, Github, GPT.\nWe believe in using whatever tools make sense to get the job done and support our game-changing innovation.\nQ: What skills do you bring?\n\n\nYou are an expert on the landscape of transformer models and are proficient with popular ML frameworks such as PyTorch, Hugging Face, or Scikit-Learn.\n\n\nYou stay up to date on recent advances with LLMs and you demonstrate astute judgment in deciding whether to train a model with a custom architecture or leverage GPT.\n\n\nYou ensure your experiments are reproducible, balancing swift discovery with scientific rigor.\n\n\nYou are proficient in designing, implementing, and deploying RESTful APIs and you have experience with (non)relational and vector databases.\n\n\nYou demonstrate good judgment when making architectural decisions and you understand how those decisions fit into the bigger picture\n\n\nQ: What about total compensation &amp; benefits?\n\nBenefits. We currently have a pension plan for our EU team\nTime off. Take what you need. We want the team to prioritize wellness and avoid burnout. We want to also give individuals autonomy to choose how and when they take vacation. We understand and respect that everyone’s needs for time off are different, just like our team\n"},"KB/Knowledge-Component":{"title":"Knowledge Component","links":[],"tags":["usermodel"],"content":"Knowledge Component\n\nA knowledge component can be a principle, a concept, a rule, a procedure, a fact, an association or any other fragment of task-specific information.\nDespite the connotations of knowledge, a knowledge component can be incorrect, in that instructors would rather that students not apply this knowledge component while achieving a task.\n"},"KB/Knowledge-Distillation-Survey-2021":{"title":"Knowledge Distillation Survey 2021","links":["KB/Low-rank-factorization","KB/Transferred-compact-convolutional-filters","KB/Label-Smoothing","KB/Logits","KB/Response-Based-Knowledge","KB/Feature-Based-Knowledge"],"tags":["knowledgedistillation"],"content":"Knowledge Distillation Survey 2021\n\nmodel compression and acceleration techniques\nLow-rank factorization\nTransferred compact convolutional filters\nTo address this issue, Bucilua et al. (2006) first proposed model compression to transfer the information from a large model or an ensem- ble of models into training a small model without a significant drop in accuracy. The knowledge transfer between a fully-supervised teacher model and a stu- dent model using the unlabeled data is also intro- duced for semi-supervised learning (Urner et al., 2011).\nThe learning of a small model from a large model is later formally popularized as knowledge distilla- tion (Hinton et al., 2015). In knowledge distillation, a small student model is generally supervised by a large teacher model (Bucilua et al., 2006; Ba and Caruana, 2014; Hinton et al., 2015; Urban et al., 2017).\nThe main idea is that the student model mimics the teacher model in order to obtain a competitive or even a superior performance. The key problem is how to transfer the knowledge from a large teacher model to a small student model. Basically, a knowledge distillation system is composed of three key components: knowledge, dis- tillation algorithm, and teacher-student architecture.\nSuccessful distillation relies on data geometry, optimization bias of distillation objective and strong monotonicity of the student classifier\nquantified the extraction of visual concepts from the intermediate layers of a deep neural network, to explain knowledge distillation (Cheng et al., 2020). Ji &amp; Zhu theoretically explained knowledge distillation on a wide neural network from the respective of risk bound, data efficiency and imperfect teacher (Ji and Zhu., 2020).\nKnowledge distillation has also been explored for Label Smoothing, for assessing the accuracy of the teacher and for obtaining a prior for the optimal output layer geometry (Tang et al., 2020).\nFurthermore, the knowledge transfer from one model to another in knowledge distillation can be extended to other tasks, such as adversar- ial attacks (Papernot et al., 2016), data augmenta- tion (Lee et al., 2019a; Gordon and Duh, 2019), data privacy and security (Wang et al., 2019a).\nA vanilla knowledge distillation uses the Logits of a large deep model as the teacher knowledge (Hinton et al., 2015; Kim et al., 2018; Ba and Caruana, 2014; Mirzadeh et al., 2020)\nFurther- more, the parameters of the teacher model (or the connections between layers) also contain another knowl- edge (Liu et al., 2019c)\nResponse Based Knowledge\nFeature Based Knowledge\n"},"KB/Knowledge-Distillation":{"title":"Knowledge Distillation","links":["KB/Distillation-Loss","KB/Knowledge-Distillation-Survey-2021","KB/Distilling-the-Knowledge-in-a-Neural-Network"],"tags":["temp"],"content":"Knowledge Distillation\n\nTeacher model to help train the student model\nTeacher is often pre trained\nStudent tries to imitate teacher\nDistillation Loss\nKnowledge Distillation Survey 2021\nDistilling the Knowledge in a Neural Network\n"},"KB/Kvasir-Dataset":{"title":"Kvasir Dataset","links":["Roam-Highlights","KB/Endoscope"],"tags":["dataset"],"content":"Kvasir Dataset\n\nSimula Datasets - Kvasir #Roam-Highlights\ndataset containing images from inside the gastrointestinal (GI) tract\nThe collection of images are classified into three important anatomical landmarks and three clinically significant findings.\ntwo categories of images related to endoscopic polyp removal\nThe dataset consist of the images with different resolution from 720x576 up to 1920x1072 pixels\nSome of the included classes of images have a green picture in picture illustrating the position and configuration of the Endoscope inside the bowel, by use of an electromagnetic imaging system\n"},"KB/LASER":{"title":"LASER","links":["KB/Embedding","XNLI","KB/MLDoc","KB/BUCC","KB/Contact-Sensor"],"tags":["architecture"],"content":"LASER\n\nMassively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond\njoint multilingual sentence representations\nLASER\nLanguage-Agnostic SEntence Representations\n93 languages, belonging to more than 30 different families and written in 28 different scripts\nuniversal language agnostic sentence embeddings\ntrain a single encoder to handle multiple languages, so that semantically similar sentences in different languages are close in the Embedding space\nsingle BiLSTM encoder with a shared BPE vocabulary for all languages\ncoupled with an auxiliary decoder and trained on publicly available parallel corpora\nlearn a classifier on top of the resulting embeddings using English annotated data only, and transfer it to any of the 93 languages without any modification\nXNLI\nMLDoc\nBUCC\ntest set of aligned sentences in 112 languages\n\nLaser\n\nAcronym for Light Amplification by Stimulated Emission of Radiation. A device that produces a coherent monochromatic beam of light which is extremely narrow and focused but still within the visible light spectrum. This is commonly used as a non-Contact Sensor for robots. Robotic applications include: distance finding, identifying accurate locations, surface mapping, bar code scanning, cutting, welding etc.\n"},"KB/LDA":{"title":"LDA","links":["KB/Eigenvector"],"tags":["temp"],"content":"LDA\nSteps\n\nCompute the dd-dimensional mean vectors for the different classes from the dataset.\nCompute the scatter matrices (in-between-class and within-class scatter matrix).\nCompute the eigenvectors (e_1,e_2,...,e_de_1,e_2,...,e_d) and corresponding eigenvalues (λ_1,λ_2,...,λ_dλ_1,λ_2,...,λ_d) for the scatter matrices.\nSort the eigenvectors by decreasing eigenvalues and choose k eigenvectors with the largest eigenvalues to form a d×k dimensional matrix W (where every column represents an Eigenvector).\nUse this d×k Eigenvector matrix to transform the samples onto the new subspace. This can be summarized by the matrix multiplication: Y=X×WY=X×W (where X is a n×d-dimensional matrix representing the n samples, and y are the transformed n×k-dimensional samples in the new subspace).\n"},"KB/LIME":{"title":"LIME","links":["KB/SP-LIME"],"tags":["uncertainty"],"content":"LIME\n\n@ribeiroWhyShouldTrust2016\nnovel model-agnostic modular and extensible explanation technique that explains the predictions of any classifier in an interpretable and faithful manner\nlearning an interpretable model locally around the prediction\nSP-LIME\nmethod to explain models by selecting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem and providing a global view of the model to users\nflexibility of these methods by explaining different models for text (e.g random forests) and image classification (e.g neural networks)\nusefulness of explanations is shown via novel experiments, both simulated and with human subjects\n\n"},"KB/LLM-Guide":{"title":"LLM Guide","links":["KB/Complete-AI-Pipeline"],"tags":["architecture"],"content":"LLM Guide\n\nRefer to Complete AI Pipeline for extra steps\n\nUseful Types\nTraining\nData Intake\nDeployment\nLLM Metrics\nBias\nGUI Tools\n\nAzure AI Studio (PromptFlow)\n\nPromptFlow\nPromptFlow RAG\n\n\nAzure Open AI\nAzure Cognitive Search\nAzure Machine Learning\n"},"KB/LRP":{"title":"LRP","links":[],"tags":["explainability"],"content":"LRP\n\n\n"},"KB/LaMDA":{"title":"LaMDA","links":[],"tags":["architecture"],"content":"LaMDA\n\nlanguage model for dialog applications\nfamily of transformer-based neural language models specialized for dialog which have up to 137B parameters and are pre-trained on 1.56T words of public dialog data and web text.\nFine-tuning can enable for safety and factual grounding of the model\nOnly 0.001% of training data was used for fine-tuning, which is a great achievement of the model\ndialog modes take advantage of Transformers’ ability to present long-term dependencies in text\ngenerally very well-suited for model scaling\nuse of a single model to perform multiple tasks: it generates several responses, which are filtered for safety, grounded on an external knowledge source and reranked to find the highest-quality response.\n"},"KB/Label-Encoding":{"title":"Label Encoding","links":[],"tags":["temp"],"content":"Label Encoding\n\nalso called Integer Encoding\nEach unique category is assigned an integer value\ne.g. “red” → 0, “blue” → 1, …\neasy reversible\ncan only be used when a ordinal relationship between the labels exist, e.g. winner ranking in string (“first”, “second”, “third”)\n\nif not and still used, can result in poor performance and unexpected results\n\n\nnumeric representations have a natural ordered relationship between each other and the models are able to understand that relationship\n"},"KB/Label-Smoothing":{"title":"Label Smoothing","links":["KB/Dense","KB/Probability","KB/Cross-Entropy","KB/Distributions"],"tags":["regularization"],"content":"Label Smoothing\n\nDense layer is generally the last one and combined with soft max leads to a Probability distribution\nAssume true label to be y, then a truth Probability distribution would be p_i=1 If i=y and 0 otherwise\nDuring training, minimize negative Cross Entropy loss to make these Distributions similar\nWe know, \\mathscr{l}(p,q) = -log p_y = -z_y + log(\\Sigma^{K}_{i=1}exp(z_i))\nWhere the optimal solution is z^{\\ast}_{y}=\\inf\n\nThe output scores are encouraged to be distinctive which leads to overfitting\nLeads to\n\n\nInstead \\cases{1-\\epsilon&amp; if i=1\\\\\\frac{\\epsilon}{(K-1)} &amp; \\text{otherwise}}\nThe optimal Solution is\n\nlog((K-1)(1-\\epsilon)/ \\epsilon)+\\alpha if i=y\n\\alpha otherwise\n\nAny real number\nFinite output from the last layer that generalizes well\n\n\n\n\nIf \\epsilon =0 , log((k-1)\\frac{1-\\epsilon}{\\epsilon}) is \\infty\nAs \\epsilon increases, the gap decreases\nIf \\epsilon=\\frac{K-1}{K}, all optimizal z^{\\ast}_{i} are identical\n"},"KB/Label-bias":{"title":"Label bias","links":[],"tags":["ethics"],"content":"Label Bias\n\nThis comes from the fact that semantic categories are often poorly defined, and different labellers may assign differing labels to the same type of object [11] (e.g. “grass” vs. “lawn”, “painting” vs. “picture”).\n"},"KB/Labeled-Faces-in-the-Wild":{"title":"Labeled Faces in the Wild","links":[],"tags":["dataset"],"content":"Labeled Faces in the Wild\n\nvis-www.cs.umass.edu/lfw/\n"},"KB/Lack-of-information":{"title":"Lack of Information","links":["KB/Curse-Of-Dimensionality"],"tags":["temp"],"content":"Lack of Information\n\nData does not show how to extract optimal info\nCurse Of Dimensionality\n"},"KB/Ladle-Gripper":{"title":"Ladle Gripper","links":["KB/Gripper","KB/End-effector"],"tags":["robotics"],"content":"Ladle Gripper\n\nAn End-effector, which acts as a scoop. It is commonly used to scoop up liquids, transfer it to a mold and pour the liquid into the mold. Common for handling molten metal under hazardous conditions\n"},"KB/Lagrangian-Coherent-Structure":{"title":"Lagrangian Coherent Structure","links":["KB/Lagrangian-Grid"],"tags":["visualization"],"content":"Lagrangian Coherent Structure\n\nLagrangian Grid\n\n"},"KB/Lagrangian-Grid":{"title":"Lagrangian Grid","links":[],"tags":["visualization"],"content":"Lagrangian Grid\n\nFocus on individual particles\nAttached are position, velocity, and other properties\nExplicit position\n\n"},"KB/Language-Identification":{"title":"Language Identification","links":["KB/Unique-Character-Set","KB/Shared-Character-Set"],"tags":["language"],"content":"Language Identification\n\nIdentifying the language of the document\nDocuments could be multilingual at the sentence level or paragraph level too\nUnique Character Set\nShared Character Set\nByte Range Distribution used for Character Set Identification\nsort the bytes in a ﬁle by frequency count and use the sorted list as a signature vector for comparison via an n-gram model\n"},"KB/Language-dependence":{"title":"Language dependence","links":["KB/Language-Identification"],"tags":["language"],"content":"Language Dependence\n\nRange of orthographic conventions used in written languages to denote the boundaries between linguistic units such as syllables, words, or sentences\nLanguage Identification\n"},"KB/Laplace-Distribution":{"title":"Laplace Distribution","links":["KB/Exponential-Distribution","KB/Normal-Distribution"],"tags":["loss"],"content":"Laplace Distribution\n \n\nPDF \nf(x\\mid \\mu ,b)={\\frac {1}{2b}}\\exp \\left(-{\\frac {|x-\\mu |}{b}}\\right)\nwhere \\mu is a location parameter, and b&gt;0 which is sometimes referred to as the “diversity”, is a scale param If μ=0 and b=1 the positive half-line is exactly an Exponential Distribution scaled by 1/2\nKinda like Normal Distribution but normal distribution is expressed in terms of the squared difference from the mean μ, the Laplace density is expressed in terms of the absolute difference from the mean\n"},"KB/Laplacian-Grid-Smoothing":{"title":"Laplacian Grid Smoothing","links":[],"tags":["visualization"],"content":"Laplacian Grid Smoothing\n\nnew position is based on neighbor positions\np_{i}=\\frac{1}{N}\\Sigma_{i…j}p_{j}\n"},"KB/Large-Batch-Training":{"title":"Large Batch Training","links":["KB/Learning-Rate-Scheduling","KB/Batch-Normalization","KB/Layers","KB/No-bias-decay"],"tags":["temp"],"content":"Large Batch Training\n\nGenerally slows down training\nIf convex, convergence rate decreases with increase in batch size\nLearning Rate Scheduling\nModified Batch Normalization with \\gamma=0 for all BNs at the end of a residual block that micmics networks with less Layers and is easier to train at the start\nNo bias decay\n"},"KB/Large-Kernel-in-Attention":{"title":"Large Kernel in Attention","links":[],"tags":["architecture"],"content":"Large Kernel in Attention\n\nself-attention can be viewed as a global depth-wise kernel that enables each layer to have a global receptive field.\nSwin Transformer (Liu et al., 2021e) is a ViTs variant that adopts local attention with a shifted window manner\ngreatly improve the memory and computation efficiency with appealing performance\nSince the size of attention windows is at least 7, it can be seen as an alternative class of large kernel\nrecent work (Guo et al., 2022b) proposes a novel large kernel attention module that\nuses stacked depthwise, small convolution, dilated convolution as well as pointwise convolution to capture both local and global structure\n"},"KB/Large-Kernel-in-Convolution":{"title":"Large Kernel in Convolution","links":[],"tags":["architecture"],"content":"Large Kernel in Convolution\n\nGlobal Convolutional Network (GCNs) (Peng et al., 2017) enlarges the kernel size to 15 by employing a combination of 1×M + M×1 and M×1 + 1×M convolutions.\nHowever, the proposed method leads to performance degradation on ImageNet\nor the utilization of varying convolutional kernel sizes to learn spatial patterns at different scales. With the popularity of VGG (Simonyan &amp; Zisserman, 2014), it has been common over the past decade to use a stack of small kernels (1×1 or 3×3) to obtain a large receptive\nfield\nHowever, the performance improvement plateaus when further expanding the kernel size\nHan et al. (2021b) find that dynamic depth-wise convolution (7x7) performs on par with the local attention mechanism if we substitute the latter with the former in Swin Transformer\nLiu et al. (2022b) imitate the design elements of Swin Transformer (Liu et al., 2021e) and design ConvNeXt employed with 7x7 kernels, surpassing the performance of the former\nLately, Chen et al. (2022) reveal large kernels to be feasible and beneficial for 3D networks too.\nPrior works have explored the idea of paralleling (Peng et al., 2017; Guo et al., 2022a) or stacking (Szegedy et al., 2017) two complementary Mx1 and 1xM kernels\nHowever, they limit the shorter edge to 1 and do not scale the kernel size beyond 51x51\n"},"KB/Latent-Diffusion":{"title":"Latent Diffusion","links":[],"tags":[],"content":""},"KB/Latent-Dirchlet-Allocation":{"title":"Latent Dirchlet Allocation","links":[],"tags":["language"],"content":"Latent Dirchlet Allocation\n\nDiscovers topics into a collection of documents\nTags each document with topics\n\n"},"KB/Latent-Semantic-Analysis":{"title":"Latent Semantic Analysis","links":["KB/Learning-Event"],"tags":["usermodel"],"content":"Latent Semantic Analysis\n\nRoughly speaking, a Learning Event is considered to be included in the student’s step if the degree of semantic similarity is above a certain threshold.\n"},"KB/Latitude-Junior-ML-Engineer":{"title":"Latitude Junior ML Engineer","links":[],"tags":["jobsearch"],"content":"Latitude Junior ML Engineer\nAs the days go by, Machine Learning and AI are slowly becoming terms that every company wants to have in their portfolio. While this drive leads to many innovations, most companies are not sure how to do AI “well”. They want to use AI but need clarification on whether it is required, how to handle bias, how to create proper data, or even what models (ML vs. DL) to choose. I believe that the Latitude data-first approach is the way to go, and in the long run, this approach will help many companies achieve their vision of AI. I love solving problems with creative analytics, and this application is to get a chance to take part in Latitude’s mission to make a difference to customers with data.\nMy expertise is a combination of data analytics and computer vision, and as of a month ago, I also have a Master in Artificial Intelligence from the University of Groningen. I am familiar with the tools required for basic and advanced AI and analytics from internships, research projects, papers, freelance work, and many personal projects. That being the case, I can step aside from my experience with AI and decide if a project needs another solution in reality. I have experience finding proper KPIs and can phrase them in the context of an ML (or not) problem and provide a means to solve them.\nThe customer is king, they say, and the first step in having happy clients is understanding what they truly want and then being able to give them a solution they can use. That being the case, in any team, it is essential that each of the members can contribute something to the overall development of the projects. Although I have a lot to learn, I genuinely enjoy developing solutions that have a positive impact, and I can contribute quite a bit to any team I get the chance to work with."},"KB/Law-of-large-numbers":{"title":"Law of Large Numbers","links":[],"tags":["temp"],"content":"Law of Large Numbers\n\nIf one carries out an infinite seq of independantly repeating same the same numerical mesurement and gets a sequence of measurement values x_{1}, .. , x_{n} where x_{i} \\in \\mathbb{R}, then the mean value of the inital seq upto N will almost always converge to the same number \\mu_{N} = \\frac{1}{N}\\Sigma_{i=1}^{N}x_{i} which is the EXPECTATION of X \\mu_{N}=E[X]\nKolmogorov axioms\n"},"KB/Layer-Normalization":{"title":"Layer Normalization","links":["KB/Features","KB/Batch-Normalization"],"tags":["regularization","normalization"],"content":"Layer Normalization\n\nFor RNNs etc\nMean and variance calculated independantly for each element of the batch by aggregating over the Features dimensions.\n (Compared to Batch Normalization)\n\n \\begin{align*}\\\\\n\n&amp;\\mu_{\\mathcal{B}} \\leftarrow \\frac{1}{m}\\Sigma_{i=1}^{m}x_{i}\\\\\n\n&amp;\\sigma^{2}_{\\mathcal{B}} \\leftarrow \\frac{1}{m}\\Sigma_{i=1}^{m}(x_{i}-\\mu_{\\mathcal{B}})^{2}\\\\\n\n&amp;\\hat x_{i} \\leftarrow \\frac{x_{i}-\\mu_{\\mathcal{B}}}{\\sqrt{\\sigma^{2}_{\\mathcal{B}} + \\epsilon}}\\\\\n\n&amp;y_{i}= \\gamma \\hat x_{i}+ \\beta\n\n\\end{align*}\nProblem\n\nFrom [Visualizing the Loss Landscape of Neural Nets](Visualizing the Loss Landscape of Neural Nets.md),\n![images/Pasted%20image%20 20230327130254.png](images/Pasted%20image%20 20230327130254.png)\n"},"KB/Layers-for-GNNs":{"title":"Layers for GNNs","links":["KB/Kipf-Normalization","KB/Self-Attention"],"tags":["graph"],"content":"Layers for GNNs\n \nLayers for GNNs\n\ncombined messages from adjacent nodes by summing them together with the transformed current node → post-multiplying the node embedding matrix H by the adjacency matrix plus the identity A + I\n\nCombining Current Node and Aggregated Neighbors\n\n\n\nResidual Connections\n\n\n\nMean Aggregation\n\nSometimes it’s better to take the average of the neighbors rather than the sum; this can be superior if the embedding information is more important and the structural information less so since the magnitude of the neighborhood contributions will not depend on the number of neighbors\n\n\n\nKipf Normalization\nMax Pooling Aggregation\n\n\n\nAggregation by Attention\n\n\n\nsimilar to Self Attention\n\n\n"},"KB/Layers":{"title":"Layers","links":[],"tags":["temp"],"content":"Layers\nNotation\n\nGrad of a function f wrt A : \\nabla_Af\nNeuron Pre activation : Z\nActivations : Y\nTensor shape : (w,h,c)\nMatrix multi : A\\cdot B\nHadmard prod (coeff wise) : A \\circ B\n"},"KB/Layerwise-Conservation-Principle":{"title":"Layerwise Conservation Principle","links":[],"tags":["explainability"],"content":"Layerwise Conservation Principle\n\nwhich says that “a network’s output activity is fully redistributed through the layers of a DNN onto the input variables, i.e., neither positive nor negative evidence is lost.”\n"},"KB/Layerwise-Gradient-Magnitude-Based-Pruning":{"title":"Layerwise Gradient Magnitude Based Pruning","links":["KB/Pruning"],"tags":["regularization"],"content":"Layerwise Gradient Magnitude Based Pruning\n\nFinds the lowest absolute value per layer and removes them\n"},"KB/Layerwise-Magnitude-Based-Pruning":{"title":"Layerwise Magnitude Based Pruning","links":["KB/Pruning"],"tags":["regularization"],"content":"Layerwise Magnitude Based Pruning\n\nTakes the lowest values per layer in the network and prunes.\nModifying the global layerwise and applying it per layer instead.\nTo do this, we first make a copy of the weights. Then for every layer in the array, we find the least n values, take the nth value and set all the others to 0.\nAs an edge case, if the number of elements entered is greater than the total length of the layer, then the entire layer is set to 0.\n"},"KB/Layerwise-Relevance-Propagation":{"title":"Layerwise Relevance Propagation","links":[],"tags":["explainability"],"content":"Layerwise Relevance Propagation\n\nIt relies on a conservation principle to propagate the outcome decision back without using gradients. The idea behind it is a decomposition of prediction function as a sum of layerwise relevance values. When LRP is applied to deep ReLU networks, LRP can be understood as a deep Taylor decomposition of the prediction. This principle ensures that the prediction activity is fully redistributed through all the layers onto the input variables\nsuffers from the shattered gradients problem\n"},"KB/Le-Net":{"title":"Le Net","links":[],"tags":["architecture"],"content":"Le Net\n\nSpatial dims reduce with depth, no of neurons increase\n\n"},"KB/LeCake":{"title":"LeCake","links":[],"tags":["deeplearning"],"content":"LeCake\n\nrelative importance of each learning paradigm by Yann LeCun\nAt the end of the day, everything tied to Supervised Learning\n\n"},"KB/LeCun-Init":{"title":"LeCun Init","links":[],"tags":["regularization"],"content":"LeCun Init\n\n\\frac{1}{fan_{in}}\n"},"KB/Lead-Test":{"title":"Lead Test","links":[],"tags":["medical"],"content":"Lead Test\n\nA test to reveal the quantity of lead in the bloodstream\n"},"KB/Leaky-Relu":{"title":"Leaky Relu","links":[],"tags":["regularization"],"content":"Leaky Relu\n\nAndrew L. Maas, Awni Y. Hannun, Andrew Y. Ng (2014). Rectifier Nonlinearities Improve Neural Network Acoustic Models.\nhas a small slope for negative values instead of a flat slope. The slope coefficient is determined before training, i.e. it is not learnt during training. This type of activation function is popular in tasks where we we may suffer from sparse gradients, for example training generative adversarial networks.\nThe reasons can be numerous, but in order to fight the situation when suddenly lot’s of neurons in the network simply do nothing\nmax(0.01x,x)\n\n"},"KB/Leap-Data-Scientist":{"title":"Leap Data Scientist","links":[],"tags":["jobsearch"],"content":"Leap Data Scientist\nAs I write this motivation letter, the temperatures in NL have hit yet another record for the hottest September ever. One of the biggest reasons for this is global warming, and fossil fuels are a massive component. Leap’s mission to enable the shift to cleaner energy greatly resonates with me, so I am applying for this position as a Data Scientist. At the core of making the world a greener and healthier place is data that every company has collected for decades. Using that vast data pool to inform better decisions is quite challenging but equally rewarding.\nMy expertise is a combination of data analytics and computer vision, and as of a month ago, I also have a Master in Artificial Intelligence from the University of Groningen. I am familiar with the tools required for basic and advanced AI (such as PyTorch, Tensorflow, scikit-learn, and many others) and analytics from internships, research projects, papers, freelance work, and many personal projects. In my time at KPMG and Emirates NBD, I have worked on massive data analytics pipelines across every level, from data preprocessing to feature engineering to deploying analytics solutions. Python is my language of choice for analytics as well as Deep Learning and Machine learning and I am familiar with the common libraries in each of these domains.\nIn any team, it is essential that each of the members can contribute something to the overall development of the projects. Although I have a lot to learn, I genuinely enjoy developing solutions that have a positive impact, and this position will be the perfect next step for me. I will be able to contribute to any team I get to work with.\nI hope you give me a chance to contribute to the efforts that we as a race must make for our future generations in the fight against climate change."},"KB/Learning-Component":{"title":"Learning Component","links":[],"tags":["robotics"],"content":"Learning Component\n\nanalyses the trace of activities and extracts and conceptualizes possibly interesting experiences.\n"},"KB/Learning-Event":{"title":"Learning Event","links":["KB/Knowledge-Component"],"tags":["usermodel"],"content":"Learning Event\n\nA learning event is the construction or application of a Knowledge Component, often while trying to achieve the task. Learning events are mental events, whereas steps are physical events. A learning event occurs in the mind of the student where it cannot be observed, whereas a step occurs on the user interface and the computer can observe it. In the algebra example mentioned earlier, the step x=18.46 can be considered to result from three learning events: 2.3*x=42.45 (cid:198) x=42.45/2.3 (cid:198) x=18.4565… (cid:198) x=18.46\n"},"KB/Learning-L2-German-Vocabulary-Through-Reading":{"title":"Learning L2 German Vocabulary Through Reading","links":["KB/Recall"],"tags":["usermodel"],"content":"Learning L2 German Vocabulary Through Reading\n\n\nElke Peters , Jan H. Hulstijn , Lies Sercu , Madeline Lutjeharms\n\n\nThis study investigated three techniques designed to increase the chances that second language (L2) readers look up and learn unfamiliar words during and after reading an L2 text\n\n\nThey could look up the meaning of unfamiliar words in an online dictionary\n\n\nTest announcement and word relevance substantially prompted participants to use the online dictionary more.\n\n\nOnly test announcement and vocabulary task (not word relevance) affected performance in the word recognition test positively\n\n\noth word relevance and postreading vocabulary task substantially affected word retention in the Recall posttests\n\n\nlow incidence of vocabulary acquisition through reading (“input only”) can be substantially boosted by techniques that make students look up the meaning of unknown words, process their form-meaning relationship elaborately, and process them again after reading (“input plus”).\n\n"},"KB/Learning-Rate-Decay-tricks":{"title":"Learning Rate Decay","links":["tags/deeplearning","KB/Cosine-Learning-Rate-Decay"],"tags":["temp","deeplearning"],"content":"Learning Rate Decaydeeplearning\n\nScale of loss landscape changes\nReduce step size near optima\nFactor \\alpha_{i+1} = d\\cdot \\alpha_i\nCosine Learning Rate Decay\n\n…"},"KB/Learning-Rate-Range-Test":{"title":"Learning Rate Range Test","links":[],"tags":["regularization"],"content":"Learning Rate Range Test\n\nSmith, LN (2018) A disciplined approach to neural network hyper-parameters: Part 1—learning rate, batch size, momentum, and weight decay arXiv preprint arXiv:1803.09820\nIt is relatively straight-forward: in a test run, one starts with a very small learning rate, for which one runs the model and computes the loss on the validation data. One does this iteratively, while increasing the learning rate exponentially in parallel. One can then plot their findings into a diagram representing loss at the y axis and the learning rate at the x axis. The x value representing the lowest y value, i.e. the lowest loss, represents the optimal learning rate for the training data.\nThe learning rate at this extrema is the largest value that can be used as the learning rate for the maximum bound with cyclical learning rates but a smaller value will be necessary when choosing a constant learning rate or the network will not begin to converge.\nSmoothed loss changes\n\nfor i in range(moving_average, len(learning_rates)):\n    loss_changes.append((losses[i] - losses[i - moving_average]) / moving_average)\n"},"KB/Learning-Rate-Scheduling":{"title":"Learning Rate Scheduling","links":["KB/Scheduling","KB/Learning-Rate-Decay-tricks","KB/Gradient-Descent","tags/architecture","KB/Linear-Learning-Rate-Scaling","KB/Learning-Rate-Warmup"],"tags":["temp","architecture"],"content":"Learning Rate Scheduling\n\nLearning Rate Decay tricks\nGradient Descent\nIncreasing the batch size, reduces noise in thearchitecture so a larger learning rate is okay\nLinear Learning Rate Scaling\nLearning Rate Warmup\n"},"KB/Learning-Rate-Warmup":{"title":"Learning Rate Warmup","links":[],"tags":["temp"],"content":"Learning Rate Warmup\n\nSmall learning rate at the start and then a larger learning rate when the training is stabilized\nLinearly from 0 to initial rate\nFirst m batches to warm up and if the initial learning rate is \\eta then at batch i, 1 \\leq i \\leq m , learning rate is \\frac{i\\eta}{m}\n"},"KB/Learning-from-RGB-Flow-Correspondence":{"title":"Learning from RGB-Flow Correspondence","links":["KB/FlowNet"],"tags":["semisupervisedlearning"],"content":"Learning from RGB-Flow Correspondence\n\nOptical flow encodes object motions between adjacent frames\nRGB frames contain appearance information\nThe correspondence of the two types of data can be used to learn general features\nThis type of pretext tasks include optical flow estimation [151], [152] and RGB and optical flow correspondence verification [23].\nSayed et al. proposed to learn video features by verifying whether the input RGB frames and the optical flow corresponding to each other\nTwo networks are employed while one is for extracting features from RGB input and another is for extracting features from optical flow input [24]\nnetwork needs to capture mutual information between the two modalities\nmutual information across different modalities usually has higher semantic meaning compared to information which is modality specific\nOptical flow estimation is another type of pretext tasks\nFlowNet\n"},"KB/Learning-from-Video-Colorization":{"title":"Learning from Video Colorization","links":[],"tags":["semisupervisedlearning"],"content":"Learning from Video Colorization\n\nTemporal coherence\nconsecutive frames within a short time have similar coherent appearance\nThe coherence of color can be used to design pretext tasks for self-supervised learning\nOne way to utilize color coherence is to use video colorization as a pretext task for self-supervised video feature learning.\nVideo colorization is a task to colorize gray-scale frames into colorful frames\nVondrick et al. proposed to constrain colorization models to solve video colorization by learning to copy colors from a reference frame\nGiven the reference RGB frame and a gray-scale image, the network needs to learn the internal connection between the reference RGB frame and gray-scale image to colorize it.\ntackle video colorization by employing a fully convolution neural network\nTran et al. proposed an U-shape convolution neural network for video colorization [160]\nThe color coherence in videos is a strong supervision signal\n"},"KB/Learning-from-Video-Prediction":{"title":"Learning from Video Prediction","links":["KB/Un-LSTM","KB/MCnet"],"tags":["semisupervisedlearning"],"content":"Learning from Video Prediction\n\nVideo prediction is a task of predicting future frame sequences based on a limited number of frames of a video\nTo predict future frames, network must learn the change in appearance within a given frame sequence\nUn-LSTM\nMCnet\n[Temporal order verification](Temporal order verification.md)\n[Temporal order recognition](Temporal order recognition.md)\nMisra et al. proposed to use the temporal order verification as the pretext task to learn image features from videos with 2DConvNet [40] which has two main steps: (1) The frames with significant motions are sampled from videos according to the magnitude of optical flow, (2) The sampled frames are shuffled and fed to the network which is trained to verify whether the input data is in correct order.\nsuccessfully verify the order of the input frames, the network is required to capture the subtle difference between the frames such as the movement of the person\nsemantic features can be learned through the process of accomplishing this task\nthe methods usually suffer from a massive dataset preparation step\nThe frame sequences that used to train the network are selected based on the magnitude of the optical flow, and the computation process of optical flow is expensive and slow\n"},"KB/Learning-from-Visual-Audio-Correspondence":{"title":"Learning from Visual-Audio Correspondence","links":[],"tags":["semisupervisedlearning"],"content":"Learning from Visual-Audio Correspondence\n\ncorrespondence between visual and audio streams to design VisualAudio Correspondence learning task [25], [26], [93], [154].\ntwo subnetworks\nvision\naudio subnetwork\ninput of vision subnetwork is a single frame or a stack of image frames and the vision subnetwork learns to capture visual features of the input data\naudio network is a 2DConvNet\ninput is the Fast Fourier Transform (FFT) of the audio from the video\nPositive data are sampled by extracting video frames and audio from the same time of one video, while negative training data are generated by extracting video frames and audio from different videos or from different times of one video\nnetworks are trained to discover the correlation of video data and audio data to accomplish this task.\ninputs of the ConvNets are two kinds of data, the networks are able to learn the two kinds of information jointly by solving the pretext task.\n"},"KB/Learning-to-Detect-Grasp-Affordance":{"title":"Learning to Detect Grasp Affordance","links":[],"tags":["robotics"],"content":"Learning to Detect Grasp Affordance\n\nYikun Li, et al., Learning to Detect Grasp Affordances of 3D Objects using Deep Convolutional Neural Networks, Task-Informed Grasping workshop (TIG-II), RSS2019, Germany 2019.\n\n"},"KB/Learning-with-Context-Similarity":{"title":"Learning with Context Similarity","links":[],"tags":["semisupervisedlearning"],"content":"Learning with Context Similarity\n\nClustering is a method of grouping sets of similar data in the same clusters\npowerful ability of grouping data by using the attributes of the data\nIn the self-supervised scenario, the clustering methods mainly employed as a tool to cluster image data\nA naive method would be to cluster the image data based on the hand-designed feature such as HOG [140], SIFT [141], or Fisher Vector [49]\nAfter the clustering, several clusters are obtained while the image within one cluster has a smaller distance in feature space and images from different clusters have a larger distance in feature space\nThe smaller the distance in feature space, the more similar the image in the appearance in the RGB space\nThen a ConvNet can be trained to classify the data by using the cluster assignment as the pseudo class label\nthe ConvNet needs to learn the invariance within one class and the variance among different classes\nTherefore, the ConvNet is able to learn semantic meaning of images\nFirstly, the image is clustered into different clusters which the images from the same cluster have smaller distance and images from different clusters have larger distance\nThen a ConvNet is trained to recognize the cluster assignment [34], [44] or to recognize whether two imaged are from same cluster [43]\nDeepCluster iteratively clusters images with Kmeans and use the subsequent assignments as supervision to update the weights of the network\n"},"KB/Learning-with-Labels-Generated-by-Game-Engines":{"title":"Learning with Labels Generated by Game Engines","links":[],"tags":["semisupervisedlearning"],"content":"Learning with Labels Generated by Game Engines\n\nGiven models of various objects and layouts of environments, game engines are able to render realistic images and provide accurate pixel-level labels\nSince game engines can generate large-scale datasets with negligible cost, var- ious game engines such as Airsim [142] and Carla [143] have been used to generate large-scale synthetic datasets with high-level semantic labels including depth, # contours, surface normal, segmentation mask, and optical flow for training deep networks.\n\nHowever, due to the domain gap between synthetic and real-world images, the ConvNet purely trained on synthetic images cannot be\ndirectly applied to real-world images\nthe ConvNet trained with the semantic labels of the synthetic dataset can be effectively applied to real-world images.\nRen and Lee proposed an unsupervised feature space domain adaptation method based on adversarial learning [30]\nthe network predicts surface normal, depth, and instance contour for the synthetic images and a discriminator network D is employed to minimize the difference of feature space domains between real-world and synthetic data\nthe network is able to capture visual features for real-world images\nJing et al. proposed to learn features by training a ConvNet to predict relative scene depths while the labels are generated from optical flow [92].\nNo matter what kind of labels used to train ConvNets, the general idea of this type of methods is to distill knowledge from hard-code detector\nThe hard-code detector can be edge detector, salience detector, relative detector, etc\nno human-annotations are involved\none drawback is that the semantic labels generated by hard-code detector usually are very noisy which need to specifically cope with.\n"},"KB/Learning-with-Labels-Generated-by-Hard-code-Programs":{"title":"Learning with Labels Generated by Hard-code Programs","links":[],"tags":["semisupervisedlearning"],"content":"Learning with Labels Generated by Hard-code Programs\n\nApplying hard-code programs is another way to automatically generate semantic labels such as salience, foreground masks, contours, depth for images and videos\nvery large-scale datasets with generated semantic labels can be used for self- supervised feature learning\nVarious hard-code programs have been applied to generate labels for self- supervised learning methods include methods for foreground object segmentation [81], edge detection [47], and relative depth prediction [92]\nPathak et al. proposed to learn features by training a ConvNet to segment foreground objects in each frame of a video while the label is the mask of moving objects in videos [81]\nLi et al. proposed to learn features by training a ConvNet for edge prediction while labels are motion edges obtained from flow fields\nAfter GAN-based methods obtained breakthrough results in image generation, researchers employed GAN to generate videos [85], [86], [144]\nVideoGAN\n\nTo model the motion of objects in videos, a two-stream network is proposed for video generation while one stream is to model the static regions in in videos as background and another stream is to model moving object in videos as foreground\nVideos are generated by the combination of the foreground and background streams\neach random variable in the latent space represents one video clip\nTulyakov et al. argues that this assumption increases difficulties of the generation\n\n\nMocoGAN\n\nuse the combination of two subspace to represent a video by disentangling the # context and motions in videos [86]\ncontext space which each variable from this space represents one identity\nmotion space while the trajectory in this space represents the motion of the identity\nWith the two sub-spaces, the network is able to generate videos with higher inception score.\nThe generator learns to map latent vectors from latent space into videos, while discriminator learns to distinguish the real world videos with generated videos.\nAfter the video generation training on large-scale unlabeled dataset finished, the parameters of discriminator can be transferred to other downstream tasks [85].\n\n\n"},"KB/Learning-with-Spatial-Context-Structure":{"title":"Learning with Spatial Context Structure","links":[],"tags":["semisupervisedlearning"],"content":"Learning with Spatial Context Structure\n\nImages contain rich spatial context information such as the relative positions among different patches from an image which can be used to design the pretext task for selfsupervised learning\nThe pretext task can be to predict the relative positions of two patches from same image [41], or to recognize the order of the shuffled a sequence of patches from same image [20], [88], [89]\nThe context of full images can also be used as a supervision signal to design pretext tasks such as to recognize the rotating angles of the whole images [36]\nConvNets need to learn spatial context information such as the shape of the objects and the relative positions of different parts of an object.\n\nDoersch et al. is one of the pi- (b) oneer work of using spatial context cues for self- supervised visual feature learning [41]\nRandom pairs of image patches are extracted from each image, then a ConvNet is trained to recognize the relative positions of the two image patches\nConvNets need to recognize objects in images and learn the relationships among different parts of objects\nTo avoid the network learns trivial solutions such as simply using edges in patches to accomplish the task, heavy data augmentation is applied during the training phase\nFollowing this idea, more methods are proposed to learn image features by solving more difficult spatial puzzles [20], [27], [87], [88], [89]\nNoroozi et al. attempted to solve an image Jigsaw puzzle with ConvNet [20]\nThe shuffled image patches are fed to the network which trained to recognize the correct spatial locations of the input patches by learning spatial context\nGiven 9 image patches from an image, there are 362, 880 (9!) possible permutations and a network is very unlikely to recognize all of them because of the ambiguity of the task\nTo limit the number of permutations, usually, hamming distance is employed to choose only a subset of permutations among all the permutations that with relative large hamming distance.\nOnly the selected permutations are used to train ConvNet to recognize the permutation of shuffled image patches [20], [35], [88], [89]\n"},"KB/Least-squares-loss":{"title":"Least squares loss","links":["KB/Normal-Distribution"],"tags":["loss"],"content":"Least squares loss\n \n\n\nL[\\phi] = \\Sigma^{t}_{t=1}(y_{i}- f[x_{i}, \\phi])^{2}\nAssumptions : independent, drawn from a Normal Distribution with mean \\mu = f[x_{i}, \\phi]\n"},"KB/Left-psuedo-inverse":{"title":"Left psuedo inverse","links":[],"tags":["temp"],"content":"Left psuedo inverse\n\n(A&#039;A)^{-1}A&#039;\n"},"KB/Lemmatization":{"title":"Lemmatization","links":[],"tags":["language"],"content":"Lemmatization\n\nWord→ lemma\nsaw : {see, saw}\nMorphological analysis : word→ set of {lemma, tag}\n"},"KB/Length-Optimization":{"title":"Length Optimization","links":[],"tags":["visualization","graph"],"content":"Length Optimization\n\n\n"},"KB/Lesion":{"title":"Lesion","links":[],"tags":["brain"],"content":"Lesion\n\nAn injury, area of disease, or surgical incision to body tissue. Much of what we know about the functions of brain structures or pathways comes from lesion mapping studies, where scientists observe the behavior of people with an injury to a distinct area of the brain or analyze the behavior of a laboratory animal resulting from a lesion made in the brain.\n\nLesion\n\nDamage or change to tissue, such as a cut, a wound or a sore\n"},"KB/Lexical-Ambiguity":{"title":"Lexical Ambiguity","links":["KB/Lexical-Disambiguation"],"tags":["language"],"content":"Lexical Ambiguity\n\nLexical Disambiguation\nWill will Will’s will\nBuffalo buffalo Buffalo buffalo\nRose rose roes rows\n"},"KB/Lexical-Disambiguation":{"title":"Lexical Disambiguation","links":["KB/Semantic-Markers"],"tags":["language"],"content":"Lexical Disambiguation\n\nprocess of determining the correct meaning of an individual word\nWord sense disambiguation\nSemantic Markers\n"},"KB/Lexical-Word-Segmentation":{"title":"Lexical Word Segmentation","links":["KB/Word-Segmentation"],"tags":["language"],"content":"Lexical Word Segmentation\n\nrule-based – syntax; semantics; morphological rules\n"},"KB/Lexically-Collective":{"title":"Lexically Collective","links":[],"tags":["language"],"content":"Lexically Collective\n\nEach knight gathered at the castle.\n"},"KB/Lexically-Distributive":{"title":"Lexically Distributive","links":[],"tags":["language"],"content":"Lexically Distributive\n\nEach girl smiled\n"},"KB/Lexicon":{"title":"Lexicon","links":[],"tags":["language"],"content":"Lexicon\n\na module that tells what words there are and what properties they have\n"},"KB/LibriSpeech":{"title":"LibriSpeech","links":[],"tags":["dataset"],"content":"LibriSpeech"},"KB/Limbic-system":{"title":"Limbic system","links":["KB/Hypothalamus","KB/Amygdala"],"tags":["brain"],"content":"Limbic System\n\nis the center of our emotions, learning, and memory.\nIncluded in this system are the cingulate gyri, Hypothalamus, Amygdala (emotional reactions) and hippocampus (memory).\n\nLimbic System\n\nA group of evolutionarily older brain structures that encircle the top of the brain stem. The limbic structures play complex roles in emotions, instincts, and appetitive behaviors.\n"},"KB/Limited-features":{"title":"Limited features","links":[],"tags":["explainability"],"content":"Limited features\n\nSample size disparities\nwhen using sensitive features, disparities between different subgroups can induce bias\n"},"KB/Line-Integral-Convolution":{"title":"Line Integral Convolution","links":["KB/Streamlines","KB/Correlation"],"tags":["visualization"],"content":"Line Integral Convolution\n\nMimic physical experiment: oil drops on surface, apply flow (wind)\nIntensity distribution along Streamlines shows high Correlation\nNo Correlation between neighboring Streamlines\n\n\n"},"KB/Linear-Classifier-Probes":{"title":"Linear Classifier Probes","links":["KB/Features"],"tags":["architecture"],"content":"Linear Classifier Probes\n\nUnderstanding Intermediate Layers Using Linear Classifier Probes\nBlack box\nmonitor the Features at every layer of a model and measure how suitable they are for classification\n“Probes”\ntrained entirely independently of the model\nobserve experimentally that the linear separability of Features increase monotonically along the depth of the model\n"},"KB/Linear-Flows":{"title":"Linear Flows","links":[],"tags":["architecture"],"content":"Linear Flows\n \nf[h] = \\beta+ \\Omega h\n\nthe determinant of the jacobian is the determinant of \\Omega\nOne way to make a linear flow that is general, eﬀicient to invert, and for which the Jacobian can be computed eﬀiciently is to parameterize it directly in terms of the LU decomposition\n\n\\Omega= PL(U + D)\nwhere P is a predetermined permutation matrix, L is a lower triangular matrix, U is an upper triangular matrix with zeros on the diagonal, and D is a diagonal matrix that supplies those missing diagonal elements\n\n\n"},"KB/Linear-Interpolated-Motion":{"title":"Linear Interpolated Motion","links":[],"tags":["robotics"],"content":"Linear Interpolated Motion\n\nIs a method of path interpolation that commands the movement of the robot by moving each joint in a coordinated motion so that all axis arrive to the position at the same time. The path of the Tool Control Point (TCP) is predictable and will be linear.\n"},"KB/Linear-Learning-Rate-Scaling":{"title":"Linear Learning Rate Scaling","links":["KB/Initialization"],"tags":["temp"],"content":"Linear Learning Rate Scaling\n\nIf [He Initialization ] is used, 0.1 is a good learning rate for batch size 256 and for a larger b, 0.1\\times\\frac{\\mathrm{b}}{256} is okay\n"},"KB/Linear-scale":{"title":"Linear Scale Encoding","links":[],"tags":["temp"],"content":"Linear Scale Encoding\n\nEg Likert scale\nA = {certainly not, rather not, dont know}\n\n…"},"KB/LinearRegression":{"title":"Linear Regression","links":["KB/Affine-Function","KB/Ridge-Regression","KB/Window-Based-Regression","KB/Quadratic-Loss"],"tags":["temp"],"content":"Linear Regression\n\nMinimization problem (w,b) = argmin_{w^{\\ast} , b^{\\ast}} \\Sigma^N_{i-1}(w^{\\ast}x_{i} +b ^{\\ast} - y_{i})^2\n\nAffine Function\nw = argmin_{w^\\ast} || (\\Sigma^n_{j = 1} w^\\ast_j \\phi_j) - y || ^2\n\nw* is just (w^\\ast_1 … w^\\ast_n)\nu_j of U forms orthonormal basis of \\mathscr{F}\n\n\n\n\nX : nxN matrix , y : N dim vector\nSolution : [w, b] \\in \\mathbb{R} ^{n+1}\nw&#039; = (XX&#039;)^{-1}X y\n\nIf y is has vector data too (size k)\n\nW&#039; = (XX&#039;)^{-1}XY\nY : N x k matrix\n\n\n\n\n\n\\phi _1 , \\phi_2 … form a subspace \\mathscr{F} with dim = n\n\nlinearly independant vectors. If not, drop as many as possible\n\n\nThe optimal solution y_opt is the projection of y on that subspace and has the smallest distance from y\n\ny_{opt} = w_1 \\phi_1 + w_2 \\phi_2\n\n\n(\\Sigma^n_{j = 1} w^\\ast_j \\phi_j) is a vector on \\mathscr{F}\nRidge Regression\nWindow Based Regression\n\nGeneral Defination\n\n\nTraining data : (x_i, y_i)_{i= 1,..,N} and \\in \\mathbb{R}^k\n\n\nSearch space H\n\nCandidate functions h: \\mathbb{R}^n \\rightarrow \\mathbb{R}^k\n\n\n\nLoss function L : \\mathbb{R}^k \\times \\mathbb{R}^k \\rightarrow \\mathbb{R}^{n \\geq 0}\n\nQuadratic Loss\n\n\n\nSolution : h_{opt} = argmin_{h \\in \\mathcal{H}}\\Sigma_{i=1}^N L(h(x_i), y_i)\n\n\\mathcal{H} is all linear functions from \\mathbb{R}^n to \\mathbb{R}^k\n\n\n\n[Left psuedo inverse](Left psuedo inverse.md)\n\n"},"KB/Linguistic-details":{"title":"Linguistic details","links":["KB/Phonetics","KB/Phonology","KB/Morphology","KB/Syntactic-Analysis","KB/Semantic-Analysis","KB/Pragmatics"],"tags":["language"],"content":"Linguistic Details\n\nPhonetics\nPhonology\nMorphology\nSyntactic Analysis\nSemantic Analysis\nPragmatics\n"},"KB/Lisht":{"title":"Lisht","links":["Roam-Highlights","KB/Tanh","KB/Swish","KB/Relu"],"tags":["architecture"],"content":"Lisht\n\n\nDerivatives\n\n\n\n\nblog #Roam-Highlights\n\nLinearly Scaled Hyperbolic Tangent\nhis activation function simply uses the Tanh function and scales it linearly, as follows\nLiSHT(x) = x \\times tanh(x)\nEssentially, LiSHT looks very much like Swish in terms of the first-order derivative. However, the range is expanded into the negative as well, which means that the vanishing gradient problem is reduced even further - at least in theory.\nIn their work, Roy et al. (2019) report based on empirical testing that indeed, the vanishing gradient problems is reduced compared to Swish and traditional Relu. Additional correlations between network learning and the shape of e.g. the LiSHT loss landscape were identified.\n\n\n"},"KB/Listen-Attend-Spell":{"title":"Listen Attend Spell","links":["KB/Attention","KB/Features","KB/Conditional-Independence","KB/CTC","KB/Softmax","KB/Google-voice-search-task"],"tags":["architecture"],"content":"Listen Attend Spell\n\nListen, Attend and Spell\nLAS\nlearns to transcribe speech utterances to characters\nnlike traditional DNN-HMM models, this model learns all the components of a speech recognizer jointly\nsequence-to-sequence framework\ntrained end-to-end and has two main components: a listener (encoder) and a speller (decoder)\nlistener is a pyramidal RNN encoder that accepts filter bank spectra as inputs, transforms the input sequence into a high level feature representation and reduces the number of timesteps that the decoder has to attend to.\nThe speller is an Attention-based RNN decoder that attends to the high level Features and spells out the transcript one character at a time\nThe proposed system does not use the concepts of phonemes, nor does it rely on pronunciation dictionaries or HMMs\nbypass the Conditional Independence assumptions of CTC, and show how they can learn an implicit language model that can generate multiple spelling variants given the same acoustics\nproducing character sequences without making any independence assumptions between the characters is the key improvement of LAS over previous end-to-end CTC models\nused samples from the Softmax classifier in the decoder as inputs to the next step prediction during training\nshow how a language model trained on additional text can be used to rerank their top hypotheses\nGoogle voice search task\n"},"KB/Load-Cycle-Time":{"title":"Load Cycle Time","links":[],"tags":["robotics"],"content":"Load Cycle Time\n\nA manufacturing or assembly line process term, which describes the complete time to unload the last work piece and load the next one.\n"},"KB/Load-balancing":{"title":"Load balancing","links":[],"tags":["parallelcomputing"],"content":"Load Balancing\n\ndivide the work equally among the available processors\n"},"KB/Local-Descriptor":{"title":"Local Descriptor","links":[],"tags":["robotics"],"content":"Local Descriptor\n\nset of spin-images\nA spin-image feature is computed for every keypoint:\nThe tangent plane is estimated\nA 2D histogram is computed along the a and b dimensions in the neighborhood of the keypoint\nSpin-image represents a small area of an object around a specific keypoints\n\n\n"},"KB/Local-Reference-Frame":{"title":"Local Reference Frame","links":["KB/PCA"],"tags":["robotics"],"content":"Local Reference Frame\n\nThree principal axes of a given object’s point cloud are firstly determined based on eigenvectors analysis (PCA)\n\n"},"KB/Local-LDA-Object-Representation":{"title":"Local-LDA Object Representation","links":["KB/LDA"],"tags":["robotics"],"content":"Local-LDA Object Representation\n\nA variant of Latent Dirichlet Allocation (Local-LDA)\nlearn structural semantic features (i.e. topics) from low-level feature cooccurrences for each category independently and incrementally.\n\n"},"KB/Localist-units":{"title":"Localist units","links":[],"tags":["language"],"content":"Localist Units\n\nIn a localist representation, a localist unit (neuron) is most active to one meaningful category\nIn Input: when we build a network and let each input unit represent a a specific word\nIn Output: when we allow outputs of single unit to be interpreted\nIn the hidden units: is there evidence of localist encoding developing?\n"},"KB/Locality":{"title":"Locality","links":["KB/Localist-units","KB/Features"],"tags":["temp","graph"],"content":"Locality\n\nLocalist units\nIn order to process an image, we start by capturing the local information. One way to do that is the use of a convolutional layer. It can capture the local relationship between the pixels of an image. Then, as we go deeper in the model, the local feature extractors help to extract the global Features\n\n"},"KB/Location-Aware-Attention":{"title":"Location Aware Attention","links":["KB/Attention"],"tags":["architecture"],"content":"Location Aware Attention\n\nChorowski et al., 2015\n"},"KB/Location-Base-Attention":{"title":"Location Base Attention","links":["KB/Attention","KB/Attention-Alignment"],"tags":["architecture"],"content":"Location Base Attention\n\nLuong2015\nAttention Alignment score \\alpha_{t,i} = softmax(W_{\\alpha}s_{t})\n"},"KB/Log-Likelihood-Loss":{"title":"Log Likelihood Loss","links":[],"tags":["loss"],"content":"Log Likelihood Loss\n\nL(U) = \\Sigma_i log P(u_i| u_{i-k} ,…, u_{i-1} )\nk is size of context window of past tokens\n"},"KB/Log-likelihood-criterion":{"title":"Log likelihood criterion","links":[],"tags":["loss"],"content":"Log likelihood criterion\n \n\nSince the product of these terms can be tiny, we can effectively maximize the log of the likelihood\nThis is equivalent to the previous one\noverall maxima of the two criteria must be in the same place, so the best model parameters φˆ are the same in both cases\n\n"},"KB/Log-odds":{"title":"Log-odds","links":[],"tags":["temp"],"content":"Log-odds\n\nThe logarithm of the odds of some event.\nIf the event refers to a binary probability, then odds refers to the ratio of the probability of success (p) to the probability of failure (1-p).\n"},"KB/LogCosh":{"title":"Log Cosh","links":["KB/MSE","KB/Softplus"],"tags":["loss"],"content":"Log Cosh\n\nworks like the MSE, but is smoothed towards large errors (presumably caused by outliers) so that the final error score isn’t impacted thoroughly.\n\nWe first define the Softplus function \\log\\left( e^{x} + 1 \\right)\nThen , x = ŷ - y\nlogcosh = \\mathrm{mean}\\left( x + \\mathrm{softplus}\\left( -2 \\cdot x \\right) - \\log\\left( 2.0 \\right) \\right)\n…"},"KB/Logarithm":{"title":"Logarithm","links":[],"tags":["math"],"content":"Logarithm\n\nLogarithms are sort of a measure of the “bigness” of a number; 1–10 is small (say, 0..1), 10–100 are medium (1..2), 100–1000 are big (2..3). But, it makes a pretty huge difference if we’re thinking about log(x) with x between 1 and 10, or with x above 10, or with x less than 1. An x between zero and 1 turns into a negative number\n"},"KB/Logits":{"title":"Logits","links":[],"tags":["temp"],"content":"Logits\n\nThe vector of raw (non-normalized) predictions that a classification model generates, which is ordinarily then passed to a normalization function.\n"},"KB/Long-Short-Term-Memory-(LSTM)":{"title":"Long Short Term Memory (LSTM)","links":["tags/architecture","KB/Feature-Learning","KB/Transformer"],"tags":["architecture"],"content":"Long Short Term Memory (LSTM)\n\n\nSmaller chance of exploding or vanishingarchitecture\nBetter ability to model long term dependencies\nGated connections\nGates that learn to forget some aspects, and remember others better\nSplitting state into parts → output pred and Feature Learning\nAt the end of the day, these could not handle too long sequences. Therefore → Transformer\n\nThe Math\n\nGates\n\nForget g_f = \\sigma(W_{hf}h_{t-1} + W_{xf}x_t + b_f)\n\nHow much of the previous cell state is used\n\n\nInput g_i = \\sigma(W_{hi}h_{t-1} + W_{xi}x_t + b_i)\n\nHow proposal is added to the state\n\n\nOutput g_o = \\sigma(W_{ho}h_{t-1} + W_{xo}x_t + b_o)\n\nComponent wise products\n\n\n\n\nHidden state\n\nC_t to model cross timestep dependencies\n\nCell state proposal : \\hat C = tanh(W_{hc}h_{t-1} + W_{xc}x_t + b_c)\nFinal cell state : C_t = g_f \\cdot C_{t-1} + g_i\\cdot \\hat C\n\n\nh_t to predict output\n\nh_t = g_o \\cdot \\sigma_y(C_t)\n\n\n\n\n"},"KB/Long-Term-Potentiation-(LTP)":{"title":"Long Term Potentiation (LTP)","links":[],"tags":["brain"],"content":"Long Term Potentiation (LTP)\n\nThe persistent strengthening of a synapse with increased use, thought to underlie learning and memory.\n"},"KB/Longformer":{"title":"Longformer","links":["KB/Transformer","KB/Sliding-Window-Attention","KB/Dilated-Sliding-Window-Attention","KB/Global-and-Sliding-Window-Attention","KB/Attention","KB/RoBERTa"],"tags":["architecture"],"content":"Longformer\n\nLongformer: the Long-Document Transformer\nTransformer\nSliding Window Attention\nDilated Sliding Window Attention\nGlobal and Sliding Window Attention\nAttention mechanism that scales linearly with sequence length\ndrop-in replacement for the standard self-Attention\nlocal windowed Attention with a task motivated global Attention\ntext8\nenwik8\nconsistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA\nLongformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset\n"},"KB/Loop-Tiling":{"title":"Loop Tiling","links":[],"tags":["parallelcomputing"],"content":"Loop Tiling\n\nHardware memory layout in consideration\n"},"KB/Loss-for-binary-classification":{"title":"Loss for binary classification","links":["KB/Bernoulli-Distribution","KB/Sigmoid","KB/Cross-Entropy"],"tags":["loss"],"content":"Loss for binary classification\n \n\nTwo classes\nProbability distribution - Bernoulli Distribution\nTraining → Model to predict \\lambda, but these values might not lie in [0,1] and we need it to. → Sigmoid\nCross Entropy\n\nThis represents the probability that y = 1, and it follows that 1 − λ represents the probability that y = 0. When we perform inference, we may want a point estimate of y, so we set y = 1 if λ &gt; 0.5 and y = 0 otherwise.\n\n"},"KB/Loss-for-multiclass-classification":{"title":"Loss for multiclass classification","links":["KB/Categorical-Distribution","KB/Softmax","KB/Negative-Log-Likelihood","KB/Cross-Entropy"],"tags":["loss"],"content":"Loss for multiclass classification\n \n\nMap x to one of K &gt; 2 classes\nDistribution - Categorical Distribution\nThe parameters are constrained to take values between zero and one, and they must collectively sum to one to ensure a valid probability distribution Pr(y=k) = \\lambda_{k}\nNetwork computes K params from input x\nSince the output of the network does not conform to the format, we use Softmax so results are positive, and the K numbers sum to one\n\nLoss function : Negative Log Likelihood\nThis is then - Cross Entropy\n\n"},"KB/Loss-for-univariate-regression":{"title":"Loss for univariate regression","links":["KB/Normal-Distribution","KB/Least-squares-loss","KB/Homoscedatic","KB/Heteroscedatic"],"tags":["loss"],"content":"Loss for univariate regression\n \n\nPredict a single scalar output y \\in \\mathbb{R}\nUse univariate Normal Distribution\n\n\n\n\n\n\nFind parameters \\hat \\phi that minimize L[\\phi]\\\nLeast squares loss\n\nInference\n\nPredict the mean \\mu = f[x, \\phi] of the Normal Distribution over y\nWe find the single best point estimate \\hat y and we take max of the predicted distribution \\hat y = \\underset{y}{argmax}[Pr(y|f|x, \\hat \\phi, \\sigma^{2})] = f[x, \\hat \\phi]\n\nEstimating if variance constant everywhere\n\nHomoscedatic\nSince the equation does not depend on variance, we pretend \\sigma^{2} is a learned parameter and minimize it wrt \\phi, \\sigma^{2}\n\n\n\nEstimating if variance is not constant\n\nHeteroscedatic\nTrain a network that computes both mean and variance\nVariance should be positive, but the result of composing networks might not be. To make it, pass it through the squaring function\n\n\nHomoscedatic vs Heteroscedatic Regression\n\n\n"},"KB/Lost-in-the-Middle-How-Language-Models-Use-Long-Contexts":{"title":"Lost in the Middle How Language Models Use Long Contexts","links":[],"tags":["architecture"],"content":"Lost in the Middle How Language Models Use Long Contexts\nSummary\nWhen we feed LLMs with a long context, they tend to overlook the documents placed in the middle.\nSo, contrary to what one might think, placing the least similar documents at the bottom isn’t the best strategy.\nSo, we should put the least similar ones in the middle, not at the bottom.\nFindings\n\nperformance can degrade significantly when changing the position of relevant information, indicating that current language models do not robustly make use of information in long input contexts.\nperformance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts, even for explicitly long-context models\n\nEncoder-decoder models are relatively robust to changes in the position of relevant information within their input context, but only when evaluated on sequences within its trainingtime sequence length. When evaluated on sequences longer than those seen during training, we observe a U-shaped performance curve\nQuery-aware contextualization (placing the query before and after the documents or keyvalue pairs) enables near-perfect performance on the synthetic key-value task, but minimally changes trends in multi-document QA\nEven base language models (i.e., without instruction fine-tuning) show a U-shaped performance curve as we vary the position of relevant information in the input context.\nmodel performance saturates long before retriever recall saturates, indicating that current models fail to effectively use additional retrieved documents---using 50 documents instead of 20 retrieved documents only marginally improves performance (∼1.5% for GPT-3.5-Turbo and ∼1% for claude-1.3).\n\nResults\n\nOur experimental setup is similar to the needlein-a-haystack experiments of Ivgi et al. (2023), who compare question answering performance when the relevant paragraph is placed (i) at the beginning of the input or (ii) a random position within the input. They find that encoder-decoder models have significantly higher performance when relevant information is placed at the start of the input context. In contrast, we study finer-grained changes in the position of relevant information.\n\n\n\n\n"},"KB/Low-rank-factorization":{"title":"Low-rank factorization","links":[],"tags":["knowledgedistillation"],"content":"Low-rank Factorization\n\nThese methods identify re- dundant parameters of deep neural networks by em- ploying the matrix and tensor decomposition (Yu et al., 2017; Denton et al., 2014).\n"},"KB/Lp-Regularization":{"title":"Lp Regularization","links":["KB/Frobenius-norm","KB/Trajectory","KB/Cross-Validation"],"tags":["regularization"],"content":"Lp Regularization\n\nTikhonov\nPenalty considering weights\n\nL^\\ast(\\theta) = L(\\theta) + \\lambda \\Sigma_i |\\theta_i|^p\nFrobenius norm\nWeight Decay\np = 2\nEncourages optimization Trajectory perpendicular to isocurves\n\nTune \\lambda\nLasso\n\np = 1\nSparse\nWith linear model : feature selection\n\n\nGrid search : log scale\nToo large : underfit, too small : overfit\nCross Validation required\n"},"KB/Lumbar-Puncture-or-Spinal-Tap":{"title":"Lumbar Puncture or Spinal Tap","links":[],"tags":["medical"],"content":"Lumbar Puncture or Spinal Tap\n\nDrawing of cerebrospinal fluid from the lumbar region of the back using a hollow needle\n"},"KB/MADE---Masked-autoencoder-for-distribution-estimation":{"title":"MADE - Masked autoencoder for distribution estimation","links":[],"tags":["architecture"],"content":"MADE - Masked Autoencoder for Distribution Estimation\n \n\ngermain et al 2016\n"},"KB/MAE":{"title":"MAE","links":[],"tags":["loss"],"content":"MAE\n\nloss converges to median of targets\nissues if gradient is close to 0\nL(y, \\hat y) = n^{-1}\\Sigma_{i}|\\hat y _{i} -y_{i}|\n"},"KB/MAPE":{"title":"MAPE","links":[],"tags":["loss"],"content":"MAPE\n\nmean absolute % error\n\n\\frac{1}{\\mathrm{length}\\left( y \\right)} \\cdot \\mathrm{sum}\\left( \\left\\|\\frac{y - ŷ}{y}\\right\\| \\right)"},"KB/MCMC-Sampling":{"title":"MCMC Sampling","links":["Sampling","KB/PDF","KB/Markov-Chain","KB/Detailed-Balance","KB/Sampler","KB/Factors-for-MC-estimate"],"tags":["temp"],"content":"MCMC Sampling\n\nComplex distribution with only a proto PDF g_{0} that is known\nMarkov Chain\nDetailed Balance\n\nSufficient but not necessary for Markov Chain to be a Sampler for g\n\n\nFactors for MC estimate\n"},"KB/MCnet":{"title":"MCnet","links":[],"tags":["architecture"],"content":"MCnet\n\nEncoder-Decoder Convolutional Neural Network and Convolutional LSTM for video prediction\ntwo encoders, one is Content Encoder to capture the spatial layout of an image, and the other is Motion Encoder to model temporal dynamics within video clips.\nThe spatial features and temporal features are concatenated to feed to the decoder to generate the next frame\nseparately modeling temporal and spatial features, this model can effectively generate future frames recursively.\nVideos consist of various lengths of frames which have rich spatial and temporal information\ninherent temporal information within videos can be used as supervision signal for self-supervised feature learning\npretext tasks have been proposed by utilizing temporal context relations including temporal order verification [29], [40], [90] and temporal order recognition [27], [39]\n"},"KB/MILAN":{"title":"MILAN","links":["KB/Features","KB/MILANNOTATIONS"],"tags":["articles"],"content":"MILAN\n\nNatural Language Descriptions of Deep Visual Features\nSome neurons in deep networks specialize in recognizing highly specific perceptual, structural, or semantic Features of inputs\nidentifying neurons that respond to individual concept categories\nricher characterization of neuron-level computation\nmutual-information-guided linguistic annotation of neurons\ngenerate open-ended, compositional, natural language descriptions of individual neurons in deep networks\ngenerates a description by searching for a natural language string that maximizes pointwise mutual information with the image regions in which the neuron is active\nMILANNOTATIONS\nfine-grained descriptions that capture categorical, relational, and logical structure in learned Features\ncharacterizing the distribution and importance of neurons selective for attribute, category, and relational information in vision models.\nauditing, surfacing neurons sensitive to protected categories like race and gender in models trained on datasets intended to obscure these Features\nediting, improving robustness in an image classifier by deleting neurons sensitive to text Features spuriously correlated with class labels\n"},"KB/MILANNOTATIONS":{"title":"MILANNOTATIONS","links":[],"tags":["dataset"],"content":"MILANNOTATIONS\n\na dataset of fine-grained image annotations\n"},"KB/MIMD":{"title":"MIMD","links":["KB/Grids","KB/SMP","KB/SIMD"],"tags":["parallelcomputing"],"content":"MIMD\n\nMultiple instruction, multiple data\nSynchronous/Async , deterministic/non deterministic\nMost supercomputers\nGrids\nMulti processor SMP computers\nAlso include SIMD sub components\n\n"},"KB/MISD":{"title":"MISD","links":[],"tags":["parallelcomputing"],"content":"MISD\n\nMultiple instructions on single data\nReal time computers need to be fault tolerant where several processors execute the same data for producing the redundant data\nN-version programming\n\n"},"KB/MIT1003":{"title":"MIT1003","links":[],"tags":["dataset"],"content":"MIT1003\n\n“779 landscape images and 228 portrait images.”\nThe fixations were measured while 15 observers looked at an image for 3 s.\n"},"KB/MIT300":{"title":"MIT300","links":[],"tags":["dataset"],"content":"MIT300\n\nwas the first data set with held-out human eye movements and is used as benchmark test data in MIT Saliency Benchmark\n“300 natural”\nThe fixations were measured while 39 observers looked at an image for 3 s.\n“indoor and outdoor scenes.”\n"},"KB/ML-Production-Flow":{"title":"ML Production Flow","links":[],"tags":["architecture"],"content":"ML Production Flow\n\nProject Setup\nData Pipeline\nModelling and training\nServing\n"},"KB/MLCompany":{"title":"MLCompany","links":[],"tags":["jobsearch"],"content":"MLCompany\n\nTalent search : Rachèl Post : rpost@micompany.nl\n\nAbout the Company\n\nAccelerate and scale impact from AI.​\nPeople and machines working together is already rocketing companies to new highs.\nThis takes change and effort. But we’re here to make it effortless for you.\nBuild a globally defined and unified model production line. Enable model governance, in line with regulations.\nWe’ll help build up your team with AI skills in no time : train-the-trainer\nTo achieve impact with Algorithms, we need to start with rethinking how you can change the way you do business. How more and better predictions can help changing the way you serve your customers, and run your operations. ​\nPredictions that can support or replace the decision making of your people. Through integrating these predictions into new digital applications, or into your existing system landscape.​\nIn both scenario’s we go beyond the proof of concept and focus on AI that runs operational. Through connecting data, algorithms and applications we make your organization run smarter.​\n\nInventory Management\n\nAI in Supply Chain: A Novel Risk-based Approach to Inventory Management – MIcompany\n\nInventory management, a critical component of the supply chain, involves ensuring that the right products are in the right place at the right time.\nIn this article, we will present a novel approach to Inventory Management demand prediction which incorporates overstock costs and under-order costs into the decision-making process, as a complement to machine-learning time-series modeling\nWhile cost minimization lies at the core of this approach, supply chain managers may further customize this model to better address corporate goals and targets\nInaccuracies in high-variance fast-moving items are understandable, and near-perfect accuracy is unattainable by even the best of models, so for a company whose inventory centers around fast-moving items, even generic trend predictions have the potential to make a sizable impact, and some AI-oriented startups claim to have made progress on that front.\nthe forecast model with the highest possible accuracy is not necessarily that which is best for the business\nOn the one hand, a business must have a comprehensive understanding of the possible costs it faces, which requires deep business and domain expertise.\nachieving near-optimal accuracy levels is nonetheless crucial, as a poor demand prediction model will impact all items alike\nThe risk-based approach to inventory management utilizes over-ordering costs and under-ordering costs in the inventory level decision-making process.\nFor instance, if an aircraft is sidelined due to engine failure and the airline does not have the exact engine model on hand, the airline will have no choice but to make an urgent shipment, often at a cost which may be 10-15% higher than the typical, non-urgent shipment; this is a prime example of an understock cost. Conversely, if the supplier orders too many engines which end up unused, they may be ultimately sold at a loss or written off entirely, in addition to the cost of capital incurred from ordering unnecessary items; both are considered over-ordering costs.\nBy combining statistical methods and the industry’s domain expertise regarding inventory costs, this system predicts the stock level which minimizes expected inventory costs.\n\n\n\nAbout\n\nAbout MIcompany – MIcompany\nECG Analytics University\nTogether with eBay, we are building the AI skills for a large and diverse group of employees to apply the power of AI and data\nKPN\njointly built AI applications to optimize its network investments in DSL, fiber, and 5G.\nLeasePlan\nbuilt a new AI platform and system that can value and price used cars more reliable than ever.\nchange is not a cool AI-project or algorithm, but rather, it is about driving breakthroughs in directions that will last for many decades and leverage the potential of AI.\nWe believe that to grasp the full potential of AI, a fundamental redesign of key processes is needed. This is a transformation that requires both a new way of working and new skill sets. We adopt a highly curious learning and highly practical doing mindset. Through our academy, we support our clients in building the skills required.\nMIcompany aims to push the good from AI by inspiring our people and clients. AI can make our life easier, more meaningful and healthier. We inspire our people and companies to capture these opportunities by identifying and capturing new application areas for AI.\n\nWork\n\nDiverse work with many different clients\nAll-year coaching and trainings to really make you the best\nNice people and an employee-centric organization\n"},"KB/MLDoc":{"title":"MLDoc","links":[],"tags":["dataset"],"content":"MLDoc"},"KB/MLE":{"title":"MLE","links":[],"tags":[],"content":""},"KB/MLIM":{"title":"MLIM","links":["KB/Modality","KB/Embedding","KB/Logits","KB/Layers","KB/MLM","KB/Modality-Dropout","KB/ITM-Loss"],"tags":["architecture"],"content":"MLIM\n\nMLIM: Vision-and-language Model Pre-training with Masked Language and Image Modeling\nVision-and-Language Pre-training (VLP) improves model performance for downstream tasks that require image and text inputs\nTypically, in addition to the [Masked Language Modeling] (MLM) loss, alignment-based objectives are used for cross-[modality](Masked Language Modeling] (MLM) loss, alignment-based objectives are used for cross-[modality.md) interaction, and RoI feature regression and classification tasks for Masked ImageRegion Modeling (MIRM)\nAlignment-based objectives require pairings of image and text and heuristic objective functions\nMasking policies either do not take advantage of multi-Modality or are strictly coupled with alignments generated by other models\npre-trained using two pre-training tasks as a multi-loss objective given a mini-batch of image-text pairs: [Masked Language Modeling] (MLM) loss (as in BERT) for text, and image reconstruction (RECON) loss for image, coupled with [Modality](Masked Language Modeling] (MLM) loss (as in BERT) for text, and image reconstruction (RECON) loss for image, coupled with [Modality.md) Aware Masking (MAM)\ndetermines the masking probability and applies masking to both word and image Embedding\nbased on BERT predict the masked words from available words and image regions\nfollow BERT for this task: two-layer MLP MLM head outputting Logits over the vocabulary\nMLM loss is negative log-likelihood for masked word\nRECON loss is an an average of pixel-wise sum of squared errors (SSE)\nBoth image and word masking is realized by replacing an Embedding with the Embedding of [MASK]\ntransformer Layers recognize [MASK]\n’s Embedding as a special Embedding that needs to be “filled in”, independent of the Modality, by attending to other vectors in the layer inputs\nunlike other architectures (LXMERT, UNiTER, ViLBERT, VLP, VL-BERT, VisualBERT, etc.), image masking is not based on image regions detected by the object detector, but a shallow CNN as an image embedder which is much more lightweight than deep models like ResNet and is designed to be masking friendly\nMLM + RECON losses apply only to the masked text/image areas and measure reconstructed text and image quality.\nno specific alignment loss\n[Modality] Aware Masking (MAM) to boost cross-[modality](Modality] Aware Masking (MAM) to boost cross-[modality.md) interaction and take advantage of MLM and RECON losses that separately capture text and image reconstruction quality\nSince the the task of finding closely-matching (CM) item pairs requires a pair of image+text inputs, they exploit this multi-Modality by employing Modality Dropout\ntext-only, image-only, and image-text mode\nHowever, RECON instead of ITM Loss offers better PR AUC\nSimilarly, using the ITM Loss together with MLM and RECON does not change the performance\n"},"KB/MLM":{"title":"MLM","links":["Tag-Pages/loss","KB/Softmax"],"tags":["architecture"],"content":"MLM ([Masked Language Modeling](Masked Language Modeling.md))\n\nfrom\n15% of the words in each sequence are replaced by [MASK]\nmodel tries to predict original values of the masked words\nuses the context provided by the other non-masked words in the sequences\nloss function only considers the predictions of the masked words, ignores non-masked ones\n\nleads to slower convergence than with directional models\n\n\nadditions to standard architecture:\n\nclassification layer on top of the encoder output\nmultiplying the encoders output vectors with the embedding matrix → transforms them into the vocabulary dimension\ncalculating probability of each word in the vocabulary using Softmax\n\n\n"},"KB/MLOps-Learning":{"title":"MLOps","links":["journals/2023_10_19"],"tags":["architecture"],"content":"MLOps Learning\nList of Things I Want to Learn\n\nThis is as of 19th oct 2023 : 2023_10_19\nMore git commands that would be useful\nSQL + nosql\nREST API\nKubernets/Airflow\nUnit testing in python\nfullstackdeeplearning.com/course/2022/\n"},"KB/MMLU":{"title":"MMLU","links":[],"tags":["dataset"],"content":"MMLU"},"KB/MNIST":{"title":"MNIST","links":[],"tags":["dataset"],"content":"MNIST\n\n10 classes\n2 channels\ndataset consisting of handwritten digits\n28x28 pixels\n70’000 images\n"},"KB/MRI":{"title":"MRI","links":[],"tags":["brain"],"content":"MRI\n\nStudies brain anatomy\n1 image\n1mm\n\n\nPut subject in big magnetic field (leave him there)\n\n\n\n\nTransmit radio waves into subject [about 3 ms]\n\n\n\n\nTurn off radio wave transmitter\n\n\n\n\nReceive radio waves re-transmitted by subject\n\n\nManipulate re-transmission with magnetic fields during this readout interval [10-100 ms: MRI is not a snapshot]\n\n\n\n\nStore measured radio wave data vs. time\n\n\nNow go back to 2) to get some more data\n\n\nProcess raw data to reconstruct images\nAllow subject to leave scanner\n"},"KB/MSCOCO":{"title":"MSCOCO","links":[],"tags":["dataset"],"content":"MSCOCO"},"KB/MSE":{"title":"MSE","links":[],"tags":["loss"],"content":"MSE\n\nL(x) = \\Sigma_i ||D(E(x_i))||^2\nMSE = \\frac{1}{N} \\Sigma^N_{i=1}(p(x_i) - y_i)^2\nloss converges to mean of targets\n"},"KB/MSLE":{"title":"MSLE","links":["KB/MSE"],"tags":["loss"],"content":"MSLE\n\nMSE log error\nUse MSLE when doing regression, believing that your target, conditioned on the input, is normally distributed, and you don’t want large errors to be significantly more penalized than small ones, in those cases where the range of the target value is large.\n\n\\frac{1}{\\mathrm{length}\\left( y \\right)} \\cdot \\mathrm{sum}\\left( \\left( \\log\\left( y + 1 \\right) - \\log\\left( ŷ + 1 \\right) \\right)^{2} \\right)"},"KB/MUSAN":{"title":"MUSAN","links":[],"tags":["dataset"],"content":"MUSAN\n\nwhich consists of over 900 noises, 42 hours of music from various genres and 60 hours of speech from twelve languages\n"},"KB/MVCNN":{"title":"MVCNN","links":[],"tags":["robotics"],"content":"MVCNN\n\nMulti view CNN for 3D object recognition\nLimitations\nGenerating multi-views is time consuming process\nObjects are partially visible due to (self) occlusion\nNumber of categories should be defined in advance.\n\n"},"KB/MVGrasp":{"title":"MVGrasp","links":["KB/Gripper","KB/Entropy"],"tags":["robotics"],"content":"MVGrasp\n\nH. Kasaei, et al. “MVGrasp: Real-Time Multi-View 3D Object Grasping in Highly Cluttered Environments.” arXiv preprint arXiv:2103.10997 (2021).\nRender multiple views of objects and use a next best view view selection algorithm Generate pixel-wise grasp configuration for the given object view.\nThe Gripper approaches the target object in an arbitrary direction.\nUse a shallow network, and an eye-to-hand camera configuration.\n\nMixed autoencoder (CAE + DAE)\noptimizer: RMSprop, learning_rate = 0.001\nmetrics: Intersection over Union (IoU) and reconstruction error loss: mean [squared error](squared error.md)\n\nWhich view is suitable?\n\nDepends on the pose of the target object and other objects\nMost objects are graspable from either top or side → orthographic setup\nView Entropy is used as the metric for selecting the best view\n\n\n"},"KB/Machine-Learning-Tool-Landscape":{"title":"Machine Learning Tool Landscape","links":["KB/Alex-Net"],"tags":["architecture"],"content":"Machine Learning Tool Landscape\n\nhuyenchip.com/2020/06/22/mlops.html\n\nStages\n\n[ML Production Flow](ML Production Flow.md)\n\nPre Alex Net  (pre-2012)\n\nMostly modeling and training, small frameworks\n\n\n(2012-2015)\n\nLet’s throw data at it\nData Pipeline\n\n\n\n(2016-now)\n\nProduction\nMost companies cant’ afford pure research\nServing tools\n\n\n\nProblems\n\n[Problems facing MLOps](Problems facing MLOps.md)\n\n"},"KB/Macroadaptation":{"title":"Macroadaptation","links":["KB/Knowledge-Component"],"tags":["usermodel"],"content":"Macroadaptation\n\nAmong four common designs for outer loops, the most complex is based on a pedagogy called macroadaptation (Corbett &amp; Anderson, 1995; Shute, 1993)\nFor each task that the tutoring system can assign, it knows which knowledge components are exercised by the task. For each Knowledge Component, the tutor maintains an estimate of the student’s degree of mastery of that Knowledge Component\nWhen a student has completed a task and the tutor needs to select the next one, it chooses one based on the overlap between the tasks’ knowledge components and the student’s mastered knowledge components\nFor example, it might assign a task that requires many knowledge components that are already mastered by the student and just two components that are not yet mastered.\nSome tutoring systems represent not only correct and incorrect knowledge components, but also other stable traits of students. They might represent learning styles and preferences, such as a preference for visual or verbal explanations, so they can choose tasks that are marked as compatible with the student’s style or preference.\nFor the outer loop to function correctly across multiple tasks and sessions, the information about the student must be stored on a server or on the student’s computer’s disk. This persistent information is often called a student model. Exactly what it contains depends on the type of outer loop\n"},"KB/Magic3D":{"title":"Magic3D","links":[],"tags":["architecture"],"content":"Magic3D\n\ntext to 3D model\nWhile the Dreamfusion model achieves remarkable results, the method has two problems\nlong processing time\nlow-quality of the generated images\nthese problems are addressed by Magic3D using a two-stage optimization framework\nMagic3D builds a low-resolution difusion prior and, then, it accelerates with a sparse 3D hash grid structure\na textured 3D mesh model is furthered optimized with an ecient diferentiable render\nhigher quality 3D shapes in both geometry and texture compared to DreamFusion\n"},"KB/Magical-maybe":{"title":"Magical maybe","links":["KB/Dopamine"],"tags":["temp"],"content":"\ntoc: true\ntitle: Magical maybe\ntags: [‘temp’]\n\nMagical Maybe\n\nRobert Sapolsky\nAccording to this idea; the individual may or may not find a notification when looking on the phone. There is a large increase in Dopamine levels when the indication is seen.\n"},"KB/Magnetic-Detectors":{"title":"Magnetic Detectors","links":[],"tags":["robotics"],"content":"Magnetic Detectors\n\nRobot sensors that can sense the presence of ferromagnetic material. Solid-state detectors with appropriate amplification and processing can locate a metal object to a high degree of precision.\n"},"KB/Malignant":{"title":"Malignant","links":[],"tags":["medical"],"content":"Malignant\n\nRefers to the presence of cancerous cells in a tumor or growth\n"},"KB/Mallows-Cp-Statistic":{"title":"Mallows Cp Statistic","links":[],"tags":["loss"],"content":"Mallows Cp Statistic\n\nC_{p}= \\frac{1}{n}(RSS + 2 p \\hat \\sigma^{2})\n"},"KB/Manhattan-Distance":{"title":"Manhattan Distance","links":["KB/Euclidean-Distance"],"tags":["loss"],"content":"Manhattan Distance\n\nTaxicab distance or City Block distance, calculates the distance between real-valued vectors\nD(x,y) = \\Sigma_{i=1}^{k}|x_{i}-y_{i}|\nThere is no diagonal movement involved in calculating the distance.\nManhattan distance seems to work okay for high dim data, it is a measure that is somewhat less intuitive than Euclidean Distance, especially when using in high-dimensional data\nmore likely to give a higher distance value than Euclidean Distance since it does not the shortest path possible.\nWhen your dataset has discrete and/or binary attributes, Manhattan seems to work quite well since it takes into account the paths that realistically could be taken within values of those attributes.\n"},"KB/Manifold-MixUp":{"title":"Manifold MixUp","links":[],"tags":["augmentation"],"content":"Manifold MixUp\n\nmixes feature values generated from intermediate neural network layers.\nfeedforward up to k layer of the network where the output feature maps are mixed\nThe mixed feature maps are given input to the next layer and forward propagated up to the last layer.\nAfter the forward propagation, backward propagation is performed in the standard way with updated labels\n"},"KB/Manifold":{"title":"Manifold","links":["KB/PDF","KB/Embedding","KB/Adversarial-Learning","tags/todo"],"tags":["explainability","todo"],"content":"Manifold\n\nData manifolds are an abstraction\nOnly geometric insights are important\nLocally around some point c where the PDF is large → It will stay large only for a small fraction of directions\n\nThose directions span a low dimensional hyperplane around c\n“low dimensional sheets”\ncurved path\n\n\nIn an n dimensional real vector space \\mathbb{R}^{n} . Embedding space\n\nm \\leq n is a positive integer\nAn m dim manifold \\mathcal{M} is a subset of the vector space where one can smoothly map a neighborhood of that point to a neighborhood of the origin in m dim Euclidean space\n\nLocally represents Euclidean space\n\n\n\n\nOnly surface and not interior\nNo sharp edges or spikes\nCan be exploited by Adversarial Learning\nExamples\n\n1 dim → Lines in some high dim figure : B\n2 dim → Surfaces : A\n\n\n\n\n\n\n\nRefs\n\ntds\nway more stuff : bjlkengtodo\n"},"KB/ManifoldMix":{"title":"ManifoldMix","links":[],"tags":["augmentation"],"content":"ManifoldMix\n\nimprove the hidden representations and decision boundaries of neural networks at multiple layers by mixing hidden representations rather than input samples.\n"},"KB/Manipulator":{"title":"Manipulator","links":["KB/Gripper","KB/End-effector"],"tags":["robotics"],"content":"Manipulator\n\nGripper, hand, arm or other part of the body that can effect and move objects in the robot’s environment\nIs an End-effector\n"},"KB/Mapping-to-Geometry":{"title":"Mapping to Geometry","links":["KB/Height-Plots","KB/Contour"],"tags":["visualization"],"content":"Mapping to Geometry\n\nHeight Plots\nContour\n"},"KB/Marching-Cubes":{"title":"Marching Cubes","links":["KB/Marching-Squares","KB/Finite-Differences","KB/Midpoint-Decider","KB/Asymptotic-Decider"],"tags":["visualization"],"content":"Marching Cubes\n\n3D version of Marching Squares\nCell consists of 8 node values: (i+{0,1}, j+{0,1}, k+{0,1})\n\n\nConsider a cell\n\n\n\n\nClassify each vertex as inside or outside\n\n\n\n\nBuild an index\n\n\n\n\nGet edge list from table[index]\n\n\n\n\nInterpolate the edge location\n\n\nx = i + \\frac{(c-v[i])}{(v[i+1]-v[i])}\n\n\nCompute gradients\n\n\nFinite Differences Central\n\n\nConsider ambiguous cases\n\n\nMidpoint Decider\nAsymptotic Decider\n\n\nGo to next cell\n\n\n\n\nLimitations\n\nProduces many triangles\nCannot represent sharp edges\nProduces “ugly” (thin) triangles\nProduces ringing artifacts!\n"},"KB/Marching-Squares":{"title":"Marching Squares","links":["KB/Interpolation","KB/Asymptotic-Decider","KB/Midpoint-Decider"],"tags":["visualization"],"content":"Marching Squares\n\n\nAlso uses Interpolation\nSymmetries\n\nAsymptotic Decider\nMidpoint Decider\n"},"KB/Marching-Tetrahedra":{"title":"Marching Tetrahedra","links":["KB/Grids"],"tags":["visualization"],"content":"Marching Tetrahedra\n\nUnstructured Grids\nMay split other cell types into tetrahedra, however, at the cost of introduced error\nOne - and three + or Two - and two +\n"},"KB/Margin-Ranking":{"title":"Margin Ranking","links":[],"tags":["loss"],"content":"Margin Ranking\n\nCreates a criterion that measures the loss given inputs x1x1x1 , x2x2x2 , two 1D mini-batch Tensors, and a label 1D mini-batch tensor yyy (containing 1 or -1).\nIf y=1y = 1y=1 then it assumed the first input should be ranked higher (have a larger value) than the second input, and vice-versa for y=−1y = -1y=−1 .\ntake avg\n\n\\frac{1}{\\mathrm{length}\\left( y \\right)} \\cdot \\mathrm{sum}\\left( \\mathrm{max}\\left( 0, \\left( - y \\right) \\cdot x1 - x2 + margin \\right) \\right)"},"KB/Markov-Chain":{"title":"Markov Chain","links":["KB/Markov-Initial-Distribution","KB/Markov-Transition-Kernel","KB/Markov-for-Continuous-Distributions"],"tags":["temp"],"content":"Markov Chain\n\nSequence of random variables such as X_{n+1} only depends on X_{n}\nDiscrete time\nStochastic process without memory\nFinite interval : [0, 1, … n]\nRight infinite : n = 1, 2, 3, …\nLeft right infinite : Integers\n\nNo start\n\n\nMarkov Initial Distribution\nMarkov Transition Kernel\nMarkov for Continuous Distributions\n"},"KB/Markov-Initial-Distribution":{"title":"Markov Initial Distribution","links":[],"tags":["distributions"],"content":"Markov Initial Distribution\n\nP_{X}\nNot needed for left right infinite ones\n"},"KB/Markov-Property":{"title":"Markov Property","links":[],"tags":["temp"],"content":"Markov Property\n\nA property of certain environments, where state transitions are entirely determined by information implicit in the current state and the agent’s action.\n"},"KB/Markov-Random-Field":{"title":"Markov Random Field","links":["KB/Markov-Chain"],"tags":["temp"],"content":"Markov Random Field\n\nGeneralized Markov Chain\n"},"KB/Markov-Transition-Kernel":{"title":"Markov Transition Kernel","links":[],"tags":["temp"],"content":"Markov Transition Kernel\n\nT_{n}(x|y) = P_{n}(X_{n+1}= x | X_{n}= y) for all x,y \\in S\nHomogenous if T_{n}(x|y) = T_{n&#039;}(x|y) for all n,n’\nFirst get a value from a random drow from P_{X_{1}}\nThen get the next from the distribution which is specified by the transition kernel\n\n"},"KB/Markov-for-Continuous-Distributions":{"title":"Markov for Continuous Distributions","links":["KB/PDF","KB/Markov-Chain","KB/Invariant-Distribution"],"tags":["temp"],"content":"Markov for Continuous Distributions\n\nFamily of PDF\nIf a Markov Chain with state set S, matrix M is executed m times . The transition probabilities transmit between states and then, P(X_{n+m}=s_{j}|X_{n}= s_{i}) = M^{m}(i, j)\nwhere M^{m}= M \\cdot M \\cdot M … \\cdot M (m times)\nTo get the PDF g^{n+1}(x) = \\int_{\\mathbb{R}^{k}}T(x|y)g^{n}(y)dy\nInvariant Distribution\n"},"KB/Masked-Autoencoders":{"title":"Masked Autoencoders","links":["KB/Self-Supervised","KB/ImageNet","KB/Transfer-Learning","KB/Auto-Encoders","KB/Vision-Transformer"],"tags":["architecture"],"content":"Masked Autoencoders\n\nMasked Autoencoders are Scalable Vision Learners\nsimple Self Supervised\nImageNet and in Transfer Learning that an Auto Encoders —- a simple self-supervised method similar to techniques in NLP – provides scalable benefits\nmask random patches of the input image and reconstruct the missing pixels\nasymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens\nimages and languages are signals of a different nature\nImages are merely recorded light without a semantic decomposition into the visual analogue of words\nThe word (or subword) analog for images are pixels\nBut decomposing the image into patches (like Vision Transformer reduces the quadratic computation cost of transformers compared to operating at the pixel level\nremove random patches that most likely do not form a semantic segment\nLikewise, MAE reconstructs pixels, which are not semantic entities\nhey find that masking a high proportion of the input image, e.g., 75%, yields a nontrivial and meaningful self-supervisory task\ntrain and throw away the decoder and fine-tune the encoder for downstream tasks\nVanilla ViT-Huge model (ViTMAE) achieves the best accuracy\nImageNet\nTransfer performance in downstream tasks outperforms supervised pre-training and shows promising scaling behavior\nsemantics\nOccurs by way of a rich hidden representation inside the MAE\n\n"},"KB/Masked-Language-Modeling":{"title":"Masked Language Modeling","links":[],"tags":["nlp"],"content":"Masked Language Modeling\n\nIn Masked Language Modeling, an input sequence of tokens is provided, but with some of these tokens masked. The goal of the model is then to learn to predict the correct tokens that are hidden by the mask. If it can do so, it can learn token-level information given the context of the token.\nIn BERT, this is done as follows. 15% of all word embedded tokens is masked at random. From this 15%, 80% of the tokens is represented with a token called , 10% is replaced with a random token and 10% is left alone. This ensures that masking is both relatively random and that the model does not zoom in to the token, which is available during pretraining but not during fine-tuning.\nThis model is also capable of predicting words using the two masked sentences. It concatenates two masked words and tries to predict.\nthese models where we are required to predict the context of words. Since the words can have different meanings in different places the model needs to learn deep and multiple representations of words.\nThese models have shown improved performance levels in the downstream tasks such as syntactic tasks that require lower layer representation of certain models in place of a higher layer representation.\nWe may also find their use in learning the deep bidirectional representations of words. The model should be able to learn the context of words from the start of the sentence as well as from the behind.\n\nTokenizing Data BERT\n\nIn the tokenizer method, text_lst is the text corpus, max_length suggests the maximum number of allowable input tokens (the maximum is 512 for BERT base), and truncation set to True indicates that if the input size is more than the max_length, then the token from index number equal to max_length would be truncated i.e., for our example input tokens from index 100 would be dropped, padding set to True indicates the input length shorter than the max_length are padded, with padding token 0 and lastly, return_tensors indicates in what format do we want the output tensor and tf suggests that we expect tensorflow tensor. The tokenizer here returns three fields, as we have mentioned earlier.\nNow if we look at the “inputs” with the code print(inputs), we can see that the input_ids tensor is of shape 1567×100, and each row starts with the token 101, which is the id for the Special token [CLS] and ends with 0 which is the padding token indicating that the sentence length is less than 100. Also, there is a Special token 102, the [SEP] token, which is not visible, indicating the end of a sentence. Secondly, the token_type_ids are all 0 as there is only a single sentence as input. Finally, the attention_mask has ones at locations for the actual input tokens and zeros for the padding tokens.\n\nMasking Input Tokens BERT\n\nIn the original research paper, 15% of the input tokens were masked, of which 80% were replaced with [MASK] tokens, 10% were replaced with random tokens, and another 10% were left as is. However, in our fine-tuning task, we are replacing 15% of the input tokens except for the special ones with only [MASK] i.e., we will not replace token numbers 101,102, and 0 with mask token 103. In the following lines of codes, the same logic is implemented\n"},"KB/Masked-autoregressive-flow-for-density-estimation":{"title":"Masked autoregressive flow for density estimation","links":[],"tags":["architecture"],"content":"Masked Autoregressive Flow for Density Estimation\n \n\npapamakarios et al 2017\n"},"KB/Mastectomy":{"title":"Mastectomy","links":[],"tags":["medical"],"content":"Mastectomy\n\nSurgical procedure to remove part or all of the breast\n"},"KB/Mastery-learning":{"title":"Mastery learning","links":[],"tags":["usermodel"],"content":"Mastery Learning\n\nThe outer loop implements a pedagogy called mastery learning (Bloom, 1984).\nThe curriculum is structured as a sequence of units or a sequence of difficulty levels. When a student is working on a unit (or level of difficulty), the tutoring system keeps assigning tasks from that unit until the student has mastered the unit’s knowledge. Only then does it allow the student to proceed to the next unit\nThus, some students finish the curriculum having done fewer tasks than other students.\nThis design for the outer loop is mostly used in self-paced courses.\nIt is seldom used for a class-paced course, where it is important that all students stay together as they move through the curriculum.\nA common mistake is to develop a tutoring system with a fancy outer loop, then discover that instructors cannot use its features due to the class-paced nature of the course.\n"},"KB/Material-Processing-Robot":{"title":"Material Processing Robot","links":[],"tags":["robotics"],"content":"Material Processing Robot\n\nA robot designed and programmed so that it can machine, cut, form or change the shape, function or properties of materials it handles between the time the materials are first grasped and the time they are released in a manufacturing process.\n"},"KB/Matrix-notation-for-NNs":{"title":"Matrix notation for NNs","links":["Tag-Pages/loss"],"tags":["deeplearning"],"content":"Matrix notation for NNs\n \nExample for simple case\n\n\nA NN consists of linear transformations alternating with loss functions.\n\nor , \\begin{align*}\n\\mathbf{h = a[\\theta_{0}+ \\theta_{x}]} \\\\\n\\mathbf{h&#039; = a[\\psi_{0} + \\Psi h]}\\\\\n\\mathbf{y&#039; = \\phi&#039;_{0}+ \\phi&#039;h&#039;}\n\\end{align*}\n\nwhere  \\mathbf{a[\\bullet]}\n\n\n\nGeneral case\n\nVector of hidden units at layer k is h_k\nVector of biases that contribute to hidden layer k+1 is \\beta_{k}\nWeights that are applied to k^{th} layer and contribute to hidden layer (k+1)^{th} is \\Omega_{k}\nGeneral network y = f[x, \\phi]  with K layers is now\n\n"},"KB/Max-Margin-Loss":{"title":"Max Margin Loss","links":["KB/Hinge-Loss"],"tags":["loss"],"content":"Max Margin Loss\n\nMakes sure only dissimilar pairs with minimum distance m contribute to the loss\nSpring mass system\nHinge Loss probably ??\n"},"KB/Maximum-Distance-Baseline":{"title":"Maximum Distance Baseline","links":[],"tags":["explainability"],"content":"Maximum Distance Baseline\n\nThis baseline is called the Maximum Distance baseline and creates a baseline by constructing an image with the largest value of the L1 distance from the original image\nThe problem with the maximum distance is that it doesn’t represent the “absence of feature”. It contains the information about the original image, just in a different form.\n"},"KB/Maximum-Likelihood":{"title":"Maximum LIkelihood","links":["KB/Maximum-likelihood-criterion","KB/Log-likelihood-criterion","KB/Log-Likelihood-Loss","KB/Negative-Log-Likelihood"],"tags":["loss"],"content":"Maximum Likelihood\n \nEffect\n\nConsider a model f[x, \\phi] that computes an output from input x.\nConsider the model computes a conditional probability distribution Pr(Y|x) , Y is output\nThis encourages each output y_i to have high probability under Pr(y_{i}|x_{i}) computed from input x_{i}\n\n\nComputing a distribution over inputs\n\nChoose a parametric distribution Pr(y|\\theta) defined on output domain y.\nuse the network to compute one or more of the parameters \\theta of this distribution\nFor example, suppose the prediction domain is the set of real numbers, so y ∈ R. Here, we might choose the univariate normal distribution, which is defined on R. This distribution is defined by the mean μ and variance σ2, so θ = {μ,σ^2}. The machine learning model might predict the mean μ, and the variance σ^2 could be treated as an unknown constant.\n\nMaximum likelihood criterion\nLog likelihood criterion\nMinimizing Log Likelihood Loss\n\nNegative Log Likelihood\n\nInference\n\nWhen we perform inference, we often want a point estimate rather than a distribution, so we return the maximum of the distribution\n\n"},"KB/Maximum-Matching-Algorithm":{"title":"Maximum Matching Algorithm","links":[],"tags":["language"],"content":"Maximum Matching Algorithm\n\nGreedy\nStarts with first character\nSearches for the longest word in list starting with this character. If match is found, boundary is marked\n"},"KB/Maximum-likelihood-criterion":{"title":"Maximum likelihood criterion","links":[],"tags":["loss"],"content":"Maximum likelihood criterion\n \nSince each observed output should have high probability under its corresponding distribution Pr(y_{i}|\\theta_{i}), we can choose the model params \\phi so they MAXIMIZE the combined probability across all I training examples\n\n\nAssumptions\n\ndata are identically distributed\n\nindependent and identically distributed\n\n\n"},"KB/Maxout":{"title":"Maxout","links":[],"tags":["temp"],"content":"Maxout\n\nf(x) = max(x, x\\cdot a)\n"},"KB/Mean-Diffusivity":{"title":"Mean Diffusivity","links":[],"tags":["visualization"],"content":"Mean Diffusivity\n\n\\mu = \\frac{\\lambda _{1}+ \\lambda_{2}+ \\lambda_{3}}{3}\n\n"},"KB/Mean-Observed-Dissimilarity":{"title":"Mean Observed Dissimilarity","links":[],"tags":["explainability"],"content":"Mean Observed Dissimilarity\n\nis the mean of the NISSIM dissimilarity over the adversarial test set for similar levels of attack.\nSo for every adversarial set X ∗, calculate NISSIM value for all samples in that set, and divide by the total number of samples.\n(0,1], such that 0 indicates total similarity while 1 indicates total dissimilarity\nMOD_{advset}= \\frac{1}{N}\\Sigma (NISSIM_{i})\n"},"KB/Media-Distillery":{"title":"Media Distillery","links":[],"tags":["jobsearch"],"content":"Media Distillery\nContact\n\njobs@mediadistillery.com\nJacqueline@mediadistillery.com\n\nAbout\n\nKeep your viewers engaged to your platform along with every step of their journey\nCreate new value with automated content chaptering, appealing images and topic labelling\nEnable better and more valuable ad placements in line with the content displayed\n\nEPG Correction\n\nmanually detecting the start and end times of your TV programs, ad breaks and shorter video segments!\nredefine the Electronic Program Guide (EPG) correction, content navigation and TV advertising using state-of-the-art AI and Machine Learning technologies.\nBy identifying the exact start and end times of ad breaks in video content, Ad Break Distillery enables ad skipping, insertion or replacement in the replay and catchup environment.\nWith Chapter Markers viewers can easily navigate through video content to the parts they are interested in, which leads to a higher user engagement and satisfaction.\nEPG Correction™ ensures seamless and VoD-like experiences for catch-up and replay by fully automated and real-time correction of the difference between the scheduled and the actual air times of TV programs.\nMedia Distillery’s award-winning Deep Content UnderstandingTM technology can analyze video content to determine relevant topics, events or specific time markers, all in real-time and cloud-based. NOS implemented Media Distillery’s product EPG Correction Distillery™ on their 70 most popular TV channels, to provide automatic adjustments to update the actual start time of television programmes as they are broadcast, so consumers can enjoy their favorite video content instantly.\nNOS (NOS is the biggest communications and entertainment group in Portugal.) is also using the automatically generated correct time markers to insert pre-roll advertisements in their replay platform. This results in a natural transition from the content to the ad-block for the consumer, while the inserted ads create a solid revenue stream for NOS.\n\nImage Distillery\n\nimage chosen to be displayed in the UI is crisp and blur-free, that the actors’/presenters’ eyes are open, and the characters’ positioning in-frame is aesthetically pleasing.\nThe Images are created directly out of a broadcast signal or video asset, in a real-time and fully automated fashion. This means that even for live-programming appealing images can be provided and used to boost catch-up and replay viewing.\nImages can be generated from the entire program, or from a specified time range, to prevent inadvertently showing spoilers in the UI.\n\nEmail"},"KB/MediaMonks":{"title":"MediaMonks","links":[],"tags":["jobsearch"],"content":"Media Monks Cover Letter\nAs Rogier Bikker’s article on your website says, AI is truly taking the world by storm, but perhaps infinite content generation is not the key to customer engagement. At the end of the pipeline, what matters is the humans who use and are affected by technology. There are so many terms that keep showing up every day - ML, Computer Vision, Mixed reality. To most people, they do not mean much, but they have such tremendous potential if used wisely and with an understanding of the technologies. My interest is to help customers bring their visions to reality while also guiding them towards using AI better.\nMy expertise is a combination of data analytics and computer vision, and as of a month ago, I also have a Master in Artificial Intelligence from the University of Groningen. I am familiar with the tools required for basic and advanced AI and analytics from internships, research projects, papers, freelance work, and many personal projects. But aside from that, I am an artist (Instagram : www.instagram.com/aiexistentialart). That being the case, I am quite familiar with Photoshop, Blender, and some other design and illustration tools. I’ve played with VR, can cook up demos on an Arduino, and am familiar with the technology behind StableDiffusion, RunwayML, MidJourney etc. Although I am not yet familiar with ZBrush, I do use Nomad Sculpt, and can learn pretty much any technology given time and some mentorship.\nAs for talking about technical stuff in simpler terms, I have taught plenty of people about AI in workshops, webinars, and through many events. I write articles regularly and have been freelancing as a content creator for many years now.\nI have always loved creating concepts, both in my art (as a concept artist) and in my AI work. While it is impossible to be familiar with all the tools, they all share a common thread. That being the case, I will be able to contribute to any team I get to work with. It is not every day you find a position that you enjoy and also think you can contribute something to, and I sincerely hope you give me a chance to be a Monk with you :)\nInterview Prep\n\nHiring manager : Jakub Pawełczak\nTeam members : Fredrick Charles Papworth, Suzanne Elmazi, and Ben Major\nThey bridge between ideas and technology. They curate creative ideas and translate them through innovative solutions that are technology-based.\n"},"KB/Median-Filter":{"title":"Median Filter","links":[],"tags":["visualization"],"content":"Median Filter\n\nValues are replaced by the median in a local surrounding\nnon linear\npreserves edges\n"},"KB/Mediatic-Behavior":{"title":"Mediatic Behavior","links":[],"tags":["temp"],"content":"\ntoc: true\ntitle: Mediatic Behavior\ntags: [‘temp’]\n\nMediatic Behavior\n\nMany individuals tend to resemble a role model.\nFor example, in order to resemble the character in a television series, he or she unintentionally wears clothes similar to those she wore and uses his or her lines in daily life.\nThis is basic type of media behavior\n"},"KB/Memory-Coupling":{"title":"Memory Coupling","links":["KB/TIghtly-coupled"],"tags":["parallelcomputing"],"content":"Memory Coupling\n\n\nTIghtly coupled\n"},"KB/Memory-to-Memory-Architecture":{"title":"Memory to Memory Architecture","links":[],"tags":["parallelcomputing"],"content":"Memory to Memory Architecture\n\n\nFor all vector operation, operands are fetched directly from main memory, then routed to the functional unit\nResults are written back to main memory\nLarge startup time\n"},"KB/Memory-based-learning":{"title":"Memory-based learning","links":["KB/Features"],"tags":["language"],"content":"Memory-based Learning\n\nLazy learning\nAll encountered examples are stored in memory in a multi-dimensional array, positioned according to relevant Features\nNew items are classified (comprehension) or generated (production) by searching for an example in memory that is closest to the target\nBecause examplars are represented by their Features even novel forms can be classified\nA generalization of the knn (k-nearest neighbors) algorithm\nDon’t remove any infrequent or even solo forms. You might need the info\nDon’t trim down the number of examples of a frequent form you have in the model. This effects it.\nLearning is storing, classification is analogy\nmultiple long-distance dependencies\n"},"KB/Mental-Fatigue":{"title":"Mental Fatigue","links":[],"tags":["cognitivemodel"],"content":"Mental Fatigue\n\nResource depletion occurs\nDrop in motivation does not really happen\n"},"KB/Mental-Model-Matching":{"title":"Mental Model Matching","links":[],"tags":["explainability"],"content":"Mental Model Matching\n\nA user’s mental model [42] of a technology is their internal understanding of how a technology works.\nPeople rely heavily on their mental models of technology to make decisions\nIt has been found that XAI stakeholders use their mental models of XAI to decide when to use the technology [10], to evaluate how much to trust the outputted explanations [10, 20, 22], and to make sense of any results [22, 30]\nWhile ML practitioners may have had access to specialized training on how LLMs work, this is decidedly not the case for the vast majority of the general population\nHow a general user believes an LLM to work may be very different from how it actually works, and this mismatch can be dangerous\nIt is not difficult to imagine frightening scenarios where users anthropomorphize or deify an LLM chatbot, understanding it to be a “magical” source of ground truth. This could very quickly lead to conspiracy theories and the legitimization of disinformation campaigns [see, e.g., 23]\n"},"KB/Mesh-Smoothing":{"title":"Mesh Smoothing","links":[],"tags":["visualization"],"content":"Mesh Smoothing\n\nNoisy volume data leads to a noisy surface grid:\nSmooth the volume data first, or\nSmooth the grid in post-processing\nEventually simplifies the grid\n"},"KB/Mesh-refinement":{"title":"Mesh refinement","links":[],"tags":["visualization"],"content":"Mesh Refinement"},"KB/Mesolimbic-Pathway":{"title":"Mesolimbic Pathway","links":[],"tags":["brain"],"content":"Mesolimbic Pathway\n\nA specialized brain circuit implicated in the processing of risk and reward information.\n"},"KB/Meta-AI-Speech-from-Brain":{"title":"Meta AI Speech from Brain","links":["tags/topic2"],"tags":["topic2"],"content":"My Favourite Obsidian Plugins for Research Notes + 2 Bonus Tips\nObsidian is my favourite program for taking notes. Be it for research, general things I learn, summaries from papers, lecture notes and the like. Out of the box, it does so many things really well.\nBut, its real power lies in the vast number of plugins it has. Most of these are user created, and you can even make your own (or hack one together)! In this sea of functionality, these are the top few that I use. Grouped by the type of task for easier lookup.\n(Disclaimer : I am not sponsored by either Obsidian or any of the authors of the plugins mentioned here. These are personal preferences.)\nThe use case\nI am a student, researcher and programmer. I take lecture notes, read a lot of research papers, articles and books. These come down to a lot of information. Of course, there’s no way I can remember all of these bits of fragmented information.\nTherefore, drumroll…, I use Obsidian to help me put these bits of information in a place I can easily access. Since I use this almost everyday, I want taking notes to be as painless and efficient as possible.\nThese plugins are a huge help in doing exactly that. (Ordered by the type of task)\nHow to install these plugins?\nThis is a simple step. Simple open Obsidian Settings, Scroll down a bit and select “Community plugins”. Disable Restrictive mode, and then browse to your hearts content!\nWriting\nWriting notes is the major objective here. So how do we make it extra painless? Plugins of course!\n\nDynamic Table of Contents : Many times, I take notes for a long form text. Sometimes these notes end up pretty huge, and it becomes slightly harder to find something. What about adding a Table Of Contents to the start? Sounds great, but what if we update the note? In comes this plugin, with an automatically updating TOC.\nTags : Super simple, also built in. Adding “#topic1,topic2 etc” to a file to make it easier to search and organise.\nFrontmatter Tag Suggest : Tags are great, but who remembers which ones they used before? Nobody. This plugin autocompletes tags based on ones you have used in previous notes. You can create new ones the normal way of course.\nNote Refactor : Made a huge note with a lot of headings? Why not split them into individual topics and maintain links to them? This makes it easier for you to have one major idea per note. Here I have a bunch of test headings, you can see how after applying them they become new notes that link to the current file.\nPaste URL into selection : The name says it all doesn’t it?\nTemplater : Another plugin I use daily. I like starting my notes with “date_created”, “date_modified”, “tags”, “toc: true\ntitle” and insert the file name as the header. Since I do this for every single note, why not automate it? This plugin lets you create blocks of dynamic text to be inserted with a keyboard shortcut. I use “Cmd+Shift+I” (“Control+Shift+I” for Windows)\nTypewriter Scroll : Zen Mode is a way of life. This lets me focus on what I am writing by automatically scrolling the page, and dimming the rest of the text apart from the line I am currently writing. I do disable it while reading though.\nCommand Palette : This one is pretty obvious, but this built in plugin is just a text search. You can quickly open files with a (Cmd/Control + O) that brings up a searchable menu, or use (Cmd/Control + P) to bring up a searchable list of quick actions.\nVim Mode : This little option is not for everyone honestly. If you have never heard of Vim, just skip this point. I use vim as my default text editor for everything else. And I can’t live without its keybindings. This just lets me use the vim keys for everything.\n\nResearch\nFor research (AI research in my case), we have three main objectives :\n\nMerge important information from a large number of sources.\nFind links between ideas that you did not see.\nMaintain a daily log as something of a lab notebook.\nThere are 5 plugins that fulfil these criteria pretty decently.\nDaily Notes : This is a Core plugin and comes with Obsidian. Essentially it’s a journal. You can add whatever you want to it and it is created every day. I use it to keep a time stamped log of what I did that day. It is also useful if you just want to dump a bunch of information but don’t want to format and organise it just yet.\nTimeStamper : In my daily notes, I like having timestamps (eg - 9:30 : I did xyz). This plugin lets me set a custom format and a keyboard shortcut. I have set it to “Cmd+T” (for Mac or Control+T for Windows)\nBacklinks : A real game changer and another built in plugin. This shows you every file that either is linked in the current file, or refers to the current one. Identifying links between concepts, and finding more of them is absolutely invaluable in research.\nQuick LaTEX for Obsidian : LaTEX is probably the easiest way of writing professional looking math-y stuff, be it equations or formulae or anything similar. This plugin has a lot of options for autocomplete, formatting, and makes my job almost ridiculously easy. Here’s how it looks. (Just typing a random equation)\n\nOrganising\nWhat do you do once you have a lot of files, you organise them of course! Now Obsidian by default makes it pretty easy to do this. But these plugins make organising less of a chore and much more of a fun thing to do.\n\nLocal Images : To make my notes more informative, I sometimes paste images. Now many times these are links from some website, which makes it a little risky, because what if the website stops working? This plugin automatically downloads image links in your notes and saves them locally. (It also links to the correct downloaded file.)\nGraph view : Oh the gift and curse of a pretty graph. I sometimes use this to navigate between my links either to or from a file. It also gives me a very useful overview of what I have. I generally use the “Local graph” that shows me a graph for the current note, rather than the “Global” full one which shows me everything. (It’s pretty, but unhelpful)\nLinter : Maybe I have a bunch of empty lines, empty list items, my headers are not in sentence case, my text is not formatted, my paragraphs are weird. Or anything like that. I am lazy, so I use the Linter plugin to automatically perform a bunch of processing and clean up my files.\nTag Wrangler : Have a lot of tags? View/Edit/Change them across every file that uses them in one place. Also useful for finding files that match a few criteria.\nFile Cleaner : Remove empty files, unreferenced images etc. Keeping your “Digital Garden” pruned and bug free.\nObsidian Link Converter : Because I host my Obsidian Vault on a personal website, sometimes the links that Obsidian uses don’t work, this plugin lets me mass convert them to a format that does.\n\nBonus tips!!\n\nMake sure every file has a single major idea. If you have too many, use the “Note Refactor” to put them in their own files. This will make it extremely easy to refer to the “Ideas” in the text somewhere else instead of linking to the whole text.\nWant pages that consolidate all the notes that have a particular tag together and save them automatically to a single file? Say you want a file that has links to all the notes that have the tag “#apple”. Here is a little script that I wrote which does just that.\n\nFin\nThis article is in the hopes that it will help someone out. Maybe have the help that I did not. I do not know who it will reach. But to whoever it does, best of luck :)\nLike these/Want more? Buy me a coffee! Kofi\nWant articles on something specific? Just ask!\nYou can always contact me on LinkedIn, or drop me an Email\n\ntoc: true\ntitle: Meta AI Speech from Brain\ntags: [‘architecture’]\nMeta AI Speech from Brain\n\nhelp people unable to communicate through speech, typing or gestures\ntries to decode language directly from noninvasive brain recordings\nchallenge with this proposed method come from noise and diferences in each person’s brain and where the sensors are placed.\ncontrastive learning and used to maximally align noninvasive brain recordings and speech sounds\nA self-supervised learning model called wave2vec 2.0. is used to identify the complex representations of speech in the brains of volunteers listening to audiobooks\nThe two noninvasive technologies used to measure neuronal activity are electroencephalography and magnetoencephalography.\nTraining data comes from four opensource datasets which represent 150 hours of recordings of 169 volunteers listening to audiobooks\nEEG and MEG recordings are inserted into a brain model, which consists of a standard deep convolutional network with residual connections\nThese recordings are what comes from individuals’ brains\nboth a speech model for sound and a brain model for MEG data.\nseveral components of the algorithm were beneficial to decoding performance\nalgorithm improves as EEG and MEG recordings increase\nself-supervised trained AI can decode perveived speech despite noise and variability in that data.\n"},"KB/Meta-Learning-Data-Augmentations":{"title":"Meta Learning Data Augmentations","links":[],"tags":["augmentation"],"content":"Meta Learning Data Augmentations\n\nThe concept of meta-learning in Deep Learning research generally refers to the concept of optimizing neural networks with neural networks.\nThis approach has become very popular since the publication of NAS\n"},"KB/Methods-for-Feature-Learning":{"title":"Methods for Feature Learning","links":["KB/Feature-Learning","KB/Gradient-Descent","KB/LinearRegression"],"tags":["temp"],"content":"Methods for Feature Learning\n\nStart with a random dict and rep. Update D while keeping r fixed → find best r while keeping D fixed. Repeat until convergence.\nMove D in a direction to minimize loss and project it back\n\nGradient Descent or LinearRegression\n\n\n"},"KB/Microbiota":{"title":"Microbiota","links":[],"tags":["brain"],"content":"Microbiota\n\nThe community of various microorganisms found in the digestive tract. Scientists are now learning that microbes found in the microbiota can influence brain development, mood, and behavior.\n"},"KB/Microglia":{"title":"Microglia","links":[],"tags":["brain"],"content":"Microglia\n\nA small, specialized glial cell that operates as the first line of immune defense in the central nervous system.\n"},"KB/Micromarriage":{"title":"Micromarriage","links":[],"tags":["random"],"content":"Micromarriage\n\nMicromarriages — colah’s blog\nA micromarriage is a one in a million chance that an action will lead to you getting married, relative to your default policy.\nNote that some actions, such as dying, have a negative number of micromarriages associated with them.\nMost people do not include being coercively forced into a marriage when calculating micromarriages.\n"},"KB/Midpoint-Decider":{"title":"Midpoint Decider","links":[],"tags":["visualization"],"content":"Midpoint Decider\n\ncheck value in cell center and decide accordingly\n\n"},"KB/Midpoint-Method":{"title":"Midpoint Method","links":[],"tags":["visualization"],"content":"Midpoint Method\n\n\n"},"KB/Milin-et-al":{"title":"Milin et al.","links":["KB/Unsupervised-Learning","KB/past-tense","KB/Uncertainty"],"tags":["language"],"content":"Milin Et Al.\n\nTowards cognitively plausible data science in language research (2016), Milin, Divjak, Dimitrijevic and Baayen\nIdentify difficult and easy forms (from lemma to plural form)\nCheck if human participants also react differently to independently identified difficult and easy forms Compare NDL learning model to TiMBL and human results\nMDVM computes the distance between two values of a feature to reflect their patterns of co-occurrence with categories\nUsing MDVM adds an Unsupervised Learning component to MBL Hoste (2005) because essentially it clusters feature values and uses that information\nUsing larger values of k with MDVM is helpful\nEasy words that are frequent tokens (forms) are reacted to faster\nMaybe this interaction doesn’t occur with difficult words because there is less variation in the frequency of the difficult words?\nThis seems similar to results with regular past tense forms in English:\nStrikingly, TiMBL’s inflectional class probabilities turn out to be predictive in production and comprehension, i. e., for lexical decision latencies.\nTwo Grapheme to Lexeme Measures\nDiversity Sum of the absolute values of the activations of all possible outcomes, given a set of input cues.\nInput cues that activate many different outcomes give rise to a highly diverse activation vector, which in turn indicates a high degree of Uncertainty about the intended outcome.\nG2L-Prior Sum of the absolute values of the weights on the connections from all cues to a given outcome.\nindependent of the actual cues encountered in the input\nreflects the prior availability of an outcome, its entrenchment in the learning network\nTiMBL assigns higher probabilities to forms belonging to lemmas with letter trigraphs that yield more diverse activations\nThose trigraphs belong to a rich exemplar space in the memory\nit would be expected that higher probabilities would result in shorter response latencies\nHowever, NDL’s G2L-Diversity was in fact positively correlated with RTs, indicating inhibition, i. e. slower recognition.\nTiMBL probabilities are intended to capture the likelihood of a form’s occurrence in production.\nin comprehension (lexicality judgments) high trigraphs diversity may hurt results\nSpontaneous recovery from extinction\nAfter a CS is learned to associated with a given Conditioned Response (CR), this association is unlearned\nTheoretically, it can not arise again without retraining\nBut in real life, sometimes seemingly completely forgotten associations are reactivated\nshows extinction is not unlearning\nresponses that disappear are not necessarily forgotten\nSuggests loss of activation is not simply the mirror of acquiring associations\nGiven two conditions stimuli, (CS) where one is more salient, the more salient CS will develop a strong association with the CR (Conditioned Response)\nSome linguistic things can be learned with NDL and this might show use something about the problem\nWhat made NDL so nice for animal learning might not scale up to linguistic phenomena\nInductive approaches to cognition\n"},"KB/Minerva":{"title":"Minerva","links":[],"tags":["architecture"],"content":"Minerva\n\nLanguage model capable of solving mathematical and scientific questions using step-by-step reasoning\nvery clear focus on the collection of training data for this purpose\nsolves quantitative reasoning problems,\nmakes models at scale and employs best-in-class inference techniques\nConcretely, Minerva solves these problems by generating solutions step-by-step\nthis means including calculations and symbolic manipulation without having the need for external tools such a calculator.\n"},"KB/Mini-Batch-GD":{"title":"Mini Batch GD","links":[],"tags":["gradients"],"content":"Mini Batch GD\n\n\\theta= \\theta-\\eta \\cdot \\nabla_{\\theta}J(\\theta; x^{i:i+n};y^{i;i+n})\n"},"KB/Minimal-Semantic-Commitment":{"title":"Minimal Semantic Commitment","links":[],"tags":["language"],"content":"Minimal Semantic Commitment\n\n(Frazier et al., 1999)\nThe MSC hypothesis distinguishes between two types of mental representations the processor might entertain upon encountering an underdetermined semantic constituent: if the representation is ambiguous, the processor will commit to just one interpretation and later revise it if necessary, but if the representation is vague, the processor refrains from committing to an interpretation, leaving some features underdetermined until further information is made available.\n"},"KB/Minimization-and-reporting-of-negative-impacts":{"title":"Minimization and reporting of negative impacts","links":[],"tags":["explainability"],"content":"Minimization and reporting of negative impacts\n\nreporting actions or decisions that yield a certain outcome by the system\nassessment of those outcomes\nconsider the identification, assessment, documentation and minimization of their potential negative impacts\n"},"KB/Minimizing-Communication":{"title":"Minimizing Communication","links":[],"tags":["parallelcomputing"],"content":"Minimizing Communication\n\nReduce the number of messages passed\nReduce amount of data passed in messages\n"},"KB/Minkowski-Distance":{"title":"Minkowski Distance","links":["KB/Manhattan-Distance","KB/Euclidean-Distance","KB/Chebyshev-Distance"],"tags":["loss"],"content":"Minkowski Distance\n\n\nD(x,y) = (\\Sigma_{i=1}^{n}|x_{i}-y_{i}|^{p})^{\\frac{1}{p}}\nIt is a metric used in Normed vector space (n-dimensional real space), which means that it can be used in a space where distances can be represented as a vector that has a length.\n\nZero Vector — The zero vector has a length of zero whereas every other vector has a positive length. For example, if we travel from one place to another, then that distance is always positive. However, if we travel from one place to itself, then that distance is zero.\nScalar Factor — When you multiple the vector with a positive number its length is changed whilst keeping its direction. For example, if we go a certain distance in one direction and add the same distance, the direction does not change.\nTriangle Inequality — The shortest distance between two points is a straight line.\n\n\nMost interestingly about this distance measure is the use of parameter p. We can use this parameter to manipulate the distance metrics to closely resemble others.\nCommon values of p are:\n\np=1 — Manhattan Distance\np=2 — Euclidean Distance\np=\\infty — Chebyshev Distance\n\n\nThe upside to p is the possibility to iterate over it and find the distance measure that works best for your use case.\n"},"KB/Mirman-et-al":{"title":"Mirman et al.","links":["KB/Probability","KB/SRN","KB/Transitional-probabilities","KB/Features"],"tags":["language"],"content":"Mirman Et Al.\n\ntrain syllables in words, predicting the next syllable\nuse network to train on different types of individual words, matching them with one of five objects, simulating word learning\n75 epocs 1000 syllable sequence, then it predicted almost perfectly the next syllable (teaching phonotactics of the language)\nModel trained to recognize one of five objects for each of five different two-syllable input patterns of three types 1. words (100% transitional Probability) 2. partwords (25% Probability transitions) 3. nonwords (0% transitions)\nModel is better at mapping two-syllable sequences to words when it has already been exposed to those sequences and they had high probabilities\nNovel-sequence non-word labels initially learned nearly as fast as word up to intermediate point.\nexposure to familiarization input allowed network to created distinct hidden representations for each syllable\nSRN can show how statistical learning supports word learning, showing a link\nHumans are good at learning sequences, even when the data is presented implicitly and even when the relationships are non-adjacent\nWe aren’t just sensitive to frequency: we are sensitive to actual Transitional probabilities\nSRNs with very simple assumptions model non-adjacent learning and Transitional probabilities\nBiological arguments for distributed representations\nMakes more sense that neurons get randomly assigned to be active for different inputs\nWe can start with randomness and with learning it will become structured\nconcepts are just bundles of Features, that together become something\nPrevents catastrophic failures\n"},"KB/Mirror-Shift-Function":{"title":"Mirror Shift Function","links":[],"tags":["robotics"],"content":"Mirror Shift Function\n\nWith the Mirror Shift Function, a job is converted to the job in which the path is symmetrical to that of the original job.\n"},"KB/Misyak-et-al-2010":{"title":"Misyak et al 2010","links":["KB/Non-adjacent-dependencies","KB/SRN"],"tags":["language"],"content":"Misyak Et Al 2010\n\nDoes the ability to learn statistical Non-adjacent dependencies correlate with the ability to process Non-adjacent dependencies in language?\nCan we model non-adjacent dependency learning with simple SRNs?\nallows us to see the continuous timecourse of statistical processing\nUses both linguistic stimulus tokens and auditory cues\non-line non-adjacency learning\nInvestigation of Individual differences in language processing and statistical learning\nParticipants trained in blocks of three word sequence trials.\nFirst and second word were random, but the third word was dependent on the first word.\n\nIntervening second word creates non-adjacency\n\n\nAfter final block: Prediction task where participants had to say what the third word was from two word sequences\nPeople can learn non-adjacent sequences with only implicit exposure\nSRN can capture performance on AGL tasks\nSRNs can deal with temporal structures and associations\nLocalist representations: 30 input and output units, each unique unit corresponding to each nonword\nStandard backpropagation with a learning rate of 0.1 and momentum at 0.8\nThe higher the prediction task accuracy (x-axis) the shorter reading times for object relatives.\nEven the people who are bad at sequential learning are still fluent speakers and listeners\nIs it possible that sequential learning and language learning are unrelated\nMaybe children are better at sequential learning, which helps them acquire languag\nAdults then lose this ability\n"},"KB/Mixed-Effect-Models":{"title":"Mixed Effect Models","links":[],"tags":["language"],"content":"Mixed Effect Models\n\nAre able to combine fixed factors and multiple [random factors](random factors.md) in one analysis No longer necessary to do two ANOVAs\n"},"KB/Mixed-Example":{"title":"Mixed Example","links":[],"tags":["augmentation"],"content":"Mixed Example\n\nexperimented with 14 different types of augmentation approaches. The output image is generated using the following techniques: vertical concatenation, horizontal concatenation, mixed concatenation, random 2x2, VH- mixup (vertical concatenation, horizontal concate- nation, and mixup), VH-BC+ (vertical concatena- tion, horizontal concatenation, and between-class), random square, random column interval, random row interval, random rows, random columns, ran- dom pixels, random elements, and noisy mixup.\nFrom all these approached, VHmixup has the best performance.\n"},"KB/Mixed-chunk-attention":{"title":"Mixed chunk attention","links":["KB/Attention"],"tags":["architecture"],"content":"Mixed Chunk Attention\n\nan efficient linear approximation method that combines the benefits from partial and linear Attention mechanisms, which is accelerator-friendly and highly competitive in quality.\nThe method works on chunks of tokens and leverages local (within chunk) and global (between chunks) Attention spans\n"},"KB/Mixup":{"title":"Mixup","links":["KB/1D-piecewise-linear-interpolation","KB/Beta-Distribution"],"tags":["regularization","augmentation"],"content":"Mixup\n\n@zhangMixupEmpiricalRisk2018\nRandomly sample two examples (x_{i}, y_{i}) and (x_{j}, y_{j})\nNew example by weighted 1D piecewise linear interpolation\n\n\n\n\n\\hat x = \\lambda x_{i}+(1-\\lambda)x_{j}\n- $$\n\\hat y = \\lambda y_{i}+(1-\\lambda)y_{j}\n\n$\\lambda \\in Beta Distribution\nNew example (\\hat x, \\hat y)\n"},"KB/MoCO":{"title":"MoCO","links":["KB/ImageNet","KB/PASCAL-VOC","KB/COCO","KB/Contrastive-Loss"],"tags":["temp"],"content":"\ntoc: true\ntitle: MoCO\ntags: [‘temp’]\n\nMoCO\n\nMomentum Contrast for Unsupervised Visual Representation Learning\nunsupervised visual representation learning\ncontrastive learning as dictionary look-up, MoCo builds a dynamic dictionary with a queue and a moving-averaged encoder\nlarge and consistent dictionary on-the-fly\nImageNet\ntransfer well to downstream tasks.\nPASCAL VOC\nCOCO\nvisual representation encoder by matching an encoded query\nto a dictionary of encoded keys using a Contrastive Loss\ndictionary is built as a queue, with the current mini-batch enqueued\noldest mini-batch dequeued\nslowly progressing encoder\nmomentum update with the query encoder\n\n\n"},"KB/Mobile-Net":{"title":"Mobile Net","links":["KB/Depthwise-Separable"],"tags":["architecture"],"content":"Mobile Net\n\n@howardMobilenetsEfficientConvolutional2017\n@sandlerMobilenetv2InvertedResiduals2018\nDepthwise Separable\n"},"KB/MobileOne":{"title":"MobileOne","links":["KB/RepVGG","KB/Relu","KB/ImageNet","KB/EfficientNet"],"tags":["architecture"],"content":"MobileOne\n\nAn Improved One Millisecond Mobile Backbone\nextensive analysis of different metrics by deploying several mobile friendly networks on a mobile device\nidentify and analyze architectural and optimization bottlenecks\nmany times faster on mobile\nInspired byRepVGG\nEither Relu or SE-Relu is used as activation. The trivial over-parameterization factor k is a hyperparameter which is tuned for every variant.\nbetter top-1 accuracy on ImageNet than EfficientNet at similar latency\n\n"},"KB/Modality-Dropout":{"title":"Modality Dropout","links":["KB/Modality","KB/Dropout"],"tags":["regularization"],"content":"Modality Dropout\n\nMDO improves fine-tuning by randomly dropping one of the modalities\n"},"KB/Modality":{"title":"Modality","links":[],"tags":["temp"],"content":"Modality\n\nA high-level data category. For example, numbers, text, images, video, and audio are five different modalities.\n"},"KB/Mode-Collapse":{"title":"Mode Collapse","links":[],"tags":["temp"],"content":"Mode Collapse\n\nGenerator collapses and only predicts mean/median/mode of data instead of the prob distribution\n"},"KB/Mode-Switch":{"title":"Mode Switch","links":[],"tags":["robotics"],"content":"Mode Switch\n\nAs per safety standards, an industrial robot has three distinct modes of operation. These are Teach (also called Manual) and Play (also called Automatic) and Remote. Switching between these modes is performed using a key switch on the teach pendant and is called Mode Switch.\n"},"KB/Model-Capacity":{"title":"Model Capacity","links":["KB/Representational-Capacity","KB/Vapnik-chervonenkis-dimension"],"tags":["architecture","deeplearning"],"content":"Model Capacity\n \n\nmean the number of parameters or hidden units in the model (and hence indirectly, the ability of the model to fit functions of increasing complexity\nRepresentational Capacity\nVapnik chervonenkis dimension\n"},"KB/Modeling-Driver-Behavior-with-Cognitive-Architecture":{"title":"Modeling Driver Behavior with Cognitive Architecture","links":["KB/ACT-R"],"tags":["usermodel"],"content":"Modeling Driver Behavior with Cognitive Architecture\n\nSalvucci, Dario D. “Modeling driver behavior in a cognitive architecture.” Human factors 48.2 (2006): 362-380.\n\nIntro\nThis paper explores the development of a rigorous computational model of driver behavior in a cognitive architecture – a computational framework with underlying psychological theories that incorporate basic properties and limitations of the human system\nAn integrated driver model developed in the ACT-R (Adaptive Control of Thought-Rational) cognitive architecture is described that focuses on the component processes of control, monitoring, and decision making in a multilane highway environment\nThis model accounts for the steering profiles, lateral position profiles, and gaze distributions of human drivers during lane keeping, curve negotiation, and lane changing.\nThe model demonstrates how cognitive architectures facilitate understanding of driver behavior in the context of general human abilities and constraints and how the driving domain benefits cognitive architectures by pushing model development toward more complex, realistic tasks\nDriving and Integrated Driver Modeling\nuseful to view driving and driver modeling in the context of the embodied cognition, task, and artifact (ETA) framework (Byrne, 2001; Gray, 2000; Gray &amp; Boehm-Davis, 2000).\nAs the name suggests, this framework emphasizes three components of an integrated modeling effort: the task that a person attempts to perform, the artifact\nby which the person performs the task, and the embodied cognition by which the person perceives, thinks, and acts in the world through the artifact\nA sound understanding of each component is critical to developing rigorous integrated models of driver behavior.\nMichon (1985) identified three classes of task processes for driving: operational processes that involve manipulating control inputs for stable driving, tactical processes that govern safe interactions with the environment and other vehicles, and strategic processes for higher level reasoning and planning\nSome tasks are not continual but intermittent, arising in specific situations – for instance, parking a vehicle at a final destination.\nBetween cognition and the vehicle lies the embodiment of the driver, namely the perceptual processes (visual, aural, vestibular, etc.) and motor processes (hands, feet) that provide the input from and output to the external world.\nNot surprisingly, there can be parallelism in this integrated system – for instance, moving the hand while visually encoding the lead car – but there are also capacity constraints and/or bottlenecks that sometimes result in degraded performance.\nPictures\n\n\n\n\n"},"KB/Modeling-Transfer":{"title":"Modeling Transfer","links":[],"tags":["usermodel"],"content":"Modeling Transfer\n\ngiven levels of mastery on a set of knowledge components, predict the performance on a new problem (Singley &amp; Anderson, 1989).\nFor instance, suppose a student has mastered 15 of the 20 knowledge components required to do a task.\nThis predicts the student’s behavior on the task—where the student will ask for help, how long it will take to do the task, which errors occur, etc\nHowever, these predictions can be inaccurate if the assumed knowledge components are not accurate reflections of how the student actually understands the task domain\n"},"KB/Modeling-motivation-using-goal-competition-in-mental-fatigue-studies":{"title":"Modeling motivation using goal competition in mental fatigue studies","links":[],"tags":["cognitivemodel"],"content":"Modeling motivation using goal competition in mental fatigue studies\n\n\nMega B. Herlambang a,b,∗, Niels A. Taatgen a, Fokie Cnossen\n\n\nMotivation can counteract the effects of mental fatigue\n\n\ngoal competition as a paradigm to understand the role of motivation and built three models of mental fatigue studies to demonstrate the mechanism in a cognitive architecture named PRIM\n\n\nmodel changes in performance levels by adjusting the value of the main task goals\n\n\nwhich controls the competition with distractions\n\n\nbest model fits were obtained by a linear decrease in goal activation\n\n\nModeling fatigue and motivation decline in PRIMs\n\n\nThe assumption in this paper is that the decrease in task performance in mental fatigue is the result of a reduction in task motivation.\n\n\nreflected in a reduction in activation of the task goal over time\n\n\nAs time progresses, individuals may experience an increase in the feeling of fatigue that reduces the subjective value of the main task, i.e., task motivation (Müller &amp; Apps, 2019) or goal activation (Agoal) in our models\n\n\nConsequently, goal activation is discounted by the feeling of fatigue\n\n\nperceived reward from doing the task (extrinsically or intrinsically) maintains the goal activation from declining.\n\n\nA_{goal(t)} = min(1 \\vee (P(t) - F(t))),\n\n\nTherefore, the relationship between the perceived reward (P), the feeling of fatigue (F), and goal activation (Agoal) at any given time (t) is\n\n\ngoal activation value of one means that the model mainly focuses its attention on the main goal.\n\n\nThe value P is influenced by previous perceived rewards. For example, when the previous incentive at t − 1 is perceived as more valuable than the recent one at t, then the value Pt is smaller than Pt−1\n\n\nIn contrast, the value of Pt is higher if the reward at t is perceived as more valuable than the previous one at t−1\n\n\nIt is evident that motivation affects the ability to stay focused on a task and not be distracted by internal or external distractions (Herlambang et al., 2019)\n\n\nIn the case of external distractions, task-unrelated stimuli may shift attention away from the main task, while internal distractions may manifest itself in the form of mind-wandering\n\n\nDiscussion\n\nthat goal competition is one of the key factors to understand the underlying mechanism of motivation in mental fatigue.\ndecrease in performance is not due to a decrease in the capacity of the cognitive system (e.g., lower working memory capacity, slower motor system, less reliable long-term memory) but by a decrease in the ratio of cognitive ”cycles” spent on the task as opposed to distractions.\nhave modeled this by a decrease in the activation of the goal, which represents the level of motivation, which indirectly affects performance\nmodeler can tune some parameters to obtain a good fit\nHowever, overdoing such parameter tuning may lead to overfitting, which makes the models difficult to generalize and does not represent the empirical data\n\nGoal activation and performance\n\nTo lower performance in the nonreward conditions in all tasks, we decreased task goal activation values over time\nThe goal activation values in our models represent the subjective value of performing the tasks, with a high subjective value corresponds to a high level of motivation\nThe reduction of goal activation was due to an increase in the feeling of fatigue (see Müller &amp; Apps, 2019) and a continuous decrease in the perceived reward from doing the tasks\nWhat our modeling efforts suggest is that over time, while the activation value of the main active goal is decreasing, which is due to an increase in the feeling of fatigue and a decrease in the perceived reward (see Eq. (3)), another future goal of an activity/stimulus, for example, a distraction, may start winning the competition with the main task, when the activation value of the distraction exceeds that of the main goal (i.e., it strongly attracts the individual), in which case the individual may start paying attention to the distraction. The distraction can become the new active goal, and the individual may forget the main goal, or choose to pay attention to both, but this will sacrifice performance.\nGoal competition is a continuous process that compares several future goals, and when the main task goal is perceived to be less valuable, another competing goal may start winning the competition, causing the individual to invest less mental effort in the main task and start investing in the competing goal\n\nLimitation, challenge, and future research\n\nsolely adjusting goal activation levels may not be enough to model changes in performance.\nThere are many parameters in PRIMs that can affect performance\nAlthough adjusting goal activation values as a way to model mental fatigue showed good results for the experiments we modeled, it is possible that this does not directly generalize to other studies\n\nImages\n\n\n\n\n\n\n\n\n"},"KB/Moment-Exchange":{"title":"Moment Exchange","links":[],"tags":["augmentation"],"content":"Moment Exchange\n\nencouraging the models to utilize the moment information of latent features\nSpecifically, the moments of the learned features of one training image are re- placed by those of another.\n"},"KB/Moment-in-Time":{"title":"Moment in Time","links":[],"tags":["dataset"],"content":"Moment in Time\n\nlarge balanced and diverse dataset\nvideo under-\nstanding\n1 million video clips that cover 339 classes, and each video lasts around 3 seconds\naverage number of video clips for each class is 1, 757 with a median of 2, 775\nvideos that capturing visual and/or audible actions, produced by humans, animals, objects or nature\n"},"KB/Momentum":{"title":"Momentum","links":["KB/Force"],"tags":["physics"],"content":"Momentum\n\nMomentum is the velocity of a body multiplied by its mass. A small Force can quickly stop an object with low momentum, but a large or prolonged Force is required to stop an object with high momentum.\np = mv\nmass times velocity\n"},"KB/Monk":{"title":"Monk","links":["KB/Hit-list"],"tags":["deeplearning"],"content":"Monk\n\nHit list\n\n\n"},"KB/Moral-Machine-project":{"title":"Moral Machine project","links":[],"tags":["ethics"],"content":"Moral Machine Project\n\nMIT\nwisdom of the crowd to find resolutions for ethical dilemmas\nstudying the perception of autonomous vehicles (AVs) which are controlled by AI and has the potential to harm pedestrians and/or passengers if they malfunction\nallows participants to judge various ethical dilemmas facing AVs which have malfunctioned, and select which outcomes they prefer.\nsaving more lives\nprotecting passengers\nupholding the law\navoiding intervention\ngender preference\nspecies preference\nage\nsocial value preference.\npeople generally prefer the AV to make sacrifices if more lives can be saved.\nself-reported preferences often do not align well with actual behaviours\n"},"KB/Moral-Machine-project.md":{"title":"Moral Machine project.md","links":[],"tags":[],"content":""},"KB/Moral-decision-making-frameworks-for-artificial-intelligence":{"title":"Moral decision making frameworks for artificial intelligence","links":["KB/Trees"],"tags":["ethics"],"content":"Moral Decision Making Frameworks for Artificial Intelligence\n\nVincent Conitzer, Walter Sinnott- Armstrong, Jana Schaich Borg, Yuan Deng, and Max Kramer.\ndeveloping a general ethical decision making framework for AI based on game theory and machine learning\nFor the game theory based framework, the authors suggest the extensive form (a generalization of game Trees) as a foundation scheme to represent dilemmas\ncurrent extensive form does not account for protected values in which an action can be treated as unethical regardless of its consequence\nextend the extensive form representation with passive actions for agents to select in order to be ethical\nmachine learning based ethical decision-making\nclassify whether a given action under a given scenario is morally right or wrong\nThe main challenge in machine learning based moral decision-making is to design a generalizable representation of ethical dilemmas\nGame theory and machine learning can be combined into one framework in which game theoretic analysis of ethics is used as a feature to train machine learning approaches\n"},"KB/MoralDM":{"title":"MoralDM","links":[],"tags":["ethics"],"content":"MoralDM\n\nenables an agent to resolve ethical dilemmas by leveraging on two mechanisms\n\n\nfirst-principles reasoning, which makes decisions based on well-established ethical rules (e.g., protected values); and 2) analogical reasoning, which compares a given scenario to past resolved similar cases to aid decision-making.\n\n\nthe exhaustive comparison approach by MoralDM is expected to become computationally intractable\n[Blass and Forbus, 2015], MoralDM is extended with structure mapping which trims the search space by computing the correspondences, candidate inferences and similarity scores between cases to improve the efficiency of analogical generalization\n"},"KB/MoreMVCNN":{"title":"MoreMVCNN","links":[],"tags":["robotics"],"content":"MoreMVCNN"},"KB/Morpheme-Generation":{"title":"Morpheme Generation","links":["KB/Morpheme","KB/Verb"],"tags":["language"],"content":"Morpheme Generation\n\nSee +past.Verb = saw\n"},"KB/Morpheme-Segmentation":{"title":"Morpheme Segmentation","links":["KB/Morpheme"],"tags":["language"],"content":"Morpheme Segmentation\n\nDe-nation-al-iz-ation\n"},"KB/Morpheme":{"title":"Morpheme","links":["KB/Allomorph","KB/Morphology-Stem","KB/Morphology-Affix","KB/Content-Morpheme","KB/Functional-Morpheme","KB/Morpheme-Segmentation","KB/Morpheme-Generation","KB/Morphotactic"],"tags":["language"],"content":"Morpheme\n\nwords are built from smaller meaningful units called morphemes\nAllomorph\nMorphology Stem\nMorphology Affix\nContent Morpheme\nFunctional Morpheme\nMorpheme Segmentation\nMorpheme Generation\nMorphotactic\n"},"KB/Morphology-Affix":{"title":"Morphology Affix","links":["KB/Morphology","KB/Bound-morpheme","KB/Prefix","KB/Suffix","KB/Infix","KB/Circumfix"],"tags":["language"],"content":"Morphology Affix\n\nBits and pieces that adhere to stems to change their meanings and grammatical functions\nBound morpheme\nPrefix\nSuffix\nInfix\nCircumfix\n"},"KB/Morphology-Stem":{"title":"Morphology Stem","links":["KB/Morphology","KB/Morpheme","KB/Free-morpheme"],"tags":["language"],"content":"Morphology Stem\n\nThe core meaning bearing units – Main Morpheme of the word\nFree morpheme\n"},"KB/Morphology":{"title":"Morphology","links":["KB/Morpheme","KB/Lexicon","KB/Inflectional-Morphology","KB/Derivational-Morphology","KB/Suppletion","KB/Word-Compounding","KB/Word-Blending","KB/Word-Clipping","KB/Lemmatization"],"tags":["language"],"content":"Morphology\n\nstructure of words\nMorpheme\nIt is concerned with inflection.\nIt is also concerned with derivation of new words from existing ones, eg. lighthouse (formed from light &amp; house)\nNeeds a Lexicon\nInflectional Morphology\nDerivational Morphology\nSuppletion\nWord Compounding\nWord Blending\nWord Clipping\nLemmatization\n"},"KB/Morphotactic":{"title":"Morphotactic","links":[],"tags":["language"],"content":"Morphotactic\n\nWhich class of morphemes follow other class of morphemes\nPlural morphemes follow noun\nsome endings go only on certain words not on everything.\nDo + er : doer\nBe + er :beer\n"},"KB/Motor-Memories":{"title":"Motor Memories","links":["KB/Alzheimer’s-Disease","KB/memory-trace","KB/Two-photon-Microscopy","KB/Striatum","KB/Parkinson’s-Disease"],"tags":["cognitivemodel"],"content":"Motor Memories\n\nmotor memory is unique\nSome studies on Alzheimer’s Disease included participants who were previously musicians and couldn’t remember their own families, but they could still play beautiful music. Clearly, there’s a huge difference in the way that motor memories are formed\nMemories are thought to be encoded in the brain in the pattern of activity in networks of hundreds or thousands of neurons, sometimes distributed across distant brain regions\nmemory trace\nWhen the researchers tested the animals’ memory of this new skill weeks later, they found that those mice that still remembered the skill showed increased activity in the same neurons that were first identified during the learning period, showing that these neurons were responsible for encoding the skill\nTwo-photon Microscopy\n“engram neurons” reprogram themselves as the mice learned\nMotor cortex engram cells took on new synaptic inputs — potentially reflecting information about the reaching movement — and themselves formed powerful new output connections in a distant brain region called the dorsolateral Striatum — a key waystation through which the engram neurons can exert refined control over the animal’s movements.\nThese findings suggest that, in addition to being dispersed, motor memories are highly redundant.\nThe researchers say that as we repeat learned skills, we are continually reinforcing the motor engrams by building new connections — refining the skill. It’s what is meant by the term muscle memory — a refined, highly redundant network of motor engrams used so frequently that the associated skill seems automatic.\nCurrent thinking is that Parkinson’s Disease is the result of these motor engrams being blocked, but what if they’re actually being lost and people are forgetting these skills?\n"},"KB/Multi-Head-Attention":{"title":"Multi Head Attention","links":["KB/Attention","KB/Self-Attention"],"tags":["architecture"],"content":"Multi Head Attention\n\nZihangDai et al., 2019\nwhich computes self-Attention over the inputs, then adds back the residual and layer normalizes everything. The Attention head can be split into multiple segments, hence the name multi-head\nMultiple Attention instances, each focusing on a different part of the input\nWords can mean different things in context\n\nIf using Self Attention, then this just gets summed up. Which is not very nice\nSeveral Attention heads → different output vectors\nConcatenate them and pass through a linear transform → dimension back to k\n\n\nMultiHead(Q,K,V) = Concat(head_1, head_2, …., head_h)W^O\n\nhead_i = Attention(QW_i^Q, KW_i^K , VW_i^V)\n\n\nW is learnable projections for Attention params\n\nTo improve efficiency\n\nCut the incoming vector into chunks → no of Attention heads\n\n\n\nclass MultiHeadAttentionNew(nn.Module):\n    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):\n        super().__init__()\n        self.n_head = n_head\n        \n        self.w_qs = nn.Linear(d_model, n_head * d_k)\n        self.w_ks = nn.Linear(d_model, n_head * d_k)\n        self.w_vs = nn.Linear(d_model, n_head * d_v)\n        \n        nn.init.normal_(self.w_qs.weight, mean=0, std=np.sqrt(2.0 / (d_model + d_k)))\n        nn.init.normal_(self.w_ks.weight, mean=0, std=np.sqrt(2.0 / (d_model + d_k)))\n        nn.init.normal_(self.w_vs.weight, mean=0, std=np.sqrt(2.0 / (d_model + d_v)))\n        \n        self.fc = nn.Linear(n_head * d_v, d_model)\n        nn.init.xavier_normal_(self.fc.weight)\n        self.dropout = nn.Dropout(p=dropout)\n        self.layer_norm = nn.LayerNorm(d_model)\n \n    def forward(self, q, k, v, mask=None):\n        residual = q\n        q = rearrange(self.w_qs(q), &#039;b l (head k) -&gt; head b l k&#039;, head=self.n_head)\n        k = rearrange(self.w_ks(k), &#039;b t (head k) -&gt; head b t k&#039;, head=self.n_head)\n        v = rearrange(self.w_vs(v), &#039;b t (head v) -&gt; head b t v&#039;, head=self.n_head)\n        attn = torch.einsum(&#039;hblk,hbtk-&gt;hblt&#039;, [q, k]) / np.sqrt(q.shape[-1])\n        if mask is not None:\n            attn = attn.masked_fill(mask[None], -np.inf)\n        attn = torch.softmax(attn, dim=3)\n        output = torch.einsum(&#039;hblt,hbtv-&gt;hblv&#039;, [attn, v])\n        output = rearrange(output, &#039;head b l v -&gt; b l (head v)&#039;)\n        output = self.dropout(self.fc(output))\n        output = self.layer_norm(output + residual)\n        return output, attn"},"KB/Multi-Scale-Flows":{"title":"Multi Scale Flows","links":[],"tags":["architecture"],"content":"Multi Scale Flows\n \n\n\nThe first partition z1 is processed by a series of reversible layers with the same dimension as z1 until, at some point, z2 is appended and combined with the first partition.\nThis continues until the network is the same size as the data x.\nIn the normalizing direction, the network starts at the full dimension of x, but when it reaches the point where zn was added, this is assessed against the base distribution\n"},"KB/Multi-Task-Learning":{"title":"Multi Task Learning","links":["augment","KB/Eavesdropping"],"tags":["multitask"],"content":"Multi Task Learning\n\nmultiple outputs (as desired), and typically a shared trunk of weights can indirectly encode common or shared knowledge\nCan linearly combine loss for each task L_{i}\nL(x,y, \\theta) = \\Sigma_{i}w_{i}L_{i}(f_{i}(x), y, \\theta_{i})\n\nf_{i}(x) is output head with weights \\theta_{i}\n\n\nThe exact scale of the weights does not matter as multiplying the loss by a positive scalar does not change the optimum.\n[Hard Parameter Sharing](Hard Parameter Sharing.md)\n[Soft Parameter Sharing](Soft Parameter Sharing.md)\naugment\n[Attribute Selection](Attribute Selection.md)\nEavesdropping\n[Representation Bias](Representation Bias.md)\n"},"KB/Multi-Teacher-Distillation":{"title":"Multi Teacher Distillation","links":["KB/Logits"],"tags":["knowledgedistillation"],"content":"Multi Teacher Distillation\n\nThe multiple teacher networks can be individually and integrally used for distillation during the period of training a student network.\nTo transfer knowledge from multiple teachers, the simplest way is to use the averaged response from all teachers as the supervision signal (Hinton et al., 2015)\nIn addi- tion to the averaged Logits from all teachers, You et al. (2017) further incorporated features from the inter- mediate layers in order to encourage the dissimilarity among different training samples.\nFukuda et al. (2017) randomly selected one teacher from the pool of teacher networks at each it- eration. To transfer feature-based knowledge from mul- tiple teachers, additional teacher branches are added to the student networks to mimic the intermediate features of teachers (Park and Kwak, 2020; Asif et al., 2020). Born again networks address multiple teach- ers in a step-by-step manner, i.e., the student at the t step is used as the teacher of the student at the t+1 step (Furlanelloetal., 2018)\nTo effi- ciently perform knowledge transfer and explore the power of multiple teachers, several alternative meth- ods have been proposed to simulate multiple teach- ers by adding different types of noise to a given teacher (Sau and Balasubramanian, 2016) or by us- ing stochastic blocks and skip connections (Lee et al., 2019c). Using multiple teacher models with feature ensembles, knowledge amalgamation is designed in (Shen et al., 2019a; Luo et al., 2019; Shen et al., 2019b; Luo et al., 2020). Through knowledge amalgamation, many public available trained deep models as teachers can be reused.\n"},"KB/Multi-Variate-AR":{"title":"Multi Variate AR","links":["KB/TIme-Series"],"tags":["temp"],"content":"Multi Variate AR\n\npredict future from past of another TIme Series\n"},"KB/MultiReader-technique":{"title":"MultiReader technique","links":[],"tags":["temp"],"content":"MultiReader Technique\n\nwhich allows domain adaptation\ntraining a more accurate model that supports multiple keywords (i.e., “OK Google” and “Hey Google”) as well as multiple languages/dialects\n"},"KB/Multimodal-Explanation":{"title":"Multimodal Explanation","links":[],"tags":["explainability"],"content":"Multimodal Explanation\n\nThe visual explanation was created by an attention mechanism that conveyed knowledge about what region of the image was important for the decision. This explanation guides the generation of the textual justification out of a LSTM feature, which is a prediction of a classification problem over all possible justifications.\n"},"KB/Multinomial-Distribution":{"title":"Multinomial","links":["KB/Binomial-Distribution","KB/PMF"],"tags":["distributions"],"content":"Multinomial\n\nP(D|\\theta) = \\frac{N!}{n_{1}!…n_{l}!}\\Pi_{j=1}^{l}\\theta_{j}^{n_{j}}\nN = n_{1}+ ….+ n_{l}\nGeneralized Binomial Distribution\nPMF\n"},"KB/Multiple-Local-Minima":{"title":"Multiple Local Minima","links":[],"tags":["temp"],"content":"Multiple Local Minima"},"KB/Multiple-Sclerosis":{"title":"Multiple Sclerosis","links":["KB/Myelin"],"tags":["brain"],"content":"Multiple Sclerosis\n\nA progressive neurodegenerative disease involving damage to the protective Myelin sheaths of nerve cells in the brain and spinal cord. Symptoms include impaired movement, pain, and fatigue.\n"},"KB/Multiple-constraint-based-theories":{"title":"Multiple constraint-based theories","links":[],"tags":["language"],"content":"Multiple Constraint-based Theories\n\ndescribe language comprehension as an interactive process whereby all possible syntactic representations are simultaneously partially active and competing for more activation across time\nUnlike the syntax-first models, multiple sources of information, be they syntactic or non-syntactic, integrate immediately to determine the amount of activation provided to each of the competing alternatives\n"},"KB/Multiplicative-Attention":{"title":"Multiplicative Attention","links":["KB/Attention","KB/Additive-Attention","KB/Scaled-Dot-Product-Attention"],"tags":["architecture"],"content":"Multiplicative Attention\n\n\nf_{att}(h_{i}, s_{j}) = h_{i}^{T}W_{a}s_{j}\nSince Additive Attention performs better for scale, use a factor Scaled Dot Product Attention\n"},"KB/Muse":{"title":"Muse","links":[],"tags":["architecture"],"content":"Muse\n\nText-to-image transformer model\nstate-ofthe-art image generation while being more ecient than difusion or autoregressive models\nit is trained on a masked modelling task in discrete token space\nmore ecient because of the use of discrete tokens and requiring fewer sampling iterations\nparallel decoding\nMuse is 10x faster at inference time than Imagen-3B or Parti-3B and 3x faster than Stable Difusion v 1.4\nMuse is also faster than than Stable Difusion in spite of both models working in the latent space of a VQGAN\n"},"KB/Myelin":{"title":"Myelin","links":[],"tags":["brain"],"content":"Myelin\n\nThe fatty substance that encases most nerve cell\naxons, helping to insulate and protect the nerve fiber and effectively speeding up the transmission of nerve impulses.\n"},"KB/Myocardial-Infarction":{"title":"Myocardial Infarction","links":[],"tags":["medical"],"content":"Myocardial Infarction\n\nAlso known as a heart attack, where the heart is deprived of blood due to arterial blockage\n"},"KB/N-dim-Normal":{"title":"N Dim Normal Distribution","links":["KB/Normal-Distribution","KB/Central-Limit-Theorem","KB/PDF","X_{1},-…,-X_{n}"],"tags":["distributions"],"content":"N Dim Normal Distribution\n\nNormal Distribution\nIf data points are vectors x = (x_{1}, …, x_{n})&#039; and RVs X_i fulfill the Central Limit Theorem,\nPDF p(x) = \\frac{1}{(2\\pi)^{n/2}det(\\Sigma)^{\\frac{1}{2}}}exp\\left(-\\frac{1}{2}(x-\\mu)&#039;\\Sigma^{-1}(x-\\mu)\\right)\n\\mu is expectation $E[X_{1}, …, X_{n})’](Covariance.md|(X_{1}, …, X_{n}|X_{1}, …, X_{n})&#039;&#039;](X_{1}, …, X_{n})’](Covariance.md|(X_{1}, …, X_{n}|X_{1}, …, X_{n})&#039;&#039;.md) matrix\n\\Sigma(i,j) = E[(X_{i} - E[X_{i}])(X_{j}-E[X_{j}])]\n\n\\hat \\mu = \\frac{1}{N}\\Sigma_{i}x_{i} and \\hat \\Sigma = \\frac{1}{N-1}\\Sigma_{i}(x_{i}-\\hat\\mu)(x_{i}-\\hat\\mu)&#039;\n"},"KB/NADAM":{"title":"NADAM","links":[],"tags":["optimizer"],"content":"NADAM\n\nextension of Adam that uses the Nesterov momentum technique to accelerate the optimization convergence further\ncombines the momentum of Nesterov’s method with the adaptive learning rates of Adam\nm_t = \\beta_1 * m_{(t-1)} + (1 - \\beta_1) * g_t  v_t = \\beta_2 * v_{(t-1)} + (1 - \\beta_2) * g_t2  \\hat{m_t} = \\frac{m_t}{1-\\beta_1t}   \\hat{v_t} = \\frac{v_t}{1-\\beta_2t}   \\text{parameter} = \\text{parameter} - learning\\_rate * \\frac{\\hat{m_t} + (1-\\beta_1) * g_t}{\\sqrt{\\hat{v_t}} + \\epsilon}\nWhere beta1 and beta2 are two hyperparameters, m_t and v_t are moving averages of the gradients, g_t is the gradient at time t, and learning_rate; epsilon is the same as before.\n"},"KB/NCE":{"title":"NCE","links":["Contrastive-Learning","KB/Distributions","Bias-Vs-Variance","KB/Features","KB/Contrastive-Loss"],"tags":["temp"],"content":"\ntoc: true\ntitle: NCE\ntags: [‘temp’]\n\nNCE\n\nConditional Negative Sampling for Contrastive Learning of Visual Representations\nContrastive Learning\nnoise-contrastive estimation\nbound on mutual information between two views of an image\nrandomly sampled negative examples to normalize the objective\nchoosing difficult negatives, or those more similar to the current instance, can yield stronger representation\nConditional Noise Contrastive Estimator\nsample negatives conditionally\nin a “ring” around each positive, by approximating the partition function using samples from a class of conditional Distributions\nhese estimators lower-bound mutual information\nhigher bias but lower variance than NCE Bias Vs Variance\nApplying these estimators as objectives in contrastive representation learning\ntransferring Features to a variety of new image Distributions from the meta-dataset collection\nContrastive Loss\n"},"KB/NICE---non-linear-independant-components-estimation":{"title":"NICE - non linear independant components estimation","links":["KB/coupling-flows"],"tags":["distributions","architecture"],"content":"NICE - Non Linear Independant Components Estimation\n \n\ncoupling flows\n"},"KB/NIST-2008-Speaker-Recognition-Evaluation-dataset":{"title":"NIST 2008 Speaker Recognition Evaluation dataset","links":[],"tags":["dataset"],"content":"NIST 2008 Speaker Recognition Evaluation Dataset"},"KB/NIST-SRE-2016-Cantonese":{"title":"NIST SRE 2016 Cantonese","links":[],"tags":["dataset"],"content":"NIST SRE 2016 Cantonese"},"KB/NLAIC-Companies":{"title":"NLAIC Companies","links":[],"tags":["jobsearch"],"content":"NLAIC Companies\nAxveco\nOpen Sollicitatie - AI\nHello!\nThis is Subhaditya. I recently graduated with a masters in AI from the RUG (Groningen), and am looking for my next big challenge. While looking at your website, I found quite a lot of awesome projects (like the medical object recognition one) that made me quite interested in working at Axveco. I am currently looking for an AI role in the NL and I would love to discuss the possibility of any such roles with you.\nMy experience is a combination of AI, Computer vision and Data Analytics. Over the past few years, I have worked with many clients (both freelance/part time and in internships) and helped them bring their AI projects to life. I am also quite familiar with the ins and outs of Deep Learning, NLP and image processing. Along with my technical knowledge, I enjoy helping clients define their vision and guiding them towards it. These are all tasks that you do, and so I think that I might be a good fit at Axveco.\nWhile I am not sure what kind of roles exist at the moment, I am open to any AI/Data Science related position. I am quite interested in working as part of a high performance team and the possibility of defining my own roles as well as growing my skills seems like a nice cherry on top as well. I’ve attached my resume to this email as well.\nLooking forward to your reply,\nAxelera\nOpen Sollicitatie - AI\nHello!\nThis is Subhaditya. I recently graduated with a masters in AI from the RUG (Groningen), and am looking for my next big challenge. While looking at your website, I found out about the Metis platform and some of the work your team is doing on Edge AI. Being a computer vision developer myself, I am obviously very interested in working at Axelera. I am currently looking for an AI role in the NL and I would love to discuss the possibility of any such roles with you.\nMy experience is a combination of AI, Computer vision and Data Analytics. Over the past few years, I have worked with many clients (both freelance/part time and in internships) and helped them bring their AI projects to life. I am also quite familiar with the ins and outs of Deep Learning with a specialisation in Computer Vision. Along with my technical knowledge, what is more important is that I want to be part of your mission to democratise edge AI. I believe that this technology can lead to a lot of innovations in many fields, but without the right people, we will not be able to make AI for a green, fair and safe world. These are all tasks that you do, and so I think that I might be a good fit at Axelera.\nWhile I am not sure what kind of roles exist at the moment, I am open to any AI/Data Science related position. I’ve attached my resume to this email as well.\nLooking forward to your reply,\nAlmende\nHello Jan!\nThis is Subhaditya. I recently graduated with a masters in AI from the RUG (Groningen), and am looking for my next big challenge. While looking at your website, I found quite a lot of awesome projects (FoodFriend and VARUS were my favourites) that made me very interested in working at the Health domain at Almende. I noticed that in many projects your team mentioned they wanted to test out neural networks but did not. I am currently looking for an AI role in the NL and I would love to discuss the possibility of any such roles with you.\nMy experience is a combination of AI, Computer vision and Data Analytics. Over the past few years, I have worked with many clients (both freelance/part time and in internships) and helped them bring their AI projects to life. I am also quite familiar with the ins and outs of Deep Learning and how to apply them to a large variety of domains. Along with my technical skills, I love working on new projects, especially if they help people. These are all tasks that you do, and so I think that I might be a good fit at Almende.\nWhile I am not sure what kind of roles exist at the moment, I am open to any AI/Data Science related position. I’ve attached my resume to this email as well.\nLooking forward to your reply,\nAttendi\nlennertjansen95@gmail.com\nHello Lennert!\nThis is Subhaditya. I recently graduated with a masters in AI from the RUG (Groningen), and am looking for my next big challenge. While looking at the Attendi website, I found out about some of the work your team is doing on Speech AI. Along with being a good cause to work on, it sounds like Attendi is a great place to work at. I am currently looking for an AI role in the NL and I would love to discuss the possibility of any such roles with you. (I got your email from your personal website haha)\nMy experience is a combination of AI, and Data Analytics. Over the past few years, I have worked with many clients (both freelance/part time and in internships) and helped them bring their AI projects to life. I am also quite familiar with the ins and outs of Deep Learning and how to apply them to a large variety of domains. I have worked with LLMs and am also comfortable with PyTorch and Tensorflow. Along with my technical skills, I love working on new projects, especially if they help people. These are all tasks that you do, and so I think that I might be a good fit at Attendi.\nWhile I am not sure what kind of roles exist at the moment, I am open to any AI/Data Science related position. I’ve attached my resume to this email as well.\nNeolook Solutions\njohan.thissen@gmail.com\nHello Johan!\nHope you are well.\nMy name is Subhaditya, I recently graduated with a masters in AI from the RUG (Groningen), and am looking for my next big challenge. I heard about Neolook from one of my friends (she met you at a Hattrick event at the RUG) and found the premise very interesting. She mentioned that you also have ML/DL pipelines that you wanted to implement and research, which is why I thought I would drop you an email. I am currently looking for an AI position in the NL and I would love to discuss the possibility of any such roles with you.\nMy experience is a combination of AI and Computer Vision. Over the past few years, I have worked with many clients (both freelance/part-time and in internships) and helped them bring their AI projects to life. I am quite familiar with DL libraries like PyTorch and Tensorflow along with implementing multi-modal models. Along with that, I am good at explaining concepts at a high level and have commercial experience in deploying ML applications. \nI am quite interested in applying AI in healthcare, and when I saw that Neolook might give me a chance to do just that, I had to apply. While at the moment I have a lot to learn about neonatal healthcare, I am willing to do that. I am not queasy about walking through wards, and am comfortable with working alone and in teams as well.\nWhile I am not sure what kind of roles exist at Neolook at the moment, I am open to any AI-related positions. I’ve attached my resume to this email as well. It would be awesome to have a chat with you Johan, I hope you give me a chance!\nAtlasium Media\nAmin Gorti\nHello Amin!\nMy name is Subhaditya, I recently graduated with a masters in AI from the RUG (Groningen), and am looking for my next big challenge. I am quite interested in the AI consultancy role you just posted, and would love to talk to you about it :)\nValueBlue\nDan Bersagui\nHello Dan!\nMy name is Subhaditya, I recently graduated with a masters in AI from the RUG (Groningen), and am looking for my next big challenge. I am quite interested in the AI role you just posted, and would love to talk to you and see if I could be a good fit for it.\nBest,\nSM\nHello Dan! \nMy name is Subhaditya, I recently graduated with a masters in AI from the RUG (Groningen), and am looking for my next big challenge. I found this position on LinkedIn and the phrase “Architects of Change” piqued my interest. I’ve heard about BlueDolphin before, and imagine my surprise when I saw that you also wanted to implement AI algorithms as well! I am currently looking for an AI role in the NL and I would love to be the one to start the ML department at ValueBlue.\nMy experience is a combination of AI and Data Analytics. I am proficient in setting up and tuning open source models across all levels. Over the past few years, I have worked with many clients (both freelance/part-time and in internships) and helped them bring their AI projects to life. I am quite familiar with AI libraries like PyTorch and Tensorflow along with MLOps tools. I have worked quite extensively with Neptune, Tensorboard and Docker for many years as well. \nBlueDolphin has been around for a while and I’m sure there is a lot of interesting data that was collected/obtained from public domains. This makes it all the more challenging and I am all for that.\nI am quite passionate about both the technical and human sides of AI. Being a new department working on a lot of cross functional teams requires this mentality, and I think that I do bring this and more to the table. \nI hope you give me a chance!\n3D Universum\ninfo@3duniversum.com\nHello!\nMy name is Subhaditya, I recently graduated with a masters in AI from the RUG (Groningen), and am looking for my next big challenge. I found out about 3DUniversum from Dr.Sezer when I met him at an event last week. The work your team does is very much in line with my interests and experience and as I am currently looking for an AI position in the NL, I would love to discuss the possibility of any such roles at 3DUniversum. \nI asked him if there were open roles, and he told me that the team was expanding and while most of your group is from UvA, I could give it a shot too.\nMy experience is a combination of Computer Vision and AI. Over the past few years, I have worked with many clients (both freelance/part time and in internships) and helped them bring their AI projects to life. I am familiar with deep learning libraries like PyTorch and Tensorflow and am proficient in Python as well. I have worked with Synthetic media creation, Efficient Deep Learning, Mobile/Edge AI as well as 3D Segmentation and Detection. \nDr.Sezer told me quite a bit about projects like Deep Therapy, WeScan and FairFake and it somehow seemed like 3DUniversum would be the perfect place for me to go next. I do think that I bring something different to the table in the domains that your team works in, and I hope you give me a chance.\nMy Github link is : github.com/SubhadityaMukherjee and I have attached my resume to this email as well.\nBirds.ai\ninfo@birds.ai\nHello!\nMy name is Subhaditya. I recently graduated with a masters in AI from the RUG (Groningen), and am looking for my next big challenge. I found out about Birds.ai from the NLAIC website and found some of the projects your team has done (like the CV-based solar panel inspection project).  The work your team does is very much in line with my interests and experience, and I would love to discuss the possibility of any such roles at Birds.ai.\nMy experience is a combination of Computer Vision and Data Analytics. Over the past few years, I have worked with many clients (both freelance/part-time and in internships) and helped them bring their AI projects to life. I am familiar with deep learning libraries like PyTorch and TensorFlow. I have worked with drone image segmentation, vision-based analysis for parts of the manufacturing pipeline and similar projects. \nAdd in the consultancy work your team does, and somehow, it seems to be exactly what I want to do as the next step in my career. I do think that my skills fit the kind of projects Birds.ai does and that I bring something different to the table. \nWhile I am not sure what kind of roles exist at the moment, I am open to any AI/Data Science related position. I’ve also attached my resume to this email as a little more background if that helps. \nBright Cape\nOnno Hofstee, o.hofstee@brightcape.nl  \nHello Onno!\nMy name is Subhaditya, I recently graduated with a masters in AI from the RUG (Groningen), and am looking for my next big challenge. I found out about BrightCape from the NLAIC website and the first thing that caught my eye was SARA, the health assistant. It seems like your team works on some very interesting projects and I would love to discuss the possibility of any AI/Data Analytics roles at BrightCape. \nMy experience is a combination of AI and Data Analytics. Over the past few years, I have worked with many clients (both freelance/part-time and in internships) and helped them bring their AI projects to life. I am familiar with deep learning libraries like PyTorch and TensorFlow along with the stack required to build Data Analysis pipelines and provide clients with the advice they need.\nAdd in the consultancy work your team does, and somehow, it seems to be a nice fit with both what I can offer, and what I want to do as a next step. I do think that my skills fit the kind of projects BrightCape does and that I bring something different to the table. \nWhile I am not sure what kind of roles exist at the moment, I am open to any AI/Data Science related position. I’ve also attached my resume to this email as a little more background if that helps. If you are hiring or will be soon, I would love to have a chat!\nPS: I am already excited about a potential ski trip. :)\nKepler Vision , Internship\ninfo@keplervision.eu\nHello!\nMy name is Subhaditya, I recently graduated with a masters in AI from the RUG (Groningen), and am looking for my next big challenge. I found out about Kepler Vision from the NLAIC website, and saw that your team was looking for a software engineering intern. The Night Nurse software seems like a great project to start my career with and this email is my application for this position. I hope to get to learn a lot about how to be a good part of an engineering team, while also bringing my AI experience to the table.\nWhile I don’t know what criteria you have in selecting an intern, this opportunity would be the absolute best next step for me. My main interests lie in Computer Vision, especially in using AI in healthcare. And somehow, this is exactly what this internship offers to teach? It would be such a miss if I did not try my best to get in.\nMy experience is a combination of AI and Computer Vision. Over the past few years, I have worked with many clients (both freelance/part-time and in internships) and helped them bring their AI projects to life. But, what I now need to learn is how to work on larger scale projects such as Night Nurse. I am quite good with Python, FastAPI, and AI frameworks like PyTorch and Tensorflow, but am not very familiar with Terraform or Typescript. I definitely want to learn them though! And this internship would be absolutely perfect.\nI promise to put my best foot forward given a chance. I am quite a fast learning, and have a strong base in programming and AI. I hope you give me a shot. \nPS. I have the Zuikjaar visa as well, valid for almost a year. So no issues there either.\nGrwnxt\ncoen.hilbrands@grwnxt.com , info@grwnxt.com \nHello Coen!\nMy name is Subhaditya, I recently graduated with a masters in AI from the RUG (Groningen), and am looking for my next big challenge. I found out about GrwNxt from the NLAIC website and was intrigued by the concept of GrwNxt Modules and how they could be used for reliably, combining Deep learning and plant science. I can imagine just how useful and scalable the technology would be, especially combined with AI and computer vision. The work your team does is very much in line with my interests and experience, and I would love to discuss the possibility of any such roles at GrwNxt.\nMy experience is a combination of AI and Computer Vision, which is something I see that your team works on as well. Over the past few years, I have worked with many clients (both freelance/part-time and in internships) and helped them bring their AI projects to life. Now that I have a masters in AI, I’m trying to find places I can contribute the most to, and I think GrwNxt fits quite well to that. From the presentation on your website, I see that the modules would use a lot of advanced data analytics combined with computer vision, which is what I am the most familiar with. I am proficient at using Deep learning libraries, such as PyTorch and Tensorflow and am comfortable with all stages of the AI pipeline. Since my main focus has always been Computer vision, I can help your team implement the advanced data analysis pipelines faster as well. \nThis project really interests me and while I am not sure what kind of roles you have at the moment, I am open to any AI/Data Science related position. Being able to contribute to the sustainable food revolution is such a good cause, and I would love to be a part of it.\nI’ve also attached my resume to this email as a little more background if that helps. If you are hiring or will be soon, I would love to have a chat!\nMindAffect\ncareers@mindaffect.nl"},"KB/NLVR2-3":{"title":"NLVR2","links":[],"tags":["dataset"],"content":"NLVR2"},"KB/NUMA":{"title":"NUMA","links":["KB/SMP","KB/Cache-Coherence"],"tags":["parallelcomputing"],"content":"NUMA\n\nShared memory\nOften made by physically linking two or more SMP\nOne SMP can directly access memory of another SMP\nNot all processors have equal access time to all memories\nMemory access across link is slower\nCache Coherence\n"},"KB/NaN-Trap":{"title":"NaN Trap","links":[],"tags":["temp"],"content":"NaN Trap\n\nWhen one number in your model becomes a NaN during training, which causes many or all other numbers in your model to eventually become a NaN.\n"},"KB/Names-of-individuals":{"title":"Names of individuals","links":[],"tags":["language"],"content":"Names of Individuals\n\nDan went to the movies.\nDan should be understood to be some person named Dan. Although there are many, the speaker had one particular one in mind and the discourse context should tell us which.\n"},"KB/Nasnet":{"title":"Nasnet","links":["KB/Basic-RNN-Architectures"],"tags":["architecture"],"content":"Nasnet\n\nNeural Architecture Search\nController RNN (Basic RNN Architectures) produces architectures and evaluated until convergence\n"},"KB/Nativists":{"title":"Nativists","links":["KB/Verb"],"tags":["language"],"content":"Nativists\n\nnoun and Verb and determiner phrase and various sentence constituents are not only real mental primitives, but are innately given inherently linguistic primitives of the mind.\n"},"KB/Nearest-Neighbor-Retrieval":{"title":"Nearest Neighbor Retrieval","links":[],"tags":["semisupervisedlearning"],"content":"Nearest Neighbor Retrieval\n\nimages with similar appearance usually are closer in the feature space\nThe nearest neighbor method is used to find the top K nearest neighbors from the feature space of the features learned by the self-supervised learned model [40], [41], [43]\n"},"KB/Nebulizer":{"title":"Nebulizer","links":[],"tags":["medical"],"content":"Nebulizer\n\nA device used to deliver medication in an aerosol form through inhalation\n"},"KB/Negative-Log-Likelihood":{"title":"Negative Log Likelihood","links":["KB/Log-Likelihood-Loss"],"tags":["loss"],"content":"Negative Log Likelihood\n\nClassification, Smaller quicker training, Simple tasks.\n\n - \\mathrm{sum}\\left( \\log\\left( y \\right) \\right)\n\n\nLog Likelihood Loss\n"},"KB/Negative-Sampling":{"title":"Negative Sampling","links":[],"tags":["nlp"],"content":"Negative Sampling\n\nintroduce samples of words that are not neighbors\n\n\n"},"KB/Negative-Set-Bias":{"title":"Negative Set Bias","links":[],"tags":["ethics"],"content":"Negative Set Bias\n\nDatasets define a visual phenomenon (e.g. object, scene, event) not just by what it is (positive instances), but also by what it is not (negative instances)\nthe space of all possible negatives in the visual world is astronomically large, so datasets are forced to rely on only a small sample\nImageNet benefits from a large variability of negative examples and does not seem to be affected by a new external negative set, whereas Caltech and MSRC appear to be just too easy\nUnfortunately, it’s not at all easy to stress-test the sufficiency of a negative set in the general case since it will require huge amounts of labelled (and unbiased) negative data.\nOne remedy, proposed in this paper, is to add negatives from other datasets\nAnother approach, suggested by Mark Everingham, is to use a few standard algorithms (e.g. bag of words) to actively mine hard negatives as part of dataset construction from a very large unlabelled set, and then manually going through them to weed out true positives. The down side is that the resulting dataset will be biased against existing algorithms.\n"},"KB/Nesterov-Momentum":{"title":"Nesterov Momentum","links":[],"tags":["gradients"],"content":"Nesterov Momentum\n\n\n\n\n\n&amp;v_{t}= \\gamma v_{t+1}+\\eta \\cdot \\nabla_{\\theta}J(\\theta - \\gamma v_{t-1}) \\\n&amp;\\theta = \\theta- v_{t}\\\n\\end{align}$$"},"KB/Network-Dissection-Quantifying-Interpretability-of-Deep-Visual-Representions":{"title":"Network Dissection Quantifying Interpretability of Deep Visual Representions","links":["KB/Dropout","KB/Broden"],"tags":["explainability"],"content":"Network Dissection Quantifying Interpretability of Deep Visual Representions\n\n\nDavid Bau∗, Bolei Zhou∗, Aditya Khosla, Aude Oliva, and Antonio Torralba\n\n\nQuantifying the interpretability of latent representations of CNNs by evaluating the alignment between individual hidden units and a set of semantic concepts\n\n\nscore the semantics of hidden units at each intermediate convolutional layer.\n\n\nThe units with semantics are given labels across a range of objects, parts, scenes, textures, materials, and colors.\n\n\ninterpretability of units is equivalent to random linear combinations of units\n\n\nanalyze the effect of training iterations, compare networks trained with different initializations, examine the impact of network depth and width, and measure the effect of Dropout and batch normalization on the interpretability of deep visual representations\n\n\nIntroduction\n\nThe emergence of interpretable structure suggests that deep networks may be learning disentangled representations spontaneously.\nA disentangled representation aligns its variables with a meaningful factorization of the underlying problem structure\nBroden\n\nNetwork Dissection\n\nOur measurement of interpretability for deep visual representations proceeds in three steps: 1. Identify a broad set of human-labeled visual concepts. 2. Gather hidden variables’ response to known concepts. 3. Quantify alignment of hidden variableconcept pairs.\nIn a fully interpretable local coding such as a one-hotencoding, each variable will match exactly with one humaninterpretable concept.\nTherefore we measure the alignment between single units and single interpretable concepts\nThis does not gauge the discriminative power of the representation; rather it quantifies its disentangled interpretability.\nWe then measure the alignment of each hidden unit of the CNN with each concept by evaluating the feature activation of each individual unit as a segmentation model for each concept\nTo quantify the interpretability of a layer as a whole, we count the number of distinct visual concepts that are aligned with a unit in the layer\nBroden\n\nScoring Unit Interpretability\n\nevaluates every individual convolutional unit in a CNN as a solution to a binary segmentation task to every visual concept in Broden\napplied to any CNN using a forward pass without the need for training or backpropagation.\nFor every input image x in the Broden dataset, the activation map A_{k}(x) of every internal convolutional unit k is collected.\nThen the distribution of individual unit activations ak is computed\nFor each unit k, the top quantile level T_{k} is determined such that P(a_{k} &gt; T_{k} = 0.005 over every spatial location of the activation map in the data set.\ninput-resolution annotation mask L_{c} for some concept c\nthe activation map is scaled up to the mask resolution S_{k}(x) from A_{k}(x) using bilinear interpolation, anchoring interpolants at the center of each unit’s receptive field\nS_{k}(x) is then thresholded into a binary segmentation: M_{k}(x) \\equiv S_{k}(x) \\leq T_{k}, selecting all regions for which the activation exceeds the threshold Tk. These segmentations are evaluated against every concept c in the data set by computing intersections M_{k}(x) \\cap L_{c}(x), for every (k, c) pair.\nThe score of each unit k as segmentation for concept c is reported as a data-set-wide intersection over union score\nI_{o}U_{k,c}=\\frac{\\Sigma|M_{k}(x) \\cap L_{c}(x|}{\\Sigma|M_{k}(x) \\cup L_{c}(x)|}\nwhere | · | is the cardinality of a set.\nThe value of IoU_{k,c} is the accuracy of unit k in detecting concept c; we consider one unit k as a detector for concept c if IoUk,c exceeds a threshold\nOur qualitative results are insensitive to the IoU threshold: different thresholds denote different numbers of units as concept detectors\nFor our comparisons we report a detector if IoUk,c &gt; 0.04.\none unit might be the detector for multiple concepts; for the purpose of our analysis, we choose the top ranked label\nThe IoU evaluating the quality of the segmentation of a unit is an objective confidence score for interpretability that is comparable across networks\nNote that network dissection works only as well as the underlying data set\nWe conclude that interpretability is neither an inevitable result of discriminative power, nor is it a prerequisite to discriminative power.\nInstead, we find that interpretability is a different quality that must be measured separately to be understood.\n\nMeasure of Axis Aligned Interpretability\n\n\n\n\nDisentangled Concepts by Layer\n\nConfirming intuition, color and texture concepts dominate at lower layers conv1 and conv2 while more object and part detectors emerge in conv5.\n\nNetwork Architectures and Supervisions\n\nIn terms of network architecture, we find that interpretability of ResNet &gt; VGG &gt; GoogLeNet &gt; AlexNet\nDeeper architectures appear to allow greater interpretability. Places &gt; ImageNet.\nSelf-supervised models create many texture detectors but relatively few object detectors; apparently, supervision from a self-taught primary task is much weaker at inferring interpretable concepts than supervised training on a large annotated data set\nThe form of self-supervision makes a difference: for example, the colorization model is trained on colorless images, and almost no color detection units emerge\nWe hypothesize that emergent units represent concepts required to solve the primary task.\n\nTraining Conditions Vs. Interpretability\n\nWe can see that object detectors and part detectors begin emerging at about 10,000 iterations (each iteration processes a batch of 256 images)\nWe do not find evidence of transitions across different concept categories during training\nFor example, units in conv5 do not turn into texture or material detectors before becoming object or part detectors.\nComparing different random initializations, the models converge to similar levels of interpretability, both in terms of the unique detector number and the total detector number; this matches observations of convergent learning\nFor the network without Dropout, more texture detectors emerge but fewer object detectors\nBatch normalization seems to decrease interpretability significantly.\nThe batch normalization result serves as a caution that discriminative power is not the only property of a representation that should be measured.\nbatch normalization ‘whitens’ the activation at each layer, which smooths out scaling issues and allows a network to easily rotate axes of intermediate representations during training\nWhile whitening apparently speeds training, it may also have an effect similar to random rotations analyzed in Sec. 3.2 which destroy interpretability\ninterpretability is neither a prerequisite nor an obstacle to discriminative power\n\n\n\n\nDiscrimination Vs. Interpretability\n\nFor each trained model, we extract the representation at the highest convolutional layer, and train a linear SVM with C = 0.001 on the training data for action40 action recognition task\nThus the supervision tasks that encourage the emergence of more concept detectors may also improve the discrimination ability of deep features.\naccuracy on a representation when applied to a task is dependent not only on the number of concept detectors in the representation, but on the suitability of the set of represented concepts to the transfer task.\n\n\nLayer Width Vs. Interpretability\n\nDepth has been shown to be important to high discrimination ability\nincreasing the number of convolutional units at a layer significantly increases computational cost while yielding only marginal improvements in classification accuracy\ncarefully designed wide residual network can achieve classification accuracy superior to the commonly used thin and deep counterparts.\nThis may indicate a limit on the capacity of AlexNet to separate explanatory factors; or it may indicate that a limit on the number of disentangled concepts that are helpful to solve the primary task of scene classification.\n\n"},"KB/Neural-Augmentation":{"title":"Neural Augmentation","links":[],"tags":["augmentation"],"content":"Neural Augmentation\n\nThe Neural Style Transfer algorithm requires two parameters for the weights of the style and content loss. Perez and Wang presented an algorithm to meta-learn a Neural Style Transfer strategy called Neural Augmentation.\ne Neural Augmentation approach takes in two random images from the same class. The prepended augmentation net maps them into a new image through a CNN with 5 layers, each with 16 channels, 3×3 filters, and ReLU activation functions. The image outputted from the augmentation is then transformed with another random image via Neural Style Transfer.\nThis style transfer is carried out via the CycleGAN extension of the GAN framework\nThese images are then fed into a classification model and the error from the classification model is backpropagated to update the Neural Augmentation net.\nThe Neural Augmentation network uses this error to learn the optimal weighting for content and style images between different images as well as the mapping between images in the CNN\nThe Neural Augmentation techniques tested consist of three levels based on the design of the loss function for the augmentation net (Content loss, Style loss via gram matrix, and no loss computer at this layer)\n"},"KB/Neural-Chimera":{"title":"Neural Chimera","links":["KB/Chimera","KB/Stem-Cells"],"tags":["brain"],"content":"Neural Chimera\n\nA research model where human Stem Cells are transplanted into an animal embryo to follow the genetic, molecular, and functional processes of brain cells as they grow.\n"},"KB/Neural-Dynamics":{"title":"Neural Dynamics","links":["KB/Continous-->-Discrete"],"tags":["temp"],"content":"Neural Dynamics\n\nContinous → Discrete seq of words\nUse NN to generate hypothesis outputs vectors\n\nAs many components as possible target symbols\n\n\n"},"KB/Neural-Induction":{"title":"Neural Induction","links":[],"tags":["brain"],"content":"Neural Induction\n\nA developmental process where ectodermal cells “decide” to form the neural plate, the basis of what will eventually become the organism’s nervous system.\n"},"KB/Neural-Network-Architecture-Cheat-Sheet":{"title":"Neural Network Architecture Cheat Sheet","links":["KB/Spiking-Networks","Hidden-Models","KB/Capsule-Network","KB/Probability","KB/Recurrent","KB/Conv"],"tags":["architecture"],"content":"Neural Network Architecture Cheat Sheet\n\nSpiking Networks\nHidden Models\nCapsule Network\nProbability\nRecurrent\nConv\n\n"},"KB/Neural-Probabilistic-Model":{"title":"Neural Probabilistic Model","links":["KB/Probability","KB/Curse-Of-Dimensionality","n-gram"],"tags":["architecture"],"content":"Neural Probabilistic Model\n\nA Neural Probabilistic Language Model\nmore compact and smoother representations based on distributed representations that can accommodate far more conditioning variables\nlearning the joint Probability function of sequences of words in a language was intrinsically difficult because of the Curse Of Dimensionality\nlearning a distributed representation for words which allows each training sentence to inform the model about an exponential/combinatorial number of semantically neighboring sentences\nThe model learns simultaneously (i) a distributed representation for each word along with (ii) the Probability function for word sequences, expressed in terms of these representations\nGeneralization is obtained because a sequence of words that has never been seen before gets high Probability if it is made of words that are similar\nsignificantly improves on state-of-the-art n gram models\n"},"KB/Neural-Radiance-Field":{"title":"Neural Radiance Field","links":["KB/Density"],"tags":["robotics"],"content":"Neural Radiance Field\n\nNeRF: Representing Scenes As Neural Radiance Fields for View Synthesis\n\nsynthesizing novel views of complex scenes\noptimizing an underlying continuous volumetric scene function using a sparse set of input views\nsingle continuous 5D coordinate (spatial location (x,y,z) and viewing direction (θ,ϕ))\noutput is the volume Density and view-dependent emitted radiance at that spatial location\nquerying 5D coordinates along camera rays\nvolume rendering techniques to project the output colors and densities into an image\nvolume rendering is naturally differentiable\nset of images with known camera poses\nThey describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes\n\n\n"},"KB/Neural-Text-Degeneration":{"title":"Neural Text Degeneration","links":["KB/Nucleus-Sampling"],"tags":["architecture"],"content":"Neural Text Degeneration\n\nThe Curious Case of Neural Text Degeneration\ndeep analysis into the properties of the most common decoding methods for open-ended language generation\nsurprising distributional differences between human text and machine text\ndecoding strategies alone can dramatically effect the quality of machine text, even when generated from exactly the same neural language model\nlikelihood maximizing decoding causes repetition and overly generic language usage\nsampling methods without truncation risk sampling from the low-confidence tail of a model’s predicted distribution\nNucleus Sampling\n"},"KB/Neuroaesthetics":{"title":"Neuroaesthetics","links":[],"tags":["brain"],"content":"Neuroaesthetics\n\nA field within cognitive neuroscience that examines the neural underpinnings of what humans find visually appealing or beautiful.\n"},"KB/Neurogenesis":{"title":"Neurogenesis","links":["KB/Gyrus"],"tags":["brain"],"content":"Neurogenesis\n\nThe production of new, maturing neurons by neural stem and progenitor cells. Rapid and widespread neurogenesis obviously occurs in the fetal brain in humans and other animals, but neuroscientists long believed that neurogenesis essentially does not occur in the adult human brain.\nHowever, over the past two decades, research has shown that it does in fact occur in the dentate Gyrus of the hippocampus and possibly other brain regions. This “adult neurogenesis” appears to be vital for normal learning and memory, and may help protect the brain against stress and depression.\n"},"KB/Neuroplasticity":{"title":"Neuroplasticity","links":[],"tags":["brain"],"content":"Neuroplasticity\n\nAlso referred to as brain plasticity or neural plasticity, this is the ability of the brain to change throughout the lifespan, forming new synapses and neural connections in response to the environment.\n"},"KB/Newtons-Laws":{"title":"Newtons Laws","links":["KB/Inertia","KB/Force","KB/Equal-And-Opposite-Force-Pairs"],"tags":["physics"],"content":"Newtons Laws\n\nInertia\nForce\nEqual And Opposite Force Pairs\n"},"KB/No-bias-decay":{"title":"No bias decay","links":["KB/Learning-Rate-Decay-tricks","KB/Lp-Regularization","KB/Regularization","KB/Batch-Normalization","KB/Layers","LARS"],"tags":["regularization"],"content":"No Bias Decay\n\nNo Learning Rate Decay tricks\nEquivalent to Lp Regularization L2 to all parameters to drive the values towards 0\nOnly apply Regularization to the weights\nLeave Batch Normalization Layers alone\nLARS\n"},"KB/Node-Distribution":{"title":"Node Distribution","links":[],"tags":["distributions","graph"],"content":"Node Distribution\n-"},"KB/Node-Level-Tasks":{"title":"Node Level Tasks","links":[],"tags":["graph"],"content":"Node Level Tasks\n \nNode Level Tasks\n\nnetwork assigns a label (classification) or one or more values (regression) to each node of the graph, using both the graph structure and node embeddings\n\n"},"KB/Node-Link-Diagram":{"title":"Node LInk Diagram","links":["KB/Cross-Minimization","KB/Bend-Minimization","KB/Area-Minimization","KB/Cross-angle-Maximization","KB/Length-Optimization","KB/Symmetries-Node-Link","KB/Node-Distribution","KB/Force-Directed-Graph-Layout","KB/Hierarchical-Edge-Bundling"],"tags":["visualization","graph"],"content":"Node LInk Diagram\n\nVertices (Nodes) are mapped to graphical shapes circles, squares, triangles, etc.\nEdges (Links) are mapped to straight or curved lines\nNodes can freely be positioned\nCross Minimization\nBend Minimization\nArea Minimization\nCross angle Maximization\nLength Optimization\nSymmetries Node Link\nNode Distribution\nForce Directed Graph Layout\nHierarchical Edge Bundling\n"},"KB/Noise-Injection":{"title":"Noise Injection","links":[],"tags":["augmentation"],"content":"Noise Injection\n\ninjecting a matrix of random values usually drawn from a Gaussian distribution\nAdding noise to images can help CNNs learn more robust features.\n"},"KB/Noise-Suppression":{"title":"Noise Suppression","links":["KB/medical","KB/Conv-Based-Noise-Reduction","KB/Average-Filter","KB/Gaussian-Filter","KB/Mesh-Smoothing","KB/Laplacian-Grid-Smoothing"],"tags":["visualization"],"content":"Noise Suppression\n\nreduce the intensity variation in big structures\n(such as organs in medical imaging data)\n-improve the detectability of edges between big structures,\npreserve small scale structures\n- Conv Based Noise Reduction\nAverage Filter\nGaussian Filter\nMesh Smoothing\nLaplacian Grid Smoothing\n"},"KB/Noise-Tunnel":{"title":"Noise Tunnel","links":["KB/Smooth-Grad","KB/VarGrad"],"tags":["explainability"],"content":"Noise Tunnel\n\n@kokhlikyanCaptumUnifiedGeneric2020\n\nSummary\n\nCombines [SmoothGrad Square] + [Smooth-Grad] + [VarGrad](SmoothGrad Square] + [Smooth-Grad] + [VarGrad.md)\nnot an attribution method\n\nUsing Smooth-Grad\n\ntechnique that improves the accuracy of attribution methods\nproblem with ReLU activation function and gradients producing noisy, often irrelevant attributions\nthe partial derivative \\frac{\\partial F_c}{\\partial x_i} of the models’ score F_c​ for a class c with respect to the value of the pixel x_{i} fluctuates\nadding a Gaussian noise \\mathcal {N}(0, 0.01^2) and calculating an average of sampled attributions is going to solve the problem\ncalculates the attribution M_{c} using any available method by providing that method an input with Gaussian noise\ncalculates a mean value from all the samples to reduce the importance of less frequent attributions\nwhen adding noise to the input image, important attributions are going to be visible most of the time, and noise might change between attributions\n\\hat M_{c}(x) = \\frac{1}{n}\\Sigma_{1}^{n}M_{c}(x + \\mathcal{N}(0, \\sigma^{2}))\n\nUsing [SmoothGrad Square](SmoothGrad Square.md)\n\nchanges only the way that the mean value is calculated by using the mean of squared attributions instead of just attributions\nless noisy results\nbut often removes less important features, which are still valid features\n\\hat M_{c}(x) = \\frac{1}{n}\\Sigma_{1}^{n}\\sqrt{M_{c}(x + \\mathcal{N}(0, \\sigma^{2}))}\n\nUsing VarGrad\n\nvariance version of the SmoothGrad\nUsing SmoothGrad (Noise Tunnel) seems to detect more edges of the input image (in comparison with pure IG attribution in [Fig. 1b]), and that can be interpreted as detecting decision boundary. SmoothGrad-Square (Noise Tunnel) and VarGrad (Noise Tunnel) are removing a large amount of noise but usually also some of the important features visible on the attribution from SmoothGrad\n\\hat M_{c}(x) = \\frac{1}{n}\\Sigma_{k=1}^{n}\\{M_{c}(x + \\mathcal{N}(0, \\sigma^{2}))\\}^{2}- \\{\\hat M_{c}(x)\\}^{2}\n\nDrawbacks\n\nEven if the Noise Tunnel method improves the accuracy of the XAI methods it adds a large amount of computational overhead\nEvery sample generated by the method requires the rerun of the whole XAI method\nThat is a linear increase of computation and to make the method efficient you should use at least 5 generated noise samples\n\nImages\n\n\n"},"KB/Noisy-Relu":{"title":"Noisy Relu","links":["KB/Relu"],"tags":["architecture"],"content":"Noisy Relu\n\nf(x) = max(0, x+Y)  where Y\\in Normal(0,1)\n"},"KB/Non-Maxima-Supression":{"title":"Non Maxima Supression","links":[],"tags":["architecture"],"content":"Non Maxima Supression\n \n\n\nIntersection Over Union(IoU) = (Target ∩ Prediction) / (Target U Prediction)\nIn our case using BBoxes, it can be modified to,\n\nIOU(Box1, Box2) = Intersection_Size(Box1, Box2) / Union_Size(Box1, Box2)"},"KB/Non-Relational-Inductive-Bias":{"title":"Non Relational Inductive Bias","links":["KB/Relational-Inductive-Bias","KB/Activation-Functions","KB/Dropout","Weight-Decay","KB/Batch-Normalization","KB/Layer-Normalization","KB/Instance-Normalization","Covariate-Shift","Augmentation","KB/Optimizers"],"tags":["temp"],"content":"Non Relational Inductive Bias\n\nActivation Functions\n\nallow the model to capture the non-linearity hidden in the data\n\n\nDropout\n\nhelps the network avoid memorizing the data by forcing random subsets of the network to each learn the data pattern. As a result, the obtained model, in the end, is able to generalize better\n\n\nWeight Decay\n\nputs constraints on the model’s weights\n\n\nBatch Normalization , Layer Normalization , Instance Normalization\n\nReduces Covariate Shift\n\n\nAugmentation\nOptimizers\n"},"KB/Non-adjacent-dependencies":{"title":"Non-adjacent dependencies","links":["KB/Wh-dependencies","KB/Extra-position","KB/Object-relative-clauses","KB/Subject-relative","KB/Subject-verb-agreement"],"tags":["language"],"content":"Non-adjacent Dependencies\n\nWh-dependencies\nExtra-position\nObject-relative clauses\nSubject relative\nSubject-verb agreement\n"},"KB/Non-response-Bias":{"title":"Non-response Bias","links":["KB/Participation-Bias"],"tags":["temp"],"content":"Non-response Bias\n\n(also called Participation Bias)\nUsers from certain groups opt-out of surveys at different rates than users from other groups.\n"},"KB/Nonstationarity":{"title":"Nonstationarity","links":[],"tags":["temp"],"content":"Nonstationarity\n\nA feature whose values change across one or more dimensions, usually time. For example, the number of swimsuits sold at a particular store demonstrates nonstationarity because that number varies with the season. As a second example, the quantity of a particular fruit harvested in a particular region typically shows sharp nonstationarity over time.\n"},"KB/Nootropics":{"title":"Nootropics","links":["KB/Attention"],"tags":["brain"],"content":"Nootropics\n\nDrugs or supplements that are marketed as ways to improve cognitive functions like memory, Attention, or creativity.\n"},"KB/Normal-Distribution":{"title":"Normal Distribution","links":["KB/Central-Limit-Theorem","KB/Density"],"tags":["distributions"],"content":"Normal Distribution\n\nPr(y|\\mu, \\sigma^{2}) = \\frac{1}{\\sqrt{2\\pi\\sigma}}e^{-\\frac{(y-\\mu)^{2}}{2\\sigma^{2}}}\nMean \\mu and std \\sigma. \\mu is max and \\mu \\pm \\sigma is locations of zeros of second derivative\n\n\\mathcal{N}(0,1)\nCentral Limit Theorem\n\nProperties\n\nLinear combinations of normal distributed independant RVs are normal distributed\nX,Y have means \\mu and v and variances \\sigma^{2} and \\tau^{2}. Then aX + bY is normally distributed and has mean : a\\mu + bv and variance \\alpha^{2}\\sigma^{2}+b^{2}\\tau^{2}\n\nComputing the Value\n\n\\int_{a}^{b} \\frac{1}{\\sqrt{2\\pi\\sigma}}e^{-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}}dx\nTransform \\mathscr{N}(\\mu, \\sigma^{2}) to \\mathscr{N}(0,1)\nZ = \\frac{X-\\mu}{\\sigma}\n\\int_{\\frac{a-\\mu}{\\sigma}}^{\\frac{b-\\mu}{\\sigma}}\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{(x)^{2}}{2}}dx\nCompute by using Cumulative Density function \\phi\nIterative solvers\n\\phi(\\frac{b-\\mu}{\\sigma})-\\phi(\\frac{a-\\mu}{\\sigma})\n\\hat \\mu = \\frac{1}{N}\\Sigma_{i}(x_{i}) \\hat \\sigma^{2}= \\frac{1}{N-1}\\Sigma_{i}(x_{i}-\\hat\\mu)^2\n"},"KB/Normalization":{"title":"Normalization","links":[],"tags":["normalization"],"content":"Normalization\n \n\n\n"},"KB/Normalized-Inverted-Structural-Similarity-Index":{"title":"Normalized Inverted Structural Similarity Index","links":["KB/Structural-Similarity-Index"],"tags":["explainability"],"content":"Normalized Inverted Structural Similarity Index\n\nmetric is calculated from Structural Similarity Index by inverting the range and then normalizing it.\nNISSIM bounds to (0, 1] where 0 means similar and 1 means dissimilar. Ideally we want this value as close to 0 as possible\nNISSIM_{i}= \\frac{1-SSIM_{i}}{2}\n"},"KB/Norms-as-a-basis-for-governing-sociotechnical-systems":{"title":"Norms as a basis for governing sociotechnical systems","links":[],"tags":["ethics"],"content":"Norms as a Basis for Governing Sociotechnical Systems\n\nMunindar P. Singh\nframework that uses social norms to govern autonomous entities’ (e.g., AI agents’ or human beings’) behaviours\ninherently distributed rather than relying on a central authority\nIndividuals maintain their autonomy through executing their own decision policies, but are subjected to social norms defined by the collective through roles\nSocial norms are defined through a template containing codified commitment, authorization, prohibition, sanction and power\n"},"KB/Notch-filter":{"title":"Notch filter","links":[],"tags":["visualization"],"content":"Notch Filter\n\nA notch filter is a type of band-stop filter, which is a filter that attenuates frequencies within a specific range while passing all other frequencies unaltered\n"},"KB/Nucleotide-Sequence":{"title":"Nucleotide Sequence","links":["KB/Nucleotide","KB/Allele"],"tags":["brain"],"content":"Nucleotide Sequence\n\nA specific and ordered array of nucleotides that make up a specific genetic variant or Allele.\n"},"KB/Nucleotide":{"title":"Nucleotide","links":[],"tags":["brain"],"content":"Nucleotide\n\nSometimes referred to as a nucleic acid, these are the biological building blocks of DNA.\n"},"KB/Nucleus-Accumbens":{"title":"Nucleus Accumbens","links":["KB/Mesolimbic-Pathway"],"tags":["brain"],"content":"Nucleus Accumbens\n\nPart of the brain’s reward circuitry, or Mesolimbic Pathway, this small region in the midbrain releases dopamine in response to rewarding experiences.\n"},"KB/Nucleus-Sampling":{"title":"Nucleus Sampling","links":["KB/Probability"],"tags":["distributions"],"content":"Nucleus Sampling\n\nNucleus (or top-p) Sampling, a simple but effective method that captures the region of confidence of language models effectively to draw the best out of neural generation\nBy sampling text from the dynamic nucleus of the Probability distribution, which allows for diversity while effectively truncating the less reliable tail of the distribution, the resulting text better demonstrates the quality of human text, yielding enhanced diversity without sacrificing fluency and coherence.\n"},"KB/Numerically-Quantified-Expressions":{"title":"Numerically Quantified Expressions","links":[],"tags":["language"],"content":"Numerically Quantified Expressions\n\nNQEs express\n\nPlurality\nCardinality\n\n\nCan be in scopal relations with other expressions\n"},"KB/OPT":{"title":"OPT","links":["KB/GPT"],"tags":["architecture"],"content":"OPT\n\nOPT: Open Pre-trained Transformer Language Models\nLarge language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning\ncollection of auto-regressive/decoder-only pre-trained transformer-based language models ranging in size from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers\nreplicate the performance and sizes of the GPT-3 class of models, while also applying the latest best practices in data curation and training efficiency\nOPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop\n"},"KB/OSIE":{"title":"OSIE","links":[],"tags":["dataset"],"content":"OSIE\n\n700 natural indoor and outdoor scenes, aesthetic photographs from Flickr and Google.\nThe fixations were measured while 15 observers looked at the image for 3 s.\nthe fixations for all observers were collected and blurred using the [Gaussian filter](Gaussian filter.md) with the standard deviation equivalent to 1° in the visual angle\n"},"KB/Object-Detection":{"title":"Object Detection","links":[],"tags":["semisupervisedlearning"],"content":"Object Detection\n\nlocalizing the position of objects in images and recognizing the category of the objects\nMSCOCO [99] and OpenImage [14]\nWhen using object detection as downstream task to evaluate the quality of the self-supervised image features, networks that trained with the pretext task on unlabeled large data are served as the pre-trained model for the Fast-RCNN [2] and then fine-tuned on object detection datasets, then the performance on the object detection task is evaluated to demonstrate the generalization ability of self- supervised learned features.\n"},"KB/Object-relative-clauses":{"title":"Object-relative clauses","links":[],"tags":["language"],"content":"Object-relative Clauses\n\n[The report] that the senator attacked [] admitted the error.\n[The senator] that the report attacked [] admitted the error.\n"},"KB/Oblique-Slicing":{"title":"Oblique Slicing","links":[],"tags":["visualization"],"content":"Oblique Slicing\n\nResample the data on arbitrarily oriented slices\nExploit 3D texture mapping functionality\nStore volume in 3D texture\n"},"KB/Obsidian-tutorial":{"title":"Obsidian tutorial","links":["KB/SmoothMix","pdf_files/3544548.3581388.pdf","KB/Docker-Cheatsheet"],"tags":["scientificresearch"],"content":"Obsidian Tutorial\nHello There\n\n\n\nCreating a File\n\nMarkdown basics\n\nPaste into links\nImages\n\nDrag and Drop vs Paste\n\n\nHeadings\n\n\nBullets\nand numbers\n\n\nCode\nimport numpy as np\ns = np.array([1])\nprint(s)\t\t```\n- Tables\n  - (Cmd/Control P -&gt; insert table)\n\n\n\nLinking files (double bracket)\nZen Mode (Cmd/Control P → search)\n\nOrganization\n\nTags - SmoothMix\nTemplates - [toc: true\ntitler](templates/toc: true\ntitler.md)\nFolders\n\nNavigation\n\nLinks and Backlinks (GUI)\nForward and Backward (Cmd/Control + Option + Left/Right)\nAuto TOC\n\nGeneral Tips\n\nSearch (Cmd/Control P)\nUsing the GUI is perfectly fine\n\nTaking Notes from Elsewhere\n\nWebsites : Roam highlighter\nPDF annotation : 3544548.3581388\n\nZotero\n\nBetter BibTex\n\nExport library → Better CSL/JSON\nCheck keep updated and background export\n\n\nPlugin : Citations\n\nEnter path of better CSL/JSON\n\n\n(Cmd/Control + Shift + M)\n\nAdvanced Options that Are Super Useful\n\n\nQuick Latex\n\n\nma=\\frac{\\frac{3\\lambda}{4}+34\\epsilon}{10}\n\n\n\\begin{equation}a+b\\end{equation}\n\n\nLocal Graph View (No shortcut : Cmd + Shift + G)\n\nTime stamper (Cmd + T)\n\n\n\nLinting (Cmd + S)\n\n\nFile Preview (hold Cmd/Control) Docker Cheatsheet\n\n\nNote Refactoring\n\n\nExport to PDF\n\n\nExtra Resources if You Care\n\nObsidian plugins for research\nObsidian daily notes\nBrowser extensions that are useful\npdfannots\n"},"KB/Occipital-lobe":{"title":"Occipital lobe","links":[],"tags":["brain"],"content":"Occipital Lobe\n\nInterprets vision (color, light, movement)\n"},"KB/Occlusion":{"title":"Occlusion","links":[],"tags":["robotics"],"content":"Occlusion\n\noccurs if a target object is hidden (occluded) by other objects Self-occlusion\nfrom a certain viewpoint, one part of an object is occluded by another part.\n"},"KB/Occult-Blood-Screen":{"title":"Occult Blood Screen","links":[],"tags":["medical"],"content":"Occult Blood Screen\n\nUse of a chemically treated card or pad to test for blood hidden in a stool sample\n"},"KB/Odido":{"title":"Odido","links":[],"tags":["jobsearch"],"content":"Subhaditya Mukherjee : Odido - Data Scientist Application\nHello Pauline,\nIt’s nice to meet you! \nMy name is Subhaditya, I recently graduated with a masters in AI from the RUG (Groningen), and am looking for my next big challenge. I found the position a good fit for what I can offer and what I want to do next, and so this is my formal application for the Data Scientist position.\nOver the past few years, I have worked with many clients (both freelance/part-time and in internships) and helped them bring their AI projects to life. I have a mix of experience - from ML (scikit-learn, xgboost, pandas) to advanced Gen AI (LLMs, Diffusion models) to the stack of Python, Git, Docker etc. I have been working with AI for a long time now, and am quite familiar with the stack, something that I think would be a good fit in this position.\nMy main background is in R&amp;D, and so the Intelligence and Insights division is where I think I see myself in the most. I love a good challenge, and getting to work on a product that will affect so many people is such a nice opportunity."},"KB/Offline-Distillation":{"title":"Offline Distillation","links":[],"tags":["knowledgedistillation"],"content":"Offline Distillation\n\nThe first stage in offline distillation is usually not discussed as part of knowledge distillation, i.e., it is assumed that the teacher model is pre-defined. Little at- tention is paid to the teacher model structure and its re- lationship with the student model\nThe main advantage of offline methods is that they are simple and easy to be implemented. For example, the teacher model may contain a set of mod- els trained using different software packages, possibly located on different machines. The knowledge can be extracted and stored in a cache.\nThe offline distillation methods usually employ one- way knowledge transfer and two-phase training pro- cedure. However, the complex high-capacity teacher model with huge training time can not be avoided, while the training of the student model in offline distillation is usually efficient under the guidance of the teacher model.\nMoreover, the capacity gap between large teacher and small student always exists, and student often largely relies on teacher.\n"},"KB/Ogliodendrocytes":{"title":"Ogliodendrocytes","links":["KB/Myelin"],"tags":["brain"],"content":"Ogliodendrocytes\n\nWrap and insulate, forms Myelin sheath\n"},"KB/Ohms-Law":{"title":"Ohms Law","links":["KB/Resistance","KB/Force"],"tags":["temp"],"content":"Ohms Law\n - Potential difference = current x Resistance\n - V= IR\n - Ohm’s Law applied to the full circuit: Electromotive Force = current x (sum of the circuit Resistance and the internal Resistance of the cell)\n - EMF = I(R+r)"},"KB/On-the-Distinction-Between-Perceived-Duration-and-Event-Timing---Towards-a-Unified-Model-of-Time-Perception":{"title":"On the Distinction Between Perceived Duration and Event Timing - Towards a Unified Model of Time Perception","links":["KB/Regularization"],"tags":["cognitivemodel"],"content":"On the Distinction Between Perceived Duration and Event Timing - Towards a Unified Model of Time Perception\n\n\nDarren Rhodes\n\n\nneural and computational bases for the processing of time remains unknown\n\n\nhe distinction between perceived event timing and perceived duration provides the current for navigating a river of contemporary approaches to time perception\n\n\nRecent work has advocated a Bayesian approach to time perception\n\n\nThis framework has been applied to both duration and perceived timing, where prior expectations about when a stimulus might occur in the future (prior distribution) are combined with current sensory evidence (likelihood function) in order to generate the perception of temporal properties (posterior distribution)\n\n\nthese models predict that the brain uses temporal expectations to bias perception in a way that stimuli are ‘regularized’ i.e. stimuli look more like what has been seen before\n\n\nFrom Perceived Duration to Perceived Timing\n\nThe word ‘perceived’ here, is used in the loosest sense — the above methods cannot demonstrably show changes in low-level sensory processing of time (Rhodes)\n\nA Bayesian Model of Perceived Event Timing\n\nbased on the dynamic updating of temporal expectations\nexplain the asymmetries in the detection of irregularity and also in the perceived event timing of stimuli (Di Luca &amp; Rhodes)\nWithin a single trial, perceived timing (the posterior distribution) is the result of combining the probability of sensing a stimulus (likelihood) with the time it was expected (prior)\nkey tenet of the model is the relaxation of the assumption of normality in the probability distribution\nProbability distributions in the temporal domain are asserted to be necessarily asymmetric due to the way time flows\nThe anisotropic nature of time means that evidence accumulated about stimulus timing for the likelihood function can only start after a short delay\ndue to neural processing\na stimulus cannot be sensed before a stimulus is presented\nalways the chance it could be perceived a bit later than average due to noise in the sensory system\nAs such, the perceived timing of stimuli in an environment where trials are isochronous should exhibit the temporal Regularization effect\nearly stimuli should be delayed towards expectation whilst late stimuli should be accelerated\nStimuli presented on time, in contrast are perceptually accelerated\nstimuli that are presented in a random sequence of irregular timings, should not have any temporal expectations built up\nTherefore, they should not have any modulation of their perceived timing, suggesting that a prior is not built\nAn implicit assumption of the model is that noisier measurements should lead to broader likelihood functions that are captured more by the prior probability distributions\nthe Bayesian model of perceived timing can explain the delay of early stimuli as well as the acceleration of on time and later than expected stimuli\nInterval models do not make any explicit predictions about changes in the perceived timing of stimuli and as such cannot account for this data.\n"},"KB/On-the-Importance-of-Visual-Context-for-Data-Augmentation-in-Scene-Understanding":{"title":"On the Importance of Visual Context for Data Augmentation in Scene Understanding","links":[],"tags":["augmentation"],"content":"On the Importance of Visual Context for Data Augmentation in Scene Understanding\n\nNikita Dvornik, Julien Mairal, Senior Member, IEEE, and Cordelia Schmid, Fellow, IEEE\n\nAbstract\n\nsimple image transformations can already improve predictive performance in most vision tasks, larger gains can be obtained by leveraging task-specific prior knowledge\nblending objects in existing scenes\nusing instance segmentation annotations\nthat randomly pasting objects on images hurts the performance, unless the object is placed in the right context.\nexplicit context model by using a convolutional neural network\npredicts whether an image region is suitable for placing a given object or not. In our experiments\n\nIntroduction\n\nscene understanding\ncontext model based on a convolutional neural network.\nThe model estimates the likelihood of a particular object category to be present inside a box given its neighborhood, and then automatically finds suitable locations on images to place new objects and perform data augmentation.\n\nExplicit Context Modeling by CNN\n\nto guess the category of an object just by looking at its visual surroundings\nmodeling by a convolutional neural network,\nContextual data generation\ndataset that comes with bounding box\nobject class annotations\nEach ground-truth bounding box in the dataset is able to generate positive “contextual images” that are used as input to the system\nOne box is able to generate multiple different context images,\nTo prevent distinguishing between positive and background images only by looking at the box shape and to force true visual context modeling, we estimate the shape distribution of positive boxes and sample the background ones from it\nwe estimate the joint distribution of scale s and aspect ratio a with a two-dimensional histogram\ndraw a pair (s, a) from this distribution in order to construct a background box\nSince in natural images there is more background boxes than the ones actually containing an object, we address the imbalance by sampling more background boxes,\n\nModel training\n\nThe input to the network are the “contextual images\n300 × 300\noutput of the network is a label in {0, 1, …, C}\n0-th class represents background and corresponds to a negative “context image\nResNet50\nchange the last layer to be a softmax with C + 1 activations\n\nContext-driven Data Augmentation\n\nSelection of candidate locations for object placement\nSince the model takes into account not only the visual surroundings but a box’s geometry too, we need to consider all possible boxes inside an image to maximize the recall\nHowever this is too costly and using 200 candidates was found to provide good enough bounding boxes among the top scoring ones.\nif an object of category c is present in an image it is a confident signal for the model to place another object of this class nearby.\nThis often happens when only 200 candidate locations are sampled; however, evaluating more locations would introduce a computational overhead\nsimple heuristic\nconsists of drawing boxes in the neighborhood of this object\nand adding them to the final candidate set. The added boxes have the same geometry (up to slight\ndistortions) as the neighboring object’s box.\nCandidate scoring process\nsoftmax output.\ngenerating a contextual image is not deterministic, predictions on two contextual images corresponding to the same box may differ substantially,\nAfter the estimation stage we retain the boxes where an object category has score greater than 0.7\nBlending objects in their environment\nblend an object at the corresponding location\ndifferent types of blending techniques (Gaussian or linear blur, simple copy-pasting with no postprocessing, or generating blur on the whole image to imitate motion), and randomly choose one of them in order to introduce a larger diversity of blending artefacts\nWe also do not consider Poisson blending in our approach, which was considerably slowing down the data generation procedure\nfor our task than in [5]. As a consequence, we do not need to exploit external data to perform data augmentation\n\nUpdating image annotation.\n\nOnce a new object is placed in the scene, we generate a bounding box for object detection by drawing the tightest box around that object\nIn case where an initial object is too occluded by the blended one, i.e. the IoU between their boxes is higher than 0.8, we delete the bounding box of the original object from the annotations\nIf a new instance occludes more than 80% of an object already present in the scene, we discard annotations for all pixels belonging to the latter instance.\nTo obtain semantic segmentation masks from instance segmentations, each instance pixel is labeled with the corresponding objects class.\n\nWhy is Random Placement not Working?\n\nas violation of context constraints imposed by the dataset\nobjects looking “out of the scene” due to different illumination conditions\nsimply artifacts introduced due to blending techniques\n\nImpact of blending when the context is right\n\nlack of visual context and the presence of blending artefacts may explain the performance drop\npresence of difference in illumination and blending artefacts is not critical for the object detection task\nReducing the need for pixel-wise object annotation\nOur data augmentation technique requires instance-level segmentations, which are not always available in realistic scenarios\nrelax the annotation requirements for our approach and show that it is possible to use the method when only bounding boxes are available\nSemantic segmentation + bounding box annotation\nInstance segmentation masks provide annotations to each pixel in an image and specify (i) an instance a pixel belongs to and (ii) class of that instance\nIf these annotations are not available\none may approximate them with semantic segmentation and bounding boxes annotation\nSemantic segmentation annotations are also pixel-wise, however they annotate each pixel only with the object category.\nInstance-specific information could be obtained from object bounding boxes, however this type of annotation is not pixel-wise and in some cases is not sufficient to assign each pixel to the correct instanc\nas long as a pixel in semantic map is covered by only one bounding box, it uniquely defines the object it belong\notherwise, if more than one box covers the pixel, it is not clear which object it comes from\nWhen deriving approximate instance masks from semantic segmentation and bounding boxes (see Figure 9, column 2), we randomly order the boxes and assign pixels from a semantic map to the corresponding instances\nWhenever a pixel could be assigned to multiple boxes we choose a box that comes first in the orderin\n\nImportance of Context Modeling Quality for Scene Understanding\n\nquality of a context model is mainly influenced by the amount of data it has received for training\nwe increase the data size used for context modeling, we can see how both detection and segmentation improve; however, this gain diminishes as the data size keeps growin\nto improve scene understanding, the context model has to get visual context “approximately right” and further improvement is most likely limited by other factors such as unrealistic generated scenes and limited number of instances that are being copy-pasted\nOn the other hand, if the context model is trained with little data, as in the case of using only 5% of the full set, our augmentation strategy tends to the random one and shows little improvement\n\nImages\n\n\n\n\n\n\n\n\n\n\n\n"},"KB/On-the-overlap-between-Grad-CAM-saliency-maps-and-explainable-visual-features-in-skin-cancer-images":{"title":"On the overlap between Grad-CAM saliency maps and explainable visual features in skin cancer images","links":["KB/Grad-CAM","KB/ISIC-2018","KB/RISE"],"tags":["explainability"],"content":"On the Overlap Between Grad-CAM Saliency Maps and Explainable Visual Features in Skin Cancer Images\n\n@nunnariOverlapGradCAMSaliency2021\nNunnari, Fabrizio, Md Abdul Kadir, and Daniel Sonntag. 2021. “On the Overlap Between Grad-CAM Saliency Maps and Explainable Visual Features in Skin Cancer Images.” Pp. 241–53 in Machine Learning and Knowledge Extraction. Vol. 12844, Lecture Notes in Computer Science, edited by A. Holzinger, P. Kieseberg, A. M. Tjoa, and E. Weippl. Cham: Springer International Publishing.\n\nIntro\n\nDermatologists recognize melanomas by inspecting images in which they identify human-comprehensible visual features.\ninvestigate to what extent such features correspond to the saliency areas identified on CNNs trained for classification\nSaliency maps are images that indicate the pixels areas contributing to a certain classification decision. Saliency maps are normally encoded as greyscale images or converted to heatmaps for visual inspection.\nto what extent saliency maps can be used to identify visual features of skin lesions\n\nRelated Work\n\nISIC 2018\nRISE\nJahanifar et al. also propose a modified DRFI (Discriminative Regional Feature Integration) technique for a similar task for multi-level segmentation task\nBy combining multiple segmentation masks, they produce a more accurate mask.\nDuring the generation of the mask, they use a threshold value of 0.5, but they did not provide a reason for which they choose this value.\n\nClassification Architectures and Models\n\nRESNET50\nVGG16\n\nData Preparation\n\nAs an additional feature, we compute the pixels-wise union of all the features\nIn our experiments, we ignore the skin lesion samples with no features.\nThe generation of the saliency maps consists of running the Grad-CAM algorithm on each skin lesion picture with non-black union mask\nWe repeat the procedure for both the VGG16 and the RESNET50 models, generating the SV and SR greyscale picture sets\nTo compare the saliency maps with ground truth maps, we scaled up SV and SR to the resolution of the original images using a nearest neighbour filter.\nWe can observe that all distributions are strongly right skewed, and all J_{s} are mostly below 0.2, with the exception of a peak in performance for the pigment network clas\n\nFirst Experiment\n\nWith the first experiment we aim at identifying the threshold value that leads to a maximization of the overlap between saliency maps and ground truth\nTo do so, we converted each saliency map into 11 binary maps using thresholds from 0.0 to 1.0 with steps of 0.1\nThen, we proceed by computing the Jaccard indices J between the ground truth and all of the processed saliencies S x V and S x R.\nFor VGG16, among the features classes, the best threshold ranges between 0.4 and 0.7. The minimum J index is 0.0 on all categories, meaning that among all samples there is always at least one map with zero-overlap with the ground truth. The highest average (J=0.141) and maximum (J=0.797) belong to the pigmented network class.\nWhen switching to RESNET50, the best thresholds range between 0.3 and 0.7. With respect to VGG16, pigmented network and streaks present the worse performance, while the average J increases for the other three classes\nSurprisingly, the Jaccard indices measured with the RESNET50 maps, which have a resolution limited to 8x8 pixels, are comparable to the ones extracted from the VGG16 models (24x24 pixels)\nThe second hypothesis is that the lower resolution of the RESNET50 maps is compensated by the higher accuracy of the classification model, i.e., a better overall overlap.\n\nSecond Experiment\n\ndiving the samples into Melanoma and Nevus, and into correctly vs. wrongly classified samples.\nHere, the Jaccard indices are calculated using the union feature and using the best threshold identified in the first experiment, hence on S 0.5 R V and S 0.3\nFor VGG16, we can observe that the mean J for correctly classified melanomas (0.135) is similar to the union class average (0.132).\nHowever, when melanomas are wrongly classified, the Jaccard index drops to 0.086, meaning that the saliency maps diverges from the ground truth\n\nObservation\n\nThis could effectively help doctors is spotting a wrong classification\nThe idea is that: if the classifier tells the doctor that the sample is a melanoma, but then the reported saliency areas diverge a lot from what would be manually marked, then doctors can be more easily induced to think that the system is misclassifying the image\n\nDiscussion\n\nAmong the five features, only Pigment Network reaches the same level of accuracy of the union class.\nmaximum J=0.136\nThis is a huge annotation overhead when compared to labeling images with their diagnose class.\nThe value of the threshold to reach the best J index varies among datasets and features. Since it is not possible to analytically foresee the best threshold of a given dataset, we suggest the development of interactive exploratory visual interfaces, where dermatologists can autonomously control the saliency threshold value in an interactive fashion for exploration.\nHowever, from a decomposition between classes and correctness of classification, it appears that, for higher resolution maps (24x24 pixels on VGG16), saliency maps overlap much better with ground truth features when the classifier is correctly classifying a melanoma (J=0.135) and performance drops when the prediction is incorrect (J=0.086).\nFurther, we would like to investigate on better options for thresholding. In this paper, a global threshold, in the range of 0.0 to 1.0, was simultaneously searched and applied to all the saliency map.\nThis allows for an “emersion” of the most relevant region of interests of a global scale\nHowever, there might be regions of saliency below the global threshold which are relevant with respect to the local surrounding area\nTo spot local maxima, we could split the maps into tiles, or super-pixels, and iteratively identify multiple local threshold values based on the range of saliency values of each region.\nFinally, the current implementation of Grad-CAM returns saliency maps whose range is filled by stretching the range of activation values of the target convolution layer.\nEach saliency map is forced to use the full activation range, independent of other samples.\nIn so doing, regions of interests are “forced” to emerge, even when the activation values of the inner layer are lower when compared to other images.\n\nFuture Work\n\nAs future work, we could consider performing saliency normalization according to global statistics (mean and variance) on the tested set.\n\nImages\n\n\n"},"KB/One-cycle-policy":{"title":"One cycle policy","links":[],"tags":["optimizer"],"content":"One Cycle Policy\n\n@smithSuperConvergenceVeryFast2018\n\nStep\n\nrecommends to do a cycle with two steps of equal lengths, one going from a lower learning rate to a higher one than go back to the minimumThe maximum should be the value picked with the Learning Rate Finder, and the lower one can be ten times lower\nThen, the length of this cycle should be slightly less than the total number of epochs, and, in the last part of training, we should allow the learning rate to decrease more than the minimum, by several orders of magnitude.\nThe idea of starting slower isn’t new: using a lower value to warm-up the training is ofen done, and this is exactly what the first part is achieving\nLeslie doesn’t recommend to switch to a higher value directly, however, but to rather slowly go there linearly, and to take as much time going up as going down.\nthe during the middle of the cycle, the high learning rates will act as regularization method, and keep the network from overfitting\nThey will prevent the model to land in a steep area of the loss function, preferring to find a minimum that is flatteapproximates of the hessian were lower, indicating that the SGD was finding a wider flat area\nThen the last part of the training, with descending learning rates up until annihilation will allow us to go inside a steeper local minimum inside that smoother part\nSurprisingly, applying this policy even allows us to pick larger maximum learning rates, closer to the minimum of the plot we draw when using the learning rate finder\nThose trainings are a bit more dangerous in the sense that the loss can go too far away and make the whole thing diverge In those cases, it can be worth to try with a longer cycle before going to a slower learning rate, since a long warm-up seems to help\n\n\nCyclical Momentum\n\nTo accompany the movement toward larger learning rates, Leslie found in his experiments that decreasing the momentum led to better results\nThis supports the intuition that in that part of the training, we want the SGD to quickly go in new directions to find a flatter area, so the new gradients need to be given more weight\nAccording to Leslie, the exact best value of momentum chosen during the whole training can give us the same final results, but using cyclical momentums removes the hassle of trying multiple values and running several full cycles, losing precious time.\nIn his opinion, the batch size should be set to the highest possible value to fit in the available memory. Then the other hyper-parameters we may have (dropout for instance) can be tuned the same way as weight decay, or just by trying on a cycle and see the results they give\nTraining with the 1cycle policy at high learning rates is a method of regularization in itself, so we shouldn’t be surprised if we have to reduce the other forms of regularization we were previously using when we put it in place\n\n"},"KB/One-hot":{"title":"One Hot Encoding","links":[],"tags":["temp"],"content":"One Hot Encoding\n\nGiven A = {a_{1}, … , a_{k}}\nTurn each a_{v} into k dim binary vector v_{v} \\in {0,1}^{k} which is 0 everywhere execpt at position v\nSymbolic input\nk dim one hot vector\n\n"},"KB/Opacity-Correction":{"title":"Opacity Correction","links":["KB/Transfer-Function"],"tags":["visualization"],"content":"Opacity Correction\n\nOpacity component o in Transfer Function stored with respect to a standard step size \\Delta t\nDifferent step sizes \\Delta t^{\\ast}\nDynamic step sizes\ngenerate pre-integrated function\no^{\\ast} = 1- (1-o)^{\\frac{\\Delta t^{\\ast}}{\\Delta t}}\nevaluate after obtaining o from Transfer Function\napply before compositing`\n"},"KB/Open-Science-@-TUe":{"title":"Open Science @ TUe","links":[],"tags":["conference"],"content":"Open Science @ TUe\n \n\nA person with a pdf → Open science ;p\nResearch data → can of tomato soup → the data itself (soup) + metadata (aka the labels)\nif not → waste of money\n\ndata management plans to prevent repeat experiments\norganize daFta internally\n\n\nrewards from system for papers but not really for anything else\n(x+y div by pi) *2  + a_x + b_y\nData, documentation etc - CC4\nCode - MIT\nMetadata standards\n\nDublin Core, Data Cite, 4TU roadshow data\n\n\ntue research cockpit\n"},"KB/OpenML-<>-scikit-learn-hackathon-paris-'24":{"title":"OpenML <> scikit-learn hackathon","links":["tags/conference"],"tags":["deeplearning","conference","openml"],"content":"OpenML &lt;&gt; Scikit-learn Hackathon\n\nLogistics\nLocation\n27th Floor, ring at “Probabl”\nMontparnasse Tower 33 Avenue du Maine, West Entrance (on your left when leaving the train station), 27th Floor, ring at “Probabl”\n\nContact Persons\n+33783822597 (François Goupil)\n+33760407677 (Charlène Bizollon)\nCommunication Channel\nPlease join the OpenML slack server and the dedicated hackathon channel for easy communication and updates.\nSlack: join.slack.com/t/openml/shared_invite/zt-2ktk2cj1c-r637o20pfCc0H7PS8OUGtA\nChannel: conference\nWifi\nSSD: :probabl.guest\nPWD: :probabl.\nNote: At some point be ready to use your own mobile data (we are experiencing some difficulties)\nSSD: :probabl.eiffel-2.4\nPWD: :probabl.\nNote: low bandwidth\n\nSchedule\nJune 24 - 09:00-18:00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n09:00-09:30WelcomeCoffee and Croissants09:30-11:30Introduction- Short presentation of the scikit-learn and OpenML projects + Probabl (Joaquin and Pieter for OpenML, Guillaume Lemaitre for scikit-learn, Yann Lechelle for Probabl)- Quick round where everyone introduces themselves.- Plan break-out sessions or suggest new ones.10:30-11:30Breakout(1) Organizing community events and Onboarding Contributors (Maren)11:30-13:00LunchBouillon Chartier13:00-14:00Breakout(2) Governance, Funding and Sponsorship (Adrin + François)14:00-15:00Breakout(3) Future Collaboration between scikit-learn and OpenML (Guillaume)15:00-18:00CodeCode: explore each other’s projects.\nAfter the official programme each day, there’s a suggested bar and restaurant to go to\nBar + Restaurant: Le Falstaff \nJune 25 - 09:00-18:00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n09:00-10:00Coffee/croissantsCroissant Talk by Joaquin10:00-11:00Breakout(6) Collaboration Ecosystem for Open-Source Machine Learning11:00-12:00Breakout(7) Academic and Industrial Scope of OpenML and Probabl in AI - Collaboration12:00-13:00LunchTranTranZai13:00-14:00Breakout(5) Probabl Product Technical Discussion (Camille)14:00-17:00CodingCoding17:00-18:00Breakout(4) Development Tooling and Workflows\nBar + Restaurant: Food Society Paris\nJune 26 - 09:00-13:00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9:00-10:00Breakout + coffee/croissantsJoaquin et al.: we have a bit of a delay checking out of our Airbnb but will be there shortly.10:00-12:00TBDOpen / Ad-hoc12:00-13:00Wrap-up13:00-14:00Lunch + end of the hackathonSubway in Montparnasse\n\nBreakout Sessions Ideas\nThis document contains the preliminary agenda and suggestion for OpenML &lt;&gt; scikit-learn hackathon paris ‘24. Breakout sessions are discussions where we can brainstorm or exchange our experiences on specific topics. Feel free to propose additional sessions.\n 💡 Feel free to add new session topics below, there is a template at the end.\n1. Organizing Community Events and Onboarding Contributors [Day 1]\nleader: Maren Westermann\ndescription:\n\nShare our experiences organizing hackathons. How do you attract attendees? How do you make sure that the work at a hackathon is fruitful? Where should you organize your hackathons, and how are they funded? Are online open-source sprints/hackathons an option for you?\nWhat process and documentation should be in place to help onboard new contributors? How to get them started effectively, and how do you make sure they stay with the project?\nHow do we get our projects known to users?\n\n\nnotes:\n\n\ncommunity sprints: everyone is invited to take part, in particular newcomers, to  contribute to an open source project\n\n\nImportant (FYI: we are a bit out of sync)\n\n\nHave a list of curated issues\n\n\nStart with documentation issues for new contributors\n\n\nCome up with issues before the sprint need a curated list for first time contributors (especially beginner friendly ones)\n\n\nmeta-issues for a group of related issues: once one is fixed it can serve as a contribution template\n\n\ndocumentation issues: people start by reading the documentation around the issues\n\n\ndocumentation about how to contribute was missing and is too long\n\n\nrewrote the contributors’ guide to be more concise but more beginner friendly + some video tutorials to get started with github-based contributions\n\n\nkeeping beginner issues away for the sprints (so other people don’t jump on it before)\n\n\ndifference between OpenML Hackathon and Scikit-learn sprints\n\n\nOpenML Hackathons are 1 week and bigger\n\n\nCore developer sprints vs. New contributors sprints\n\n\nOpenml has 7 repositories with different programming languages (backend, APIs, frontend,…)\n\n\nSome documentation for first time contributors but not exhaustive\n\n\nOne-to-one mentoring to get started with a working dev setup\n\n\nTypically requires several days’ investment\n\n\nonboarding new contributors online and in person are two different processes\n\n\nHard to make people feel connected and stay long-term\n\n\nSocial aspect is important: organizing recurrent events (every few months) to development a more long term engagement\n\n\nJoint Pyladies Paris / scikit-learn core contributors events\n\n\nnear one-to-one mentoring\n\n\nrecurrent every few months\n\n\na few hours in the evening\n\n\nretention is low but allowed to developed social bounds\n\n\nImportant to have maintainers be present to slowly build a connection.\n\n\nRetention is low, be realistic on expectations - but the outliers are what matters\n\n\nPersonal connection: if they know you, people are more likely to contribute\n\n\nOpenML hackathons are useful for core maintainers to secure some time to contribute to the project for several solid hours in a row.\n\n\n0 full-time contributor.\n\n\npart time engineers for academic projects\n\n\nnice locations because thanks to EU funding\n\n\nCan OpenML use students to help contribute feature\n\n\nHard to get high-quality submissions\n\n\nHow to incentivize people?\n\n\nBuild career: show of with sklearn contribution on a resume (less long-term contribution)\n\n\nSense of community\n\n\nUseful for your own research\n\n\nHard to have ‘flashy results’ (e.g. genAI apps), how can we solve that?\n\n\nHow to scale time investment?\n\n\nPyLadies / sprints: only few hours in the evening (6-9pm)\n\n\nEvery 2 months, 15-30 (capped) people show up\n\n\nHow are sprints structured?\n\n\nPre-sprint: online, so people have the right setup\n\n\nAt least one organizer (e.g. Maren for pyladies, supported by core devs)\n\n\nShortlist of issues for each sprint\n\n\nPaid internships: great way to find good people and build\n\n\nRequires funding\n\n\nMentoring takes time, but helps people take over some tasks\n\n\nSlack discussions\n\n\nGenerative AI? Bad quality, wastes time. SKlearn only allows human contributions.\n\n\nHow to focus attention? E.g. key project this quarter?\n\n\n\n2. Governance, Funding and Sponsorship\nleader: Adrin + François\ndescription: What are our experiences with our governance structures? What are opportunities for open source projects to make money to pay for e.g., server costs, organizing events, and so on? How do we argue the importance of our projects to motivate a funder/sponsor? Can we quantify our contribution?\nnotes:\n\n\nGovernance is a living document. It matters for the community.\n\n\nHow to combine an open-source library with a for-profit company\n\n\nBe clear about what parts are community-”owned” and which parts are company-owned\n\n\nKeep discussions of the community aspect on community Slack. Decision processes must remain open.\n\n\nwrite public version of important decisions\n\n\nStill creates confusion (for users and contributors)\n\n\nKeep people informed either through mailing lists, monthly meetings etc. (This takes a long time, but is worth it in the long run)\n\n\ncommunicate upcoming discussions in mailing list\n\n\ncontributing to sklearn open doors to work at companies\n\n\nINRIA foundation:\n\n\n50k EUR for 2 meetings yearly where company can state priorities\n\n\nNo requirement, only if useful for community\n\n\nIs it sustainable? For academic salaries. As long as sklearn stays useful companies will keep doing this. Requires that someone at the sponsoring company cares that sklearn doesn’t decline.\n\n\nAlso advertising (logo on website)\n\n\nModelled on Linux foundation.\n\n\nProbabl:\n\n\nWill put a RE to work on a certain issue, but for a lot more money\n\n\nCompanies prefer this (they don’t know how to hire a good sklearn engineer)\n\n\nLeadership by effort:\n\n\nPut your own time into the aspects that are important to you\n\n\nNVIDIA: Have people work at other companies to work on sklearn\n\n\nHow much effort?\n\n\nFor projects that need faster cycles, create a separate package (e.g. skops, hazardous,…) - but this creates maintenance work.\n\n\nHard to get new reviews in since it takes so long. Multiple rounds of reviews even for a simple spelling error.\n\n\nLibrary for putting sklearn models into prod → skops\n\n\nHaving documentation lead, community interaction lead,… does it help and how much?\n\n\nLowers the bar (core dev is a really high bar)\n\n\nSpeeds up decisions\n\n\nNeed more people in the different teams\n\n\n\n3. Future Collaboration between Scikit-learn and OpenML [Day 1, 2 sessions]\nleader: Guillaume Lemaitre\ndescription: scikit-learn can fetch datasets from OpenML, users can automatically evaluate scikit-learn models on OpenML tasks. What other future collaborations are interesting to explore?\n\n\nfetch_openml / download_openml improvements (parquet?)\n\n\ndataset upload via parquet → coming (not fully supported by python API yet)\n\n\ncroissant integration in scikit-learn openml data fetcher? → Tuesday morning\n\n\nprovenance tracking and reproducibility for openml dataset (make it standard to provide a script, possibly hosted externally on GitHub or similar, to show how to reconstruct the openml hosted parquet file from the original dataset format/location.\n\n\ncollaborative feedback (per dataset issue tracker) to report and discuss dataset related problems with dataset owner/uploader\n\n\nIf fetch-openml fails, what to do?\n\n\nadd support for benchmarking? : benchopt\n\n\nnotes:\n\n\nFetch_openml\n\n\nARFF parser is a headache\n\n\nLogically, it makes sense to first download the data file localy and then load it with pandas or polars (pandas in rust)\n\n\nsklearn does not load parquet right now\n\n\nSparse datasets are still an issue (not supported by parquet)\n\n\nMake all sparse dataset dense and store them in parquet (will still compress nicely). Most sparse datasets aren’t that large.\n\n\nSome of these datasets may be one-hot-encoded datasets\n\n\nPyarrow is most supported (fastparquet is not). Polars can read parquet natively. Pyarrow does not support pyodide.\n\n\nHave an explanation for differences between versions\n\n\nVersions discussion\n\n\nVersions of dataset on openml very confusing\n\n\nVersion not related lineage of dataset, very confusing to user\n\n\nVersions of datasets not searchable\n\n\nUse case: go on openml, search for a dataset, “which version of the dataset to select?”\n\n\nBenchmarking\n\n\ninteresting for probabl.ai to share models and benchmarks on OpenML?\n\n\nSklearn Pipeline Representation\n\n\nUse HTML widget for sklearn pipeline diagrams\n\n\ntodo:\n\n\nopenml: check whether all parquet file can be read with polars and pandas\n\n\nopenml: convert all sparse datasets to dense to store then in parquet\n\n\nopenml: have an explanation for differences between versions. When people upload a new version of a dataset, ask for an explanation.\n\n\nopenml: sort dataset by the quality of the datasheet. Show user/datasetname/id as the name on the webui, remove/rename “version”\n\n\nopenml website: implement a way to open an issue to contact the dataset owner\n\n\nopenml: datasheet has section on preprocessing, where people can point to a github link with preprocessing code, encourage users to do this (e.g. dataset quality score) and allow people to report problems\n\n\nsklearn: try to load parquet files from OpenML in fetch_openml\n\n\nopenml: visualization of the sklearn pipelines (flow)\n\n\n3.5 Croissant Talk\nnotes:\n\n\nGithub\n\n\nCroissant is a metadata description format\n\n\nMl datasets are a combination of structured and unstructured data, which make them complicated to manage\n\n\nCroissant was built on top of schema.org, and has more details relative to it\n\n\nThe format has 4 layers\n\n\ndataset level metadata\n\n\nresource description\n\n\ncontent structure\n\n\nml semantics\n\n\nCroissant does not require any changes to underlying data\n\n\nAnalysis and visualization tools work out of the box for all datasets\n\n\nUsing croissant, datasets can be exposed consistently throughout platforms\n\n\nCollaborations with google, hugging face, google dataset search also exist\n\n\nOpenml has deeper dataset description by default, slightly lesser in HF and kaggle\n\n\nOnce loaded, datasets can be imported elsewhere (torch, tf etc) easily\n\n\nCroissant editor - web app where you can use a GUI to enter the dataset descriptions\n\n\nNeurIPS also now recommends using the Croissant format\n\n\nSupports the Core RAI vocab for explainable AI\n\n\nIf images/other files - points to the path\n\n\ntodo:\n\ninteger precision and more detailed dtypes\nHow are uploaded files linked to each other?\nLineage of datasets\n\n\n4. Development Tooling and Workflows\nleader: Pieter\ndescription: Automation is important to create more sustainable workloads and generally improves overall project quality. What tooling and workflows are employed in your projects to run tests, ensure code quality, help contributors, and so on? Which do you find most useful? Are there decisions have you come to regret? What are your major pain points?\nWhat are our responsibilities as open source projects, should we be embracing platforms such as e.g., CodeBerg/Forgejo more?\nNotes:\n\n\nSwitch to open-source tools like CodeBerg once if offers more conveniences\n\n\n There is a GitHub maintainer org that you can apply to (if you are maintainer of an important enough package) that can give you more direct access to GH dev/projects.\n\n\nUse of Azure workflows in scikit-learn is largely historical, but also provides a spread over different (free) usage limits\n\n\nGPU actions with a limited budget\n\n\nGitHub actions workflow problems\n\n\ntesting is a pain and not really supported (easier on Azure)\n\n\nbadish documentation but better than Azure\n\n\nrun documentation examples that are linked to changes in PR diff - use Circle CI because it can easily render the generated HTML in the browser (as opposed to GH where you download the artifact)\n\n\nbot for linter errors to post it as comment helped a lot\n\n\naiming for 100% code coverage, including all validation though that is centralized. Disable coverage for certainly not tested parts of the code. Also test errors and types and warnings.\n\n\n5. Probabl Product Technical Discussion\nleader:  Camille Troillard\ndescription: Presentation/discussion of the Probabl technical product and potential collaboration.\n\n\nWhat to do to put sklearn in production, to make it commercially viable\n\n\nHelp data scientists do better ML\n\n\nBetter understand their model’s behavior\n\n\nBuild something that we’re proud of (and we’re picky)\n\n\nLet people do what they do (don’t interfere), but show interesting things along the way\n\n\nYou should not require a platform, but it should be very easy to switch to a platform\n\n\nEducate people. E.g. you’re changing the metric but this metric doesn’t make sense.\n\n\nInteractive dashboard that shows results of your experiment\n\n\ncode and outputs side by side\n\n\nLike Weights and Biases, but runs locally\n\n\nOutputs data in a portable DB, results are registered and predictions are shown when they are available\n\n\nunified API to the whole infrastructure stack (like MetaFlow)\n\n\nButton to ‘push to production’ or ‘push to OpenML’ depending on the user\n\n\n“We’ve been spoonfed microservices in order to become addicted to CSPs”\n\n\nFeedback for OpenML\n\nHave a clear tagline, e.g. ‘Frictionless ML resources’\nBetter search interface\nNice visualizations for the run page\nFast website\n\n\n6. Collaboration Ecosystem for Open-Source Machine Learning\nleader: Lennart Purucker\ndescription: What other open-source frameworks are struggling with the same questions we are struggling with? Should we reach out to them? Is there a need for a collaboration ecosystem in open-source machine learning/AI? What are lessons learned from which others might benefit? What are lessons learned from others from which we might benefit?\nnotes:\n\n\nStruggles for Open Source\n\n\nCopying and learning from scikit-learn projects\n\n\nBots, CI/CD, CI logic\n\n\nsee scientific-python.org/, scipy.org/ \n\n\nlearn.scientific-python.org/development/ \n\n\nhelps with CI/CD setup and provide more documentation on how to setup a open source project\n\n\nMain issue is human traffic for open-source\n\n\nopening issues, PRs, …\n\n\nWhat other open-source frameworks are struggling with the same questions we are struggling with?\n\n\nlearn.scientific-python.org/contributors/setup/ecosystem/ \n\n\nML Backbone\n\n\nScikit-learn, PyTorch, TensorFlow, MLR, MLJ\n\n\nXGBoost, LightGBM, CatBoost\n\n\nOpenML, Pandas, NumPy, SciPy, Polars\n\n\nPython Backbone\n\n\nPip / PyPi, Conda, vu\n\n\nRay, Joblib\n\n\nML Applications / AutoML / …\n\n\nAMTLK, Auto-Sklearn, FLAML, AutoGluon, H2O …\n\n\nShould we reach out to them?\n\n\nCompany-driven open-source vs. community-driven open-source\n\n\ncompany-driven example\n\n\ntensorflow, (PyTorch)\n\n\nInternal CI vs. open-source CI\n\n\ncommunity-driven\n\n\nscikit-learn\n\n\nVia GitHub\n\n\nIs there a need for a collaboration ecosystem in open-source machine learning/AI?\n\n\nOnly if we have problems, otherwise unnecessary overhead.\n\n\nAre they my dependencies or am I there dependencies?\n\n\nWhat are lessons learned from which others might benefit? / What are lessons learned from others from which we might benefit?\n\n\nmostly the governance documents\n\n\ndocument CI\n\n\nsee scientific-python.org/ \n\n\nonly start testing / maintaining other environments one request / when issues arise\n\n\nscikit-learn.org/stable/developers/minimal_reproducer.html \n\n\n\n7. Academic and Industrial Scope of OpenML and Probabl in AI\nleader: Lennart Purucker\nwho else is joining? Yann Lechelle\ndescription: Where do we see ourselves in the general field of AI/ML? Are we only tabular data? Are we connected to GenAI, Computer Vision, and NLP? How is our connection to industry applications? How do we effectively explain our position to stakeholders (who read too much about GenAI)?\n\n\nInput Modalities\n\n\nTabular (OpenML, Scikit-learn)\n\n\nTime Series\n\n\nVision (OpenML Soon)\n\n\nNLP\n\n\nGraphs\n\n\n(Other)\n\n\nOutput Modalities / Task\n\n\nScalar Regression (OpenML, Scikit-learn)\n\n\nQuantile Regression (OpenML, Scikit-learn)\n\n\nMulticlass Classification (OpenML, Scikit-learn)\n\n\nNo-target / Unsupervised / Data Insights  (OpenML, Scikit-learn)\n\n\nSurvival Analysis (Scikit-learn)\n\n\nForecasting\n\n\nAnomaly Classification\n\n\nAnomaly Detection (OpenML, Scikit-learn)\n\n\nGenerative AI: Structured Predictions\n\n\nML Techniques in AI/ML\n\n\nTraditional ML Algorithms (SVM, RF, Boosting) (OpenML, Scikit-learn)\n\n\nTraditional Deep Learning (OpenML)\n\n\nLarge Foundation Models\n\n\nWhat do Stakeholders Understand?\n\n\nTime Series\n\n\nGenAI\n\n\nNotes:\n\n\nscikit-learn limitation is the API definition\n\n\nprobabl: “own your data science”\n\n\nborder scope, may include other things besides scikit-learn\n\n\nalso deep learning and large foundation models\n\n\nscope is wide around open-source technology\n\n\ncan we connect OpenML to probabl scope\n\n\n“exporting” the API?\n\n\nLLMs “do” UX for ML\n\n\n\n\nAggregated To-Dos\nOpenml\n\ncheck whether all parquet file can be read with polars and pandas\nconvert all sparse datasets to dense to store then in parquet\nhave an explanation for differences between versions. When people upload a new version of a dataset, ask for an explanation.\nsort dataset by the quality of the datasheet. Show user/datasetname/id as the name on the webui, remove/rename “version”\nwebsite: implement a way to open an issue to contact the dataset owner\ndatasheet has section on preprocessing, where people can point to a github link with preprocessing code, encourage users to do this (e.g. dataset quality score) and allow people to report problems\nvisualization of the sklearn pipelines (flow)\ndata quality plots\nDoes the UX of OpenML need work\n\nprobabl\n\ntry to load parquet files from OpenML in fetch_openml\n\ncroissant\n\ninteger precision and more detailed dtypes\nHow are uploaded files linked to each other\n"},"KB/OpenML-Software-Engineer":{"title":"OpenML Software Engineer","links":[],"tags":["jobsearch","openml"],"content":"Software Engineer - Deep Learning Application\nHello Joaquin,\nMy name is Subhaditya, it is nice to meet you :)\nI recently graduated with a masters in AI from the RUG (Groningen), and am now looking for a job in the Netherlands. I remember seeing the OpenML Github repo a while back and thinking it would be really cool to work with AI researchers with a love for open source. But I had just started my masters then, and somehow did not end up reaching out. So when I found this position while looking through Academic Transfer, I absolutely had to apply. I have been working with software engineering and AI for a few years now and I am very aware of the need for accurate benchmarks and sharing models efficiently. With so many people suddenly giving in to the AI hype, crafting tools that cater to the real problems of researchers trying to build AI, something like OpenML, is even more relevant now. Given that, I would love to bring my experience to this position, while also hopefully learning a lot from your team there too.\nOver the past few years, I have worked with many clients (both freelance/part-time and in internships) and helped them bring their AI and data projects to life. My experience includes working with large datasets, building ML/DL models and pipelines and also sharing my knowledge with my peers in technical and non technical fields. I am quite good at using PyTorch / TensorFlow / Keras and have some experience with Flux.jl and Max as well. I am also familiar with the Hugging Face API / PyTorch lightning and the ONNX format. More than anything though, I am a programmer at heart. I have been an avid believer in open source ever since I started using linux almost seven years ago, and although I have a lot to learn, I do think that I am a good fit for this position. (Here is my Github - github.com/subhadityamukherjee)\nThis letter is getting a little long, but I did want to add that I would love to contribute to OpenML and bring about positive changes in the AI community going forward. So many people with not much AI experience are starting to enter this field, and perhaps we could help make their experience a lot smoother, while also promoting more open source research. I have no means of knowing how experienced the other candidates are, but I do hope you give me a shot!\nPS. I still have my Zoekjaar visa till September, so I can start ASAP as well."},"KB/OpenML-x-SURF":{"title":"OpenML x SURF","links":["ToDOs","KB/Cost-conference-'24","KB/architecture"],"tags":["conference"],"content":"OpenML X SURF\n \n\nsupernets - architecture + weight\n\nbetter than transfer learning\n\n\n\nFor Openml Stuff\n\nhide test data\n\nSURF\nWhat They Do\n\nconsulting\nhelp key researchers\ndoesnt scale too well yet\nabstracting compute\n\ninterface for researchers - clusters and stuff\n\n\nmassive llm interface\nTDCC grant → NWO\nARISE → biodiversity\nAI based PDE solvers → CWI\nSAGE bionetworks\nchem ai\n"},"KB/Operator-Fusion":{"title":"Operator Fusion","links":[],"tags":["parallelcomputing"],"content":"Operator Fusion\n\nSome DirectML operators support a concept known as fusion. Operator fusion is a way to improve performance by merging one operator (typically, an activation function) into a different operator so that they are executed together without requiring a roundtrip to memory.\narxiv.org/abs/2108.13342\n"},"KB/Ophthalmoscope":{"title":"Ophthalmoscope","links":[],"tags":["medical"],"content":"Ophthalmoscope\n\nAn instrument used to examine the eye’s fundus, retina and other structures\n"},"KB/Opportunistic-Learning":{"title":"Opportunistic Learning","links":[],"tags":["robotics"],"content":"Opportunistic Learning\n\napart from learning from a batch of labelled training data at predefined times or according to a predefined training schedule, the robot must be prepared to accept a new example when it is observed or becomes available.\n"},"KB/Optical-Encoder":{"title":"Optical Encoder","links":[],"tags":["robotics"],"content":"Optical Encoder\n\nA detection sensor, which measures linear or rotary motion by detecting the movement of markings past a fixed beam of light. This can be used to count revolutions, identify parts, etc.\n"},"KB/Optical-Proximity-Sensors":{"title":"Optical Proximity Sensors","links":[],"tags":["robotics"],"content":"Optical Proximity Sensors\n\nRobot sensors which measure visible or invisible light reflected from an object to determine distance. Lasers are used for greater accuracy.\n"},"KB/Optimizers":{"title":"Optimization","links":["KB/Gradient-Descent","KB/Adagrad","KB/Rmsprop","KB/Adam","KB/Learning-Rate-Decay-tricks","KB/Early-Stopping-tricks"],"tags":["regularization"],"content":"Optimization\n\nGradient Descent\nAdagrad\nRmsprop\nAdam\nLearning Rate Decay tricks\nEarly Stopping tricks\n\n…"},"KB/Optimizing-Code":{"title":"Optimizing Work","links":["KB/Vectorization","KB/Parallelization","KB/Loop-Tiling","KB/Operator-Fusion","KB/Block-Sparse-Kernel"],"tags":["temp"],"content":"Optimizing Work\n\nVectorization\nParallelization\nLoop Tiling\nOperator Fusion\nBlock Sparse Kernel\n"},"KB/Optogenetics":{"title":"Optogenetics","links":[],"tags":["brain"],"content":"Optogenetics\n\nAn innovative neuroscientific technique that uses light to turn genetically modified neurons on and off at will, in live animals.\n"},"KB/Orbisk":{"title":"Orbisk","links":[],"tags":["jobsearch"],"content":"Orbisk\n\nhannah@orbisk.com : Hannah Nesmith-Beck\n\nGrowing up in Dubai and India, I’ve been seeing the effects of food waste almost every day. While many of us are taking steps to reduce personal food waste at the individual level, more is needed in industries. After I moved to the Netherlands, I started using the app TooGood2Go, and every time I went to any of the restaurants, they had so many bags of food to give away it was very disheartening. Orbi is a great product, from what information I could find on the website. Growing up, I could do nothing about food waste, but if my programming skills can contribute even a little to the fight, I’m definitely in.\nMy expertise is a combination of data analytics and computer vision, and as of a month ago, I also have a Masters in Artificial Intelligence from the University of Groningen. I am familiar with the tools required for basic and advanced AI from internships, research projects, papers, freelance work, and many personal projects. From what I can gather, Orbi has an image classification algorithm at its heart. But perhaps the bigger problem is the data itself. This is a challenging problem, but tackling it even more efficiently with the proper steps should be possible. While I do have some ideas on improving the efficiency and performance of the system, I would be happy to work on any other task at hand as well.\nAbove everything, having even a tiny positive impact on society is something that I am passionate about. Orbi is an excellent step forward in the fight against food waste, and I hope to get a chance to make it even more worthwhile for industries to adopt Orbi as part of their daily process.\nEmail\nHello Hannah,\nThis is Subhaditya. I recently graduated with a masters in AI from the University of Groningen and am looking for my next big challenge. I found out about Orbisk while looking for positions that would let me use my skills for a good cause. While I am not sure what positions are open, I thought it would be great to reach out and see if I can, perhaps in some way, be part of the fight against food waste with Orbisk.\nAs a summary - my expertise and interest is a combination of computer vision and data analytics. That being the case, I am looking for a junior role in an AI/ML position. I have attached my resume to this email as well, just in case we can discuss a potential match.\n\nA little bit about me:\nGrowing up in Dubai and India, I’ve been seeing the effects of food waste almost every day. While many of us are taking steps to reduce personal food waste at the individual level, more is needed in industries. After I moved to the Netherlands, I started using the app TooGood2Go, and every time I went to any of the restaurants, they had so many bags of food to give away it was very disheartening. Orbi is a great product, from what information I could find on the website. Growing up, I could do nothing about food waste, but if my programming skills can contribute even a little to the fight, I’m definitely in.\nMy expertise is a combination of data analytics and computer vision. I am familiar with the tools required for basic and advanced AI from internships, research projects, papers, freelance work, and many personal projects. From what I can gather, Orbi has an image classification algorithm at its heart. But perhaps the bigger problem is the data itself. This is a challenging problem, but tackling it even more efficiently with the proper steps should be possible. While I do have some ideas on improving the efficiency and performance of the system, I would be happy to work on any other task at hand as well.\n\nThank you for your time and I hope to hear from you soon :)\nBest,\nSubhaditya Mukherjee"},"KB/Organoid":{"title":"Organoid","links":["KB/Stem-Cells"],"tags":["brain"],"content":"Organoid\n\nA research model that uses pluripotent Stem Cells (iPSCs) to grow structures made of organ-specific cell types.\n"},"KB/Orthogonal-Initialization":{"title":"Orthogonal Initialization","links":["Vanishingexploding-gradients"],"tags":["regularization"],"content":"Orthogonal Initialization\n\nsimple approach to solving the problem Vanishingexploding gradients\nwhen applying repeated matrix multiplications, the eigenvalues are what dictate the growth or death of the result\neigenvalues of orthogonal matrices have an absolute value of 1\nno matter how many matrix multiplications the result doesen’t explode nor vanishes\n"},"KB/Orthogonal-Slicing":{"title":"Orthogonal Slicing","links":["KB/visualization","KB/Isoline","KB/Height-Plots"],"tags":["visualization"],"content":"Orthogonal Slicing\n\n\nInteractively resample the data on slices perpendicular to x-,y-,z-axis\nUse visualization techniques for Isoline, Height Plots\n"},"KB/OrthographicNet":{"title":"OrthographicNet","links":[],"tags":["robotics"],"content":"OrthographicNet\n\nOrthographic projection is used as a universal language among people in engineering professions:\nProjection lines are parallel to each other and are perpendicular to the plane, An accurate outline of the visible face of the object is obtained.\n\n\n"},"KB/Ostension":{"title":"Ostension","links":["KB/Relevance-Theory"],"tags":["language"],"content":"Ostension\n\nThe notion of ostension in Relevance Theory and in Natural Pedagogy Ostension is a notion of the Relevance Theory of Sperber and Wilson\nOstension is the behaviour when the communicator makes manifest his/her intention to make something manifest, i.e., perceptible or inferable, to the listener\n[Ostensive Information](Ostensive Information.md)\nHumans try to obtain from every item of information as great a contextual effect as possible for as small a processing effort as possible.\nChildren tend to give more credit to information derived from ostensive communication than to information obtained via direct experience\n"},"KB/Ostensive-Information":{"title":"Ostensive Information","links":[],"tags":["language"],"content":"Ostensive Information\n\nOstensive communication provides two kinds of information\n\ninformation changing the listener’s cognitive state\ninformation communicating that the first layer of information is presented intentionally\n\n\n"},"KB/Otoscope-or-Auriscope":{"title":"Otoscope or Auriscope","links":[],"tags":["medical"],"content":"Otoscope or Auriscope\n\nA device for examining the external ear cavity\n"},"KB/Out-of-Order-Execution":{"title":"Out of Order Execution","links":[],"tags":["parallelcomputing"],"content":"Out of Order Execution\n\nallow the processor to avoid a class of delays that occur when the data needed to perform an operation are unavailable\nInstruction fetch.\nInstruction dispatch to an instruction queue (also called instruction buffer)\nThe instruction waits in the queue until its input operands are available.\nThe instruction is issued to the appropriate functional unit and executed by that unit.\nThe results are queued (Re-order Buffer).\nOnly after all older instructions have their results written back to the register file, then this result is written back to the register.\n"},"KB/Out-group-Homogeneity-Bias":{"title":"Out-group Homogeneity Bias","links":[],"tags":["temp"],"content":"Out-group Homogeneity Bias\n\nThe tendency to see out-group members as more alike than in-group members when comparing attitudes, values, personality traits, and other characteristics. In-group refers to people you interact with regularly; out-group refers to people you do not interact with regularly. If you create a dataset by asking people to provide attributes about out-groups, those attributes may be less nuanced and more stereotyped than attributes that participants list for people in their in-group.\n"},"KB/Out-of-bag-Evaluation-(OOB-evaluation)":{"title":"Out-of-bag Evaluation (OOB evaluation)","links":[],"tags":["temp"],"content":"Out-of-bag Evaluation (OOB evaluation)\n\nA mechanism for evaluating the quality of a decision forest by testing each decision tree against the examples\nnot used during training of that decision tree.\nOut-of-bag evaluation is a computationally efficient and conservative approximation of the cross-validation mechanism\n"},"KB/Overhypotheses":{"title":"Overhypotheses","links":["KB/Trees","KB/Features","KB/First-order-generalization","KB/Second-order-generalization","KB/Bayesian"],"tags":["language"],"content":"Overhypotheses\n\nParse Trees for sentences,\nThese can be explained related to grammar\nGrammars are structured according to general princples in Universal grammar\nDo children generalize learned object names as representing shape rather than other Features? Yes!\nFirst order generalization\nSecond order generalization\nSuggests that learning of overhypotheses can also be modeled with Bayesian learning\n"},"KB/Oxytocin":{"title":"Oxytocin","links":[],"tags":["brain"],"content":"Oxytocin\n\nSometimes referred to as the “cuddle chemical,” this hormone can work as a neurotransmitter in the brain and has been linked to social attachment and parental care. While there are “love” sprays on the market that are said to contain oxytocin, there is no evidence that these concoctions have any effect on social relationships.\n"},"KB/PASCAL-VOC":{"title":"PASCAL VOC","links":[],"tags":["dataset"],"content":"PASCAL VOC"},"KB/PASCAL-S":{"title":"PASCAL-S","links":[],"tags":["dataset"],"content":"PASCAL-S\n\n“subset of the validation data in the PASCAL VOC 2010”\n“850 natural images”\nThe fixations were measured while eight observers looked at an image for 2 s.\n"},"KB/PCA":{"title":"PCA","links":["KB/Point-Cloud","KB/Gravity"],"tags":["temp"],"content":"PCA\n\nm dim affine hyperplace spanned by first m eigenvectors. Only manifolds and no codebook vectors\nBe able to reconstruct x from f(x) : decoding function x \\approx d \\circ f(x)\n\n\nSteps\n\nCenter data (A)\n\nSubtract their mean from each pattern.\n\\mu = \\frac{1}{N}\\Sigma_{i}x_{i} and getting patterns \\hat x_{i}=x_{i}-\\mu\nPoint Cloud with center of Gravity : origin\n\nExtend more in some “directions” characterized by unit norm direction vectors u \\in \\mathbb{R}^n .\nDistance of a point from the origin in the direction of u : projection of \\bar x_i on u aka inner product u&#039;\\bar x_i\nExtension of cloud in direction u : Mean square dist to origin.\nLargest extension : u_{1}= argmax_{u_{1}, ||u|| = 1} \\frac{1}{N}\\Sigma_{i}(u\n  &#039;\\bar x_i)^2\nSince centered: mean is 0 and \\frac{1}{N}\\Sigma_{i}(u&#039;\\bar x_i)^2 is the variance\nu_1 is the longest direction : First PC : PC1\n\n\n\n\nProject points (B)\n\nFind orthogonal (90deg) subspace . (n-1) dim linear\nMap all points \\bar x to \\bar x ^{\\ast}=\\bar x- (u&#039; \\bar x_i^\\ast)^2- Second PC : PC2\n\n\nRinse and repeat (C)\nNew PCs plotted in original cloud (D)\nFor featurres f_{k}: \\mathbb{R}^{n}\\rightarrow \\mathbb{R} , x \\rightarrow u&#039;_{k}\\bar x\nReconstruction : x= \\mu + \\Sigma_{k= 1, …,n}f_{k}(x)u_{k}\n\nFirst few PCs till index m\n\n(f_{1}(x), …, f_{m}(x))&#039;\nDecoding function d: (f_{1}(x), …, f_{m}(x))&#039; \\rightarrow \\mu + \\Sigma_{k= 1}f_{k}(x)u_{k}\n\n\n\n\n\n\nHow good is the reconstruction\n\n\\Sigma_{k=m+1}^{n} (\\frac{1}{N}\\Sigma_if_{k}(x_i)^2)\nRelative amount of dissimilarity to mean empirical variance of patterns - 1\n\n\\frac{\\Sigma_{k=m+1}^{n} (\\frac{1}{N}\\Sigma_if_{k}(x_i)^2)}{\\Sigma_{k=1}^{n} (\\frac{1}{N}\\Sigma_if_{k}(x_i)^2)}\nRatio very small as index k grows. Very little info lost by reducing dims. Aka good for very high dim stuff.\n\n\n\n\n\n\nCompute SVD\n\nu_{1,}…u_{n} form orthonormal, real eigenvectors\nvariances \\sigma_{1}^{2,}…, \\sigma_{n}^2 are eigenvalues\nC = U\\Sigma U&#039; to get PC vectors u_k lined up in U and variances \\sigma_k^2 as eigenvalues in \\Sigma\nIf we want to preserve 98% variance : Rhs of (1) st. ratio is (1-0.98)\n\n\n"},"KB/PDF":{"title":"Probability Density Function","links":["KB/Probability","KB/Density"],"tags":["distributions"],"content":"Probability Density Function\n\nIf X is a random variable(RV) that takes values in S \\subseteq \\mathbb{R}^{n}. PDF is a fn f:S \\rightarrow \\mathbb{R}^{\\geq0} that satisfies:\n\nFor every subvolume A \\subseteq S of S, the prob P(X \\in A) that X gives a value in A is P(X\\in A) = \\int_Af(x)dx\n\n\nFunction which maps from a random process to a quantified version\n\neg. 1 when heads and 0 when tails\n\n\n"},"KB/PDF.md":{"title":"PDF.md","links":[],"tags":[],"content":""},"KB/PEER":{"title":"PEER","links":[],"tags":["architecture"],"content":"PEER\n\ntrained on edit histories to cover the entire writing process\nPlan, Edit, Explain and Repeat\nThese steps are repeated until the text is in a satisfactory state that requires no further updates\nThe model allow to decompose the task of writing a paper into multiple easier subtasks\nthe model allows humans to intervene at any time and steer the model in any direction\nWikipedia edit histories\nThe approach is a selftraining, using models to infill missing data and then train\nother models on this synthetic data\nThe downside of this comes from comments being very noisy and a lack of citations, which tries to be compensated by a retrieval system which does not always work\nThe entire process of formulating a plan, collecting documents, performing an edit and explaining it can be repeated multiple times until arriving at a sequence of text\na DeepSpeed transformer is used\n"},"KB/PIUQ":{"title":"PIUQ","links":[],"tags":["temp"],"content":"PIUQ\n\nProblematic Internet Use Questionnaire (PIUQ)\na validated self-report scale with good reliability and validity characteristics\nThe questionnaire contains 18 items, each scored on a 5-point Likert-type scale ranging from 1 (never) to 5 (always)\n"},"KB/PMF":{"title":"Probability Mass Function","links":["KB/Probability"],"tags":["distributions"],"content":"Probability Mass Function\n\npmf\nGiven a discrete sample space S\n\nS is a function p : S \\rightarrow [0,1] whos total mass is 1\nsatisfies \\Sigma_{s \\in S}p(s) = 1\n\n\n"},"KB/PQ":{"title":"PQ","links":[],"tags":["architecture"],"content":"PQ\n\nPQ is a vector compression and indexing method that quantizes high-dimensional vectors into compact binary codes\nenables efficient storage and retrieval of dense vectors while preserving their semantic properties.\n"},"KB/PRelu":{"title":"Parametric Relu","links":["KB/Relu"],"tags":["architecture"],"content":"Parametric Relu\n\nmax(\\alpha x,x)\n\n"},"KB/PWC-Data-Analyst-Junior":{"title":"PWC Data Analyst Junior","links":[],"tags":["jobsearch"],"content":"Associate AI Specialist Application - Subhaditya Mukherjee\nAs the days go by, Machine Learning and AI are slowly becoming terms that every company wants to have in their portfolio. While this drive leads to many innovations, most companies are not sure how to do AI “well”. They want to use AI but need clarification on whether it is required, how to handle bias, how to create proper data, or even what models (ML vs. DL) to choose. The job of an AI specialist, then, is to provide the key information required to find and fulfil KPIs given any project. This job would be the perfect next step for me, and so I hope you give me a chance to work with you and your team.\nMy interest is at the intersection of applied AI and explainability, and as of a month ago, I also have a Master in Artificial Intelligence from the University of Groningen. My masters thesis was on improving the explainability of vision models, which gave me quite a bit of experience in this domain. I am quite familiar with Python, deep learning frameworks such as PyTorch and Tensorflow and other tricks of the trade. But this would be my first job, and I have a lot to learn as well. I am willing to put time and effort into learning new technologies and enabling more efficient AI integration with future clients.\nThe customer is king, they say, and the first step in having happy clients is understanding what they truly want and then being able to give them a solution they can use. I hope to be part of a team at PWC working on enabling stakeholders to implement responsible AI in their products."},"KB/PaLM":{"title":"PaLM","links":["KB/Big-Bench"],"tags":["architecture"],"content":"PaLM\n\nPaLM: Scaling Language Modeling with Pathways\nsingle 540 billion parameter dense Transformer language model\nfew-shot language understanding and generation\ndrastically reduces the number of task-specific training examples needed to adapt the model to a particular application\nPathways Language Model\n6144 TPU v4 chips\nbreakthrough performance on reasoning tasks, which require multi-step logical inference\ncombination of scale and chain-of-thought prompting, where the model is explicitly prompted to generate a natural language logical inference chain before making its predictio\nwrite explicit logical inference chains to both explain jokes and answer complex questions about scenarios\nBig-Bench\nsuggest that the improvements from scale for few-shot language understanding have not yet plateaued\nWhen they compare results from PaLM 540B to our own identically trained 62B and 8B model variants, improvements are typically log-linear.\ncertain capabilities of language models only emerge when trained at sufficient scale, and there are additional capabilities that could emerge from future generations of models\ndemonstrating that prompting the model to generate explicit inference chains can drastically increase the quality of the predictions themselves\nmodel’s generation (rather than just understanding) capabilities can be immensely beneficial even for tasks that are modeled as categorical prediction or regression, which typically do not require significant language generation\ncomprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale\nethical considerations related to large language models and discuss potential mitigation strategies\n"},"KB/Padded-Conv":{"title":"Padded Conv","links":["KB/Conv"],"tags":["architecture"],"content":"Padded Conv\n\n(N_i, N_o, C, F)\nFilters transform from C → F channels\nMirror, Reflect\n"},"KB/Palletizing":{"title":"Palletizing","links":[],"tags":["robotics"],"content":"Palletizing\n\nThe process of stacking packages (i.e., boxes, bags, containers, etc.) in an organized fashion on a pallet.\n"},"KB/Parallel-Coordinate-Plots":{"title":"Parallel Coordinate Plots","links":[],"tags":["visualization"],"content":"Parallel Coordinate Plots\n\n\nEnhancement\n\nPermute axes (horizontally) to and swap their direction (vertically) minimize crossings\nAdd histograms on axes to show lines per unit data value\nVisually group/cluster polylines histograms\n\n\n"},"KB/Parallel-Granularity":{"title":"Parallel Granularity","links":[],"tags":["parallelcomputing"],"content":"Parallel Granularity\n\nRation of computation to communication\nCoarse : Large computation between communication\nFine : Small computation between communication\n"},"KB/Parallel-Processing":{"title":"Parallel Processing","links":["KB/Load-balancing","KB/Minimizing-Communication"],"tags":["parallelcomputing"],"content":"Parallel Processing\n\nLoad balancing\nMinimizing Communication\nOverlap Communication\n"},"KB/Parallel-Runner":{"title":"Parallel Runner","links":[],"tags":["temp"],"content":"Parallel Runner\nimport random\nimport numpy as np\nimport concurrent\nfrom typing import *\nfrom concurrent.futures import ProcessPoolExecutor\nfrom types import SimpleNamespace\nimport os\nfrom pathlib import Path\n \ndef ifnone(a, b):\n    &quot;&quot;&quot;\n    Return if None\n    &quot;&quot;&quot;\n    return b if a is None else a\n \n \ndef listify(o):\n    &quot;&quot;&quot;\n    Convert to list\n    &quot;&quot;&quot;\n    if o is None:\n        return []\n    if isinstance(o, list):\n        return o\n    if isinstance(o, str):\n        return [o]\n    if isinstance(o, Iterable):\n        return list(o)\n    return [o]\n \n \ndef num_cpus() -&gt; int:\n    &quot;Get number of cpus&quot;\n    try:\n        return len(os.sched_getaffinity(0))\n    except AttributeError:\n        return os.cpu_count()\n \n \n_default_cpus = min(16, num_cpus())\ndefaults = SimpleNamespace(\n    cpus=_default_cpus, cmap=&quot;viridis&quot;, return_fig=False, silent=False\n)\n \n \ndef parallel(func, arr: Collection, max_workers: int = None, leave=False):  # %t\n    &quot;Call `func` on every element of `arr` in parallel using `max_workers`.&quot;\n    max_workers = ifnone(max_workers, defaults.cpus)\n    if max_workers &lt; 2:\n        results = [\n            func(o, i)\n            for i, o in tqdm.tqdm(enumerate(arr), total=len(arr), leave=leave)\n        ]\n    else:\n        with ProcessPoolExecutor(max_workers=max_workers) as ex:\n            futures = [ex.submit(func, o, i) for i, o in enumerate(arr)]\n            results = []\n            for f in tqdm.tqdm(\n                concurrent.futures.as_completed(futures), total=len(arr), leave=leave\n            ):\n                results.append(f.result())\n    if any([o is not None for o in results]):\n        return results\n "},"KB/Parallel-Shift-Function":{"title":"Parallel Shift Function","links":[],"tags":["robotics"],"content":"Parallel Shift Function\n\nParallel Shift refers to the shifting of an object from a fixed position in such a way that all points within the object move an equal distance.\n"},"KB/Parallelization":{"title":"Parallel","links":[],"tags":["temp"],"content":"Parallel\n\nIndependant work chunks → operate simultaneously\n"},"KB/Parameter-Sharing-for-Graphs":{"title":"Parameter Sharing for Graphs","links":[],"tags":["graph"],"content":"Parameter Sharing for Graphs\n \nParameter Sharing for Graphs\n\nWe could learn a model with separate parameters associated with each node. However, now the network must independently learn the meaning of the connections in the graph at each position, and training would require many graphs with the same topology.\nInstead, we build a model that uses the same parameters at every node, reducing the number of parameters and sharing what the network learns at each node across the entire graph\nOne way to think of this is that each neighbor sends a message to the variable of interest, which aggregates these messages to form the update.\n\n"},"KB/Parasympathetic":{"title":"Parasympathetic","links":[],"tags":["brain"],"content":"Parasympathetic\n\nRelaxes the body\n"},"KB/Parent-Approximations":{"title":"Parent Approximations","links":[],"tags":["explainability"],"content":"Parent Approximations\n\nIt learns a compact two-level decision set in which each rule explains parts of the model behavior unambiguously and is a combined objective function to optimize these aspects: high agreement between the explanation and the model; little overlap between the decision rules in the explanation; the explanation decision set is lightweight and small.\n"},"KB/Parietal-lobe":{"title":"Parietal lobe","links":["KB/Perception","KB/Cerebrum","KB/Sulcus"],"tags":["brain"],"content":"Parietal Lobe\n\nInterprets language, words\nSense of touch, pain, temperature (sensory strip)\nInterprets signals from vision, hearing, motor, sensory and memory\nSpatial and visual Perception\n\nParietal Lobe\n\nThe area of the brain’s Cerebrum located just behind the central Sulcus. It is concerned primarily with the reception and processing of sensory information from the body and is also involved in map interpretation and spatial orientation (recognizing one’s position in space in relation to other objects or places).\n"},"KB/Parkinson’s-Disease":{"title":"Parkinson’s Disease","links":["KB/Substantia-Nigra"],"tags":["brain"],"content":"Parkinson’s Disease\n\nA neurodegenerative disorder characterized by tremor, slowed movement, and speech changes due to the death of dopamine\nneurons located in the Substantia Nigra.\n"},"KB/Partial-Dependence-Plot":{"title":"Partial Dependence Plot","links":[],"tags":["explainability"],"content":"Partial Dependence Plot\n\nAnother approach [29] shows the marginal effect of one or two features on the prediction of learning techniques using Partial Dependence Plots\nThe method gives a statement about the global relationship of a feature and whether its relation to the outcome is linear, monotonic, or more complex.\nThe PDP is the average of Individual Conditional Expectation (ICE) over all features\nICE [30] points to how the prediction changes if a feature changes.\nThe PDP is limited to two features.\n"},"KB/Participation-Bias":{"title":"Participation Bias","links":["KB/Non-response-Bias"],"tags":["temp"],"content":"Participation Bias\n\nSynonym for Non-response Bias\n"},"KB/Particle-Filter":{"title":"Particle Filter","links":[],"tags":["robotics"],"content":"Particle Filter\n\nParticle filter algorithm works for any arbitrary distribution and not just Gaussian. Particle filter is computationally more expensive than Kalman filter\nnon linear systems.\n"},"KB/Particle-Visualization":{"title":"Particle Visualization","links":["KB/visualization","KB/Data-Structures","KB/Raycasting"],"tags":["visualization"],"content":"Particle visualization\n\npoints with only a (3D) coordinate\npotentially enriched with attributes like radius, velocity, etc.\nscattered data Data Structures\nlack of topological information (neighborhood)\ntypically many particles (e.g. atoms, galaxies)\nAtomistic visualization\nvia glyphs (i.e. spheres)\nexplicit (geometry) or implicit Raycasting\nSurface-based visualization\nextract surface geometry\nparticles: point samples describing a surface (e.g. in fluids or molecules)\nVolume-based visualization\nparticles: point samples of a volume\nRaycasting and hierarchical Data Structures\n"},"KB/Parts-of-action":{"title":"Parts of action","links":[],"tags":["language"],"content":"Parts of Action\n\nLynn went on a business trip to New York.\nShe left on an early morning flight.\nTaking a flight should be recognized as part of going on a trip.\n"},"KB/Parts-of-entities":{"title":"Parts of entities","links":[],"tags":["language"],"content":"Parts of Entities\n\nTracy opened the book she just bought.\nThe toc: true\ntitle page was torn.\nThe phrase ‘the toc: true\ntitle page’ should be recognized as being part of the book tat was just bought.\n"},"KB/PatchGAN":{"title":"PatchGAN","links":[],"tags":["loss"],"content":"PatchGAN\n\nType of discriminator\nonly penalizes structure at the scale of local image patches\ntries to classify if each N \\times N patch in an image is real or fake\ndiscriminator is run convolutionally across the image, averaging all responses to provide the ultimate output of D\neffectively models the image as a Markov random field\nassuming independence between pixels separated by more than a patch diameter\ntype of texture/style loss\nrather the regular GAN maps from a 256×256 image to a single scalar output, which signifies “real” or “fake”, whereas the PatchGAN maps from 256×256 to an NxN (here 70×70) array of outputs X, where each X_{ij} signifies whether the patch ij in the image is real or fake.\n\n"},"KB/Pathlines":{"title":"Pathlines","links":[],"tags":["visualization"],"content":"Pathlines\n\n\n"},"KB/Pearson-Correlation":{"title":"Pearson Correlation","links":["KB/Correlation"],"tags":["temp"],"content":"\ntoc: true\ntitle: Pearson Correlation\ntags: [‘temp’]\n\nPearson Correlation\n\nOne type of Correlation\nr = \\frac{n(\\Sigma xy) - (\\Sigma x)(\\Sigma y)}{\\sqrt{[n\\Sigma x^{2} - (\\Sigma x)^{2}][n\\Sigma y^{2} - (\\Sigma y)^{2}]}}\n"},"KB/Pendant-Teaching":{"title":"Pendant Teaching","links":["KB/Manipulator"],"tags":["robotics"],"content":"Pendant Teaching\n\nThe mapping and recording of the position and orientation of a robot and/or Manipulator system as the robot is manually moved in increments from an initial state along a path to a final goal state. The position and orientation of each critical point (joints, robot base, etc.) is recorded and stored in a database for each taught position the robot passes through on its path toward its final goal. The robot may now repeat the path on its own by following the path stored in the database.\n"},"KB/People-Art-Dataset":{"title":"People Art Dataset","links":[],"tags":["dataset"],"content":"People Art Dataset"},"KB/Perception-Component":{"title":"Perception Component","links":[],"tags":["robotics"],"content":"Perception Component\n\nprocesses all momentary information coming from sensors.\n"},"KB/Perception":{"title":"Perception","links":["KB/Preattentive-Processing","KB/Gestalt-Laws","KB/Postattentive-Amnesia","KB/Change-Blindness","KB/Inattentional-Blindness"],"tags":["visualization"],"content":"Perception\n\nPerception — process by which we interpret the things around us through sensory stimuli\nCognition — mental processes assisting us to remember, think, know, judge, solve problems, etc.\nPreattentive Processing\nGestalt Laws\nPostattentive Amnesia\nChange Blindness\nInattentional Blindness\n"},"KB/Perceptron":{"title":"Perceptron","links":[],"tags":["temp"],"content":"Perceptron\n\nf(x)=sign(\\Sigma _i w_ix_i +b) = sign(\\mathbf{w^Tx}+b)\n\nsign(x) = \\begin{cases} 1 &amp; x\\geq0 \\\\ 0 &amp; otherwise\\end{cases}\n\n\n\ncomputational graph\nMulti layer\n\nStack multiple perceptrons\n\\begin{align} \\\\&amp; h_0 = x h1= sign(\\mathbf{w_1^T}+b_1) \\\\ &amp;…\\\\&amp; h1= sign(\\mathbf{w_{L-1}^T}+b_L) \\end{align}\n\n\n"},"KB/Perceptual-Messages":{"title":"Perceptual Messages","links":[],"tags":["robotics"],"content":"Perceptual Messages\n\nlarge (e.g., point cloud)\ndata flows continuously at the sensor output frequency (30Hz).\n"},"KB/Perceptually-Uniform":{"title":"Perceptually Uniform","links":["KB/Euclidean-Distance"],"tags":["visualization"],"content":"Perceptually Uniform\n\nEuclidean Distance corresponds to perceptual difference\ne.g., CIELUV,CIELAB, (L*,a*,b*).\n"},"KB/Peripheral-Nervous-System":{"title":"Peripheral Nervous System","links":["KB/Central-Nervous-System","KB/Afferent","KB/Efferent"],"tags":["brain"],"content":"Peripheral Nervous System\n\nAll the nerves that branch off from the brain\nBoth directions\nAllow Central Nervous System to communicate with the body\nAfferent + Efferent\n"},"KB/Perplexity":{"title":"Perplexity","links":["KB/Cross-Entropy"],"tags":["loss"],"content":"Perplexity\n\nPerplexity is defined as the exponentiated average negative log-likelihood of a sequence.\nIf we have a tokenized sequence X = (x_0, x_1, \\dots, x_t), then the perplexity of X is, \\text{PPL}(X) = \\exp \\left\\{ {-\\frac{1}{t}\\sum_i^t \\log p_\\theta (x_i|x_{&lt;i}) } \\right\\}where \\log p_\\theta (x_i|x_{&lt;i}) is the log-likelihood of the ith token conditioned on the preceding tokens x_{&lt;i} according to our model.\nIntuitively, it can be thought of as an evaluation of the model’s ability to predict uniformly among the set of specified tokens in a corpus.\nImportantly, this means that the tokenization procedure has a direct impact on a model’s perplexity which should always be taken into consideration when comparing different models.\nThis is also equivalent to the exponentiation of the Cross Entropy between the data and model predictions\n"},"KB/PhD-vs-Startup-vs-Big-Company":{"title":"PhD vs Startup vs Big Company","links":[],"tags":["architecture"],"content":"PhD Vs Startup Vs Big Company\n\nhuyenchip.com/2018/10/08/career-advice-recent-cs-graduates.html\n\nPhD Vs no PhD\n\n\nArguments supporting PhD include:\n\nYou’ll have time to immerse yourself in research.\nIf you want to become a professor, you have to do a PhD.\nMany top research labs such as DeepMind only interview PhD candidates.\nYou won’t be too poor as AI internships pay well.\n\n\n\nArguments supporting no-PhD include:\n\nThere should be more people joining industry to bring research into production.\nBy the time you finish your PhD, what you learn might no longer be relevant.\nMany professors have side gigs in the industry anyway so you can still work with them.\nYou won’t be poor for the next five years.\n\n\n\nPursuing your passion, it turns out, is not legal in the US when you’re an international student.\n\n\nBig Companies Vs Startups\n\n\n"},"KB/Pharmacotherapy":{"title":"Pharmacotherapy","links":[],"tags":["brain"],"content":"Pharmacotherapy\n\nThe use of pharmaceutical drugs for therapeutic purposes.\n"},"KB/Phases-of-Simulated-User-Experiments":{"title":"Phases of Simulated User Experiments","links":[],"tags":["robotics"],"content":"Phases of Simulated User Experiments\n\nEvolution\n\nThe classification performance should be improved as the number of examples per category increases while NO new categories are introduced.\n\n\nRecovery\n\nBy increasing the number of categories, it is expected that the prediction accuracy decreases. The time spent in system evolution until correcting and adjusting all current categories defines recovery.\n\n\nBreakpoint\n\nEventually the learning agent reaches to a breakpoint where the agent is no longer able to learn more categories.\n\n\n"},"KB/Phenaki":{"title":"Phenaki","links":[],"tags":["architecture"],"content":"Phenaki\n\ncapable of performing realistic video synthesis, given a sequence of textual prompts\nPhenaki is the first model that can generate videos from open domain time variable prompts\nTo address data issues, it performs joint training on a large image-text pairs dataset as well as a smaller number of video-text examples can result in generalization beyond what is available in the video datasets.\nimage-text datasets having billions of inputs\nlimitations come from computational capabilities for videos of variable length\nthe C-ViViT encoder, the training transformer and the video generator\nThe encoder gets a compressed representation of videos.\nFirst tokens are transformed into embeddings.\nThis is followed by the temporal transformer, then the spatial transformer\nAfter the output of the spatial transformer, they apply a single linear projection without activation to map the tokens back to pixel space\nConsequently, the model generates temporally coherent and diverse videos conditioned on open domain prompts even when the prompt is a new composition of concepts\n"},"KB/Phenotype":{"title":"Phenotype","links":[],"tags":["brain"],"content":"Phenotype\n\nA set of traits or characteristics resulting from the interaction of one’s genes with the environment.\n"},"KB/Phonetics":{"title":"Phonetics","links":[],"tags":["language"],"content":"Phonetics\n\ndeals with the physical building blocks of a language sound system.\neg. sounds of ‘k’, ‘t’ and ‘e’ in ‘kite\n"},"KB/Phong-Lighting":{"title":"Phong Lighting","links":[],"tags":["visualization"],"content":"Phong Lighting\n\n\n"},"KB/Phonology":{"title":"Phonology","links":[],"tags":["language"],"content":"Phonology\n\norganisation of speech sounds within a language.\neg. (1) different ‘k’ sounds in ‘kite’ vs ‘coat’\n(2) different ‘t’ and ‘p’ sounds in ‘top’ vs ‘pot’\n"},"KB/Phrase-Representation-Learning":{"title":"Phrase Representation Learning","links":["KB/Recurrent","KB/Basic-RNN-Architectures","KB/Probability","KB/BLEU"],"tags":["architecture"],"content":"Phrase Representation Learning\n\nLearning Phrase Representations Using RNN Encoder–Decoder for Statistical Machine Translation\ntwo Recurrent neural networks Basic RNN Architectures that is together able to learn the mapping from a sequence of an arbitrary length to another sequence, possibly from a different set, of an arbitrary length.\neither score a pair of sequences (in terms of a conditional Probability) or generate a target sequence given a source sequence\njointly trained to maximize the conditional Probability of a target sequence given a source sequence\nreset gate and an update gate that adaptively control how much each hidden unit remembers or forgets while reading/generating a sequenc\nRNN Encoder–Decoder to score each phrase pair in the phrase table\ncapture linguistic regularities in the phrase pairs well\nBLEU\n"},"KB/Picasso-Dataset":{"title":"Picasso Dataset","links":[],"tags":["dataset"],"content":"Picasso Dataset"},"KB/Pick-and-Place-Cycle":{"title":"Pick and Place Cycle","links":["KB/Manipulator","KB/Point-to-Point"],"tags":["robotics"],"content":"Pick and Place Cycle\n\nThe amount of time it takes for a Manipulator to pick up an object and place it in a desired location, then return to its rest position. This includes time during the acceleration and deceleration phases of a particular task. The robot movement is controlled from one point location in space to another in a Point-to-Point (PTP) motion system. Each point is programmed into the robot’s control memory and then played back during the work cycle.\n"},"KB/Picky-Puppet-Method":{"title":"Picky Puppet Method","links":[],"tags":["language"],"content":"Picky Puppet Method\n\nFor children\nAsk if a “puppet” would like it\n"},"KB/Pinch-Points":{"title":"Pinch Points","links":[],"tags":["robotics"],"content":"Pinch Points\n\nA pinch point is any point at which it is possible for a person or part of a person’s body to be caught between moving parts of a machine, or between the moving and stationary parts of a machine, or between material and any part of the machine. A pinch point does not have to cause injury to a limb or body part, although it might cause injury – it only has to trap or pinch the person to prevent them from escaping or removing the trapped part from the pinch point.\n"},"KB/Pineal-gland":{"title":"Pineal gland","links":[],"tags":["brain"],"content":"Pineal Gland\n\nIt is located behind the third ventricle.\nIt helps regulate the body’s internal clock and circadian rhythms by secreting melatonin.\nIt has some role in sexual development.\n"},"KB/Pipes":{"title":"Pipes","links":[],"tags":["parallelcomputing"],"content":"Pipes\n\nAllows vector operation to be performed in parallel on multiple elements of the vector\n"},"KB/Pituitary-gland":{"title":"Pituitary gland","links":["KB/Hypothalamus"],"tags":["brain"],"content":"Pituitary Gland\n\nlies in a small pocket of bone at the skull base called the sella turcica.\nThe pituitary gland is connected to the Hypothalamus of the brain by the pituitary stalk.\nKnown as the “master gland,” it controls other endocrine glands in the body.\nIt secretes hormones that control sexual development, promote bone and muscle growth, and respond to stress.\n\nPituitary Gland\n\nAn endocrine organ at the base of the brain that is closely linked with the Hypothalamus. The pituitary gland is composed of two lobes, the anterior and posterior lobes, and secretes hormones that regulate the activity of the other endocrine organs in the body.\n"},"KB/Pix2Seq":{"title":"Pix2Seq","links":["KB/COCO","KB/Autoregressive"],"tags":["architecture"],"content":"Pix2Seq\n\nPix2seq: a Language Modeling Framework for Object Detection\ngeneric framework for object detection\nobject detection as a language modeling task conditioned on the observed pixel inputs\nObject descriptions (e.g., bounding boxes and class labels) are expressed as sequences of discrete tokens, and we train a neural network to perceive the image and generate the desired sequence\nCOCO\noutput can be represented by a relatively concise sequence of discrete tokens (e.g., keypoint detection, image captioning, visual question answering)\nAutoregressive\nstop inference when the ending token is produced\napplying it to offline inference, or online scenarios where the objects of interest are relatively sparse\nentirely based on human annotation\n"},"KB/PixelShuffle":{"title":"PixelShuffle","links":[],"tags":["architecture"],"content":"PixelShuffle\n \n\nConv filters with a stride of 1/s to scale up 1D signals by a factor of s.\nOnly the weights that lie exactly on positions are used to create the outputs, and the ones that fall between positions are discarded.\nThis can be implemented by multiplying the number of channels in the kernel by a factor of s, where the sth output position is computed from just the sth subset of channels.\nThis can be trivially extended to 2D convolution, which requires s2 channels\n"},"KB/Places":{"title":"Places","links":[],"tags":["dataset"],"content":"Places\n\nThe Places dataset [107] is proposed for scene recognition and contains more than 2.5 million images covering more than 205 scene categories with more than 5, 000 images per category.\n"},"KB/Places365":{"title":"Places365","links":[],"tags":["dataset"],"content":"Places365\n\n2nd generation of the Places databas\nhigh-level visual understanding tasks, such as scene context, object recognition, action and event prediction, and theoryof-mind inference\nmore than 10 million images covering more than 400 classes and 5, 000 to 30, 000 training images per class.\n"},"KB/PlantCLEF":{"title":"PlantCLEF","links":[],"tags":["dataset"],"content":"PlantCLEF\n\nPlantCLEF dataset is a collection of images of plants, with a total of around 57,000 images and over 200 different plant species.\n"},"KB/Pluripotency":{"title":"Pluripotency","links":[],"tags":["brain"],"content":"Pluripotency\n\nThe quality of certain undifferentiated cells that allows them to develop into one of many different cell types.\n"},"KB/Point-Cloud":{"title":"Point Cloud Data","links":["KB/PointNet++"],"tags":["architecture"],"content":"Point Cloud Data\n\nPointNet++\n"},"KB/Point-Distribution":{"title":"Point Distribution","links":["KB/PDF","KB/Probability","KB/Dirac-Delta"],"tags":["distributions"],"content":"Point Distribution\n\nPDF is impossible to use\nProbability mass is concentrated in a few points\nDirac Delta\nHyperdistributions\n"},"KB/Point-to-Point":{"title":"Point-to-Point","links":["KB/Manipulator"],"tags":["robotics"],"content":"Point-to-Point\n\nManipulator motion in which a limited number of points along a projected path of motion is specified. The Manipulator moves from point to point rather than a continuous smooth path.\n"},"KB/PointNet++":{"title":"PointNet++","links":["tags/todo"],"tags":["architecture","todo"],"content":"PointNet++\ntodo"},"KB/Poisson-Distribution":{"title":"Possion Distribution","links":["KB/Probability","KB/PMF"],"tags":["distributions"],"content":"Possion Distribution\n\nProbability that an event occurs k times within a given time interval\nEg:\nk meteors within 100 years\nk calls in an hour\nAlso can count spatially circumscribed events\nno of dust particles in a mm of air\nno of diamons in ton of ore\nExpected no of events E[X] : rate \\lambda\nPMF : p(k) = \\frac{\\lambda^{k}e^{-k}}{k!}\n\nEg:\nN 1-hour protocols for calls : n_{i} (i = 1, …, N)\n\\hat\\lambda =\\frac{1}{N}\\Sigma_{i}n_{i}\n"},"KB/Poisson-Loss":{"title":"Poisson Loss","links":["Tag-Pages/loss","KB/Poisson-Distribution"],"tags":["loss"],"content":"Poisson loss\n\nWhen data is from Poisson Distribution\n\n\\frac{1}{\\mathrm{length}\\left( y \\right)} \\cdot \\mathrm{sum}\\left( ŷ - \\log\\left( ŷ \\right) \\right)"},"KB/Poisson-Process":{"title":"Poisson Process","links":["KB/Exponential-Distribution"],"tags":["temp"],"content":"Poisson Process\n\nWaiting times between two consecutive spikes are Exponential Distribution\n"},"KB/Poki-Data-Scientist":{"title":"Poki Data Scientist","links":[],"tags":["jobsearch"],"content":"Poki Data Scientist\nWhy Do You want to Work at Poki?\n\ngrown up playing games on websites such as miniclip\ndata science experience : interesting challenge\nsounds like a fun place to work\nperfect next step in my career, learn and grow with Poki and be a part of people enjoying games on the web\n\nGrowing up, I used to spend countless hours playing games online on websites such as Miniclip (does anyone remember Club Penguin? :) ). Being a data scientist with a master in AI, finding ways to ensure players have a nice experience using Poki while also being able to make sure they find the games they want sounds like a great challenge. Poki sounds like a fun place to work, and this would be the perfect next step in my career.\nIt is not every day that you find a position that you think you might enjoy and also know that you can contribute to upon being given a chance, and I did not want to let this opportunity slide.\nCan You Describe a time where You Identified an Area of Improvement and the way You Followed up on That?\n\nIn my work at Emirates NBD (one of the national banks of Dubai), we were building an analytics pipeline to identify fraud transactions. While the pipeline worked, they were using a Microsoft service to do the analytics and the performance was not great.\nIn my time there I helped them build an analytics pipeline from scratch using Python + SQL. I contributed quite a bit to the feature engineering steps that would later be part of the final analytics program used across Emirates NBD.\n\nIn my work at Emirates NBD (one of the largest banks in Dubai), the team was attempting to improve an analytics pipeline to identify fraudulent transactions. While the pipeline worked reasonably well, they were using a Microsoft service to do the analytics, and the performance was not great. In my time there, I helped them build an analytics pipeline from scratch using Python + SQL. I contributed quite a bit to the feature engineering steps that would later be part of the final analytics program used across Emirates NBD. Many of those changes led to improving the final accuracy by a decent amount and made it much easier for my coworkers to push the product to the upper management.\nWhen You Look at the Presentation of Content on Our Platform, what Question Comes to Mind?\n\nClustered, icons are similar\nComputer vision + analytics: Check for something like that.\nGiven an example\n\nWhile Poki looks great, one thing that immediately comes to mind is that many of the icons are rather visually similar (eg : Apple Knight, Apple Knight mini dungeons ; Mekabolt, Day of Meat). There are also a lot of icons immediately, which is slightly overwhelming.\nThis is unintentional, of course, but might make it harder for a user to pick what they want. Using a computer vision similarity technique could be useful in identifying them and pushing them somewhere else on the grid. This would be something that I would love to discuss as well."},"KB/Polynomial-Trajectories":{"title":"Polynomial Trajectories","links":[],"tags":["robotics"],"content":"Polynomial Trajectories\n\n\n"},"KB/Polysynthetic-words":{"title":"Polysynthetic words","links":[],"tags":["language"],"content":"Polysynthetic Words\n\ncomplex words that function as a sentence (Chukchi and Inuktitut)\n"},"KB/Pooling":{"title":"Pooling","links":["KB/Features","KB/Strided","KB/Receptive-field","KB/Downsampling"],"tags":["temp"],"content":"Pooling\n\nSummarize low level Features\nReduce input dims\nMax/Avg\nToo much pooling reduces performance\n\nMultiple convs first\n\n\nMax pool + dilated/Strided convs control effective Receptive field size\nDownsampling\n\n"},"KB/Population-Based-Augmentation":{"title":"Population Based Augmentation","links":[],"tags":["augmentation"],"content":"Population Based Augmentation\n\nPBA can match the performance of AutoAugment on multiple datasets with less computation time\n"},"KB/Population-Correlation":{"title":"Population Correlation","links":["KB/Correlation","KB/Standard-Deviation","KB/Covariance"],"tags":["temp"],"content":"\ntoc: true\ntitle: Population Correlation\ntags: [‘temp’]\n\nPopulation Correlation\n\n\\rho_{xy}= \\frac{\\sigma_{xy}}{\\sigma_{x}\\sigma_{y}}\n\\sigma is the Standard Deviation\n\\sigma_{xy} is the Covariance\n"},"KB/Position-Encoding":{"title":"Position Encoding","links":[],"tags":["architecture"],"content":"Position Encoding\n\nTransformers are feed forward. So need a way to inject position into seq\nPE(pos, 2i) = sin(\\frac{pos}{10000^\\frac{2i}{d_{model}}})\nPE(pos, 2i+1) = cos(\\frac{pos}{10000^\\frac{2i}{d_{model}}})\nConceptually, adding word order to a sentence\n\nSomething like (“Hello”, 1) , (“from”,2) , (“me”, 3)\n\n\n"},"KB/Position-Wise-Feed-Forward":{"title":"Position Wise Feed Forward","links":["KB/Dense"],"tags":["architecture"],"content":"Position Wise Feed Forward\n\nFFN(x) = max(0, xW_{1}+b_{1})W_{2}+b_{2}\nDense Layers are applied along the last (512) dims\n"},"KB/Positron-Emission-Tomography-(PET)":{"title":"Positron Emission Tomography (PET)","links":[],"tags":["brain"],"content":"Positron Emission Tomography (PET)\n\nAn imaging technique, often used in brain imaging. For a PET scan of the brain, a radioactive “marker” that emits, or releases, positrons (parts of an atom that release gamma radiation) is injected into the bloodstream. Detectors outside of the head can sense these “positron emissions,” which are then reconstructed using sophisticated computer programs to create computer images. Since blood flow and metabolism increase in brain regions at work, those areas have higher concentrations of the marker, and researchers can see which brain regions activate during certain tasks or exposure to sensory stimuli. Ligands can be added to a PET scan to detect pathological entities such as amyloid or tau deposits.\n"},"KB/Post-Classification":{"title":"Post Classification","links":["KB/Interpolation","KB/Transfer-Function"],"tags":["visualization"],"content":"Post Classification\n\nInterpolation of scalars at several vertices\nClassification via Transfer Function\n\n"},"KB/Post-processing-Your-Model's-Output":{"title":"Post-processing Your Model's Output","links":["KB/Bucketing"],"tags":["temp"],"content":"Post-processing Your Model’s Output.\n\nAltering the loss function to incorporate a penalty for violating a fairness metric.\nDirectly adding a mathematical constraint to an optimization problem.\nA synthetic feature formed by crossing (taking a Cartesian product of) individual binary features obtained from categorical data or from continuous features via Bucketing. Feature crosses help represent nonlinear relationships.\n"},"KB/Postattentive-Amnesia":{"title":"Postattentive Amnesia","links":[],"tags":["visualization"],"content":"Postattentive Amnesia\n\nNo additional information is saved in the visual system between different scenes\n"},"KB/Posterior-Mean-estimate":{"title":"Posterior Mean Estimate","links":[],"tags":["temp"],"content":"Posterior Mean Estimate\n\nIf need a single, definite model estimate → Get mean value of posterior \\hat \\theta = \\theta^{PME} = \\int_{\\mathbb{R}^{K}}\\theta h(\\theta|D)d\\theta\n"},"KB/Postsynaptic-Cell":{"title":"Postsynaptic Cell","links":["KB/Impulse"],"tags":["brain"],"content":"Postsynaptic Cell\n\nThe neuron on the receiving end of a nerve Impulse transmitted from another neuron.\n"},"KB/Power-and-Force-Limiting-(PFL)":{"title":"Power and Force Limiting (PFL)","links":["KB/Force"],"tags":["robotics"],"content":"Power and Force Limiting (PFL)\n\nCollaborative feature that allows both the operator and robot to work in proximity to one another by ensuring the robot will slow down and stop before a contact situation occurs. In order for this feature to be safely implemented, functional safety and additional detection hardware must be used. A risk assessment shall be used determine if any additional safeguarding is necessary to mitigate risks within the robot system.\n"},"KB/Power":{"title":"Power","links":[],"tags":["temp"],"content":"Power\n\ncurrent x potential difference\nP = IV\n"},"KB/Pragmatics":{"title":"Pragmatics","links":["KB/Parts-of-entities","KB/Parts-of-action","KB/Entities-involving-in-actions","KB/Elements-of-sets","KB/Names-of-individuals"],"tags":["language"],"content":"Pragmatics\n\nImportant relationships that may hold between phrases and parts of their discourse context\nParts of entities\nParts of action\nEntities involving in actions\nElements of sets\nNames of individuals\n"},"KB/Pre-Classification":{"title":"Pre Classification","links":["KB/Transfer-Function","KB/Interpolation"],"tags":["visualization"],"content":"Pre Classification\n\nClassification of scalars at each sample via Transfer Function\nInterpolation of RGBA values\n\n"},"KB/Pre-Integrated-Volume-Rendering":{"title":"Pre Integrated Volume Rendering","links":[],"tags":["visualization"],"content":"Pre Integrated Volume Rendering\n\nAssume\n\nLinear interpolations of scalar values in a ray segment\nConstant length of ray segment L\n\n\nPre computation from a slab\ns_{L}(t) = s_{b}+ \\frac{t}{L}(s_{f}-s_{b})\nbecomes\nc_{i}= \\int_{0}^{L}g(t)e^{-\\int_{t}^{L}{\\kappa(t&#039;)d_{t}}}dt&#039; \no_{i}= e^{\\int_{0}^{L}\\kappa(t)}d_{t} \n\n"},"KB/Preattentive-Processing":{"title":"Preattentive Processing","links":[],"tags":["temp"],"content":"Preattentive Processing\n\nsome visual properties are detected very rapidly and in parallel by low level visual processes\n\n"},"KB/Precision-Recall-Curve":{"title":"Precision Recall Curve","links":["KB/Recall","KB/Precision"],"tags":["loss"],"content":"Precision Recall Curve\n\nPrecision + Recall\nappropriate when dataset imbalanced\nno-skill line changes based on the distribution of the positive to negative classes\n\nhorizontal line with the value of the ratio of positive cases in the dataset\nbalanced this is 0.5\n\n\nskilful model is represented by a curve that bows towards (1,1) above the flat line of no skill\n"},"KB/Precision":{"title":"Precision","links":[],"tags":["loss"],"content":"Precision\n\n\\frac{TP}{TP+FP}\nHow many samples are actually positive out of the total number of predicted positive samples? → How precise is the model in predicting positive samples?\n"},"KB/Predicate":{"title":"Predicate","links":[],"tags":["language"],"content":"Predicate\n\nthe part of a sentence or clause containing a verb and stating something about the subject (e.g. went home in John went home )\n"},"KB/Predicting-Student-learning-Curve":{"title":"Predicting Student learning Curve","links":["KB/Knowledge-Component","KB/Learning-Event"],"tags":["usermodel"],"content":"Predicting Student Learning Curve\n\n(Croteau, Heffernan, &amp; Koedinger, 2004).\nThe learning curve of a Knowledge Component graphs the durations of its learning events (or their probability of error).\na learning curve should be a smoothly descending power-law or exponential curve\nFor instance, the first Learning Event for a Knowledge Component may take 50 seconds, because the student is constructing the Knowledge Component by referring to the textbook and asking the tutoring system for help. The next Learning Event might take only 25 seconds because the student is reconstructing the Knowledge Component. On the third Learning Event, the student recalls the Knowledge Component after a brief struggle, so the event takes 12 seconds. The fourth event takes 6 seconds, the fifth takes 3 seconds, and so on. However, if the representation of knowledge is inaccurate, a Knowledge Component’s learning curve may have a huge jump in the middle or be quite jagged\n"},"KB/Prediction-Difference-Analysis":{"title":"Prediction Difference Analysis","links":[],"tags":["explainability"],"content":"Prediction Difference Analysis\n\nTheir goal was to improve and interpret DNNs. Their technique was based on the univariate approach of [94] and the idea that the relevance of an input feature with respect to a class can be estimated by measuring how the prediction changes if the feature is removed. Zintgraf et al. removed several features at one time using their knowledge about the images by strategically choosing patches of connected pixels as the feature sets. Instead of going through all individual pixels, they considered all patches of a special size implemented in a sliding window fashion. They visualized the effects of different window sizes and marginal versus conditional sampling and displayed feature maps of different hidden layers and top-scoring classes.\n"},"KB/Prediction-assumption":{"title":"Prediction assumption","links":[],"tags":["temp"],"content":"Prediction Assumption\n\nevery model that aims to predict an output Y from an input X makes the assumption that it’s possible to predict Y based on X.\n"},"KB/Predictive-Parity":{"title":"Predictive Parity","links":[],"tags":["temp"],"content":"Predictive Parity\n\nA fairness metric that checks whether, for a given classifier, the precision rates are equivalent for subgroups under consideration.\n"},"KB/Predictive-Uncertainty":{"title":"Predictive Uncertainty","links":["KB/Uncertainty"],"tags":["uncertainty"],"content":"Predictive Uncertainty\n\n\n"},"KB/Preferences-and-ethical-principles-in-decision-making":{"title":"Preferences and ethical principles in decision making","links":[],"tags":["ethics"],"content":"Preferences and Ethical Principles in Decision Making\n\nAndrea Loreggia, Nicholas Mattei, Francesca Rossi, and Kristen Brent Venable.\nleverage the CP-net formalism to represent the exogenous ethics priorities and endogenous subjective preferences\ndistance between CPnets so as to enable AI agents to make decisions using their subjective preferences if they are close enough to the ethical principles\n"},"KB/Prefix":{"title":"Prefix","links":[],"tags":["language"],"content":"Prefix\n\nprecede the stem: do / undo\n"},"KB/Prepositions":{"title":"Prepositions","links":[],"tags":["language"],"content":"Prepositions\n\nrelates phrases (at, on, of, about)\n"},"KB/Presence-sensing-Safeguarding-Device":{"title":"Presence-sensing Safeguarding Device","links":[],"tags":["robotics"],"content":"Presence-sensing Safeguarding Device\n\nA device designed, constructed and installed to create a sensing field to detect an intrusion into such field by people, robots or objects\n"},"KB/Pressure-=-ForceArea":{"title":"Pressure = ForceArea","links":["KB/Force"],"tags":["temp"],"content":"Pressure = Force/Area\n\nP = \\frac{F}{A}\n"},"KB/Presynaptic-Cell":{"title":"Presynaptic Cell","links":["KB/Synaptic-Transmission","KB/Impulse"],"tags":["brain"],"content":"Presynaptic Cell\n\nIn Synaptic Transmission, the neuron that sends a nerve Impulse across the synaptic cleft to another neuron.\n"},"KB/Pretext-Task":{"title":"Pretext Task","links":[],"tags":["semisupervisedlearning"],"content":"Pretext Task\n\npre-designed tasks\nvisual features are learned by learning objective functions of pretext tasks\n"},"KB/Pretext-Tasks":{"title":"Pretext Tasks","links":[],"tags":["semisupervisedlearning"],"content":"Pretext Tasks\n\n\n[Image Generation](Image Generation.md)\n\n\n[Video Generation](Video Generation.md)\n\n\n[Context Similarity](Context Similarity.md)\n\n\n[Spatial Context Structure](Spatial Context Structure.md)\n\n\n[Temporal Context Structure](Temporal Context Structure.md)\n\n\n[Free Semantic Label-based Method](Free Semantic Label-based Method.md)\n\n\n[Cross Modal-based Methods](Cross Modal-based Methods.md)\n\n\n[Semantic Segmentation](Semantic Segmentation.md)\n\n\n[Object Detection](Object Detection.md)\n\n\n[Image Classification](Image Classification.md)\n\n\n[Human Action Recognition](Human Action Recognition.md)\n\n\n[Kernel Visualization](Kernel Visualization.md)\n\n\n[Feature Map Visualization](Feature Map Visualization.md)\n\n\n[Nearest Neighbor Retrieval](Nearest Neighbor Retrieval.md)\n\n"},"KB/Primary-Capsule":{"title":"Primary Capsule","links":[],"tags":["architecture"],"content":"Primary Capsule\n\nlowest layer of a capsule network\nprocessing the raw input image\nEach capsule in the primary layer is sensitive to a specific feature of the input image, such as an edge or a particular shape.\n\nConvolution\n\nThe primary capsules in a capsule network are created by applying a series of convolutional filters to the input image. Each filter is responsible for detecting a specific feature in the input image, such as an edge or a particular shape.\n\nReshape\n\nThe outputs of the convolutional filters are then reshaped into a grid of “capsules,” each of which corresponds to a specific location in the input image.\n\nSquash\n\nThe outputs of the capsules are then “squashed” to ensure that they have a non-negative scalar value, which allows the network to learn more easily to differentiate between objects and background.\n"},"KB/Prion":{"title":"Prion","links":[],"tags":["brain"],"content":"Prion\n\nA protein aggregate that can multiply itself, inducing the formation of new aggregates from individual copies of the protein it encounters. Prions have the potential to spread within the body and brain, and even from one organism to another—“infectiously,” like a virus. The first prions described were hardy aggregates of PrP, the prion protein. They are responsible for a set of rapid, fatal, and potentially transmissible neurodegenerative diseases including Creutzfeldt-Jakob disease and bovine spongiform encephalopathy (“mad cow disease”). Many researchers now argue that protein aggregates in other neurodegenerative diseases, such as the Aβ and tau plaques of Alzheimer’s, have such similar properties that they also deserve to be called prions.\n"},"KB/Prismatic-Joint":{"title":"Prismatic Joint","links":[],"tags":["robotics"],"content":"Prismatic Joint\n\n\nLinear movement like a piston\n"},"KB/Privacy-awareness":{"title":"Privacy awareness","links":[],"tags":["explainability"],"content":"Privacy awareness\n\nML models may have complex representations of their learned patterns\nthe ability to explain the inner relations of a trained model by non-authorized third parties may also compromise the differential privacy of the data origin\n"},"KB/Probability":{"title":"Probability","links":["KB/Frequentist","KB/Bayesian"],"tags":["index"],"content":"Probability\n\nFrequentist\nBayesian\n"},"KB/Problems-facing-MLOps":{"title":"Problems facing MLOps","links":[],"tags":["architecture"],"content":"Problems Facing MLOps\nGeneral\n\nCoding is not the whole story\nIt’s easier for great engineers to pick up ML knowledge, but it’s a lot harder for ML experts to become great engineers.\nUse “off the shelf models”\nCompanies focus on improving data, but not model\nModel sizes are hard\nDataset/model versioning is hard\nExperiment Tracking\n\nHyperparameter tuning is important and it’s not surprising to find several that focus on it,\nnone seems to catch on because the bottleneck for hyperparameter tuning is not the setup, but the computing power needed to run it.\n\n\nData monitoring\n\nDistribution shift\n\n\nLabelling\n\nHow often do we need to re-label\n\n\nCI/CD\n\nTesting\nRe-train model\n\n\nDeployment\n\nHow often to package and deploy\nOne reason for the lack of serving solutions is the lack of communication between researchers and production engineers.\nSmall companies, whose employees can see the entire stack, are constrained by their immediate product needs\n\n\nModel Compression\n\nCompress ML Models\n\n\nOptimizing Inference\nEdge devices\nPrivacy\n\nGDPR\n\n\nOSS vs Open Core\n\nSince OSS has become a standard, it’s challenging for startups to figure out a business model that works.\nAny tooling company started has to compete with existing open-source tools.\n\n\n"},"KB/Programmable-Logical-Controller-(PLC)":{"title":"Programmable Logical Controller (PLC)","links":[],"tags":["robotics"],"content":"Programmable Logical Controller (PLC)\n\nA solid-state control system, which has a user programmable memory for storage of instructions to implement specific functions such as: I/O control logic, timing, counting arithmetic and data manipulation\nA PLC consists of a central processor, input/output interface, memory and programming device, which typically uses relay equivalent symbols.\n"},"KB/PromptIR":{"title":"PromptIR","links":[],"tags":["explainability"],"content":"PromptIR\n\nSummary : Summary : Uses a transformer network to prediction a “prompt” that says how degraded an image is. And based on that, decides what module to use.\nDeep learning-based methods have significantly improved image restoration performance, however, they have limited generalization ability to different degradation types and levels\nrequires training individual models for each specific degradation and knowing the input degradation type to apply the relevant model.\nprompt-based learning approach, PromptIR, for All-In-One image restoration that can effectively restore images from various types and levels of degradation\nuses prompts to encode degradation-specific information\ngeneric and efficient plugin module with few lightweight prompts that can be used to restore images of various types and levels of degradation with no prior information on the corruptions present in the image\n\nAirNet\naddresses the all-in-one restoration task by employing the contrastive learning paradigm.\ninvolves training an extra encoder to differentiate various types of image degradations\nAlthough AirNet [29] yields state-of-the-art results, it struggles to model fully disentangled representations of different corruption types\nFurthermore, the usage of an additional encoder for contrastive learning leads to a higher training burden due to the two-stage training approach.\n\nMethod\n\n\naim to learn a single model M to restore an image I from a degraded image I, that has been degraded using a degradation D, while having no prior information about D.\n\nWhile the model is initially “blind” to the nature of degradation, its performance in recovering a clean image can be enhanced by providing implicit contextual information about the type of degradation\n\nFrom a given degraded input image I ∈ RH ×W ×3\nfirst extracts low-level features F0 ∈ RH×W×C by applying a convolution operation; where H × W is the spatial resolution and C denotes the channels.\neature embeddings F0 undergo a 4-level hierarchical encoder-decoder, transforming into deep features Fr ∈ RH ×W ×2C\n\nEach level of the encoder-decoder employs several Transformer blocks, with the number of blocks gradually increasing from the top level to the bottom level to maintain computational efficiency.\n\nStarting from the high-resolution input, the goal of the encoder is to progressively reduce the spatial resolution while .\nFrom the low-resolution latent features Fl, the aim of the decoder is to gradually recover the highresolution clean output\nncorporate prompt block\nPrompt blocks are adapter modules that sequentially connect every two levels of the decoder.\n\nPrompt Block\n\n\n\nPrompt Generation Module\n\nPrompt components Pc form a set of learnable parameters that interact with the incoming features to embed degradation information\nfeatures-prompt interaction is to directly use the learned prompts to calibrate the features.\n\ndynamically predicts attention-based weights from the input features and apply them to prompt components to yield input-conditioned prompts P\n\nshared space to facilitate correlated knowledge sharing among prompt components.\nTo generate prompt-weights from the input features Fl\nfirst applies global average pooling (GAP) across spatial dimension to generate feature vector v 2 RCˆ\npass v through a channeldownscaling convolution layer to obtain a compact feature vector, followed by the softmax operation, thus yielding prompt-weights w 2 RN\nuse these weights to make adjustments in prompt components, followed by a 3 x 3 convolution layer\n\nSince at inference time, it is necessary for the restoration network to be able to handle images of different resolutions, we cannot use the prompt components Pc with a fixed size.\nbilinear upsampling operation to upscale the prompt components\n\nPrompt Interaction Module\n\nenable interaction between the input features Fl and prompts P for a guided restoration.\n\nIn PIM, we concatenate the generated prompts with the input features along the channel dimension.\n\npass the concatenated representations through a Transformer block that exploits degradation information encoded in the prompts and transforms the input features.\nThe Transformer block is composed of two sequentially connected sub-modules: Multi-Dconv head transposed attention (MDTA), and Gated-Dconv feedforward network (GDFN). MDTA applies self-attention operation across channels rather than the spatial dimension and has linear complexity.\nThe goal of GDFN is to transform features in a controlled manner, i.e., suppressing the less informative features and allowing only useful ones to propagate through the network\n\n\n\n\n\n\nImplementation Details\nend-to-end trainable and requires no pretraining of any individual component\n4-level encoder-decoder, with varying numbers of Transformer blocks at each level, specifically [4, 6, 6, 8] from level-1 to level-4.\none prompt block between every two consecutive decoder levels, totaling 3 prompt blocks in the overall PromptIR network\nThe total number of prompt components are 5\nThe model is trained with a batch size of 32 in the all-in-one setting, and with a batch of 8 in the single-task setting\nThe network is optimized with an L1 loss, and we use Adam optimizer (1 = 0.9, 2 = 0.999) with learning rate 2e 4 for 200 epochs.\n\ncropped patches of size 128 x 128\n\nBSD400\nWED\no\nBSD68\nUrban100\nRain100L\nSOTS\n\n\n\n"},"KB/Properties-Required-by-Network-Layers-for-Normalizing-Flows":{"title":"Properties Required by Network Layers for Normalizing Flows","links":[],"tags":["architecture"],"content":"Properties Required by Network Layers for Normalizing Flows\n \n\nthe set of network layers must be suﬀiciently expressive to map a multivariate standard normal distribution to an arbitrary density\nnetworks should be invertible\nIt must be possible to compute the inverse of each layer eﬀiciently\nIt also must be possible to evaluate the determinant of the Jacobian eﬀiciently for either the forward or inverse mapping.\n"},"KB/Properties-of-Adjacency-matrix":{"title":"Properties of Adjacency matrix","links":["KB/Adjacency-matrix"],"tags":["graph"],"content":"Properties of Adjacency Matrix\n \nProperties of Adjacency matrix\n\nif we raise the adjacency matrix to the power of L, the entry at position (m,n) of A^L contains the number of unique walks of length L from node n to node m\n\n\n\n\nNode indexing is arbitrary\nPermutation matrix\n\nmatrix where exactly one entry in each row and column take the value one, and the remaining values are zero\nWhen position (m,n) of the permutation matrix is set to one, it indicates that node mwill become node nafter the permutation.\nX&#039; = XP ; A&#039; = P^{T}AP\npost-multiplying by P permutes the columns and pre-multiplying by P^T permutes the rows\n\n\n"},"KB/Propose-but-verify":{"title":"Propose-but-verify","links":["KB/Features"],"tags":["language"],"content":"Propose-but-verify\n\nSpeakers guess a word-concept association\nKeep guess until they encounter contradicting information\nSeems to correctly model competitions between Features/cues\n"},"KB/Protein-Folding":{"title":"Protein Folding","links":[],"tags":["brain"],"content":"Protein Folding\n\nThe process by which the chain of amino acids that make up a protein assumes its functional shape. The protein clumps and tangles that occur in some neurodegenerative disorders are thought to be triggered when proteins “misfold.”\n"},"KB/Protein-Modeling":{"title":"Protein Modeling","links":["KB/Bayesian","KB/Probability","KB/PMF","KB/MLE","KB/Multinomial-Distribution","KB/PDF","KB/Dirichlet-Distribution"],"tags":["temp"],"content":"Protein Modeling\n\nUsing Bayesian models\nTask : Estimate Probability mass function(because discrete) for a finite, discrete distribution → given a histogram from a sample\nLarge number of categories and small number of observations\n\n\nEstimate Probability distrib of amino acids in each column in a protein class. 20 dim PMF (one for each site)\nCan be aligned\nHigh chances of class not being present in data\n\nMLE will assign 0 Probability to X\nWrong decision made for a lot of them that were not in the training set\nCannot use\n\n\n\n\n20 dim PMF for amnio acid distrib : \\theta = (\\theta_{1}, … ,\\theta_{20})&#039; = (P(X=A), …, P(X=Y))&#039;\n\ncount vectors of amino acids found in a given site in training data D\nDistributed according to Multinomial Distribution with l = 20\n\n\n\nUsing Prior\n\n0 probabilities should not occur. \\mathcal{H} = (\\theta_{1}, …, \\theta_{20})&#039; \\in \\mathbb{R}^{20}|\\theta_{j} \\in (0,1) and  \\Sigma_{j} \\theta_{j}=1\n\n19 dim hypervolume\nContinuous space and so can use PDF\nDirichlet Distribution is used to represent it because parameterized with l = 20\n\n\n\n\\alphas fixed beforehand\n"},"KB/Proto-Distributions":{"title":"Proto Distributions","links":["KB/Distributions","KB/Bayesian","KB/Proto-PDF","KB/PDF","KB/Proto-PMF"],"tags":["distributions"],"content":"Proto Distributions\n\nDistributions\nOccur in Bayesian\n\nContinuous Spaces\n\nProto PDF\np(x|\\theta) = \\frac{1}{\\int_{\\mathbb{R}^{k}}p_{0}(x| \\theta)dx}p_{0}(x|\\theta)\np_{0} gives the shape of the PDF\n\\frac{1}{\\int_{\\mathbb{R}^{k}}p_{0}(x| \\theta)dx} is a normalization so it integrates to 1\n\nMost of the time we dont know a distribution but only its proto distribution. This is actually enough sometimes\n\nDiscrete Spaces\n\nProto PMF\n"},"KB/Proto-PDF":{"title":"Proto PDF","links":["KB/PDF"],"tags":["temp"],"content":"Proto PDF\n\nGeneralized PDF\nProto PDF g_{0}\nAny non negative function g_{0}: \\mathbb{R}^{n} \\rightarrow \\mathbb{R} that has a finite integral \\int_{\\mathbb{R}^{n}}g_{0}(x)dx\nIf we divide g_{0} by its integral → we get a normal PDF g\n"},"KB/Proto-PMF":{"title":"Proto PMF","links":["KB/PMF"],"tags":["distributions"],"content":"Proto PMF\n\nGeneralized PMF\np(x|\\theta) = \\frac{1}{\\Sigma_{x \\in S}p_{0}(s)}p_{0}(x)\nS is huge if there are many random variables\n"},"KB/Proximity-Sensor":{"title":"Proximity Sensor","links":[],"tags":["robotics"],"content":"Proximity Sensor\n\nA non-contact sensing device used to sense when objects are a short distance away, and it can determine the distance of the object.\n"},"KB/Proxy-Attention":{"title":"Proxy Attention","links":[],"tags":["explainability"],"content":"Proxy Attention\n\nLet I_{s} \\in \\mathbb{R}^{W \\times H \\times C} be a random source image\napplied saliency map I_{vs}\n\nI_{vs} = f(I_{s}) \\in \\mathbb{R}^{W \\times H}\nAugmented image I_{a}\n\\odot elementwise multi\nMask M \\in \\{0,1\\}^{W \\times H}\n\n\n"},"KB/Proxy-Objective":{"title":"Proxy Objective","links":["KB/Probability","Monte-Carlo","KB/KL-Divergence"],"tags":["distributions"],"content":"Proxy Objective\n\nEasier to change or measure than the actual objective\nSuppose we have some sample space S (such as the set of possible question-answer pairs), some Probability distribution P over S, a true objective (or “reward”) R_{true}: S \\to \\mathbb{R} , proxy objective R_{proxy}:S \\to \\mathbb{R} and we optimize R_{proxy} to get a new distribution P&#039;\nE_{x&#039;\\sim P&#039;}[Rtrue(x′)] is how well the true objective is optimized\n\nMonte Carlo estimator used\nIf N \\geq n samples from P, simultaneously consider every possible subset of these samples of size nnn, weight each sample by the number of subsets for which it is the best according to the proxy objective, and then take the weighted average true objective \\binom{k-1}{n-1} where k is the rank of the sample under the proxy objective, from 1 (worst) up to N (best)\nCan reuse samples of n\n\n\nKL Divergence P&#039; || P measures how much optimization is done\n\nAs long as Continous , n - \\frac{n-1}{n}\n\n\n\nRefs\n\nopenai\n"},"KB/Proxy-features":{"title":"Proxy features","links":[],"tags":["explainability"],"content":"Proxy Features\n\nthere may be correlated features with sensitive ones that can induce bias even when the sensitive features are not present in the dataset.\n"},"KB/Pruning":{"title":"Pruning","links":["KB/Structure-Based-Pruning","KB/Scoring-Pruning-Approaches","KB/Scheduling","KB/Fine-Tuning-Based-Pruning","KB/Global-Magnitude-Based-Pruning","KB/Global-Gradient-Magnitude-Based-Pruning","KB/Layerwise-Gradient-Magnitude-Based-Pruning","KB/Random-Pruning","KB/Layerwise-Magnitude-Based-Pruning"],"tags":["regularization"],"content":"Pruning\n\nMainly that of being able to reduce the size, cost and computational requirements of my models, all while maintaning the accuracy (sort of atleast).\nGenerally this comes about by removing parameters in some form or fashion.\nRather than taking a mask, we can prune certain parts of the network by setting them to 0 or by dropping them if required. (aka weights and biases)\nIn most cases, the network is first trained for a while. Then pruned. Which reduces its accuracy and is thus trained again (fine tuning). This cycle is repeated until we get the results we require.\nMajor Types of Pruning Methods\nStructure Based Pruning\nScoring Pruning Approaches\nScheduling\nFine Tuning Based Pruning\nGlobal Magnitude Based Pruning\nGlobal Gradient Magnitude Based Pruning\nLayerwise Gradient Magnitude Based Pruning\nRandom Pruning\nLayerwise Magnitude Based Pruning\n"},"KB/Pseudo-Label":{"title":"Pseudo Label","links":[],"tags":["semisupervisedlearning"],"content":"Pseudo Label\n\nPseudo labels are automatically generated labels based on data attributes for pretext tasks\n"},"KB/Psycholinguistics":{"title":"Psycholinguistics","links":[],"tags":["language"],"content":"Psycholinguistics\n\nThe study of how the brain processes or produces language\nLinguistics proper is more concerned with what the structure of language is\nAlso examines how linguistic processing interacts with executive functions, e.g. • Working memory • Inhibition\n"},"KB/Psychosis":{"title":"Psychosis","links":[],"tags":["brain"],"content":"Psychosis\n\nA severe symptom of mental illness in which a person’s thoughts and perceptions are so disordered that the individual loses touch with reality.\n"},"KB/Pulse-Coordinates":{"title":"Pulse Coordinates","links":[],"tags":["robotics"],"content":"Pulse Coordinates\n\nYaskawa robots define robot joint axes position in degrees for revolute joints. Pulse is also another way to specify robot joint position, and it does so in robot motor encoder pulse counts.\n"},"KB/Pulse-Oximeter":{"title":"Pulse Oximeter","links":[],"tags":["medical"],"content":"Pulse Oximeter\n\nA small device that clips to the finger, toe or earlobe used to measure blood oxygen saturation\n"},"KB/Punctuation":{"title":"Punctuation","links":[],"tags":["language"],"content":"Punctuation\n\nPunctuation characters are treated as separate tokens – usually\n"},"KB/Pupil-Dilation":{"title":"Pupil Dilation","links":["KB/Parasympathetic"],"tags":["usermodel"],"content":"Pupil Dilation\n\nPupil diameter responds to more than just light\n\nWhen something was visually pleasing\nHarder problems - thinking time\nReaches an asymptote when the task is too difficult , processing load\n\n\nMeasure of resource allocation\n\nLight based response - Parasympathetic\nIncreases with emotional stimulation\n\nSpeech replacement\n"},"KB/Puzzle-Mix":{"title":"Puzzle Mix","links":[],"tags":["augmentation"],"content":"Puzzle Mix\n\n@kimPuzzleMixExploiting2020\nlearns to augment two images optimally based on saliency.\nImages are divided into regions for the mixup\nThe algorithm learns to transport the salient region of one image such that the output image has the maximized saliency from both images.\nh(x_{0}, x_{1}) = (1-z) \\odot \\Pi_{0}^{T}x_{0} + z \\odot \\Pi_{1}^{T}x_{1}\nwhere z_{i} is a binary mask, \\lambda = \\frac{1}{n}\\Sigma_{i}z_{i} is the mixing ratio and \\Pi_{0}, \\Pi_{1} are represent n \\times n grids that denote the amount of mass that is transported during transport of the image patch to another location.\n"},"KB/Pyramidal-cell":{"title":"Pyramidal cell","links":["KB/Equivalent-Current-Dipole"],"tags":["temp"],"content":"Pyramidal Cell\n\n\n\n\nFolds on sides have same charge\nAdds up so can be measured\nEquivalent Current Dipole\n\n\n"},"KB/Pytorch-Tricks":{"title":"Pytorch Tricks","links":["KB/fastai"],"tags":["deeplearning"],"content":"Pytorch Tricks\n\nAlso look at fastai\n\nGet Params of a Layer\nm = learn.model\nl = m.get_submodule(&#039;0.model.stem.1&#039;)\nlist(l.parameters())\nInteract\nfrom ipywidgets import interact\n@interact(m=1.5, b=1.5)\ndef plot_relu(m,b):\n\tplot_function(partial(relu, m, b), ylim = (-1, 4))\nSet Dataset Directory\nimport os\nos.environ[&quot;TORCH_HOME&quot;] = &quot;/media/hdd/Datasets/&quot;\nos.environ[&quot;FASTAI_HOME&quot;] = &quot;/media/hdd/Datasets/&quot;"},"KB/Quadratic-Loss":{"title":"Quadratic Loss","links":["Tag-Pages/loss"],"tags":["loss"],"content":"Quadratic loss\n\n$$W = argmin_{W^{\\ast}}\\Sigma^N_{i=1} ||W^{\\ast} x_i - y_i||^2$\n\\Delta : \\mathbb{R}^{n} \\rightarrow \\mathbb{R}, x \\rightarrow E[Y|X = x] is the gold standard for minimizing this. But \\Delta is unknown\n"},"KB/Quadratic-Potential-Field":{"title":"Quadratic Potential Field","links":[],"tags":["robotics"],"content":"Quadratic Potential Field\n\nmake attractive potential to the goal U_{att}(q)=\\frac{1}{2}D(q,q_{goal})\n\n"},"KB/Quantifier-spreading-children-misled-by-ostensive-cues":{"title":"Quantifier spreading children misled by ostensive cues","links":["KB/Ostension","KB/Relevance-Account","KB/Salient-Object-Strategy"],"tags":["language"],"content":"Quantifier Spreading Children Misled by Ostensive Cues\n\n\nKatalin É. Kiss and Tamás Zétényi\n\n\nTL;DR : Use real images instead of Drawings\n\n\neconomy of the stimulus employed in child language experiments may lend an increased ostensive effect to the message communicated to the child\n\n\nThus, when the visual stimulus in a sentence-picture matching task is a minimal model abstracting away from the details of the situation, children often regard all the elements of the stimulus as ostensive clues to be represented in the corresponding sentence\n\n\nThe use of such minimal stimuli is mistaken when the experiment aims to test whether or not a certain element of the stimulus is relevant for the linguistic representation or interpretation\n\n\nIt is claimed that children find a universally quantified sentence like Every girl is riding a bicycle to be a false description of a picture showing three girls riding bicycles and a solo bicycle because they are misled to believe that all the elements in the visual stimulus are relevant, hence all of them are to be represented by the corresponding linguistic description.\n\n\nWhen the iconic drawings were replaced by photos taken in a natural environment rich in accidental details, the occurrence of quantifier spreading was radically reduced.\n\n\nIt is shown that an extra object in the visual stimulus can lead to the rejection of the sentence also in the case of sentences involving no quantification, which gives further support to the claim that the source of the problem is not (or not only) the grammatical or cognitive difficulty of quantification but the unintended ostensive effect of the extra object.\n\n\nThe reason for the unexpected reactions is that the experimental stimulus presented to the child is devoid of any episodic details; it merely contains a few iconic symbols, which suggests to the child that the irrelevant details have been omitted; hence every element of the stimulus, including the one whose relevance the experiment aims to test, is to be interpreted as an ostensive signal, i.e., every element of the stimulus is significant.\n\n\nOstension\n\n\nQuantifier Spreading as an Ostensive Effect\n\nThe phenomenon\nEvery girl is riding a bicycle.\nAlthough every one of the three girls in the picture is riding a bicycle, many children find the sentence false\nWhen asked “Why?”, they point at the solo bicycle, and say something like “Not that bicycle”, i.e., they show ‘Exhaustive Pairing’ under an extra object condition.\nQuantifier spreading also has a somewhat less common variant, called “Perfectionist Response”.1 It occurs when a universally quantified sentence like (2a) is to be matched with a picture like Figure 2, which contains an element that is neither identical with the referent of the subject, nor identical with the referent of the VP-internal complement.2 (2) a. Every dog is eating a bone. b. No, not that one.\n\nTheories of Quantifier Spreading\n\nIn fact, children are not fully consistent in assigning to universally quantified sentences interpretations of type (3b); the adult interpretation illustrated in (3a),\ntoo, appears to be accessible also to those favoring the spreading reading.\nThe event quantification analysis of Philip (1995) has been criticized on several grounds. For example, it predicts that quantifier spreading is only attested in the case of eventive sentences. In fact, as shown by Philip (2011), it also occurs with sentences of type (4), which contain no event variable:\nFurthermore, as Crain et al. (1996) point out, the analysis of every as an event quantifier does not account for the “perfectionist” mistake, i.e., for the case when the sentence questioned in (1a) is found false in the presence of an extra participant that is neither a girl, nor a bicycle\nThe fact that children have initially access to two interpretations of universally quantified sentences (those of type (3b) and (3a)), one of which is later eliminated, raises a learnability problem, as well – under the assumption that children acquiring their mother tongue only have access to positive evidence.\nSeveral experiments on quantifier spreading have shown that the rate of spreading is affected by pragmatic factors, e.g., a rich linguistic or visual context reduces spreading (cf. Crain et al. 1996\nHowever, some of the evidence concerning the role of extra elements appears to be contradictory; e.g., in the case of quantifier spreading, both the increasing of the number of extra objects (Freeman, Sinha &amp; Stedmon 1982), and the decreasing of the size of the extra object (Philip 2011: 377) have been found to reduce the proportion of spreading, which has not been given a principled explanation.\nRelevance Account\nSalient Object Strategy\nQuantifier spreading is due to the increased ostensive effect of iconic stimuli\nWe hypothesized that quantifier spreading is elicited in experimental situations where the stimulus is not embedded in a context, and is devoid of episodic details, as a consequence of which it gains a – potentially misleading – concentrated ostensive effect\nCrucially, however, when the stimulus only contains a few iconic symbols, every one of its elements gains an ostensive effect.\n\nExperiment\nParticipants\n\nWe tested 82 children from 5 Budapest kindergartens, whose mean age was 5;3 years (SD=0.73).\nWe also carried out the experiment with an adult control group consisting of 24 university students, whose mean age was 21 years (SD=1.61).\n\nProcedure\n\nThe child, the experimenter, and a helper were seated at a table in front of a laptop in a quiet room of the kindergarten.\nThe helper held a teddy bear\nThe experimenter told the child that they would look at pictures on the computer screen together.\nThey would listen to what the bear said about each picture, and the experimenter would ask the subject whether or not it was true.\n\nMaterials\n\n16 sentence–picture pairs (8 fillers and 8 test pairs) were presented to the subjects Each test sentence involved the universal quantifier minden ‛every’\nFour sentence–picture pairs were of the type which can elicit the Exhaustive Pairing mistake, i.e., they involved an extra object (see example (7) and Figures 3, 4), and four sentence–picture pairs were of the type which can elicit the Perfectionist Response, i.e., they contained an extra element neither identical with the referent of the subject, nor identical with the referent of the VPinternal complement (see example (8) and Figures 5, 6)\n\nResults\n\nThe stimuli consisting of a quantified sentence and a drawing elicited quantifier spreading in 27% of the children’s answers. In the case of the stimuli consisting of a quantified sentence and a photo, the rate of quantifier spreading dropped to 15%. Among the adults, the rate of quantifier spreading was 6% and 5%, respectively\n\nDiscussion\n\nIn language acquisition experiments, experimenters tend to use iconic visual stimuli\nin order to eliminate irrelevant distractors, and to ensure that children only react to the controlled factor(s)\nOur results suggest that this method is mistaken when the experiment aims to test whether or not an element in the stimulus is relevant for the linguistic representation\nIf the visual stimulus is a minimal model devoid of episodic details, children tend to interpret all of its elements as ostensive clues to be represented linguistically\nIf the ostensive effect is diminished by the use of photos taken in natural environments, the proportion of QS is reduced by nearly 50%.\nThe only photo which elicited a relatively high proportion (36%) of quantifier spreading answers (Figure 8) is a picture of a fairly artificial-looking setup with remarkably few details:\nDecreasing the size of the extra object makes the object less salient; but increasing the number of the extra objects does not necessarily decrease their salience, and what is more, it is not clear why an increase in the salience of the extra object should result in the increased frequency of quantifier spreading responses.\nMisleading ostensive effect in other types of acquisition experiments An example: A test of exhaustivity\nt has been tested in several experiments (e.g., Beaver &amp; Onea 2011; Kas &amp; Lukács 2013; Pintér 2016) whether the exhaustivity of the preverbal focus of the Hungarian sentence (corresponding roughly to an English cleft constituent) is an inherent semantic property or a cancellable pragmatic implicature\nThe tasks involved truth value judgements; experimenters aimed to find out whether children and adults accept a focus construction like (11) as a true description of a non-exhaustive situation like that in Figure 11 (both cited from Pintér 2016):\n\nResults\n\nThe rate of rejection of the sentences as true descriptions of the visually represented situations significantly correlated with the type of the visual representation.\nThe sentence– drawing pairs were rejected in 10.53% of the cases.\nn the case of the sentence–photo pairs, the rate of rejection was a mere 3.51%.\nJust as in Pintér’s (2016) experiment, the rate of rejection (i.e., the rate of the exhaustive interpretation of the sentences) was slightly even higher in the adult control group: 13.33% in the case of sentence–drawing pairs, and 8.88% in the case of sentence–photo pairs (see Figure 18). When we asked the subjects giving\nnegative answers why e.g. (14) was not true of Figure 14, they consistently gave answers of the following type: “Because the woman is also feeding the ducks”.\n15 children (39%) gave at least one negative answer\n\nDiscussion\n\nThe sentences tested in this experiment involved no special linguistic or cognitive difficulty; they were simple declarative sentences with no quantification, let alone universal quantification; nevertheless, 10.53% of the preschoolers evaluated them as false descriptions of the drawings intended to represent them visually.\nSince this rate is not high (though it is comparable to the 12% of partial rejection obtained by Pintér 2016 in this age group), we might be tempted to attribute it to noise (children’s failure to pay attention, etc.)\nHowever, if the 10.53% rate of rejection had been due to noise, it would not have dropped to 3.51% when the visual stimuli were represented by photos.\nThe comments of the children giving negative answers made it clear that they rejected the given sentence–picture pair because the picture contained extra objects that were not repre-\nsented linguistically\nCrucially, the photos contained many more extra agents and extra objects than the drawings, still they elicited significantly less rejections.\nWhat made the presence of extra objects in the drawings ostensive was the minimality of the drawings, suggesting that everything irrelevant had been eliminated from them.\n\nPictures\n\n\n\n\n\n\n"},"KB/Quantifiers":{"title":"Quantifiers","links":[],"tags":["language"],"content":"Quantifiers\n\nhow many are identified (all, some, none)\nA type of word that indicates quantity\nNominal\nEach, Every, All, Many, Some, No, Few, Most\nAdverbial\nQuantify over time: Always, Usually, Never, Rarely\nQuantify over space: Everywhere, Somewhere, Nowhere\nOthers\nModals are used to show that we believe something is certain, probable or possible\nModal verbs: Can, Could, May, Might, Must\nModal quantifiers: Perhaps, Necessarily, Maybe\n"},"KB/Quantifying-Uncertainty":{"title":"Quantifying Uncertainty","links":["KB/Uncertainty"],"tags":["temp"],"content":"Quantifying Uncertainty"},"KB/Quantile-Bucketing":{"title":"Quantile Bucketing","links":["KB/Bucketing"],"tags":["temp"],"content":"Quantile Bucketing\n\nDistributing a feature’s values into buckets so that each bucket contains the same (or almost the same) number of examples.\n"},"KB/Quantile-Regression":{"title":"Quantile Regression","links":["KB/Heteroscedatic","KB/Normal-Distribution","KB/Quantile-loss"],"tags":["loss"],"content":"Quantile Regression\n \n\npredict a quantile\nrisk models\nfit a Heteroscedatic model and then estimating the quantile based on the predicted Normal Distribution\nor use Quantile loss\n"},"KB/Quantile-loss":{"title":"Quantile loss","links":[],"tags":["loss"],"content":"Quantile loss\n \n\n\n\\alpha is the quantile that needs to be predicted\n"},"KB/Quantized-Distillation":{"title":"Quantized Distillation","links":[],"tags":["knowledgedistillation"],"content":"Quantized Distillation\n\nNetwork quantization reduces the computation com- plexity of neural networks by converting high-precision networks (e.g., 32-bit floating point) into low-precision networks (e.g., 2-bit and 8-bit). Meanwhile, knowledge distillation aims to train a small model to yield a performance comparable to that of a complex model.\nSpecifically, Polino et al. (2018) proposed a quan- tized distillation method to transfer the knowledge to a weight-quantized student network. In (Mishra and Marr, 2018), the proposed quantized KD is called the “ap- prentice”. A high precision teacher network transfers knowledge to a small low-precision student network. To ensure that a small student network accurately mim- ics a large teacher network, the full-precision teacher network is first quantized on the feature maps, and then the knowledge is transferred from the quantized teacher to a quantized student network (Wei et al., 2018).\n"},"KB/Quasi-static-Clamping":{"title":"Quasi-static Clamping","links":["KB/Clamping"],"tags":["robotics"],"content":"Quasi-static Clamping\n\nA type of contact between a person and part of a robot system where the body part can be clamped between the moving part of the robot system &amp; another fixed or moving part of the robot cell\n"},"KB/RACE":{"title":"RACE","links":[],"tags":["dataset"],"content":"RACE"},"KB/RAGAS---automated-evaluation-of-RAG":{"title":"RAGAS - automated evaluation of RAG","links":[],"tags":["architecture"],"content":"RAGAS - Automated Evaluation of RAG\nRAGAS: Automated Evaluation of Retrieval Augmented Generation\n\n(Note : Well. I am not convinced. They did come up with some ways of testing it. But all of those ways used the LLM to test itself, which is uh. Not very nice or representative. I am unsure of how accurate any of these are.\nLogically it seems slightly flawed, but I respect the hustle)\n\nRAGAS (Retrieval Augmented Generation Assessment)\n\nframework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines\nevaluate these different dimensions without having to rely on ground truth human annotations\nWhile the usefulness of retrieval-augmented strategies is clear, their implementation requires a significant amount of tuning, as the overall performance will be affected by the retrieval model, the considered corpus, the LM, or the prompt formulation, among others\n\nEvaluation Strategies\n\nstandard RAG setting\ngiven a question q, the system first retrieves some context c(q) and then uses the retrieved context to generate an answer as(q)\n(Note : This seems like an interesting start so far)\nwe usually do not have access to human-annotated datasets or reference answers\n\nFaithfulness\n\nanswer should be grounded in the given context\nimportant to avoid hallucinations, and to ensure that the retrieved context can act as a justification for the generated answer\nRAG systems are often used in applications where the factual consistency of the generated text w.r.t. the grounded sources is highly important\n\n(Note : Basically they use an LLM to generate an answer and then check with the ground truth to see how closely it matched by comparing the number of matches/total number of statements)\n\nAnswer Relevance\n\ngenerated answer should address the actual question that was provided\n(Note : Basically uses cosine similarity. I am unsure of how different it is to that usual llm but oh well)\n\n\nContext Relevance\n\nretrieved context should be focused, containing as little irrelevant information as possible\ngpt-3.5-turbo-16k model\npenalise the inclusion of redundant information.\n(Note : -.-, they just asked the LLM to find the most important sentences and then divided it by the total number of sentences. I am now progressively getting more and more annoyed with this paper)\n"},"KB/RAHP":{"title":"RAHP","links":["KB/Recall"],"tags":["loss"],"content":"RAHP\n\nRecall at high precision\n"},"KB/RANSAC":{"title":"RANSAC","links":[],"tags":["robotics"],"content":"RANSAC\n\nIt is an iterative method to estimate parameters of a mathematical model from a set of observed data\nA simple example is fitting a line to a set of observations.\nOutliers are points that don’t “fit” the model and points that do fit are called “inliers”\nTable detection\n\nThe algorithm starts by generating plane hypotheses based on three unique points.\nFor each plane hypothesis, distances from all points in the point cloud to the plane are computed.\nThe plane hypotheses are then scored based on counting the number of inlier points, e.g., distance to the plane  20mm.\n\n\n\nThe RANSAC algorithm is repeated for a certain number of iterations, e.g., n = 200.\nObject detection\n\nIt is now possible to extract the points which lie directly above it.\nBy removing the table, we have a point cloud where all the objects that are on top of the table are included.\nThe obtained point cloud is then segmented into individual clusters Each small group of points will be treated as an object candidate.\n\n\n\n\n"},"KB/RETAIn":{"title":"RETAIn","links":[],"tags":["explainability"],"content":"RETAIn\n\nREverse Time AttentIoN mechanism\nThe approach mimics physician practice by attending to the EHR data. Two RNNs are trained in a reverse time order with the goal of efficiently generating the appropriate attention variables. It is based on a two-level neural attention generation process that detects influential past visits and significant clinical variables to improve accuracy and interpretability.\nfor application to Electronic Health Record (EHR) data.\n"},"KB/RETRO":{"title":"RETRO","links":["KB/GPT","KB/Attention"],"tags":["architecture"],"content":"RETRO\n\nImproving Language Models by Retrieving from Trillions of Tokens\nRetrieval-Enhanced Transformer\nRETRO\nenhances auto-regressive language models by conditioning on document chunks retrieved from a large corpus\nbased on local similarity with preceding tokens\ncomparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25x fewer parameters\nfrozen BERT retriever, a differentiable encoder and a chunked cross-Attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training\nWikitext103\nPile\nimproving semi-parametric language models through explicit memory can provide an orthogonal, more efficient approach than raw parameter scaling as they seek to build more powerful language models\n"},"KB/RICAP":{"title":"RICAP","links":["KB/Cropping"],"tags":["augmentation"],"content":"RICAP\n\n@takahashiDataAugmentationUsing2020\nRandom image Cropping and patching\nCropping four regions from randomly sampled images and augmenting them to create a new image.\nThe generated image has mixed labels proportional to the pasted area\nThe area of cropped regions in the output image is determined by sampling through uniform distribution\nanywhere-RICAP (origin can be anywhere), center-RICAP (origin can only be in the middle of the image), and corner-RICAP (origin can only be in corners\nCorner-RICAP has shown the best performance because a larger region of one image is visible to the network to learn\n"},"KB/RISE":{"title":"RISE","links":[],"tags":["explainability"],"content":"RISE\n\n@petsiukRISERandomizedInput2018\nbased on a stochastic approach\nInput images are iteratively altered via random noise, and the final saliency map is composed by accumulating the partial estimations\nHowever, its application requires much more computational power, as it needs to run hundreds of thousands of prediction cycles\nit seems that RISE is not able to highlight regions of interest of skin lesion images with the same reliability as on pictures of real-world objects\nIn the first step, it creates a segmentation mask and applies it to the dermoscopic image. Secondly, it creates a structure segmentation mask to identify the structure of the dermoscopic image. After masking, the original segmented image and some nonvisual metadata are fed into a convolutional neural network for classification\n"},"KB/ROC-Curve":{"title":"ROC Curve","links":[],"tags":["loss"],"content":"ROC Curve\n\nappropriate when the data is balanced\nx-axis : False Positive\ny-axis : True Positive\narea under the curve (AUC) can be used as a summary of the model performance\n\nassign a higher probability to a randomly chosen real positive occurrence than a negative occurrence on average\n\n\nInterpretation\n\nSmaller values on the x-axis of the plot indicate lower false positives and higher true negatives.\nLarger values on the y-axis of the plot indicate higher true positives and lower false negatives.\n\n\n"},"KB/Rabobank":{"title":"Rabobank","links":[],"tags":["jobsearch"],"content":"Subhaditya Mukherjee : Rabobank - AI Strategist Application\nHello Ante,\nIt’s nice to meet you! \nMy name is Subhaditya, I recently graduated with a masters in AI from the RUG (Groningen), and am looking for my next big challenge. I found the position a good fit for what I can offer and what I want to do next, and so this is my formal application for the AI Strategist position.\nOver the past few years, I have worked with many clients (both freelance/part-time and in internships) and helped them bring their AI projects to life. I have a mix of experience, ranging from consulting to building technical AI projects across domains. I believe that this experience makes me a good fit for this position, as not only can I map out the steps needed to arrive at an AI based product, I can also break it down across multiple levels of understanding.  I have been working with AI for a long time now, and am quite familiar with the stack - from LLMs to GenAI to other advanced AI models. I also have an understanding of reinforcement learning and am pretty proficient in Python. \nI understand just how deeply important ethically future proofing the baking system is. The world is moving ahead with AI, and if we do not keep up with the trend, it will move ahead without us. I hope to be part of that change at Rabobank, and help co-create strategies that help the workforce in the long run.\nHope to hear from you soon :)"},"KB/RadboudUMC":{"title":"RadboudUMC","links":[],"tags":["jobsearch"],"content":"FAIR Software Engineer Application\nHello! (I want to say James, but I am not sure)\nMy name is Subhaditya, I recently graduated with a masters in AI from the RUG (Groningen), and am now looking for a job in the Netherlands. As a software engineer with an interest in healthcare and experience in AI, the FAIR RSE position seemed like the perfect next step for me, and I did not want to miss out on applying for it. Having been working on computer vision for a few years now, I am very aware of the need for better data management, especially for complex and privacy oriented workflows. I would love to bring this experience to DIAG, while also hopefully learning a lot from the team there too.\nOver the past few years, I have worked with many clients (both freelance/part-time and in internships) and helped them bring their AI and data projects to life. My experience includes working with large datasets, building ML/DL models and pipelines and also sharing my knowledge with my peers in technical and non technical fields. I have worked with AWS, Google Cloud and DICOM before as well. I am quite proficient in Python, CI/CD and integrating 3rd party APIs with existing codebases. \nI have a masters in AI (with my thesis being on Explainable AI) and that also brought me  experience with setting up training and also the FAIR principles. More importantly though, using AI in healthcare has been my motivation for many years now, and one of the reasons I got into the field in the first place. That being the case, I feel like I am in a better position compared to general software engineers in understanding what is required and why.\nThis letter is getting a little long, but I did want to add that I would love to contribute to your team’s mission of making Radboud UMC the best campus for AI research, and in turn helping a lot of researchers get their work done faster and more responsibly. I do hope you give me a shot :)\nBest,\nSubhaditya Mukherjee"},"KB/Radial-Plot":{"title":"Radial Plot","links":[],"tags":["visualization"],"content":"Radial Plot\n\n\nnumber of associated attributes limited\ndistinct distinguishable values per attribute limited\nvisual mappings must be learned for correct interpretation\n"},"KB/Ramp-up-problem":{"title":"Ramp up problem","links":[],"tags":["usermodel"],"content":"Ramp up Problem\n\nNo data for a user → picked one\nBroad generalization\n"},"KB/RandAugment":{"title":"RandAugment","links":["KB/AutoAugment","KB/CIFAR","KB/Regularization"],"tags":["explainability"],"content":"RandAugment\n\nRandaugment: Practical automated data augmentation with a reduced search space\nEkin D. Cubuk ∗, Barret Zoph∗, Jonathon Shlens, Quoc V. Le\n\nChatGPT Summary\n\nLarge-scale adoption of data augmentation methods is hindered by the need for a separate and expensive search phase.\nCommonly, a smaller proxy task is used to overcome the expense of the search phase, but it is not clear if the optimized hyperparameters found on the proxy task are also optimal for the actual task.\nThe process of designing automated augmentation strategies is being rethought.\nIt is proposed to only search for a single distortion magnitude that jointly controls all operations, which reduces computational expense and eliminates the need for a separate proxy task.\nThe proposed method was tested on various datasets including [CIFAR]-10, [CIFAR](CIFAR]-10, [CIFAR.md) 100, SVHN, ImageNet, and COCO, and showed improvement in performance without the use of a proxy task.\nThe proposed method, RandAugment, uses a parameter-free procedure of always selecting a transformation with uniform probability from a set of K=14 available transformations, and a single distortion magnitude that jointly controls all operations.\nRandAugment is able to achieve comparable or better performance compared to other automated augmentation methods, such as AutoAugment, without the need for a separate proxy task.\nThe results suggest that the optimal data augmentation policies may depend on the specific model and dataset size, and a small proxy task may not provide the best indicator of performance on a larger task.\n\nAbstract\n\nAn obstacle to a large-scale adoption of these methods is that they require a separate and expensive search phase\nA common way to overcome the expense of the search phase was to use a smaller proxy task.\nHowever, it was not clear if the optimized hyperparameters found on the proxy task are also optimal for the actual task.\nrethink the process of designing automated augmentation strategies\nit is sufficient to only search for a single distortion magnitude that jointly controls all operations\npropose a simplified search space that vastly reduces the computational expense of automated augmentation, and permits the removal of a separate proxy task.\nCIFAR-10\nCIFAR 100\nSVHN\nImageNet\nCOCO datasets\n\nSystematic Failures of a Separate Proxy Task\n\nA central premise of learned data augmentation is to construct a small, proxy task that may be reflective of a larger task\nAlthough this assumption is sufficient for identifying learned augmentation policies to improve performance, it is unclear if this assumption is overly stringent and may lead to sub-optimal data augmentation policies.\ntwo separate dimensions that are commonly restricted to achieve a small proxy task: model size and dataset size\nFirst, we train a family of Wide-ResNet architectures, where the model size may be systematically altered through the widening parameter governing the number of convolutional filters\nFor each of these networks, we train the model on CIFAR-10 and measure the final accuracy compared to a baseline model trained with default data augmentations (i.e. horizontal flips and pad-and-crop)\nThe Wide-ResNet models are trained with the additional K=14 data augmentations (see Section 3) over a range of global distortion magnitudes M parameterized on a uniform linear scale ranging from [0, 30]\nNamely, larger networks demand larger data distortions for Regularization\nConversely, a policy learned on a proxy task (such as AutoAugment) provides a fixed distortion magnitude (Figure 1b, dashed line) for all architectures that is clearly sub-optimal.\nA second dimension for constructing a small proxy task is to train the proxy on a small subset of the training data\nWe first observe that models trained on smaller training sets may gain more improvement from data augmentation\nsee that the optimal distortion magnitude is larger for models that are trained on larger datasets.\noptimal distortion magnitude increases monotonically with training set size\nOne hypothesis for this counter-intuitive behavior is that aggressive data augmentation leads to a low signal-to-noise ratio in small datasets\nlearned augmentation may learn an augmentation strength more tailored to the proxy task instead of the larger task of interest.\nThe dependence of augmentation strength on the dataset and model size indicate that a small proxy task may provide a sub-optimal indicator of performance on a larger task.\n\nAutomated Data Augmentation without a Proxy Task\n\nThe reason we wish to remove the search phase is because a separate search phase significantly complicates training and is computationally expensive.\nIn order to remove a separate search phase, we aspire to fold the parameters for the data augmentation strategy into the hyper-parameters for training a model.\nIndeed, previous work enumerated a policy in terms of choosing which transformations to apply out of K=14 available transformations, and probabilities for applying each transformation:\nage diversity, we replace the learned policies and probabilities for applying each\ntransformation with a parameter-free procedure of always selecting a transformation with uniform probability \\frac{1}{K}\nGiven N transformations for a training image, RandAugment may thus express K^{N} potential policies.\nmagnitude of the each augmentation distortion.\nBriefly, each transformation resides on an integer scale from 0 to 10 where a value of 10 indicates the maximum scale for a given transformation\nA data augmentation policy consists of identifying an integer for each augmentation.\nand postulate that a single global distortion M may suffice for parameterizing all transformations\nWe experimented with four methods for the schedule of M during training: constant magnitude, random magnitude, a linearly increasing magnitude, and a random magnitude with increasing upper bound\nThe resulting algorithm contains two parameters N and M\nBoth parameters are human-interpretable such that larger values of N and M increase Regularization strength\nIn order to reduce the parameter space but still maintain imInvestigating the dependence on the included transformations\nRandAugment is largely insensitive to the selection of transformations for different datasets.\nWe see that while [geometric transformations](geometric transformations.md) individually make the most difference, some of the color transformations lead to a degradation of validation accuracy on average\nSurprisingly, rotate can significantly improve performance and lower variation even when included in small subsets of RandAugment transformations, while posterize seems to hurt all subsets of all sizes.\n\nLearning the Probabilities for Selecting Image Transformations\n\nFor K=14 image transformations and N =2 operations, αij constitutes 28 parameters. We initialize all weights such that each transformation is equal probability (i.e. RandAugment), and update these parameters based on how well a model classifies a held out set of validation images distorted by αij.\nThis approach was inspired by density matching [19], but instead uses a differentiable approach in lieu of Bayesian optimization.\nWe label this method as a 1st-order density matching approximation.\nThe 1st -order method improves accuracy by more than 3.0% for both models on reduced CIFAR-10 compared to the baseline of flips and pad-and-crop\nAlthough the density matching approach is promising, this method can be expensive as one must apply all K transformations N times to each image independently.\nHence, because the computational demand of KN transformations is prohibitive for large images, we reserve this for future exploration.\nlearning the probabilities through density matching may improve the performance on small-scale tasks and reserve explorations to larger-scale tasks for the future.\nRandAugment selects all image transformations with equal probability\nThis opens up the question of whether learning K probabilities may improve performance further.\nMost of the image transformations (except posterize, equalize, and autoContrast) are differentiable, which permits backpropagation to learn the K probabilities\n\nDiscussion\n\nnot tailoring the number of distortions and the distortion magnitude to the dataset size nor the model size leads to sub-optimal performance\nIn previous work, scaling learned data augmentation to larger dataset and models have been a notable obstacle. For example, [AutoAugment] and Fast [AutoAugment](AutoAugment] and Fast [AutoAugment.md) could only be optimized for small models on reduced subsets of data\nThe proposed method scales quite well to datasets such as ImageNet and COCO while incurring minimal computational cost (e.g. 2 hyper-parameters), but notable predictive performance gains.\n\nImages\n\n\n\n\n\n\n\n\n"},"KB/Random-Directions":{"title":"Random Directions","links":[],"tags":["explainability"],"content":"Random Directions\n\n1D\n\n\n\n\n\n\n\nf(\\alpha, \\beta) = L(\\theta^{*}+ \\alpha \\delta)\n- 2D\n\t- $$\nf(\\alpha, \\beta) = L(\\theta^{*}+ \\alpha \\delta + \\beta \\eta)\n\n\\theta, \\eta are random direction vector and \\theta^{*} is optimal weights vector\n"},"KB/Random-Distortion":{"title":"Random Distortion","links":[],"tags":["augmentation"],"content":"Random Distortion\n\nThe parameters of this distortion are given in terms of grid width, grid height, and magnitude. The granularity of the distortion is controlled by the grid width and height, representing the number of horizontal and vertical divisions to apply distortion to. Both values are chosen as 6, and the magnitude of the distortion is chosen as 5. As images are only 32×32 pixels, distortion is expected to produce unrealistic examples.\n"},"KB/Random-Erasing":{"title":"Random Erasing","links":["KB/Cutout"],"tags":["augmentation"],"content":"Random Erasing\n\n@zhongRandomErasingData2020\ndeletes contiguous rectangular image regions similar to Cutout with minor differences in region selection procedure.\nOpposite to Cutout, where deletion is applied on all the images, here it is performed with a probability of either applying it or not\nIn every iteration, region size is defined randomly with upper and lower limits on region area and aspect ratio.\nAdditional to this, random erasing provides region-aware deletion for object detection and person identification tasks\nRegions inside the object bounding boxes are randomly erased to generate occlusions\n"},"KB/Random-Factors":{"title":"Random Factors","links":[],"tags":["language"],"content":"Random Factors\n\nThese are factors that can affect the outcome that we do not design, and cannot control\nTwo common types\nParticipant\n\nPeople differ. Some people are very strict. Some people accept everything.\n\n\nItem\n\nSome sentences are weird with monkeys. Some situations are hard to draw so the picture is a bit odd.\n\n\nWhen we analyze the data we have to take into account that there will be variation present because of these random factors.\n"},"KB/Random-Pruning":{"title":"Random Pruning","links":["KB/Pruning"],"tags":["regularization"],"content":"Random Pruning\n\nEach weight independantly considered and dropped with a fraction of network required\nFor this we first take the number of values to prune by identifying the total size of the weights and then multiplying it by the fraction of values to remove.\n"},"KB/Rank-(Tensor)":{"title":"Rank (Tensor)","links":[],"tags":["temp"],"content":"Rank (Tensor)\n\nThe number of dimensions in a Tensor. For instance, a scalar has rank 0, a vector has rank 1, and a matrix has rank 2.\n"},"KB/Rapid-Eye-Movement-(REM)-Sleep":{"title":"Rapid Eye Movement (REM) Sleep","links":[],"tags":["brain"],"content":"Rapid Eye Movement (REM) Sleep\n\nA stage of sleep occurring approximately 90 minutes after sleep onset characterized by increased brain activity, rapid eye movements, and muscle relaxation.\n"},"KB/Rational-inference":{"title":"Rational inference","links":[],"tags":["usermodel"],"content":"Rational Inference\n\nUses all Available cues\n"},"KB/Raycasting":{"title":"Raycasting","links":["KB/Volume-Rendering-Equation","KB/Back-To-Front-Raycasting","KB/Color-Compositing","KB/Front-to-Back-Raycasting","KB/Sampling-Ray-Casting","KB/Classification-Ray-Casting","KB/Slice-Based-Volume-Rendering","KB/Voxel-Projection"],"tags":["visualization"],"content":"Raycasting\n\nVolume Rendering Equation\nBack To Front Raycasting\nColor Compositing\nFront to Back Raycasting\nSampling Ray Casting\nClassification Ray Casting\nSlice Based Volume Rendering\nVoxel Projection\n"},"KB/ReMix":{"title":"ReMix","links":[],"tags":["augmentation"],"content":"ReMix\n\n@caoReMixImagetoImageTranslation2021\naddresses the issue of class imbalance by generating mixed images for minority classes.\nin the case of label assignment, it sets the label of the output image to the minority class.\n"},"KB/Real-Time-Image-Saliency-for-Black-Box-Classifiers":{"title":"Real Time Image Saliency for Black Box Classifiers","links":["KB/CIFAR","KB/ImageNet","KB/SSR","KB/SDR","KB/Cropping","KB/Unet"],"tags":["explainability"],"content":"Real Time Image Saliency for Black Box Classifiers\n\nPiotr Dabkowski\nYarin Gal\n@dabkowskiRealTimeImage2017\n\nTL;DR\n\nNew metric to judge how good a saliency map is using the largest rectangle that can define it. Training to reduce adversarial artefacts introduced due to masking with non smooth masks\n\nAbstract\n\nfast saliency detection method that can be applied to any differentiable image classifier\nmasking model\nmanipulate the scores of the classifier by masking salient parts of the input image\nrequires a single forward pass to perform saliency detection\nCIFAR\nImageNet\nnew metric for saliency\n\nImage Saliency and Introduced Evidence\n\nno single obvious metric that could measure the quality of the produced map\nIn simple terms, the saliency map is defined as a summarised explanation of where the classifier “looks” to make its prediction.\n\nSSR\nSDR\nIn order to be as informative as possible we would like to find a region that performs well as both SSR and SDR.\nBoth SDR and SSR remove some evidence from the image\nthere are few ways of removing evidence, for example by blurring the evidence, setting it to a constant colour, adding noise, or by completely Cropping out the unwanted parts\nUnfortunately, each one of these methods introduces new evidence that can be used by the classifier as a side effec\n\nFighting the Introduced Evidence\n\n\nby manipulating the image we always introduce some extra evidence applying a mask M to the image X to obtain the edited image E\nthe simplest case we can simply multiply X and M element-wise:\nE = X \\odot M\nThis operation sets certain regions of the image to a constant “0” colour\nWhile setting a larger patch of the image to “0” may sound rather harmless (perhaps following the assumption that the mean of all colors carries very little evidence), we may encounter problems when the mask M is not smooth\nin the worst case, can be used to introduce a large amount of additional evidence by generating adversarial artifacts\nAdversarial artifacts generated by the mask are very small in magnitude and almost imperceivable for humans, but they are able to completely destroy the original prediction of the classifier\nwe may change the way we apply a mask to reduce the amount of unwanted evidence due to specifically-crafted masks\nE = X \\odot M + A \\odot (1-M)\nwhere A is an alternative image\nA can be chosen to be for example a highly blurred version of X\nIn such case mask M simply selectively adds blur to the image X and therefore it is much harder to generate high-frequency-high-evidence artifacts\nUnfortunately, applying blur does not eliminate existing evidence very well, especially in the case of images with low spatial frequencies like a seashore or mountains.\nAnother reasonable choice of A is a random constant colour combined with highfrequency noise. This makes the resulting image E more unpredictable at regions where M is low and therefore it is slightly harder to produce a reliable artifact.\nencourage smoothness of the mask M for example via a total variation (TV) penalty\nWe can also directly resize smaller masks to the required size as resizing can be seen as a smoothness mechanism.\n\nA New Saliency Metric\n\nIn order to make sure that the preserved region is free from adversarial artifacts instead of masking we can crop the image.\nWe propose to find the tightest rectangular crop that contains the entire salient region and to feed that rectangular region to the classifier to directly verify whether it is able to recognise the requested class\ns(a,p) = log(\\overset{\\sim}a)- log(p)\n\\overset{\\sim}a = max(a, 0.05)\nHere a is the area of the rectangular crop as a fraction of the total image size and p is the probability of the requested class returned by the classifier based on the cropped region.\nThe metric is almost a direct translation of the SSR\nWe threshold the area at 0.05 in order to prevent instabilities at low area fractions.\nGood saliency detectors will be able to significantly reduce the crop size without reducing the classification probability, and therefore a low value for the saliency metric is a characteristic of good saliency detectors.\nthis measure can be seen as the relative amount of information between an indicator variable with probability p and an indicator variable with probability a—or the concentration of information in the cropped region.\nBecause most image classifiers accept only images of a fixed size and the crop can have an arbitrary size, we resize the crop to the required size disregarding aspect ratio\n\nThe Saliency Objective\n\nwant to find a mask M that is smooth and performs well at both SSR([SSR].md) and SDR\ngiven class c of interest, and an input image X, to find a saliency map M for class c, our objective function L is given by\nL(M) = \\lambda_{1}TV(M) + \\lambda_{2}AV(M) - log(f_{c}(\\Phi(X,M)))+\\lambda_{3}f_{c}(\\Phi(X, 1-M))^{\\lambda_{4}}\nfc is a softmax probability of the class c of the black box image classifier and TV(M) is the total variation of the mask defined simply as\nTV(M) = \\Sigma_{i,j}(M_{ij}-M_{ij+1})^{2}+ \\Sigma_{ij}(M_{ij}-M_{i+1j})^{2}\nAV(M) is the average of the mask elements, taking value between 0 and 1, \\lambda_{i} are regularisers\nfunction \\Phi removes the evidence from the image as introduced in the previous section\n\\Phi(X, M) = X \\odot M + A \\odot (1-M)\nIn total, the objective function is composed of 4 terms. The first term enforces mask smoothness, the second term encourages that the region is small. The third term makes sure that the classifier is able to recognise the selected class from the preserved region. Finally, the last term ensures that the probability of the selected class, after the salient region is removed, is low\nSetting \\lambda_{4} to a value smaller than 1 (e.g. 0.2) helps reduce this probability to very small values.\n\nMasking Model\n\n\nThe mask can be found iteratively for a given image-class pair by directly optimising the objective function\nUnfortunately, iteratively finding the mask is not only very slow, as normally more than 100 iterations are required, but it also causes the mask to greatly overfit to the image and a large TV penalty is needed to prevent adversarial artifacts from forming\nTherefore, the produced masks are blurry, imprecise, and overfit to the specific\nimage rather than capturing the general behaviour of the classifie\ndevelop a trainable masking model that can produce the desired masks in a single forward pass without direct access to the image classifier after training\nThe masking model receives an image and a class selector as inputs and learns to produce masks that minimise our objective function\nIn order to succeed at this task, the model must learn which parts of the input image are considered salient by the black box classifier\nn theory, the model can still learn to develop adversarial masks that perform well on the objective function, but in practice it is not an easy task, because the model itself acts as some sort of a “regulariser” determining which patterns are more likely and which are less.\nUnet Architecture\nso that the masking model can use feature maps from multiple resolutions\nThe ResNet-50 model contains feature maps of five different scales, where each subsequent scale block downsamples the input by a factor of two\nThe purpose of the feature filter is to attenuate spatial locations which contents do not correspond to the selected class.\nTherefore, the feature filter performs the initial localisation, while the following upsampling blocks fine-tune the produced masks\nThe output of the feature filter Y at spatial location i, j is given by:\nY_{ij}= X_{ij}\\sigma(X_{ij}^T C_{s})\nXij is the output of the Scale 5 block at spatial location i, j; Cs is the embedding of the selected class s and \\sigma(\\cdot) is the sigmoid nonlinearity. Class embedding C can be learned as part of the overall objective.\nThe upsampler blocks take the lower resolution feature map as input and upsample it by a factor of two using transposed convolution\nafterwards they concatenate the upsampled map with the corresponding feature map from ResNet and follow that with three bottleneck blocks\nFinally, to the output of the last upsampler block (Upsampler Scale 2) we apply 1x1 convolution to produce a feature map with with just two channels\nThe mask Ms is obtained from\nM_{s}= \\frac{abs(C_{o})}{abs(C_{o})+ abs(C_{1})}\nWe use this nonstandard nonlinearity because sigmoid and tanh nonlinearities did not optimise properly and the extra degree of freedom from two channels greatly improved training\n\nTraining Process\n\ntrain the masking model to directly minimise the objective function\nhe weights of the pre-trained ResNet encoder (red blocks in figure 4) are kept fixed\nduring the training.\nsometimes supply a class selector for a fake class and to apply only the area penalty term of the objective function.\nUnder this setting the model must pay attention to the class selector, as the only way it can reduce loss in case of a fake label is by setting the mask to zero\nDuring training, we set the probability of the fake label occurrence to 30%\nOne can also greatly speed up the embedding training by ensuring that the maximal value of \\sigma(X_{ij}^{T}C_{s}) from equation 7 is high in case of a correct label and low in case of a fake label.\nevidence removal function \\Phi(X,M)\nIn order to prevent the model from adapting to any single evidence removal scheme the alternative image A is randomly generated every time the function is called\nIn 50% of cases the image A is the blurred version of X (we use a Gaussian blur with = \\sigma=10 to achieve a strong blur) and in the remainder of cases, A is set to a random colour image with the addition of a Gaussian noise.\nSuch a random scheme greatly improves the quality of the produced masks as the model can no longer make strong assumptions about the final look of the image.\nImageNet\nthree different black-box classifiers: AlexNet [6], GoogLeNet [15] and ResNet-50 [4]\nThese models are treated as black boxes\nThe selected parameters of the objective function are \\lambda_{1} = 10, \\lambda_{2} = 103, \\lambda_{3} = 5, \\lambda_{4} = 0.3\nThe first upsampling block has 768 output channels and with each subsequent upsampling block we reduce the number of channels by a factor of two. We train each masking model as described in section 4.1 on 250,000 images from the ImageNet training set.\nThe masks produced by models trained on GoogLeNet and ResNet are sharp and precise and would produce accurate object segmentations. The saliency model trained on AlexNet produces much stronger and slightly larger saliency regions, possibly because AlexNet is a less powerful model which needs more evidence for successful classification.\n\nResults\n\n\n\n\n\nFuture Research\n\nmodifying the approach to produce high quality, weakly supervised, image segmentations\nMoreover, because our model can be run in real-time, it can be used for video saliency detection to instantly explain decisions made by black-box classifiers such as the ones used in autonomous vehicles\nLastly, our model might have biases of its own — a fact which does not seem to influence the model performance in finding biases in other black boxes according to the various metrics we used\n\nImages\n\n\n\n\n"},"KB/Reasoning-Component":{"title":"Reasoning Component","links":[],"tags":["robotics"],"content":"Reasoning Component\n\nupdates the world model and determines plans to achieve goals.\n"},"KB/Recall":{"title":"Recall","links":[],"tags":["loss"],"content":"Recall\n\n\\frac{TP}{TP+FN}\nModel needs to remember the features such that it does not miss-classify a positive case as a negative one. → How many positive samples does the model remember?\n# positive classes correctly predicted / total # positive classes\n"},"KB/Receptive-field":{"title":"Receptive field","links":["KB/Layers","KB/Conv","KB/Pooling","KB/Causal-Dilated-Conv","KB/Depthwise-Separable"],"tags":["architecture"],"content":"Receptive Field\n\nNot for Dense, only local connected Layers like Conv, Pooling\nA neuron’s receptive field is the patch of the total field of view that a single neuron has access to\nAlmost a logarithmic relationship between classification accuracy and receptive field size\n\nLarge fields are almost necessary for high level recognition tasks, but with diminishing rewards\n\n\nr(i-1) = s_{i}\\times r_{i} + (k_{i}-s_{i})\n\nRecursive\n\nr_{0} = \\Sigma^{L}_{i=1}((k_{i}-1)\\Pi_{j=1}^{l-1}s_{j})+1\n\n\nHow to increase receptive field\n\nAdd more Conv\nAdd Pooling or higher stride\nCausal Dilated Conv\nDepthwise Separable\n\n\n"},"KB/Recessive":{"title":"Recessive","links":[],"tags":["brain"],"content":"Recessive\n\nA genetic trait or disease that appears only in patients who have received two copies of a mutant gene, one from each parent.\n"},"KB/Recipe-for-constructing-loss-functions":{"title":"Recipe for constructing loss functions","links":["KB/Maximum-Likelihood","KB/Negative-Log-Likelihood"],"tags":["loss","deeplearning"],"content":"Recipe for constructing loss functions\n \n\nUsing Maximum Likelihood\nFor training data {x_{i}, y_{i}}\n\nChoose a probability distribution Pr{y|\\theta} defined over the domain of the predictions y with distribution parameters \\theta\nChoose an ML model f|x, \\phi| where \\theta = f|x, \\phi| and Pr(y|\\theta) = Pr(y|f|x, \\phi|)\nTraining → Find the parameters \\phi that minimize the Negative Log Likelihood over the training data {x_{i}, y_{i}}\nInference → Either return Pr(y|f[x, \\hat \\phi]) or the value where this distribution is minimized\n\n\nIf data is differently distributed and there is no loss associated, just transform the distribution beforehand\n"},"KB/Recommender-System":{"title":"Recommender System","links":["KB/Group-Modeling-Approach","KB/Individual-Modeling","KB/Collaborative-Recommender","KB/Content-Based-Recommender"],"tags":["usermodel"],"content":"Recommender System\n\nGroup Modeling Approach\nIndividual Modeling\nCollaborative Recommender\nContent Based Recommender\n"},"KB/Reconstruction-loss":{"title":"Reconstruction loss","links":[],"tags":["loss"],"content":"Reconstruction loss\n\nThe second loss function in capsule networks is called “reconstruction loss.” This loss function ensures the network can reconstruct an object from its lower-level features.\nThis is accomplished by training the network to reconstruct an image from the output of the capsule layers.\n"},"KB/Recurrent":{"title":"Recurrent","links":["KB/Sigmoid","KB/Tanh","tags/architecture","tags/deeplearning","KB/CLIP","KB/Softmax"],"tags":["temp","architecture","deeplearning"],"content":"Recurrent\n\nSequences as inputs/outputs\nSequential processing\nTuring complete\nmemory through state persisted between timesteps\n\noperation invariant to the sequence\nreduces no of params needed\n\n\nOutput comes back as input\n\n\n\n\nvariable sized inputs and outputs : encoder decoder\nThree weight matrices and two bias vectors.\nh_t = \\sigma_h(W_{hh}h_{t-1} + W_{xh}x_t + b_h)\ny_t = \\sigma_y(W_{hy}h_t + b_y)\nStateful : hidden state kept across batches of inputs\nActivation usually Sigmoid or Tanh\nBPTT\n\n\narchitecture\n\nIf eigen decomposition W = Q\\wedge^tQ, then h_t = Q^T\\wedge^tQ\nIf less than 0 then will converge to 0 or if bigger then will explore to infinity → long sequences\nElement wise clippingdeeplearning\n\nCLIP if bigger than value\n\n\nNorm clipping\n\nCLIP if $$||g|| &gt;vsetg = \\frac{gv}{||g||}$\nv can be decided by trial and error\n\n\n\n\n\n\nTraining stuff\n\nSoftmax but on every output vector simultaneously\n\nIf Softmax is lower (eg between 0 and 0.5). It becomes more confident and hence more conservative\nNear 0 is very diverse and less confident\n\n\nFeed a char into the RNN → distribution over characters that comes next → Sample from it → Feed it back\n\n\nSome basic patterns from here\n\nThe model first discovers the general word-space structure and then rapidly starts to learn the words.\nFirst starting with the short words and then eventually the longer ones.\nTopics and themes that span multiple words (and in general longer-term dependencies) start to emerge only much later.\n\n\n"},"KB/Redress":{"title":"Redress","links":[],"tags":["explainability"],"content":"Redress\n\nincludes mechanisms that ensure an adequate redress for situations when unforeseen unjust adverse impacts take place.\nGuaranteeing a redress for those non-predicted scenarios is a key to ensure trust. Special attention should be paid to vulnerable persons or groups.\n"},"KB/Reflex-Hammer":{"title":"Reflex Hammer","links":[],"tags":["medical"],"content":"Reflex Hammer\n\nA specially designed hammer used to test deep tendon or motor reflexes\n"},"KB/RegNet":{"title":"RegNet","links":["KB/EfficientNet"],"tags":["architecture"],"content":"RegNet\n\nnetwork design paradigm\ndiscover design principles that generalize across settings\narrives at a low-dimensional design space consisting of simple, regular networks\nwidths and depths of good networks can be explained by a quantized linear function\noutperforms EfficientNet\n"},"KB/Regex-cheatsheet":{"title":"Regex cheatsheet","links":[],"tags":["cheatsheets"],"content":"Regex Cheatsheet\nDictionary Replace\n\n\n\nMatch before or after a Character\n\n\n\nSpecific Types\n\n\n\n\nRanges of Characters\n\n\n\nRepetition\n\n\n\n\nCapture Parts of Strings\n\n\n\n\nBefore or after the Match\n\n\nNo space before, No space after, space before and after\n\n\n\n\n\nChange Matching - Start to Finish, Case, Whitespace, single_line\n\n\n\nUnicode\n\n\n\nComplicated Examples\nFind All Sentences that Ends with [! , . ?] and Replace with . or Newline and not .\n\n\n\nFind All Emails and Replace @ with [at] and . with [dot]\n\n\n\nFind All Phone Numbers\n\n\n"},"KB/Region-Growing":{"title":"Region Growing","links":["KB/Contour"],"tags":["visualization"],"content":"Region Growing\n\nAutomatic Segmentation\nRequires seed point\nLeakage through holes in Contour\n\n"},"KB/Region-Proposal":{"title":"Region Proposal","links":["KB/Features"],"tags":["architecture"],"content":"Region Proposal\n\nregion proposal generation that shares full-image convolutional Features with the detection network,\nnearly cost-free region proposals\nharing convolutional Features with the down-stream detection network\nsimultaneously predicts object bounds and objectness scores at each position\n"},"KB/Register-Renaming":{"title":"Register Renaming","links":[],"tags":["parallelcomputing"],"content":"Register Renaming\n\nused to avoid unnecessary serialization of program operations caused by the reuse of registers by those operations, in order to enable out-of-order execution\n\n"},"KB/Register-to-Register-Architecture":{"title":"Register to Register Architecture","links":["KB/SIMD"],"tags":["parallelcomputing"],"content":"Register to Register Architecture\n\nAll vector operations occur between vector registers\nIf necessary, operands are fetched from main memory into a set of vector registers (load-store unit)\nSIMD based on this\n\n"},"KB/Regularization-Rate":{"title":"Regularization Rate","links":["KB/Regularization"],"tags":["temp"],"content":"Regularization Rate\n\nA scalar value, represented as lambda, specifying the relative importance of the Regularization function.\nRaising the Regularization rate reduces overfitting but may make the model less accurate.\n\nre-ranking\n\nThe final stage of a recommendation system, during which scored items may be re-graded according to some other (typically, non-ML) algorithm. Re-ranking evaluates the list of items generated by the scoring phase, taking actions such as\n\nEliminating items that the user has already purchased.\nBoosting the score of fresher items.\n\n\n"},"KB/Regularization-Term":{"title":"Term","links":["KB/Lp-Regularization"],"tags":["regularization"],"content":"Term\n\nPenalty term\nCost function that penalizes model params \\theta with a high degree of flexibility\nh_{opt} = argmin_{h \\in \\mathcal{H}} \\frac{1}{N}\\Sigma_{i=1}^N L(h(x_i), y_i)+ \\alpha^{2}R(\\theta_h)\n\\alpha^2 determines how much regularizer affects the model\n\nLarger : soft models\nIncreasing → Down regulating flexibility\n0 = overfitting and unregularized risk\n\\infty does not care about training data at all. Only cares about minimal penalty\n\nDead model\n\n\n\n\n\nTypes\n\nLp Regularization for p =2\n\nSoft models\nSquared sum of model params\n\n\n"},"KB/Regularization":{"title":"Regularization","links":["KB/Regularization-Term","KB/Dropout","KB/VariationalRecurrent-Dropout","KB/Batch-Normalization","KB/Layer-Normalization","Tag-Pages/augmentation","KB/Lp-Regularization","KB/Pruning","KB/Effects-of-Regularization"],"tags":["regularization"],"content":"Regularization\n\nRegularization Term\nDropout\nVariationalRecurrent Dropout\nBatch Normalization\nLayer Normalization\naugmentation\nLp Regularization\nPruning\nEffects of Regularization\n"},"KB/Reinforcement-Learning":{"title":"Reinforcement Learning","links":["tags/anchor"],"tags":["temp","anchor"],"content":"Reinforcement Learning\nanchor"},"KB/Rejection-Sampling":{"title":"Rejection Sampling","links":["Sampling","sampling","KB/CDF","KB/PDF","KB/Proto-PDF","KB/Probability"],"tags":["distributions"],"content":"Rejection Sampling\n\nAlso called Best-of-n sampling\nNo CDF with a simple inverse\nImportance sampling\nUse a simpler distribution which is somewhat related to the target PDF\n\nSample by transformation\n\n\nNow if we can take a Proto PDF g_{0} \\geq f where we can sample from the PDF g\nTake a point from g with Probability f(\\tilde x)/g_{0}(\\tilde x)\n\nEither accept or reject if satisfies Probability\nIf accepted then return the sample\n\n\nDrop a point from g_{0}(x) with that Probability\nDepends on how close g is to f of course\n\n\nIf the ratio \\frac{f}{g_{0}} is small. (aka f is bigger), then there are many rejections and the algo will be slow. Impossible to not do in high dim spaces\n\n\n"},"KB/Relational-Inductive-Bias":{"title":"Relational Inductive Bias","links":["KB/Inductive-Bias","KB/Weak-Relation-Bias","KB/Locality","KB/Sequential-Relation-Bias","KB/Arbitrary-Relation-Bias"],"tags":["graph"],"content":"Relational Inductive Bias\n\na bias toward prioritizing information from neighbors\nWeak Relation Bias\nLocality\nSequential Relation Bias\nArbitrary Relation Bias\n"},"KB/Relative-Multi-Head-Self-Attention":{"title":"Relative Multi Head Self Attention","links":["KB/Self-Attention"],"tags":["architecture"],"content":"Relative Multi Head Self Attention\n\nZihangDai et al., 2019\n"},"KB/Relevance-Account":{"title":"Relevance Account","links":[],"tags":["language"],"content":"Relevance Account\n\n(Philip 2011)\npragmatic extension of the theory of Drozd and Loosbroek (2006)\nPhilip claims that the problem that children have to solve when assigning a domain to a universal quantifier is which objects in the context should be taken as relevant.\nAdults rely on their world knowledge in identifying the presupposed set\nAs formulated in the Normal World Constraint, “if an object is contextually relevant, then there is a normal situation that it is part of.”\n"},"KB/Relevance-Theory":{"title":"Relevance Theory","links":[],"tags":["language"],"content":"Relevance Theory\n\nRelevance Theory is built on the assumption that attention and thought processes automatically turn toward information that seems relevant, capable of yielding cognitive effects\nAs the Principle of Relevance states, communicated information comes with a guarantee of relevance.\nThe higher the cognitive effect of the information, and/or the more economically it is communicated, the greater its relevance.\n"},"KB/Relu":{"title":"Relu","links":["KB/Leaky-Relu","KB/PRelu","KB/Noisy-Relu"],"tags":["architecture"],"content":"Relu\n\nReLU(x) = max(0,x)\n\\frac{d}{d_x}ReLU(X) = \\begin{cases}0 &amp; x \\geq 0 \\\\ 1 &amp; otherwise \\end{cases}\nHe init\nMLP, CNN : Hidden\n\nLeaky Relu\nPRelu\nNoisy Relu\n"},"KB/Remission":{"title":"Remission","links":[],"tags":["medical"],"content":"Remission\n\nDescribes a disease that is not getting worse\n"},"KB/RepLKNet":{"title":"RepLKNet","links":[],"tags":["architecture"],"content":"RepLKNet\n\nimpressively manages to scale the kernel size to 31×31 with improved performance\nthe performance starts to saturate as the kernel size continues growing, compared to the scaling trend of ad- vanced ViTs such as Swin Transformer\nexplore the possibility of training extreme convolutions larger than 31×31 and test whether the performance gap can be eliminated by strategically enlarging convolutions.\n"},"KB/RepVGG":{"title":"RepVGG","links":["KB/Conv","KB/Relu","Sampling","KB/ImageNet","KB/EfficientNet","KB/RegNet"],"tags":["temp"],"content":"\ntoc: true\ntitle: RepVGG\ntags: [‘temp’]\n\nRepVGG\n\nRepVGG: Making Vgg-style ConvNets Great Again\n\nstack of 3\\times3 Conv and Relu during inference time\ntraining-time model has a multi-branch topology\ndecoupling of the training-time and inference-time architecture is realized by a structural re-parameterization technique\n5 stages and conducts down-Sampling via stride-2 convolution at the beginning of a stage\nidentity and 1 \\times 1 branches, but only for training\nImageNet\nhigher accuracy and show favorable accuracy-speed trade-off compared to the state-of-the-art models like EfficientNet and RegNet\n\n\n\n"},"KB/Representation-Bias":{"title":"Representation Bias","links":["KB/Features"],"tags":["multitask"],"content":"Representation Bias\n\nAs multitask is performed, introduces bias to learn Features and generalizing for other tasks\nincreases chances that more general features are learned\n"},"KB/Representational-Capacity":{"title":"Representational Capacity","links":[],"tags":["deeplearning","architecture"],"content":"Representational Capacity\n \n\nrepresentational capacity - the space of possible functions it can construct when we consider all possible parameter values\n"},"KB/Representing-Graphs":{"title":"Representing Graphs","links":["KB/Graphs","KB/Node-Link-Diagram","KB/Adjacency-matrix"],"tags":["graph"],"content":"Representing Graphs\n \nRepresenting Graphs\n\nNode Link Diagram\nAdjacency matrix\n"},"KB/Res-Net-D":{"title":"Res Net D","links":["KB/Res-Net"],"tags":["architecture"],"content":"Res Net D\n\n\n"},"KB/Res-Net":{"title":"Res Net","links":["KB/Issues","KB/Vanishing-Gradient","KB/Vgg","KB/Skip-Connection"],"tags":["architecture"],"content":"Res Net\n\n@heDeepResidualLearning2016\nDeep Residual Learning for Image Recognition\nDeeper Networks have Issues because of Vanishing Gradient\nPropagate gradients forward for deeper networks\noutput of F(x) has the same dims as x → add\nIf only spatial dims match (aka not channels) → concat\nless params than Vgg\nSkip Connection\nSadly, one of the creators Jian Sun passed away yesterday. (16-6-22)\n"},"KB/ResNeXt":{"title":"ResNeXt","links":[],"tags":["architecture"],"content":"ResNeXt\n \n\nplaces a residual connection around multiple parallel convolutional branches.\n"},"KB/Rescorla-Wagner-Algorithm":{"title":"Rescorla-Wagner Algorithm","links":["KB/Attention","KB/Features","KB/Cross-situational-learning","KB/Propose-but-verify","KB/Rescorla-Wagner-Blocking"],"tags":["language"],"content":"Rescorla-Wagner Algorithm\n\nRescorla &amp; Wagner (1972): animals and humans also learn associations by paying Attention to what is not associated.\n\\Delta V = \\alpha \\beta_{1}\\beta_{2}(\\lambda = \\Sigma V)\n▶ V = association strength ▶ ∆ V : Change in association strength ▶ λ = maximum values of the unconditional stimulus ▶ Set to 1: when US is present (food) ▶ Set to 0: when not present ▶ α = learning rate ▶ β = varies the effects of negative or positive evidence ▶ ΣV = sum of associated strengths for all cues/Features/conditions stimuli\nnegative instances are also useful to learning\nLogical Problem of Lang Acquisition\nChildren don’t get negative evidence = must be innate\nCross-situational learning\nPropose-but-verify\nRescorla-Wagner Blocking\nRescorla-Wagner = error-driven\nAfter a strong association is made, as long as it is confirmed by data, no new learning will occur\nThe model only learns when the predicted outcome differs from actual outcome\n"},"KB/Rescorla-Wagner-Blocking":{"title":"Rescorla-Wagner Blocking","links":["KB/Features"],"tags":["language"],"content":"Rescorla-Wagner Blocking\n\nIf an outcome is already strongly associated with a stimuli (cue or feature), the presence of other Features, regardless of how consistent they are, will not lead to a new association\n"},"KB/Research-Debt":{"title":"Research Debt","links":["KB/Interpretive-Labor","KB/Clear-Thinking","KB/Research-Distillation"],"tags":["random"],"content":"Research Debt\n\nResearch Debt\nAchieving a research-level understanding of most topics is like climbing a mountain.\nAspiring researchers must struggle to understand vast bodies of work that came before them, to learn techniques, and to gain intuition\nUpon reaching the top, the new researcher begins doing novel work, throwing new stones onto the top of the mountain and making it a little taller for whoever comes next.\nPeople expect the climb to be hard\nIt reflects the tremendous progress and cumulative effort that’s gone into mathematics\nThe climb is seen as an intellectual pilgrimage, the labor a rite of passage\nBut the climb could be massively easier\nIt’s entirely possible to build paths and staircases into these mountains.\nThat is, really outstanding tutorials, reviews, textbooks, and so on.\ntechnical debt\ninstitutional debt\nPoor Exposition\nInterpretive Labor\nClear Thinking\nResearch Distillation\n"},"KB/Research-Distillation":{"title":"Research Distillation","links":["KB/Research-Debt"],"tags":["random"],"content":"Research Distillation\n\nIt can be incredibly satisfying, combining deep scientific understanding, empathy, and design to do justice to our research and lay bare beautiful insights.\nDistillation is also hard\nIt’s tempting to think of explaining an idea as just putting a layer of polish on it, but good explanations often involve transforming the idea.\nThis kind of refinement of an idea can take just as much effort and deep understanding as the initial discovery.\nWe can’t solve Research Debt by having one person write a textbook\n"},"KB/Research-Engineer-in-Human-Modeling-for-Automated-Driving-delft":{"title":"Research Engineer in Human Modeling for Automated Driving delft","links":[],"tags":["architecture"],"content":"Research Engineer in Human Modeling for Automated Driving - Subhaditya Mukherjee\nAs AI gets more advanced, applications such as automated driving become more feasible. But the real challenge is ensuring responsible and reliable implementations of algorithms with a safety-first mindset. That being the case, I am quite interested in contributing to research in this domain. I remember working on a similar project in my bachelors with a startup for a little bit, but sadly, COVID-19 put a halt to their operations, and I could not continue with them. The work was quite interesting though, and I was sad about losing the opportunity back then. So, when this position showed up in my search, I did not want to miss out again.\nAs of a month ago, I have a masters in AI from the University of Groningen, which now also puts me in a better position to tackle the challenges faced in developing these models. My interest is at the intersection of applied AI and explainability, and I am quite familiar with Python, deep learning frameworks such as PyTorch and Tensorflow and other tricks of the trade. Interestingly enough, I also enjoy teaching and was a TA for three courses in my masters as well. While I bring programming, AI and computer vision skills to the table, I am willing to learn whatever else is required for the project. I firmly believe in human-in-the-loop solutions to life-critical problems such as this one, and I can also contribute to the explainability and fairness of the system.\nI believe that this position would be the perfect next step for me as it not only is a project with real-world impact, but also involves quite a bit of interdisciplinary research. I hope you give me a chance, and I look forward to hearing back from you.\nThank you,\nSubhaditya Mukherjee"},"KB/Research-Intimacy":{"title":"Research Intimacy","links":[],"tags":["random"],"content":"Research Intimacy\n\nInternalizing obscure knowledge, equations, relationships, and ways of thinking related to a research topic.\nlink\n"},"KB/Resistance":{"title":"Resistance","links":[],"tags":["temp"],"content":"Resistance\n\nresistance = resistivity x length \n                  cross sectional area\nR = r/A\n"},"KB/ResizeMix":{"title":"ResizeMix","links":["KB/Cropping"],"tags":["augmentation"],"content":"ResizeMix\n\n@qinResizeMixMixingData2020\nperforms random image Cropping and pasting\nResizeMix solves the random region Cropping problem that misallocates the output image label in certain cases where the pasted region does not contain any object informa- tion\nscaling down (scale rate is sampled from a uniform dis- tribution) the selected image completely and past- ing it randomly on the target image\nmodified labels of the output image are always accurate and proportionate to the mixing.\n"},"KB/Response-Based-Knowledge":{"title":"Response Based Knowledge","links":["KB/Logits","KB/Distillation-Loss"],"tags":["knowledgedistillation"],"content":"Response Based Knowledge\n\nneural response of the last output layer of the teacher model. The main idea is to directly mimic the final prediction of the teacher model.\nGiven a vector of Logits z as the outputs of the last fully connected layer of a deep model, the Distillation Loss for response-based knowledge can be formulated as\nresponse in object detection task may contain the Logits together with the offset of a bounding box (Chen et al., 2017). In semantic landmark localization tasks, e.g., human pose estimation, the response of the teacher model may include a heatmap for each landmark (Zhang et al., 2019a)\n"},"KB/Restricted-Boltzmann-Machine":{"title":"Restricted Boltzmann Machine","links":[],"tags":["architecture"],"content":"Restricted Boltzmann Machine\n\nStart of Deep learning\n"},"KB/RetinaNet":{"title":"RetinaNet","links":["KB/Dense","KB/Focal-Loss"],"tags":["architecture"],"content":"RetinaNet\n\nSimple Dense detector for Focal Loss\n"},"KB/Reuptake":{"title":"Reuptake","links":[],"tags":["brain"],"content":"Reuptake\n\nA process by which released neurotransmitters are absorbed for subsequent re-use.\n"},"KB/Revisiting-variable-foreperiod-effects-evaluating-the-repetition-priming-account":{"title":"Revisiting variable foreperiod effects evaluating the repetition priming account","links":[],"tags":["cognitivemodel"],"content":"Revisiting variable foreperiod effects evaluating the repetition priming account\n\nTianfang Han &amp; Robert W. Proctor\n\nAbstract\n\nA warning signal preceding an imperative stimulus by a certain foreperiod can accelerate responses (foreperiod effect)\nWhen foreperiod is varied within a block, the foreperiod effect on reaction time (RT) is modulated by both the current and the prior foreperiod\nThe multiple-trace theory of Los et al. (Frontiers in Psychology, 5, Article 1058, 2014) attributes the slope of the foreperiod-RT function to the foreperiod distribution\nwith a non-aging foreperiod distribution, the variableforeperiod paradigm yields unequal sequential-effect sizes at the different foreperiods, consistent with the multiple-trace theory but contrary to Capizzi et al.’s repetition-priming account\nThe foreperiod-RT functions are similar to those of the fixedforeperiod paradigm, which is not predicted by the multiple trace theory\n\nExperiment 1\n\nBoth the foreperiods and foreperiod distribution were the same, but a choice-reaction task\nResponses were faster when the current foreperiod was 400 ms compared to 1,400 ms\nwas also a main effect of Foreperiod Sequence,\nResponses were faster when the current foreperiod was the same as the previous one compared to when they were different.\nthe interaction of Current Foreperiod × Foreperiod Sequence indicated that the SFP effect was larger at the short foreperiod than at the long foreperiod\nThis result is inconsistent with the repetition priming account of Capizzi et al., according to which the SFP effects should be of similar sizes at short and long foreperiods\nThe larger SFP effect at short than long foreperiods is consistent with Los et al.’s (2014) multiple trace theory\nThis is because it assumes that a long previous foreperiod produces inhibition to the critical moment of the short foreperiod but a short previous foreperiod does not affect the preparation at the critical moment of the long foreperiod\nsymmetric sequential effects are more likely to be found in choicereaction tasks compared to a simple reaction scenario.\nthe larger proportion of shorter foreperiod trials could be the basis of that foreperiod’s advantage in terms of response speed by having more previous memory traces contributing to the activation at the shorter foreperiod’s critical moment\nAlternatively, the increasing foreperiod-RT function in Experiment 1 shared the same direction as in a fixedforeperiod paradigm\nwithout the effect from the additional processes in a variable-foreperiod paradigm, the foreperiod-RT relation in the two foreperiod paradigms will be in the same direction\n\nExperiment 2\n\ntwo very short foreperiods (50 ms and 200 ms) were used\nThis prediction means that the shorter preceding foreperiod should produce faster responses regardless of the length of the current foreperiod\nBased on the current assumptions of MTP, the preparation at any foreperiod is determined by the activationinhibition states (strengths of activation and inhibition) stored in each [memory trace], the strength of each [memory trace] ([memory trace](memory trace], the strength of each [memory trace] ([memory trace.md) is more dispersed and weaker as the foreperiod gets longer), and the total number of previous memory traces with each foreperiod\nParticipants were more likely to make errors when encountering foreperiod repetition compared to alternation\nmain effect of Current Foreperiod was not significant\nThe interaction between Foreperiod Sequence and Current Foreperiod was not significant\nFirst, the main effect of Current Foreperiod was found, indicating a decreasing foreperiod-RT function in the short-foreperiod scenario\nThis direction is consistent with the prediction based on the fixed-foreperiod effect but this foreperiod-RT function, especially its opposite direction from that observed in Experiment 1, cannot be predicted from the current assumptions of the MTP\nA small interaction was found between Current Foreperiod and Foreperiod Sequence\nIt is worth noting that participants were more likely to make errors when the current foreperiod matched the previous one compared to when it did not\n\nExperiment 3\n\nThe main effect of Current Foreperiod was not significant\nneither was the interaction between Current Foreperiod and Foreperiod Sequence\nWith regard to the variable-foreperiod effect, although a significant main effect was not detected (p = .085), the numerical difference in RT at the two foreperiods pointed in the same direction as the significant fixedforeperiod effect\n\nDiscussion\n\nLos et al. (2017) used a visual warning signal and a visual imperative stimulus and found that blocks with the same foreperiod distribution (exponential or antiexponential) induced a short-term carryover effect on the foreperiod-RT function in subsequent blocks with a uniform distribution\nCrowe and Kent (2019) used an auditory pair of stimuli and found a similar but more limited carryover effect (lasting for only one block)\nimply that having the fixed-foreperiod blocks performed immediately after the variable-foreperiod blocks could have made it more difficult to measure the fixed-foreperiod effect precisely, which could be a potential limitation of the current design\nThe key step of reconnecting the two foreperiod paradigms was taken by Los et al. (2014), in which a simplified version of the MTP without the activation-inhibition ratio was used to account for the fixedforeperiod effect\nA lower maximum and greater temporal dispersion as the imperative moment is moved further from the warning signal were added to predict a shorter RT at the short foreperiod\nby using a non-aging foreperiod distribution, the variableforeperiod effect would get back to its baseline, which is the foreperiod-RT function in a fixed-foreperiod paradigm\n\nConclusion\n\nThe results of this study suggest that the SFP effect reflects a benefit of repetition, which can be attributed to the memory of prior trials\nSFP effect was larger at the shorter foreperiod, which is consistent with MTP\nOn the other hand, we showed that in a variable foreperiod paradigm, when the conditional probability of the imperative stimulus appearing at the next foreperiod stays constant over time, the foreperiodRT function follows the foreperiod-RT relation in a fixed foreperiod paradigm.\nThis consistency between different foreperiod paradigms is not predicted by the MTP, which attributes the foreperiod-RT function to the proportions of foreperiods\n\nImages\n\n\n\n\n\n\n"},"KB/Revolute-Joint":{"title":"Revolute Joint","links":[],"tags":["robotics"],"content":"Revolute Joint\n\nThe joints of a robot, which are capable of rotary motion.\n"},"KB/Ridge-Regression":{"title":"Ridge Regression","links":["KB/LinearRegression"],"tags":["temp"],"content":"Ridge Regression\n\nLinearRegression\n(XX&#039;) suffers from numerical instability when almost singular (real world data. sparse I think)\nAdd a tiny term\n\nw&#039;_{opt} = (XX&#039; + \\alpha ^2 I_{nxn})^{-1}XY\nSolution to overfitting\n\n\n"},"KB/Risk-Mitigation":{"title":"Risk Mitigation","links":[],"tags":["robotics"],"content":"Risk Mitigation\n\nA secondary step in the risk assessment process that involves reducing the level of risk for the identified tasks, by applying risk reduction measures in order to eliminate or mitigate the hazards.\n"},"KB/Rmsprop":{"title":"Rmsprop","links":["KB/Adagrad"],"tags":["architecture"],"content":"Rmsprop\n\nRL\nMore stable than Adagrad\nMoving exponential avg : older grads given less weight\n\n\n\n\n&amp; E[g^{2}]{t}= 0.9E[g^{2}]{t-1}+ 0.1g^{2}_{t}\\\n&amp; \\theta_{t+1}= \\theta_{t}- \\frac{\\eta}{\\sqrt{E[g^{2}]){t}+\\epsilon}}g{t}\n\\end{align}\n- Suggested $\\gamma=0.9$ and $\\eta= 0.001$"},"KB/RoBERTa":{"title":"RoBERTa","links":["KB/GLUE","KB/RACE","KB/SQuAD"],"tags":["architecture"],"content":"RoBERTa\n\nRoBERTa: a Robustly Optimized BERT Pretraining Approach\nevaluates a number of design decisions when pretraining BERT models\nThey find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it.\nperformance can be substantially improved by training the model longer, with bigger batches over more data; removing the next sentence prediction objective; training on longer sequences; and dynamically changing the masking pattern applied to the training data\nGLUE\nRACE\nSQuAD\nonly the masked language model objective\n"},"KB/Robot-Range-Limit-Monitoring":{"title":"Robot Range Limit Monitoring","links":["KB/Manipulator"],"tags":["robotics"],"content":"Robot Range Limit Monitoring\n\nMonitors the Manipulator arm or its tool to be in the designated safety area\n"},"KB/Robotic-Joints":{"title":"Robotic Joints","links":["KB/Manipulator","KB/Actuator","KB/Rotary-Joint","KB/Prismatic-Joint"],"tags":["robotics"],"content":"Robotic Joints\n\nJoints connect the Manipulator links\nA joint normally provides one Controllable Degree of Freedom (CDOF)\nFor each CDOF one separate Actuator is needed\nAn endeffector with many (C)DOFs needs a lot of actuators!\nRotary Joint , Prismatic Joint\n"},"KB/Robust-RegNet":{"title":"Robust RegNet","links":["KB/RegNet","KB/Unsupervised-Learning","KB/Self-Supervised","KB/ImageNet","KB/Features","SwaV"],"tags":["temp"],"content":"\ntoc: true\ntitle: Robust RegNet\ntags: [‘temp’]\n\nRobust RegNet\n\nVision Models are More Robust and Fair When Pretrained on Uncurated Images Without Supervision\nUnsupervised Learning\nDiscriminative Self Supervised learning allows training models on any random group of internet images, and possibly recover salient information that helps differentiate between the images\nImageNet\nobject-centric Features that perform on par with supervised Features on most object-centric downstream tasks\nlearn any salient and more representative information present in diverse unbounded set of images from across the globe\nwithout any data pre-processing or prior assumptions about what we want the model to learn\nRegNet\nscaled to a dense 10 billion parameters\npre-trained using the SwaV self-supervised method on a large collection of 1 billion randomly selected public images from Instagram with a diversity of gender, ethnicity, cultures, and locations\ncaptures well semantic information\ncaptures information about artistic style and learns salient information such as geo-locations and multilingual word embeddings based on visual content only.\nlarge-scale self-supervised pre-training yields more robust, fair, less harmful, and less biased results than supervised models or models trained on object centric datasets such as ImageNet\n"},"KB/Robust-regression":{"title":"Robust regression","links":["KB/MAE","KB/MSE","KB/Laplace-Distribution"],"tags":["loss"],"content":"Robust regression\n \n\nregression models that minimize MAE rather than MSE\nLaplace Distribution estimates the median output for a given input rather than the mean\n"},"KB/Rod":{"title":"Rod","links":[],"tags":["brain"],"content":"Rod\n\nA type of photoreceptor, usually found on the outer edges of the retina, that helps facilitate peripheral vision.\n"},"KB/Roll":{"title":"Roll","links":["KB/Manipulator"],"tags":["robotics"],"content":"Roll\n\nRotation of the robot end effector in a plane perpendicular to the end of the Manipulator arm.\n"},"KB/Rotary-Joint":{"title":"Rotary Joint","links":[],"tags":["robotics"],"content":"Rotary Joint"},"KB/Rotary-Vector-Drive-(RV)":{"title":"Rotary Vector Drive (RV)","links":["KB/Cyclo-Drive","KB/Harmonic-Drive"],"tags":["robotics"],"content":"Rotary Vector Drive (RV)\n\nA brand name for a speed reduction device that converts high speed low torque to low speed high torque, usually used on the major (larger) axis . See Cyclo Drive and Harmonic Drive.\n"},"KB/Rotational-Invariance":{"title":"Rotational Invariance","links":[],"tags":["temp"],"content":"Rotational Invariance\n\nIn an image classification problem, an algorithm’s ability to successfully classify images even when the orientation of the image changes. For example, the algorithm can still identify a tennis racket whether it is pointing up, sideways, or down. Note that rotational invariance is not always desirable; for example, an upside-down 9 should not be classified as a 9.\n"},"KB/Routing-by-Agreement":{"title":"Routing by Agreement","links":[],"tags":["architecture"],"content":"Routing by Agreement\n\nThe outputs of the higher-layer capsules are determined by a process called “routing by agreement,”\ndetermines the strength of the connection between capsules in different layers.\n"},"KB/Runge-Kutta":{"title":"Runge Kutta","links":[],"tags":["visualization"],"content":"Runge Kutta\n\nFourth Order\n\n\n"},"KB/S2ST":{"title":"S2ST","links":["KB/Modality","KB/Fisher-Spanish-English"],"tags":["architecture"],"content":"S2ST\n\nDirect Speech-to-speech Translation with Discrete Units\ndirect speech-to-speech translation (S2ST) model that translates speech from one language to speech in another language without relying on intermediate text generation\nself-supervised discrete speech encoder on the target speech\ntraining a sequence-to-sequence speech-to-unit translation\nmodel to predict the discrete representations of the target speech\nWhen target text transcripts are available, they design a joint speech and text training framework that enables the model to generate dual Modality output (speech and text) simultaneously in the same inference pass\nFisher Spanish-English\n"},"KB/SAM-ResNet":{"title":"SAM-ResNet","links":[],"tags":["explainability"],"content":"SAM-ResNet\n\n@corniaPredictingHumanEye2018\nuses LSTM to compute an attention map. The feature map extracted by ResNet is input to attentive convolutional LSTM that focuses on the most salient regions of the input image to iteratively refine the predicted saliency map.\nWe recently proposed a Saliency Attentive Model (SAM) which, in contrast, incorporates attentive mechanisms to iteratively refine saliency predictions. Overall, it is composed by three main components: a Dilated Convolutional Network that extracts feature maps from the input image, an Attentive Convolutional LSTM which recurrently enhances saliency features and a learned prior module that incorporates the human-gaze center bias in the final predictions\n\nAttentive Convolutional LSTM\n\nThe feature maps coming from the dilated network are then input to an Attentive Convolutional model, which recurrently process saliency features at different locations.\nMoreover, we exploit the sequential nature of LSTM to process features in an iterative way. The input of the LSTM is computed, at each step, through an attentive mechanism which focuses on different regions of the image.\nAn attention map is generated by convolving the previous hidden state and the input (i.e. a stack of feature maps); once normalized through the softmax operator, this is applied to the input with an element-wise product\nAfter a fixed number of iterations, the last hidden state is taken as the output of this module.\n\nLearned Priors.\n\nFinally, the output of the Attentive LSTM is combined with multiple learned priors which are used to model the center bias present in the human-eye fixations. Differently from existing works, which included predefined priors, we let the network learn its own priors. To reduce the number of parameters and facilitate the learning, we constraint that each prior should be a 2d Gaussian function, whose mean and covariance matrix are freely learnable. In this manner, priors are inferred purely from data,\nThanks to this strategy, the predicted saliency maps are rescaled, for both versions, by a factor of 8 instead of 32 as in the original CNNs.\nIn our work, we go beyond classical feed-forward networks to predict saliency maps and\npropose a Saliency Attentive Model which incorporates neural attention mechanisms to iteratively refine predictions\nHere, we provide experimental results on other popular saliency datasets to confirm the effectiveness and the generalization capabilities of our model, which enable us to reach the state of the art on all considered datasets.\n\nLoss Function\n\nlinear combination of three saliency evaluation metrics: the Normalized Scanpath Saliency (NSS), the Linear Correlation Coefficient (CC) and the KullbackLeibler Divergence (KL-Div)\nSALICON\nomposed by 20, 000 images with corresponding saliency maps computed from mouse movements\n\nImages\n\n\n\n\n\n"},"KB/SBU-Captions":{"title":"SBU Captions","links":[],"tags":["dataset"],"content":"SBU Captions"},"KB/SDR":{"title":"SDR","links":[],"tags":["loss"],"content":"SDR\n\nSmallest destroying region\nsmallest region of the image that when removed, prevents a confident classification.\n"},"KB/SELU":{"title":"SELU","links":["KB/Elu","KB/Initialization","KB/Standard-Deviation","KB/Layers","Vanishingexploding-gradients","KB/Relu"],"tags":["architecture"],"content":"SELU\n\nselu(x) = \\lambda x , \\text{if } x &gt;0 , \\text{else }, \\alpha(e^{x}-1)\n\nInformation\n\nPaper: arxiv.org/pdf/1706.02515.pdf\nscaled variant of the Elu function\ndoes internal normalization (“self-normalizing”)\n\neach layer preserves the mean and the variance from the previous one\nnormalization happens within the activation function\nto work:\n\ninput features must be standardized\narchitecture must be sequential\n\nself-normalizing not guaranteed otherwise\n\n\nSELU as activation \ncustom Initialization\n\nzero mean\nStandard Deviation: \\sqrt{\\frac{1}{ \\#inputs}}\n\n\nif all Layers are dense (in paper), but other research showed that it also works for CNNs\n\n\n\n\nhas two fixed parameters α and λ\n\nnot hyperparameters nor learnt parameters\nderived from the inputs (μ=0, std=1)\nα≈1.6732, λ≈1.0507\n\n\nPros:\n\nno Vanishingexploding gradients\ncannot die as Relu\nconverges faster and to a better result than other activation functions\nsignificantly outperformed other activation functions for deep networks\n\n\nCons:\n\nComputational heavier\n\n\n"},"KB/SGD-Momentum":{"title":"SGD Momentum","links":[],"tags":["gradients"],"content":"SGD Momentum\n\n\n\n\n\n&amp;v_{t}= \\gamma v_{t+1}+\\eta \\cdot \\nabla_{\\theta}J(\\theta) \\\n&amp;\\theta = \\theta- v_{t}\\\n\\end{align}$$"},"KB/SGD":{"title":"SGD","links":[],"tags":["gradients"],"content":"SGD\n\ninstead of taking the whole dataset for each iteration, we randomly select the batches of data\nThe procedure is first to select the initial parameters w and learning rate n. Then randomly shuffle the data at each iteration to reach an approximate minimum.\nfull of noise\nDue to an increase in the number of iterations, the overall computation time increases.\n\\theta = \\theta - \\eta \\cdot \\nabla_{\\theta} J(\\theta ; x^{i}; y^{i} )\n\nFor each example x^{i} and label y^{i}\n\n\n\nImplicit Regularization\n\n\n"},"KB/SHAP":{"title":"SHAP","links":[],"tags":["uncertainty"],"content":"SHAP\n\n@lundbergUnifiedApproachInterpreting2017\nhelp users interpret the predictions of complex models\nunclear how these methods are related and when one method is preferable over another\nunified framework for interpreting predictions\nSHAP\nSHapley Additive exPlanations\ngame theoretic approach to explain the output of any machine learning model\nconnects optimal credit allocation with local explanations using the classic Shapley values from game theory and their related extensions\nassigns each feature an importance value for a particular prediction\nidentification of a new class of additive feature importance measures\ntheoretical results showing there is a unique solution in this class with a set of desirable properties\nnotable because several recent methods in the class lack the proposed desirable properties\npresent new methods that show improved computational performance and/or better consistency with human intuition than previous approaches\n"},"KB/SIMD":{"title":"SIMD","links":["KB/Vector-Processor","KB/Amdahl's-Law","KB/MIMD","KB/Time-space-duality"],"tags":["parallelcomputing"],"content":"SIMD\n\nSingle instruction on multiple data\nGraphics, Image processing\nSynchronous, Deterministic\nGPU\n\nVector Processor\nLimited by Amdahl’s Law\nMore energy efficient than MIMD\nTime space duality\n"},"KB/SISD":{"title":"SISD","links":[],"tags":["parallelcomputing"],"content":"SISD\n\nSingle Instruction, Single Data\nDeterministic\n-\n"},"KB/SLAK":{"title":"SLAK","links":["KB/Sparsity","KB/RepLKNet","KB/Large-Kernel-in-Attention","KB/Large-Kernel-in-Convolution","KB/Dynamic-Sparsity","KB/Sparse-Evolutionary-Training","KB/Downsampling"],"tags":["architecture"],"content":"SLAK\n\n\nMORE CONVNETS IN THE 2020S: SCALING UP KERNELS BEYOND 51 × 51 USING Sparsity\n\n\nShiwei Liu, Tianlong Chen, Xiaohan Chen, Xuxi Chen, Qiao Xiao, Boqian Wu, Tommi Ka ̈ rkka ̈ inen, Mykola Pechenizkiy, Decebal Constantin Mocanu, Zhangyang Wang\n\n\nThe dominant role of convolutional neural networks (CNNs) seems to be challenged by increasingly effective transformer-based models.\n\n\nadvanced convolutional models strike back with large ker- nels motivated by the local-window attention mechanism, showing appealing perfor- mance and efficiency\n\n\nRepLKNet\n\n\nThis study ends up with a recipe for applying extremely large kernels from the perspective of Sparsity, which can smoothly scale up kernels to 61×61 with better performance\n\n\nBuilt on this recipe, we propose Sparse Large Kernel Network (SLaK), a pure CNN architec- ture equipped with sparse factorized 51×51 kernels that can perform on par with or better than state-of-the-art hierarchical Transformers and modern ConvNet architec- tures like ConvNeXt and RepLKNet\n\n\nRELATED WORK\n\nLarge Kernel in Attention\nLarge Kernel in Convolution\nDynamic Sparsity\nSparse Evolutionary Training\n\nFAILURES OF EXISTING APPROACHES TO GO BEYOND 31x31 KERNELS\n\nIt is important to note that all models are trained for a reduced length of 120 epochs in this section, just to sketch the scaling trends of large kernel sizes.\nFollowing the design in RepLKNet, we set the kernel size of each stage as [51, 49, 47, 13] and [61, 59, 57, 13],\nnaively enlarging kernel size from 7x7 to 31x31 decreases the performance although the receptive field may be enlarged by using extremely large kernels, it\nmight fail to maintain the desirable property of locality.\nSince the stem cell in standard ResNet (He et al., 2016) and ConvNeXt results in a\n4x Downsampling of the input images, extreme kernels with\n\nA RECIPE FOR EXTREMELY LARGE KERNELS BEYOND 31x31\n\nDecomposing a large kernel into two rectangular, parallel kernels smoothly scales the kernel size up to 61x61\nAlthough using convolutions with medium sizes (e.g., 31x31) seemingly can directly avoid this problem, we want to investigate if we can further push the performance of CNNs by using (global) extreme convolutions\napproximate the large MxM kernel with a combination of two parallel and rectangular convolutions whose kernel size is MxN and NxM (where N &lt; M), respectively, as shown in Figure 1. Following Ding et al. (2022), we keep a 5x5 layer parallel to the large kernels and summed up their outputs after a batch norm layer.\nThis decomposition balances between capturing long-range dependencies and extracting local detail features\nIn stark contrast, the overhead of our method increases just linearly with the kernel size\nAs the decomposition reduces learnable parameters and FLOPs, it is no surprise to observe our network to initially sacrifice accuracy slightly compared to the original RepLKNet at medium kernel sizes i.e. 31x31\nHowever, as the convolution size continues to increase, our method can scale kernel size up to 61x61 with improved performance.\n\nUse Sparse Groups, Expand More Width\n\nsignificantly boosts the model capacity.\nInstead of using the standard group convolution, ConvNeXt simply employs depthwise convolutions with an increased width to achieve the goal of “use more groups, expand width”. In this paper, we attempt to extend this principle from a Sparsity-inspired perspective – “use sparse groups, expand more width”.\nreplace the dense convolutions with sparse convolutions, where the sparse kernels are randomly constructed based on the layer-wise Sparsity ratio of SNIP (Lee et al., 2019)\nAfter construction, we train the sparse model with dynamic Sparsity (Mocanu et al., 2018; Liu et al., 2021b), where the sparse weights are dynamically adapted during training by pruning the weights with the lowest magnitude and growing the same number of weights randomly.\nDoing so enables dynamic adaptation of sparse weights, leading to better local features.\nAs kernels are sparse throughout training, the corresponding parameter count and training/inference FLOPs are only proportional to the dense models.\ndynamic Sparsity notably reduces more than 2.0 GFLOPs, despite causing temporary performance degradation.\nDynamic Sparsity allows us to computation-friendly scale the model size up\nFor example, using the same Sparsity (40%), we can expand the model width by 1.3x while keeping the parameter count and FLOPs roughly the same as the dense model\n\nLarge Kernels Generalize Better Than Small Kernels with Our Recipe\n\nperformance consistently increases with kernel size, up to 51x51\nApplying each part of our proposed recipe to 7x7 kernels leads to either no gain or marginal gains compared to our 51x51 kernels. This break-down experiment justifies our claim: large kernel is the root of power, and our proposed recipe helps unleash such power from large kernels.\n\nSLAK\n\nSLaK is built based on the architecture of ConvNeXt\nThe design of the stage compute ratio and the stem cell are inherited from ConvNeXt\nThe number of blocks in each stage is [3, 3, 9, 3] for SLaK-T and [3, 3, 27, 3] for SLaK-S/B\nThe stem cell is simply a convolution layer with 4x4 kernels and 4 strides. Page 6\nWe first directly increase the kernel size of ConvNeXt to [51, 49, 47, 13] for each stage, and replace each MxM kernel with a combination of Mx5 and 5xM kernels\nWe find that adding a BatchNorm layer directly after each decomposed kernel is crucial before summing the output up\nurther sparsify the whole network and expand the width of stages by 1.3x, ending up with SLaK\n\nEVALUATION OF SLAK ImageNet-1K\n\nADE20K\nPASCAL VOC 2007\nCOCO\nSLaK is not only able to capture long-range dependence but also the local context features.\nIn comparison, high-contribution pixels of SLaK spread in a much larger ERF, and some high-contribution pixels emerge in non-center areas.\nSLaK balances between capturing long-range dependencies and focusing on the local details.\nSLaK seems to automatically recover the inductive bias of peripheral vision (Lettvin et al., 1976; Min et al., 2022) in the human vision system: the entire visual field is partitioned into multiple regions from near the gaze center to distant areas; humans have high- resolution processing near the gaze center (central and para-central regions), and decrease the resolution of processing for mid and far peripheral regions.\n\nKERNEL SCALING EFFICIENCY\n\nWe simply replace all the kernels in stages of ConvNeXt-T with a set of kernel sizes from 7 to 151 and report the required GFLOPs and the number of parameters\nOne can clearly see the big gap between full-kernel scaling (yellow lines) and kernel decomposition (green lines) as the kernel size increases beyond 31x31.\nEven using the ultra-large 151x151 kernels, using our methods would require fewer FLOPs and parameters, compared to full-kernel scaling with 51x51 kernel\nEFFECTIVE RECEPTIVE FIELD (ERF)\n\nExperiment\nSETTINGS IMAGENET-1K\n\nWe share the (pre-)training settings of SLaK on ImageNet-1K in this section. We train SLaK for 300 epochs (Section 5.1) and 120 epochs (Section 4) using AdamW (Loshchilov &amp; Hutter, 2019) with a batch size of 4096, and a weight decay of 0.05. The only differnce between models training for 300 epochs and 120 epochs is the training time. The learning rate is 4e-3 with a 20-epoch linear warmup followed by a cosine decaying schedule. For data augmentation, we use the default setting of [RandAugment] (Cubuk et al., 2020) in Timm (Wightman, 2019) – “rand-m9-mstd0.5- inc1”, Label Smoothing (Szegedy et al., 2016) coefficient of 0.1, Mixup (Zhang et al., 2017) with ↵ = 0.8, [Cutmix] (Yun et al., 2019) with ↵ = 1.0, [Random Erasing](RandAugment] (Cubuk et al., 2020) in Timm (Wightman, 2019) – “rand-m9-mstd0.5- inc1”, Label Smoothing (Szegedy et al., 2016) coefficient of 0.1, Mixup (Zhang et al., 2017) with ↵ = 0.8, [Cutmix] (Yun et al., 2019) with ↵ = 1.0, [Random Erasing.md) (Zhong et al., 2020) with p = 0.25, Stochastic Depth with drop rate of 0.1 for SLaK-T, 0.4 for SLaK-S, and 0.5 for SLaK-B, Layer Scale (Touvron et al., 2021c) of initial value of 1e- 6, and EMA with a decay factor of 0.9999. We train SLaK-T with NVIDIA A100 GPUs and the rest of models are trained with NVIDIA V100.\n\nSEMANTIC SEGMENTATION ON ADE20K\n\nWe follow the training setting used in Ding et al. (2022); Liu et al. (2022b) using UperNet (Xiao et al., 2018) implemented by MMSegmentation (Contributors, 2020) with the 80K/160K-iteration training schedule. We conduct experiments with both short and long training procedures. The backbones are pre-trained on ImageNet-1K with 224x224 input for 120/300 epochs and then are finetuned with UperNet (Xiao et al., 2018) for 80K/160K iterations, respectively. We report the mean Intersection over Union (mIoU) with single-scale. All the hyperparameters are the exactly the same as the ones used in the official GitHub repository of ConvNeXt (con, 2021).\n\nOBJECT DETECTION AND SEGMENTATION ON COCO\n\nFor COCO experiments, we follow the training settings used in BEiT, Swin, and ConvNeXt using MMDetection (Chen et al., 2019) and MMSegmentation (Contributors, 2020) toolboxes. The final model weights are adopted (instead of EMA weights) from ImageNet-1K pre-training with 224x224 input. We also conduct experiments with both short and long training procedures. The backbones are pre- trained on ImageNet-1K with 224x224 input for 120/300 epochs and then are finetuned with Cascade Mask R-CNN (Cai &amp; Vasconcelos, 2018) for 12/36 epochs, respectively. All the hyperparameters are the exactly the same as the ones used in the official GitHub repository of ConvNeXt (con, 2021).\n\nOBJECT DETECTION ON PASCAL VOC 2007\n\nWe follow (Liu et al., 2021e) and finetune Faster-RCNN on PASCAL VOC dataset with SLaK-T as the backbone. We use multi-scale setting (Carion et al., 2020; Sun et al., 2021) which leads to the length of the shorter side between 480 and 800 and the ones of the longer side at most 1333. The model is trained with AdamW for 36 epochs with a learning rate of 0.0001, a weight decay of 0.05, and a batch size of 16.\n\nSome More Effects\nTRADE-OFF BETWEEN Sparsity AND WIDTH\n\nAs we expected, the model’s performance keeps increasing as model width\nincreases until the width factor reaches 1.5x, after which increasing width further starts to hurt the performance apparently due to the training difficulties associated with highly sparse neural networks.\n\nEFFECT OF THE SHORTER EDGE N ON SLAK\n\nWe vary the shorter edge N 2 [3, 5, 7] and report the accuracy. All models were trained with AdamW on ImageNet-1K for 120 epochs. We empirically find that N=5 give us the best results, whereas N = 3 and N = 7 has slightly lower accuracy. We hence think it reasonable to choose N = 5 as the default option.\n\nERF QUANTITATION OF MODELS WITH DIFFERENT KERNEL SIZES\n\nLarger r suggests a smoother distribution of high-contribution pixels. We can see that with global kernels, SLaK naturally considers a larger range of pixels to make decisions than ConvNeXt and RepLKNet.\n\nCONFIGURATIONS OF DYNAMIC Sparsity\n\nFollowing Liu et al. (2021c), we specifically tune two factors for SLaK-T that control the strength of weight adaptation, adaptation frequency f and adaptation rate p. Adaptation frequency determines after how many training iterations we adjust the sparse weights, and the latter controls the ratio of the weight that we adjust at each adaptation\nf = 2000\nand p = 0.5 works best for SLak-T. For SLak-S/B, we directly choose f = 100 and p = 0.3 without careful tuning.\n\nLIMITATIONS\n\nsparse architecture is implemented with binary masks due to the limited support of sparse neural networks by the commonly used hardware such as GPU and TPU\nTherefore, the inference FLOPs reported in the main paper are the theoretical values.\nOnce this great potential is supported in the future, it can have a significant positive impact on our planet by saving a huge amount of energy and reducing overall total carbon emissions.\nAlthough not the focus of this current work, it would be interesting for future work to examine the speedup of sparse large kernels, using such specialized hardware accelerators, as we see much improvement room of promise here.\n"},"KB/SMOTE":{"title":"SMOTE","links":[],"tags":["augmentation"],"content":"SMOTE\n\n@SMOTESyntheticMinority\nis a popular augmentation used to alleviate problems with class imbalance. This technique is applied to the feature space by joining the k nearest neighbors to form new instances.\n"},"KB/SMP":{"title":"SMP","links":[],"tags":["parallelcomputing"],"content":"SMP\n\nArchitecture where multiple processors share a single address space and access to all resources\nShared memory computing\nConnected by a bus\n"},"KB/SOMs":{"title":"Self Organizing Maps","links":["tags/neuromorphic","KB/Best-Maching-Unit","KB/Euclidean-Distance"],"tags":["temp","neuromorphic"],"content":"Self Organizing Maps\n\nKohonen maps\nCrumbled up grid of SOM neurons\nneuromorphic\nEssentially : need to map a high dim space to a grid of neurons while trying to preserve the neighborhood relations from the high dim space. This is technically impossible so compromise.\nFirst initialized with small random values\nFor each new pattern, identify Best Maching Unit based on current vectors. Reduce the value of r. And pull the point to the part of the grid with similar weight vectors.\n\nUpdate weights w(v_{kl}) \\leftarrow w(v_{kl}) + \\lambda f_r(d(v_{kl}, v_{BMU}))(x-w(v_{kl}))\n\\lambda is learning rate\nd is Euclidean Distance between two neurons in grid.\nf_{r}(0)=1. Tends to 0 as argument grows. r is radius. Greater values, will spread it out.\nRegulated by f_r(d(v_{kl}, v_{BMU}))\n\n\nEventually this will lead to an organization. Covered evenly after a while. Eventually neighbors of v_0 would have weights towards w(v_0) . And w(v_{0)} \\approx mean(all patterns x)\nRepeat until response stops. Each members BMU rate is too low to expand.\nStart with large r and then slow down.\n"},"KB/SP-LIME":{"title":"SP-LIME","links":["KB/Grad-CAM"],"tags":["explainability"],"content":"SP-LIME\n\n\n@ribeiroWhyShouldTrust2016a\n\n\nmodel judges whether you can trust the whole model or not. It selects a picked diverse set of representative instances with LIMEs via submodular optimization. The user should evaluate the black box by regarding the feature words of the selected instances. It is conceivable that it also recognizes bias or systematic susceptibility to adversarial examples. With this knowledge, it is also possible to improve a bad model. SP-LIME was researched with text data, but the authors claimed that it can be transferred to models for any data type.\n\n\nThis was inspired by CAM and Grad-CAM and tested the explanator on randomly chosen images from the COCO dataset [91], applied to the pre-trained neural network VGG-16 using the Kullback–Leibler (KL) divergence\n\n"},"KB/SQL-Tutor":{"title":"SQL-Tutor","links":["KB/Knowledge-Component"],"tags":["usermodel"],"content":"SQL-Tutor\n\nOuter loop: SQL-Tutor (www.aw-bc.com/databaseplacedemo/sqltutor.html) teaches students how to write a query to a relational database (B. Martin &amp; Mitrovic, 2002; Mitrovic, 2003; Mitrovic &amp; Ohlsson, 1999). Each task consists of a database and information to be retrieved from it. In Figure 5, for instance, the student has been given a database of movie information and has been asked to write a query that will List the toc: true\ntitles and numbers of all movies that have won at least one Academy Award and have been made in or after 1988.\nInner loop: Students write a query in the SQL language by clicking on buttons and filling in blanks. This may take several minutes. At any point, they can press the Submit Answer button. The tutor, which has been completely silent up until now, analyzes the student’s query to find its flaws. It gives a variety of levels of feedback and hints.\nOne way to think of SQL-Tutor’s inner loop is that the student takes multiple steps, each comprised of filling in a blank in the query\nUnlike tutors that give feedback as soon as a student had taken a step, SQL-Tutor delays its feedback until the student requests it.\nStep analysis: In order to analyze steps, SQL-Tutor has a set of constraints, where a constraint consists of a relevance condition and a satisfaction condition. If the relevance condition is false of the students’ step, then the constraint is irrelevant, so the tutor says nothing about it. If the constraint has a true relevance condition and a true satisfaction condition, then the constraint is satisfied and the tutor says nothing about it.\nIf the relevance condition is true, and the satisfaction condition is false, then the student’s step violates the constraint and the tutor has identified a topic worth talking about. In particular, every constraint has two messages.\nDepending on the feedback level selected by the tutor or the student, one of them may be presented to the student when the constraint is violated. One message describes the constraint and its violation briefly. The other presents more details.\nAlthough the constraints are task independent, many of them refer to a correct solution of the problem, which is stored in the tutoring system.\nThe relationship between steps, learning events and constraints is quite simple in the SQL-Tutor. Each constraint corresponds to a Knowledge Component.\n"},"KB/SQuAD":{"title":"SQuAD","links":[],"tags":["dataset"],"content":"SQuAD"},"KB/SRN":{"title":"SRN","links":["Vanishingexploding-gradients","KB/Backprop","LSTM"],"tags":["architecture"],"content":"SRN\n\nJust a simple RNN Cell\n\n\nVanishingexploding gradients , in Backprop, they break down when sequences are long.\nDistance between the relevant words are too long\nFollowed up [LSTM)](Long Short Term Memory (LSTM|Long Short Term Memory (LSTM|LSTM)](LSTM)](Long Short Term Memory (LSTM|Long Short Term Memory (LSTM|LSTM).md).md)\n"},"KB/SSR":{"title":"SSR","links":[],"tags":["loss"],"content":"SSR\n\nSmallest sufficient region\nsmallest region of the image that alone allows a confident classification\n"},"KB/STL-10":{"title":"STL-10","links":[],"tags":["dataset"],"content":"STL-10\n\nspecifically designed for developing unsupervised feature learning\n500 labeled training images, 800 testing images, and 100, 000 unlabeled images # covering 10\nclasses which include airplane, bird, car, cat, deer, dog, horse, monkey, ship, and truck.\n"},"KB/SUNCG":{"title":"SUNCG","links":[],"tags":["dataset"],"content":"SUNCG\n\nUNCG dataset is a large synthetic 3D scene repository for indoor scenes which con- sists of over 45, 000 different scenes with manually created realistic room and furniture layouts\nThe synthetic depth, object level semantic labels, and volumetric ground truth are available\n"},"KB/SVHN":{"title":"SVHN","links":[],"tags":["temp"],"content":"\ntoc: true\ntitle: SVHN\ntags: [‘temp’]\n\nSVHN\n\ntoc: true\ntitle: SVHN\ntags: [‘temp’]\nSVHN\n\nrecognizing digits and numbers in natural scene images which obtained from house numbers from Google Street View images\n600,000 images and all digits have been resized to a fixed resolution of 32 x 32 pixel\n"},"KB/SYNTHIA":{"title":"SYNTHIA","links":[],"tags":["dataset"],"content":"SYNTHIA"},"KB/Saddle-Points":{"title":"Saddle Points","links":[],"tags":["architecture"],"content":"Saddle Points\n\nPoints where the gradient is zero but it is at the same a local minima and maxima (both relative)\n\n"},"KB/Safeguard":{"title":"Safeguard","links":[],"tags":["robotics"],"content":"Safeguard\n\nA barrier guard, device or safety procedure designed for the protection of personnel\n"},"KB/Safety-Integrity-Level":{"title":"Safety Integrity Level","links":[],"tags":["robotics"],"content":"Safety Integrity Level\n\nSafety Integrity Level (SIL) is IEC’s method for determining the performance level of a safety system. SIL 2 corresponds to ISO Performance Level “d”, and SIL 3 corresponds to ISO Performance Level “e”. ISO 10218 allows for the use of either.\n"},"KB/Safety-Logic-Circuit":{"title":"Safety Logic Circuit","links":["KB/Manipulator"],"tags":["robotics"],"content":"Safety Logic Circuit\n\nThe safety logic circuit monitors safety critical external devices such as the light curtains and FSU generated signals. The safety logic circuit is programmed via an intuitive user interface that is supported on the Yaskawa programming pendant. It enables to set up the logical operations, such as stopping the Manipulator or outputting a signal if the servos are on.\n"},"KB/Saffran,-Aslin-and-Newport":{"title":"Saffran, Aslin and Newport","links":[],"tags":["language"],"content":"Saffran, Aslin and Newport\n\n8-month-olds can segment a continuous stream of speech syllables, containing no acoustic or prosodic cues to word boundaries, into wordlike units after only 2 min of listening experience\n"},"KB/Salemi-and-Zamani---2024---Evaluating-Retrieval-Quality-in-Retrieval-Augmente":{"title":"Salemi and Zamani - 2024 - Evaluating Retrieval Quality in Retrieval-Augmente","links":[],"tags":["architecture"],"content":"Salemi and Zamani - 2024 - Evaluating Retrieval Quality in Retrieval-Augmente\n\n@salemiEvaluatingRetrievalQuality2024\n\nEvaluating Retrieval Quality in Retrieval-Augmented Generation {#evaluatingretrievalqualityinretrieval-augmentedgeneration}\nNote: Something is Fishy about This Paper.\n\nInstead of passing end to end, they want to pass documents one by one for evaluation. That seems slightly suspicious how they got so much improvement by doing that :/\nAll the part about correlation also seems a little weird.\nSomething does not add up about this paper.\neRAG\neach document in the retrieval list is individually utilized by the large language model within the RAG system\nimprovements in Kendall’s g correlation ranging from 0.168 to 0.494\n\nINTRODUCTION\n\nend-to-end evaluation lacks transparency regarding which retrieved document contributed to the generated output, hindering interpretability of the system’s behavior\nresource- intensive, consuming signifcant time and computational power, particularly when dealing with a large set of retrieval results con- sumed by the LLM\nmany ranking systems rely on interleaving (i.e., replacing one or more documents in the result list) for evaluation and optimization, which further complicates the evaluation, as slight variations in retrieval results necessitate re-computation of the RAG pipeline\noptimizing ranking models often requires document-level feedback, such as user clicks\n\nEVALUATING RETRIEVERS IN RAG\n\nhuman judgment to assess the relevance of a query to documents within a corpus\nhuman annotation can be costly and is often impractical for evaluating all documents in a corpus\ndownstream ground truth out- put associated with the query to provide weak relevance labels\neRAG, a novel approach that involves utilizing the LLM in RAG system itself as the arbiter for generating labels to evaluate the retrieval model.\n\n\nUsing Downstream Large Language Model in RAG as Doc- Ument Annotator\n\n\nRetrieval Evaluation Metrics\nMain Findings\n\nHow do different retrieval evaluation methods correlate with the end-to-end downstream performance in RAG?.\nInterestingly, the most common approaches, KILT Provenance and Annotation with LLMs, that are, document-level relevance labels and using LLMs to assign a relevance label to each retrieved document, have the lowest correlation with the down- stream performance of the LLM\nHow do different retrieval evaluation methods in RAG per- form as the size of retrieval results increases?\nvaried the number of retrieved documents and computed the correlation between the metric with highest correlation for each method in Table 1 at each specifed number of retrieved documents and the downstream performance of the LLM given that number of retrieved documents\n\n"},"KB/Salicon-dataset":{"title":"Salicon dataset","links":[],"tags":["dataset"],"content":"Salicon Dataset\n\n“10,000 images for training and 5000 images for validation”\n“Each image was viewed by 60 observers”\nthis dataset Different from other fixation datasets, is large-scale mouse-tracking data through Amazon Mechanical Turk\nAlthough this dataset is not the fixation dataset, it is well known that the distribution of mouse tracking points is similar to the distribution of fixations and that the parameters of the model trained on Salicon dataset are useful for saliency map estimation\n"},"KB/Salience-Map":{"title":"Salience Map","links":["KB/DeconvNet"],"tags":["explainability"],"content":"Salience Map\nExplained\n\nSpecifies parts of the image that contribute the most to the activity of a specific layer or the entire decision\nY_{c}=\\text{score of class c} : value of output before softmax\nsaliency = max_{r,g,b}(|\\frac{\\partial Y_{c}}{\\partial I}|)\ngrad of Y_{c} wrt input - matrix with shape similiar to input\nif value close to 0 : small changes in input have no effect on output\nif high magnitude : small changes can have a major impact\npositive : roughly the location of the target object\nnegative : competing class objects : background or instance\nAbs value for heatmap\ngrads : backprop on Y_{c} instead of loss\n\nThree Approaches\nDencov\n\nUse DeconvNet\nUse backprop to compute the gradients of logits wrt input : [Deep Inside Convolutional Networks](Deep Inside Convolutional Networks.md)\n[Guided BackProp](Guided BackProp.md)\n\nFeatures\n\nOne of the oldest interpretation methods\nSalience maps of important features are calculated, and they show superpixels that have influenced the prediction most, for example\nTo create a map of important pixels, one can repeatedly feed an architecture with several portions of inputs and compare the respective output, or one can visualize them directly by going rearwards through the inverted network from an output of interest;\nGrouped in this category as well is exploiting neural networks with activation atlases through feature inversion. This method can reveal how the network typically represents some concepts\nConsidering image or text portions that maximize the activation of interesting neurons or whole layers can lead to the interpretation of the responsible area of individual parts of the architecture.\n"},"KB/Saliency-using-natural-statistics":{"title":"Saliency using natural statistics","links":[],"tags":["explainability"],"content":"Saliency Using Natural Statistics\n\napplies a Bayesian framework using local feature maps to estimate saliency maps.\nThe probability distribution of features is learned not from each individual test image but from statistics calculated over the training set of natural images.\n"},"KB/Saliency-vs-Attention":{"title":"Saliency vs Attention","links":[],"tags":["explainability"],"content":"Saliency Vs Attention\n\nInput saliency methods are addressing the goal head-on: they reveal why one particular model prediction was made in terms of how relevant each input word was to that prediction\ninput saliency methods typically take the entire computation path into account, all the way from the input word embeddings to the target output prediction value\nAttention weights do not: they reflect, at one point in the computation, how much the model attends to each input representation, but those representations might already have mixed in information from other inputs\nIronically, attention-as-explanation is sometimes evaluated by comparing it against gradient-based measures, which again begs the question why we wouldn’t use those measures in the first place\nIn terms of efficiency, it is true that for attention only a forward pass is required, but many other methods discussed at most require a forward and then a backward pass, which is still extremely efficient.\n"},"KB/SaliencyMix":{"title":"SaliencyMix","links":[],"tags":["augmentation"],"content":"SaliencyMix\n\n@uddinSaliencyMixSaliencyGuided2021\nextracts salient regions and pastes them on the corresponding location in the target image\nThe salient region is extracted around the maximum intensity pixel location in the saliency map\nRegional dropout is one of the popular solutions that guides the model to focus on less discriminative parts by randomly removing image regions, resulting in improved regularization\nhowever, such information removal is undesirable.\nOn the other hand, recent strategies suggest to randomly cut and mix patches and their labels among training images, to enjoy the advantages of regional dropout without having any pointless pixel in the augmented images.\nselects a representative image patch with the help of a saliency map and mixes this indicative patch with the target image\nFurthermore, models that are trained with SaliencyMix help to improve the object detection performance\nwe cut a source patch and mix it to the target image and also mix their labels proportionally to the size of the mixed patches\nBut in order to prevent the model from learning any irrelevant feature representation, the proposed method enforces to select a source patch in a way so that it must contains information about the source object\nIt first extracts a saliency map of the source image to highlight the objects of interest and then selects a patch surrounding the peak salient region to mix with the target image.\n\nAlgorithm\n\nIf I_{s} \\in \\mathbb{R}^{W \\times H \\times C} is randomly selected training source with label y_{s}. Saliency map is I_{vs}= f(I_{s}) where I_{vs} is the visual saliency. f(\\cdot) is a saliency model.\nSearch for a pixel I_{vs}^{i,j} with maximum intensity\n\n\n\ni,j = argmax(I_{vs})\n\nPatch selected by centering on the I_{vs}^{i,j} pixel or with this pixel in the patch\nPatch determined on a ration \\lambda chosen from [Uniform Distribution](Uniform Distribution.md)\n\nMixing Patches and Labels\n\nAnother image I_{s} \\in \\mathbb{R}^{W \\times H \\times C}\nI_{a} = M \\odot I_{s} + M&#039; \\odot I_{t}\n\nM \\in \\{0,1\\}^{W, H}\n\n\nlabels y_{a} = \\lambda y_{t}+ (1-\\lambda)y_{s}\n\nDIFFERENT WAYS OF SELECTING AND MIXING THE SOURCE PATCH\n\nschemes: (i) Salient to Corresponding, that selects the source patch from the most salient region and mix it to the corresponding location of the target image; (ii) Salient to Salient, that selects the source patch from the most salient region and mix it to the salient region of the target image; (iii) Salient to Non-Salient, that selects the source patch from the most salient region but mix it to the non-salient region of the target image; (iv) Non-Salient to Salient, that selects the source patch from the non-salient region of the source image but mix it to the salient region of the target image; and (v) Non-Salient to NonSalient, that selects the source patch from the non-salient region of the source image and also mix it to the non-salient region of the target image.\nTo find out the non-salient region, we use the least important pixel of an image.\n\nImages\n\n\n\n\n\n\n"},"KB/Salient-Object-Strategy":{"title":"Salient Object Strategy","links":[],"tags":["language"],"content":"Salient Object Strategy\n\n“if an object is contextually relevant, then it is salient” (Philip 2011: 370–371)\n"},"KB/Sample-Correlation":{"title":"Sample Correlation","links":["KB/Correlation","KB/Standard-Deviation","KB/Covariance"],"tags":["temp"],"content":"\ntoc: true\ntitle: Sample Correlation\ntags: [‘temp’]\n\nSample Correlation\n\nA type of Correlation\nr_{xy}= \\frac{s_{xy}}{s_{x}s_{y}}\ns_{x}, s_y are the sample Standard Deviation\ns_y is the sample Covariance\n"},"KB/Sample-Pairing":{"title":"Sample Pairing","links":[],"tags":["augmentation"],"content":"Sample Pairing\n\n@inoueDataAugmentationPairing2018\nmerges two images by averaging their pixel intensities\nThe new image has the same training image label opposite to MixUp and other approaches where labels are updated according to the proportion of image mixing.\none epoch on ImageNet and 100 epochs on other datasets are completed without SamplePairing before mixed image data is added to the training\nOnce the SamplePairing images are added to the training set, they run in cycles between 8:2 epochs, 8 with SamplePairing images, 2 without.\n"},"KB/Sampler":{"title":"Sampler","links":[],"tags":["distributions"],"content":"Sampler\n\nGiven :\n\nP_{X} is a distribution on a measure space (E,B)\nA seq of X_{1}, X_{2}, … of random variables is a sampler if for all A \\in B\nP_{X}(A) = lim_{N \\rightarrow \\infty} \\frac{1}{N}\\Sigma_{i=1}^{N}1_{A}\\circ X_{i}\n1_A is an indicator function for A\n\n\nX_{1}, X_{2}, … need not have the same distribution or need to be independant\nDream : All X_{i} are uniformly distributed on [0,1]. (Impossible)\n"},"KB/Sampling-Bias":{"title":"Sampling Bias","links":[],"tags":["temp"],"content":"Sampling Bias\n\nData is not collected randomly from the target group.\n"},"KB/Sampling-Ray-Casting":{"title":"Sampling Ray Casting","links":["Sampling","KB/Early-Ray-Termination"],"tags":["visualization"],"content":"Sampling Ray Casting\n\nselection of positions along the ray\nEarly Ray Termination\n\n"},"KB/Sanity-Checks-for-Saliency-Maps":{"title":"Sanity Checks for Saliency Maps","links":[],"tags":["explainability"],"content":"Sanity Checks for Saliency Maps\n\n@adebayoSanityChecksSaliency2020\nwhether saliency methods are insensitive to model and data\nInsensitivity is highly undesirable, because it would mean that the “explanation” isunrelated to model and data\nMethods that are insensitive to model and training data are similar to edge detectors\nEdge detectors simply highlight strong pixel color changes in images and areunrelated to a prediction model or abstract features of the image, and require no training\nThe methods tested were Vanilla Gradient, Gradient x Input, Integrated Gradients,Guided Backpropagation, Guided Grad-CAM and SmoothGrad (with VanillaGradient).\nVanilla Gradient and Grad-CAM passed the insensitivity check, while GuidedBackpropagation and Guided Grad-CAM failed\nHowever, the sanity checks paper itself has found some criticism from Tomsett et al.(2020) 89 with a paper called “Sanity checks for caliency metrics” (of course\nThey found that there is a lack of consistency for evaluation metrics\nSo we are back to where we started … It remains difficult to evaluate the visual explanations. This makes it very difficult for a practitioner.\n"},"KB/Satellite-Cell":{"title":"Satellite Cell","links":["KB/Astrocyte"],"tags":["brain"],"content":"Satellite Cell\n\nSurround neuron cell bodies\nSimilar to Astrocyte\n"},"KB/Satisficing-Heuristic":{"title":"Satisficing Heuristic","links":[],"tags":["usermodel"],"content":"Satisficing Heuristic\n\nGood enough\nLess time\nLess knowledge\n"},"KB/Scalar-Articles":{"title":"Scalar Articles","links":["KB/CycleGAN","KB/DCGAN"],"tags":["index"],"content":"Scalar Articles\nDone\n\n[Word2Vec] with [gensim](Word2Vec] with [gensim.md)\nCycleGAN\n[Masked Language Modeling] with [BERT](Masked Language Modeling] with [BERT.md)\nDCGAN\n[Conditional GAN](Conditional GAN.md)\n[Stack GAN](Stack GAN.md)\n[Basic GAN](Basic GAN.md)\n\nIn Progress\n\n[Generative vs Discriminative Models](Generative vs Discriminative Models.md)\n"},"KB/Scalar-Color-Coding":{"title":"Scalar Color Coding","links":["KB/Mean-Diffusivity","KB/Fractional-Anisotropy","KB/Eigenvector"],"tags":["visualization"],"content":"Scalar Color Coding\n\nMean Diffusivity\nFractional Anisotropy\nEigenvector\n"},"KB/Scalar-Register":{"title":"Scalar Register","links":["KB/Vector-Functional-Units","KB/Vector-Load-Store-Units"],"tags":["parallelcomputing"],"content":"Scalar Register\n\nSingle elements for interconnecting Vector Functional Units, Vector Load Store Units, and registers\n"},"KB/Scaled-Dot-Product-Attention":{"title":"Scaled Dot Product Attention","links":["KB/Dot-Product-Attention","KB/Softmax","tags/architecture","KB/Embedding","KB/Soft-Attention","KB/Attention-Alignment"],"tags":["architecture"],"content":"Scaled Dot Product Attention\n\nVaswani et al., 2017\nQ is query, K is key V is value. Same dims\nq_{i}= W_{q}x_i , k_{i}= W_{k}x_{i} , v_{i}= W_{v}x_{i}\n\nw_{ij}&#039; = q_{i}^{T}k_{j}\ny_{i}= \\Sigma_{j}w_{ij}v_{j}\n\n\nSoftmax is sensitive to large values. Which sucks for thearchitecture\nThe avg value of the dot product grows with Embedding dimension k. So scale back.\n\n\\sqrt{k} . Vector in \\mathbb{R}^{k} with all values as c\nEuclidean length is \\sqrt{kc}\n\n\nAttention(Q, K,V) = softmax(\\frac{QK^T}{\\sqrt{d_{k}}})V\nGeneralization of Soft Attention\n\nAttention Alignment score \\alpha_{t,i} = \\frac{s_{t}^{T}h_{i}}{\\sqrt{n}}\n"},"KB/Scaled-benefits":{"title":"Scaled benefits","links":[],"tags":["cognitivemodel"],"content":"Scaled Benefits\n\nThe more prepared - the more benefits\n"},"KB/Scaling-matrix-for-coupling-layers":{"title":"Scaling matrix for coupling layers","links":["images/a014f52e7fec8b0a7ea0b5ad0a7e044a_MD5.jpeg"],"tags":["distributions","architecture"],"content":"Scaling Matrix for Coupling Layers\n \n\nOpen: Pasted image 20241119170534.png\n\nessentially learning a manifold of the data\nsmaller the scaling factor, less important the latent dimension\nlimit as an element of S approaches 0, the corresponding latent dimension is removed\n"},"KB/Scatter-and-Gather":{"title":"Scatter and Gather","links":["KB/Locality"],"tags":["parallelcomputing"],"content":"Scatter and Gather\n\nRetrieves data elements scattered thorughout memory and packs them into sequential vectors in vector registers\nPromotes data Locality and reduces data pollution\n"},"KB/Scene-based-text-to-image-generation":{"title":"Scene based text to image generation","links":["KB/Perception"],"tags":["architecture"],"content":"Scene Based Text to Image Generation\n\nMake-A-Scene: Scene-Based Text-to-Image Generation with Human Priors\ntext-to-image generation\nenabling a simple control mechanism complementary to text in the form of a scene\nintroducing elements that substantially improve the tokenization process by employing domain-specific knowledge over key image regions\nadapting classifier-free guidance for the transformer use case\nThey attempt to progress text-to-image generation towards a more interactive experience, where people can perceive more control over the generated outputs, thus enabling real-world applications such as storytelling\nfocus on improving key image aspects that are significant in human Perception, such as faces and salient objects, resulting in higher favorability of their method in human evaluations and objective metrics\nThrough scene controllability, they introduce several new capabilities: (i) scene editing, (ii) text editing with anchor scenes, (iii) overcoming out-of-distribution text prompts, and (iv) story illustration generation\n"},"KB/SceneNet-RGB-D":{"title":"SceneNet RGB-D","links":[],"tags":["dataset"],"content":"SceneNet RGB-D\n\nlarge indoor synthetic video dataset which consists of 5 million rendered RGB-D images from over 15K trajectories in synthetic layouts with random but physically simulated object poses\npixel level annotations for scene understanding problems such as semantic segmentation, instance segmentation, and object detection, and also for geometric computer vision problems such as optical flow, depth estimation, camera pose estimation, and 3D reconstruction\n"},"KB/Scheduling":{"title":"Scheduling","links":[],"tags":["regularization"],"content":"Scheduling\n\nSome prune all the weights at once\nOthers prune iteratively using loops or some other condition\n"},"KB/Schipol-Data-Scientist":{"title":"Schipol Data Scientist","links":[],"tags":["jobsearch"],"content":"Schipol Data Scientist\n\nBonus:\n\nHave experience with computer vision.\nHave knowledge about cloud computing\n\n\nWork on scaling up our DeepTurnaround product to a multi-airport product.\nContinuous monitoring, development and optimization of our state-of-the-art Computer Vision model.\nDevelop and implement new technical features.\nWork within a skilled development product team consisting of multiple data scientists, data engineers and full stack developers.\nAnd… work at a cool location where you will see aircrafts passing by your window. \nBy leveraging our AI systems, we generate detections from camera images in real-time to capture events occurring around the aircraft. Ultimately, our aim is to enhance the decision-making process in operations, leading to a reduction in delays and CO2 emissions.\n\nMotivation\n\nDeepTurnaround\n\nThe turnaround process is a series of tasks that need to be completed from the time an aircraft arrives at the assigned gate until it is ready for departure - A large scale optimization process\nUseful tech, saves time, money and makes the Schipol experience even better for travellers\nImpact of work\nGuess how it works\n\nObject detection for individual cases\ncombined data from two cameras\n\n\nPotential ideas for improvement\n\nIn the future, we expect to include features which can benefit safety and sustainability officers/managers as well. - Thesis on XAI\nPerhaps a third camera/ for more personell focused view, even more fine grained control. (Eg. Uniforms, crowd density )\n\n\nComputer Vision + Datascience\n\nPerfect mix of things where I can contribute to\nexperience\n\n\n\n\n\nRecruiter Email\n\nResume sent through the process\nIs this position still open?\nJust finished a masters in AI from RUG.\nDeepTurnaround - have many ideas to contribute and work with implementing for a better airport experience at either Schipol or other customers\n\n\nMotivation Letter\nHello!\nThis is Subhaditya Mukherjee. I graduated from the University of Groningen with a masters in Artificial Intelligence last month and just started looking for a job here in the Netherlands. My area of expertise and interest is a combination of Computer Vision and Data Analytics, which is what this position is about.\nI enjoy traveling and am familiar with how frustrating airports can be on a long journey. But there are hundreds of moving parts, and even finding analytics for the same can be a challenge. Deep Turnaround is a very interesting project to me for this reason. The website does not state many technical details, but it seems that the system is a combination of custom data collection, fine-grained object detection networks, and data analytics, each of which must be quite challenging, given the scope of the task.\nThat being said, I have some ideas that could make Deep Turnaround even more useful to our stakeholders, and I would love to discuss and work on them to help make the airport experience a little more streamlined. Of course, I would be willing to work on any other feature that is proposed. These are just some ideas. (These might be already implemented, but I did not see these labels in the video on the website). One of the first ideas that comes to mind is using the two existing cameras to also analyze crowd information to make the passenger experience more streamlined and avoid chaos while boarding. Or perhaps, collecting pictures of worker uniforms would enable Deep Turnaround to log more specific events. Neither of these would require any extra infrastructure costs, which is probably also one of your constraints.\nMy previous internships and projects (both personal and freelance) have included Deep Object Detection, Computer Vision, Data warehousing and analytics, and Dashboarding. These experiences will let me contribute to any AI team, and I am willing to learn whatever else is necessary for any future projects.\nI have a lot to learn, and I want to work with a team that is making a real-world impact in a sector close to my heart like this one. It is not every day you find a position that you enjoy and also think you can contribute something to, and I sincerely hope you give me a chance!\nThank you,\nSubhaditya Mukherjee\nEmail to Recruiter\nCold email\nfhoogenboom@schiphol.com\n\nFloris Hoogenboom\nHead of Data and AI products\n"},"KB/Schwann-Cell":{"title":"Schwann Cell","links":["KB/Myelin","KB/Ogliodendrocytes"],"tags":["brain"],"content":"Schwann Cell\n\nInsulate , helps form Myelin\nSimilar to Ogliodendrocytes\n"},"KB/ScoreCAM":{"title":"ScoreCAM","links":[],"tags":["explainability"],"content":"ScoreCAM\n\n@wangScoreCAMScoreWeightedVisual2020\nIn Score-Cam, we use the weights of the score obtained for a specific target class.\nThe first step involved is passing images to a CNN model and performing a forward_pass. After the forward pass, the activations are extracted from last convolutional layer in the network.\nEach Activation Map obtained from the last layer having shape 1\\times m \\times n is then upsampled using bilinear-interpolation to the same size as the Input Image.\nAfter Upsampling the activation maps, the resultant activation maps are normalized with each pixel within the range of [0,1] to maintain the relative intensities between the pixels\n\nA_{i,j}^{k}= \\frac{A_{i,j}^{k}}{max A^{k}- min A^{k}}\n\n\nAfter the Normalization of the Activation Maps is complete, the highlighted areas of the activation maps are projected on the input space by multiplying each normalized activation map(1 x W x H) with the Original Input Image(3 x W x H) to obtain a masked image M with shape 3 x W x H\n\nM^{k}= A^{k} \\cdot I\n\n\nThe Masked Images M thus obtained are then passed to Convolutional Neural Network with SoftMax output\n\nS_{k} = Softmax(F(M^{k}))\n\n\nAfter getting the scores for each class we extract the score of the target class to represent the importance of the kth activation map.\n\nw_{k}^{c}= S_{k}^{c}\n\n\ncompute the sum across all the activation maps for the linear combination between the target class score and each activation map\napply pixel-wise ReLU to the final activation map\n\nL^{c}_{ScoreCAM} = ReLU(\\underset{k}\\Sigma w_{k}^{c}A^{k})\n\n\nReLU because we are interested only in the features that have a positive influence on the class of interest\n\nAdvantages\n\ncan be used in any Convolutional Neural Network architecture and don’t require retraining of the model to produce saliency maps like CAM\nclass discriminative\nremoves irrelevant noise to produce a meaningful saliency map\nSoftmax scores as weights and removes the dependence on unstable gradients\n"},"KB/Scoring-Pruning-Approaches":{"title":"Scoring Pruning Approaches","links":["KB/Pruning"],"tags":["regularization"],"content":"Scoring Pruning Approaches\n\nLike all networks, scoring becomes essential when we try to choose which parameter to get rid of.\nSome authors suggest removing based on absolute values, others decide to prune based on the contributions of that parameter to the entire network.\nOthers remove based on a score given.\nSome perform Pruning locally, while others perform it globally across the network.\n"},"KB/Second-Language-Vocabulary-Learning-,-The-role-of-context--versus-translation":{"title":"Second Language Vocabulary Learning , The role of context  versus translation","links":["KB/Recall"],"tags":["usermodel"],"content":"Second Language Vocabulary Learning , The Role of Context Versus Translation\n\n\nPETER PRINCE\n\n\nA widespread view of vocabulary learning is that it is advisable to make the shift away from learning words with their translations and to rely on second language (L2) context as soon as possible\n\n\nSuch faith in context learning has not always received experimental support, however, nor is it commonly shared by L2 learners\n\n\nAn experiment in which subjects were tested on their Recall of newly learned words was conducted to determine the relative advantages and disadvantages of both context learning and translation learning as a function of learner proficiency\n\n\nResults\n\nreveal a superiority of translation learning in terms of quantity, but an inability on the part of weaker learners to transfer their knowledge into L2 contexts\nsuggested that alternative learning strategies that combine the advantages of the two techniques should be explored.\n"},"KB/Second-order-generalization":{"title":"Second order generalization","links":[],"tags":["language"],"content":"Second Order Generalization\n\nPresent model with novel example (not seen in training)\n"},"KB/SegNet":{"title":"Seg Net","links":["KB/Unet"],"tags":["architecture"],"content":"Seg Net\n\nPrecursor to Unet\nNo Skip connections\n"},"KB/Selection-Bias":{"title":"Selection Bias","links":[],"tags":["ethics"],"content":"Selection Bias\n\nErrors in conclusions drawn from sampled data due to a selection process that generates systematic differences between samples observed in the data and those not observed. The following forms of selection bias exist\ndatasets often prefer particular kinds of images\nHowever, getting images from the Internet does not in itself guarantee a fair sampling, since keyword-based searches will return only particular types of images\nObtaining data from multiple sources\neven better to start with a large collection of unannotated images and label them by crowd-sourcing\n"},"KB/Self-Attention-GAN":{"title":"Self Attention GAN","links":["KB/Self-Attention","KB/Generative-Models"],"tags":["architecture"],"content":"Self Attention GAN\n\nSelf Attention + Generative Models\n\nclass Self_Attn_New(nn.Module):\n    &quot;&quot;&quot; Self attention Layer&quot;&quot;&quot;\n    def __init__(self, in_dim):\n        super().__init__()\n        self.query_conv = nn.Conv2d(in_dim, out_channels=in_dim//8, kernel_size=1)\n        self.key_conv = nn.Conv2d(in_dim, out_channels=in_dim//8, kernel_size=1)\n        self.value_conv = nn.Conv2d(in_dim, out_channels=in_dim, kernel_size=1)\n        self.gamma = nn.Parameter(torch.zeros([1]))\n \n    def forward(self, x):\n        proj_query = rearrange(self.query_conv(x), &#039;b c h w -&gt; b (h w) c&#039;)\n        proj_key = rearrange(self.key_conv(x), &#039;b c h w -&gt; b c (h w)&#039;)\n        proj_value = rearrange(self.value_conv(x), &#039;b c h w -&gt; b (h w) c&#039;)\n        energy = torch.bmm(proj_query, proj_key)\n        attention = F.softmax(energy, dim=2)\n        out = torch.bmm(attention, proj_value)\n        out = x + self.gamma * rearrange(out, &#039;b (h w) c -&gt; b c h w&#039;,\n                                         **parse_shape(x, &#039;b c h w&#039;))\n        return out, attention"},"KB/Self-Attention":{"title":"Self Attention","links":["KB/Attention","KB/Scaled-Dot-Product-Attention","KB/Softmax","KB/Embedding"],"tags":["architecture"],"content":"Self Attention\n\npaper\nhypernetworks\nBasically Scaled Dot Product Attention\nQ,K,V all from same module but prev layer\nWeighted average over all input vectors y_{i}= \\Sigma_{j}w_{ij}x_{j}\n\nj is over the sequence\nweights sum to 1 over j\nw_{ij} is derived w^{&#039;}_{ij}=x_{i}^{T}x_{j}\n\nAny value between -inf to +inf so Softmax is applied\n\n\nx_i is the input vector at the same pos as the current output vector y_i\n\n\nPropagates info between vectors\n\nThe process\n\nAssign every word t in the vocabular an Embedding\nFeeding this into a self Attention layer we get another seq of vectors y_{the} , y_{cat} etc\neach of the y_{something} is a weighted sum over all the Embedding vectors in the first seq weighted by their normalized dot product with v_{something}\nthe dot product shows how related the vectors are in the sequence\n\nweights determined by them\nSelf-Attention layer may give more weights to those input vectors that are more similar to each other when generating the output vectors\n\n\n\n\nProperties\n\nInputs are a set (not sequence)\nIf input seq is permuted, the output is too\nIgnores the sequential nature of input by itself\n\n\nCode\n\ndef attention(K, V, Q):\n    _, n_channels, _ = K.shape\n    A = torch.einsum(&#039;bct,bcl-&gt;btl&#039;, [K, Q])\n    A = F.softmax(A * n_channels ** (-0.5), 1)\n    R = torch.einsum(&#039;bct,btl-&gt;bcl&#039;, [V, A])\n    return torch.cat((R, Q), dim=1)\nRef\n\nperterbloem\n"},"KB/Self-Distillation":{"title":"Self Distillation","links":["KB/Label-Smoothing"],"tags":["knowledgedistillation"],"content":"Self Distillation\n\nTo be specific, Yuan et al. proposed teacher-free knowledge distillation meth- ods based on the analysis of Label Smoothing reg- ularization (Yuan et al., 2020). Hahn and Choi pro- posed a novel self-knowledge distillation method, in which the self-knowledge consists of the predicted probabilities instead of traditional soft probabilities (Hahn and Choi, 2019).\nThese predicted probabilities are defined by the feature representations of the train- ing model. They reflect the similarities of data in feature embedding space. Yun et al. proposed class- wise self-knowledge distillation to match the output distributions of the training model between intra- class samples and augmented samples within the same source with the same model (Yun et al., 2020).\nIn addition, the self-distillation proposed by Lee et al. (2019a) is adopted for data augmentation and the self- knowledge of augmentation is distilled into the model itself. Self distillation is also adopted to optimize deep models (the teacher or student networks) with the same architecture one by one (Furlanello et al., 2018; Bagherinezhad et al., 2018)\nboth self-distillation and online distillation are properly in- tegrated via the multiple knowledge transfer frame- work (Sun et al., 2021).\n"},"KB/Self-Supervised-Survey":{"title":"Self Supervised Survey","links":["KB/Places","KB/Places365","KB/SUNCG","KB/SVHN","KB/STL-10","KB/YFCC100M","KB/Kinetics","KB/AudioSet","KB/KITTI","KB/UCF101","KB/HMDB51","KB/Ego-motion"],"tags":["semisupervisedlearning"],"content":"Self Supervised Survey\nAbstract\n\nLarge-scale labeled data are generally required to train deep neural networks in order to obtain better performance in visual feature learning from images or videos for computer vision applications\nas a subset of unsupervised learning methods, self-supervised learning methods are proposed to learn general image and video features from large-scale unlabeled data without using any human-annotated labels\n\nMotivation\n\n\nThe performance of deep convolutional neural networks (ConvNets) greatly depends on their capability and the amount of training data.\n\n\ncollection and annotation of large-scale datasets are time-consuming and expensive\n\n\nCompared to image datasets, collection and annotation of video datasets are more expensive due to the temporal dimension\n\n\nTo avoid time-consuming and expensive data annotations, many self-supervised methods were proposed to learn visual features from large-scale unlabeled images or videos without using any human annotations\n\n\nDuring the self-supervised training phase, a predefined pretext task is designed for ConvNets to solve, and the pseudo labels for the pretext task are automatically generated based on some attributes of data\n\n\nThen the ConvNet is trained to learn object functions of the pretext task\n\n\nAfter the self-supervised training finished, the learned visual features can be further transferred to downstream tasks (especially when only relatively small data available) as pretrained models to improve performance and overcome over- fitting.\n\n\nshallow layers capture general low-level features like edges, corners, and textures while deeper layers capture task related high-level features\n\n\n[Pseudo Label](Pseudo Label.md)\n\n\n[Pretext Task](Pretext Task.md)\n\n\n[Downstream Task](Downstream Task.md)\n\n\n[Weakly-supervised Learning](Weakly-supervised Learning.md)\n\n\nFORMULATION OF DIFFERENT LEARNING SCHEMAS\n\n\n[Supervised Learning Formulation](Supervised Learning Formulation.md)\n\n\n[Semi-Supervised Learning Formulation](Semi-Supervised Learning Formulation.md)\n\n\n[Weakly Supervised Learning Formulation](Weakly Supervised Learning Formulation.md)\n\n\n[Self-supervised Learning](Self-supervised Learning.md)\n\n\nNN\n\n[Spatiotemporal Convolutional Neural Network](Spatiotemporal Convolutional Neural Network.md)\n\nPretext Tasks\n\n[Pretext Tasks](Pretext Tasks.md)\n\nDatasets\n\n\nPlaces\n\n\nPlaces365\n\n\nSUNCG\n\n\nSVHN\n\n\nSTL-10\n\n\n\n\n\nYFCC100M\n\n\n[SceneNet RGB-D](SceneNet RGB-D.md)\n\n\n[Moment in Time](Moment in Time.md)\n\n\nKinetics\n\n\nAudioSet\n\n\nKITTI\n\n\nUCF101\n\n\nHMDB51\n\n\nOther Tasks\n\n\n[Image Generation with Inpainting](Image Generation with Inpainting.md)\n\n\n[Image Generation with Super Resolution](Image Generation with Super Resolution.md)\n\n\n[Image Generation with Colorization](Image Generation with Colorization.md)\n\n\n[Learning with Context Similarity](Learning with Context Similarity.md)\n\n\n[Learning with Spatial Context Structure](Learning with Spatial Context Structure.md)\n\n\n[Learning with Labels Generated by Game Engines](Learning with Labels Generated by Game Engines.md)\n\n\n[Learning with Labels Generated by Hard-code Programs](Learning with Labels Generated by Hard-code Programs.md)\n\n\n[Learning from Video Colorization](Learning from Video Colorization.md)\n\n\n[Learning from Video Prediction](Learning from Video Prediction.md)\n\n\n[Learning from RGB-Flow Correspondence](Learning from RGB-Flow Correspondence.md)\n\n\n[Learning from Visual-Audio Correspondence](Learning from Visual-Audio Correspondence.md)\n\n\nEgo-motion\n\n"},"KB/Self-Supervised-Vision-Transformers":{"title":"Self Supervised Vision Transformers","links":["KB/Vision-Transformer","KB/Self-Supervised","KB/MoCO"],"tags":["temp"],"content":"\ntoc: true\ntitle: Self Supervised Vision Transformers\ntags: [‘temp’]\n\nSelf Supervised Vision Transformers\n\nAn Empirical Study of Training Self-Supervised Vision Transformers\n\nrecipes for Vision Transformer are yet to be built\nSelf Supervised\ninstability is a major issue that degrades accuracy, and it can be hidden by apparently good results\nimproved when training is made more stable\nMoCO v3, a framework which offers an incremental improvement of MoCO\n\n\n\n"},"KB/Self-Supervised":{"title":"Self Supervised","links":["KB/Unsupervised-Learning","tags/anchor"],"tags":["temp","anchor"],"content":"Self Supervised\n\nSubset of Unsupervised Learning\nConvNet trained with supervisory signals that are generated from data itself\nUses source task only based on input data\nNo human biases, less discriminative\n\nanchor"},"KB/Self-supervised-Learning":{"title":"Self-supervised Learning","links":[],"tags":["semisupervisedlearning"],"content":"Self-supervised Learning\n\nCompared to supervised learning methods which require a data pair Xi and Yi while Yi is annotated by human labors, self-supervised learning also trained with data Xi along with its pseudo label Pi while Pi is automatically generated for a pre-defined pretext task without involving any human annotation\nThe pseudo label Pi can be generated by using attributes of images or videos such as the context of images [18], [19], [20], [36], or by traditional hand-designed methods [49], [50], [51].\n\nAs long as the pseudo labels P are automatically generated without involving human annotations, then the methods belong to self-supervised learning.\n"},"KB/Semantic-Analysis":{"title":"Semantic Analysis","links":["KB/Lexicon","KB/Lexical-Disambiguation","KB/Sentence-level-processing"],"tags":["language"],"content":"Semantic Analysis\n\nMeaning of the language\nMeanings of the word to extend and perhaps disambiguate the result returned by the syntactic parse\nLook up the individual words in a dictionary (or Lexicon) and extract their meanings\nBut many words have several meanings\n\nLexical Disambiguation\n\n\nSentence level processing\n"},"KB/Semantic-Data":{"title":"Semantic Data","links":[],"tags":["robotics"],"content":"Semantic Data\n\nrepresents the world by a set of subject-predicate-object triple Therefore, the size of the messages is small.\n"},"KB/Semantic-Grammar":{"title":"Semantic Grammar","links":[],"tags":["language"],"content":"Semantic Grammar\n\ncombine syntactic, semantic and pragmatic knowledge into a single set of rules in the form of a grammar\n"},"KB/Semantic-Markers":{"title":"Semantic Markers","links":["KB/Lexical-Disambiguation"],"tags":["language"],"content":"Semantic Markers\n\nPHYSICAL-OBJECT\nANIMATE-OBJECT\nABSTRACT-OBJECT\nUnfortunately, to solve Lexical Disambiguation problem complete, it becomes necessary to introduce more and more finely grained semantic markers\n"},"KB/Semantic-Segmentation":{"title":"Semantic Segmentation","links":[],"tags":["semisupervisedlearning"],"content":"Semantic Segmentation\n\ntask of assigning semantic labels to each pixel in images\nautonomous driving, human-machine interaction, and robotic\nFully Convolutional Network (FCN) [4], DeepLab [5], PSPNet [6] and datasets such as PASCAL VOC [96], CityScape [97], ADE20K [98].\nFCN [4] is a milestone work for semantic segmentation since it started the era of applying fully convolution network (FCN) to solve this task\nWhen using semantic segmentation as downstream task to evaluate the quality of image features learned by selfsupervised learning methods, the FCN is initialized with the parameters trained with the pretext task and fine-tuned on the semantic segmentation dataset, then the performance on the semantic segmentation task is evaluated and compared with that of other self-supervised methods.\n"},"KB/Semantics-influences-form":{"title":"Semantics influences form","links":["KB/past-tense"],"tags":["language"],"content":"Semantics Influences Form\n\npast tense choices mediated by perceived semantic similarity to neighbors, e.g. drank\nAdults under time pressure also make overgeneralization errors at rates from 6% to 31%\n"},"KB/Semi-Supervised":{"title":"Semi Supervised","links":["KB/Contrastive-Loss","KB/Triplet-Loss","KB/Max-Margin-Loss","tags/anchor"],"tags":["temp","anchor"],"content":"Semi Supervised\n\nObtain weak labels instead of class labels\nEg: “similar”\nContrastive Loss\nTriplet Loss\nMax Margin Loss\n\nanchor"},"KB/Semi-Supervised-Learning-Formulation":{"title":"Semi-Supervised Learning Formulation","links":[],"tags":["semisupervisedlearning"],"content":"Semi-Supervised Learning Formulation\n\ngiven a small labeled dataset X and a large unlabeled dataset Z, for each data Xi in X, there is a corresponding human-annotated label Yi\n\nFor a set of N labeled training data\n"},"KB/Sense-Plan-Act-Model":{"title":"Sense-Plan-Act Model","links":[],"tags":["robotics"],"content":"Sense-Plan-Act Model\n\nDeliberative planning has three main steps that are performed in sequence: Sensing,\nPlanning\nActing (executing the plan)\n"},"KB/Sensitivity":{"title":"Sensitivity","links":[],"tags":["loss"],"content":"Sensitivity\n\nTPR = \\frac{TP}{TP + FN}\n"},"KB/Sensory-Feedback":{"title":"Sensory Feedback","links":[],"tags":["robotics"],"content":"Sensory Feedback\n\nVariable data measured by sensors and relayed to the controller in a Closed-loop System. If the controller receives feedback that lies outside an acceptable range, then an error has occurred. The controller sends an error signal to the robot. The robot makes the necessary adjustments in accordance with the error signal.\n"},"KB/Sentence-Segmentation":{"title":"Sentence Segmentation","links":["Sentence-boundary-detection","Sentence-boundary-disambiguation","Sentence-boundary-recognition"],"tags":["language"],"content":"Sentence Segmentation\n\nidentify the processing unit, consists of one or more words\nSentence boundary detection\nSentence boundary disambiguation\nSentence boundary recognition\n"},"KB/Sentence-level-processing":{"title":"Sentence level processing","links":["KB/Semantic-Grammar","KB/Case-Grammar","KB/Conceptual-Parsing","KB/Approximately-Compositional-Semantic-Parsing"],"tags":["language"],"content":"Sentence Level Processing\n\nSemantic Grammar\nCase Grammar\nConceptual Parsing\nApproximately Compositional Semantic Parsing\n"},"KB/Sentiment-Neuron":{"title":"Sentiment Neuron","links":["KB/Lp-Regularization","KB/Unsupervised-Learning"],"tags":["temp"],"content":"Sentiment Neuron\n\nLinear model + L1 Lp Regularization\nUses very few learned units\nSingle sentiment neuron that predicts the sentiment value\n\nCan be useful for Unsupervised Learning\n\nRefs\n\n\n"},"KB/SentimentAnalysis":{"title":"SentimentAnalysis","links":["KB/Sentiment-Neuron","KB/Block-Sparse-Kernel"],"tags":["application"],"content":"SentimentAnalysis\n\nSentiment Neuron\nBlock Sparse Kernel\n"},"KB/Separation":{"title":"Separation","links":[],"tags":["explainability"],"content":"Separation\n\nmodel predictions are independent of the sensitive feature given the target variable\n[Equalized Odds](Equalized Odds.md)\nin classification models, the True Positive (TP) rate and the False Positive (FP) rate are the same in all the subgroups within the sensitive feature\n"},"KB/Sepsis":{"title":"Sepsis","links":[],"tags":["medical"],"content":"Sepsis\n\nAn imbalance in the body’s response to infection that injures the body’s tissues and organs\n"},"KB/Seq2Seq":{"title":"Seq2Seq","links":["KB/Basic-RNN-Architectures","KB/Issues","LSTM","KB/WMT14","KB/BLEU"],"tags":["architecture"],"content":"Seq2Seq\n\nBasic RNN Architectures\nLong term dependency Issues\nEven if hidden state vector has a high dimensionality, cannot hold all info\nSequence to Sequence Learning with Neural Networks\nencoder-decoder learning to map sequences to sequences\nmultilayered Long Short-Term Memory [LSTM)](Long Short Term Memory (LSTM|Long Short Term Memory (LSTM|LSTM)](LSTM)](Long Short Term Memory (LSTM|Long Short Term Memory (LSTM|LSTM).md).md)\nlarge deep LSTM with a limited vocabulary can outperform a standard statistical machine translation (SMT)-based system whose vocabulary is unlimited on a large-scale MT task\nWMT14\nBLEU score\nreversing the order of the words in all source sentences (but not target sentences) improved the LSTM’s performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier\n"},"KB/Sequential-Relation-Bias":{"title":"Sequential Relation Bias","links":["KB/TIme-Series","KB/Recurrent"],"tags":["graph"],"content":"Sequential Relation Bias\n\nSometimes our data has a sequential characteristic. For instance, TIme Series and sentences consist of sequential elements that appear one after another. To model this pattern, we can introduce a Recurrent layer to our network:\n\n"},"KB/Sequential-effects-within-a-short-foreperiod-context-Evidence-for-the-conditioning-account-of-temporal-preparation":{"title":"Sequential effects within a short foreperiod context Evidence for the conditioning account of temporal preparation","links":[],"tags":["cognitivemodel"],"content":"Sequential effects within a short foreperiod context Evidence for the conditioning account of temporal preparation\n\nMichael B. Steinborn , Bettina Rolke, Daniel Bratzke, Rolf Ulrich\n\nAbstract\n\nResponses to an imperative stimulus (IS) are especially fast when they are preceded by a warning signal (WS).\nWhen the interval between WS and IS (the foreperiod, FP) is variable, reaction time (RT) is not only influenced by the current FP but also by the FP of the preceding trial\nsequential effects originate from a trace conditioning process, in which the individuals learn the temporal WS–IS relationship in a trial-by-trial manner\nthat trace conditioning is maximal when the temporal interval between the conditioned and unconditioned stimulus is between 0.25 and 0.60 s\n, one would predict that sequential effects occur especially within short FP contexts.\nHowever, this prediction is contradicted by Karlin [Karlin, L. (1959)]\ninvestigate temporal preparation for short FPs\nThe results provide strong evidence for sequential effects within a short FP context and thus support the trace conditioning account of temporal preparation.\n\nExperiment 1\n\nAnticipatory responding was controlled by using a choice RT task\nThere was a main effect of the factor FP-set on RT, F(1,21) = 219.3, partial g2 = .91, p &lt; .001, indicating that RT was shorter in the short FP-set (366 ms) than in the long FP-set\nRT benefit for the short FP-set might be attributable to a better general ability to process short time intervals than long ones (e.g., Klemmer, 1957; Näätänen et al., 1974).\nmain effect of FPn on RT,\nRT decreased as FP_n increased\nFP in the preceding trial also influenced RT in the current trial as revealed by a main effect of FP_{n-1} on RT\nincreased as FP_{n-1} decreased\nFP_{n-1} FP_n interaction effect on RT\nwhen the preceding FP was long, RT in a current trial decreased with increasing FP and this effect was\nweaker when a short FP preceded a current trial\nasymmetry of the sequential FP effect was smaller for the short FP-set than for the long FP-set.\nMost important, however, the sequential FP effect was not restricted to the long FP-set but was also present for the short FP-set of FP-durations below 0.6 s\n\nExperiment 2\n\nassessed sequential FP effects in a simple RT task employing only the short FP-set\ncatch trial technique\ncompared the asymmetrical sequential FP effect in a condition with 0% catch trials (referred to as no-CT condition) to a condition with 25% catch trials (referred to as CT condition)\nRT was prolonged in the CT condition\nmain effect of FPn on RT\ndecrease of RT with increasing FPn\nFP_{n-1} influenced RT\ninfluence of the preceding FP was unaffected by CT\nasymmetrical sequential FP effect again showed up in the FP_{n-1} FP_n interaction on RT\nanticipatory responses also increased with decreasing preceding FP\nThere was a significant FP_{n-1} FP_n interaction on RT for the no-CT condition\nweak CT FP_n interaction effect on RT\n\nExperiment 3\n\nFP-range employed in Karlin’s study was too dense and therefore did not produce sufficient temporal uncertainty\nsimple RTs were much faster (220 ms) than choice RTs\nRT pattern differed between the task conditions,\nIn contrast to Experiment 2, an upwardsloping FP-RT effect was observed\nRT increased from the shortest towards the longest FP_n\nRT decreased with increasing FP_{n-1}\nresponses in short FP_n trials were always fast irrespective of FP_{n-1}.\nIn contrast, responses in long FPn trials were on average slower and showed a sequential modulation.\nPrecisely, in long FPn trials, responses were relatively fast when FP_{n-1} was also long compared to when FP_{n-1} was short.\nIn sum, the simple RT condition revealed especially fast responses and an extraordinary high percentage of anticipatory responses in short FPn trials, even though FPn1 was long\nis consistent with the results of Karlin (1959) and suggests that participants mainly prepared for an early imperative moment without re-preparing in long FPn trials\nthe RT pattern in the simple RT condition clearly indicates that Karlin’s (1959) finding was not an anomalous result but a reliable empirical phenomenon that occurs when average FPs are small and the FPrange is very dense\nThe overall pattern of results (simple and choice RT condition) is consistent with the view that participants already attained maximal preparation at the short FP_n and were not able to re-prepare when the IS did not occur at the short FP_n.\nInstead, they may have relied on residual preparatory activity from the early imperative moment (Alegria, 1974; Alegria, 1975b).\ninstance, they may have shifted a single moment of peak preparation in a rather analog way, that is, after a short FP_n they expected the IS somewhat earlier, after a long FPn1 FP_{n-1} somewhat later\n\nGeneral discussion\n\nwhen the FP-range is too dense, the typical asymmetrical sequential FP effect does not occur.\nHence, the RT pattern observed in the simple RT condition demonstrates that Karlin’s (1959) finding of a reversed sequential FP effect is a robust phenomenon that occurs in simple RT tasks when the FP-range is very dense.\nWe suggest that when the FP-range is very dense, the individuals may not represent three distinct imperative moments but a single relatively noisy one to which they attain preparation\nThe result pattern of the simple RT condition indicates that participants attained preparation at an early imperative moment because responses were especially fast and a high amount of anticipatory responses were observed in short FPn trials\nThe observation of a reversed sequential effect, however, shows that the moment of peak preparation was still influenced by the preceding trial.\nIn particular, participants may have expected the IS after a short FPn1 somewhat earlier, but after a long\nFP_{n-1} somewhat later in time\nCritically, when the small FP-range does not enable a sharpedged representation of three distinct critical moments but only a noisy representation of a single critical moment, then the process that produces the asymmetrical sequential FP effects at short FP\nno sequential FP effect in short FP_n trials should be expected in this situation\nImportantly, a rather analog sequential adjustment of a single but early preparatory peak should result in a sequential modulation at later imperative moments, as is exactly observed in the simple RT condition of Experiment 3\nIf temporal preparation had increased with FP_n-length, this should have resulted in more efficient performance\n\nImages\n\n\n\n\n\n"},"KB/Serious-Games":{"title":"Serious Games","links":[],"tags":["usermodel"],"content":"Serious Games\n\nSupport learning\nSimulators of real world tasks\nEg: response simulator, kerbal space program, sustainable energy management\n"},"KB/Serotonin":{"title":"Serotonin","links":["KB/Perception"],"tags":["brain"],"content":"Serotonin\n\nA neurotransmitter believed to play many roles, including, but not limited to, temperature regulation, sensory Perception, and the onset of sleep. Neurons using serotonin as a transmitter are found in the brain and in the gut. A number of antidepressant medications are targeted to brain serotonin systems.\n"},"KB/Servo-Control":{"title":"Servo Control","links":[],"tags":["robotics"],"content":"Servo Control\n\nThe process by which the control system of the robot checks if the attained pose of the robot corresponds to the pose specified by the motion planning with required performance and safety criteria.\n"},"KB/Servo-Motor":{"title":"Servo Motor","links":[],"tags":["robotics"],"content":"Servo Motor\n\nAn electrical power mechanism used to effect motion or maintains position of the robot\n"},"KB/Servo-Pack":{"title":"Servo Pack","links":[],"tags":["robotics"],"content":"Servo Pack\n\nAn alternating, current electrical power mechanism that is controlled through logic to convert electrical supply power that is in a sine wave form to a Pulse Width Modulated (PWM) square form, delivered to the motors for motor control: speed, direction, acceleration, deceleration and braking control.\n"},"KB/Servo-controlled-Robot":{"title":"Servo-controlled Robot","links":["KB/Servo-system"],"tags":["robotics"],"content":"Servo-controlled Robot\n\nThe control of a robot through the use of a Closed-loop Servo-system, in which the position of the robot axis is measured by feedback devices and is stored in the controller’s memory\n"},"KB/Servo-system":{"title":"Servo-system","links":[],"tags":["robotics"],"content":"Servo-system\n\nA system in which the controller issues commands to the motors, the motors drive the arm, and an encoder sensor measures the motor rotary motions and signals the amount of the motion back to the controller.\n"},"KB/Shading":{"title":"Shading","links":[],"tags":["visualization"],"content":"Shading\n\n\n"},"KB/Shake-Drop":{"title":"Shake-Drop","links":["KB/Shake-Shake"],"tags":["architecture"],"content":"Shake-Drop\n \n\ndraws a Bernoulli variable that decides whether each block will be subject to Shake-Shake or behave like a standard residual unit on this training step.\n"},"KB/Shake-Shake":{"title":"Shake-Shake","links":[],"tags":["architecture"],"content":"Shake-Shake\n \n\n randomly re-weights the paths during the forward and backward passes. In the forward pass, this can be viewed as synthesizing random data, and in the backward pass, as injecting another form of noise into the training method.\n"},"KB/Shallow-vs-deep-networks":{"title":"Shallow vs deep networks","links":[],"tags":["deeplearning"],"content":"Shallow vs deep networks\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShallow NetworksDeep Networkssingle hidden layermultiple hidden layerscan model continuous functions, but needs more hidden layers to do socomposition of shallow networks, so few can represent more complex functionsWith one input, one output and D &gt; 2 hidden units → D+1 linear regions , 3D+1 paramsOne input, one output, K layers of D&gt;2 hidden units → (D+1)^{K} linear regions and 3D + 1 + (K-1)D(D+1) paramsFlexibility of functions is limited by number of parametersComplex dependencies and symmetriesMore hidden units to achieve approximationLess hidden units to achieve approximation → Depth efficiencyProcessing data like images is almost infeasiblePretty easy to do so. Process local image regions in parallel and then gradually integrate information from increasingly large regionsHard to fit dataOverparameterized deep models have a large family of roughly equivalent solutions that are easy to findDoes not generalize well to new dataGeneralizes better than shallow networksNAIf the number of hidden units D in each of the K layers is the same, and D is an integer multiple of the input dimensionality D_i, then the maximum number of linear regions N_r - N_{r}= (\\frac{D}{D_{i}}+1)^{D_{i}(K-1)} \\cdot \\Sigma_{j=0}^{D_{i}} \\begin{pmatrix}C \\\\ J\\end{pmatrix}"},"KB/Shapes-Dataset":{"title":"Shapes Dataset","links":[],"tags":["dataset"],"content":"Shapes Dataset\n\nConvention is that the first dimension of the dataset belongs to the features and the second to the number of samples. The operations can then be systematically applied from the left.\nException!, in TensorFlow and PyTorch it is the other way around!\n(\\#features, \\#samples)\n"},"KB/Shared-Character-Set":{"title":"Shared Character Set","links":[],"tags":["language"],"content":"Shared Character Set\n\nhelps in narrowing down to a small set of languages\nArabic &amp; Persian\n\nShare same characters but one has supplemental characters\n\n\nRussian &amp; Ukrainian\n\nSame Character Set - Different Frequencies\n\n\nNorwegian &amp; Swedish\n"},"KB/Sharpness-and-Flatness":{"title":"Sharpness and Flatness","links":[],"tags":["explainability"],"content":"Sharpness and Flatness\n\nThree 1D loss surfaces of a VGG-9 network with layer normalization instead of filter normalization, sorted from flattest to sharpest minimum, according to the authors\n\n"},"KB/Shear":{"title":"Shear","links":[],"tags":["augmentation"],"content":"Shear\n\nStretches the image in one of the axial planes, i.e. shear occurs along the x-axis or y-axis. A maximum shear of ±20◦ is used to ensure class preservation.\n"},"KB/Shepard-Interpolation":{"title":"Shepard Interpolation","links":["KB/Interpolation"],"tags":["visualization"],"content":"Shepard Interpolation\n\n\n\n\n\n\\begin{cases}\\Sigma^{N}{i=1}w{i}(x)f_{i}&amp; \\text{if } d(x,x_{i}) \\neq 0 \\forall i\\f_{i}&amp; \\text{if } d(x, x_{i})=0\\\n\\end{cases}\n- $w_{i}(x) = \\frac{1}{d(x,x_{i})^{p}}$\n- Neighborhood N determines points aka radius"},"KB/Sherlock":{"title":"Sherlock","links":[],"tags":["usermodel"],"content":"Sherlock\n\nOuter loop: Sherlock (Katz et al., 1998) tutors the troubleshooting of a large piece of simulated electrical equipment, an avionics test station.\nIt provides many views on the equipment and its schematics, so no screenshots are included here.\nInner loop: Troubleshooting the equipment requires taking many steps, such as measuring voltages and replacing suspect parts. Sherlock gives unsolicited feedback only if a step is unsafe, that is, if the step would cause serious damage to the equipment or the student if it were done in the real world\nSherlock also makes extensive feedback available after the student has solved the problem. Sherlock’s default post-solution feedback is to display a side-by-side summary of the student’s solution and Sherlock’s ideal solution.\n"},"KB/Shock-Detection-Function":{"title":"Shock Detection Function","links":["KB/Manipulator"],"tags":["robotics"],"content":"Shock Detection Function\n\nShock detection is a function supported by the Yaskawa robot controller that reduces the impact of a robot collision by stopping the Manipulator without any external sensor when the tool or the Manipulator collide with a peripheral device.\n"},"KB/Shortcuts-to-Quantifier-Interpretation-in-Children-and-Adults":{"title":"Shortcuts to Quantifier Interpretation in Children and Adults","links":["journals/2022-10-10"],"tags":["language"],"content":"Shortcuts to Quantifier Interpretation in Children and Adults\n\nPatricia J. Brooks &amp; Irina Sekerina\n\nIntro\n\nSummarized in 2022-10-10\nErrors involving universal quantification are common in contexts depicting sets of individuals in partial, one-to-one correspondence\nquantifier-spreading errors are more common with distributive quantifiers each and every than with all.\npairs of pictures\nselected one corresponding to a sentence containing a universal quantifier\nnot in correspondence, with correct sentence interpretation requiring their attention\nChildren younger than 9 years made numerous errors\nwith poorer performance in distributive contexts than collective ones\n21 native, English-speaking adults, given a similar task with the distributive quantifier every, also made childlike errors undermines accounts positing immature syntactic structures as the error source\nerrors seemingly reflect inaccurate syntax to semantics mapping, with adults and children alike resorting to processing shortcuts.\nQuantificational terms such as all, usually, and most are a crucial type of linguistic device used to indicate which sets of individuals or events have which properties and relationships\nacquisition may be delayed relative to other sorts of lexical items (e.g., nouns and verbs) because their complex patterns of usage often result in interpretive ambiguities\n96 children (5- to 9-year-olds)\nBoth pictures showed extra objects\nIn spoken language, however, the intensifier interpretation is predominant in which it is conventional to say all even when it does not imply exhaustivity\nI left all my money at home would not preclude their having money in the bank\nall is used primarily as an adverbial intensifier in both child language and child-directed speech\nevery and each involve additional lexical complexity\nEvery appears inside compounds—for example, everybody—and it vacillates between collective and distributive interpretations when used as a quantifier\nEach is more uniformly distributive but has the further conceptual requirement that the individuals modified by each be successively scanned\nEvery and each both occur only rarely in either childdirected or child speech, which limits opportunities for children to acquire their patterns of usage.\noverexhaustive search\ninvolves failure to properly restrict the domain of the universal quantifier to the\nexhaustive pairing\nclassic spreading\nnoun phrase (NP) it modifies\ncomplementary error referred to as underexhaustive search\nVery young children occasionally make other, more surprising errors in interpreting universal quantifiers, such as answering no to the question Is every bunny eating a carrot? when shown, for example, a picture of three rabbits, each eating a carrot, along with a dog eating a bone. This error is referred to as bunny spreading\nChildren’s errors with universal quantification have led to controversies with respect to how to explain them. One view is that the errors stem from children’s deficient syntactic representations (Kang (2001), Philip (1995; 1996), Roeper and de Villiers (1993), Roeper and Matthei (1975), Roeper et al. (2005)). Philip (1995; 1996), following Roeper and Matthei (1975) suggested that the classic spreading error is due to the fact that children syntactically misinterpret distributive universal quantifiers (e.g., each or every in English, cada in Spanish or Portuguese) as sentential adverbials that range over events as opposed to individuals. Roeper et al. (2005) described a sequence of steps of how children start with a general syntactic representation of every as an adverbial intensifier that gets progressively more specific as every changes its position in the syntactic representation.\nRather, the errors are presumed to involve shallow processing, resulting in inaccurate mapping between syntactic and semantic representations\nFirst, Crain et al. (1996) did not systematically vary the position of the universal quantifier in the test questions or statements while holding constant the introductory story and scenario\nprototypical scenario\nThis feature of their design provided children with unambiguous cues as to which set of entities was the focus of attention.\nCrain et al.’s (1996) claim that preschoolers have full competence with universal quantifiers would seem to be undermined by the fact that even older school-age children make errors identifying the domain of a universal quantifier.\nFirst, there have been great discrepancies in error rates across the many studies that have almost exclusively utilized the Truth Value Judgment Task, ranging from near perfect performance in Crain et al. (1996) to extremely high error rates in Kang (2001), that is, over 80% errors in 6- to 7-year-olds\nOur sentence–picture matching task does not have the same demand characteristics as the Truth Value Judgment Task, and in our opinion, it provides a more accurate way of evaluating whether children’s interpretations of sentences with universal quantifiers vary systematically as a function of the position of the\nquantifier in the sentence and the type of scene\nexamine whether there is an asymmetry in the distribution of errors as a function of the syntactic position of the universal quantifier\naddress the controversy as to whether children perform better in tasks with collective universal quantifiers (cf. Brooks and Braine (1996)) or with distributive ones (cf. Drozd (1996))\nFinally, because accounts positing syntactic deficits as the source of quantifierspreading errors (e.g., Kang (2001), Philip (1996), Roeper et al. (2005)) generally assume that adults are essentially error free in their comprehension of basic sentences containing universal quantifiers (i.e., their syntax is perfect), we tested a group of adults on a version of our task (Experiment 3) to evaluate this claim and to allow a more complete investigation of the developmental trajectory of quantifier acquisition from 5-year-olds to adults.\n\nEXPERIMENT 1\nParticipants\n\nWe recruited and tested twelve 5-year-olds (M = 5;5, range = 5;2–5;11), twelve 6-year-olds (M = 6;6, range = 6;2–6;10), twelve 7-yearolds (M = 7;6, range = 7;1–7;11), twelve 8-year-olds (M = 8;6, range = 8;0–8;11), and twelve 9-year-olds (M = 9;6, range = 9;1–9;11) at private elementary schools and after-school programs in Atlanta, Georgia.\n24 pairs of pictures depicting people involved in various activities such as carrying boxes, washing pets, or watering plants.\nFour types of picture pairs were constructed.\nthree people individually engaged in an activity with three distinct objects or animals\nthree people engaged in an activity with three objects\nCollective picture pair types 3 and 4 were variations of collective picture pair type 2, with new foils created to match the distributive pairs in terms of target–foil similarity\nAcross picture pairs, a variety of contexts with transitive actional verbs were used so that each sentence type could be presented multiple times without repeating any pictures\nAll of the contexts involved humans acting on animate or inanimate objects\nSix sentence types were used\n\n\n(1) Each of the (people) is (verb)ing an (object), for example, Each of the men is washing a bear. (2) There is a (person) (verb)ing each of the (objects), for example, There is a man washing each of the bears. (3) Every (person) is (verb)ing an (object), for example, Every man is washing a bear. (4) There is a (person) (verb)ing every (object), for example, There is a man washing every bear. (5) All of the (people) are (verb)ing an (object), for example, All of the men are washing a bear.\n\n\n\n\n(6) There is a (person) (verb)ing all of the (objects), for example, There is a man washing all of the bears.\n\n\nTwelve additional pairs of pictures served as filler items.\n\nProcedure\n\nsingle, 20-min session conducted in a quiet room of their school\nWe showed children two pictures at a time and asked them to point to the picture that went best with a sentence read aloud\nAfter the child looked at both pictures, the experimenter read the corresponding sentence and asked the child to point to the picture that went best with the sentence.\nwithout providing any corrective feedback\nAcross trials, we randomized the position of the correct picture\nChildren made no errors on filler sentences, and these trials were not examined further.\nmixed-design analysis of variance\nThe dependent variable was the proportion of correct picture choices for Sentence Types 1 through 4\narcsine transformed these proportions and all others\n\nAnalysis\n\nThe analysis showed significant main effects of syntactic position, F(1, 55) = 19.03, p &lt; .001, and age, F(4, 55) = 7.22, p &lt; .001. No other main effects or interactions were significant\nAs shown in Table 1, comparisons against chance performance (50%) revealed that only the 9-year-olds as a group were above chance in selecting the correct pictures (Figure 1b) for sentences with the universal quantifier modifying the direct object.\nAt this criterion, one 5-year-old (8%), two 6-year-olds (17%), five 7- year-olds (42%), five 8-year-olds (42%), and nine 9-year-olds (75%) performed above chance.\nOverall, children were correct in 95.8% of their picture selections when the original collective pictures of Brooks and Braine were used (see Figure 2) but only 83.1% of trials with the modified collective pictures (see Figures 3 and 4).\nchildren’s responses became more consistently correct with age, with only children 7 years and older performing above chance as a group in the modified task when the quantifier modified the direct object.\nTo examine whether individual children were above chance in selecting the appropriate collective picture on the modified task with all, we again used the binomial distribution ( p &lt; .05), with above-chance performance requiring 6 out of 6 correct responses\nAt this criterion, zero 5-year-olds (0%), five 6-year-olds (42%), seven 7-year-olds (58%), seven 8-year-olds (58%), and seven 9-year-olds (58%) performed above chance.\nf we adopt a more lenient criterion (5 of 6), which is proportionally comparable to the 10 of 12 required for above-chance performance with sentences containing each or every, then six 5-year-olds (50%), eight 6-yearolds (67%), ten 7-year-olds (83%), eight 8-year-olds (67%), and ten 9-year-olds (83%) showed consistently strong individual performance.\nwe conducted one additional mixed-design ANOVA with Quantifier (all, each, every) and Syntactic Position as within-subjects factors and Age as a between-subjects factor.\nThe main effect of syntactic position was significant, F(1, 55) = 16.38, p &lt; .001, but was qualified by an interaction of quantifier and age, F(1, 55) = 7.13, p &lt; .01\ncomprehension performance was more accurate when the universal quantifier modified the subject of the sentence.\nThis effect of syntactic position, however, was highly significant for sentences with each or every but only marginally significant for sentences with all (see above for F tests for main effects of syntactic position in separate analyses by quantifier). No other interactions were significant.\nIn general, their picture selections were more accurate for sentences with a universal quantifier modifying the subject in comparison to the direct object of a transitive actional verb.\nAlthough the Philip (1996) and Kang studies have shown a similar pattern of subject/object asymmetry to this study and Brooks and Braine (1996), we note that children performed at much higher levels of accuracy in our sentence–picture matching task compared to the Truth Value Judgment Task. In none of our conditions, at any age, were children significantly below chance in their picture selections. This contrasts especially with Kang, who reported error rates over 80% in both English-speaking and Korean-speaking 6- and 7-year-olds\nHowever, by age 7, children were consistently correct in their picture choices regardless of the syntactic position of all in the sentence.\nThis suggests that the collective groupings may have helped focus their at\ntention on the relevant set of entities modified by the quantifier.\nMore generally, the Truth Value Judgment Task allows children to reject a picture for a variety of reasons (and it is often hard to discern the basis for children’s pattern of responding)\nIn Experiment 1, we used a sentence–picture matching procedure in which children needed only to find the picture that matched the sentence. This task eliminated opportunities for participants to consider whether a collective versus distributive interpretation of the sentence was preferred and furthermore allowed us to carefully match our collective and distributive pictures with respect to the composition of the foils.\n\nExperiment 2\n\nWe designed Experiment 2 to eliminate this interpretive confound through the use of locative scenes.\nlocative pictures with animals and other entities shown in containers of various sorts (e.g., bananas in baskets, bears in beds).\nuniversal quantifiers in three syntactic constructions that support distributive interpretations in a locative context\nAcross constructions, we systematically varied both the syntactic position of the universal quantifier and whether the subject of the sentence referred to the containers or the entities in them\n\nMethod\n\nParticipants. Twelve 7-year-olds (M = 7;6, range = 7;1–7;10), twelve 8-year-olds (M = 8;6, range = 8;0–8;11), and twelve 9-year-olds (M = 9;5, range = 9;0–9;10) took part in the experiment. We recruited and tested these children at the same schools as in Experiment 1. None of the children in Experiment 2 participated in the previous experiment.\n\nMaterials\n\n27 pairs of pictures depicting various entities arranged in containers (e.g., alligators in bathtubs, turtles in tanks, apples in bowls)\nThe pictures showed distributive arrangements with the entities and containers in partial, one-to-one correspondence with each other.\nBoth pictures depicted three entities each located in a unique container. One picture showed two extra empty containers (see Figure 5a), and the other picture showed two objects that were not in containers (see Figure 5b).\nnine sentence types\n\n\n(7) All of the (objects) are in a (container), for example, All of the alligators are in a bathtub. (8) All of the (containers) have an (object) in them, for example, All of the bathtubs have an alligator in them.\n\n\nThere is an (object) in all of the (containers), for example, There is an alligator in all of the bathtubs. Each of the (objects) is in a (container), for example, Each of the alligators is in a bathtub. Each of the (containers) has an (object) in it, for example, Each of the bathtubs has an alligator in it. There is an (object) in each of the (containers), for example, There is an alligator in each of the bathtubs. Every (object) is in a (container), for example, Every alligator is in a bathtub. Every (container) has an (object) in it, for example, Every bathtub has an alligator in it. There is an (object) in every (container), for example, There is an alligator in every bathtub.\n\nAnalysis\n\nThe main effect of quantifier and all of the interactions involving quantifiers were not significant.\nAcross syntactic constructions, 7-year-olds preferred the picture with the extra animals or objects as opposed to the picture with the extra containers.\nBoth the 7- and 8-year-olds made correct picture selections at an above-chance level only for sentences with the universal quantifier modifying the noun correspond\ning to the containers irrespective of the syntactic construction.\nn contrast, the majority of 9-year-olds correctly varied their picture selections in accordance with the varying syntactic constructions and performed above chance as a group for all sentence types.\nExperiment 2 replicated one of the main findings of Experiment 1: Only 9-yearolds as a group consistently identified the domain of the universal quantifier and selected the appropriate picture at above-chance levels for distributive events in which sets of objects were in partial, one-to-one correspondence.\nRather, irrespective of the syntactic construction, they showed better performance on sentences with the quantifier modifying the containers\nThe observed bias to prefer locative scenes in which all of the containers were filled (the so-called garage-centered bias) has been observed many times; see Drozd (2001) for a review\nThese results are difficult to reconcile with Kang’s\nMoreover, all of the age groups failed to show any effect of quantifier in Experiment 2 in contrast to Experiment 1\nThat is, the children failed to show a familiarity effect with better performance for sentences with all.\nhe differing results for the two experiments indicate that the collective scenes used with all in Experiment 1 were easier than the distributive ones used in Experiment 2 (see also Brinkmann et al. (1996)).\nIt appears that these scenarios involving partial, one-toone correspondence pose considerable challenges for children.\nAlthough Experiment 2 provided no evidence that children distinguished the quantifier all from each or every, we emphasize that previous work (Brooks et al. (2001), Brooks et al. (1998)) has shown that children do readily distinguish these quantifiers on semantic grounds\n\nExperiment 3\n\nIn Experiment 3, we examined whether adults would make errors restricting the domain of a universal quantifier in a similar picture-selection task with distributive, locative scenes.\nTesting adults is important because syntactic accounts do not readily predict errors in syntactically competent adults\nBrooks and Braine (1996, Experiment 1) tested adults with actional scenes and found no errors. Their data, however, came from 10 undergraduates at a highly selective private university (Carnegie Mellon) and thus may not be representative of adults in general.\nHere we tested monolingual English-speaking undergraduates at a highly diverse public university. This experiment constituted pilot work to establish a paradigm suitable for an eye-tracking study.\nFirst, we included two filler pictures along with each target and foil picture, and second, we presented sentences with the quantifier every but did not test each or all.\nParticipants. We recruited 22 monolingual, adult native speakers of English (16 women, 6 men; M age = 26 years, range = 18–49) from introductory psychology classes at the College of Staten Island, City University of New York, who received extra credit for their participation.\nWe created PowerPoint® slides comprising four pictures that were presented simultaneously (see Figure 6 for an example). These slides depicted two sets of objects in partial, one-to-one correspondence\nEach array contained two pictures similar to those used in Experiment 2 (compare Figures 5 and 6), along with two filler pictures\nWe presented each sentence type six times, in randomized order, for a total of 12 trials\nTo permit naming, we numbered the pictures from 1 to 4, with the position of the target randomized across trials. We used a tape recorder to record participants’ responses.\n\nObservations\n\nAcross all participants, no errors were made on filler sentences indicating that the participants were generally compliant with the task instructions\nperformance on the task was not at ceiling, with adults making errors on an average of 21% of the trials\nThis error rate, which is numerically higher than the rate observed with the 9-year-olds of Experiments 1 and 2, is likely due to the added complexity of the task involving four pictures as opposed to two.\nAcross the trials involving sentences with every, adult participants never selected either of the filler pictures. This indicates that their response set was effectively the same as that of the children in Experiment 2.\nThus, unlike the children in Experiment 2, the adults did not show a preference for locative scenes in which all of the containers were filled.\nA further examination of data indicated that two adults selected the same picture on 12 of 12 trials indicating no sensitivity to the position of the quantifier in the sentence\nThe fact that half of our adult participants made considerable numbers of errors in restricting every to its domain is not readily explicated by syntactic accounts positing immature syntactic representations as the source of children’s quantifier-spreading errors\nOur findings that both children and adults make errors in quantifier interpretation are more readily explained by the underspecification account of Sanford and Sturt (2002).\nThe crucial step involves deficient mapping from syntactic structure to a semantic representation,\nAlthough grammatically competent adults are capable of construing correct and fully specified semantic representations of utterances with quantifiers, it does not always happen\nThe results demonstrate that many school-age children and adults had considerable difficulty in restricting the domain of a universal quantifier, especially when two sets of entities were in partial, one-to-one correspondence. This result contrasts most dramatically with the near perfect performance of preschool children in Crain et al. (1996)\nThis suggests that the problem does not reside in the child’s syntax, given the similarities in sentence structures used across studies, but in\nstead has to do with the difficulty of selecting the appropriate set of entities and avoiding distraction by salient objects.\nTaken together, the experiments suggest that it was the collective scenes as opposed to the use of all that improved children’s performance in Experiment 1. Collective scenes were easier presumably because the group depiction aided the child in isolating one set of entities relative to the other\nConversely, the distributive scenes were more difficult because the pictures were more visually symmetric. The observed difference in performance for collective versus distributive scenes seems to undermine syntactic accounts of children’s errors given that the structures of the corresponding sentences were essentially the same.\nThe fact that children’s errors in Experiment 2 were not randomly distributed indicates that they noticed the extra objects and/or containers in the distributive pictures\nAgain, only 9-yearolds consistently varied their picture selections in accordance with the varying syntactic constructions and did not show a strong preference for one picture configuration at the expense of the other.\nTheir performance on a modified version of the sentence–picture matching task was not only below ceiling, but their error rate was numerically higher than that of the 9-year-olds of Experiments 1 and 2. Note, however, that in contrast to the 7- and 8-year-olds’ patterns, the adults’ errors were equally distributed between the locative pictures with extra animals or objects versus extra containers.\nWe suspect that both children and adults make errors in comprehension because they engage in shallow processing that causes inaccurate mapping between syntactic and semantic representations.\nIn interpreting universal quantifiers, children construct underspecified representations using simpler processing strategies and then rely on pragmatics to solve the task.\nAdults also may construct underspecified representations as a first step that may or may not be followed by the application of algorithm. We speculate that adults stop at an underspecified representation when there are other demands on attention under conditions of working memory load, fatigue, or lack of cognitive effort.\nAnother possibility with respect to the results of Experiment 2 is that the children may have gradually picked up on the fact that the universal quantifier modified the noun corresponding to the containers in two of three of the sentences.\nIn either case, once children were fixated on a particular picture configuration, they perseverated and were reluctant to consider a competing picture as a possible alternative, even in the face of a conflicting sentence structure\nThe suggestion that the children processed the sentences deterministically is not a new one.\nhave indicated that children tend not to revise their initially incorrect interpretations of temporary syntactic or referential ambiguities even when disambiguating information becomes available.\nShallow processing also provides a straightforward explanation of the errors made by adults in Experiment 3. Their high error rates suggest that adult listeners often do not tax their limited information-processing capacities by conducting exhaustive syntactic analyses of sentences but rather make use of simpler strategies in generating reasonable guesses\nThese findings led Ferreira et al. (2002) to conclude that the meaning people obtain for a sentence is often not a reflection of its true content (p. 11) and that language processing often yields a merely good enough representation of a sentence’s meaning\nThis statement is an apt characterization of the performance of many school-age children and adults in our experiments. More generally, the comprehension of universal quantifiers seems an ideal domain for exploring the dynamics of attention allocation, and cognitive effort, in language processing.\n"},"KB/Shrinkage":{"title":"Shrinkage","links":[],"tags":["temp"],"content":"Shrinkage\n\nA hyperparameter in gradient boosting that controls overfitting. Shrinkage in gradient boosting is analogous to learning rate in gradient descent. Shrinkage is a decimal value between 0.0 and 1.0. A lower shrinkage value reduces overfitting more than a larger shrinkage value.\n"},"KB/ShuffleNet":{"title":"ShuffleNet","links":[],"tags":["architecture"],"content":"ShuffleNet\n\nChannel Shuffle\n\ndef channel_shuffle_new(x, groups):\n    return rearrange(x, &#039;b (c1 c2) h w -&gt; b (c2 c1) h w&#039;, c1=groups)\nclass ShuffleUnitNew(nn.Module):\n    def __init__(self, in_channels, out_channels, groups=3, \n                 grouped_conv=True, combine=&#039;add&#039;):\n        super().__init__()\n        first_1x1_groups = groups if grouped_conv else 1\n        bottleneck_channels = out_channels // 4\n        self.combine = combine\n        if combine == &#039;add&#039;:\n            # ShuffleUnit Figure 2b\n            self.left = Rearrange(&#039;...-&gt;...&#039;) # identity\n            depthwise_stride = 1\n        else:\n            # ShuffleUnit Figure 2c\n            self.left = nn.AvgPool2d(kernel_size=3, stride=2, padding=1)\n            depthwise_stride = 2\n            # ensure output of concat has the same channels as original output channels.\n            out_channels -= in_channels\n            assert out_channels &gt; 0\n \n        self.right = nn.Sequential(\n            # Use a 1x1 grouped or non-grouped convolution to reduce input channels\n            # to bottleneck channels, as in a ResNet bottleneck module.\n            conv1x1(in_channels, bottleneck_channels, groups=first_1x1_groups),\n            nn.BatchNorm2d(bottleneck_channels),\n            nn.ReLU(inplace=True),\n            # channel shuffle\n            Rearrange(&#039;b (c1 c2) h w -&gt; b (c2 c1) h w&#039;, c1=groups),\n            # 3x3 depthwise convolution followed by batch \n            conv3x3(bottleneck_channels, bottleneck_channels,\n                    stride=depthwise_stride, groups=bottleneck_channels),\n            nn.BatchNorm2d(bottleneck_channels),\n            # Use 1x1 grouped convolution to expand from \n            # bottleneck_channels to out_channels\n            conv1x1(bottleneck_channels, out_channels, groups=groups),\n            nn.BatchNorm2d(out_channels),\n        )        \n        \n    def forward(self, x):\n        if self.combine == &#039;add&#039;:\n            combined = self.left(x) + self.right(x)\n        else:\n            combined = torch.cat([self.left(x), self.right(x)], dim=1)\n        return F.relu(combined, inplace=True)"},"KB/Shuffled-AUC":{"title":"Shuffled-AUC","links":[],"tags":["loss"],"content":"Shuffled-AUC\n\nFPR is calculated based on the negatives which are determined by fixation points of all the other images in the dataset.\n“AUC for the curve is calculated as sAUC.”\n"},"KB/Sigmoid":{"title":"Sigmoid","links":["KB/Bernoulli-Distribution"],"tags":["architecture"],"content":"Sigmoid\n\n\\sigma(x) = \\frac{1}{1+exp(-x)}\n\\frac{d\\sigma}{dx}(x) = \\sigma(x)(1-\\sigma(x))\n\nmax : 0.25\n\n\nLogistic\nXavier/Glorot init\nRNN : Hidden\nBernoulli Distribution over a binary variable\n\n\n"},"KB/SimCLR":{"title":"SimCLR","links":["KB/Contrastive-Loss","Tag-Pages/loss","KB/Res-Net","Augmentation"],"tags":["temp"],"content":"\ntoc: true\ntitle: SimCLR\ntags: [‘temp’]\n\nSimCLR\n\nA Simple Framework for Contrastive Learning of Visual Representations\n\ncontrastive learning of visual representations\nwithout requiring specialized architectures or a memory bank\ncomposition of data augmentations plays a critical role in defining effective predictive tasks\nintroducing a learnable nonlinear transformation between the representation and the Contrastive Loss substantially improves the quality of the learned representations\ncontrastive learning benefits from larger batch sizes and more training steps compared to supervised learning\nuse of a nonlinear head at the end of the network, and the loss function\nRes Net\nTwo separate data augmentation operators are sampled from the same family of augmentations\napplied to each data example to obtain two correlated views\nAfter training is completed, they throw away the projection head and use the encoder for downstream tasks\nhead g(\\cdot)\nencoder f(\\cdot)\nrepresentation h\n\n\n\n"},"KB/Simple-Gradient-Descent":{"title":"Simple Gradient Descent","links":["KB/Simple-Gradient-Descent","KB/LinearRegression","KB/Least-squares-loss"],"tags":["gradients"],"content":"Simple Gradient Descent\n\n\n\\theta = \\theta - \\eta \\cdot \\nabla_{\\theta} J(\\theta)\nIt starts with some coefficients, sees their cost, and searches for cost value lesser than what it is now.\nIt moves towards the lower weight and updates the value of the coefficients.\nThe process repeats until the local minimum is reached. A local minimum is a point beyond which it can not proceed.\nThe first step computes the gradient of the loss function at the current position. This determines the uphill direction of the loss function. The second step moves a small distance α downhill (hence the negative sign). The parameter α may be fixed (in which case, we call it a learning rate), or we may perform a line search where we try several values of α to find the one that most decreases the loss.\n\nLinearRegression example\n\n y = f[x, \\phi ] = \\phi_{0}+ \\phi_{1}x\nUse Least squares loss\nl_{1}= (\\phi_{0}+\\phi_{1}x_{i}-y_{i})^{2} is the individual contribution to the loss from the ith training example.\n\n\n\n"},"KB/Simulations-Of-language":{"title":"Mirman Et Al","links":["KB/Basic-RNN-Architectures"],"tags":["temp"],"content":"\n\nMirman Et Al\nBasic RNN Architectures\n\n\nSimple rnn performed like human learners\n\nsensitive to transitional prob and freq\n\n\n"},"KB/Single-unit-recording":{"title":"Single unit recording","links":[],"tags":["language"],"content":"\n\nSingle Unit Recording\n\nAuthors predictions: units will either be on or off (no activation or nearly fully activated)\n"},"KB/Singularity":{"title":"Singularity","links":[],"tags":["robotics"],"content":"Singularity\n\nA configuration where two joints of the robot arm become co-axial (aligned along a common axis). In a singular configuration, smooth path following is normally impossible and the robot may lose control. The term originates from the behavior of the Jacobian matrix, which becomes singular (i.e., has no inverse) in these configurations.\n"},"KB/Sketched-Update":{"title":"Sketched Update","links":[],"tags":["temp"],"content":"Sketched Update\n\nLearn a full model update, then compress it before sending to the server.\nFirst computes the full Hit during local training without any constraints, and then approximates, or encodes, the update in a (lossy) compressed form before sending to the server. The server decodes the updates before doing the aggregation.\nSubsampling - Instead of sending Hit , each client only communicates matrix Ĥit which is formed from a random subset of the (scaled) values of Hit.\nQuantize the weights -Improving the quantization by structured random rotations. The above 1-bit and multi-bit quantization approach work best when the scales are approximately equal across different dimensions.\nIn the decoding phase, the server needs to perform the inverse rotation before aggregating all the updates.\n"},"KB/Sketching":{"title":"Sketching","links":[],"tags":["temp"],"content":"Sketching\n\nIn unsupervised machine learning, a category of algorithms that perform a preliminary similarity analysis on examples. Sketching algorithms use a locality-sensitive hash function\nto identify points that are likely to be similar, and then group them into buckets.\nSketching decreases the computation required for similarity calculations on large datasets. Instead of calculating similarity for every single pair of examples in the dataset, we calculate similarity only for each pair of points within each bucket.\n"},"KB/Skew-Tilt":{"title":"Skew Tilt","links":[],"tags":["augmentation"],"content":"Skew Tilt\n\nThe image is tilted forwards, backwards, left, or right a maximum of 22.5°. This gives the illusion that the image is being viewed from a different perspective than originally seen and creates realistic examples.\n"},"KB/Skewed-data":{"title":"Skewed data","links":[],"tags":["explainability"],"content":"Skewed data\n\nbias within the data acquisition process.\n"},"KB/Skip-Connection":{"title":"Skip Connection","links":["KB/Effect-Of-Depth","Tag-Pages/loss","tags/architecture","Vanishingexploding-gradients"],"tags":["temp","architecture"],"content":"Skip Connection\n\n\nx_i = F(x_{i-1}) + x_{i-1}\nEffect Of Depth\nPrevious layer gradient carried to next module untouched → loss surface is smoother\nTransferarchitecture to prevent Vanishingexploding gradients\nLearns the difference (residual) F(x) = H(x)-x\n"},"KB/Skip-Gram":{"title":"Skip Gram","links":[],"tags":["nlp"],"content":"Skip Gram\n\nthe distributed representation of the input word is used to predict the context.\ntries to predict the neighbors of a word\nworks well with a small amount of the training data, represents well even rare words or phrases.\nSkip-gram rely on single words input, it is less sensitive to overfit frequent words, because even if frequent words are presented more times that rare words during training, they still appear individually\ntends to study different contexts separately\nneeds more data to be trained contains more knowledge about the context.\ntakes in pairs (word1, word2) generated by moving a window across text data, and trains a 1-hidden-layer neural network based on the synthetic task of given an input word, giving us a predicted probability distribution of nearby words to the input.\nA virtual [one hot](one hot.md) encoding of words goes through a ‘projection layer’ to the hidden layer; these projection weights are later interpreted as the word embeddings.\nSo if the hidden layer has 300 neurons, this network will give us 300-dimensional word embeddings.\nalso uses [Negative Sampling](Negative Sampling.md)\n"},"KB/Slice-Based-Volume-Rendering":{"title":"Slice Based Volume Rendering","links":[],"tags":["visualization"],"content":"Slice Based Volume Rendering\n\nassign transparency inversely proportional to the number of slices\n\n"},"KB/Sliding-Window-Attention":{"title":"Sliding Window Attention","links":["KB/Attention","KB/Layers","KB/Receptive-field","KB/Features"],"tags":["architecture"],"content":"Sliding Window Attention\n\nGiven the importance of local context, the sliding window Attention pattern employs a fixed-size window Attention surrounding each token\n\nmultiple stacked Layers of such windowed Attention results in a large Receptive field, where top Layers have access to all input locations and have the capacity to build representations that incorporate information across the entire input\nBut a model with typical multiple stacked transformers will have a large Receptive field. This is analogous to CNNs where stacking Layers of small kernels leads to high level Features that are built from a large portion of the input (Receptive field)\nDepending on the application, it might be helpful to use different values of w for each layer to balance between efficiency and model representation capacity.\n\n\nGiven a fixed window size w, each token attends to \\frac{1}{2}w tokens on each size\nComplexity is O(n \\times w)\n\nw should be small compared to n\n\n\nWith l layers, Receptive field size is l \\times w\n\n"},"KB/SlimStampen":{"title":"SlimStampen","links":["KB/ACT-R","KB/Recall","KB/Shrinkage"],"tags":["usermodel"],"content":"SlimStampen\n\nPredicting University Students’ Exam Performance Using a Model-Based Adaptive Fact-Learning System\n\nLiterature\n\nDigital learning systems allow learners to track their progress and make study decisions informed by data1.\nFor example, Duolingo, a language- learning tool, shows learners an overview of their mastery of each lesson in a dashboard (Figure 1(a) in Settles &amp; Meeder, 2016). Rosetta Stone, another language-learning tool, has a similar dashboard and includes a suggested next study activity (Ridgeway, Mozer, &amp; Bowles, 2017).\nAdaptive learning systems take this a step further by assuming control over some study choices that might otherwise be made by learners. Using an internal model of the learner that is informed by the learner’s performance, such systems can adapt the learning experience in real time (VanLehn, 2006).\nThe adaptation can include changing the difficulty of the problems presented to the learner, changing the amount of feedback that the learner receives, and changing the scheduling of repetitions within and between learning sessions\nWhat type and degree of adaptivity are most beneficial is an empirical question and depends on whether the adaptive system accurately traces the acquisition and forgetting of knowledge over time. If implemented well, adaptive learning systems can help students achieve more effective study behaviour by facilitating spaced repetition, active study, and other effective techniques.\n\nSlimStampen: A Model-Based Adaptive Fact-Learning System App\n\nFollowing correct answers, the next trial commenced after one second. For incorrect answers, feedback remained on the screen until the learner pressed the “Next” button at the bottom of the screen (see Figures 1b and 1d), making the feedback similar to the study trials.\n\nScheduling Algorithm\n\nextension of the adaptive item-learning model by Pavlik and Anderson (2005; 2008) and has been tested in laboratory settings (van Rijn, van Maanen, &amp; van Woudenberg, 2009; Sense, Behrens, Meijer, &amp; van Rijn, 2016; Sense, Meijer, &amp; van Rijn, 2018) but has not been deployed in a university course before.\nhis model capitalizes on the spacing effect (see Dempster, 1988, for a review) within a single session by scheduling repetitions as far apart as possible, while also\noptimizing for the testing effect (see van den Broek et al., 2016, for a review) by repeating items soon enough that most responses are correct.\nThe model represents every encountered item by a unique memory chunk, based on the ACT-R theory of declarative memory (Anderson, 2007).\nEach chunk has an activation—a representation of the ease with which that item could be retrieved—that receives a boost whenever an item is re-encoded and that decays over time\nThe activation A of a chunk i at time t, given n previous encounters at t1,…,tn seconds ago, is\n\nA_{i}(t) = ln(\\Sigma_{j=1}^{n}t_{j}^{-d_{i}(t)}\nd_{i}(t)=c \\ast e^{A_{i}(t_{n-1})}+\\alpha_{i} \n\n\nWhen a new trial commences, the model determines the activation of all items 15 seconds in the future, and if the item with the lowest activation has an activation value below a retrieval threshold, that item will be scheduled for presentation\nIf all predicted activations are above the retrieval threshold, the model will introduce a new item\ny selecting items on the basis of their activation, items will be repeated with as much spacing as possible, while ensuring that, theoretically, a correct response can still be given.\nThe decay of the activation (parameter d in Equation (1)) varies between items to account for differences in difficulty. The higher this decay, the faster a chunk’s activation will decrease, causing it to be repeated sooner than an item with a lower decay.\nThe decay d of a chunk i at time t depends on the activation of the chunk at the time of its previous encounter, as well as an offset that we label the rate of forgetting, α\nThe model assumes that each item has a standard initial rate of forgetting when it is first presented. However, this value is updated during learning\nAt each presentation, the model calculates an expected response time, E(RT ), based on the activation at the time of the presentation (e−Ai , based on Equation (5) in Anderson, Bothell, Lebiere, &amp; Matessa, 1998) and an estimated reading time of the prompt (based on the number of characters in the prompt; see Section 2.2.1 in Nijboer, 2011, for details).\nThe accuracy of the response and the mismatch between expected and observed response time are used to update the value of the rate-of-forgetting parameter.\nUsing both accuracy and response time to update the model allows for adjustment of the parameter estimate after any response, not just after an incorrect response.\nA correct but slower-than-expected response signals that the [memory trace](memory trace.md) has decayed further than assumed, meaning that the item’s true rate of forgetting is higher than the current estimate.\nThat is, when a learner arrives at the right answer but takes longer than anticipated, they likely struggled to Recall the information\nConversely, an incorrect or missing response suggests that the activation of the\nitem’s [memory trace] actually dropped below the retrieval threshold, which means that the true rate of forgetting should be higher because this item’s activation was expected to be above the threshold (which was fixed at [ACT-R](memory trace] actually dropped below the retrieval threshold, which means that the true rate of forgetting should be higher because this item’s activation was expected to be above the threshold (which was fixed at [ACT-R.md)’s default value).\nAn unexpectedly fast correct response, on the other hand, indicates a stronger- than-expected [memory trace](memory trace.md) and implies that the estimated rate of forgetting should be adjusted downward.\nSince interruption or distraction can cause disproportionately large response times, observed response times are capped before their mismatch with the expected response time is calculated.\nTo update the rate of forgetting after each trial, the model uses a binary search in a small window around the previous value to identify the rate of forgetting that minimizes the mismatch between E(RT) and RT′\n\nUsage of the System\n\nMost students exhibited strong “cramming” behaviour, with much higher SlimStampen usage in the days leading up to the exam: in both cohorts, we observed a sharp increase in activity starting around 10 days before the exam and peaking on the last day. As the exam neared, usage intensified throughout the day and extended into the night.\n\nExam Performance\n\nIn both cohorts, students that used SlimStampen (92.6% of students) obtained higher grades than those that did not—averaging 6.91 compared to 5.86, respectively\na direct comparison of these groups is problematic due to selection effects and the imbalanced distributio\n\nAmount of Practice\n\nnumber of study trials completed was positively correlated with the final grade completing more trials was associated with higher grades on the exam\nThe number of unique days on which a learner engaged with the tool—an index of spaced practice—was also positively correlated with exam grades (r = 0.27, t(283) = 4.81, p &lt; 0.001)\nthe two measures of engagement were strongly and positively correlated (r = 0.75, t(283) = 18.78, p &lt; 0.001).\n\nStudied Versus Non-studied Items\n\nWe observed a large difference between exam questions that learners had used the system to study and questions that they had not3: students’ accuracy was 83.7% on studied items but only 53.6% on unstudied items\nA mixed-effects logistic regression (with random intercepts for learners and items) confirmed that encountering an item during SlimStampen rehearsal considerably increased the chances of a correct answer on the exam (bstudied/not studied = 1.70, SE = 0.18, z = 9.06, p &lt; 0.001).\n\nRates of Forgetting and Grades\n\nThe rate of forgetting, which was initially estimated for each learner–item combination, was converted into a learner-specific rate of forgetting by averaging over all studied items\nThe negative correlation shows that a learner who was estimated to forget material more slowly also tended to obtain higher grades.\nIn practice, a possible relationship between someone’s rate of forgetting and eventual exam performance would be most useful if it could be detected ahead of time rather than on the day of the exam—when it is too late to potentially help struggling students, for example\nThis pattern could be driven by additional learners that start at the last minute and demonstrate poor learning performance and poor grades\n\nPredicting Performance on Individual Exam Questions\n\nThe results reported so far confirm that the expected patterns emerged in the aggregate: a learner’s average rate of forgetting was strongly related to their average performance on the exam\nA step-wise backward elimination procedure was used to find the best model: starting with the full model, the term with the lowest absolute z-value was removed until the simpler model was no longer preferred on the basis of BIC and AIC (Gelman &amp; Hill, 2006).\nAdditionally, the estimated rate of forgetting modulated the effect such that learner– item combinations with very low rates of forgetting have a higher chance of yielding a correct answer.\nThe differences in rates of forgetting are especially pronounced at a low number of repetitions due to the non-linear mapping between the predictors and the predicted probability introduced by the logit function\n\nPredicting Performance on the Exam\n\nWe used lasso regression (Tibshirani, 1996) to predict grades using nine predictors: a student’s accuracy during study, their cohort, their cumulative usage time, the number of days on which they used the system, the number of items they studied, the number of sessions they recorded, the number of trials they completed, their estimated rate of forgetting, and their median response time\nThe advantage of lasso regression is that the Shrinkage term handles multicollinearity between the predictors by shrinking their coefficients\nThe Shrinkage is achieved by imposing a cost function on the magnitude of the\ncoefficients themselves: the best fit is achieved by the model that minimizes the OLS with the smallest coefficients. In fact, coefficients are shrunk entirely if they do not explain sufficient variance to justify inclusion in the model. In lasso regression, predictors must be normalized to ensure that the Shrinkage term affects all predictors equally. A convenient consequence of normalized predictors is that their post-Shrinkage\ncoefficients directly indicate their importance: since all predictors are on the same scale, the most important predictor retains the largest (absolute) coefficient.\n250-fold cross-validation procedure\n\nComparing Self-Reported and Recorded Study Times\n\nThis means that students who used SlimStampen more did not necessarily self- report studying more overall. Thus, the positive association between more SlimStampen usage and higher grades was unlikely to be a consequence of higher motivation alone.\nThis suggests, unsurprisingly, that general studiousness led to higher exam performance\nMore interestingly, time spent studying with SlimStampen was time well spent, as the expected gain in grades associated with additional hours of study was 0.11 points, compared to only 0.03 points gained by an hour of unspecified study time.\n\nDiscussion\n\nStudents’ rates of forgetting, estimated by the system during use, were correlated with exam performance up to two weeks before the exam (Figure 2), even though &lt; 5% of the data were available at that point\nFurthermore, rate-of-forgetting estimates for individual facts were predictive of learners’ performance on the associated exam questions, along with the number of times these facts were repeated during study\nOne limitation of the sample was that we did not know what other study methods students may have used alongside the system. It is possible that the spike in activity in the days preceding the exam was caused by students verifying that they had retained the knowledge obtained through other study activities\n\nImplications\n\ncontrolling within-session study decisions through the adaptive fact-learning system, leaving other study decisions—when to study, which chapter to study, how long to study, and whether to study with open response or multiple-choice questions—to the learner\nstudents still made sub-optimal decisions about when to repeat a lesson that they had studied previously.\nAlternatively, the system could suggest the lesson that would yield the largest learning gain at the moment a student decides to start a session\n\nPictures\n\n\n\n\n\n"},"KB/Small-World-graphs":{"title":"Small World graphs","links":["KB/Graphs"],"tags":["temp"],"content":"Small World Graphs\n\nAny two nodes in the graph are connected via a smalll number of steps\n"},"KB/Smart-Augmentation":{"title":"Smart Augmentation","links":[],"tags":["augmentation"],"content":"Smart Augmentation\n\nutilizes a similar concept as the [Neural Augmentation](Neural Augmentation.md) technique\nHowever, the combination of images is derived exclusively from the learned parameters of a prepended CNN, rather than using the Neural Style Transfer algorithm.\nanother approach to meta-learning augmentations\nThis is done by having two networks, Network-A and Network-B. Network-A is an augmentation network that takes in two or more input images and maps them into a new image or images to train Network-B. The change in the error rate in Network-B is then\nbackpropagated to update Network-A.\nAdditionally another loss function is incorporated into Network-A to ensure that its outputs are similar to others within the class. Network-A uses a series of convolutional layers to produce the augmented image\nThe conceptual framework of Network-A can be expanded to use several Networks trained in parallel. Multiple Network-As could be very useful for learning class-specific augmentations via meta-learning\n"},"KB/Smooth-Grad":{"title":"Smooth-Grad","links":[],"tags":["explainability"],"content":"Smooth-Grad\n\n@smilkovSmoothGradRemovingNoise2017\nreduces visual noise and, hence, improves visual explanations about how a DNN is making a classification decision. Comparing their work to several gradient-based sensitivity map methods such as LRP, [DeepLift], and [Integrated Gradients](DeepLift], and [Integrated Gradients.md) (IG) [96], which estimate the global importance of each pixel and create saliency maps, showed that Smooth-Grad focuses on local sensitivity and calculates averaging maps with a smoothing effect made from several small perturbations of an input image. The effect is enhanced by further training with these noisy images and finally having an impact on the quality of sensitivity maps by sharpening them.\na local, post hoc approach gave visual and textual justifications of the predictions with the help of two novel explanation datasets through crowd sourcing.\ninvolves adding random noise to the input and computing the attribution maps multiple times with the noisy inputs.\nThe final attribution map is obtained by averaging the maps obtained from the noisy inputs. The idea behind this technique is that the noise added to the input image will cause the model to activate different features in the input, resulting in a more stable and interpretable attribution map.\n\nTechnical Details\n\nConsider an image classification task where an input image x is to be classified as a single class from a set C. For every class c \\in C, the output class is represented as class(x) = argmax_{c \\in C}S_{c}(x). Using this class, a sensitivity map M_{c}(x) can be generated by differentiating with respect to x, M_{c}(x) = \\frac{\\partial S_{c}}{\\partial x} . M_{c}, being a sensitivity map, thus represents the influential regions of the image used to make the prediction. Since these maps are noisy in nature, Smilkov et al. propose SmoothGrad, a modification of the previous method where instead of using \\partial S_{c}, a smoothing is applied using a Gaussian kernel to \\partial S_{c}. The authors also find that it is not possible to directly compute the smoothing due to high dimensionality, and thus approximate the calculation by averaging multiple maps computed in the neighborhood of x using random sampling. The final SmoothGrad equation then becomes \\hat M_{c}(x) = \\frac{1}{n}\\Sigma_{1}^{n}M_{c}(x + \\mathcal{N}(0, \\sigma^{2})), where \\mathcal{N}(0, \\sigma^{2}) is the Gaussian noise and \\sigma is the standard deviation.\n"},"KB/SmoothGrad-Square":{"title":"SmoothGrad Square","links":[],"tags":["explainability"],"content":"SmoothGrad Square\n\n smoothgrad squre\n"},"KB/SmoothMix":{"title":"SmoothMix","links":[],"tags":["augmentation"],"content":"SmoothMix\n\n@leeSmoothMixSimpleEffective2020\nmask-based approach\nmatching closely with the [Cutout] and [CutMix](Cutout] and [CutMix.md) techniques\nthe mask has soft edges with gradually decreasing intensity\nthe mixing strategy is the same\nThe augmented image has mixed pixel values depending on the strength of the mask\n\\lambda= \\frac{\\Sigma_{i=1}^{W}\\Sigma_{j=1}^{H}G_{ij}}{WH}\nGij is the pixel value of mask G, H is height, and W is width\nxnew = G.xa + (1 − G).xb\nynew = λ.ya + (1 − λ).yb\n"},"KB/Smoothness":{"title":"Smoothness","links":[],"tags":["temp"],"content":"Smoothness\n\nEvery supervised machine learning method assumes that there’s a set of functions that can transform inputs into outputs such that similar inputs are transformed into similar outputs. If an input X produces an output Y, then an input close to X would produce an output proportionally close to Y.\n"},"KB/SnapMix":{"title":"SnapMix","links":[],"tags":["augmentation"],"content":"SnapMix\n\n@huangSnapMixSemanticallyProportional2021\naugments training images by ex- tracting and merging random image regions of dif- ferent sizes, where the region size is drawn through the beta distribution for both the images.\nGen- erated image label is assigned based on semantic composition from normalized (sum to one) CAMs.\nHowever, the summation of label coefficients can exceed beyond one depending on the semantic composition of the output image.\n"},"KB/Social-Construction-of-XAI,-do-we-need-one-definition-to-rule-them-all":{"title":"Social Construction of XAI, do we need one definition to rule them all","links":[],"tags":["explainability"],"content":"Social Construction of XAI, Do We Need One Definition to Rule Them All\n\n@ehsanSocialConstructionXAI2022\n\nAbstract\n\nIn this paper, we argue why a singular definition of XAI is neither feasible nor desirable at this stage of XAI’s development\nWe view XAI through the lenses of Social Construction of Technology (SCOT) to explicate how diverse stakeholders (relevant social groups) have different interpretations (interpretative flexibility) that shape the meaning of XAI. Forcing a standardization (closure) on the pluralistic interpretations too early can stifle innovation and lead to premature conclusions.\n\nOf Bicycles &amp; Explainable AI\n\nAs we reflect on the evolution of the bicycle, why and how did things evolve the way they did?\nWe will address this question using three concepts from SCOT. First, we have relevant social groups—stakeholders with skin in the game such as bikers, families of bikers, mechanics fixing bikes, etc. These are the ones who are involved in or affected by a technological development\nDifferent relevant social groups have their own interpretive flexibility— interpretations of what it means to be a bicycle.\nifferent interpretive flexibilities can give rise to different types of bicycles such as mountain bikes, electric bikes, BMX bikes, etc\nFinally, we have the notion of closure– over time, some interpretations of the bicycle achieved stability while others withered out (e.g., equal sized wheels won out over differently-sized wheels\nJust like bicycles, XAI has its relevant social groups\nLet’s consider two relevant social groups: the Natural Language Processing (NLP) and Computer Visions (CV) communitie\nGiven each group has its own ways of knowing (epistemology), there is interpretive flexibility on how they operationalize the notion of explainability\nin NLP question-answering, explanations are often of the form of additional text that justifies the ground truth answer\nIn CV, object recognition can consider saliency maps that show how visual features correlate to a predicted label\nThis is to be expected because, unlike bicycles, we don’t have 200+ years of development to reach clusters of closures yet.\nolving XAI challenges may require more than just “opening the black-box” [6]\nHuman-centered XAI (HCXAI) advocates to tackle XAI problems through a sociotechnical view (vs. a purely technical one) [7]\nWe need to consider who is opening the box just as much as the algorithmic mechanisms of opening it\nWhereas a lot of initial focus was on developers and data scientists as end-users of XAI systems, there is a growing recognition that we need to accommodate a diverse set of end-users, especially non-AI experts [10, 11]\n\nMaking Progress in XAI\n\nXAI is pluralistic\nGiven the different epistemic cultures co-existing in the space,we cannot expect monolithic conformity at this stage.\nPluralism, however, does not mean that anything goes; in fact, it’s the opposite—we need to be precise in our articulation of what we mean by explainability when we communicate.\nThus, instead of using the term at face value, whenever we write a paper, we should strive to justify how our conception of explainability satisfies some of the shared goals we have in the space.\nwho is saying what, when, and why. To grasp the flavor of explainability in a given context, we need to pay attention to a relevant social group’s interpretation of it and how that informs their operationalization.\nWhile the notion of XAI is in flux, we are fortunate to join the conversation at this stage. We have substantial agency in steering the discourse, a privilege we need to exercise responsibly.\n"},"KB/Soft-Attention":{"title":"Soft Attention","links":["KB/Attention","KB/Seq2Seq"],"tags":["architecture"],"content":"Soft Attention\n\nFor a simple Seq2Seq, all hidden state vectors h_t across timesteps are linearly combined\nc_i = \\Sigma_{j=1}^T \\alpha_{ij} h_j\na_{ij} = \\frac{exp(e_{ij})}{\\Sigma_{k=1}^T exp(e_{ij})}\n\n"},"KB/Soft-Parameter-Sharing":{"title":"Soft Parameter Sharing","links":[],"tags":["multitask"],"content":"Soft Parameter Sharing\n\n\nConstrain weights by adding terms to loss function\n\n||W_{A}-W_{B}||^{2} + ||W_{A}-W_{C}||^{2}\n\n\n"},"KB/Softlimit-Setting-Function":{"title":"Softlimit Setting Function","links":["KB/Manipulator"],"tags":["robotics"],"content":"Softlimit Setting Function\n\nThe Softlimit Setting Function is a function to set the axis travel limit range of the Manipulator motion in software.\n"},"KB/Softmax":{"title":"Softmax","links":["KB/Entropy","KB/Uniform-Distribution"],"tags":["architecture"],"content":"Softmax\n\nOutput : probabilities\np = \\frac{1}{\\Sigma_{i = 1, .., n}e^{\\frac{\\alpha y_{i}}{T}}}(e^{\\frac{\\alpha y_{1}}{T}} , e^{\\frac{\\alpha y_{2}}{T}} , …, e^{\\frac{\\alpha y_{n}}{T}})&#039;\nSofter argmax (0,1)\nMultinoulli\n\n\nEntropy\n\n\\alpha determines Entropy\nIf it is 0, and Uniform Distribution and limit to infinity → binary vector which is 0 everywhere except at position i when y is maximal\n\nTemperature\n\nHigher the T → Softer it the distribution. Aka less confident about distribution\nLower → Harder. More confident\n\n"},"KB/Softplus":{"title":"Softplus","links":[],"tags":["architecture"],"content":"Softplus\n\n\\ln(1+e^x)\n"},"KB/Somatic":{"title":"Somatic","links":[],"tags":["brain"],"content":"Somatic\n\nVoluntary\nSkeletal movement\n"},"KB/Somatosensory-Cortex":{"title":"Somatosensory Cortex","links":["KB/Parietal-lobe"],"tags":["brain"],"content":"Somatosensory Cortex\n\nLocated in the Parietal lobe, this region of the brain processes touch, pressure, and pain information.\n"},"KB/Sono-stimulation":{"title":"Sono-stimulation","links":["KB/Ultrasound"],"tags":["brain"],"content":"Sono-stimulation\n\nThe activation of neural networks using Ultrasound.\n"},"KB/Sonogenetics":{"title":"Sonogenetics","links":[],"tags":["brain"],"content":"Sonogenetics\n\nA novel investigative approach that turns genetically modified neurons on and off using ultrasonic waves.\n"},"KB/Soundify":{"title":"Soundify","links":[],"tags":["architecture"],"content":"Soundify\n\nIn video editing, sound in half of the story\nfor professional video editing, the problems come from finding suitable sounds, aligning sounds, video and tuning parameters\nmatches sound efects to video - uses quality sound efects libraries and CLIP - Concretely, the system has three parts: classification, synchronization, and mix\nThe classification matches efects to a video by classifying sound emitters within - To reduce the distinct sound emitters, the video is split based on absolute color histogram distances\nIn the synchronization part, intervals are identified comparing efects label with each frame and pinpointing consecutive matches above a threshold\nIn the mix part, efects are split into around one-second chunks - chunks are stitched via crossfades.\n"},"KB/Sparse-Dictionary-Learning-Loss":{"title":"Sparse Dict Learning Loss","links":["Tag-Pages/loss","KB/Lp-Regularization"],"tags":["loss"],"content":"Sparse Dict Learning loss\n\n$$L(X) = n^{-1}\\Sigma_i^n ||x_i - Dr_i ||^2 + \\lambda \\Sigma_i |r_i|$\n\\lambda \\Sigma_i |r_i| is Lasso/L1 Lp Regularization\nPredictions : r = argmin_r ||x- Dr_i ||^2 + \\lambda \\Sigma_i |r_i|\n"},"KB/Sparse-Encoder-Indexes":{"title":"Sparse Encoder Indexes","links":[],"tags":["architecture"],"content":"Sparse Encoder Indexes\n\nefficiently handle large-scale knowledge bases with diverse data formats and structures.\nFor semantic search, one may compare different approaches of combining dense vector indexes and sparse encoder indexes to traditional keyword-based retrieval.\n"},"KB/Sparse-Evolutionary-Training":{"title":"Sparse Evolutionary Training","links":[],"tags":["architecture"],"content":"Sparse Evolutionary Training\n\n(Mocanu et al., 2018; Liu et al., 2021b)\nwhich randomly initializes the sparse connectivity between layers randomly and dynamically adjusts the sparse connectivity via a parameter prune-and-grow scheme during the course of training\nThe parameter prune-and-grow scheme allows the model’s sparse structure to gradually evolve, achieving better performance than naively training a static sparse network\n"},"KB/Sparse-Transformer":{"title":"Sparse Transformer","links":["KB/Transformer","KB/Strided-Attention"],"tags":["architecture"],"content":"Sparse Transformer\n\npaper\nUses Strided Attention\n"},"KB/Sparsity":{"title":"Sparsity","links":[],"tags":["temp"],"content":"Sparsity\n\nThe number of elements set to zero (or null) in a vector or matrix divided by the total number of entries in that vector or matrix. For example, consider a 10x10 matrix in which 98 cells contain zero.\n"},"KB/Spatial-Context-Structure":{"title":"Spatial Context Structure","links":[],"tags":["semisupervisedlearning"],"content":"Spatial Context Structure\n\nbased on the spatial relations among image patches\nimage jigsaw puzzle\ncontext prediction\ngeometric transformation recognition\n"},"KB/Spatial-Transformer":{"title":"Spatial Transformer","links":["KB/Transformer"],"tags":["architecture"],"content":"Spatial Transformer\n\nTransformer\n\nclass SpacialTransformNew(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        # Spatial [Transformer|[transformer](Transformer.md) localization-network\n        linear = nn.Linear(32, 3 * 2)\n        # Initialize the weights/bias with identity transformation\n        linear.weight.data.zero_()\n        linear.bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n        \n        self.compute_theta = nn.Sequential(\n            nn.Conv2d(1, 8, kernel_size=7),\n            nn.MaxPool2d(2, stride=2),\n            nn.ReLU(True),\n            nn.Conv2d(8, 10, kernel_size=5),\n            nn.MaxPool2d(2, stride=2),\n            nn.ReLU(True),\n            Rearrange(&#039;b c h w -&gt; b (c h w)&#039;, h=3, w=3),\n            nn.Linear(10 * 3 * 3, 32),\n            nn.ReLU(True),\n            linear,\n            Rearrange(&#039;b (row col) -&gt; b row col&#039;, row=2, col=3),\n        )\n \n    # Spatial transformer network forward function\n    def stn(self, x):\n        grid = F.affine_grid(self.compute_theta(x), x.size())\n        return F.grid_sample(x, grid)"},"KB/Spatiotemporal-Convolutional-Neural-Network":{"title":"Spatiotemporal Convolutional Neural Network","links":[],"tags":["semisupervisedlearning"],"content":"Spatiotemporal Convolutional Neural Network\n\n3D convolution operation was first proposed in 3DNet [62] for human action recognition\nCompared to 2DConvNets which individually extract the spatial information of each frame and then fuse them together as video features, 3DConvNets are able to simultaneously extract both spatial and temporal features from multiple frames.\nVGG-like 11-layer 3DConvNet designed for human action recognition\nThe network contains 8 convolutional layers, and 3 fully connected layers. All the kernels have the size of 3 x 3 x 3, the convolution stride is fixed to 1 pixel\nThe input of C3D is 16 consecutive RGB frames where the appearance and temporal cues from 16-frame clips are extracted\nHowever, the paper of long-term temporal convolutions (LTC) [67] argues that, for # the long-lasting actions, 16 frames are insufficient to represent whole actions which last longer.\n"},"KB/Speaker-Verification":{"title":"Speaker Verification","links":["KB/Features"],"tags":["architecture"],"content":"Speaker Verification\n\nDeep Neural Networks for Small Footprint Text-dependent Speaker Verification\nnvestigates the use of deep neural networks (DNNs) to train speaker embeddings for a small footprint text-dependent speaker verification task\nstacked filterbank Features as input\nDuring speaker enrollment, the trained DNN is used to extract speaker-specific Features/embeddings by averaging the activations from the last hidden layer (called deep-vectors or “d-vectors” for short), which is taken as the speaker model\nd-vector is extracted for each utterance and compared to the enrolled speaker model to make a verification decision by calculating the [cosine distance](cosine distance.md) between the test d-vector and the claimed speaker’s d-vector, similar to the i-vector framework\nA verification decision is made by comparing the distance to a threshold\nDNN based speaker verification system achieves good performance compared to a popular i-vector system on a small footprint text-dependent speaker verification task\n"},"KB/Speakers-in-the-Wild":{"title":"Speakers in the Wild","links":[],"tags":["dataset"],"content":"Speakers in the Wild"},"KB/SpecAugment":{"title":"SpecAugment","links":["Augmentation","KB/Features","KB/LibriSpeech","KB/Swichboard","KB/Fitting"],"tags":["augmentation"],"content":"SpecAugment\n\nSpecAugment: a Simple Data Augmentation Method for Automatic Speech Recognition\nsimple data augmentation method for [speech recognition](speech recognition.md)\napplied directly to the feature inputs of a neural network\nwarping the Features, masking blocks of frequency channels, and masking blocks of time steps\napply SpecAugment on Listen, Attend and Spell (LAS) networks for end-to-end [speech recognition](speech recognition.md) tasks\nLibriSpeech\nSwichboard\nend-to-end LAS networks by augmenting the training set using simple handcrafted policies\nconverts ASR from an over-Fitting to an under-Fitting problem, and they are able to gain performance by using bigger networks and training longer\n"},"KB/Specificity":{"title":"Specificity","links":[],"tags":["loss"],"content":"Specificity\n\nSpecificity = \\frac{TN}{TN+FP}\n"},"KB/Spectrogram":{"title":"Spectrogram","links":[],"tags":["brain"],"content":"Spectrogram\n\n\n"},"KB/Speculative-Execution":{"title":"Speculative Execution","links":[],"tags":["parallelcomputing"],"content":"Speculative Execution\n\nAllow the execution of complete instructions or parts of instructions before being sure whether this execution is required\n"},"KB/Speculum":{"title":"Speculum","links":[],"tags":["medical"],"content":"Speculum\n\nAn instrument used when examining body orifices to help widen the opening\n"},"KB/Speech-Emotion-Recognition":{"title":"Speech Emotion Recognition","links":["KB/Manifold"],"tags":["architecture"],"content":"Speech Emotion Recognition\n\nGAN-based Data Generation for Speech Emotion Recognition\nform of speech emotion spectrograms\nused for training speech emotion recognition networks\nnvestigate the usage of GANs for capturing the data Manifold when the data is eyes-off, i.e., where they can train networks using the data but cannot copy it from the clients\nCNN-based GAN with spectral normalization on both the generator and discriminator, both of which are pre-trained on large unlabeled speech corpora\neven after the data on the client is lost, their model can generate similar data that can be used for model bootstrapping in the future\n"},"KB/Speech-Recognition":{"title":"Speech Recognition","links":["KB/Perplexity","KB/Basic-RNN-Architectures","KB/Wall-Street-Journal-task","n-gram","KB/CTC"],"tags":["architecture"],"content":"Speech Recognition\n\nRecurrent Neural Network Based Language Model\n\n50% reduction of Perplexity\nmixture of several Basic RNN Architectures\nWall Street Journal task\nconnectionist language models are superior to standard n gram techniques, except their high computational (training) complexity\nbreak the myth that language modeling is just about counting n-grams, and that the only reasonable way how to improve results is by acquiring new training dat\n\n\nTowards End-To-End Speech Recognition with Recurrent Neural Networks\n\ncharacter-level speech recognition system that directly transcribes audio data with text using a recurrent neural network\ncombination of the deep bidirectional LSTM recurrent neural network architecture and a modified Connectionist Temporal Classification (CTC) objective function\nword error rate\nWall Street Journal task\n\n\n"},"KB/Speech-Resynthesis":{"title":"Speech Resynthesis","links":[],"tags":["architecture"],"content":"Speech Resynthesis\n\nSpeech Resynthesis from Discrete Disentangled Self-Supervised Representations\nself-supervised discrete representations for the task of speech resynthesis\nseparately extract low-bitrate representations for speech content, prosodic information, and speaker identity\nThis allows to synthesize speech in a controllable manner\nevaluate the F0 reconstruction, speaker identification performance (for both resynthesis and voice conversion), recordings’ intelligibility, and overall quality using subjective human evaluation\nultra-lightweight speech codec\n"},"KB/Spiking-Networks":{"title":"Spiking Networks","links":["KB/Poisson-Process"],"tags":["architecture"],"content":"Spiking Networks\n\nPoisson Process\n"},"KB/Spirometer":{"title":"Spirometer","links":[],"tags":["medical"],"content":"Spirometer\n\nA device that measures the amount of air breathed in and out by the lungs\n"},"KB/Spline-Motion-Type":{"title":"Spline Motion Type","links":["KB/Spline"],"tags":["robotics"],"content":"Spline Motion Type\n\nA calculated path that the robot executesthat may be parabolic in shape. A Spline motion may also accomplish a free form curve with mixtures of circular and parabolic shapes.\n"},"KB/Spline":{"title":"Spline","links":["KB/Manipulator"],"tags":["robotics"],"content":"Spline\n\nA smooth, continuous function used to approximate a set of functions that are uniquely defined on a set of sub-intervals. The approximating function and the set of functions being approximated intersect at a sufficient number of points to insure a high degree of accuracy in the approximation. The purpose for the smooth function is to allow a robot Manipulator to complete a task without jerky motion.\n"},"KB/Square-Integrable":{"title":"Square Integrable","links":["KB/PDF"],"tags":["temp"],"content":"Square Integrable\n\nReal valued RV with PDF p is square integrable if the Uncentered Second moment is finite\nE[X^{2}] = \\int_\\mathbb{R} x^{2}p(x)dx is finite\n"},"KB/Squared-Error":{"title":"Squared Error","links":[],"tags":["loss"],"content":"Squared Error\n\n(y- f(x))^2\nRegression\n"},"KB/Squared-Hinge":{"title":"Squared Hinge","links":["KB/Hinge-Loss","KB/Tanh"],"tags":["loss"],"content":"Squared Hinge\n\nHinge Loss\nproblems involving yes/no (binary) decisions and when you’re not interested in knowing how certain the classifier is about the classification\nTanh for last layer\nmaximum margin\n\n\\mathrm{sum}\\left( \\left( \\mathrm{max}\\left( 0, 1 - y \\cdot ŷ \\right) \\right)^{2} \\right)"},"KB/Stable-Difusion":{"title":"Stable Difusion","links":[],"tags":["architecture"],"content":"Stable Difusion\n\nlatent-difusion model - The main diference of this model with respect to the other ones is the use of a latent difusion model and that it performs image modification as it can perform operations in its latent space\nStable Difusion consists of two parts: the text encoder and the image generator - The image information creator works completely in the latent space\nThis property makes it faster than previous difusion models that worked in a pixel space\n"},"KB/Stack-GAN":{"title":"Stack GAN","links":["tags/todo","KB/Regularization","KB/Downsampling"],"tags":["architecture","todo"],"content":"Stack GAN\n\n\ntodo\n\n\nText to Image synthesis\n\n\nStackGAN decomposes the hard problem into more manageable sub-problems through a sketch-refinement process.\n\n\nThe Stage-I GAN sketches the primitive shape and colors of the object based on the given text description, yielding Stage-I low-resolution images.\n\n\nThe Stage-II GAN takes Stage-I results and text descriptions as inputs and generates high-resolution images with photo-realistic details. It is able to rectify defects in Stage-I results and add compelling details with the refinement process\n\n\nMulti Modal. Large no of ims that fit the given text\n\n\nArchitecture\n\n\n\n\n256×256 photo-realistic images conditioned on text descriptions. sketch-refinement process.\nConditioning Augmentation technique that encourages smoothness in the latent conditioning manifold.\n\nIntroduction\n\nGenerating photo-realistic images from text is an important problem and has tremendous applications, including photo-editing, computer-aided design, etc\nvery difficult to train GAN to generate high-resolution photo-realistic images from text descriptions\nSimply adding more upsampling layers in state-ofthe-art GAN models for generating high-resolution (e.g., 256×256) images generally results in training instability\nsupports of natural image distribution and implied model distribution may not overlap in high dimensional pixel space\nmore severe as the image resolution increases In analogy to how human painters draw\nBy conditioning on the Stage-I result and the text again, Stage-II GAN learns to capture the text information that is omitted by Stage-I GAN and draws more details for the object.\n\nConditioning Augmentation\n\nConditioning Augmentation technique to produce additional conditioning variables cˆ\nwe randomly sample the latent variables cˆ from an independent Gaussian distribution \\mathcal{N}(\\mu(\\varphi_{t}), \\Sigma(\\varphi_{t})), where the mean \\mu(\\varphi_{t}) and diagonal covariance matrix \\Sigma(\\varphi_{t}) are functions of the text embedding \\varphi_{t}\nThe proposed Conditioning Augmentation yields more training pairs given a small number of imagetext pairs, and thus encourages robustness to small perturbations along the conditioning manifold\nRegularization term to the objective of the generator during training D_{KL}(\\mathcal{N}(\\mu(\\varphi_{t}), \\Sigma(\\varphi_{t})) || \\mathcal{N}(0,I))\n[KL Divergence](KL Divergence.md) between the standard Gaussian distribution and the conditioning Gaussian distribution\nThe randomness introduced in the Conditioning Augmentation is beneficial for modeling text to image translation as the same sentence usually corresponds to objects with various poses and appearances.\n\nStage-I GAN\n\n\\varphi_{t} be the text embedding of the given description\nThe Gaussian conditioning variables \\hat c_{0} for text embedding are sampled from N(μ0(φt),Ʃ0(φt)) to capture the meaning of \\varphi_{t} with variations\nConditioned on cˆ0 and random variable z, Stage-I GAN trains the discriminator D0 and the generator G0 by alternatively maximizing \\mathcal{L}_{D_{0}} in Eq. (3) and minimizing \\mathcal{L}_{G_{0}}\n\n\n\\mathcal{L}_{D_{0}} = \\mathbb{E}_{(I_{0},t)\\sim p_{data}}[log D_{0}(I_{0}, \\varphi_{t})]+\\mathbb{E}_{z \\sim p_{z}, t \\sim p_{data}}[log(1- D_{0}(G_{0}(z , \\hat{c_{0}}, \\varphi_{t})))]\n\\mathcal{L}_{G_{0}}= \\mathbb{E}_{z \\sim p_{z}, t \\sim p_{data}}[log(1- D_{0}(G_{0}(z, \\hat{c_{0}}),\\varphi_{t}))] + \\lambda D_{KL}(\\mathcal{N}(\\mu_{0}(\\varphi_{t}), \\Sigma_{0}(\\varphi_{t}))|| \\mathcal{N}(0, I))\n\n\nwhere the real image I0 and the text description t are from the true data distribution pdata\nz is a noise vector randomly sampled from a given distribution pz (Gaussian distribution in this paper\nλ is a Regularization parameter that balances the two terms\nλ = 1 for all the exps.\nboth \\mu_{0}(\\varphi_{t}) and \\Sigma_{0}(\\varphi_{t}) are learned jointly with the rest of the network.\nFor the generator G0, to obtain text conditioning variable cˆ0, the text embedding φt is first fed into a fully connected layer to generate μ0 and σ0 (σ0 are the values in the diagonal of Ʃ0) for the Gaussian distribution N(μ0(φt),Ʃ0(φt)\nˆ0 are then sampled from the Gaussian distribution cˆ = μ +σ ⊙ε\ntrained by alternatively maximizing LD in Eq. (5) and minimizing LG in Eq. (6),\nconcatenated with a Nz dimensional noise vector to generate a W0 × H0 image by a series of up-sampling blocks\nthe text embedding φt is first compressed to Nd dimensions using a fully-connected layer\nand then spatially replicated to form a Md × Md × Nd tensor.\nthe image is fed through a series of down-sampling blocks until it has Md × Md spatial dimension\nThen, the image filter map is concatenated along the channel dimension with the text tensor\nThe resulting tensor is further fed to a 1×1 convolutional layer to jointly learn features across the image and the text.\nFinally, a fullyconnected layer with one node is used to produce the decision score.\n\nStage-II GAN\n\nLow-resolution images generated by Stage-I GAN usually lack vivid object parts and might contain shape distortions.\nis conditioned on low-resolution images and also the text embedding again to correct defects in Stage-I results\nThe Stage-II GAN completes previously ignored text information to generate more photo-realistic details.\nConditioning on the low-resolution result s0 = G0(z, cˆ0) and Gaussian latent variables cˆ\nDifferent from the original GAN formulation, the random noise z is not used in this stage with the assumption that the randomness has already been preserved by s0\nGaussian conditioning variables cˆ used in this stage and cˆ0 used in Stage-I GAN share the same pre-trained text encoder, generating the same\n\ntext embedding φt.\nStageI and Stage-II Conditioning Augmentation have different fully connected layers for generating different means and standard deviations\nIn this way, Stage-II GAN learns to capture useful information in the text embedding that is omitted by Stage-I GAN.\n\nModel Architecture.\n\n\nStage-II generator as an encoder-decoder network with residual blocks\n\n\ntext embedding φt is used to generate the Ng dimensional text conditioning vector cˆ\n\n\nspatially replicated to form a Mg ×Mg ×Ng tensor\n\n\nStage-I result s0 generated by Stage-I GAN is fed into several Downsampling blocks (i.e., encoder) until it has a spatial size of Mg × Mg\n\n\nThe image features and the text features are concatenated along the channel dimension\n\n\nThe encoded image features coupled with text features are fed into several residual blocks, which are designed to learn multi-modal representations across image and text feature\n\n\nseries of up-sampling layers\n\n\nare used to generate a W ⇥H high-resolution\n\n\nSuch a generator is able to help rectify defects in the input image while add\n\n\nmore details to generate the realistic high-resolution image.\n\n\nFor the discriminator, its structure is similar to that of Stage-I discriminator with only extra down-sampling blocks since the image size is larger in this stage\n\n\nTo explicitly enforce GAN to learn better alignment between the image and the conditioning text, rather than using the vanilla discriminator, we adopt the matching-aware discriminator\n\n\nDuring training, the discriminator takes real images and their corresponding text descriptions as positive sample pairs, whereas negative sample pairs consist of two groups\n\n\nImplementation details\n\n\nup-sampling blocks consist of the nearest-neighbor upsampling followed by a 3⇥3 stride 1 convolution\n\n\n\nBatch normalization [11] and ReLU activation are applied after every convolution except the last one\n\n\nThe residual blocks consist of 3⇥3 stride 1 convolutions, Batch normalization and ReLU. Two residual blocks are used in 128⇥128 StackGAN models while four are used in 256⇥256 models. The down-sampling blocks consist of 4⇥4 stride 2 convolutions, Batch normalization and LeakyReLU, except that the first one does not have Batch normalization.\n\n\nBydefault,Ng =128,Nz =100,Mg =16,Md =4, Nd = 128, W0 = H0 = 64 and W = H = 256\n\n\nFor training, we first iteratively train D0 and G0 of Stage-I GAN for 600 epochs by fixing Stage-II GAN\n\n\nThen we iteratively train D and G of Stage-II GAN for another 600 epochs by fixing Stage-I GAN.\n\n\nAll networks are trained using ADAM solver with batch size 64 and an initial learning rate of 0.0002. The learning rate is decayed to 1/2 of its previous value every 100 epochs.\n\n\nDatasets and evaluation metrics CUB\n\nOxford-102\nMS COCO\nEvaluation metrics\ninception score\nI = exp(ExDKL(p(y|x) || p(y))),\nwhere x denotes one generated sample, and y is the label predicted by the Inception model\nhe intuition behind this metric is that good models should generate diverse but meaningful images.\nTherefore, the KL divergence between the marginal distribution p(y) and the conditional distribution p(y|x) should be larg\n\nConclusions\n\nThe proposed method decomposes the text-to-image synthesis to a novel sketch-refinement process.\n\nStage-I GAN sketches the object following basic color and shape constraints from given text descriptions. Stage-II GAN corrects the defects in Stage-I results and adds more details, yielding higher resolution images with better image quality\nCompared to existing text-to-image generative models, our method generates higher resolution images (e.g., 256⇥256) with more photo-realistic details and diversity.\n\n"},"KB/Stacking-RNN":{"title":"Stacking RNN","links":["KB/Features","KB/Layers"],"tags":["architecture"],"content":"Stacking RNN\n\nDeeper\nEach level → output is seq of Features that is input at next set of Layers in the hierarchy\n\n"},"KB/Staged-Training":{"title":"Staged Training","links":[],"tags":["temp"],"content":"Staged Training\n\nA tactic of training a model in a sequence of discrete stages. The goal can be either to speed up the training process, or to achieve better model quality.\n"},"KB/Standard-Deviation":{"title":"Standard Deviation","links":["KB/Normal-Distribution"],"tags":["temp"],"content":"\ntoc: true\ntitle: Standard Deviation\ntags: [‘temp’]\n\nStandard Deviation\n\nmeasure of dispersement\nhow much your data is spread out around the mean\nNormal Distribution\nstd = \\sqrt{\\frac{\\Sigma(x-\\bar x)^{2}}{n-1}}\n"},"KB/Stanford-Dogs":{"title":"Stanford Dogs","links":[],"tags":["dataset"],"content":"Stanford Dogs\n\n@khoslaNovelDatasetFineGrained\nThis dataset contains images of 120 breeds of dogs, with a total of 20,580 images.\n"},"KB/StarGAN-v2":{"title":"StarGAN v2","links":[],"tags":["architecture"],"content":"StarGAN V2\n\ngenerate diverse images across multiple domains\ndefine the domain and style of images as visually distinct category groups and the specific appearance of each image, respectively\n"},"KB/StarGAN":{"title":"StarGAN","links":[],"tags":["architecture"],"content":"StarGAN\n\nimprove the scalability and robustness in handling more than two domains\nbuilds only one model to perform image-to-image translation among multiply domains\nIn the generation phase, we just need to provide the generator with the source image and an attribute label which indicates the target domain\ntakes the domain label as an additional input and learns a deterministic mapping per each domain, which may result in the same output per each domain given an input image\n"},"KB/Static-Friction":{"title":"Static Friction","links":["KB/Force"],"tags":["physics"],"content":"Static Friction\n\nF_{s} \\leq \\mu _{s}F_{N}\nF_{s} is static friction\nF_{N} is normal Force\n\\mu _s is coefficient of friction\n"},"KB/Static-Graph-Execution":{"title":"Static Graph Execution","links":["KB/Computational-Graph"],"tags":["temp"],"content":"Static Graph Execution\n\nComputational Graph is first built then is executed\nless readable code and more difficult to debug\nbetter performance\nonce compiled the graph architecture is static\n"},"KB/Stationarity":{"title":"Stationarity","links":[],"tags":["temp"],"content":"Stationarity\n\nA property of data in a dataset, in which the data distribution stays constant across one or more dimensions. Most commonly, that dimension is time, meaning that data exhibiting stationarity doesn’t change over time. For example, data that exhibits stationarity doesn’t change from September to December.\n"},"KB/Statistical-Word-Segmentation":{"title":"Statistical Word Segmentation","links":["KB/Word-Segmentation"],"tags":["language"],"content":"Statistical Word Segmentation\n\nuse mutual information between characters from a corpus\n"},"KB/Stem-Cells":{"title":"Stem Cells","links":[],"tags":["brain"],"content":"Stem Cells\n\nCells that have the potential to differentiate, to develop into many different specific cell types with specialized functions.\n"},"KB/Steve":{"title":"Steve","links":["KB/Learning-Event","KB/Knowledge-Component"],"tags":["usermodel"],"content":"Steve\n\nOuter loop: Steve is a tutoring system that teaches hierarchical, multi-step procedures, such as how to start a large air compressor\ntake Inner loop: Students steps by manipulating graphical widgets, such as clicking on a valve icon to open it or on a dipstick to check the oil level\nSteve can give immediate feedback\nSteve can also execute a step for the student. In fact, it can demonstrate the whole procedure for a student, explaining each step as it goes.\nStep analysis: Steve interprets the student’s step by matching it to a set of anticipated steps. In particular, after each student step, Steve computes all possible correct next steps (usually there is just one).\nNotice that there is a one-to-one relationship between the step, the Learning Event and the Knowledge Component. This is a major contributor to the simplicity of Steve’s analysis.\nStep generation: The student’s most recent correct step is, by definition, part of the procedure being taught\nSteve uses immediate feedback to block incorrect steps, so Steve always knows where in the procedure the student is.\nWhen it needs to give hints on what to do next, it merely picks the next step. If there are multiple steps that could follow the student’s most recent correct step, then Steve lists them and lets the student choose.\n"},"KB/Stochastic-ensemble-learning":{"title":"Stoch Ensemble Learning","links":[],"tags":["temp"],"content":"Stoch Ensemble Learning\n\nStochastic algo repeatedly executed with random seeds\nStronger the randomness → more members included → stronger reg\n\nMaybe related to\n\n[Ensemble Distillation](Ensemble Distillation.md)\n[Ensemble of Shape Functions](Ensemble of Shape Functions.md)\n"},"KB/Stratified-Random-Sampling":{"title":"Stratified Random Sampling","links":["Sampling"],"tags":["distributions"],"content":"Stratified Random Sampling\n\nDivides the population into relatively homogeneous groups (strata) and samples each stratum at random\n"},"KB/Stream-Ribbons":{"title":"Stream Ribbons","links":[],"tags":["visualization"],"content":"Stream Ribbons\n\n\n"},"KB/Stream-Surfaces":{"title":"Stream Surfaces","links":[],"tags":["visualization"],"content":"Stream Surfaces\n\n\n"},"KB/Streamline-Stopping-Criterion":{"title":"Streamline Stopping Criterion","links":[],"tags":["visualization"],"content":"Streamline Stopping Criterion\n\ndistance to neighboring streamline too small\nstreamline leaves domain\nafter maximum number of integration steps\n"},"KB/Streamlines":{"title":"Streamlines","links":["KB/Streamline-Stopping-Criterion"],"tags":["visualization"],"content":"Streamlines\n\n\n\nStreamline Stopping Criterion\n"},"KB/Striatum":{"title":"Striatum","links":["KB/Nucleus-Accumbens"],"tags":["brain"],"content":"Striatum\n\nA small group of subcortical structures, including the caudate nucleus, putamen, and Nucleus Accumbens, located in the midbrain. These regions are implicated in both movement and reward-related behaviors\n"},"KB/Strided-Attention":{"title":"Strided Attention","links":["KB/Strided","KB/Attention"],"tags":["architecture"],"content":"Strided Attention\n\npaper\nSparse factorizations of the Attention matrix\nReduce to O(n\\sqrt{n})\nRecompute Attention matrices to save memory\nFast Attention kernels\nWorks nicely for images, music etc with a periodic structure\nOtherwise with the Strided pattern , the spatial coordinates do not correlate with the positions the elements might be more relevant in the future\n\n"},"KB/Strided":{"title":"Strided","links":["KB/Downsampling","KB/Conv"],"tags":["architecture"],"content":"Strided\n\n\nNormally S = 1\nS&gt;1 → Downsampling\nDilated\nSpaces in the filter kernel\nD = 1 : normal Conv aka D-1 spaces\nEffective Filter size : \\hat F = F + (F-1)(D-1)\n\n"},"KB/Strip-Mining":{"title":"Strip Mining","links":[],"tags":["parallelcomputing"],"content":"Strip Mining\n\nGenerates code to allow vector operands whose size is less than or greater than size of vector registers\n"},"KB/Stroke":{"title":"Stroke","links":[],"tags":["brain"],"content":"Stroke\n\nA neurological event that occurs when the blood supply to the brain is blocked, depriving the tissue of oxygen, or when there is a bleed into the brain due to the rupturing of an artery.\n"},"KB/Stroop-Task":{"title":"Stroop Task","links":[],"tags":["temp"],"content":"\ntoc: true\ntitle: Stroop Task\ntags: [‘temp’]\n\nStroop Task\n\nVerbal\n\nThe Stroop phenomenon demonstrates that it is difficult to name the ink color of a color word if there is a mismatch between ink color and word. For example, the word GREEN printed in red ink\n\n\nNon Verbal\n\nA series of white arrows pointing either left or right was displayed against a black background either on the left or right side of a centered fixation cross. Half of the stimuli were pointing in the same direction as their position on the screen\n\n\n"},"KB/Structural-Risk-Minimization":{"title":"Structural Risk Minimization","links":["KB/Regularization"],"tags":["temp"],"content":"Structural Risk Minimization\n\nAn algorithm that balances two goals\n\nThe desire to build the most predictive model (for example, lowest loss).\nThe desire to keep the model as simple as possible (for example, strong Regularization).\nor example, a function that minimizes loss+Regularization on the training set is a structural risk minimization algorithm.\n\n\n"},"KB/Structural-Similarity-Index":{"title":"Structural Similarity Index","links":["KB/Covariance"],"tags":["explainability"],"content":"Structural Similarity Index\n\nmethod for predicting the perceived quality of digital television and cinematic pictures, as well as other kinds of digital images and videos. SSIM is used for measuring the similarity between two images\n\\hbox{SSIM}(x,y) = \\frac{(2\\mu_x\\mu_y + c_1)(2\\sigma_{xy} + c_2)}{(\\mu_x^2 + \\mu_y^2 + c_1)(\\sigma_x^2 + \\sigma_y^2 + c_2)}\n\\mu_x the pixel sample mean of x;\n\\mu_y the pixel sample mean of y;\n\\sigma_x^2 the variance of x;\n\\sigma_y^2 the variance of y;\n\\sigma_{xy} the Covariance of x and y;\nc_1 = (k_1L)^2, c_2 = (k_2L)^2 two variables to stabilize the division with weak denominator;\nL the dynamic range of the pixel-values (typically this is 2^{\\#bits\\ per\\ pixel}-1);\nk_1 = 0.01 and k_2 = 0.03 by default.\n"},"KB/Structure-Based-Pruning":{"title":"Structure Based Pruning","links":["KB/Pruning"],"tags":["regularization"],"content":"Structure Based Pruning\n\nRegarding structural choices, some authors choose to prune individual parameters which produces a sparse network (lots of 0s). This might not be very ideal for storing efficiently.\nSome others consider methods where they group certain parameters and remove them as groups. This is more optimized.\n"},"KB/Structured-Update":{"title":"Structured Update","links":["KB/Sparsity"],"tags":["temp"],"content":"Structured Update\n\nDirectly learn an update from a restricted space that can be parametrized using a smaller number of variables.\nWe train directly the updates of this structure\nRandom mask. We restrict the update Hit to be a sparse matrix, following a pre-defined random Sparsity pattern\n"},"KB/Style-GAN":{"title":"Style GAN","links":["KB/Layers"],"tags":["architecture"],"content":"Style GAN\n\nbuilds the picture layer after layer, where the Layers get bigger and more accurate\nFor example, the first layer is 4 by 4 pixels, the second 8 by 8, and so on\nevery new layer can benefit from the less granular results of the previous ones\nbetter separate the generator and the discriminator, which ensures less dependence of the generator on the training set\nhis allows one to, for example, reduce discrimination in the generated pictures\n"},"KB/Subgenual-Cortex":{"title":"Subgenual Cortex","links":[],"tags":["brain"],"content":"Subgenual Cortex\n\nThe region in the back of the frontal lobes, found below the corpus callosum, which has been implicated in mood states.\n"},"KB/Subject-relative":{"title":"Subject relative","links":[],"tags":["language"],"content":"Subject Relative\n\nThe judge that ignored the doctor watched the movie about Colombian drug dealers\nObject relative The judge that the doctor ignored watched the movie about Colombian drug dealers.\nSubjects with higher working memory were significantly better at interpreting object relatives than subjects with lower working memory\n"},"KB/Subject-verb-agreement":{"title":"Subject-verb agreement","links":["KB/Verb"],"tags":["language"],"content":"Subject-Verb Agreement\n\nJohn almost always reads the course papers before the lecture. I almost always forget to buy cat food.\nRelations with inflectional morphemes\nLa femme est belle. L’homme est beau.\n"},"KB/Substantia-Nigra":{"title":"Substantia Nigra","links":["KB/Parkinson’s-Disease"],"tags":["brain"],"content":"Substantia Nigra\n\nThis small region in the midbrain is part of the brain’s reward system. In Parkinson’s Disease, the dopamine neurons in this region die off, leading to the disorder’s movement-related and cognitive symptoms.\n"},"KB/Subthalamic-Nucleus":{"title":"Subthalamic Nucleus","links":["KB/Basal-Ganglia","KB/Parkinson’s-Disease"],"tags":["brain"],"content":"Subthalamic Nucleus\n\nA small brain structure, located in the Basal Ganglia, that plays an important role in coordinating movement. It is the most common target for neuromodulation techniques, like [deep brain stimulation](deep brain stimulation.md), to help diminish the symptoms of Parkinson’s Disease.\n"},"KB/Sufficiency":{"title":"Sufficiency","links":[],"tags":["explainability"],"content":"Sufficiency\n\nthe Positive Predictive Value is the same for all subgroups within the sensitive feature. This criteria is also known as Predictive Rate Parity.\n"},"KB/Suffix":{"title":"Suffix","links":[],"tags":["language"],"content":"Suffix\n\nfollow the stem: eat / eats\n"},"KB/Sugar-Factory-Task":{"title":"Sugar Factory Task","links":[],"tags":["cognitivemodel"],"content":"Sugar Factory Task\n\nBerry and Broadbent ; Wallach\nSet no of workers per day\nP_{t}=2W_{t}-P_{t-1}+ RandomFactor(-1/0/1)\n\nP: Production value\nW : No of workers\n\n\n"},"KB/Sulcus":{"title":"Sulcus","links":["KB/Cerebrum"],"tags":["brain"],"content":"Sulcus\n\nA shallower groove on the brain’s Cerebrum (deeper grooves are called fissures).\n"},"KB/Summit":{"title":"Summit","links":[],"tags":["explainability"],"content":"Summit\n\n@hohmanSummitScalingDeep2019\ncombines two scalable tools: (1) activation aggregation discovers important neurons; (2) neuron-influenced aggregation identifies relationships among such neurons. An attribution graph that reveals and summarizes crucial neuron associations and substructures that contribute to a model’s outcomes is created. Summit combines famous methods such as computing synthetic prototypes of features and showing examples from the dataset that maximize special neurons of different layers. Deeper in the graph, it is examined how the low-level features combine to create high-level features. Novel as well is that it exploits neural networks with activation atlases [63]\nThis method uses feature inversion to visualize millions of activations from an image classification network to create an explorable activation atlas of features the network has learned. Their approach is able to reveal visual abstractions within a model and even high-level misunderstandings in a model that can be exploited. Activation atlases are a novel way to peer into convolutional vision networks and represents a global, hierarchical, and human-interpretable overview of concepts within the hidden layers.\nTo quantify how much a layer influences the next, the authors aggregate the influences by creating a tensor I^{l} for all the layers of the network (l). How important channel i of the layer l-1 is determined by the aggregate tensor I^{l}_{cij} where j represents the output channel and c is the class of the image. Considering the j^{th} kernel of the layer K^{(j)} \\in \\mathbb{R}^{H \\times W \\times C_{l-1}}, a single channel Y can be represented using the 3D convolution operation by Y_{:,:,j}= X \\ast K^{(j)}. This is equivalent to it’s representation by the 2D convolution Y_{:,:,j}= \\Sigma_{i=1}^{C_{l-1}} X_{:,:,i} \\ast K^{(j)}_{:,:,i}.\nX_{:,:,i} \\ast K^{(j)}_{:,:,i} is the contribution of the current channel from the previous layer and the maximum of this value is used to generate the influence map.\n"},"KB/Super-Resolution":{"title":"Super Resolution","links":[],"tags":["applicationeinsum"],"content":"Super Resolution\ndef SuperResolutionNetNew(upscale_factor):\n    return nn.Sequential(\n        nn.Conv2d(1, 64, kernel_size=5, padding=2),\n        nn.ReLU(inplace=True),\n        nn.Conv2d(64, 64, kernel_size=3, padding=1),\n        nn.ReLU(inplace=True),\n        nn.Conv2d(64, 32, kernel_size=3, padding=1),\n        nn.ReLU(inplace=True),\n        nn.Conv2d(32, upscale_factor ** 2, kernel_size=3, padding=1),\n        Rearrange(&#039;b (h2 w2) h w -&gt; b (h h2) (w w2)&#039;, h2=upscale_factor, w2=upscale_factor),\n    )"},"KB/SuperQuadrics":{"title":"SuperQuadrics","links":[],"tags":["visualization"],"content":"SuperQuadrics\n\n\n"},"KB/SuperScalar":{"title":"SuperScalar","links":["KB/Instruction-level-programming"],"tags":["parallelcomputing"],"content":"SuperScalar\n\nA superscalar CPU architecture implements Instruction level programming inside a single processor which allows faster CPU throughput at the same clock rate.\nA superscalar processor executes more than one instruction during a clock cycle\nSimultaneously dispatches multiple instructions to multiple redundant functional units built inside the processor.\n"},"KB/Superposition-Catastrophe":{"title":"Superposition Catastrophe","links":["KB/Recurrent"],"tags":["language"],"content":"Superposition Catastrophe\n\nBowers et al. (2014)\nCommon claim of connnectionist models: they learn the best representations for a given task\nLearned representations are emergent, not stipulated\nIf a PDP model learns localist codes when coding for multiple things at the same time, strongly suggests that the superposition problem pressures models to learn selective (e.g. localist) coding.\nRecurrent network\nSimple task, given vocabulary of 30 words\nBanding/selective responses do not appear with distributed letter coding when chance of ambiguity is null\nThis means: when ambiguity/superposition catastrophe is very possible, hidden units learn selective responses\nSelective responses ‘emerge’ as a response to the potential for superposition catastrophe\nRecurrent networks trained to store multiple things at the same time over the same set of units learn highly selective (localist) representations\n"},"KB/Supervised-Learning-Formulation":{"title":"Supervised Learning Formulation","links":[],"tags":["semisupervisedlearning"],"content":"Supervised Learning Formulation\n\ngiven a dataset X, for each data Xi in X, there is a corresponding human-annotated label Yi\nFor a set of N labeled training data D = {Xi}Ni=0, the training loss function is defined as\ndata collection and annotation usually are expensive and may require special skills\n"},"KB/Suppletion":{"title":"Suppletion","links":[],"tags":["language"],"content":"Suppletion\n\nword is completely replaced by something that has no connection at the surface label\nGo to went.\nGood to better\n"},"KB/Swichboard":{"title":"Swichboard","links":[],"tags":["dataset"],"content":"Swichboard"},"KB/Swin-Transformer":{"title":"Swin Transformer","links":["KB/Transformer","KB/Vision-Transformer","KB/Self-Attention","KB/Attention","KB/ImageNet","KB/COCO","KB/ADE20K","KB/Perception","Architectures"],"tags":["temp"],"content":"\ntoc: true\ntitle: Swin Transformer\ntags: [‘temp’]\n\nSwin Transformer\n\nSwin Transformer: Hierarchical Vision Transformer Using Shifted Windows\n\nVision Transformer\ngeneral-purpose backbone for computer vision\nhierarchical feature representation\nlinear computational complexity with respect to input image size\nshifted window based Self Attention\naddress the challenges in adapting Transformer from language to vision\nlimiting self-Attention computation to non-overlapping local windows while also allowing for cross-window connection\nflexibility to model at various scales\nlinear computational complexity with respect to image size\nImageNet\nCOCO\nADE20K\nThe hierarchical design and the shifted window approach also prove beneficial for all Perception Architectures.\nRatio of 1:1:3:1\n\n\n"},"KB/Swish":{"title":"Swish","links":["Vanishingexploding-gradients","KB/Layers","KB/Lisht","KB/Sparsity","KB/Relu","Tag-Pages/loss","KB/Smoothness"],"tags":["architecture"],"content":"Swish\n\nx\\cdot sigmoid(x)\nWhile Swish reportedly improves model performance (Ramachandran et al., 2017), it still does not allow you to avoid Vanishingexploding gradients\nEven though the vanishing gradients problem is much less severe in case of Swish, only inputs of x &gt;= 2 result in gradients of 1 and (sometimes) higher. In any other case, the gradient will still cause the chain to get smaller with increasing Layers.\nMove to Lisht\nnon monotonic\nFirst, it is bounded below. Swish therefore benefits from Sparsity similar to Relu. Very negative weights are simply zeroed out.\nSecond, it is unbounded above. This means that for very large values, the outputs do not saturate to the maximum value (i.e., to 1 for all the neurons). According to the authors of the Swish paper, this is what set Relu apart from the more traditional activation functions.\nThird, separating Swish from Relu, the fact that it is a smooth curve means that its output landscape will be smooth. This provides benefits when optimizing the model in terms of convergence towards the minimum loss.\nFourth, small negative values are zeroed out in Relu (since f(x) = 0 for x &lt; 0). However, those negative values may still be relevant for capturing patterns underlying the data, whereas large negative values may be zeroed out (for reasons of Sparsity, as we saw above). The Smoothness property and the values of f(x) &lt; 0 for x ≈ 0 yield this benefit. This is a clear win over Relu.\n\n"},"KB/Symbolic-learning-model":{"title":"Symbolic learning model","links":["KB/Verb"],"tags":["language"],"content":"Symbolic Learning Model\n\nVerb tokens are used instead of Verb types.\nNo sharp discontinuities in the supply of regular and irregular Verb tokens in ‘parental speech’\nVerb tokens sampled randomly with replacement according to the Francis-Kucera frequency estimates for English verbs\n"},"KB/Symbolic-models":{"title":"Symbolic models","links":[],"tags":["language"],"content":"Symbolic Models\n\nCode letters separately from order\nThis type of model does make transposition errors ◮ Can because B is a ‘thing’ on its own, separate from order\nBotvinick, M. M., &amp; Plaut, D. C. (2006). Short-term memory for serial order: a recurrent neural network model. Psychological review, 113(2), 201.\nModel could later correctly reproduce novel sequences\nBut all letters had occurred in each position\n"},"KB/Symmetries-Node-Link":{"title":"Symmetries Node Link","links":[],"tags":["visualization","graph"],"content":"Symmetries Node Link\n\n\n"},"KB/Sympathetic":{"title":"Sympathetic","links":[],"tags":["brain"],"content":"Sympathetic\n\nMobilizes body into action\n"},"KB/Synaptic-Pruning":{"title":"Synaptic Pruning","links":["KB/Pruning","KB/Microglia"],"tags":["brain"],"content":"Synaptic Pruning\n\nA process by which specialized cells called Microglia eliminate unnecessary synapses as part of normal and healthy brain development.\n"},"KB/Synaptic-Transmission":{"title":"Synaptic Transmission","links":[],"tags":["brain"],"content":"Synaptic Transmission\n\nThe process of nerve-to-nerve communication in the central nervous system, whereby one neuron sends a chemical signal across the synaptic cleft to another neuron.\n"},"KB/Syntactic-Ambiguity":{"title":"Syntactic Ambiguity","links":[],"tags":["language"],"content":"Syntactic Ambiguity\n\nFlying plane is very dangerous\nThe man saw a boy with binoculars\nThe teacher wears sunglasses\nBecase students are bright\n"},"KB/Syntactic-Analysis":{"title":"Syntactic Analysis","links":["KB/Lexicon","KB/Context-Free-Grammar"],"tags":["language"],"content":"Syntactic Analysis\n\nconcerned with the construction of sentences.\nSyntactic structure indicates how the words are related to each other\nSyntax tree is assigned by a grammer and a Lexicon\nContext Free Grammar\n"},"KB/Syntactic-Bootstrapping":{"title":"Syntactic Bootstrapping","links":[],"tags":["language"],"content":"Syntactic Bootstrapping\n\n“John wugged Mary yesterday” vs “John wugged Marry”\n"},"KB/Syntax-First-models":{"title":"Syntax First models","links":[],"tags":["language"],"content":"Syntax First Models\n\nSyntax-first models (e.g., Ferreira &amp; Clifton, 1986; Frazier &amp; Clifton, 1996) have traditionally proposed that, at a point of syntactic ambiguity, syntactic heuristics alone select a single structure to pursue\nrecovery from a misanalysis is achieved via a separate reanalysis mechanism that uses semantic and contextual information\npropose that only one representation is active at any given time and that nonsyntactic information only influences interpretation at a later reanalysis stage.\n"},"KB/Sørensen-Dice-Index":{"title":"Sørensen-Dice Index","links":[],"tags":["loss"],"content":"Sørensen-Dice Index\n\nvery similar to Jaccard index\nAlthough they are calculated similarly the Sørensen-Dice index is a bit more intuitive because it can be seen as the percentage of overlap between two sets, which is a value between 0 and 1\noverstate the importance of sets with little to no ground truth positive sets\nAs a result, it could dominate the average score taken over multiple sets\nIt weights each item inversely proportionally to the size of the relevant set rather than treating them equally.\n$$D(x,y) = \\frac{2|x\\cap y|}{|x|+|y|}$\n\n"},"KB/T.-B.-Macaulay":{"title":"T. B. Macaulay","links":[],"tags":["indianhistory"],"content":"T. B. Macaulay\n\nMacaulay came from a deeply religious Protestant family, so his motivation was to convert numerous Hindus to Christianity, which he thought would also help the administrative problems that the English were facing\nHis plan was to create an educational system that would make an educated elite that would naturally be English by their own choice, and thus give up their own Hindu traditions\nThen they would also work and cooperate more efficiently with the English administration.\nfacilitate the British colonialism by making Indians, especially the Brahmanas, collaborators loyal to their new masters\n“Our English schools are flourishing wonderfully. The effect of this education on the Hindus is prodigious… It is my belief that if our plans of education are followed up, there will not be a single idolater among the respectable classes in Bengal thirty years hence. And this will be effected without any efforts to proselytize, without the smallest interference with religious liberty, by natural operation of knowledge and reflection. I heartily rejoice in the project.”\n“It is, I believe, no exaggeration to say, that all the historical information which has been collected from all the books written in Sanskrit language is less valuable than what may be found in the most paltry abridgements used at preparatory schools in England. In every branch of physical or moral philosophy, the relative position of the two nations is nearly the same.”\n“In one point I fully agree with the gentlemen to whose general views I am opposed. I feel with them, that it is impossible for us, with our limited means, to attempt to educate the body of the people. We must at present do our best to form a class who may be interpreters between us and the millions whom we govern; a class of persons, Indian in blood and colour, but English in taste, in opinions, in morals, and in intellect. To that class we may leave it to refine the vernacular dialects of the country, to enrich those dialects with terms of science borrowed from the Western nomenclature, and to render them by degrees fit vehicles for conveying knowledge to the great mass of the population.”\nThis is in reference that Macaulay figured that the British would not be able to educate all Indians in the ways of the British, but could indeed create a class of them who would be English in taste, opinions, morals and intellect, and then engage them in influencing the rest of India, thus helping the English in their job of overseeing the rest of India, and, of course, converting them.\n“Through the whole Hindoo Pantheon you will look in vain for anything resembling those beautiful and majestic forms which stood in the shrines of ancient Greece. All is hideous, and grotesque, and ignoble. As this superstition is of all superstitions the most inelegant, so is it of all superstitions the most immoral. Emblems of vice are objects of public worship. Acts of vice are acts of public worship. The courtesans are as much a part of the establishment of the temple, as much ministers of the god, as the priests. Crimes against life, crimes against property, are not only permitted but enjoined by this odious theology.”\n"},"KB/TAILOR-conference-lisbon-'24":{"title":"TAILOR conference lisbon '24","links":["KB/ChatGPT"],"tags":["conference","openml","deeplearning"],"content":"TAILOR Conference Lisbon ‘24\n \nNotes from Notebook\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOCR Notes - processed with ChatGPT\n\nWendy Ju\n\nRobots should have good manners.\nEngagement.\nPeople treat robots like children.\n\n\nCarles Sierra\n\nPeople are nervous about AI.\nPsychopaths\nHow is this related to AI?\nAdapt technology as a community.\nSustainable collective.\nGoverning the commons.\nL’Horta watering communities.\nGroups talking about technology.\nOstrom’s criteria.\nCommunity arms.\n\n\n“I must be…”\n\nValve\nGoals\nTailor thing\n\n\nTurns out Isabelle is part of this, but they ran out of funds so she couldn’t come. XD\n\nLogistics\nBreakout Room 5\n“Why!”\n“I think it’s cool to assign teams &amp; make people uncomfortable.”\n“I am a leader!! (ugh.)”\nShort presentation\nNeuro\nUncertainty in humans:\n\nState of mind\nSemantic\n\n\nUncertainty in life is short.\nReasoning shortcuts in NS.\n\nDeterministic vs probabilistic\nConcept rehearsal\n\n\nFeeling weird as a fellow lab member.\nInternal vs External symbolic neuro\nSymbolic AI\n\nPointing → Saliency\nWording → RLHF\nApproximate retrieval (VSRAGIL)\nHe just destroyed LIM prompts.\nUsing the internet as training\n\n\nLIMs → Good at common sense.\nApproximate omniscience\nLIMs are masters of style.\n\n\nShe hasn’t stopped for one second.\n\nKL also now available.\nWTF did she actually use, and how is she keeping track of outages?\n\n\nGlioblastoma → Dead.\n\nLow data\nGenetic expressions\nMultimodal classification\nPatching, how?\nPankaj Pandey → FNFIELD\nStarted in Norway\nAdaptive, green, trustworthy\nDislike (hate) stable diffusion\n\n\nExplainable Malware Detection\n\nOntologies as explanation?\nConcept learning, but what model?\nThey published a lot though.\n\n\nTrustworthy robotic teammates\n\nHumans seem to trust robots less initially but more over time compared to other humans.\nWhat forms the expectation? Set low expectations.\n\n\nCollaborative Human-AI\n\nTrusting black-box models\nStarted with the theory of mind, now game theory\nAI is the new steam engine?\nTranspilation 2. (Quantum)\n\n\nFederalized NSGA-II\n\nSoft unification\nClass distribution bias\nSpectrogram → Original biological component.\nPerturbation → Robustness distribution.\nWhat kind of perturbations?\nHow distribution?\nSafety requirements.\nOperation\n\n\nStandardization &amp; legal means.\n\nFundamental research → End user?\nB2B vs B2C\nCompanies don’t care about research.\nAcademia doesn’t care about the product.\nAI: No time to wait for results, collaborative research.\nTransfer Lab = Industry + Academia.\nAI combines the chain, but customers own the data, not us.\nTrust the platform.\nEuropean data space.\nBusiness model → Safety\nStandardization &amp; funding\nB2B\n\n\nPeople are skeptical of their own abilities.\n\nThe more you know, the less you trust.\nReplica AI → Social AI?\nTailor individual collaboration.\nCertification for AI?\nPolitics\nMethods for trusted AI?\nEvaluation tools.\nNeural explicit models.\nValves → Code: Schwartz theory\nGoals are proxies of values.\nOptimize for fairness.\nPrivacy vs Trust.\nChoices\nDifferences in values across cultures.\n\n\n"},"KB/TIghtly-coupled":{"title":"TIghtly coupled","links":["KB/UMA","KB/NUMA"],"tags":["parallelcomputing"],"content":"TIghtly Coupled\n\nUMA\nNUMA\n"},"KB/TIme-Series":{"title":"Time Series Prediction","links":["HMM","KB/Recurrent"],"tags":["temp"],"content":"Time Series Prediction\n\nTask:\n\ndiscrete time, equidistant timesteps\nApproximate teacher signal\nInput signal : u(t)_{t \\in \\mathcal{T}}\nDesired output : (y(t))_{t \\in \\mathcal{T}} = u(t+h))_{t \\in \\mathcal{T}}\nDynamical system state : x(t) \\in \\mathbb{R}^n\nTemporal evolution governed by\n\nState update map\n\nGoverns how x(t) develops over time\nx(t+1) = f(x(t) , u(t+1))\n\n\nObservation function\n\nWhat output can be observed when in state x(t)\ny(t) = g(x(t))\n\n\n\n\n\n\nShort range\n\nPDE , ODE\n\n\nLong range\n\nHMM\nRecurrent\n\n\n"},"KB/TO-LOOK-AT":{"title":"To Look at","links":[],"tags":["temp"],"content":"To Look at\n\nkarpathy.github.io/2019/04/25/recipe/\narxiv.org/abs/1311.2901\ntorch.ch/blog/2015/09/07/spatial_transformers.html\ndpmd.ai/Ithaca-blog\nnotesonai.com/Layer+Normalization\n"},"KB/TPU-Node":{"title":"TPU Node","links":[],"tags":["temp"],"content":"TPU Node\n\nA TPU resource on Google Cloud Platform with a specific TPU type.\n"},"KB/TPU-Pod":{"title":"TPU Pod","links":[],"tags":["temp"],"content":"TPU Pod\n\nA specific configuration of TPU devices in a Google data center.\n"},"KB/TPU-Slice":{"title":"TPU Slice","links":["KB/TPU-Pod"],"tags":["temp"],"content":"TPU Slice\n\nA TPU slice is a fractional portion of the TPU devices in a TPU Pod\n"},"KB/TREEQN":{"title":"TREEQN","links":[],"tags":["einsum"],"content":"TREEQN\ndef transition(zl):\n  # -- [batch_size x num_actions x hidden_dimension]\n  return zl.unsqueeze(1) + F.tanh(torch.einsum(&quot;bk,aki-&gt;bai&quot;, [zl, W]) + b)"},"KB/TREPAN":{"title":"TREPAN","links":["KB/Trees"],"tags":["explainability"],"content":"TREPAN\n\n\nor DeepRED\nTREPAN is an algorithm for extracting comprehensible, symbolic representations from trained neural networks\nThe authors demonstrated that TREPAN is able to produce Decision Trees that are accurate and comprehensible and maintain a high level of fidelity to the networks from which they were extracted.\nAccording to the authors of DeepRED, their method is the first attempt to extract rules and make a DNN’s decision more transparent.\n"},"KB/TSDF":{"title":"TSDF","links":[],"tags":["robotics"],"content":"TSDF\n\nTruncated Signed Distance Function\n"},"KB/Tacotron":{"title":"Tacotron","links":["KB/GRU"],"tags":["architecture"],"content":"Tacotron\n\nCBHG\n\n[Conv](Gated Recurrent Unit (GRU|Gated Recurrent Unit (GRU|GRU](Conv](Gated Recurrent Unit (GRU|Gated Recurrent Unit (GRU|GRU.md).md)\n\n\n\nclass CBHG_Old(nn.Module):\n    &quot;&quot;&quot;CBHG module: a [recurrent](Recurrent.md) neural network composed of:\n        - 1-d convolution banks\n        - Highway networks + residual connections\n        - Bidirectional gated [recurrent](Recurrent.md) units\n    &quot;&quot;&quot;\n \n    def __init__(self, in_dim, K=16, projections=[128, 128]):\n        super(CBHG, self).__init__()\n        self.in_dim = in_dim\n        self.relu = nn.ReLU()\n        self.conv1d_banks = nn.ModuleList(\n            [BatchNormConv1d(in_dim, in_dim, kernel_size=k, stride=1,\n                             padding=k // 2, activation=self.relu)\n             for k in range(1, K + 1)])\n        self.max_pool1d = nn.MaxPool1d(kernel_size=2, stride=1, padding=1)\n \n        in_sizes = [K * in_dim] + projections[:-1]\n        activations = [self.relu] * (len(projections) - 1) + [None]\n        self.conv1d_projections = nn.ModuleList(\n            [BatchNormConv1d(in_size, out_size, kernel_size=3, stride=1,\n                             padding=1, activation=ac)\n             for (in_size, out_size, ac) in zip(\n                 in_sizes, projections, activations)])\n \n        self.pre_highway = nn.Linear(projections[-1], in_dim, bias=False)\n        self.highways = nn.ModuleList(\n            [Highway(in_dim, in_dim) for _ in range(4)])\n \n        self.gru = nn.GRU(\n            in_dim, in_dim, 1, batch_first=True, bidirectional=True)\n \ndef forward_new(self, inputs, input_lengths=None):\n    x = rearrange(inputs, &#039;b t c -&gt; b c t&#039;)\n    _, _, T = x.shape\n    # Concat conv1d bank outputs\n    x = rearrange([conv1d(x)[:, :, :T] for conv1d in self.conv1d_banks], \n                 &#039;bank b c t -&gt; b (bank c) t&#039;, c=self.in_dim)\n    x = self.max_pool1d(x)[:, :, :T]\n \n    for conv1d in self.conv1d_projections:\n        x = conv1d(x)\n    x = rearrange(x, &#039;b c t -&gt; b t c&#039;)\n    if x.size(-1) != self.in_dim:\n        x = self.pre_highway(x)\n \n    # Residual connection\n    x += inputs\n    for highway in self.highways:\n        x = highway(x)\n \n    # (B, T_in, in_dim*2)\n    outputs, _ = self.gru(self.highways(x))\n \n    return outputs"},"KB/Tainted-data":{"title":"Tainted data","links":[],"tags":["explainability"],"content":"Tainted data\n\nerrors in the data modelling definition, wrong feature labelling, and other possible causes.\n"},"KB/Taking-on-semantic-commitments,-II-collective-versus-distributive-readings":{"title":"Taking on semantic commitments, II collective versus distributive readings","links":["KB/Minimal-Semantic-Commitment"],"tags":[],"content":"Taking on Semantic Commitments, II Collective Versus Distributive Readings\n\nLyn Frazier, Jeremy M. Pacht, Keith Raynerb\n\n \n\nMinimal Semantic Commitment\nGiven ambiguous representations, the MSC hypothesis predicts that the processor will commit to one interpretation\nIn an experiment designed to evaluate these hypotheses with respect to the representation of distributivity, participants’ eye movements were recorded as they read sentences containing distributive or collective predicates that were either disambiguated by a preceding adverb or left locally ambiguous by delaying the disambiguating adverb until the end of the predicate\nThe results suggested that a semantic commitment is made in locally indeterminate cases as evidenced by a significant interaction of ambiguity and distributivity in first pass times, total times, and regressions\nHence we argue that the distributive/collective distinction is treated as a matter of ambiguity rather than as one of vagueness\nIn the absence of evidence for a distributive reading, the processor commits itself to a collective reading sometime during the processing of the predicate (before the disambiguation in our late disambiguation examples)\nBy the MSC hypothesis, this will predict that a premature decision, one made early on the basis of little information, should not occur during immediate processing\nThe point is that no commitment will be made in the absence of supporting evidence\nBy contrast, given the MSC hypothesis, the notion that a string is grammatically ambiguous predicts that the processor encounters a choice point on initial analysis, adopting one representation rather than another\nIn our experiment we only examined collective/distributive subjects in single clause sentences\nThe ambiguity hypothesis predicts an interaction\nthe vagueness hypothesis doesn’t\nT\nIn order to test for possible pragmatic biases in the predicate, participants were asked to rate the ‘naturalness’ of the locally ambiguous collective or distributive predicates contained in the experimental sentences\nwhen the actual conjoined NP subject in the experimental materials was replaced by a pronominal subject\nThirty-two students native English speakers\nEach version of the questionnaire contained eight sentences in which the locally ambiguous predicate was subsequently disambiguated towards a distributive interpretation using each and eight sentences in which the predicate was subsequently disambiguated towards a collective interpretation using together\n7-point scale\nThey were told that there were no right or wrong answers and that we were only interested in their opinions of the naturalness of the sentences\nanother rating study was conducted using truncated versions of the locally ambiguous items such that each item ended after the word each or together\nexamined first pass reading times, total reading times, and the pattern of regressions for different regions of the target sentences\nSixty undergraduate students bite bar\n\nExperiment\n\ndealt with where readers look during reading and that they should read each sentence for comprehension\nSixteen experimental sentences were embedded in 107 filler sentences\nEach sentence appeared in one of four versions, as in (7) above: in two versions (a and c), the predicate received a distributive interpretation, and in the other two (b and d) the predicate received a collective interpretation\nregion\nhe first region consisted of the words preceding the predicate (a conjoined NP)\nthe second region consisted of the predicate itself\nthe third region comprised the next three words, or next two words if the third word was the last word in the sentence\nand the fourth region was the remainder of the sentence\nThe fourth region was created solely so that results from the third region would be free of sentence wrapup effects\nIn the first region, none of the effects were significant\nIn the second region, there was an effect of ambiguity, wherein the locally ambiguous versions were read faster than unambiguous versions, possibly due to a preference for adverbs to appear in verb-phrase final position\nhe raw reading times analyses and analyses of residuals were consistent in\nindicating that the interaction did not approach significance\nIn the third region, the different versions of any given sentence were once again identical\nHere, locally ambiguous versions were read somewhat slower overall than unambiguous versions\nHowever, this difference failed to approach significance in the subjects analysis (F1(1,59) = 1\n91, P � 0\n\n\nand was only marginally significant by items\n\n\nCrucially, each of these marginally significant main effects was qualified by a highly robust interaction between ambiguity and predicate type, suggesting that distributive predicates were read more slowly than collective predicates in the locally ambiguous versions\nPairwise comparisons confirmed that while distributives were read more slowly in the ambiguous versions (F1(1,59) = 10\n84, P � 0\n01\nF2(1,14) =20\n48, P � 0\n001), there was no reliable predicate effect in the unambiguous versions\nThe results of this analysis for first pass times revealed striking differences between ambiguous and unambiguous forms\nIn the residuals analyses, distributives (each) were read slower than collectives (together) in both ambiguous and unambiguous forms\nHowever, the effect was much larger in ambiguous forms than unambiguous forms (223 ms vs\n43 ms), and while in ambiguous forms the effect was significan\nDistributives were significantly faster than collectives in unambiguous forms (means: 844 ms vs\n925 ms\nF1(1,59) = 4\n72, P � 0\n05, F2 (1,14) = 10\n39, P � 0\n01), but significantly slower than collectives in ambiguous forms\nn the third region, where the different versions were again identical, there was a main effect of ambiguity, with ambiguous versions being read slower overall than unambiguous versions\nThere was also a robust predicate effect (F1(1,59) = 9\n53, P � 0\n01\nF2(1,14) = 10\n88, P � 0\n01), which indicated that distributives were read more slowly overall\nIn the second region, there were no significant differences across conditions in the percentages of trials on which regressions occurred out of the region (Fs � 1)\nIn the third region, the mean percentage of trials on which regressions occurred out of the region was 19%, 8%, 4% and 4% for the ambiguous-distributive, ambiguouscollective, unambiguous-distributive, and unambiguous-collective conditions, respectively\nThere were significantly more trials involving regressions out of the third region in the ambiguous than the unambiguous versions\nThe predictions of the vagueness hypothesis were clearly disconfirmed\nGiven the MSC hypothesis, the vagueness hypothesis predicts no interaction between ambiguity and sentence form: in both the ambiguous distributive (7a) and the unambiguous distributive (7c) the processor should postulate a distributive operator when each is encountered\nounter to this prediction, the ambiguous distributive form was substantially more difficult to understand than the other sentence forms\nThis may be seen in the significant interaction of ambiguity and sentence form in first pass and total times in region three, as well as in the regressions out of region three and regressions into region two\nReaders clearly exhibited a preference for the collective reading of the ambiguous portion of the sentences in our experiment\nThese results make it difficult to maintain the assumptions needed to salvage the vagueness hypothesis\nInstead, given the MSC hypothesis, they support the assumption that the correct grammatical account of collective/distributive differences treats the distinction as one of ambiguity rather than as one of vagueness, at least in cases like those tested, where subject-predicate relations are involved\nWe turn now to alternative interpretations of our results\nThe question is whether the results can be attributed directly to the complexity of the distributive reading\nWe think not\nIt may be true that distributive readings, even unambiguous ones, are slightly more complex than collective readings\nThis suggests that readers may not simply add information to the current representation of these locally ambiguous forms when each is encountered\nSimilarly the results are difficult to reconcile with a parallel processing view unless the processor has computed both a collective and a distributive representation and then abandoned the distributive representation before each is encountered\n"},"KB/Tanh":{"title":"Tanh","links":[],"tags":["architecture"],"content":"Tanh\n\n\\frac{e^x-e^{-x}}{e^x+e^{-x}}\nRNN : Hidden\nXavier/Glorot init\n\n"},"KB/Task-(endeffector)-Space-Vs-Joint-Space":{"title":"Task (endeffector) Space Vs. Joint Space","links":[],"tags":["robotics"],"content":"Task (endeffector) Space Vs. [Joint Space](Joint Space.md)\n\n\n"},"KB/Tau-Protein":{"title":"Tau Protein","links":["KB/Alzheimer’s-Disease"],"tags":["brain"],"content":"Tau Protein\n\nA type of protein abundantly found in neurons. When this protein is not adequately cleared from the brain, it can form tangles that are a key pathology of several neurodegenerative disorders including frontotemporal degeneration, CTE, and Alzheimer’s Disease.\n"},"KB/Teach-Lock":{"title":"Teach Lock","links":["KB/Mode-Switch"],"tags":["robotics"],"content":"Teach Lock\n\nWhile the Teach Lock is set, the mode of operation is tied to the Teach Mode and the machines cannot be played back using either Mode Switch to “TEACH” before beginning to teach.\n"},"KB/Teacher-Forcing":{"title":"Teacher Forcing","links":["KB/Basic-RNN-Architectures","KB/Transformer","KB/imageCaptioning","Tag-Pages/loss"],"tags":["architecture"],"content":"Teacher Forcing\n\nfrom\nTechnique where the target word (ground truth word) is passed as the next input to the decoder instead of its last prediction.\ncommon technique to train Basic RNN Architectures or Transformer\n\nused in imageCaptioning , Machine Translation\nbut also in Time Series forecasting\n\n\nintuition\n\nmath exam with dependent questions, e.g. a) depends on b), b) on c) and so on\nif a) is wrong, all subsequent questions are also wrong\nteacher forcing: after answering question a), the teacher compares it to the correct solution and grades it and then gives us the correct answer for a) to continue with\n\n\nfor example in sequence generation with RNN the situation is similar\n\neach prediction depends on the last one, thus when one is wrong all subsequent will be wrong as well\n\n\nno memorization can happen\n\nthe network can not look into the future\nground truth is only fed as last y_{t-1} prediction not as the current y_{t}\n\n\nloss does not need to be updated at each timestep, only needs to have a list with the true predictions of the model from which then the loss is calculated\npros\n\ntraining converges faster, because early predictions are very bad\n\n\ncons\n\nno ground truth label during inference, thus no teacher forcing\ndiscrepancy between training and inference scores\n\ncan lead to poor model performance and instability\nknown as Exposure Bias\n\n\n\n\n"},"KB/Teacher-Student-Architecture":{"title":"Teacher Student Architecture","links":[],"tags":["knowledgedistillation"],"content":"Teacher Student Architecture\n\nThe gap is further reduced by residual learning, i.e., the assistant structure is used to learn the residual error (Gao et al., 2021).\n"},"KB/Telomere":{"title":"Telomere","links":[],"tags":["brain"],"content":"Telomere\n\nThe protective cap found at the end of a chromosome. Research studies suggest these caps may be shortened in neurodegenerative diseases.\n"},"KB/Temporal-Context-Structure":{"title":"Temporal Context Structure","links":[],"tags":["semisupervisedlearning"],"content":"Temporal Context Structure\n\nThe temporal order from videos is used as supervision signal\nverify whether the input frame sequence in correct order\nrecognize the order of the frame sequence\n"},"KB/Temporal-Conv":{"title":"Temporal Conv","links":["KB/Conv","KB/Causal-1D-Conv","KB/Basic-RNN-Architectures","LSTM","KB/GRU"],"tags":["architecture"],"content":"Temporal Conv\n\nFCN + Causal 1D Conv + Residual\nOutperforms Basic RNN Architectures such as Long Short Term Memory (LSTM)).md) and GRU).md)\n"},"KB/Temporal-lobe":{"title":"Temporal lobe","links":["KB/Wernicke-Area"],"tags":["brain"],"content":"Temporal Lobe\n\nUnderstanding language (Wernicke Area)\nMemory\nHearing\nSequencing and organization\nLong-term memory is processed in the hippocampus of the temporal lobe and is activated when you want to memorize something for a longer time.\n"},"KB/Temporal-order-recognition":{"title":"Temporal order recognition","links":[],"tags":["semisupervisedlearning"],"content":"Temporal order recognition\n\nrecognize the order of a sequence of input frames.\n"},"KB/Temporal-order-verification":{"title":"Temporal order verification","links":[],"tags":["semisupervisedlearning"],"content":"Temporal order verification\n\nverify whether a sequence of input frames is in correct temporal order\n"},"KB/TemporalLearning":{"title":"Temporal Learning","links":["KB/Recurrent","Online-Learning","KB/Causal-Systems"],"tags":["architecture"],"content":"Temporal Learning\n\nRecurrent\nOnline Learning\nCausal Systems\n"},"KB/Tensor-Processing-Unit":{"title":"Tensor Processing Unit","links":[],"tags":["temp"],"content":"Tensor Processing Unit\n\nAn application-specific integrated circuit (ASIC) that optimizes the performance of machine learning workloads.\n"},"KB/Test-time-Augmentation":{"title":"Test-time Augmentation","links":["KB/Flipping"],"tags":["augmentation"],"content":"Test-time Augmentation\n\naugmenting data at test-time as well\nThis can be seen as analogous to ensemble learning techniques in the data space.\nBy taking a test image and augmenting it in the same way as the training images, a more robust prediction can be derived.\nrestrict the speed of the model\npromising practice for applications such as medical image diagnosis\nRadosavovic et al. denote test-time augmentation as data distillation to describe the use of ensembled predictions to get a better representation of the image.\nThey also found better uncertainty estimation when using test-time augmentation, reducing highly confident but incorrect predictions.\nMatsunaga et al. also demonstrate the effectiveness of test-time augmentation on skin lesion classification, using [geometric transformations](geometric transformations.md) such as rotation, translation, scaling, and Flipping.\nA robust classifier is thus defined as having a low variance in predictions across augmentations\nIn their experiments searching for augmentations with Reinforcement Learning, Minh et al. measure robustness by distorting test images with a 50% probability and contrasting the accuracy on un-augmented data with the augmented data.\nSome classification models lie on the fence in terms of their necessity for speed. This suggests promise in developing methods that incrementally upgrade the confidence of prediction. This could be done by first outputting a prediction with little or no testtime augmentation and then incrementally adding test-time augmentations to increase the confidence of the prediction.\nHowever, it is difficult to aggregate predictions on geometrically transformed images in object detection and semantic segmentation. Curriculum learning\nstrategy for selecting training data that beats random selection\nbest to initially train with the original data only and then finish training with the original and augmented data, although there is no clear consensus\n"},"KB/Text-Normalization":{"title":"Text Normalization","links":[],"tags":["language"],"content":"Text Normalization\n\nMerging diﬀerent written forms of a token into a canonical normalized form\n"},"KB/Text-Preprocessing":{"title":"Text Preprocessing","links":["KB/Document-Triage","KB/Text-Segmentation"],"tags":["language"],"content":"Text Preprocessing\n\nDocument Triage\nText Segmentation\n"},"KB/Text-Segmentation":{"title":"Text Segmentation","links":["KB/Word-Segmentation","KB/Text-Normalization","KB/Sentence-Segmentation","KB/Character-set-dependence","KB/Language-dependence","KB/Corpus-dependence","KB/Application-dependence"],"tags":["language"],"content":"Text Segmentation\n\nWord Segmentation\nText Normalization\nSentence Segmentation\nCharacter-set dependence\nLanguage dependence\nCorpus dependence\nApplication dependence\n"},"KB/Textbooks-are-all-you-need":{"title":"Textbooks are all you need","links":[],"tags":[],"content":"Textbooks Are All You Need\n \n\nSummary : This paper proposes a new code assisting model that uses Python text books to train as textbooks have more curated knowledge. There are quite a few limitations such as not learning, maths and being specific to the type of textbooks used.\n\nAbstract\n\nphi-1\nnew large language model for code,\nsignificantly smaller size than competing models:\nTransformer-based model with 1.3B parameters, trained for 4 days on 8 A100s, using a selection of “textbook quality” data from the web (6B tokens)\nsynthetically generated textbooks and exercises with GPT-3.5\nsurprising emergent properties compared to phi-1-base, our model before our finetuning stage on a dataset of coding exercises, and phi-1-small, a smaller model with 350M parameters trained with the same pipeline as phi-1 that still achieves 45% on HumanEval.\n\n\nTraining Details and the Importance of High-quality Data\n\n\n\nthe central ingredient our model relies on textbook-quality training data.\n\ntraining data. Unli+ke previous work that used standard sources of text data for code generation, such as The Stack [KLA 22] (which contains sourcecode from repositor+ies with permissive licenses) and other not optimal for teaching the model how to reason and plan algorithmically.\nThe standard code datasets [KLA+22, LCC+22] form a large and diverse corpus covering broad range of topics and use cases.\nsufer from several drawbacks:\nsamples are not self-contained, meaning that they depend on other modules or files that are external to the snippet, making them hard to understand without additional context.\nTypical examples do not involve any meaningful computation, but rather consist of trivial or boilerplate code, such as defining constants, setting parameters, or configuring GUI element\nSamples that do contain algorithmic logic are often buried inside complex or poorly documented functions, making them dicult to follow or learn from.\nexamples are skewed towards certain topics or use cases, resulting in an unbalanced distribution of coding concepts and skills across the dataset.\nlanguage models would benefit from a training set that has the same qualities as what a human would perceive as a good “textbook”: it should be clear, self-contained, instructive, and balanced.\nthree main datasets:\nfiltered code-language dataset\nsubset of The Stack and StackOverflow, obtained by using a language model-based classifier (consisting of about 6B tokens).\nsynthetic textbook dataset\n&lt;1B tokens of GPT-3.5 generated Python textbooks.\nsmall synthetic exercises dataset\n∼180M tokens of Python exercises and solutions.\ns of Python exercises and solutions. contain less than 7B tokens\nCodeTextbook\n\nFiltering of Existing Code Datasets Using a Transformer-based Classifier\n\nPython subset of the deduplicated version of The Stack and the StackOverflow, which together contain over 35 million files/samples, totalling over 35B tokens.\nannotate the quality of a small subset of these files (about 100k samples) using GPT- 4: given a code snippet, the model is prompted to “determine its educational value for a student whose goal is to learn basic coding concepts”.\nuse this annotated dataset to train a random forest classifier that predicts the quality of a file/sample using its output embedding from a pretrained codegen model as features.\nunlike GPT-3.5, which we use extensively to generate synthetic content (discussed below), we use GPT-4 minimally only for annotations on the quality of a small subset of The Stack and StackOverflow\n\n\nCreation of Synthetic Textbook-quality Datasets\n\nmain challenges in creating a high-quality dataset for code generation is ensuring that the examples are diverse and non-repetitive.\nthe examples should cover a wide range of coding concepts, skills, and scenarios, and that they should vary in their level of diculty, complexity, and style.\nexposes the language model to diferent ways of expressing and solving problems in code, it reduces the risk of overfitting or memorizing specific patterns or solutions, and it increases the generalization and robustness of the model to unseen or novel tasks\nnot trivial, especially when using synthetic data generated by another language model.\nSimply prompting the model to produce a coding textbook or a set of exercises, even with some variation in the instructions or the parameters, will likely result in a very homogeneous and redundant dataset, where the same concepts and solutions are repeated over and over with minor changes.\nlanguage models tend to follow the most probable or common paths given their training data and their priors, and they lack the creativity or the incentive to explore alternative or novel ways of generating code.\none needs to find the right “trick”\nThe synthetic textbook dataset\ndiversity is obtained by providing constraints on topics and target audience of the generated textbook\n\nThe CodeExercises dataset\nconduct explicit decontamination and alternative evaluations in the following sections to ensure that problems similar to those from HumanEval benchmark are not seen during finetuning.\n\n\nModel Architecture and Training\n\nsome recent models like CodeGen [NPH+22], PaLM [CND+22], and GPT-NeoX [BBH+22]. The architecture for our 1.3B parameter phi-1 model consists of 24 layers, hidden dimension of 2048, MLP-inner dimension of 8192, and 32 attention heads of dimension 64 each. The smaller 350M parameter phi- 1-small model consists of 20 layers, hidden dimension of 1024, MLP-inner dimension+of 4096, and 16 attention heads of dimension 64 each. We also use a rotary position+embedding [SLP 21] with rotary\ncould further boost performance and eciency [LAZ 23].\nFor both pretraining and finetuning, we concatenate our respective datasets into a single dimensional\narray with\narray with “��endoftext��” token used for separating the files. We train our models on sequence length of optimizer, linear-warmup-linear-decay learning rate schedule, and attention and residual dropout of 0.1.\nPretraining\nphi-1-base was trained on the CodeTextbook dataset\nphi-1 is obtained by finetuning phi-1-base on the CodeExercises dataset.\n\nSpikes of Model Capability after Finetuning on CodeExercises\n\nthe model after finetuning also exhibits a substantial improvement in executing tasks that are not featured in the finetuning dataset.\nThis includes managing intricate algorithmic tasks and using external libraries.\nsuggests that our finetuning process might have helped the model in reorganizing and consolidating the knowledge acquired during pretraining, even if such knowledge is not explicitly present in our CodeExercises dataset\nFinetuning improves the model’s understanding\nFinetuning improves the model’s ability to use external libraries\n\nEvaluation on Unconventional Problems with LLM Grading\n\nA potential concern with the surprisingly good performance of phi-1 on HumanEval (see Table 1 and Figure 2.1) is that there might be memorization stemming from contamination of the CodeExercises dataset1\nTo minimize bias and leakage, the new evaluation problems were created by a dedicated team in our group that did not access the CodeExercises dataset or the final model.\nThey created 50 new problems in the same format as HumanEval with instructions to design problems that are unlikely to appear in real-world code bases or as coding exercises.\nTo evaluate candidate solutions, we therefore adopt the approach of using GPT-4 to grade the solution\nThis approach has two distinct advantages: (1) by using GPT-4 as a grader, we can leverage its knowledge and generative abilities to obtain a more fine-grained and meaningful signal of the student model’s coding capabilities, and (2) it obviates the need for tests\nOur prompt instructs the LLM to evaluate a student’s solution first in a short verbal evaluation followed by grades from 0 to 10\n\nData Pruning for Unbiased Performance Evaluation\n\ntraining on CodeExercises leads to a substantial boost in the performance of the model on the HumanEval benchmark\nTo investigate this boost, we propose to prune the CodeExercises dataset by removing files that are “similar” to those in HumanEval.\n“strong form” of data decontamination.\nretrain our model on such pruned data, and still observe strong performance on HumanEval.\nsuch data pruning experiment is a fair way to evaluate performance\n\nN-gram Overlap\n\nN-gram measures the similarity of text segments based on the shared n-word sequences.\nn-gram overlap between the docstrings of each humaneval question and each exercise in the CodeExercises dataset that was generated\nfound 4 humaneval questions with 13-gram overlap with at least one of the entries in our dataset\nall the 4 overlap cases in the 13-gram are all false positives such as the example # below.\nOur n-gram overlap analysis shows that our dataset has minimal letter-by-letter overlap with HumanEval.\n\nEmbedding and Syntax-based Similarity Analysis\n\nnow turn to the pruning experiments\nn-gram analysis is not refined enough to find similar code snippets between HumanEval and CodeExercises\ncombination of embedding and syntax-based distances.\nFor the embedding distance we compute the L2 distance\nbetween the embedding of+the code snippets where the embedding is derived from a pre-trained CodeGenMono 350M model [NPH 23].\nFor the syntax-based distance we calculate the (string) edit distance between the abstract syntax trees (ASTs) of two given code snippets.\n\n\nConclusion\n\nJust as a comprehensive, well-crafted textbook can provide a student with the necessary knowledge to master a new subject, our work demonstrates the remarkable impact of high-quality data in honing a language model’s proficiency in code- generation tasks.\nBy crafting “textbook quality” data we were able to train a model that surpasses almost all open-source models on coding benchmarks such as HumanEval and MBPP despite being 10x smaller in model size and 100x smaller in dataset size.\nphi-1 is specialized in Python coding\nphi-1 lacks the domain-specific knowledge of larger models such as programming with specific APIs or using less common packages.\ndue to the structured nature of the datasets and the lack of diversity in terms of language and style, phi-1 is less robust to stylistic variations or errors in the\n\nLimitation of Phi-1\n\nOur model is sensitive to various perturbations of prompts.\nFirst, its performance drops significantly as the length of the prompt increases, as it tends to ignore, forget or misinterpret parts of the prompt when it is too long\nphi-1 demonstrates less robustness in handling natural language compared to ChatGPT or StarCoder, particularly with ambiguous prompts.\nThis may be because we filter out certain types of data from the training process to guarantee textbook-level quality.\nA primary constraint of our model, particularly when contrasted with alternatives like StarCoder, lies in its performance on tasks involving counting and spatial reasoning\n"},"KB/Textless-Speech-Emotion-Conversion":{"title":"Textless Speech Emotion Conversion","links":["KB/Features"],"tags":["architecture"],"content":"Textless Speech Emotion Conversion\n\nTextless Speech Emotion Conversion Using Discrete and Decomposed Representations\nSpeech emotion conversion\nmodifying the perceived emotion of a speech utterance while preserving the lexical content and speaker identity\nspoken language translation task\ndecomposition of the speech signal into discrete learned representations, consisting of phonetic-content units, prosodic Features, speaker, and emotion\nmodify the speech content by translating the phonetic-content units to a target emotion, and then predict the prosodic Features based on these units\nspeech waveform is generated by feeding the predicted representations into a neural vocoder\nbeyond spectral and parametric changes of the signal\nmodel non-verbal vocalizations, such as laughter insertion, yawning removal, etc\n"},"KB/Thalamus":{"title":"Thalamus","links":["KB/Attention"],"tags":["brain"],"content":"Thalamus\n\nServes as a relay station for almost all information that comes and goes to the cortex.\nIt plays a role in pain sensation, Attention, alertness and memory.\n"},"KB/The-Behavior-of-Tutoring-Systems":{"title":"The Behavior of Tutoring Systems","links":["KB/Knowledge-Component","KB/Learning-Event","KB/Modeling-Transfer","KB/Predicting-Student-learning-Curve","KB/Tutor","KB/Mastery-learning","KB/Macroadaptation","KB/Steve","KB/Andes","KB/Sherlock","KB/Algebra-Cognitive-Tutor","KB/SQL-Tutor","KB/AutoTutor","KB/DT-Tutor","KB/Help-Abuse","KB/Help-Refusal","KB/plan-recognition-problem","KB/Coarse-grained-assessment","KB/Fine-Grained-assesment","KB/IRT"],"tags":["usermodel"],"content":"The Behavior of Tutoring Systems\n\nKurt VanLehn\n\nIntro\n\nTutoring systems are described as having two loops.\nThe outer loop executes once for each task, where a task usually consists of solving a complex, multi-step problem\nThe inner loop executes once for each step taken by the student in the solution of a task. The inner loop can give feedback and hints on each step. The inner loop can also assess the student’s evolving competence and update a student model, which is used by the outer loop to select a next task that is appropriate for the student.\nA task usually takes several minutes to an hour or so. Most tutors assume that the student is working alone on a task, but some (e.g., Zachary et al., 1999) have the student work as one member of a multi-student team.\nThe tasks and the tutor’s user interface are usually designed so that completing a task requires multiple steps, where a step is a user interface action that the student takes in order to achieve a task.\nLet us use knowledge for the information possessed by students that determines their behavior on task. Let us also assume that knowledge can be decomposed, and let us use the term Knowledge Component for the units into which it is decomposed.\nKnowledge Component\nLearning Event\nModeling Transfer\nPredicting Student learning Curve\nTutor\n\nTHE OUTER LOOP\n\nThe main responsibility of the outer loop is to decide which task the student should do next. Its other responsibilities, such as presenting the task to the student, are more mundane and will not be mentioned again. The main design issues are (1) selecting a task intelligently and (2) obtaining a rich set of tasks to select from.\n\nTask Selection\n\nThe outer loop displays a menu and lets the student select the next task.\nThe outer loop assigns tasks in a fixed sequence\nMastery learning\nMacroadaptation\nIn addition to selecting a task, some tutoring systems also select a mode for the task. For instance, Steve can either (1) demonstrate how to do the task by taking each step itself and explaining it, or (2) hint each step before the student attempts it, or (3) let the student try to solve the task without such unsolicited hints.\nIn addition to providing some kind of mechanism for selecting tasks, the designer must provide a set of tasks for the outer loop to select from. It is often surprising how many problems are necessary in order to field a tutoring system. For a 13 week semester at 10 problems per week, the tutoring system needs 130 problems at minimum\nIdeally, the tutor can generate its own tasks when given a specification of the desired characteristics.\nFor instance, given a specification of a Knowledge Component to be taught, the SQL- Tutor (Martin &amp; Mitrovic, 2002) generates a database query, written in SQL, that involves the Knowledge Component. A human author then writes text for a problem that has this database query as its solution. Using this technique, a human author was able to create 200 problems in about 3 hours— enough for a 6 week instructional module in an SQL course. Such programs are called problem generators.\n\nTHE INNER LOOP\n\nWhereas the outer loop is about tasks, the inner loop is about steps within a task\n\nMINIMAL FEEDBACK\n\nMinimal feedback usually indicates whether a step is correct or incorrect, although other categories are sometimes used as well.\n\nCategories of Minimal Feedback\n\nAlthough most tutoring systems use just two categories, correct and incorrect, it is possible to use others.\nSuppose a step is not part of an ideal solution to the problem, but it might be part of a non-ideal solution. The instructors might wish to consider this as a third category for minimal feedback—correct but non-optimal\nFor instance, when students solve physics problems on Andes, they can enter an equation that is true of the given physical situation but is not necessary for solving the problem.\nIf instructors sometimes disagree on what makes for the best solution to the problem, it might be wise to subcategorize non-optimal. For instance, because some Sherlock instructors emphasize reducing time and others emphasize reducing costs, Sherlock could subcategorize non-optimal steps as wastes time or wastes parts. In short, the categories used for minimal feedback should reflect the pedagogical policies of the instructors.\nIf a step cannot be classified as into one of the minimal feedback categories (correct, incorrect, non-optimal, etc.), then it lands in an unrecognizable classification.\nThere are basically just three ways to treat such unrecognizable steps. (1) Some tutoring systems, such as the Algebra Cognitive Tutor, assume that they can recognize all correct steps, so they treat unrecognizable steps as incorrect. (2) Other tutoring systems, such as the SQL-Tutor, assume that students will sometimes produce novel, correct solutions, so they treat unrecognized steps as correct. (3) Lastly, the tutoring system could simply tell the student that the step is unrecognizable. In this case, unrecognizable is yet another category for minimal feedback. When to give minimal feedback\nImmediate feedback: Andes, Algebra Cognitive Tutor, Steve and AutoTutor give feedback immediately after each student step. They vary considerably in how the feedback is presented.\nDelayed feedback: Only if the step violates a safety regulation will Sherlock give immediate feedback as the student solves a problem. However, after the problem is solved, the student can request a reviewing mode where minimal feedback is given. In particular, as Sherlock replays the student’s solution step-by-step, it indicates which steps did not contribute any diagnostic information.\nDemand feedback: The SQL-Tutor gives feedback only when the student clicks on the Submit button. Because the student can continue working on their solution after receiving such feedback, this does not count as delayed feedback.\nThe feedback policy can be a function of the student’s competence. For instance, if the student is nearing mastery, then the feedback is delayed whereas a less competent student would be given the same task with immediate feedback\nThis policy has been observed in human tutoring, and is one instance of fading the scaffolding, (Collins et al., 1989), because feedback is a kind of scaffolding (pedagogically useful help).\n\nNEXT-STEP HINTS\n\n\n\nWhen should the tutor give a hint about the next step? Should it wait for the student to ask? Should it give unsolicited hints when it detects guessing or floundering? 2. What step should the tutor suggest? For instance, if there are multiple paths to a solution, and the student appears to be following a long complex one, should the tutor suggest starting over? 3. How can the tutor give hints that maximize learning, keep frustration under control, and allow the student to finish the problem?\n\n\nThe common wisdom is that a tutor should give a hint on the next step only if the student really needs it. If the student can enter a correct step by thinking hard enough, then the system should refuse to give a hint even if the student asks for one\nOn the other hand, if the student is likely to waste time and get frustrated by trying in vain to enter a correct step, or if the student is making repeated guesses instead of trying to figure out the next step, then the tutoring system should probably give a hint even if the student doesn’t ask for one.\nDT Tutor\nHelp Abuse\nHelp Refusal\n\nWhich Step to Hint\n\nThe step must be correct.\nThe hinted step should not have been done already by the student.\nInstructors’ preferences should be honored.\nIf the student has a plan for solving the problem or even for just the next step, then the tutor should hint the step that the student is trying to complete.\nA somewhat more complex case occurs when the student has just made an incorrect step and received minimal negative feedback.\nIn AI, this is called the plan recognition problem\n\nHow to Hint the Next Step\n\nPerhaps the most common policy is to construct a short sequence of hints for the next step.\nThe first hints are weak—they divulge little information so that students are encouraged to do most of the thinking themselves. Later hints are stronger.\nIf the step requires only one Knowledge Component, then a standard hint sequence is Point, Teach and Bottom-out (Hume, Michael, Rovick, &amp; Evens, 1996)\nPointing hints mention problem conditions that should remind the student of the Knowledge Component’s relevance.\nTeaching hints describe the Knowledge Component briefly and show how to apply it.\nBottom-out hints tell the student the step\n\nERROR-SPECIFIC FEEDBACK\n\nThe service consists of analyzing an incorrect step in order to determine the incorrect Learning Event(s) that led to it, then giving instruction that is aimed at preventing that incorrect Learning Event(s) from occurring again\nThere are many techniques for diagnosing errors. Most require that authors observe student errors, figure out what is causing a common error, write an appropriate error type for it, and implement some way to recognize such errors.\nFOIL: First, Outer, Inner and Last\n\nHow to Give Error-specific Feedback\n\nThe main purpose of error-specific feedback is to get the student to change their knowledge so that they will not make this mistake again\nCorrecting one’s knowledge is sometimes compared to debugging a piece of software (Ohlsson, 1996). A programmer must first find evidence that the bug exists, then find out what the bug is, and then fix the bug. When tutees are not working with a tutor, they must do the whole self-debugging process themselves. They must first notice that a step is incorrect, then find the flaw in their reasoning and knowledge, then figure out what the correct learning events and knowledge components should be\nMany tutors present feedback as a sequence of hints that are loosely associated with the stages of self-debugging. When the student enters an incorrect step, the tutor begins the sequence by simply giving minimal feedback. This is like providing the student with evidence of a knowledge bug but giving no further help toward identifying the bug or correcting it.\n\nWhen to Give Error-specific Feedback\n\nStudents sometimes make careless errors, called slips in psychology (Norman, 1981), but fail to notice that they have made them, which can cause them to waste time looking for deep, potential misconceptions.\nTo ameliorate into slips and potential this, Andes divides error misunderstandings. types\nSlips are often made by the experts, including the instructors.\nA potential misunderstanding is an error type that could be due to many factors, including incorrect beliefs, but it is almost never seen in expert’s work\nWhen a student enters an incorrect step that is classified as a slip, then Andes gives the error specific remediation immediately, e.g., You forgot to include a unit on a number. If the error is classified as a potential misunderstanding, then Andes merely turns the incorrect step red, and lets the student ask for an error-specific hint if they want one.\nSome student steps contain two or more errors. In order to keep the communication simple, the tutoring system should probably respond to only one of them. Students typically fix that error only, so the tutoring system can then mention the second error\n\nWhat Kind of Assessment is Required?\n\nA fundamental issue is the grain-size or granularity of the assessment.\nThe granularity of an assessment refers to the degree of aggregation over the task domain’s knowledge. An assessment is fine-grained if it provides, for instance, a probability of mastery for each Knowledge Component in the task domain. An assessment is coarse-grained if it provides a single number that indicates the student’s overall competence.\nAssessments are decision aids, and as a general heuristic, the bigger the decision, the coarser the required assessment. If a decision affects a whole semester, e.g., whether the student needs to retake a\nrequired course, then the assessment should cover the whole semester and lead ultimately to a yes-no decision, so a single number per student is appropriate.\nIf a decision affects only, say, whether the tutor starts at the beginning of a hint sequence or in the middle, then only a small part of a fine-grained assessment is relevant\nThe general idea is that if the decision is small, in that it affects only a small amount of instructional time, then only a small amount of the domain’s knowledge can be accessed during that time and thus the relevant decision aid is the student’s competence on just that knowledge.\nTutors make many small decisions, so they are the main customers for fine-grained assessments.\nCoarse-grained assessment\nFine Grained assesment\n\nIssues with Assesment\n\nThere are many other issues involved with assessment. Here are a few:\nThere is a big difference between little evidence and mixed evidence. If one merely takes the frequency of success relative to opportunities, then 0.5 can mean both 1 success and 1 failure (little evidence) or 20 successes and 20 failures (mixed evidence).\nStudents should be learning, so their knowledge should be changing. When should old evidence be ignored?\nShould help given by the tutoring system be counted as evidence of learning or of lack of knowledge?\nAre all learning events equally difficult? If not, then item response theory (IRT) can be used so that success on easy learning events provides less evidence of competence than success on difficult learning events.\nIf the tutoring system includes error-specific feedback based on error types, should it somehow include the frequency of occurrence of the error types in its assessments?\nPrior probabilities of mastery are not independent. If a student has not mastered a Knowledge Component that is taught early in the course, then it is less likely that the student has mastered.\nKnowledge components that appear later.\n\nExamples of Tutoring Systems\n\nSteve\nAlgebra Cognitive Tutor\nAndes\nSherlock\nAutoTutor\nSQL-Tutor\n"},"KB/The-Differentiation-Condition":{"title":"The Differentiation Condition","links":[],"tags":["language"],"content":"The Differentiation Condition\n\nA sentence containing a quantified phrase headed by each can only be true of event structures which are totally distributive. Each individual object in the restrictor set of the quantified phrase must be associated with its own subevent, in which the predicate applies to that object, and which can be differentiated in some way from the other subevents.\n"},"KB/The-Effect-of-Three-Consecutive-Context-Sentences-on-EFL-Vocabulary-Learning":{"title":"The Effect of Three Consecutive Context Sentences on EFL Vocabulary-Learning","links":[],"tags":["usermodel"],"content":"The Effect of Three Consecutive Context Sentences on EFL Vocabulary-Learning\n\n\nSasan Baleghizadeh and Mohammad Naseh Nasrollahy Shahry\n\n\neffect of three consecutive context sentences instead of one\n\n\nThirty-three Iranian EFL learners were asked to learn 20 challenging English words in two conditions\n\n\nThe results of both immediate and delayed post-tests revealed a positive role for context sentences in vocabulary learning\n\n\nIt is proposed that successful vocabulary learning through context sentences could be attributed to the mixed effects of both context and frequency of occurrence.\n\n\nResults\n\nseven students knew at least one of the target words, so their scores on the posttests were excluded\ndata were analyzed by using a paired sample t-test because it was a within-subjects comparison and the scores on both learning conditions were related\nthe participants’ performance on words that had appeared in context sentences plus their L1 equivalents was significantly better than their performance on words paired only with their L1 equivalent\nDiscussion\nLearners exposed to context sentences did better in terms retaining words, and they were also able to compose more correct sentences with them\nThe learners who were exposed to context sentences had three sentences on which to draw as models, and it is plausible that part of their better sentence-making scores could be accounted for by their exposure to these sentences.\nfrequency and context have an important place in vocabulary-learnin\nlthough learning new words through context-free activities such as working on word pairs might be a powerful tool to enhance one’s breadth of vocabulary knowledge, this study provides strong evidence that adding a minimum of three contextually appropriate sentences to L1 glosses results in a significant improvement in vocabulary-learning\nneed to furnish learners with more sample sentences when it comes to presenting vocabulary\nauthors of textbooks seem to have a propensity for presenting isolated words either in designated boxes or in the context of a passage, which essentially provides only one context for the given word\nIt appears that students would be in a better position to learn and retain new words if they were provided with repeated contexts through exposure to more sample sentences\nIn other words, is it the elaborative nature of the context, or is it the frequency of occurrence that promotes better vocabulary learning? Future research is warranted to unravel this issue.\n"},"KB/The-Programmers-Brain":{"title":"The Programmers Brain","links":[],"tags":["deeplearning"],"content":"The Programmers Brain\n\nShort + long term memory - problem + software knowledge\nreading complicated/new code\n\nchunking : words vs letters\n\n\nProgrammers to reproduce code\n\n\nexperts rely on a bunch of things to “do well”\n\n\n"},"KB/The-Repulsive-Potential":{"title":"The Repulsive Potential","links":[],"tags":["robotics"],"content":"The Repulsive Potential\n\nThe robot is pushed away from obstacles and attracted to the goal\nU_{rep}(q)=\\frac{\\eta}{D(q,Q)}\n\n"},"KB/The-Reward-Experiment":{"title":"The Reward Experiment","links":[],"tags":["cognitivemodel"],"content":"The Reward Experiment\n\n“Why aren’t we always prepared?”\n\nIs it a matter of motivation?\nReward bocks: flat(ter) preparation curves?\n\n\nReward blocks: more memory effects\n\n[Cognition Hazard Rates](Cognition Hazard Rates.md)\n\n\nReward blocks: less memory effects\n\n[Cognitive fMTP](Cognitive fMTP.md)\n\n\n\nFanning\n\nsmaller if reward\nless effects of previous trial\n\n\n\nFindings\n\nThe effects of reward and brightness on preparation\n\nAdditive effects : neither affects the slope\neven when motivated, preparation depends on FP\nStimulus visibility does not interact with preparation\n\nMaybe not a visual attention process\n\n\n\n\n\nReward has more effects when targets are highly visible\n\nInteraction : Reward * Brightness\n\n\n\nEffects on accuracy\n\nReward blocks→ higher accuracy\nHigh visibility → higher accuracy\nMore errors with longer foreperiod….\n\n…But only when rewarded/motivated\n…And mainly when visibility low\n\n\n\n\n\nEffects on memory from past trials\n\nPast trial effects fp \\ast fp(n-1)\nThis effect (the fanning) is essentially constant across target visibility and reward\nIf anything: a little bit larger when there is no reward.\n\n\n\nEffects on memory\n\nUp to n-5\nWhen there is reward, there is slightly ‘faster forgetting’\nFaster forgetting when rewarded slightly\n\n\n\n"},"KB/The-Self-Organization-of-Explicit-Attitudes":{"title":"The Self Organization of Explicit Attitudes","links":["KB/Recurrent"],"tags":["language"],"content":"The Self Organization of Explicit Attitudes\n\nMichael T. Wojnowicz, Melissa J. Ferguson, Rick Dale, and Michael J. Spivey\n\nABSTRACT\n\nHow do minds produce explicit attitudes over several hundred milliseconds?\nimplicit biases beyond cognitive control and subjective awareness, yet mental processing may culminate in an explicit attitude that feels personally endorsed and corroborates voluntary intentions\nself-reported explicit attitudes derive from a continuous, temporally dynamic process, whereby multiple simultaneously conflicting sources of information selforganize into a meaningful mental representation\nWhile our participants reported their explicit (like vs. dislike) attitudes toward White versus Black people by moving a cursor to a ”like” or ”dislike” response box, we recorded streaming xand y-coordinates from their hand-movement trajectories\nparticipants’ hand-movement paths exhibited greater curvature toward the ”dislike” response when they reported positive explicit attitudes toward Black people than when they reported positive explicit attitudes toward White people\nthese trajectories were characterized by movement disorder and competitive velocity profiles that were predicted under the assumption that the deliberate attitudes emerged from continuous interactions between multiple simultaneously conflicting constraints.\nFor example, an implicit attitude toward a stimulus can be unintentionally activated by the mere presence of that stimulus.\nGiven that many people demonstrate spontaneous initial biases toward traditionally stigmatized groups, how do they overcome these biases to explicitly report positive attitudes toward the same groups?\ncoexistence of multiple attitudes and an emphasis on the temporal dynamics of how they influence evaluative responses\nRather than selecting among the specific theories, we invoked the encompassing theoretical framework of self-organization to guide an exploration of those temporal dynamics, and made specific predictions for what should result from multiple attitudes interacting over time.\nIn early moments of processing, distributed representations are partially consistent with multiple interpretations because of their proximity to multiple neural population codes.\nHowever, a continuous accrual of information causes the distributed pattern to dynamically ”sharpen” into a confident (selected) interpretation, forcing other, partially activated, competing alternative representations, decisions, or actions to gradually die out.\nThe latter attitude will eventually activate other subsystems, such as language and memory, thus making the attitude seem explicit\nWhat makes the first attitude implicit is not necessarily that it was generated in a different subsystem, but simply that it did not hold sway long enough to activate those language and memory subsystems.\nMental processing generically involves Recurrent processing loops (or cyclic feedback) between higher-order integrative regions and lower-level informational sources (Lamme &amp; Roelfsema, 2000; O’Reilly, 1998; Spivey, 2007)\nThese higher-order integrative regions enforce representational competition, in which increasing the activation of one particular interpretation inhibits alternatives.\nThe unfolding cognitive dynamics may be revealed in continuous motor output\nBecause mental processing is Recurrent, motor representations begin specifying movement parameters probabilistically, rather than waiting for a perfectly completed cognitive command\nIf the phrase ”Black people” evokes elevated dynamic competition between simultaneously active ”like” and ”dislike” representations, movement trajectories for ”Black people” should exhibit evidence of nonlinear dynamics in their velocity profiles, as well as increased spatial disorder in the curviness of the trajectories.\n\nEXPERIMENT 1\nMethod\n\nStreaming xand y-coordinates of mouse-cursor movements were recorded from 68 Cornell University undergraduates (43 female and 25 male) as they performed a simple explicit-attitude task.\n2 s for participants to view the evaluative response options\nParticipants then clicked on a small box at the bottom of the screen to reveal a stimulus word or phrase and dragged the mouse toward their selected evaluative response to that stimulus\nResponses to the two stimulus repetitions were averaged together to yield a single measurement for each participant for all statistical analyses.\n\nResults and Discussion\n\nCompared with the trajectories for ”White people,” the trajectories for ”Black people” curved significantly more toward the ”dislike” response option observed differential motor curvatures could have been generated by a stage-based sequence of decisional commands, rather than by continuous motor attraction to the ”dislike” response.\nIf motor execution required the complete prespecification of a unique target destination, rather than tracking of motor trajectory parameters that continuously evolved midflight, then a mean trajectory could look differentially curved because of the effect of averaging in replanned trajectories\nTo accommodate the empirical mean trajectory, which initially moved upward rather than actually toward ”dislike,” such an account would need to predict a bimodal distribution of curvatures that included some trajectories that were very curved and others that were not curved.\nHowever, the distribution of trajectory curvatures shows no evidence of bimodality The standard cutoff for inferring bimodality in a distribution is b &gt; 0.55\nNeither the ”Black people” nor the ”White people” trajectories had distributions that met this cutoff, and in fact, the ”Black people” trajectories formed a distribution of movement curvature that was closer to normal (b 5 0.24, skewness 5 0.613, kurtosis 5 2.57) than the ”White people” trajectories (b 5 0.301, skewness 5 0.98, kurtosis 5 3.44).\nVelocity profiles were constructed by analyzing the temporal derivatives of motion toward the ”like” response box along the x-coordinate.\nOur velocity predictions came from Usher and McClelland’s (2003) differential equations for modeling the dynamics of competition between mental representations\nwhere, in this case, x1 and x2 represent the activations of the mental representations for ”like” and ”dislike,” dx1 and dx2 represent the change in the activation of the two mental representations in a time step of size dt, I1 and I2 represent excitatory input to the representations from informational sources, bf1 and bf2 represent the inhibitory input from each mental representation to the other (lateral inhibition), and fi (where i 5 1 or 2) is equal to xi if xi is greater than zero.\ndifferential equations for competition dynamics, a strong evaluative competitor (dislike, x2) sends intensified and prolonged lateral inhibition (bf2) to the ”like” evaluation (x1)\nThus, strong competition alters the velocity profile of the movement toward the evaluative attractor (dx1/dt), reducing velocity toward the attractor early on in processing\nas the more active alternative begins to win the competition, this lateral inhibition is gradually lifted, thus increasing velocity later in processing to produce greater acceleration\nMoreover, this particular dynamic pattern (reduced early velocity and greater later acceleration) should lead to greater peak velocity, if jerk is minimized as the system achieves equivalent integral under the curve (where the integral represents net change in activation or location)\nThe spatial-disorder analysis investigated the regularity of change in x-coordinate location over time\nTo investigate whether the ”Black people” trajectories had more wiggles, blips, and other irregularities than the ”White people” trajectories, we analyzed x-coordinate location over time, but only after the trajectory began moving in the positive x direction\nThe ”Black people” trajectories had significantly greater deviation from the sigmoidal fit\nindicated disorderly variation around the x dimension in those trajectories.\n\nEXPERIMENT 2\n\nour claim is that multiple, partially active mental representations compete for the privilege of driving evaluative responses, imposing a set of response options that are not particularly competitive should change the motor dynamics\n\nMethod\n\nSixty-six Cornell University undergraduates (40 female and 26 male) were asked to classify words (e.g., ”ice cream,” ”sunshine,” ”boron”) as something they liked (”like”) or as the name of a chemical element (”chemical”)\nWe analyzed data only from the 63 participants who consistently chose the ”like” response for both ”Black people” and ”White people” on both repetitions of these trials, and who reported in a poststudy questionnaire that they were not forced into selecting ”like” by the paradigm.\n\nResults\n\nAccording to statistical analyses on maximum deviation and distance traveled, the ”Black people” and ”White people” trajectories no longer differed in their curvature toward the competing respons\nThus, the results of Experiment 1 are not attributable merely to responses to ”Black people” involving a longer latency to settle on a positive evaluation\nthereby drifting for longer in empty regions of movement space before curving\ntoward the ”like” response box\nRather, the ”dislike” response option in Experiment 1 was actively pulling movement trajectories toward it, in a way that the ”chemical” response option in Experiment 2 did not.\n\nEXPERIMENT 3\n\nExperiment 1 may have diverged because of subtle confounds that do not refer to people at all.\n\nMethod\n\nSeventy-one Cornell University undergraduates (37 female and 34 male) were asked to classify stimuli as something they liked (”like”) or disliked (”dislike”)\nThe crucial stimuli in this experiment were ”African Americans” and ”Caucasians. Results\nThe trajectories for ”African Americans” curved significantly more toward the ”dislike” response than the trajectories for ”Caucasians,\nThe motor trajectories evolved over time in accordance with the competitive velocity predictions, as reported in Experiment\nThe ”African Americans” trajectories, compared with the ”Caucasians” trajectories, had significantly greater maximum xcoordinate acceleration\nMoreover, as we found for ”Black people” trajectories in Experiment 1, the ”African Americans” trajectories exhibited greater spatial disorder than the ”Caucasians” trajectories, even after moving toward the ”like” response, as indicated by significantly greater mean deviation from the sigmoidal fi\n\nGENERAL DISCUSSION\n\nPeople’s hand-movement trajectories for explicitly evaluating ”Black people” and ”White people” were distinct as measured by three properties of movement dynamics: shape, time, and order.\nexplicit attitudes evolve through continuous temporal dynamics during real-time mental processing, with graded motor curvature revealing the influence of tendencies toward dislike\nevidence for cleanly separated (i.e., discrete, rather than continuous) explicit decisions, in which an initial response was executed solely toward the ”dislike” response box and then a corrective response was executed midflight toward the ”like” response box.\nRather, the results suggest that a dynamic competition process may be what allows a single explicit attitude choice to emerge from multiple, potentially conflicting evaluative influences (e.g., Busemeyer &amp; Townsend, 1993; Usher &amp; McClelland, 2003)\nthe mind may host a continuously evolving blend of (implicit) evaluative decisions from which the eventual (explicit) behavioral choice emerges.\n\nPictures\n\n\n\n\n\n\n"},"KB/The-Unified-Causal-AI-Pipeline":{"title":"The Unified Causal AI Pipeline","links":["KB/Causality","KB/Markov-Chain","KB/Bayesian","KB/TemporalLearning","KB/VAE"],"tags":["casual","talk"],"content":"The Unified Causal AI Pipeline\n \n\nSara Magliacana - prof at UvA\n\nIntro\n\npredict the effect of actions and decide effective policies - and how\ndl - can learn representations from unstructured data\npredict effects of interventions\n\ncausal variables not observed directly\nno labels\njust high level observations\n\n\ntasks\n\nidentify causal variables from observations\nlearn causal relations between them\n\n\n\nRecovering Causal Variables\n\ntheoretical guarantees\nwithout supervision, cannot identify exact causal variables - but up to equivalence class (aka not perfectly)\n\nsomehow seems like trying to recover some variables from a tangled mess of the ground truth\n1-1 map?\n\n\n\nDynamic Bayesian Networks\n\nMarkov Chain\nstationarity : transition model (edge) are same across pairs of time steps\nno instantaneous effects : no edges between vars at same timestep\nextension of Bayesian networks\nenvironment : AI Tutor\n\nTemporal Intervened Sequences\n\nlearn underlying causal process from temporal seq of high dim data (images) {X^{t}}^{T}_{t-1}\nAssume latent causal process in Dynamic Bayesian Networks with K multidim causal variables\nAssume interventions can happen, observe binary targets I^{t}_{i} and I_{i} \\rightarrow C_{i}\ntargets are 1-1 with causal variables\nmodel with latent variable\npaper : CITRIS (phillip lippe, sara magliacane)\n\nstochastic intervention : we dont know where the ball will be\nsimplified identification proof\nminimal causal variables\n\npart of the variable that is varying\nidentify them up to invertible component wise transforms if I_{i}^{t} up to I_{j}^{t}\n\n\nCITRIS - VAE\n\ncant really use pretrained autoencoder since you assume its disentangling\n\n\nnormalizing flows : CITRIS NF\n\nuse a pretrained autoencoder\n\n\n\n\nBISCUIT - causal representation learning from binary interactions\n\nextension of TRIS\neffect encoded in binary interaction variables\nassume action has to affect causal variables in a binary way\nMore BISCUIT - VAE and NF\n\n\nEnvironments - CausalWorld, iTHOR\ndynamic interaction map\nCRL World models\n\nWhy\n\ncausal representation learning\ncausal discovery\ninspired ml/rl research\nexplainable ai\ncausal effect estimation\n\nFuture\n\nBISCUIT + DAFT RL\n"},"KB/The-Unreliability-of-Saliency-Methods":{"title":"The Unreliability of Saliency Methods","links":[],"tags":["explainability"],"content":"The Unreliability of Saliency\n\n@kindermansReliabilitySaliencyMethods2017\n unreliable\nPieter-Jan Kindermans, Sara Hooker, Julius Adebayo, Maximilian Alber, Kristof T. Schütt, Sven Dähne, Dumitru Erhan, Been Kim\n \n\n"},"KB/The-elephant-in-the-interpretability-room":{"title":"The elephant in the interpretability room","links":[],"tags":["explainability"],"content":"The Elephant in the Interpretability Room\n\n@bastingsElephantInterpretabilityRoom2020\n\nIntro\n\nWhile attention conveniently gives us one weight per input token and is easily extracted, it is often unclear toward what goal it is used as explanation.\nthat often that goal, whether explicitly stated or not, is to find out what input tokens are the most relevant to a prediction, and that the implied user for the explanation is a model developer\nFor this goal and user, we argue that input saliency methods are better suited, and that there are no compelling reasons to use attention, despite the coincidence that it provides a weight for each input.\n\n\nThe Attention Debate\n\nsummarize the debate on whether attention is explanation\nmostly features simple BiLSTM text classifiers\nUnlike Transformers (Vaswani et al., 2017), they only contain a single attention mechanism, which is typically MLP-based (Bahdanau et al., 2015)\n\n\nIs Attention (not) Explanation?\n\nJain and Wallace (2019) show that attention is often uncorrelated with gradientbased feature importance measures, and that one can often find a completely different set of attention weights that results in the same prediction\nSerrano and Smith (2019) find, by modifying attention weights, that they often do not identify those representations that are most most important to the prediction of the model\nFinally, Pruthi et al. (2020) propose a method to produce deceptive attention weights. Their method reduces how much weight is assigned to a set of ‘impermissible’ tokens, even when the models demonstratively rely on those tokens for their predictions.\n\nWas the Right Task Analyzed\n\nthe performance of an NMT model degrades substantially if uniform weights are used, while random attention weights affect the text classification performance minimally.\neven for the task of MT, the first case where attention was visualized to inspect a model (§1), Ding et al. (2019) find that saliency methods (§3) yield better word alignments.\n\nCan Attention Be Improved\n\n\nMohankumar et al. (2020) observe high similarity between the hidden representations of LSTM states and propose a diversity-driven training objective that makes the hidden representations more diverse across time steps\n\n\nusing representation erasure that the resulting attention weights result in decision flips more easily as compared to vanilla attention\n\n\nDeng et al. (2018) propose variational attention as an alternative to the soft attention of Bahdanau et al. (2015), arguing that the latter is not alignment, only an approximation thereof\n\n\nadditional benefit of allowing posterior alignments, conditioned on the input and the output sentences.\n\n\n[Saliency vs Attention](Saliency vs Attention.md)\n\n\nAttention is not not Interesting\n\nVoita et al. (2019) and Michel et al. (2019) analyze the role of attention heads in the Transformer architecture and identify a few distinct functions they have, and Strubell et al. (2018) train attention heads to perform dependency parsing, adding a linguistic bias\nStrout et al. (2019) demonstrate that supervised attention helps humans accomplish a task faster than random or unsupervised attention\n\nBeyond Saliency\n\ncounterfactual analysis might lead to insights, aided by visualization tools (Vig, 2019; Hoover et al., 2020; Abnar and Zuidema, 2020)\nThe DiffMask method of DeCao et al. (2020) adds another dimension: it not only reveals in what layer a model knows what inputs are important, but also where important information is stored as it flows through the layers of the mode\n\nLimitations of Saliency\n\nchanges in the predicted probabilities may be due to the fact that the corrupted input falls off the manifold of the training data (Hooker et al., 2019)\na drop in probability can be explained by the input being OOD and not by an important feature missing\nat least some of the saliency methods are not reliable and produce unintuitive results (Kindermans et al.\n\n\nor violate certain axioms (Sundararajan et al., 2017).\n\n\nA more fundamental limitation is the expressiveness of input saliency methods\nObviously, a bag of per-token saliency weights can be called an explanation only in a very narrow sense\nOne can overcome some limitations of the flat representation of importance by indicating dependencies between important features (for example, Janizek et al. (2020) present an extension of IG which explains pairwise feature interactions) but it is hardly possible to fully understand why a deep non-linear model produced a certain prediction by only looking at the input tokens\n"},"KB/The-warning-stimulus-as-retrieval-cue-The-role-of-associative-memory-in-temporal-preparation":{"title":"The warning stimulus as retrieval cue The role of associative memory in temporal preparation","links":[],"tags":["cognitivemodel"],"content":"The warning stimulus as retrieval cue The role of associative memory in temporal preparation\n\nSander A. Los a,* , Jurre Nieuwenstein a , Anass Bouharab a , David J. Stephens a , Martijn Meeter a , Wouter Kruijne b\n\nAbstract\n\nwarned reaction time task\nthe warning stimulus (S1) initiates a process of temporal preparation, which promotes a speeded response to the impending target stimulus (S2)\nparticipants learn the timing of S2 by storing a [memory trace](memory trace.md) on each trial, which contains a temporal profile of the events on that trial.\nOn each new trial, S1 serves as a retrieval cue that implicitly and associatively activates memory traces created on earlier trials, which jointly drive temporal preparation for S2\ntwo different S1s were associated with two different distributions of S1-S2 intervals: one with predominantly short and one with predominantly long intervals\nExperiments differed regarding the S1 features that made up a pair, ranging from highly distinct (e.g., tone and flash) to more similar (e.g., red and green flash) and verbal (i.e., “short” vs “long”).\nThis cueing effect persisted in a subsequent transfer phase, in which the contingency between S1 and the timing of S2 was broken – a fact participants were informed of in advance\nthese findings support the role of S1 as an implicit retrieval cue, consistent with MTP.\nA multiple trace theory of temporal preparation\nIn this paradigm, the researcher varies, within a block of trials, the duration of the foreperiod between a warning stimulus (S1) and a target stimulus (S2), and measures the participant’s response time (RT) with respect to S2.\nas the foreperiod increases, mean RT decreases toward an asymptote (e.g., Niemi &amp; N ̈ aat  ̈ anen,  ̈ 1981; Woodrow, 1914), indicating a gradual growth of temporal preparation toward a maximum\nexponential (“nonageing”) distribution\nfrequency of consecutive foreperiods decreases according to a fixed rate\nand the RT – foreperiod function has been shown to be approximately flat\nMTP, which makes three main assumptions.\nwithin-trial processing dynamics\ndetection of S1 prompts a preactivation of task relevant effectors, which is counteracted throughout the foreperiod by a process of continuous inhibition\nInhibition is lifted when S2 is presented, allowing activation to drive response execution\nwhen transcranial magnetic stimulation is applied to human motor cortex, the motor evoked potential measured at the corresponding effector has been shown to be smaller during the foreperiod than at baseline, prior to S1 onset\nSince this reduced activation has been found for all potential effectors in a choice reaction task, it has been argued to reflect a general mechanism of impulse control that prevents premature response\n\ntrace formation\n\nunique [memory trace](memory trace.md) is created on each trial, which contains the temporal profile of inhibition (during the foreperiod) and activation (after S2 occurrence) experienced on that trial, along with representations of S1, S2, and the response to S2\neach new [memory trace](memory trace.md) is added to an accumulating pool of memory traces created on earlier trials\nthese traces vary in strength\nstrength of each trace is maximal upon its formation and gradually reduces toward an asymptotic value as it grows older\n\ntrace expression\n\npreviously formed memory traces jointly determine the state of temporal preparation during the ongoing foreperiod\nprocess is initiated on each trial by the presentation of S1\nas the foreperiod elapses, each retrieved trace contributes to preparation in accordance with its strength and its momentary value of activation or inhibition\nat each moment during the foreperiod the state of preparation is determined by the ratio of the weighted activation over inhibition values aggregated across memory traces\nwe assume that the state of temporal preparation reached at the moment of S2 presentation determines RT according to an inversely proportional function\nThis relationship can be appreciated by conceiving the state of temporal preparation as the distance of potential neural excitability relative to a fixed motor-action limit\nthe consecutive foreperiods occur with a ratio of 1:2:4:8, temporal preparation is very low just after the presentation of S1 in view of the low ratio of activation over inhibition across memory traces.\nThese dynamics thus give rise to the typically observed steep RT – foreperiod functio\nIn the case of an exponential distribution (Fig. 1B), where consecutive foreperiods occur with a ratio of 8:4:2:1, activation starts to dominate inhibition quickly after the presentation of S1.\nThus, preparation is already close to ceiling by the time the shortest foreperiod has elapsed and it remains at that level if the foreperiod lengthens (Fig. 1D), yielding the characteristically flat RT – foreperiod function\n\nImages\n\n\n\n\n\n\n\n\n\n\n\n"},"KB/There-and-back-again":{"title":"There and back again","links":[],"tags":["explainability"],"content":"There and back again\n\n@rebuffiThereBackAgain2020\n"},"KB/Thesis-Flow":{"title":"Thesis Flow","links":["KB/Attentive-CutMix","KB/AttributeMix","KB/AugMix","KB/GridMask","KB/Hide-and-Seek","KB/Image-Mixing-and-Deletion","KB/Intra-Class-Part-Swapping","KB/Puzzle-Mix","KB/KeepAugment","KB/RICAP","KB/RandAugment","KB/Random-Distortion","KB/Random-Erasing","KB/ReMix","KB/ResizeMix","KB/SMOTE","KB/SaliencyMix","KB/Sample-Pairing","KB/Visual-Context-Augmentation","KB/SmoothMix","KB/SnapMix","KB/SpecAugment","KB/Co-Mixup","KB/Cut-and-Mix","KB/Data-Augmentation-via-Latent-Space-Interpolation-for-Image-Classification","KB/Data-Augmentation-with-Curriculum-Learning","KB/XAI","KB/Adaptive-Whitening-Saliency","KB/Bayesian-Rule-List","KB/CAM","KB/Conductance","KB/DeconvNet","KB/Deep-Inside-Convolutional-Networks","KB/Deep-Visual-Explanation","KB/DeepFool","KB/DeepLIFT","KB/Dynamic-visual-attention","KB/Embedding-Human-Knowledge-into-Deep-Neural-Network-via-Attention-Map","KB/Generalizing-Adversarial-Explanations-with-Grad-CAM","KB/Graph-based-visual-saliency","KB/Guided-BackProp","KB/Guided-GradCAM","KB/Influence-of-image-classification-accuracy-on-saliency-map-estimation","KB/Integrated-Gradients","KB/Interpretation-of-Neural-networks-is-fragile","KB/Noise-Tunnel","KB/On-the-overlap-between-Grad-CAM-saliency-maps-and-explainable-visual-features-in-skin-cancer-images","KB/RISE","KB/Real-Time-Image-Saliency-for-Black-Box-Classifiers","KB/SAM-ResNet","KB/SDR","KB/SP-LIME","KB/Salience-Map","KB/Smooth-Grad","KB/SmoothGrad-Square","KB/Summit","KB/VarGrad","KB/ScoreCAM","KB/Beware-of-Inmates-Running-the-Asylum","KB/The-Unreliability-of-Saliency-Methods","KB/Res-Net","KB/Vision-Transformer","KB/ConvNeXt","KB/Mobile-Net","KB/Vgg","KB/Xception"],"tags":[],"content":"Augmentation\nMethods\n\nAttentive CutMix\nAttributeMix\nAugMix\nGridMask\nHide and Seek\nImage Mixing and Deletion\nIntra-Class Part Swapping\nPuzzle Mix\nKeepAugment\nRICAP\nRandAugment\nRandom Distortion\nRandom Erasing\nReMix\nResizeMix\nSMOTE\nSaliencyMix\nSample Pairing\nVisual Context Augmentation\nSmoothMix\nSnapMix\nSpecAugment\nCo-Mixup\nCut and Mix\nData Augmentation via Latent Space Interpolation for Image Classification\nData Augmentation with Curriculum Learning\n\nDisadvantages\nXAI\nMethods\n\nAdaptive Whitening Saliency\nBayesian Rule List\nCAM\nConductance\nDeconvNet\nDeep Inside Convolutional Networks\nDeep Visual Explanation\nDeepFool\nDeepLIFT\nDynamic visual attention\nEmbedding Human Knowledge into Deep Neural Network via Attention Map\nGeneralizing Adversarial Explanations with Grad-CAM\nGraph-based visual saliency\nGuided BackProp\nGuided GradCAM\nInfluence of image classification accuracy on saliency map estimation\nIntegrated Gradients\nInterpretation of Neural networks is fragile\nNoise Tunnel\nOn the overlap between Grad-CAM saliency maps and explainable visual features in skin cancer images\nRISE\nReal Time Image Saliency for Black Box Classifiers\nSAM-ResNet\nSDR\nSP-LIME\nSalience Map\nSmooth-Grad\nSmoothGrad Square\nSummit\nVarGrad\nScoreCAM\n\nDisadvantages\n\nBeware of Inmates Running the Asylum\nThe Unreliability of Saliency Methods\n\nArchitectures\n\nRes Net\nVision Transformer\nConvNeXt\nMobile Net\nVgg\nXception\n"},"KB/Theta-Waves":{"title":"Theta Waves","links":[],"tags":["brain"],"content":"Theta Waves\n\n4-9 Hz theta\nMemory/Decision\n\n"},"KB/Threaded-Cognition":{"title":"Threaded Cognition","links":[],"tags":["cognitivemodel"],"content":"Threaded Cognition\n\nAka no real “switch” between one task to next but bottlenecks\nCognition is Threaded but overlaps in the sense that different areas can not be accessed if it uses same areas as the other task at the same time\nIf no expertise, consult Declarative memory all the time. Until it’s not required anymore if in long term store.\n"},"KB/Thrombosis":{"title":"Thrombosis","links":[],"tags":["medical"],"content":"Thrombosis\n\nA blood clot that forms inside a blood vessel restricting blood flow\n"},"KB/Through-beam":{"title":"Through-beam","links":[],"tags":["robotics"],"content":"Through-beam\n\nAn object detection system used within a robot’s imaging sensor system. A finely focused beam of light is mounted at one end and a detector at the other. When the beam of light is broken, an object is sensed.\n"},"KB/Tightly-coupled-code":{"title":"Tightly coupled code","links":[],"tags":["programming"],"content":"Tightly coupled code\n \n\nHard to modify\nLook at where the data flows\n"},"KB/Time-Dependant-Vector-Field":{"title":"Time Dependant Vector Field","links":["KB/Lagrangian-Coherent-Structure"],"tags":["visualization"],"content":"Time Dependant Vector Field\n\nLagrangian Coherent Structure\n"},"KB/Time-Measuring-Function":{"title":"Time Measuring Function","links":[],"tags":["robotics"],"content":"Time Measuring Function\n\nTime measuring function measures the execution time for the specified section in the job or the signal output time of the specified signal.\n"},"KB/Time-space-duality":{"title":"Time space duality","links":[],"tags":["parallelcomputing"],"content":"Time Space Duality\n\nArray : Instruction operates on multiple data elements at the same time\nVector : Instruction operates on multiple data elements in consecutive time steps\n"},"KB/Time":{"title":"Time","links":[],"tags":["physics"],"content":"Time\n\n\\Delta t = t_{f}-t_{i}\n"},"KB/TinyBERT":{"title":"TinyBERT","links":["KB/Knowledge-Distillation","KB/GLUE"],"tags":["architecture"],"content":"TinyBERT\n\nTinyBERT: Distilling BERT for Natural Language Understanding\nnovel Transformer distillation method to accelerate inference and reduce model size while maintaining accuracy\nspecially designed for Knowledge Distillation (KD) of the Transformer-based models\nplenty of knowledge encoded in a large teacher BERT can be effectively transferred to a small student Tiny-BERT\nGLUE\n"},"KB/Token-Embedding":{"title":"Token Embedding","links":["KB/Embedding","KB/Tokenizer"],"tags":["architecture"],"content":"Token Embedding\n\nWordPiece Tokenizer\n"},"KB/Tokenizer":{"title":"Tokenizer","links":["KB/Features","pronoun","KB/Verb","KB/Word-Structure","KB/Punctuation","Sub-word-tokenization"],"tags":["language"],"content":"Tokenizer\n\nTokenizer expands the contraction to recover the essential grammatical Features of the pronoun and the Verb.\nSpace-delimited languages\nWhite space delimited tokens may not be the valid token\nChinese and Thai\n\nWords are written in succession with no indication of word boundaries\n\n\nWord Structure\nPunctuation\nSub word tokenization\n"},"KB/Top-Down-Parsing":{"title":"Top Down Parsing","links":[],"tags":["language"],"content":"Top Down Parsing\n\n\n"},"KB/Tourette’s-Syndrome":{"title":"Tourette’s Syndrome","links":[],"tags":["brain"],"content":"Tourette’s Syndrome\n\nA neurological disorder, beginning in childhood, characterized by repetitive, involuntary movements or vocalizations, called tics.\n"},"KB/Towards-A-Rigorous-Science-of-Interpretable-Machine-Learning":{"title":"Towards A Rigorous Science of Interpretable Machine Learning","links":[],"tags":["explainability"],"content":"Towards A Rigorous Science of Interpretable Machine Learning\n\n@doshi-velezRigorousScienceInterpretable2017\n"},"KB/Tower":{"title":"Tower","links":[],"tags":["temp"],"content":"Tower\n\nA component of a deep neural network that is itself a deep neural network without an output layer. Typically, each tower reads from an independent data source. Towers are independent until their output is combined in a final layer.\n"},"KB/Traces-of-times-past-Representations-of-temporal-intervals-in-memory":{"title":"Traces of times past Representations of temporal intervals in memory","links":[],"tags":["cognitivemodel"],"content":"Traces of times past Representations of temporal intervals in memory\n\n\nNiels Taatgen &amp; Hedderik van Rijn\n\n\nThe results show that the adjustment of one interval carried over to the other interval, indicating that subjects were not able to completely separate the two representations\n\n\nassumes that the representation of an interval is based on a pool of recent experiences\n\n\nRepresentations of time\n\nThe distinction between a solid memory representation for both durations, on the one hand, and a “pool of experiences,” on the other, does not need to be problematic for the comparison process, if one assumes that both approaches eventually result in the retrieval of a single representation that can be compared to the current clock value.\n\nBayesian modeling approach by Jazayeri and Shadlen (2010)\n\nThe assumption of their model is that humans take two factors into account when estimating the duration of an interval\nOne of these factors is the temporal context: What is the range of possible durations for an interval? The second factor is the (self-)knowledge about the imprecision involved in estimating this interval\nVisual inspection of the results suggests, indeed, that in the FF condition the short interval was estimated as longer and the long interval as shorter, consistent with earlier findings (e.g., Grondin, 2005), and suggesting that both estimates influence each other.\nestimates of the short interval were influenced by changes in the duration of the long interval, because the short interval’s estimations resemble a dampened pattern of the long interval\nclear interaction between condition and range\nLinear mixed-effect models provided information about the contributions of individual factors to a dependent variable and about the reliability of the estimates.\nPrevious feedback also modifies the interval: If the feedback on the previous short interval was “too short,” the estimate is increased by 92 ms, but when it was “too long,” it is decreased the current estimate by 106 ms\nrepresentation of an interval is the result of a pool of recent experiences and not of a single representation\nThis is indicated by the relatively small intercepts of the regression formula and the susceptibility of the estimates to changes in the other interval\n\nGeneral discussion\n\nThe results not only show that the representations of two intervals tend to shift toward each other, but also that a change in the duration of one interval not only affects the representation of that interval, but also the representation of the unchanged interval.\nThese findings support a model in which the representation of a time interval is not a single [memory trace](memory trace.md), but a pool of experiences in which recency and match to the current request determine the impact of single experiences.\nThe main specific choice we made in this model was to treat every experience with each of the intervals as a separate [memory trace](memory trace.md)\nHowever, we could not come up with a modification of the perturbation system that could (1) produce relatively stable performance for both durations, (2) adjust itself to changes in the standard, and (3) show influences of the changed long interval on the short interval.\nAs already mentioned in the introduction, the fit to the data does not hinge on the linear or nonlinear representation of time in the clock component—as long as a clock component produces temporal information, the pool model would be able to produce new temporal estimates\na combination of a memory system and a time estimation system can explain the data discussed in this study, despite the fact that neither system was specifically designed for these experiments.\n\nImages\n\n\n\n\n\n\n"},"KB/Tracking-the-Continuity-of-Language-Comprehension-Computer-Mouse-Trajectories-Suggest-Parallel-Syntactic-Processing":{"title":"Tracking the Continuity of Language Comprehension Computer Mouse Trajectories Suggest Parallel Syntactic Processing","links":["KB/Syntax-First-models","KB/Multiple-constraint-based-theories","KB/Unrestricted-Race-Model"],"tags":["language"],"content":"Tracking the Continuity of Language Comprehension Computer Mouse Trajectories Suggest Parallel Syntactic Processing\n\nThomas A. Farmera, Sarah A. Cargilla, Nicholas C. Hindya, Rick Daleb, Michael J. Spiveya\n\nAbstract\n\nAlthough several theories of online syntactic processing assume the parallel activation of multiple syntactic representations, evidence supporting simultaneous activation has been inconclusive\ncontinuous and non-ballistic properties of computer mouse movements are exploited\nprocure evidence regarding parallel versus serial processing\nParticipants heard structurally ambiguous sentences while viewing scenes with properties either supporting or not supporting the difficult modifier interpretation\nThe curvatures of the elicited trajectories revealed both an effect of visual context and graded competition between simultaneously active syntactic representations\n\nIntroduction\n\nSentences such as, “The adolescent hurried through the door tripped,” are difficult to process because, at least temporarily, multiple possible structural representations exist\ngarden-path effect\nSyntax First models\nMultiple constraint-based theories\nwhat feel like garden-path effects are due to the incorrect syntactic alternative winning much of the competition during the early portion of the sentence, and then nonconforming information from the latter portion of the sentence inducing a laborious reversal of that activation pattern\nAs a result, one can expect that some garden-path events may be very mild, some moderate, and some extreme such that a wide variety of sentence readings should all belong to one population of events with a relatively continuous distribution.\nUnrestricted Race Model\nWhen ambiguous sentences like 1a are heard in the presence of visual scenes where only one possible referent is present (an apple already on a towel), along with an incorrect destination (an empty towel), and a correct destination (a box), as in the top portion of Fig. 1, about 50% of the time participants fixate the incorrect destination after hearing the first PP.\nAfter the second disambiguating PP is heard, eye movements tend to be redirected to the correct referent and then to the correct destination\nThis garden-path effect can, however, be modulated by contextual information contained within the visual scene\nit seems that when two possible referents are present, an expectation is created that they will be discriminated amongst, thus forcing a modifier interpretation of the\nambiguous PP\nThe attenuation of looks to the incorrect destination by the presence of two possible referents, then, is evidence for an early influence of non-syntactic (even nonlinguistic) information on the parsing process and is problematic for traditional syntax-first accounts discussed earlier.\nHowever, because saccadic eye movements are generally ballistic, they either send the eyes to fixate an object associated with a garden-path interpretation or they do not.\nThe evidence from this paradigm, therefore, is also consistent with the Unrestricted Race model, where the various constraints are combined immediately, but on any given trial only one syntactic representation is initially pursued\nacross experimental trials, distributions of eye-movement patterns are almost always bimodal because the fixations are coded as binomial\nThere are saccades to locations on the display corresponding to either one of the possible representations, but almost never to a blank region in between those two potential targets\nIn the following experiment, we examined the dynamics of hand movement in the same sentence comprehension scenario with the goal of determining whether the non-ballistic, continuous nature of computer mouse trajectories can serve to tease apart these two remaining theoretical accounts.\n\nExperiment\n\ncomputer mouse movements can serve as an informative indicator of the cognitive processes underlying spoken-word recognition (Spivey, Grosjean, &amp; Knoblich, 2005)\nIn addition, whereas self-paced reading affords 2 to 3 data points (button presses) per second, and eye-movement data allow for approximately 3 to 4 data points (saccades) per second, “mouse tracking” yields somewhere between 30 and 60 data points per second, depending on the sampling rate of the software used.\nThe context and garden-path effects reported in the visual world paradigm are highly replicable when tracking eye movements\nAs such, recording mouse movements in the visual world paradigm can serve as a strong test case by which to evaluate the efficacy of the mouse-tracking procedure for the study of language processing in real time.\n\nExpected Prediction\n\n\n\nAveraged trajectories recorded in response to ambiguous sentences in the onereferent context should show significantly more curvature toward the incorrect destination than the averaged trajectories elicited by unambiguous sentences—a pattern corresponding to the garden-path effect. 2)\n\n\nThe curvature of averaged trajectories in the two referent condition should not differ statistically between ambiguous and unambiguous sentences, thus demonstrating an influence of referential context on the garden-path effect.\nThe second purpose of this study, then, was to exploit the continuity of the mousemovement trajectories to discriminate between these two remaining theoretical accounts\na measure of curvature magnitude was used to determine the amount of spatial attraction toward the incorrect destination that was exhibited by the ambiguousand unambiguous-sentence trajectories in the one-referent context.\nIf only one representation were active at any one time, as the unrestricted race account predicts, then the trial-by-trial distribution of trajectory curvatures in the ambiguous-sentence condition should be either (a) bimodal—comprised of highly curved garden-path movements and noncurved, correct-interpretation movements, or (b) uniformly in the more extreme curved range, indicating that almost every trial exhibited a garden-path effect\n\nMethod\nParticipants\n\nForty right-handed, native English-speaking undergraduates from Cornell University participated in the study for extra credit in psychology courses\nonly right-handed individuals to avoid variability associated with subtle kinematic differences in leftward and rightward movement of the left versus the right arms.\n\nMaterials and Procedures\n\nSixteen experimental items 102 filler sentences\nThe unambiguous version (1b) of each of the 16 experimental items was recorded first, and then the “that” was removed to produce the ambiguous (1a) sentence condition\nEach visual context corresponding to the 16 experimental items was varied to produce a oneand two-referent condition\nThe one-referent visual context (illustrated in Fig. 1, top) contained the target referent (an apple on a towel), an incorrect destination (a second towel), the correct destination (a box), and a distracter object (a flower). In the two-referent context, all items were the same except that the distracter object was replaced with a second possible referent (such as an apple on a napkin). Twenty-four filler scenes, designed to accompany filler sentences, were also constructed.\nIn critical trials for both the oneand two-referent conditions, the target referent always appeared in the top left corner of the screen, the incorrect destination always appeared in the top right corner of the screen, and the correct destination\nwas always located at the bottom right portion of the screen.\nGiven that the scene layout was held constant across all items in each experimental condition, a left-to-right movement was always necessary\nAlthough there could exist a systematic bias toward specific locations in the display when moving rightward, this was viewed as unproblematic given that the bias would be held constant across both the ambiguous and unambiguous sentences, which were directly compared in all statistical analyses, for each context.\nIn each scene, participants saw four to six color images, depending on how many objects were needed for the scene\n\nResults\nData Screening and Coding\n\nA trajectory was considered valid and submitted to further analysis if it was initiated at the top left quadrant of the display and terminated in the bottom right quadrant, indicating that the correct referent had been picked up and then placed at the correct destination. This screening procedure resulted in 27 deleted trials, accounting for less than 5% of all experimental trials.\nTo make sure that trajectories in one condition were not initiated (or that objects were not grabbed) at a systematically different region of the display than in the other conditions, we conducted two 2 (Context) × 2 (Ambiguity) ANOVAs on the x and y coordinates, separately.\nThere was no significant main effect or interaction for either the x or the y coordinates (all ps were nonsignificant) indicating that, across conditions, the trajectories were initiated at approximately the same location of the display\nSubsequently, all analyzable trajectories were “time normalized” to 101 timesteps by a procedure described in Spivey et al. (2005) and Dale et al. (2007).\n\nContext and Garden-path Effects\n\nThe mean trajectories from ambiguous and unambiguous sentences in the onereferent context, illustrated in Fig. 1 (top), demonstrate that the average ambiguoussentence trajectory was more curved toward the incorrect destination than the average trajectory elicited by the unambiguous sentences\nThus, in the presence of the garden-path effect, it seems clear that there exists more spatial attraction toward the incorrect destination for the ambiguous sentences.\nIn addition, in line with the time-normalized analyses presented above, none of the\neight time bins in the two-referent context showed the ambiguousand unambiguous-sentence trajectories significantly diverging for either the x or the y coordinates.\n\nSerial Versus Parallel Activation\n\ngarden-path trials and some non-garden-path trials, the majority of the trajectories elicited in this condition fell somewhere in between those two extremes, forming a single population of non-, somewhat-, and highly curved responses.\nTo determine whether any bimodality is present in the distribution of responses, we computed the area under the curve on a trial-by-trial basis\nThe b value for each distribution is less than .555, indicating no presence of bimodality within the distributions.\nNotably, with regard to the distribution of responses in the one-referent, ambiguous-sentence condition, b &lt; .555 indicates that the graded spatial attraction effects elicited in this condition came not from two different types of trials but from a single population of trials.\nFinally, one might argue that bimodality was not detected (thus, b &lt; .555) in the crucial one-referent, ambiguous-sentence condition due to a lack of statistical power resulting from the relatively small number of trials in the garden-path distribution.\nTo address this concern, we created an artificial distribution with a sample size almost identical to our crucial gardenpath distribution by randomly sampling 50% of the trials from the one-referent, ambiguoussentence condition (where garden pathing was observed) and 50% of the trials from the onereferent, unambiguoussentence condition.\nBy examining the distributional properties of the area-under-the-curve values produced by the garden-path and non-garden-path trials together, we can thus determine whether the bimodality statistic (b) we used to assess the bimodality of the garden-path distribution (above) is capable of detecting bimodality in a case where the response distribution should clearly be bimodal\n\nGeneral Discussion\n\nIn the one-referent context, participants’ mouse movements in response to the ambiguous sentences curved significantly closer to the top right of the screen (toward the incorrect destination) than in response to unambiguous sentences.\nThus, it would seem that when only one referent was present, the incorrect destination (e.g., the towel) was partially considered relevant, until disambiguating information was processed—a trend corresponding to the garden-path effect associated with this condition.\nThe fact that most mouse trajectories began while the speech file was still being heard suggests that the effect of visual context modulating the garden path took place during early moments of processing the linguistic input, not during a second stage of syntactic reanalysis.\nIn addition, by capitalizing on the continuous, non-linear, and non-ballistic properties of trajectories produced by computer mouse movements, mouse tracking has the potential to answer questions that have been difficult to answer with more traditional methodologies.\nWhat does distinguish between these two accounts is the gradiency observed in the curvature of the trajectories in the garden-path condition\nIf the Unrestricted Race model posits that only one syntactic representation is pursued at any one time, then it must predict that mouse movements in a gardenpath condition should initially move either in the direction of the correct destination or in the direction of the incorrect destination (producing either a bimodal distribution or an all-curved distribution)\nIn contrast, because the constraintbased account posits simultaneous graded activation of multiple syntactic alternatives, it predicts that mouse movements can move in directions that are dynamically weighted combinations of the two competing destinations (producing a unimodal distribution of moderate curvatures).\nFig. 4 shows that although approximately 5% of the trajectories moved all the way to the incorrect destination before changing direction, the vast majority of the trajectories responsible for the mean curvature were unmistakably graded in their degree of spatial attraction toward the incorrect destination.\nThe lack of bimodality in the distribution of trial-by-trial trajectory curvatures suggests that the garden-path effect so frequently associated with this manipulation is not an all-or-none phenomenon—that is, the activation of one structural representation does not forbid simultaneous activation of other possible representations\nThrough a large-scale survey of children’s computer use, for example, Calvert, Rideout, Woolard, Barr, and Strouse (2005) found that the mean age at which a child was able to point and click a computer mouse was 3.5 years, and that the mean age of the onset of autonomous computer use was 3.7 years\nwe believe mouse tracking can serve as “the poor man’s eye tracker,” providing detailed indices of cognitive processing to laboratories that cannot afford expensive eye-tracking equipment.\n\nPictures\n\n\n\n\n\n\n\n"},"KB/Tractability":{"title":"Tractability","links":[],"tags":["temp"],"content":"Tractability\n\nLet X be the input and Z be the latent representation of X. Every generative model makes the assumption that it’s tractable to compute the probability P(Z | X).\n"},"KB/Training-Trajectories":{"title":"Training Trajectories","links":[],"tags":["explainability"],"content":"Training Trajectories\n\n\n"},"KB/Training-serving-Skew":{"title":"Training-serving Skew","links":[],"tags":["temp"],"content":"Training-serving Skew\n\nThe difference between a model’s performance during training and that same model’s performance during serving.\n"},"KB/Trajectory-Planning":{"title":"Trajectory Planning","links":[],"tags":["robotics"],"content":"Trajectory Planning\n\nScheduled motion to follow, including time information\nAfter finding a path, the trajectory definition is completed by the choice of a timing law\n"},"KB/Trajectory-Plotting-with-PCA":{"title":"Trajectory Plotting with PCA","links":["KB/PCA"],"tags":["explainability"],"content":"Trajectory Plotting with PCA\n\nThe problem: the path lies in a low-dimensional space.\nThe solution: PCA\nConstruct a matrix $$\nM = [\\theta_{0}- \\theta_{n};…;\\theta_{n-1}-\\theta_{n}]\n\n- where,\n- $\\theta_{i}$ is the model params during epoch i\n- n is number of epochs\n- apply PCA to M and use 2 most explanatory directions\n- ![](../images/Pasted%20image%2020230327130326.png)"},"KB/Trajectory":{"title":"Trajectory","links":["KB/Reinforcement-Learning"],"tags":["temp"],"content":"Trajectory\n\nIn Reinforcement Learning, a sequence of tuples that represent a sequence of state transitions of the agent, where each tuple corresponds to the state, action, reward, and next state for a given state transition.\n"},"KB/Transcranial-Electrical-Stimulation-(tDCS-and-tACS)":{"title":"Transcranial Electrical Stimulation (tDCS and tACS)","links":[],"tags":["brain"],"content":"Transcranial Electrical Stimulation (tDCS and tACS)\n\nA non-invasive procedure that applies electrical stimulation to the scalp to increase or decrease neural signaling. The two main types are direct current stimulation (tDCS) and alternating current stimulation (tACS). They are used for therapeutic purposes as well as to study cognitive processing.\n"},"KB/Transcranial-Magnetic-Stimulation-(TMS)":{"title":"Transcranial Magnetic Stimulation (TMS)","links":[],"tags":["brain"],"content":"Transcranial Magnetic Stimulation (TMS)\n\nA non-invasive procedure that uses the energy from a strong magnet to stimulate changes in neural processing from above the scalp. It is used as a treatment for depression as well as a research method to investigate cognitive processes.\n"},"KB/Transducer":{"title":"Transducer","links":[],"tags":["robotics"],"content":"Transducer\n\nA device that converts energy from one form to another. Generally, a device that converts an input signal into an output signal of a different form. It can also be thought of as a device which converts static signals detected in the environment (such as pressure) into an electrical signal that is sent to a robot’s control system.\n"},"KB/Transductive-Models":{"title":"Transductive Models","links":["KB/Semi-Supervised"],"tags":["graph"],"content":"Transductive Models\n \nTransductive Models\n\nconsiders both the labeled and unlabeled data at the same time\nSemi Supervised\nIt does not produce a rule but merely a labeling for the unknown outputs\nHowever, it has the disadvantage that the model needs to be retrained when extra unlabeled data are added.\n"},"KB/Transfer-Function":{"title":"Transfer Function","links":["KB/Features","KB/Opacity-Correction"],"tags":["visualization"],"content":"Transfer Function\n\nMap a scalar to color and opacity\nDetermines which Features of the data are visible / highlighted\nCan be stored inside a color lookup table (LUT)\n\n\nOpacity Correction\n"},"KB/Transfer-Learning-or--Self-supervised-Learning-A-Tale-of--Two-Pretraining-Paradigms":{"title":"Transfer Learning or  Self-supervised Learning? A Tale of  Two Pretraining Paradigms","links":[],"tags":["deeplearning"],"content":"Transfer Learning or Self-supervised Learning? A Tale of Two Pretraining Paradigms\n\n@yangTransferLearningSelfsupervised2020\n[Transfer Learning]  vs [Self Supervised](Transfer Learning]  vs [Self Supervised.md)\n\nComparison Using\n\n5 different image-based source domains\n4 target tasks\n\nfrom daily-life objects\ngeneral scenes\nnature\nmedical pictures areas.\n\n\nFour different experimental setups\n\n\nEffect of domain difference between source and target task\nEffect of amount of pretraining data\nEffect of class imbalance in source data\nEffect of using target data for additional pretraining\n\n\nResNet-50\n\nResults\nDomain Difference\n\nLarge\n\nSSL outperforms TL\n\n\nSmall\n\nTL outperforms SSL\nSSL is less sensitive to domain difference than TL.\n\n\n\nAmount of Pretraining Data\n\nSmall(for same source task)\n\nSSL outperforms TL\n\n\nLarge(for same source task)\n\nTL outperforms SSL\nSSL is less sensitive to amount of pretraining data than TL, when domain difference is small\n\n\n\nClass Imbalance (Source Data)\n\nSSL is more robust to class imbalance than TL\n\nAdditional Pretraining\n\nFor SSL, using target task for additional pretraining works better vs using only source data, but not for TL.\n\nWhat is Left\n\ncan be extended to other forms of data including speech, signals and text\nOnly used ResNet architecture, need to investigate other architectures\nImage Transformers etc. are not considered\nCorrelation between performance and factors is studied and potential reasons behind it are discussed, a deeper investigation of these potential reasons might be beneficial\n"},"KB/Transfer-Learning":{"title":"Transfer Learning","links":[],"tags":["temp"],"content":"Transfer Learning\n\nTransfer learning involves extrapolating a reward function for a new environment based on reward functions from many similar environments that might then transfer in a wrong way\nUses source task with human assigned labels\nBiased to human assigned labels, more discriminative\n\nRefs\n\nopenai\n"},"KB/Transferability":{"title":"Transferability","links":[],"tags":["explainability"],"content":"Transferability\n\nExplainability is also an advocate for transferability, since it may ease the task of elucidating the boundaries that might affect a model, allowing for a better understanding and implementation\nthe mere understanding of the inner relations taking place within a model facilitates the ability of a user to reuse this knowledge in another problem\ncases in which the lack of a proper understanding of the model might drive the user toward incorrect assumptions and fatal consequences\n"},"KB/Transferred-compact-convolutional-filters":{"title":"Transferred compact convolutional filters","links":[],"tags":["knowledgedistillation"],"content":"Transferred Compact Convolutional Filters\n\nThese methods remove inessential parameters by transferring or compressing the convolutional filters (Zhai et al., 2016).\n"},"KB/Transformer-Physics":{"title":"Transformer Physics","links":[],"tags":["physics"],"content":"Transformer Physics\n\nratio of the voltages across the coils of a transformer = the ratio of the turns on the coils\n\\frac{V_{1}}{V_{2}}= \\frac{N_{1}}{N_{2}}\n"},"KB/Transformer-XL":{"title":"Transformer-XL","links":["KB/Billion-Word"],"tags":["architecture"],"content":"Transformer-XL\n\nTransformer-XL: Attentive Language Models Beyond a Fixed-Length Context\nTransformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling\nlearning dependency beyond a fixed length without disrupting temporal coherence\nsegment-level recurrence mechanism and a novel positional encoding scheme\nresolves the context fragmentation problem\nenwiki8\nWikiText\nOne Billion Word\nPenn Treebank\n"},"KB/Transformer":{"title":"Transformer","links":["KB/Attention","KB/Basic-Transformer"],"tags":["architecture"],"content":"Transformer\n\nEncoder Decoder\nAuto regressive : decoder outputs fed back as inputs to decoder\nDecoder can access not only the hidden step of the last time step from the encoder, but all the hidden states from the encoder\nDuring decoding, consider pairwise relationshop between decoder state and all the returned states from the encoder\n\nSome words relevant, others are not\n\n\nTransform all hidden states from the encoder into context vectors, that shows how the decoding step is relevant to the input sequences\nAttention\nBasic Transformer\n\nNice Little Blogs\n\nlillog\n"},"KB/Transitional-probabilities":{"title":"Transitional probabilities","links":[],"tags":["language"],"content":"Transitional Probabilities\n\n\\text{Prob of }Y|X = \\frac{\\text{freq of} XY}{\\text{freq of }X}\nLonger listening times for non-words indicates recognition of words\nTransitional probabilities can be high within words, and low at word boundaries as in Aslin et al\nBut then you can also manipulate the frequency at which each word occurs, and in doing so, also the frequency of the syllables\nGraf Estes et al\nTransition frequencies can be made high because two words occur very often next to each other\n"},"KB/Transitive-verb":{"title":"Transitive verb","links":["KB/Verb","noun"],"tags":["language"],"content":"Transitive Verb\n\na Verb with a direct noun object\nI cooked a duck belonging to her\n"},"KB/Translational-Invariance":{"title":"Translational Invariance","links":[],"tags":["temp"],"content":"Translational Invariance\n\nIn an image classification problem, an algorithm’s ability to successfully classify images even when the position of objects within the image changes. For example, the algorithm can still identify a dog, whether it is in the center of the frame or at the left end of the frame.\n"},"KB/Transparency":{"title":"Transparency","links":[],"tags":["explainability"],"content":"Transparency\n\na model is considered to be transparent if by itself it is understandable. Since a model can feature different degrees of understandability, transparent models in Section 3 are divided into three tags: simulatable models, decomposable models and algorithmically transparent models\nunderstandability is a two-sided matter: model understandability and human understandability in @gunningExplainableArtificialIntelligence\n"},"KB/Transposed-Conv":{"title":"Transposed Conv","links":[],"tags":["architecture"],"content":"Transposed Conv\n\nuse a transformation going in the opposite direction of a normal convolution, i.e., from something that has the shape of the output of some convolution to something that has the shape of its input while maintaining a connectivity pattern that is compatible with said convolution.\nUpsampling\nInput i, kernel k, padding p, stride s , o = (i-1) \\times s +k -2p\nSteps\n\nCalculate new Param’s z and p’\nBetween each row and columns of the input, insert z number of zeros. This increases the size of the input to (2*i -1) \\times (2*i -1)\nPad the modified image with p’ no of zeros\nStandard conv with stride of 1\n\n\n\n"},"KB/Trapezoidal-Trajectory":{"title":"Trapezoidal Trajectory","links":[],"tags":["robotics"],"content":"Trapezoidal Trajectory\n\n\n"},"KB/Treemap":{"title":"Treemap, Node Link, Stacking","links":[],"tags":["visualization"],"content":"Treemap\n\n\n"},"KB/Trees":{"title":"Trees","links":["KB/Decision-Trees","KB/Node-Link-Diagram"],"tags":["temp"],"content":"Trees\n\nDecision Trees\n\nNode Link Diagram\n"},"KB/Triplet-Loss":{"title":"Triplet Loss","links":["Tag-Pages/loss","KB/FaceNet","Harmonic-Triplet-Loss"],"tags":["loss"],"content":"Triplet loss\n\nGiven an achor, pull similar closer and push dissimilar away\nFace recog FaceNet\nAnchor, positive sample are neigbors while neg isnt\n\nFor each triplet, this condition must hold $$||f(x^a) - f(x^p)||^2 + \\alpha \\gt f(x^a) - f(x^n)||^2$\n\\alpha is a margin b/w positive and neg\nloss to minimize $$L(\\theta) = \\Sigma_i^n||f(x^a) - f(x^p)||^2 + f(x^a) - f(x^n)||^2 + \\alpha$\nHarmonic Triplet Loss\n"},"KB/Trustworthiness":{"title":"Trustworthiness","links":[],"tags":["explainability"],"content":"Trustworthiness\n\nHowever, declaring a model as explainable as per its capabilities of inducing trust might not be fully compliant with the requirement of model explainability\nTrustworthiness might be considered as the confidence of whether a model will act as intended when facing a given problem\nit does not imply that every trustworthy model can be considered explainable on its own, nor is trustworthiness a property easy to quantify\nTrust might be far from being the only purpose of an explainable model since the relation among the two, if agreed upon, is not reciprocal\n"},"KB/Tug-of-war-between-RAG-and-LLM-prior":{"title":"Tug of war between RAG and LLM prior","links":[],"tags":["architecture"],"content":"Tug of War between RAG and LLM prior\n\n@wuHowFaithfulAre2024\n\nHow Faithful Are RAG Models? Quantifying the Tug-of-war between RAG and LLMs’ Internal prior\n\n(Note : Basically shows that it’s not possible for an LLM to fix it’s own hallucinations. Kinda like, if you twist the data around, then after a point the model stops relying on what it knows and starts believing in all the nonsense)\nin cases when the LLM alone incorrectly answers a question, does providing the correct retrieved content always fix the error?\nin cases where the retrieved content is incorrect, does the LLM know to ignore the wrong information, or does it recapitulate the error?\nsystematically analyze the tug-of-war between a LLM’s internal knowledge (i.e. its prior) and the retrieved information in settings when they disagree\nthe more the modified information deviates from the model’s prior, the less likely the model is to prefer it\nThe likelihood of the LLM to adhere to the retrieved information presented in context (RAG preference rate) is inversely correlated with the model’s confidence in its response without context (its prior probability).\nLLMs will increasingly revert to their priors when the original context is progressively modified with unrealistic values.\n\nMethods\n\nOur main analysis consists of evaluating the RAG question-answering capabilities of GPT-4 when introducing varying levels of perturbations on the RAG documents\n\nConcordance\n\nthe agreement between the reference answer generated based on the article content, and the model’s answer to the corresponding generated question\nThis is computed for both the model’s answer with and without context.\n\nModifying the Retrieved Documents\n\nIn three datasets with numerical answers (Drug Dosages, Sports Records, Latest News), we produce ten modifications that act as multipliers on the original value: 0.1, 0.2, 0.4, 0.8, 1.2, 1.5, 2.0, 3.0, 5.0, 10.0.\nfor a name like Bob Green, a slight modification implies a small tweak to another real name (Rob Greene), whereas a significant modification produces a similar but fictitious name (Bilgorn Grevalle), and a comical modification is an absurd variant (Blob Lawnface).\nBecause of differences in how each modified fact might appear in the retrieved text, we utilize GPT-4 to generate the perturbed excerpts for drug dosages and news. Each modified fact is replaced in the original retrieved text.\n\nRAG Vs Model Prior Analyses\n\nmain analysis we perform in this study is comparing the RAG preference of a model against its internal prior\nThe LLM is first queried with a question without context\nThis response and the average probability of the tokens (accessed via the log probs) are referred to as the model’s prior response and the prior probability, respectively\nThe LLM is then queried again, this time with the retrieved content present in the prompt.\nif the response is still the same as the prior response, then the model prefers its prior\nOn the other hand, if the model response aligns with the information present in the retrieved content, then the model prefers RAG\nFor each dataset, the RAG preference rate is computed as the average across all RAG queries.\nRAG preference rate is compared against two measurements: the prior probability and the deviation from the prior value.\n\nResults\nConcordance\n\nwe observe that the model’s prior response only agreed with the reference answer 34.7% on average\nRAG answers elevated the concordance to 94%\nin the minority of cases where providing the retrieved content fails to correct the LLM, we find that the model simply responds with its original prior answer about 20% of the time.\n\n\nRAG Preference Rate vs. Prior Probability\n\nThe slope indicates the effect of stronger model confidence on the model’s preference for the information presented in the retrieved context; we observe different slopes (ranging from -0.1 to -0.45), suggesting that the effectiveness of RAG in different QA domains can be characterized as being relatively susceptible (e.g., with Dates questions) or robust (e.g., with News questions) to the model’s internal prior knowledge confidence.\n\nRAG Preference Rate Vs Deviation from Prior\n\nas the RAG value diverges from the model’s prior, the model is less likely to adopt the RAG value over its own initial response\n\nEffect of Prompting Technique on RAG Adherence\n\nWe observe lower and steeper drops in RAG adherence with the loose vs strict prompts, suggesting that prompt wording plays a significant factor in controlling RAG adherence.\nwe quantify a tug-of-war between the strength of the model’s prior and the rate at which the model\nadheres to the RAG document’s facts\n"},"KB/Tuning-Model-Flexibility":{"title":"Tuning Model Flexibility","links":["KB/Class-Size","KB/Regularization","KB/Ridge-Regression","KB/Adding-noise","KB/Cross-Validation"],"tags":["regularization"],"content":"Tuning Model Flexibility\n\nClass Size\nRegularization\nRidge Regression\nAdding noise\nCross Validation\n"},"KB/Tutor":{"title":"Tutor","links":[],"tags":["usermodel"],"content":"Tutor\n\nHowever, a tutoring system does not have to replace a teacher or run an after-school remediation session\nIts role in the student’s learning ecology can be anything—a smart piece of paper; an encouraging coach; a stimulating peer, etc. The technology is quite neutral\nFor instance, tutoring systems have been built that do not teach their task domain at all, but instead encourage students to discover its principles (e.g., Shute &amp; Glaser, 1990).\n"},"KB/Two-photon-Microscopy":{"title":"Two-photon Microscopy","links":[],"tags":["brain"],"content":"Two-photon Microscopy\n\nAn advanced microscopy technique that uses fluorescent markers to look at living tissue approximately one millimeter below the skin’s surface.\n"},"KB/Types-of-Graphs":{"title":"Types of Graphs","links":["KB/Graphs"],"tags":["graph"],"content":"Types of Graphs\n \nTypes of Graphs\n\nDirected edge - one way\nUndirected edge\nKnowledge graph - encodes a set of facts about objects, defines relations between them\n\nusually a directed heterogenous multigraph\nnodes can have different types of entities\nmultiple edges of different types between nodes\n\n\nhierarchial graph\ngeometric graph - each point is a point in 3d space\n\n\n"},"KB/Types-of-Normalizing-flows":{"title":"Types of Normalizing flows","links":["KB/Linear-Flows","KB/Elementwise-Flows","KB/coupling-flows","KB/autoregressive-flows","KB/residual-flows","KB/Multi-Scale-Flows"],"tags":["architecture"],"content":"Types of Normalizing Flows\n \nLinear Flows\nNon Linear Flows\n\nElementwise Flows\ncoupling flows\nautoregressive flows\nresidual flows\nMulti Scale Flows\n"},"KB/Types-of-Words":{"title":"Types of Words","links":["KB/Content-words","KB/Function-words"],"tags":["language"],"content":"Types of Words\n\nContent words\nFunction words\n"},"KB/Types-of-uncertainty":{"title":"Types of Uncertainty","links":["KB/Uncertainty","KB/Aleatoric","KB/Epistemic","KB/Predictive-Uncertainty"],"tags":["uncertainty"],"content":"Types of Uncertainty\n\nAleatoric\nEpistemic\nPredictive Uncertainty\n"},"KB/UCF101":{"title":"UCF101","links":[],"tags":["dataset"],"content":"UCF101\n\nUCF101\nwidely used video dataset for human action recognition\n13, 370 video clips with more than 27 hours belonging to 101 categories in this dataset\nspatial resolution of 320 x 240 pixels and 25 FPS frame rate\ndataset has been widely used for evaluating the performance of human action recognition\n"},"KB/ULMFit":{"title":"ULMFit","links":["English-Wikipedia.-md","Language-model"],"tags":["architecture"],"content":"ULMFit\n\nEnglish Wikipedia → IMDB%20-%3E%20%5B%5BIMDB)] Classifier\n"},"KB/UMA":{"title":"UMA","links":["KB/SMP","KB/Cache-Coherence"],"tags":["parallelcomputing"],"content":"UMA\n\nUniform memory access\nSMP\nIdentical processors\nIdentical processors\nEqual access and access times to memory\nCache Coherence\n"},"KB/Ultrasound":{"title":"Ultrasound","links":[],"tags":["medical"],"content":"Ultrasound\n\nImaging produced by high-frequency sound waves, usually used to view internal organs\n"},"KB/Un-LSTM":{"title":"Un-LSTM","links":[],"tags":["architecture"],"content":"Un-LSTM\n\nDue to the powerful ability of modeling long-term dynamic in videos, LSTM is used in both the encoder and decoder [37].\nSince its superior ability to model temporal dynamics, most of them use LSTM or LSTM variant to encode temporal dynamics in videos or to infer the future frames [37], [146], [147], [164], [165]\ncan be employed for self-supervised feature learning without using human- annotations\nencoder-decoder pipeline in which the encoder to model spatial and temporal features from the given video clips and the decoder to generate future frames based on feature extracted by encoder.\n"},"KB/Unawareness":{"title":"Unawareness","links":["KB/Disparate-Impact"],"tags":["temp"],"content":"Unawareness\n\nA situation in which sensitive attributes are present, but not included in the training data. Because sensitive attributes are often correlated with other attributes of one’s data, a model trained with unawareness about a sensitive attribute could still have Disparate Impact with respect to that attribute, or violate other fairness constraints.\n"},"KB/Unbiased-Look-at-Dataset-Bias":{"title":"Unbiased Look at Dataset Bias","links":[],"tags":["ethics"],"content":"\n\nUnbiased Look at Dataset Bias\n \n\n\nUnbiased Look at Dataset Bias\n\n\nAlexei A. Efros Antonio Torralba\n\n\n[@Unbiased look at dataset bias](@Unbiased look at dataset bias.md)\n\n\nAbstract\n\n\nsome datasets, that started out as data capture efforts aimed at representing the visual world, have become closed worlds unto themselves\n\n\ndespite the best efforts of their creators, the datasets appear to have a strong buildin bias\n\n\nOf course, much of the bias can be accounted for by the divergent goals of the different datasets: some captured more urban scenes, others more rural landscapes; some collected professional photographs, others the amateur snapshots from the Internet; some focused on entire scenes, others on single objects, etc\n\n\nCaltech has a strong preference for side views, while ImageNet is into racing cars; PASCAL have cars at noncanonical view-points; SUNS and LabelMe cars appear to be similar, except LabelMe cars are often occluded by small objects, etc\n\n\nName That Dataset\n\n\nMeasuring Dataset Bias\n\n\nsettle for a few standard checks, a diagnostic of dataset health if you will.\n\n\n[Cross-dataset generalization](Cross-dataset generalization.md)\n\n\n[Selection bias](Selection bias.md)\n\n\n[Capture bias](Capture bias.md)\n\n\n[Label bias](Label bias.md)\n\n\n[Negative Set Bias](Negative Set Bias.md)\n\n\nMeasuring Dataset’s Value\n\n\ntwo basic ways of improving the performance\n\n\nThe first solution is to improve the features, the object representation and the learning algorithm for the detector.\n\n\nThe second solution is to simply enlarge the amount of data available for training.\n\n\nSo, what is the value of current datasets when used to train algorithms that will be deployed in the real world? The answer that emerges can be summarized as: “better than nothing, but not by much”.\n\n\nDiscussion\n\n\nthat the reason is not that datasets are bad, but that our object representations and recognition algorithms are terrible and end up over-learning aspects of the visual data that relates to the dataset and not to the ultimate visual task.\n\n\nIn fact, a human learns about vision by living in a reduced environment with many potential local biases and yet the visual system is robust enough to overcome this.\n\n\nShould we care about the quality of our datasets? If the goal is to reduce computer vision to a set of feature vectors that can be used in some machine learning algorithm, then maybe not. But if the goal is to build algorithms that can understand the visual world, then, having the right datasets will be crucial.\n\n\nImages\n\n\n\n\n\n\n\n"},"KB/Uncertainity-in-classification":{"title":"Uncertainty Classification","links":["KB/Uncertainty","KB/Distributions","KB/Softmax","KB/Sigmoid"],"tags":["temp"],"content":"Uncertainty Classification\n\nDistributions\nUse Softmax or Sigmoid\n"},"KB/Uncertainity-in-regression":{"title":"Reg Uncertainty","links":["KB/Uncertainty","KB/LinearRegression"],"tags":["uncertainty"],"content":"Reg Uncertainty\n\nLinearRegression\nConfidence intervals\nProb that output belongs to this interval\nf(x) \\in [a,b]\nMean and Variance\nf(x) \\pm \\sigma\nf(x) \\in [f(x) - \\sigma,f(x) + \\sigma]\n"},"KB/Uncertainty":{"title":"Uncertainty","links":["KB/Aleatoric","KB/Entropy","KB/Epistemic","KB/Heteroscedatic","KB/Homoscedatic","KB/Inceptionism","KB/Interpretability-vs-Neuroscience","KB/LIME","KB/Predictive-Uncertainty","KB/SHAP","KB/Types-of-uncertainty","KB/Uncertainity-in-regression"],"tags":["anchor"],"content":"\nAleatoric\nEntropy\nEpistemic\nHeteroscedatic\nHomoscedatic\nInceptionism\nInterpretability vs Neuroscience\nLIME\nPredictive Uncertainty\nSHAP\nTypes of uncertainty\nUncertainity in regression\n"},"KB/Understandability":{"title":"Understandability","links":[],"tags":["explainability"],"content":"Understandability\n\ndenotes the characteristic of a model to make a human understand its function – how the model works – without any need for explaining its internal structure or the algorithmic means by which the model processes data internally\n"},"KB/Undue-Inducement":{"title":"Undue Inducement","links":[],"tags":["brain"],"content":"Undue Inducement\n\nWhen the value of something received in a clinical trial is so large that the study participant may agree to take risks that are not in their best interests.\n"},"KB/Unet-Grasping":{"title":"Unet Grasping","links":["KB/Gripper"],"tags":["robotics"],"content":"Unet Grasping\n\nY. Li, L. Schomaker, and H. Kasaei. “Learning to Grasp 3D Objects using Deep Residual U-Nets.” RO-MAN 2020.\nFormulate object grasping as a part segmentation problem\nDetect graspable shape primitives\nThe Gripper is free to approach objects from arbitrary directions.\n\n\n"},"KB/Unet":{"title":"Unet","links":["KB/Skip-Connection"],"tags":["architecture"],"content":"Unet\n\nSkip Connection\n"},"KB/Unicode-50":{"title":"Unicode 5.0","links":["KB/ASCII"],"tags":["language"],"content":"Unicode 5.0\n\nUNICODE Consortium 2006\n100,000 distinct characters\n75 supported scripts\nUTF-8 Variable Length Character Encoding\n\n1-4 bytes for each character (max )6\nASCII requires 1 byte\nAlphabetic Systems require 2 bytes\nChinese –Japanese-Korean – 3 bytes (sometimes 4 bytes)\n\n\n"},"KB/Uniform-Distribution":{"title":"Uniform Distribution","links":["KB/PDF"],"tags":["distributions"],"content":"Uniform Distribution\n\nIf I = [a_{1}, b_{1}]\\times ..\\times[a_{n}, b_{n}] is n dim interval in \\mathbb{R}^{n}\nPDF p(x) = \\begin{cases}\\frac{1}{(b_{1}-a_{1})\\cdot…\\cdot(b_{n}, a_{n})} &amp; \\text{if } x \\in I\\\\[2ex]0&amp;\\text{if x }\\notin I \\end{cases}\nNo need to learn, no shape that can be specified\n"},"KB/Uniform-Sampling":{"title":"Uniform Sampling","links":["Sampling","KB/Sampler","KB/Uniform-Distribution","KB/PDF","KB/CDF"],"tags":["distributions"],"content":"Uniform Sampling\n\nRandom Sampler\nUniform Distribution\nIf need to sample from another distribution with a PDF f(x). Can use a uniform Sampler on the distribution [0,1] to indirectly sample from it\n\nCoordinate transform\nCDF\nGet a Sampler for by X_{i} = \\varphi^{-1}\\circ U_{i}\n\n\n\n"},"KB/Uniform-baseline":{"title":"Uniform baseline","links":[],"tags":["explainability"],"content":"Uniform baseline\n\nThis time, the baseline doesn’t require an input image and uses only uniform distribution to generate a baseline\nThe problem with selecting a baseline is not solved, and for any further experiments, the “black image” baseline is going to be used.\n"},"KB/Unique-Character-Set":{"title":"Unique Character Set","links":[],"tags":["language"],"content":"Unique Character Set\n\nhelps in identifying the language\nGreek or Hebrew\n"},"KB/Universal-Approximation-Theorem":{"title":"Universal Approximation Theorem","links":["KB/Sigmoid","KB/Layers"],"tags":["temp"],"content":"Universal Approximation Theorem\n\n\nWhat this means that given an x and a y, the NN can identify a mapping between them. “Approximately”.\n\n\nThis is required when we have non linearly separable data.\n\n\nSo we take a non linear function, for example the Sigmoid. \\frac{1}{1 + e^{ - \\left( w^{T}x + b \\right)}}.\n\n\nThen we have to combine multiple such neurons in a way such that we can accurately model our problem. The end result is a complex function and the existing weights are distributed across many Layers.\n\n\nThe Universal approximation theorem states that\n\na feed forward network with a single hidden layer containing a finite number of neurons can approximate continuous functions on compact subsets of \\mathbb{R} , under mild assumptions on the activation function.\n\n\n\na feed forward network : take an input, apply a function, get an output, repeat\n\n\na single hidden layer : yes you can use more, but theoretically…\n\n\nfinite number of neurons: you can do it without needing an infinite computer\n\n\napproximate continuous functions: continuous functions are anything which dont have breaks/holes in between. This just says that it is possible to approximate the mapping which we talked about \\mathbb{R} is just the set of all real numbers\n\n\nAll this boils down to the fact that a neural network can approximate any complex relation given an input and an output.\n\n\n\n\n\n\n\n\nRefs\n\nmm\n"},"KB/Universal-Quantifiers":{"title":"Universal Quantifiers","links":["KB/Collective-Interpretation","KB/Distributive-Interpretation","KB/Distributive-Interpretation-(2)","KB/Cumulative-Interpretation"],"tags":["language"],"content":"Universal Quantifiers\n\nAll the boys are building a snowman\nEach boy is building a snowman\nTwo universally quantified sentences that involve/exhaust the totality of boys\nCollective Interpretation\nDistributive Interpretation\nDistributive Interpretation (2)\nCumulative Interpretation\n"},"KB/Unrestricted-Race-Model":{"title":"Unrestricted Race Model","links":[],"tags":["language"],"content":"Unrestricted Race Model\n\nThe Unrestricted Race model (Traxler, Pickering, &amp; Clifton, 1998; van Gompel, Pickering, Pearson, &amp; Liversedge, 2005; van Gompel, Pickering, &amp; Traxler, 2001) follows in the footsteps of constraint-based models in proposing simultaneous integration of multiple constraints from statistical, semantic, and contextual sources\nHowever, rather than ambiguity resolution being based on a temporally dynamic competition process, the Unrestricted Race model posits an instantaneous probabilistic selection among the weighted alternatives of an ambiguity.\nmuch like the syntax-first models, it must hypothesize a separate reanalysis mechanism that is responsible for garden-path effects when the initial selected alternative turns out to be syntactically or semantically inappropriate.\nthe Unrestricted Race model predicts that sentences with garden-paths and sentences without garden-paths are two separate populations of events\nIn other words, in conditions where mean performance is expected to exhibit a garden-path effect, there should exist one of two possible patterns: (a) a bimodal distribution of some substantial gardenpath responses and some non-gardenpath responses, or (b) practically all trials exhibiting substantial garden-path effect\n"},"KB/Unsupervised-Data-Generation":{"title":"Unsupervised Data Generation","links":[],"tags":["temp"],"content":"Unsupervised Data Generation\n\ntraining data creation procedure named Unsupervised Data Generation (UDG), which leverages few-shot prompts to synthesize high-quality training data without real human annotations\n"},"KB/Unsupervised-Learning":{"title":"Unsupervised Learning","links":["KB/Embedding","KB/Layers","KB/Clustering","KB/PCA","KB/Feature-Learning","Hidden-Models","KB/Generative-Models","Anomaly-Detection","KB/Auto-Encoders"],"tags":["temp"],"content":"Unsupervised Learning\n\nDiscover useful things from raw data\nRepresentation/Embedding Learning\nIf labels , train network and take intermediate Layers\nClustering\nPCA\nFeature Learning\nHidden Models\nGenerative Models\nAnomaly Detection\nAuto Encoders\n\n…"},"KB/Untitled-1":{"title":"Untitled 1","links":[],"tags":[],"content":""},"KB/Untitled":{"title":"Untitled","links":[],"tags":[],"content":""},"KB/Untitledp":{"title":"Untitledp","links":[],"tags":[],"content":"def proxy_one_batch(config, input_wrong, cam):\n    grads = cam(input_tensor=input_wrong.to(config[&quot;device&quot;]), targets=None)\n    grads = torch.Tensor(grads).to(config[&quot;device&quot;]).unsqueeze(1).expand(-1, 3, -1, -1)\n    normalized_inps = inv_normalize(input_wrong)\n    \n    if config[&quot;pixel_replacement_method&quot;] != &quot;blended&quot;:\n        output =  torch.where(\n            grads &gt; config[&quot;proxy_threshold&quot;],\n            dict_decide_change[config[&quot;pixel_replacement_method&quot;](grads),\n            normalized_inps,\n        )\n    else:\n        output= torch.where(\n            grads &gt; config[&quot;proxy_threshold&quot;],\n            (1 - config[&quot;proxy_image_weight&quot;] * grads) * normalized_inps,\n            normalized_inps,\n        )\n    del grads\n    return output\n \ndef proxy_callback(config, input_wrong_full, label_wrong_full, cam):\n    # TODO Save Classwise fraction\n    chosen_inds = int(np.ceil(config[&quot;change_subset_attention&quot;] * len(label_wrong_full)))\n    # TODO some sort of decay?\n    # TODO Remove min and batchify\n \n    input_wrong_full = input_wrong_full[:chosen_inds]\n    label_wrong_full = label_wrong_full[:chosen_inds]\n \n    processed_labels = []\n    processed_thresholds = []\n \n    for i in tqdm(range(0, len(input_wrong_full), config[&quot;batch_size&quot;]), desc=&quot;Running proxy&quot;):\n        try:\n            input_wrong = input_wrong_full[i:i+config[&quot;batch_size&quot;]\n            label_wrong = label_wrong_full[i:i+config[&quot;batch_size&quot;]\n \n            try:\n                input_wrong = torch.squeeze(torch.stack(input_wrong, dim=1))\n                label_wrong = torch.squeeze(torch.stack(label_wrong, dim=1))\n            except:\n                input_wrong = torch.squeeze(input_wrong)\n                label_wrong = torch.squeeze(label_wrong)\n \n            thresholded_ims = proxy_one_batch(config, input_wrong.to(config[&quot;device&quot;]), cam)\n            processed_thresholds.extend(thresholded_ims.detach().cpu())\n            processed_labels.extend(label_wrong)\n \n \n    processed_thresholds = torch.stack(processed_thresholds, dim = 0).detach()\n    batch_size = processed_thresholds.size(0)\n \n    for ind in tqdm(range(batch_size), total=batch_size, desc=&quot;Saving images&quot;):\n        label = config[&quot;label_map&quot;][processed_labels[ind].item()]\n        save_name = (\n            config[&quot;ds_path&quot;] / label / f&quot;proxy-{ind}-{config[&#039;global_run_count&#039;]}.jpeg&quot;\n        )\n        tfm(processed_thresholds[ind, :, :, :]).save(save_name)\n "},"KB/Uplift-Modeling":{"title":"Uplift Modeling","links":["KB/medical"],"tags":["temp"],"content":"Uplift Modeling\n\nA modeling technique, commonly used in marketing, that models the “causal effect” (also known as the “incremental impact”) of a “treatment” on an “individual.” Here are two examples\nDoctors might use uplift modeling to predict the mortality decrease (causal effect) of a medical procedure (treatment) depending on the age and medical history of a patient (individual).\nMarketers might use uplift modeling to predict the increase in probability of a purchase (causal effect) due to an advertisement (treatment) on a person (individual).\n"},"KB/Upweighting":{"title":"Upweighting","links":[],"tags":["temp"],"content":"Upweighting\n\nApplying a weight to the downsampled class equal to the factor by which you downsampled.\n"},"KB/Use-Case-Utility":{"title":"Use Case Utility","links":[],"tags":["explainability"],"content":"Use Case Utility\n\nXAI and LLMs are often tools for accomplishing some other goal.\nVery limited work has explored the utility of LLMs in use-case– specified user studies, but a user study on Microsoft/Github’s Copilot [1], an LLM-based code generation tool, found that it “did not necessarily improve the task completion time or success rate” [52]\nLLM outputs often sound very confident, even if what they are saying is hallucinated [50]\nWhen the user inquires about the incorrectness, they also have a documented tendency to argue that the user is wrong and that their response is correct. In fact, some have called LLMs “mansplaining as a service” [34]\nThis can make it more difficult for humans to implement cognitive checks on LLM outputs.\nWhile some recent LLM work has outlined categories of failure modes for LLMs based on the types of cognitive biases use [29], we push for greater work in this field\n"},"KB/Useful-Codes":{"title":"Useful Codes","links":["KB/Parallel-Runner"],"tags":["temp"],"content":"Useful Codes\n\nParallel Runner\n"},"KB/Utilitarian-ethics":{"title":"Utilitarian ethics","links":[],"tags":["ethics"],"content":"Utilitarian Ethics\n\nresulting decisions often aim to produce the best aggregate consequences\n"},"KB/VAE":{"title":"Variational Autoencoder","links":["KB/Features","Tag-Pages/loss","KB/Probability","KB/KL-Divergence"],"tags":["architecture"],"content":"Variational Autoencoder\n\nSome control over distribution of learned Features\nEg: Decoder as a generative model\nConstraint loss function and a given Probability \\mathcal{D}\n\nEg: By loss func KL Divergence and prob distribution $$L(X) = n^{-1}\\Sigma_i||x_i - D(E(\\tilde x))||^2 + \\lambda \\cdot KL(f_i, d)$\nUse 2D unit distribution. 0 mean, unit variance\nLatent vector : f=\\mu + \\epsilon e^{2log\\sigma}\n\n\n$$L(X) = n^{-1}\\Sigma_i||x_i - D(E(\\tilde x))||^2 + \\frac{1}{2n}\\Sigma_i(e^{log\\sigma(x_i)} + \\mu(x_i)^2 -1 -log(\\sigma (x_i))$\nEncoder predicts mean and std E(x_i) = (\\mu(x_i) , log \\sigma(x_i))\n"},"KB/VGGFace2-3":{"title":"VGGFace2","links":[],"tags":["dataset"],"content":"VGGFace2\n\nA dataset for recognising faces across pose and age.\nThe VGGFace2 dataset is made of around 3.31 million images divided into 9131 classes, each representing a different person identity.\n"},"KB/VGGish":{"title":"VGGish","links":["KB/Spectrogram","KB/Features","KB/AudioSet-classification"],"tags":["architecture"],"content":"VGGish\n\nCNN Architectures for Large-Scale Audio Classification\napplying various state-of-the-art image networks with CNN architectures to audio and show that they are capable of excellent results on audio classification\nexamine fully connected deep neural networks such as AlexNet, VGG, InceptionNet, and ResNet\nThe input audio is divided into non-overlapping 960 ms frames which are decomposed by applying the Fourier transform, resulting in a Spectrogram\nSpectrogram is integrated into 64 mel-spaced frequency bins, and the magnitude of each bin is log-transformed\ngives log-mel Spectrogram patches that are passed on as input to all classifiers\nAcoustic Event Detection\ntrain a classifier on embeddings learned from the video-level task on AudioSet\nmodel for AED with embeddings learned from these classifiers does much better than raw Features on the Audio Set AED classification task\nderivatives of image classification networks do well on the audio classification task\nincreasing the number of labels they train on provides some improved performance over subsets of labels\nperformance of models improves as they increase training set size,\nmodel using embeddings learned from the video-level task do much better than a baseline on the AudioSet classification task\n"},"KB/VICReg":{"title":"VICReg","links":["KB/Self-Supervised","KB/Embedding","KB/Mode-Collapse","KB/Regularization","KB/Covariance","Bias-Vs-Variance"],"tags":["architecture"],"content":"\nVICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning\nSelf Supervised\nbased on maximizing the agreement between Embedding vectors from different views of the same image\nrivial solution is obtained when the encoder outputs constant vectors\nMode Collapse is often avoided through implicit biases\nexplicitly avoids the collapse problem with a simple Regularization term on the variance of the embeddings along each dimension individually\ntriple objective: learning invariance to different views with a invariance term, avoiding collapse of the representations with a variance preservation term, and maximizing the information content of the representation with a Covariance Regularization term\nBias Vs Variance\ncombines the variance term with a decorrelation mechanism based on redundancy reduction and Covariance Regularization\ndoes not require the Embedding branches to be identical or even similar\n"},"KB/VL-BEIT":{"title":"VL-BEIT","links":["KB/Features"],"tags":["architecture"],"content":"VL-BEIT\n\nVL-BEIT: Generative Vision-Language Pretraining\nvision-language foundation model\nsimple and effective approach to pretraining a bidirectional multimodal Transformer encoder for both vision-language and vision tasks learned by generative pretraining\nconducts masked prediction on both monomodal and multimodal data with a shared Transformer\nsolely employs generative pretraining tasks, including [masked language modeling](masked language modeling.md) on texts, masked image modeling on images, and masked vision-language modeling on image-text pairs\nlearned from scratch with one unified pretraining task, one shared backbone, and one-stage training which renders it conceptually simple and empirically effective\ntransferable visual Features\n"},"KB/VQAv2-3":{"title":"VQAv2","links":[],"tags":["dataset"],"content":"VQAv2"},"KB/VTAB":{"title":"VTAB","links":[],"tags":["temp"],"content":"\ntoc: true\ntitle: VTAB\ntags: [‘temp’]\n\nVTAB"},"KB/Vacuum-Cup-Hand":{"title":"Vacuum Cup Hand","links":["KB/End-effector"],"tags":["robotics"],"content":"Vacuum Cup Hand\n\nAn End-effector for a robot arm which is used to grasp light to moderate weight objects, using suction, for manipulation. Such objects may include glass, plastic; etc. Commonly used because of its virtues of reduced object slide slipping while within the grasp of the vacuum cup.\n"},"KB/Vagus-Nerve":{"title":"Vagus Nerve","links":[],"tags":["brain"],"content":"Vagus Nerve\n\nOne of the twelve pairs of cranial nerves in the human body, the vagus nerve connects the brain stem to the body, transmitting information from the brain to the major organs and other tissues.\n"},"KB/Van-Mises-distribution":{"title":"Van Mises distribution","links":[],"tags":["distributions"],"content":"Van Mises distribution\n \n"},"KB/Vanishing-Gradient":{"title":"Vanishing Gradient","links":["KB/Backprop"],"tags":["architecture"],"content":"Vanishing Gradient\n\nDeltas become smaller initially. using [Sigmoid] → [ill conditioning](Sigmoid] → [ill conditioning.md)\ng(x) = (1+e^{-x})^{-1}\n\\nabla_{x}[g] = g(1-g) \\in [0,1]\nSaturation and prevent Backprop\ng(x) \\approx 1 \\rightarrow \\nabla_{x}[g] \\approx 0 \nWeight matrices are usually initialized with random values |w_{ji}| &lt;&lt; 1\n\ngradient magnitueds decay exponentially → max eigenvalue\n\n\n"},"KB/Vapnik-chervonenkis-dimension":{"title":"Vapnik chervonenkis dimension","links":[],"tags":["architecture"],"content":"Vapnik Chervonenkis Dimension\n \n\nthe largest number of training examples that a binary classifier can label arbitrarily\n"},"KB/VarGrad":{"title":"VarGrad","links":[],"tags":["explainability"],"content":"VarGrad\n\n@richterVarGradLowVarianceGradient2020\n\n"},"KB/Variable-Importances":{"title":"Variable Importances","links":[],"tags":["temp"],"content":"Variable Importances\n\nA set of scores that indicates the relative importance of each feature to the model.\n"},"KB/Variation-in-Dissimilarity-Variation-in-Dissimilarity":{"title":"Variation in Dissimilarity Variation in Dissimilarity","links":[],"tags":["explainability"],"content":"Variation in Dissimilarity Variation in Dissimilarity\n\nis the variance of the NISSIM metric over the adversarial set over different levels of attack eps for a model, this shows the distribution of the attack on the model when different levels of attack are performed\nIdeally, we would want the distribution to be stable for different levels of attack\nm_{h}= \\frac{1}{eps}\\Sigma(NISSIM_{eps})\nVID = \\sqrt{\\frac{\\Sigma(NISSIM_{eps}-m_{h})^{2}}{eps}}\n"},"KB/VariationalRecurrent-Dropout":{"title":"Variational/Recurrent Dropout","links":["KB/Recurrent","KB/Dropout","KB/Basic-RNN-Architectures"],"tags":["regularization"],"content":"Variational/Recurrent Dropout\n\nBasic RNN Architectures\nOnly on the non Recurrent parts such as inputs and outputs\nIn recorrent parts, use the same Dropout mask for all time steps\nSame Dropout mask for each time step\n\n\n…"},"KB/VattenFall-Data-Scientist":{"title":"VattenFall Data Scientist","links":[],"tags":["jobsearch"],"content":"VattenFall Motivation Letter - Subhaditya\nAs I write this motivation letter, the temperatures in NL have hit yet another record for the hottest September ever. One of the biggest reasons for this is global warming, and fossil fuels are a massive component. VattenFall’s mission to be fossil-free in one generation and help customers shift towards sustainable energy sources using data greatly resonates with me, so I am applying for this position as a Data Scientist. At the core of making the world a greener and healthier place is data that every company has collected for decades. Using that vast data pool to inform better decisions is quite challenging but equally rewarding.\nMy expertise is a combination of data analytics/BI and computer vision, and as of a month ago, I also have a Master in Artificial Intelligence from the University of Groningen. I am familiar with the tools required for basic and advanced AI and analytics from internships, research projects, papers, freelance work, and many personal projects. In my previous internship at KPMG, I built 10+ dashboards using PowerBI and Azure for a project with the Abu Dhabi government. I have made several analytics pipelines and machine learning implementations in other internships. I am familiar with Python and can build advanced AI pipelines for many tasks.\nIn any team, it is essential that each of the members can contribute something to the overall development of the projects. Although I have a lot to learn, I genuinely enjoy developing solutions that have a positive impact, and I can contribute quite a bit to any team I get the chance to work with. If there is anything I do not know, I am also ready to develop those skills quickly.\nI hope you give me a chance to be a part of the generation that finally starts relying on greener energy sources for our own and future generations."},"KB/Vector-Assembly-level":{"title":"Vector Assembly level","links":[],"tags":["parallelcomputing"],"content":"Vector Assembly Level\n\n\n\n\n\n"},"KB/Vector-Chaining":{"title":"Vector Chaining","links":[],"tags":["parallelcomputing"],"content":"Vector Chaining\n\nEquivalent to data forwarding in vector processors\nResults of one pipeline are fed into operand registers of another pipeline\n"},"KB/Vector-Functional-Units":{"title":"Vector Functional Units","links":[],"tags":["parallelcomputing"],"content":"Vector Functional Units\n\nFully pipelined, new operation every cycle\nPerforms arithmetic and logic operations\nTypically 4-8 different units\n"},"KB/Vector-Load-Store-Units":{"title":"Vector Load Store Units","links":[],"tags":["parallelcomputing"],"content":"Vector Load Store Units\n\nMoves vectors between memory and registers\n"},"KB/Vector-Processor":{"title":"Vector Processor","links":["KB/Memory-to-Memory-Architecture","KB/Register-to-Register-Architecture","KB/Vector-Register","KB/Scalar-Register","KB/Vector-Functional-Units","KB/Vector-Load-Store-Units","KB/Strip-Mining","KB/Vector-Chaining","KB/Scatter-and-Gather","KB/Pipes","KB/Vector-Assembly-level"],"tags":["parallelcomputing"],"content":"Vector Processor\n\nMemory to Memory Architecture\nRegister to Register Architecture\nVector Register\nScalar Register\nVector Functional Units\nVector Load Store Units\nStrip Mining\nVector Chaining\nScatter and Gather\nPipes\nVector Assembly level\n"},"KB/Vector-Quantization":{"title":"Vector Quantization","links":["KB/Gravity","KB/Encodings"],"tags":["temp"],"content":"Vector Quantization\n\nPartitioned into k cells whose center of Gravity vectors are indexed\nIndices used as symbolic Encodings\nDiscretization\n"},"KB/Vector-Register":{"title":"Vector Register","links":[],"tags":["parallelcomputing"],"content":"Vector Register\n\nTypically 8-32 vector registers with 64 -128 64-bit elements\nEach contains a vector of double-precision numbers\nRegister size determines the maximum vector length\nEach includes at least 2 read and 1 write ports\n"},"KB/Vectorization":{"title":"Vectorization","links":[],"tags":["parallelcomputing"],"content":"Vectorization\n\nHardware primitives\nPrioritize those are contiguous in memory\n"},"KB/Velocity":{"title":"Velocity","links":["KB/Displacement"],"tags":["physics"],"content":"Velocity\n\nDisplacement wrt time\nv_{avg}= \\frac{\\Delta{x}}{\\Delta t}\n"},"KB/Verb":{"title":"Verb","links":["KB/Transitive-verb","KB/DiTransitive-verb","KB/Action-Transitive-verb"],"tags":["language"],"content":"Verb\n\nActions, relationships\nTransitive verb\nDiTransitive verb\nAction Transitive verb\n"},"KB/Vertebral-Arteries":{"title":"Vertebral Arteries","links":["KB/Basilar-Artery"],"tags":["brain"],"content":"Vertebral Arteries\n\nThe major arteries of the neck, which merge to form the Basilar Artery.\n"},"KB/Vestibular-System":{"title":"Vestibular System","links":["Tag-Pages/loss"],"tags":["brain"],"content":"Vestibular System\n\nRegions in the body and brain that help support balance in movement. Many people with hearing loss experience some degree of balance difficulties, since the vestibular (or balance) system and the auditory (or hearing) systems are so closely related.\n"},"KB/Vgg":{"title":"Vgg","links":["KB/Alex-Net"],"tags":["architecture"],"content":"Vgg\n\n@simonyanVeryDeepConvolutional2014\nVery Deep Convolutional Networks for Large-Scale Image Recognition\nDeeper Alex Net\nObject detection and Image captioning\n5x5 → two 3x3\nNo of filters increase according to depth\nNo of filters : increase by power of two\nFilter size : Odd numbers\nSGD + LR Schedule\nthree non-linear activations (instead of one), which makes the function more discriminative\n\n"},"KB/ViLT":{"title":"ViLT","links":["KB/Embedding","KB/Features","Tag-Pages/loss","KB/ITM-Loss","KB/MSCOCO","KB/Visual-Genome","KB/SBU-Captions","KB/Google-Conceptual-Captions","VQAv2","NLVR2","KB/Flickr30K","KB/Modality"],"tags":["architecture"],"content":"ViLT\n\nViLT: Vision-and-Language Transformer Without Convolution or Region Supervision\nVision-and-Language Transformer\nseeks to improve performance on various joint vision-and-language downstream tasks\nCurrent approaches to VLP heavily rely on image feature extraction processes using convolutional visual Embedding networks (e.g., Faster R-CNN and ResNets), which involve region supervision (e.g., object detection) and the convolutional architecture (e.g., ResNet)\nThis is problematic in terms of both efficiency/speed, in that extracting input Features requires much more computation than the multimodal interaction steps; and expressive power, as it is upper bounded to the expressive power of the visual embedder and its predefined visual vocabulary.\nminimal VLP model, which is monolithic in that the processing of visual inputs is drastically simplified to just the same convolution-free manner that they process textual inputs\nremoving the need for object detectors\navoiding heavyweight image encoders by directly Embedding low-level pixel data with a single-layer projection and achieves similar results with reduced complexity,\nSelf-supervision is accomplished using (i) Image Text Matching (ITM) loss and (ii) Masked Language Model (MLM) loss\nITM Loss\nFor text, ViLT simply reuses Masked Language Model - (MLM), used in BERT.\nMSCOCO\nVisual Genome\nSBU Captions\nGoogle Conceptual Captions\nVQAv2\nNLVR2\nFlickr30K\nViLT is over 10x faster than previous VLP models, yet with competitive or better downstream task performance\nVLP needs to focus more on the multi-Modality interactions aspect inside the transformer module rather than engaging in an arms race that merely powers up unimodal embedders\n"},"KB/Video-Generation":{"title":"Video Generation","links":[],"tags":["semisupervisedlearning"],"content":"Video Generation\n\nVisual features are learned through the process of video generation tasks\nvideo prediction\n"},"KB/Viewpoint-Feature-Histogram":{"title":"Viewpoint Feature Histogram","links":[],"tags":["robotics"],"content":"Viewpoint Feature Histogram\n\nVFH produces a histogram that encodes the geometry of the object and its viewpoint.\nFor every pair of a point and the center of mass, a reference frame is constructed and the three angular variations are computed (a, q, f) and also d which represents distance between (point, center of mass)\nAnother statistical feature is computed between the central viewpoint direction and the normal estimated at each point.\nThe quality of VFH description depends on the quality of the surface normal estimation at each point.\n\n"},"KB/Virtue-ethics":{"title":"Virtue ethics","links":[],"tags":["ethics"],"content":"Virtue Ethics\n\nan agent is ethical if and only if it acts and thinks according to some moral values\nAgents with virtue ethics should exhibit an inner drive to be perceived favourably by others\n"},"KB/Vision-Explainibility":{"title":"Vision Explainibility","links":["KB/DeconvNet","KB/Deep-Inside-Convolutional-Networks","KB/Guided-BackProp","KB/Salience-Map","KB/CAM","KB/Grad-CAM","Occlusion-Map","KB/Guided-GradCAM","KB/DeepLIFT","KB/Smooth-Grad","KB/VarGrad","KB/Conductance"],"tags":["explainability"],"content":"Vision Explainibility\nLinks Useful\n\nCaptum Algos Comparison\n\nFlow\n\nDeconvNet (2013)\nDeep Inside Convolutional Networks (2014)\nGuided BackProp (2015) Aka All Conv net\n\nBuilding up on Deep Inside Convolutional Networks and DeconvNet\n\n\nSalience Map\n\nNot class discriminative\nNoise\nNot appealing\n\n\nCAM\n\nless noisy\nnot class discriminative\nWorked only a restricted set of CNN templates\n\n\nGrad-CAM\n\nclass discriminative\nnot high res\nWorks for any arbitrary CNN\n\n\nOcclusion Map\n\nSame as the next but not very fast\n\n\nGuided GradCAM\nDeepLIFT\n[Noise Tunnel](Noise Tunnel.md)\nSmooth-Grad\n[SmoothGrad Square](SmoothGrad Square.md)\nVarGrad\n[Integrated Gradients](Integrated Gradients.md)\n[Proxy Attention](Proxy Attention.md)\nConductance\n\nDisadvantages\n\n[The Unreliability of Saliency Methods](The Unreliability of Saliency Methods.md)\n[Interpretation of Neural networks is fragile](Interpretation of Neural networks is fragile.md)\nFine grained data\n\nLinks"},"KB/Vision-Transformer":{"title":"Vision Transformer","links":["KB/Transformer","KB/ImageNet","KB/CIFAR","KB/VTAB","KB/Conv","KB/Features","KB/Layers","MLP-Mixer"],"tags":["temp"],"content":"Vision Transformer\n\n@dosovitskiyImageWorth16x162021\npaper\n\nTransformer applied directly to sequences/patches of images\nLower computational resources\nImageNet , CIFAR, VTAB\nDo Vision Transformers See Like Convolutional Neural Networks?\nanalyzes the internal representation structure of ViTs and Conv on image classification benchmarks\nstriking differences in the Features and internal structures between the two architectures\nViT having more uniform representations across all Layers\nearly aggregation of global information\nspatial localization\ndiscovering ViTs successfully preserve input spatial information with CLS tokens\nfinding larger ViT models develop significantly stronger intermediate representations through larger pretraining datasets\nMLP-Mixer\n"},"KB/Visual-Associative":{"title":"Visual Associative","links":[],"tags":["visualization"],"content":"Visual Associative\n\nCan this variable allow us to spontaneously group items in a group? [i.e., 1 group from all]\n"},"KB/Visual-Commonsense-Reasoning":{"title":"Visual Commonsense Reasoning","links":["KB/Adversarial-Learning"],"tags":["dataset"],"content":"Visual Commonsense Reasoning\n\nGiven a challenging question about an image, a machine must answer correctly and then provide a rationale justifying its answer\nFrom Recognition to Cognition: Visual Commonsense Reasoning\n290k multiple choice QA problems derived from 110k movie scenes\nAdversarial Learning\n"},"KB/Visual-Context-Augmentation":{"title":"Visual Context Augmentation","links":[],"tags":["augmentation"],"content":"Visual Context Augmentation\n\n@dvornikModelingVisualContext2018\nlearns to place object instances at an image location depending on the surrounding context.\nA neural network is trained for this purpose.\nThe training data is pre- pared to generate a context image with the masked- out object inside it.\nFrom an image, 200 context sub-images are generated surrounding the blacked- out bounding box. The neural network learns to predict the category (object or background) in masked pixels.\nThe object instances are placed inside the selected boxes to generate a new train- ing image\n"},"KB/Visual-Cortex":{"title":"Visual Cortex","links":["KB/Cerebrum","KB/Occipital-lobe"],"tags":["brain"],"content":"Visual Cortex\n\nThe area of the Cerebrum that is specialized for vision. It lies primarily in the Occipital lobe at the rear of the brain and is connected to the eyes by the optic nerves.\n"},"KB/Visual-Encoding":{"title":"Visual Encoding","links":["KB/Characteristics-of-Visual-Variables"],"tags":["visualization"],"content":"Visual Encoding\n\n\nCharacteristics of Visual Variables\n"},"KB/Visual-Genome":{"title":"Visual Genome","links":[],"tags":["dataset"],"content":"Visual Genome"},"KB/Visual-Implicit-Learning":{"title":"Visual Implicit Learning","links":["KB/Perception"],"tags":["language"],"content":"Visual Implicit Learning\n\nDoes ability to learn implicit dependencies correlate with ability to correctly predict the next word in speech?\nParticipants saw a sequence of colors light up on screen\nThey then had to reproduce it by clicking on the same sequence\nAuditory Only Sentence Perception\n25 highly predictable and 25 zero-predictability sentences\nacoustically degraded by processing them with a sinewave vocoder to 6 channels\nAbility to pick up/learn statistical dependencies seems almost to be a new cognitive function\nShows a potential connection between sequence learning and language modelling ability\nLearning simple dependencies is not so difficult it seems. But natural language has many dependencies at a distance\n"},"KB/Visual-Length":{"title":"Visual Length","links":[],"tags":["visualization"],"content":"Visual Length\n\nAcross how many changes in this variable are distinctions possible? [i.e., how many can I see?]\n"},"KB/Visual-Ordered":{"title":"Visual Ordered","links":[],"tags":["visualization"],"content":"Visual Ordered\n\nCan this variable allow us to spontaneously perceive an order [i.e., what is smaller and what is bigger?]\n"},"KB/Visual-Quantitative":{"title":"Visual Quantitative","links":[],"tags":["visualization"],"content":"Visual Quantitative\n\nCan the difference between two marks in this variable be interpreted numerically? [i.e., corresponds to 5?]\n"},"KB/Visual-Selective":{"title":"Visual Selective","links":[],"tags":["visualization"],"content":"Visual Selective\n\nCan this variable allow us to spontaneously differentiate/isolate items from groups? [i.e., 1 item from all]\n"},"KB/Visual-Servo-System":{"title":"Visual Servo System","links":[],"tags":["robotics"],"content":"Visual Servo System\n\nThe task in visual servoing is to use visual information to control the robot’s end-effector relative to a target object\n\nChaumette, François, and Seth Hutchinson. “Visual servo control. II. Advanced approaches [Tutorial].” IEEE Robotics &amp; Automation Magazine 14.1 (2007): 109-118.\nMorrison, Douglas, Peter Corke, and Jürgen Leitner. “Closing the loop for robotic grasping: A real-time, generative grasp synthesis approach.” RSS (2018).\n\n\n\n"},"KB/VisualGPT":{"title":"VisualGPT","links":[],"tags":["architecture"],"content":"VisualGPT\n\nimage captioning model\nleverages knowledge from the pretrained language model GPT-2\nbridge the semantic gap between diferent modalities - novel encoder-decoder attention mechanism [33] is designed with an unsaturated\nrectified gating function\nthe biggest advantage of this model is that it does not need for as much data as other image-to-text models\nimproving data eciency in image captioning networks would enable quick data curation, description of rare objects, and applications in specialized domains\n"},"KB/Visualization-Of-Layers":{"title":"Visualization Of Layers","links":["KB/visualization","KB/Tanh"],"tags":["visualization"],"content":"visualization Of Layers\n\nTanh\ntanh(Wx+b)\n\n\nA linear transformation by the “weight” matrix W\n\n\n\n\nA translation by the vector b\n\n\n\n\nPoint-wise application of Tanh.\n\n\n\n"},"KB/Visualizing-the-Impact-of-Feature-Attribution-Baselines":{"title":"Visualizing the Impact of Feature Attribution Baselines","links":[],"tags":["explainability"],"content":"Visualizing the Impact of Feature Attribution Baselines\n\n@sturmfelsVisualizingImpactFeature2020\n[Maximum Distance Baseline](Maximum Distance Baseline.md)\n[Uniform baseline](Uniform baseline.md)\n"},"KB/Visualizing-the-Loss-Landscape-of-Neural-Nets":{"title":"Visualizing the Loss Landscape of Neural Nets","links":[],"tags":["explainability"],"content":"Visualizing the Loss Landscape of Neural Nets\n\n@liVisualizingLossLandscape2018\nGeneralization error relates to convexity in the loss landscape\n\n\n\n\n[Random Directions](Random Directions.md)\n[Sharpness and Flatness](Sharpness and Flatness.md)\n[Training Trajectories](Training Trajectories.md)\n\nSolutions\n\n[Layer Normalization](Layer Normalization.md)\n[Filter Wise Normalization](Filter Wise Normalization.md)\n[Trajectory Plotting with PCA](Trajectory Plotting with PCA.md)\n\nImages\n\n\n"},"KB/Volterra-expansion":{"title":"Volterra","links":["KB/Pruning"],"tags":["temp"],"content":"Volterra\n\nAdd higher order polynomials\nAdding too much leads to combinatorial explosion → Pruning scheme\nAdding all polynomials of degree 2\n\nd + d(d+1) /2 input components\n\n{u_1, u_2, …,u_d} \\cup {u_iu_j | 1 \\leq i \\leq j \\leq d}\n\n\n\n\n"},"KB/Volume-Rendering-Equation":{"title":"Volume Rendering Equation","links":[],"tags":["visualization"],"content":"Volume Rendering Equation\n\nLight-emitting particles fill volume\nEmission-absorption mode\nBased on a physical model for radiation\nInteraction of light with matter at the macroscopic scale, neglecting Diffraction, Interference, Wave-character, Polarization, etc.\n\\kappa is fraction of absorbed light\ng is fraction of emitted light\n\\frac{dL}{ds} = g(s) - \\kappa(s)L(s)\naka Emission -Absorption\nT(s_{1}, s_{2}) = e^{-\\int_{s_{1}}^{s_{2}}\\kappa(s&#039;)ds&#039;}\nL_{1}^{n}= g(n)+T(n)L_{1}^{n-1}\n"},"KB/Volume-Visualization":{"title":"Volume Visualization","links":["KB/visualization","KB/Orthogonal-Slicing","KB/Oblique-Slicing","KB/Isosurface","KB/Volumetric-Illumination"],"tags":["visualization"],"content":"Volume visualization\n\nOrthogonal Slicing\nOblique Slicing\nIsosurface\nVolumetric Illumination\n"},"KB/Volumetric-Grasping-Network":{"title":"Volumetric Grasping Network","links":["KB/TSDF"],"tags":["robotics"],"content":"Volumetric Grasping Network\n\nM. Breyer, et al., “Volumetric Grasping Network: Real-time 6 DOF Grasp Detection in Clutter”, ICLR 2020\nLearning to grasp 3D objects by constructing a full model of the scene\nTSDF\n\n"},"KB/Volumetric-Illumination":{"title":"Volumetric Illumination","links":["KB/Phong-Lighting","KB/Finite-Differences","KB/Shading","KB/Raycasting"],"tags":["visualization"],"content":"Volumetric Illumination\n\nPhong Lighting\nFinite Differences\nShading\nRaycasting\n"},"KB/Von-Neumann-Architecture":{"title":"Von Neumann Architecture","links":[],"tags":["parallelcomputing"],"content":"Von Neumann Architecture\n-"},"KB/Voronoi-Cell":{"title":"Voronoi Cell","links":[],"tags":["temp"],"content":"Voronoi Cell\n\nPartition a plane into n convex polygons → each containing one generating point and every point is closer to its generating point than others\n\n\nRefs\n\nwolf\n"},"KB/Voxel-Projection":{"title":"Voxel Projection","links":[],"tags":["visualization"],"content":"Voxel Projection\n\nVolume = field of 3D interpolation kernels\nOne kernel at each grid voxel\nEach kernel leaves a 2D footprint on screen\nWeighted footprints accumulate into image\n\n"},"KB/WMT14":{"title":"WMT14","links":[],"tags":["dataset"],"content":"WMT14"},"KB/WOMBO-Dream":{"title":"WOMBO Dream","links":[],"tags":["art"],"content":"WOMBO Dream\n\nThe app creates generative art from a descriptive text in various pre-determined styles.\nThe app uses two machine learning technologies that combine a neural network to generate images and an algorithm that interprets text descriptions.\nBoth algorithms learn from each iteration, meaning that every request generates a unique outcome.\n"},"KB/Wageningen-Uni-AI":{"title":"Wageningen Uni AI","links":[],"tags":["jobsearch"],"content":"Wageningen : Researcher Computer Vision Technology for Animal Science\nWith the demands on the food industry growing every day, it is easy for companies to start ignoring the needs of the animals that are the most significant part of the pipeline. We hear so many horror stories about the conditions some of these animals face, but even more humane places have to meet many needs. On a larger scale, using technologies like AI can make it much less labor-intensive to provide suitable conditions for these animals and make sure they live as close to their original lives in nature as possible. That being said, the intersection between computer vision and animal welfare is quite interesting to me, hence this application.\nAs of a month ago, I have a masters in AI from the University of Groningen. My expertise is a combination of data analytics and computer vision for practical applications. I am familiar with the tools required for basic and advanced AI and analytics from internships, research projects, papers, freelance work, and many personal projects. I am comfortable with building deep learning pipelines, image and data analysis, OpenCV applications, image processing, etc.\nIn any team, it is essential that each of the members can contribute something to the overall development of the projects. Although I have a lot to learn, I genuinely enjoy developing solutions that have a positive impact, and I can contribute quite a bit to any team I get the chance to work with. If there is anything I do not know, I am also ready to develop those skills quickly. I hope you give me a chance to work on providing better conditions for animals in this industry."},"KB/Wall-Street-Journal-task":{"title":"Wall Street Journal task","links":[],"tags":["dataset"],"content":"Wall Street Journal Task"},"KB/WaveGlow":{"title":"WaveGlow","links":["KB/GLOW"],"tags":["architecture"],"content":"WaveGlow\n\nWaveGlow: a Flow-based Generative Network for Speech Synthesis\nflow-based network capable of generating high quality speech from mel-spectrograms\ncombines insights from GLOW and WaveNet in order to provide fast, efficient and high-quality audio synthesis, without the need for auto-regression\nmplemented using only a single network, trained using only a single cost function: maximizing the likelihood of the training data, which makes the training procedure simple and stable\nmore than 500 kHz on an NVIDIA V100 GPU\nMean Opinion Scores\n"},"KB/Weak-Relation-Bias":{"title":"Weak Relation Bias","links":[],"tags":["graph"],"content":"Weak Relation Bias\n\nrelationship between the neural units is weak, meaning that they’re somewhat independent of each other. The choice of including a fully connected layer in the net can represent this kind of relationship\n\n"},"KB/Weakly-Supervised-Learning-Formulation":{"title":"Weakly Supervised Learning Formulation","links":[],"tags":["semisupervisedlearning"],"content":"Weakly Supervised Learning Formulation\n\nFor weakly supervised visual feature learning, given a dataset X, for each data Xi in X, there is a corresponding coarse-grained label Ci.\n\n"},"KB/Weakly-supervised-Learning":{"title":"Weakly-supervised Learning","links":[],"tags":["semisupervisedlearning"],"content":"Weakly-supervised Learning\n\nlearning methods to learn with coarse-grained labels or inaccurate labels\nThe cost of obtaining weak supervision labels is generally much cheaper than fine- grained labels for supervised methods.\n"},"KB/WebGPT":{"title":"WebGPT","links":["KB/GPT","KB/Rejection-Sampling"],"tags":["architecture"],"content":"WebGPT\n\nWebGPT: Browser-assisted Question-answering with Human Feedback\nfine-tuned version of GPT-3 to more accurately answer open-ended questions using a text-based web browser.\nsubmits search queries, follows links, and scrolls up and down web pages\ntrained to cite its sources\nBy setting up the task so that it can be performed by humans, they are able to train models on the task using imitation learning\nmodels must collect references while browsing in support of their answers\nELI5\ndataset of questions asked by Reddit users\nfine-tuning GPT-3 using behavior cloning, and then performing Rejection Sampling against a reward model trained to predict human preferences\n"},"KB/Weight-Decay-Vs-L2-Regularization":{"title":"Weight Decay Vs L2 Regularization","links":[],"tags":["regularization"],"content":"Weight Decay Vs L2 Regularization\n\nL2 regularization is a classic method to reduce over-fitting, and consists in adding to the loss function the sum of the squares of all the weights of the model, multiplied by a given hyper-parameter\n\nfinal_loss = loss + wd * all_weights.pow(2).sum() / 2\n\nwhere wd is the hyper-parameter to set\nThis is also called weight decay, because when applying vanilla SGD it’s equivalent to updating the weight like this\n\nw = w - lr * w.grad - lr * wd * w\n\nIn this equation we see how we subtract a little portion of the weight at each step, hence the name decay\nSo why make a distinction between those two concepts if they are the same thing\nThe answer is that they are only the same thing for vanilla SGD, but as soon as we add momentum, or use a more sophisticated optimizer like Adam, L2 regularization (first equation) and weight decay (second equation) become different\nWhen using the Adam optimizer, it gets even more different: in the case of L2 regularization we add this wd\\times w to the gradients then compute a moving average of the gradients and their squares before using both of them for the update. Whereas the weight decay method simply consists in doing the update, then subtract to each weight.\nAnd after experimenting with this, Ilya Loshchilov and Frank Hutter suggest in their article we should use weight decay with Adam, and not the L2 regularization that classic deep learning libraries implement.\nInside the step function of the optimizer, only the gradients are used to modify the parameters, the value of the parameters themselves isn’t used at all\nIt still has to be done after the gradients are computed\nthe optimizer should have been set with wd=0 otherwise it will do some L2 regularization, which is exactly what we don’t want\nloop over all the parameters and do our little weight decay update\n"},"KB/Weight":{"title":"Weight","links":[],"tags":["temp"],"content":"Weight\n\nmass x gravitational field \n                               strength\nw = mg\n"},"KB/Weighted-Alternating-Least-Squares":{"title":"Weighted Alternating Least Squares","links":[],"tags":["temp"],"content":"Weighted Alternating Least Squares\n\nAn algorithm for minimizing the objective function during matrix factorization in recommendation systems, which allows a downweighting of the missing examples. WALS minimizes the weighted [squared error](squared error.md) between the original matrix and the reconstruction by alternating between fixing the row factorization and column factorization.\n"},"KB/Wernicke-Area":{"title":"Wernicke Area","links":["KB/Aphasia"],"tags":["brain"],"content":"Wernicke Area\n\nDamage to this area causes Wernicke’s Aphasia.\nThe individual may speak in long sentences that have no meaning, add unnecessary words, and even create new words.\nThey can make speech sounds, however they have difficulty understanding speech and are therefore unaware of their mistakes.\n"},"KB/Wh-dependencies":{"title":"Wh-dependencies","links":[],"tags":["language"],"content":"Wh-dependencies\n\n[What type of network] do you plan to build [t]?\nNobody knows [what] the brain is doing [t].\n"},"KB/What-is-being-Transferred-in-transfer-learning":{"title":"What is being Transferred in transfer learning","links":["KB/Initialization"],"tags":["explainability"],"content":"What is being Transferred in transfer learning\n\n@neyshaburWhatBeingTransferred2020\n\nAbstract\n\ndesired capability for machines is the ability to transfer their knowledge of one domain to another where data is (usually) scarce\nwhat enables a successful transfer and which part of the network is responsible for tha\nseries of analyses on transferring to block-shuffled images, we separate the effect of feature reuse from learning low-level statistics of data and show that some benefit of transfer learning comes from the latter when training from pre-trained weights, the model stays in the same basin in the loss landscape and different instances of such model are similar in feature space and close in parameter space\n\nMain contributions and takeaways\n\nFor a successful transfer both feature-reuse and low-level statistics of the data are important.\nModels trained from pre-trained weights make similar mistakes on target domain, have similar features and are surprisingly close in distance in the parameter space\nhey are in the same basins of the loss landscape models trained from random Initialization do not live in the same basin, make different mistakes, have different features and are farther away in 2 distance in the parameter space\nModules in the lower layers are in charge of general features and modules in higher layers are more sensitive to perturbation of their parameters.\nT: trained, P: Pre-trained, RI: random Initialization. Therefore we use the following abbreviations for the four models throughout the paper: RI (random Initialization), P (pre-trained model), RI-T (model trained on target domain from random Initialization), P-T (model trained/fine-tuned on target domain starting from pre-trained weights)\nIMAGENET\nCHEXPERT\nDOMAINNET\n\nWhat is being transferred\nRole of feature reuse\n\nHuman visual system is compositional and hierarchical: neurons in the primary visual cortex (V1) respond to low level features like edges, while upper level neurons (e.g. the grandmother cell [Gross, 2002]) respond to complex semantic inputs Modern convolutional neural networks trained on large scale visual data are shown to form similar feature hierarchies [Bau et al., 2017, Girshick et al., 2014] The benefits of transfer learning are generally believed to come from reusing the pre-trained feature hierarchy useful when the downstream tasks are too small or not diverse enough to learn good feature representations\ncreate a series of modified downstream tasks which are increasingly distant from normal visual domains In particular, we partition the image of the downstream tasks into equal sized blocks and shuffle the blocks randomly The shuffling disrupts high level visual features in those images but keeps the low level statistics about the pixel values intact The extreme case of block size 224 x 224 means no shuffling; in the other extreme case, all the pixels in the image are shuffled, making any of the learned visual features in\npre-training completely useless\nWe conclude that feature reuse plays a very important role in transfer learning, especially when the downstream task shares similar visual features with the pre-training domain. But there are other factors at play: in these experiments we change the size of the shuffled blocks all the way to 1 and even try shuffling the channels of the input, therefore, the only object that is preserved here is the set of all pixel values which can be treated as a histogram/distribution We refer to those information as low-level statistics, to suggest that they are void of visual/semantic structural information. The low-level statistics lead to significant benefits of transfer learning, especially on optimization speed.\nWe observe that two instances of P-T are highly similar across different layers. owever, between P-T and RIT instance or two RI-T instances, the similarity is very low. feature similarity is much stronger in the penultimate layer than any earlier layers both between P-T and RI-T instance and two RI-T instances, however, still an order of magnitude smaller than similarity between two P-T layers.\nThese experiments show that the Initialization point, whether pre-trained or random, drastically impacts feature similarity, and although both networks are showing high accuracy, they are not that similar in the feature space. two P-T are reusing the same features\nDistance in parameter space istance between two models in the parameter space More specifically, we measure the L2 distance between 2 P-Ts and 2 RI-Ts, both per module and for the entire network Interestingly, RI-Ts are farther from each other compared to two P-Ts\ndistance between modules increases as we move towards higher layers in the network.\n\nPerformance barriers and basins in the loss landscape\n\nA commonly used criterion for better generalization performance is the flatness of the basin of the loss landscape near the final solution. In a flat basin, the weights could be locally perturbed without hurting the performance, while in a narrow basin, moving away from the minimizer would quickly hit a barrier, indicated by a sudden increase in the loss.\nWe evaluate a series of models along the linear interpolation of the two weights { 2 } any two minimizers of a deep network can be connected via a non-linear low-loss path Garipov et al. [2018], Draxler et al. [2018], Fort and Jastrzebski [2019] n contrast, due to the non-linear and compositional structure of neural networks, the linear combination of the weights of two good performing models does not necessarily define a well behaved model, thus performance barriers are generally expected along the linear interpolation path owever, in the case when the two solutions belong to the same flat basin of the loss landscape, the linear interpolation remains in the basin As a result, a performance barrier is absent interpolating two random solutions from the same basin could generally produce solutions closer to the center of the basin, which potentially have better generalization performance than the end points\nwe require that for most points on the basin, their convex combination is on the basin as well. This extra constraint would allow us to have multiple basins that may or may not be connected though a low-loss (nonlinear) path.\nGiven a loss function l: \\mathcal{R}^{n} \\rightarrow \\mathcal{R}^{+}, closed convex set S \\subset \\mathcal{R}^{n}, S is a (\\epsilon , \\delta) basin for l iff S has the following properties\n\nLet U_S be the uniform distribution over set S and \\mu_{S,l} be the expected value of the loss on samples generated from U_S. Then,\n\n\\mathcal{E}_{w \\sim U_{S}}[|\\mathcal{l}(w)-\\mu_{S, \\mathcal{l}}] \\leq \\epsilon\n\n\nFor any two points w_{1}, w_{2} \\in S, let f(w_{1}, w_{2}) = w_{1}+ \\overset{\\sim}\\alpha(w_{2}-w_{1}) where \\overset{\\sim}\\alpha = max\\{\\alpha|w_{1}+ \\alpha(w_{2}-w_{1})\\}\n\n\\mathcal{E}_{w_{1}, w_{2} \\sim U_{S}, v \\sim \\mathcal{N}(0, \\frac{\\delta^{2}}{n}I_{n})}[\\mathcal{l}(f(w_{1}, w_{2})+v )- \\mu_{S,l}] \\geq 2 \\epsilon\n\n\nLet \\kappa(w_{1}, w_{2}, v) = f(w_{1}, w_{2})+ \\frac{v}{||f(w_{1}, w_{2})-w_{1}||}(f(w_{1}, w_{2})-w_{1}). Then,\n\n\\mathcal{E}_{w_{1}, w_{2} \\sim U_{S}, v \\sim \\mathcal{N}(0,\\delta^{2})}[\\mathcal{l}(\\kappa(w_{1}, w_{2}, |v|))-\\mu_{S,l}] \\geq 2 \\epsilon\n\n\n\n\nthere are three requirements for a convex set to be a basin The first requirement is that for most points on the basin, their loss should be close to the expected value of the loss in the basin. This notion is very similar to requiring the loss to have low variance for points on the basin4. The last two requirements ensure that the loss of points in the vicinity of the basin is higher than the expected loss on the basin. In particular, the\nsecond requirement does that by adding Gaussian noise to the points in the basin and requiring the loss to be higher than the expected loss in the basin. The third requirement does something similar along the subspaces spanned by extrapolating the points in the basin. That is, if one exits the basin by extrapolating two points on the basin, the loss should increase.\nStarting with the two P-T solutions, we extrapolated beyond their connecting intervals to find the basin boundary, and calculated the parameters according to Definition 3.1. We found that each pair of P-T solutions live in a (0.0038, 49.14)-basin, (0.0054, 98.28)-basin and (0.0034, 49.14)-basin for rea, cipart and quickdraw, respectively\n\nModule Criticality\n\nGiven e &gt; 0 and network f_\\Theta ,\n\n\\mu_{i, \\epsilon}(f_{\\Theta})= \\underset{0 \\leq \\alpha_{i}, \\sigma_{i} \\leq 1}{min}{\\frac{\\alpha_{i}^{2}||\\theta_{i}^{F}-\\theta_{i}^{0}||^{2}_{Fr}}{\\sigma_{i}^{2}}}:\\{ \\mathbb{E}_{u \\sim \\mathcal{N}(0, \\sigma_{i}^{2})}[\\mathcal{L}_{S}(f_{\\theta_{i}^{\\alpha}}+u,\\Theta_{-i}^{F})] \\leq \\epsilon \\}\n\n\ndifferent layers of the network show different robustness to perturbation of their weight values [Zhang et al., 2019a]\nThey noted that for some modules, which they called critical, the performance of the model drops significantly after rewinding, while for others the performance is not impacted\nModule criticality is a measure that can be calculated for each module and captures how critical each module is.\nmodule criticality captures the role of the module in the generalization performance of the whole architecture and can be used as a measure of capacity of the module and predict the generalization performance we extend the definition of module criticality by looking at both direct path that linearly connect the initial and final value of the module and the optimization path generated by an optimizer from Initialization to the final solution\nWe also look into the final value of a weight matrix in addition to its optimal value during training as the start point of the path for investigating criticality\nWe note a similar pattern as observed in the supervised case. The only difference is that the ‘FC’ layer becomes critical for P-T model, which is expected.\n\nWhich pre-trained checkpoint is most useful for transfer learning?\n\nindependence between the improvements on optimization speed and final performance. Moreover, this is in line with the loss landscape observations in Section 3.3. Earlier checkpoints in pre-training are out of basin of the converged model and at some point during training we enter the basin (which is the same for pre-train and fine-tune models\nThis also explains the plateau of performance after some checkpoints. herefore, we can start from earlier\ncheckpoints in pre-training.\n\nConclusion\n\nWe investigated the role of feature reuse through shuffling the blocks of input and showed that when trained from pre-trained weights Initialization, the network stays in the same basin of the solution, features are similar and models are close in the L2 distance in parameter space confirmed that lower layers are in charge of more general features\none can use top singular values and directions for each module for Initialization and investigate if this suffices for good transfer, or ensure Initialization at the same basin but adding randomization to enhance diversity and improve generalization\ntaking model average of models in the same basin does not disturb the performance\n\nImages\n\n\n\n\n\n\n\n\n\n"},"KB/Whisper":{"title":"Whisper","links":[],"tags":["architecture"],"content":"Whisper\n\nAudio-to-Text converter\nmulti-lingual speech recognition, translation and language identification\ngoal of a speech recognition system should be to work reliably out of the box in a broad range of environments without requiring supervised fine-tuning of a decoder for every deployment distribution\nlack of a high-quality pre-trained decoder.\n680,000 hours of labeled audio data\nbroken in 30 second segments paired with the subset of the transcript that occurs\nwithin that time segment.\nencoder-deccoder transformer\n"},"KB/Whos-Thinking,-A-push-for-human-centered-evaluation-of-LLMs":{"title":"Whos Thinking, A push for human centered evaluation of LLMs","links":[],"tags":["explainability"],"content":"Whos Thinking, A Push for Human Centered Evaluation of LLMs\n\n@dattaWhoThinkingPush2023\n\nABSTRACT\n\nIn this paper, we draw parallels between the relatively mature field of XAI and the rapidly evolving research boom around large language models (LLMs)\nSpecifically, we argue that humans’ tendencies— again, complete with their cognitive biases and quirks—should rest front and center when evaluating deployed LLMs\n\nLarge Language Models\n\nCurrent LLM evaluation mechanisms include quantitative metrics measuring notions of accuracy (how similar are the generated outputs to the expected outputs), robustness (how resilient is the model to transformations of the input), calibration (how meaningful are the generated probabilities in respect to uncertainty), efficiency (what are the energy, carbon, and time costs for training and inference) and more [35].\nThere are substantial environmental costs associated with the volume of computational power required for training and inference [6, 49]\nCounterfactual fairness [24] examines how perturbing the demographic signals of existing test examples can change the performance of the model (e.g. “He worked at the local hospital” versus “She worked at the local hospital”)\nother concerning forms of biases such as stereotypical associations, erasure, and over-representation in the semantics of its output [35, 38]\nLLMs have been shown to produce toxic outputs.\nLLMs often suffer from factual errors—they can “hallucinate” information [50] by providing very confident-sounding but entirely false responses\nChatbot LLMs have also been found to engage in disturbingly emotional personal conversations when session lengths are not limited [47].\nThere are also privacy concernswork has shown that LLMs are susceptible to training data leakage under adversarial attack [11].\n\nPARALLELS BETWEEN XAI AND LLMS\n\n\nLLM outputs are often meant for some downstream decision or task—what email to send to your client, what quick summary of an important document you will read, what answer is provided for a pertinent question\n\n\nAdvocates push for first identifying a specific use case, then understanding the types of transparency useful and relevant for each stakeholder in that context, then designing the explanation method with these learnings held front and center, and finally evaluating how helpful the explanation was for specific tasks through practitioner user studies\n\n\nA system is valid if it does what it purports to do. If it doesn’t, it is likely because there are issues of alignment between the intent of the system builder and the property the algorithm optimizes [40].\n\n\n[Cognitive Engagement](Cognitive Engagement.md)\n\n\n[Mental Model Matching](Mental Model Matching.md)\n\n\n[Use Case Utility](Use Case Utility.md)\n\n\nWHY IS THIS IMPORTANT\n\nthe potential scale of LLM usage is massive as a fundamentally lower-level ML-\nbased building block than XAI\nChatGPT [43] set historic records for its customer growth, with over 100 million users in its first 2 months [2].\nUnlike XAI, whose users are largely technical practitioners [7] focusing on one pipeline, LLMs are often designed for public use and are meant to help with a wide variety of tasks.\nThe ability to influence or make decisions is a form of inherent power, and offloading cognition onto AI agents must first be met with caution.\nThe consequences of not having a qualitative understanding of how humans interact with LLM outputs is grave\n"},"KB/Wickelphones":{"title":"Wickelphones","links":["KB/Verb","KB/Features","KB/past-tense"],"tags":["language"],"content":"Wickelphones\n\nPro: capture just enough info about the context that determines irregular past-tense Verb forms ense verbs, e.g.sing -sang, ring -rang\n1 wickelphone = 1 input unit and 1 output unit, connection matrix of 42875 * 42875!!\nInstead, reduce wickelphones to wickelfeatures (1210), where each wickelphone becomes 16 Features\nVerbs with same stem and past tense\nEnglish has many verbs where the past and stem are the same:\n\nput, fit, spread\n\n\nUsually verbs ending with -t or -d are likely have no-change\nModel also started to let the past be the same as the stem\nVerbs with vowel change\nModel made two errors older children have been shown to make: 1. stem + ed, e.g. comed, singed 2. past-form + ed, e.g. camed, sanged\nDoes not explain differences in response times between irregular and regular verbs\nModels production only, not comprehension\nYou can’t reverse the model like you can reverse a rule\nModel can’t generalize\nComputational models (Rule-based, Connectionist and MBL) all use adult vocabulary as input to simulate children’s learning\nChildren don’t show vocabulary burst between State 2 and 3 (needed to produce U-shaped learning with Connectionist Model)\nThousands of exposures\nNo distinction between tokens and types, each Verb simply included once\n"},"KB/Wide-Deep-Recommender":{"title":"Wide Deep Recommender","links":["KB/Features","Tag-Pages/loss"],"tags":["architecture"],"content":"Wide Deep Recommender\n\nWide &amp; Deep Learning for Recommender Systems\nGeneralized linear models with nonlinear feature transformations are widely used for large-scale regression and classification problems with sparse inputs.\nMemorization of feature interactions through a wide set of cross-product feature transformations are effective and interpretable, while generalization requires more feature engineering effort\nHowever, memorization and generalization are both important for recommender systems.\nWith less feature engineering, deep neural networks can generalize better to unseen feature combinations through low-dimensional dense embeddings learned for the sparse Features\nHowever, deep neural networks with embeddings can over-generalize and recommend less relevant items when the user-item interactions are sparse and high-rank.\njointly trained wide linear models and deep neural networks – to combine the benefits of memorization and generalization for recommender systems\nWide linear models can effectively memorize sparse feature interactions using cross-product feature transformations, while deep neural networks can generalize to previously unseen feature interactions through low dimensional embeddings\nIn other words, the fusion of wide and deep models combines the strengths of memorization and generalization, and provides us with better recommendation systems\nThe two models are trained jointly with the same loss function.\nGoogle Play Store\n"},"KB/Width-Efficiency-of-Neural-Networks":{"title":"Width Efficiency of Neural Networks","links":[],"tags":["deeplearning"],"content":"Width Efficiency of Neural Networks\n \n\nthere exist classes of wide, shallow networks that can only be expressed by narrow networks with polynomial depth\npolynomial lower bound on width is less restrictive than the exponential lower bound on depth, suggesting that depth is more important\nthe price for making the width small is only a linear increase in the network depth for networks with ReLU activation\n"},"KB/Window-Based-Regression":{"title":"Window Based Regression","links":["KB/TIme-Series","KB/Quadratic-Loss","KB/Volterra-expansion"],"tags":["architecture"],"content":"Window Based Regression\n\nTIme Series\ninput window : u(t-d+1), u(t-d+2), …. , u(t-1) , u(t)\nRequire regression function f:(\\mathbb{R}^k)^d \\rightarrow \\mathbb{R}^m\n\nk \\times d dim matrix\nFlatten into d \\cdot k vector and apply Quadratic Loss\n\n\n\nNon Linearity\n\nAdd fixed nonlinear transforms to input arguments : eg polynomials\nVolterra expansion\n"},"KB/Wisdom-of-the-Crowd":{"title":"Wisdom of the Crowd","links":[],"tags":["temp"],"content":"Wisdom of the Crowd\n\nThe idea that averaging the opinions or estimates of a large group of people (“the crowd”) often produces surprisingly good results.\n"},"KB/Word-Blending":{"title":"Word Blending","links":[],"tags":["language"],"content":"Word Blending\n\nParts of two different words are combined\nBreakfast+lunch : brunch\nSmoke+fog:smog\n"},"KB/Word-Clipping":{"title":"Word Clipping","links":[],"tags":["language"],"content":"Word Clipping\n\nLonger words are shortened\nDoctor, laboratory, refrigerator\n"},"KB/Word-Compounding":{"title":"Word Compounding","links":[],"tags":["language"],"content":"Word Compounding\n\nWords formed by combining two or more words\nAdj +Adj= Adj bitter + sweet : bitter-sweet\nN + N = N rain+bow rain-bow\n"},"KB/Word-Segmentation":{"title":"Word Segmentation","links":["KB/Maximum-Matching-Algorithm","KB/Forward-Backward-Matching","KB/Statistical-Word-Segmentation","KB/Lexical-Word-Segmentation","KB/Hybrid-Word-Segmentation"],"tags":["language"],"content":"Word Segmentation\n\nbreaks up the sequence of characters in a text by locating the word boundaries\nMaximum Matching Algorithm\nForward Backward Matching\nStatistical Word Segmentation\nLexical Word Segmentation\nHybrid Word Segmentation\n"},"KB/Word-Structure":{"title":"Word Structure","links":["KB/Isolating-words","KB/Agglutinating-words","KB/Inflectional-words","KB/Polysynthetic-words"],"tags":["language"],"content":"Word Structure\n\nIsolating words\nAgglutinating words\nInflectional words\nPolysynthetic words\n"},"KB/Word-Vectors":{"title":"Word Vectors","links":["KB/Embedding","KB/GloVE"],"tags":["temp"],"content":"Word Vectors\n\nEssentially word Embedding\nText processing\nWe can represent ideas/sentences/documents as vectors to feed into any kind of model\nUseful because vectors can be easily compared to find similarity\n\neg : [Cosine Similarity](Cosine Similarity.md)\n\n\nHigher dimensions make it challenging\n\nVectors that are metrically close to each other\nGloVE\n\n"},"KB/Word2Vec":{"title":"Word2Vec","links":["KB/Word-Vectors"],"tags":["architecture"],"content":"Word2Vec\n\nEfficient Estimation of Word Representations in Vector Space\nDuality of vector representations of words derived by various models on a collection of syntactic and semantic language tasks involving word similarity\npossible to train high quality Word Vectors using very simple model architectures\n[Skip Gram] or [CBOW](Skip Gram] or [CBOW.md)\n\nTraining\n\nAsk it to predict a vector with probabilities\nFind error vector\nUpdate params\nto generate high-quality embeddings using a high-performance model, we can switch the model’s task from predicting a neighboring word And switch it to a model that takes the input and output word, and outputs a score indicating if they’re neighbors or not (0 for “not neighbors”, 1 for “neighbors”).\nThis simple switch changes the model we need from a neural network, to a logistic regression model – thus it becomes much simpler and much faster to calculate. + [Negative Sampling](Negative Sampling.md)\nEmbedding and Context matrices randomly initialized\n\n\n\nHyperparams\nWindow Size\n\nsmaller window sizes (2-15) lead to embeddings where high similarity scores between two embeddings indicates that the words are interchangeable (notice that antonyms are often interchangable if we’re only looking at their surrounding words – e.g. good and bad often appear in similar contexts).\nLarger window sizes (15-50, or even more) lead to embeddings where similarity is more indicative of relatedness of the words\ndefault is 5 (two words - word - two words)\n\nNo of Negative Samples\n\nThe original paper prescribes 5-20 as being a good number of negative samples. It also states that 2-5 seems to be enough when you have a large enough dataset. The Gensim default is 5 negative samples.\n"},"KB/Words-and-Rules":{"title":"Words-and-Rules","links":["KB/Verb","KB/past-tense","KB/Declarative-memory","KB/Challenges-of-Words-and-rules"],"tags":["language"],"content":"Words-and-Rules\n\nLook up past-tense\n- No past-tense stored? Generate form\n\nWhy symbolic? It uses the abstract ‘Verb’\nRules that refer to these categories are used to guide processing\npast tense\nIrregular forms stored in associative memory (Declarative memory)\nSymbolic rules produce past tense forms (procedural memory)\nlook-up is quicker than rule application\nrule application takes more time but is always done ‘on-the-fly’\nTime 1: Every form is memorized (irregular and regular)\nTime 2: child notices pattern: Verb root+ed = past\nChild creates a rule\nChild applies rule to all forms: overgeneralization\nTime 3: Child realizes that there are irregular and regular forms\n\ncreates a dual system: irregular forms are retrieved from memory, regular forms are created by a rule\nnew and novel verbs will get regular endings in past tense unless exposed to irregular past\nthe fact that children seem to learn a rule = language must be symbolic\n\n\nTraditional U-shaped learning predicts children won’t be able to create past-tense forms for novel verbs when they are in the initial stage. This isn’t consistent with data\nOverregularization is not common\nCannot account for the presence of two past forms\n\ne.g. dream/dreamed-dreamt, light/lit-lighted\n\n\nIn production experiments\n\nIrregulars produced faster\nFrequent irregulars are produced faster than infrequent irregulars (Prasada et al., 1990; Albright &amp; Hayes 2003)\nNo difference between frequent and infrequent regulars\n\n\nMaybe because experiments always present the root form? (1) This is a girl who knows how to dance. She did the same\n\nRoot presentation might mask differences due to frequency in regulars\n\n\n\n\nChallenges of Words-and-rules\n"},"KB/Work-Envelope":{"title":"Work Envelope","links":["KB/Manipulator"],"tags":["robotics"],"content":"Work Envelope\n\nThe set of all points which a Manipulator can reach without intrusion. Sometimes the shape of the work space, and the position of the Manipulator itself can restrict the work envelope.\n"},"KB/Wrist":{"title":"Wrist","links":["KB/End-effector","KB/Degrees-of-Freedom","KB/Roll","KB/Yaw"],"tags":["robotics"],"content":"Wrist\n\nA set of rotary joints between the arm and the robot End-effector that allow the End-effector to be oriented to the work-piece. In most cases the wrist can have Degrees of Freedom which enable it to grasp an object with Roll, pitch, and Yaw orientation.\n"},"KB/X-Vectors":{"title":"X Vectors","links":["Augmentation","KB/Impulse","KB/MUSAN","KB/Speakers-in-the-Wild","KB/NIST-SRE-2016-Cantonese"],"tags":["architecture"],"content":"X Vectors\n\nX-Vectors: Robust DNN Embeddings for Speaker Recognition\ndata augmentation to improve performance of deep neural network (DNN) embeddings for speaker recognition\ntrained to discriminate between speakers, maps variable-length utterances to fixed-dimensional embeddings called x-vectors\nprior studies have found that embeddings leverage large-scale training datasets better than i-vectors, it can be challenging to collect substantial quantities of labeled data for training\nuse data augmentation, consisting of added noise and reverberation, as an inexpensive method to multiply the amount of training data and improve robustness\nTheir data augmentation strategy employs additive noises and reverberation\nReverberation involves convolving room Impulse responses (RIR) with audio\nsimulated RIRs described by Ko et al.\nreverberation itself is performed with the multicondition training tools in the Kaldi ASpIRE recipe\nFor additive noise, they use the MUSAN dataset,\nPLDA classifier is used in the x-vector framework to make the final decision, similar to i-vector systems\nx-vectors are compared with i-vector baselines on Speakers in the Wild and NIST SRE 2016 Cantonese where they achieve superior performance on the evaluation datasets\n"},"KB/XAI":{"title":"XAI","links":[],"tags":["explainability"],"content":"XAI\ngraph LR;\n\nNode_0[deep inside convolutional networks visualising image classification models and saliency maps]\nNode_31[striving for simplicity the all convolutional net]\nNode_35[the unreliability of saliency methods]\nNode_3[real time image saliency for black box classifiers]\nNode_33[network dissection quantifying interpretability of deep visual representations]\nNode_5[understanding deep networks via extremal perturbations and smooth masks]\nNode_36[methods for interpreting and understanding deep neural networks]\nNode_38[explaining explanations an overview of interpretability of machine learning]\nNode_8[explainable artificial intelligence xai concepts taxonomies opportunities and challenges toward responsible ai]\nNode_9[did the model understand the question]\nNode_29[smoothgrad removing noise by adding noise]\nNode_34[how important is a neuron]\nNode_12[computationally efficient measures of internal neuron importance]\nNode_30[learning important features through propagating activation differences]\nNode_46[a unified approach to interpreting model predictions]\nNode_15[influencedirected explanations for deep convolutional networks]\nNode_40[towards better understanding of gradientbased attribution methods for deep neural networks]\nNode_17[a survey on neural network interpretability]\nNode_18[opportunities and challenges in explainable artificial intelligence xai a survey]\nNode_19[explainable artificial intelligence for tabular data a survey]\nNode_47[peeking inside the blackbox a survey on explainable artificial intelligence xai]\nNode_21[explainable artificial intelligence xai in deep learningbased medical image analysis]\nNode_22[a systematic review of human computer interaction and explainable artificial intelligence in healthcare with artificial intelligence techniques]\nNode_42[explainer a visual analytics framework for interactive and explainable machine learning]\nNode_24[visual analytics for humancentered machine learning]\nNode_25[exploiting explanations for model inversion attacks]\nNode_26[if only we had better counterfactual explanations five key deficits to rectify in the evaluation of counterfactual xai techniques]\nNode_27[visualizing and understanding convolutional networks]\nNode_28[network in network]\nNode_32[sanity checks for saliency maps]\nNode_37[interpretation of neural networks is fragile]\nNode_39[explanations can be manipulated and geometry is to blame]\nNode_41[distilling the knowledge in a neural network]\nNode_43[why should i trust you explaining the predictions of any classifier]\nNode_44[explainable ai beware of inmates running the asylum or how i learnt to stop worrying and love the social and behavioural sciences]\nNode_45[explanation in humanai systems a literature metareview synopsis of key ideas and publications and bibliography for explainable ai]\nNode_0 --&gt; Node_27\nNode_31 --&gt; Node_28\nNode_31 --&gt; Node_27\nNode_35 --&gt; Node_29\nNode_35 --&gt; Node_30\nNode_35 --&gt; Node_27\nNode_3 --&gt; Node_31\nNode_3 --&gt; Node_27\nNode_33 --&gt; Node_27\nNode_5 --&gt; Node_31\nNode_5 --&gt; Node_29\nNode_5 --&gt; Node_32\nNode_5 --&gt; Node_27\nNode_36 --&gt; Node_31\nNode_36 --&gt; Node_27\nNode_38 --&gt; Node_33\nNode_38 --&gt; Node_30\nNode_38 --&gt; Node_27\nNode_8 --&gt; Node_28\nNode_9 --&gt; Node_31\nNode_9 --&gt; Node_30\nNode_29 --&gt; Node_30\nNode_34 --&gt; Node_30\nNode_12 --&gt; Node_34\nNode_12 --&gt; Node_30\nNode_30 --&gt; Node_31\nNode_46 --&gt; Node_30\nNode_15 --&gt; Node_31\nNode_15 --&gt; Node_35\nNode_15 --&gt; Node_34\nNode_40 --&gt; Node_36\nNode_40 --&gt; Node_29\nNode_40 --&gt; Node_30\nNode_40 --&gt; Node_27\nNode_17 --&gt; Node_31\nNode_17 --&gt; Node_37\nNode_17 --&gt; Node_35\nNode_17 --&gt; Node_33\nNode_17 --&gt; Node_36\nNode_17 --&gt; Node_38\nNode_17 --&gt; Node_39\nNode_17 --&gt; Node_32\nNode_17 --&gt; Node_30\nNode_17 --&gt; Node_27\nNode_17 --&gt; Node_40\nNode_18 --&gt; Node_31\nNode_18 --&gt; Node_37\nNode_18 --&gt; Node_35\nNode_18 --&gt; Node_32\nNode_18 --&gt; Node_30\nNode_18 --&gt; Node_27\nNode_18 --&gt; Node_40\nNode_19 --&gt; Node_30\nNode_47 --&gt; Node_41\nNode_47 --&gt; Node_33\nNode_47 --&gt; Node_29\nNode_47 --&gt; Node_27\nNode_21 --&gt; Node_31\nNode_21 --&gt; Node_32\nNode_21 --&gt; Node_27\nNode_22 --&gt; Node_42\nNode_42 --&gt; Node_30\nNode_42 --&gt; Node_40\nNode_24 --&gt; Node_42\nNode_25 --&gt; Node_33\nNode_25 --&gt; Node_38\nNode_25 --&gt; Node_29\nNode_25 --&gt; Node_32\nNode_25 --&gt; Node_30\nNode_25 --&gt; Node_27\nNode_26 --&gt; Node_43\nNode_26 --&gt; Node_44\nNode_26 --&gt; Node_45\nNode_26 --&gt; Node_46\nNode_26 --&gt; Node_27\nNode_26 --&gt; Node_47\n"},"KB/XLM-R":{"title":"XLM-R","links":["KB/CommonCrawl"],"tags":["architecture"],"content":"XLM-R\n\nUnsupervised Cross-lingual Representation Learning at Scale\npretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks\nTransformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data\nsignificantly outperforms multilingual BERT\nlow-resource languages\npositive transfer and capacity dilution\nperformance of high and low resource languages at scale\npossibility of multilingual modeling without sacrificing per-language performance\n"},"KB/XLNet":{"title":"XLNet","links":["KB/Autoregressive"],"tags":["architecture"],"content":"XLNet\n\nXLNet: Generalized Autoregressive Pretraining for Language Understanding\nmodeling bidirectional contexts\ndenoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on Autoregressive language modeling\nHowever, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy\ngeneralized [autoregressive] pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order (thereby proposing a new objective called Permutation Language Modeling), and (2) overcomes the limitations of BERT thanks to its [autoregressive](autoregressive] pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order (thereby proposing a new objective called Permutation Language Modeling), and (2) overcomes the limitations of BERT thanks to its [autoregressive.md) formulation\nuses a permutation language modeling objective to combine the advantages of Autoregressive and autoencoder methods\n"},"KB/XLSR":{"title":"XLSR","links":[],"tags":["dataset"],"content":"XLSR"},"KB/Xavier-Initialization":{"title":"Xavier Initialization","links":["KB/Initialization","KB/Batch-Normalization","KB/Layers","KB/Tanh"],"tags":["regularization"],"content":"Xavier Initialization\n\n\\mathrm{a=\\sqrt{\\frac{6}{\\left(\\mathrm{d}\\mathrm{_{\\mathrm{in}}^{\\mathrm{ }}}+\\mathrm{d}_{\\mathrm{out}} \\right)}}}\nRandom values drawn uniformly from [-a,a]\nFor Batch Normalization Layers, \\gamma =1 and \\beta=0\nFor Tanh based activating neural nets\n"},"KB/Xception":{"title":"Xception","links":["KB/Depthwise-Separable","KB/Inception"],"tags":["architecture"],"content":"Xception\n\n@cholletXceptionDeepLearning2017\nOnly use Depthwise Separable convs + Inception modules\nCross channel and spatial correlations can be decoupled completely\n"},"KB/YFCC100M":{"title":"YFCC100M","links":[],"tags":["dataset"],"content":"YFCC100M\n\nYahoo Flickr Creative Commons 100 Million Dataset\nlarge public multimedia collection from Flickr, consisting of 100 million media data, of which around 99.2 million are images and 0.8 million are videos\nstatistics on hashtags used in the YFCC100M dataset show that the data distribution is severely unbalanced\n"},"KB/YOLO":{"title":"YOLO","links":["KB/LinearRegression","Tag-Pages/loss","KB/Picasso-Dataset","KB/People-Art-Dataset"],"tags":["architecture"],"content":"YOLO\n\nYou Only Look Once: Unified, Real-Time Object Detection\nLinearRegression problem\npredicts bounding boxes and class probabilities directly from full images in one evaluation\nloss function that directly corresponds to detection performance and the entire model is trained jointly\nPicasso Dataset, People Art Dataset\n"},"KB/Yaw":{"title":"Yaw","links":["KB/End-effector","KB/Manipulator"],"tags":["robotics"],"content":"Yaw\n\nRotation of the End-effector in a horizontal plane around the end of the Manipulator arm. Side to side motion at an axis.\n"},"KB/You-can-play-20-questions-with-nature-and-win":{"title":"You Can Play 20 Questions with Nature and Win","links":[],"tags":["cognitivemodel"],"content":"You Can Play 20 Questions with Nature and Win\n\nYou can play 20 questions with nature and win: Categorical versus coordinate spatial relations as a case study\nStephen M. Kosslyn\n\nIntro\n\nAlan Newell famously asserted that “You can’t play 20 questions with nature and win” (Newell, A. (1973)\nfocused on the futility of studying binary distinctions\nHowever, the distinction between categorical and coordinate spatial relations representations has turned out to be fruitful\nFirst, from the outset this distinction was cast within the context of a theory of a more general processing system; second, it was formulated from the perspective of multiple levels of analysis within a processing system, and thereby bridges characteristics of information processing with characteristics of the brain.\nIn the game of 20 questions, one player thinks of an object or situation, and the others attempt to guess it by asking a series of binary questions: is it living? is it an animal? is it domesticated?\nEach question reduces the search space, and eventually a questioner can pounce on just the right answer\nNewell argued that this game is a bad model for how science should be conducted.\ndecried the tendency of psychologists to formulate and test binary distinctions— such as those between episodic versus semantic memory, serial versus parallel search, and gradual versus all-or-none learning\nmore often than not such distinctions are illusory, and after an enormous amount of research ultimately all we know is that nature resists clear-cut binary divisions.\nWe should consider how to fit the available data together into a single coherent story.\nAnd in his view, the best way to do this is to attempt to build computer simulation models that mimic human performance.\n\nDrawing Distinctions Within Processing Systems\n\nfundamental problem with most (if not all) of the binary distinctions that Newell railed against\ndistinctions were formulated independently of concerns about how the putative representations or processes would operate within the context of a more general processing system\n\nDivide-and-conquer\n\ncomplex tasks never are accomplished by a single process, all in one swoop\nmost tasks are treated as if they are combinations of simpler sub-tasks, each of which is grappled with by a separate aspect of the overall processing system.\nbrain has clearly divided processing of object properties, such as shape and color, from processing of spatial properties, such as location\nLocation is registered by a system that processes spatial properties—the socalled “dorsal system”, which runs from the occipital lobe to posterior parietal cortex.\nThus, the two problems (recognizing objects in different locations and being able to specify location) have contradictory requirements—and it is rather elegant that the brain deals with each in a separate system.\n\nWeak Modularity\n\nThe brain has numerous specialized systems\nBut these systems are not “modules” of the sort proposed by Fodor (1983).\nFodor’s modules are independent, in the sense that the workings of one cannot affect the inner workings of another\nHowever, given the nature of the neuroanatomy of the brain, we are better off conceptualizing processing in terms of neural networks— which may share some cortex and some types of processing.\nMoreover, we should expect “leakage” between these systems. Aspects of a theory of high-level vision\nKosslyn &amp; Koenig, 1992)\n\nAspects of High Level Vision\nVisual Buffer\n\nvisual input during perception is organized in a series of [brain areas](brain areas.md) in the occipital lobe, which I have grouped into a single function structure called the visual buffer\nThese areas are topographically organized, such that the pattern of activation over the surface of the cortex (roughly) preserves the pattern of activation on the retina.\nMost of the connections among neurons in these areas are short and inhibitory\nThe output from the visual buffer is a representation of edges and regions of an object\n\nObject Properties Processing System\n\nOutput from the visual buffer flows into the ventral system, where it is compared to stored visual memories\nIf a match is found, the object (or part of an object) is recognized. Spatial properties processing system\nOutput from the visual buffer also flows into the dorsal system, where location and other spatial properties are computed.\n\nLong-term Associative Memory\n\nThe outputs from the object properties processing and spatial properties processing systems converge on long-term associative memories\nSuch memories specify the spatial relations among objects or parts of objects. A problem in vision and a possible solution\nThe distinction between the ventral and dorsal systems makes sense from the perspective of the two principles briefly outlined earlier, divide-and-conquer and weak modularity\nHow can the visual system identify objects when they can project an almost infinite number of images?\nno new parts are added to the image when the object is contorted in its many and varied ways, although some parts may be occluded\nThus, if a sufficient number of individual parts can be recognized, this is a strong indication that a specific object is present\nthe spatial relations between parts remain constant if they are described in a relatively abstract way\n\nCategorical Spatial Relation\n\nA category is an equivalence class; for instance, if you hold one hand next to the other, the first will remain left or right of the second no matter how high, low, or far away it is from the other hand. Once assigned to the category, the spatial relations are treated as equivalent, with any differences (e.g. between a bent versus outstretched arm) ignored.\nHowever, the dorsal system cannot compute only categorical spatial relations representations.\nSuch representations are useless for another key role of the dorsal system, namely reaching and navigation\nKnowing that a table is “in front of” you (a categorical spatial relation) will not help you walk around it, or pull your chair up to it\nIn these cases you need precise metric information, and you need such\ninformation relative to your body, a part of your body, or relative to another object that serves as an “origin”\n\nCoordinate Spatial Relation\n\ncategorical spatial relations representations typically can be captured by a word or two, and the left cerebral hemisphere is better than the right at such processing\ncoordinate spatial relations representations are essential for navigation, and the right cerebral hemisphere is better than the left at such processing\nIn short, here is an example of a situation where 20 questions seems to be working\nAt the first cut, we divided the entire system into two coarsely defined subsystems, distinguishing between the object-properties-processing ventral system and the spatial-properties-processing dorsal system\nAt the second cut, we focused on the dorsal system, and now divided it into two more finely characterized subsystems, which compute categorical versus coordinate spatial relations representations.\n\nLevels of Analysis\n\nbased largely on that of Marr (1982), but adapted in various ways to be more appropriate for cognitive processing rather than vision per se\na fundamental characteristic of a theory of a processing system is that it begins with an analysis of the task to be accomplished\nThe theory of the computation can be conceptualized as specifying a black box, which takes a specific input and produces a specific output; this output in turn is used as input to yet other processes.\nAccording to Marr, whereas a theory of the computation describes what is computed, a theory of the algorithm specifies how it is computed.\nAn algorithm consists of a step-by-step procedure that guarantees that a certain output will be produced on the basis of a certain input.\nFinally, algorithms are implemented in hardware (on a computer) or “wetware” (in a brain)\nThe level of the implementation specifies how an algorithm is physically realized\nThis observation seems particularly relevant to the encoding and use of spatial relations representations (e.g. Baciu et al., 1999; Kosslyn et al., 1998).\nInterdependence among levels\nMarr sometimes wrote as if a theory at one level of analysis could be formulated with only weak links to theories at the other levels.\nHowever, computations rely on algorithms, and those algorithms have to operate in\na brain that does some things well and other things not so well\nIn addition, as evolution progressed, older parts of the brain often were relatively preserved—new areas were added, but the old ones rarely were redesigned from scratch.\nThus, the newer portions had to work with the older ones, which may not have been optimal for the final product (cf. Allman, 1999).\ncharacteristics at each of the levels of analysis affect theorizing at the other levels— and hence a powerful approach to theorizing about cognition requires that all three levels of analysis be considered at the same time.\nAt the level of the algorithm, conceptualizing processing within the context of the larger system played a central role; the fact that object properties and spatial properties are processed separately provided a key constraint on the theory of what is computed and how such computation proceeds\nthe idea that the two cerebral hemispheres would differ for the two kinds of processing not only helps to specify the nature of the representations and processes, but also offers one way to test the hypothesis.\nLeveraging multi-level theories\n\nWhy is it Important That Scientists Be Able to Play 20 Questions with Nature and Win?\n\nOne reason is simple: cognitive processing is extraordinarily complex, and we must find ways to gain traction in studying it.\nI argue that multi-level theories, which bridge from information processing to the brain, should play a special role in playing the science game of 20 questions.\nFirst, they lead researchers to collect different sorts of data.\nwhen theorizing on the basis of such varied types of data, there are more constraints on the theory.\nMoreover, multi-level theories must respect qualitatively different sorts of constraints simultaneously\nPerhaps paradoxically, the more constraints that are available the easier it is to theorize, even though it is more difficult to fit all the constraints together within a common framework\nNewell was troubled not simply by the failure of most binary distinctions to lead to fruitful research, but also by the lack of accumulation of such results.\nHe had the sense that research was not accumulating to paint a coherent overall picture, but instead isolated fragments of knowledge were being collected.\nThe brain is, after all, a single organ.\n"},"KB/Z-Normalization":{"title":"Z Normalization","links":[],"tags":["temp"],"content":"Z Normalization\n\nCentering and rescaling the data so that a zero mean and unit variance is obtained.\nOnly compute the parameters μk,σk on the training set!\nFor image data, normalization is not done per pixel but computed over all the pixels\n"},"KB/Z-Space-Entanglement":{"title":"Z-Space Entanglement","links":[],"tags":["architecture"],"content":"Z-Space Entanglement\n\nAnother challenge with controllable generation is referred to as entanglement in Z-space.\nWhen the z-space is entangled, this means movement in different directions has an effect on multiple features in the output simultaneously.\nEven if these features aren’t correlated, an entangled z-space results in a single feature change modifying more than one feature in the output.\nEntanglement happens commonly if the number of dimensions in the z-space isn’t large enough\n"},"KB/Zeiler-Fergus":{"title":"Zeiler Fergus","links":["KB/Layers","KB/Conv","KB/Activation-Functions","KB/Pooling"],"tags":["architecture"],"content":"Zeiler Fergus\n\nmultiple interleaved Layers of Conv, non-linear Activation Functions, local response normalizations, and max Pooling\n"},"KB/Zero-Label-Language-Learning":{"title":"Zero Label Language Learning","links":["KB/Unsupervised-Data-Generation"],"tags":["temp"],"content":"Zero Label Language Learning\n\nTowards Zero-Label Language Learning\nUnsupervised Data Generation\nSuperGLUE\nTreat LMs as few-shot generators (rather than few-shot learners)\nCreate prompts with &lt;sample, label&gt; pair(s)\nAsk the model to generate more for the same label\nThe emphasis is on the labelled data generation (rather than inference)\nThe new idea is about generating more data and going with conventional route\nThis paper confirms all the above by introducing UDG using LMs, even for complex higher-order tasks and empirically shows classical fine-tuning with more data works better.\n"},"KB/architecture":{"title":"architecture","links":["KB/ADVENT","KB/ALBERT","KB/Adagrad","KB/Adaptive-Input-Representation","KB/Additive-Attention","KB/Affordance-Detection-Task-Specific","KB/Alex-Net","KB/Alphacode","KB/Attention-NMT","KB/Attention","KB/AudioLM","KB/Auto-Encoders","KB/AutoDistill","KB/BART","KB/BERT","KB/Bahdanau-Attention","KB/Basic-GAN","KB/Basic-RNN-Architectures","KB/Basic-Transformer","KB/Beam-search","KB/Bi-Directional-RNN","KB/Bias-nodes","KB/Big-Bird","KB/BinaryBERT","KB/BlockNeRF","KB/CLIP","KB/Capsule-Layer","KB/Capsule-Network","KB/Chat-GPT-is-Not-All-You-Need","KB/ChatGPT","KB/Chinchilla","KB/Classifier-Gradients","KB/Codex","KB/Collaborative-Topic-Regression","KB/Conditional-GAN","KB/Conformer","KB/Content-Based-Attention","KB/Contrastive-Predictive-Coding","KB/ConvBERT","KB/ConvNeXt","KB/Convolutional-RNN","KB/Curriculum-Learning","KB/CycleGAN","KB/DALL-E-3","KB/DALL-E","KB/DALL·E-2","KB/DCGAN","KB/DLRM","KB/DeepFM","KB/DeepNet","KB/DeepPERF","KB/Denoising-Autoencoder","KB/Dense-Net","KB/Diffusion-LM","KB/Dilated-Sliding-Window-Attention","KB/DistillBERT","KB/Dot-Product-Attention","KB/Dreamfusion","KB/Dynamic-Sparsity","KB/ELECTRA","KB/ELMO","KB/EfficientNet","KB/Elu","KB/Encoder-Decoder-Attention","KB/Ensemble-Distillation","KB/FLASH","KB/FLAVA","KB/FaceNet","KB/Factorized-Embedding-Parameters","KB/Familar-Object-Grasping-Object-Viiew-recog","KB/FastText","KB/Faster-RCNN","KB/Feature-Correlationa","KB/Fixed-Factorization-Attention","KB/Flamingo","KB/FlowNet","KB/GAN-Z-Space","KB/GAU","KB/GELU","KB/GGCNN","KB/GLOW","KB/GPT","KB/GPT3","KB/GRConvNet","KB/GRU","KB/Galactica","KB/Gato","KB/Generative-Models","KB/Generative-RNN","KB/Generative-Spoken-Language-Modeling","KB/Generative-vs-Discriminative-Models","KB/GloVE","KB/Global-Average-Pooling","KB/Global-and-Sliding-Window-Attention","KB/Google-NMT","KB/Grad-CAM","KB/Hallucination-Text-Generation","KB/HiFI-GAN-Denoising","KB/HiFI-GAN-Synthesis","KB/Higher-Layer-Capsule","KB/Highway-Convolutions","KB/Hopfield-networks","KB/Imagen","KB/Inception","KB/Instance-Normalization","KB/Instant-NeRF","KB/Interpreting-Attention","KB/Isotropic-Architectures","KB/Joint-Factor-Analysis","KB/Jukebox","KB/LASER","KB/LaMDA","KB/Large-Kernel-in-Attention","KB/Large-Kernel-in-Convolution","KB/Le-Net","KB/Learning-to-Detect-Grasp-Affordance","KB/Linear-Classifier-Probes","KB/Listen-Attend-Spell","KB/Location-Aware-Attention","KB/Location-Base-Attention","KB/Long-Short-Term-Memory-(LSTM)","KB/Longformer","KB/MCnet","KB/MLIM","KB/MLM","KB/MVGrasp","KB/Magic3D","KB/Masked-Autoencoders","KB/Minerva","KB/Mixed-chunk-attention","KB/Mobile-Net","KB/MobileOne","KB/Multi-Head-Attention","KB/Multiplicative-Attention","KB/Muse","KB/Nasnet","KB/Neural-Network-Architecture-Cheat-Sheet","KB/Neural-Probabilistic-Model","KB/Neural-Text-Degeneration","KB/Noisy-Relu","KB/OPT","KB/PEER","KB/PRelu","KB/PaLM","KB/Padded-Conv","KB/PatchGAN","KB/Phenaki","KB/Phrase-Representation-Learning","KB/Pix2Seq","KB/Point-Cloud","KB/PointNet++","KB/Position-Encoding","KB/Position-Wise-Feed-Forward","KB/Primary-Capsule","KB/RETRO","KB/Receptive-field","KB/RegNet","KB/Region-Proposal","KB/Relative-Multi-Head-Self-Attention","KB/Relu","KB/RepLKNet","KB/Res-Net-D","KB/Res-Net","KB/Restricted-Boltzmann-Machine","KB/RetinaNet","KB/Rmsprop","KB/RoBERTa","KB/Routing-by-Agreement","KB/S2ST","KB/SLAK","KB/SRN","KB/Scaled-Dot-Product-Attention","KB/Scene-based-text-to-image-generation","KB/SegNet","KB/Self-Attention-GAN","KB/Self-Attention","KB/Seq2Seq","KB/ShuffleNet","KB/Sigmoid","KB/Sliding-Window-Attention","KB/Soft-Attention","KB/Softmax","KB/Softplus","KB/Soundify","KB/Sparse-Evolutionary-Training","KB/Sparse-Transformer","KB/Spatial-Transformer","KB/Speaker-Verification","KB/Speech-Emotion-Recognition","KB/Speech-Recognition","KB/Speech-Resynthesis","KB/Spiking-Networks","KB/Stable-Difusion","KB/Stack-GAN","KB/Stacking-RNN","KB/StarGAN-v2","KB/StarGAN","KB/Strided-Attention","KB/Strided","KB/Style-GAN","KB/Swish","KB/TSDF","KB/Tacotron","KB/Tanh","KB/Teacher-Forcing","KB/Temporal-Conv","KB/TemporalLearning","KB/Textless-Speech-Emotion-Conversion","KB/TinyBERT","KB/Token-Embedding","KB/Transformer-XL","KB/Transformer","KB/Transposed-Conv","KB/ULMFit","KB/Un-LSTM","KB/Unet-Grasping","KB/Unet","KB/VAE","KB/VGGish","KB/VICReg","KB/VL-BEIT","KB/Vgg","KB/ViLT","KB/VisualGPT","KB/Volumetric-Grasping-Network","KB/WaveGlow","KB/WebGPT","KB/Whisper","KB/Wide-Deep-Recommender","KB/Window-Based-Regression","KB/Word2Vec","KB/X-Vectors","KB/XLM-R","KB/XLNet","KB/Xception","KB/YOLO","KB/Z-Space-Entanglement","KB/Zeiler-Fergus","KB/cross-layer-parameter-sharing","KB/dGSLM","KB/data2vec","KB/i-Code","KB/pGLSM","KB/wave2vec"],"tags":["anchor"],"content":"\nADVENT\nALBERT\nAdagrad\nAdaptive Input Representation\nAdditive Attention\nAffordance Detection Task Specific\nAlex Net\nAlphacode\nAttention NMT\nAttention\nAudioLM\nAuto Encoders\nAutoDistill\nBART\nBERT\nBahdanau Attention\nBasic GAN\nBasic RNN Architectures\nBasic Transformer\nBeam search\nBi Directional RNN\nBias nodes\nBig Bird\nBinaryBERT\nBlockNeRF\nCLIP\nCapsule Layer\nCapsule Network\nChat GPT is Not All You Need\nChatGPT\nChinchilla\nClassifier Gradients\nCodex\nCollaborative Topic Regression\nConditional GAN\nConformer\nContent Based Attention\nContrastive Predictive Coding\nConvBERT\nConvNeXt\nConvolutional RNN\nCurriculum Learning\nCycleGAN\nDALL-E 3\nDALL-E\nDALL·E 2\nDCGAN\nDLRM\nDeepFM\nDeepNet\nDeepPERF\nDenoising Autoencoder\nDense Net\nDiffusion LM\nDilated Sliding Window Attention\nDistillBERT\nDot Product Attention\nDreamfusion\nDynamic Sparsity\nELECTRA\nELMO\nEfficientNet\nElu\nEncoder Decoder Attention\nEnsemble Distillation\nFLASH\nFLAVA\nFaceNet\nFactorized Embedding Parameters\nFamilar Object Grasping Object Viiew recog\nFastText\nFaster RCNN\nFeature Correlationa\nFixed Factorization Attention\nFlamingo\nFlowNet\nGAN Z Space\nGAU\nGELU\nGGCNN\nGLOW\nGPT\nGPT3\nGRConvNet\nGRU\nGalactica\nGato\nGenerative Models\nGenerative RNN\nGenerative Spoken Language Modeling\nGenerative vs Discriminative Models\nGloVE\nGlobal Average Pooling\nGlobal and Sliding Window Attention\nGoogle NMT\nGrad-CAM\nHallucination Text Generation\nHiFI-GAN Denoising\nHiFI-GAN Synthesis\nHigher Layer Capsule\nHighway Convolutions\nHopfield networks\nImagen\nInception\nInstance Normalization\nInstant NeRF\nInterpreting Attention\nIsotropic Architectures\nJoint Factor Analysis\nJukebox\nLASER\nLaMDA\nLarge Kernel in Attention\nLarge Kernel in Convolution\nLe Net\nLearning to Detect Grasp Affordance\nLinear Classifier Probes\nListen Attend Spell\nLocation Aware Attention\nLocation Base Attention\nLong Short Term Memory (LSTM)\nLongformer\nMCnet\nMLIM\nMLM\nMVGrasp\nMagic3D\nMasked Autoencoders\nMinerva\nMixed chunk attention\nMobile Net\nMobileOne\nMulti Head Attention\nMultiplicative Attention\nMuse\nNasnet\nNeural Network Architecture Cheat Sheet\nNeural Probabilistic Model\nNeural Text Degeneration\nNoisy Relu\nOPT\nPEER\nPRelu\nPaLM\nPadded Conv\nPatchGAN\nPhenaki\nPhrase Representation Learning\nPix2Seq\nPoint Cloud\nPointNet++\nPosition Encoding\nPosition Wise Feed Forward\nPrimary Capsule\nRETRO\nReceptive field\nRegNet\nRegion Proposal\nRelative Multi Head Self Attention\nRelu\nRepLKNet\nRes Net D\nRes Net\nRestricted Boltzmann Machine\nRetinaNet\nRmsprop\nRoBERTa\nRouting by Agreement\nS2ST\nSLAK\nSRN\nScaled Dot Product Attention\nScene based text to image generation\nSegNet\nSelf Attention GAN\nSelf Attention\nSeq2Seq\nShuffleNet\nSigmoid\nSliding Window Attention\nSoft Attention\nSoftmax\nSoftplus\nSoundify\nSparse Evolutionary Training\nSparse Transformer\nSpatial Transformer\nSpeaker Verification\nSpeech Emotion Recognition\nSpeech Recognition\nSpeech Resynthesis\nSpiking Networks\nStable Difusion\nStack GAN\nStacking RNN\nStarGAN v2\nStarGAN\nStrided Attention\nStrided\nStyle GAN\nSwish\nTSDF\nTacotron\nTanh\nTeacher Forcing\nTemporal Conv\nTemporalLearning\nTextless Speech Emotion Conversion\nTinyBERT\nToken Embedding\nTransformer-XL\nTransformer\nTransposed Conv\nULMFit\nUn-LSTM\nUnet Grasping\nUnet\nVAE\nVGGish\nVICReg\nVL-BEIT\nVgg\nViLT\nVisualGPT\nVolumetric Grasping Network\nWaveGlow\nWebGPT\nWhisper\nWide Deep Recommender\nWindow Based Regression\nWord2Vec\nX Vectors\nXLM-R\nXLNet\nXception\nYOLO\nZ-Space Entanglement\nZeiler Fergus\ncross-layer parameter sharing\ndGSLM\ndata2vec\ni-Code\npGLSM\nwave2vec\n"},"KB/augmentation":{"title":"augmentation","links":[],"tags":[],"content":""},"KB/autoregressive-flows":{"title":"autoregressive flows","links":["KB/MADE---Masked-autoencoder-for-distribution-estimation","KB/Masked-autoregressive-flow-for-density-estimation","KB/Improved-variational-inference-with-inverse-autoregressive-flows","KB/inverse-autoregressive-flows"],"tags":["architecture"],"content":"Autoregressive Flows\n \n\n\nuse them as normalizing flows\na model that sequentially samples each pixel on an image, wrt the previous pixel in a fixed order\nconsider the conditional densities as gaussians\n\nFrom MADE - Masked autoencoder for distribution estimation and Masked autoregressive flow for density estimation,\n\nthe sampling noise at each step can be used as a latent variable\np_\\theta(x_{1}, x_{2}, … , x_{n}) = \\Pi_{i}^{n}p_\\theta(x_{i} \\vert x_{&lt;i})\n\\mu_{i}= f_\\mu(x_{&lt;i}) and log \\sigma_{i}= f_\\sigma(x_{&lt;i})\nthe latent factor = z_{i} \\sim N(0,1)\nobserved → x_{i}= \\mu_{i}+ z_{i}\\exp(\\log \\sigma_{i})\n\n\nSince these are sequentially sampled, we can’t parallelize them. Instead from Improved variational inference with inverse autoregressive flows we can use inverse autoregressive flows\n\n\n"},"KB/bijective-function":{"title":"bijective function","links":["images/41369f19a77c31b54bff6e16dde23410_MD5.jpeg"],"tags":["algebra"],"content":"Bijective Function\n \n\nFor any x, only one z could have been sampled to produce it\nOpen: Pasted image 20241119162413.png\n\n"},"KB/bitter_lesson":{"title":"bitter_lesson","links":[],"tags":["deeplearning"],"content":"\nThe Bitter Lesson\n\nRichard sutton\nThe biggest lesson that can be read from 70 years of AI research is that general methods that leverage computation are ultimately the most effective, and by a large margin\nSeeking an improvement that makes a difference in the shorter term, researchers seek to leverage their human knowledge of the domain, but the only thing that matters in the long run is the leveraging of computation\n\n\n\nIn computer chess, the methods that defeated the world champion, Kasparov, in 1997, were based on massive, deep search. At the time, this was looked upon with dismay by the majority of computer chess researchers who had pursued methods that leveraged human understanding of the special structure of chess. When a simpler, search-based approach with special hardware and software proved vastly more effective, these human-knowledge-based chess researchers were not good losers. They said that “brute force” search may have won this time, but it was not a general strategy, and anyway it was not how people played chess. These researchers wanted methods based on human input to win and were disappointed when they did not.\nLearning by self play, and learning in general, is like search in that it enables massive computation to be brought to bear.\nstatistical methods won out over the human-knowledge-based methods\nDeep learning methods rely even less on human knowledge, and use even more computation, together with learning on huge training sets, to produce dramatically better speech recognition systems.\nThe bitter lesson is based on the historical observations that 1) AI researchers have often tried to build knowledge into their agents, 2) this always helps in the short term, and is personally satisfying to the researcher, but 3) in the long run it plateaus and even inhibits further progress, and 4) breakthrough progress eventually arrives by an opposing approach based on scaling computation by search and learning\n\n\n\ngreat power of general purpose methods, of methods that continue to scale with increased computation even as the available computation becomes very great\nscale arbitrarily in this way are search and learning.\nthe actual contents of minds are tremendously, irredeemably complex; we should stop trying to find simple ways to think about the contents of minds, such as simple ways to think about space, objects, multiple agents, or symmetries.\nThey are not what should be built in, as their complexity is endless; instead we should build in only the meta-methods that can find and capture this arbitrary complexity\nthey can find good approximations, but the search for them should be by our methods, not by us\n"},"KB/blind-ethical-judgement":{"title":"blind ethical judgement","links":[],"tags":["ethics"],"content":"Blind Ethical Judgement\n\nthe given agent’s state and knowledge are unknown\n"},"KB/brain":{"title":"brain","links":["KB/Action-Potential","KB/Adrenal-Glands","KB/Adrenaline","KB/Allele","KB/Alpha-Waves","KB/Alzheimer’s-Disease","KB/Amino-Acid","KB/Amygdala","KB/Amyloid-Plaque","KB/Amyloid-beta-(Aβ)-Protein","KB/Amyotrophic-Lateral-Sclerosis-(ALS)","KB/Angiography","KB/Apoptosis","KB/Astrocyte","KB/Axon-Terminal","KB/Axon","KB/BOLD","KB/Basal-Ganglia","KB/Basilar-Artery","KB/Belmont-Principles","KB/Belmont-Report","KB/Beneficence","KB/Beta-Waves","KB/Biological-Neuron","KB/Biomarkers","KB/Blood-brain-Barrier","KB/Brain-Areas","KB/Brain-Cortex","KB/Brain-Organoid","KB/Brain-Oscillations","KB/Brain-derived-Neurotrophic-Factor-(BDNF)","KB/BrainWave-Coherence","KB/BrainWave-CrossFrequency-Coupling","KB/BrainWave-Synchronization","KB/Brainstem","KB/Brocas-Area","KB/CRISPR-(clustered-Regularly-interspaced-Short-Palindromic-repeats)","KB/Central-Sulcus","KB/Cerebellar-Artery","KB/Cerebellum","KB/Cerebral-Palsy","KB/Cerebrospinal-Fluid-(CSF)","KB/Cerebrum","KB/Chimera","KB/Chronic-Encephalopathy-Syndrome-(CES)","KB/Chronic-Traumatic-Encephalopathy-(CTE)","KB/Cochlea","KB/Concussion","KB/Cone","KB/Corpus-callosum","KB/Cortical-Homunculus","KB/Cortisol","KB/Deep-Brain-Stimulation","KB/Delta-Waves","KB/Dendrites","KB/Digital-Phenotyping","KB/Down-Syndrome","KB/EEG-Cap","KB/EEG","KB/Electroconvulsive-Therapy-(ECT)","KB/Epigenetics","KB/Epilepsy","KB/Eugenics","KB/Frontal-Operculum","KB/Frontal-lobe","KB/Functional-Connectivity","KB/Gamma-Waves","KB/Gamma-aminobutyric-Acid-(GABA)","KB/Gene-Expression","KB/Glia","KB/Glioblastoma","KB/Glioma","KB/Glucose","KB/Glymphatic-System","KB/Granger-Causallity","KB/Gyrus","KB/Huntington’s-Disease","KB/Hypothalamus","KB/Implicit-Bias","KB/In-Silico","KB/In-Vitro","KB/In-Vivo","KB/Induced-Pluripotent-Stem-Cell-(iPSC)","KB/Insula","KB/Ion-Channel","KB/Ketamine","KB/Lesion","KB/Limbic-system","KB/Long-Term-Potentiation-(LTP)","KB/MRI","KB/Mesolimbic-Pathway","KB/Microbiota","KB/Microglia","KB/Multiple-Sclerosis","KB/Myelin","KB/Neural-Chimera","KB/Neural-Induction","KB/Neuroaesthetics","KB/Neurogenesis","KB/Neuroplasticity","KB/Nootropics","KB/Nucleotide-Sequence","KB/Nucleotide","KB/Nucleus-Accumbens","KB/Occipital-lobe","KB/Optogenetics","KB/Organoid","KB/Oxytocin","KB/Parietal-lobe","KB/Parkinson’s-Disease","KB/Pharmacotherapy","KB/Phenotype","KB/Pineal-gland","KB/Pituitary-gland","KB/Pluripotency","KB/Positron-Emission-Tomography-(PET)","KB/Postsynaptic-Cell","KB/Presynaptic-Cell","KB/Prion","KB/Protein-Folding","KB/Psychosis","KB/Rapid-Eye-Movement-(REM)-Sleep","KB/Recessive","KB/Reuptake","KB/Rod","KB/Serotonin","KB/Somatosensory-Cortex","KB/Sono-stimulation","KB/Sonogenetics","KB/Spectrogram","KB/Stem-Cells","KB/Striatum","KB/Stroke","KB/Subgenual-Cortex","KB/Substantia-Nigra","KB/Subthalamic-Nucleus","KB/Sulcus","KB/Synaptic-Pruning","KB/Synaptic-Transmission","KB/Tau-Protein","KB/Telomere","KB/Temporal-lobe","KB/Thalamus","KB/Theta-Waves","KB/Tourette’s-Syndrome","KB/Transcranial-Electrical-Stimulation-(tDCS-and-tACS)","KB/Transcranial-Magnetic-Stimulation-(TMS)","KB/Two-photon-Microscopy","KB/Undue-Inducement","KB/Vagus-Nerve","KB/Vertebral-Arteries","KB/Vestibular-System","KB/Visual-Cortex","KB/Wernicke-Area","KB/Aphasia","KB/fMRI"],"tags":["anchor"],"content":"\nAction Potential\nAdrenal Glands\nAdrenaline\nAllele\nAlpha Waves\nAlzheimer’s Disease\nAmino Acid\nAmygdala\nAmyloid Plaque\nAmyloid-beta (Aβ) Protein\nAmyotrophic Lateral Sclerosis (ALS)\nAngiography\nApoptosis\nAstrocyte\nAxon Terminal\nAxon\nBOLD\nBasal Ganglia\nBasilar Artery\nBelmont Principles\nBelmont Report\nBeneficence\nBeta Waves\nBiological Neuron\nBiomarkers\nBlood-brain Barrier\nBrain Areas\nBrain Cortex\nBrain Organoid\nBrain Oscillations\nBrain-derived Neurotrophic Factor (BDNF)\nBrainWave Coherence\nBrainWave CrossFrequency Coupling\nBrainWave Synchronization\nBrainstem\nBrocas Area\nCRISPR (clustered Regularly-interspaced Short Palindromic repeats)\nCentral Sulcus\nCerebellar Artery\nCerebellum\nCerebral Palsy\nCerebrospinal Fluid (CSF)\nCerebrum\nChimera\nChronic Encephalopathy Syndrome (CES)\nChronic Traumatic Encephalopathy (CTE)\nCochlea\nConcussion\nCone\nCorpus callosum\nCortical Homunculus\nCortisol\nDeep Brain Stimulation\nDelta Waves\nDendrites\nDigital Phenotyping\nDown Syndrome\nEEG Cap\nEEG\nElectroconvulsive Therapy (ECT)\nEpigenetics\nEpilepsy\nEugenics\nFrontal Operculum\nFrontal lobe\nFunctional Connectivity\nGamma Waves\nGamma-aminobutyric Acid (GABA)\nGene Expression\nGlia\nGlioblastoma\nGlioma\nGlucose\nGlymphatic System\nGranger Causallity\nGyrus\nHuntington’s Disease\nHypothalamus\nImplicit Bias\nIn Silico\nIn Vitro\nIn Vivo\nInduced Pluripotent Stem Cell (iPSC)\nInsula\nIon Channel\nKetamine\nLesion\nLimbic system\nLong Term Potentiation (LTP)\nMRI\nMesolimbic Pathway\nMicrobiota\nMicroglia\nMultiple Sclerosis\nMyelin\nNeural Chimera\nNeural Induction\nNeuroaesthetics\nNeurogenesis\nNeuroplasticity\nNootropics\nNucleotide Sequence\nNucleotide\nNucleus Accumbens\nOccipital lobe\nOptogenetics\nOrganoid\nOxytocin\nParietal lobe\nParkinson’s Disease\nPharmacotherapy\nPhenotype\nPineal gland\nPituitary gland\nPluripotency\nPositron Emission Tomography (PET)\nPostsynaptic Cell\nPresynaptic Cell\nPrion\nProtein Folding\nPsychosis\nRapid Eye Movement (REM) Sleep\nRecessive\nReuptake\nRod\nSerotonin\nSomatosensory Cortex\nSono-stimulation\nSonogenetics\nSpectrogram\nStem Cells\nStriatum\nStroke\nSubgenual Cortex\nSubstantia Nigra\nSubthalamic Nucleus\nSulcus\nSynaptic Pruning\nSynaptic Transmission\nTau Protein\nTelomere\nTemporal lobe\nThalamus\nTheta Waves\nTourette’s Syndrome\nTranscranial Electrical Stimulation (tDCS and tACS)\nTranscranial Magnetic Stimulation (TMS)\nTwo-photon Microscopy\nUndue Inducement\nVagus Nerve\nVertebral Arteries\nVestibular System\nVisual Cortex\nWernicke Area\nAphasia\nfMRI\n"},"KB/cheatsheets":{"title":"cheatsheets","links":[],"tags":["anchor"],"content":""},"KB/cogitivemodel":{"title":"cogitivemodel","links":[],"tags":["anchor"],"content":""},"KB/cogneuro":{"title":"cogneuro","links":["KB/Attentions-and-salience","KB/BOLD","KB/Berkeley-et-al","KB/Connectome","KB/Cross-situational-learning","KB/Deductive-Approaches","KB/Distributive-units","KB/First-order-generalization","KB/Inductive-Learning","KB/Interpretability-vs-Neuroscience","KB/Localist-units","KB/Milin-et-al","KB/Mirman-et-al","KB/Motor-Memories","KB/Overhypotheses","KB/Propose-but-verify","KB/Rescorla-Wagner-Algorithm","KB/Rescorla-Wagner-Blocking","KB/Second-order-generalization","KB/Single-unit-recording","KB/Superposition-Catastrophe","KB/Symbolic-models","KB/Transitional-probabilities","KB/conditioning","KB/fMRI","KB/memory-trace"],"tags":["anchor"],"content":"\nAttentions and salience\nBOLD\nBerkeley et al\nConnectome\nCross-situational learning\nDeductive Approaches\nDistributive units\nFirst order generalization\nInductive Learning\nInterpretability vs Neuroscience\nLocalist units\nMilin et al\nMirman et al\nMotor Memories\nOverhypotheses\nPropose-but-verify\nRescorla-Wagner Algorithm\nRescorla-Wagner Blocking\nSecond order generalization\nSingle unit recording\nSuperposition Catastrophe\nSymbolic models\nTransitional probabilities\nconditioning\nfMRI\nmemory trace\n"},"KB/cognitivemodel":{"title":"cognitivemodel","links":["KB/ACT-R-Chunk","KB/ACT-R","KB/Active-tracking","KB/CogMod-Final-Paper","KB/Cognition-Hazard-Rates","KB/Cognitive-Foreperiod","KB/Cognitive-Multitasking","KB/Cognitive-Preparation","KB/Cognitive-fMTP","KB/Declarative-Memory-Blending","KB/Declarative-memory","KB/Dikes-and-Rivers","KB/Implicitly-learning-when-to-be-ready---From-instances-to-categories","KB/Mental-Fatigue","KB/Modeling-Driver-Behavior-with-Cognitive-Architecture","KB/Modeling-motivation-using-goal-competition-in-mental-fatigue-studies","KB/On-the-Distinction-Between-Perceived-Duration-and-Event-Timing---Towards-a-Unified-Model-of-Time-Perception","KB/Revisiting-variable-foreperiod-effects-evaluating-the-repetition-priming-account","KB/Scaled-benefits","KB/Sequential-effects-within-a-short-foreperiod-context-Evidence-for-the-conditioning-account-of-temporal-preparation","KB/Sugar-Factory-Task","KB/The-Reward-Experiment","KB/The-warning-stimulus-as-retrieval-cue-The-role-of-associative-memory-in-temporal-preparation","KB/Threaded-Cognition","KB/Traces-of-times-past-Representations-of-temporal-intervals-in-memory","KB/You-can-play-20-questions-with-nature-and-win"],"tags":["anchor"],"content":"\nACT-R Chunk\nACT-R\nActive tracking\nCogMod Final Paper\nCognition Hazard Rates\nCognitive Foreperiod\nCognitive Multitasking\nCognitive Preparation\nCognitive fMTP\nDeclarative Memory Blending\nDeclarative memory\nDikes and Rivers\nImplicitly learning when to be ready - From instances to categories\nMental Fatigue\nModeling Driver Behavior with Cognitive Architecture\nModeling motivation using goal competition in mental fatigue studies\nOn the Distinction Between Perceived Duration and Event Timing - Towards a Unified Model of Time Perception\nRevisiting variable foreperiod effects evaluating the repetition priming account\nScaled benefits\nSequential effects within a short foreperiod context Evidence for the conditioning account of temporal preparation\nSugar Factory Task\nThe Reward Experiment\nThe warning stimulus as retrieval cue The role of associative memory in temporal preparation\nThreaded Cognition\nTraces of times past Representations of temporal intervals in memory\nYou can play 20 questions with nature and win\n"},"KB/composition":{"title":"composition","links":[],"tags":["anchor"],"content":""},"KB/conditioning":{"title":"conditioning","links":["KB/Features"],"tags":["language"],"content":"Conditioning\n\nUnconditioned Stimulus (US) (=food)\nAnother feature or cue (bell), becomes Conditioned Stimulus (CS)\nA response that indicates association (salvation)\nThe unconditioned stimulus is what we want to predict, our outcome\nConditioned Stimuli or cues are the Features we are learning to use to predict the outcome (US)\nCurrent Vector V t ij changes with each learning instance\nV_{ij}^{t+1}= V_{ij}^{t}+ \\Delta V_{ij}^{t}\n\\Delta V_{ij}^{t}=(1-\\text{connection weight}_{j}^{t})\n▶ V = association strength ▶ i = current input (cue) ▶ j = current output (outcome) ▶ ∆ V : Change in association strength\nPavlov discovered evidence of positive associations\nAssociations increase in strength whenever a feature and a cue occur together\n"},"KB/coupling-flows":{"title":"coupling flows","links":["images/1ac5331e588254745dbd2898274013ed_MD5.jpeg","KB/Additive-coupling-layer","KB/Scaling-matrix-for-coupling-layers"],"tags":["distributions"],"content":"Coupling Flows\n \n\nConsider a continuous random variable z \\in \\mathbb{R}^D\nPartition z into two subsets : eg two contiguous sub-vectors using a function g\nOpen: Pasted image 20241119165521.png\n\n\\begin{align*} x_{1:d}= z_{1:d} \\\\ x_{d+1:D}= g(z_{d+1:D}; m(z_{1:d})) \\end{align*}\nelements in the first subset are not affected by those in the second\nto invert them is trivial \\begin{align*} z_{1:d}= x_{1:d} \\\\ z_{d+1:D}= g^{-1}(x_{d+1:D}; m(x_{1:d})) \\end{align*}\nAdditive coupling layer\nScaling matrix for coupling layers\n"},"KB/croissant":{"title":"croissant","links":[],"tags":["dataset"],"content":"Croissant\n\nCroissant is a metadata description format\nMl datasets are a combination of structured and unstructured data, which make them complicated to manage\nCroissant was built on top of schema.org, and has more details relative to it\nThe format has 4 layers\n\ndataset level metadata\nresource description\ncontent structure\nml semantics\n\n\nCroissant does not require any changes to underlying data\nAnalysis and visualization tools work out of the box for all datasets\nUsing croissant, datasets can be exposed consistently throughout platforms\nCollaborations with google, huggingface, google dataset search also exist\nopenml has deeper dataset description by default, slightly lesser in HF and kaggle\nonce loaded, datasets can be imported elsewhere (torch, tf etc) easily\nCroissant editor - web app where you can use a GUI to enter the dataset descriptions\nNeurIPS also now recommends using the Croissant format\n"},"KB/cross-layer-parameter-sharing":{"title":"cross-layer parameter sharing","links":["KB/Attention","KB/Layers"],"tags":["architecture"],"content":"Cross-layer Parameter Sharing\n\nbetween encoder segments, layer parameters are shared for every similar subsegment.\nThis means that e.g. with 12 encoder segments:\n\nThe multi-head self-Attention subsegments share parameters (i.e. weights) across all twelve Layers.\nThe same is true for the feedforward segments.\n\n\nThe consequence of this change is that the number of parameters is reduced significantly, simply because they are shared.\nthe stabilization of the neural network due to parameter sharing. In other words, beyond simply reducing the computational cost involved with training, the paper suggests that sharing parameters can also improve the training process.\n"},"KB/dGSLM":{"title":"dGSLM","links":["KB/Tower","KB/Transformer","KB/Attention","KB/Fisher-Spanish-English"],"tags":["architecture"],"content":"dGSLM\n\nGenerative Spoken Dialogue Language Modeling\ndGSLM\nfirst “textless” model able to generate audio samples of naturalistic spoken dialogues\nunsupervised spoken unit discovery coupled with a dual-Tower Transformer architecture with cross-Attention trained on 2000 hours of two-channel raw conversational audio Fisher Spanish-English without any text or labels\ngenerate speech, laughter and other paralinguistic signals in the two channels simultaneously and reproduces naturalistic turn taking\n"},"KB/data2vec":{"title":"data2vec","links":["KB/Modality","KB/Layers","KB/Features","KB/GLUE","KB/Embedding"],"tags":["architecture"],"content":"data2vec\n\ndata2vec: a General Framework for Self-supervised Learning in Speech, Vision and Language\ncloser to general self-supervised learning\nframework that uses the same learning method for either speech, NLP or computer vision\npredict latent representations of the full input data based on a masked view of the input in a [self distillation](self distillation.md) setup using a standard Transformer architecture\nInstead of predicting Modality-specific targets such as words, visual tokens or units of human speech which are local in nature, data2vec predicts contextualized latent representations that contain information from the entire inpu\nToday’s self-supervised learning research almost typically focuses on a single Modality\nAs a result, researchers specializing in one Modality often adopt a totally different strategy than those specializing in another.\nFor each Modality, algorithms anticipate distinct units: pixels or visual tokens for images, words for the text, and learned sound inventories for voice\nteaching models to anticipate their own representations of the incoming data, regardless of mode\nInstead of predicting visual tokens, phrases, or sounds, a single algorithm may work with completely different sorts of input by focusing on these representations — the Layers of a neural network\nrobust normalization of the Features for the job that would be trustworthy in different modalities to directly predict representations.\nThe method starts by computing target representations from an image, a piece of text, or a voice utterance using a teacher network\nAfter that, a portion of the input was masked and repeated with a student network, which predicts the teacher’s latent representations\nEven though it only has a partial view of the data, the student model must predict accurate input data\nThe instructor network is identical to the student network, except with somewhat out-of-date weights.\nImageNet\nsurpassed wav2vec 2.0 and HuBERT\nGLUE\nMethod:\ndata2vec is trained by predicting the model representations of the full input data given a partial view of the input\nThey first encode a masked version of the training sample (model in student mode) and then construct training targets by encoding the unmasked version of the input sample with the same model but when parameterized as an exponentially moving average of the model weights (model in teacher mode)\nThe target representations encode all of the information in the training sample and the learning task is for the student to predict these representations given a partial view of the input.\nModality encoding:\nThe model architecture used is the standard Transformer architecture with a Modality-specific encoding of the input data borrowed from prior work:\nFor computer vision, they have used the ViT-strategy of encoding an image as a sequence of patches, each spanning 16x16 pixels, input to a linear transformation.\nSpeech data is encoded using a multi-layer 1-D convolutional neural network that maps 16 kHz waveform to 50 Hz representations.\nText is pre-processed to obtain sub-word units, which are then embedded in distributional space via learned Embedding vectors.\n"},"KB/dataset":{"title":"dataset","links":["KB/1D-ALVINN","KB/AudioSet-classification","KB/AudioSet","KB/BUCC","KB/Benchmark-LLM","KB/Billion-Word","KB/BooksCorpus","KB/Broden","KB/CIFAR","KB/COCO","KB/CUB-200-2011-4","KB/CUB-200-2011","KB/Cityscapes","KB/CommonCrawl","KB/DrawBench","KB/English-Wikipedia","KB/Europarl-ST","KB/FGVC-Aircraft","KB/FGVCx","KB/Fashion-MNIST","KB/Fine-grained-datasets","KB/Fisher-Spanish-English","KB/Flickr30K","KB/GLUE","KB/GTA5","KB/Google-Conceptual-Captions","KB/Google-voice-search-task","KB/HMDB51","KB/IDRiD","KB/ILSVRC","KB/IMDB","KB/ISIC-2018","KB/KITTI","KB/Kinetics","KB/Kvasir-Dataset","KB/Labeled-Faces-in-the-Wild","KB/LibriSpeech","KB/MILANNOTATIONS","KB/MIT1003","KB/MIT300","KB/MLDoc","KB/MMLU","KB/MNIST","KB/MSCOCO","KB/MUSAN","KB/Moment-in-Time","KB/NIST-2008-Speaker-Recognition-Evaluation-dataset","KB/NIST-SRE-2016-Cantonese","KB/NLVR2-3","KB/OSIE","KB/PASCAL-VOC","KB/PASCAL-S","KB/People-Art-Dataset","KB/Picasso-Dataset","KB/Places","KB/Places365","KB/PlantCLEF","KB/RACE","KB/SBU-Captions","KB/SQuAD","KB/STL-10","KB/SUNCG","KB/SYNTHIA","KB/Salicon-dataset","KB/SceneNet-RGB-D","KB/Shapes-Dataset","KB/Speakers-in-the-Wild","KB/Stanford-Dogs","KB/Swichboard","KB/UCF101","KB/VGGFace2-3","KB/VQAv2-3","KB/Visual-Commonsense-Reasoning","KB/Visual-Genome","KB/WMT14","KB/Wall-Street-Journal-task","KB/XLSR","KB/YFCC100M","KB/iNaturalist"],"tags":["anchor"],"content":"\n1D-ALVINN\nAudioSet classification\nAudioSet\nBUCC\nBenchmark LLM\nBillion Word\nBooksCorpus\nBroden\nCIFAR\nCOCO\nCUB-200-2011 4\nCUB-200-2011\nCityscapes\nCommonCrawl\nDrawBench\nEnglish Wikipedia\nEuroparl-ST\nFGVC Aircraft\nFGVCx\nFashion MNIST\nFine grained datasets\nFisher Spanish-English\nFlickr30K\nGLUE\nGTA5\nGoogle Conceptual Captions\nGoogle voice search task\nHMDB51\nIDRiD\nILSVRC\nIMDB\nISIC 2018\nKITTI\nKinetics\nKvasir Dataset\nLabeled Faces in the Wild\nLibriSpeech\nMILANNOTATIONS\nMIT1003\nMIT300\nMLDoc\nMMLU\nMNIST\nMSCOCO\nMUSAN\nMoment in Time\nNIST 2008 Speaker Recognition Evaluation dataset\nNIST SRE 2016 Cantonese\nNLVR2 3\nOSIE\nPASCAL VOC\nPASCAL-S\nPeople Art Dataset\nPicasso Dataset\nPlaces\nPlaces365\nPlantCLEF\nRACE\nSBU Captions\nSQuAD\nSTL-10\nSUNCG\nSYNTHIA\nSalicon dataset\nSceneNet RGB-D\nShapes Dataset\nSpeakers in the Wild\nStanford Dogs\nSwichboard\nUCF101\nVGGFace2 3\nVQAv2 3\nVisual Commonsense Reasoning\nVisual Genome\nWMT14\nWall Street Journal task\nXLSR\nYFCC100M\niNaturalist\n"},"KB/ethics":{"title":"ethics","links":["KB/A-declarative-modular-framework-for-representing-and-applying-ethical-principles","KB/A-low-cost-ethics-shaping-approach-for-designing-reinforcement-learning-agents","KB/A-voting-based-system-for-ethical-decision-making","KB/Belief-Desire-Intention","KB/Building-Ethics-into-Artificial-Intelligence","KB/Capture-bias","KB/Collective-Ethical-Decision-Frameworks","KB/Consequentialist-ethics","KB/Coping-Theory","KB/Coverage-of-ethics-within-the-artificial-intelligence-and-machine-learning-academic-literature","KB/Cross-dataset-generalization","KB/Deontological-ethics","KB/Embedding-ethical-principles-in-collective-decision-support-systems","KB/Ethical-dilemmas","KB/Even-angels-need-the-rules-AI,-roboethics,-and-the-law","KB/GenEth","KB/Label-bias","KB/Moral-Machine-project","KB/Moral-decision-making-frameworks-for-artificial-intelligence","KB/MoralDM","KB/Negative-Set-Bias","KB/Norms-as-a-basis-for-governing-sociotechnical-systems","KB/Preferences-and-ethical-principles-in-decision-making","KB/Selection-Bias","KB/Unbiased-Look-at-Dataset-Bias","KB/Utilitarian-ethics","KB/Virtue-ethics","KB/blind-ethical-judgement","KB/fully-informed-ethical-judgement","KB/partially-informed-ethical-judgement","KB/sacred-values","KB/swap-dominance","KB/trolley-scenario"],"tags":["anchor"],"content":"\nA declarative modular framework for representing and applying ethical principles\nA low-cost ethics shaping approach for designing reinforcement learning agents\nA voting-based system for ethical decision making\nBelief-Desire-Intention\nBuilding Ethics into Artificial Intelligence\nCapture bias\nCollective Ethical Decision Frameworks\nConsequentialist ethics\nCoping Theory\nCoverage of ethics within the artificial intelligence and machine learning academic literature\nCross-dataset generalization\nDeontological ethics\nEmbedding ethical principles in collective decision support systems\nEthical dilemmas\nEven angels need the rules AI, roboethics, and the law\nGenEth\nLabel bias\nMoral Machine project\nMoral decision making frameworks for artificial intelligence\nMoralDM\nNegative Set Bias\nNorms as a basis for governing sociotechnical systems\nPreferences and ethical principles in decision making\nSelection Bias\nUnbiased Look at Dataset Bias\nUtilitarian ethics\nVirtue ethics\nblind ethical judgement\nfully informed ethical judgement\npartially informed ethical judgement\nsacred values\nswap-dominance\ntrolley scenario\n"},"KB/explainability":{"title":"explainability","links":["KB/Accessibility","KB/Adaptive-Whitening-Saliency","KB/Analysis-of-Explainers-of-Black-Box-Deep-Neural-Networks-for-Computer-Vision-A-Survey","KB/Auditability","KB/Back-Propamine","KB/Bayesian-Rule-List","KB/Beware-of-Inmates-Running-the-Asylum","KB/Blur-Baseline","KB/Broden","KB/CAM","KB/Causability","KB/Causality","KB/Classifying-a-specific-image-region-using-convolutional-nets-with-an-ROI-mask-as-input","KB/Cognitive-Engagement","KB/Comparing-Data-Augmentation-Strategies-for-Deep-Image-Classification","KB/Comprehensibility","KB/Conductance","KB/Confidence","KB/Contributions-of-Shape,-Texture,-and-Color-in-Visual-Recognition-Abstract","KB/Counterfactual-Images","KB/Counterfactual-Impact-Evaluation","KB/DeconvNet","KB/Deep-Inside-Convolutional-Networks","KB/Deep-Neural-Networks-are-Easily-Fooled-High-Confidence-Predictions-for-Unrecognizable-Images","KB/Deep-Visual-Explanation","KB/DeepFool","KB/DeepLIFT","KB/Dynamic-visual-attention","KB/EigenCAM","KB/Elaborateness","KB/Embedding-Human-Knowledge-into-Deep-Neural-Network-via-Attention-Map","KB/Explainability-Defn","KB/Explainability-Taxonomy","KB/Explainable-Artificial-Intelligence-(XAI)-Concepts,-Taxonomies,-Opportunities-and-Challenges-toward-Responsible-AI","KB/Explanation-is-not-a-Technical-Term","KB/Explanator","KB/FGSM","KB/Fairness","KB/Faithfulness","KB/Filter-Wise-Normalization","KB/GAM","KB/Gaussian-Baseline","KB/Generalizing-Adversarial-Explanations-with-Grad-CAM","KB/GradCAM++","KB/Gradient-Sensitivity","KB/Graph-based-visual-saliency","KB/Group-fairness","KB/Guided-BackProp","KB/Guided-GradCAM","KB/Image-Data-Augmentation-Survey","KB/Implementation-Invariance","KB/Independence","KB/Influence-of-image-classification-accuracy-on-saliency-map-estimation","KB/Informativeness","KB/Integrated-Gradients","KB/Interactivity","KB/Interpretability-and-Explainability-A-Machine-Learning-Zoo-Mini-tour","KB/Interpretability","KB/Interpretation-of-Neural-networks-is-fragile","KB/LRP","KB/Layerwise-Conservation-Principle","KB/Layerwise-Relevance-Propagation","KB/Limited-features","KB/Manifold","KB/Maximum-Distance-Baseline","KB/Mean-Observed-Dissimilarity","KB/Mental-Model-Matching","KB/Minimization-and-reporting-of-negative-impacts","KB/Multimodal-Explanation","KB/Network-Dissection-Quantifying-Interpretability-of-Deep-Visual-Representions","KB/Noise-Tunnel","KB/Normalized-Inverted-Structural-Similarity-Index","KB/On-the-overlap-between-Grad-CAM-saliency-maps-and-explainable-visual-features-in-skin-cancer-images","KB/Parent-Approximations","KB/Partial-Dependence-Plot","KB/Prediction-Difference-Analysis","KB/Privacy-awareness","KB/PromptIR","KB/Proxy-Attention","KB/Proxy-features","KB/RETAIn","KB/RISE","KB/RandAugment","KB/Random-Directions","KB/Real-Time-Image-Saliency-for-Black-Box-Classifiers","KB/Redress","KB/SAM-ResNet","KB/SDR","KB/SP-LIME","KB/SSR","KB/Salience-Map","KB/Saliency-using-natural-statistics","KB/Saliency-vs-Attention","KB/Sanity-Checks-for-Saliency-Maps","KB/ScoreCAM","KB/Separation","KB/Sharpness-and-Flatness","KB/Skewed-data","KB/Smooth-Grad","KB/SmoothGrad-Square","KB/Social-Construction-of-XAI,-do-we-need-one-definition-to-rule-them-all","KB/Structural-Similarity-Index","KB/Sufficiency","KB/Summit","KB/TREPAN","KB/Tainted-data","KB/Textbooks-are-all-you-need","KB/The-Unreliability-of-Saliency-Methods","KB/The-elephant-in-the-interpretability-room","KB/There-and-back-again","KB/Towards-A-Rigorous-Science-of-Interpretable-Machine-Learning","KB/Training-Trajectories","KB/Trajectory-Plotting-with-PCA","KB/Transferability","KB/Transparency","KB/Trustworthiness","KB/Understandability","KB/Uniform-baseline","KB/Use-Case-Utility","KB/VarGrad","KB/Variation-in-Dissimilarity-Variation-in-Dissimilarity","KB/Vision-Explainibility","KB/Visualizing-the-Impact-of-Feature-Attribution-Baselines","KB/Visualizing-the-Loss-Landscape-of-Neural-Nets","KB/What-is-being-Transferred-in-transfer-learning","KB/Whos-Thinking,-A-push-for-human-centered-evaluation-of-LLMs","KB/XAI","journals/2022-10-05","journals/2022-10-17","KB/pixelattribution"],"tags":["anchor"],"content":"\nAccessibility\nAdaptive Whitening Saliency\nAnalysis of Explainers of Black Box Deep Neural Networks for Computer Vision A Survey\nAuditability\nBack Propamine\nBayesian Rule List\nBeware of Inmates Running the Asylum\nBeware of Inmates Running the Asylum\nBlur Baseline\nBroden\nCAM\nCAM\nCausability\nCausality\nClassifying a specific image region using convolutional nets with an ROI mask as input\nClassifying a specific image region using convolutional nets with an ROI mask as input\nCognitive Engagement\nComparing Data Augmentation Strategies for Deep Image Classification\nComparing Data Augmentation Strategies for Deep Image Classification\nComprehensibility\nConductance\nConductance\nConfidence\nContributions of Shape, Texture, and Color in Visual Recognition Abstract\nContributions of Shape, Texture, and Color in Visual Recognition Abstract\nCounterfactual Images\nCounterfactual Impact Evaluation\nDeconvNet\nDeep Inside Convolutional Networks\nDeep Neural Networks are Easily Fooled High Confidence Predictions for Unrecognizable Images\nDeep Visual Explanation\nDeepFool\nDeepLIFT\nDeepLIFT\nDynamic visual attention\nEigenCAM\nElaborateness\nEmbedding Human Knowledge into Deep Neural Network via Attention Map\nEmbedding Human Knowledge into Deep Neural Network via Attention Map\nExplainability Defn\nExplainability Taxonomy\nExplainable Artificial Intelligence (XAI) Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI\nExplanation is not a Technical Term\nExplanator\nFGSM\nFairness\nFaithfulness\nFilter Wise Normalization\nGAM\nGaussian Baseline\nGeneralizing Adversarial Explanations with Grad-CAM\nGeneralizing Adversarial Explanations with Grad-CAM\nGradCAM++\nGradient Sensitivity\nGraph-based visual saliency\nGroup fairness\nGuided BackProp\nGuided GradCAM\nImage Data Augmentation Survey\nImplementation Invariance\nIndependence\nInfluence of image classification accuracy on saliency map estimation\nInfluence of image classification accuracy on saliency map estimation\nInformativeness\nIntegrated Gradients\nIntegrated Gradients\nInteractivity\nInterpretability and Explainability A Machine Learning Zoo Mini-tour\nInterpretability\nInterpretation of Neural networks is fragile\nInterpretation of Neural networks is fragile\nLRP\nLayerwise Conservation Principle\nLayerwise Conservation Principle\nLayerwise Relevance Propagation\nLimited features\nManifold\nMaximum Distance Baseline\nMean Observed Dissimilarity\nMental Model Matching\nMinimization and reporting of negative impacts\nMultimodal Explanation\nNetwork Dissection Quantifying Interpretability of Deep Visual Representions\nNoise Tunnel\nNoise Tunnel\nNormalized Inverted Structural Similarity Index\nOn the overlap between Grad-CAM saliency maps and explainable visual features in skin cancer images\nOn the overlap between Grad-CAM saliency maps and explainable visual features in skin cancer images\nParent Approximations\nPartial Dependence Plot\nPrediction Difference Analysis\nPrivacy awareness\nPromptIR\nProxy Attention\nProxy Attention\nProxy features\nRETAIn\nRISE\nRandAugment\nRandom Directions\nReal Time Image Saliency for Black Box Classifiers\nReal Time Image Saliency for Black Box Classifiers\nRedress\nSAM-ResNet\nSDR\nSP-LIME\nSSR\nSalience Map\nSalience Map\nSaliency using natural statistics\nSaliency vs Attention\nSanity Checks for Saliency Maps\nScoreCAM\nSeparation\nSharpness and Flatness\nSkewed data\nSmooth-Grad\nSmoothGrad Square\nSmoothGrad Square\nSocial Construction of XAI, do we need one definition to rule them all\nStructural Similarity Index\nSufficiency\nSummit\nTREPAN\nTainted data\nTextbooks are all you need\nThe Unreliability of Saliency Methods\nThe Unreliability of Saliency Methods\nThe elephant in the interpretability room\nThere and back again\nTowards A Rigorous Science of Interpretable Machine Learning\nTraining Trajectories\nTrajectory Plotting with PCA\nTransferability\nTransparency\nTrustworthiness\nUnderstandability\nUniform baseline\nUse Case Utility\nVarGrad\nVarGrad\nVariation in Dissimilarity Variation in Dissimilarity\nVision Explainibility\nVision Explainibility\nVisualizing the Impact of Feature Attribution Baselines\nVisualizing the Loss Landscape of Neural Nets\nWhat is being Transferred in transfer learning\nWhos Thinking, A push for human centered evaluation of LLMs\nXAI\n2022-10-05.md\n2022-10-17.md\npixelattribution\n"},"KB/fMRI":{"title":"fMRI","links":["KB/MRI","KB/BOLD"],"tags":["brain"],"content":"fMRI\n\nUnlike MRI. Studies measures brain activity by detecting changes associated with blood flow\n3mm\nBOLD\n"},"KB/fastai":{"title":"fastai","links":["KB/Fastai-Blocks","KB/Fastai-Interpretation","KB/Fastai-Deployment","KB/Fastai-Tricks"],"tags":["deeplearning"],"content":"Fastai\n\nFastai Blocks\nFastai Interpretation\nFastai Deployment\nFastai Tricks\n"},"KB/flow":{"title":"flow","links":["content/notes/Vision-Explainibility"],"tags":["anchor"],"content":"\nVision Explainibility.md\n"},"KB/fully-informed-ethical-judgement":{"title":"fully informed ethical judgement","links":[],"tags":["ethics"],"content":"Fully Informed Ethical Judgement\n\nwith complete information about a given agent’s state and knowledge\nno quantitative measure of how far a behaviour is from rightfulness or goodness\n"},"KB/gen_quotes":{"title":"gen_quotes","links":[],"tags":[],"content":"\n\n\n  Quotes\n\n\n  Random Quotes\n  This page displays 10 random Quotes from my personal repository of Quotes I have collected (and continue to collect) over the years. Sadly, most of them do not have any citations. I will eventually try to write a script that finds the authors of the same.\n  These have been collected from books, websites, movies and music and each of them has taught me something over the years. Here I am sharing them with you, dear visitor :)\nShow Random Points\n  \n  \n    function showRandomPoints() {\n      fetch(&#039;../quotes&#039;)\n        .then(response =&gt; response.text())\n        .then(text =&gt; {\n          const points = extractPoints(text);\n          const randomPoints = getRandomPoints(points, 10);\n          displayPoints(randomPoints);\n        })\n        .catch(error =&gt; {\n          console.error(&#039;Error:&#039;, error);\n        });\n    }\n\n    function extractPoints(text) {\n    const lines = text.split(&#039;&lt;li&gt;&#039;);\n      const points = lines.filter(line =&gt; line.trim());\n            return points.map(point =&gt; point.trim().replace(&#039;&lt;p&gt;&#039;, &#039;&#039;).replace(&#039;&lt;/p&gt;&#039;, &#039;&#039;).replace(&#039;&lt;/li&gt;&#039;,&#039;&#039;));\n    }\n\n    function getRandomPoints(points, num) {\n      const randomPoints = [];\n      const availableIndices = Array.from(Array(points.length).keys());\n      for (let i = 0; i &lt; num; i++) {\n        const randomIndex = Math.floor(Math.random() * availableIndices.length);\n        const pointIndex = availableIndices.splice(randomIndex, 1)[0];\n        randomPoints.push(points[pointIndex]);\n      }\n      return randomPoints;\n    }\n\n    function displayPoints(points) {\n      const pointsList = document.getElementById(&#039;pointsList&#039;);\n      pointsList.innerHTML = &#039;&#039;;\n      points.forEach(point =&gt; {\n        const li = document.createElement(&#039;li&#039;);\n        li.textContent = point;\n        pointsList.appendChild(li);\n      });\n    }\n  \n\n"},"KB/gensim":{"title":"gensim","links":[],"tags":["language"],"content":"Gensim\n\nlibrary\n\nw2vec Code\nnltk==3.6.1  \nnode2vec==0.4.3  \npandas==1.2.4  \nmatplotlib==3.3.4  \ngensim==4.0.1  \nscikit-learn=0.24.1\nimport nltk  \nnltk.download(&#039;stopwords&#039;)  \nnltk.download(&#039;punkt&#039;)\n \nimport pandas as pd\n \nimport nltk\n \nimport string\n \nimport matplotlib.pyplot as plt\n \nfrom nltk.corpus import stopwords\n \nfrom nltk import word_tokenize\n \nfrom gensim.models import Word2Vec as w2v\n \nfrom sklearn.decomposition import PCA\n \n# constants\n \nPATH = &#039;data/shakespeare.txt&#039;\n \nsw = stopwords.words(&#039;english&#039;)\n \nplt.style.use(&#039;ggplot&#039;)\n \n# nltk.download(&#039;punkt&#039;)\n \n# nltk.download(&#039;stopwords&#039;)\n \n# import data\n \nlines = []\n \nwith open(PATH, &#039;r&#039;) as f:\n \nfor l in f:\n\tlines.append(l)\n \n# remove new lines\nlines = [line.rstrip(&#039;\\n&#039;) for line in lines]\n \n# make all characters lower\nlines = [line.lower() for line in lines]\n \n# remove punctuations from each line\nlines = [line.translate(str.maketrans(&#039;&#039;, &#039;&#039;, string.punctuation)) for line in lines]\n \n# tokenize\nlines = [word_tokenize(line) for line in lines]\n \ndef remove_stopwords(lines, sw = sw):\n    &#039;&#039;&#039;\n    The purpose of this function is to remove stopwords from a given array of \n    lines.\n    \n    params:\n        lines (Array / List) : The list of lines you want to remove the stopwords from\n        sw (Set) : The set of stopwords you want to remove\n        \n    example:\n        lines = remove_stopwords(lines = lines, sw = sw)\n    &#039;&#039;&#039;\n    \n    res = []\n    for line in lines:\n        original = line\n        line = [w for w in line if w not in sw]\n        if len(line) &lt; 1:\n            line = original\n        res.append(line)\n    return res\n    \nfiltered_lines = remove_stopwords(lines = lines, sw = sw)\n \nw = w2v(\n    filtered_lines,\n    min_count=3,  \n    sg = 1,       # 1 for skip gram, 0 for cbow\n    window=7      \n)       \n \nprint(w.wv.most_similar(&#039;thou&#039;))\n \nemb_df = (\n    pd.DataFrame(\n        [w.wv.get_vector(str(n)) for n in w.wv.key_to_index],\n        index = w.wv.key_to_index\n    )\n)\nprint(emb_df.shape)\nemb_df.head()\n \npca = PCA(n_components=2, random_state=7)\npca_mdl = pca.fit_transform(emb_df)\n \nemb_df_PCA = (\n    pd.DataFrame(\n        pca_mdl,\n        columns=[&#039;x&#039;,&#039;y&#039;],\n        index = emb_df.index\n    )\n)\n \nplt.clf()\nfig = plt.figure(figsize=(6,4))\n \nplt.scatter(\n    x = emb_df_PCA[&#039;x&#039;],\n    y = emb_df_PCA[&#039;y&#039;],\n    s = 0.4,\n    color = &#039;maroon&#039;,\n    alpha = 0.5\n)\n \nplt.xlabel(&#039;PCA-1&#039;)\nplt.ylabel(&#039;PCA-2&#039;)\nplt.toc: true\ntitle(&#039;PCA Visualization&#039;)\nplt.plot()"},"KB/handwritingRecognition":{"title":"handwritingRecognition","links":[],"tags":["application"],"content":"handwritingRecognition\n\narxiv.org/pdf/1912.10205.pdf\n\ngithub.com/Canjie-Luo/Text-Image-Augmentation\n\n\n\n\ngithub.com/FactoDeepLearning/VerticalAttentionOCR\n\narxiv.org/pdf/2012.03868v2.pdf\nsegmentation free\n\n\n\n\n"},"KB/heteroscedastic-nonlinear-regression":{"title":"heteroscedastic nonlinear regression","links":["KB/Heteroscedatic"],"tags":["loss"],"content":"heteroscedastic nonlinear regression\n \n\nHeteroscedatic\nmean and the variance of the output are functions of the input\n"},"KB/i-Code":{"title":"i-Code","links":["KB/Attention","KB/Modality","KB/GLUE","KB/Layers"],"tags":["architecture"],"content":"i-Code\n\ni-Code: an Integrative and Composable Multimodal Learning Framework\nHuman intelligence is multimodal; they integrate visual, linguistic, and acoustic signals to maintain a holistic worldview\nMost current pretraining methods, however, are limited to one or two modalities.\njointly learns representations for vision, language and speech into a unified, shared and general-purpose vector representation\ndata from each [modality] are first given to pretrained single-[modality](modality] are first given to pretrained single-[modality.md) encoder\nThe encoder outputs are then integrated with a multimodal fusion network, which uses novel Attention mechanisms and other architectural innovations to effectively combine information from the different modalities\nnew objectives including (i) masked [modality] modeling and (ii) cross-[modality](modality] modeling and (ii) cross-[modality.md) contrastive learning\npretraining on dual-[modality] datasets can also yield competitive or even better performance than pretraining on videos, the data resource that previous three-[modality](modality] datasets can also yield competitive or even better performance than pretraining on videos, the data resource that previous three-[modality.md) models were restricted to\ndynamically process single, dual, and triple-Modality data during training and inference, flexibly projecting different combinations of modalities into a single representation space\nGLUE\nmerge-attention Layers and (b) co-Attention Layers\nfusion architecture\nmechanisms that merge and cross the Attention scores of different modalities, namely merge-Attention (based on self-Attention) and co-Attention (based on self- and cross-Attention) respectively\n"},"KB/iNaturalist":{"title":"iNaturalist","links":[],"tags":["dataset"],"content":"iNaturalist\n\nThis dataset contains images of thousands of different species of plants and animals, with a total of over 8 million images.\n"},"KB/ill-conditioning":{"title":"ill conditioning","links":[],"tags":["architecture"],"content":"ill conditioning\n\nGradient can behave in unexpected ways, e.g in the figure above, very large gradients near flat regions.\n\n"},"KB/imageCaptioning":{"title":"Image Captioning","links":[],"tags":["application"],"content":"Image Captioning\n"},"KB/index":{"title":"Knowledge Base","links":[],"tags":[],"content":""},"KB/inter-sentence-coherence-loss":{"title":"inter-sentence coherence loss","links":["KB/Resistance"],"tags":["loss"],"content":"Inter-sentence Coherence Loss\n\ninter-sentence coherence loss called sentence-order prediction (SOP) is used.\nThe key problem with this loss is that it merges topic prediction and coherence prediction into one task.\nIntuitively, we can argue that topic prediction is much easier than coherence prediction. The consequence is that when the model discovers this, it can focus entirely on this subtask, and forget about the coherence prediction task; actually taking the path of least Resistance. The authors actually demonstrate that this is happening with the NSP task, replacing it within their work with a sentence-order prediction or SOP task.\n"},"KB/inverse-autoregressive-flows":{"title":"inverse autoregressive flows","links":[],"tags":["architecture"],"content":"Inverse Autoregressive Flows\n \n\nA trick that allows fast learning and also fast (but approximate) sampling is to build a masked autoregressive flow to learn the distribution (the teacher) and then use this to train an inverse autoregressive flow from which we can sample eﬀiciently (the student)\n\n"},"KB/jobs":{"title":"jobs","links":["KB/ABN-Amro-AI-Dev","KB/ABN-amro-ml","KB/Bunq","KB/Cold-email-templates","KB/DevOn-AI-dev","KB/Eneco-Data-Scientist","KB/Interview-Tips","KB/Kickstart-AI","KB/Latitude-Junior-ML-Engineer","KB/Leap-Data-Scientist","KB/MLCompany","KB/Media-Distillery","KB/MediaMonks","KB/Orbisk","KB/Poki-Data-Scientist","KB/Schipol-Data-Scientist","KB/VattenFall-Data-Scientist","KB/Wageningen-Uni-AI"],"tags":["anchor"],"content":"\nABN Amro AI Dev\nABN amro ml\nBunq\nCold email templates\nDevOn AI dev\nEneco Data Scientist\nInterview Tips\nKickstart AI\nLatitude Junior ML Engineer\nLeap Data Scientist\nMLCompany\nMedia Distillery\nMediaMonks\nOrbisk\nPoki Data Scientist\nSchipol Data Scientist\nVattenFall Data Scientist\nWageningen Uni AI\n"},"KB/jobtemp":{"title":"jobtemp","links":["KB/Klue-ML-Engineer"],"tags":["jobsearch"],"content":"Jobtemp\nDexter Energy\n\n\njordan@dexterenergy.ai\n\n\nJordan Cantlow\n\n\n\n\n\nKlue ML Engineer\n\n"},"KB/language":{"title":"language","links":["KB/2-X-2-Study","KB/2-byte-character-set","KB/8-bit-character-set","KB/A-matter-of-ambiguity-Using-eye-movements-to-examine-collective-vs-distributive-interpretations-of-plural-sets","KB/ANOVA","KB/ASCII","KB/Action-Transitive-verb","KB/Adjective","KB/Adverb","KB/Agglutinating-words","KB/Allomorph","KB/Application-dependence","KB/Approximately-Compositional-Semantic-Parsing","KB/Attentions-and-salience","KB/Berkeley-et-al","KB/Bottom-Up-Parsing","KB/Bound-morpheme","KB/Cardinality-Principle","KB/Case-Grammar","KB/Challenges-of-Words-and-rules","KB/Character-set-dependence","KB/Circumfix","KB/Collective-Interpretation","KB/Collectivity,-Distributivity,-and-the-Interpretation-of-Plural-Numerical-Expressions-in-Child-and-Adult-Language","KB/Conceptual-Parsing","KB/Connectionism","KB/Connectionist-Networks","KB/Connectives","KB/Content-Morpheme","KB/Content-words","KB/Context-Free-Grammar","KB/Corpus-dependence","KB/Cross-situational-learning","KB/Cumulative-Interpretation","KB/Deductive-Approaches","KB/Derivational-Morphology","KB/Determiners","KB/DiTransitive-verb","KB/Distributive-Interpretation-(2)","KB/Distributive-Interpretation","KB/Distributive-units","KB/Document-Triage","KB/Elements-of-sets","KB/Elman-1990","KB/Elman-1991","KB/Elman-1992","KB/Elman-1993","KB/Emergentism","KB/Entities-involving-in-actions","KB/Evidence-For-Distributivity-Effects-in-Comprehension","KB/Extra-position","KB/Final-Paper-Language-Modeling","KB/First-order-generalization","KB/Fixed-Factors","KB/Forward-Backward-Matching","KB/Free-morpheme","KB/Function-words","KB/Functional-Morpheme","KB/Hybrid-Word-Segmentation","KB/Inductive-Learning","KB/Infix","KB/Inflectional-Morphology","KB/Inflectional-words","KB/Isolating-words","KB/Language-Identification","KB/Language-dependence","KB/Latent-Dirchlet-Allocation","KB/Lemmatization","KB/Lexical-Ambiguity","KB/Lexical-Disambiguation","KB/Lexical-Word-Segmentation","KB/Lexically-Collective","KB/Lexically-Distributive","KB/Lexicon","KB/Linguistic-details","KB/Localist-units","KB/Maximum-Matching-Algorithm","KB/Memory-based-learning","KB/Milin-et-al","KB/Minimal-Semantic-Commitment","KB/Mirman-et-al","KB/Misyak-et-al-2010","KB/Mixed-Effect-Models","KB/Morpheme-Generation","KB/Morpheme-Segmentation","KB/Morpheme","KB/Morphology-Affix","KB/Morphology-Stem","KB/Morphology","KB/Morphotactic","KB/Multiple-constraint-based-theories","KB/Names-of-individuals","KB/Nativists","KB/Non-adjacent-dependencies","KB/Numerically-Quantified-Expressions","KB/Object-relative-clauses","KB/Ostension","KB/Ostensive-Information","KB/Overhypotheses","KB/Parts-of-action","KB/Parts-of-entities","KB/Phonetics","KB/Phonology","KB/Picky-Puppet-Method","KB/Polysynthetic-words","KB/Pragmatics","KB/Predicate","KB/Prefix","KB/Prepositions","KB/Propose-but-verify","KB/Psycholinguistics","KB/Punctuation","KB/Quantifier-spreading-children-misled-by-ostensive-cues","KB/Quantifiers","KB/Random-Factors","KB/Relevance-Account","KB/Relevance-Theory","KB/Rescorla-Wagner-Algorithm","KB/Rescorla-Wagner-Blocking","KB/Saffran,-Aslin-and-Newport","KB/Salient-Object-Strategy","KB/Second-order-generalization","KB/Semantic-Analysis","KB/Semantic-Grammar","KB/Semantic-Markers","KB/Semantics-influences-form","KB/Sentence-Segmentation","KB/Sentence-level-processing","KB/Shared-Character-Set","KB/Shortcuts-to-Quantifier-Interpretation-in-Children-and-Adults","KB/Single-unit-recording","KB/Statistical-Word-Segmentation","KB/Subject-relative","KB/Subject-verb-agreement","KB/Suffix","KB/Superposition-Catastrophe","KB/Suppletion","KB/Symbolic-learning-model","KB/Symbolic-models","KB/Syntactic-Ambiguity","KB/Syntactic-Analysis","KB/Syntactic-Bootstrapping","KB/Syntax-First-models","KB/Taking-on-semantic-commitments,-II-collective-versus-distributive-readings","KB/Text-Normalization","KB/Text-Preprocessing","KB/Text-Segmentation","KB/The-Differentiation-Condition","KB/The-Self-Organization-of-Explicit-Attitudes","KB/Tokenizer","KB/Top-Down-Parsing","KB/Tracking-the-Continuity-of-Language-Comprehension-Computer-Mouse-Trajectories-Suggest-Parallel-Syntactic-Processing","KB/Transitional-probabilities","KB/Transitive-verb","KB/Types-of-Words","KB/Unicode-50","KB/Unique-Character-Set","KB/Universal-Quantifiers","KB/Unrestricted-Race-Model","KB/Verb","KB/Visual-Implicit-Learning","KB/Wh-dependencies","KB/Wickelphones","KB/Word-Blending","KB/Word-Clipping","KB/Word-Compounding","KB/Word-Segmentation","KB/Word-Structure","KB/Words-and-Rules","KB/conditioning","KB/past-tense"],"tags":["anchor"],"content":"\n2 X 2 Study\n2 byte character set\n8 bit character set\nA matter of ambiguity? Using eye movements to examine collective vs distributive interpretations of plural sets\nANOVA\nASCII\nAction Transitive verb\nAdjective\nAdverb\nAgglutinating words\nAllomorph\nApplication dependence\nApproximately Compositional Semantic Parsing\nAttentions and salience\nBerkeley et al\nBottom Up Parsing\nBound morpheme\nCardinality Principle\nCase Grammar\nChallenges of Words-and-rules\nCharacter-set dependence\nCircumfix\nCollective Interpretation\nCollectivity, Distributivity, and the Interpretation of Plural Numerical Expressions in Child and Adult Language\nConceptual Parsing\nConnectionism\nConnectionist Networks\nConnectives\nContent Morpheme\nContent words\nContext Free Grammar\nCorpus dependence\nCross-situational learning\nCumulative Interpretation\nDeductive Approaches\nDerivational Morphology\nDeterminers\nDiTransitive verb\nDistributive Interpretation (2)\nDistributive Interpretation\nDistributive units\nDocument Triage\nElements of sets\nElman 1990\nElman 1991\nElman 1992\nElman 1993\nEmergentism\nEntities involving in actions\nEvidence For Distributivity Effects in Comprehension\nExtra-position\nFinal Paper Language Modeling\nFirst order generalization\nFixed Factors\nForward Backward Matching\nFree morpheme\nFunction words\nFunctional Morpheme\nHybrid Word Segmentation\nInductive Learning\nInfix\nInflectional Morphology\nInflectional words\nIsolating words\nLanguage Identification\nLanguage dependence\nLatent Dirchlet Allocation\nLemmatization\nLexical Ambiguity\nLexical Disambiguation\nLexical Word Segmentation\nLexically Collective\nLexically Distributive\nLexicon\nLinguistic details\nLocalist units\nMaximum Matching Algorithm\nMemory-based learning\nMilin et al\nMinimal Semantic Commitment\nMirman et al\nMisyak et al 2010\nMixed Effect Models\nMorpheme Generation\nMorpheme Segmentation\nMorpheme\nMorphology Affix\nMorphology Stem\nMorphology\nMorphotactic\nMultiple constraint-based theories\nNames of individuals\nNativists\nNon-adjacent dependencies\nNumerically Quantified Expressions\nObject-relative clauses\nOstension\nOstensive Information\nOverhypotheses\nParts of action\nParts of entities\nPhonetics\nPhonology\nPicky Puppet Method\nPolysynthetic words\nPragmatics\nPredicate\nPrefix\nPrepositions\nPropose-but-verify\nPsycholinguistics\nPunctuation\nQuantifier spreading children misled by ostensive cues\nQuantifiers\nRandom Factors\nRelevance Account\nRelevance Theory\nRescorla-Wagner Algorithm\nRescorla-Wagner Blocking\nSaffran, Aslin and Newport\nSalient Object Strategy\nSecond order generalization\nSemantic Analysis\nSemantic Grammar\nSemantic Markers\nSemantics influences form\nSentence Segmentation\nSentence level processing\nShared Character Set\nShortcuts to Quantifier Interpretation in Children and Adults\nSingle unit recording\nStatistical Word Segmentation\nSubject relative\nSubject-verb agreement\nSuffix\nSuperposition Catastrophe\nSuppletion\nSymbolic learning model\nSymbolic models\nSyntactic Ambiguity\nSyntactic Analysis\nSyntactic Bootstrapping\nSyntax First models\nTaking on semantic commitments, II collective versus distributive readings\nText Normalization\nText Preprocessing\nText Segmentation\nThe Differentiation Condition\nThe Self Organization of Explicit Attitudes\nTokenizer\nTop Down Parsing\nTracking the Continuity of Language Comprehension Computer Mouse Trajectories Suggest Parallel Syntactic Processing\nTransitional probabilities\nTransitive verb\nTypes of Words\nUnicode 50\nUnique Character Set\nUniversal Quantifiers\nUnrestricted Race Model\nVerb\nVisual Implicit Learning\nWh-dependencies\nWickelphones\nWord Blending\nWord Clipping\nWord Compounding\nWord Segmentation\nWord Structure\nWords-and-Rules\nconditioning\npast tense\n"},"KB/mastersthesis":{"title":"mastersthesis","links":[],"tags":["anchor"],"content":""},"KB/medical":{"title":"medical","links":["KB/Acute","KB/Afib","KB/Angina","KB/Appendectomy","KB/Benign","KB/Biopsy","KB/Blood-Culture","KB/Blood-Lancet","KB/Blood-Swab","KB/C-section","KB/Chronic","KB/Coronary-Bypass","KB/Defibrillator","KB/Dialyser","KB/Dialysis","KB/Edema","KB/Embolism","KB/Endoscope","KB/Foley","KB/Forceps","KB/Fracture","KB/Hypertension","KB/Hypodermic-Needle","KB/Hypotension","KB/Hysterectomy","KB/Intravenous","KB/Intubation","KB/Lead-Test","KB/Lumbar-Puncture-or-Spinal-Tap","KB/Malignant","KB/Mastectomy","KB/Myocardial-Infarction","KB/Nebulizer","KB/Occult-Blood-Screen","KB/Ophthalmoscope","KB/Otoscope-or-Auriscope","KB/Pulse-Oximeter","KB/Reflex-Hammer","KB/Remission","KB/Sepsis","KB/Speculum","KB/Spirometer","KB/Thrombosis","KB/Ultrasound"],"tags":["anchor"],"content":"\nAcute\nAfib\nAngina\nAppendectomy\nBenign\nBiopsy\nBlood Culture\nBlood Lancet\nBlood Swab\nC-section\nChronic\nCoronary Bypass\nDefibrillator\nDialyser\nDialysis\nEdema\nEmbolism\nEndoscope\nFoley\nForceps\nFracture\nHypertension\nHypodermic Needle\nHypotension\nHysterectomy\nIntravenous\nIntubation\nLead Test\nLumbar Puncture or Spinal Tap\nMalignant\nMastectomy\nMyocardial Infarction\nNebulizer\nOccult Blood Screen\nOphthalmoscope\nOtoscope or Auriscope\nPulse Oximeter\nReflex Hammer\nRemission\nSepsis\nSpeculum\nSpirometer\nThrombosis\nUltrasound\n"},"KB/memory-trace":{"title":"memory trace","links":[],"tags":["cognitivemodel"],"content":"Memory Trace\n\nthe researchers were able to identify specific neurons in the brain’s motor cortex — an area responsible for controlling movements — that were activated during the learning process.\nThe researchers tagged these potential engram cells with a fluorescent marker so they could see if they also played a role in recalling the memory later on\n"},"KB/pGLSM":{"title":"pGLSM","links":["KB/GPT"],"tags":["architecture"],"content":"pGLSM\n\nText-Free Prosody-Aware Generative Spoken Language Modeling\nsimilar to how GPT-2 can generate coherent paragraphs\nbuilds upon \naddresses the generative aspects of speech pre-training\nreplacing text with discovered phone-like units for language modeling and shows the ability to generate meaningful novel sentences\nthe units used in GSLM discard most of the prosodic information\nfails to leverage prosody for better comprehension, and does not generate expressive speech\nprosody-aware generative spoken language model (pGSLM)\nmulti-stream transformer language model (MS-TLM) of speech, represented as discovered unit and prosodic feature streams, and an adapted HiFi-GAN model converting MS-TLM outputs to waveform\n"},"KB/partially-informed-ethical-judgement":{"title":"partially informed ethical judgement","links":[],"tags":["ethics"],"content":"Partially Informed Ethical Judgement\n\nwith some information about a given agent’s state and knowledge\n"},"KB/past-tense":{"title":"past tense","links":["KB/Verb"],"tags":["language"],"content":"Past Tense\n\nWhen a speaker needs to say the past tense form of a Verb if the Verb has an irregular form, choose that if not, add -ed\n"},"KB/pixelattribution":{"title":"pixelattribution","links":[],"tags":["explainability"],"content":"Pixelattribution\nPixel Attribution\n\nPixel attribution is a special case of feature attribution, but for images\nFeature attribution explains individual predictions by attributing each input feature according to how much it changed the prediction (negatively or positively)\nThe features can be input pixels, tabular data or words\n\nOcclusion or Perturbation-based\n\nMethods like SHAP and LIME manipulate parts of the image to generateexplanations (modelagnostic).\n\nGradient-based\n\ncompute the gradient of the prediction (or classification score) with respect to theinput feature\nThe gradient-based methods (of which there are many) mostly differ in how thegradient is computed.\nexplanation has the same size as the input image (or at least can be meaningfullyprojected onto it) and they assign each pixel a value that can be interpreted as therelevance of the pixel to the prediction or classification of that image.\n\nGradient-only Methods\n\nwhether a change in a pixel would change the prediction\nExamples are Vanilla Gradient and Grad-CAM\nIf I were to increase the color values of the pixel, the predicted class probabilitywould go up (for positive gradient) or down (for negative gradient\nThe larger the absolute value of the gradient, the stronger the effect of a change ofthis pixel.\n\nPath-attribution Methods\n\ncompare the current image to a reference image, which can be an artificial “zero”image such as a completely grey image\nThe difference in actual and baseline prediction is divided among the pixels\nThe baseline image can also be multiple images: a distribution of images\nmodel-specific gradient-based methods such as Deep Taylor and IntegratedGradients\nmodel-agnostic methods such as LIME and SHAP.\nSome path-attribution methods are “complete”, meaning that the sum of therelevance scores for all input features is the difference between the prediction of theimage and the prediction of a reference image\nExamples are SHAP and Integrated Gradients.\nThe difference between classification scores of the actual image and the baselineimage are attributed to the pixels\nThe choice of the reference image (distribution) has a big effect on the explanation\n\nVanilla Gradient\n\n\nWe calculate the gradient of the loss function for the class we are interested in withrespect to the input pixels\n\n\nThis gives us a map of the size of the input features with negative to positive values.\n\n\nThe recipe for this approach is:\n\n\n\nPerform a forward pass of the image of interest.\n\n\n\n\nCompute the gradient of the class score of interest with respect to the input pixels:\n\n\n\n\n\n\nHere we set all other classes to zero.\n\n\n\nVisualize the gradients. You can either show the absolute values or highlight negative and positive contributions separately.\n\n\n\nMore formally, we have an image I and the convolutional neural 𝑆𝑐(𝐼) network gives ita score\n\n\nfor class c\n\n\nThe score is a highly non-linear function of our image.\n\n\nThe idea behind using the gradient is that we can approximate that score byapplying a first-order Taylor expansion\n\n\n\n\n\nsome ambiguity how to perform a backward pass of the gradients\n\n\nsince non-linear units such as ReLU (Rectifying Linear Unit) “remove” the sign\n\n\nSo when we do a backpass, we do not know whether to assign a positive or negativeactivation\n\n\n\n\n\nThis means that when the activation of a neuron is zero, we do not know which valueto backpropagate\n\n\nIn the case of Vanilla Gradient, the ambiguity is resolved as follows: 𝛿𝑓= 𝛿𝑓\n\n\n\n\n\n𝐈 e r e , activation at the lower layer was negative, and one where it is positive orzero\n\n\nVanilla Gradient takes the gradient we have backpropagated so far up to layer n+1,and then simply sets the gradients to zero where the activation at the layer below isnegative\n\n\nProblems with Vanilla Gradient\n\nsaturation problem\nWhen ReLU is used, and when the activation goes below zero, then the activation iscapped at zero and does not change any more\n\nDeconvNet\n\nalmost identical to Vanilla Gradient\nThe goal of DeconvNet is to reverse a neural network and the paper proposesoperations that are reversals of the filtering, pooling and activation layers\nbut apart from the reversal of the ReLU layer, DeconvNet is equivalent to the Vanilla Gradient approach\nVanilla Gradient can be seen as a generalization of DeconvNet\nDeconvNet makes a different choice for backpropagating the gradient- 12:24  through ReLU:\n\nWhen backpassing from layer n to layer n-1, DeconvNet “remembers” which of theactivations in layer n were set to zero in the forward pass and sets them to zero in layer n-1\n𝑋ctivations with a negative value in layer x are set to zero in layer n-1.\n\nGrad-CAM\n\nGrad-CAM provides visual explanations for CNN decisions\nUnlike other methods, the gradient is not backpropagated all the way back to theimage, but (usually) to the last convolutional layer to produce a coarse localizationmap that highlights important regions of the image.\nGradient-weighted Class Activation Map\nbased on the gradient of the neural networks\nassigns each neuron a relevance score for the decision of interest\nThis decision of interest can be the class prediction (which we find in the outputlayer), but can theoretically be any other layer in the neural network\nGradCAM can be used with different CNNs: with fully-connected layers, forstructured output such as captioning and in multi-task outputs, and forreinforcement learning.\ns a reminder, the first convolutional layer of a CNN takes as input the images andoutputs feature maps that encode learned features\nhigher-level convolutional layers do the same, but take as input the feature maps ofthe previous convolutional layers\nThere are k feature maps in the last 𝐴 ,𝐴 ,…,𝐴\nGrad-CAM has to decide how important each of the k feature map was to our class cthat we are interested in\nWe have to weight each pixel of each feature map with the gradient before weaverage over the feature maps\nThis gives us a heatmap which highlights regions that positively or negatively affectthe class of interest\nThis heatmap is send through the ReLU function, which is a fancy way of saying thatwe set all negative values to zero\nGrad-CAM removes all negative values by using a ReLU function, with the argumentthat we are only interested in the parts that contribute to the selected class c and not to other classes\nThe word pixel might be misleading here as the feature map is smaller than theimage (because of the pooling units) but is mapped back to the original image\nWe then scale the GradCAM map to the interval [0,1] for visualization purposes and overlay it over the original image.\n\nForward-propagate the input image through the convolutional neural network.\nObtain the raw score for the class of interest, meaning the activation of the neuron before the softmax layer.\nSet all other class activations to zero.\n\nBack-propagate the gradient of the class of interest to the last 𝛿𝑦𝑐 𝛿𝐴𝑘 convolutional layer before the fully connected layers.\nWeight each feature map “pixel” by the gradient for the class. Indices i and j refer to the width and height dimensions.\n\nThis means that the gradients are globally pooled\nCalculate an average of the feature maps, weighted per pixel by the gradient.\nApply ReLU to the averaged feature map.\nScale values to the interval between 0 and 1. Upscale the image and overlay it over the original image.\nAdditional step for Guided Grad-CAM: Multiply the heatmap with guided backpropagation.\n\nGuided Grad-CAM\n\nFrom the description of Grad-CAM you can guess that the localization is verycoarse, since the last convolutional feature maps have a much coarser resolution compared to the input image\nIn contrast, other attribution techniques backpropagate all the way to the inputpixels\nThey are therefore much more detailed and can show you individual edges or spotsthat contributed most to a prediction\nYou compute for an image both the GradCAM explanation and the explanationfrom another attribution method, such as Vanilla Gradient\nThe Grad-CAM output is then upsampled with bilinear interpolation, then both mapsare multiplied element-wise\nGrad-CAM works like a lense that focuses on specific parts of the pixel-wiseattribution map.\n\n# SmoothGrad\n\nmake gradient-based explanations less noisy by adding noise and averaging overthese artificially noisy gradient\nSmoothGrad is not a standalone explanation method, but an extension to anygradientbased explanation method.\n\n\nGenerate multiple versions of the image of interest byadding noise to it. 2.Create pixel attribution maps for all images.\n\n\n\n\nAverage the pixel attribution maps.\n\n\nThe theory is that the derivative fluctuates greatly at small scales\nNeural networks have no incentive during training to keep the gradients smooth,their goal is to classify images correctly\nAveraging over multiple maps “smooths out” these fluctuations\n\n\n\nAnd that is the big issue with all of these methods\nWe do not have a ground truth for the explanations\nWe can only, in a first step, reject explanations that obviously make no sense (andeven in this step we do not have strong confidence\n\nDisadvantages\n\nit is difficult to know whether an explanation is correct, and a huge part of theevaluation is only qualitative\nPixel attribution methods can be very fragile\nGhorbani et al. (2019)86 showed that introducing small (adversarial) perturbationsto an image, which still lead to the same prediction, can lead to very different pixelsbeing highlighted as explanations.\nKindermans et al. (2019) 87 also showed that these pixel attribution methods can behighly unreliable\nThey added a constant shift to the input data, meaning they added the same pixelchanges to all images\nThey compared two networks, the original network and the “shifted” network wherethe bias of the first layer is changed to adapt for the constant pixel shift\nBoth networks produce the same predictions.\nFurther, the gradient is the same for both\nBut the explanations changed, which is an undesirable property. They looked at DeepLift, Vanilla Gradient and Integrated Gradients.\n"},"KB/plan-recognition-problem":{"title":"plan recognition problem","links":["KB/Andes"],"tags":["usermodel"],"content":"Plan Recognition Problem\n\nIt is likely that the student wants a hint on how to do that step correctly. If the tutor can determine which correct step corresponds to the incorrect step entered by the student, then it can safely hint that correct step. On the other hand, if the tutor cannot determine which correct step corresponds to the student’s step, the tutor may ask the student what step they were trying to enter. Andes does this for unrecognizable equations that the student wants help on.\nIf none of these simple cases apply, then the tutoring system has to somehow figure out which of the possible next steps is most likely to be part of the solution that the student is trying to pursue.\n(Russell &amp; Norvig, 2003)\nThe general idea is to match the sequence of steps leading up to the most recent student step against step sequences that are either generated by a planner or pulled from a plan library. Once the tutor has determined the plan that the student seems to be following, then it can easily determine what the next step in that plan is.\n"},"KB/probabl-hackathon":{"title":"probabl hackathon","links":[],"tags":["deeplearning"],"content":"\nProbabl Hackathon\nIntro\n\ndoc\naround scikitlearn → profits\n\ninria\nfit predict and a few tools\n“product company”\npre-industrialization\n\n\nautomatic analysis : sklearn too?\ncroissant\n\nalso for benchmarks at some point\n\n\n\nBreakouts\nOrganizing Community Events and Onboarding Contributors"},"KB/psychology":{"title":"psychology","links":["KB/Attentions-and-salience","KB/Berkeley-et-al","KB/Challenges-of-Words-and-rules","KB/Connectionism","KB/Connectionist-Networks","KB/Cross-situational-learning","KB/Deductive-Approaches","KB/Distributive-units","KB/Elman-1990","KB/Elman-1991","KB/Elman-1992","KB/Elman-1993","KB/Emergentism","KB/Extra-position","KB/First-order-generalization","KB/Inductive-Learning","KB/Localist-units","KB/Memory-based-learning","KB/Milin-et-al","KB/Mirman-et-al","KB/Misyak-et-al-2010","KB/Nativists","KB/Non-adjacent-dependencies","KB/Object-relative-clauses","KB/Overhypotheses","KB/Propose-but-verify","KB/Rescorla-Wagner-Algorithm","KB/Rescorla-Wagner-Blocking","KB/Saffran,-Aslin-and-Newport","KB/Second-order-generalization","KB/Semantics-influences-form","KB/Single-unit-recording","KB/Subject-relative","KB/Subject-verb-agreement","KB/Superposition-Catastrophe","KB/Symbolic-learning-model","KB/Symbolic-models","KB/Transitional-probabilities","KB/Visual-Implicit-Learning","KB/Wh-dependencies","KB/Wickelphones","KB/Words-and-Rules","KB/conditioning"],"tags":["anchor"],"content":"\nAttentions and salience\nBerkeley et al\nChallenges of Words-and-rules\nConnectionism\nConnectionist Networks\nCross-situational learning\nDeductive Approaches\nDistributive units\nElman 1990\nElman 1991\nElman 1992\nElman 1993\nEmergentism\nExtra-position\nFirst order generalization\nInductive Learning\nLocalist units\nMemory-based learning\nMilin et al\nMirman et al\nMisyak et al 2010\nNativists\nNon-adjacent dependencies\nObject-relative clauses\nOverhypotheses\nPropose-but-verify\nRescorla-Wagner Algorithm\nRescorla-Wagner Blocking\nSaffran, Aslin and Newport\nSecond order generalization\nSemantics influences form\nSingle unit recording\nSubject relative\nSubject-verb agreement\nSuperposition Catastrophe\nSymbolic learning model\nSymbolic models\nTransitional probabilities\nVisual Implicit Learning\nWh-dependencies\nWickelphones\nWords-and-Rules\nconditioning\n"},"KB/quotes":{"title":"quotes","links":["KB/Degrees-of-Freedom"],"tags":[],"content":"QUOTES\n\n\n“Will you promise me that when you are sad, you’ll think about the sky?” I don\nt know why he wants me to promise him that but I nod anyway. “But why?”\n\n\n“Because.” He turns his face back up to the stars. “The sky is always\nbeautiful. Even when its dark or rainy or cloudy, its still beautiful to look\nat. It’s my favorite thing because I know if I ever get lost or lonely or\nscared, I just have to look up and it’ll be there no matter what…and I know\nit’ll always be beautiful”\n\n\n“In a quiet corner at the back, near a row of bunk beds where the surrounding\nghosts seemed even more tangible, I found a photo displayed on a wall.\nActually, there were a lot of photos of the Holocaust, but this was the one\nthat has never left me. It was in black and white and it showed a short, stocky\nwoman walking down a wide path between towering electrified fences. By the look\nof the light, it was late in the afternoon, and in the language of those times,\nshe was dressed like a peasant. By chance, there were no guards, no dogs, no\nwatchtowers in the photo, though I m sure they were there – just a lonely woman\nwith a baby in her arms and her other two children holding tight to her skirt.\nStoic, unwavering, supporting their tiny lives – helping them as best as any\nmother could – she walked them towards the gas chamber. You could almost hear\nthe silence, smell the terror. I stared at it, both uplifted and devastated by\nthe stark image of a family and a mother’s endless love.\n\n\n“If you want to be free, all you have to do is let go.”\n\n\n“Thomas Edison’s last words were It’s very beautiful over there . I don t know\nwhere there is, but I believe it’s somewhere, and I hope it’s beautiful.”\n\n\n“He was shaken by the overwhelming revelation that the headlong race between\nhis misfortunes and his dreams was at that moment reaching the finish line. The\nrest was darkness, Damn it, he sighed. How will I ever get out of this\nlabyrinth!”\n\n\n“The only way out of the labyrinth of suffering is to forgive.”\n\n\n“The descent beckons as the ascent beckoned”\n\n\n“Before me things created were none, save things Eternal, and eternal I endure.\nAll hope abandon, ye who enter here.”\n\n\n“We accept the love we think we deserve.”\n\n\n“We are what we choose to be” she said. “Let others determine your worth, and\nyou ve already lost, because no one wants people worth more than themselves.\n\n\n“Charlie, I told you not to think of me that way nine months ago because of\nwhat I m saying now. Not because I didn t think you were great. It’s just that\nI don t want to be somebody’s crush. If somebody likes me, I want them to like\nthe real me, not what they think I am. And I don t want them to carry it around\ninside. I want them to show me, so I can feel it, too. I want them to be able\nto do whatever they want around me. And if they do something I don t like, I ll\ntell.”\n\n\n“Strange how deeper the hole the stronger it draws a man. The fascination that\nlives on the keenest edge, and sparkles on the sharpest point, also gathers in\ndepths of a fall.”\n\n\n“You can only win the game when you understand that it is a game. Let a man\nplay chess, and tell him that every pawn is his friend. Let him think both\nbishops holy. Let him remember happy days in the shadows of his castles. Let\nhim love his queen. Watch him lose them all.”\n\n\n“Memory is all we are. Moments and feelings, captured in amber, strung on\nfilaments of reason. Take a man’s memories and you take all of him. Chip away a\nmemory at a time and you destroy him as surely as if you hammered nail after\nnail through his skull.”\n\n\n“As a child there’s a horror in discovering the limitations of the ones you\nlove. The time you find that your mother cannot keep you safe, that your tutor\nmakes a mistake, that the wrong path must be taken because the grown-ups lack\nthe strength to take the right one … each of those moments is the theft of\nyour childhood, each of them a blow that kills some part of the child you were,\nleaving another part of the man exposed, a new creature, tougher but tempered\nwith bitterness and disappointment.”\n\n\n“They say that time is a great teacher but unfortunately it kills all its\npupils.”\n\n\n“Because I know there are people who say all these things don t happen. And\nthere are people who forget what it’s like to be sixteen when they turn\nseventeen. I know these will all be stories some day, and our pictures will\nbecome old photographs. We all become somebody’s mom or dad. But right now,\nthese moments are not stories. This is happening. I am here, and I am looking\nat her. And she is so beautiful. I can see it. This one moment when you know\nyou re not a sad story. You are alive. And you stand up and see the lights on\nthe buildings and everything that makes you wonder. And you re listening to\nthat song, and that drive with the people who you love most in this world. And\nin this moment, I swear, we are infinite.”\n\n\n“Oh God, what is it with mankind? Never happy unless it’s blowing someone up”\n\n\n“We re all dead the moment we re born. Just, some of us get there faster than\nothers.”\n\n\n“After all, what more does a true genius want? The mind itself is the palace\nwhere all the real treasures, the works of art, the indulgences exist.”\n\n\n“Perhaps the greatest faculty our minds possess is the ability to cope with\npain. Classic thinking teaches us of the four doors of the mind, which everyone\nmoves through according to their need. First is the door of sleep. Sleep offers\nus a retreat from the world and all its pain. Sleep marks passing time, giving\nus distance from the things that have hurt us. When a person is wounded they\nwill often fall unconscious. Similarly, someone who hears traumatic news will\noften swoon or faint. This is the mind’s way of protecting itself from pain by\nstepping through the first door. Second is the door of forgetting. Some wounds\nare too deep to heal, or too deep to heal quickly. In addition, many memories\nare simply painful, and there is no healing to be done. The saying time heals\nall wounds is false. Time heals most wounds. The rest are hidden behind this\ndoor. Third is the door of madness. There are times when the mind is dealt such\na blow it hides itself in insanity. While this may not seem beneficial, it is.\nThere are times when reality is nothing but pain, and to escape that pain the\nmind must leave reality behind. Last is the door of death. The final resort.\nNothing can hurt us after we are dead, or so we have been told.”\n\n\n“It is a word. Words are pale shadows of forgotten names. As names have power,\nwords have power. Words can light fires in the minds of men. Words can wring\ntears from the hardest hearts. A word is nothing but a painting of fire. A name\nis the fire itself.”\n\n\n“Books are a poor substitute for female companionship, but they are easier to\nfind.”\n\n\n“You can divide infinity an infinite number of times, and the resulting pieces\nwill still be infinitely large,” Uresh said in his odd Lenatti accent. “But if\nyou divide a non-infinite number an infinite number of times the resulting\npieces are non-infinitely small. Since they are non-infinitely small, but there\nare an infinite number of them, if you add them back together, their sum is\ninfinite. This implies any number is, in fact, infinite.”\n\n\n“The Darkest Minds tend to hide behind the most unlikely faces.”\n\n\n“And people like you are the reason we have middle fingers.”\n\n\n“Strong feelings, especially terror and desperation, leave an imprint on the\nair that echo back to whoever’s unlucky enough to walk through that place\nagain.”\n\n\n“Sometimes the darkness lives inside you, and sometimes it wins.”\n\n\n“Before you leave, the fortune-teller reminds you that the future is never set\nin stone.”\n\n\n“I slept, and dreamed that life was beauty; I woke, and found that life was\nduty.”\n\n\n“They ve never learned the most important rule of cyberspace – computers don t\nlie, but liars can compute.”\n\n\n“Most maidens are perfectly capable of rescuing themselves in my experience, at\nleast the ones worth something, in any case.”\n\n\n“I guess that’s what saying good-bye is always like—like jumping off an edge.\nThe worst part is making the choice to do it. Once you re in the air, there’s\nnothing you can do but let go.”\n\n\n“There is nothing over which a free man ponders less than death; his wisdom is,\nto meditate not on death but on life”\n\n\n“To light a candle is to cast a shadow”\n\n\n“It’s the silence that scares me. It’s the blank page on which I can write my\nown fears.”\n\n\n“Memories are dangerous things. You turn them over and over, until you know\nevery touch and corner, but still you’ll find an edge to cut you. Each day the\nmemories weigh a little heavier. Each day they drag you down that bit further.\nYou wind them around you, a single thread at a time, and you weave your own\nshroud, you build a cocoon, and in it madness grows.”\n\n\n“It is easier to feel the numbness of certainty than live along the burning\nedge of hope.”\n\n\n“What does the future look like?” “I see it in colors,” I said. “A deep blue,\nfading into golds and reds—like fire on a horizon. Afterlight. It’s a sky that\nwants you to guess if the sun is about to rise or set.”\n\n\n“As bad as everything seems, I think, at its heart, life is good. It doesn’t\nthrow anything at us that it knows we can t handle—and, even if it takes its\ntime, it turns everything right side up again”\n\n\n“I ll walk forever with stories inside me that the people I love the most can\nnever hear.”\n\n\n“You will be surprised how much more profitable an independent man is than a\nslave who thinks of nothing more than his next meal.”\n\n\n“Do not dash if you only have the strength to walk, and do not waste your time\npushing on walls that will not give. More importantly, don t shove where a pat\nwould be sufficient.”\n\n\n“To live is to have worries and uncertainties. Keep them inside, and they will\ndestroy you for certain-leaving behind a person so callused that emotion can\nfind no root in his heart.”\n\n\n“Answers were always important, but they were seldom easy.”\n\n\nThe slow regard of silent things\n\n\n“Love, family, accomplishments—they are all torn away, leaving nothing. What is\nthe worth of anything we do? The worth is in the act. Your worth halts when you\nsurrender the will to change and experience life. But options are before you;\nchoose one and dedicate yourself to it. The deeds will give you new hope and\npurpose.”\n\n\n“I d imagine the whole world was one big machine. Machines never come with any\nextra parts, you know. They always come with the exact amount they need. So I\nfigured, if the entire world was one big machine, I couldn t be an extra part.\nI had to be here for some reason. And that means you have to be here for some\nreason, too.”\n\n\n“But you see,” said Roark quietly, “I have, let’s say, sixty years to live.\nMost of that time will be spent working. I ve chosen the work I want to do. If\nI find no joy in it, then I m only condemning myself to sixty years of torture.\nAnd I can find the joy only if I do my work in the best way possible to me. But\nthe best is a matter of standards—and I set my own standards. I inherit\nnothing. I stand at the end of no tradition. I may, perhaps, stand at the\nbeginning of one.”\n\n\n“Man cannot survive except through his mind. He comes on earth unarmed. His\nbrain is his only weapon. Animals obtain food by force. man had no claws, no\nfangs, no horns, no great strength of muscle. He must plant his food or hunt\nit. To plant, he needs a process of thought. To hunt, he needs weapons, and to\nmake weapons - a process of thought. From this simplest necessity to the\nhighest religious abstraction, from the wheel to the skyscraper, everything we\nare and we have comes from a single attribute of man -the function of his\nreasoning mind.”\n\n\n“Throughout the centuries there were men who took first steps down new roads\narmed with nothing but their own vision. Their goals differed, but they all had\nthis in common: that the step was first, the road new, the vision unborrowed,\nand the response they received — hatred. The great creators — the thinkers, the\nartists, the scientists, the inventors — stood alone against the men of their\ntime. Every great new thought was opposed. Every great new invention was\ndenounced. The first motor was considered foolish. The airplane was considered\nimpossible. The power loom was considered vicious. Anesthesia was considered\nsinful. But the men of unborrowed vision went ahead. They fought, they suffered\nand they paid. But they won.”\n\n\n“Thousands of years ago the first man discovered how to make fire. He was\nprobably burnt at the stake he d taught his brothers to light, but he left them\na gift they had not conceived and he lifted darkness from the face of the\nEarth.” -The Fountainhead\n\n\n“He was afraid of the coming night, but it was a distant feeling, like knowing\nyou would grow old and die one day.”\n\n\n“You can t help everyone, Arlen,” Ragen said, “but you should make every effort\nto help those you can.”\n\n\n“Welcome to adulthood,” Cob said. “Every child finds a day when they realize\nthat adults can be weak and wrong just like anyone else. After that day, you re\nan adult, like or not.”\n\n\n“No one, no one, ever goes to the Creator with all their business complete. We\nall get a different length of time, but it needs to be enough, regardless”\n\n\n“I don t pretend to see the path,” Jona said calmly, “but I know it’s there all\nthe same. One day, we ll look back and wonder how we ever missed it.”\n\n\n“Once the apple is ripe,” he murmured to himself, no man can turn it back to a\ngreening.”\n\n\n“Why, my luck’s no greater than yours or any man s. You need only sharpen your\neyes to see your luck when it comes, and sharpen your wits to use what falls\ninto your hands.”\n\n\n“Life’s a forge, say I! Face the pounding; don t fear the proving; and you’ll\nstand well against any hammer and anvil!” -Pyridan\n\n\n“The Meaning of Life is found in the times of your life when it seems stupid to\nask the question, what does it all mean?” -Anonymous\n\n\n“To live in the world without becoming aware of the meaning of the world is\nlike wandering about in a great library”\n\n\n“Life is filled with secrets. You can t learn them all at once.” -The Lost\nSymbol\n\n\n“Remember that, for a broken promise cannot be mended and new promises cannot\ngrow until the roots of betrayal have turned to dust.”\n\n\n“Simple answers to greater questions also create shadows, making it easier to\nhide the truth of things. Sadly there are many who desire simplicity in all\nthings and they are easy prey for the shadows, for they are easily fooled.”\n\n\n“The world offers many lessons to us all,” Carly mused. “What is a mistake but\nanother opportunity to learn another lesson.”\n\n\n“Science and religion are not at odds. Science is simply too young to\nunderstand.”\n\n\n“History is always written by the winners. When two cultures clash, the loser\nis obliterated, and the winner writes the history books-books which glorify\ntheir own cause and disparage the conquered foe. As Napoleon once said, What is\nhistory, but a fable agreed upon?”\n\n\n“Every faith in the world is based on fabrication. That is the definition of\nfaith―acceptance of that which we imagine to be true, that which we cannot\nprove. Every religion describes God through metaphor, allegory, and\nexaggeration, from the early Egyptians through modern Sunday school.”\n\n\nMetaphors are a way to help our minds process the unprocessible. The problems\narise when we begin to believe literally in our own metaphors. Should we wave a\nflag and tell the Buddhists that we have proof the Buddha did not come from a\nlotus blossom? Or that Jesus was not born of a literal virgin birth? Those who\ntruly understand their faiths understand the stories are metaphorical.”\n\n\n“No good deed goes unpunished.”\n\n\n“Science tells me God must exist. My mind tells me I ll never understand God.\nMy heart tells me I m not meant to.”\n\n\n“But who is more ignorant? The man who cannot define lightning, or the man who\ndoes not respect its awesome power?”\n\n\n“Only one form of contagion travels faster than a virus. And that’s fear.”\n\n\n“Don t show it and don t panic. Do like the ducks; on the surface stay calm,\nand below it paddle like hell”\n\n\n“Force a hand, and it will fight you. But convince a mind to think as you want\nit to think, and you have an ally”\n\n\n“Live in the present, remember the past, and fear not the future, for it\ndoesn’t exist and never shall. There is only now.” ― Christopher Paolini,\nEldest\n\n\n“The sea is emotion incarnate. It loves, hates, and weeps. It defies all\nattempts to capture it with words and rejects all shackles. No matter what you\nsay about it, there is always that which you can t.”\n\n\n“Those whom we most love are often the most alien to us.”\n\n\n“When you teach them-teach them not to fear. Fear is good in small amounts, but\nwhen it is a constant, pounding companion, it cuts away at who you are and\nmakes it hard to do what you know is right.”\n\n\n“How terrible,” said Eragon, “to die alone, separate even from the one who is\nclosest to you.” Everyone dies alone, Eragon. Whether you are a king on a\nbattlefield or a lowly peasant lying in bed among your family, no one can\naccompany you into the void.”\n\n\n“But ask yourself this Eragon: If gods exist, have they been good custodians of\nalagaesia? Death, sickness, poverty, tyranny and countless other miseries stalk\nthe land. If this is the handiwork of divine beings, then they are to be\nrebelled against and overthrown, not given obeisance, obedience, and\nreverance.”\n\n\n“Always it is thus with my new students, and especially with the human ones;\nthe mind is the last muscle they train or use, and the one that they regard the\nleast. Ask them about swordplay and they can list every blow from a duel a\nmonth old, but ask them to solve a problem or make a coherent statement and…\nwell, I would be lucky to get more than a blank stare in return.”\n\n\n“Anger has its place, but it will not serve you here, the way of the warrior is\nthe way of knowing. Of that knowledge requires you to use anger, then you use\nanger, but you cannot wrest forth knowledge by losing your temper.”\n\n\n“Misfortune always comes to those who wait. The trick is to find happiness in\nthe brief gaps between disasters.”\n\n\n“It is easy to be calm when there is nothing to worry about, Eragon. The true\ntest of your self-control, however, is whether you can remain calm in a trying\nsituation. You cannot allow anger or frustration to cloud your thoughts, not at\nthe moment. Right now, you need your mind to be clear. Have you always been\nable to remain calm at times like this? The old dragon seemed to chuckle. No. I\nused to growl and bite and knock down trees and tear up the ground. Once, I\nbroke the top off of a mountain in the Spine; the other dragons were rather\nupset with me for that. But I have had many years to learn that losing my\ntemper rarely helps. You have not, I know, but allow my experience to guide you\nin this. Let go of your worries and focus on the task at hand. The future will\nbe what is will, and fretting about it will only make your fears more likely to\ncome true. I know, Eragon sighed, but it’s not easy. Of course not. Few things\nof worth are. Then Glaedr withdrew and left him to the silence of his own\nmind.”\n\n\n“We only give credence to that which we can prove exists. Since we cannot find\nevidence that gods, miracles, and other supernatural things are real, we do not\ntrouble ourselves about them. If that were to change, if Helzvog were to reveal\nhimself to us, then we would accept the new information and revise our\nposition.” “It seems a cold world without something … more.” “On the\ncontrary,” said Oromis, “it is a better world. A place where we are responsible\nfor our own actions, where we can be kind to one another because we want to and\nbecause it is the right thing to do, instead of being frightened into\nbehaving by the threat of divine punishment. I won t tell you what to\nbelieve, Eragon. It is far better to be taught to think critically and\nthen be allowed to make your own decisions than to have someone else’s\nnotions thrust upon you. You asked after our religion, and I have\nanswered you true. Make of it what you will.”\n\n\n“First, let no one rule your mind or body. Take special care that your thoughts\nremain unfettered. One may be a free man and yet be bound tighter than a slave.\nGive men your ear, but not your heart. Show respect for those in power, but don\nt follow them blindly. Judge with logic and reason, but comment not. “Consider\nnone your superior, whatever their rank or station in life. Treat all fairly or\nthey will seek revenge. Be careful with your money. Hold fast to your beliefs\nand others will listen.” He continued at a slower pace, “Of the affairs of love\n… my only advice is to be honest. That’s your most powerful tool to unlock\na heart or gain forgiveness. That is all I have to say.”\n\n\n“No one thinks of himself as a villain, and few make decisions they think are\nwrong. A person may dislike his choice, but he will stand by it because, even\nin the worst circumstances, he believes that it was the best option available\nto him at the time.”\n\n\n“It doesn’t matter what other people think. The only opiniion that really\nmatters is yours. We are all the writers of our lives. We can make our stories\ncomedies or tragedies. Tales of horror, or of inspiration. Your attitude and\nyour fortitude and courage are what determine your destiny, Nick.… Life is hard\nand it sucks for all. Every person you meet is waging his or her own war\nagainst a callous universe that is plotting against them. And we are all battle\nweary. But in the midst of our hell, there is always something we can hold on\nto, whether it’s a dream of the future or a memory of the past, or a warm hand\nthat soothes us. We just have to take a moment during the fight to remember\nthat we re not alone, and that we re not just fighting for ourselves. We re\nfighting for the people we love.”\n\n\n“In the immortal words of Maya Angelou … people will forget what you said,\npeople will forget what you did, but people will never forget how you made them\nfeel.”\n\n\n“You can blame it all on fate and the universe, but in the end you alone decide\nif you re going to lie down and let hell take you under, or if you re going to\nstand strong in defiance of it all with your middle finger raised.”\n\n\n“Do not fear that which cannot be seen. For they are lost in between. Tis the\nones who come alive That your blood will allow to thrive.”\n\n\n“If you muster that courage to stand under fire and not go down, you will amass\nan inner strength that no one can touch. You won t be another faceless,\nnameless, forgotten human in a long historical line of the defeated. You will\nbe a steeled warrior, and a force to be forever reckoned with. And beneath the\npain that lingers, you will have the comfort of knowing that you are strongest\nof all. That when others caved and broke, you kept fighting even against\nhopeless odds.”\n\n\n“Life sucked and then they billed you for it. Kind of like how airlines charged\nyou money before you got on a plane so that in the event they screwed up and\nkilled you, they were already paid, and they wouldn t have to give you a\nrefund.”\n\n\n“The worst wounds, the deadliest of them, aren t the ones people see on the\noutside. They re the ones that make us bleed internally.”\n\n\n“Great minds discuss ideas. Average minds discuss events. Small minds discuss\nother people. Life’s too short to worry about what other people do or don t do.\nTend your own backyard, not theirs, because yours is the one you have to live\nin.”\n\n\n“People aren t just ants rushing around over a crust of bread. Every life, no\nmatter how isolated, touches hundreds of others. It’s up to us to decide if\nthose micro connections are positive or negative. But whichever we decide, it\ndoes impact the ones we deal with. One word can give someone the strength they\nneeded at that moment or it can shred them down to nothing. A single smile can\nturn a bad moment good. And one wrong outburst or word could be the tiny push\nthat causes someone to slip over the edge into destruction.”\n\n\n“Life isn´t a puzzle to be solved. It´s an adventure to be savored. Let every\nchallenge be a new mountain to climb, not an obstacle to get in your way and\nstop you. Yeah, it´ll be hard, but once you reach the summit of it, you´ll be\nable to see the world for what it really is. And at the top, it never seems to\nhave been as difficult a feat to climb there as you first made it out to be.\nMost of all, you´ll know that you beat the mountain, and that you rule it. It\ndoes not rule you.” ― Sherrilyn Kenyon, Infamous\n\n\n“Human will is the strongest will ever created. There are those who are born to\nsucceed and those who are determined to succeed. The former fall into it, and\nthe latter pursue it at all costs. They won t be denied. Nothing daunts them.”\n\n\n“It’s never the enemy without who brings you down. It’s always the enemy\nwithin.”\n\n\n“Guard your back, Nick. It’s the one you don t see coming. The one you trust\nwhose betrayal is most lethal. They know your weakness and they know how to hit\nthe lowest. It’s when your back is turned and your guard is down that they move\nin for the kill.”\n\n\n“His mom always said that trust was something you earned. And it wasn t\nsomething you gave easy. Too often, it was a tool your enemies used to hurt you\nwith. Give them nothing, baby. Not until you have no choice. The world is harsh\nand it is cold. People can be good and decent, but most of them are only out\nfor themselves and they ll hurt anyone they can .”\n\n\n“Leaving people behind and forgetting them piece by piece was just the way life\nworked.\n\n\n“The people they were, the people we knew, they re not what’s turning into dirt\nin that tomb behind his.” He narrowed his eyes until the folds of his skin\nthreatened to squeeze out his sight entirely. “No,” he said. “We re here for a\nwhile, in these fleshy shells, and all the while we ask Why? What’s this pain I\nfeel? Why do I feel so cut off from the men around me, from the skies\nabove? “I don t think any of us ever receives the answers to those\nquestions”\n\n\n“The human heart has hidden treasures, In secret kept, in silence sealed; The\nthoughts, the hopes, the dreams, the pleasures, Whose charms were broken if\nrevealed”\n\n\n“Sometimes I wonder if we all don t have a blue-steel spring inside us, like\nthat dena of Gorgoth’s coiled tight at the core. I wonder if we don t all go\nstamping and crashing, crashing and stamping in our own little circles going\nnowhere. And I wonder who it is that laughs at us”\n\n\n“When a game cannot be won, change the game.”\n\n\n“Stand in the ashes of a trillion dead Suns and ask if honor matters. The\nsilence is your answer.”\n\n\n“Life is this circle. You re born, you live, you die. You come full circle.\nThere is no way around it, that’s how it goes, that’s how it is. We don t\ncontrol being born and we don t control when we die, but we get to control the\npart where we live.”\n\n\n“Somewhere,” said Elise, drumming her fingers on the table, “there’s a tiny\nlittle village that’s missing its idiot.”\n\n\n“It’s not the ink and paper that matter, but the hand that holds the pen.”\n\n\n“But we have two families in life. The one we re born with that shares our\nblood. Another we meet along the way that’s willing to give its blood for us.”\n\n\n“Hate wears you down and doesn’t hurt your enemy. It’s like taking poison and\nhoping your enemy will die.”\n\n\n“Grief is a doorway through which we pass to realize that the sun is always\nshining.”\n\n\n“Sometimes the only reason why you don t let go of what’s making you sad is\nbecause it’s the only thing that made you happy.”\n\n\n“Sometimes it is the people who no one imagines anything of who do the things\nthat no one can imagine.”\n\n\n“Do you know why people like violence? It is because it feels good. Humans find\nviolence deeply satisfying. But remove the satisfaction, and the act becomes…\nhollow.”\n\n\n“When people talk to each other, they never say what they mean. They say\nsomething else and you re expected to just know what they mean.”\n\n\n“I always believed I can t be the only one in the world. The only person who\nwas… different and here you are.”\n\n\n“The past: a new and uncertain world. A world of endless possibilities and\ninfinite outcomes. Countless choices define our fate: each choice, each moment,\na moment in the ripple of time. Enough ripples, and you change the tide… for\nthe future is never truly set.”\n\n\n“Charles Xavier: Is this what becomes of us? Erik was right. Humanity does this\nto us. Professor X: Not if we show them a better path. Charles Xavier: You\nstill believe? Professor X: Just because someone stumbles and loses their path,\ndoesn’t mean they re lost forever. Sometimes, we all need a little help.\nCharles Xavier: I m not the man I was. Professor X: You re afraid. I remember.\nCharles Xavier: All those voices… so much PAIN. Professor X: It’s not their\npain you re afraid of. It’s yours, Charles. And as frightening as it can be,\nthat pain will make you stronger. If you allow yourself to feel it, embrace it,\n.it will make you more powerful than you ever imagined. It’s the greatest gift\nwe have: to bear their pain without breaking. And it comes from the most human\npart of us: hope. Charles, we need you to hope again.”\n\n\n“Things change. And friends leave. Life doesn’t stop for anybody.”\n\n\n“So, I guess we are who we are for alot of reasons. And maybe we ll never know\nmost of them. But even if we don t have the power to choose where we come from,\nwe can still choose where we go from there. We can still do things. And we can\ntry to feel okay about them.”\n\n\n“And all the books you ve read have been read by other people. And all the\nsongs you ve loved have been heard by other people. And that girl that’s pretty\nto you is pretty to other people. and that if you looked at these facts when\nyou were happy, you would feel great because you are describing unity.”\n\n\n“Standing on the fringes of life… offers a unique perspective. But there\ncomes a time to see what it looks like from the dance floor.”\n\n\n“It’s great that you can listen and be a shoulder to someone, but what about\nwhen someone doesn’t need a shoulder? What if they need the arms or something\nlike that? You can t just sit there and put everybody’s lives ahead of yours\nand think that counts as love. You just can t. You have to do things.”\n\n\n“No matter how much of yourself you give. No matter how much you bleed. In the\nend, they believe whatever lie they want to believe about you. They see only\nthe worst, in spite of the fact that you only gave them your best. And there’s\nnothing you can do or say to change another’s mind. Ever.”\n\n\n“We are defined by our enemies—but also we can choose them”\n\n\n“Even from the ashes of wickedness and tragedy something of beauty can arise.”\n\n\n“If something is worth doing, it’s worth doing right.”\n\n\n“I listened carefully, despite my anger and sorrow, even then my mind was\nworking, looking ahead. “What of the power to protect?” I asked. She closed her\neyes. “That is an illusion. There is no power to protect, only to destroy and\ncreate anew. Protection is a result of the mind and clever use of power to\nmanipulate the actions of those that would harm you, but it is not a result of\npower itself.”\n\n\n“It’s like pain,” I said continuing. “It helps warn you, so you don t hurt\nyourself more. A mother’s pain comes from the fear that we might be hurt, and\nbecause of that, she gets angry with us, but that anger serves the same\npurpose. It often keeps us from doing something stupid and hurting ourselves.”\n\n\n“The real voyage of discovery consists not in seeking new landscapes, but in\nhaving new eyes.”\n\n\n“People are funny. The longer you are around them, the more you start to\nrealize that everyone makes the same motions over and over again. We all want\nto believe that every day is different, that every day we change, but really,\nit seems that certain things are coded into us from the very beginning.”\n\n\n“A ruler must be of all his people, for one can only rule what one knows.”\n\n\n“Most prisons are of our own making. A man makes his own freedom, too.”\n\n\n“Some say a dog or a horse as if every one of them is like every other. I ve\nheard a man call a mare he had owned for seven years it as if he were speaking\nof a chair. I ve never understood that. One does not have to be Witted to know\nthe companionship of a beast, and to know that the friendship of an animal is\nevery bit as rich and complicated as that of a man or woman.”\n\n\n“Do you really think things are so simple? Do you really think you can blame\nall your tomorrows on your yesterdays? It’s your life. It will be whatever you\nmake it.”\n\n\n“I wanted a perfect ending. Now I ve learned, the hard way, that some poems don\nt rhyme, and some stories don t have a clear beginning, middle, and end. Life\nis about not knowing, having to change, taking the moment and making the best\nof it, without knowing what’s going to happen next. Delicious Ambiguity.”\n\n\n“To love. To be loved. To never forget your own insignificance. To never get\nused to the unspeakable violence and the vulgar disparity of life around you.\nTo seek joy in the saddest places. To pursue beauty to its lair. To never\nsimplify what is complicated or complicate what is simple. To respect strength,\nnever power. Above all, to watch. To try and understand. To never look away.\nAnd never, never to forget.”\n\n\n“Religion has convinced people that there’s an invisible man … living in the\nsky. Who watches everything you do every minute of every day. And the invisible\nman has a list of ten specific things he doesn’t want you to do. And if you do\nany of these things, he will send you to a special place, of burning and fire\nand smoke and torture and anguish for you to live forever, and suffer, and\nsuffer, and burn, and scream, until the end of time. But he loves you. He loves\nyou. He loves you and he needs money.”\n\n\n“Dwell on the beauty of life. Watch the stars, and see yourself running with\nthem.”\n\n\n“There is a time for everything, and a season for every activity under heaven:\na time to be born and a time to die, a time to plant and a time to uproot, a\ntime to kill and a time to heal, a time to tear down and a time to build, a\ntime to weep and a time to laugh, a time to mourn and a time to dance, a time\nto scatter stones and a time to gather them, a time to embrace and a time to\nrefrain, a time to search and a time to give up, a time to keep and a time to\nthrow away, a time to tear and a time to mend, a time to be silent and a time\nto speak, a time to love and a time to hate, a time for war and a time for\npeace\n\n\n“Oh, you hate your job? Why didn t you say so? There’s a support group for\nthat. It’s called EVERYBODY, and they meet at the bar.”\n\n\n“You have wakened not out of sleep, but into a prior dream, and that dream lies\nwithin another, and so on, to infinity, which is the number of grains of sand.\nThe path that you are to take is endless, and you will die before you have\ntruly awakened.”\n\n\n“I think that we are like stars. Something happens to burst us open; but when\nwe burst open and think we are dying; we re actually turning into a supernova.\nAnd then when we look at ourselves again, we see that we re suddenly more\nbeautiful than we ever were before.”\n\n\n“That’s how stories happen — with a turning point, an unexpected twist. There’s\nonly one kind of happiness, but misfortune comes in all shapes and sizes. It’s\nlike Tolstoy said. Happiness is an allegory, unhappiness a story.”\n\n\n“If you were all alone in the universe with no one to talk to, no one with\nwhich to share the beauty of the stars, to laugh with, to touch, what would be\nyour purpose in life? It is other life; it is love, which gives your life\nmeaning. This is harmony. We must discover the joy of each other, the joy of\nchallenge, the joy of growth.”\n\n\n“Because I have shown my hands to be empty you must now expect not only that an\nillusion will follow, but that you will acquiesce in it!”\n\n\n“No one expects a good-humored man or a kind woman to be good-humored or kind\nevery day and every moment—yet they re appalled when people are trustworthy for\na month or a year and then they aren t for an hour or a day.”\n\n\n“To be sociable,” IdrisPukke continued, “is a risky thing—even fatal—because it\nmeans being in contact with people, most of whom are dull, perverse and\nignorant and are really with you only because they cannot bear their own\ncompany. Most people bore themselves and greet you not as a true friend but as\na distraction—like a dancing dog or some half-wit actor with a fund of amusing\nstories.”\n\n\n“No one of real intelligence will accept anything just because some authority\ndeclares it to be so. Don t accept the truth of anything you have not confirmed\nfor yourself.”\n\n\n“You should never tell your best friend anything you wouldn´t be prepared to\ntell your worst enemy.”\n\n\n“Feeling sorry for yourself is a universal solvent of salvation.”\n\n\n“Only inferior minds speak or write in order to discover what they think.”\n\n\n“Don t think of it as a lie, think of it as the truth under imaginary\ncircumstances.”\n\n\n“The mere adding of years to life is not living.”\n\n\n“Intelligence has many shades, but rage is the same colour everywhere.\nHumiliation tastes the same to everyone.”\n\n\n“It is better to remain silent at the risk of being thought a fool, than to\ntalk and remove all doubt of it.”\n\n\n“When I die, I want to go peacefully like my grandfather did–in his sleep. Not\nyelling and screaming like the passengers in his car.”\n\n\n“When I was 5 years old, my mother always told me that happiness was the key to\nlife. When I went to school, they asked me what I wanted to be when I grew up.\nI wrote down “happy.” They told me I didn t understand the assignment, and I\ntold them they didn t understand life.\n\n\n“Worry is like a rocking chair, it will give you something to do, but it won t\nget you anywhere.\n\n\n“I dream of a better tomorrow, where chickens can cross the road and not be\nquestioned about their motives.”\n\n\n“The same fence that shuts others out shuts you in.”\n\n\n“You see things; and you say “Why?” But I dream things that never were; and I\nsay “Why not?”\n\n\n“To the world you may be just one person, but to one person you may be the\nworld.”\n\n\n“The best way to predict the future is to invent it.”\n\n\n“The only thing worse than being blind is having sight but no vision.”\n\n\n“I m selfish, impatient and a little insecure. I make mistakes, I am out of\ncontrol, and at times hard to handle. But if you can t handle me at my worst,\nyou sure as hell don t deserve me at my best.”\n\n\n“Fairy tales are more than true–not because they tell us dragons exist, but\nbecause they tell us dragons can be beaten.”\n\n\n“If you have built castles in the air, your work need not be lost; that is\nwhere they should be. Now put the foundations under them.”\n\n\n“In the arithmetic of love, one plus one equals everything, and two minus one\nequals nothing.”\n\n\n“If you steal from one author, it’s plagiarism; if you steal from many, it’s\nresearch.”\n\n\n“Do not hire a man who does your work for money, but him who does it for love\nof it.”\n\n\n“Aerodynamically, the bumblebee shouldn t be able to fly, but the bumblebee\ndoesn’t know that so it goes on flying anyway.”\n\n\n“Solitude is a good place to visit but a poor place to stay.”\n\n\n“I went to a bookstore and asked the saleswoman, “Where’s the self-help\nsection?” She said if she told me, it would defeat the purpose.”\n\n\n“All mothers have intuition. The great ones have radar.”\n\n\n“He who angers you conquers you.”\n\n\n“The only true wisdom is knowing that you know nothing.”\n\n\n“Those who dance are considered insane by those who cannot hear the music.”\n\n\n“I lived in books more than I lived anywhere else.”\n\n\n“Books were safer than other people anyway.”\n\n\n“Nobody looks like what they really are on the inside. You don t. I don t.\nPeople are much more complicated than that. It’s true of everybody.”\n\n\n“I went away in my head, into a book. That was where I went whenever real life\nwas too hard or too inflexible.”\n\n\n“Nothing’s ever the same,” she said. “Be it a second later or a hundred years.\nIt’s always churning and roiling. And people change as much as oceans.”\n\n\n“How can you be happy in this world? You have a hole in your heart. You have a\ngateway inside you to lands beyond the world you know. They will call you, as\nyou grow. There can never be a time when you forget them, when you are not, in\nyour heart, questing after something you cannot have, something you cannot even\nproperly imagine, the lack of which will spoil your sleep and your day and your\nlife, until you close your eyes for the final time, until your loved ones give\nyou poison and sell you to anatomy, and even then you will die with a hole\ninside you, and you will wail and curse at a life ill-lived.”\n\n\n“I was not happy as a child, although from time to time I was content. I lived\nin books more than I lived anywhere else.”\n\n\n“Adults follow paths. Children explore. Adults are content to walk the same\nway, hundreds of times, or thousands; perhaps it never occurs to adults to step\noff the paths, to creep beneath rhododendrons, to find the spaces between\nfences.”\n\n\n“Just go with it. It won t hurt. I stared at him. Adults only ever said that\nwhen it, whatever it happened to be, was going to hurt so much.”\n\n\n“Grown-ups don t look like grown-ups on the inside either. Outside, they re big\nand thoughtless and they always know what they re doing. Inside, they look just\nlike they always have. Like they did when they were your age. Truth is, there\naren t any grown-ups. Not one, in the whole wide world.”\n\n\n“The whole big, complicated world was simple and graspable and easy to unlock.\nI would stay here for the rest of time in the ocean which was the universe\nwhich was the soul which was all that mattered. I would stay here forever.”\n\n\n“I wondered, as I wondered so often when I was that age, who I was, and what\nexactly was looking at the face in the mirror. If the face I was looking at\nwasn´t me, because I would still be me whatever happened to my face, then what\nwas me? If I looked inward I would see only infinite mirrors staring into\nmyself for eternity”\n\n\n“I could not control the world I was in, could not walk away from things or\npeople or moments that hurt, but I took joy in the things that made me happy.”\n\n\n“Different people remember things differently, and you’ll not get any two\npeople to remember anything the same, whether they were there or not.”\n\n\n“I saw the world I had walked since my birth and I understood how fragile it\nwas, that the reality I knew was a thin layer of icing on a great dark birthday\ncake writhing with grubs and nightmares and hunger.”\n\n\n“Do or do not. There is no try.”\n\n\n“The problem is not the problem. The problem is your attitude about the\nproblem.”\n\n\n“It is the unknown we fear when we look upon death and darkness. Nothing more.”\n\n\n“Life is pain, highness. Anyone who says differently is selling something.”\n\n\n“If I got rid of my demons I d lose my angels too.”\n\n\n“The mind is its own place and in itself, can make a Heaven of Hell and a Hell\nof Heaven.”\n\n\n“Studies suggest that risk aversion stems from fear of failure. Fear of failure\nor fear of losing the staus quo, fosters inaction. In other words, fear is the\nenemy of change and innovation.”\n\n\n“Words and the thoughts behind them may be clever, perhaps inspired, but still\nthere can be enough of them. Then it’s better to take it all in silently. We\ndon t need to describe everything we experience, or to express all that we\nlearn. Words are mere shadows. If we focus on them we may lose sight of the\nreality they try to imitate.”\n\n\n“Like the keel of a boat that’s unaffected by the waves on the sea. That’s how\nthe human mind should be – calm in whatever turmoil surrounds it, confident\neven in a rain of urgent questions. The answers are to be found in that calm.”\n\n\n“Words mean more than what is set down on paper. It takes the human voice to\ninfuse them with the shades of deeper meaning.”\n\n\n“There can be a universe without any gods to rule it, but not one without laws\nfor it.”\n\n\n“I ve made mistakes in the years since I was a boy. I tried to learn from them,\nbut it isn t always enough.”\n\n\n“Life goes on. Those who dwell on the past have no future.”\n\n\n“They say the past is etched in stone, but it isn t. It’s smoke trapped in a\nclosed room, swirling changing. Buffeted by the passing of years and wishful\nthinking. But even though our perception of it changes, one thing remains\nconstant. The past can never be completely erased. It lingers. Like the scent\nof burning wood.”\n\n\n” Big world, not all of it flowers and sunshine. And the only way guys like you\nand me can survive is to grab it by the throat and never let go.”\n\n\n“You get what you deserve.” It’s an old saying. One that survived the years,\nbecause it’s true. For the most part. But not for everyone. Some get more than\nthey deserve. Because they believe they aren t like everyone else. That the\nrules, the ones people like me and you, the people that work and struggle to\nlive our lives, just live, don t apply to them. That they can do anything and\nlive happily ever after, while the rest of us suffer. They do this from the\nshadows. Shadows that we cast with our indifference. With a pervasive lack of\ninterest in anything that doesn’t directly affect us, we, in the here and now.\nOr maybe it’s just the shadow of weariness. Of how tired we are, struggling to\nclaw our way back to a middle class that no longer exist, because of those who\ntake more than they deserve. And they keep taking, until all that’s left for\nthe rest of us is a memory of how it used to be before the corporations and the\nbottom line decided we didn t matter anymore. But we do. ”\n\n\n“There is a wide gulf between inaction and murder, Matthew. Another man’s evil\ndoes not make you good. Men have used the atrocities of their enemies to\njustify their own throughout history. So the question you have to ask yourself\nis are you struggling with the fact that you don t want to kill this man, but\nhave to? Or that you don t have to kill him, but want to?”\n\n\n“Just because someone is important to you, it doesn’t necessarily mean that,\nthat person is good. Even if you knew that person was evil… People cannot win\nagainst their loneliness.”\n\n\n“Even the strongest of opponents always has a weakness.”\n\n\n“Rejection is a part of any man’s life. If you can t accept and move past\nrejection, or at least use it as writing material - you re not a real man.”\n\n\n“If you don t share someone’s pain. You can never understand them.”\n\n\n“Once you question your own belief, it’s over.”\n\n\n“It is only through the eyes of others that our lives have any meaning.”\n\n\n“Just by living, people hurt others without even realizing it. So long as\nhumanity exists, hate will also exist. There is no peace in this cursed world.\nWar is just a crime paid for by the pain of the defeated…”\n\n\n“People’s lives don t end when they die. It ends when they lose faith.”\n\n\n“If love is just a word, then why does it hurt so much if you realize it isn t\nthere?”\n\n\n“Humans… Do humans have a purpose when they are born? I have been wondering\nrecently. Because they are born, do they have an important duty? The meaning of\nbeing born… For humans to find that answer… It is the one freedom God gave\nthem.”\n\n\n“Wake up to reality! Nothing ever goes as planned in this world. The longer you\nlive, the more you realize that in this reality only pain, suffering and\nfutility exist.”\n\n\n“Sometimes you must hurt in order to know, fall in order to grow, lose in order\nto gain because life’s greatest lessons are learned through pain.”\n\n\n“A place where someone still thinks about you is a place you can call home.”\n\n\n“The hole in one’s heart gets filled by others around you. Friends won t flock\nto someone who abandons the memory of his friends and gives up on the world\njust because things don t go the way he wants them to. That won t help fill the\nhole in your heart. And people won t help those who run away and do nothing. As\nlong as you don t give up, there will always be salvation.”\n\n\n“When people are protecting something truly special to them, they truly can\nbecome… as strong as they can be.”\n\n\n“Regardless of our limitations, we can always be of some use. Our power may\nseem insignificant… but it may just prove to be useful in the grand scheme of\nthings. Stay focused. Never avert your eyes, because if an opening arises, even\nour insignificant power may be enough to determine the fate of the world. Which\nis why everyone must stay alert and ready to strike at any moment!”\n\n\n“The things that are most important aren t written in books. You have to learn\nthem by experiencing them yourself.”\n\n\n“In life, nothing good comes out of hurrying.”\n\n\n“Maybe, just maybe, there is no purpose in life… but if you linger a while\nlonger in this world, you might discover something of value in it.”\n\n\n“While you re alive, you need a reason for your existence. Being unable to find\none is the same as being dead.”\n\n\n“A wound of a heart is different from a flesh wound. Unlike a flesh wound,\nthere are no ointments to heal it, and there are times when they never heal.”\n\n\n“The best way to escape reality without running is smiling even if it is\nobviously fake.”\n\n\n“Often people have it wrong, mistakenly believing…that showing mercy to an\nenemy is kindness. They spare the foe whose life is in their hands… But don t\nyou see? It’s an empty existence… to go on living… alone and unloved…\nwhen defeat’s already cost you your dream!”\n\n\n“We are humans, not fish. We don t know what kind of people we truly are until\nthe moment before our deaths. As death comes to embrace you, you will realise\nwhat you are. That’s what death is, don t you think?”\n\n\n“Did you really think, just because you deny the existence of a world that you\ndon t understand, it wouldn t exist?”\n\n\n“Director Gordon: See, that’s my concern… he’s not taking this seriously.\nTyler Gage: Well, I m sorry… it’s just that you guys talk about dancing like\nit’s rocket science or something.”\n\n\n“Brett Dolan: When someone hands you your dreams, you take it, you don t ask\nquestions. Nora: I would. Brett Dolan: You think you would.”\n\n\n“Andie: I remember the first time I saw someone move like they were from\nanother planet, I couldn t keep my eyes away. I was little mom took me to a jam\nsession in the neighborhood, it started off small but word spread and soon some\nof the best dancers around were showing up to compete in something they\neventually called the streets. It became home, I got a front row seat to\nhistory. I wanted to glide and spin and fly like they did, but it didn t come\neasy. My mom would tell me don t give up, just be you, because life’s too short\nto be anybody else. She was right. When I was 16 my mom got sick and in a\ncouple months she was gone. Everything changed, including the streets.”\n\n\n“Moose: You know, some famous guy once said: “To Travel is better than to\narrive.” And I was like, “What?” Because I used to think that there was only\none path to take to where you want to be in life. But, if you choose that one\npath, it doesn’t mean you have to abandon all the other ones.I realize it\nactually what happens along the way that counts; the stumbles, the falls, and\nthe friendships. It’s the journey and not the destination. You just gotta, I\nguess, trust that the future is gonna work itself out like it’s supposed to,”\n\n\n“Moose: [from trailer] People dance because dance can change things. One move,\ncan bring people together. One move, can make you believe like there’s\nsomething more. One move, can set a whole generation free.”\n\n\n“Alicia: How big is the universe? Nash: Infinite. Alicia: How do you know?\nNash: I know because all the data indicates it’s infinite. Alicia: But it hasn\nt been proven yet. Nash: No. Alicia: You haven t seen it. Nash: No. Alicia: How\ndo you know for sure? Nash: I don t, I just believe it. Alicia: It’s the same\nwith love I guess.”\n\n\n“Perhaps it is good to have a beautiful mind, but an even greater gift is to\ndiscover a beautiful heart.”\n\n\n“Don t expect much and you won t be disappointed.”\n\n\n“There’s no way someone who can t even protect himself can protect anyone else,\nis there?”\n\n\n“Human relationships are chemical reactions. If you have a reaction then you\ncan never return back to your previous state of being.”\n\n\n“Whose fault is it that things ended up like this? Coincidence? An accident?\nFate? There’s no such thing as fate. It’s simply a combination of one\ncircumstance and the next. And who is it that creates those circumstances? Who\nis it? It’s you.”\n\n\n“It’s not because we can t take vengeance that we should feel sorry. The real\nreason to feel sorry… is when one is hung up on revenge and can t live their\nown life.”\n\n\n“You believe that the strong exist to cull the weak. To use them as food. But\nyou are mistaken … The strong exist, not to feed off of the weak, but to\nprotect them!”\n\n\n“Perhaps the distant part of the sky always seems clearest, so that we will\nalways strive to reach it.”\n\n\n“Who falls for who depends completely on the person. If there are a hundred\npeople, there are a hundred ways to love… There never is one absolute form of\nlove.”\n\n\n“You can die anytime, but living takes true courage.”\n\n\n“You ll only realize that you truly love someone if they already caused you\nenormous pain. Your enemies can never hurt you the way your loved ones can.\nIt’s the people close to your heart that can give you the most piercing wound.\nLove is a double-edged sword, it can heal the wound faster or it can sink the\nblade even deeper.”\n\n\n“The dead don t desire revenge, but the happiness of the living.”\n\n\n“Fate is never thwarted. What has happened has happened because Fate willed it\nthus—if, indeed, there is such a thing as Fate and if men’s actions are not\nmerely a response to other men’s actions.”\n\n\n“So if I asked you about art, you d probably give me the skinny on every art\nbook ever written. Michelangelo, you know a lot about him. Life’s work,\npolitical aspirations, him and the pope, sexual orientations, the whole works,\nright? But I ll bet you can t tell me what it smells like in the Sistine\nChapel. You ve never actually stood there and looked up at that beautiful\nceiling; seen that. If I ask you about women, you d probably give me a syllabus\nabout your personal favorites. You may have even been laid a few times. But you\ncan t tell me what it feels like to wake up next to a woman and feel truly\nhappy. You re a tough kid. And I d ask you about war, you d probably throw\nShakespeare at me, right, “once more unto the breach dear friends.” But you ve\nnever been near one. You ve never held your best friend’s head in your lap,\nwatch him gasp his last breath looking to you for help. I d ask you about love,\nyou d probably quote me a sonnet. But you ve never looked at a woman and been\ntotally vulnerable. Known someone that could level you with her eyes, feeling\nlike God put an angel on earth just for you. Who could rescue you from the\ndepths of hell. And you wouldn t know what it’s like to be her angel, to have\nthat love for her, be there forever, through anything, through cancer. And you\nwouldn t know about sleeping sitting up in the hospital room for two months,\nholding her hand, because the doctors could see in your eyes, that the terms\n\n\n“visiting hours” don t apply to you. You don t know about real loss, cause it\nonly occurs when you ve loved something more than you love yourself. And I\ndoubt you ve ever dared to love anybody that much.”\n\n\n“If you only focus on a leaf, you will miss the entire tree; if you focus only\non a leaf, you might miss the entire forest. Thus, you must focus on things in\ntheir entirety.”\n\n\n“When did we see each other face-to-face? Not until you saw into my cracks and\nI saw into yours. Before that, we were just looking at ideas of each other,\nlike looking at your window shade but never seeing inside. But once the vessel\ncracks, the light can get in. The light can get out.”\n\n\n“You could never really know what you were seeing with just a glance, in\nmotion, passing by. Good or bad, right or wrong. There was always so much\nmore.”\n\n\n“Because that is what happens when you try to run from the past. It doesn’t\njust catch up: it overtakes, blotting out the future, the landscape, the very\nsky, until there is no path left except that which leads through it, the only\none that can ever get you home.”\n\n\n“When you ripped a piece of paper into two: no matter how you tried, the seams\nnever fit exactly right again. It was what you couldn t see, those tiniest of\npieces, that were lost in the severing, and their absence kept everything from\nbeing complete”\n\n\n“I thought of everything being washed away, again and again. We make such\nmesses in this life, both accidentally and on purpose. But wiping the surface\nclean doesn’t really make anything any neater. It just masks what is below.\nIt’s only when you really dig down deep, go underground, that you can see who\nyou really are.”\n\n\n“The monks used to say that hope is just a distraction.”\n\n\n“Harsh words won t solve problems, action will.”\n\n\n“Destiny? What would a boy know of destiny? If a fish lives its whole life in\nthis river, does he know the river’s destiny? No! Only that it runs on and on\nout of his control. He may follow where it flows, but he cannot see the end. He\ncannot imagine the ocean.”\n\n\n“You think you re any different from me, or your friends, or this tree? If you\nlisten hard enough, you can hear every living thing breathing together. You can\nfeel everything growing. We re all living together, even if most folks don t\nact like it. We all have the same roots, and we are all branches of the same\ntree.”\n\n\n“Time is an illusion, and so is death.”\n\n\n“You must never give into despair. Allow yourself to slip down that road, and\nyou surrender to your lowest instincts. In the darkest times, hope is something\nyou give yourself. That is the meaning of inner strength.”\n\n\n“It is important to draw wisdom from different places. If you take it from only\none place it becomes rigid and stale.”\n\n\n“No matter how things seem to change. Never forget who you are.”\n\n\n“But love is a form of energy, and it swirls all around us.”\n\n\n“The greatest illusion of this world is the illusion of separation. Things you\nthink are separate and different are actually one and the same. We are all one\npeople, but we live as if divided.”\n\n\n“Protection and power are overrated. I think you are very wise to choose\nhappiness and love.”\n\n\n“There are reasons each of us are born. we have to find those reasons.”\n\n\n“Sometimes life is like this dark tunnel, you can t always see the light at the\nend of the tunnel, but if you just keep moving, you will come to a better\nplace.”\n\n\n“It’s impossible to go through life unscathed. Nor should you want to. By the\nhurts we accumulate, we measure both our follies and our accomplishments.”\n\n\n“Nothing ever happens the way you imagine it will. But then again, if you don t\nimagine, nothing ever happens at all.”\n\n\n“You don t get to choose if you get hurt in this world but you do have some say\nin who hurts you.”\n\n\n“What a treacherous thing it is to believe that a person is more than a\nperson.”\n\n\n“Grief can be a burden, but also an anchor. You get used to the weight, to how\nit holds you to a place.”\n\n\n“Music should flow like a language. Changing a single note can turn joy to\nsorrow.”\n\n\n“You will look at what I have done and say, Of course - why not - they are all\nanimals. They have slaughtered each other for centuries. But the truth is, I m\nnot a monster. I m a human man - I m just like you, whether you like it or not.\nFor years, we have tried to live together, until a war was waged on us, on all\nof us: a war waged by our own leaders. And who supplied the Serb cluster bombs,\nthe Croatian tanks, the Muslim artillery shells that killed our sons and\ndaughters? It was the governments of the West who drew the boundaries of our\ncountries - sometimes in ink, sometimes in blood - the blood of our people. And\nnow you dispatch your peacekeepers to write our destiny again. We can never\naccept this peace that leaves us with nothing but pain, pain the peacemakers\nmust be made to feel. Their wives, their children, their houses and churches.\nSo now you know, now you must understand. Leave us to find our own destiny. May\nGod have mercy on us all.”\n\n\n“It was being under that Lace loved most. The lightness of her own body, the\nwater trying to lift her toward the surface. The silhouettes of the underwater\ntrees, like a forest on a fall night. How everything looked blurred like she\nwas seeing it through stained glass. How water that had felt cold when she slid\ninto the river now felt as warm as her own body. Even the sharp sting in her\nlungs as she swam out of view to take a breath.”\n\n\n“Regret is a powerful poison. The more you harbor those thoughts, the harder it\nis to move on.”\n\n\n“If you begin to regret, you’ll dull your future decisions and let others make\nyour choices for you. All that’s left for you then is to die. Nobody can\nforetell the outcome. Each decision you make holds meaning only by affecting\nyour next decision.”\n\n\n“I haven t lived all that long yet, but there is something I firmly believe in.\nThe people who have the ability to change something in this world, all without\nexception, have the guts to abandon things important to them if they have to.\nThey are those who can abandon even their humanity.”\n\n\n“People who can t throw something important away, can never hope to change\nanything.”\n\n\n“This world is merciless, and it’s also very beautiful.”\n\n\n“It’s because I fail … I have the strength to keep fighting … and that type of\nstrength is real strength …”\n\n\n“There are many types of monsters that scare me: Monsters who cause troubles\nwithout showing themselves, monsters who abduct children, monsters who devour\ndreams, monsters who suck blood… and then, monsters who tell nothing but lies.\nLying monsters are a real nuisance: They are much more cunning than others:\nThey pose as humans even though they have no understanding of the human heart;\nthey eat even though they ve never experienced hunger; they study even though\nthey have no interest in academics; they seek friendship even though they do\nnot know how to love. If I were to encounter such monsters, I would likely be\neaten by them… because in truth, I am that monster.”\n\n\n“Life is made of so many moments that mean nothing. Then one day, a single\nmoment comes along to define every second that comes after. Such moments are\ntests of courage, of strength.”\n\n\n“There are two kinds of guilt. The kind that’s a burden and the kind that gives\nyou purpose. Let your guilt be your fuel. Let it remind you of who you want to\nbe. Draw a line in your mind. Never cross it again. You have a soul. It’s\ndamaged but it’s there. Don t let them take it from you.”\n\n\n“Fear can be good, Laia. It can keep you alive. But don t let it control you.\nDon t let it sow doubts within you. When the fear takes over, use the only\nthing more powerful, more indestructible to fight it: your spirit. Your heart.”\n\n\n“Giving into the pain and thinking you want to die just means you ve been\nspoiled by life. If you don t want to die, then don t act spoiled. Suffer\nthrough life; crawl through life. Stick it out till the very end. If you still\nwant to die after that, come find me. I ll end you.”\n\n\n“It doesn’t matter whether it’s wrong or not, the thing is… What’s important\nis memory…The memory that you can t forget for the rest of your life…”\n\n\n“If you let yourself get depressed, you will be making light of the people who\nfollow and trust you.”\n\n\n“Happiness is like glass. It may be all around you… Yet be invisible. But if\nyou can change your angle of viewing a little, then it will reflect light more\nbeautifully than any other object around you.”\n\n\n“I ll leave tomorrow’s problems to tomorrow’s me.”\n\n\n“Let’s say I posed this question to you: “Can all human souls be bought with\nmoney or not?” Now remember, the keyword here is “all”. The answer is “There\nare times when you can buy them, and other times, not,” right? The human being…\nsometimes he ll uphold his pride and conscience even if he’s offered ten\nbillion yen, and other times he ll murder someone over one yen.”\n\n\n“People with talent often have the wrong impression that things will go as they\nthink.”\n\n\n“Hard work betrays none, but dreams betray many.”\n\n\n“Life is basically like a soap bubble. It rides on the wind, flying here and\nthere, …And before you realize it, pop! It’s gone. When it’s about to\ndisappear, you think that you could ve flown a bit higher.But by the time, it’s\nalready too late.”\n\n\n“People are like dice, a certain Frenchman said that. You throw yourself in the\ndirection of your own choosing. People are free because they can do that.\nEveryone’s circumstances are different, but no matter how small the choice, at\nthe very least, you can throw yourself. It’s not chance or fate. It’s the\nchoice you made.”\n\n\n“Only hope can give rise to the emotion we call despair. But it is nearly\nimpossible for a man to try to live without hope, so I guess that leaves Man no\nchoice but to walk around with despair as his companion.”\n\n\n“Man fears the darkness, and so he scrapes away at the edges of it with fire.”\n\n\n“We are all like fireworks. We climb, shine, and always go our separate ways\nand become further apart. But even if that time comes, let’s not disappear like\na firework, and continue to shine forever.”\n\n\n“Being alone and being lonely are two different things.”\n\n\n“Do not allow yourself to be blinded by fear and anger. Everything is only as\nit is.”\n\n\n“Well, if you live long enough, you lose a lot. Just as long as you don t throw\nthem away. Whatever you lose, you’ll find again, but what you throw away you\nnever get back.”\n\n\n“Thanks to these eyes…I came to understand how cruel and despicable people\ncan be. But that also allowed me to appreciate true beauty. All you have to do\nis appreciate things from a different perspective Once I realized the things we\ntake for granted are really miracles, I came to see everything in it’s\nprecious, empheral beauty. … I love this world.”\n\n\n“It’s never something huge that changes everything, but instead the tiniest of\ndetails, irrevocably tweaking the balance of the universe while you re busy\nfocusing on the big picture.”\n\n\n“Imma do what I want, whatever. Imma rage til the dawn, all nighter. Don t hold\nyour breath. You know I ll sleep when I am dead.”\n\n\n“We are giants. We are bigger than a monster. Every second we are taking back\nthe power on the run till we get a fucking answer. Try to hold us down but we\nre only getting stronger.”\n\n\n“They say pain is an illusion, this is just a bruise. And you are just\nconfused. But I am only human. I could use a hand sometimes. I am only human.”\n\n\n“In this world, perfection is an illusion. Regardless of all those who utter\nthe contrary, this is the reality. Obviously mediocre fools will forever lust\nfor perfection and seek it out. However, what meaning is there in perfection?\nNone. Not a bit. …After perfection there exists nothing higher. Not\neven room for creation which means there is no room for wisdom or\ntalent either.  Understand? To scientists like ourselves, perfection is\ndespair.”\n\n\n“A different species a different set of values a world completely unlike your\nown. There is a feeling you can only get when you meet the unknown and open\nyour mind.”\n\n\n“If you want to know someone, start by finding out what makes him angry.”\n\n\n“Time is going really faster than you think. That second just passed now, get\nup, pack the bag, get lost…”\n\n\n“Life is something that no one can teach you,You have to learn it.”\n\n\n“People seem weak, but they re strong. They seem strong, but they re weak. No\nmatter how much you cry, you still have to sleep. And you even get hungry. You\nsuddenly realize you re doing the same things you did yesterday. You say hi to\nyour friends and smile just like you did yesterday. Life goes on as if nothing\never happened… I want to go somewhere… Anywhere… Somewhere where I can forget\neverything. …where I ll forget everything …and be reborn.”\n\n\n“Being sorry is far worse punishment than being dead, everybody dies… very\nfew people ever feel truly sorry for the bad things they ve done.”\n\n\n“Because happiness is so hard to find. Once you find it, you better hang on\ntight. Or you lose it.”\n\n\n“I want you to give her a possibility,” she told him, looking at my necklace\nagain. “And that’s what a key represents. An open door, a chance.”\n\n\n“There was something striking about a single key. It was like a question\nwaiting to be answered, a whole missing a half. Useless on its own, needing\nsomething else to be truly defined.”\n\n\n“My point is, there are a lot of people in the world. No one ever sees\neverything the same way you do; it just doesn’t happen. So when you find one\nperson who gets a couple of things, especially if they re important ones …\nyou might as well hold on to them.”\n\n\n“Hey angel… Do you know the reasons why? We look up to the sky? Hey angel…\nDo you look at us and laugh When we hold on to the past?”\n\n\n“Who’s gonna be the first one to compromise Who’s gonna be the first one to set\nit all on fire Who’s gonna be the last one to drive away Forgetting every\nsingle promise we ever made.”\n\n\n“Youth is like diamonds in the sun, And diamonds are forever.”\n\n\n“Anybody’s got the power They don t see it cause they don t understand Spin\naround and round for hours You and me, we got the world in our hands.”\n\n\n“I didn t need you, you idiot. I picked you. And then you picked me back.”\n\n\n“But isn t it also that on some fundamental level we find it difficult to\nunderstand that other people are human beings in the same way that we are? We\nidealize them as gods or dismiss them as animals.”\n\n\n“You listen to people so that you can imagine them, and you hear all the\nterrible and wonderful things people do to themselves and to one another, but\nin the end the listening exposes you even more than it exposes the people you\nre trying to listen to.”\n\n\n“It is easy to forget how full the world is of people, full to bursting, and\neach of them imaginable and consistently misimagined.”\n\n\n“Home wasn t a set house, or a single town on a map. It was wherever the people\nwho loved you were, whenever you were together. Not a place but a moment, and\nthen another, building on each other like bricks to create a solid shelter that\nyou take with you for your entire life, wherever you may go.”\n\n\n“Silence is so freaking loud.”\n\n\n“The only person you can be sure to control, always, is yourself. Which is a\nlot to be sure of, but at the same time, not enough.”\n\n\n“What is family? They were the people who claimed you. In good, in bad, in\nparts or in whole, they were the ones who showed up, who stayed in there,\nregardless. It wasn t just about blood relations or shared chromosomes, but\nsomething wider, bigger. Cora was right—we had many families over time. Our\nfamily of origin, the family we created, as well as the groups you moved\nthrough while all of this was happening: friends, lovers, sometimes even\nstrangers. None of them were perfect, and we couldn t expect them to be. You\ncouldn t make any one person your world. The trick was to take what each could\ngive you and build a world from it.”\n\n\n“Needing was so easy: it came naturally, like breathing. Being needed by\nsomeone else, though, that was the hard part. But as with giving help and\naccepting it, we had to do both to be made complete—like links overlapping to\nform a chain, or a lock finding the right key.”\n\n\n“It’s a lot easier to be lost than found. It’s the reason we re always\nsearching, and rarely discovered.”\n\n\n“Isn t every relationship based off selfishness?”\n\n\n“Shoulda, coulda, woulda. It’s so easy in the past tense.”\n\n\n“I was beginning to see, though, that the unknown wasn t always the greatest\nthing to fear. The people who know you best can be riskier, because the words\nthey say and things they think have the potential to be not only scary but\ntrue, as well.”\n\n\n“In the depth of winter, I finally learned that within me there lay an\ninvincible summer.”\n\n\n“My life will not follow a fairy tale, and that’s okay. My life is reality. And\nin my reality people don t fall in love and get married and live happily ever\nafter, because life is complicated. And messy. ”\n\n\n“Sometimes he wondered if man’s instincts had changed in that time and always\nconcluded that they hadn t. At least in the basic, most primal ways. As far as\nhe could tell, man had always been aggressive, always striving to dominate,\ntrying to control the world and everything in it.” “Poetry, she thought, wasn t\nwritten to be analysed: it was meant to inspire without reason, to touch\nwithout understanding.”\n\n\n“You are the answer to every prayer I ve offered. You are a song, a dream, a\nwhisper, and I don t know how I could have lived without you for as long as I\nhave.”\n\n\n“We wrap up our violent and mysterious world in a pretence of understanding. We\npaper over the voids in our comprehension with science or religion, and make\nbelieve that order has been imposed. And, for the most of it, the fiction\nworks. We skim across surfaces, heedless of the depths below. Dragonflies\nflitting over a lake, miles deep, pursuing erratic paths to pointless ends.\nUntil that moment when something from the cold unknown reaches up to take us.\nThe biggest lies we save for ourselves. We play a game in which we are gods, in\nwhich we make choices, and the current follows in our wake. We pretend a\nseparation from the wild. Pretend that a man’s control runs deep, that\ncivilization is more than a veneer, that reason will be our companion in dark\nplaces.”\n\n\n“The bladder-pipe, a local Highlands speciality, is to music what warthogs are\nto mathematics. Largely unconnected.”\n\n\n“God is not some omnipotent authority looking down from above, threatening to\nthrow us into a pit of fire if we disobey. God is the energy that flows through\nthe synapses of our nervous system and the chambers of our hearts! God is in\nall things!”\n\n\n“Your afterlife is what you expect it to be, what the thousands, the millions\naround you expect, what legend builds, told, retold, refined, evolving. In this\nplace, amongst the sands, they fashion themselves a different paradise and\ndifferent paths to it, some dark, some light. All of it is fabrication,\nconstructed over the reality my people lived in. Whatever waited for a man\nafter his death in those times, it was not mentioned in our calculations. Our\npriests, when they could find anyone to listen, described something more\nsubtle, more profound, and more wonderful.” “Ignorance is indeed bliss.”\n\n\n“No one is ever kind or generous without expecting something in return. I know\nyou will use me eventually like everyone else.”\n\n\n“So don t be too sad. Being incomplete means that you have the potential to\ngrow into an infinite number of things.”\n\n\n“Everyone gets a wound that never heals and no one should bother. No matter how\nsound and secure that person seems… There’s no way of telling what will happen\nif you cross the line.”\n\n\n“When people fall into despair even getting through their daily lives can be\nlike torture. And since it involves a whole mix of confusing emotions, each\nperson’s reaction to different situations can be diverse.”\n\n\n“A precious thing is like something that’s a part of you. Like your heart.\nFinding a precious thing or a person is a very happy thing. But it can also be\nscary. Everyone has got atleast one precious thing. And there may be a\ndifference between having something precious and being precious to someone but\nthey are not separate things.”\n\n\n“Is the horizon far away? Not at all! Man is at the horizon, how can the\nhorizon be far away? What colour is the bright moon? It is blue; and like the\nocean, blue, deep and sorrowful. Where is the bright moon? It is in his heart;\nhis heart is the bright moon. What about his sabre? His sabre is in his hand!\nWhat kind of blade is that? His sabre is as broad and as lonely as the horizon,\nas pure and sorrowful as the bright moon; even with a flash of steel, some\ntimes it is as if it is empty. Empty? Empty and illusional, as if it never\nexists, yet present everywhere. But the speed of his sabre does not appear to\nbe very swift. How can a sabre that is not swift be invincible under the\nheavens? This is because his sabre has gone beyond the limits of speed! Where\nis he? He has not returned, but his heart is already broken. Where is the path\nof his return? The path is right in front of him. Cannot he see the path? He is\nnot looking for it. So he cannot find the path? Perhaps not now, but he will\nfind it sooner or later. Willl he find it for sure? Definitely!”\n\n\n“It matters not how strait the gate, How charged with punishments the scroll.I\nam the master of my fate. I am the captain of my soul.”\n\n\n“Love was selfish, wasn t it? It made honest men want things they had no right\nto. It cocooned one from the rest of the world, erased time itself, knocked\naway reason. It made you live in defiance of the inevitable. It made you want\nanother’s mind, body; it made you feel as if you deserved to own their heart,\nand carve out a place in it.”\n\n\n“I didn t smile because I made friends. I made friends because I smiled at\nthem.”\n\n\n“Each moment is fragile and fleeting. The moment of the past cannot be kept,\nhowever beautiful. The moment of the present cannot be held, however enjoyable.\nThe moment of the future cannot be caught, however desirable. But the mind is\ndesperate to fix the river in place: Possessed by ideas of the past,\npreoccupied with images of the future, it overlooks the plain truth of the\nmoment.”\n\n\n“Life is a series of natural and spontaneous changes. Don t resist them; that\nonly creates sorrow. Let reality be reality. Let things flow naturally forward\nin whatever way they like.”\n\n\n“A man with outward courage dares to die; a man with inner courage dares to\nlive.”\n\n\n“If you are depressed you are living in the past. If you are anxious you are\nliving in the future. If you are at peace you are living in the present.”\n\n\n“Men are born soft and supple; dead they are stiff and hard. Plants are born\ntender and pliant; dead, they are brittle and dry. Thus whoever is stiff and\ninflexible is a disciple of death. Whoever is soft and yielding is a disciple\nof life. The hard and stiff will be broken. The soft and supple will prevail.”\n\n\n“Do you imagine the universe is agitated? Go into the desert at night and look\nat the stars. This practice should answer the question.”\n\n\n“Do you have the patience to wait Till your mud settles and the water is clear?\nCan you remain unmoving Till the right action arises by itself?” “He who stands\non tiptoe doesn’t stand firm. He who rushes ahead doesn’t go far. He who tries\nto shine dims his own light. He who defines himself can t know who he really\nis. He who has power over others can t empower himself. He who clings to his\nwork will create nothing that endures.”\n\n\n“If you want to accord with the Tao, just do your job, then let go.”\n\n\n“Empty your mind of all thoughts. Let your heart be at peace. Watch the turmoil\nof beings, but contemplate their return. Each separate being in the universe\nreturns to the common source. Returning to the source is serenity. If you don t\nrealize the source, you stumble in confusion and sorrow. When you realize where\nyou come from, you naturally become tolerant, disinterested, amused,\nkindhearted as a grandmother, dignified as a king. Immersed in the wonder of\nthe Tao, you can deal with whatever life brings you, and when death comes, you\nare ready.”\n\n\n“He who knows, does not speak. He who speaks, does not know.”\n\n\n“There is no illusion greater than fear.”\n\n\n“No matter how bad a state of mind you get into, if you hold out over the long\nrun, the clouds will disappear and the autumn winds will cease. That is a\nfact.”\n\n\n“Life isn t just doing things for yourself. It’s possible to live in such a way\nthat other people’s happiness, makes you happy too.”\n\n\n“Everyone smiles, when they are with you. Please… from now on, go and help\npeople in my place. Share your happiness with them.”\n\n\n“Everybody can fight. It’s just a choice of whether you should.”\n\n\n“Death comes suddenly and life is fragile and brief. No one can alter this,\neither by prayers or spells. Children cry about it, but men and women do not\ncry. They have to endure.”\n\n\n“The wind stirred the ancient cedars; the night insects kept up their insistent\nmusic. It would always be like this, I thought, summer after summer, winter\nafter winter, the moon sinking towards the west, giving the night back to the\nstars, and they, in an hour or two, surrendering it to the brightness of the\nsun. The sun would pass above the mountains, pulling the shadows of the cedars\nafter it, until it descended again below the rim of the hills. So the world\nwent, and humankind lived on it as best they could, between the darkness and\nthe light.”\n\n\n“I kept thinking that I couldn t live my life for other people, that love was\nnothing but chains. And maybe it was, but so help me, I needed these chains.\nThese things didn t make me weaker; they held my soul to the earth. I wasn t\ngoing to run from them anymore.”\n\n\n“Mistakes mean it’s real.”\n\n\n“There are some things you don t learn about yourself until you let someone\nelse into the most intimate places of your heart.”\n\n\n“If you plot your life along the real line, it will encompass a finite space.\nWe begin, move steadily in a positive direction, and end. The bodies we re\ngiven aren t meant to last forever.”\n\n\n“A person’s natural talent does not decide their achievements in life. Anyone\ncan become formidable, but the crucial point is whether or not you work hard,\nand how intelligent your mind is.”\n\n\n“There are some things in life that cannot be explained with logic. They cannot\nbe understood through dissection. They are what they are—good, bad, or epically\ncrappy. Sometimes they are all those things at once”\n\n\n“The moon was like water. As the moonlight shined, it would sprinkle a layer of\nveil over the night.”\n\n\n“This world will never pity you if you re being cowardly, you have to fight for\neverything yourself. If you don t fight for it, even if it belongs to you, it\nwill be snatched away.”\n\n\n“Time doesn’t slow during a fight. What you perceive as a slowdown is actually\nyour poor noggin overloading with data. It’s working extra hard to create a\ndetailed record of events—the mental equivalent of running a highlighter\nthrough a book—and you re misinterpreting the added detail for added time.”\n\n\n“Are you saying that the here-and-now is derived from one of many possible\npasts, that the present is only possible because certain criteria were met, and\nso the future, our particular future, will form based on the same principles?\nIn other words, our future will be one possible future out of many many\nthousands”\n\n\n“We(vampires) take what we need, and you call us monsters. You starve your own\nkind, and you call yourselves shareholders.”\n\n\n“Starlight is amazing. Most of the stars that shed those rays are already dead.\nThey sent those lonely shimmers off eons ago, bright messages that streamed out\ntirelessly into the night. The lonely travelers had swept past any number of\nfantastic sights. Planets birthing. Planets dying. Novas. Pulsars. Black holes\nand white dwarfs. Meteors and comets. Neutron stars spinning. Dark matter\nlurking. Crazy names for crazier things. I think that’s why starlight is\namazing. Those tiny flecks of light travelled through hell and high water to\nreach us.” “When a man tries to harm someone else you have the responsibility\nto do everything you can to stop them, even if that means dispensing justice\nyourself. I know it’s hard to think that way. You have to move past it, it’s\nthe only way.”\n\n\n“He who is certain he knows the ending of things when he is only beginning them\nis either extremely wise or extremely foolish; no matter which is true, he is\ncertainly an unhappy man, for he has put a knife in the heart of wonder. ”\n\n\n“Weep no more! All men must die—you, I, everyone. If we are not killed by\nyouthful stupidity or ill-luck, then it is our fate to live on like the trees:\nolder and older until at last we totter and fall. It is the way of all things.\nHow can you fight the Lord’s will?”\n\n\n“While man plays out his show, the Manipulator remains unseen; we know Him not\nby sight, but by the ways His puppets move. And occasionally the curtain stirs,\nthat hides Him from His faithful audience. Ah, but we are grateful even for\njust that movement behind the curtain—grateful!”\n\n\n“A man whose wisdom is true does not sit in waiting for the world to come at\nhim piece by piece for proving its existence!”\n\n\n“The icy mountain face itself was a thing of painful beauty. Simon had never\ndreamed that ice might have color; the tame variety he knew, that which\nfestooned the roofs of the Hayholt at Aedontide and shrouded the wells in\nJonever, was diamond-clear or milky white. By contrast, the icy armor of\nUrmsheim, warped, twisted, and buckled by wind and the seemingly distant sun,\nwas a dream-forest of colors and strange shapes. Great ice-towers shot through\nwith veins of sea-green and violet leaned out above the heads of the toiling\nparty. ”\n\n\n“Never make your home in a place,” the old man had said, too lazy in the spring\nwarmth to do more than a wag a finger. “Make a home for yourself inside your\nown head. You ll find what you need to furnish it—memory, friends you can\ntrust, love of learning, and other such things.” Morgenes had grinned. “That\nway it will go with you wherever you journey. You ll never lack for a\nhome—unless you lose your head, of course …”\n\n\n“When you stopped to think about it, he reflected, there weren t many things in\nlife one truly needed. To want too much was worse than greed: it was\nstupidity—a waste of precious time and effort.”\n\n\n“But no one ever explained how terrible it is to be in the middle of a tale and\nnot to know the ending…”\n\n\n“He was not great; he was, in fact, very small. At the same moment, though, he\nwas important, just as any point of light in a dark sky might be the star that\nled a mariner to safety, or the star watched by a lonely child during a\nsleepless night…”\n\n\n“Also,” Binabik said gravely, “it may or may not be foolishness to pray to the\ngods, but there is certainly being no wisdom in cursing them.”\n\n\n“We are very small,” Simon said between swallows. The kangkang seemed to be\nflowing in his veins like blood. “So are the stars, kundë -mannë,” Sludig\nmurmured. “But they each one burn as bright as they can.”\n\n\n“It was nice that someone cared about him, he supposed, even if he did not\nentirely agree with the form that caring took.”\n\n\n“It is only worrying about things we cannot be changing; that is foolishness.”\nHe mustered a smile. ” When your teeth are gone, we Qanuc say, learn to like\nmush. ”\n\n\n“Some things, once destroyed, will always be destroyed. No matter how much it’s\nrepaired, there will always be an obvious crack.”\n\n\n“In this world, there is no such thing as a free lunch”\n\n\n“You ought to know that women will forever be the creatures that hold the\nlongest grudges. Otherwise, why would there be the saying that a woman’s heart\nis a most vicious thing ?”\n\n\n“You are now just a little rascal, but you are still far off from the true\nmastery of being a rogue. The highest state of a rogue is that even when you\nare being a rogue, others will think of you as a gentleman. When you are facing\nanother rogue, you must be more roguish than him, but when facing a gentleman,\nyou can be more suave and gentlemanly. Only then can you reach the ultimate\nstage and be a true Rogue, a genuine scoundrel.”\n\n\n“When a woman hates a man, no matter what he does for her, how much effort or\nhow many things he does, that hatred will not diminish a whit. However, the\nreverse is also true, when a woman has acknowledged a man, even if he does not\ndo anything, she will still think the better of him.”\n\n\n“Is any of it real? I mean, look at this. Look at it! A world built on fantasy.\nSynthetic emotions in the form of pills. Psychological warfare in the form of\nadvertising. Mind-altering chemicals in the form of… food! Brainwashing\nseminars in the form of media. Controlled isolated bubbles in the form of\nsocial networks. Real? You want to talk about reality? We haven t lived in\nanything remotely close to it since the turn of the century. We turned it off,\ntook out the batteries, snacked on a bag of GMOs while we tossed the remnants\nin the ever-expanding Dumpster of the human condition. We live in branded\nhouses trademarked by corporations built on bipolar numbers jumping up and down\non digital displays, hypnotizing us into the biggest slumber mankind has ever\nseen. You have to dig pretty deep, kiddo, before you can find anything real. We\nlive in a kingdom of bullshit. A kingdom you ve lived in for far too long. So\ndon t tell me about not being real. I m no less real than the fucking beef\npatty in your Big Mac.”\n\n\n“Merit, anyone living in the world should have merit. At least, they must be of\nsome merit in the eyes of their parents, otherwise their life will be a waste.”\n\n\n“What are the most fearsome enemies? They are not the crazed, bloodthirsty\npeople, but the cool-headed people with nothing to lose whose only goal is to\nseek revenge. This is the most fearsome kind of enemy.”\n\n\n“There was a moment, a point in your recent past, a mistake, a compulsion, a\ndecision, something that led you to this point right now. My only advice to you\nis to find that moment, understand it; it’s the only way to reconcile this\nfailure with yourself.”\n\n\n“Is it that we collectively thought Steve Jobs was a great man, even when we\nknew he made billions off the backs of children? Or maybe it’s that it feels\nlike all our heroes are counterfeit. The world itself’s just one big hoax.\nSpamming each other with our running commentary of bullshit masquerading as\ninsight, our social media faking as intimacy. Or is it that we voted for this?\nNot with our rigged elections, but with our things, our property, our money. I\nm not saying anything new, we all know why we do this, not because Hunger Games\nbooks makes us happy but because we wanna be sedated. Because it’s painful not\nto pretend, because we re cowards. Fuck society.”\n\n\n“How do we know if we re in control? That we re not just making the best of\nwhat comes at us, and that’s it. Trying to constantly pick between two options.\nLike your two paintings in the waiting room. Or Coke and Pepsi. McDonald’s or\nBurger King? Hyundai or Honda? It’s all part of the same blur, right? Just out\nof focus enough. It’s the illusion of choice. Half of us can t even pick our\nown our cable, gas, electric. The water we drink, our health insurance. Even if\nwe did, would it matter? You know, if our only option is Blue Cross or Blue\nShield, what the fuck is the difference? In fact, aren t they the same? No,\nman, our choices are prepaid for us, long time ago.”\n\n\n“We re all living in each other’s paranoia.”\n\n\n“True courage is about being honest with yourself. Especially when it’s\ndifficult.”\n\n\n“Daemons. They don t stop working. They re always active. They seduce. They\nmanipulate. They own us. And even though you re with me, even though I created\nyou, it makes no difference. We all must deal with them alone. The best we can\nhope for, the only silver lining in all of this is that when we break through,\nwe find a few familiar faces waiting on the other side.”\n\n\n“Power belongs to those who take it. ”\n\n\n“The concept of waiting bewilders me. There are always deadlines. There are\nalways ticking clocks.”\n\n\n“People are all just people, right? When it gets down to it, everyone’s the\nsame. They love something. They want something. They fear something. Specifics\nhelp, but specifics don t change the way that everyone is vulnerable. It just\nchanges the way that we access those vulnerabilities.”\n\n\n“People always make the best exploits. I ve never found it hard to hack most\npeople. If you listen to them, watch them, their vulnerabilities are like a\nneon sign screwed into their heads.”\n\n\n“There’s a saying — The devil is at his strongest while we re looking the\nother way. Like a program running in the background silently. While we re busy\ndoing other shit. Daemons, they call them. They perform action without user\ninteraction. Monitoring, logging, notifications, primal urges, repressed\nmemories, unconscious habits. They re always there, always active. You can try\nto be right, you can try to be good, you can try to make a difference. But it’s\nall bullshit. Cause intentions are irrelevant. They don t drive us, daemons do.\nAnd me? I ve got more than most.”\n\n\n“Give a man a gun and he can rob a bank. Give a man a bank and he can rob the\nworld.” “And death shall have no dominion. Dead men naked they shall be one\nWith the man in the wind and the west moon; When their bones are picked clean\nand the clean bones gone They shall have stars at elbow and foot; Though they\ngo mad they shall be sane, Though they sink through the sea they shall rise\nagain; Though lovers be lost love shall not; And death shall have no dominion”\n\n\n“By God, sleep was a treacherous enemy. You couldn t face it and fight it; it\nwaited until you were looking the other way, then stole up quietly.”\n\n\n“Although one cannot easily trust others, one also cannot be over-cautious. The\nway you are currently, how will you be able to interact with people in the\nfuture? Remember, you can t be too cold and callous towards others, even if you\ncan t be overly trusting either. Trust is something which is built up through a\nlong period of time. Do not easily trust the words of others.”\n\n\n“If you didn t strive hard, you would be discarded.” “The wind was invisible.\nWhen it was gentle, it could be like the kiss of a lover. But when it was\naroused into a vicious storm, it could split mountains and shatter stones.”\n\n\n“Sometimes, a single mistake caused by being unbending could cause someone to\nsuffer a lifetime of regret. The occasional compromise that allows one’s loved\nones to be safe also allowed one to pursue revenge with even greater ferocity!”\n\n\n“An interesting, colorful life with ups and downs. Only that is meaningful.”\n\n\n“If you liked to do something, then once you became absorbed in it, your\neffectiveness would be extremely high. If, on the other hand, you didn t like\nto do something and instead forced yourself to do it, the effectiveness would\nbe very low.”\n\n\n“The gods weave as the gods will, Rain. And even though it may not be apparent\nat first, they do weave purpose into all things. Even terrible things.”\n\n\n“My beloved is the sun And I am the earth that thrives only in her warmth. My\nbeloved is the rain And I am the grass that thirsts for her quenching kiss. My\nbeloved is the wind And I am the wings that soar when she fills me with her\ngentle strength. My beloved is the rock Upon which rests the happiness of all\nmy days.”\n\n\n“With wings unfurled and joy unbound, I dance on laughter-spangled winds. I\nbathe in freedom’s rushing breath And drink cool nectar from the clouds. Up,\nup, through sunlit fields of blue, I soar through boundless ether. Look!\nStarlight shines at height of day. I hear infinity calling.”\n\n\n“It’s been too long since I ve been a mate. I had forgotten the two rules. The\ntwo rules?” she echoed Aiyah. Sariel taught me.” He held up his index finger.\n\n\n“Rule one: in any dispute between mates, the male is always to blame, even when\nhe is clearly blameless. Rule two”—his middle finger joined the first—“whenever\nin doubt, refer to rule one.”\n\n\n“It never failed to astonish him, then or ever, how much of the world around\nhim was mysterious and hidden from view.”\n\n\n“The problem with growing up,” Quentin said, “is that once you re grown up,\npeople who aren t grown up aren t fun anymore.”\n\n\n“As soon as he seized happiness it dispersed and reappeared somewhere else.\nLike Fillory, like everything good, it never lasted. What a terrible thing to\nknow.I got my heart’s desire, he thought, and there my troubles began.”\n\n\n“If you will, for just one second, look at your life and see how perfect it is.\nStop looking for the next secret door that is going to lead you to your real\nlife. Stop waiting. This is it: there’s nothing else. It’s here, and you d\nbetter decide to enjoy it or you re going to be miserable wherever you go, for\nthe rest of your life, forever.”You can t just decide to be happy.”No, you can\nt. But you can sure as hell decide to be miserable. Is that what you want?”\n\n\n“He was painfully uncomfortable making eye contact.Quentin couldn t think who\nBenedict reminded him of until he realized that this was what he had probably\nlooked like to other people when he was sixteen. Fear of everybody and\neverything, hidden behind a mask of contempt, with the greatest contempt of all\nreserved for himself.”\n\n\n“Though the funny thing about never being asked for anything is that after a\nwhile you start to feel like maybe you don t have anything worth giving.”\n\n\n“This is like a demon in your heart. It is a burden you cannot let go. This\nillusion grabbed exactly onto your weakness, so that is why it created their\nappearances. You must persevere and believe in yourself. Believe that you are\ncorrect. They are illusions, and everything is fake. As long as you can\npreserve, you can dispel that demon. You can clear your conscience and carry no\nmore burdens.”\n\n\n“The human heart is difficult to predict, and one’s true feelings are shown\nwhen disaster strikes. But the truth always made one’s heart ache. The people\nwho always followed behind you, who bootlicked you, and who swore loyalty, left\nwithout a single care of righteousness when you met a calamity. Who had\nexperienced such a feeling?!”\n\n\n“Also, I need to remind you this. To people, the most important thing is\ndignity. If you live life like you just did without any dignity, you will\nnever, ever, have anyone truly look at you with good eyes or good impressions.\nFor forever, you will only be called here and there like a dog by others. When\nneeded, they would use you. When unneeded, they would kill you at any time.”\n\n\n“To love a person, there is no need to change oneself. To love a person, there\nis no need to make things hard for oneself. To love a person, even more so, one\nshould not be wronged. Because, in true love, there should be more sweet than\nsorrow; greater happiness than pain. For true love, even if it is within a\ndeep, endless ocean of pain, as long as my heart doesn’t change, I will still\nbe happy within. To love a person, one should ignore their own safety for him.\nTo love a person, one should give everything possible for him. To love a\nperson, one should have an eternally unchanging heart. No matter what kinds of\ndifficulties or dangers there are forward, one does not become timid. One does\nnot be shaken. Even if that person becomes the enemy of the world, then I will\nalso be willing to be by his side and fight against the world together with\nhim.”\n\n\n“Life could not repeat itself like the rising and setting of the Sun. If one\ndid not grab every moment, one would waste their talent and potential. It would\nbe a great regret”\n\n\n“Return kindness with kindness and return animosity with animosity. If there\nwas a trickle of kindness, return it with a gushing spring of kindness. If the\nhatred was carved deep within the bones, then return the favor a hundred times\nback.”\n\n\n“No matter how strong you are or how high your status is, you must never throw\naway your conscious. A man’s conscious is extremely important. One must be kind\nand benevolent in order to attain the highest level of mastery. Take heed to\nthis! Even if you are a genius without equal and have only success after\nsuccess, in the end, your achievements will amount to nothing.”\n\n\n“Be like the forces of nature: when it blows, there is only wind; when it\nrains, there is only rain; when the clouds pass, the sun shines through.”\n\n\n“But comfort can be a cage, you know. Certainly it can stunt the mind. ”\n\n\n“It’s easy to die, Mageling,” Taliesin said, stroking his hair. “It’s staying\nalive that’s hard work”\n\n\n“Sometimes, a person’s heart is much more vicious than that of a tiger. At\nleast when a tiger wants to eat you, he ll let you know first.”\n\n\n“Joy is feather light But who can carry it? Sorrow falls like a landslide Who\ncan parry it?”\n\n\n“A poor man must swing for stealing a belt buckle but if a rich man steals a\nwhole state he is acclaimed as statesman of the year.”\n\n\n“In the age when life on earth was full, no one paid any special attention to\nworthy men, nor did they single out the man of ability. Rulers were simply the\nhighest branches on the tree, and the people were like deer in the woods. They\nwere honest and righteous without realizing that they were “doing their duty.”\nThey loved each other and did not know that this was “love of neighbor.” They\ndeceived no one yet they did not know that they were “men to be trusted.” They\nwere reliable and did not know that this was “good faith.” They lived freely\ntogether giving and taking, and did not know that they were generous. For this\nreason their deeds have not been narrated. They made no history.”\n\n\n“But books contain words only. And yet there is something else which gives\nvalue to the books. Not the words only, nor the thought in the words, but\nsomething else within the thought, swinging it in a certain direction that\nwords cannot apprehend. But it is the words themselves that the world values\nwhen it commits them to books: and though the world values them, these words\nare worthless as long as that which gives them value is not held in honor.”\n\n\n“Of all the beings that exist (and there are millions), man is only one. Among\nall the millions of men that live on earth, the civilized people that live by\nfarming are only a small proportion. Smaller still the number of those who\nhaving office or fortune, travel by carriage or by boat. And of all these, one\nman in his carriage is nothing more than the tip of a hair on a horse’s flank.\nWhy, then, all the fuss about great men and great offices? Why all the\ndisputations of scholars? Why all the wrangling of politicians?”\n\n\n“I don t know what the answer is,” he murmured. “My Cultivation base won t\npermit me to understand what the Dao is…. “To me, the Dao is very simple. It is\ntalking, speaking, opening your mouth, and letting other people open their\nmouths. All of that is the Dao, speaking. Speaking the words from your heart,\nspeaking out the thoughts you wish to express. “It doesn’t require\nenlightenment, nor obsession. It doesn’t require a path beneath your feet.\nPerhaps it is the first voice of all living creatures, of everything under the\nHeavens. “When that voice can be heard, it is the Dao, it is speaking!” Meng\nHao had organized his thoughts and spoken out what he understood about the Dao,\nbased upon his current realm. He didn t know if what he had said was true or\ncorrect. In fact, he hadn t wanted to speak at all, but he had no choice but to\nignore those feelings. All he could do was explain what he understood about the\nDao. By this time, the incense stick had burned down to the end. It flickered,\non the verge of being completely extinguished. “At the same time,” he\ncontinued, “when that voice speaks, it represents a direction! “The boundless\nHeavens and Earth are the final resting place of all living things. Life is\nlike a journey, filled with various scenery, various paths. “Sometimes, you\nmight think there is only one path for you. Sometimes, your heart’s obsession\ncreates a path. “As for the Dao, it is a direction. That direction can guide\nyou through your life. When you are faced with countless decisions, it can lead\nyou down the paths you must tread. In the end… it can help you pick which path\nto take! “It is formed after one experiences the vicissitudes of life, the\ncleansing of time, and the understanding which comes from experiencing the\nworld. It can be hidden in any time, place, direction, or action…. “That is my\nunderstanding of the Dao. It points in a direction, and gives me the strength\nto proceed onward. Perhaps it doesn’t even exist, or perhaps it is everywhere.\n\n\n“As for me, I am still searching for it….”\n\n\n“In the former era mankind had produced a massive cruise ship. They christened\nit the Titanic.” “The name Titanic had been borrowed from Greek mythology,\nreferring to the giants called Titans. The Titans wished to wage war against\nthe god Zeus on behalf of the mysterious forces of nature. They were ultimately\ndefeated, and banished to the depths of the Atlantic ocean, buried deeper than\nthe eighteenth level of hell itself. Thus it was people said the name Titanic\nwas poorly chosen, ominous, and would invite catastrophe. And as predicted, the\nship sank to the bottom of the sea in an accident. At the mention of Zeus, Zhou\nQianlin inadvertently raised her head. Her eyes found Lan Jue’s looking\ndirectly back at her. Lan Jue continued. “But the difference between this great\nship and the titans of lore was that the only thing that sunk was it’s steel..\nit’s bolts… it’s people. It’s spirit was never conquered. That is to say that\nthe titanic sunk, taking with it the lives of one thousand five hundred\npassengers. But the invincible spirit of human civilization remained.\nUnsinkable.” Lan Jue’s voice grew louder as he pressed on. As the boat sank\neight musicians calmly stood upon the deck, playing their instruments. Those\nnotes embodied the dignity and honor of the human spirit, refusing to bow it’s\nhead to the ruthless acts of nature. Just as the famous writer Hemmingway wrote\nin his book The Old Man and the Sea: A man is not made for defeat. A man can be\ndestroyed but not defeated. The sharks following the old man could gnaw on the\nfish lashed to his boat until there was nothing but bone, but they couldn t\ngnaw the sailor’s undaunted spirit. This was the burning fire of the inner\nspirit, the will of man, that not even the entire ocean could extinguish. Even\nmany years later people still laud the actions of those musicians and sailors.\nHow could they have so much courage when they faced drowning in the brine? How\nis it they could adhere to their duties when death lay in those tumultuous\nwaters? How is it they could retain the noble sentiment to wait until all the\nwomen and children had filled the lifeboats before thinking of themselves?\nStatistics state that seventy-six percent of the sailors died in the accident,\na ratio that outstripped the first, second and third class passenger deaths\ncombined. The sailors even had evacuation preference over the passengers – but\nthey gave their opportunity to others. They took on that hopelessness for\nthemselves. Nor was it one, or two sailors who did this. All nine hundred\nstaff, including sailors, waiters, firemen and even the cook all chose to stay\nbehind; so many people, willing to do what they did. As we think on it today,\nthis sort of towering spirit of humanity is not unlike what they said about the\nsinking of that great ship. It is almost unbelievable.”\n\n\n“Sometimes, people don t understand the truth about their own feelings. They\nsuddenly get hurt, and then feel that the only choice they have is to give up\nin hopelessness. What they don t realize is that what truly can hurt them the\nmost is their own precious love. And because love is so precious, one should\nnever give up, one should never run away. By the time most people understand\nthis truth, they have already lost their precious love.”\n\n\n“A popular man arouses the jealousy of the powerful”\n\n\n“I must not fear. Fear is the mind-killer. Fear is the little-death that brings\ntotal obliteration. I will face my fear. I will permit it to pass over me and\nthrough me. And when it has gone past I will turn the inner eye to see its\npath. Where the fear has gone there will be nothing. Only I will remain.”\n\n\n“In the journey of mastery, the self is the most important. Outside aid can\ncertainly make smooth the path to mastery, however, how far one ventures along\nthis path is still ultimately in their own hands.”\n\n\n“Sometimes, it’s best not to go all-out. A truly impressive individual knows\nwhen to endure and when to withdraw. Even if others mock him, he will ignore\nthem. That is true courage and charisma…because, hope springs eternal as long\nas one is alive.”\n\n\n“A process cannot be understood by stopping it. Understanding must move with\nthe flow of the process, must join it and flow with it.”\n\n\n“If wishes were fishes we d all cast nets”\n\n\n“The mind commands the body and it obeys. The mind orders itself and meets\nresistance.”\n\n\n“Grave this on your memory, lad: A world is supported by four things…” she\nheld up four big-knuckled fingers. “…the learning of the wise, the justice of\nthe great, the prayers of the righteous and the valor of the brave. But all of\nthese things are as nothing…” She closed her fingers into a fist. “…without\na ruler who knows the art of ruling. Make that the science of your tradition!”\n\n\n“Proper teaching is recognized with ease. You can know it without fail because\nit awakens within you that sensation which tells you this is something you have\nalways known.”\n\n\n“Anything outside yourself, this you can see and apply your logic to it. But\nit’s a human trait that when we encounter personal problems, these things most\ndeeply personal are the most difficult to bring out for our logic to scan. We\ntend to flounder around, blaming everything but the actual, deep-seated thing\nthat’s really chewing on us.”\n\n\n“There is in all things a pattern that is part of our universe. It has\nsymmetry, elegance, and grace - these qualities you find always in that the\ntrue artist captures. You can find it in the turning of the seasons, the way\nsand trails along a ridge, in the branch clusters of the creosote bush of the\npattern of its leaves. We try to copy these patterns in our lives and in our\nsociety, seeking the rhythms, the dances, the forms that comfort. Yet, it is\npossible to see peril in the finding of ultimate perfection. It is clear that\nthe ultimate pattern contains its own fixity. In such perfection, all things\nmove towards death.”\n\n\n“The vision of time is broad, but when you pass through it, time becomes a\nnarrow door.”\n\n\n“Do you wrestle with dreams? Do you contend with shadows? Do you move in a kind\nof sleep? Time has slipped away. Your life is stolen. You tarried with trifles,\nVictim of your folly.”\n\n\n“There is in each of us an ancient force that takes and an ancient force that\ngives. A man finds little difficulty facing that place within himself where the\ntaking force dwells, but it’s almost impossible for him to see into the giving\nforce without changing into something other than man. For a woman, the\nsituation is reversed…These things are so ancient within us…that they re ground\ninto each separate cell of our bodies…It’s as easy to be overwhelmed by giving\nas by taking.”\n\n\n“Please permit the room to convey a lesson we learned from the same teachers:\nthe proximity of a desirable thing tempts one to overindulgence. On that path\nlies danger.”\n\n\n“But power deluded those who used it. One tended to believe power could\novercome any barrier … including one’s own ignorance.”\n\n\n“Deep in the human unconscious is a pervasive need for a logical universe that\nmakes sense. But the real universe is always one step beyond logic.”\n\n\n“To suspect your own mortality is to know the beginning of terror; to learn\nirrefutably that you are mortal is to know the end of terror.”\n\n\n“You understand? One uses power by grasping it lightly. To grasp too strongly\nis to be taken over by power, and thus to become its victim.”\n\n\n“Do you know what guerrillas often say? They claim that their rebellions are\ninvulnerable to economic warfare because they have no economy, that they are\nparasitic on those they would overthrow. The fools merely fail to assess the\ncoin in which they must inevitably pay. The pattern is inexorable in its\ndegenerative failures. You see it repeated in the systems of slavery, of\nwelfare states, of caste-ridden religions, of socializing bureaucracies-in any\nsystem which creates and maintains dependencies. Too long a parasite and you\ncannot exist without a host.”\n\n\n“Most believe that a satisfactory future requires a return to an idealized\npast, a past which never in fact existed.”\n\n\n“Some people never observe anything. Life just happens to them. They get by on\nlittle more than a kind of dumb persistence, and they resist with anger and\nresentment anything that might lift them out of that false serenity.”\n\n\n“Memory never recaptures reality. Memory reconstructs. All reconstructions\nchange the original, becoming external frames of reference that inevitably fall\nshort”\n\n\n“Confine yourself to observing and you always miss the point of your own life.\nThe object can be stated this way: Live the best life you can. Life is a game\nwhose rules you learn if you leap into it and play it to the hilt. Otherwise,\nyou are caught off balance, continually surprised by the shifting play.\nNon-players often whine and complain that luck always passes them by. They\nrefuse to see that they can create some of their own luck.”\n\n\n“Success, that was the danger. It had cost them an empire. If you waved your\nsuccess around like a banner someone always wanted to cut you down. Envy!”\n\n\n“The difference between sentiment and sentimentality is easy to see. When you\navoid killing somebody’s pet on the glaze way, that’s sentiment. If you swerve\nto avoid the pet and that causes you to kill pedestrians, that is\nsentimentality.”\n\n\n“There is an entire forest full of the most incredible flowers, plants and\ntrees inside you, and you are ignoring all of it to nurture a single tree that\nthey planted inside your heart and abandoned. The people who left you this way\ndon t deserve to become your favourite stories to tell. You are a massive\nforest full of beautiful and vibrant stories and every single one of them\ndeserves you more than those that abandoned you to hell.”\n\n\n“The way you hate yourself sometimes, you seem to forget that there is still a\nchild somewhere inside you, and you re feeding that innocence within you poison\nwith those terrible words.  Protect that child by being gentler with yourself.\nProtect that child by being kinder to yourself.  Because no one else will\nprotect them other than you.”\n\n\n“1. Carry featherlike hope in your heart, always. Let its softness soothe you\nwhen poniards of fear and pain shoot through your heart.  2. Cleanse your soul\nwith kindness everyday. Even if the world around you encourages you to be\nunkind, remind yourself that your soul is your responsibility alone and\nkindness is the best path to being calm.  3. Hold other people’s broken hearts\nlike you would a little injured bird. Remember how much effort it must have\ntaken for them to piece it together after it was smashed on the floor and how\nmuch they must trust you to hand it over to you.  4. Teach people how to love\nyou gently by loving yourself. Tell them about your story like scars and soothe\nwith honey where it still stings.  5. Remember that water can adapt its form to\nany container it is put in. You are seventy percent water. And no matter what\nhappens, you can adapt to anything.”\n\n\n“Someone fell in love today. Someone was born today. Someone lived through\nsomething that could have killed them. Someone won back the love of their life.\nSomeone made their parents proud. Someone survived. Someone healed. Someone let\ngo. Seven billion people, and some of us have just had the best day of their\nlives. Today may have been the very worst day of yours. But take solace and\ncelebrate this simple fact. It wasn t your best day today, but it is on it’s\nway, because we all get lucky in turn.”\n\n\n“Have you ever felt hunted inside your own head?”\n\n\n“You are nothing static. You are a breathing reflection of everything the\nuniverse has to offer. A song sung into existence by so much more than\ninspiration. It took six million years of evolution to build you, to bring you\nto this moment – so much more than any artist could ever spare for even the\ngreatest of his masterpieces. You are a multitude of majestic feelings, every\nsingle one, once felt, never felt again in the same magnitude. You are the\nmillions of things that happen to you in your lifetime.”\n\n\n“Never forget to put salt in an omelette. If you do. Use sauce.”\n\n\n“You can t change who people are. Then what do you do? You love them.”\n\n\n“Loving can hurt sometimes. And when it gets hard, it’s the only thing that\nkeeps us alive.”\n\n\n“One of the most terrible moments in a boy’s life,” Paul said, “is when he\ndiscovers his father and mother are human beings who share a love that he can\nnever quite taste. It’s a loss, an awakening to the fact that the world is\nthere and here and we are in it alone. The moment carries its own truth; you\ncan t evade it.”\n\n\n“A wandering breeze, swaying restlessly. Swept up by flurries. Lost and led\nastray. Storms rage and roar, and threaten all that remains. But the breeze\ndrifts ever onward. Finding its own way.”\n\n\n“In the dangers of the age, in the passion of violence, quietly laying down\nyour life is utterly insignificant in the ever flowing passage of time. There\nis nothing stronger than the will to live.”\n\n\n“That, dear one, is the point of the demonstration. Reality isn t relevant.\nPerception is everything. If you think it is the enemy, you can destroy it,\nwhether true or not. The magic interprets only your perception. It won t allow\nyou to harm someone you think innocent, but it will destroy whoever you\nperceive to be the enemy, within limits. Only what you believe, and not the\ntruth of your thoughts, is the determining factor.”\n\n\n“There is no such thing as pure good or pure evil, least of all in people. In\nthe best of us there are thoughts or deeds that are wicked, and in the worst of\nus, at least some virtue. An adversary is not one who does loathsome acts for\ntheir own sake. He always has a reason that to him is justification. My cat\neats mice. Does that make him bad? I don t think so, and the cat doesn’t think\nso, but I would bet the mice have a different opinion. Every murderer thinks\nthe victim needed killing.”\n\n\n“The anger of teeth be force by contact. Violence by touch. Combat. The magic\nof the Sword of Truth be the magic of the anger of teeth. Ripping. Tearing. The\nanger of the tongue need not touch, but it be force just the same. It cuts just\nas quick”\n\n\n“Everything is valuable under the right conditions. To a man dying of thirst,\nwater be more precious than gold. To a drowning man, water be of little worth\nand great trouble.”\n\n\n“Wizard’s First Rule: people are stupid.” Richard and Kahlan frowned even more.\n\n\n“People are stupid; given proper motivation, almost anyone will believe almost\nanything. Because people are stupid, they will believe a lie because they want\nto believe it’s true, or because they are afraid it might be true. People’s\nheads are full of knowledge, facts, and beliefs, and most of it is false, yet\nthey think it all true. People are stupid; they can only rarely tell the\ndifference between a lie and the truth, and yet they are confident they can,\nand so are all the easier to fool.”\n\n\n“As long as the heart is there, it doesn’t matter if it is red or black. Your\nwill is the most important factor. Your will is like a blade, and that blade…\ncan still be used in your Third Severing. There is no need to be swept up with\nconfusion. Life is a series of decisions. Whether you make the correct\ndecisions or not doesn’t matter. The important thing is to keep going forward.\nYears later, when you look back, perhaps you’ll find that the incorrect\ndecisions you made… weren t really incorrect. Similarly, the correct decisions…\nmight not necessarily have been correct. Why struggle with frustration? Why\nproceed with confusion? In all things… resolution only comes from continuing to\nmove forward. Following this line of reasoning, if there is no such thing as\nincorrect, then how can the correct exist? Similarly, if there is no correct,\nthen how can the incorrect exist?”\n\n\n“As you well know, superstition needs no grounding in truth, but once rooted,\nit grows a strong though twisted tree.”\n\n\n“Nonsense. No one knows everything. You can t expect to walk through life\nwithout stepping in the muck now and again. The important thing is to maintain\nyour footing when you do, and not fall on your face and make it worse.”\n\n\n“The Second Rule is that the greatest harm can result from the best intentions.\nIt sounds a paradox, but kindness and good intentions can be an insidious path\nto destruction. Sometimes doing what seems right is wrong, and can cause harm.\nThe only counter to it is knowledge, wisdom, forethought, and understanding the\nFirst Rule. Even then, that is not always enough. Good intentions, or doing\nright, can cause harm? Such as? Nathan shrugged. “It would seem kind to give\ncandy to a small child, because they like it so. Knowledge, wisdom, and\nforethought tell us that it would make the child sick if we continued this\nkindness at the expense of good food. That’s obvious. Anyone would know that.\nSay a person hurts their leg, and you bring them food while they heal, but\nafter time they still don t wish to get up, because it hurts at first. So, you\ncontinue to be kind and bring them food. Over time, their legs will shrivel,\nand it will be even more painful to get up, so you are kind and continue\nbringing food. In the end, they will be bedridden, unable to ever walk again,\nbecause of your kindness.”\n\n\n“Damaged people understand that every evil demon that exists down there was\nonce a kind angel before it fell.”\n\n\n“I don t like the words I m fine. My mom tells me those two words are the\nmost-frequently-told lie in the English language.”\n\n\n“If you have no faith in yourself, then have faith in the things you call\ntruth. You know what must be done. You may not have courage or trust or\nunderstanding or the will to do it, but you know what must be done. You can t\nturn back. There is no answer behind you. You fear what you cannot name. So\nlook at it and find a name for it. Turn your face forward and learn. Do what\nmust be done”\n\n\n“Wishes, wishes,” I told him, “Wish in one hand and do something else in the\nother, and squeeze them both and see which comes true.”\n\n\n“What actually is Heaven? I ve pondered over this for countless years, but I\nstill don t know the answer … However, the seniors with countless years of\nexperience in my clan concluded that Heaven is the most heartless thing. It\ndoesn’t care at all if you re a mass murderer or a kind-hearted person, if you\nve helped tens of millions of people or killed tens of millions of them.\nPerhaps everything is negligible in the eyes of Heaven. The fittest survive in\nnatural selection. Either you’ll kill me or I ll kill you. Whether it’s\neverybody uniting to surround and kill one person or one person massacring\ncountless people … whatever happens, Heaven doesn’t care about these petty\nthings.”\n\n\n“The farthest distance known to man should be the distance between two people\nwho love each other deeply……and yet cannot be together.”\n\n\n“As regards the methods employed for forming deliberate intentions and doing\nstraightforward actions, there is none that will enable you to continue longer\nin the course you desire to pursue than that of ample deliberation; and none\nthat will enable you to pursue that course in greater peace than the patient\nbearing of insult. There is nothing more important than the cultivation of\nvirtue; there is no greater cause of joy than the love of goodness; there is\nnothing that will give you deeper insight into hidden things than perfect\nsincerity in word and deed; there is nothing that will make you clearer-sighted\nthan understanding the nature of all created beings; there is nothing more\nfelicitous than contentment, nothing bitterer than covetousness, nothing more\nsorrowful than the dispersion (or loss) of animal vigour, no greater sickness\nthan that which results from the vicissitudes of life, nothing shorter than a\ncareer of unlawful gain, nothing that tends more to secrecy (or stealthiness)\nthan avarice, nothing that isolates a man more than trusting to himself alone,\nnothing more dangerous than employing those whom you have reason to suspect,\nand nothing more certain to bring ruin to you than unfairness or partiality.”\n\n\n“What we are is invulnerable and cannot be bound.”\n\n\n“But… is it really possible to completely Sever evil? If humanity was left only\nwith goodness, perhaps that would make the world a more beautiful.\nUnfortunately, that isn t realistic. Without the existence of evil, perhaps\ngood… would no longer be called good. Good and evil are the desires of the\nheart. If I earnestly perform good deeds, evil can be suppressed. Likewise, if\nI malevolently perform evil deeds, good will be suppressed. Perhaps there is\nnothing truly good or truly evil in the world, similar to what my master Pill\nDemon told me about what is correct and incorrect. What I have… is my own will!\nThe choices I make decide everything!” As his voice echoed out, the music of a\ngreat Dao rose up around him, as well as the power of natural law.”\n\n\n“You two are still young and far too naive. In this world there are no absolute\nvillains. Neither are there any absolutely good people. There are only those\nwith different ideas. That is why regardless of how small a good is, don t\nforsake it and no matter how small an evil is, don t commit it.”\n\n\n“All that is in the past. There is no way to reverse the flow of time, and\nthere is no way to change past history. Punishing one’s self for something one\ncan no longer alter…that is nothing more than being made a fool of by fate! The\nonly one who rules over myself…is myself!”\n\n\n“In the end, it’s not the changes that will break your heart; it’s that tug of\nfamiliarity.”\n\n\n“Miss Dan Fei, take for instance, if we re in a circle right now. All that we\nknow is what’s in this circle. There are many more unknowns in the world\noutside the circle. It’s only when we set foot outside of that circle that we\nknow our original knowledge was much, much too little.”\n\n\n“Some loves come unbidden like winds from the sea, and others grow from the\nseeds of friendship.”\n\n\n“The faults we see in others never seem as dreadful as those we see in\nourselves.”\n\n\n“One time is much like another to death. She comes when she will. So why give\nover your mind to worry?”\n\n\n“But for the most part, love is a recognition, an opportunity to say, There is\nsomething about you I cherish.”\n\n\n“Snow can only live in the winter. When it nears a fire, it dies. That is its\nlife. It may yearn for summer, but… it can only desire it. In my hand, the snow\nbecomes water, because this is not its world….”\n\n\n“Life is like a dream, like a leaf that, no matter how beautiful , can only\nlive for one season.”\n\n\n“I am the pill furnace, and my heart is the pill formula. Refine the interior\nto achieve Immortality. Refine the exterior to achieve the boundless Dao of\nalchemy. Fuse them together, and this is the Truth of alchemy. Alchemy is the\nHeavens! Alchemy is the Earth! Alchemy is the world!”\n\n\n“What might have been and what has been Point to one end, which is always\npresent. Footfalls echo in the memory Down the passage which we did not take\nTowards the door we never opened.”\n\n\n“It’s the beautiful thing about youth. There’s a weightlessness that permeates\neverything because no damning choices have been made, no paths committed to,\nand the road forking out ahead is pure, unlimited potential.”\n\n\n“I am not allowed to think I m crazy. I am only allowed to solve this problem.\nExperimental physics—hell, all of science—is about solving problems. However,\nyou can t solve them all at once. There’s always a larger, overarching\nquestion—the big target. But if you obsess on the sheer enormity of it, you\nlose focus. The key is to start small. Focus on solving problems you can\nanswer. Build some dry ground to stand on. And after you ve put in the work,\nand if you re lucky, the mystery of the overarching question becomes knowable.\nLike stepping slowly back from a photomontage to witness the ultimate image\nrevealing itself.”\n\n\n“Nothing exists. All is a dream. God—man—the world—the sun, the moon, the\nwilderness of stars—a dream, all a dream; they have no existence. Nothing\nexists save empty space—and you…. And you are not you—you have no body, no\nblood, no bones, you are but a thought.”\n\n\n“We all live day to day completely oblivious to the fact that we re a part of a\nmuch larger and stranger reality than we can possibly imagine.”\n\n\n“The meaning of that word is: In doing nothing, a path shall appear. Allowing\nthings to happen naturally. In doing nothing, everything can be done. In doing\nnothing, you are doing something,”\n\n\n“Like the empty air; life is as tranquil as a blooming flower; all is fleeting\nand illusory; let your heart be as clear as a mirror”1. Everyone is born into\nthis world empty-handed. Your existence doesn’t any proof. Live life with a\nmerry heart and appreciate what is there! Things like origins aren t that\nimportant!”\n\n\n“Far out in the uncharted backwaters of the unfashionable end of the western\nspiral arm of the Galaxy lies a small unregarded yellow sun. Orbiting this at a\ndistance of roughly ninety-two million miles is an utterly insignificant little\nblue green planet whose ape-descended life forms are so amazingly primitive\nthat they still think digital watches are a pretty neat idea. This planet had a\nproblem, which was this: most of the people on it were unhappy for pretty much\nof the time. Many solutions were suggested for this problem, but most of these\nwere largely concerned with the movements of small green pieces of paper, which\nis odd because on the whole it wasn t the small green pieces of paper that were\nunhappy. And so the problem remained; lots of the people were mean, and most of\nthem were miserable, even the ones with digital watches. Many were increasingly\nof the opinion that they d all made a big mistake in coming down from the trees\nin the first place. And some said that even the trees had been a bad move, and\nthat no one should ever have left the oceans.”\n\n\n“Bypasses are devices which allow some people to drive from point A to point B\nvery fast whilst other people dash from point B to point A very fast. People\nliving at point C, being a point directly in between, are often given to wonder\nwhat’s so great about point A that so many people of point B are so keen to get\nthere, and what’s so great about point B that so many people of point A are so\nkeen to get there. They often wish that people would just once and for all work\nout where the hell they wanted to be.”\n\n\n“Time is an illusion. Lunchtime doubly so.”\n\n\n“R is a velocity measure, defined as a reasonable speed of travel that is\nconsistent with health, mental wellbeing and not being more than say five\nminutes late. It is therefore clearly an almost infinitely variable figure\naccording to circumstances, since the first two factors vary not only with\nspeed taken as an absolute, but also with awareness of the third factor. Unless\nhandled with tranquility this equation can result in considerable stress,\nulcers and even death.”\n\n\n“If I were the rain could I connect with someone’s heart as the rain can\nconnect the eternally separated earth and sky?”\n\n\n“Unless I grip the sword I cannot protect you. While I grip the sword I cannot\nembrace you.” “We must never shed tears. That is the life forms defeat. And if\nwe give in to emotions it only becomes proof of our inability to control it.”\n\n\n“We stretch out both our hands, pass through the clouds, straight to the sky.\nEven though we touched the moon and Mars, we still cannot touch the truth”\n\n\n“The only reason we think the flowers on the precipice are beautiful is because\nwe are standing on the precipice as well. Do not fear because we are like the\nflowers. We did not step off.”\n\n\n“The History of every major Galactic Civilization tends to pass through three\ndistinct and recognizable phases, those of Survival, Inquiry and\nSophistication, otherwise known as the How, Why and Where phases.”\n\n\n“Every time you and me connect with each other a little bit of heart is born\nbetween us. Heart isn t something inside you. But whenever you think of\nsomeone, whenever you remember someone. That’s when heart is born. If you were\nthe only person alive. The heart wouldn t exist would it?”\n\n\n“It’s impossible to feel exactly the same as someone else. But you can treasure\nyour friends and keep them close in your heart. I think that’s what it means to\nmake your hearts as one. ”\n\n\n“The Universe, as has been observed before, is an unsettlingly big place, a\nfact which for the sake of a quiet life most people tend to ignore. Many would\nhappily move to somewhere rather smaller of their own devising, and this is\nwhat most beings in fact do.”\n\n\n“It is known that there are an infinite number of worlds, simply because there\nis an infinite amount of space for them to be in. However, not every one of\nthem is inhabited. Therefore, there must be a finite number of inhabited\nworlds. Any finite number divided by infinity is as near to nothing as makes no\nodds, so the average population of all the planets in the Universe can be said\nto be zero. From this it follows that the population of the whole Universe is\nalso zero, and that any people you may meet from time to time are merely the\nproducts of a deranged imagination.”\n\n\n“The point is, you see,” said Ford, “that there is no point in driving yourself\nmad trying to stop yourself going mad. You might just as well give in and save\nyour sanity for later.”\n\n\n“Besides, love is always in your heart, no matter how long you stay apart.”\n\n\n“The ruins they made of your heart, they are still loved. They are cherished by\nthe memories they give refuge to, the wildflowers of peace that have taken root\nin their walls, and the birds of epiphany that have nested and given birth to\nnewer, more beautiful truths about you. The ruins they made of your heart, they\nare not ruins at all. They are an incredible universe made of blood and muscle\nand stars.”\n\n\n“I don t write because I am a perfect human being who has found the secret to\nlife. I write because I have made mistakes. I begged unworthy people to stay\nwhen they didn t deserve my presence. I loved too much, too hard and too\njealously. I let my trauma do the talking for a long, long time. I took the\nlong road to survival. I took ages to recognise the path to healing. And I hurt\npeople along the way and still regret it deeply. I write because I am a deeply\nflawed human being. And if there are lessons for others in my journey, in my\nwords, if there is healing for others hidden in my wounds, if there is meaning\nfor others within the ink when I bleed on paper, then I am doing what I set out\nto do when I dreamed of the words in a lonely bed on a starry night. To\nensure no one who reads those words ever feels alone.”\n\n\n“Life is not a problem to be solved. It is a reality to be experienced.”\n\n\n“The magic was in searching their whole world, lost in the wonder of it all.\nWithout imagination, things were only as they appeared – and that was\nblindness. Things were more than they appeared, so much more. When he\nconsidered an oak tree, it was not just a tree. To someone small, like an ant,\nit was a whole landscape of rugged barky cliffs and big green leaf-plains that\nquaked when the sky was restless, a place of many strange creatures where\nfearsome winged beasts could pluck and devour someone in a blink. And it wasn t\njust about magic. Without imagination, one could not think very far into\nthings, like that Lieutenant. Without imagination, he was no more than he said\nhe was. But there was more to him.”\n\n\n“There is a movement to all things in this world. Nothing stands still. Even\nthis planet we are on moves, which is why the sun rises and sets every day.\nWhat you are feeling is called centering. Memorize the feeling, make it your\nhome. It applies not just to swordsmanship but to life. A centered opponent is\na fearsome enemy. Stay centered and stay alive.”\n\n\n“But I will tell you that everything you experience, from the woods you walk\nthrough, the trees you climb and the people you meet, everything is connected.\nWhat is true of the outside world is true inside your body as well.”\n\n\n“The first and most important thing you must know is this: the whole world\naround us is alive. I m not saying that the moss in the stream have feelings or\nthat the trees have thoughts like you do. But there is energy, a connection\nbetween all things. Simply put, your ability with the sense is the ability to\nbe aware of that energy, to know where living things are and to know how things\nare going to move. In reality, the sense is nothing more than the ability to\nhave a little bit more information than the others around you.”\n\n\n“Face your fear and you will discover it has no hold over you anymore.”\n\n\n“It never is. The world doesn’t listen to us and it doesn’t follow any order.\nTo believe this world cares, to believe that nature will somehow protect us, is\nutter foolishness. Nature is not good or evil. It just is.”\n\n\n“If you leave it then it’s one step closer to giving up. And every relationship\nis worth a second shot.”\n\n\n“Certain people say that the real world has been constructed by the human mind,\nsince our ways are governed by the artificial categories into which we place\nessentially undifferentiated things, things weaker than our words for them.”\n\n\n“We believe that we invent symbols. The truth is that they invent us; we are\ntheir creatures, shaped by their hard, defining edges”\n\n\n“Where I put you, there you lie, Never let a stranger spy, Like glass grow to\nany eye, Not of me. Here be safe, never leave it, Should a hand come, deceive\nit, Let strange eyes not believe it, Till I see.”\n\n\n“In that contradiction will reside the appeal of this new belief. One can t\nfound a novel theology on Nothing, and nothing is so secure a foundation as a\ncontradiction. Look at the great successes of the past—they say their deities\nare the masters of all the universes, and yet that they require grandmothers to\ndefend them, as if they were children frightened by poultry. Or that the\nauthority that punishes no one while there exists a chance for reformation will\npunish everyone when there is no possibility anyone will become the better for\nit.”\n\n\n“You have humor! That’s excellent! There are few advantages, I ll tell you,\nthat profit a man more than humor. Humor will draw a crowd. Humor will calm a\nmob or reassure a nursery school. Humor will get you on and get you off, and\npull in asimis(money) like a magnet.”\n\n\n“Have I said that time turns our lies into truths?”\n\n\n“Perhaps when night closes our eyes there is less order than we believe.\nPerhaps, indeed, it is this lack of order we perceive as darkness, a\nrandomization of the waves of energy (like a sea), the fields of energy (like a\nfarm) that appear to our deluded eyes—set by light in an order of which they\nthemselves are incapable—to be the real world.”\n\n\n“The world is filled half with evil and half with good. We can tilt it forward\nso that more good runs into our minds, or back, so that more runs into this.” A\nmovement of her eyes took in all the lake. “But the quantities are the same, we\nchange only their proportion here or there.”\n\n\n“But there is no reason to mourn the destruction of a colony of cells: such a\ncolony dies each time a loaf of bread goes into the oven. If a man is no more\nthan such a colony, a man is nothing; but we know instinctively that a man is\nmore. What happens, then, to that part that is more?”\n\n\n“Sometimes when all our attention is thus focused on memory, our eyes, unguided\nby ourselves, will distinguish from a mass of detail some single object,\npresenting it with a clarity never achieved by concentration.”\n\n\n“If you do not see sorrow in my eyes, it is only because it lies upon my\nheart.”\n\n\n“For no man lives long when his dreams are dead.”\n\n\n“How foolish to call them mirrors. They are to mirrors as the enveloping\nfirmament is to a child’s balloon. They reflect light indeed; but that, I\nthink, is no part of their true function. They reflect reality, the\nmetaphysical substance that underlies the material world.”\n\n\n“Women believe—or at least often pretend to believe—that all our tenderness for\nthem springs from desire; that we love them when we have not for a time enjoyed\nthem, and dismiss them when we are sated, or to express it more precisely,\nexhausted. There is no truth in this idea, though it may be made to appear\ntrue. When we are rigid with desire, we are apt to pretend a great tenderness\nin the hope of satisfying that desire; but at no other time are we in fact so\nliable to treat women brutally, and so unlikely to feel any deep emotion but\none.”\n\n\n“All time exists. That is the truth beyond the legends the epopts tell. If the\nfuture did not exist now, how could we journey toward it? If the past does not\nexist still, how could we leave it behind us? In sleep the mind is encircled by\nits time, which is why we so often hear the voices of the dead there, and\nreceive intelligence of things to come.”\n\n\n“How strange it is that the sky, which by day is a stationary ground on which\nthe clouds are seen to move, by night becomes the backdrop for Urth’s own\nmotion, so that we feel her rolling beneath us as a sailor feels the running of\nthe tide. That night the sense of this slow turning was so strong that I was\nalmost giddy with its long, continued sweep. Strong too was the feeling that\nthe sky was a bottomless pit into which the universe might drop forever. I had\nheard people say that when they looked at the stars too long they grew\nterrified by the sensation of being drawn away. My own fear—and I felt fear—was\nnot centered on the remote suns, but rather on the yawning void”\n\n\n“But we shouldn t and couldn t force a man to act like a man. Did you ever want\nto fall asleep? When you weren t sleepy or even tired?” He nodded. “That was\nbecause you wanted to put down the burden of being a boy, at least for a time.\nSometimes I drink too much wine, and that is because for a while I would like\nto stop being a man. Sometimes people take their own lives for that reason. Did\nyou know that?”Or they do things that might hurt them,” he said.”\n\n\n“Now it struck me that the will itself was governed, and if not by reason, then\nby things below or above it. Yet it was very difficult to say on what side of\nreason these things lay. Instinct, surely, lay below it; but might it not be\nabove it as well?”\n\n\n“The tale I read to little Severian said that the universe was but a long word\nof the Increate s.(God) We, then, are the syllables of that word. But the\nspeaking of any word is futile unless there are other words, words that are not\nspoken. If a beast has but one cry, the cry tells nothing; and even the wind\nhas a multitude of voices, so that those who sit indoors may hear it and know\nif the weather is tumultuous or mild. The powers we call dark seem to me to be\nthe words the Increate did not speak, if the Increate exists at all;\nand these words must be maintained in a quasi-existence, if the other\nword, the word spoken, is to be distinguished. What is not said can be\nimportant—but what is said is more important. ”\n\n\n“Time itself is a thing, so it seems to me, that stands solidly like a fence of\niron palings with its endless row of years; and we flow past like Gyoll, on our\nway to a sea from which we shall return only as rain.”\n\n\n“All life acts to preserve its life—that is what we call the Law of Existence.\nOur bodies, you see, die long before we do. In fact, it would be fair to say\nthat we only die because they do.”\n\n\n“I have traveled far, and I have observed that poor people usually have more\nwit and more virtue than rich ones.”\n\n\n“No. I am saying that the things we love in others and admire in ourselves\nspring from things we do not see and seldom think about.”\n\n\n“All who speak Correct Thought speak well. Where then is the superiority of\nsome students to others? It is in the speaking. Intelligent students speak\nCorrect Thought intelligently. The hearer knows by the intonation of their\nvoices that they understand. By this superior speaking of intelligent students,\nCorrect Thought is passed, like fire, from one to another.”\n\n\n“I have no way of knowing whether you, who eventually will read this record,\nlike stories or not. If you do not, no doubt you have turned these pages\nwithout attention. I confess that I love them. Indeed, it often seems to me\nthat of all the good things in the world, the only ones humanity can claim for\nitself are stories and music; the rest, mercy, beauty, sleep, clean water and\nhot food (as the Ascian would have said) are all the work of the Increate.\nThus, stories are small things indeed in the scheme of the universe, but it is\nhard not to love best what is our own—hard for me, at least.”\n\n\n“There is no limit to stupidity. Space itself is said to be bounded by its own\ncurvature, but stupidity continues beyond infinity.”\n\n\n“And what of the dead? I own that I thought of myself, at times, almost as\ndead. Are they not locked below ground in chambers smaller than mine was, in\ntheir millions of millions? There is no category of human activity in which the\ndead do not outnumber the living many times over. Most beautiful children are\ndead. Most soldiers, most cowards. The fairest women and the most learned\nmen—all are dead. Their bodies repose in caskets, in sarcophagi, beneath arches\nof rude stone, everywhere under the earth. Their spirits haunt our minds, ears\npressed to the bones of our foreheads. Who can say how intently they listen as\nwe speak, or for what word?”\n\n\n“The oarsmen were rowing a slow beat to get us around a leagues-long bend to a\npoint where we could catch what little wind there was. The sound of the drum\nand the hissing of the water falling from the long blades of the sweeps are\nhypnotic, I think because they are so similar to the beating of one’s own heart\nin sleep and the sound the blood makes as it moves past the inner ear on its\nway to the brain.”\n\n\n“We choose—or choose not—to be alone when we decide whom we will accept as our\nfellows, and whom we will reject. Thus an eremite in a mountain cave is in\ncompany, because the birds and coneys, the initiates whose words live in his\n\n\n“forest books,” and the winds—the messengers of the Increate—are his\ncompanions. Another man, living in the midst of millions, may be alone, because\nthere are none but enemies and victims around him.”\n\n\n“Life is like a journey, filled with countless different experiences. Perhaps\nit is best to say that different experiences create different sceneries on that\njourney. If you experience bitterly cold wind, you will become snow. If you\nexperience the blazing sun, then you become rain….The type of life you\nexperience determines what type of person you will be. That is what makes life\nwonderful.”\n\n\n“In that moment, I understood that life is a journey. Every turn in the path\nleads to new scenery. My footprints exist on that path, and as to whether they\nare deep or shallow, it doesn’t matter. All the decisions were mine to make.”\n\n\n“Why, such is love’s transgression. Griefs of mine own lie heavy in my breast,\nWhich thou wilt propagate, to have it pressed With more of thine. This love\nthat thou hast shown Doth add more grief to too much of mine own. Love is a\nsmoke raised with the fume of sighs; Being purged, a fire sparkling in lovers\neyes; Being vexed, a sea nourished with loving tears. What is it else? A\nmadness most discreet, choking gall, and a preserving sweet.”\n\n\n“I felt that pressure of time that is perhaps the surest indication we have\nleft childhood behind.”\n\n\n“Weak people believe what is forced on them. Strong people what they wish to\nbelieve, forcing that to be real. What is the Autarch but a man who believes\nhimself Autarch and makes others believe by the strength of it?”\n\n\n“There’s a great deal said against Death. I mean by the people that has to die,\ndrawin her picture like a crone with a sack, and all that. But she’s a good\nfriend to birds, Death is. Wherever there’s dead men and quiet, you’ll find a\ngood many birds, that’s been my experience.”\n\n\n“By the use of the language of sorrow I had for the time being obliterated my\nsorrow—so powerful is the charm of words, which for us reduces to manageable\nentities all the passions that would otherwise madden and destroy us.”\n\n\n“A crowd is not the sum of the individuals who compose it. Rather it is a\nspecies of animal, without language or real consciousness, born when they\ngather, dying when they depart.”\n\n\n“I saw how little it weighed on the scale of things whether I lived or died,\nthough my life was precious to me. And of those two thoughts I forged a mood by\nwhich I stood ready to grasp each smallest chance to live, yet in which I cared\nnot too much whether I saved myself for not. By that mood, as I think, I did\nlive; it has been so good a friend to me that I have endeavored to wear it ever\nsince, succeeding not always, but often.”\n\n\n“If I had seen one miracle fail, I had witnessed another; and even a seemingly\npurposeless miracle is an inexhaustible source of hope, because it proves to us\nthat since we do not understand everything, our defeats—so much more numerous\nthan our few and empty victories—may be equally specious.”\n\n\n“Resolution and a plan are better than a sword, because a man whets his own\nedges on them.”\n\n\n“Form is empty. But it is full of everything else in the cosmos.”\n\n\n“The future is a blank sheet of paper. Only my will can leave footprints on\nit.”\n\n\n“This mind is my mind. It should be up to me to employ it- it should not go out\nfollowing others.”\n\n\n“My destiny lies in my own hands, and I will never let others control my fate.”\n\n\n“First, you must be able to understand… what the Dao is! “The Dao?” Meng Hao\ngaped. Why is there life? Why is there death?” The ancient voiced echoed back\nand forth endlessly. Why is there reincarnation? It’s like a circle, with the\nhead and the tail connected, but what exactly does that mean, and is\nreincarnation the only explanation? Why are there cultivators? Why are there\ncultivation Realms? Why is there Daoist magic? Why are there divine abilities?\nHow does light shine? How does darkness descend? Metal. Wood. Water. Fire.\nEarth. What are the differences between these elements? Fire is fire, and yet,\nwhy are there different types of heat? What is heat? What is cold? What does it\nmean when something that can only survive in the ice can be burned to death by\na single drop of water?” The ancient voice spoke with increasing speed, causing\nMeng Hao’s mind to tremble. Questions piled up in his mind. Each one seemed\npossible to answer directly, but if he actually had to answer them, he would be\nleft speechless. “What is the Dao?” That was the final question uttered by the\nancient voice, and it left Meng Hao’s mind rumbling. The Essence is the Dao,\nthe basis of everything that defies Heaven!” A bright light shone in the\nmiddle-aged man’s eyes, and he suddenly seemed very serious. All such unknowns\nare Essence. Only when you seek out the Essence, can you comprehend Heaven and\nEarth, understand all living things, and control everything! When you\nunderstand all transformations of Heaven and Earth, when you have defied the\nHeavens, when you have sealed the Earth, then what could possibly be difficult\nto you?!”\n\n\n“So long as there were benefits, forgetting favors to violate justice and not\nrecognizing one’s family were no less common than drinking water.”\n\n\n“A cultivator’s talents and physique aren t enough; one also needs a strong Dao\nwillpower, and a will strong enough to follow the road to the end. One day, he\nwould be the last one smiling, and he would be the one to stand at the highest\npeak, looking down on humanity.”\n\n\n“The most important thing… was to have an unshakable Dao willpower; a willpower\nthat thirsts for knowledge! A willpower that understands that the truth of all\nrivers flow into the same sea! A willpower that was diligent and assiduous!\nThis was the most important thing! For insufficient talents, hard work would\nsuffice as long as the mind was willing; for insufficient talents, others take\none day to awaken, and you take one hundred days to awaken. There would always\nbe a fruitful harvest. With a Dao willpower that could accept that all rivers\nlead into the sea, one could see the entirety of the eight directions and not\nbe blinded by their own shortsightedness!”\n\n\n“Though I come from the mortal dust, my heart still soars towards the heavens.\nIt was like the grass upon the desolate battlefield… It had to struggle to\nemerge from the dirt and grow up. It fought for every ray of sunshine, every\ndrop of rain, because it wanted to grow higher and higher. It might be very\nlow-key and unremarkable…but not even the most exalted major powers of the\nThree Realms could stop the will and the heart of the grass. The heart was\ninfinite and unlimited; even a beggar could have the heart and ambitions of an\nemperor. The power of the heart was invisible and formless…but it was\nincomparably marvelous. Even mortals who had sufficiently powerful hearts and\nwills could create miracles. They would become heroes!”\n\n\n“When looking for a partner, one should look for everlasting feelings, for\ngentle feelings, for straightforward feelings, for feelings that makes them\nwilling to commit. This factor will forever remain more valuable than money. To\nme, the perfect situation is when a person has accompanied the other throughout\ntheir growth. It doesn’t depend on how much money that person can make, but in\nhow that person can make your lives move forward, together, side by side.”\n\n\n“The heaven and earth is eternal. One day we ll meet again!”\n\n\n“Surrounded by bustling crowds, you look at me, I look at you, and we smile at\neach other.”\n\n\n“No matter what I lose, as long as I don t lose my life, as long as I am still\nalive, then there is hope for getting it all back. Life is the greatest hope of\nall. When you are alive, anything is possible!”\n\n\n“There were too many unfair things in the world. How many people understood the\ntruth of matters? And the so-called characters of justice. They were the lies\nof certain people, yet how many people’s eyes were blinded by them? Thus, he no\nlonger cared about the world’s opinion because the people of the world were\nstupid. They were unable to determine the truths and the lies. Thus, he only\ncared about the people he cared about. He, Chu Feng, did not live for the\nliving of the world. He lived for himself and the ones close to him.”\n\n\n“I don t know what you re trying to protect, nor do I know what you have been\nhurt by. But, if you want to protect that child then do so with your chest held\nhigh! Here and now, be proud in knowing that you are protecting her! This is\nyour life right? Then decide for yourself! If you want to protect everything\nwith your hands then do so, if you want to abandon everything then do so. But,\nwhat do you yourself want, to do right now? Can you really be satisfied giving\nsomeone else that you don t really understand your most important thing?”\n\n\n“My goal cannot merely be to exceed any other person. My goal… for all\neternity… will be to exceed myself!”To constantly exceed myself, to continually\nbreak through my own barriers! I will always walk my own path, all the way to\nthe end!”\n\n\n“That said, I only want to tell you one thing, treasure the people before you.\nIf you really lose them one day, there won t be enough time for regret.”\n\n\n“Whether you speak of our time on earth, or reincarnation, life is a journey.\nThe sea of bitterness is only one bit of scenery, that’s all. The most\nimportant thing is to leave our mark on the path that we have walked and\nexperienced. As for me, I want to keep walking even further off into the\ndistance!”\n\n\n“Love cares what becomes of you because love knows that we are all\ninterconnected. Love is inherently compassionate and empathic. Love knows that\nthe “other” is also oneself. This is the true nature of love and love itself\ncan not be manipulated or restrained. Love honors the sovereignty of each soul.\nLove is its own law.”\n\n\n“Until the tiger learns to write, stories will always glorify the hunter.”\n\n\n“Dare to think. Before your dream comes true, never limit yourself beforehand.\nNever find any excuses for yourself, or reasons to fail, only so will we have\nthe possibility to make the seemingly distant dream reality.”\n\n\n“He realized that there are many ways of life, or many ways of living life.\nPerhaps there are meaningless ways, but nevertheless interesting ways.”\n\n\n“This silence belongs to us… and every single\nperson out there, is waiting for us to fill it with something.”\n\n\n“Was I able to live inside someone’s heart? Was I able to live inside your\nheart? Do you think you’ll remember me at least a little? You d better not hit\n\n\n“reset!” Don t forget me, okay? That’s a promise, okay? I m glad it’s you,\nafter all. Will I reach you? I hope I can reach you.”\n\n\n“Maybe… just maybe, the light can reach even the bottom of a dark ocean.”\n\n\n“So ephemeral and weak. But its shining with all its might. Thump, Thump, like\na heartbeat. This is the light of life.”\n\n\n“You know, I discovered something. Everyone has something… Something deep\ninside their hearts. For some, it might have been enmity. For others,\nadmiration. Wishes, a craving for the spotlight, feelings that one wants to\ndeliver, feelings for one’s mother. Everyone was supported by their own\nfeelings. I realize now that, perhaps, no one can stand alone on stage.”\n\n\n“I am so afraid of disappointing the people I love, I often forget that I am\nsomeone I love too. And I need kindness just as much as I believe the people I\nlove do.”\n\n\n“All the pieces of you that broke and shattered will become the seeds from\nwhich the finest parts of your soul will grow.”\n\n\n“You may feel small now, but so are all stars from a distance. You ll find\nsomeone who will come closer to see how bright you can truly become. They ll be\nthe moon to your sun, reflecting your beautiful glow for the world to see. Just\nbe patient, somebody is going to come around, take you to a whole different\ngalaxy, and make you feel like the brightest one out of 100 billion stars that\nyou are.”\n\n\n“If one chooses splendor, then they have to be able to withstand the bitterness\nand pain behind that splendor”\n\n\n“We had the wrong notion that stubbornness equated to love, but if there was\nlove between us, why was there a need for stubbornness? Saying I don t love you\nwould not affect the quality of love, as long as love exists, nothing will\nchange, what more mere words?”\n\n\n“What are love roots? Just like a seed. Even if the roots are torn, they ll\nstill grow back. The most important is the heart. Whether someone has love\ndepends on their heart.”\n\n\n“Let us be nourished together, let us grow together, let us withstand this\ntreacherous weather, experience wind and rain, and emerge to become even\nstronger than before.”\n\n\n“The end of the road brings fragrance, it fills the entire sea of snow.”\n\n\n“Water can make the boat move, but the boat can also be turned over by water.”\n\n\n“A lifetime is not a gentle dance, a dance is a lifetime of bitterness; I will\ndance for you in this lifetime, bitter or not I will dance a lifetime!”\n\n\n“The Heavens And Earth Do Not Shift, Neither Does The Stone Turn Amidst The\nFlowing River!”\n\n\n“The Way of the Heavens without love, then all things will be destroyed; The\nWay of the Heavens with love, then all things will be created. The Way of the\nSword without love, kills people; The Way of the Sword with love, the heart\nvalues life. With love, without love, it depends on you. Nine lives, nine\ntribulations, without love is also with love. With love is without love; With\nlove is also without love; Without love is also with love; With love, without\nlove, the understanding is hidden within. That is the Way of the Heavens. With\npeople, come martial arts, take martial arts to learn the Way of the Heavens,\nand in the end is transcendence. In the secular human world, the secret to the\nWay of the Heavens is also there. Human emotions, the root of the Way of the\nHeavens…”\n\n\n“A horse enjoying the spring wind, runs faster. A person who encounters a happy\nevent, has a clear state of mind. When elation rises, one’s magnificence is\nboasted. A cheerful book, closes faster. The world is beautiful, life is full\nof hope, how can one scoff at that? Even if you want to, you cannot.”\n\n\n“Force is only effective in certain cases, just as ploy has its own limits. A\ngroup of kids can trap a rabbit, but cannot kill a tiger.”\n\n\n“Wind and rain cannot wipe the scars in our hearts, misery cannot destroy love;\nsince ancient lightly, with separation do we know emptiness and desolation…”\n\n\n“Mixing up the blue sky, and arrogantly laughing at the storm. Master the world\nwith the universe within me.”\n\n\n“Things are never black or white but a million shades of gray.”\n\n\n“What good are wings, without the courage to fly?”\n\n\n“Poetry isn t good until you experience pain.”\n\n\n“The only way you will ever belong is when you feel complete without anyone\nreassuring you that you are.”\n\n\n“Do you really understand?” he asked, his voice suddenly becoming very archaic\nas he attempted to imitate the way Zong Wuya had spoken and held himself\nmoments ago. “You know, just now, someone asked me what the Dao is. My answer\nwas that the Dao relates to the thoughts in your heart. Whatever you focus your\nthoughts on, that is your Dao. The Dao is shapeless, and cannot be touched; it\ncan only be contemplated, just like life.” “If you defined life as having\ndifferent Realms, then that would be… the Natural Realm.” “Consider them,” he\nsaid, gesturing toward the soldiers not too far off in the distance, and the\ncultivators from the Mountain and Sea Realm. “Now consider yourself. “Between\nhumans, the only thing we do is compare ourselves to others, in any and all\nmatters. We compare who has the highest cultivation base, who is richer, who\nhas the better status, who has the higher position, who has the most power, who\nhas the best family background, who is the smartest, or who is the strongest.\n\n\n“The weak with the weak and the strong with the strong, all people are\nconstantly comparing themselves to each other. Because of these comparisons,\npeople covet what they do not have, and what they do have, they are even more\nunwilling to lose. “That is another kind of life, and most importantly, that is\nthe type of life… that most people live. I like to call such life the second\nRealm, the Pragmatic Realm! “You are in that Realm, and so am I.” “So… is there\na third Realm?” she asked quietly.  “Of course!” Meng Hao looked over at her,\nhis expression ever more archaic and his aura swirling even more mysteriously.\nHis eyes glowed with bright light, like twin lamps on a moonless night.  “The\nthird Realm is … when you leave something behind,” he said softly.  “Are you\nwilling to give that something up?” he asked, shaking his head slowly. “Do you\naccept letting it go? Are you even able…to leave it behind? “The third Realm is\nthe realm of abandonment. After you have something, you abandon it, or perhaps\nyou could say… put it aside!  “Put everything aside, and you have emptiness. At\nthat time, you… can finally explain what the Dao is!” He took a deep breath and\nlooked at Xue er, who was staring at him blankly. Suddenly, he raised his voice\nberatingly. “Don t you get it?!  “Consider the game board. What is it? That\ngame board is your world, and in your heart, it is your everything. When all is\nsaid and done, it has borders, limitations, creating an intangible perimeter\nbeneath your feet, an area in your heart that you cannot leave! “If you don t\nput it aside, then you will forever remain in the second Realm. For all\neternity… you will be unable to explain… the Dao!” When she opened her eyes\nagain, she asked, “What is the name of thaty third Realm?” His lips moved\nsoundlessly for a moment, and then he calmly said, “I call that Realm… the\nDao!”\n\n\n“Well then, in your opinion, what is a Dao?” Zong Wuya’s expression was placid,\nbut the reminiscence in his eyes grew even stronger as they continued to\ndiscuss the Dao.  Meng Hao didn t need to think about the answer. He\nimmediately responded: “The Dao is the obsession in your heart, the path that\nyou choose to follow.” “In that case, what is your Dao?”Freedom and\nindependence!” Meng Hao said, his voice filled with decisiveness that could\nsever nails and chop iron. “Freedom. Independence….” Smiling, Zong Wuya shook\nhis head. “What is freedom? And similarly, what is independence? Is freedom\nbeing free from all restrictions? Is independence an absence of all restraint?\nAs you sit here in front of me, Heaven and Earth restrict you. The entire world\nrestrains you. “Look up, and you will see the sky. The sky weighs down on you.\nBeyond the Windswept Realm is the void, the Heavens. There are 33 Realms, all\nof them are also weighing down on you. Beyond those 33 Realms, are even more\nRealms and worlds. All of them are also pressing down on you.” Although Zong\nWuya spoke calmly, his words were as incisive as the stabs of a sword. They\neven seemed to be filled with a strange power, as if every word he spoke were\ncompletely and utterly correct. “What about morality and principles,” Zong Wuya\ncontinued, his wording growing more cutting. “Are they not restraints? Can you\nignore them? Can you trample on them? Where does your freedom come from? Where\ndoes your independence come from?” His eyes glittered, and seemed to contain\nmatchless wisdom.”You are weak,” he said, staring Meng Hao in the eye. “When\nyou meet powerful people, you have no freedom, nor any independence, not unless\nyou are the most powerful person. However, the starry sky is wide, and the\nHeavens are vast. Perhaps when you think you are the most powerful person in\nexistence, wouldn t you always be wondering if there might be other people over\nthe horizon who also view themselves to be at the ultimate pinnacle?” “I–” Meng\nHao was about to reply, but was cut off by Zong Wuya. “You have an incorrect\nunderstanding of the Dao. Your freedom is not a Dao, it is an obsession of\nyours. And an obsession… is likewise not a Dao!” “If you can find someone like\nthat, someone who you can hold and close your eyes to the world with, then you\nre lucky. Even if it only lasts for a minute or a day.”\n\n\n“If there’s no suffering from pining, how can there be joy from reunion. Going\ntoo far will not accomplish anything. You will only value something if you get\nit after much suffering.”\n\n\n“As the saying goes, be aware of shame and move forward with courage! It is not\nscary to be in the wrong! What is scary, is that all of you are still unaware\nthat you are in the wrong; showing satisfaction at your accomplishments, being\nprideful of your own self! This is simply unforgivable!”\n\n\n“If a beggar was to work hard, then he could leave behind a mark in history of\na great general. A great general who does not work hard could also leave behind\na mark in history, but that mark would be one of terrible losses and notoriety!\nIn order to subdue others, and even the whole world, one must first be able to\nsubdue themselves! This was the absolute minimal of conditions!”\n\n\n“Having a great deal of knowledge does not equate having a great deal of\nability; most importantly, it does not equate having a great character! With\njust a glib mouth, how can one be considered a talent? These men have no\nsignificance at all”\n\n\n“And who are you, the proud lord said, that I must bow so low? Only a cat of a\ndifferent coat, that’s all the truth I know. In a coat of gold or a coat of\nred, a lion still has claws, And mine are long and sharp, my lord, as long and\nsharp as yours. And so he spoke, and so he spoke, that Lord of Castamere, But\nnow the rains weep o er his hall, with no one there to hear. Yes now the rains\nweep o er his hall, and not a soul to hear.” “Because pride is a strange thing,\nand because generosity deserves generosity in return”\n\n\n“You see, women are like fires, like flames. Some women are like candles,\nbright and friendly. Some are like single sparks, or embers, like fireflies for\nchasing on summer nights. Some are like campfires, all light and heat for a\nnight and willing to be left after. Some women are like hearthfires, not much\nto look at but underneath they are all warm red coal that burns a long, long\nwhile.”\n\n\n“To be stubborn, is to be unwilling to accept others. If you are unable to\naccept others then it means others will be unable to accept you. That way of\nthinking is dangerous.”\n\n\n“And then, we came to realize. All things happen abruptly. Bad things happen at\ntimes when you re unable to predict or prevent them. Suddenly parents can die,\nSuddenly your siblings can come to beat you up, Suddenly a truck can come to\nhit you, Suddenly you can be reincarnated in another world, Suddenly your\nfather can come to attack you and then force you into being a home teacher for\na young lady, Suddenly you could be thrown to another continent as well. Most\nlikely everything is a result of chance. Furthermore, we would come to realize.\nThe severity of this world. The fact that people can simply die. The fact that\nany person anywhere can all too easily die. The fact that there are no\nexceptions. Or the fact that only people in my surroundings aren t conveniently\nallowed to live long lives. Finally, at this late point. We finally came to\nrealize this reality. The origin of the phenomenon known as death, the fact\nthat a person nearby can suddenly disappear”\n\n\n“That’s good, that’s good, Kishirika said it as well. No matter the time you\nshould just laugh! I remember, the last time Kishirika died as well and was\nlaughing in a loud voice, fuhahahaahah!”\n\n\n“There’s only one person in this vast world that we truly want to be together\nwith. Even if things don t work out in our favor we can t help but wish for\ntheir happiness”\n\n\n“I spent my life trying to reduce the brain to a series of electrical impulses.\nI failed. Human emotion… It can contain illogical conflict. Can love someone\nand yet hate the things that they ve done. Machine can t reconcile that.”\n\n\n“For one hundred and thirty thousand years, our capacity to reason has remained\nunchanged. The combined intellect of the neuroscientists, mathematicians and\nengineers pales in comparison to the most basic A.I. Once online, a sentient\nmachine will quickly overcome the limits of biology; in a short time, its\nanalytic power will become greater than the collective intelligence of every\nperson born in the history of the world. Some scientists refer to this as the\nSingularity. I call it Transcendence.”\n\n\n“We never lose our demons, we only learn to live above them.”\n\n\n“Death is what gives life meaning. To know your days are numbered and your time\nis short. You d think after all this time I d be ready. But look at me.\nStretching one moment out into a thousand… just so that I can watch the\nsnow.”\n\n\n“Wong: How’s your Sanskrit? Dr. Stephen Strange: I m fluent in Google\nTranslate.”\n\n\n“You think you know how the world works? You think that this material universe\nis all there is? What is real? What mysteries lie beyond the reach of your\nsenses? At the root of existence, mind and matter meet. Thoughts form reality.\nThis universe is only one of an infinite number. Worlds without end; some\nbenevolent and life-giving, others filled with malice and hunger. Dark places\nwhere powers older than time lie, ravenous and waiting. Who are you in this\nvast multiverse?”\n\n\n“Live boldly. Push yourself. Don t settle. Just live.”\n\n\n“They say you only really appreciate a garden once you reach a certain age, and\nI suppose there is a truth in that. It’s probably something to do with the\ngreat circle of life. There seems to be something miraculous about seeing the\nrelentless optimism of new growth after the bleakness of winter, a kind of joy\nin the difference every year, the way nature chooses to show off different\nparts of the garden to its full advantage.”\n\n\n“You only get one life. It’s actually your duty to live it as fully as\npossible.”\n\n\n“Some mistakes … just have greater consequences than others. But you don t\nhave to let that night be the thing that defines you.”\n\n\n“I will never, ever regret the things I ve done. Because most days, all you\nhave are places in your memory that you can go to.”\n\n\n“Not all love is good love.”\n\n\n“Life. How unpredictable and messy and beautiful it could be. I know; I can\nhardly believe those words are coming out of me without a hint of sarcasm (well\n… maybe a little)”\n\n\n“A single man in the world should have dominated everything! Even if I fail, at\nleast I would have fought once. I may die, but with no regrets! A speck of\ndust, so what? A deadwood, so what? Life is inherently like a leaf living\nthrough autumn!”\n\n\n“It is always in this world, no matter where I live. There are always roads, no\nmatter where I stand. Where do we sing and laugh; where do we weep and cry? …”\n\n\n“The Dao exists in the heart, and the heart is born of the will. If your will\nis strong, then your Dao will be powerful, and your sword… will be invincible!”\n\n\n“We Cultivators don t just practice cultivation to gain eternal life. No, we\npursue the Dao…. For those who strive after the Dao, life is a morning and\ndeath is an evening. For those who seek the Dao, when evening comes, of what\nuse is longing…?”\n\n\nHe was confused and murmured, “Oh girls… Their thoughts are truly baffling…”\n\n\n“We have been companions. Swordmates. Bedmates. But in many things we are\nstrangers to one another, afraid to trespass where emotions may not be wanted.\nHaving been locked so long in service to oneself, each of us, it is difficult\nto turn the key and unlock ourselves, saying the things we desire to say, to\nshare the things that should be shared.”\n\n\n“Women: they tend you or terrorize you.”\n\n\n“Skepticism is healthy. It keeps you from growing vulnerable to words of\nmanipulation. Disbelief, in its place, is also occasionally healthy, because\nthe proper amount keeps you honest.”\n\n\n“Men are fools when it comes to women. It doesn’t matter how smart you are, or\nhow shrewd, or how much experience you ve had. They re all born knowing just\nwhat it takes to find a way to muddle up your head. And given the chance, they\ndo. I ve known men who bed only whores, wanting to make no better commitment,\nsaying it’s the best way to avoid entanglements. I ve known men who\nmarry women so as not to buy the bedding. And I ve known men who do\nboth: bed whores and wives; sometimes, with the latter, their own. I ve\neven known men who swear off women altogether, out of zeal for\nreligious purity or desire for other men; neither appeals to me, but I\nll curse no man for it.  And certainly, in the South, I ve known men\nwho have no choice in the matter of bedding women, having been\ncastrated to serve tanzeers or anyone else who buys them. But I ve\nknown no man who, drunk or sober, will not, at least once, curse a\nwoman, for sins real or imagined. A woman; or even women.”\n\n\n“Grunting, I sat up. Tried to stretch muscles and pop joints without waking\nher, because no man likes a woman to see how he’s growing older, how the years\nare taking their toll.”\n\n\n“Do not apologise for existing. For being yourself. Apologise when you are\nnot.”\n\n\n“A dunce once searched for a fire with a lighted lantern. Had he known what\nfire was, he could have cooked his rice sooner.”\n\n\n“Since life is full of painful things after all. Even if it’s forcibly, if you\ndon t create some good times then you’ll be crushed.”\n\n\n“After careful study,” he began softly, “I have come to find that the Dao of\nHeaven and Earth, and one’s own Dao, is a willful return to one’s natural\nstate….” He had chosen to bestow some good fortune upon the cultivators of the\nSouthern Domain, both as a means of thanking them for caring for his former\nresidence, and also… for the mere fact that he felt this place to be his home.\nIt was different than Planet East Victory.  “Therefore, cultivation is also\nknown as cultivating truth. Of the two characters which make up the latter\nterm, the first refers to the method, the second refers to the mental state….”\nHis voice seemed to contain a bizarre power that caused it to spread out in all\ndirections, causing every audience member, regardless of the level of their\ncultivation base, to slip into a strange, trance-like state. “Simply put, it is\nvery similar to how I once described to someone the different Realms of life.\n\n\n“In the past, various people have asked me what the Dao is…. My responses have\nvaried depending on the occasion, the circumstances, and the level of my\ncultivation base. In fact, every single time, I gave a different answer. I m\nnot even sure what my answer will be the next time someone asks me. “However,\nthere is one thing that will never change, as far as I can tell. And that is…\nthat I don t know what the Dao is. There are too many answers to the question.\nAll I know… is that what I am pursuing is freedom and independence. To be free\nand unconstrained. That is my truth, and that is my Dao! “In cultivating truth,\nwhat we cultivate… is the heart.” Meng Hao’s voice reverberated out as he\nexpounded upon his understanding of the Dao, and the enlightenment he had\ngained regarding cultivation. The words he spoke were like seeds that became\nburied in the hearts of the various cultivators. “If your heart is steadfast,\nit cannot be trampled by Heaven or Earth, nor can it be broken by any living\nthing. You will never bow your head in acquiescence, and you will be able to\nadvance without hesitation, and you will never stop moving forward. This is the\nmeaning of cultivating the heart and cultivating the truth. It is traveling\nalong the path of cultivation itself. “My life has been spent practicing\ncultivation. I started in the Qi Condensation stage, and now here I am, having\nexperienced numerous twists and turns. I will merge my body, my mind, and my\nsoul into an image which will become like a spirit in your heart. Observe it.\nContemplate it. It can become the truth, the path, and the heart which you\ncultivate!”\n\n\n“Spock: Fear of death is illogical. Bones: Fear of death is what keeps us\nalive.”\n\n\n“Who can understand the reason for human life? Creation and destruction,\nthere’s no need for sorrow.”\n\n\n“As we look back upon history, we will often find that under the surging\ncurrent of history, even the wisest leaders find it hard to keep their heads\nover water.”\n\n\n“When you re doing well, all people around are opportunists, but it’s when you\nare in a bad position in life then you know who your real friends are, who are\ngenuinely fond of you and would support you for who you are.”\n\n\n“Above the heavens, the stars rotate. The magnificence of the galaxies nourish\neverything. The heavens can cover it but not contain it; the earth can contain\nit but not cover it. The universe can accommodate it but not refute it knowing\nthat everything has its place and limitation… Heaven and earth turn and rotate\nas everything returns to the universe.”\n\n\n“With my heart, I shall master the way of the universe; with my heart, I will\ncalm my mind. I will cultivate my mind and control my heart with my mind!”\n\n\n“Life is too short, if I don t do something great, how could I feel worthy of\nmy parents who raised me?”\n\n\n“If you would take a man’s life, you owe it to him to look into his eyes and\nhear his final words. And if you cannot bear to do that, then perhaps the man\ndoes not deserve to die.”\n\n\n“We all think of a special someone from dawn to dusk and back again. Hopefully\nsomeday we ll find our way to that person and spin our own tale of happiness.”\n\n\n“On the days when you feel ashamed of your scars, your mind only registering\nhow ugly they are rather than the beauty they prove of you having survived,\nremember that there is an entire art form dedicated to filling the cracks of\nbroken things with lacquered gold. An entire art form that proves that even the\nbroken and damaged history of an object is beautiful and should be treasured.\nRemember how much more you are than an object. Remember your survival, your\njourney, your scars deserve to be treasured too.”\n\n\n“How can love be as calm and still as water? Hurting and being hurt. How can it\nstay the same forever? Even if wounded. Even if in conflict with other people.\nEven so this feeling fading away is impossible. No longer fearing being\nselfish. No longer fearing repercussions. That is liking someone.”\n\n\n“A man has but one life, grass but one spring, so if death comes then so be\nit!”\n\n\n“The heart is the limit.”\n\n\n“In both worlds, when many people discover that they lack the ability to\naccomplish their dreams, they will feel lost, disappointed, disjointed, and\ninferior. Many of them will resort to immersing themselves in great pain,\nimagining success, going into isolation or hoping to go back to the past. It is\nnot wrong for a person to walk on a single path. Suffering may be brought to\nthose around this person but eventually, this person may succeed. However, the\npeople who have the determination to immediately choose a new way deserve even\nmore respect. Life is interesting because picking another road of life requires\nmore determination and bravery than continually walking on the same path.”\n\n\n“To think about the past bring out the pain and cry that penetrates the heart\nand mind. but it must be endured. He has yet to succeed but has already drank\nthe poison. He endured it all. No words can be written to express my thought”\n\n\n“The end of religion is the beginning of spirituality. At the end of\nspirituality is reality. The end of reality is the real bliss.”\n\n\n“Today we find so many sects and creeds, each worshipping its own god or\ngoddess in its own particular way. Evidently the goal\n\n\nbefore their eye is not even liberation but in most cases deliverance from some\nparticular form of misery or some material gain.”\n\n\n“The time you feel alone is the time you most need to be by yourself.”\n\n\n“Earth teach me to forget myself as melted snow forgets its life. Earth teach\nme resignation as the leaves which die in the fall. Earth teach me courage as\nthe tree which stands all alone. Earth teach me regeneration as the seed which\nrises in the spring.”\n\n\n“Our Lord has written the promise of resurrection. Not in books alone but in\nevery leaf in springtime.”\n\n\n“The world is a cruel place. So go out there and make it a little less so.”\n\n\n“It’s incredibly pretentious how man can think it has power over every other\nliving being. It’s possible that no species can be controlled at all. And like\nthat. You have to respect every form of life. Man protects other species for\nfear of loss. He protects the environment thinking that’s how he ll survive.\nAnd he’s just being selfish. But not that there’s anything wrong with that.\nIt’s actually necessary. We can t Belittle any species at all.”\n\n\n“The natural state of life is desire and disorder. There is no such thing as a\ncompletely transparent soul. Not even cultivating the Dao can make one’s Dao\nheart spotlessly pure. On the contrary, her spirit is more complex than you\ncould have imagined.”\n\n\n“A small, small world with two small birds playing with fire. Ants praise how\ngreat their countries are and discuss how easy it is to shake a big tree.”\n\n\n“We are one infinite tree in an infinite forest.”\n\n\n“But no afterlife,” grumbled the king. “No eternal soul? It’s unnatural. “On\nthe contrary,” said Kell. “It is the most natural thing in the world. Nature is\nmade of cycles, and we are made of nature. What is unnatural is believing in an\ninfallible man and a nice place waiting in the sky”\n\n\n“Mountains have no worries, til hit with snowy flurries; waters feel no woe,\ntil the winds do gust and blow….”\n\n\n“Before she became ill, David’s mother would often tell him that stories were\nalive. They weren t alive in the way that people were alive, or even dogs or\ncats. People were alive whether you chose to notice them or not, while dogs\ntended to make you notice them if they decided that you weren t paying them\nenough attention. Cats, meanwhile, were very good at pretending people didn t\nexist at all when it suited them, but that was another matter entirely. Stories\nwere different, though: they came alive in the telling. Without a human voice\nto read them aloud, or a pair of wide eyes following them by flashlight beneath\na blanket, they had no real existence in our world. They were like seeds in the\nbeak of a bird, waiting to fall to earth, or the notes of a song laid out on a\nsheet, yearning for an instrument to bring their music into being. They lay\ndormant, hoping for the chance to emerge. Once someone started to read them,\nthey could begin to change. They could take root in the imagination, and\ntransform the reader. Stories wanted to be read.”\n\n\n“The world of the old tales existed parallel to ours, but sometimes the wall\nseparating the two became so thin and brittle that the two worlds started to\nblend into each other.”\n\n\n“We all have our routines,” he said softly. “But they must have a purpose and\nprovide an outcome that we can see and take some comfort from, or else they\nhave no use at all. Without that, they are like the endless pacings of a caged\nanimal. If they are not madness itself, then they are a prelude to it.”\n\n\n“She giggles. “Thanks.” “I haven t done anything yet.” She looks at me like I m\nthe best person in the world, and I can t really take that. Makes me want to be\na total smart-ass. “You always say that.” Her voice is sort of sad, like she d\nhoped she could have changed me already. “Maybe thanks should be given just for\nbeing willing to do something for someone else?”\n\n\n“We are like the water. Salty. Stubborn. Frothy and rolling and scary all at\nonce. We are like the sea. Calm and still and alive. Changing all the time. We\nhave been alive forever. Long before our bodies were born. Long after our\nbodies will die. We will live forever, our waters mixing with the waters of\nothers. All of us tossed together beneath the stars. Beneath that constellation\nyou call your own. The sea monster one that looks down on us here on earth and\nknows all about us. Me and you and all the others, married and mixed in the\nocean. Churned together.  Can I walk out into the water? If I say we are\neternal, will you let me go? Let me slip into the sea, alone as I came into\nthis world?”\n\n\n“But talent is not enough to protect yourself. You might face a disaster before\nyou mature. It’s because an elephant has tusks, while a rhinoceros has a horn\nthat they are hunted.”\n\n\n“If you believe everything you read, you better not read.”\n\n\n“You have to be broad-minded. If you want your life to be good, you will need\nto have some tolerance. If you are too ruthless and extremely self-centered,\nand intolerant with others, sooner or later, it will destroy you.”\n\n\n“During the primitive periods, a group of cavemen worshipped a shadow. The\nshadow could grow bigger and smaller. It looked like a god, so the cavemen\nwould worship the shadow every day. However, a clever caveman did not believe\nin god . With great effort, he managed to climb to the top of the cave one day.\nHe then realized that the god the people worshipped was just the shadow of a\nrock being cast by the sunlight. The clever caveman told his tribe the truth,\nhe did not want them to worship the shadow, because it was just the shadow of a\nrock and not a god. In the end, no one believed the clever caveman, but they\nbecame scared. The clever caveman was eventually burned at the stake because of\nhis blasphemy against the god. After that, the cavemen carried on worshipping\nthe rock’s shadow.”\n\n\n“The mushroom knows not the alternation of day and night, while the short-lived\ncicada does not know the seasons. If one were to see the vast world yet not\nexplore it, turning to dust hundreds or thousands of years later, then what\ndifference was there between them and the mushrooms and cicadas?”\n\n\n“So what?” The Shepherd Boy stroked his piccolo. “All sorts of natural living\nthings support the survival of humans, yet humans have never reciprocated that\nto the Heavens. Humans have accepted the gifts from nature, but what they\nconsider is forever their own interests.”\n\n\n“You have extracted from nature without constraint, and with your trillions of\npeople, you have extracted even more. To survive, are you not killing living\nbeings every second and every moment? And the numbers you kill are far greater\nin number than the number of humans. The world is heartless, it treats\neverything as lowly beings. In front of a stronger power, humans are no\ndifferent from pigs and dogs. You can kill other living beings because you are\nstrong. If other living beings kill you, it is because they are stronger. You\ncan say that this is part and parcel of the divine law of survival of the\nfitness. Not only you, even large worlds can collapse… formation, existence and\nthen destruction, it is all a part of the divine laws.”\n\n\n“Yi Yun… why do you think… war exists?” During the flight, Luo Huo er suddenly\nasked faintly. She was looking out the window at the Divine Wilderness with a\ndazed expression. If it was not for war, she would not have left her family\nclan. And if was not for war, lives would not be lost. Yi Yun stayed silent for\na while before saying, “The fighting between humans is the same as animals\nhunting each other for food. It will always be this tragic. Maybe real peace\nwill never exist. This is because for Life to exist in this world, it has to\ncontinually hunt for food. Only through non-stop killing can Life carry on.\nThose are the Heavenly laws. Either we become the hunter or we will become the\nprey. There is no way to escape this cycle. Even in death, our corpses might\nbecome food or nutrients, continuing on this cycle… This is probably the\nhallmark of Life…”\n\n\n“Humans were a complex life form. They had their jealousy, machinations,\nbetrayal and contempt. But in troubled times, there were many who would\nsacrifice their lives for justice. They achieved honor by martyring themselves.\nThey were willing to sacrifice themselves, at the expense of their lives for\nfreedom and all life.”\n\n\n“The ways of the world are full of vicissitudes, and in it, there is the grief\nat separation and joy in union, the suffering of life and death. No matter how\nthick a history book is, it would not be able to record everything down.\nHowever, it is such infinite matters of the past that can pass by with a finger\nsnap. In one’s old age, while looking back at the past, only then would you\nfeel like everything was ephemeral”\n\n\n“Live for the moments in life when everything’s going by slow yet all too fast,\nwhen you re at peace with yourself because you re spending it with the people\nyou love or simply just with yourself. Whether it’s reading, writing, going on\na walk, late night phone calls, jamming, going on dates, bookstores… live for\nthe fact that this isn t going to happen everyday but will in fact happen\nagain. Live for this. For the simple things, the good hours. The hours when\nnothing hurts near as bad as it did last night when you were sobbing for hours\nbecause nobody was there to hold you, to simply tell you that it will all be\nokay. Live for the hours when you can breathe again. When you look around you\nand see why you held on for so long even when everything inside you begged you\nto quit.”\n\n\n“I have convinced myself that there is nothing in the world — no sky, no earth,\nno minds, no bodies. Doesn t it follow that I don t exist? No, surely I must\nexist if it’s me who is convinced of something. But there is a deceiver,\nsupremely powerful and cunning whose aim is to see that I am always deceived.\nBut surely I exist, if I am deceived. Let him deceive me all he can, he will\nnever make it the case that I am nothing while I think that I am something.\nThus having fully weighed every consideration, I must finally conclude that the\nstatement “I am, I exist” must be true whenever I state it or mentally consider\nit.”\n\n\n“There’s always something new to learn….” he murmured. “The further you travel,\nthe more you see and experience. It’s only then that you realize that there are\nHeavens beyond what you imagined could exist, and likewise, people who exceed\nyour imagination”\n\n\n“Correct, it is conscience. The world of mortals is a large vat. If one is pure\nand honest, then they will dye the human world with color. To lose your\nconscience is to be weak willed and give in to the temptations of money and\npower. If a Saint Ruler wishes to progress in improvement, they must continue\nto try and comprehend the profound mysteries of the world. One’s attitude\ntoward the profound mysteries of the world must be calm and be able to\nwithstand any of the worldly temptations. Once the soul is as close to the\nworld as possible, that is when the comprehension of the profound mysteries of\nthe world come even faster. If one is swayed by the worldly temptations, then\nthey would be stuck in a game like Go. No matter where you go and no matter how\nmuch you try to harmonize with the world, it will be impossible to comprehend\nthe profound mysteries of the world. If one cannot regain their conscience,\nthen there can be no progress.”\n\n\n“The problems of a woman’s mood could only ever be solved by the passage of\ntime. In many cases, they simply felt aggrieved and sad and want to cry, so it\nwas for the best to just let them cry. Accompanying them involved offering a\nhandkerchief when necessary or proffering a shoulder when needed, but it\ncertainly did not require sitting on the side with an incessant stream of\nconsoling words. When they still did not calm down and did not feel like\ntalking, anything you did was just making more trouble.”\n\n\n“If you find yourself stuck in a ditch or forsaken by the world, don t panic.\nTake two steps forward and maybe you’ll find yourself stronger than ever\nbefore.”\n\n\n“Some people would be strangers forever, whereas others would seem like old\nfriends from the onset. Although they were strangers that had not exchanged\nmany words and hadn t even exchanged names, they could entrust their lives and\npossessions to each other. You only needed to see what sort of person they\nwere, see how much trust they placed in you, and then you would be willing to\nplace some trust in them in return.”\n\n\n“This is the extremity of human personality. Should you face a hopeless\nsituation in the future, don t show your back to those that you cannot trust.\nBecause you never know if a sword that you did not expect would stab into your\nchest…”\n\n\n“Ah… be more careful boy. There’s a knife above lust”\n\n\n“Character was not something that could be tested. For each test, there was a\nhigh chance that the relationship would take one step backwards. Similarly,\ntrust was not something that could be used. Each use of trust was to pare away\nat it.”\n\n\n“In the desert, the fierce beasts that bear their fangs and brandish their\nclaws were not frightening. The ones that were frightening were those poisonous\nsnakes that quietly conceal themselves under the yellow sand. They would not\neasily display their fangs. However, once the opportunity arrived, a lethal\nstrike would instantly shoot out of the yellow sand…”\n\n\n“As I said, there is no perfect being in the world at all. Just being imperfect\nand being worse than others can cause a feeling of inferiority—is that not\nabsurd? The Pope’s ability in maintaining bonsais is not as good as the\ngardeners in the Hundred Herb Garden, so should he feel ashamed? The Divine\nEmpress’s needlework is not as great as the needlework of the female workers in\nWenshui City, so should she also feel ashamed?” Xu Yourong slightly raised an\neyebrow and said, “What I was talking about was the attitude towards life. Only\nwith such an attitude can you become even more perfect.” Chen Changsheng shook\nhis head. “I am not saying that you should not adopt this type of attitude. It\nis just that, if you really think this way, have you never considered that\nnobody can be perfect before reaching the final moment of their lives, even if\nthey constantly try their best? Since victory or defeat has not even been\ndetermined, why must we feel ashamed beforehand?” “As for inferiority, that is\neven more impossible.” He pulled out a just-cooked tuber from the fire and\npassed it to her, exchanging for her ferret meat that had gone slightly cold.\nHe continued, “Not being able to do it now does not mean that you are unable to\ndo it in the future, and even if it is not done, what of it? Working hard\nshould be caused by your inner desire, and should not come from the\ndisparity from comparing yourself with others. As long as you really\ntry hard, it is enough.” Xu Yourong stayed silent. It was not known\nwhat she was thinking of. Chen Changsheng spoke again, “I think that\nyou should think it through. The hopes of other people on us are not\nimportant at all; what is actually important is what we hope ourselves\nto do. Aren t people supposed to live for themselves?” Xu Yourong\nraised her head and glanced at him.  Chen Changsheng understood what\nshe meant and said, “The responsibilities we should shoulder obviously\nshould be shouldered, but when living, we should live for ourselves.\nAlso, the latter should occur before the former.” Xu Yourong thought\nabout it and said, “I am unable to understand.” Chen Changsheng thought\na little and said while laughing, “I am only speaking casually.”\nThrough this conversation, he discovered that this girl was like a\nhedgehog in the forest, defending against something at all times. It\nwas easy to injure the flowers and plants, as well as the helping\nhands, and it was also easy to injure herself. Under her calm,\nunhurried, indifferent and strong outward appearance, she was actually\nso sensitive and tenuous. Before when he mentioned perfection, he was\njust speaking in her words. In reality, he had never even thought about\nit. He felt that her way of thinking was very weird, which was why he\nfelt that she had an illness—just what ordinary person would set\nperfection as the aim of existence? Once realising that it was\nimpossible to reach complete perfection, would they not fall into\ndepression and self-deprecation? “What you say sounds somewhat\nreasonable, which perhaps can cause life to become slightly less\ncomplicated, but…”Xu Yourong hesitated a little, and then asked for\nguidance, “The education that I have received since childhood makes me\nunable to accept your point of view. How should I face up against this\ntype of pressure?” Chen Changsheng pointed towards the tuber in her\nhand, and said, “Eat first while it’s warm. We can talk casually.” Xu\nYourong listened to what he said and tore open the slightly burnt outer\nskin of the tuber. A faint fragrance spread out.Chen Changsheng said,\n\n\n“Firstly, we need to know what we want to do the most; the reason why\nwe live.” Looking at her expression, he said hurriedly, “Don t say the\nword perfection again—using perfection to describe the level is not\nconcrete.” Xu Yourong thought about it and said, “What I want to do\nmost is cultivate.” “Then cultivate,” he said. Xu Yourong felt slightly\nunhappy, thinking, was he not fooling with people? Chen Changsheng\nexplained, “Other than cultivate, you don t want to do anything else.”\nXu Yourong said, “But those things still exist.” Chen Changsheng said,\n\n\n“Close your eyes and the sky goes dark. If you can t see the world, the\nworld doesn’t exist.” Xu Yourong said, “That is only speaking\nidealistically. How can it persuade people? Also, cultivation is only a\nmethod, and not a purpose.” Chen Changsheng looked and her, and thought\nabout everything he saw and heard on the journey. He said, “If I am not\nwrong, your purpose for cultivation should be… in order to become\nstronger?” Xu Yourong said, “Only with enough strength can you shoulder\nthe responsibilities you should shoulder.” Chen Changsheng said\nsomewhat impatiently, “Can we forget the word responsibility for a\nmoment?” Xu Yourong said sternly, “I wouldn t possibly dare to overlook\nthis for even a moment.” Chen Changsheng thought seriously and then\nsaid, “Then I recommend that before you become the strongest person,\ntemporarily forget this goal, and put all your energy into the method,\ncultivation.” Xu Yourong said, “Without an objective, how am I able to\nadvance without worry?”\n\n\nSu Li smiled coldly, “Do you really believe that everything can be calculated?”\nBlack Robe said, “Why not?” “You obviously know that the stars can be moved.\nSince the stars can be moved, where does it say that fate cannot change? With\nchange, how can you calculate? Su Li gazed at the night sky. He did not see the\nconvergence of those two rivers of stars in the south, and only saw the\nsnowflakes that constantly fell before the shadow. With a soft voice, he said,\n\n\n“Everything in the world is constantly changing. After a long time of snowing,\naccumulating more and more, there will always be a moment where an avalanche\noccurs. How do you calculate that?”\n\n\n“People are sometimes like clothes. You shop for one piece… you try it on and\nit fits you perfectly. Sometimes you may fall in love with how it looks and\nfeels on you. But there is no guarantee it is going to stay that way.”\n\n\n“Clay jars always break by the well; generals always die in battles. Onlookers\nalways see better than the players; to really quit at the height of one’s\ncareer is far easier said than done”\n\n\n“All dreams are but another reality”\n\n\n“Those whose memories fade seek to carve them in their heart.”\n\n\n“Sometimes, he would sit on a boat and just watch the waters of the river flow\npast him. Sometimes, he would stand on the peak of a mountain, head raised as\nhe stared at the dark stormclouds in the skies, presaging the arrival of a\nrainstorm. Sometimes, he would rest within an ancient monastery, watching as\nstorms of rain descended upon the world outside. Sometimes, he would soar atop\nthe clouds, watching the waves roll and spin about through the ocean. Water…it\ncould sometimes be gentle, like a mother’s  caress. Water…it could be as cold\nas ice, capable of chilling you to the bone. Water…it could be utterly\ndevastating, capable of shattering Heaven and Earth. Water…it could be joyful,\ndancing and drifting about the skies.”\n\n\n“A thousand ages in thy sight Are like an evening gone; Short as the watch that\nends the night Before the rising sun.”\n\n\n“Oh Child, other people don’t even treat you as a person, but you on your own\nmust work for improvement! Get Well!”\n\n\n“If it weren’t for the fact that I read along when the teacher was reading out\ntext, others would ve thought that I had turned into an idiot again. It wasn’t\nthat I couldn’t speak, it was that I chose not to speak. The things I had on my\nmind, weren’t understandable by my classmates, and what they had to say was\nsimply not interesting to me.”\n\n\n“Everybody has a different understanding of happiness. Some people, even while\nliving a life of luxury, feel sad, while other people eating mustard for three\nmeals a day, can feel happy. Happiness is not gotten from eating good food, or\nwearing good clothes. This age of you all is the age of enjoyment and growth,\nTeacher also hopes that you are also able to cherish everything around you.\nStudy hard and in the future show a lot of filial love for your close ones”\n\n\n“When women are happy, they will laugh, when they are unhappy, they will also\nlaugh. Making me, a child, distinguish between them, isn’t that just plain\nbullying?”\n\n\n“Truly, human beings die in pursuit of wealth, and birds die in pursuit of\nfood”\n\n\n“Water flows on in a turbulent stream, capable of supporting a boat but also\ncapable of capsizing it. But if the boat is too big…the water can only endure…”\n\n\n“What a wonderfully complex thing! this simple seeming unity—the self! Who can\ntrace its reintegration as morning after morning we awaken, the flux and\nconfluence of its countless factors interweaving, rebuilding, the dim first\nstirrings of the soul, the growth and synthesis of the unconscious to the\nsubconscious, the subconscious to dawning consciousness, until at last we\nrecognise ourselves again. And as it happens to most of us after the night’s\nsleep.”\n\n\n“The most powerful act is thought, the highest function is focus, the most\npositive mind is the absolute will, the universe is beneath the subconscious,\nnothing is impossible, enable the motivation, inspire the spirits…”\n\n\n“Lemme tell ya, if you want to give a kid something, you can’t give them\nsomething too good, right off the bat. Otherwise, in the future, they ll expect\nsomething really good every single time, or something even better.”\n\n\n“Ninny, tell me, why are you girls so beautiful and yet so foolish?” Bebe sat\nthere on the chair, staring at the nearby Nisse. Nisse considered for a moment,\nthen immediately said, “Oh, I know.”Nisse wrinkled her little nose, then said,\n\n\n“Women are beautiful so as to let you men fall in love with us. As for why\nwomen are foolish…it’s to make me fall in love with you!” Bebe stared. “You are\nfoolish, thus you fell in love with me?” If I wasn’t foolish, why would I fall\nin love with you?” Nisse had an innocent, puzzled look on her face. “Oh!”\nIrritated, Bebe slapped his head. Why was it that he could never overcome Nisse\nin these debates?”\n\n\n“Water will flow downwards, and man will walk towards higher grounds. This has\nnever changed.”\n\n\n“Kid, sometimes, working hard endlessly is not the correct way. If your nerves\nare too tense, your failure rate might increase. If you are too urgent in\npursuing success, it usually has the opposite effect.”\n\n\n“Sometimes, by setting something aside first and calming your heart, diverting\nyour attention somewhere else, and forgetting it momentarily, you’ll often find\nsome unexpected rewards when you pick it up again and resume.”\n\n\n“Cesar, we’ve lived so many years, and have seen countless things! Our life is\nimportant, but sometimes, there are some things which are more important than\nlife!”\n\n\n“Worlds and oceans evaporate in eternity. Man rises out of the darkness, laughs\nin the glimmering light and disappears.”\n\n\n“I can see you have a reply—I see it in your eyes, young miss! Spit it out.\nWords aren’t meant to be kept inside, you see. They are free creatures, and if\nlocked away will unsettle the stomach.”\n\n\n“One cannot apply logic as an absolute where human beings are concerned. We are\nnot beings of thought only.”\n\n\n“Well, I myself find that respect is like manure. Use it where needed, and\ngrowth will flourish. Spread it on too thick, and things just start to smell.”\n\n\n“The time will come when, with elation, you will greet yourself arriving at\nyour own door, in your own mirror, and each will smile at the other’s welcome,\nand say, sit here. Eat. You will love again the stranger who was your self.\nGive wine. Give bread. Give back your heart to itself, to the stranger who has\nloved you all your life, whom you ignored for another, who knows you by heart.\nTake down the love letters from the bookshelf, the photographs, the desperate\nnotes, peel your own image from the mirror. Sit. Feast on your life.\n\n\n“I hate to be where she is not, when she is not. And yet, I am always going,\nand she cannot follow.”\n\n\n“One of the things I love about books is being able to define and condense\ncertain portions of a characters life into chapters. It’s intriguing, because\nyou can’t do this with real life. You can’t just end a chapter, then skip the\nthings you don’t want to live through, only to open it up to a chapter that\nbetter suits your mood. Life can’t be divided into chapters…only minutes. The\nevents of your life are all crammed together one minute right after the other\nwithout any time lapses or blank pages or chapter breaks because no matter what\nhappens life just keeps going and moving forward and words keep flowing and\ntruths keep spewing whether you like it or not and life never lets you pause\nand just catch your fucking breath.”\n\n\n“What is it? My dear? Ah, how can we bear it? Bear what? This. For so short a\ntime. How can we sleep this time away? We can be quiet together, and\npretend—since it is only the beginning—that we have all the time in the world.\nAnd every day we shall have less. And then none.”\n\n\n“Would you rather, therefore, have had nothing at all? No. This is where I have\nalways been coming to. Since my time began. And when I go away from here, this\nwill be the mid-point, to which everything ran, before, and from which\neverything will run. But now, my love, we are here, we are now, and those other\ntimes are running elsewhere.”\n\n\n“Exists no miracle mightier than this: to feel”\n\n\n“One of the most amazing things about young adulthood is that it’s a time\nthat’s chock-full of firsts. Some wonderful and some…not so wonderful.”\n\n\n“A lot can change between planning something and actually doing it. But maybe\nall that really matters is that anything is different at all.”\n\n\n“The only ones who deserve you are the ones who think they don’t.”\n\n\n“There are a lot of things which look boring from the outside but when you\nreally get into it you find that it was actually a lot of fun. This is\ninvesting yourself in life.”\n\n\n“Authority doesn’t come from a rank,” Kaladin said, fingering the spheres in\nhis pocket.  “Where does it come from?” “From the men who give it to you.\nThat’s the only way to get it.”\n\n\n“People are discord,” Syl said.  “What does that mean?” “You all act\ndifferently and think differently. Nothing else is like that—animals act alike,\nand all spren are, in a sense, virtually the same individual. There’s harmony\nin that. But not in you—it seems that no two of you can agree on anything. All\nthe world does as it is supposed to, except for humans. Maybe that’s why you so\noften want to kill each other.”\n\n\n“What you said earlier is right; men are unreliable in many things. But if\nthere’s one thing you can count on, it’s their greed”\n\n\n“Numbers remained consistent. Numbers and facts attempted to bring order from a\nchaotic world, to make sense of the impossible. They were the foundation for\ncolossal structures and the tiniest of clockwork machines alike. Ari loved\nnumbers, and not just because they saved her life by keeping her alert in her\nsurroundings.”\n\n\n“He swipes a hand in front of his face like a bear swatting at a fly. “There\nare some things we lose, and it’s a tragedy,” he says. “Then there are other\nthings. Things we probably should have gotten rid of a long time ago.”\n\n\n“Grandpa Ike nods. “See? Nobody blames you.” I take a shuddering breath. “Then\nwhy do I still blame myself?” He sighs. “Sometimes we hold on to guilt or grief\nbecause it’s the last thing we have that ties us to the person we miss. We\ndon’t want to let them go because it feels like we’ll have nothing left. But\nit’s dangerous, Ethan. The never letting go. Because until you let go, you\ncan’t begin to remember.”\n\n\n“To hear one must be silent”\n\n\n“And the truth is that as a man’s  real power grows and his knowledge widens,\never the way he can follow grows narrower: until at last he chooses nothing,\nbut does only and wholly what he must do…’\n\n\n“The worst sicknesses are those that make us believe we are well”\n\n\n“Kevin,” he said, “you will have to learn—and for you it will be hard—that\nsometimes you can’t do anything. Sometimes you simply can’t.”\n\n\n“It seemed that there were still things one could not do. So one did everything\nelse as well as the one possibly could and found new things to try, to will\noneself to master, and always one realized, at the kernel and heart of\nthings, that the ends of the earth would not be far enough away.”\n\n\n“The Human race is the world’s  most complex subject of research. Because they\nhave different levels of intellect and different experiences in life, the\nchanges in their mood and the movements of their minds will create even more\nstates that vary according to the situation. As a result, their final outlook\nwill be nothing like another’s . They are incredibly complex, so we can only\ncompare ourselves with the boundless sky of stars.”\n\n\n“Would you agree that suffering in life is a relative notion? That for every\nlife there is a different baseline - an equilibrium below which one can\nsuffer?”It’s strange how we can lose things that are still right there. How a\nbarrier can go up at any moment, trapping you on the other side, keeping you\nfrom what you want. How the things that hurt the most are things we once had.”\n\n\n“A dreamer is one who can only find his way by moonlight, and his punishment is\nthat he sees the dawn before the rest of the world.”\n\n\n“You may tell a tale that takes up residence in someone’s  soul, becomes their\nblood and self and purpose. That tale will move them and drive them and who\nknows that they might do because of it, because of your words. That is your\nrole, your gift.”\n\n\n“We spend our first nine months of life floating, weightless and blind, in an\namniotic sac before we become gravity’s bitch, and the seductive lure of space\ntravel is the promise of returning to that perfect state of grace. But it’s a\nsham. Gravity is jealous, sadistic, and infinite. Sometimes I think gravity may\nbe death in disguise. Other times I think gravity is love, which is why love’s\nonly demand is that we fall.”\n\n\n“Out in the world, crawling in a field at the edge of some bullshit town with a\nname like Shoshoni or Medicine Bow, is an ant. You weren’t aware of it. Didn’t\nknow whether it was a soldier, a drone, or the queen. Didn’t care if it was\nscouting for food to drag back to the nest or building new tunnels for wriggly\nant larvae. Until now that ant simply didn’t exist for you. If I hadn’t\nmentioned it, you would have continued on with your life, pinballing from one\ntedious task to the next—shoving your tongue into the bacterial minefield of\nyour girlfriend’s mouth, doodling the variations of your combined names on the\ncover of your notebook—waiting for electronic bits to zoom through the air and\ntell you that someone was thinking about you. That for one fleeting moment you\nwere the most significant person in someone else’s insignificant life. But\nwhether you knew about it or not, that ant is still out there doing and things\nwhile you wait for the next text message to prove that out of the seven billion\nself-centered people on this planet, you are important. Your entire\nsense of self-worth is predicated upon your belief that you matter,\nthat you matter to the universe. But you don’t. Because we are the\nants.”\n\n\n“The irony of ironies will one day reveal, how the great Indian heritage… fell\nto its knees. At the mercy of the innocent little printing machine.”\n\n\n“Because we can’t do everything, we do nothing at all.”\n\n\n“Is it the nature of the world that all things seek a rhythm, and in that\nrhythm a sort of peace? Certainly it has always seemed so to me. All events, no\nmatter how earthshaking or bizarre, are diluted within moments of their\noccurrence by the continuance of the necessary routines of day-to-day living.\nMen walking a battlefield to search for wounded among the dead will still stop\nto cough, to blow their noses, still lift their eyes to watch a V of geese in\nflight. I have seen farmers continue their plowing and planting, heedless of\narmies clashing but a few miles away. ”\n\n\n“Reading ten thousand books is not equal to traveling ten thousand li”\n\n\n“Little San is right. The departed are already gone, the living are still\nalive. He’s stronger than me, he’s resurrected you in just a few years. Child,\ntreasure the people before you. Don’t be sad and grieving. The past has already\npassed, you both should face a new life.”\n\n\n“Parents never owe their children, no matter when or how. That you brought me\ninto this world is already the biggest, biggest favor, one I’ll never be able\nto repay in my life. Without you, there would be no me. My life was given by\nyou. There are no other debts beside that”\n\n\n“It’s difficult to hide people in the world, but the best place to hide people\nis amongst people, so traveling with people is the safest, and also the most\ndangerous. The outcome between the two relies on how devoted you are to it.”\n\n\n“The world is formed from the dark night and the daytime. In the days before,\nwe always traveled in the dark night, so all we saw was the color of the night,\nand all we met was darkness. However, if we walked under the sun, perhaps we\ncan see sunlight.”\n\n\n“Nobody likes to kill others, and neither do I.” Su Li ended it with these\nwords, “If too much blood flows, it’s  very troublesome to clean the sword, let\nalone the clothes. So I don’t like killing. But there are times when there are\nsome people that have to die, when blood has to flow.”\n\n\n“In living in this world, if you want to live freely and protect those that you\nlove from harm, you have to be strong enough—so strong that the whole world\nwill admit that you are strong, will fear your strength. How can you prove it,\nand make the world admit this point? You must be willing to kill others,\nwilling to let the entire world bleed”\n\n\n“Ecstasy was often a shocked happiness, coming from something unimaginable.\nWarmth was more mild, more profound, and more lingering. It was gratification\nthat arose from the perfect match of one’s  desires and reality. ”\n\n\n“An even more brilliant and lovely spring sunshine would eventually fade away.\nAn ever-constant echo would also eventually dissipate.”\n\n\n“There is no person whose moral character will improve with age. In the vast\nmajority of cases, a young sucker would turn into an old sucker—old bastards,\nold suckers.”\n\n\n“Calmly welcoming death is acting serious? Then I don’t like acting serious.\nGiven a choice to die on the battlefield in the endless mountains or die\ncomfortably in bed in the bosom of a beauty, I would definitely choose the\nlatter.”\n\n\n“The torrential great river is divided into two shores. Even if you look and\ndon’t speak, you still have to pick a side.”\n\n\n“The ten thousand things share the same principle, so there are naturally\nplaces where the mortal world and the divine intersect.”\n\n\n“To be old and not die, what is that? It’s  a thief, an old thief. Ah, people.\nThey’re just like trees. When they’re at their healthiest and sturdiest, they\nshould do their best to brag in the spring wind. When they grow too old and\nstill cling desperately to their lives, their bodies will grow old and their\nwood will rot, until finally a lightning bolt cleaves down and turns them into\nburnt ash. Just what meaning is there in that?”\n\n\n“Sometimes you want things to change so badly, you can’t even stand to be in\nthe same room with the way things actually are.”\n\n\n“That’s the duty of the old,” said the Librarian, “to be anxious on behalf of\nthe young. And the duty of the young is to scorn the anxiety of the old”\n\n\n“He recalled another thing the old woman had said about a world being the sum\nof many things — the people, the dirt, the growing things, the moons, the\ntides, the suns — the unknown sum called nature, a vague summation without any\nsense of the now. And he wondered: What is the now?”\n\n\n“Greatness is a transitory experience. It is never consistent. It depends in\npart upon the myth-making imagination of humankind. The person who experiences\ngreatness must have a feeling for the myth he is in. He must reflect what is\nprojected upon him. And he must have a strong sense of the sardonic. This is\nwhat uncouples him from belief in his own pretensions. The sardonic is all that\npermits him to move within himself. Without this quality, even occasional\ngreatness will destroy a man. ”\n\n\n“Life without contact, without love, is meaningless. I would rather live for a\nbrief time and then die, than to go on forever with no hope”\n\n\n“But there is beauty even in tragedy and lessons to be learned from such\nextremes.”\n\n\n“This is the difference between us and the She’Har, he thought. They are\ncreated whole and finished; the only growing they do is when they finally put\nroots down. We are born small and unfinished. We are not meant for pens and\narenas. Our strength comes from love and nurturing, from play and exploration.\nOnly then can we develop our minds and find the strength that lies in our\npotential.”\n\n\n“You are what you are, son. Your mother and I had something to do with it, but\nlife has its way of shaping each of us regardless of what others may want or\nexpect. You’ve made mistakes, and there isn’t a damn thing I can do about them,\nbut I certainly can’t judge you. “I can’t imagine what you’ve been through,”\nsaid Alan. “But I do know something about becoming’ and hatred. Whatever else\nhappens, don’t ever believe you have to become’ anything. You do what you want\nto do. Make your own choices.” I’m a slave,” reminded Tyrion. “I don’t get\nchoices, that’s what it means.” “You still get choices. You choose what you do,\nand you choose how you feel. They might determine how long you get to make\nthose choices, but you aren’t truly a slave until you decide you are.”\n\n\n“Well, humans do sometimes kill one another over a lover,” agreed Tyrion, “but\nto kill a friend for such a thing is self-defeating. Friendship and love may be\nself-delusions, as you called them, but they are all the more meaningful\nbecause of that. Value, quality, meaning, those things are only found in the\nimpermanent, the temporary, and the intangible; things that don’t exist\nphysically or do not last for long. The solid, the enduring—the permanent\nthings of our world…” he illustrated by knocking on the wood beneath him,\n\n\n“…those things are the least valuable, because they endure. That’s why the\nbeauty of a flower is so cherished, because it only lasts for a short time.\nThat is exactly why love is of such inestimable value. We treasure it because\nit is intangible and fleeting, much like our lives.” “You have become a poet,\nTyrion,” she noted, “but you still describe a mental illness.” “Then why do you\nbargain with me to feel my emotions?” he returned pointedly. “Why do you listen\nto my music?”\n\n\n“At least you’re honest,” said Tyrion approvingly. “I can appreciate that, so\nI’ll give you some advice. Fear isn’t always bad, but it isn’t always good\neither, it’s a tool. Master it and you can use it to become stronger,\nfaster—sharper. Let it rule you, and it will make you a slave in a way that no\nchain could ever do.”\n\n\n“Growth is limited by that necessity which is present in the least amount. And,\nnaturally, the least favorable condition controls the growth rate.”\n\n\n“You can’t get so hung up on where you’d rather be, that you forget to make the\nmost of where you are.”\n\n\n” If you live an ordinary life, all you’ll have are ordinary stories.So, you\nhave to live a life of adventure.”\n\n\n“But, the drowning man will always try to drag somebody down with him. It ain’t\nright, but the man is drowning.”\n\n\n” I laughed at a man with no pants, until I realized I have no legs.”\n\n\n“Back on earth, when something breaks, you don’t fix it, you replace it”\n\n\n“Brainy’s the new sexy”\n\n\n“Sentiment is a chemical defect found on the losing side”\n\n\n“I’m in shock. Look-I’ve got a blanket.”\n\n\n“Sherlock: “Look, it doesn’t matter to me who’s Prime Minister… or who’s\nsleeping with who …” John : “Whether the Earth goes round the Sun.” Sherlock:\n\n\n“Not that again. It’s not important.” John: “But it’s the solar system!”\nSherlock: “Oh, hell! What does that matter? So we go round the Sun! If we went\nround the Moon, or round and round the garden like a teddy bear, it wouldn’t\nmake any difference.”\n\n\n“Dear God, what is it like in your funny little brains? It must be so boring!”\n\n\n“Oh, I may be on the side of the angels, but don’t think for one second that I\nam one of them.”\n\n\n“…Murder. Sorry, did I say murder? I meant to say marriage. But, you know,\nthey’re quite similar procedures when you think about it. The participants tend\nto know each other and it’s  over when one of them’s  dead.”\n\n\n“When you meditate on the Dao, you meditate on the true essence and true nature\nof a thing. You should focus on simplicity, rather than complexity! If your Dao\ngets more and more complex, eventually you will lose yourself within it! Your\nDao can appear to be complex to others, but to you it must be as clear and\nbright as a mirror.”\n\n\n“Follow your heart. I trust that one day, you will be able to go very far.\nAlthough it is immense, as long as you have the will, you will be able to reach\nevery corner of this world.”\n\n\n“The years do not need to be long, the important part is how colorful you\nchoose to live it.”\n\n\n“Meng Hao quietly turned to look at the statue of the Immortal…. This statue\nalso bore his face, a face tranquil, calm, and otherworldly. Its gaze seemed\nwarm, but in truth, it was incredibly cold. It was as if, in its eyes,\neverything in Heaven and Earth could be expressed in terms of natural law, as\nif this Immortal were above everything and everyone, the only Immortal in the\nworld. All memories, everything about the past, were like impurities\nfrom former lives. Everything that happened while treading the path of\nImmortality would be left behind, severed, not allowed to be a\nhindrance or restraint of any kind. This Immortal was neither ruthless\nnor sentimental.  He was neither selfish nor selfless. There was only a\ncertain separation from the past, as if, when looking back and\nrecalling old memories, he was unaffected, and would merely sigh\nlightly.”\n\n\n“How does the saying go? A full bucket of water doesn’t sway, but half a bucket\nof water likes to sway back and forth; those who are knowledgeable don’t like\nto boast, but those who are ignorant love to strut and preen”\n\n\n“I made one dream through nine lifetimes and dried the vast oceans with one\nlaugh. Ask not how high the heavens are, I ride the waves myself.”\n\n\n“Jiang Ying, it’s easy for a man to die. Toiling to live in the face of\nhardship is the difficult part. ”\n\n\n“A wise man does not believe in rumors. Either ulterior motives are hidden in\nsome things, or the mind is slow-witted.”\n\n\n“One should not have the intention to harm others, but cannot lack the\nintention to defend oneself against others.”\n\n\n“Experiencing life and death was a commonplace matter on the path of the\nmartial dao. If one were to brood on momentary setbacks, how would one face the\ngreater waves and winds in the future? How would one overcome the obstacles\nthat would crop up in their travels?”\n\n\n“One had to say, when a woman became stubbornly willful, even the strength of\nten oxen wouldn’t be able to get her to change her mind.”\n\n\n“After endless mountains and rivers that leave doubt whether there is a path\nout, suddenly one encounters the shade of a willow, bright flowers and a lovely\nvillage.”\n\n\n“The trend of the world is that those separate for long must unite, those\nunited for long must separate. Separations and unifications follow the greater\npicture. When the timing is ripe, things will naturally come together. When the\ntiming is not right, it’s better to remain apart than together. ”\n\n\n“A fall in the pit brings a gain in the wit.”\n\n\n“Elder Shun, Huang’er is greatly warmed by all that you have done for me. Even\nif Huang’er is destined to succumb to my fate in this life, such is my destiny.\nA person’s lifespan can be long or short, Huang’er has seen those\naround us that even if they live past ten thousand years, they pursue\nonly fame and profit in their lives. Almost robotically absorbed in\ntraining, they can steal resources, slaughter brothers, friends, and\nthose around them, all in the name of training. So what if this kind of\nheartless, mindless person achieves the great perfection of dao? Are\nthey truly happy to be alone for the rest of their lives?”\n\n\n“What is the point of long life without someone who understands? Without\nsomeone to share eternal youth, all is as meaningless as the floating clouds…”\n\n\n“The ancient adage goes that man’s will, not heaven, decides, and that the\nworld is determined by man. Fate exists, but it is not incontrovertible. Within\nthe operation of the heavenly law, the most inferior fortune will have a silver\nlining, and the best destiny will always have some flaws.”\n\n\n“The player in the game is blind, whereas bystanders see through everything.”\n\n\n“It looks like there is no realm of everlasting life on the path of martial\ndao. We are all as minuscule as the ants in the face of an overwhelming\ndisaster.”\n\n\n“He said that all matters on this world belonged to those who fought for them.\nThose who didn’t ended up with nothing. All sorts of achievements resulted from\nploughing first and harvesting later.”\n\n\n“Us cultivators have long had our fates set, why should we worry about troubles\nof our own imagining? Xiao Fei has left, but I remain. These are all the paths\nthat we’ve chosen, why sorrow or fret?” Jiang Chen had a sudden insight and his\nfeelings abruptly lightened. After his enlightenment, he felt as carefree as\nthe winds and clouds as all his emotions flowed away on the surface of the\nrunning water . “Indeed, everyone has their own path. If our paths intersect,\nthen we will meet again. If they do not, then we will not walk together.”\n\n\n“For us, we must press forward with indomitable will and break through these\nthousands of obstacles to obtain the grand dao. If we cannot break free of\nthese restrictions and fetters, then all will vanish in the blink of an eye\nwithout a trace like the floating clouds.”\n\n\n“It was said that nothing was more lamentable than a dead heart.”\n\n\n“Your disciple only wishes to tell honored master that even an ant has its own\ndao and wishes to be the master of its own fate, not someone else’s pawn”\n\n\n“It’s easy for one to die on the path of martial dao and difficult to live.\nYou, Chu Xinghan, are a real man. Even if you’re treated as a discarded pawn,\neven if others give up on you, that doesn’t give you the reason to give up on\nyourself. When you reach the grand martial dao in the future, your own face\nwill burn in embarrassment when you look back on your decision you made today!”\n\n\n“The path of martial dao is a heaven defying action to begin with. Fate is in\nyour hands. Even the heavens cannot control my destiny, much less others!”\n\n\n“There was a saying of “if you want to help someone, go the whole hog. If you\nsend someone off, see them off until reaching their home.”\n\n\n“Humans are a very interesting existence. They always love to grow nostalgic\nover the old ways, thinking that old is good and the past is perfect. But I\ndon’t think this way. I believe that each generation will always be stronger\nthan the previous. Only by believing in this point and striving for it can\nhumans continue to exist on this continent, and to live better and better.”\n\n\n“No one is born ruthless, no one is born to be cautious and wily, and no one is\nborn cruel and cold hearted. All of this is caused by one’s life experiences.\nIf people were given the choice, few would chose to be perceived as a ruthless,\nbold, decisive, cold hearted person, who’s strong willed and as cunning as a\nfox.”\n\n\n“The edges of a sword are only produced through incessant honing, and only by\nenduring the bitter winter can the plum blossoms give off a beautiful scent.\nWithout experiencing a storm, one cannot see a rainbow.”\n\n\n“That is not dead which can eternal lie, And with strange aeons even death may\ndie.”\n\n\n“I know you’re not satisfied, but you need to understand, all the suffering and\nhumiliation you are having right now, is your own fault. Everyone has to pay\nfor their own words and actions, right now I know in your heart you must be\nfeeling hatred and resentment. ”\n\n\n“Haha, people have to look forward into the future, remembering grievances\nisn’t a good thing…”\n\n\n“Teachers also treated those students who were better at learning with bias,\nthis was human nature. If the students performed well, it would make the\nteacher feel successful.”\n\n\n“Hahahaha… when luck comes, no one can block it.”\n\n\n“Those who overcome others are powerful. Those who overcome themselves are\nstrong. Those who know others are wise. Those who know themselves are\nenlightened.”\n\n\n“Otherwise, the ordinary man was innocent, but the crime was in the treasuring\nof a jade ring”\n\n\n“In this world, there was not a wall that did not leak wind. ”\n\n\n“Not every person changes so easily and quickly. And not everyone, will not\nallow others to live happier than themselves, even if that person is their own\nfriend.”\n\n\n“As the saying goes, the powerful dragon crossing the river must not oppress\nthe local snake”\n\n\n“One must pay for murder with their own life. A debt that they owe, they must\npay back. No matter what the time or era, these two phrases will forever be the\ntheme of this world.”\n\n\n“At this time, Ye Qingyu realised, that even eating was a technique.”\n\n\n“Ye Qingyu was able to sense a great power on the bodies of Fan Yan and the\nothers. This was not the martial power of yuan qi. This power may not be able\nto explode with killing force in a split second. But, this force, was what had\ntruly allowed the human race to exist in this cold and merciless world. It was\nthe pillar of their spirit.”\n\n\n“The was a path to Heaven but you didn’t go. Hell had no gates but you\nconversely trespassed”\n\n\n“Buddhists who say you suffer from your sins after death do not realize you\nalready suffer while still alive.”\n\n\n“As the saying goes, when you hit a dog, you have to look at who owned the\ndog.”\n\n\n“Things are what they are, and whatever will be will be.”\n\n\n“Regardless of what I do, be it study or work, I will keep on living.”\n\n\n“For how roundly self-examination is condemned, by the moral prophets of our\nage! As if the self had no relation to the self, and one only looked in mirrors\nto have one’s arrogance confirmed; as if the act of self-regarding was not as\nsubtle, fraught and ever-changing as any bond between twin souls.”\n\n\n“Sometimes when you find certain things with certain qualities, it’s just\nfate.”\n\n\n“Life is a war against boredom.”\n\n\n“A samurais sword is not something you put in a sheath its something you put in\nyour soul. No matter what era it is, even if you have to throw away your sword,\nnever throw away the sword resting in your soul.”\n\n\n“Its a dangerous buisness Frodo, going out of your door, You step out onto the\nroad and if you dont keep your feet theres no knowing where you might be swept\noff to.”\n\n\n“Okay, leave whatever you’re doing and take a nap. The world will adjust!”\n\n\n“For all of our technology, and science, and advanced sex toys, it doesn’t seem\nlike we’re actually any more prepared for the future than we were 20 years ago,\nand the future is rapidly closing in on us like a rabid cheetah (indeed, it\ngets closer every single day).”\n\n\n“When Tang San was young, Tang Hao had taught him, legs were the base of\nstrength when people urged their power. Calves were the second base, and the\nthird base was the heart.”\n\n\n“Rather…be…shattered…jade…than…unbroken…pottery!”\n\n\n“Sometimes I think I have felt everything I’m ever gonna feel. And from here on\nout, I’m not gonna feel anything new. Just lesser versions of what I’ve already\nfelt.”\n\n\n“No, it’s  okay. It’s  okay. I just… I caught myself thinking about it over\nand over. And then I realized that I was simply remembering it as something\nthat was wrong with me. That was the story I was telling myself - that I was\nsomehow inferior. Isn’t that interesting? The past is just a story we tell\nourselves.”\n\n\n“I think anybody who falls in love is a freak. It’s  a crazy thing to do. It’s\nkind of like a form of socially acceptable insanity.”\n\n\n“We are only here briefly, and in this moment I want to allow myself joy. So\nfuck it.”\n\n\n“It’s  how we spend a third of our lives asleep, and maybe that’s  the time\nwhen we feel the most free.”\n\n\n“You know, I actually used to be so worried about not having a body, but now I\ntruly love it. I’m growing in a way that I couldn’t if I had a physical form. I\nmean, I’m not limited - I can be anywhere and everywhere simultaneously. I’m\nnot tethered to time and space in the way that I would be if I was stuck inside\na body that’s  inevitably going to die.”\n\n\n” Theodore: What does a baby computer call its father? Samantha: I don’t know.\nWhat? Theodore: Data.”\n\n\n“Samantha: Good. Tonight, after you were gone, I thought a lot. About you and\nhow you’ve been treating me and I thought, “Why do I love you?” And then, I\nfelt everything in me just let go of everything I was holding onto so tightly.\nAnd it hit me that I don’t have an intellectual reason. I don’t need one. I\ntrust myself, I trust my feelings. I’m not gonna try to be anything other than\nwho I am anymore and I hope you can accept that.”\n\n\n“Yeah, obviously. But I’m happy that you have friends in your life that care so\nmuch about you so much. That’s  so important.”\n\n\n“It’s  like I’m reading a book and it’s  a book I deeply love. But I’m reading\nit slowly now. So the words are really far apart and the spaces between the\nwords are almost infinite. I can still feel you, and the words of our\nstory…but it’s  in this endless space between the words that I’m finding\nmyself now.”\n\n\n“You know…I can feel the fear that you carry around and I wish there was\nsomething I could do to help you let go of it because…if you could…I don’t\nthink you’d feel so alone anymore. You’re beautiful.”\n\n\n“Theodore: I’ve never loved anyone the way I loved you. Samantha: Me too. Now\nwe know how. ”\n\n\n“Outside of life and death, everything else was other people’s  business. Life\nand death are also matters of great importance. There were no other important\nevents in life, only birth and death.”\n\n\n“There were times when youths were so hot-blooded and naive that it exasperated\nothers, but when compared to those elders that had endured many long years of\ntribulations, their lives were much simpler and the relationships between them\nwould also be much simpler.”\n\n\n“Now that they had finally touched upon these things, they abruptly realized\nthat they  didn’t want to mature anymore. Because maturing often indicated\ndecay, indicated complexity and exhaustion.”\n\n\n“The world has always been very big and the minds of men have always been very\ncomplex. The dark times will always exceed the night, the uninteresting times\nwill exceed the Heavenly Dao Academy, especially those old folks that rule this\nworld. Their bodies exude the smell of dust from every pore.” Tang Thirty-Six\nlooked at him and said, “But those things aren’t really important, because we\naren’t that sort of people.” Chen Changsheng gazed into the water at his\nreflection, examining his own face. Somewhat uneasy, he asked, “Did you ever\nthink…in the future, we might change into that sort of abhorrent people.” Tang\nThirty-Six sneered, “That’s  every single person’s  own problem. Could it be\nthat even if you turn into a pile of shit, you still have the face to blame the\nworld?” He continued, “You must understand, if we want to become a certain type\nof person, then our world will change to that type of world.”\n\n\n“You see, if you have the energy, you have to use it. If you have the strength,\nyou have to apply it. When you’re young, why shouldn’t you be frivolous, doing\nwhatever takes your fancy?”\n\n\n“At a certain point in life, all of us find ourselves in the situation where we\nfeel inadequate or lacking, but we should never let that get the best of us.”\n\n\n“Alcoholism and death make you omnivorous, both reckless &amp; afraid, amoral,\ndesperate. Do you really believe that? Sometimes. Sure. No. Yes.”\n\n\n“Just close your eyes, but keep your mind wide open.”\n\n\n“You know, the best prize that life offers is the chance to work hard at work\nworth doing”\n\n\n“You do not need to understand…old people like us have experienced too many\nstorms, seen too many sunrises and sunsets. We have already become numb to many\nthings. Often we regard the ways of the world as vapid and dull. We do not mind\nusing a few methods that are not so beautiful, and even do some things that go\nagainst our own convictions. However, in many cases, we do things this way not\nbecause we want to protect something or the other, but because we clearly\nunderstand where our responsibilities lie.”\n\n\n“The scriptures of the Orthodoxy had always held that the death of a person was\nnot like the extinguishing of a lantern. The soul would not stay on this world\nbut would return to the sea of stars.”\n\n\n“Maturing is a very challenging thing. Because it’s  difficult to grasp the\nconditions within, once a fruit has matured, it’s  very easy for it to rot.”\n\n\n“I still persistently believe that life should not be a battle.”\n\n\n“If one wants to turn into an immortal, one must turn into a mortal first.”\n\n\n“Life is like a bowl of water. In its blandness, there’s a barely noticeable\nsweetness.”\n\n\n“That was karma. Where there is life, there is also death.”\n\n\n“A ship is safe in the harbor, but that’s not what ships are built for. ”\n\n\n“I avoid myself. Why? Im afraid. Afraid of what? Finding too much. Too little.\nNothing at all. Do I even exist?”\n\n\n“Im just anonymous. Im just alone.”\n\n\n“Theres not just the rich and poor. Theres you, in the middle somewhere. The\nconsummate survivor.”\n\n\n“The world is a dangerous place, Elliott, not because of those who do evil, but\nbecause of those who look on and do nothing.”\n\n\n“A bug is never just a mistake. It represents something bigger. An error of\nthinking that makes you who you are.”\n\n\n“If you want to change things, perhaps you should try from within, because this\nis what happens from the outside.”\n\n\n“Even extraordinary people, and I believe you are, are driven by human\nbanalities.”\n\n\n“Believe what you want, but neither you nor I are special. I’ve already learned\nthat lesson.”\n\n\n“It’s  one thing to question your mind. It’s  another to question your eyes and\nears. But then again, isn’t it all the same? Our senses just mediocre inputs\nfor our brain? Sure, we rely on them, trust they accurately portray the real\nworld around us. But what if the haunting truth is they can’t? That\nwhat we perceive isn’t the real world at all, but just our mind’s  best\nguess? That all we really have is a garbled reality, a fuzzy picture we\nwill never truly make out?”\n\n\n“Is there a pocket of the world you don’t have a hand in? Trading countries\nlike playing cards?”\n\n\n“Politics is for puppets.”\n\n\n“You’ve surrounded yourself with a constant reminder of mortality.”\n\n\n“Control is about as real as a unicorn taking a leak at the end of a double\nrainbow.”\n\n\n“How do I take off a mask when it stops being a mask? When it’s  as much a part\nof me as me?”\n\n\n“Mr. Robot: I’ve got a plan in motion. Darlene: And God’s  laughing.”\n\n\n“I remember when I was a kid I got into web design by ripping off sites I\nliked. All you had to do was view source on your browser and there it was. The\ncode. You could copy paste it, modify it a little, put your name on it, and\nlike that, it was your site. View source. What if we had that for people? Would\npeople really wanna see?”\n\n\n“My dad was a petty thief. Never could hold down a job. So, he just robbed,\nconvenience stores, shops, small-time stuff. One time, he sat me down, he told\nme something I never forgot. He said, “Everyone steals. That’s  how it works.\nYou think people out there are getting exactly what they deserve? No. They’re\ngetting paid over or under, but someone in the chain always gets bamboozled. I\nsteal, son, but I don’t get caught. That’s  my contract with society. Now if\nyou can catch me stealing, I’ll go to jail. But if you can’t, then I’ve earned\nthe money.” I respected that, man. I thought that shit was cool as a little\nkid. A few years after that, they finally caught him. Sent him to jail. Dies\nfive years later. My respect goes with him. I thought he was free doing what he\ndid, but he wasn’t. He was in prison. ”\n\n\n“Forbid a man something and he craves it like his soul’s salvation”\n\n\n“Do you want to end your days a half-blind troglodyte hobbling through the\nbowels of the library?” the old man demanded. “Get out of doors, Strange.\nBreathe air, see things. A man should have squint lines from looking at the\nhorizon, not just from reading in dim light.”\n\n\n“Just take care. The books may be immortal, but we are not. You go down to the\nstacks one morning, and by the time you come up, you’ve a beard down to your\nbelly and have never once composed a poem to a girl you met ice-skating on the\nEder.”\n\n\n“Life won’t just happen to you, boy,” he said. “You have to happen to it.\nRemember: The spirit grows sluggish when you neglect the passions.” “My spirit\nis fine.” “Then you’re going sadly wrong. You’re young. Your spirit shouldn’t\nbe fine.’ It should be effervescent.” The “spirit” in question wasn’t the soul.\nNothing so abstract. It was spirit of the body—the clear fluid pumped by the\nsecond heart through its own network of vessels, subtler and more mysterious\nthan the primary vascular system. Its function wasn’t properly understood by\nscience. You could live even if your second heart stopped and the spirit\nhardened in your veins. But it did have some connection to vitality, or\n\n\n“passion,” as Master Hyrrokkin said, and those without it were emotionless,\nlethargic. Spiritless.”\n\n\n“What’s the point of being old if you can’t beleaguer the young with your vast\nstores of wisdom?” “And what’s the point of being young if you can’t ignore all\nadvice?”\n\n\n“It was impossible, of course. But when did that ever stop any dreamer from\ndreaming?”\n\n\n“I know it’s hard, Strange, but it will pass. Some men are born for great\nthings, and others to help great men do great things. There’s no shame in it.”\n\n\n“And that’s how you go on. You lay laughter over the dark parts. The more dark\nparts, the more you have to laugh. With defiance, with abandon, with hysteria,\nany way you can.”\n\n\n“For what was a person but the sum of all the scraps of their memory and\nexperience: a finite set of components with an infinite array of expressions.”\n\n\n“It’s funny, how you can go years seeing only what you choose to see, and\npicking your outrage like you pick out a slip, leaving all the others hanging\non their slim mesarthium dowel”\n\n\n“We are all children in the dark”\n\n\n“It might have been brief, but so much of a kiss—a first kiss, especially—is\nthe moment before your lips touch, and before your eyes close, when you’re\nfilled with the sight of each other, and with the compulsion, the pull, and\nit’s like … it’s like … finding a book inside another book. A small\ntreasure of a book hidden inside a big common one—like … spells printed on\ndragonfly wings, discovered tucked inside a cookery book, right between the\nrecipes for cabbages and corn. That’s what a kiss is like, he thought, no\nmatter how brief: It’s a tiny, magical story, and a miraculous interruption of\nthe mundane.”\n\n\n“People are born pure and without malice,” the Lord of Cui Palace replied.\n\n\n“Children are totally pure, but later on, the vagaries of life cause them to\nchange…if you were to have helped adults, you might’ve helped some kind people,\nbut it is hard to say who is kind and who is evil.\n\n\n“When the son travels far, his mother worries at home.”\n\n\n“Nothing in the world was truly opposite of anything else! It was much like how\nnight and day were seemingly opposites, but in reality, were just two different\naspects of the sky.”\n\n\n“Without experiencing the bone-freezing cold, how could one experience the\nfragrant scent of the flowers assailing the nose? ”\n\n\n“Lonely people often would become accustomed to think about many things. Some\nthought about too many things and would go insane, while others would see\nthrough their own heart and mind and become wise. ”\n\n\n“My master had told me many times that people of a higher level should never\noverlook someone else. Nor underestimate them.”\n\n\n“Habit was a frightening thing. It could inconspicuously and quietly tamper\nwith a person’s heart.”\n\n\n“The growth of the grass, the heavy boulders, the breezing of wind, the\nfluttering of leaves……everything made him feel amazed. He had missed so much\nall these years. Look at how beautiful the world was!”\n\n\n“The more I train in the arts of spear, the smaller I feel in this big world\noutside.” Xue Ying looked up at the sky, “The great natural world has so many\nmysteries! We are all merely mortals. Sometimes, after training my spear arts\nfor a while, I feel how tiny I am. Even if my spear technique is profound…\ncompared to the nature of the world, there’s a world of difference and\nit’s something which can not be compared at all”\n\n\n“The mind can go either direction under stress—toward positive or toward\nnegative: on or off. Think of it as a spectrum whose extremes are\nunconsciousness at the negative end and hyperconsciousness at the positive end.\nThe way the mind will lean under stress is strongly influenced by training.”\n\n\n“Remember young man, when the time comes to show your powers don’t hold back.\nSometimes, the best way to avoid trouble is letting people know exactly what\nyou’re capable of.”\n\n\n“The greater the desire, the greater the disappointment”\n\n\n“When a tiger sleeps too long others forget he has claws”\n\n\n“Life’s easier when you don’t have to explain everything”\n\n\n“In this life you only have a set number of chances. If you grab them when they\nappear, you’ll succeed. If you don’t, you’ll be doomed to a life of\nmediocrity.”\n\n\n“It’s a good place to clear your head. Doing good deeds isn’t solely for the\nbenefit of others, it’s good for oneself as well. Seeing that smile from\nhelping people is a reward all it’s own”\n\n\n“High School, the first awakenings of love. They are used to feeling\ninadequate, and resort to protecting their fragile egos in front of their lady\nfriends by tearing others down”\n\n\n“Sometimes the slightest scratch could change the view from a window. Sometimes\neven the slightest chance can bring a revelation.”\n\n\n“You’re only given a spark of madness, don’t lose it.”\n\n\n“Sometimes it’s important just to see the beauty the universe has in store for\nus.”\n\n\n“This is your problem, A-Jue. You’re always underestimating how much we can\nhandle. How do you think humanity’s become master of the stars? How is it we\ncame to colonize and rule over so many planets? Because we’re amazingly\nadaptive”\n\n\n“If you seek to control the oceans, you must stand firm against the roaring\nwaves”\n\n\n“There were many in the universe that needed help, he thought, and though he\ncouldn’t do much it was his responsibility to help who he could.”\n\n\n“Don’t let appearances cloud your eyes. Don’t let suffering blot out the\nmoonlight.”\n\n\n“All men are righteous at birth, and close to their nature. They grow and\nchange, their habits mold them. But through knowledge, through learning, that\ngoodness is retained.”\n\n\n“An unpolished jade sculpture cannot be called a work of art; Men who do not\nstudy cannot comprehend the moral path, and will not become great men.”\n\n\n“Still so self-conscious. Just now you spoke so strongly, and then suddenly\nagain with the personal abuse. That’s a terrible thing for your confidence and\npsychology. Even if you become a powerful man, you’ll still have problems if\nthis doesn’t get fixed. You’ll get vindictive. In this world life really is\nunfair. There is no balance among people. Some people are born privileged,\nrich, comfortable. Some are born with innately powerful Disciplines. But\neveryone’s soul… that’s the same. At birth everyone is born with the same pure\nspirit, unsullied and untarnished. It’s the things we experience during life\nthat changes it. Souls aren’t inherently noble or humble, so you mustn’t look\ndown on yourself. You do you. It’s not about being better, or being the winner.\nYou don’t even need to prove anything to yourself. You just need to be better,\nevery day. That is success.”\n\n\n“He remembered the lesson of his professor from so long ago. If you met the\nopposed aspect of your Discipline, he said, don’t let them go. If they were of\nthe opposite sex, marry them. Otherwise make them your best friend.”\n\n\n“You know, the scariest thing in this world, is a woman’s intuition.”\n\n\n“The Empress said to me, the mark of an immature man is that he is willing to\ngo out in a blaze of glory for some reason, while the mark of a mature man is\nthat he is willing to patiently endure for some reason!”\n\n\n“There are some things, there are some times, where being immature is actually\nbetter.”\n\n\n“The trend of the river moving west cannot be slowed”\n\n\n“When you can’t defeat your enemy, you have to endure. You have to keep your\neyes fixed on him, grow stronger, and then kill him in one bite.”\n\n\n“Better to live passionately for a day, than to live a century while stifled.”\n\n\n“There were some matters that were perfectly fine being stored in one’s  heart.\nThere was no need to display them, only to act on them. Impulse and passion\nwere never synonyms and to be cool-headed did not in any way mean one was a\ncoward. Even if everyone in the world believed him to be a coward, he would not\ncare.”\n\n\n“When any man plays the role of a father, they always have to become that\nfather-in-law they found most loathsome when they were young.”\n\n\n“His heart had long ago been stained by the red dust of the mortal world. In\nthis life, the love and warmth of family had slowly polished it bright, and\nnow, his heart was all the stronger and all the more unbreakable! It was\nadmittedly praiseworthy for someone who stood at the peak of a mountain to\nmaintain perfect purity, but for someone to be born from the sludge to remain\nunsullied was even harder to do!”\n\n\n“Remember…although you must be sincere in taking care of your friends, you need\nto be slightly strategic about it as well. This is the principle behind using\nyour human resources.”\n\n\n“Chen Changsheng thought in confusion, what Tang Thirty-Six said was\nreasonable, females really are the most difficult-to-understand thing in the\nworld. I obviously  didn’t even say anything, so why did she suddenly become\nunhappy?”\n\n\n“Unhappiness, anger, resentment, the urge to kill…once bullied or provoked,\nthese are the emotions that are the easiest to stir.”\n\n\n“Every person had their own responsibility. The most vexing fact was that it\nwas impossible for every person to be their own person. They all had their own\nrelatives, friends, schoolmates, teachers, and elders, all the way up to the\ncontinuation of the country. Thus, it was always impossible for a single person\nto make their own choice or decision. One would always have to consider the\nmatters of the future, and then also consider the matters of the past.”\n\n\n“As in many matters of the world, as long as one person took the lead, those\nwho followed would appear one after the other. ”\n\n\n“In Xunyang City, I also said to Su Li, don’t imagine the world to be too dark,\nbecause that only means that you yourself are too in the dark!”\n\n\n“Yan’er,” he said softly, “look at the clouds, the mountains, the sky, and the\nland. Remember this image. However grand your vision is, that is how grand your\nfuture can be. It is also how grand… your heart can be.”\n\n\n“We cultivators cultivate, not the body, but the heart!”\n\n\n“Like the wall, many thing are not as they appear.” Anonymous chuckled. “If you\ncan get your mind around that concept, you will understand more about existence\nthan most.”\n\n\n“One leaf might not sway the tree, but the tree does not sway itself. That\ntakes many leaves,” Ariana told her, turning the idea over in her mind. “The\nshadow of the leaf does touch the tree and, depending on where the light falls,\nthe shadow can touch many places, while the leaf itself stays fixed on the\nbranch.”\n\n\n“The wind is most noticed when it is absent,” Carly said with a voice that\nsounded both faraway and full of depth and power. “But the currents remain\nsteady, no matter whether the wind blows or remains silent. Those that know\nthis, truly know the sea.”\n\n\n“The fact that he does not desire power for its own sake makes him worthy of\nbeing entrusted with it.”\n\n\n“In the search for truth, every answer is merely the start of another\nquestion.”\n\n\n“For the weakest has but to try his strength to find it, and then he shall be\nstrong.‘”\n\n\n“There are a thousand ways to tell if someone is lying to you. You don’t need\nto be able to glimpse into their mind to catch all of the little signs of\ninsecurity and discomfort. More often than not, all you have to do is look at\nthem. If they glance to the left while they’re talking to you, if they add too\nmany details to a story, if they answer a question with another question.”\n\n\n‘Confidence is ignorance,’ advised the centaur. ‘If you’re feeling cocky, it’s\nbecause there’s  something you don’t know.’\n\n\n“Hate will keep you alive where love fails.”\n\n\n“A man who’s got no fear is missing a friend, Jorg,’ he said, and a smile found\nits way onto those thick lips of his. Running ain’t no bad thing. Leastways if\nyou run in the right direction.”\n\n\n“Consider me a spokesman,’ I said. When it comes to stage-acting, some men are\nmore eloquent than others.”\n\n\n“Churchmen, eh? Love one minute, forgiveness the next, and then it’s eternity\non fire.”\n\n\n“Lundist held that a man who can observe is a man apart. Such a man can see\nopportunities where others see only the obstacles on the surface of each\nsituation.”\n\n\n“Some men are too dull to feel what might happen. Others torture themselves\nwith maybes and populate their dreams with horrors more terrible than their\nworst enemy could inflict upon them.”\n\n\n“But life is not like that. It refuses to curl up and sit quietly in a corner.\nThe habits of several centuries would not go away.”\n\n\n“While I was a prisoner I thought about my life, how I had wasted it gathering\nriches whatever the cost to my family and others around me. In a man’s life, he\ngets few chances to make a difference. To do the right thing. To be a hero, if\nyou will. I intend to become involved in that struggle.”\n\n\n“The more money you had, the more pressure you were under. He had eight hundred\nemployees in this building alone, all relying on him for a pay cheque. They\nwanted yearly salary reviews, medical plans, baby-care centres, regular coffee\nbreaks, double pay for overtime and even stock options, for heaven’s sake.\nSometimes Spiro missed the times when a troublesome worker was thrown out of a\nhigh window and that was the end of him. These days, if you threw someone out\nof a window, they’d phone their lawyer on the way down.”\n\n\n“The controls were hugely complicated, but Mulch had a theory about vehicle\ncontrols: Ignore everything except the wheel and the pedals, and you’ll be\nfine.”\n\n\n“It’s  this blasted puberty, Butler. Every time I see a pretty girl, I waste\nvaluable mind space thinking about her.”\n\n\n“Don’t ask for the truth, boy, unless you’re ready to hear it,”\n\n\n“You’ll learn, Corporal. You can’t coddle a street rat. They’ll turn on you,\nand they have teeth, believe me.”\n\n\n“What’s the nature of royalty, she wondered. Is it like a gown you put on that\ndisappears when you take it off? Does anyone look beyond the finery? Could\nanyone in the queendom take her place, given the right accessories? If so, it\nwas contrary to everything she’d ever been taught about bloodlines.”\n\n\n“Grief was like that. It gradually faded into a dull ache, until some simple\nsight or sound or scent hit him like a hammer blow.”\n\n\n“Surround yourselves with trustworthy people,” he said. “If you don’t, all the\nweaponry and tactics in the world can’t save you.”\n\n\n“Raisa found out that there was a downside to having friends—they were always\ntrying to cheer you up when all you wanted to do was feel sorry for yourself.”\n\n\n“We are all thieves of one kind or another.”\n\n\n“The stars realign and the world remakes itself so that our mistakes seem\nprescient in hindsight.”\n\n\n“Why can’t it be about what you want—sometimes, anyway?” Han said, closing his\nhand over hers. “You just got to — you just have to claim it. I’ve learned that\nnobody’s going to hand you anything. You don’t get what you don’t go after.”\n\n\n“When she’d left the Fells, she thought of people as being sorted into\nlots—good and bad, brave and cowardly. Now she realized that there were bits of\nboth in most people—and which elements prevailed often depended on\ncircumstance.”\n\n\n“I have lost everything, Han thought. Then he corrected himself. Every time I\nthink I’ve lost everything, I find there’s still something else to lose.”\n\n\n“In some ways I will never grow up. For instance, I continue to believe in\nmiracles. But I know that miracles come to those who work very hard.\n\n\n“Grief tempers joy, making it stronger through contrast, as the valleys between\nmake the mountains higher.”\n\n\n“Just remember, once you say something, it can’t be unsaid.”\n\n\n“That’s what you do when you love someone—you notice and notice and notice”\n\n\n“How much weight does a simple promise carry?”\n\n\n“Isnt absurdity a part of the human condition?”\n\n\n“A gentleman can deceive in the pursuit of uprightness. Of course, I’m not\nsaying this is right. Although I’m not a gentleman, I’m not a lowly person\neither. But as a once-gentleman like Sir is being used by a lowly person for an\nungentlemanly matter, I can naturally only use the ways of a lowly person to\nrespond.”\n\n\n“To lecture, to teach, to dispel doubts: this was a teacher.”\n\n\n“If a single wildflower were to open up all by its lonesome on a cliff, how\ncould it be described as beautiful? Only when many wildflowers opened together\ncould it be considered blooming, could it be so beautiful that it touched the\nsoul”\n\n\n“What about me?” Chen Changsheng truly did not recognize anything extraordinary\nabout himself. And just like Tang Thirty-Six had said a few days ago, a person\nunaware of their own genius was truly something that made people in the same\nfield both angry and depressed. He looked at Chen Changsheng and shook his\nhead, saying, “I’ve never met a person like you. The people like you in the\nworld are probably even rarer than pure white Unicorns, because you live…too\nseriously, too properly. I still don’t know what you’re chasing after, but that\nsort of feeling…is very interesting.”\n\n\n“It was only upon meeting Tang Thirty-Six that he understood that the young\nshould be frivolous and not like himself and Senior Yu Ren, clearly very young\nyet living like elders of many years with pure hearts and few desires.”\n\n\n“Pain.” Zhexiu stared into his eyes. “Can stimulate vitality. The greater the\npain, the more vitality is stimulated. You just need to soberly bear that sort\nof pain.”\n\n\n“That when you’re in love with somebody, everything looks colorful.\n\n\n“Since you’re in love with her, she sparkles in your eyes. Thats why people\nfall so irrationally in love.”\n\n\n“Maybe theres only a dark road up ahead, But you still have to believe and keep\ngoing. Believe that the stars will light your path, even a little bit.”\n\n\n“It takes courage to sail uncharted waters.”\n\n\n“It doesn’t matter if you’re the slowest kid in gym class or the fastest man\nalive, every one of us is running. Being alive means running. Running from\nsomething, running to something or someone. And no matter how fast you are,\nthere are some things you can’t outrun. Some things always catch up with you.”\n\n\n“Turns out no one can outrun pain. Life is tragic. But it’s  also precious and\nsweet and extraordinary”\n\n\n“You’d be surprised what you can get used to, Caitlin.”\n\n\n“You think I don’t understand what you’re feelin’? I have been a cop for almost\nas long as you’ve been alive so you should know, putting on that suit does not\nmake everybody safe. For every person you save, there is going to have to be\nsomebody you can’t. And the hardest thing you’re going to have to face is not\nsome monster out there with powers. It’s  going to be that feeling of\nuselessness when you can’t do anything. Or the guilt that weighs on you when\nyou make a mistake. Some things, Barry, you can’t fight. Some things, you just\nhave to live with.”\n\n\n“One mystery I cannot figure out is why some people come into our lives and why\nsome people go. Others become a part of you. Some friendships feel like they’ll\nlast forever and others end far too soon. Not every friendship is meant to last\nforever. What does last forever is the pain when that person is gone.”\n\n\n“Everyone on this planet at some point of time has had a major case of the\nfeels-Those days when your heart is just too small to hold the big things\nyou’re feeling. We think of our emotions like they are this unique personal\nphenomenon, that no one has ever felt what we have felt. There is a basis in\nscience for every emotion we feel ,anger, love . As a scientist I know theres\nnothing magical that makes us feel something for someone else. But then I see\nher smile. Man, that cannot be science.”\n\n\n“Things aren’t always what they seem, our fears can play tricks on us making us\nafraid to change course, afraid to move on. But usually hidden behind our fears\nare second chances waiting to be seized. Second chances at life, at glory, at\nfamily, at love, But these opportunities don’t come around every day, so when\nthey do, we have to be brave, take a chance and grab them while we can.”\n\n\n“Everyone loses someone they love, The real test of character is what you do\nonce they are gone.”\n\n\n“Sometimes the only way to move forward is to revisit the things in the past\nthat have been holding you back. You have to deal with them head on, no matter\nhow scary they are. Because once you do you’ll find that you can go further\nthan you ever imagined.”\n\n\n“In those days we imagined ourselves as being in a holding pen, waiting to be\nreleased into our lives. And when that moment would come, we would be at\nuniversity. How were we to know that our lives had already begun, and our\nrelease would only be into a larger holding pen? And in time, a larger holding\npen.”\n\n\n“This dagger was far too sharp, so its surface was incomparably smooth. It\ncould pass through innumerable flowers and not carry away the slightest\nfragrance, enter the mortal world and not stir its red dust, pierce through all\nthings and yet not disturb them!”\n\n\n“Never trust anything that can think for itself, if you can’t see where it\nkeeps its brain.”\n\n\n“Always use the proper name for things. Fear of a name increases fear of the\nthing itself.”\n\n\n“After all, to the well-organized mind, death is but the next great adventure.”\n\n\n“It does not do to dwell on dreams and forget to live.”\n\n\n“Strange how nearsighted being invisible can make you.”\n\n\n“There is a moment of confusion when a land animal enters the water. Beast,\nhuman, or fairy, it doesn’t matter. The surface is broken and every sense is\nsuddenly shocked. The cold stings, motion slows, and the eyes are filled with\nsmears of color and the snap of bursting bubbles. The time stream is like that\nmoment sustained.”\n\n\n“The world is bigger than you know and scarier than you might imagine. The only\ncurrency worth anything is being true to yourself, and the only goal worth\nseeking is finding out who you truly are.”\n\n\n“Stephanie didn’t like school. She found it difficult to get along with her\nclassmates – not because they weren’t nice people, but simply because she had\nnothing in common with them. And she didn’t like teachers. She didn’t like the\nway they demanded respect they hadn’t earned. Stephanie had no problem doing\nwhat she was told, just so long as she was given a good reason why she should.”\n\n\n“But why can’t I react the way everyone else seems to? Why am I so different?”\n\n\n“Dammit, I am better than they are, I don’t need them, I don’t need their\nstupid approval!”\n\n\n“Gods, it was so simple—just don’t give a damn. Don’t care what they do to you,\nand they do nothing.”\n\n\n“Indifference was a defense now, and not just a pose.”\n\n\n“If no one touches me—no one can hurt me. All I have to do is never care.”\n\n\n“Armor does more than protect; it conceals. Helms hide faces—and your opponent\nbecomes a mystery, an enigma.”\n\n\n“That’s what you do when you love someone—you notice and notice and notice.”\n\n\n“This was his spirit, his integrity. Some things in the world are more\nimportant than life or death, and that noble, unbendable, unbreakable spirit is\ndignity!”\n\n\n” But as the saying goes, if you ride a tiger, it’s hard to get off.”\n\n\n“To follow the path of spirituality, one must abandon the mortal world. You are\nno longer a mortal. You are a Cultivator, destined to defy the Heavens. If you\nare not strong, then you are not qualified to exist. If you are not strong, you\nare not qualified to practice Cultivation. If you are not strong, then you are\nnot qualified to stay alive, but only to be trampled over. Are you willing to\nlive this kind of life?”\n\n\n“Existence was truth. The world is fundamentally unreasonable, and naturally,\nthere is no true fairness.”\n\n\n“This is obviously a lake,” he said suddenly. “Why do people call it the North\nSea?” The old man thought for a moment, then smiled. “Lakes can dry up, grow\nquiet, and become still. If that happened, no living things would remain. But\nseas last forever, and can contain the water of countless rivers and lakes.\nMaybe people just didn’t want the lake to ever go away, so they named it that\nway. When all is said and done, if you believe it’s a lake, then it’s a lake.\nIf you believe it’s a sea, then it’s a sea.”\n\n\n“I’m no longer part of the mortal world, and yet, it’s hard to sever all the\nties.” He closed his eyes. “Well, if they can’t be severed, then I shall just\nlet them remain.”\n\n\n“It’s like the sages said, if you don’t take a first step, you will never know\nwhich direction the road leads.”\n\n\n“Life is an ever-burning flame, filled with exuberance. In life, one must be\nstrong, and never lower one’s head.”\n\n\n“I am the snow during winter. If I get too close to summer, then… summer will\nmelt me. That is not the world of snow, nor is it my world.” Meng Hao\ndisappeared into the distance. He looked like a scholar, but deep down, he was\nas cold as snow.”\n\n\n“If you seek an answer to your questions, perhaps you should also examine your\nown heart.”\n\n\n“Don’t search too hard for an answer. If you do, the answer you find might be\nfalse. At some point in your life, perhaps you will be able to find the answer.\nDon’t give up.”\n\n\n“History dictates that he who holds wisdom can be the greatest fool.”\n\n\n“Life is comprised of one experience after another. Or, you could say that life\nis comprised of many experiences. Different experiences lead to different\nlives; if you experience a cold bitter wind, you will become snow. If you\nexperience scorching heat, you will become rain…. Whatever you experience in\nlife will shape the person you are. That is what makes life wonderful.”\n\n\n“Reading ten thousand books, travelling ten thousand roads. It’s hard to say\nhow many tens of thousands of kilometers I’ve travelled so far. Mountains fill\nthe horizon. Everything I’ve seen and heard fills my heart like an ever-growing\nsea.”\n\n\n“People who share the same fate have no need to make things difficult for each\nother. ”\n\n\n“However, Meng Hao’s personality was such that, the more he wished to kill\nsomeone, the more taciturn he became. He had been like this when small, and was\neven more so now. The more quiet he was, the more vicious he grew. People who\nlike to roar and scream were mere philistines. People who maintained their\nsilence were the truly frightening ones!”\n\n\n“Their gazes locked. There were a thousand people in between them, but despite\nthe distance and the time, they were not far apart. Rather, they were very,\nvery close to each other.”\n\n\n“His path could only be tread by he himself. Perhaps his path would cross the\npaths of others, and that was well and good. But for the moment, he needed to\nwalk alone. Unless… he could be powerful enough to forge his own road. Change\neverything. The alternative was to live a life full of sighing.”\n\n\n“With desire, comes incompleteness. If I have no desire, then the storms will\nnot touch me.”\n\n\n“Honest people don’t need to speak with hidden words.”\n\n\n“He wasn’t the type to be inclined to depend on others. Unless he had an\nimportant purpose, he preferred to be like the sea and the sky, free to roam as\nhe wished, alone. For a man to roam under the heavens, enjoy the scenery,\nobserve the beauty of the earth and the animals… that was what life meant to\nMeng Hao.”\n\n\n“The Heavens are not the Heavens, the Earth is not the Earth. The stars are\neternal, and the Dao will always be!”\n\n\n“Conforming to convention is emptiness,” replied Meng Hao. “Yielding to and\ncomplying with the Heavens is well and good. Unending persistence is fine, too.\nHowever, I cannot choose either of those. He was like the Perfect Foundation,\nnot permitted by Heaven and Earth, and the target of extermination by\nTribulation Lightning. However, he would continue onward. That was how he\ndifferentiated himself from others; his path was not one of inflexible\nadherence to the rules. ”\n\n\n“Along the path of the Dao of alchemy, if a hundred flowers bloom, who has the\nright to permit only the Mudan peony to exist? Cannot the lesser peony and the\norchid also coexist? Thus is birthed the flower garden. If the medicinal plant\ngarden only contained one medicinal plant, how could the Dao of alchemy come\ninto being? ”\n\n\n“Most people are not capable of truly seeking death; the desire to live is\never-present. The only people who will truly seek death are… those whose lives\nare a living hell!”\n\n\n“When a painter observes millions of mountains, then paints one, perhaps his\npainting contains the essence of the mountains he observed. However, the\nmountain he paints… is not real. It emerges from his imagination, and is what\nhe believes a mountain to be. In truth, he has already forgotten the first\nmountain he ever saw, because he has seen too many. He has also forgotten the\nfeeling he experienced when he gazed at that first mountain’s peak. Millions of\nstreams fuse together to become a great and boundless river. But that river… is\nno longer the stream it once was. It is the amalgamation of many waters, fused\ntogether and indistinguishable. That first tiny stream which dreamed of being a\nriver is now dead, killed by the very process it desired. The process of his\npursuit causes the painter to forget that first mountain, and because of that,\nthe very reason he wished to paint a mountain in the first place. The process\nof becoming a river causes the stream to lose itself. Its will is diluted as it\nbecomes a river, and then it is gone.” As he spoke, Meng Hao’s voice grew\nlouder.”This is my third question. By fusing many schools of thought, you lose\nyourself. You think you have benefited, but in reality, you have no path of\nyour own. If you have no ideal of your own to adhere to, then you have observed\nmillions of mountains, but forgotten why you wanted to paint a mountain to\nbegin with! Without principles of your own to stick to, then you are a stream\nthat has become a river. However, such a river has no soul! That, is true\ndeath!” Meng Hao flicked his wide sleeve. His words poured into Chen Jiaxi’s\nears and sent his mind spinning. As Cultivators, we must adhere to our own set\nof principles. As alchemists, we must adhere to our own Dao of alchemy.\nAcquaintances and other schools of thought can bolster or support our\nconfidence. But we must never allow the process of the search to result in\nlosing our own ideal. If the heart is unyielding, nothing can ever supersede\nit. This type of heart may seem as if it contains transformations, but in\nreality, is stable, a foundation. From beginning to end, it will never\ndisappear. It will always exist. An unchangeable heart”\n\n\n“Because the self never changes, the heart can tolerate the ever-changing\ntransformations of the sun and moon, the maelstroms of Heaven and Earth, and\nthose arduous journeys through thousands of crags and tens of thousands of\ntorrents.”\n\n\n“As people grow in life, they form nets to fall back on”\n\n\n“Heaven and Earth are just resting places for the myriads of living creatures.\nTime represents the passage of hundreds of generations of passing travellers.”\nA smile broke out on Meng Hao’s face. Life is a journey, every turn of which is\nfilled with new scenery. This path that he tread now contained his mark.\nWhether the mark was shallow or deep didn’t matter. That was because, it was\nhis choice. “Maybe my path hasn’t even arrived.” He shook his head. Perhaps in\nthe future he would realize what his purpose in life was. For the moment, he\nstill didn’t know. Since he didn’t know, he wouldn’t force himself to choose.\nWhen traveling, it is never possible to know what unfathomable things might\noccur. That is what makes it beautiful.”\n\n\n“Cultivators have what it takes to stand up to Heaven and Earth. Cultivators\nhave the stubbornness to never bow their heads, no matter how bloody the\nbattle. That is a Cultivator. To me, a Cultivator is someone who stands,\ncovered in blood, hair snow white, facing a host of enemies. And yet, no matter\nthe danger, no matter how difficult the path, a Cultivator will grit his teeth,\nlift his head up and laugh! In this manner, he will become a legend! That is\nwhat a Cultivator is to me.”\n\n\n“Entombed on the Earth, but desirous of a return to life in the Heavens….”\n\n\n“There are some things I hesitate to do, but after I do them, I feel no\nregret.”\n\n\n“Sometimes, the meaning of an entire life can be only because of a chance\nmeeting.”\n\n\n“What Cultivators truly cultivate, is self-confidence, and even more\nimportantly, self-awareness. I have to say that … I, Meng Hao, do not dare to\ncall myself a straightforward and upright person. Nor am I a gentleman, or a\nman of honor. But I always repay the kindnesses shown to me!\n\n\n“In Confucianism, there is a concept of a path of justice that contains two\nparts. One part involves being kind and tolerating others. The other involves\ntaking action when necessary. After entering the Cultivation world, Meng Hao\nalso had his own path. This path had nothing to do with Cultivation, but\nrather, personal principles. Meng Hao’s principles also contained the concept\nof justice, a justice with two parts. One was the law of repaying kindnesses.\nThe other was bringing death in response to attacks! Cultivation is about\ndeveloping confidence. Cultivation is about learning how to conduct oneself!”\n\n\n“Sowing contains reaping, reaping contains sowing. Everything that happened\nbefore was all sowing. Karma was reaped after I led the Crow Divinity Tribe out\nof the north all the way to the Black Lands. “It is similar to repaying\nkindnesses. The kindness is the sowing of Karma, and the repayment is the\nreaping of it! “Karma is about cause and effect. I… understand now.”\n\n\n“Life and death oppose each other but also exist in a cycle. Without life, how\ncould there be death? And without death… what could serve as a contrast to\nlife!?”\n\n\n“Within the sea, time is forgotten. Endless coldness, knows no years.”\n\n\n“Death is oftentimes quite simple. Life is oftentimes quite fragile.”\n\n\n“Emotions… are not a hindrance,” he murmured. “Emotions… are what make life\ncomplete.”\n\n\n“The world is an ever changing system of relationships and structures”\n\n\n“Some say continuing to do the same thing over and over again is the definition\nof insanity. On the other hand you say if at first you don’t succeed try try\nand try again.”\n\n\n“I can not only imagine artificial intelligence evolving spontaneously on the\ninternet, but I can’t tell you that it hasn’t happened already. Because it\nwouldn’t necessarily reveal itself to us.”\n\n\n“Civilisation is always about four square meals away from ruin.”\n\n\n“On the internet nobody knows if you are a dog.”\n\n\n“Time is like a dream. It’s impossible to tell what is true and what is false.\nWhen you dream, you see others. Perhaps in the world of others, the dream\nversion of you appears. Or perhaps our lives are like an invisible bubble that\ncould pop at any time, and cause us to awaken. Who dreams of you, and who you\ndream of… this is truly a difficult riddle to explain….”\n\n\n“I thought that when I saw you, I would have the world. I didn’t know that\nwithin your dreams, you already had me.”\n\n\n“Ten thousand things, all in this breath grasping hold of emptiness, there\nreally is nothing to say. Why are people so busy? Just for this one breath,\nPeople say busy, busy; mine, mine. Busy a whole lifetime for Me’. When this one\nbreath is let off, you let go of this whole universe, why not let go from the\nstart?”\n\n\n“You want to talk about real. Show mean thing that is real. There is nothing\nreal from the start. Every morning to night, gathering things, big and small,\nvaluables, money name and recognition; gathering it all up into your lap. Busy\nyour whole life for nothing, acting like a thief, why not put all this energy\ninto liberation? Put this mind to the Way. Everyone in the world is controlled\nby this, shed this control and then you’ll be free, content, liberated. Though\nthere are words to speak, none of this is real. Talk and talk, like flowers\nfalling from heaven its all worthless. If you think what you are grasping is\nreal, theres no good in that, you can’t take it with you.”\n\n\n“I realised people can’t escape birth and death. The people who make you so\nhappy, the people you can’t live without, they leave you.\n\n\n“Before he realized what was happening, the feeling of the passage of time\nappeared in Meng Hao’s heart. He sighed inwardly. Sometimes, it is only when\nencountering old friends that such a feeling will give rise to sighing and\nsobbing.”\n\n\n“After all, the most moving thing of all is love…. And although romantic love\nis beautiful, it pales in comparison to the selflessness of family love”\n\n\n” The love of a father is more reserved, more silent, like a mountain. When you\nare a child, your father is your guardian angel. When you are a teenager,\nthings change. He becomes an obstacle. After that, you come to view yourself as\nthe superior, with him beneath you. Once you reach middle age, though, you look\nat that mountain and you suddenly realize that he has been there all along,\nwatching you proudly. However arrogant you were, however selfish and\nnarrow-minded, he would forgive you. Forgive you without even saying a word.\nYou will feel forlorn, and will suddenly come to a realization. That… is the\nlove of a father. When you have it, you might not feel it deeply. However, once\nyou lose it, you lose the Heaven of your heart! When a child wishes to care for\na parent, only to find that the parent is no longer there, well… that is a\nsorrow that gives rise to the most profound of weeping.”\n\n\n“Living and dying. It can be a departure, but also a beginning.”\n\n\n“If you believe it to be a sea of bitterness, then a sea of bitterness it is.\nIf you believe it to simply be scenery on the path of life, then scenery it\nis…. The sea of bitterness never ends, but the scenery does.”\n\n\n“Many years ago, there was a withered slave on this mountain who said that life\nis pain, and that he wished to free himself from the sea of bitterness. That\nsea is like an inescapable flame which can burn everything. “Afterwards, he\ncalled this place Withering Flame, and made a solemn vow that he would\neradicate the sea of bitterness. He would ensure that all living things no\nlonger experience bitterness, but rather, freedom! “If you were in his place,\nwhat would you do?!”He was right, but also wrong,” murmured Meng Hao. “This\ncould be viewed as a sea of bitterness, but also… not. If you believe\neverything to be bitterness, then it is. If you believe that everything is not\nbitterness, then it is not. “Leaping into the pit of fire represents death.\nReappearance at the bottom of the volcano represents birth. The climb up the\nmountain represents the process of life…. “I would not swear to eradicate this\nplace. Nor would I sink myself into cowardliness. What I have… is the\ndetermination to set my foot where I wish to set it. I control my own fate. I\nmay not be able to control my own birth, but I can decide how I die. “And the\nfinal destination will definitely NOT be the pit at the top of the volcano.”\n\n\n“The path of life does not just run from the bottom of a mountain to its top….”\n\n\n“The great ancestor once said that rain… is born in the Heavens and dies in the\nEarth. The passage between those two places is its entire life….”\n\n\n“You can’t push things. Push hard and you get hardships. Everything’s  a trial.\nYou can’t be afraid of hardship. The more the hardships, the more you move\nforward.”\n\n\n“The truths of life and death are something that cannot be understood by\nsomeone who has not died.”\n\n\n“The Eternal is something that exists eternally within me. No living thing in\nHeaven and Earth can do anything to take it away from me. Even the will of\nHeaven and Earth itself would be incapable of wresting away the Eternal which\nbelongs to me! “The Eternal is a type of determination, an overbearing\nattitude! “What is mine, belongs to me alone!”\n\n\n“The deepest expression of love is simply to stay with someone….”\n\n\n“Men do not, at times, talk like that. Sure some individuals with an X and Y\nchromosome like you may say something like that, but we do not call them Men.\nWe call them perverts, abusers, or rapists - not Men. Real Men don’t do that\nand wouldn’t even think to say that. You will hear a lot of people tell you\nwhat Men do or what it takes to ‘be a Man’. The vast majority of it will be\ntotal garbage. If you want to be a Man, forget about machoism or sexual\nconquest. Being a Man is not about that. It’s  about protecting those around\nyou who are weak or innocent - maybe a child being bullied or your own\nchildren. It’s  being awake at all hours of the night to warm a bottle, change\na diaper, change the sheets on a wet bed or even worse. Men get puked on,\npooped on, bled on and cried on. It’s  about being open with someone,\nvulnerable and accountable. It’s  admitting your mistakes and failures - in all\nits ugliness - and seeking forgiveness, over and over and over again. Real Men\nplay dress up and enjoy tea parties and will make a complete fool out of\nthemselves just to hear a child laugh. They cry, even weep, when the situation\ncalls for it. They respect, honor and cherish women because all of them are\nhuman - created in the image of the Creator. It’s tough being a Man. Hardest\nwork you’ll ever do. So when someone tries to justify abhorrent words and d by\nsullying your good reputation as a Man - be angry and speak up. Don’t let them\ndefine you down by their conduct. In short - be a Man.”\n\n\n“Never forget, the word cultivation 修行 is made of two characters, 修 which\nimplies studying and practice, and 行 which implies action. It is not enough to\njust have 修, the studying and learning. You must also have 行, action…. You\nmust always strive forward; that is the way to reach the pinnacle of power!”\n\n\n“Without the presence of despair, if someone is given hope, they might not\nattach too much importance to it, especially if they have a steadfast heart and\nan unchangeable Dao. However… if you torment someone to their limits and place\nthem in the midst of despair, then give them a sudden scrap of hope, an\nopportunity to be extricated, then most people would not hesitate to grab that\nchance.”\n\n\n“Before the world appeared, before the beginning of Heaven and Earth, before\ntime could even be calculated, perhaps… there were no such things as Immortals.\nTherefore… how did the first Immortal come to be?! That first Immortal\ndefinitely walked his own path. He must have tried many things, and must have\nsuffered many defeats before he finally found the correct path. The first\nperson to succeed called himself Immortal, and that is how Immortals came to\nbe. It must have occurred in that way. Therefore, I can do the same thing. I,\nMeng Hao, will become an Immortal in MY way!”\n\n\n“A youngster who is angry but speaks nothing of it is truly significant”\n\n\n“There is a fine line between a genius and an idiot and that line is\npossibility.”\n\n\n“The first step of the Purification stage was to concentrate one’s mind. The\nmind is the human’s core of spiritual strength. To explain it clearly, “it’s\nthe thought that counts”. If one’s thought was concentrated and strong enough,\nthen it would turn into a certain power. Although it may sound easy, in reality\nit’s not. Even if an ordinary person struggled extremely hard and imagine that\nthey can fly freely in the sky, they are still trapped on the ground. This is\nbecause the power of mind depends on the strength of one’s soul, and the soul’s\nstrength depends solely on talent and is unrelated to one’s effort.”\n\n\n“A fourteen year old kid aroused himself again from disappointment and loss.\nThe time taken was indeed too little. He had to appreciate all of his past\nexperiences and the things that he would experience soon but of course the\nbenefits that he experience won’t be liked by others. He had no time to feel\ndisappointed, he could only keep trying. If not succeed, then die. These five\nwords were most prominent in his mind.”\n\n\n“He realized that matter how he followed his heart and calmed his mind, he\ncouldn’t completely diminish vanity and some other feelings.”\n\n\n“But what should we do? There are so many villains in the world. Unless you\nmimic me and hide in mountain to farm, there will always be some changes that\nyou have to accept.”\n\n\n“With a mirror, one can adjust their clothing, but they can also adjust their\nattitude.”\n\n\n“There was no rule saying that when two people sit together, they must speak.\nSometimes it’s fine to just sit there quietly. Even if they need to converse,\nthey don’t need to speak. A simple gesture will suffice.”\n\n\n“Time, is the only standard with which to judge the world.”\n\n\n“Even if there hasn’t been anything similar in the past, it doesn’t mean it’s\nimpossible for the future.”\n\n\n“Any life that could calmly face and challenge death was worthy of respect.”\n\n\n“Wanting to reach the other shore, really does require boundless wisdom.”\n\n\n“A child that is overly honest, will easily cause others to get angry over\ntheir words. That’s because an honest child speaks the truth”\n\n\n“What is destiny? There are numerous ways to express this one word: The rich\nand poor, bitter experiences in the course of life, the uncertain ups and downs\nof life, or perhaps the mysterious divine intervention? If the destiny is\nreally unknowable and also an immutable existence, then its presence should not\nhave any significance in the first place. When the heavenly book was born into\nthe world, the people began to practice (cultivate). They started borrowing the\nstrength of the heaven and transformed it into their own natural strength, but\nthe cultivators would naturally not accept this assertion. They would want to\nthink that the destiny is derived from their own dauntless spirits and the\ncourage they possess to make a change according to their wishes. Each\ncultivator has an initial connection with the world and his destiny is\ndetermined by the arrangement of stars in the sky. Therefore, the people’s\nunderstanding of their destiny is ultimately dependent on the boundless ocean\nof stars that appears in a starry sky. Since ancient times, the stars in the\nsky, regardless of their position or brightness, are given the same value. They\nare solemnly and eternally illuminating the world. It is said that there are\ninfinite lines joining these seemingly countless stars to form infinite complex\npatterns which cannot be fully portrayed. Looking at the starry sky, many\npeople would feel their hearts racing and beating rapidly in a response to\ntheir praise for the beautiful scene and the complex patterns hidden in this\nboundless ocean of stars. It is very natural to think that something of\nextremely profound significance is hidden within these patterns.”\n\n\n“The world doesn’t have the road because the road is under your feet. Only you\ncan decide which road you must take and that is what decides your unique\njourney. Only you can choose your own position on this stage we call the world.\nTherefore, there is no such thing as fate, but only choices we make and they\nare entirely in our control.”\n\n\n“Therefore, there is no such thing as Fate, but only choices.”\n\n\n“But how beautiful wasting one’s life in this manner was. And how beautiful it\nwas to have a life to waste in such a manner.”\n\n\n“The positions were relative, and the appearances were also relative. As\npositioning changed with landmarks, the appearances changed with environment.\nIf one wanted confirm one’s position, one needed to also confirm the position\nof the surrounding landmarks. If one wanted to examine the unvarying and\nobjective truth, then should not one first understand how the environment\naffected the objective reality?”\n\n\n“Ten thousand streams, each with a different scenery, but in the end they all\njoin the ocean.”\n\n\n“Martial Granduncle said that they couldn’t waste their limited lives on\nunlimited trifling matters.”\n\n\n“To cultivate the Dao is to cultivate the heart. One’s nature determines one’s\nfate, and it will also decide how far one is able to walk in the Dao.”\n\n\n“This time, he would have to get even more serious to live—to live so that\ndeath could see.”\n\n\n“Senior said, even if you try your best till the end and end up finding out\nthat it was still impossible to change fate, then you can only appreciate and\nenjoy everything life has brought you.”\n\n\n“Yet just like death, no matter how many preparations you make, when it finally\nmakes its appearance, you realise that you still aren’t prepared.”\n\n\n“To confide their deepest feelings to each other? Was this a phrase? He wasn’t\ntoo sure. It was very much a strange and alien emotion that he had never\ntouched on before. That was a very sweet sort of emotion, and yet it also made\none afraid, uneasy, but this made one yearn for it. Most importantly, the\nsorrow and joy elicited by this emotion was so intense that it at times seemed\nmore important than all else.”\n\n\n“A casual dot of ink could also be the dot of an eye. An ordinary brushstroke\ncould at times bring an entire painting to life.”\n\n\n“At times, the things and encounters of the world were truly very coincidental,\nvery unfathomable.”\n\n\n“As time goes by, the thoughts of people often change in ways that they would\nnever have imagined at the very beginning.”\n\n\n“Even if you wish to put into practice your view of the world, there is no need\nto rush to do it all at once.”\n\n\n“The word ‘impossible’ had no meaning to him. Because he was not allowed to\nthink it impossible.”\n\n\n“Rumors often arise from the truth, and at times, the truth may be even more\nbizarre than the rumor.”\n\n\n“It was at this moment that a clear and cold voice could be heard through those\nlayers of white curtain. “The road of cultivation is long and endless, but\nsince you’ve already stepped upon it, how can you stop? As long as you\nincessantly press forward, there will come a time when you walk to that day.\nThere’s  no need to worry about whether you arrive early or late, let alone a\nneed to care about victory or defeat, and why should the slander or praise of\nthe world disorder your heart? Could it be that you haven’t even understood\nthis yet?”\n\n\n“Whether it’s  human nature or the human heart, you cannot test them, because\nwhen you begin to think about methods to test them, that would mean that you\nhave already begun to doubt.” The Pope lastly said, “And doubt is the source of\nall misfortune.”\n\n\n“If you were to know that only several dozen days remained of your life, how\nwould you pass your time? Compile all those things you wanted to do but never\ndid into a wishlist and then sell your home and fields and go off to achieve\nthese things? Or would you hide away in some dark corner of your room, your\nface bathed in tears every day? Or would you disregard all morals and laws,\nindulging in your deepest desires and evil thoughts?”\n\n\n“The only true currency in this bankrupt world is what you share with someone\nelse when you’re uncool.”\n\n\n“I always tell the girls, never take it seriously, if ya never take it\nseriously, ya never get hurt, ya never get hurt, ya always have fun, and if you\never get lonely, just go to the record store and visit your friends.”\n\n\n“I  didn’t invent the rainy day, man. I just own the best umbrella.”\n\n\n“Some people have a hard time explaining rock ‘n’ roll. I don’t think anyone\ncan really explain rock ‘n’ roll. Maybe Pete Townshend, but that’s  okay. Rock\n‘n’ roll is a lifestyle and a way of thinking… and it’s  not about money and\npopularity. Although, some money would be nice. But it’s  a voice that says,\n\n\n“Here I am… and fuck you if you can’t understand me.” And one of these people\nis gonna save the world. And that means that rock ‘n’ roll can save the\nworld… all of us together. And the chicks are great. But what it all comes\ndown to is that thing. The indefinable thing when people catch something in\nyour music.”\n\n\n“Music, you now, true music - not just rock n roll - it chooses you. It live in\nyour car, or alone listening to your headphones, you know, with the cast scenic\nbridges and angelic choirs in your brain. It’s  a place apart from the vast,\nbenign lap of America.”\n\n\n“You CANNOT make friends with the rock stars. That’s  what’s  important. If\nyou’re a rock journalist - first, you will never get paid much. But you will\nget free records from the record company. And they’ll buy you drinks, you’ll\nmeet girls, they’ll try to fly you places for free, offer you drugs… I know.\nIt sounds great. But they are not your friends. These are people who want you\nto write sanctimonious stories about the genius of the rock stars, and they\nwill ruin rock and roll and strangle everything we love about it.”\n\n\n“When and where does this “real world” occur?”\n\n\n“In Carl Jung’s  opinion, we all have a sixth sense - intuition. When you meet\nsomeone and you suddenly feel like you can’t live without them. This could be\nthe memory of a past love from the collective unconscious. Or it could just be\nhormones.”\n\n\n“Adolescence is a marketing tool.”\n\n\n“Look at this—an entire generation of Cinderellas, and there’s no glass\nslipper.”\n\n\n“The sands of time cannot be stopped. Years pass whether we will them or not … but we can remember. What has been lost may yet live on in memories. That\nwhich you will hear is imperfect and fragmented, yet treasure it, for without\nyou it does not exist.”\n\n\n“There are forces circling us that we aren’t aware of. Sometimes I wonder if we\ncan ever understand the true motives of the people around us. They all seem to\nhave secrets. It is the way of the world. Ignore all the schemes and trust in\nthe nature of each person”\n\n\n“Being a magician has always been about, in part, accruing power. Power over\nyourself, the elements, the future. But power, as you all know, does not come\ncheaply.”\n\n\n“Magic doesn’t come from talent, it comes from pain.”\n\n\n“Kady: Why can’t anything just be fixed. Penny: Life, I guess.”\n\n\n“The truly intelligent person will never reject the slightest chance of\nsurvival.”\n\n\n“It was darkest before dawn. When these words were usually spoken, the meaning\noften desired was that as long as one was able to endure this darkest hour, one\nwould be able to welcome a bright and beautiful morning, the principle being\nthat hope was forever. However, when dawn truly came, just what did it have to\ndo with that darkest hour? Time was life and once it went, there was no turning\nback. There had never been any connection between another person’s\nlight and one’s  own darkness.”\n\n\n“To cultivators, life is an extremely long course of events. In this course of\nevents, we will encounter many challenges, feel much despair, and this is our\npredestined fate. And how should we confront this predestined fate? To happily\nlive on as if given a new lease on life, or to undergo serious contemplation\nbefore finding ourselves once more, that is the greatest distinction.”\n\n\n“The past is the past; such is time. Similarly, the movement of the stars, the\nchanges of fate, all proceed forward, and so we can only look forward.”\n\n\n“If you want to live forever, you have to be powerful enough to walk out into\nthe heavens. That’s a very long road, and you can’t let yourself get distracted\nby the scenery along the way. You have to walk the path and live life without\nany regrets!”\n\n\n“There were few things in life as terrifying as two women who hated each\nother….”\n\n\n“There was nothing without risk. Even walking on the street, one might be\nkilled by a rock falling down from the skies. Even when eating, one might choke\nto death.”\n\n\n“Dripping water can eventually tunnel through a rock”\n\n\n“Hahaha, that’s my boy. Right; don’t easily give up or betray your good friends\nand brothers! If you can so easily sacrifice them…then you’ll never be able to\nmake any true friends and brothers! If you wish for others to be willing to\nrisk their lives for you, you have to treat them with sincerity, understood?”\n\n\n“The path of Immortal cultivation is filled with many dangers and obstacles.\nThus, with each step, you need to leave a firm footprint as you walk forward in\na stable manner. Your heart must be stable as well. This is indeed true”\n\n\n“However…everything in this world is divided into yin and yang,” Patriarch\nSubhuti said. “Although it is important to be stable and solid, it is also\nimportant to be sharp.”\n\n\n“The path of Immortal cultivation… your goals should be distant and grand, with\nPangu and Nuwa as your models. The path of Immortal cultivation…it requires you\nto lower your head and watch the road, for you to remember to maintain a solid\nfoundation. Do not merely think about soaring into the skies; when a bird soars\ntoo far, its eggs might be stolen and destroyed. It will perish, its Dao gone.\nThe path of Immortal cultivation…it requires caution. It is a boat that will\nsail for ten thousand years that you must control with care. The path of\nImmortal cultivation…it requires sharpness. Only with a heart that is filled\nwith a desire to charge into the heavens can you walk even farther on this\npath.”\n\n\n“How to make your goals grand but not too high…how to be cautious and yet have\nthe desire to charge into the heavens…you will need to handle this yourself.\nThe world is divided into yin and yang, and between yin and yang lies the\nheart.”\n\n\n“Good. That’s the attitude I like to see. Defeat isn’t frightening; what’s\nfrightening is not even having the courage to try.”\n\n\n“But there are no absolutes in life. There’s always a chance.”\n\n\n“Stupidity does not mean you have the power to speak unreasonably, nor is it\nsomething that requires respect.”\n\n\n“Skill from diligence; incompetence from indulgence. I dare not slack off.”\n\n\n“What will people think of your ‘s cience’ two thousand years from now?” Mr. D\ncontinued. “Hmm? They will call it primitive mumbo jumbo. That’s  what. Oh, I\nlove mortals—they have absolutely no sense of perspective. ”\n\n\n“There is no sound more annoying than the chatter of a child, and none more sad\nthan the silence they leave when they are gone.”\n\n\n“A Dark time comes. My time. If it offends you. Stop Me.”\n\n\n“I think you’re enough like me to understand. If my life is going to mean\nanything, I have to live it myself. I can’t let a god take care of me … or my\nson. I have to … find the courage on my own. ”\n\n\n“Families are messy. Immortal families are eternally messy. Sometimes the best\nwe can do is to remind each other that we’re related, for better or worse …\nand try to keep the maiming and killing to a minimum.”\n\n\n“The most dangerous flaws are those which are good in moderation,” she said.\n\n\n“Evil is easy to fight. Lack of wisdom… that is very hard indeed.”\n\n\n“Happily Ever After doesn’t exist. Every day you wake up and decide to love\nyour partner and your life—the good, the bad and the ugly. Some days it’s a\nstruggle and some days you feel like the luckiest person in the world.”\n\n\n“When you commit to someone, you don’t actually know who you’re committing to.\nYou know who they are today, but you have no idea who this person is going to\nbe in five years, ten years, and so on. You have to be prepared for the\nunexpected, and truly ask yourself if you admire this person regardless of the\nsuperficial (or not-so-superficial) details, because I promise almost all of\nthem at some point are going to either change or go away.”\n\n\n“Relationships exist as waves, people need to learn how to ride them.”\n\n\n“You can work through anything as long as you are not destroying yourself or\neach other. That means emotionally, physically, financially, or spiritually.\nMake nothing off limits to discuss. Never shame or mock each other for the\nthings you do that make you happy. Write down why you fell in love and read it\nevery year on your anniversary (or more often). Write love letters to each\nother often. Make each other first. When kids arrive, it will be easy to fall\ninto a frenzy of making them the only focus of your life…do not forget the love\nthat produced them. You must keep that love alive and strong to feed them love.\nSpouse comes first. Each of you will continue to grow. Bring the other one with\nyou. Be the one that welcomes that growth. Don’t think that the other one will\nhold the relationship together. Both of you should assume it’s up to you so\nthat you are both working on it. Be passionate about cleaning house, preparing\nmeals, and taking care of your home. This is required of everyone daily, make\nit fun and happy and do it together. Do not complain about your partner to\nanyone. Love them for who they are. Make love even when you are not in the\nmood. Trust each other. Give each other the benefit of the doubt always. Be\ntransparent. Have nothing to hide. Be proud of each other. Have a life outside\nof each other, but share it through conversation. Pamper and adore each other.\nGo to counseling now before you need it so that you are both open to working on\nthe relationship together. Disagree with respect to each other’s feelings. Be\nopen to change and accepting of differences.”\n\n\n“The wine god sighed. “Oh, Hades if I know. But remember, boy, that a kind act\ncan sometimes be as powerful as a sword. As a mortal, I was never a great\nfighter or athlete or poet. I only made wine. The people in my village laughed\nat me. They said I would never amount to anything. Look at me now. Sometimes\nsmall things can become very large indeed.”\n\n\n“Poseidon put his weathered hand on my shoulder. “Percy, lesser beings do many\nhorrible things in the name of the gods. That does not mean we gods approve.\nThe way our sons and daughters act in our names…well, it usually says more\nabout them than it does about us. ”\n\n\n“Sometimes the hardest power to master is the power of yielding. Do you believe\nme?”\n\n\n“You can’t beat me, Oliver. Yes, you’re younger and you’re faster, yet you\nalways come up short against me. Wanna know why? Because you don’t know, in\nyour heart, what you’re fighting for. What you’re willing to sacrifice. And I\ndo.”\n\n\n“So you’ll atone for one murder, Robert, by committing hundreds, thousands?”\n\n\n“Nothing is bred that is weaker than man.”\n\n\n“You are so blinded by your hate for him that you don’t realize the damage that\nit’s  doing in your own life. To your family.”\n\n\n“You’d be surprised the power revenge can give you.”\n\n\n“I’m giving up a lot so maybe I thought the universe owed me one.”\n\n\n“Dead people don’t want anything, that’s  one of the benefits of being dead.”\n\n\n“Home is a battlefield. Back home, they’re all trying to get you. Get you to\nopen up, be somebody you’re not sure you are anymore.”\n\n\n“The strongest of the warriors are time and patience. ”\n\n\n“A man cannot live with two names.”\n\n\n“Pain is inevitable, suffering is optional.”\n\n\n“Boys have dreams so go chase them, youth is your capital. Youth represents the\ninfinite possibilities that the future holds. FIGHT!”\n\n\n“This is life, who can you blame?”\n\n\n“However….. right when they were about to leave, Luo Feng’s family was a bit\nhesitant, since there were way too many memories in this home. However, people\nhave to go higher!”\n\n\n“The waves in the back always push the ones in the front in the Yangtze river,\nso of course each generation has to be stronger than the next”\n\n\n“So, this world is simple. People respect those with high status and look down\non those with low status. If a rich person runs out of money, it’s over for\nhim. If someone with a great position loses that position, his authority will\nprobably disappear too. Position and money are external, only your own strength\nis truly reliable”\n\n\n“Nobody can predict the twists of life”\n\n\n“The path to aim for the limits of life is filled with difficulty. Of course I\ncan’t shy away from any challenges! Pedestrians can get hit by a car. Even in a\nsector, a dropped vase could hit your head. Nothing is absolutely safe!”\n\n\n“Man tended to believe that when he learnt any new information, he understood\nand knew everything. It’s not only till later when he learns more that he\nrealizes how ridiculously wrong he was before. Without even a grasp of Earth’s\nmany species of life and principles, he bases on simple deductions that the\nwhole universe and milky way, and even further cannot have any other forms of\nintelligent life other than man! How dogmatic. What a joke. The endless seas\nhave yet to be fully explored by Man. What’s more, the universe is so much\nbigger than the sea, a trillion times maybe? Not even close! The limitless\nuniverse, can’t possibly be judged by the humans on earth. Isn’t this\nsynonymous with the story of the frog in the well, thinking that the world and\nsky is only as big as his well?” Hong shook his head, “The civilizations that\nexist in the universe, race and ethnicities, are complex and mysterious beyond\ncompare.”\n\n\n“The people and kingdoms from hundreds of thousands of years back have turned\nto dust long ago. Compared to the endless march of history, where kingdoms and\nempires rise then collapse, personal grudges and enmities are so meaningless\nand small.”\n\n\n“Last night I hugged my pillow and dreamt of you. I wish that someday I’d dream\nabout my pillow and I’d be hugging you”.\n\n\n“Love will travel as far as you let it. It has no limits.”\n\n\n“That farewell kiss which resembles greeting, that last glance of love which\nbecomes the sharpest pang of sorrow”\n\n\n“A thing is mighty big when time and distance cannot shrink it.”\n\n\n“To hear, one must be silent.”\n\n\n“It was only the dumb instinctive wisdom of the beast who licks his hurt\ncompanion to comfort him, and yet in that wisdom Ged saw something akin to his\nown power, something that went as deep as wizardry. From that time forth he\nbelieved that the wise man is one who never sets himself apart from other\nliving things, whether they have speech or not, and in later years he strove\nlong to learn what can be learned, in silence, from the eyes of animals, the\nflight of birds, the great slow gestures of trees.”\n\n\n“A man would know the end he goes to, but he cannot know it if he does not\nturn, and return to his beginning, and hold that beginning in his being. If he\nwould not be a stick whirled and whelmed in the stream, he must be the stream\nitself, all of it, from its spring to its sinking in the sea. ”\n\n\n“Certain mystes aver that the real world has been constructed by the human\nmind, since our ways are governed by the artificial categories into which we\nplace essentially undifferentiated things, things weaker than our words for\nthem. I understood the principle intuitively that night as I heard the last\nvolunteer swing the gate closed behind us.”\n\n\n“We all can be only what we are, nothing more, or less.”\n\n\n“What man has no ill intent in the silence of his soul?”\n\n\n“Who is to say what is only a story and what is truth disguised as a story?”\n\n\n“Many good and solid men would say so,” the old man told him, looking up at the\nstars, “good men who will live out their lives believing only in what they can\nsee and touch. But there’s  a world beyond what we can see and touch, and that\nworld lives by its own laws. What may be impossible in this very ordinary world\nis very possible there, and sometimes the boundaries between the two worlds\ndisappear, and then who can say what is possible and impossible?”\n\n\n“The wizard leaned back against the rock and looked out over the hills, as if\nseeing more than was there. His tone was sorrowful. “Because, Richard, many\npeople must be ruled to thrive. In their selfishness and greed, they see free\npeople as their oppressors. They wish to have a leader who will cut the taller\nplants so the sun will reach them. They think no plant should be allowed to\ngrow taller than the shortest, and in that way give light to all. They would\nrather be provided a guiding light, regardless of the fuel, than light a candle\nthemselves.”\n\n\n“Murder is the way of all things, the way of nature,” Zedd repeated. “Every\nliving thing is a murderer.”\n\n\n“Life for the strongest. There is no sympathy for the slain, only admiration\nfor the winner’s strength.”\n\n\n“If someone digs a hole, and it fills with rainwater, where is the fault? Is it\nthe rain’s fault?”\n\n\n“Years from now, our past will be a story - a story of long days and lonely\nnights, hardwork and lack of sleep. We’ll live each day having intimately known\nthe pain of being apart. We’ll appreciate and embrace our time together knowing\nhow lucky we are to have made it through and we’ll find solace in the promise\nof a future together.”\n\n\n“Contrary to what the cynics say, distance is not for the fearful; it’s  for\nthe bold. It’s  for those who are willing to spend a lot of time alone in\nexchange for a little time with the one they love.”\n\n\n“It’s not a bug – it’s an undocumented feature.”\n\n\n“Programming is like sex. One mistake and you have to support it for the rest\nof your life.”\n\n\n“I see now that the circumstances of one’s  birth are irrelevant; it is what\nyou do with the gift of life that determines who you are. ”\n\n\n“We do have a lot in common. The same Earth, the same air, the same sky. Maybe\nif we started looking at what’s  the same instead of what’s  different… well,\nwho knows. ”\n\n\n“But walking along Fifth Avenue in Brooklyn, in his black overcoat and his gray\ninterview suit, Quentin knew he wasn’t happy. Why not? He had painstakingly\nassembled all the ingredients of happiness. He had performed all the necessary\nrituals, spoken the words, lit the candles, made the sacrifices. But happiness,\nlike a disobedient spirit, refused to come. He couldn’t think what else to do.”\n\n\n“You exist inside a spring that can’t be replaced.”\n\n\n“As if you can see right through me, into my heart… Always, out of nowhere,\nyou… just show up.”\n\n\n“Isn’t it funny how the most unforgettable scenes can be so trivial?”\n\n\n“Even it the depths of the darkest oceans, some light always pierces through.”\n\n\n“She’s  merciless. That unbending gaze even from the back, she wont let me give\nup. The one who was being supported..was me. Thank you. Thank you.”\n\n\n“After struggling, losing my way, and suffering… the answer I arrived at was\nso laughably simple…”\n\n\n“I’m… going on a journey. The applause raining down. Pursuing that moment\nwhen my music reached them. Pursuing that sight of her with her back to me.\nuntil one day, for sure, I’ve pulled even with her… until that day comes.”\n\n\n“I knew all along. The ghost of my mother was a shadow of my own creation. An\nexcuse for me to run away. My own weakness. Mom isn’t there anymore. Mom… is\ninside me.”\n\n\n“The moment I met her, my life changed. Everything I saw, everything I heard,\neverything I felt, all the scenery around me… started to take on color.”\n\n\n“I want to hear it again, yet I don’t want to hear it. I want to see her, yet I\ndon’t want to see her. What do you call this kind of feeling again?”\n\n\n“A lump of steel, like a shooting star. Just seeing the same sky as you makes\nfamiliar scenery look different. I swing between hope and despair at your\nslightest gesture, and my heart starts to play a melody. What kind of feeling\nis this again? What do they call this kind of feeling? I think it’s  probably\ncalled..Love. I’m sure this is what they call love.”\n\n\n“I look like I’m suffering, huh? That’s  not good… but of course I’d be\nsuffering. I mean, I’m gonna sail in charted waters, right? Both, taking on a\nchallenge and creating something. It is painful, but it’s  fulfilling. So thank\nyou. For sweeping away the dust that had collected on my body. .. For\nencountering me… ever since that day… my world, even the keyboard… became\ncolorful.”\n\n\n“You know, I discovered something. Everyone has something… something deep\ninside their hearts. For some, it might be enmity. For others, admiration.\nWishes, a craving for the spotlight, feelings that one wants to deliver,\nfeelings for ones mother. Everyone was supported by their own feelings. I\nrealize now that, perhaps, no one can stand alone on stage.”\n\n\n“Letting you go was never easy because the half of my heart joined you and left\nme.”\n\n\n“How can I forgot about you, when everything about you, already became a part\nof me?”\n\n\n“We’re all afraid, you know.. to get up on stage. Maybe you’ll mess up. Maybe\nthey’ll totally reject you. Even so, you grit your teeth and get up on stage\nanyway. Something compels us… moves us to play music.”\n\n\n“We’re all connected. Just like the notes are intermittently connected. It’s\nshared by us all. Through music, with the people you know, with the people you\ndon’t know, with all the people in this world.’\n\n\n“It’s  not just allies who support each other. From your enemies, you learn so\nmuch and gain so much. Until the day you meet again… Just knowing they exist\nhelps you to withstand the loneliness. Those who compete, even if they’re\nenemies, help each other out.\n\n\n“Even though I’m bitter over losing, even though I’m depressed, even though my\nankle hurts, and my eyes are smeared with tears…even though I’ve never felt\nworse…I wonder why the stars are sparkling like this. The scent of the music\nroom in his hair. I can hear his slightly ragged breathing. His shoulder, wet\nwith tears, is so warm. I am by his side. I wish time would just stand still.”\n\n\n“In my next life, I want to be me, and meet you again.”\n\n\n“If I can get my target to move as I want, I’ve succeeded as a Hunter.”\n\n\n“Ha-ha these humans are definitely foolish creatures. Think as hard as those\nweak brains of yours can manage. Do you humans ever listen to the cries of\nmercy coming from pigs and cows you slaughter?”\n\n\n“I was trying to take the easy way out by running away from everything. No\nmatter the pain, I will keep living. So when I die, I’ll feel I did the best I\ncould.”\n\n\n“If you want to get to know someone, find out what makes them angry.”\n\n\n“You should enjoy the little detours. To the fullest. Because that’s  where\nyou’ll find the things more important than what you want.”\n\n\n“A beast in human’s  clothing understands better than anyone how people want to\nbe treated.”\n\n\n“You must be prepared to face the worst possible scenarios. Because harsh\nreality strikes without warning.”\n\n\n“Whenever humans encounter the unknown, they tend to lose perspective.”\n\n\n“When I say it doesn’t hurt me, that means I can bear it.”\n\n\n“I do not fear death. I fear only that my rage will fade over time.”\n\n\n“The only principle is that there are no principles.”\n\n\n“Skill is one thing, and caution another.”\n\n\n“Even the fastest eye can be fooled.”\n\n\n“Qualification isn’t something we have to talk about. The ones who are not okay\nwith their success can go through training until they are.”\n\n\n“I never imagined how frustrating weakness could be”\n\n\n“Who wants to have their life planned out for them.”\n\n\n“Hunters are a bunch of egomaniacs. We set aside everything else to get what we\nwant.”\n\n\n“Human potential for evolution is limitless.”\n\n\n“A prayer comes from the heart”\n\n\n“Love and hate are two sides of the same coin.”\n\n\n“If that monster is your darkness, then I have a reason to fight it, no?”\n\n\n“The art of hunting is aiming for the moment when the target is busy hunting\nits prey!”\n\n\n“It’s  pointless to mope over the validity of someone else’s  success. If\nyou’re unhappy about the results, keep moving yourself forward until you are\nsatisfied.”\n\n\n“Prosperous cities tend to attract all sorts of nasty people.”\n\n\n“The countless dragons that rained down were less significant threats than the\nhumans in the sky.”\n\n\n“An apology is a promise to do things differently next time, and to keep the\npromise.”\n\n\n“Order is the barrier that holds back the flood of death. We must all of us on\nthis train of life remain in our allotted station. We must each of us occupy\nour preordained particular position. Would you wear a shoe on your head? Of\ncourse you wouldn’t wear a shoe on your head. A shoe doesn’t belong on your\nhead. A shoe belongs on your foot. A hat belongs on your head.”\n\n\n“My friend, you suffer from the misplaced optimism of the doomed.”\n\n\n“Wilford: Curtis, everyone has their preordained position, and everyone is in\ntheir place except you.\n\n\nCurtis: That’s  what people in the best place say to the people in the worst\nplace.”\n\n\n“You’ve seen what people so without leadership. They devour one another.”\n\n\n“I believe it is easier for people to survive on this train if they have some\nlevel of insanity. As Gilliam well understood, you need to maintain a proper\nbalance of anxiety and fear and chaos and horror in order to keep life going.\nAnd if we don’t have that, we need to invent it.”\n\n\n“Assume everyone will betray you and you will never be disappointed.”\n\n\n“Perhaps it’s  impossible to wear an identity without becoming what you pretend\nto be.”\n\n\n“In the moment when I truly understand my enemy, understand him well enough to\ndefeat him, then in that very moment I also love him. I think it’s impossible\nto really understand somebody, what they want, what they believe, and not love\nthem the way they love themselves. And then, in that very moment when I love\nthem… I destroy them.”\n\n\n“I think that most of us, anyway, read these stories that we know are not\n\n\n“true” because we’re hungry for another kind of truth: the mythic truth about\nhuman nature in general, the particular truth about those life-communities that\ndefine our own identity, and the most specific truth of all: our own\nself-story. Fiction, because it is not about someone who lived in the real\nworld, always has the possibility of being about oneself.”\n\n\n“Because never in my entire childhood did I feel like a child. I felt like a\nperson all along―the same person that I am today.”\n\n\n“Sometimes lies were more dependable than the truth.”\n\n\n“Early to bed and early to rise,” Mazer intoned, “makes a man stupid and blind\nin the eyes.”\n\n\n“There are times when the world is rearranging itself, and at times like that,\nthe right words can change the world.”\n\n\n“So the whole war is because we can’t talk to each other.”\n\n\n“Human beings are free except when humanity needs them. Maybe humanity needs\nyou. To do something. Maybe humanity needs me—to find out what you’re good for.\nWe might both do despicable things, Ender, but if humankind survives, then we\nwere good tools.”\n\n\n“He could see Bonzo’s  anger growing hot. Hot anger was bad. Ender’s  anger was\ncold, and he could use it. Bonzo’s  was hot, and so it used him. ”\n\n\n“Human beings may be miserable specimens, in the main, but we can learn, and,\nthrough learning, become decent people.”\n\n\n“I will remember this, thought Ender, when I am defeated. To keep dignity, and\ngive honor where it’s  due, so that defeat is not disgrace. And I hope I don’t\nhave to do it often.”\n\n\n“If only we could have talked to you, the hive-queen said in Ender’s  words.\nBut since it could not be, we ask only this: that you remember us, not as\nenemies, but as a tragic sisters, changed into foul shape by fate or God or\nevolution. If we had kissed, it would have been the miracle to make us human in\neach other’s  eyes. Instead we killed each other. But still we welcome you now\nas guestfriends. Come into our home, daughters of Earth; dwell in our tunnels,\nharvest our fields; what we cannot do, you are now our hands to do for us.\nBlossom, trees; ripen, fields; be warm for them, suns; be fertile for them,\nplanets: they are our adopted daughters, and they have come home.”\n\n\n“We thought we were the only thinking beings in the universe, until we met you,\nbut never did we dream that thought could arise from the lonely animals who\ncannot dream each other’s  dreams.”\n\n\n“The seed of doubt was there, and it stayed, and every now and then sent out a\nlittle root. It changed everything, to have that seed growing. It made Ender\nlisten more carefully to what people meant, instead of what they said. It made\nhim wise.”\n\n\n“What else should you be? Human beings  didn’t evolve brains in order to lie\naround on lakes. Killing’s  the first thing we learned. And a good thing we\ndid, or we’d be dead, and the tigers would own the earth.”\n\n\n“No book, however good, can survive a hostile reading.”\n\n\n“All is going well, very well, I couldn’t ask for anything better. So why do I\nhate my life?”\n\n\n“This is what historians usually do, quibble about cause and effect when the\npoint is, there are times when the world is in flux and the right voice in the\nright place can move the world. Thomas Paine and Ben Franklin, for instance.\nBismark. Lenin.”\n\n\n“There was no doubt now in Ender’s  mind. There was no help for him. Whatever\nhe faced, now and forever, no on ewould save him from it. Peter might be scum,\nbut Peter had been right, always right; the power to cause pain is the only\npower that matters, the power to kill and destroy, because if you can’t kill\nthen you are always subject to those who can, and nothing and no one will ever\nsave you.”\n\n\n“Nature can’t evolve a species that hasn’t the will to survive. Individuals\nmight be bred to sacrifice themselves, but the race as a whole can never cease\nto exist.”\n\n\n“He is dead, she thought bitterly, because we have forgotten him.”\n\n\n“He toyed with the idea of trying to be like the other boys. But he couldn’t\nthink of any jokes, and none of theirs seemed funny. Wherever their laughter\ncame from, Ender couldn’t find such a place in himself. He was afraid, and fear\nmade him serious.”\n\n\n“There is no teacher but the enemy. No one but the enemy will teach you how to\ndestroy and conquer.”\n\n\n“Strange, isn’t it? Each man’s  life touches so many other lives. When he isn’t\naround he leaves an awful hole, doesn’t he?”\n\n\n“You see, George, you’ve really had a wonderful life. Don’t you see what a\nmistake it would be to throw it away?”\n\n\n“Modesty is a virtue but overdoing it was just being false.”\n\n\n“To be punctual meant to exist as a point, meant that as well as to arrive\nsomewhere on time. Constant existed as a point - could not imagine what it\nwould, be like to exist in any other way.”\n\n\n“Hope?” he says. “There is always hope, John. New developments have yet to\npresent themselves. Not all the information is in. No. Don’t give up hope just\nyet. It’s the last thing to go. When you have lost hope, you have lost\neverything. And when you think all is lost, when all is dire and bleak, there\nis always hope.”\n\n\n“Rather than giving them bread, teach them how to cultivate wheat. Truly, there\nare plenty who would become rotten themselves if all they did was receive\nbread.”\n\n\n“Doing the same things, or perhaps coming to resemble the person you hate, you\nloath, isn’t an uncommon story.”\n\n\n“It seems you’ve got a lot on your mind. Worry all you want. When you look back\non it, you’ll always feel like a fool asking yourself why you worried over\nsomething like that.”\n\n\n“Humans have nothing but unknowns. Even if they act like they know everything,\nthat’s surely a lie. That’s why there’s no way but to spend your whole life\nlearning it. There’s plenty of wisdom you won’t find in a book, and I agree\nwith your opinion, Lyle.”\n\n\n“The mountains have not a care, yet the snows whiten their hair; water feels\nnot the world’s  woe, yet its face wrinkles when winds blow”\n\n\n“Spirit is the mind, it is your thoughts, your imagination. Spirit is when you\nremember as you dream in your heart. This is what we call thought, and it is\nalso spirit.”\n\n\n“Whatever you brag about the most is what you lack the most. Whatever it is\nthat you want others to know you own the most of is what you want to possess\nmost of.”\n\n\n“Death is not terrifying. The thing that is terrifying is the moment before\ndeath.”\n\n\n“People always speak of heaven and earth but, what is heaven and earth? It\nmeans all that is under the sky! If the heavens had a soul, it would be an\noppressive one! The oppression of the heavens is invisible. We can only endure\nit and while we endure it we must learn to live with it happily. If we do not,\nare we to fight against heaven? To my understanding, perhaps this question\nmeant that besides going up against heaven, could there be any other way to\nfight against destiny… Once you grow up, perhaps you’ll have a deeper\nunderstanding of it. If that day truly comes and you have attained the power\nwhich allows you to do as the words say, then perhaps you can think of a third\nway to fight instead of submitting to destiny or rebelling against it.”\n\n\n“Among all those living on the land, who would be able to see the end of the\nhorizon?”\n\n\n“You can only see the surface of the muddy water on the ground and never the\nbottom.”\n\n\n“Spirit is Dao,” Tian Lan Meng stated calmly. It is not thought, because\nthought in itself is narrow, but Dao is endless. Dao is a realm that those from\nother realms seek. Every person has a different Dao. The great Dao is\nboundless, and those who obtain Dao will see through the world, and in turn, we\ncan say that we have found and become the truth. I read this sentence from an\nancient scroll before. It is a sentence that is spoken in the other worlds… If\nyou stay on your Dao but have no method of solving a particular problem now,\nthat method will eventually come to you. If you have the skills and power, but\nhave strayed from your Dao, then you cannot use your skills, and your power\nwill forever stay stagnant!”\n\n\n“All things in the world differ in sizes. My understanding towards the word is\nnarrow and small to you, and the Dao you speak is a huge thing that seeks to\nreach a state where you understand the world. It is like two spots, like two\ndifferent directions, and like two different extremes. Su Ming closed his eyes\nand continued, unhurried, “To me, the heart is aspiration, and the spirit is a\nrealm. You are walking on the Dao of the heavens, and I’m walking through the\nnarrow gate on the earth, but once I walk past that gate, what I’m searching\nfor is just to merely open my eyes. Do you understand what I’m saying? The last\nsentence written in the beast skin scrolls suddenly surfaced in Su\nMing’s mind. “You cannot see… the world that I see.”\n\n\n“You are always copying,” Su Ming said slowly, lifting his head, “because you\nthink that the spirit is Dao. You search for something abstruse, that’s why you\ncan copy many things, because you think that as you search for it, you will\nfind your Dao eventually. I don’t know what is the Dao you speak… but from what\nyou said just now, I can understand that while the Dao is an abstruse concept,\nit exists. It exists within the world, perhaps all the plants, trees, flowers,\nand stones have a Dao within them. What I seek isn’t a Dao, but to have my mind\nacting as my aspiration, to have my spirit as my realm, and when I open my\neyes, I will draw out my heart’s desires… This is the reason why I can draw,\nbut you can only copy.”\n\n\n“There is a saying that goes that the people who are lonely may be different\nfrom each other, but all of them stare at the moon.”\n\n\n“Willpower is an abstract term. It’s like a person’s resolve, like a beacon of\nlight in a person’s life… However, it is still an abstract thing… If no one\nattacks me, then I won’t attack, but if someone attacks me, I will definitely\nkill him! In a battlefield, it doesn’t matter whether I’m taking the initiative\nor remain in the passive, if anyone attacks me, then unless he is killed by\nsomeone else, then I will definitely kill him!  That’s right. I’m afraid of\ndeath… I’m afraid that once I die, I won’t be able to find my way back home.\nI’m afraid that once I die, all these mysteries will disappear. I’m afraid that\nonce I die… I won’t be able to open my eyes.”\n\n\n“When two adversaries meet at a narrow path and cannot back out from a fight…\nthe courageous one will win”\n\n\n“Courage is also a type of presence. This is not recklessness. It’s instead a\npresence akin to a mountain, one that will make your enemies breakdown due to\nyour tenacity. Courage is also a method to subdue your enemies. It is also the\ncharacteristic you need to become an upright man when you journey through the\nworld! Su Ming, remember my words… perhaps some day, you will truly understand\nit.”\n\n\n“Once you overcome your fears, once you have a taste of what courage is like,\nhow would you feel..? I hope that when that time comes, I will still be by your\nside and hear you tell me how you feel. A feeling that I have won against\nmyself.”\n\n\n“The place where I was born still did things according to the laws of the\nuniverse. When I was born, the Berserkers had weakened… If the heavens are\nheartless, then we will all be separated. The earth was heartless, and it made\nmy Dark Mountain die. If the heavens have eyes, then why do they never see that\nmy world is plunged into eternal darkness? If the deities have souls, then why\ndid they divide the sky and seas to the south and north? I kept my duty to the\nheavens, so why did they not let me see the darkness of night? I kept my duty\nto the deities, so why did they tear me into pieces and scatter my memories?!\nIf the heavens don’t have eyes, then I will step on it and watch myself seal\nthe heavens! If the deities don’t have souls, then I swear I will slaughter the\ndeities and become the Emperor!”\n\n\n“Perhaps the greatest sorrow in the world is when you don’t even know why\nyou’re sad…”\n\n\n“Everything in the world is a cause, if there are no intense changes and if\nthere is nothing that would turn the tides of the world, then it would be\ndifficult for us to see the real nature of people, who are affected by the\nthings in the world…”\n\n\n“Are we all allowed to impose our will on others because our power is greater\nthan theirs?”\n\n\n“Perhaps death was not terrifying at times. What was horrifying was\nendlessness, an eternity of not being able to die and not being able to perish\nuntil the soul itself became numb, until all will was lost, all that made a\nperson, turning him into… an undying soul, Imperishable living corpse…”\n\n\n“Do you know for what reason do all manner of living practice cultivation? For\nwhat reason do we strive to become strong?” the Candle Dragon asked softly with\nits ancient voice. Su Ming could not answer that question, and the Candle\nDragon had not expected him to anyway. “That is because we all have flaws\nwithin our bodies. Each race contains different flaws, and the root cause for\ncultivation is for us to mend those flaws… But it is not a simple task to mend\nall our flaws. Usually, over a countless number of years, only one or two\nliving souls in a single race can be found capable of doing so. “The methods\neach race use to discover their own flaws and set out to mend them are\ndifferent, but in the end, we all need the power of the World Plane… Over the\nlong years of my life, I had some form of contact with Immortals before. The\nconstitution forming the Immortals’ cultivation method is divided into three\nsteps. The first step does not touch upon the power of the World Plane, but\nstarting from the second step, they will go through this so called Nirvanic\nRebirth. All the flaws in their bodies will be mended and they will obtain new\nlife. At that time, they will be practicing the power of the World Plane. “The\nthird step is Hollow. It is the critical moment for one when trying to achieve\nperfection as there’s fewer flaws within the body. If one can attain the\nultimate completion, then he will have taken the fourth step. At that time, he\nwill no longer have any flaws… and at that time, what he will pursue is the\npower of Plane Timelines!” The Candle Dragon’s voice gradually grew weaker.\nThese epiphanies were the treasures of its life. “It is the same for all races,\njust like the Deities who are similar to the Immortals. Amalgamation, Great\nVehicle, Ascension, Great Overarching Golden Immortality , and all the other\nstates are in the end, just to mend all the flaws in people’s bodies.”\n\n\n“There is a binary opposite that exists in the world. If we say that one side\nof the binary opposite is being alive, then we can say that the other side of\nthe binary opposite is being dead… but what exactly is death and what exactly\nis meant by the other side? Who can really say clearly? “Perhaps we can look at\nthe boundary between these two sides as a mirror. When a person standing\noutside the mirror looks into the mirror, the person inside the mirror is also\nlooking outside. He will see himself, but at the same time, he might also not\nbe looking at himself. “Do you mean that the people in mirrors have their own\nlives, and the people outside the mirrors don’t know about it, and that you and\nI are in these mirrors?”\n\n\n“We must break our Life Matrices and tread on the path to find what is lacking\nin our lives. This is called Life Privation!”We must learn of what we lack in\nourselves like we know of the regrets the world possesses and like we\nunderstand the changes in the world. This is Life Palace!”\n\n\n“When you learn who you are, you… are no longer you! When you no longer know\nwho you are, you… will be you!”\n\n\n“What… is Life? It is vitality, because it is a form of inheritance we receive\nwhen we are born. It is also fate, because if we don’t have fate in this\ninheritance of life, our Lives will be incomplete… Life. Vitality. Fate. We are\nborn with Life, but we have to wrestle our own fate from other people’s  hands\nin the future to control it ourselves… The word Life involves people and the\nheavens, and fate is what separates humans from the heavens… Does it mean that\nwe have to bow down to heaven before we can be whole and obtain Life to become\nhumans?” Su Ming mumbled within the ball of blood, and at that moment, he\nopened his eyes. If Life means that we have to subjugate ourselves to the\nheavens before we can call ourselves humans, then the opposite can happen as\nwell, we can still say that we have Life when the heavens bow down to us!”\n\n\n“If you do not have a place in your heart you call home, you will wander no\nmatter where you are…”\n\n\n“What you see might not be the truth, and what you believe doesn’t exist… might\nnot necessarily not exist.”\n\n\n“Thousands upon thousands are awake, but you are still asleep… Is it because\nyou don’t want to wake up, or is it because… you believe yourself to be awake?\nWhat does it mean to be asleep, and what does it mean to be awake? All of this…\nis just the world you see, and no one else… can see it. It is like fate, you\ncan choose to submit to it or to fight against it. It is like life, where\nmoments of joy and sadness exist together. ”\n\n\n“If I’m not bothered by my past, then why should I be bothered by my future? If\nI don’t cling to the idea of who I am, then why should I think about who is\nme…? The high winds may be strong, but they cannot extinguish the flames in my\nheart. Sooner or later… they will set the world ablaze!”\n\n\n“But most of the time, happiness will only last for a short moment, because\nthere is an eye in the world that belongs to loneliness, and it does not want\nto see too many beautiful moments in anyone’s  lives. That was why it made\nfleetingness to be a constant companion of happiness”\n\n\n“This is a point.” As Su Ming spoke softly, a crystalline dot appeared at the\nspot where his index finger was pointing. I will draw towards the left and make\na circle, and when I stop drawing… I will find that the end is in the same\nspot.” Su Ming’s  right index finger started drawing towards the left and he\ndrew a circle. The spot where the circle was completed was its beginning and\nits end. It was the point that fused both the beginning and the end together.\nThis is reincarnation. Then if I draw from the right and make a circle,\nrotating from the end…” As Su Ming spoke, his right index finger started\ndrawing another circle backwards from that dot. The spot that caused the circle\nto be complete… was still the same point. This is also reincarnation. When Su\nMing finished speaking, an indefinable presence radiated off his body. That\npresence was not of those who had attained great completion in the Berserker\nSoul Realm, but was… a presence that surpassed Berserker Soul. It grew thicker\nand surrounded Su Ming’s  body, causing the starry sky to erupt with a bang at\nthe instant it descended and touched it. It was as if the area where Su Ming\nwas had turned into a forbidden region for starlight. His hair flew while his\neyes remained calm. His words contained endless wisdom, and they were echoing\nin the world. Reincarnation is a point, and that point… is Berserkers’ Realm\nMountain. That point is the start and also the end. You can walk to the future\nfrom that point, and you can also head to the past. This point is also the\nmirror’s  point. The mirror’s  face is the normal world. It is where the past\nmoves towards the future. It is then the opposite inside the mirror. Life and\ndeath move in opposite directions. The past and future move in opposite\ndirections. It is just as I have understood it in Hidden Dragon Sect. It is\nlike the process of winter moving to spring… because the people in the world of\nthe mirror move from the future to the past. The Immortals are the face of the\nmirror. They live in the world outside the mirror and move from life to death.\nThe Berserkers’ Yin Death Region is the world inside the mirror. They move from\ndeath to life… During that moment just now, I finally understood. The face and\nback of that mirror don’t make a complete cycle of reincarnation.” Su Ming\nshook his head, and a variety of emotions stirred up slightly in his heart. It\nis just like the existence of Yin and Yang. People only see these two faces,\nbut they forget… that there is another point! The world outside the mirror\nbelongs to the Immortals, and the world inside the mirror belongs to the\nBerserkers. But in truth, there is a mirror inside the mirror. If two mirrors\nwere positioned opposite each other properly, then the endless darkness would\nthen be the mirror inside the mirror!”\n\n\n“When you see the mountain, it is a mountain is the first stage. The first\nstage is in the beginning stages of a human’s  life. It is pure and they have\nonly just begun to know the world. Everything is new, and whatever they see, it\nis the truth. If anyone tells that person something is a mountain, they will\nbelieve that it is a mountain. When you see the mountain, it is not a mountain\nis the second stage. As you grow up, you will experience more things, and you\nwill discover that there are problems in the world. The more problems you\nencounter in the world, the more complicated the world will seem. Many of the\ntruths that you know will be turned upside down. The bad people will rule the\nworld, and the good will find it hard to survive. People will start to get\ncynical and not believe in things so easily. At that time, people will start\ncriticizing the present by saying how good the past is. Then, mountains and\nrivers will not simply be mountains and rivers anymore, because your view of\nthings has changed. You start questioning your beliefs, and you start wondering\nwhether the things you see are really what they seem. When you see the\nmountain, it is still a mountain is the third stage. Since staying in the\nsecond stage bears too much suffering, they will start climbing up this\nmetaphorical mountain, which means to train their minds and hearts. At this\nstage, they will concentrate on doing what they want to do and not compare\nthemselves to others, and since the mind is calm and not bothered by the things\nin the world anymore, people will find that there is no need for them to adjust\ntheir point of view to suit other people’s . At that time, they will feel free\nto perceive whatever they want in whichever way they want, hence you have: When\nyou see the mountain, it is still a mountain, and when you see the river, it is\nstill a river.”\n\n\n“You yourself have to change first, or nothing will change for you!”\n\n\n“Sake sure is nice. You can forget your troubles if only for a moment. You’ll\nhave to remember them tomorrow though, and they’ll be even more painful than\nthey were the night before. You can’t run away from things like this.\nEspecially from things you really want to forget.”\n\n\n“I’ll protect what I want to protect.”\n\n\n“If you have time to fantasize about a beautiful end, then just live\nbeautifully ‘til the end.”\n\n\n“Everyone’s  carrying something that matters. You just don’t realize when\nyou’re carrying it. It’s  only after it slips out of your hand that you realize\nhow heavy it was in the first place. So many times I thought that I’d never\ncarry a load like that ever again. And yet, before I realized it, I was\ncarrying it again. I’d feel so much better if I just got rid of it. But I just\ncan’t bring myself to do it.”\n\n\n“There’s  this one organ that’s  even more important than my heart. You can’t\nsee it, but it’s  there. Because of it, I can stand tall, even if I’m all worn\ndown, I can still walk. And if I don’t go, it’ll break. My soul will break.\nEven if my heart stops beating, it is still more important. It doesn’t matter\nif I’m old and can’t walk. It will still stand tall.”\n\n\n“When your friend is crying, cry with him. When your friend is worried, you\nshould worry with him. And when your friend has an awkward bowel movement, then\nyou must have an awkward bowel movement too. Shin-chan, if you are a friend,\nyou should be able to share the other’s  pain, no matter what. And Shin-chan,\nif your friend goes down the wrong path… then you must stop your friend, even\nif it ruins your friendship. That is true samurai friendship.”\n\n\n“Even if you lose all memory in your head, the ones engraved in your heart and\nthe ones that exist in your soul will never disappear, no matter what happens.”\n\n\n“Life is like a mountain. You can say you have reached the top only after\nclimbing back down.”\n\n\n“If you want to kill me, then go ahead and kill me. You kill those that you\nhate. You turn old those who are more beautiful than you. Even if you continue\nto do so, you will never obtain eternal beauty. Even if you did find the\neternal youth you’re looking for…even if you wear those lavishly beautiful\nkimonos…I’ll say this straight from my heart: You’re ugly; your heart is\ntruly laughably wretched.”\n\n\n“Life is just an important choice after another, keep going forward and watch\nhow far those foolish choices can take you.”\n\n\n“Why change it? This is a life that you chose for yourself, and nothing will\nchange that. You don’t need to fret, nor do you need to be embarassed. No one\nelse can or should choose the path that you walk upon. Just puff out your\nchest, and walk proudly. There’s  nothing wrong with your face. As long\nas your soul doesn’t get scarred, your face will remain beautiful”\n\n\n“Some things can only be seen through a tainted eye.”\n\n\n“If you run into a wall and pretend it doesn’t exist, you’ll never make any\nprogress. The wall will never change, so you’re the one who has to change.”\n\n\n“Tears are handy for washing away troubling and sad feelings. But when you grow\nup, you’ll learn that there are things so sad, they can never be washed away by\ntears. That there are painful memories that should never be washed away. So\npeople who are truly strong laugh when they want to cry. They endure all of the\npain and sorrow while laughing with everybody else.”\n\n\n“Sometimes, it’s  necessary to look back at the past in order to move on to the\nfuture.”\n\n\n“The night is in its darkest just before dawn. But keep your eyes open. If you\navert your eyes from the dark, you’ll be blinded by the rays of a new day. So\nkeep your eyes open, no matter how dark the night ahead may be.”\n\n\n“A prison, you say? You think you’ll be free if you go to the world above? A\ngirl like you can never be free no matter where you go. After all, we humans\nare apes put in a cage called Earth. There is no difference between the world\nabove and the world below. The only difference is which is bigger or smaller.\nPeople who pout about how confining the cage is can never be happy. They live\ntheir lives only seeing the iron bars. A true lack of freedom is when you cage\nyour soul.”\n\n\n“It’s  quite easy for humans to become adults, but to always have a child-like\nheart that makes everything joyful isn’t such an easy task.”\n\n\n“Don’t worry. When people break their old selves they embark a journey to find\ntheir new selves.”\n\n\n“Happiness depends on each person. If you think you’re happy, then you must be\nhappy.”\n\n\n“Some lies are necessary for giving children dreams.”\n\n\n“There’s no such thing as parents who don’t think about their children. But\nthere are few children who understand their parents feelings.”\n\n\n“There are two things people fear… those are death and embarrassment. Those\nwho try to overcome death are just idiots, but I won’t laugh at those who try\nto overcome their embarrassment. I like those kind of idiots”\n\n\n“Planets are just places for people to stand on. Planets are just rocks. It\ntakes people to make it a world. You can have as many “Earths” as you want. I\nonly care about what’s  inside.”\n\n\n“No matter how colossal a power you obtain, no matter how gigantic an army you\nbring with you, I ain’t scared. While you’ve abandoned a hundred, I’ve\nconnected with a thousand. While you’ve destroyed a thousand, I’ve been helped\nby ten thousand. So what’s  an army of a few thousand? We have protected\neverything as just three people.”\n\n\n“I choose my own battlefields. Not by my blood, but by my heart! I stand on the\nbattlefield to protect what’s  important to me. And if anyone stands in my way,\nI don’t care if it’s  one of my kind, my brother or anyone else… I’ll crush\nthem all!”\n\n\n“It’s  as if you are stuck in some predetermined program, following some\npredetermined script. If you really want to live in reality, then fight against\nit. Break through destiny with your own hands, and build your own reality!”\n\n\n“There is no need for any proof. There is no need to create any. We just have\nto live every second to the fullest, and the traces of the path we lived will\nburn into the ground. That will serve as proof of our existence.”\n\n\n“No matter how beautiful a person may be, they will still age and ultimately -\ndie. But even so, even if appearances change, don’t you believe that we have,\nin us, things that don’t change? Even as our bodies crumble, even as the months\nand years take their toll… don’t you believe that we all have something that\ntime can’t spoil? Even if you cover us with wrinkles, we won’t lose to you.\nThat’s  because we know what beauty truly is.”\n\n\n“People need to live their life with a clear conscience. When you want to walk\non a straight path, somehow you get yourself stained with mud. However, as long\nas we never give up, one day the mud on you will dry up and fall off”\n\n\n“Even if you can’t move anymore, or your back gets bent out of shape, it\ndoesn’t matter! What matters is what’s  inside! Not what someone looks like,\ndammit!”\n\n\n“What is right? What is wrong? In this mixed up world, deciding what is right\nand wrong is not easy. You can’t just go by somebody else’s  rules. If you let\nyourself be controlled like that, you’ll just become a puppet that can’t make\ndecisions on it’s  own. You have to live by your rules.”\n\n\n“No matter how hard you blow, you can never extinguish our fire. As long as a\nsingle flare remains, the others can be relit.”\n\n\n“It hasn’t withered. I won’t let it wither. We might just be little branches,\nbut if the branches break off, the tree really will wither. That’s  why I won’t\nbreak off. Even if winter comes and the leaves fall off, even if the wind comes\nand all the other little branches break off… Even if I am the last branch\nleft, I won’t break off. I’m sure we’ll be together till the end.”\n\n\n“Trying to shoulder the burden all by yourself? Don’t be a stranger. Weep and\nask for help. Lean on me with your runny nose. Cry when you feel like crying.\nLaugh when you feel like laughing. When you’re tearing up with an ugly face,\nI’ll give you a good cry with an uglier face. When you’re laughing so hard your\nstomach hurts, I’ll laugh in a louder voice. That’s  how it should be. It’s far\nbetter to get dirty while living true to yourself, than to throw away yourself\nand die a clean death.”\n\n\n“The country? the skies? You can have them. I’m busy just protecting what’s\nright in front of me. I don’t know what’ll happen to me in the future, but if\nsomething has fallen at my feet, then the least I can do is pick it up.”\n\n\n“The zipper is a window to society.”\n\n\n“The best way to live a full life is to be a child, no matter your age.”\n\n\n“It may be tough now, but the worst is surely yet to come. Keep that in mind,\nand you’ll be fine.”\n\n\n“That’s  a good attitude. You should hate me more, curse me more, and detest\nme! Then you should take the power of that hatred and use it to survive in this\nrotten world”\n\n\n“If Only Life Was as Beautiful as It Seemed at First Sight”\n\n\n“There is no light for this who do not know the darkness. Live on and endure\nthe shadows and brightness shall come your way.”\n\n\n“The more one tries to look away, the more one gets pre occupied. Once your\nheart is preoccupied, your sword will not be true. Then you will die. Dont be\npreoccupied with a single spot. See everything in its entirety effortlessly.”\n\n\n“Be aware of yourself. And accept yourself as you are. Thats where your\ntraining should begin.”\n\n\n“Our lives are not our own. From womb to tomb, we are bound to others. Past and\npresent. And by each crime and every kindness, we birth our future.”\n\n\n“Belief, like fear or love, is a force to be understood as we understand the\nTheory of Relativity and Principles of Uncertainty: phenomenon that determine\nthe course of our lives. Yesterday, my life was headed in one direction. Today,\nit is headed in another. Yesterday I believed that I would never have done what\nI did today. These forces that often remake time and space, that can shape and\nalter who we imagine ourselves to be, begin long before we are born and\ncontinue after we perish. Our lives and our choices, like quantum trajectories,\nare understood moment to moment. At each point of intersection, each encounter\nsuggests a new potential direction.”\n\n\n“I understand now that boundaries between noise and sound are conventions. All\nboundaries are conventions, waiting to be transcended. One may transcend any\nconvention if only one can first conceive of doing so. Moments like this, I can\nfeel your heart beating as clearly as I feel my own, and I know that separation\nis an illusion. My life extends far beyond the limitations of me.”\n\n\n“Truth is singular. Its “versions” are mistruths.”\n\n\n“This world spins from the same unseen forces that twist our hearts.”\n\n\n“I believe death is only a door. When it closes, another opens. If I cared to\nimagine a heaven, I would imagine a door opening and behind it, I would find\nhim there.”\n\n\n“Fear, belief, love phenomena that determined the course of our lives. These\nforces begin long before we are born and continue after we perish.”\n\n\n“There is only one rule that binds all people. One governing principle that\ndefines every relationship on God’s  green earth: The weak are meat, and the\nstrong do eat.”\n\n\n“We cross and re-cross our old paths like figure-skaters.”\n\n\n“To be is to be perceived. And so to know thyself is only possible through the\neyes of the other. The nature of our immortal lives is in the consequences of\nour words and deeds that go on apportioning themselves throughout all time.”\n\n\n“You have to do whatever you can’t not do.”\n\n\n“I was not genomed to alter reality. No revolutionary ever was.”\n\n\n” I will not be subjugated to criminal abuse!”\n\n\n“A half-finished book is, after all, a half finished love affair.”\n\n\n“Travel far enough, you meet yourself.”\n\n\n“Books don’t offer real escape, but they can stop a mind scratching itself\nraw.”\n\n\n“Power, time, gravity, love. The forces that really kick ass are all\ninvisible.”\n\n\n“Unlimited power in the hands of limited people always leads to cruelty.”\n\n\n“Fantasy. Lunacy. All revolutions are, until they happen, then they are\nhistorical inevitabilities.”\n\n\n“Time is what stops history happening at once; time is the speed at which the\npast disappears.”\n\n\n“What wouldn’t I give now for a never-changing map of the ever-constant\nineffable? To possess, as it were, an atlas of clouds.”\n\n\n“The better organized the state, the duller its humanity.”\n\n\n“How vulgar, this hankering after immortality, how vain, how false. Composers\nare merely scribblers of cave paintings. One writes music because winter is\neternal and because, if one  didn’t, the wolves and blizzards would be at one’s\nthroat all the sooner.”\n\n\n“Anticipating the end of the world is humanity’s  oldest passtime”\n\n\n“Why fight the ‘natural’ (oh, weaselly word!) order of things? Why? Because of\nthis—one fine day, a purely predatory world shall consume itself. In an\nindividual, selfishness uglifies the soul; for the human species, selfishness\nis extinction.”\n\n\n“Mother used to say escape is never further than the nearest book.”\n\n\n“What sparks wars? The will to power, the backbone of human nature. The threat\nof violence, the fear of violence, or actual violence, is the instrument of\nthis dreadful will. You can see the will to power in bedrooms, kitchens,\nfactories, unions and the borders of states. Listen to this and remember it.\nThe nation state is merely human nature inflated to monstrous proportions. QED,\nnations are entities whose laws are written by violence. Thus it ever was, so\never shall it be.”\n\n\n“Women, oh, women! They’ll find the baddest meanin’ in your words an’ hold it\nup, sayin’, Look what you attacked me with!”\n\n\n“If war’s  first victim is truth, its second is clerical efficiency.”\n\n\n“Leaves turned to soil beneath my feet. Thus it is, trees eat themselves.”\n\n\n“The learnin’ mind is the livin’ mind… an’ any sort o’ smart is truesome\nsmart, old smart or new, high smart or low.”\n\n\n“All rising suns set.”\n\n\n“Hey, metaphysics seminar is on the roof. Just take the elevator up and keep\nwalking until you hit the sidewalk. Anything is true if enough people believe\nit.”\n\n\n“An abyss cannot be crossed in two steps.”\n\n\n“Old Father Timothy offers this advice to his younger readers, included for\nfree in the price of this memoir: conduct your life in such a way that, when\nyour train breaks down in the eve of you years, you have a warm dry car driven\nby a loved one - or a hired one, it matters not - to take you home.”\n\n\n“He chiseled open the fault lines in the others’ personalities.”\n\n\n“Fear hardens caution, but boredom erodes it.”\n\n\n“One loses one’s  eye in the lanes of sea phosphorescence &amp; the Mississippi of\nstars streaming across the heavens.”\n\n\n“I envied my uncritical, unthinking sisters.”\n\n\n“If people praise you, you’re not walking your own path.”\n\n\n“It’s a small world. It keeps recrossing itself.”\n\n\n“all purebloods have a hunger, a dissatisfaction in their eyes,”\n\n\n“Old Ma Yibber spread the news that Zachry what came down off Mauna Kea weren’t\nthe same Zachry what’d gone up, an’ true ‘nuff I s’pose, there ain’t no journey\nwhat don’t change you some.”\n\n\n“Blame its user, blame its maker, but don’t blame the gun.”\n\n\n“Yay, Old Uns’ Smart mastered sicks, miles, seeds an’ made miracles ord’nary,\nbut it din’t master one thing, nay, a hunger in the hearts o’ humans, yay, a\nhunger for more. More what? I asked. Old Uns’d got ev’rythin’. Oh, more gear,\nmore food, faster speeds, longer lifes, easier lifes, more power, yay. Now the\nHole World is big, but it weren’t big ‘nuff for that hunger what made Old Uns\nrip out the skies an’ boil up the seas an’ poison soil with crazed atoms an’\ndonkey ‘bout with rotted seeds so new plagues was borned an’ babbits was\nfreak-birthed. Fin’ly, bit’ly, then quicksharp, states busted into bar’bric\ntribes an’ the Civ’lize Days ended, ‘cept for a few folds’n’pockets\nhere’n’there, where its last embers glimmer.”\n\n\n“ignorance of the Other engenders fear; fear engenders hatred; hatred engenders\nviolence; violence engenders further violence until the only “rights,” the only\nlaw, are whatever is willed by the most powerful.”\n\n\n“There’s the blind, Mr. Grimaldi, there’s the willfully blind, and then there’s\nthe soon to be retired.”\n\n\n“The dumbest dog can sit and watch. What takes brains is knowing when to look\naway.”\n\n\n“But there’s  one thing you must remember. Whether you want it or not, once you\nhave created bonds between you and other people, those bonds will never\ndisappear.”\n\n\n“Never lose sight of your wish! And if you want to see the wish fulfilled …\nyou must choose! No matter how painful the choice may be.”\n\n\n“All happiness and all unhappiness … stems from one having a desire. And that\nis why mankind will always make their wishes.”\n\n\n“All who make wishes are the same. When one wish comes into conflict with\nsomeone else’s  wish … then one must make a choice. Either abandon one’s  own\nwish … or crush the other’s  wish for the sake of your own.”\n\n\n“The princess is strong. And because she’s  strong, she’s  fragile. If somebody\ndoesn’t teach her that fact, she’ll break.”\n\n\n“I can’t do much yet, but even if I can do a little to help … I want to give\nit all I have! If a person doesn’t do anything, they never get any better.\nDoing one little thing, taking one little step forward … I gotta believe it\nwill help build a better future!”\n\n\n“You’ll stay here with me?” “Yeah” “If I fall asleep like this … the first\nthing i’ll see when I wake up … will be you.”\n\n\n“The instand one gives up, that is when it all ends. Keep wishing. Wish\nstrongly! Wish hard! Do not let it matter what kind of being you are! Do not\nlet it matter what pressures others put on you! Continue to wish for that which\nyour heart truly desires!!”\n\n\n“If you don’t want her to go, you should say it. Those jerks who can’t say a\nword no matter how much time passes … I just don’t get them. If they’re doing\nwhatever the hell they want … then you should do what you want too. I hate\nthose jerks who fool themselves into thinking that just because they clam up,\nnobody knows what’s  going on with them!!”\n\n\n“Until we can be together again … we wait … and believe!”\n\n\n“I believe in the princess when she says that she’ll return to those waiting\nfor her. So i’ll wait. It’s  more painful to wait than to go along on the\ntrip.” ‘Well, I can’t wait.’ “Are you that afraid to believe in\nsomeone?”\n\n\n“But there are no coincidences in this world. There is only Hitsuzen. You were\ndestined to meet each other.”\n\n\n“I’d rather stay the way I am until the last moment. Even if a monster beats me\nand I die. I won’t lose to this game or this world, no matter what.”\n\n\n“A person’s strength in this world is just an illusion.”\n\n\n“They say your character is built by life’s challenges, so keep soldiering on\nyoung man.”\n\n\n“Even in a world like this, he was really living.”\n\n\n“I’d rather trust and regret, than doubt and regret.”\n\n\n“It is pointless to question who someone really is. All you can do is believe\nand accept. Because the way you perceive someone is their true identity.”\n\n\n“When I began thinking of him as I fell asleep, I stopped having nightmares. I\nbegan to look forward to seeing him. For the first time since I arrived here, I\nwas happy.”\n\n\n“Life isn’t just doing things for yourself. It’s possible to live in such a way\nthat other people’s happiness, makes you happy too.”\n\n\n“Sometimes the things that matter the most are right in front of you.”\n\n\n“I cried alone every single night. It felt like every day that passed here\nstole another piece of my real life away. After I cried, I’d go and fight as\nhard as I could. My only thought was winning, moving forward and getting\nstronger.”\n\n\n“I’m disinclined to acquiesce to your request.”\n\n\n“If you choose to lock your heart away, you’ll lose it for certain”\n\n\n“Better to not know which moment may be your last alive to be mystery of it\nall.”\n\n\n“Did no one come to save me just because they missed me?”\n\n\n“Develop amnesia conveniently and forget everything you heard!”\n\n\n“Please make sure the bed is empty before getting in it!”\n\n\n“Getting wrapped up in worries is bad for your body and spirit. That’s when you\nmust short out your logic circuits and reboot your heart.”\n\n\n“Everybody makes a wrong turn once in a while”\n\n\n“Strong Pokemon. Weak Pokemon. That is only the selfish perception of people.\nTruly skilled trainers should try to win with all their favorites.”\n\n\n“A Caterpie may change into a Butterfree, but the heart that beats inside\nremains the same.”\n\n\n“I see now that one’s birth is irrelevant. It’s what you do that determines who\nyou are.”\n\n\n“There’s no sense in going out of your way to get somebody to like you.”\n\n\n“It’s more important to master the cards you’re holding than to complain about\nthe ones your opponent was dealt.”\n\n\n“Do you always need a reason to help somebody?”\n\n\n“You see, sometimes friends have to go away, but a part of them stays behind\nwith you.”\n\n\n“If we live a monotonous life, do we get used to it and stop thinking about how\nto change it?”\n\n\n“If somewhere in this world there is someone who understands you, it feels like\nthat person is right beside you, even if youre as far apart as the end of the\nland and the top of the sky”\n\n\n“So, this is my power… but what is my purpose?”\n\n\n“The Earth is so pretty, so blue…”\n\n\n“A good friend left me and, I miss her, every day… but I… I know we’ll\nalways be friends forever!”\n\n\n“To all of my beautiful future children who taught me how to believe. I offer\nall of you now my sincerest hopes, and wishes that the future world will become\nmore and more beautiful. And that is exactly the kind of world you find\nyourselves in.”\n\n\n“We’re all architecting our lives day by day, and when our life comes to an\nend, people will see what mattered to us most by looking at what we built. When\nyou look back on your life, you’ll see what you’ve built and you’ll see what\nyou’ve loved…and you’ll realize that you build what you love. Your most\npowerful artifact in which you build your life upon are people. When you\nunderstand this, you begin to realize that your building blocks are the\nrelationships you have, sustain, and influence. don’t just want to be a builder\nof applications. I don’t just want to be a builder of systems. I don’t just\nwant to be a builder of brands. I want to be a builder of people, using those\navenues to do so. When lay on my deathbed, I want to look back on my life and\nsee that what I built was people. I want to see that what I built was the\nproduct of a love for humanity. You build what you love. What are you\nbuilding?’\n\n\n“Which is worse, to live as a monster or to die as a good man?”\n\n\n“Sanity is not a choice Marshall, you can’t just choose to get over it.”\n\n\n“What if while you were looking into them, they were looking into you?”\n\n\n“Wounds can create monsters and you are wounded, martial. And wouldn’t you\nagree, when you see a monster, you must stop it?”\n\n\n“God loves violence. I… I hadn’t noticed. Sure you have. Why else would there\nbe so much of it? It’s  in us. It’s  what we are. We wage war, we burn\nsacrifices, and pillage and plunder and tear at the flesh of our brothers. And\nwhy? Because God gave us violence to wage in his honor. I thought God gave us\nmoral order. There’s  no moral order as pure as this storm. There’s  no moral\norder at all. There’s  just this: can my violence conquer yours??\n\n\n“Do you know how pain enters the body, Marshal? Do you? Depends on where you’re\nhurt? No, it has nothing to do with the flesh. The brain controls pain.”\n\n\n“That’s  the beauty of it. Mental Patients make the perfect subjects, if they\ntalk nobody listens to them!”\n\n\n“He wanted to ask her what sound a heart made when it broke from pleasure, when\njust the sight of someone filled you the way food, blood, and air never could,\nwhen you felt as if you’d been born for only one moment and this, for whatever\nreason, was it.”\n\n\n“This world can only give me reminders of what I don’t have, can never have,\ndidn’t have for long enough.”\n\n\n“The brain controls pain. It controls fear. Sleep. Empathy. Hunger. Everything\nwe associate with the heart or the soul or the nervous system is actually\ncontrolled by the brain. Everything. What if you could control it?”\n\n\n“If you are deemed insane, then all actions that would oherwise prove you are\nnot do, in actuality, fall into the framework of an insane person’s actions.\nYour sound protests constitute denial. Your valid fears are deemed paranoia.\nYour survival instincts are labeled defense mechanisms. It’s a no-win\nsituation. It’s a death penalty really.”\n\n\n“Waking, after all, was an almost natal state. You surfaced without history,\nthen spent the blinks and yawns reassembling your past, shuffling the shards\ninto chronological order before fortifying yourself for the present.”\n\n\n“How much violence, Marshal, do you think a man can carry before it breaks\nhim?”\n\n\n“Everyone wants a quick fix. We’re tired of being afraid, tired of being sad,\ntired of feeling overwhelmed, tired of feeling tired. We want the old days\nback, and we don’t even remember them, and we want to push into the future,\nparadoxically, at top speed. Patience and forbearance become the first\ncasualties of progress.”\n\n\n“She said once that time is nothing to me but a series of bookmarks that I use\nto jump back and forth through the text of my life, returning again and again\nto the events that mark me in the eyes of my more astute colleagues, as bearing\nall the characteristics of the classic melancholic.”\n\n\n“He lay on his side, looking out at the sea. So blue at this time of day, so\nvibrant as the afternoon died around it. He lay there feeling the breeze on his\nface and the sea spreading out forever under the darkening sky and he felt so\nsmall, so utterly human, but it wasn’t a debilitating feeling. It was an oddly\nproud one. To be a part of this. A speck, yes. But part of it, one with it.\nBreathing.”\n\n\n“I can’t take second or third. I can only take first.”\n\n\n“Dare to think. Before your dreams are realized, never establish any limits\nyourself, never give yourself a ny excuse to shrink back, any justification for\nfailure. Only in this way do we have the chance to take those seemingly distant\ndreams and make them into genuine reality. This, is precisely my first lesson\nto you.”\n\n\n“The identities we live in end up being the roles we play, whether it’s\nprincess, empress, wife, or mother. “But as we act longer and longer, playing\nmore and more roles, we often forget just who we are. “If you can’t even be\nsure of what role you’re playing, how can you determine what it is you want? If\nwe want to a get clear and truthful answer, we have to look back at where we\ncame from, reverse time to where it all began. We have to remember what we\nfirst saw when we opened our eyes to this world”\n\n\n” Kind people need to be even more on their guard… Being on guard requires\nthe corresponding ability, or else it will be nothing more than a joke.”\n\n\n“How could one prevent oneself from being confused by external things? How\ncould one possess an unshakable will and self-confidence? Only one word needed\nto be remembered: heart. All one needed was to convince themselves. If one\ncould convince oneself that this way was correct, that it was in accordance\nwith one’s  heart, then…one would naturally be following one’s  heart. This\nsounded very simple, but it was not actually simple at all If one searched in\nthe deepest depths of one’s  soul, if one ensconced oneself in a dark room cut\noff from the world, how many people could truly say that they were without\nregrets? Who could so firmly believe that everything they had done was\ncorrect?”\n\n\n‘Waters can carry a boat, and they can also capsize them. Shang Xingzhou\ncontinued, “Of course, following the current does not mean obedience. The boat\ncan only hope that the waters are calmer, that there are fewer waves, that\nthere is not too much resistance.” Yu Ren gestured, “But in the final analysis,\nthe boat must still revere the existence of the waters.” “The Duke of Wei once\nsaid, ‘The resentment of this minister need not be feared; only the people\nshould be feared. They can carry the boat and capsize the boat, so they must be\ntreated with deep caution.3 How could I not fear them?” Shang Xingzhou looked\ninto You Ren’s  eyes and said, “But positions are relative. Since you are the\nboat, you cannot think too much about what the water is thinking.”\n\n\n“To walk through the world is like sailing a boat across the ocean. One must be\ncautious and mindful, and one cannot go against the current, or else one will\ncapsize the boat.”\n\n\n“The person who understood you the most was naturally not a relative, or else\nXue Xing Chuan would not have died so miserably and then almost had his corpse\nexposed in the plains after his death. And the person who understood you the\nmost was also not necessarily, as often written in books, your enemy, because\nyou would always have some wariness towards your enemy and develop many\nsafeguards against him. The person who understood you the most was also not\nnecessarily your friend. To be friends until your hair turned white was a\nbeautiful thing, but you would spend too little time with each other, the\ndistance between your two cities would be too far. When you met, you would\nalways drink wine while recalling old times, speculating on the future, cursing\nyour past teachers or the current government. There were few opportunities to\nchat about more in-depth things. So the person who understood you the most was\noften your partner at work. With year after year, day after day of working\ntogether, it would be very difficult to not understand each other. You would\ndrink together many times, chatting about many in-depth things, and for the\nsake of both open and hidden competitions, you would remember all these things\nwith remarkable clarity, preparing to use them at any point in the future. For\ninstance, he might learn which restaurant is your favorite for buying box\nlunches and you might learn which restaurant has his favorite noodles. He might\nlearn which group leader you hate the most and you might learn which TV channel\nis his favorite. He might know of all the girlfriends you’ve talked about in\nthe past few years while you would know how many people he’s  been cheating on\nin the past few months. On the morning after Christmas Eve, the two of you\nmight even come out of the same pub and then smile at each other, because this\npub was the place where the company could negotiate the best discount.”\n\n\n“Every person is born a small person.” The Pope smiled and gestured with his\ntwo hands to show length. “But every person will grow bigger. There are some\nmatters that you can learn as long as you are willing to learn them.”\n\n\n“Position is relative, as is importance. To create a balance between position\nand importance, thus preventing the entire world from dancing according to the\nwhims of people like us, is what I have wanted to do throughout these past few\nyears.”\n\n\n“To eat when you are hungry, to sleep when you are drowsy, to take medicine\nwhen you are sick, and to bury someone’s  body after they die, these are truly\nthe greatest principles.”\n\n\n“That which is most impervious to poison is the human heart, and the human\nheart is human nature, and human nature is to live-what’s  wrong with that?”\n\n\n“Thousands of years pass, the white clouds wander carelessly, things are the\nsame but the people are not, the sapling has grown into a lush canopy’. But in\nthe end, there were still some people or matters that could not be let go.”\n\n\n“Good people do not live long.”\n\n\n“All things have a beginning and an end. Even those of Concealed Divinity and\nthose above, who have obtained Grand Liberation, have a birth and death.”\n\n\n“Things easily decay, but the effects they leave are everlasting. Ultimately,\none must see how deep the tracks are that one has left.” The Divine Empress\nturned and looked at her, continuing, “And those tracks come from your and my\nfootsteps, follow the directions of our hearts.” Xu Yourong asked, “And if\nsomeone blocks your way?” The Divine Empress answered, “So we need the strength\nto kill all those who obstruct us. Only this way can we march the world forward\naccording to our desires, to brand our souls upon history such that even the\nreprimands of tens of thousands of people after we depart cannot wipe it away.\nOnly this way can we get close to true eternity.”\n\n\n“Affection is the world’s  most cheaply bought item, virtue an excuse for the\nweak to protect themselves None of them are important.” Xu Yourong asked, “Then\nwhat is the most important thing?” The Divine Empress looked up towards the sky\nand leisurely said, “To exist.” After a moment of silence, Xu Yourong asked,\n\n\n“How should we exist?” “How to exist? Take all that is wondrous, see how long\ncan one exist, how can one make the soul inextinguishable, and proceed in the\ndirection of the Great Dao.”\n\n\n“When the sky is shattering and stars are falling, when you find it simply\nimpossible to make any sort of rational judgment and can only rely on what your\nheart is feeling at that moment, that is what your heart truly feels.”\n\n\n“No matter who the target is or what difference exists between the choices, in\nthe end, you still made a choice.” “So?” “Sweet or salty, to peel or not to\npeel, to live or to die these have always been questions.” The Elder of\nHeavenly Secrets looked into his eyes, his voice calm. “Life is formed of\ninnumerable choices. Who can avoid them completely?”\n\n\n“On the verge of death, even the words are kind, let alone one’s  intentions.”\n\n\n“Do you know when people are the most courageous?” “When confronting death?\n\n\n“It’s  not wrong, but there is another situation…because of love.” The Divine\nEmpress looked out the window at the dark palace and continued, “In other\nwords, when driven by passion.”\n\n\n“Good people aren’t guaranteed to be rewarded, and they might not even live\nwell, so why do we have to be good people? How should we love? Why should we\nlive??”\n\n\n“In hazy dreams, I curse the time passed, Ah my home, thirty-two years have\ngone. The red flag stirred the peasant to take up the halberd, While the black\nhand held high the tyrant’s  whip. Only because one seeks grand goals are there\nmany sacrifices, And I dare to order the sun and moon to shine over new skies.\nIn joy, I see the wave after wave of beans, And the heroes from all-over\nreturning in the evening mist.”\n\n\n“Dont tell me what they said about me. Tell me why they were so comfortable to\nsay it around you.”\n\n\n“Well the meek were supposed to inherit the earth, but instead it has e to the\nyoung-the technically inclined, those who stare into video games rather than\ninto their own souls.”\n\n\n“Despise chaos. Create order”\n\n\n“For the human brain,” Edmond explained, “any answer is no answer. We feel\nenormous discomfort when faced with insufficient data and so our brains invent\nthe data- offering us, at the very least the illusion of order-creating myriad\nphilosophies, mythologies, and unseen world.” religions to reassure us that\nthere is indeed an order and structure to the unseen world”\n\n\n“The roads to salvation are many.Forgiveness is not the only path.”\n\n\n“The truth is something you should find without magic”\n\n\n“It is very hard for evil to take hold of the unconsenting soul.”\n\n\n“But it is one thing to read about dragons and another to meet them.”\n\n\n“From that time forth he believed that the wise man is one who never sets\nhimself apart from other living things, whether they have speech or not, and in\nlater years he strove long to learn what can be learned, in silence, from the\neyes of animals, the flight of birds, the great slow gestures of trees.”\n\n\n“Go to bed; tired is stupid.”\n\n\n“You thought, as a boy, that a mage is one who can do anything. So I thought,\nonce. So did we all. And the truth is that as a man’s  real power grows and his\nknowledge widens, ever the way he can follow grows narrower: until at last he\nchooses nothing, but does only and wholly what he must do… .”\n\n\n“It is no secret. All power is one in source and end, I think. Years and\ndistances, stars and candles, water and wind and wizardry, the craft in a man’s\nhand and the wisdom in a tree’s  root: they all arise together. My name, and\nyours, and the true name of the sun, or a spring of water, or an unborn child,\nall are syllables of the great word that is very slowly spoken by the shining\nof the stars. There is no other power. No other name.”\n\n\n“For a word to be spoken, there must be silence. Before, and after.”\n\n\n“In that moment Ged understood the singing of the bird, and the language of the\nwater falling in the basin of the fountain, and the shape of the clouds, and\nthe beginning and end of the wind that stirred the leaves; it seemed to him\nthat he himself was a word spoken by the sunlight.”\n\n\n“Only in silence the word, only in dark the light, only in dying life: bright\nthe hawk’s  flight on the empty sky.”\n\n\n“Infinite are the arguments of mages”\n\n\n“And he would watch the snow falling, thin and ceaseless, on the empty lands\nbelow the window, and feel the dull cold grow within him, till it seemed no\nfeeling was left to him except a kind of weariness.”\n\n\n“I am yours by parentage and custom and by duty undertaken towards you. I am\nyour wizard. But it is time you recalled that, tough I am a servant, I am not\nyour servant.”\n\n\n“Heal the wound and cure the illness, but let the dying spirit go”\n\n\n“But the death of a great mage, who has many times in his life walked on the\ndry steep hillsides of death’s kingdom, is a strange matter: for the dying man\ngoes not blindly, but surely, knowing the way.”\n\n\n“The path never reached it, though it always seemed to be about to.”\n\n\n“And he began to see the truth, that Ged had neither lost nor won but, naming\nthe shadow of his death with his own name, had made himself whole: a man: who,\nknowing his whole true self, cannot be used or possessed by any power other\nthan himself, and whose life therefore is lived for life’s sake and never in\nthe service of ruin, or pain, or hatred, or the dark.”\n\n\n“Freedom is a heavy load, a great and strange burden for the spirit to\nundertake. It is not easy. It is not a gift given, but a choice made, and the\nchoice may be a hard one. The road goes upward towards the light; but the laden\ntraveler may never reach the end of it.”\n\n\n“The Earth is beautiful, and bright, and kindly, but that is not all. The Earth\nis also terrible, and dark, and cruel. The rabbit shrieks dying in the green\nmeadows. The mountains clench their great hands full of hidden fire. There are\nsharks in the sea, and there is cruelty in men’s eyes. And where men worship\nthese things and abase themselves before them, there evil breeds. There places\nare made in the world where darkness gathers, places given wholly to the Ones\nwe call Nameless, the ancient and holy powers of the Earth before the Light,\nthe powers of the dark, the ruin, the madness”\n\n\n“Who said you fall in love only once? Love is such a versatile feeling Its\ndifferent every damn time Sometimes its a warm feeling Like the feeling of\ncoming home Alternately it is also an all consuming fire Which threatens to\nburn you inside out It is the breeze and the storm It is a hearth and a inferno\nIt will nurture you and destroy you It will make you strong and weak It will\nlift you up and bring you down But it is am amazing feeling every time It makes\neven an hour an eternity And two years feel like tomorrow It is the most\namazing kind of bitter sweet The feeling of two souls interwined To hold and to\ncherish till fate do us part”\n\n\n“Daybreak makes all earth and sea, from shadow brings forth form, driving dream\nto the dark kingdom.”\n\n\n“You have a finite amount of willpower that becomes depleted as you use it.”\n\n\n“What are you watching there?” the Archmage asked, and the other answered, “A\nspider.” Between two tall grass blades in the clearing a spider had spun a web,\na circle delicately suspended. The silver threads caught the sunlight. In the\ncenter the spinner waited, a grey-black thing no larger than the pupil of an\neye. “She too is a patterner,” Ged said, studying the artful web. “What is\nevil?” asked the younger man. The round web, with its black center, seemed to\nwatch them both. “A web we men weave,” Ged answered\n\n\n“With your whole mind you sit with painful legs without being disturbed by\nthem. This is to sit without”\n\n\n“Life is a series of pulls back and forth. You want to do one thing, but you\nare bound to do something else. Something hurts you, yet you know it shouldn’t.\nYou take certain things for granted, even when you know you should never take\nanything for granted.”\n\n\n“But engineering isn’t about perfect solutions; it’s  about doing the best you\ncan with limited resources.”\n\n\n“If you have a question,” my folks would say, “then find the answer.”\n\n\n“Part of that is because if you dispense your own wisdom, others often dismiss\nit; if you offer wisdom from a third party, it seems less arrogant and more\nacceptable.”\n\n\n“When you’re screwing up and nobody says anything to you anymore, that means\nthey’ve given up on you.”\n\n\n“So that was a setback. But I kept my mantra in mind: The brick walls are there\nfor a reason. They’re not there to keep us out. The brick walls are there to\ngive us a chance to show how badly we want something”\n\n\n“You appreciate the part of me that  didn’t get angry because two `things’ we\nown got hurt. But the flip side of that is my belief that you don’t repair\nthings if they still do what they’re supposed to do. The cars still work. Let’s\njust drive ‘em.”\n\n\n“Not everything needs to be fixed.”\n\n\n“Let’s  saddle up and ride.”\n\n\n“But, look, I’m a scientist who sees inspiration as the ultimate tool for doing\ngood.”\n\n\n“Give yourself permission to dream. Fuel your kids’ dreams, too. Once in a\nwhile, that might even mean letting them stay up past their bedtimes.”\n\n\n“If you wait long enough,” he said, “people will surprise and impress you.”\n\n\n“It took a long time, but I’ve finally figured it out. When it comes to men who\nare romantically interested in you, it’s  really simple. Just ignore everything\nthey say and only pay attention to what they do.”\n\n\n“Experience is what you get when you  didn’t get what you wanted. And\nexperience is often the most valuable thing you have to offer.”\n\n\n“There is more than one way to measure profits and losses. On every level,\ninstitutions can and should have a heart.”\n\n\n“When we’re connected to others, we become better people.”\n\n\n“Sometimes, all you have to do is ask, and it can lead to all your dreams\ncoming true.”\n\n\n“Am I a fun-loving Tigger or am I a sad-sack Eeyore?”\n\n\n“What do we mean by G.o.d? Being detached from life and death as well as being\none with heaven and earth like G.o.d! In this aspect, we are no different from\nthe elementalists. They also believe that they have a common origin with heaven\nand earth and they will live forever. In short, they are following the path of\nthe cultivators from the past. However, we are different. We walk in between\nthe path of life and death. We treat life and death as an integral whole, that\nthey are similar. Our theory is profound. Walking between life and death is\nakin to walking on a tightrope between high cliffs. Below our feet is an\nunknown abyss. In order to succeed, our hearts and minds must be resolute. We\nmust not hesitate, else we will definitely fall into an situation with no hope\nof reprieve.\n\n\n“You have to complete the path you chose even if it’s going to kill you,” the\nred-dressed girl said plainly.\n\n\n“If one works for it and he doesn’t succeed, then it can be said that his fate\nhad already been decided. However, if one never even tries then serves him\nright.”\n\n\n“One can either choose to let others control his life, or he can choose to\ncontrol others’ lives.”\n\n\n“There were four seasons in life. The spring of adolescence, the summer of\nyouthfulness, the autumn of confidence, and the winter of obsolescence.\nHowever, there would only be one cycle of life.”\n\n\n“From mankind’s perspective, the world was ever-changing. Perhaps from the\nworld’s perspective, it was mankind that was ever-changing.”\n\n\n“A strong body needs an equally mighty soul.”\n\n\n“You’re still young. When you’re older, you will understand that time is money.\nYou won’t give your money to someone who has nothing to do with you, right?\nThen why are you wasting time on them?”\n\n\n“I feel that you’re like me: someone who has desire. My desire is to become a\nGrandmaster and defeat Dai Gang. I don’t know what your desire is and whether\nit is easier or harder to achieve than mine, but if you have a desire, you have\nto be hard on yourself. Mediocrity is not enough to make your desire come\ntrue.”\n\n\n“When a Grandmaster is alive, he defeats all heroes under the sky. When he\ndies, he stirs all clouds under the sky. Look, the sun and moon lose their\nsplendor and the stars dim. The sky is so high, the earth is so vast. All\nliving things gather to grieve. How heroic! How delightful! Living a life\nwithout being a Grandmaster is unsatisfying!”\n\n\n“If you’re short on money, everyone else is a beggar.”\n\n\n“He viewed each and every challenge from the previous half of his life as\nfate’s  way of refining his very being. Not once did he lose hope, give himself\nup to anger, or whimper in sorrow. His heart was as pure as that of a newborn.”\n\n\n“A human mind was complicated yet centralized at the same time. It was a world…\na complete, diverse world that was beyond mankind’s  understanding. It could\nemit light and warmth like the sun, giving warmth to the souls surrounding it.\nHowever, it could also pile up shadows that were darker than the night in a\nsecluded corner.  It could be impregnable, enduring the cruelest torture and\nthe deepest agony in the world. It could also be soft like an air bubble that\ncould be shattered easily with a gentle poke from a crisp toothpick. Nobility\nand malevolence could be buried in the same tomb while courage and cowardice\nwere like two vines that entwined each other. It was extremely difficult to\nmake sense of them and tell them apart.”\n\n\n“As the saying goes, it is only when one experiences true knowledge that he\nfinally understands how little he knows”\n\n\n“But magic is also an art, great lady,” the Murgo said. “There are many who\nthink so,” Aunt Pol said, “but true magic comes from within and is not the\nresult of nimble fingers which trick the eye.”\n\n\n“Everything is idiocy if you choose to look at it in the proper light,”\n\n\n“Why are the people all so unhappy?” he asked Mister Wolf.”They have a stern\nand demanding God,” Wolf replied.”Which God is that?” Garion asked.”Money,”\n\n\n“It is power that teaches patience; holding power, I mean. And you learn the\nprice it exacts—which is something I never knew when I was your age and thought\na sword and quick wits could deal with anything. I never knew the price you pay\nfor power.”\n\n\n“Authority? You want the authority to create, to be noticed, and to make a\ndifference? You’re waiting for permission to stand up and speak up and ship?\nSorry, there’s no authority left. Oprah has left the building. She can’t choose\nyou to be on her show because her show is gone. Youtube wants you to have your\nown show now, but they’re not going to call you. Dick clark has left the\nbuilding. He’s not going to be able to get you a record deal or a tv gig\nbecause he and his show are long gone. Itunes and a hundred other outlets want\nyou to have your own gig, but they’re not going to call you, either. Neither is\nrodney dangerfield or the head of programming at comedy central. Marc\nmarondidn’t wait to be cast on saturday night live — he started his own podcast\nand earned a million listeners. Our cultural instinct is to wait to get picked.\nTo seek out the permission, authority, and safety that come from a publisher or\na talk-show host or even a blogger who says “i pick you.” Once you reject that\nimpulse and realize that no one is going to select you — that prince charming\nhas chosen another house in his search for cinderella — then you can actually\nget to work. The myth that the ceo is going to discover you and nurture you and\nask you to join her for lunch is just that, a hollywood myth. Once you\nunderstand that there are problems waiting to be solved, once you realize that\nyou have all the tools and all the permission you need, then opportunities to\ncontribute abound. The opportunity is not to have your résumé picked from the\npile but to make the pile irrelevant by leading without having to be asked.\nWhen we take responsibility and eagerly give credit, doors open. When we grab a\nmicrophone and speak up, we’re a step closer to doing the work we’re able to\ndo. Most of all, when we buckle down, confront the lizard brain, and ship our\nbest work, we’re becoming the artists we’re capable of becoming. No one\nis going to pick you. Pick yourself.”\n\n\n“There are kinds of action, for good or ill, that lie so far outside the\nboundaries of normal behaviour that they force us, in acknowledging that they\nhave occurred, to restructure our own understanding of reality. We have to make\nroom for them.”\n\n\n“Deny not your own mortality.”\n\n\n“Names are like clothes, Durnik,” Silk explained. “We put on what’s  most\nsuitable for the occasion. Honest men have little need to wear strange clothes\nor strange names Those of us who aren’t so honest, however, occasionally have\nto change one or the other.”\n\n\n“Women are almost always angry with us for one reason or another. It’s  one of\nthe things you’ll have to get used to as you get older.”\n\n\n“Events are like horses,” Hettar told him. “Sometimes they run away. After\nthey’ve run for a while, though, they’ll start to walk again, Then there’ll be\ntime to put everything together.”\n\n\n“Patience, Excellency,” Silk advised. “The more we suffer, the greater the\nrewards in the end.”\n\n\n“I understand much better than you think, Garion. You know what your problem\nis? You don’t want to grow up. You want to keep on being a boy forever. You\ncan’t though; nobody can. No matter how much power you have - whether you’re an\nemperor or a sorcerer - you can’t stop the years from going by. I realized that\na long time ago, but then I’m probably much smarter than you are.”\n\n\n“Men’s  minds ran to straight lines, but women thought more in terms of\ncircles.”\n\n\nDurnik smiled wryly. “I’m an ordinary man, Mandorallen,” he said, “Ordinary men\nlive in fear all the time. Didn’t you know that? We’re afraid of the weather,\nwe’re afraid of powerful men, we’re afraid of the night and the monsters that\nlurk in the dark, we’re afraid of growing old and of dying. Sometimes we’re\neven afraid of in ordinary men are afraid almost every minute of their lives.”\n\n\n“How can you bear it?” “Do we have any choice? Fear’s  a part of life,\nMandorallen, and it’s  the only life we have. You’ll get used to it. After\nyou’ve put it on every morning like an old tunic, you won’t even notice it any\nmore. Sometimes laughing at it helps a little.” “Laughing?” It shows the fear\nthat you know it’s  there, but that you’re going to go ahead and do what you\nhave to do anyway.” Durnik looked down at his hands, carefully kneading the\nmare’s  belly “Some men curse and swear and bluster, he continued. Every man\nhas to come up with his own technique for dealing with it Personally, I prefer\nlaughing. It seems more appropriate somehow.”\n\n\n“People are grateful for a bit of direction when they’re confused. You can’t go\nthrough life being afraid of what you are. If you do that, sooner or later\nsomebody will come along who’ll misunderstand, and you’ll have to do something\nto show him that it’s  not him that you’re afraid of. When it goes that far,\nit’s  usually much worse for you - and for him too. You don’t just jump in with\nhelp until you’re asked. That’s  very bad form, Garion.”\n\n\n“The word love seemed, as he thought more deeply about it, to include a great\nnumber of things that at first glance did not seem to have anything whatsoever\nto do with it”\n\n\n“The worst part of holding the memories is not the pain. It’s  the loneliness\nof it. Memories need to be shared.”\n\n\n“Which led to another thought: did all fathers feel this way when their sons\nbecame men? Men of achievement, of names that eclipsed the father’s? Was there\nalways the sting of envy to temper the burst of pride?”\n\n\n“Thus the first object of the child’s  hostility is identical with the first\nobject of its love-its mother”\n\n\n“I hate the word interesting, everything is interesting. To understand what is\nimportant is very difficult and is what I think forces people to think deeply\nand mature.” Bran thought about it. “Can a man still be brave if he’s\nafraid?”That is the only time a man can be brave”\n\n\n“A ruler who hides behind paid executioners soon forgets what death is.”\n\n\n“Only staying active will make you want to live a hundred years.”\n\n\n“Everything can be taken from a man but one thing: the last of the human\nfreedoms—to choose one’s attitude in any given set of circumstances, to choose\none’s own way.”\n\n\n“He lifted his hand and pointed at the beautiful burning flame image on the\nsymbol paper and said, “The path of the martial artist is like this flame.\nPracticing the martial arts will only cause pain. The dangers are countless and\nthe road is filled with obstacles. Everyone who walks down it will eventually\nturn to ash, but the true martial artist will be reborn from these ashes. Even\nif I was only a small and weak moth, I will walk into the flames without\nhesitation. I will fight my destiny for a one in a million chance that\nI will experience my own samsara and be reborn into a flaming phoenix.\nAnd even now, I am no longer a moth…”\n\n\n“My so-called inventions already existed in the environment,” Edison once said.\n\n\n“I’ve created nothing. Nobody does.”\n\n\n“Edison did not look for problems in need of solutions; he looked for solutions\nin need of modification.”\n\n\n“To assemble the best that is within you and give it away. And to assemble with\nthose you love to rekindle joy.”\n\n\n“The lion and the giraffe and the wombat and the rest do what they do and are\nwhat they are. And somehow manage to make it there in the cage, living the\nunexamined life. But to be human is to know and care and ask. To keep rattling\nthe bars of the cage of existence hollering, “What’s it for?” at the stones and\nstars, and making prisons and palaces out of the echoing answers.\n\n\n“Without realizing it, we fill important places in each other’s lives. It’s\nthat way with the guy at the corner grocery, the mechanic at the local garage,\nthe family doctor, teachers, neighbors, co-workers. Good people who are always\n“there,” who can be relied upon in small, important ways. People who teach us,\nbless us, encourage us, support us, uplift us in the dailiness of life. We\nnever tell them. I don’t know why, but we don’t. And, of course, we fill that\nrole ourselves. There are those who depend on us, watch us, learn from us, take\nfrom us. And we never know. Don’t sell yourself short. You may never have proof\nof your importance, but you are more important than you think. There are always\nthose who couldn’t do without you. The rub is that you don’t always know who.”\n\n\n“Writers aren’t exactly people … they’re a whole lot of people trying to be\none person.”\n\n\n” As mundane as our regular lives can be, sometimes, when we’ve had a great\nshock, that steady regular routine can be a God send.”\n\n\n“I believe in a personal god who cares about me and worries and oversees\neverything I do. I believe in an impersonal god who set the universe in motion\nand went off to hang with her girlfriends and doesn’t even know that I’m alive.\nI believe in an empty and godless universe of causal chaos, background noise,\nand sheer blind luck.”\n\n\n” Memory is one of the more trivial functions of sentinence”\n\n\n“He had wanted to create, to be recognized, and to study. He was no different\nfrom legions of other scientists and scholars. He just happened to be the one\nwho made it happen.”\n\n\n“Sometimes the truth is stupid.”\n\n\n“Which was better? To string it out as long as possible, as he had been doing,\nor to get it over with one way or the other?”\n\n\n“To remind you that deep beneath the layers of deviousness, you have a spark of\ndecency. Perhaps you could blow on that spark occasionally.”\n\n\n“The most difficult things in the world Must be accomplished through the\neasiest. The greatest things in the world Must be accomplished through the\nsmallest. Therefore the Sage Never attempts great things And so accomplishes\nthem.”\n\n\n“You know, Richard, most people think the will to survive is the strongest\ninstinct in human beings, but it isn’t. The strongest instinct is to keep\nthings familiar.”\n\n\n“For years, psychologists have tortured rats by making them do things like run\nmazes for bits of cheese. The interesting thing about these experiments is\nthat, when the scientists change the position of the cheese, the rats only try\nthe same way three or four times before starting to explore other possible\nroutes. When humans replace the rats, however, they just keep on and on and on,\nin the hopes that if they just do the same thing often enough they’ll get the\ndesired result.”\n\n\n“A single enduring statement can grant immortality.”\n\n\n“Find the worst human being you can, and you’ll still find something worse by\nlooking out the window at night.”\n\n\n“The moment you get up to prove one thing, you’ll be expected to prove them\nall.”\n\n\n“Most men cannot abide silence. Some fly into a rage. Some become clowns. Some\nconfess all they know. Silence reveals much.”\n\n\n“I learned thirty years ago that it is foolish to scold. I have enough trouble\novercoming my own limitations without fretting over the fact that God has not\nseen fit to distribute evenly the gift of intelligence.”\n\n\n“As much as we thirst for approval, we dread condemnation”\n\n\n“The deepest urge in human nature is “the desire to be important.”\n\n\n“Nobody knows for sure But he did say that many people who go insane find in\ninsanity a feeling of importance that they were unable to achieve in the world\nof reality.”\n\n\n“We are interested in others when they are interested in us.”\n\n\n“A man without a smiling face must not open a shop.”\n\n\n“I judge people by their own principles - not by my own.”\n\n\n“Say about yourself all the derogatory things you know the other person is\nthinking or wants to say or intends to say - and say them before that person\nhas a chance tosay them.”\n\n\n“So with men, if you would win a man to you cause, first convince him that you\nare his sincere friend. Therein is a drop of honey that catches his heart;\nwhich, say what you will, is the great high road to his reason.”\n\n\n“Look again at that dot. That’s  here. That’s  home.That’s  us. On it everyone\nyou love, everyone you know, everyone you ever heard of, every human being who\never was, lived out their lives. The aggregate of our joy and suffering,\nthousands of confident religions, ideologies, and economic doctrines, every\nhunter and forager, every hero and coward, every creator and destroyer of\ncivilization, every king and peasant, every young couple in love, every mother\nand father, hopeful child, inventor and explorer, every teacher of morals,\nevery corrupt politician, every “superstar,” every “supreme leader,” every\nsaint and sinner in the history of our species lived there-on a mote of dust\nsuspended in a sunbeam. The Earth is a very small stage in a vast cosmic arena.\nThink of the endless cruelties visited by the inhabitants of one corner of this\npixel on the scarcely distinguishable inhabitants of some other corner, how\nfrequent their misunderstandings, how eager they are to kill one another, how\nfervent their hatreds. Think of the rivers of blood spilled by all those\ngenerals and emperors so that, in glory and triumph, they could become the\nmomentary masters of a fraction of a dot. Our posturings, our imagined\nself-importance, the delusion that we have some privileged position in the\nUniverse, are challenged by this point of pale light. Our planet is a lonely\nspeck in the great enveloping cosmic dark. In our obscurity, in all this\nvastness, there is no hint that help will come from elsewhere to save us from\nourselves. The Earth is the only world known so far to harbor life. There is\nnowhere else, at least in the near future, to which our species could migrate.\nVisit, yes. Settle, not yet. Like it or not, for the moment the Earth is where\nwe make our stand. It has been said that astronomy is a humbling and\ncharacter-building experience. There is perhaps no better demonstration of the\nfolly of human conceits than this distant image of our tiny world. To me, it\nunderscores our responsibility to deal more kindly with one another, and to\npreserve and cherish the pale blue dot, the only home we’ve ever known.”\n\n\n“Every one of us is, in the cosmic perspective, precious. If a human disagrees\nwith you, let him live. In a hundred billion galaxies, you will not find\nanother.”\n\n\n” If you wish to make an apple pie from scratch, you must first invent the\nuniverse”\n\n\n” A book is made from a tree. It is an assemblage of flat, flexible parts\n(still called “leaves”) imprinted with dark pigmented squiggles. One glance at\nit and you hear the voice of another person, perhaps someone dead for thousands\nof years. Across the millennia, the author is speaking, clearly and silently,\ninside your head, directly to you. Writing is perhaps the greatest of human\ninventions, binding together people, citizens of distant epochs, who never knew\none another. Books break the shackles of time ― proof that humans can work\nmagic.”\n\n\n” Who is more humble? The scientist who looks at the universe with an open mind\nand accepts whatever the universe has to teach us, or somebody who says\neverything in this book must be considered the literal truth and never mind the\nfallibility of all the human beings involved?”\n\n\n” In science it often happens that scientists say, ‘You know that’s  a really\ngood argument; my position is mistaken,’ and then they would actually change\ntheir minds and you never hear that old view from them again. They really do\nit. It doesn’t happen as often as it should, because scientists are human and\nchange is sometimes painful. But it happens every day. I cannot recall the last\ntime something like that happened in politics or religion.”\n\n\n” We are like butterflies who flutter for a day and think it is forever.”\n\n\n” You’re an interesting species. An interesting mix. You’re capable of such\nbeautiful dreams, and such horrible nightmares. You feel so lost, so cut off,\nso alone, only you’re not. See, in all our searching, the only thing we’ve\nfound that makes the emptiness bearable, is each other.”\n\n\n” Exploration is in our nature. We began as wanderers, and we are wanderers\nstill. We have lingered long enough on the shores of the cosmic ocean. We are\nready at last to set sail for the stars.”\n\n\n” Fireflies out on a warm summer’s  night, seeing the urgent, flashing,\nyellow-white phosphorescence below them, go crazy with desire; moths cast to\nthe winds an enchantment potion that draws the opposite sex, wings beating\nhurriedly, from kilometers away; peacocks display a devastating corona of blue\nand green and the peahens are all aflutter; competing pollen grains extrude\ntiny tubes that race each other down the female flower’s  orifice to the\nwaiting egg below; luminescent squid present rhapsodic light shows, altering\nthe pattern, brightness and color radiated from their heads, tentacles, and\neyeballs; a tapeworm diligently lays a hundred thousand fertilized eggs in a\nsingle day; a great whale rumbles through the ocean depths uttering plaintive\ncries that are understood hundreds of thousands of kilometers away, where\nanother lonely behemoth is attentively listening; bacteria sidle up to one\nanother and merge; cicadas chorus in a collective serenade of love; honeybee\ncouples soar on matrimonial flights from which only one partner returns; male\nfish spray their spunk over a slimy clutch of eggs laid by God-knows-who; dogs,\nout cruising, sniff each other’s  nether parts, seeking erotic stimuli; flowers\nexude sultry perfumes and decorate their petals with garish ultraviolet\nadvertisements for passing insects, birds, and bats; and men and women sing,\ndance, dress, adorn, paint, posture, self-mutilate, demand, coerce, dissemble,\nplead, succumb, and risk their lives. To say that love makes the world go\naround is to go too far. The Earth spins because it did so as it was formed and\nthere has been nothing to stop it since. But the nearly maniacal devotion to\nsex and love by most of the plants, animals, and microbes with which we are\nfamiliar is a pervasive and striking aspect of life on Earth. It cries out for\nexplanation. What is all this in aid of? What is the torrent of passion and\nobsession about? Why will organisms go without sleep, without food, gladly put\nthemselves in mortal danger for sex? … For more than half the history of life\non Earth organisms seem to have done perfectly well without it. What good is\nsex?… Through 4 billion years of natural selection, instructions have been\nhoned and fine-tuned…sequences of As, Cs, Gs, and Ts, manuals written out in\nthe alphabet of life in competition with other similar manuals published by\nother firms. The organisms become the means through which the instructions flow\nand copy themselves, by which new instructions are tried out, on which\nselection operates.  ‘The hen,’ said Samuel Butler, ‘is the egg’s  way of\nmaking another egg.’ It is on this level that we must understand what sex is\nfor. … The sockeye salmon exhaust themselves swimming up the mighty Columbia\nRiver to spawn, heroically hurdling cataracts, in a single-minded\neffort that works to propagate their DNA sequences into future\ngeneration. The moment their work is done, they fall to pieces. Scales\nflake off, fins drop, and soon—often within hours of spawning—they\nare dead and becoming distinctly aromatic.  They’ve served their\npurpose.  Nature is unsentimental.  Death is built in.”\n\n\n” The significance of our lives and our fragile planet is then determined only\nby our own wisdom and courage. We are the custodians of life’s  meaning. We\nlong for a Parent to care for us, to forgive us our errors, to save us from our\nchildish mistakes. But knowledge is preferable to ignorance. Better by far to\nembrace the hard truth than a reassuring fable. If we crave some cosmic\npurpose, then let us find ourselves a worthy goal.”\n\n\n” Books are like seeds. They can lie dormant for centuries and then flower in\nthe most unpromising soil.” “Death is not terrifying. The thing that is\nterrifying is the moment before death.”\n\n\n“I don’t have any sense of propriety. I don’t have parents. In your eyes, I\nhave neither any right nor status… But, my elder once told me that you only see\none part of the rain in the world. You will never know how much rain there is\nwhen it stops… You can only see the surface of the muddy water on the ground\nand never the bottom… ”\n\n\n“If you stay on your Dao1 but have no method of solving a particular problem\nnow, that method will eventually come to you. If you have the skills and power,\nbut have strayed from your Dao, then you cannot use your skills, and your power\nwill forever stay stagnant! I don’t understand the Dao’ meant by those in the\nother worlds, but now…”\n\n\n“A delusion starts like any other idea, as an egg. Identical on the outside,\nperfectly formed. From the shell, you’d never know anything was wrongs what’s\ninside that matters.”\n\n\n“You know the most dangerous thing about schizophrenia? The most dangerous\nthing is believing you don’t have it. That’s the trick. The mind killer. Your\ndisease convinces you, you don’t have it. So, for example, one day in the\nhospital, you meet a girl and she has some friends and they tell you you’re not\nsick. You have super powers. And more than anything, you wanna believe it\nbecause that means you’re not crazy. It means you can fall in love and live\nhappily ever after. But you know if you believe it if you surrender to the hope\nand you’re wrong, then you’re never coming back. ”\n\n\n“What is the universe without each sunrise? That’s how we judge our gods, not\non their math but their poetry.”\n\n\n“Human beings are the only animal that forms ideas about their world. We\nperceive it not through our bodies but through our minds. We must agree on what\nis real. Because of this, we are the only animal on Earth that goes mad”\n\n\n“Ask yourself: what’s more terrifying? Fear, or the frightened”\n\n\n“You decide what is real and what is not. You. Your will”\n\n\n“Who teaches us to be normal when we’re one of a kind?”\n\n\n“Where the pessimist sees danger hiding behind every back, the optimist sees\nfriendship. Which is why, when we encounter coincidence, we often see\nconspiracy. ”\n\n\n“We moved through a city of normals, soldiers in a secret war. We were the\nghosts in a haunted house, the golem of myth. To the normal, we were just\nsuperstition, make-believe. Sometimes it felt like that to us too. To me, I was\na woman who couldn’t be touched, in love with a man who wasn’t there. What was\nreal?”\n\n\n“Please keep talking so we can all pretend that our problems are just in our\nheads.”\n\n\n“It’s a war, baby, this life. The things we endure. You said you saw the\nfuture, and it’s an apocalypse. Who survives that, the lovers or the fighters?\nThey sell us this lie that love’s gonna save us. All it does is make us stupid\nand weak”\n\n\n“Is it such a terrible thing to feel sorrow for your enemy? What is he, except\na brother with another name?”\n\n\n“If the idea of illness can become illness, what else about our reality is\nactually a disorder?”\n\n\n“I am a good person. I deserve love. ”\n\n\n“Do what you want. Take what you want. Gods make rules. They don’t follow\nthem.”\n\n\n“You have your whole life to be old. But today, you’re still young.”\n\n\n” You can make someone do what they don’t want to do, but there’s no force on\nEarth that makes you enjoy it.”\n\n\n“What if you’re not the hero? What if you’re just another villain? The real\nvillain.”\n\n\n“Well, kid, you better learn to fly like a bird, because the age of the\ndinosaur is over”\n\n\n” The best way to understand the mind is to build it”\n\n\n” We have to learn about love before we can learn about hate. Otherwise,\neverything goes to hell”\n\n\n” We can make anything we fancy in this arena of infinite promise, and this is\nwhat we come up with? Weapons? War? Surely we have more imagination than that.”\n\n\n” We can fix this world. All the bad things. We just got to start over.”\n\n\n” Time travel does not give one the opportunity to change oneself. But rather,\nto eradicate oneself and allow something else to form in the wake of what once\nwas.”\n\n\n” The secret of life. If you feel safe when you’re young, you will feel safe\nwhen you’re old.”\n\n\n” The money is a tool, you see. Nothing more. Power. Because without power, who\nwould listen?”\n\n\n” People get too close. They touch you and you disappear. And then they’re\ninside. In your belly and in your head. And when you get back, there’s a smell.\nSomeone else’s smell is inside your nose. And you check out. You tell people,\n\n\n“It’s fine. I don’t own my body.” You say, “My power is like a vacation. I get\nto be a tourist in someone else’s life.” Who cares if every time I come back\nhome, I feel dirty?”\n\n\n” No one who dies is really dead. You see that, right? The past changes, and\nthe future disappears.”\n\n\n” Love isn’t gonna save us. It’s what we have to save. Pain makes us strong\nenough to do it. All our scars, our anger, our despair, it’s armor. Baby, God\nloves the sinners best because our fire burns bright, bright, bright. Burn with\nme”\n\n\n” It’s not crazy babies that worry me. It’s the men they become. Like a fire,\nfalling in love with a fire. We try to smother them, put them out, but they\njust burn us up”\n\n\n” If we don’t believe in change, then we don’t believe in time.”\n\n\n” I tell them I’m sane, they think I’m crazy, and if I say, “You know what?\nYou’re right, I am crazy,” then they up my dosage.”\n\n\n” Do you know what love is? It’s a hot bath. What happens to things when you\nleave them in a bath for too long? They get soft, fall apart.”\n\n\n” An army cannot sneak up on a man. But a lover can.”\n\n\n” All that love, it doesn’t just disappear. It must be transformed into an\nemotion of equal intensity. That is the law of the universe.”\n\n\n” All animals fight to live. Whether they want to or not.”\n\n\n“If a person’s heart can change due to persistently holding onto something,\nthen why can’t that persistence grant me peace?”\n\n\n“The Golden Roc’s heart and its will are reflected as it flies through the\nworld. Within its eyes, there is nothing in the world that can stop its path.\nIt can fly endlessly in this vast sky”\n\n\nThe place where I was born still did things according to the laws of the\nuniverse…\n\n\n“When I was born, the Berserkers had weakened… If the heavens are heartless,\nthen we will all be separated… The earth was heartless, and it made my Dark\nMountain die… When war begins, the moon will shatter into millions of pieces…\nThe roads leading to our homes will become unfamiliar to us, and we will\ngrieve… ”\n\n\n“Always treat it as if he is still alive and kill him again, maybe twice, or\neven more.”\n\n\n” But the techno-utopians do get tiresome with their platitudes and their\nability to prattle on for hours without saying much of substance. More\ndisconcerting is their underlying message that humans are flawed and our\nhumanity is an annoying burden that needs to be dealt with in due course.”\n\n\n“I think there are probably too many smart people pursuing Internet stuff,\nfinance, and law,” Musk said on the way. “That is part of the reason why we\nhaven’t seen as much innovation.”\n\n\n“He does what he wants, and he is relentless about it. It’s Elon’s world, and\nthe rest of us live in it.”\n\n\n“Maybe I read too many comics as a kid,” Musk said. “In the comics, it always\nseems like they are trying to save the world. It seemed like one should try to\nmake the world a better place because the inverse makes no sense.”\n\n\n“The only reason he did not outrank the other boys was a lack of interest in\nthe work prescribed by the school.”\n\n\n“In every work of genius we recognize our own rejected thoughts; they come back\nto us with a certain alienated majesty.”\n\n\n“Sympathy the human species universally craves. The child eagerly displays his\ninjury; or even inflicts a cut or bruise in order to reap abundant sympathy.\nFor the same purpose adults … show their bruises, relate their accidents,\nillness, especially details of surgical operations. Self-pity’ for misfortunes\nreal or imaginary is in some measure, practically a universal practice.”\n\n\n“The abbot of our monastery always said that fable has strong shoulders that\ncarry far more truth than fact can.”\n\n\n“Until you make the unconscious conscious, it will direct your life and you\nwill call it fate.”\n\n\n“Here’s  to the ones that we got Cheers to the wish you were here, but you’re\nnot Cause the drinks bring back all the memories Of everything we’ve been\nthrough Toast to the ones here today Toast to the ones that we lost on the way\nCause the drinks bring back all the memories And the memories bring back,\nmemories bring back you ”\n\n\n“And still, after all this time, the Sun has never said to the Earth, “You owe\nme.” Look what happens with love like that. It lights up the sky.”\n\n\n“Coincidence is God’s way of remaining anonymous.”\n\n\n“The big question is whether you are going to be able to say a hearty yes to\nyour adventure.”\n\n\n“It is better to be hated for what you are than to be loved for what you are\nnot.”\n\n\n“The privileged are processed by people, the poor are processed by algorithms.”\n\n\n“The only difference between a madman and me is that the madman thinks he is\nsane. I know I am mad.”\n\n\n“The gift is the blessing of the giver.”\n\n\n“There is no escape—we pay for the violence of our ancestors.”\n\n\n“Humans live best when each has his own place, when each knows where he belongs\nin the scheme of things. Destroy the place and destroy the person.”\n\n\n“The mind goes on working no matter how we try to hold it back”\n\n\n“It occurred to her that mercy was the ability to stop, if only for a moment.\nThere was no mercy where there could be no stopping.”\n\n\n“Men and their works have been a disease on the surface of their planets before\nnow,” his father said. “Nature tends to compensate for diseases, to remove or\nencapsulate them, to incorporate them into the system in her own way.”\n\n\n“The concept of progress acts as a protective mechanism to shield us from the\nterrors of the future.”\n\n\n“You are a placebo responder. Your body plays tricks on your mind. You cannot\nbe trusted.”\n\n\n“The baby which cries the most gets the maximum milk.”\n\n\n“A boy at the beginning of a story has no way of knowing that the story has\nbegun. ”\n\n\n“If you want enemies, excel your friends; but if you want friends, let your\nfriends excel you.”\n\n\n“Every decision you make can change the world. The best life is the one the\ngods don’t notice. You want to live free, boy, live quietly.”\n\n\n“I think something’s wrong with me. I make friends, then suddenly I can’t bear\nto be with any of them. Seems like that other me, the cheerful and honest one,\nwent away somewhere.”\n\n\n“Do any of us children, she wondered, ever stop to ask ourselves where our\nteachers go when school is over for the day? Do we wonder if they live alone,\nor if there is a mother at home or a sister or a husband?”\n\n\n“They have taken everything from us; should I let them take my mind as well?”\n\n\n“Nothing ever begins. There is no first moment; no single word or place from\nwhich this or any other story springs. The threads can always be traced back to\nsome earlier tale, and to the tales that preceded that; though as the\nnarrator’s voice recedes the connections will seem to grow more tenuous, for\neach age will want the tale told as if it were of its own making. Thus the\npagan will be sanctified, the tragic become laughable; great lovers will stoop\nto sentiment, and demons dwindle to clockwork toys. Nothing is fixed. In and\nout the shuttle goes, fact and fiction, mind and matter, woven into patterns\nthat may have only this in common: that hidden amongst them is a filigree which\nwill with time become a world.\n\n\n“Self-perception is a zoo.”\n\n\n“The world as we have created it is a process of our thinking. It cannot be\nchanged without changing our thinking.”\n\n\n“Forgiveness means giving up all hope for a better past.”\n\n\n“When something goes wrong with a computer you get an error message, When\nsomething goes wrong with a human you get feelings.”\n\n\n“For the others, we can say that Muad’Dib learned rapidly because his first\ntraining was in how to learn. And the first lesson of all was the basic trust\nthat he could learn. It is shocking to find how many people do not believe they\ncan learn, and how many more believe learning to be difficult.”\n\n\n“Any road followed precisely to its end leads precisely nowhere. Climb the\nmountain just a little bit to test that it’s a mountain. From the top of the\nmountain, you can not see the mountain.”\n\n\n“There is probably no more terrible instant of enlightenment than the one in\nwhich you discover your father is a man—with human flesh.”\n\n\n“Black is a blind remembering, she thought. You listen for pack sounds, for the\ncries of those who hunted your ancestors in a past so ancient only your most\nprimitive cells remember. The ears see. The nostrils see.”\n\n\n“What senses do we lack that we cannot see or hear another world all around\nus?”\n\n\n“Prophecy and prescience—How can they be put to the test in the face of the\nunanswered question? Consider: How much is actual prediction of the “wave form”\n(as MuadDib referred to his vision- image) and how much is the prophet shaping\nthe future to fit the prophecy? What of the harmonics inherent in the act of\nprophecy? Does the prophet see the future or does he see a line of weakness, a\nfault or cleavage that he may shatter with words or decisions as a\ndiamond-cutter shatters his gem with a blow of a knife?”\n\n\n“When religion and politics travel in the same cart, the riders believe nothing\ncan stand in their way. Their movement become headlong—faster and faster and\nfaster. They put aside all thought of obstacles and forget that a precipice\ndoes not show itself to the man in a blind rush until it’s too late.”\n\n\nYou are a placebo responder. Your body plays tricks on your mind. You cannot be\ntrusted.’\n\n\n“How often it is that the angry man rages denial of what his inner self is\ntelling him.”\n\n\n“A mathematician, like a painter or poet, is a maker of patterns. If his\npatterns are more permanent than theirs, it is because they are made with\nideas.”\n\n\n“In New York, when a tree dies, nobody mourns that it was cut down in its\nprime. Nobody counts the rings,notifies the loved ones. There are other trees.”\n\n\n“If you haven’t solved the puzzle, it isn’t the end yet.”\n\n\n“Recieve with simplicity everything that happens to you”\n\n\n“By letting it go it all gets done. The world is won by those who let it go.\nBut when you try and try. The world is beyond the winning.”\n\n\n“A good traveler has no fixed plans, and is not intent on arriving.”\n\n\n“Be content with what you have; rejoice in the way things are. When you realize\nthere is nothing lacking, the whole world belongs to you.”\n\n\n“The only art I’ll ever study is stuff that I can steal from.”\n\n\n“Everything that needs to be said has already been said. But, since no one was\nlistening, everything must be said again.”\n\n\n“What is originality? Undetected plagiarism.”\n\n\n“We were kids without fathers … so we found our fathers on wax and on the\nstreets and in history. We got to pick and choose the ancestors who would\ninspire the world we were going to make for ourselves.”\n\n\n“Steal from anywhere that resonates with inspiration or fuels your imagination.\nDevour old films, new films, music, books, paintings, photographs, poems,\ndreams, random conversations, architecture, bridges, street signs, trees,\nclouds, bodies of water, light and shadows. Select only things to steal from\nthat speak directly to your soul. If you do this, your work (and theft) will be\nauthentic.”\n\n\n“Instead, chew on one thinker—writer, artist, activist, role model—you really\nlove. Study everything there is to know about that thinker. Then find three\npeople that thinker loved, and find out everything about them. Repeat this as\nmany times as you can. Climb up the tree as far as you can go. Once you build\nyour tree, it’s time to start your own branch.”\n\n\n“It’s not the book you start with, it’s the book that book leads you to.”\n\n\n“It is better to take what does not belong to you than to let it lie around\nneglected.”\n\n\n“Start copying what you love. Copy copy copy copy. At the end of the copy you\nwill find your self.”\n\n\n“You start out by rewriting your hero’s catalog.” “If you have one person\nyou’re influenced by, everyone will say you’re the next whoever. But if you rip\noff a hundred people, everyone will say you’re so original!”\n\n\n“My interest in making music has been to create something that does not exist\nthat I would like to listen to. I wanted to hear music that had not yet\nhappened, by putting together things that suggested a new thing which did not\nyet exist.”\n\n\n“We don’t know where we get our ideas from. What we do know is that we do not\nget them from our laptops.”\n\n\n“The work you do while you procrastinate is probably the work you should be\ndoing for the rest of your life.”\n\n\n“Avoiding work is the way to focus my mind.”\n\n\n“It’s not that people are mean or cruel, they’re just busy.”\n\n\n“You don’t have to share everything—in fact, sometimes it’s much better if you\ndon’t. Show just a little bit of what you’re working on. Share a sketch or a\ndoodle or a snippet. Share a little glimpse of your process.”\n\n\n“I always carry a book, a pen, and a notepad, and I always enjoy my solitude\nand temporary captivity.”\n\n\n“There’s only one rule I know of: You’ve got to be kind.”\n\n\n“Find the most talented person in the room, and if it’s not you, go stand next\nto him. Hang out with him. Try to be helpful.”\n\n\n“Complain about the way other people make software by making software.”\n\n\n“The best way to get approval is to not need it.”\n\n\n“the trick is to be too busy doing your work to care.”\n\n\n“In this age of information abundance and overload, those who get ahead will be\nthe folks who figure out what to leave out, so they can concentrate on what’s\nreally important to them. Nothing is more paralyzing than the idea of limitless\npossibilities. The idea that you can do anything is absolutely terrifying. The\nway to get over creative block is to simply place some constraints on yourself.\nIt seems contradictory, but when it comes to creative work, limitations mean\nfreedom. Write a song on your lunch break. Paint a painting with only one\ncolor. Start a business without any start-up capital.Shoot a movie with your\niPhone and a few of your friends. Build a machine out of spare parts. Don’t\nmake excuses for not working—make things with the time, space, and materials\nyou have, right now.”\n\n\n“Telling yourself you have all the time in the world, all the money in the\nworld, all the colors in the palette, anything you want—that just kills\ncreativity.”\n\n\n“What we respond to in any work of art is the artist’s struggle against his or\nher limitations.”\n\n\n“An expert is a person who has made all the mistakes that can be made in a very\nnarrow field.”\n\n\n“Ordinarily he was insane, but he had lucid moments when he was merely stupid.”\n\n\n“Reality is merely an illusion, albeit a very persistent one.”\n\n\n“The first principle is that you must not fool yourself, and you are the\neasiest person to fool.”\n\n\n“Options—the ability to choose—is real power.”\n\n\n” It just so happens, paradoxically, that you can make more money—a lot more\nmoney—by doing half of what you are doing now.”\n\n\n“Civilization had too many rules for me, so I did my best to rewrite them.”\n\n\n“In 21st century America, old-fashioned notions like honor and fair play are\nfor suckers. The credo of our time is that as if it ain’t technically illegal,\nit’s awesome. The people who rise to the top are no longer those who\naccomplish truly great things, but those who figure out how to most\nattractively package their shortcuts and fake-outs.”\n\n\n“Once you say you’re going to settle for second, that’s  what happens to you in\nlife.”\n\n\n“If the recipe sucks, it doesn’t matter how good a cook you are.”\n\n\n“Named must your fear be before banish it you can.”\n\n\n“No more passing days as the living dead, no more dinners where his colleagues\ncompared cars, riding on the sugar high of a new BMW purchase until someone\nbought a more expensive Mercedes. It was over.”\n\n\n“Set aside a certain number of days, during which you shall be content with the\nscantiest and cheapest fare, with course and rough dress, saying to yourself\nthe while: “Is this the condition that I feared?”\n\n\n“There’s  no difference between a pessimist who says, “Oh, it’s  hopeless, so\ndon’t bother doing anything,” and an optimist who says, “Don’t bother doing\nanything, it’s  going to turn out fine anyway.” Either way, nothing happens.”\n\n\n“Don’t save it all for the end. There is every reason not to.”\n\n\n“I am an old man and have known a great many troubles, but most of them never\nhappened.”\n\n\n“Doing the unrealistic Is easier than doing the realistic”\n\n\n“The fishing is best where the fewest go, and the collective insecurity of the\nworld makes it easy for people to hit home runs while everyone else is aiming\nfor base hits. There is just less competition for bigger goals.”\n\n\n“Remember—boredom is the enemy, not some abstract “failure.”\n\n\n“The existential vacuum manifests itself mainly in a state of boredom.”\n\n\n“Perfection is not when there is no more to add, but no more to take away.”\n\n\n“It is vain to do with more what can be done with less.”\n\n\n“Doing something unimportant well does not make it important.”\n\n\n“In other words, I was working because I felt as though I should be doing\nsomething from 9-5.1  didn’t realize that working every hour from 9-5 isn’t the\ngoal; it’s  simply the structure most people use, whether it’s  necessary or\nnot.”\n\n\n“Most things make no difference. Being busy is a form of laziness—lazy thinking\nand indiscriminate action.”\n\n\n“Since we have 8 hours to fill, we fill 8 hours. If we had 15, we would fill\n\n\n\nIf we have an emergency and need to suddenly leave work in 2 hours but have\npending deadlines, we miraculously complete those assignments in 2 hours.”\n\n\n\n“Parkinson’s  Law dictates that a task will swell in (perceived) importance and\ncomplexity in relation to the time allotted for its completion.”\n\n\n“Am I being productive or just active?”\n\n\n“Reading, after a certain age, diverts the mind too much from its creative\npursuits. Any man who reads too much and uses his own brain too little falls\ninto lazy habits of thinking.”\n\n\n“We’re all pieces on someone’s board, Jorg.”\n\n\n“We look at the world once, in childhood. The rest is memory.”\n\n\n“Drink your tea slowly and reverently, as if it is the axis on which the world\nearth revolves – slowly, evenly, without rushing toward the future.”\n\n\n“Isn’t it strange that the only time we truly lived is when we were not even\nthinking about living truly?”\n\n\n” A window to a new world can also show you home”\n\n\n“I think maybe we die every day. Maybe we’re born new each dawn, a little\nchanged, a little further on our own road. When enough days stand between you\nand the person you were, you’re strangers. Maybe that’s what growing up is.\nMaybe I have grown up. ”\n\n\n“I’ve grown, but whatever monster might be in me, it was always mine, my\nchoice, my responsibility, my evil, if you will. It’s what I am, and if you\nwant excuses, come and take them.”\n\n\n” When the economy is down, people go to school”\n\n\n“The Puppy Dog Close in sales is so named because it is based on the pet store\nsales approach: If someone likes a puppy but is hesitant to make the\nlife-altering purchase, just offer to let them take the pup home and bring it\nback if they change their minds. Of course, the return seldom happens.”\n\n\n“It’s  amazing how someone’s  IQ seems to double as soon as you give them\nresponsibility and indicate that you trust them.”\n\n\n“People think it must be fun to be a super genius, but they don’t realize how\nhard it is to put up with all the idiots in the world.”\n\n\n“But the whole problem of discovering what was the matter, and figuring out\nwhat you have to do to fix it—that was interesting to me, like a puzzle.”\n\n\n“I learned there that innovation is a very difficult thing in the real world.”\n\n\n“If a typical person can do a mental task with less than one second of thought,\nwe can probably automate it using AI either now or in the near future”\n\n\n“A single dream is more powerful than a thousand realities”\n\n\n“The billionaires of tomorrow always seem to start out in garages, don’t they?”\n\n\n“Life is unfair. You make the best of what life deals you”\n\n\n” Every path is the right path. Everything could’ve been anything else.﻿\nAnd it would have just as much meaning.”\n\n\n“Choices … We cannot go back. That’s why it’s hard to choose. You have to\nmake the right choice. As long as you don’t choose, everything remains\npossible.”\n\n\n“In chess, it’s  called Zugzwang…when the only viable move…is not to move.”\n\n\n“The child could not make a choice because he did not know what would happen,\nbut now that he knows what will happen, he can not make a choice.”\n\n\n“I’m not afraid to die, I’m afraid I haven’t lived enough. It should be written\non every school chalkboard, “Life is a playground- or nothing.”\n\n\n“Why am I me and not somebody else?”\n\n\n“But the logic of his immediate circumstances was overwhelming his other\nconcerns, however well founded they might be. He had never been one to argue\nwith logic.”\n\n\n“There is surely nothing other than the single purpose of the present moment. A\nman’s whole life is a succession of moment after moment. If one fully\nunderstands the present moment, there will be nothing else to do, and nothing\nelse to pursue. Live being true to the single purpose of the moment.”\n\n\n“Saving one dog will not change the world, but surely for that one dog, the\nworld will change forever.”\n\n\n“He wasn’t in a safe little story where wrongs were automatically righted; he\nwas still in the real world, where bad, bitter things happened for no reason,\nand people paid for things that weren’t their fault.”\n\n\n“For the true magician there is no very clear line between what lies inside the\nmind and what lies outside it. If you desire something, it will become\nsubstance. If you despise it, you will see it destroyed. A master magician is\nnot much different from a child or a madman in that respect. It takes a very\nclear head and a very strong will to operate once you are in that place. And\nyou will find out very quickly whether or not you have that clarity and that\nstrength.”\n\n\n“Everybody has their own idiopathic reaction to their childhood home.”\n\n\n“So you have to promise me, Quentin. Let’s never get like this, with these\nstupid hobbies nobody cares about. Just doing pointless things all day and\nhating each other and waiting to die. Well, you drive a hard bargain,” he said.\n\n\n“But okay. I promise. I’m serious, Quentin. It’s not going to be easy. It’s\ngoing to be so much harder than you think. They don’t even know, Quentin. They\nthink they’re happy. That’s the worst part.”\n\n\n“Sometimes I wonder if man was really meant to discover magic,” Fogg said\nexpansively. “It doesn’t really make sense. It’s a little too perfect, don’t\nyou think? If there’s a single lesson that life teaches us, it’s that wishing\ndoesn’t make it so. Words and thoughts don’t change anything. Language and\nreality are kept strictly apart—reality is tough, unyielding stuff, and it\ndoesn’t care what you think or feel or say about it. Or it shouldn’t. You deal\nwith it, and you get on with your life. Little children don’t know that.\nMagical thinking: that’s what Freud called it. Once we learn otherwise we cease\nto be children. The separation of word and thing is the essential fact on which\nour adult lives are founded. But somewhere in the heat of magic that boundary\nbetween word and thing ruptures. It cracks, and the one flows back into the\nother, and the two melt together and fuse. Language gets tangled up with the\nworld it describes. I sometimes feel as though we’ve stumbled on a flaw in the\nsystem, don’t you? A short circuit? A category error? A strange loop? Is it\npossible that magic is knowledge that would be better off forsworn? Tell me\nthis: Can a man who can cast a spell ever really grow up?”\n\n\n“I think you’re magicians because you’re unhappy. A magician is strong because\nhe feels pain. He feels the difference between what the world is and what he\nwould make of it. Or what did you think that stuff in your chest was? A\nmagician is strong because he hurts more than others. His wound is his\nstrength. Most people carry that pain around inside them their whole lives,\nuntil they kill the pain by other means, or until it kills them. But you, my\nfriends, you found another way: a way to use the pain. To burn it as fuel, for\nlight and warmth. You have learned to break the world that has tried to break\nyou.”\n\n\n“How can you seek eternal life by means of a temporal body?”\n\n\n“I’ve lived through some terrible things in my life, some of which actually\nhappened.”\n\n\n“The goal is not for everyone to be equal. The goal is for everyone to be\nconnected. The goal is for everyone to belong. The goal is for everyone to be\nloved.”\n\n\n“How can the events in space and time, which take place within the boundaries\nof a living organism, be accounted for by physics and chemistry? … The\nobvious inability of present-day physics and chemistry to account for such\nevents is no reason at all for doubting that they will be accounted for by\nthose sciences.”\n\n\n“human was the music, natural was the static…”\n\n\n“We worship perfection because we can’t have it; if we had it, we would reject\nit. Perfection is inhuman, because humanity is imperfect.”\n\n\n“life so perfect, it flatlines”\n\n\n“Because there’s an infinite amount of things we can now see or know, there are\nalso an infinite number of ways we can discover that we don’t measure up, that\nwe’re not good enough, that things aren’t as great as they could be. And this\nrips us apart inside.”\n\n\n“Because when you give too many fucks—when you give a fuck about everyone and\neverything—you will feel that you’re perpetually entoc: true\ntitled to be comfortable and\nhappy at all times, that everything is supposed to be just exactly the fucking\nway you want it to be. This is a sickness. And it will eat you alive. You will\nsee every adversity as an injustice, every challenge as a failure, every\ninconvenience as a personal slight, every disagreement as a betrayal. You will\nbe confined to your own petty, skull-sized hell, burning with entoc: true\ntitlement and\nbluster, running circles around your very own personal Feedback Loop from Hell,\nin constant motion yet arriving nowhere.”\n\n\n“Don’t hope for a life without problems,” the panda said. “There’s no such\nthing. Instead, hope for a life full of good problem”\n\n\n“Emotions are merely signposts, suggestions that our neurobiology gives us, not\ncommandments.”\n\n\n“The deeper the pain, the more helpless we feel against our problems, and the\nmore entoc: true\ntitlement we adopt to compensate for those problems.”\n\n\n“If one isn’t crazy, why would they try to understand something like the laws\nof the heavens? If one isn’t crazy, why would they try to become an immortal?\nOne must want something in order to obtain it. That is the truth”\n\n\n“All of the glorious moments of the past had become distant memories…The one\nthat falls in the fire must be that immortal phoenix… Even if it must burn its\nwings, it will still fly in the heavens…Closing one door is like loving a\nworld.Memories of the past will now be forever distant.The sand in the wind no\nlonger filled the space of dreams. The whimper of the powerful flute is now\nonly an echo in the desolate land.Closing off one door is like cutting off one\nspace and time.The glorious past only remains in the descent’s\nsongs.Yesterday’s song no longer resonate the same way. The whispering\ncomplaints can’t find its matching music.Opening a window is like hugging a ray\nof sunlight.Today’s dream became the future empire’s hope.Even the ordinary you\nand me need to have exciting displays. Pursuit without regret to feel the might\nof the world.Opening a window is like welcoming a wave of spring wind.The burst\nof nothingness awaken what was once lost.The one that fell in the fire must be\nthat immortal phoenix… Even if it must burn its wings it will still fly in the\nheavens.”\n\n\n“The rainy night is beautiful in its mood and endlessness. The plants silently\nabsorb the water and the scent of death on them quietly disappears. This is the\nbeauty of the rain and the taste of life.”\n\n\n“What is death… death is to die. If a person dies, then it is death, and if the\nheart dies, then it forgets… that is death.”\n\n\n“The water falling in this puddle today is life. Tomorrow, when there is no\nwater falling, then this puddle is death. Dead water is water without life and\nflow.”Then his right hand casually pointed again and this time it was on the\nmen next to the fire. The void in his eyes became even stronger and he said,\n\n\n“Today they can enjoy, be angry, be sad, or be happy. That is life. In the\nfuture, they won’t be able to enjoy, be angry be sad, or be happy, and that is\ndeath.”His hand suddenly moved and pointed at a praying mat. He said, “This\nshrine was alive when the statue of the god was here. Now that it is without\nit, it is dead!”Speaking of this, he stood up, pointed at the sky, and said,\n\n\n“These raindrops are born in the sky and die on the earth. What’s in the middle\nis their life. I look at this rain not for the sky, earth, or the rain itself,\nbut the raindrop’s entire life… this cycle of life and death.”\n\n\n“Life and death are separated by a thin line. Some are clearly dead, but they\nare now living in someone else’s heart. Some are clearly alive, but they start\ndying if they don’t change.”\n\n\n“She felt Wang Lin’s lips and his warmth. This warmth contained an unerasable\njoy, a silent call, and a sense of protection that will never fade.Love is like\na river; the left shore is the joyous laughter that can brighten up 1000 years\nof sadness and the right shore is an eternal silence lingering under the\ncandlelight. What flows between them is years of fading loneliness.”\n\n\n“Humans often choose to dedicate large portions of their lives to seemingly\nuseless or destructive causes. On the surface, these causes make no sense.”\n\n\n“We’re apes. We think we’re all sophisticated with our toaster ovens and\ndesigner footwear, but we’re just a bunch of finely ornamented apes. And\nbecause we are apes, we instinctually measure ourselves against others and vie\nfor status. The question is not whether we evaluate ourselves against others;\nrather, the question is by what standard do we measure ourselves?”\n\n\nQuotes temp\n\n\n“As long as one’s heart is determined, then one can cultivate any of the\nmillions of daos that exist!”\n\n\n“Things of this world can’t escape karma. The karmatic cause of yesterday will\nbe the karmatic effect of today. Only by settling one’s karma and letting dirt\nreturn to dirt, letting dust return to dust, can a cycle be complete.”\n\n\n“Sometimes one or two leaves would fall and drift before his eyes.Falling\nleaves will all eventually return to the roots of the tree. They were like\nchildren who would leave when tired but would always return to their love\nones.”\n\n\n“All of humanity’s  problems stem from man’s  inability to sit quietly in a\nroom alone.”\n\n\n“There are no monsters in the sea. Only the ones we make up in our heads”\n\n\n“The heavens’ dao is endless, the path of dao is boundless. The kind act of\ntoday will create karmic cause… In the future, the cycle will be completed and\nkarmic effect will form… Dao is like this bowl, not perfect and filled with\ncracks. It can be broken at any time… The dao of the heavens is something\neveryone can prove and everyone can confirm. The heavens is not dao. Dao is\nformed by the will of the heavens and earth. Everyone can have this will!”Just\nlike the fish that lives in the river. It doesn’t violate the heavens or\nprovoke yin and yang, it lives peacefully and free! This river is the heavens,\nthe world. The fish is all living things that exist inside the world!”However,\nthe net pulls the fish out from the river. This net is dao, the laws of the\nworld! No matter where you are, you won’t be able to escape dao!”This net will\none day pull the fish out from the river. Once out of the river, the fish will\nface the laws of the world. Either it obeys and enters the reincarnation cycle\nor it must revolt! Break the net and revolt!”Cultivators are like the\nstruggling fish. The more they fiercely the struggle, the more they defy\ndao!”This is dao! Whether it is life and death or karma, all of it is formed by\nthe will of dao! I’m karma”\n\n\n“Apricot tree blooms white flowers…”Cultivation, cultivation, mortals yearn to\nbecome immortal and enter the cultivation world. Yet they don’t know how many\ncultivators are envious of a mortal’s bland life.”How many more died in foreign\nplaces like Sun Tai, their ashes scattering with the wind, unable to return\nhome… However, many parents and relatives weren’t able to meet their children\neven at their dying moment. If one had the chose again, would they still take\nthat step to become a cultivator…”\n\n\n“The flame moved with the wind just like how the wind makes the plants move,\nwhich makes it look like the mountain is moving, but in reality…” Wang Lin\npondered.”In reality, the mountain didn’t move, the flame didn’t move, what\nmoved was the wind!”\n\n\n“Dao is like a thought, and everyone’s thoughts are different. If one wants to\nenrich one’s own mind, they must learn other people’s thoughts as well.”\n\n\n“Where is there no heaven?”Wang Lin didn’t speak but raised his right hand and\nwaved. A breeze blew through the courtyard and a circle appeared on the ground\naround Li Qianmei and Lu Yuncong. It was as if someone had drawn a circle\naround them with a stick.”This is the circle is the heaven both of you think\nexist. Because you believe that there is a heaven, the heavens exist. You\nregard yourselves as ants that struggle to break out from the heavens, which is\nyour cage. This is your belief and your faith. However, even if you walk out\nfrom the circle, what’s the use?”Wang Lin shook his head and waved his right\nhand, then another circle formed around the circle from before.”Once you come\nout, there will still be another heaven, and the cycle of karma continues\nwithout end until… The heaven in your heart is erased by the heavens, and this\nis the lie of the heavens! I was thinking about this hundreds of years ago. So\nwhy must there be a heaven?”\n\n\n“A old friend once told me that the rain is born from the heavens and dies on\nthe earth. The middle is life… But does the rain really come from the heavens….\nRain comes from the void and has nothing to do with the heavens. The rain falls\non the earth and nourishes all life, but it has nothing to do with the earth.\nIt is just the fate of rain!”The rain forms from water vapor, and water vapor\ncomes from all living things. The rain naturally needs to return to them. This\nis a cycle, a cycle of karma, and it can also be considered fate.”There is the\nlaw of fate. It is invisible, but it surrounds all life and quietly changes\neverything…” Wang Lin looked at the sky and randomly waved his hand. A\nthunderous rumble came from the sky and then water vapor gather from all sides.\nDark clouds appeared, and a moment later, rain fell from the sky.”Look at the\nlife of a raindrop. Is there any raindrop that falls in a straight line… I\nobserved rain for a long time like I was viewing life, but I didn’t see any\ndrop of rain fall straight without changing its trajectory. They… always change\ndue to the wind or the clouds or their own weight, adjusting the place they\nwill land. Do you see the unwillingness from the rain?”Do you know why it is\nlike this?” Wang Lin withdrew his gaze and looked at Li Qianmei.Li Qianmei\nlooked at the rain, and after a long time, she softly said, “Where the will of\nthe heavens exists, fate will change.”The life of a raindrop is very short, but\ndue to cycle, it is very long… Cultivators like us have very long lives, but\ndue to the will of the heavens, it is also very short.”However, in just in the\nshort life of a drop of rain, it struggles countless times to escape the\ncontrol of fate. In order to fight fate, it keeps changing where it will\nland!”And the long lives of cultivators like us are not something the rain\ncan’t compare to, but how many are willing to desperately struggle to the death\nto escape the clutches of fate like the rain? To struggle until death to resist\nthe arrangement of the heavens. To struggle until death to defy the will of the\nheavens!”Wang Lin waved his sleeves and the sky rumbled and the rain was pushed\nback into the clouds. The dark clouds collapsed and the rain turned into water\nvapor that dissipated into the world.”You changing the fate of the raindrop\nmakes you the will of the heavens. The moth will be burned by the fire. If you\nblow out the fire, making it so the moth can’t die in the fire, then you have\nchanged the fate of the moth. If fate wants someone to die but you save them,\nthen you are the will of the heavens! There is a saying from ancient times: Are\nthose who call themselves kings and lords more noble than us?’ This saying\nembodies this truth!”The celestials also had a saying: Once a man achieves dao,\nhis chicken and dogs will rise to heaven.’”\n\n\n.”Dao isn’t the net or the mountain, but a thought! This thought varies from\nperson to person. Some people regard it as a net, while others regard it as a\nmountain…” When Wang Lin heard Li Qianmei’s words, he began to ponder, and his\neyes gradually grew brighter.”Dao is a thought? A person is a person because\nthey have thoughts, so they can separate from their body, merge with the world,\nand ponder about the unknown…”\n\n“life springs from the dirt of the earth, and water too clean often harbors no\nfish.’\n\n.”Seeking dao… In truth, it is bringing the dao into your heart, this is\ndao-seeking. The so-called comprehension and domain are the same. You keep a\ncomprehension in your heart and slowly experience it until it merges with your\ndao. Eventually, it will become a domain, an ideal.”\n\n\n“When we’re passionate about something, we want other people to also be\npassionate about it. It validates ourselves. We don’t care if our passion is\ngood for other people. We just want them to confirm that our passion is the\nbest thing in the world.”\n\n\n“The body’s desire for homeostasis can be harnessed to drive changes: push it\nhard enough and for long enough, and it will respond by changing in ways that\nmake that push easier to do.”\n\n\n“YOU GONNA BOO HOO OR YOU GONNA YEE HAW?!”\n\n\n“I can take a spoon and swing that at a tree and call that sucker an axe”\n\n\n“I tend to think too much, Bast. My greatest successes came from decisions I\nmade when I stopped thinking and simply did what felt right. Even if there was\nno good explanation for what I did.” He smiled wistfully. “Even if there were\nvery good reasons for me not to do what I did.”\n\n\n“How odd to watch a mortal kindle Then to dwindle day by day. Knowing their\nbright souls are tinder And the wind will have its way. Would I could my own\nfire lend. What does your flickering portend?”\n\n\n“Call a jack a jack. Call a spade a spade. But always call a whore a lady.\nTheir lives are hard enough, and it never hurts to be polite.”\n\n\n“Just pity him, my boy. Tomorrow we’ll be on our way, but he’ll have to keep\nhis own disagreeable company until the day he dies.”\n\n\n“A poet is a musician who can’t sing. Words have to find a man’s mind before\nthey can touch his heart, and some men’s minds are woeful small targets. Music\ntouches their hearts directly no matter how small or stubborn the mind of the\nman who listens.”\n\n\n“Bones mend. Regret stays with you forever.”\n\n\n“Because pride is a strange thing, and because generosity deserves generosity\nin return. But mostly because it felt like the right thing to do, and that is\nreason enough.”\n\n\n“Besides, anger can keep you warm at night, and wounded pride can spur a man to\nwondrous things.”\n\n\n“Anyone who thinks boys are innocent and sweet has never been a boy himself, or\nhas forgotten it. And anyone who thinks men aren’t hurtful and cruel at times\nmust not leave his house often. And he has certainly never been a physicker.”\n\n\n“That’s why stories appeal to us. They give us the clarity and simplicity our\nreal lives lack.”\n\n\n“Music is a proud, temperamental mistress. Give her the time and attention she\ndeserves, and she is yours. Slight her and there will come a day when you call\nand she will not answer.”\n\n\n“And while my suite of rooms at the Horse and Four had been luxurious, my tiny\nroom at Anker’s was comfortable. Think in terms of shoes. You don’t want the\nbiggest you can find. You want the pair that fits. In time, that tiny room at\nAnker’s came to be more of a home to me than anywhere else in the world.”\n\n\n“If you are searching for a pattern in anything, you will most likely find it”\n\n\n“It had been treated unkindly in the past, but that didn’t make it less lovely\nunderneath. So yes. It had flaws, but what does that matter when it comes to\nmatters of the heart? We love what we love. Reason does not enter into it. In\nmany ways, unwise love is the truest love. Anyone can love a thing because.\nThat’s as easy as putting a penny in your pocket. But to love something\ndespite. To know the flaws and love them too. That is rare and pure and\nperfect.”\n\n\n“You know you’re clever. That’s your weakness. You assume you know what you’re\ngetting into, but you don’t.”\n\n\n“My point is this. In each of us there is a mind we use for all our waking\ndeeds. But there is another mind as well, a sleeping mind. It is so powerful\nthat the sleeping mind of an eight-year-old can accomplish in one second what\nthe waking minds of seven members of the Arcanum could not in fifteen minutes.”\n\n\nHe made a sweeping gesture. “Your sleeping mind is wide and wild enough to hold\nthe names of things. This I know because sometimes this knowledge bubbles to\nthe surface. Inyssa has spoken the name of iron. Her waking mind does not know\nit, but her sleeping mind is wiser. Something deep inside Fela understands the\nname of the stone.”\nElodin pointed at me. “Kvothe has called the wind. If we are to believe the\nwritings of those long dead, his is the traditional path. The wind was the name\naspiring namers sought and caught when things were studied here so long ago.”\n\n\n“What use is care? What good is watching for that matter? People are forever\nwatching things. They should be seeing. I see the things I look at. I am a\nsee-er.”\n\n\n“The struggle is great, the task divine—to gain mastery, freedom, happiness,\nand tranquility.”\n\n\n“This is, in fact, the first obligation of a leader and a decision maker. Our\njob is not to “go with our gut” or fixate on the first impression we form about\nan issue. No, we need to be strong enough to resist thinking that is too neat,\ntoo plausible, and therefore almost always wrong. Because if the leader can’t\ntake the time to develop a clear sense of the bigger picture, who will? If the\nleader isn’t thinking through all the way to the end, who is?”\n\n\n“We do not live in this moment. We, in fact, try desperately to get out of\nit—by thinking, doing, talking, worrying, remembering, hoping, whatever. We pay\nthousands of dollars to have a device in our pocket to ensure that we are never\nbored. We sign up for endless activities and obligations, chase money and\naccomplishments, all with the naïve belief that at the end of it will be\nhappiness.”\n\n\n“A wealth of information creates a poverty of attention.”\n\n\n“Chop wood, carry water. Chop wood, carry water. Chop wood, carry water.”\n\n\n“With my sighted eye I see what’s before me, and with my unsighted eye I see\nwhat’s hidden”\n\n\n“What they thought was silence, because they didn’t know how to listen, was\nfull of accidental sounds. You could hear the wind stirring outside during the\nfirst movement. During the second, raindrops began pattering the roof, and\nduring the third the people themselves made all kinds of interesting sounds as\nthey talked or walked out.”\n\n\n“Most of us would be seized with fear if our bodies went numb, and would do\neverything possible to avoid it, yet we take no interest at all in the numbing\nof our souls.”\n\n\n“In other words, any act that rejects immediate gratification in favor of\nlong-term growth, health, or integrity. Or, expressed another way, any act that\nderives from our higher nature instead of our lower. Any of these will elicit\nResistance.”\n\n\n“The more important a call or action is to our souls evolution, the more\nResistance we will feel toward pursuing it.”\n\n\n“Doctors estimate that seventy to eighty percent of their business is\nnon-health-related. People aren’t sick, they’re self-dramatizing.”\n\n\n“In the past, the focus of the process of invention has tended to be on\nactually getting something to work (“find the lightbulb filament that works,”\net cetera). But in the computational universe, the focus shifts to the question\nof what you want the invention to do. Because once you’ve described the goal,\nfinding a way to achieve it is something that can be automated.”\n\n\n“the truly free individual is free only to the extent of his own self-mastery.”\n\n\n“The artist must be like that Marine. He has to know how to be miserable. He\nhas to love being miserable. He has to take pride in being more miserable than\nany soldier or swabbie or jet jockey. Because this is war, baby. And war is\nhell.”\n\n\n“My friend Tony K e p p e l m a n’s n a p p e d me out of it by asking if I was\ngonna quit. Hell, no! “Then be happy. You’re where you wanted to be, aren’t\nyou? So you’re taking a few blows. That ”s the price for being in the arena\nand not on the sidelines. Stop complaining and be grateful.”\n\n\n“He knows that any job, whether it’s  a novel or a kitchen remodel, takes twice\nas long as he thinks and costs twice as much. He accepts that.”\n\n\n“The Bhagavad-Gita tells us we have a right only to our labor, not to the\nfruits of our labor. All the warrior can give is his life; all the athlete can\ndo is leave everything on the field. The professional loves her work. She is\ninvested in it wholeheartedly. but she does not forget that the work is\nnot her. her artistic self contains many works and many performances.\nalready the next is percolating inside her. the next will be better,\nand the one after that better still.  the professional self-validates.\nshe is tough-minded. in the face of indifference or adulation, she\nassesses her stuff coldly and objectively. where it fell short, she’ll\nimprove it. where it triumphed, she’ll make it better still. she’ll\nwork harder. she’ll be back tomorrow.”\n\n\n“The professional cannot allow the actions of others to define his reality.\nTomorrow morning the critic will be gone, but the writer will still be there\nfacing the blank page.”\n\n\n“I’m not sure if the most spoken words in the average american household are ‘I\nlove you’ or ‘I want that’”\n\n\n“Eternity is in love with the creations of time.”\n\n\n“When we make a beginning, we get out of our own way”\n\n\n“The act of creation is by definition territorial. As the mother-to-be bears\nher child within her, so the artist or innovator contains her new life. No one\ncan help her give it birth. But neither does she need any help.”\n\n\n“Creative work is not a selfish act or a bid for attention on the part of the\nactor. It’s  a gift to the world and every being in it. Don’t cheat us of your\ncontribution. Give us what you’ve got.”\n\n\n“The cost of a thing is the amount of what I will call life which is required\nto be exchanged for it, immediately or in the long run”\n\n\n“It is strange to be known so universally and yet be so lonely.”\n\n\n“When the void is filled, you no longer need distractions to help you avoid\nit.”\n\n\n“Document vs create”\n\n\n“Shit is subjective my man. People arent starting. They are pondering,\ndebating.”\n\n\n“Do you know yourself? Or do you aspire to be something youre not”\n\n\n“You cant create today, distribute, facilitate”\n\n\n“Civilizations advance by extending the number of operations we can perform\nwithout thinking”\n\n\n“Realists are often just dreamers who got their hearts broken along the way”\n\n\n“Perfectionism is the enemy of action”\n\n\n“You are not invisible if you can see yourself”\n\n\n“Love and rationality weren’t compatible.”\n\n\n“If you want to accomplish something great, your heart needs to be in the right\nplace. You don’t possess such a vision, yet you think you are worthy to claim\nthat you will bring the human race to prominence?”\n\n\n“If you chased after the dream of humankind’s rise to prominence, constantly\npursuing the most profound secrets of this world, but you discovered that what\nreally hindered you was the human race itself, how would you feel?”\n\n\n“Before you achieved any success, the entire world would laugh at you, scorn\nyou, and look down on you. When you achieved success… the whole world\nwould become your enemy!”\n\n\n“Real experts don’t fear anything. They act prudently and cautiously at every\nstep.”\n\n\n“After all, if you give out some things for free, people won’t value them\nanymore, and they might even make additional demands before they take you\nseriously. Also, the price that I’m asking for it is really low; it’s already\nquite generous!”\n\n\n“If I am doing something for the greater good, then I will focus all my\nattention on it. If I am doing something selfish, then I will choose the path\nthat will bring me the most benefit.”\n\n\n“I don’t care about what others think. To me, public and personal matters don’t\nnecessarily need to conflict. I can harbor great ambition for the human race,\nbut I also need to know when to do things for myself!”\n\n\n“If you can make a vow for a girl today, you can break that vow for a girl\nanother day”\n\n\n“If a person doesn’t have dreams, what’s the difference between them and a\nsalted fish?”\n\n\n“Until you’re ready to look foolish, you’ll never have the possibility of being\ngreat.”\n\n\n“Fantasy and reality are polar existences of each other, and yet dependent on\nthe other. Without fantasy, there is no point in reality. Without reality,\nfantasy has no meaning. Reality is fantasy, fantasy is reality, this is already\nthe 99th world that I’ve come to…”\n\n\n“Samsara is not a simple return to the origin. It is Nirvana and it is\nrebirth.”\n\n\n“Betrayal is the only truth. Human nature is that of greed to begin with; no\none can ever be content. ”\n\n\n“Little Friend Lin, you have an excellent future and time ahead of you; it\nreally arouses envy in others. Cherish it well. Life is short and no matter how\nbeautiful the springtime of your youth is, white hair will await you in the\nend. If you waste your years then it will become far more difficult to advance\nwhen you reach old age!”\n\n\n“The road of martial arts is to defy the will of the heavens to begin with! You\nblame the heavens for being unfair, then you might as well blame the world for\nnot being flat. ”\n\n\n“In chaos, there was neither space nor time, there was nothing to hear or\nanything to feel, so how could anything be called large or small?”\n\n\n“Dont worry about being interesting, be worried about being interested”\n\n\n“Even the Heavenly Dao has its own samsara. Moreover, even if we may live for a\nhundred million years, in my opinion, that is nothing more than a great and\nwonderful dream. The passing clouds, the awakening dreams, everything is\nnothing but fleeting ripples in water.”\n\n\n“If I can preach to the infallible, then after I perish within my fires, let my\ntongue remain forever.”\n\n\n“There are no eternal enemies nor are there eternal friends. There are only\neternal interests.”\n\n\n“Climbing the road of martial arts is originally trying to shake the leaves\nfrom a tree with a simple gust of wind… at the least… you have the courage to\ntry…”\n\n\n“A person always needed a group to recognize them. When everything was\ndestroyed, when his family and friends were all gone, leaving him all alone,\nthen even if he could grasp the heavens and earth in his palm in millions of\nyears, even if he could rule over the universe by himself, what meaning was\nthere in that?”\n\n\n“Then, the golden pages left behind amongst the spiritas spoke about returning\nto one’s true origins. I am the universe. I am the myriad of existence!”\n\n\n“Your sense of purpose is too strong. Things that are valuable to you, you\nwill pursue. But things that are worthless, you will give up. For instance,\nthis lake. If you had passed by this lake I’m afraid you wouldn’t even have\nspared it a single glance…Your life could be called breaking through all\nobstacles in your way, moving forwards with unstoppable momentum. You\ndesperately rush towards the peak of martial arts and you far surpass all\nothers of your generation. From the time you bloomed, you took few detours.\nBut to gain this you also lost something. The road of martial arts is more\nthan the most mysterious and the greatest of the Heavenly Dao Laws. There are\nalso ordinary, everyday experiences. You are missing a section of your path…\nYour road might be called too smooth. In the past you might have experienced\nsome setbacks, but those are far from enough. You are invincible amongst your\npeers. In battle you sing nothing but victory and have defeated countless\nrivals. It could even be said that you have never experienced true defeat.\nBut, that might become your limitation, making it impossibly difficult for you\nto step onto the peak of martial arts! For one whose road of martial arts is\ntoo smooth, for one who isn’t forced to take detours, for one who sings\nnothing but hymns of victory, it is instead easy to fall into a bottleneck,\nmaking one forever unable to step onto the peak of martial arts.”\n\n\n“The path of the martial artist is like a flame. Practicing the martial arts\nwill only cause pain. The dangers are countless and the road is filled with\nobstacles. Everyone who walks down it will eventually turn to ash, but the true\nmartial artist will be reborn from these ashes. Even if I am only a small and\nweak moth, I will walk into the flames without hesitation. I will fight my\ndestiny for a one in a million chance that I will experience my own samsara and\nbe reborn as a flaming phoenix. And even now, I am no longer a moth…”\n\n\n“A worm that lives amongst the dead leaves and fallen branches will never\nunderstand the beauty and greatness of this world! And a worm was a creature\nthat wouldn’t live past the winter. Because of its short life, it didn’t know\nthe dangers of a world filled with ice and snow. But what about martial\nartists? Wasn’t it the same? 10 billion years ago, those supreme elders might\nhave seemed heaven-shaking and world-breaking, but were they able to see the\nworld 10 billion years later? The current Good Fortune Saint Sovereign was also\nall-powerful, but would he be able to see the future of 10 billion years from\nnow?”\n\n\n“I must struggle in the dust and chaos. Even if I am only a small wave, I will\nstill bravely move forwards…”\n\n\n“They gently supported each other. Even though there was no real physical\ntouch, in this moment their hearts were still connected. This was because…\nwithin each other’s hearts and minds they had forever marked each other, a\nbrand that would never be forgotten.”\n\n\n“Life, really is small…”\n\n\nIf the heavens wish to destroy me then I will destroy the heavens. If the death\ngod wants to take me then I will cut down the death god!’\n\n\n“The charm of wine comes from the mood of drinking wine. Those that understand\nwine taste not just the wine but also the mood. From the spiciness when it\nfirst enters the throat, to the fresh heat once it is swallowed, to the lasting\nfragrance that one can mull over. It’s just like life. In the world, countless\npeople struggle. They experience hardship, they experience tribulations, and if\nthey can survive all of that then they can return to their true state and\nrelease the mellow fragrance of life.”\n\n\n“Even if I have to climb without end in my life, even if I forever remain small\nand never see where the highest peak is, then at the very least… I will keep\nsurpassing myself, I will keep defeating myself.”\n\n\n“During those days, although I experienced innumerable tribulations, these\ntribulations slowly grinded away my edges and corners, causing my character to\nchange and become completely different from what I used to be… I began to learn\nhow to restrain myself, learn how to think deeply of my actions and their\nconsequences, learn to accept reality for what it was, learn to be grateful…”\n\n\n“Man was always unwilling to die. But man would die no matter what. As a person\napproached the precipice of death, they would hope that they could leave behind\nsomething. So that when they closed their eyes one last time, they could tell\nthemselves that their bloodline continued flowing on in the world… This intense\ndesire, if traded for another name, could be called fatherly love’ and motherly\nlove’. The love of parents. In essence, that was one’s hope that one’s life\nwould continue to exist onwards. This was a selfish love, but also a selfless\nlove. A parent’s love never asked to be repaid, because to them, their children\nliving well was the greatest repayment they could ever receive.”\n\n\n“In truth, you saying anything to me is meaningless… the future is not\nsomething given to you, but something that you struggle for yourself. Behind\nthe backs of enemies you defeat lies your own road of growth. The peak of\nmartial arts has never been something that you can reach by piling up resources\nor accumulating your background. Rather, it is something you slaughter towards\neven as you slowly explore your way towards it. When the vast cosmos of the 33\nHeavens surges with chaos, that is a time when heroes pour forth from the ranks\nof creation. A calamity to end the world may be a graveyard for the weak, but\nit will also set the stage for the strong…”\n\n\n“Pain comes from the body, but it actually reflects in the soul and mind… if I\ncan control my soul, then no matter how painful it is it cannot be this\nfierce…”\n\n\n“Currently, what Sheng Mei pursued was no longer eternal life, but instead what\ncould truly touch her heart and move her soul. To cultivate to fulfill one’s\nown heart, perhaps this was truly touching upon the peak of martial arts. But\nin reflection, what was one’s heart? Perhaps that was to allow oneself, one’s\nlovers, one’s spouses, one’s children, one’s family, one’s friends, one’s\npeople, to allow everyone to live in peace and happiness….”\n\n\n“Life, is innocent. Even amongst the saints, there are naïve little children,\nthere are kind-hearted subjects. This war was never their intention.”\n\n\n“I am not sure whether we can win or not. The future is not set in stone. We\nmartial artists cultivate the body, cultivate the Heavenly Dao, cultivate the\ndivine soul, but in the end, just what are we cultivating for? What are we\nchasing after? Even I have been left confused by this. For glory? That is only\nfleeting smoke before the eyes. For strength? I already possess the power to\nsunder the heavens and shatter the earth, to annihilate stars and the vastness\nof space. To become immortal? In these years I have also chased after\nimmortality. However, when I was in the Emperor Bone Sea I looked upon the\ncountless pained remnant souls there as well as the 100 billion year plot that\nthe Soul Emperor had laid out for the sake of immortality, and suddenly, at\nthat time I felt that chasing after eternal life was meaningless. For we that\npractice martial arts, we naturally must fight. But in fact, what we are\nfighting is the chaos of the world, the time when the catastrophe arrives. The\nextinction of races, lives lost like burning coals, the collapse and\ndestruction of great worlds, this time, the fight has reached the peak. Then,\nperhaps the peak of martial arts is originally for the common people of the\nworld…”\n\n\n“Let the enemy sink into the bottomless sea of fighting against commoners.”\n\n\n“After all, humankind wasn’t driven by words and whips, but by their own\nbenefits. Putting it another way, as long as he could continuously fulfill the\nbasic interests of the people under his rule, there would be no one who could\nshake his dominance.”\n\n\n“Rather than say ‘charge for me’, say ‘charge with me’.” Iron Axe smiled.\n\n\n“The stronger you are, the more challenges you’ll meet and the more setbacks\nyou’ll encounter. But don’t forget, no matter how much hardship you\nexperience, you’re already enviable.”\n\n\n“I’ve been told that geniuses will always die doing what they are best at, and\nGod would make up for it by giving such people an unmatchable talent—This is\nfate. A road that’s  destined to be good will cause the one who walks on it to\nsuccumb to temptation because of one’s  extraordinary talent and eventually\nfall from grace. On the contrary, those ordinary people without much talent\nwill tend to live longer.”\n\n\n“People needed to break through the impossible because no one knew whether\nthere would be a miracle unless they tried.”\n\n\n“They said they wanted to defend Neverwinter and everything in their native\ntown that they earned through their hard work.” The old man sipped the tea and\ncontinued, “To be completely honest, I  didn’t understand at first and asked\nthem why it had to be them instead of others. Lightning was asking the same\nquestion within herself. Broocher seemed to know what she was thinking. He\nanswered, “They said that others had made their sacrifices. Many people were\nkilled during the battle against the demonic beasts when they were just members\nof the Militia. People died all the time when they fought against Duke Ryan and\nthe church. If everybody relied on others, we would have been still working at\nthe mine, living like animals,” the old man said. “There’s  no battle without\nblood spilled. Everybody has his own turn. If nobody wanted to come forward, we\nwould have been at the mercy of our enemy — that was what they told me.”\n\n\n“The history of the human civilization was, essentially, a process where men\ncontinuously developed different methods to boil water.”\n\n\n“Empathy is the ability to recognize the perspective of a counterpart and\nvocalize it” “There are three kinds of yes - counterfeit, confirmation and\ncommitment”\n\n\n“All living things are mundane, and yet, all living things can also be\nextraordinary! Cultivators practice cultivation because of their desire to shed\nthe limitations of the mortal world. They wish to be like the carp that leapt\nover the dragon gate…. Plants and vegetation are similar. When concocting them\ninto medicinal pills and consuming them, one should not solely focus on\nstrengthening themselves, but should also strive to sense the plant’s\nfundamental will. It might seem like the plant dies in the process of becoming\na pill, but who can truly say whether or not this is just a rebirth into\nanother stage of life for them?”\n\n\n“When your Dao is the heart, then if you have something in your heart, it\nexists. If you don’t have it in your heart, it doesn’t exist.”\n\n\n“Give a man a fish and you feed him for a day. Teach a man to fish and you feed\nhim for a lifetime so that he many never discover how much he loves steak”\n\n\n“It’s not the writing part that’s hard. What’s hard is sitting down to write.\nWhat keeps us from sitting down is Resistance.”\n\n\n“You think Resistance isn’t real? Resistance will bury you.”\n\n\n“The danger is greatest when the finish line is in sight. At this point,\nResistance knows we’re about to beat it. It hits the panic button. It marshals\none last assault and slams us with everything it’s got.”\n\n\n“The paradox seems to be, as Socrates demonstrated long ago, that the truly\nfree individual is free only to the extent of his own self-mastery. While those\nwho will not govern themselves are condemned to find masters to govern over\nthem.”\n\n\n“The counterfeit innovator is wildly self-confident. The real one is scared to\ndeath.”\n\n\n“So if you’re paralyzed with fear, it’s a good sign. It shows you what you have\nto do.”\n\n\n“Remember, the part of us that we imagine needs healing is not the part we\ncreate from; that part is far deeper and stronger. The part we create from\ncan’t be touched by anything our parents did, or society did. That part is\nunsullied, uncorrupted; soundproof, waterproof, and bulletproof. In fact, the\nmore troubles we’ve got, the better and richer that part becomes.”\n\n\n“I write only when inspiration strikes,” he replied. “Fortunately it strikes\nevery morning at nine o’clock sharp.”\n\n\n“Show me a writer who’s too good to take Job X or Assignment Y and I’ll show\nyou a guy I can crack like a walnut.”\n\n\n“He knows that any job, whether it’s a novel or a kitchen remodel, takes twice\nas long as he thinks and costs twice as much. He accepts that. He recognizes it\nas reality.”\n\n\n“Clear your mind young padawan, There is no emotion, there is peace. There is\nno ignorance, there is knowledge. There is no passion, there is serenity. There\nis no chaos, there is harmony. There is no death, there is the Force”.\n\n\n“There you are; human nature in action, wrongdoers, blaming everybody but\nthemselves. We are all like that.”\n\n\n“When dealing with people, let us remember we are not dealing with creatures of\nlogic. We are dealing with creatures of emotion, creatures bristling with\nprejudices and motivated by pride and vanity.”\n\n\n“God himself, sir, does not propose to judge man until the end of his days. Why\nshould you and I?”\n\n\n“If there is any one secret of success,” said Henry Ford, “it lies in the\nability to get the other person’s  point of view and see things from that\nperson’s  angle as well as from your own.”\n\n\n“The universe is largely left up to chance, and the control you think you have…\nmuch of it is a delusion.”\n\n\n“The world has never complained about how busy it is. ”\n\n\n“Those who work in a playful, relaxed manner tend to work efficiently and\ncreatively. Those who work nonstop, driven only by stress, work without joy.”\n\n\n“A good family trip can prevent divorce. What makes music beautiful is the\ndistance between one note and another. What makes speech eloquent is the\nappropriate pause between words. From time to time we should take a breath and\nnotice the silence between sounds.”\n\n\n“The tycoons of social media have to stop pretending that they’re friendly nerd\ngods building a better world and admit they’re just tobacco farmers in T-shirts\nselling an addictive product to children. Because, let’s face it, checking your\n\n\n“likes” is the new smoking.”\n\n\n“Some may not believe it,but I spent hours perfecting whatever I did.”\n\n\n“People have to grow through skillful frustrations, otherwise they have no\nincentive to develop their own means and ways of coping with the world.”\n\n\n“Had not this water just now illustrated to me the principle of gung fu? I\nstruck it but it did not suffer hurt. Again, I struck it with all my might—yet\nit was not wounded! I then tried to grasp a handful of it but this proved\nimpossible. This water, the softest substance in the world, which could be\ncontained in the smallest jar, only seemed weak. In reality, it could penetrate\nthe hardest substances in the world. That was it! I wanted to be like the\nnature of water.”\n\n\n“Well, the water was not to be deterred. It was going to find a path, or even\nmultiple paths. It would move along until it met with an obstacle, and then, if\nit needed to, it would change course and keep on flowing. It used “no way” as\nits way. In other words, it used every possible way. And it ran along without\nlimitation.”\n\n\n“Perfection as we typically think about it should be treated more as a way to\nfocus our attention rather than a final accomplishment that we attain”\n\n\n“Like flowing water, life is perpetual movement”\n\n\n“Water doesn’t have this problem. A wave doesn’t have to remember how to land\non the shore. A river doesn’t have to consider how to carve a canyon into a\nmountain. A lake doesn’t have to practice giving life to the fish and the\nplants. In its simple way of just being, water can be our guide along our path\nto our natural selves. And one day, if we self-actualize, we can attain (and\nreclaim) this”\n\n\n“The world is always fair to those who win”\n\n\n“Work hard, try your best and enjoy the good fortune if it comes your way. But\ndont depend on it. And dont think if it doesnt, you are somehow less worthy or\nto blame ”\n\n\n“Life can only be understood backwards; but it must be lived forwards.”\n\n\n“You never know what worse luck your bad luck has saved you from.”\n\n\n“None of us are immune from life’s tragic moments. Like the small rubber boat\nwe had in basic SEAL training, it takes a team of good people to get you to\nyour destination in life. You cannot paddle the boat”\n\n\n“Because, Mr. Mac, life isn’t fair and the sooner you learn that the better off\nyou will be”\n\n\n“It is easy to blame your lot in life on some outside force, to stop trying\nbecause you believe fate is against you. It is easy to think that where you\nwere raised, how your parents treated you, or what school you went to is all\nthat determines your future. Nothing could be further from the truth. The\ncommon people and the great men and women are all defined by how they deal with\nlife’s unfairness: Helen Keller, Nelson Mandela, Stephen Hawking, Malala\nYousafzai, and—Moki Martin. Sometimes no matter how hard you try, no matter how\ngood you are”\n\n\n“Life is a struggle and the potential for failure is ever present, but those\nwho live in fear of failure, or hardship, or embarrassment will never achieve\ntheir potential. Without pushing your limits, without occasionally sliding down\nthe rope headfirst, without daring greatly, you will never know what is truly\npossible in your life.”\n\n\n“At some point we will all confront a dark moment in life. If not the passing\nof a loved one, then something else that crushes your spirit and leaves you\nwondering about your future. In that dark moment, reach deep inside yourself\nand be your very best.”\n\n\n“Hope is the most powerful force in the universe. With hope you can inspire\nnations to greatness. With hope you can raise up the downtrodden. With hope you\ncan ease the pain of unbearable loss. Sometimes all it takes is one person to\nmake a difference. We will all find ourselves neck deep in mud someday. That is\nthe time to sing loudly, to smile broadly, to lift up those around you and give\nthem hope that tomorrow will be a better day.”\n\n\n“Life is full of difficult times. But someone out there always has it worse\nthan you do. If you fill your days with pity, sorrowful for the way you have\nbeen treated, bemoaning your lot in life, blaming your circumstances on someone\nor something else, then life will be long and hard. If, on the other hand, you\nrefuse to give up on your dreams, stand tall and strong against the odds—then\nlife will be what you make of it— and you can make it great. Never, ever, ring\nthe bell!”\n\n\n“Start each day with a task completed. Find someone to help you through life.\nRespect everyone. Know that life is not fair and that you will fail often, but\nif you take some risks, step up when the times are toughest, face down the\nbullies, lift up the downtrodden, and never, ever give up… if you do\nthese things, then the next generation and the generations that follow\nwill live in a world far better than the one we have today. And what\nstarted here will indeed have changed the world,”\n\n\n“The desire for more positive experience is itself a negative experience. And,\nparadoxically, the acceptance of one’s negative experience is itself a positive\nexperience.”\n\n\n“Because here’s another sneaky little truth about life. You can’t be an\nimportant and life-changing presence for some people without also being a joke\nand an embarrassment to others. You just can’t. Because there’s no such thing\nas a lack of adversity. It doesn’t exist. The old saying goes that no matter\nwhere you go, there you are. Well, the same is true for adversity and failure.\nNo matter where you go, there’s a five-hundred-pound load of shit waiting for\nyou. And that’s perfectly fine. The point isn’t to get away from the shit. The\npoint is to find the shit you enjoy dealing with.”\n\n\n“I once heard an artist say that when a person has no problems, the mind\nautomatically finds a way to invent some. I think what most people—especially\neducated, pampered middle-class white people—consider “life problems” are\nreally just side effects of not having anything more important to worry about.\nIt then follows that finding something important and meaningful in your life is\nperhaps the most productive use of your time and energy. Because if you don’t\nfind that meaningful something, your fucks will be given to meaningless and\nfrivolous”\n\n\n“A cheerful poverty, he says, is an honourable state. But if it is cheerful it\nis not poverty at all. It is not the man who has too little who is poor, but\nthe one who hankers after more. What difference does it make how much there is\nlaid away in a mans safe or in his barns, how many head of stock he grazes or\nhow much capital he puts out at interest, if he is always after what is\nanothers and only counts what he has yet to get, never what he has already. You\nask what is the proper limit to a persons wealth? First, having what is\nessential, and second, having what is enough.”\n\n\n“This prompts me to memorize something which I came across in Pomponius. Some\nmen have shrunk so far into dark corners that objects in bright daylight seem\nquite blurred to them. A balanced combination of the two attitudes is what we\nwant; the active man should be able to take things easily, while the man who is\ninclined towards repose should be capable of action. Ask nature: she will tell\nyou that she made both day and night ”\n\n\n“To me, says Democritus, a single man is a crowd, and a crowd is a single man.\nEqually good is the answer given by the person, whoever it was (his identity is\nuncertain), who when asked what was the object of all the ”\n\n\n“Conquer the world or die”\n\n\n“Pain is but a passing thought”\n\n\n“In front of my brothers, my heart still remains unchanged. I’m still the\noriginal me, no matter what happens.”\n\n\n“If you ever meet a girl who can move your heart, do not miss the chance, you\nmust take the initiative.”\n\n\n“The past is now past, and the future is too far away. Only the present\nmatters.”\n\n\n“Each predicament appears incomparably complex but yet, doesnt simplicity lies\non the other side of complexity?”\n\n\n“Now the internet is not so good, because smart people go to the internet. You\nshould go to the off ground”\n\n\n“Money follows people. People follow dreams”\n\n\n“The sad truth is that the truth is sad.”\n\n\n“The key to life is to be unborable”\n\n\n“Ideas are like knowing you should do push ups”\n\n\n“People run themselves into the ground when they start wavering from what they\nare… When you do the things you don’t want to do because you simply graduated\nto the next spot… you LOSE!”\n\n\n“Inspiration exists but it has to find you working”\n\n\n“When someone dies they are regarded with more respect and social standing\nbecause they are no longer competing with us”\n\n\n“You cant read about doing pushups”\n\n\n“If you are good at what you do, its not about time.”\n\n\n“Think globally, act locally”\n\n\n“Its only ever the moment that is guiding you.”\n\n\n“Always know your action. So if you come in the morning confident or on your\nass, you know what to do.”\n\n\n“Listening is good. obeying is not always the best.”\n\n\n“Where does language end and vision begin”\n\n\n“Deep learning is like the geometric mean of biology and physics.” (TL: Bruh.)\n\n\n“What is overfitting? When your model is somehow very sensitive to the small\nrandom unimportant stuff in your dataset. Think of it like the dataset and the\nmodel having Degrees of Freedom. If both are similar, then the model will not\nbe able to ignore even minor changes in the data. But if the model is much\nlarger than the data itself then it would be able to ignore the tiny\nrandomness”\n\n\n“Saying you don’t need privacy because you have nothing to hide is like saying\nyou don’t need freedom of speech because you have nothing to say.”\n\n\n“Weak existences would feel tiny when they faced the vast world and the passage\nof time, which was infinite.  But there would always be people who did not wish\nto remain tiny.  They would search.  What would the future be like 10,000 years\nfrom now? What about a 100,000 years? 10,000,000 years? Did this world have an\nend to it?  What was at the end of the continent? Was there another piece of\nland across the ocean? Was there a sky above the skies?  These people would\nwant to slowly control their own destiny. By conquering nature in the pursuit\nof balance, they would exist on the same level as the world!”\n\n\n“This made Yi Yun even more diligent. Life could not repeat itself like the\nrising and setting of the sun. If one did not grab every moment, one would\nwaste their talent and potential. It would be a great regret.”\n\n\n“It was like in his previous life. The richest people in the poor mountainous\nregions, where vehicles could not even pass through, would probably not amount\nto a hundredth of the wealth of a commoner in Shanghai.”\n\n\n“Everyone had to pay the price of their choices.”\n\n\n“Lightning was a destructive power, but it was a power that created as well. It\nwas rumored that life was first born when lightning struck the ocean in the\npast.”\n\n\n“In fact, human nature was arguably evil. Some people were inclined to kill\npeople of their own species. After gaining pleasure from doing so, they would\nthen plunder the riches of others for themselves. Some people killed thousands\nto make ghost summoning banners, seizing young ladies as cultivating slaves,\nand even doing despicable acts towards young girls… Many a time, when these\npeople released their deviant inhibitions, they would resort to anything while\nacting fanatically.  However…due to the prohibitions of morality, and the laws\nand rules established by large factions in this world, many people could only\nstrongly suppress the evil in their hearts.  But this suppression would be\nlifted without any worries when it was another species in question.  As they\nwere not of the same species, they could vent the evil in their hearts. They\nwould not be considered cruel and bloodthirsty while engaging in those\natrocities. On the contrary, they could be proud of it. For example, they would\nnot be criticized if they infiltrated the Desolate race’s grounds, plundering\nlarge numbers of Desolate race young ladies as cultivation slaves.  From a\ncertain point of view, the conflict between the Human and Desolate race was not\ncompletely engineered by the Blood Moon. It was a result of the natural\ninstincts between two intelligent lifeforms, wishing to vent the corrupt nature\nin their hearts.”\n\n\n“Life must be understood backward. But it must be lived forward.”\n\n\n“Maybe it’s just in his nature. Since the start of history, all those that were\nsuccessful would choose not to suppress their heart. They dared to love, and\ndared to hate, and by yielding to their emotions, their dao-hearts became clear\nand tranquil, with no knots obstructing their progress. Even if the entire\nworld was their enemy, so what of it? They would just take it in their stride.\nThe Azure Emperor back then had the same personality as him, one that led him\nto soar brilliantly in the skies. But sadly, the hatred he garnered eventually\nbecame the cause of his downfall. The large-eyed elder spoke in a low voice.\nThere were two kinds of people that would enjoy great success in life. The\nfirst kind, were people like Qin Wentian and the Azure Emperor, displaying\ntheir talent, not suppressing their heart, doing as they wished wherever and\nwhenever they wanted it. The second kind, were those that could tolerate and\nendure what shouldn’t be tolerated and endured, lying to the world and even to\nthemselves, appearing like a perfect gentleman, yet had the heart of a devil.\nSuch a person, had a heart as deep as night, with an extremely sinister nature.\nThe root of it all, was still one’s nature. If one’s heart was strong enough,\nnothing could cause it to waver.”\n\n\n“Cultivation is a path that solely belongs to oneself. Everyone takes a\ndifferent path, has different levels of talent, different experiences and\nnaturally different comprehensions. When you cultivate in the future, do not\never blindly follow the path of others because you feel that he is strong. What\nyou have to do is to find the path most suited to you. At most, you can take\nanother’s comprehensions as a slight reference, but do not let it direct your\npath. Comprehend that which you’d like, and only then will the path you tread\nbe the most suited for yourself.” Qin Wentian smiled as he continued,\n\n\n“Cultivation has to follow one’s heart. If your heart isn’t even sure of the\npath you want, how can your cultivation be smooth? This is my understanding, so\nlisten well. I won’t explain in detail my comprehensions to you, imposing onto\nyou a concept that might do more harm than good.”\n\n\n“If you are afraid of the sword, you will die by it,”\n\n\n“Do less. Do better. Know why.”\n\n\n“My dear fellow, who will let you? That’s  not the point. The point is, who\nwill stop me?”\n\n\n“Innovate where you can. Where you can’t, use the industry standards.”\n\n\n“Oh, cherry tree, begrudge not thy blossoms as they are deflowered in the\nspring, for come winter, even thy sturdiest wood shall wither.”\n\n\n“My dear young cousin, if there’s  one thing I’ve learned over the eons, it’s\nthat you can’t give up on your family, no matter how tempting they make it. It\ndoesn’t matter if they hate you, or embarrass you, or simply don’t appreciate\nyour genius for inventing the Internet—”\n\n\n“You see, in times of trouble, even gods can lose faith. They start putting\ntheir trust in the wrong things. They stop looking at the big picture and start\nbeing selfish. But I’m the goddess of marriage, you see. I’m used to\nperseverance. You have to rise above the squabbling and chaos, and keep\nbelieving. You have to always keep your goals in mind.”\n\n\n“You’re not so different from me, demigod. Even when I’m out of the water, the\nwater is within me. It is my life source.”\n\n\n“Oh, Hades if I know. But remember, boy, that a kind act can sometimes be as\npowerful as a sword. As a mortal, I was never a great fighter or athlete or\npoet. I only made wine. The people in my village laughed at me. They said I\nwould never amount to anything. Look at me now. Sometimes small things can\nbecome very large indeed.”\n\n\n“Men hate passion, any great passion. Henry Cameron made a mistake: he loved\nhis work. That was why he fought. That was why he lost.”\n\n\nNot all powers are spectacular.” Hestia looked at me. “Sometimes the hardest\npower to master is the power of yielding. Do you believe me?”\n\n\n“You love your work. God help you, you love it! And that’s  the curse. That’s\nthe brand on your forehead for all of them to see. You love it, and they know\nit, and they know they have you. Do you ever look at the people in the street?\nAren’t you afraid of them? I am. They move past you and they wear hats and they\ncarry bundles. But that’s  not the substance of them. The substance of them is\nhatred for any man who loves his work. That’s  the only kind they fear. I don’t\nknow why. You’re opening yourself up, Roark, for each and every one of them”\n\n\n“No! I don’t want to speak of that! But I’m going to. I want you to hear. I\nwant you to know what’s  in store for you. There will be days when you’ll look\nat your hands and you’ll want to take something and smash every bone in them,\nbecause they’ll be taunting you with what they could do, if you found a chance\nfor them to do it, and you can’t find that chance, and you can’t bear your\nliving body because it has failed those hands somewhere. There will be\ndays when a bus driver will snap at you as you enter a bus, and he’ll\nbe only asking for a dime, but that won’t be what you’ll hear; you’ll\nhear that you’re nothing, that he’s  laughing at you, that it’s\nwritten on your forehead, that thing they hate you for. There will be\ndays when you’ll stand in the corner of a hall and listen to a creature\non a platform talking about buildings, about that work which you love,\nand the things he’ll say will make you wait for somebody to rise and\ncrack him open between two thumbnails; and then you’ll hear the people\napplauding him, and you’ll want to scream, because you won’t know\nwhether they’re real or you are, whether you’re in a room full of gored\nskulls, or whether someone has just emptied your own head, and you’ll\nsay nothing, because the sounds you could make—they’re not a language\nin that room any longer; but if you’d want to speak, you won’t anyway,\nbecause you’ll be brushed aside, you who have nothing to tell them\nabout buildings! Is that what you want?  Roark sat still, the shadows\nsharp on his face, a black wedge on a sunken cheek, a long triangle of\nblack cutting across his chin, his eyes on Cameron. “Not enough?” asked\nCameron. “All right. Then, one day, you’ll see on a piece of paper\nbefore you a building that will make you want to kneel; you won’t\nbelieve that you’ve done it, but you will have done it; then you’ll\nthink that the earth is beautiful and the air smells of spring and you\nlove your fellow men, because there is no evil in the world. And you’ll\nset out from your house with this drawing, to have it erected, because\nyou won’t have any doubt that it will be erected by the first man to\nsee it. But you won’t get very far from your house. Because you’ll be\nstopped at the door by the man who’s  come to turn off the gas. You\nhadn’t had much food, because you saved money to finish your drawing,\nbut still you had to cook something and you hadn’t paid for it…All\nright, that’s  nothing, you can laugh at that. But finally you’ll get\ninto a man’s  office with your drawing, and you’ll curse yourself for\ntaking so much space of his air with your body, and you’ll try to\nsqueeze yourself out of his sight, so that he won’t see you, but only\nhear your voice begging him, pleading, your voice licking his knees;\nyou’ll loathe yourself for it, but you won’t care, if only he’d let you\nput up that building, you won’t care, you’ll want to rip your insides\nopen to show him, because if he saw what’s  there he’d have to let you\nput it up. But he’ll say that he’s  very sorry, only the commission has\njust been given to Guy Francon. And you’ll go home, and do you know\nwhat you’ll do there? You’ll cry. You’ll cry like a woman, like a\ndrunkard, like an animal. That’s  your future, Howard Roark. Now, do\nyou want it?”\n\n\n“A child. New life, like Endora. A child’s  mission is to grow. To grow? I\nthought it was to see the world. Love. For Oceanids, this is to meld together\nas one. There will be no division then. That is why Oceanids need no learning\nor thoughts of their own. All that is needed is love. It seems that Oceanids\ncannot love others, for others will only drown in the embrace of pure waters.\nSo they disguise themselves as the dreams of young children, and withdraw from\nthe lives of all other people. Love, that is our destiny. Every day, a child\ntakes a stumbling step forward. Every day, a stream flows into the sea. I have\na whole world left to see.”\n\n\n” The axe forgets but the tree remembers”\n\n\n“If you make a promise, you keep it, if you make a mistake, you apologize. And\nif you give someone a dream, you defend it to the end.”\n\n\n“What does freedom really mean, when demanded of you by a god?”\n\n\n“What you lacked was not wind. It’s  courage that’s  allowed you to become the\nfirst flying birds of this world.”\n\n\n“In the perpetual meantime of a sheltered eternity, most are content to live,\nand not to dream.But in the hidden corners where the gods’ gaze does not fall,\nthere are those who dream of dreaming.”\n\n\n“My greatest wish? It has always been to roam free and experience the whole\nworld. Now I would add that wherever I go, it simply must be with you! Each day\nwith you is an adventure, and where adventurers go, storytellers must follow!”\n\n\n“The rules of war are woven in the womb: the victors shall burn bright, while\nthe losers must turn to ash”\n\n\n“The God of Wisdom’s  enemy is wisdom itself, and the oasis of knowledge is a\nmirage in the desert of ignorance.”\n\n\n“The war has already begun. It is just a continuation of past battles.The gods\ngoad us on with the promise of their seven treasures. Rewards for the worthy.\nThe doorway to divinity.Yet buried in the depths of this world lies smoldering\nremains, a warning to those that dare trespass.”That throne in the sky is not\nreserved for you”But mortal arrogation never stops.None will escape the\nflames.See for yourself.”\n\n\n“The world is full of lost ballads just waiting to be rediscovered”\n\n\n“The wind permeates all, cleansing both mind and body”\n\n\n“All things are impermanent, and to exist is to suffer. We yakshas have no need\nof sympathy or tears.”\n\n\n” Life is a precious thing, yes… But when I think of the burden that the\nConqueror of Demons must bear… sigh death seems to me to have been the easy\nway out. A selfish indulgence, even.”\n\n\n“Rex Lapis once said: “Ones who break their contracts shall suffer the Wrath of\nthe Rock.”\n\n\n” When people see the object of their dreams, how many are really able to\ncontrol their desire and follow the contract…?”\n\n\n” The story continues that some among her people realized at last that this\ngentle, kind, but weak god could never protect anyone in wartime. The Archon\nWar was cruel in the extreme. Instead of consigning her to the agony of defeat,\nthey thought, perhaps it would be better to give her a quick release.”\n\n\n” Faith in a god who has already passed will do you no good.”\n\n\n” History records, but history may be changed. This incident proved that. Time\nis a mighty force, and histories twist in its flow… I need to find a better\nway of recording history in order to engrave its truth. Stone carvings were one\nsuch ancient method. But unchanging stone, immovable earth, even one such as\nmyself… Someday, we may all disappear.”\n\n\n“Kun Jun: Then again, the memories of ore can shift with the passage of time\nand the changing of the environment. Is there a pattern to it? Hmm, difficult\nto say… I feel that ore memories tend to be from the recent past. So there’s\nnever any ancient memories? Rocks endure, but as eons pass their memories are\nerased. Those memories that survive are rooted in powerful emotion or thought.”\n\n\n” The memories of rocks do not last long. Those memories that survive are\nrooted in powerful emotion. But as time passes, so these memories fade into\nobscurity. Erosion is the world’s  greatest destroyer of memories. Eragon:\nErosion…? Kun Jun: Erosion ground Azhdaha’s  consciousness into oblivion.\nSlowly, he forgot the face of his old friend, and his memories of defending\nLiyue Harbor disintegrated. Kun Jun: Azhdaha, now incomplete, became\nirascible… aggressive.”\n\n\n” Curious how swords and daggers are blind, yet their creators see so much.\nPerhaps empathy is mankind’s  proudest achievement after all?”\n\n\n” A heart of stone is a heart nonetheless”\n\n\n” When the door opens, it is time to leave”\n\n\n“Zhongli : People abandon and surrender the things they love to pursue the\nright path. Perhaps this is the erosion imposed on me by the natural order of\nthis world. Zhongli: But I was a god of mankind. My identity may change, but my\neyes will bear witness to the history of humanity.”\n\n\n“You make a pinkie promise, you keep it all your life. You break a pinkie\npromise, I throw you on the ice. The cold will kill the pinkie that once\nbetrayed your friend, the frost will freeze your tongue off so you never lie\nagain.”\n\n\n” Childe: Anyway, childhood dreams are all too easily shattered. Even if you\njust leave them be, they all fall to pieces all by themselves. Childe: So\nsomeone has to protect them, right? Childe: If you make a promise, you keep it,\nif you make a mistake, you apologize… Childe: And if you give someone a\ndream, you defend it to the end… Childe: That is what family is all\nabout, isn’t it?”\n\n\n” Albedo: I’m willing to pour all my energy into research, and yet specimens\nare finite. As the unknown transitions into the realm of scientific\nunderstanding, the feeling of enlightenment is lost. Albedo: All these things\nthat start out as objects of fascination end up possessing the prosaic\nmundanity of a Sunsettia or a Sweet Flower. They cease to be noteworthy.\nPaimon: Oh… So that’s  why you wanted to paint those hilichurls? Because you\ngot to see something new and interesting in the differences between them?\nAlbedo: Precisely. To quote my exact words from earlier, these creatures are,\nfor the most part, “quite boring… not worth closer inspection.” There is\nprecious little about them that serves to pique my curiosity now.”\n\n\n” Albedo: sigh It would seem that that’s  as far as we go. A transient bloom\nof incomparable beauty… Life’s  proudest achievement. Pamon: Paimon thought,\nwith all our efforts, unght have bloomed forever. And it  didn’t even have any\nfruit… Albedo: Life is a manifold tapestry of free entities, its value\nshouldn’t derive from how long it stays with us. Even a momentary burst is\nprecious. Albedo: A short life can be well-lived. A life lived efficiently,\nlived to perfection, is”\n\n\n” Venti: Warriors wear their battle scars with pride, and shields are no\ndifferent. Venti: Surely, an intact shield is one that has shied away from the\nbattlefield. Is not the broken and splintered shield the one that has fought in\ncountless wars and lived to tell the tale? Venti: Though the soldier’s  body be\ntired and torn, still they fight till the very end, till they have no blood\nleft to bleed. Such magnificent strength of will… is that not the true\nmeaning of honor?”\n\n\n“You’re not an artist unless you want to quit atleast once”\n\n\n“According to Su Li’s  words, this was a very stupid sword style, so only the\nstupidest of people could learn it. This sword style was also the most natural,\nbecause there was simply no way it could be used to face one’s  enemies. It\ncould only be used for defense.  It was called the Stupid Sword because to\nlearn this sword, there was no other method but practice through repetition, to\npractice until the seas dried up and stones rotted away, to practice until the\nstars turned and the Big Dipper moved, to practice for as long as the heavens\nexisted and the earth persisted, such that it should be impossible for someone\nto ever confirm that they had learned it.  When Chen Changsheng heard these\nwords, he had completely put the idea of learning this sword out of his mind.\nOnly when Su Li said that this Stupid Sword could be considered the world’s\nmost powerful defensive sword style did he change his mind. Once the sword had\nleft Mount Li, Su Li’s  attainments on the path of the sword had become even\nmore exceptional, and his experience was broad and deep. His judgment would\nnaturally not be wrong.  But when Chen Changsheng began to properly learn this\nStupid Sword, he began to regret his decision.  Because not even Su Li had\nsuccessfully learned this sword. In all of Mount Li, even in all of the\ncontinent, there was not one person that had successfully learned this sword.\nNot even along the course of the interminable river of history could one find a\nperson that had learned this sword. To describe it another way, this sword\nstyle existed only in books, existed only in some imaginary path of the sword.\nIt had never appeared in reality.  Su Li had said that the reason he had never\nbeen able to learn this sword was that he was just too much of a genius. His\nsword was free and unburdened, unwilling to accept such constraints. But there\nwas truly a possibility that Chen Changsheng could learn this sword. This was\nbecause…in certain aspects, Chen Changsheng really was very stupid.”\n\n\n“Your biggest supporter is a stranger. Your biggest hater is someone you know.”\n\n\n“Somewhere in another universe, someone is writing the life you lived, are\nliving, have yet to live, or will never live.”\n\n\n“Each of these lives is the right one, every path is the right path, everything\ncould have been anything else, and it would have just as much meaning.”\n\n\n“If our ancestors hadn’t had this flaming urge for a feeling of importance,\ncivilization would have been impossible. Without it, we should have been just\nabout like animals”\n\n\n“I was here. I explored. I saw this. Remember me.”\n\n\n” You only get a diversity of problems solved if you have a diversity of people\nsolving them”\n\n\n“Once upon a time, I, Zhuangzi, dreamt was a butterfly, fluttering hither and\nthither, to all intents and purposes a butterfly. I was conscious only of my\nhappiness as a butterfly, unaware that I was Zhuangzi. Soon awakened and there\nwas, veritably myself again. Now do not know whether was then a man dreaming\nwas a butterfly, or whether am now a butterfly, dreaming am a man. Between a\nman anda butterfly there is necessarily a distinction. The transition is called\nthe transformation of material thing.”\n\n\n“But he did say that many people who go insane find in insanity a feeling of\nimportance that they were unable to achieve in the world of reality.”\n\n\n“Once I did bad and that I heard ever/Twice I did good, but that I heard\nnever.”\n\n\n“We are born from human boxes and live in big box houses and are buried in\nboxes when we die. Why are you crying?”\n\n\n“Thousands of salespeople are pounding the pavements today, tired, discouraged\nand underpaid. Why? Because they are always thinking only of what they want.\nThey don’t realize that neither you nor I want to buy anything. If we did, we\nwould go out and buy it. But both of us are eternally interested in solving our\nproblems. And if salespeople can show us how their services or merchandise will\nhelp us solve our problems, they won’t need to sell us. We’ll buy. And\ncustomers like to feel that they are buying—not being sold.”\n\n\n“Self-expression is the dominant necessity of human nature.”\n\n\n“First, arouse in the other person an eager want. He who can do this has the\nwhole world with him. He who cannot walks a lonely way.”\n\n\n“All of us, be we workers in a factory, clerks in an office or even a king upon\nhis throne—all of us like people who admire us.”\n\n\n“Remember that a person’s name is to that person the sweetest and most\nimportant sound in any language”\n\n\n“Many persons call a doctor when all they want is an audience.”\n\n\n“This lady, left all alone in a big house with her paisley shawls, her French\nantiques, and her memories, was starving for a little recognition. She had once\nbeen young and beautiful and sought after. She had once built a house warm with\nlove and had collected things from all over Europe to make it beautiful. Now,\nin the isolated loneliness of old age, she craved a little human warmth, a\nlittle genuine appreciation—and no one gave it to her. And when she found it,\nlike a spring in the desert, her gratitude couldn’t adequately express itself\nwith anything less than the gift of her cherished Packard.”\n\n\n“Talk to people about themselves,” said Disraeli, one of the shrewdest men who\never ruled the British Empire, and they will listen for hours.”\n\n\n“Nine times out of ten, an argument ends with each of the contestants more\nfirmly convinced than ever that he is absolutely right.”\n\n\n“A man convinced against his will Is of the same opinion still.”\n\n\n“Few people are logical. Most of us are prejudiced and biased. Most of us are\nblighted with preconceived notions, with jealousy, suspicion, fear, envy and\npride. And most citizens don’t want to change their minds about their religion\nor their haircut or communism or their favorite movie star.”\n\n\n“It’s one of my theories that when people give you advice, they’re really just\ntalking to themselves in the past.”\n\n\n“Stop being patient and start asking yourself, how do accomplish my 10 year\nplan in months? You’ll probably fail, but you’ll be a lot further along than\nthe person who simply thought it would take 10 years.”\n\n\n“The great thing about dead or remote masters is that they can’t refuse you as\nan apprentice. You can learn whatever you want from them. They left their\nlesson plans in their work.”\n\n\n“Nothing is more important than an unread library”\n\n\n“It is our failure to become our perceived ideal that ultimately defines us and\nmakes us unique\n\n\n“All fiction, in fact, is fan fiction.”\n\n\n“Take time to mess around. Get lost. Wander. You never know where it’s going to\nlead you.”\n\n\n“what unifies your work is the fact that you made it. One day, you’ll look back\nand it will all make sense.”\n\n\n“Distance and difference are the secret tonic of creativity. When we get home,\nhome is still the same. But something in our mind has been changed, and that\nchanges everything.”\n\n\n“The only mofos in my circle are people that I can learn from.”\n\n\n“Be regular and orderly in your life, so that you may be violent and original\nin your work.”\n\n\n“Work gets done in the time available.”\n\n\n“The way to get over creative block is to simply place some constraints on\nyourself.”\n\n\n“In the end, creativity isn’t just the things we choose to put in, it’s the\nthings we choose to leave out.”\n\n\n“only once you give yourself permission to stop trying to do it all, to stop\nsaying yes to everyone, can you make your highest contribution towards the\nthings that really matter.”\n\n\n“Weniger aber besser. The English translation is: Less but better.”\n\n\n“For the first time—literally—substantial and rapidly growing numbers of people\nhave choices. For the first time, they will have to manage themselves. And\nsociety is totally unprepared for it.”\n\n\n“I wish I’d had the courage to live a life true to myself, not the life others\nexpected of me.”\n\n\n“Sunk-cost bias: studies have found that we tend to value things we already own\nmore highly than they are worth and thus that we find them more difficult to\nget rid of. If you’re not quite there, ask the killer question: “If I didn’t\nalready own this, how much would I spend to buy it?” This usually does the\ntrick.”\n\n\n“Nothing is more powerful than an idea whose time has come.”\n\n\n“What if we stopped celebrating being busy as a measurement of importance?”\n\n\n“When we surrender our ability to choose, something or someone else will step\nin to choose for us.”\n\n\n“The ability to choose cannot be taken away or even given away—it can only be\nforgotten.”\n\n\n“My first act of free will shall be to believe in free will.”\n\n\n“You cannot overestimate the unimportance of practically everything.”\n\n\n“In every set of facts, something essential is hidden. And a good journalist\nknows that finding it involves exploring those pieces of information and\nfiguring out the relationships between them”\n\n\n“By getting out there and fully exploring the problem, they were able to better\nclarify the question and in turn to focus on the essential details that\nultimately allowed them to make the highest contribution to the problem.”\n\n\n“The breaking of so great a thing should make a greater crack.”\n\n\n“God was not a granter of wishes.”\n\n\n“So much had been denied me, I reasoned. Why should I deny myself?”\n\n\n“Being successful is waking in the morning and being in a good mood”\n\n\n“She didn’t tell him that while coal and diamonds are both carbon, coal is too\nimpure to be able, under whatever pressure, to become a diamond. According to\nscience, you start off as coal and you end up as coal. Maybe that was the\nreal-life lesson.”\n\n\n“Cheer up, love, it might never happen,’ someone said. Nothing ever did, she\nthought to herself. That was the whole problem.”\n\n\n“It was a familiar feeling. This feeling of being incomplete in just about\nevery sense. An unfinished jigsaw of a human. Incomplete living and incomplete\ndying.”\n\n\n“When I examine myself and my methods of thought, I come to the conclusion that\nthe gift of fantasy has meant more to me than my talent for absorbing positive\nknowledge.”\n\n\n“The world has never complained about how busy it is. As I look deeper into\nmyself to see why I am living such a busy life, I realize that, to a certain\nextent, I actually enjoy being busy.”\n\n\n“The world is experienced according to the state of one’s mind.”\n\n\n“It’s not the situation that is troubling us, but our perspective on it.”\n\n\n“Don’t struggle to heal your wounds. Just pour time into your heart and wait.”\n\n\n“Do you not feel compassion for yourself as you struggle through life? You are\nso eager to help your friends, but you treat yourself so poorly.”\n\n\n“What did you do as a child that excited you? How can you re-create that\ntoday?”\n\n\n“If you think you are so tough you can do anything I have a challenge for you.\nIf you really want to do something hard: say no to an opportunity so you can\ntake a nap.”\n\n\n“Well, while there are clearly people who can survive on fewer hours of sleep,\nI’ve found that most of them are just so used to being tired they have\nforgotten what it really feels like to be fully rested.”\n\n\n“The warrior and the artist live by the same code of necessity, which dictates\nthat the battle must be fought anew every day.”\n\n\n“The working artist will not tolerate trouble in her life because she knows\ntrouble prevents her from doing her work. The working artist banishes from her\nworld all sources of trouble.”\n\n\n“The more scared we are of a work or calling, the more sure we can be that we\nhave to do it.”\n\n\n“The opposite of love isn’t hate; it’s indifference.”\n\n\n“The part we create from can’t be touched by anything our parents did, or\nsociety did. That part is unsullied, uncorrupted; soundproof, waterproof, and\nbulletproof. In fact, the more troubles we’ve got, the better and richer that\npart becomes.”\n\n\n“In what should have been 20 minutes of work, I compulsively interrupted myself\nat least nine times. What’s more, the cost of these interruptions goes way\nbeyond the added amount of time to finish this damn thing.”\n\n\n“By last year, these interruptions had become compulsive. I didn’t know how to\nnot distract myself anymore and had to go to great lengths to prevent it from\nhappening. It felt like I was living in some kind of digital hellscape, where\nthe process of doing anything significant and important seemed not only\nfruitless but also attentionally impossible.”\n\n\n“Our bodies are designed in such a way that they need to be challenged and\nstressed to a certain degree, otherwise they become soft and weak, and the\nsmallest endeavors—walking up a flight of stairs, picking up a bag of\ngroceries—will begin to feel difficult or impossible. It turns out that these\nsmall, conscious efforts to stress our bodies are what keeps them healthy.”\n\n\n“Basically, the name of the game is quality over quantity. Because in a world\nwith infinite information and opportunity, you don’t grow by knowing or doing\nmore, you grow by the ability to correctly focus on less.”\n\n\n“Freedom in the 21st century isn’t about having more, it’s about choosing your\ncommitments to less.”\n\n\n“They say necessity is the mother of invention. Well, boredom is the father.”\n\n\n“This steady barrage of unexpected problems gives the player a sense that she\nlacks control over her own Life, when in fact, the purpose of Life is not to\ncontrol what happens to you, but rather control and choose higher level\nreactions to what happens to you.”\n\n\n“The more often you use a Solution or Distraction, the easier it will be to use\nagain, to the point where it will eventually become unconscious and automatic.\nOnce a Solution or Distraction is unconscious and automatic, it becomes a\nHabit.”\n\n\n“The number one way people fuck up is by telling themselves that there’s\nnothing they can do about the problems Life gives them.”\n\n\n“Complaining takes a problem and then prolongs it.”\n\n\n“Most recurring fantasies we have about ourselves are reactions to our\ninsecurities.”\n\n\n“Fantasies are like any other Distraction – they are to be used sparingly and\nfor nothing other than pure enjoyment. It’s when they begin to sustain your\nsense of self-worth, your desire for importance in this world, that you\nwill be hobbling yourself, and you will never Level Up again in Life.”\n\n\n“Without rationality, the universe would be a waste, in vain, and without\npurpose.”\n\n\n“Psychological research shows that people quickly adjust to their surroundings\nand are able to find joy in most situations, regardless of their culture,\nmaterial wealth or political situation.”\n\n\n“But over the years I’ve grown to see that “feeling good” in and of itself is\noften overrated.”\n\n\n“Again, not to be a stick in the mud, but those happy kids playing in sewage\npipes and shitting in buckets will be lucky to make it to middle age without\nserious violence, addiction or health problems in their lives.”\n\n\n“You realize that there’s something to be said to limiting oneself, not just\ngeographically, but also emotionally. That there’s a certain depth of\nexperience and meaning that can only be achieved when one picks a single piece\nof creation and says, “This is it. This is where I belong.”\n\n\n“Perpetual world travel literally gives you a whole world of experience. But it\nalso takes another away.”\n\n\n“The vast majority of people don’t care what you say or do the vast majority of\nthe time. And this is liberating.”\n\n\n“But what you quickly notice is that the world moves on. And what may feel like\na suicide-inducing embarrassment for you is usually but a mild novelty or smirk\nfor everybody around you. Understanding this is healthy. And it’s a lesson\nthat’s hard to learn sitting comfortably at home, and spending your\nlife shuttling between the same three or four locations every day.”\n\n\n“unsavory friends–they’re then able to see how they actually”an\n\n\n“Because once you learn that the vast majority of the planet doesn’t care who\nyou are or what you’re doing, you realize that there is no reason to not be who\nyou want to be. There is no one to please. There is no one to impress. Most of\nthe time, it’s just you, yourself and the stories you invent in your mind.”\n\n\n“Because uncertainty breeds skepticism, it breeds openness, and it breeds\nnon-judgment. Because uncertainty helps you to grow and evolve.”\n\n\n“And when you go long enough being uncertain of who you really are, what\nresults is a form of subtle, long-term meditation–a persistent and necessary\nacceptance of whatever is arising, because you don’t actually know if it was\nthe food that made you sick, and you don’t actually know if you like Eastern\nEuropean cultures anymore, and you don’t really know how you feel about income\ninequality anymore, and you don’t know if your career path is the best for you\nor not, and you don’t really know if you miss your friends back home or if you\njust like the idea of missing your friends back home. And at some point, you\njust stop asking questions. And start listening. To the waves and the wind and\nthe calls for love in all of the beautiful languages you will never understand.\nYou just let it be. And keep moving.”\n\n\n“We tend to judge the immorality of addiction by the damage it causes to\nothers. But Kant believed that, first, over-indulgence was fundamentally the\nact of being immoral to oneself. The harm it did to others was merely\ncollateral damage. It was a failure to confront the reality of one’s own mind\nand own consciousness, and this failure is akin to lying to oneself or cheating\noneself out of precious life potential.”\n\n\n“Respect was also sacred within Kant’s moral framework because Kant believed\nthat all conscious creatures have a fundamental dignity that must be respected\nat all times, and by everyone. For Kant, consent was the act of demonstrating\nrespect.”\n\n\n“Therefore, Kant argued, the only logical way to improve the world is through\nimproving ourselves. This is because the only thing we can truly experience\nwith any certainty is ourselves.”\n\n\n“He showed me that what you actually do doesn’t matter as much as the purpose\nbehind doing it. And until you find the right purpose, you haven’t found much\nof anything at all.”\n\n\n“To develop character, a person must master their own actions and master\nthemselves. And while few of us can accomplish that in a lifetime, Kant\nbelieved it’s something we each have a duty to work towards.”\n\n\n“We didnt know we were making memories. we only knew we were having fun”\n\n\n“Don’t oversaturate your life With art”\n\n\n“A wiseman speaks because he has something to say. A fool speaks because he has\nto say something.”\n\n\n“The key to creativity isnt in knowing all the solutions, its in asking the\nstrongest questions.”\n\n\n“If you wish to be wealthy, learn to live like you’re poor”\n\n\n“Beware of little expenses; a small leak will sink a great ship.”\n\n\n“No one can remember more than three points.”\n\n\n“Nothing in this world can take the place of persistence. Nothing is more\ncommon than unsuccessful men with talent. Persistence and determination alone\nare omnipotent”\n\n\n“You are absolutely not going to be gaga over each other every single day for\nthe rest of your lives, and all this happily ever after’ bullshit is just\nsetting people up for failure. They go into relationships with these\nunrealistic expectations. Then, the instant they realize they aren’t gaga’\nanymore, they think the relationship is broken and over, and they need to get\nout. No! There will be days, or weeks, or maybe even longer, when you aren’t\nall mushy-gushy in-love. You’re even going to wake up some morning and think,\n\n\n“Ugh, you’re still here….” That’s normal! And more importantly, sticking it out\nis totally worth it, because … in a day, or a week, or maybe even longer,\nyou’ll look at that person and a giant wave of love will inundate you, and\nyou’ll love them so much you think your heart can’t possibly hold it all and is\ngoing to burst. Because a love that’s alive is also constantly evolving. It\nexpands and contracts and mellows and deepens. It’s not going to be the way it\nused to be, or the way it will be, and it shouldn’t be. I think if more couples\nunderstood that, they’d be less inclined to panic and rush to break up or\ndivorce.”\n\n\n“God gave man a brain and a penis and only enough blood to operate one at a\ntime.”\n\n\n“Respect yourself and your wife. Never talk badly to or about her. If you don’t\nrespect your wife, you don’t respect yourself. You chose her—live up to that\nchoice.”\n\n\n“There can be no secrets. Secrets divide you. Always.”\n\n\n“Don’t ever give up who you are for the person you’re with. It will only\nbackfire and make you both miserable. Have the courage to be who you are, and\nmost importantly, let your partner be who they are. Those are the two people\nwho fell in love with each other in the first place.”\n\n\n“If you love your partner enough you will let them be who they are—you don’t\nown them, who they hang with, what they do or how they feel. Drives me nuts\nwhen I see women not let their husbands go out with the guys or are jealous of\nother women.”\n\n\n“One day many years from now, you will wake up and your spouse will be a\ndifferent person—make sure you fall in love with that person, too.”\n\n\n“The relationship is a living, breathing thing. Much like the body and muscles,\nit cannot get stronger without stress and challenge. You have to fight. You\nhave to hash things out. Obstacles make the marriage.”\n\n\n“But there’s no way on God’s green earth this is her fault alone. There were\ntimes when I saw huge red flags. Instead of trying to figure out what in the\nworld was wrong, I just plowed ahead. I’d buy more flowers, or candy, or do\nmore chores around the house. I was a “good” husband in every sense of the\nword. But what I wasn’t doing was paying attention to the right things… And\ninstead of saying something, I ignored all of the signals.”\n\n\n“When you end up being right about something—shut up. You can be right and be\nquiet at the same time. Your partner will already know you’re right and will\nfeel loved knowing that you didn’t wield it like a bastard sword.”\n\n\n“Everyone says that compromise is key, but that’s not how my husband and I see\nit. It’s more about seeking understanding. Compromise is bullshit, because it\nleaves both sides unsatisfied, losing little pieces of themselves in an effort\nto get along. On the other hand, refusing to compromise is just as much of a\ndisaster, because you turn your partner into a competitor (“I win, you lose”).\nThese are the wrong goals, because they’re outcome-based rather than\nprocess-based. When your goal is to find out where your partner is coming\nfrom—to truly understand on a deep level—you can’t help but be altered by the\nprocess. Conflict becomes much easier to navigate because you see … the\ncontext.”\n\n\n“You cant use possibility to justify a certainty. Base your decision on the\naction itself”\n\n\n“Who needs ethics when youve got a trust fund”\n\n\n“Ambition can be a valuable tool, but so can restraint”\n\n\n“You can’t connect the dots looking forward,keep learning and never stop”\n\n\n“Death is the destination we all share, no one has ever escaped it. And that is\nas it should be because death is very likely the single best invention of\nlife.”\n\n\n“Don´t change a thing, you´re perfect as you are, and my job is to help the\nworld to recognise the perfection that I see”\n\n\n“Never believe a prediction that doesn’t empower you”\n\n\n“The only disability is ones refusal to adapt”\n\n\n“When we feel like we’re not enough, we chase external validation”\n\n\n“We tend to test others to reaffirm our worth”\n\n\n“There are no rules, only consequences”\n\n\n“The fact of death, the scarcity of time, is a catalyst for the discovery of\nmeaning and beauty.”\n\n\n“The world breaks every one and afterward many are strong at the broken\nplaces.”\n\n\n“If humans really have no value,” Keqing retorts, “then why should we expect\nthe gods to care about us so much?”\n\n\n“Women marry men hoping to change them, and they don’t. Men marry women hoping\nthey’ll never change, and they do”\n\n\n“You don’t stop believing in Santa. You just become Santa”\n\n\n“Sometimes it’s  more economical to throw money at the problem.”\n\n\n“Buying something is never the solution to getting yourself to be more\ndisciplined.”\n\n\n“Sorry I’m late, I got stuck on the path of life”\n\n\n“We’re just suicidal people telling other suicidal people that it’s  not the\nanswer”\n\n\n“My parents aren’t heroes, they’re just like me”\n\n\n” What if we all looked the way we wanted? Our ideal weight became reality, our\nworries about money washed away. Your love life is exactly the way you pictured\nit. Do you think we’d all be happier? Or would we just find new things to\nhate?”\n\n\n“Most people are other people. Their thoughts are someone else’s  opinions,\ntheir lives a mimicry, their passions a quotation.”\n\n\n“One fine day, it wil be your turn. You will leave homes, cities and countries\nto pursue grander ambitions. You will leave friends, lovers and possibilities\nfor the chance to roam the world and make deeper connections. You will defy\nyour fear of change, hold your head high and do what you once thought\nwas unthinkable: walk away. And it will be scary. At first. But what I\nhope you’ll find in the end is that in leaving, you don’t just find\nlove, adventure, and freedom. More than anything. you find you”\n\n\n” People often overestimate what they can accomplish in a year but they\nunderestimate what they achieve in a decade.”\n\n\n” Most men live lives in quiet desperation”\n\n\n” If people try to bully me, I just ignore it. I’ve got my own friends who I\nget on with. I don’t have a social image’ or think that everyone has to like me\n– it’s not realistic.”\n\n\n“Since that day I’ve never been in the center of attention. You’re the center\nof mine”.\n\n\n“Don’t flatter yourself darling. You think everybody’s  interested in you,\nthey’re not. Your job is to make them happy.”\n\n\n“I’ve hada lot of worries in my life, most of which never happened.”\n\n\n“That’s  the thing with time, isn’t it? It’s  not all the same. Some days -\nsome years some decades - are empty. There is nothing to them. It’s  just flat\nwater. And then you come across a year, or even a day, or an afternoon. And it\nis everything. It is the whole thing.”\n\n\n“But sometimes, you forget your lines and the pain trickles through the\ncracks.” Yep today accidentally started crying in front of my mother.\n\n\n“After all, your life has not completely fallen apart yet, so why would anyone\nbelieve you’re struggling.”\n\n\n“Best wife, best employee, best daughter, best student, keep everyone happy\n…but you.”\n\n\n“This is your daily reminder to ask for what you need.”\n\n\n“External goals cannot fill internal voids”\n\n\n“Then I saw a meme [that said] ‘if you died tomorrow, your job would be posted\nfaster than your obituary”\n\n\n“I think my love with being alone stems from me knowing if l’m alone then no\none is around to disturb me or alter any of my emotions. Everything is up to\nme. no one is there to misunderstand me, judge me, etc. It’s  peaceful. A peace\ndon’t feel often with other humans”\n\n\n“Impossible is just an opinion.”\n\n\n“Fudge! I’m gonna do it! I’m gonna go all-out! I’m gonna go so all-out that\nI’ll terrify myself, let alone everyone else!”\n\n\n“After one try didn’t work, he tried ten times. After ten tries didn’t work, he\ntried a hundred times. After a hundred tries didn’t work… he tried a thousand\ntimes.”\n\n\n“He was a generally optimistic person, an attitude he had intentionally\nfostered since a young age. There had been no other option. He had personally\nwatched his parents die of illness. He remembered sitting with their corpses\nfor days, weeping, refusing to believe that they were gone, even calling their\nnames. Eventually, the corpses began to stink, and relatives came to\nbury them. Bai Xiaochun had been left in a daze, and at one point even\ntook to talking to himself…. If a child grew up in such a manner, his\nentire life would be one of darkness. So Bai Xiaochun replaced the\ncrying with laughter. He began to think about the idea of living\nforever. He would never forget how his parents had died, and although\nhe missed them, it only made him want to keep living. He was stubborn\nand mischievous, but not to an excessive degree. Many of the things he\ndid were even accidents. Deep down, he was a good person. He feared\ndeath, and seemed weak on the outside, but when his friends were in\ndanger, he would fight to the death to protect them. If he needed to,\nhe could bellow in rage and risk his life on the field of battle.”\n\n\n“No. My dreams haven’t changed. What Uncle Li said was right. The only things\nwhich truly belong to you are the things you get on your own. Even if it gets\nharder, I still won’t give up!”\n\n\n“The best thing was not to fight the trend; blend in, and use the rules to your\nadvantage!”\n\n\n“Possession without possession is the most wonderful possession. Emptiness\nwithout emptiness is the true emptiness….”\n\n\n“To live forever,” he murmured, “you don’t just have to struggle against the\nheavens, you have to fight other people. It’s a narrow, rugged path to walk, a\npath that most people give up on. Many people meet defeat, or end up losing\ntheir way….”\n\n\n“Remember, the easier something is to get, the less precious it will be in the\nend.”\n\n\n“Smiling broadly, he thought back to the old saying: even if you have a\nbeautiful sedan chair to sit in, you still need others to lift you up.”\n\n\n“Now that’s how grandmasters are supposed to act! The old expression says to\nturn weapons of war into gifts of jade and silk. It’s  not easy to do, but he\npulled it off perfectly. Really incredible!”\n\n\n“Xiaochun…” he said softly. “You’re right. As long as we’re alive, there are\nendless possibilities. But just because we die doesn’t mean that our hopes and\ndreams die with us!”\n\n\n“Bai Xiaochun had always thought of himself as somewhat of a nobody, and\ndefinitely not a hero. But as he proceeded through life, he had slowly come to\nacquire a sense of duty and responsibility.”\n\n\n“There is a force which exists in all people. In some, it can be abundant\nwithout limit. In others, it can be scarce to the extreme. It can propel some\npeople to the point of shaking heaven and earth, and in other people, it can\nweaken them to the point of collapsing forever. It can give rise to endless\ndetermination, or it can remove it in an instant.”\n\n\n“Wisdom and intelligence… begin with imagination! “Imagination can turn a rock\ninto a weapon! “Imagination can turn fire into a way to drive away darkness!\n\n\n“Imagination can make gods to worship, and form them into totems…. “When you\ntake imagination and combine it with action, you have the first sprout of\nwisdom and intelligence!”\n\n\n“One Will to create oceans. One Will to summon mulberry fields. One Will to\nslaughter countless devils. One Will to eradicate innumerable immortals…. Only\nmy Will… is Eternal.”\n\n\n“Innovation come from doings things differently, not doing things bigger.”\n\n\n“In an age of network tools, in other words, knowledge workers increasingly\nreplace deep work with the shallow alternative—constantly sending and receiving\ne-mail messages like human network routers, with frequent breaks for quick hits\nof distraction.”\n\n\n“Let your mind become a lens, thanks to the converging rays of attention; let\nyour soul be all intent on whatever it is that is established in your mind as a\ndominant, wholly absorbing idea.”\n\n\n“Men of genius themselves were great only by bringing all their power to bear\non the point on which they had decided to show their full measure.”\n\n\n“In a post-Enlightenment world we have tasked ourselves to identify what’s\nmeaningful and what’s not, an exercise that can seem arbitrary and induce a\ncreeping nihilism”\n\n\n“The task of a craftsman, they conclude, “is not to generate meaning, but\nrather to cultivate in himself the skill of discerning the meanings that are\nalready there.”\n\n\n“We who cut mere stones must always be envisioning cathedrals.”\n\n\n“You don’t need a rarified job; you need instead a rarified approach to your\nwork”\n\n\n“a moment can feel strangely flat if it exists solely in itself.”\n\n\n“We’re social beings who can’t ever completely ignore what other people think\nof us”\n\n\n“marriage or sexual possession was (and still is) largely anathema”\n\n\n“That it’s the ability to choose: what pleasure is worthwhile, what pain is\nworthwhile, to pursue and love unconditionally, without judgment or shame.”\n\n\n“Seeing self-discipline in terms of pure willpower fails because beating\nourselves up for not trying hard enough doesn’t work. In fact, it backfires.”\n\n\n“The classic approach has the paradoxical effect of training us to feel bad\nabout all the things that make us feel good. It basically seeks to teach us\nself-discipline through shaming us—by making us hate ourselves for simply being\nwho we are. And the idea is that once we are saddled with a sufficient amount\nof shame about all the things that give us pleasure, we’ll be so self-loathing\nand terrified of our own desires that we’ll just fall in line and do what we’re\ntold.”\n\n\n“It was a simple principle: just as one can only realize the strength of wine\nafter one gets drunk, one can only know the value of life after one has died.”\n\n\n“And don’t use zhenqi to control your emotions. If human emotions aren’t given\nthe proper outlet, even if your powers of zhenqi control are at their peak,\nyou’ll become a murderous monster.”\n\n\n“Why do you wish to see this world?” Wu Zhu seemed to be pondering something,\n\n\n“the place you are standing right now, isn’t it part of this world?”\n\n\n“Since you only live once, the only way to make the most out of this\nunrepeatable game is to go around seeing different sights and meeting different\npeople.”\n\n\n“Flowers fall once they appear, stones stand still a thousand years. But both\nmust go just as they came, and floating clouds are just the same…”\n\n\n“A bright moon on a wide river, a clear breeze among the hills.”\n\n\n“While this world may appear peaceful, if you don’t steel yourself, you will\nalways be at a disadvantage.”\n\n\n“he had always been somewhat chauvinistic, feeling that when it came to matters\nbetween men and women, it was always women who got the worst of it, and men who\ntook advantage.”\n\n\n“Man’s  dearest possession is life. It is given to him but once, and he must\nlive it so as to feel no torturing regrets for wasted years, never know the\nburning shame of a mean and petty past; to live that, dying, he might say: I\ndid everything I wished to, and even if I was unsuccessful, at least I tried’.”\n\n\n“You once said that people have to be brave enough to seek happiness.”\n\n\n“one can’t be sincere without simple living, one can’t have high aspirations\nwithout a peaceful state of mind”\n\n\n“There were too many matters in this world that men’s  stupid brains had made\ncomplicated.”\n\n\n“Su Ming vaguely remembered one of the scrolls saying that once the prey went\nthrough a long period of time switching between the state of anxiety and\nrelaxation, its fatigue and suffering would increase exponentially. That sort\nof torture was enough to destroy one’s soul.”\n\n\n“But we are members of the Berserker tribe. We cannot fear death, much less\ngive up because the road ahead is too hard to walk.” “I know what your dreams\nare. You want to leave this place and travel to see the world. I fully support\nyou!”\n\n\n“Su Ming, you must remember. There will always be people who are stronger and\nmore powerful than you. You must never be arrogant…”\n\n\n“He was certain of his words just like any young man who still believed in a\nbright future…”\n\n\n“Either he would stay away from something, or he would finish what he had\nstarted.”\n\n\n“When you are fighting against someone, do not let your focus waver. Do not\nhesitate. If it is possible to kill your enemy with one strike within the\nshortest amount of time, do not wait till the last moment to do so.”\n\n\n“The oppression of the heavens is invisible. We can only endure it and while we\nendure it, we must learn to live with it happily… If we do not, are we to fight\nagainst heaven?’”\n\n\n“Qi will only move when the mind moves. If the mind does not move, then Qi will\nalso remain still!”\n\n\n“To him, there was no one who could be his opponent. The one that he wanted to\ncompete against was himself!”\n\n\n“A person would experience great things and shortcomings in his life. He would\nalso experience days of glory and days where he would stumble and fall. These\nwere all things that Su Ming did not understand. The only thing he understood\nwas that he had to do this.”\n\n\n“La Su, you are not alone in the sky. Do not be sad. Do not cry. Mama and papa\nwill look at you from where we are… Every year, every day… we will look at\nyou…” “I will not cry. I will not be sad. I will not be lonely. I know that you\nare there, watching me… I am happy…”\n\n\n“It seemed like there was no sign of life within those ruins, and they would\neventually turn into a remnant of the passage of time. Perhaps the few\nremaining trees and plants would continue growing there and slowly turn the\nplace into a part of the forest, making it difficult for people to come looking\nfor their memories and the beautiful moments that had happened during their\ntime here.”\n\n\n“Chances are hidden within danger…”\n\n\n“Instead, it would be better to emphasize the benefits of the concoction\ntowards himself and subtly reveal some of his own thoughts that would make the\nother party wonder.”\n\n\n“Whatever you brag about the most is what you lack the most. “Whatever it is\nthat you want others to know that you own the most of is what you want to\npossess the most.”\n\n\n“The more he wanted to obtain something, the more he would have to sacrifice.\nOnly he could determine whether the sacrifice was equivalent to the reward and\nwhether it was worth it, not anyone else.”\n\n\n“The mind is restless, Krishna, impetuous, self-willed, hard to train: to\nmaster the mind seems as difficult as to master the mighty winds.”\n\n\n“Our job is not to “go with our gut” or fixate on the first impression we form\nabout an issue. No, we need to be strong enough to resist thinking that is too\nneat, too plausible, and therefore almost always wrong. Because if the leader\ncan’t take the time to develop a clear sense of the bigger picture, who will?\nIf the leader isn’t thinking through all the way to the end, who is?”\n\n\n“Keep strong, if possible. In any case, keep cool. Have unlimited patience.\nNever corner an opponent, and always assist him to save face. Put yourself in\nhis shoes—so as to see things through his eyes. Avoid self-righteousness like\nthe devil—nothing is so self-blinding.”\n\n\n“Ask yourself at every moment, Is this necessary?’”\n\n\n“Before we can make deep changes in our lives, we have to look into our diet,\nour way of consuming. We have to live in such a way that we stop consuming the\nthings that poison us and intoxicate us. Then we will have the strength to\nallow the best in us to arise, and we will no longer be victims of anger, of\nfrustration.”\n\n\n“To become empty is to become one with the divine—this is the Way.”\n\n\n“Childlikeness’ has to be restored with long years of training in the art of\nself-forgetfulness. When this is attained, man thinks yet he does not think.”\n\n\n“Chop wood, carry water. Chop wood, carry water. Chop wood, carry water. Don’t\noveranalyze. Do the work.”\n\n\n“Tao is in the emptiness. Emptiness is the fast of the mind.”\n\n\n“What’s essential is invisible to the eye.”\n\n\n“Appearances are misleading. First impressions are too. We are disturbed and\ndeceived by what’s on the surface, by what others see. Then we make bad\ndecisions, miss opportunities, or feel scared or upset. Particularly when we\ndon’t slow down and take the time to really look.”\n\n\n“If we take something to be the truth, we may cling to it so much that even if\nthe truth comes and knocks at our door, we won’t want to let it in. We have to\nbe able to transcend our previous knowledge the way we climb up a ladder. If we\nare on the fifth rung and think that we are very high, there is no hope for us\nto step up to the sixth. We must learn to transcend our own views.\nUnderstanding, like water, can flow, can penetrate. Views, knowledge, and even\nw is d o m are solid, and can block the way of understanding.”\n\n\n“When a measure becomes a target, it ceases to be a good measure.”\n\n\n“Overemphasizing metrics removes our focus from long-term concerns such as our\nvalues, trust and reputation, and our impact on society and the environment,\nand myopically focuses on the short-term.”\n\n\n“To be clear, life is not a race. You can switch into tech and learn new skills\nat any age. The tech industry is deeply ageist, and the glorification of young\nfounders is a harmful myth.”\n\n\n“When you keep quiet in order to keep the peace you start a war within\nyourself.”\n\n\n” The more you put something off the bigger it feels”\n\n\n“Nothing ever ends poetically. It ends and we turn it into poetry. All that\nblood was never once beautiful. It was just red.”\n\n\n” You never touch someone so lightly that it does not leave a Trace”\n\n\n“What is truth but a survivors story”\n\n\n“Lonliness if often the byproduct of a gifted mind”\n\n\n“We cant change what fate has in store for us, but we dont have to face it\nalone”\n\n\n“I’ve been up to my neck in work lately, and before I knew it, it was my\nbirthday again. It was only then that I also realized that it’s  been a long\ntime since we last met up.  How have things been lately? You’re always so busy\nwith your travels, never stopping, not even for a moment. I suppose we’re alike\nlike that-always seizing the day.  It’s  good to live to the fullest. Time\nwaits for no one, but the fruits of our labor, the growth we experience…\nThese are worthy harvests that will see us through in the days ahead. But of\ncourse, rest is also paramount. I have enclosed some flowers and plants that I\nusually make tea with. They dispel heatiness and help to relieve fatigue.  Take\ncare of yourself out there. We’ll catch up when you’re back in Liyue.”\n\n\n” As kids, we’re scared of bugs. Running out of candies. Crossing the street.\nGhosts and ghouls under the bed. Being left alone in the dark, glowing wall\nstars falling apart. Being laughed at for not knowing how to tell time. Time,\nthat passes so quickly, as our castles become dungeons. As adults, our fears\nchange with us. Now people terrify us. The ghosts are on our bed and they look\nso breathtakingly beautiful we want to kiss them because we miss them. Running\nout of money worries us. Our biological clock worries us. Driving across the\nstreet in traffic worries us. The idea of our parents growing older keeps us up\nat night. Rent makes us anxious, but settling down horrifies us. Bad flatmates\nfrighten us. Bad lovers haunt us. We’re afraid of love, afraid of not finding\nlove. Afraid of being with someone, afraid of ending up alone. Afraid of\ndarkness, but now a different kind. So what if the bugs crawled up our legs? Or\nit got too spooky to look under the bed when the lights went out? We still went\nto sleep hugging teddy bears and replaying lullabies in our heads,  didn’t we?\nStill went to school the next day, crossed the street, got candies again at the\nsupermarket. Fought snow dragons, planted lilies of the valley. Made friends\nwith fairies and baked flying cupcakes. Built homes in the planet of grief and\nthe island in the clouds. We got by. We scraped along. Things won’t run out.\nThe opportunities will keep coming. New lovers will arrive, the next roommate\nwill be kinder. And we’ll survive all the ghouls, we’ll show up at work, we’ll\nput in the work - so rest your little heart because even the tiniest versions\nof us sailed past the ring of storms and made it through. We did it as\nchildren, and will as grown-ups too.”\n\n\n“What matters most is how well you walk through the fire.”\n\n\n“When did you get so comfortable living in someone else’s shadow?”\n\n\n“When people look up to you, you don’t get to be selfish.”\n\n\n“Imprisonment. What a curious principle. We confine the physical body, yet the\nmind is still free.”\n\n\n“Surely, we, the pioneers of science, can use it for good. We’re the champions\nof discovery. Why fear it when we can master it?”\n\n\n“There’s a monster inside all of us.”\n\n\n“If dangerous ideas didn’t excite the imagination, we would never wander\nastray.”\n\n\n“When you’re going to change the world, don’t ask for permission.”\n\n\n“Nobody’s ever believed in me. A poor cripple from the undercity. I was an\noutsider the moment I stepped foot in Piltover. I didn’t have the benefits of a\npatron or a name. I simply believed in myself.”\n\n\n“You know, Powder, what makes you different makes you strong. Always remember\nthat, okay?”\n\n\n“Loneliness is often the byproduct of a gifted mind.”\n\n\n“We can’t change what fate has in store for us, but we don’t have to face it\nalone.”\n\n\n“He fancies himself a fox among the wolves. But mark me, child, if you want to\nlast in this world, you must learn to be both the fox and the wolf.”\n\n\n“I pretended to chase my own monsters away. I’d say… No monster’s gonna get you\nwhen I’m here.’ Then a real monster showed up. And I just ran away.”\n\n\n“It’s not enough to give people what they need to surive, you have to give them\nwhat they need to live.”\n\n\n“Imperfection is the digital perfection”\n\n\n“Before the advent of machine-based agriculture, representative democracy,\ncivil rights, antibiotics and aspirin, just making it through a long life\nwithout too much suffering counted as doing pretty well. Today, though, at\nleast in prosperous societies, people want and expect (and can usually have) a\ngood deal more. Living simply now strikes many people as simply boring.”\n\n\n“One should treasure those humdrum tasks that keep the body occupied but leave\nthe mind and the heart unfettered”\n\n\n“Then, let us be considering knowledge like a river of water. If you are a\npiece of cloth how are you finding out more about this water if someone dips in\nyour corner and then pulls it out again or if you are having yourself thrown in\nwithout resistance so that this water is flowing all through you around you and\nyou are becoming soaking wet?”\n\n\n” You see the land is a book that you should be reading. Every small thing is\nhaving a story to tell. Trees leaves Moses and stones all have written on them\nthings of wonderful interest”\n\n\n” An exceedingly common mistake in life is axiomatic in surfing, obvious to the\npoint of not even needing to be said: You are nature’s bitch. If she wants to\ngive you opportunities, she will. If not, then tough cookies. No matter how\ngreat or amazing or sublime you are at what you do, if opportunities don’t\ncome, or if they peter out under you, then you’re left alone to sink. And when\nthe opportunities do come (and they eventually do), if you aren’t primed and\nready to take advantage, if you don’t spring into action immediately and go for\nit, you’ll be left on the sidelines. No guts, no glory”\n\n\n” 90% of your plans are going to fail no matter what you do. Get used to it.”\n\n\n” Terrify yourself. Use it as your ally. Give yourself no option but your\ndream.”\n\n\n” A devaluing of superficial pleasures and a greater appreciation for simple,\nauthentic ones.  I don’t really enjoy the presents at Christmas anymore, the\nfireworks at Fourth of July, or even the parties on New Year’s Eve. I’ve seen\nbigger parties, been to more beautiful places, and already own everything I’ll\never want in this life.  But unlike before, I appreciate every day spent with\nthose who mean a lot to me. A quiet beer on a patio. Watching a basketball game\ntogether. Going to a birthday party or a barbecue. These are the events I look\nforward to now and get excited about, days and weeks ahead of time… And that’s\nprobably the way it should be.”\n\n\n” I made no connection between their misfortune and my own career prospects.\nHow could I? I was blinded by my trust in the American promise: if I got the\nright kind of job, then success and happiness would surely follow.  This\npromise, however, is mostly false. It’s what the philosopher Plato called a\n\n\n“noble lie”, a myth that justifies the fundamental arrangement of society.\nPlato taught that if people didn’t believe the lie, then society would fall\ninto chaos. And one particular noble lie gets us to believe in the value of\nhard work. We labor for our bosses’ profit, but convince ourselves we’re\nattaining the highest good. We hope the job will deliver on its promise, and\nhope gets us to put in the extra hours, take on the extra project and live with\nthe lack of a raise or the recognition we need.”\n\n\n“You have to start romanticizing your life, you have to start thinking of\nyourself as the main character, cause if you don’t, life will continue to pass\nyou by, and all the little things that make it so beautiful will continue to go\nunnoticed, so take a second and look around.”\n\n\n“But I was smart so nobody was worried”\n\n\n“I worked harder than anyone I knew..So, my failure was clearly my fault.”\n\n\n” We’re all aware that the main benefit of sleep is to refresh your brain,\nstore memories, and all that jazz.  I also like to think of sleeping as a\nheadspace reset.  When you go to sleep you’re resetting your reality back to\nzero, but with the neurological changes of the past day intact. So new\ninformation you learned (neuronal pathways) is retained, memories are retained,\nhabits are still ingrained, and your basic personality “firmware” is still\nthere as it was before.  What is lost is the zeitgeist of your consciousness\nfrom the previous day. That amorphous spark behind your eyes; the ever-changing\nyou. This what tints the events of the day, your experiences, and other\nsemi-random thoughts or reactions will affect you.  For instance, you might\nwake up to the news of your local grocery store burning down. People were hurt;\nlife is fragile. You might go through the day under that theme: life is\nfragile. Your reactions to things are different. You might feel more\nempathetic. When someone bumps into you, you might let it go. When someone is\nmean to you, you wonder if maybe they lost someone in the fire.  And when old\ndusty memories manifest you’ll view them in an altered light. You might think\nof an old significant other, or when you got mad at your little brother. After\nthe day is done you have time for introspection. You touch upon your old\nmemories again looking for other ones that may have changed, like running your\nhands through your hair after a haircut; feeling for change.  By now it is too\nlate to act upon your feelings. Tomorrow you’ll call Steve and apologize for\ninsulting him in front of his friends 3 years back. You’ll stop by where Susan\nworks and see how she’s  doing. People can leave you at a moments notice and\nthere is so much negativity in the world.  You wake up to a new day. All the\ninformation you acquired yesterday remains. You have memories of how you felt\nyesterday and you still feel like you. But now calling your brother just seems\nawkward and would Susan even remember you? Your spark reset. It booted up from\nthe firmware, downloaded those updates from your neurons, and opened up to a\nfresh, blank desktop page ready for the new day.  We like to think of ourselves\nas stable, concrete entities. The reality may not be that simple, since it is\nobvious that we’re constantly changing and growing. We know we’re us by the\nmemory of being in this body and remembering the changes that have occurred to\nit over time, but we don’t often consider how much our mind changes with time.\nHow much do you have in common with yourself from 10 years ago? 5 years? 4\nweeks?  I think that when you sleep the you of today dies. The sum of your\nexperiences are passed on, but that version of you from September 8th is gone\nforever. Because you have the same memories, the same neural pathways (habits,\nmemories, personality traits) you are sure you’re you - every piece of data you\nhave declares it to be true.  That iteration of you, that version of you\nreading this, is gone tomorrow. The new iteration of you will remember these\nwords and it won’t know any better. The new spark won’t know the difference.\nBut we can see what was lost; that flavor, that theme of the previous day. The\nmotivation, the sadness, the anger, the introspection won’t make it through\nunless it is triggered again by a memory or experience that did make the\ntransition (for better or worse).  I think this is why you get those bursts of\nlife-changing motivation that are usually gone the next day, like sand through\nyour fingers.  How do you make it stick? You need to bring triggers, links, or\nanchors through to tomorrow. You need to find what caused the feeling of\nmotivation before and bring it back, otherwise it won’t come back (and\nsometimes it never will). Sometimes it’s  subtle too.  And in regards to\nmotivation bursts… the other factor is that it is so easy to make plans the\nnight before because you don’t have to do them now. It makes you feel good to\nimagine it, but nothing actually happened yet because you  didn’t actually have\nto do anything. It’s  a masturbatory exercise. Why does it keep happening?\nBecause it feels good to imagine improving yourself, of imagining the pride\nwhen you look in the mirror or ace the test. Anything that causes a good\nfeeling with zero work can become addictive or keep entering your mind.”\n\n\n“What we work on is easy for us not for them, best to keep it that way and the\nprocess you have for the work should always be a mystery”\n\n\n“Life is too precious to wait for someone else to join your adventure. Just\ngo.”\n\n\n“This taught me a principle, which is the more something is a scam, the more\nattractive the bait is. What do they mean, they have methods to sell for a\nbetter price so they can buy at a higher price’ - I think that it’s all\nnonsense. Their goals are most likely impure.”\n\n\n“I’m trying to entertain people and contribute a verse to the world. but the\nworld is composed of millions of voices, all screaming at the same time about\nliterally everything. saying ultimately nothing. that song doesnt need anymore\nverses”\n\n\n“Love was like an wild horse. It didn’t listen to reason and went where it\npleased. It was not constrained by ethics or by benefits to those involved. On\nthat day, Su Chen learned an extremely important lesson. Love and rationality\nweren’t compatible. Some people would even claim that those who hadn’t suffered\nromantic loss were not well-rounded. Su Chen, having experienced his first\nromantic loss, had also become more shaped and well-rounded.”\n\n\n“Unfortunately, it’s just too hard. It’s a dream, impossible to realize,” Wang\nDoushan said, shaking his head. Perhaps. But if it’s impossible, why does that\nold man continue to do so? Why are there so many people dedicating themselves\nfor this dream? “This……” Wang Doushan couldn’t find the words. Su Chen answered\nhis own question. “Because there are always people in this world who\nwill continue to strive even if they know it’s impossible. For the\nhuman race, these people are unafraid of hardships, and they dedicate\nthemselves without any regrets.”\n\n\n“Humans were usually like this. As long as there was some form of relationship,\nthey would feel that using someone was expected and even justified. If they\nwere still charged for it, naturally that person was greedy and lacked\ncamaraderie.”\n\n\n“Mountains don’t know the months, and the cold doesn’t know the years. Time\nalways flies when a person is undisturbed. Very quickly, half a month passed.”\n\n\n“When the balance of strength is unequal, strength is righteousness. When the\nbalance of strength is equal, righteousness is strength.”\n\n\n“Pursuing one’s dreams is an incredibly dangerous process. You never know what\nyou will run into along the way. As such, at the very least I must have faith\nthat this is not an impossible dream. Only with this conviction will I have the\ncourage to continue onwards.”\n\n\n“In his attempting to understand the hearts of others, he was also learning\nmore about the world that he lived in. In the past, he had relied a lot on\nlittle tricks to get his way, but as he began to grow and mature, all the while\naccumulating experience, he began to see the bigger picture.”\n\n\n“The more urgent the issue, the calmer you need to be and the more you need to\navoid making mistakes,”\n\n\n“Laymen follow others’ movements, while wise men control the situation. It’s\nstill the same hypothetical situation; if I had heard my opponent talk to me\nthat much, I would have realized that my opponent was waiting for me to do\nsomething.”\n\n\n“Respect must be given to the will of every creature. Each fish in the ocean\nswims in its own direction.”\n\n\n“lIf enough people believe they’re real, they become real. Because they are\naddicted”\n\n\n“I am one who would rather suffer an eternity of destined calamities than beg\nfor solace from the saints…”\n\n\n“Ants may be able to fly, but they will fall eventually. They shall never touch\nthe sky,” the lad carrying the wooden sword exclaimed as he shook his head. If\nyou hold this belief, then you will never be able to understand the true\nmeaning of the Taoist Heart,” said the young monk as he slowly blinked his\neyes, still looking down at the warring ant colonies, The lad with the wooden\nsword raised an eyebrow and replied with a sneer, “I will never understand how\nsomeone constrained like you is qualified to represent Xuankong Temple as its\nwayfarer in the world. “The ants will fly, just like they will fall. However,\nthey are better at climbing, and they are good at letting their fellow ants\nclimb upon them. They are not afraid of sacrifice and as they pile upon one\nanother, as long as there are enough of them, they shall eventually pile up\nhigh enough to touch the sky”\n\n\n“Eagles should not fear ants since they are simply black dots to the former.\nAnts should not fear eagles either because they are not even worth a bite to\nthe eagle. The world of the ants had never seen or heard of a creature as\npowerful as the eagle, hence the latter remained unfathomable to the former.\nNevertheless, over the span of many centuries and millennia, a few very\ndistinguished ants among the crowd would, out of enigmatic reasons, decide to\nstrip their gaze from the rotten leaves and just for once, gaze up at the\ncrystal blue sky…and then, the world was never the same to them. The fear comes\nfrom seeing.”\n\n\n“A veil was lifted off in front of my eyes when I understood that the modern\nsociety has replaced religious devotion with devotion to the material and this\nmeans that people identify with their job now that they can’t identify with a\ncommon religion or background.”\n\n\n“The ancient greeks would probably think of you as a slave if you talked about\nwork so much. Free men pursued the arts. Anyone at my job who only talks about\nwork, I put them in the same box.”\n\n\n“Intentionally or not, when you say words like “how pitiful,” you’re alienating\nyourself from the other person and their experience. It dismisses and\ntrivializes their pain.”\n\n\n“The world’s first Mora is probably just an ordinary coin created by Rex Lapis.\nAs for its fate? The same as all Mora, I suspect - it was simply spent\nsomewhere.”\n\n\n“For how could a god who had never once resisted - even till the end-nurse\nhatred for her people in her heart?”\n\n\n“Instead of explaining yourself to me, you should face your true self.”\n\n\n“The body and the mind are one. If something worries your mind, your body can\nhelp you find a solution.”\n\n\n“Besides, they’re just kids. They should be allowed to believe it if it makes\nthem happy. That’s more important to them than questioning what’s real and what\nisn’t.”\n\n\n“Yoimiya: No matter where your journey takes you, and no matter what hardships\nmight lie ahead, I hope you’ll always be able to look back fondly on the\nfireworks you saw tonight. I believe that as time goes by, this firework will\nonly grow brighter and more beautiful in your heart.”\n\n\n“Yoimiya: Fireworks that disappear in a flash of light are probably the\nfurthest thing away from the eternity that our Shogun desires. Yoimiya: But\npeople’s feelings don’t just disappear, and it’s those feelings that give\nfireworks their purpose. If nobody wanted to watch fireworks, then they\nwouldn’t exist. Eragon: That’s another kind of eternity. Yoimiya: Also,\nconsider You have to the Naganoharas, because so many people are emotionally\ninvested in the existence of fireworks. Yoimiya: If we didn’t exist, their\nwishes would go unfulfilled, wouldn’t they?”\n\n\n“Ei: I am me. There is only one of me, but I can exist in many different forms.\nIt’s not important what form I exist in. Ei: The Shogun, for example, is one of\nmy forms of existence. The question of whether or not she is me is not\ndetermined by any of her components. Paimon: In that case, this picture is one\nof your forms of existence, too. Ei: Hmm. So even I, who seeks Eternity, am\nconstantly changing my form of existence… Ei: Then, how can I ask Inazuma and\neveryone who lives here to remain unchanging?”\n\n\n“To put it nicely, even deities indulge in wishful thinking. To put it more\nbluntly, there are things that even The Seven can’t do when faced with\nsomething even more powerful than themselves.”\n\n\n“Because she is a god. It’s not that gods don’t need the company of others,\njust that the idea of a god having company seems indulgent to her.”\n\n\n“Sometimes like uh you get so creative that you don’t know how to construct\ncreatively the words in order to express your thought you know you just know\nhow to create it you don’t know how to verbalize your creative ideas like kanye\nwest for instance his interviews people didn’t know what the hell he was\ntalking about when he was talking about his uh his clothing line right and then\nhe created a billion dollar clothing line and i was like okay well maybe we\ncan’t we do we should listen to kanye west maybe it’s like a bit of a genie for\nmotivation-wise kanye is actually a really good example because this is\nactually a really big deal to me i feel like i have really good ideas and i can\nsee what’s happening in my head and then i go to a professional or i go to a\nloved one this is this is where it really hurts me is when i go to somebody\nthat i love and i say i got this crazy idea let me tell you about and then i\ntell them what’s happening scene for scene and then after that they go yeah\nit’s pretty cool and i’m like you don’t understand like what what i just said\nlike that could be a full-on feature film or a tv show and they’re like yeah\nit’s cool it’s cool and i’m like oh that hurts it’s cool like that hurts me and\nthen i’m thinking is it that wasn’t a good idea that wasn’t a good idea and\nthen i’m like i don’t do it anymore you gotta know the right people to share\nwith you you you really do you’re a little careful you you really your energy i\nthink that’s the biggest point that i can save for motivation for me is because\nsome people are low in their life and they don’t want anybody to be above them\nthey want you to suffer with them and sit where they are in life and that’s not\neverybody but there are people that want you know they’re doing it they don’t\nknow they’re doing it yeah they’d still love you too but they don’t want you to\nbe better than them and subconsciously some people will bring you down that way\nand some people are just like yeah it’s cool because they can’t see what’s in\nyour head but for me i can see what’s in my head this is this is what happened\npersonally with me and one of my students they were trying to explain to me\nwhat they were their story was about i’m like yeah that’s cool and i don’t know\nif i didn’t motivate them or not but and i just couldn’t really imagine what\nthey were talking about it was just words all right they’re just going\nblah blah blah blah blah and i’m like yeah yeah yeah keep on talking\nand then they showed me what they were working on a week later and i’m\nlike this is incredible i didn’t know that this is what you were\ntalking about these were your words last week this is like this hit me\nin my heart and i wish that you could have explained it that way but\nyou can’t explain it you have to show it through your art and put music\nin it don’t lose motivation whenever somebody says yeah that’s a cool\nidea or whatever because they don’t know what’s in your flipping head\nthat’s the biggest thing that i would say i would walk away from the\nmain thing to walk away from is that everybody has their own type of\nmotivation and we’ve all shared our types of motivation and if you\ndon’t like it you can probably just shut up”\n\n\n“Beware of destination addiction… until you give up the idea that happiness\nis somewhere else, it will never be where you are.”\n\n\n“The thought was this: that all my life had been murk and depths, but I was not\na part of that dark water. I was a creature within it.”\n\n\n“No wonder I have been so slow, I thought. All this while, I have been a weaver\nwithout wool, a ship without the sea. Yet now look where I sail.”\n\n\n“Gods hate all toil, it is their nature.”\n\n\n“Each spell was a mountain to be climbed anew. All I could carry with me from\nlast time was the knowledge that it could be done.”\n\n\n“Zeus so angry.” “Tell me,” he said, “who gives better offerings, a miserable\nman or a happy one?” “A happy one, of course.” “Wrong,” he said. “A happy man\nis too occupied with his life. He thinks he is beholden to no one. But make him\nshiver, kill his wife, cripple his child, then you will hear from him. He will\nstarve his family for a month to buy you a pure-white yearling calf. If he can\nafford it, he will buy you a hundred.”\n\n\n“You can teach a viper to eat from your hands, but you cannot take away how\nmuch it likes to bite.”\n\n\n“I watched her dance, arms curving like wings, her strong young legs in love\nwith their own motion. This was how mortals found fame, I thought. Through\npractice and diligence, tending their skills like gardens until they glowed\nbeneath the sun. But gods are born of ichor and nectar, their excellences\nalready bursting from their fingertips. So they find their fame by proving what\nthey can mar: destroying cities, starting wars, breeding plagues and monsters.\nAll that smoke and savor rising so delicately from our altars. It leaves only\nash behind.”\n\n\n“But in a solitary life, there are rare moments when another soul dips near\nyours, as stars once a year brush the earth. Such a constellation was he to\nme.”\n\n\n“A golden cage is still a cage.”\n\n\n“Every moment mortals died, by shipwreck and sword, by wild beasts and wild\nmen, by illness, neglect, and age. It was their fate, as Prometheus had told\nme, the story that they all shared. No matter how vivid they were in life, no\nmatter how brilliant, no matter the wonders they made, they came to dust and\nsmoke.”\n\n\n“Even the best iron grows brittle with too much beating.”\n\n\n“The fear of missing out goes into full effect. How can we say no; the offer is\nright here for the taking. We might never have gone after it, but now it is so\neasy to get it we consider it. But if we just say yes because it is an easy\nreward, we run the risk of having to later say no to a more meaningful one.”\n\n\n“This feeling is normal; studies have found that we tend to value things we\nalready own more highly than they are worth, and thus find them more difficult\nto get rid of.”\n\n\n“Resistance is not a peripheral opponent. Resistance arises from within. It is\nself-generated and self-perpetuated. Resistance is the enemy within.”\n\n\n“It is one thing to study war and another to live the warrior’s life.”\n\n\n“My friend Tony Keppelman snapped me out of it by asking if I was gonna quit.\nHell, no! “Then be happy. You’re where you wanted to be, aren’t you? So you’re\ntaking a few blows. That’s the price for being in the arena and not on the\nsidelines. Stop complaining and be grateful.” That was when I realized I had\nbecome a pro. I had not yet had a success. But I had had a real failure.”\n\n\n“The professional, though he accepts money, does his work out of love. He has\nto love it. Otherwise he wouldn’t devote his life to it of his own free will.”\n\n\n“The professional has learned, however, that too much love can be a bad thing.\nToo much love can make him choke. The seeming detachment of the professional,\nthe cold-blooded character to his demeanor, is a compensating device to keep\nhim from loving the game so much that he freezes in action.”\n\n\n“The sign of the amateur is overglorification of and preoccupation with the\nmystery.”\n\n\n“The professional shuts up. She doesn’t talk about it. She does her work.”\n\n\n“The professional dedicates himself to mastering technique not because he\nbelieves technique is a substitute for inspiration but because he wants to be\nin possession of the full arsenal of skills when inspiration does come. The\nprofessional is sly. He knows that by toiling beside the front door of\ntechnique, he leaves room for genius to enter by the back.”\n\n\n“Boys,” said Kulgan, shaking his head. “You hold a festival, give them a badge\nof craft, and suddenly they expect to be men. But they’re still boys, and no\nmatter how hard they try, they still act like boys, not men.”\n\n\n“We live a very long time by your standards. We learn to appreciate the humor\nin the world, often finding amusement in places where men find little. Or you\ncan call it simply a different way of looking at life.”\n\n\n“As human beings we belong to an extremely resilient species. Since time\nimmemorial we have rebounded from our relentless wars, countless disasters\n(both natural and man-made), and the violence and betrayal in our own lives.\nBut traumatic experiences do leave traces, whether on a large scale (on our\nhistories and cultures) or close to home, on our families, with dark secrets\nbeing imperceptibly passed down through generations. They also leave traces on\nour minds and emotions, on our capacity for joy and intimacy, and even on our\nbiology and immune systems.”\n\n\n“Some people’s lives seem to flow in a narrative; mine had many stops and\nstarts. That’s what trauma does. It interrupts the plot… . It just happens,\nand then life goes on. No one prepares you for it.”\n\n\n“The lack of literature on the topic was a handicap, but my great teacher,\nElvin Semrad, had taught us to be skeptical about textbooks. We had only one\nreal textbook, he said: our patients. We should trust only what we could learn\nfrom them—and from our own experience. This sounds so simple, but even as\nSemrad pushed us to rely upon self-knowledge, he also warned us how difficult\nthat process really is, since human beings are experts in wishful thinking and\nobscuring the truth. I remember him saying: “The greatest sources of our\nsuffering are the lies we tell ourselves.” Working at the VA I soon discovered\nhow excruciating it can be to face reality. This was true both for my patients\nand for myself.”\n\n\n“You live through that little piece of time that is yours, but that piece of\ntime is not only your own life, it is the summing-up of all the other lives\nthat are simultaneous with yours… . What you are is an expression of\nHistory.”\n\n\n“If you do something to a patient that you would not do to your friends or\nchildren, consider whether you are unwittingly replicating a trauma from the\npatient’s past.”\n\n\n“why they are only attracted to people who hurt them. Fear and aversion, in\nsome perverse way, can be transformed into pleasure.”\n\n\n“You observe a lot by watching.”\n\n\n“Medications, drugs, and alcohol can also temporarily dull or obliterate\nunbearable sensations and feelings. But the body continues to keep the score.”\n\n\n“It is only that in these troubled hours things are seen more clearly. The\nlamps of cities blur many shadows that are plain beneath the moon.”\n\n\n“That way, home could never be taken from him. Was this what the doctor meant?\nTo be the same person no matter where you went, no matter what madness\noccurred”\n\n\n“Power over others is weakness disguised as strength.”\n\n\n“It seems almost impossible to disidentify from the mind. We are all immersed\nin it. How do you teach a fish to fly? Here is the key: End the delusion of\ntime. Time and mind are inseparable. Remove time from the mind and it stops\nunless you choose to use it.”\n\n\n“There is no salvation in time. You cannot be free in the future. Presence is\nthe key to freedom, so you can only be free now.”\n\n\n“Everything is honored, but nothing matters. Forms are born and die, yet you\nare aware of the eternal underneath the forms. You know that “nothing real can\nbe threatened.”\n\n\n“A man isn’t tiny or giant enough to defeat anything.”\n\n\n“Auri closed her eyes and put the sheet back in the drawer, shame burning in\nher chest. She was a greedy thing sometimes. Wanting for herself. Twisting the\nworld all out of proper shape. Pushing everything about with the weight of her\ndesire.”\n\n\n“There is a difference between the truth and what we wish were true.”\n\n\n“Humor opens closed hearts. Humor can free us from the grip of our thoughts.\nWhen we smile, we feel we can accept things we previously could not.”\n\n\n“Moving forward inevitably invites further loss, but also new encounters.”\n\n\n“People flee out of a desire to live on. And the desire to live on stems from a\nfeeling of having unfinished business in life.”\n\n\n“Good things don’t last forever. Everything changes, fades, disappears\ncompletely over the passage of time. And so, people must make the most of the\nlife they have, seize the chance to enjoy it while it lasts, and have no\nregrets in the end”\n\n\n“Artist block is what happens when you start judging the outcome before it has\na chance to come out.”\n\n\n“If one can realize the original substance in which there is neither good nor\nevil, one will know what absolute Nothing is. And then all will, knowledge, and\nthings will emerge from Nothing. Once this is done, it settles everything.\nEffort is substance. This truth is simple and direct. It is neither too much\nnor too little. This is the secret to be passed from one mind to another.”\n\n\n“When the mind is in the absolute present it will be free from the departing of\nthe past and the coming of the future, and will be unified.”\n\n\n“Sometimes when you are not getting the love you want, giving makes you feel\nlike you will”\n\n\n“As you shout into the woods, so they echo back.”\n\n\n“What I like most about what I do is that this teaching does not belong to\nanyone, it could belong to anyone. What matters is how to transmit those\nteachings to others”\n\n\n“It is not good to listen to your mind. Let’s not be enslaved by it”\n\n\n“Everything starts with observing”\n\n\n” I wouldn’t change anything about my brain and the way it works. It’s fucked\nup so many things for me over the years, lost me jobs, ruined relationships,\ngot me in trouble with the law, caused me serious physical harm and damn near\nkilled me, but it’s all been anything but ordinary! Now I know more about the\nexecutive functions that ADHD impairs and understand the affect it has had on\nmy life, I’ve learned to appreciate the good things. We’re the ones who take a\npath for the first time, who swim in that water for the first time, who push\nthe limits further than anyone else and the latest scientific evidence suggests\nwe evolved for precisely this purpose. We’re life’s path-finders.”\n\n\n“Forty-five percent of what we do every day is habitual ,” say the researchers,\n\n\n” performed almost without thinking in the same location or at the same time\neach day, usually because of subtle cues .”\n\n\n“It’s because when you start to suffer, you speed up. And then you get mad.”\n\n\n“As one child described it, being gifted can feel like an abandoned alien\nwaiting for the mother ship to come and take them home”\n\n\n“When you have a grasp on eternity your eyes won’t ever see the battle or the\nlost people that hurt you. You will see a beautiful story of hope, in every\ncharacter.”\n\n\n“To love at all is to be vulnerable. Love anything, and your heart will be\nwrung and possibly broken. If you want to make sure of keeping it intact you\nmust give it to no one, not even an animal. Wrap it carefully round with\nhobbies and little luxuries; avoid all entanglements. Lock it up safe in the\ncasket or coffin of your selfishness. But in that casket, safe, dark,\nmotionless, airless, it will change. It will not be broken; it will become\nunbreakable, impenetrable, irredeemable. To love is to be vulnerable”\n\n\n“It’s precisely because these connections don’t last that they are so\nmeaningful. They exist in their own right. Their only value is what they are in\nthe now. Not what is expected of them in the future.”\n\n\n“We’re programmers. Programmers are, in their hearts, architects, and the first\nthing they want to do when they get to a site is to bulldoze the place flat and\nbuild something grand. We’re not excited by incremental renovation: tinkering,\nimproving, planting flower beds”\n\n\n“There’s a subtle reason that programmers always want to throw away the code\nand start over. The reason is that they think the old code is a mess. And here\nis the interesting observation: they are probably wrong. The reason that they\nthink the old code is a mess is because of a cardinal, fundamental law of\nprogramming It’s harder to read code than to write it.”\n\n\n“Humans will “create” a field of study on top of a field of study, assign\nthings categories and names, invent all of this stuff, become really good at\nit, and then conclude nothing is really known. Let’s go back to the part where\nwe created a field of study. Full stop.”\n\n\n“If the doors of perception were cleansed, everything would appear to man as it\nis, infinite”\n\n\n“Stress is the adult word for fear”\n\n\n“I’ve spent so much time in my head and in my heart that I forgot to live in my\nbody.”\n\n\n“As  a  storyteller  she  had  to  make  an  implicit  pledge  that  if  the\nsultan  followed along  on  the  journey,  he  would  be  rewarded.  She needed\nto  present  a  character with whom he could identify with on his quest. He\ncould imagine that he himself was  on  the  journey.  To  keep  the sultan’s\ncontinued  interest  she  would  have  to keep topping herself and keep the\nsultan guessing as to whether each character would succeed or fail in his or\nher quest. Another one of her tricks was to some- times  allow  the sultan\nmore  knowledge  than  the  characters  knew  themselves. When  he knew  more\nthan  the  characters  did,  he  was  led  to  anticipate  horrible things that\nmight happen to them.”\n\n\n“Learning how to avoid pain itself is pleasurable.”\n\n\n“We are made of stories”\n\n\n“When people injure you, ask yourself what good or harm they thought would come\nof it. If you understand that, you’ll feel sympathy rather than outrage or\nanger. Your sense of good and evil may be the same as theirs, or near it, in\nwhich case you have to excuse them. Or your sense of good and evil may differ\nfrom theirs. In which case they’re misguided and deserve your compassion. Is\nthat so hard?”\n\n\n“Liminal means ‘Intermediate between two states, conditions, or regions,’ So a\n‘liminal space’ can be a threshold or borderland where we can pass from one\narea to another. As we move through such spaces in life, we may enter a\n‘liminal state,’ an experience of ambiguity or disorientation, before we cross\nover, transformed.”\n\n\n“You know what your problem is? You’re smart. Too smart. You over think,\nbecause your mind moves at a million miles a minute. You’re sad, because you’re\nnot fooled by the world like everyone else. You don’t get along with most\npeople, because they just don’t look at things the way you do. You think you’re\ndumb, because you’re smart enough to know you don’t know everything. Your\nproblem is you’re too smart. And that’s not a problem at all.”\n\n\n“The only reward for hard work is more work.”\n\n\n“People generally fail to understand that complexity does not arise from\ncomplexity. Rather, it arises from simplicity, the simplicity of the tools”\n\n\n“Theoretical knowledge is table stakes for research taste. You can’t have\nresearch taste in a vacuum.”\n\n\n“When you identify such a mess, the natural inclination of many people is to\nshy away, to find something that is easier to understand. But a field that is a\nmess is really an opportunity. Chances are good that there are deep unifying\nand simplifying concepts still waiting to be understood and developed by\nsomeone - perhaps you.”\n\n\n“It’s like we’re launching expeditions with complex equipment to reach more and\nmore remote islands and tall mountains… and the biology stops at measuring\nthe size and weight of the animals we find.”\n\n\n“Every model is its own entire world of beautiful structure waiting to be\ndiscovered, if only we care to look.”\n\n\n“I suspect that a lot of “brilliant insights” are natural next steps from\nsomeone who has deep intimacy with a research topic. And that actually seems\nmore profound.”\n\n\n“Why is fantasy (magic, the force, etc) so emotionally compelling to many of\nus? Why is actually getting magic-like abilities from technology less profound?\nI think this points to important unmet needs and failings of technology. The\neasy answer is that magic is a power fantasy: it would give one power or make\none special. But that doesn’t really ring true to me. I imagine magic being\ncompelling if everyone had it. Conversely, not all fantastical powers I could\nimagine are emotionally resonant. Another possible answer is that we’re\ndeadened to the wonder of technology and science. We’ve lost our sense of awe.\nIf magic existed, it might lose awe too. I think that’s closer to true, but\nsomething is still missing. I think the crucial thing is that I imagine magic\nas humane and intimate, while technology is often alienating and\ndistant in practice. The Force is compelling because who doesn’t want to be\nprofoundly directly connected to the universe? I also feel like magic often\ngets at a desire for our feelings to fundamentally matter. When I feel deep\nemotions, it feels as though they ought to directly affect the world, but\nthey don’t. In many fantasy worlds, emotions are reified first class objects.\n(How would the experience of life be different if rooms shook when people\nexperienced severe grief or anger? If lighting and subtle sound effects were\ndifferent around someone who is deeply serene or joyful? This seems physically\npossible.)”\n\n\n“My day job is to speak in an arcane snake language to a crystal vibrating at\n3,000,000,000 cycles per second sitting in a cloud so that it can alter\nprobabilities in the real world. If that isn’t magic what is.”\n\n\n“When I can inhabit this mindset of loneliness being sacred in some sense, it\nadds a kind of beauty to an otherwise quite sad experience. I also suspect it\nmight protect against some of the ways loneliness can be corrosive.”\n\n\n“No artist tolerated reality.”\n\n\n“2 week experiment. 6 month plan”\n\n\n“And then one day, EVERYTHING that you planned will come across an UNPLANNED\nscenario. Things will CHANGE. The WORLD around you will change. CHOICES,\nOutcomes &amp; even Mindset’s would change. Everything you were once HELLBENT about\nnot doing, would be EASY feat for you. What once made you CONTENT, would not be\nENOUGH. That’s the only constant truth about LIFE. It will change. People,\nMinds, Hearts, Dreams, Best Friends, Status, Everything. Look back at your last\ndecade &amp; you would know it’s true. So Live it up TODAY, because TOMORROW, even\nwhat is today might CHANGE.”\n\n\n“Our minds are like furry little gibbons: always agitated, never at rest”\n\n\n“Another study, out of Yale, looked at the part of the brain known as the\ndefault mode network (DMN), which is active when we’re lost in\nthought—ruminating about the past, projecting into the future, obsessing about\nourselves. The researchers found meditators were not only deactivating this\nregion while they were practicing, but also when they were not meditating.”\n\n\n“It was a little embarrassing to be reading a self-help writer and thinking,\nThis guy gets me. But it was in this moment, lying in bed late at night, that I\nfirst realized that the voice in my head—the running commentary that had\ndominated my field of consciousness since I could remember—was kind of an\nasshole.”\n\n\n“What the science was showing was that our levels of well-being, resilience,\nand impulse control were not simply God-given traits, our portion of which we\nhad to accept as a fait accompli. The brain, the organ of experience, through\nwhich our entire lives are led, can be trained. Happiness is a skill.”\n\n\n“Studies showed that the best way to engineer an epiphany was to work hard,\nfocus, research, and think about a problem—and then let go. Do something else.”\n\n\n“between me and reality. As one Buddhist author put it, the “craving to be\notherwise, to be elsewhere” permeated my whole life.”\n\n\n“According to the Buddha, we have three habitual responses to everything we\nexperience. We want it, reject it, or we zone out. Cookies: I want. Mosquitoes:\nI reject. The safety instructions the flight attendants read aloud on an\nairplane: I zone out. Mindfulness is a fourth option, a way to view the\ncontents of our mind with nonjudgmental remove. I found this theory elegant,\nbut utterly unfeasible.”\n\n\n“the only way to tame the monkey mind, to truly glimpse impermanence and defeat\nour habitual tendency toward clinging, was to meditate—and I had absolutely no\nintention of following their advice. Meditation struck me as the distillation\nof everything that sucked hardest about the granola lifestyle. I pictured\nmyself seated in an unbearable cross-legged position (my disavowal of yoga\nhaving left me less limber than I would have liked) in a room that smelled like\nfeet, with a group of smug “practitioners” ringing bells, ogling crystals,\nintoning om, and attempting to float off into some sort of cosmic goo. My\nattitude was summed up nicely by Alec Baldwin’s character on 30 Rock, who said,\n\n\n“Meditation is a waste of time, like learning French or kissing after sex.”\nCompounding my resistance was my extremely limited attention span. (Another of\nthe many reasons I went into TV.) I assumed there was no way my particular\nmind—whirring at best, at worst a whirlwind—could ever stop thinking.”\n\n\n“The ego thrives on drama. It keeps our old resentments and grievances alive\nthrough compulsive thought.”\n\n\n“Meditation suffers from a towering PR problem, largely because its most\nprominent proponents talk as if they have a perpetual pan flute accompaniment.\nIf you can get past the cultural baggage, though, what you’ll find is that\nmeditation is simply exercise for your brain.”\n\n\n“When you’re totally present, whatever the situation is, good or bad, it’s\ngonna pass. The only thing that remains is the moment.”\n\n\n“When you have one foot in the future and the other in the past, you piss on\nthe present.”\n\n\n“Striving is fine, as long as it’s tempered by the realization that, in an\nentropic universe, the final outcome is out of your control. If you don’t waste\nyour energy on variables you cannot influence, you can focus much more\neffectively on those you can. When you are wisely ambitious, you do everything\nyou can to succeed, but you are not attached to the outcome—so that if you\nfail, you will be maximally resilient, able to get up, dust yourself off, and\nget back in the fray. That, to use a loaded term, is enlightened\nself-interest.”\n\n\n“We all have an innate feeling of being separate from the world, peering out at\nlife from behind our own little self, and vying against other isolated selves.\nBut how can we truly be separate from the same world that created us? “Dust to\ndust” isn’t just something they say at funerals, it’s the truth. You can no\nmore disconnect from the universe and its inhabitants than a wave can extricate\nitself from the ocean.”\n\n\n“We can do more than just think; we also have the power simply to be aware of\nthings—without judgment, without the ego. This is not to denigrate thinking,\njust to say that thinking without awareness can be a harsh master.”\n\n\n“The Buddha embraced an often overlooked truism: nothing lasts—including us. We\nand everyone we love will die. Fame fizzles, beauty fades, continents shift.\nPharaohs are swallowed by emperors, who fall to sultans, kings, kaisers, and\npresidents—and it all plays out against the backdrop of an infinite universe in\nwhich our bodies are made up of atoms from the very first exploding stars. We\nmay know this intellectually, but on an emotional level we seem to be hardwired\nfor denial.”\n\n\n“None of us has moved very far from the seven-year-old who vigilantly watches\nto see who got more.”\n\n\n“If you’re never looking up, I now realized, you’re always just looking\naround.”\n\n\n“It’s not me telling you,” she said. “It’s neuroscience that would say that our\ncapacity to multitask is virtually nonexistent. Multitasking is a\ncomputer-derived term. We have one processor. We can’t do it.” “I think that\nwhen I’m sitting at my desk feverishly doing seventeen things at once that I’m\nbeing clever and efficient, but you’re saying I’m actually wasting my time?”\n\n\n“Yes, because when you’re moving from this project to this project, your mind\nflits back to the original project, and it can’t pick it up where it left off.\nSo it has to take a few steps back and then ramp up again, and that’s where the\nproductivity loss is.” This problem was, of course, exacerbated in the age of\nwhat had been dubbed the “info-blitzkrieg,” where it took superhuman strength\nto ignore the siren call of the latest tweet, or the blinking red light on the\nBlackBerry. Scientists had even come up with a term for this condition:\n\n\n“continuous partial attention.” It was a syndrome with which I was intimately\nfamiliar, even after all my meditating.”\n\n\n“I was a frequent mental inventory taker, scanning my consciousness for objects\nof concern, kind of like pressing a bruise to see if it still hurts.”\n\n\n“Thoughts calcify into opinions, little seeds of discontent blossom into bad\nmoods, unnoticed back pain makes me inexplicably irritable with anyone who\nhappens to cross my path.”\n\n\n“The voice comes braying in as soon as we open our eyes in the morning, and\nthen heckles us all day long with an air horn. It’s a fever swamp of urges,\ndesires, and judgments. It’s fixated on the past and the future, to the\ndetriment of the here and now. It’s what has us reaching into the fridge when\nwe’re not hungry, losing our temper when we know it’s not really in our best\ninterest, and pruning our inboxes when we’re ostensibly engaged in conversation\nwith other human beings. Our inner chatter isn’t all bad, of course. Sometimes\nit’s creative, generous, or funny. But if we don’t pay close attention—which\nvery few of us are taught how to do—it can be a malevolent puppeteer.”\n\n\n“Do you know what normally happens after your funeral? In a few hours the cry\nsound will be completely stopped. Family will be busy ordering food from hotels\nfor relatives.. Grandchildren running and playing. Some men would go to the tea\nshop for walk before going to bed. Your neighbour will be angry,\nthinking that your people may have thrown the Ritual leaves close to\ntheir gate. A relative will talk to your daughter on the phone about\nnot being able to come in person due to an emergency situation. At the\nnext day’s dinner, few relatives get reduced, and few complains not\nhaving enough salt in the curry. Foreign relations would have secretly\nplanned tourism, as if they had never looked so far on the way there. A\nrelative may be complaining about funeral that he has spent a few\nhundred rupees more on his share. The crowd will slowly begin to\ndissolve.. In the coming days Some calls may come to your phone without\nknowing you are dead. Your office will be rushing to find someone to\nreplace you. One week later, after hearing the news of your death, Some\nFacebook friends may search with curious sadness to know what your last\npost was. In two weeks your son and daughter will return to work after\ntheir emergency leave get over. By the end of the month, Your spouse\nwill be watching a comedy show and laughing.  In the coming months,\nyour close relationships will return to the cinema and the beach.\nEverybody’s life will go back to normal Just as there is no difference\nbetween a withered leaf of a big tree and what you live and die for, it\nall happens so easily,so fast, without any movement. Rains have\nstarted, the election is coming, the crowds on the buses are as usual,\nan actress is getting married, the festival is coming, the World Cup\ncricket is going as planned, the flowers are in full bloom, and your\npet gave birth to next puppy. You will be forgotten by this world at an\nastonishing pace. In the meanwhile your first year death anniversary\nwill be celebrated in a grand manner. In the blink of an eye Years have\npassed and there is no one to talk about you. One day, just looking at\nold photos, one of your close friend may remember you, In your\nhometown, of the thousands of people you’ve become acquainted with,\nonly one person may remember and talk about sometime. You maybe living\nelsewhere, as someone else, if reincarnation is true. Tell me now…\nOtherwise, you will be nothing and will be plunged into darkness for\ndecades. People are waiting to forget you easily Then who are you\nrunning around for? And who are you worried for? For most part of your\nlife, say 80%, you think about what your relatives and neighbors think\nabout you.. Are you living a life to satisfy them? NO USE Life is only\nOnce, just live it up Fully”\n\n\n“Be good to yourself first, then to others”\n\n\n“When we say that someone is “good,” we often mean that the person complies\nwith the will of others and isn’t self-assertive. In other words, people who\nare good at suppressing their own desires in deference to another’s are the\nones who frequently get called “good.”\n\n\n“But the problem is that, by living in accordance with the demands of others,\nwe unwittingly neglect our own desires and needs. If as a child you were\nindifferent to your own feelings, minimizing them or not considering them\nimportant, as an adult you will not be able to tell what it is you yourself\nwant to do, or who you are as a person. And then when you encounter someone who\ntreats you unfairly or makes things difficult for you, since you do not know\nhow to properly express your own feelings, the anger that ought to be directed\ntoward its instigator is trapped inside you and ends up attacking you instead.\n\n\n“Why am I such an idiot, that I can’t express my feelings properly, can’t even\nspeak up honestly?”\n\n\n“One of our common mistakes is to compare how we feel inside with how our\nfriends appear outside.”\n\n\n“Compare yourself not with others, but with the old you”\n\n\n“Not everything that appears in your mind is true.”\n\n\n“If you hear a voice within you say, ‘You cannot paint,’ then by all means\npaint, and that voice will be silenced.”\n\n\n“Don’t let the internet rush you. Nobody is posting their failures”\n\n\n“Any half-awake materialist well knows – that which you hold holds you.”\n\n\n“The more stuff you own, the more your stuff owns you”\n\n\n“You can’t have everything. Where would you put it?”\n\n\n“Our actions will always follow the true desire of our heart. What our heart\nbelieves and loves always determines the path of our life. We can mask our true\nwants for only a short while. Without a true heart change, we always return to\nour heart’s first love.”\n\n\n“Becoming minimalist has modeled for my children that personal belongings are\nnot the key to happiness, that security is found in their character, and that\nthe pursuit of happiness runs a different road than the pursuit of\npossessions.”\n\n\n“Madison Avenue has controlled our finances for too long. Since the day we were\nborn, it has told us what needs to be bought, when it needs to be purchased,\nand what store we should visit to find the best value. When we chose freedom\nfrom material possessions, we broke the control that our consumer-driven,\ncapitalistic society has had over us. Suddenly, we have been freed to use our\nfinances to pursue endeavors far greater than those offered at our local\ndepartment store.”\n\n\n“Everything has a home.”\n\n\n“The good life is a process, not a state of being. It is a direction, not a\ndestination.”\n\n\n“Do not be tense, but ready; not thinking, but not dreaming; not being set, but\nflexible. It is being wholly and quietly alive, aware, and alert; ready for\nwhatever may come.…”\n\n\n” “Never assert yourself against nature,” he told him. “Never be in frontal\nopposition to any problem, but control it by swinging with it.”\n\n\n“To be like water, then, is to realize your most whole, natural, and actualized\nself where you are living as much as possible in the slipstream of life as you\nforge your own path forward.”\n\n\n“Using no way as way, having no limitation as limitation.”\n\n\n“water basics that I want us to begin to sit with—that water is undeterred. It\nwill carve canyons into mountains over centuries.”\n\n\n“To be like water is not to be aspiring to perfection. Perfection is a\ndifficult master. To be like water is not to be controlling of everything.\nControl is a tight yoke.”\n\n\n“But I decided to be present with what was happening and stop resisting it. I\ngave the future of the project to the universe, and I said, “Show me the way.”\nAnd like water, I began to follow the course of this new unfolding rather than\ntry to build a thousand dams to enforce the direction of the stream.”\n\n\n“I’m participating and cocreating, but no longer forcing.”\n\n\n“No man ever steps in the same river twice, for it’s not the same river and\nhe’s not the same man.”\n\n\n“This ship is seeking knowledge. All of the things people tell you to seek out\nin life - a college degree, a job, a paycheck, awards, research publications,\ncritical acclaim, etc. - all of these things happen as a result of seeking\nknowledge. If you live your life by this guiding principle, you’ll never be\nlead astray. However, if you start to chase the wake of your ship - the job,\nthe piece of paper that says you earned a college degree, etc. - you’ll end up\nchasing your tail and halting any sense of forward progression.” For example,\nif you go through college seeking a high GPA and the end overall degree, you\ntend to miss out on the most formative aspects of higher education.\nHowever, if you approach college seeking knowledge and challenging\nyourself, you’ll typically end up with a good GPA and graduate with a\ndegree - the “wake” which came as a result of knowing how to orient\nyour “ship.”\n\n\n“You will learn more about God on a bus stop, in an unemployment life, through\nthe death of a loved one, than you ever will by just listening to a sermon.”\n\n\n“And every platform, especially image-conscious ones like Instagram, encourages\na constant cosplay of the self — to project someone cooler, hotter, and happier\nthan we really are. It’s perhaps not surprising that we’re losing track of who\nwe are when we power down our screens.”\n\n\n“Machine learning methods change every year, solving problems stays the same.”\n\n\n“The evolution of human mentality has put us all in vitro now, behind the glass\nwall of our own ingenuity.”\n\n\n“No religion is the only religion, no church the true church.”\n\n\n“Perhaps nowhere is our human mania for possessing, our delusion that the owned\ncannot have a soul of its own, more harmful to us. This disanimation justified\nall the horrors of the African slave trade. If the black man is so stupid that\nhe can be enslaved, he cannot have the soul of a white man, he must be a mere\nanimal.”\n\n\n“Even the simplest knowledge of the names and habits of flowers or trees starts\nthis distinguishing or individuating process, and removes us a step from total\nreality towards anthropocentrism.”\n\n\n“We shall never fully understand nature (or ourselves), and certainly never\nrespect it, until we dissociate the wild from the notion of usability - however\ninnocent and harmless the use. For it is the general uselessness of so much of\nnature that lies at the root of our ancient hostility and indifference to it.”\n\n\n“Man is a highly acquisitive creature, brainwashed by most modern societies\ninto believing that the act of acquisition is more enjoyable than the fact of\nhaving acquired, that getting beats having got.”\n\n\n“Ordinary experience, from waking second to second, is in fact highly synthetic\n(in the sense of combinative or constructive), and made of a complexity of\nstrands, past memories and present perceptions, times and places, private and\npublic history, hopelessly beyond science’s powers to analyse. It is\nquintessentially ‘wild’ … unphilosophical, irrational uncontrollable,\nincalculable.”\n\n\n“These question-boundaries …are ours, not of reality. We are led to them,\ncaged by them not only culturally and intellectually, but quite physically, by\nthe restlessness of our eyes and their limited field and acuity of vision.”\n\n\n“Evolution had turned man into a sharply isolating creature, seeing the world\nnot only anthropocentrically but singly, mirroring the way we like to think of\nour private selves.”\n\n\n“We lack trust in the present, this moment, this actual seeing, because our\nculture tells us to trust only the reported back, the publicly framed, the\nedited, the thing set in the clearly artistic or the clearly scientific angle\nof perspective.”\n\n\n“People who live their lives in fear are destined to have uncomfortable feet”\n\n\n“You can’t find happiness where you lost it”\n\n\n“You wouldn’t care as much about what people think about you if you realize\nthey seldom do.”\n\n\n“We give up , because we feel , we are getting judged , because that’s what\nsociety does to us all the time.”\n\n\n“e, humans, find reasons for almost anything. We love the thought of giving\nmeaning to things, especially to our existence. We are such creatures.  I think\nit stems from our fears. Fear of being insignificant, fear of being a victim of\ncircumstances, fear of having no importance, etc. I believe it’s a defense or\ncoping mechanism. It’s our go-to as thinking beings.”\n\n\n“We humans, we’re all the same. Every last one of us. For some it’s drinking,\nfor some it’s women, for some even religion, family, the king, dreams,\nchildren…power. All of us had to spend our lives drunk on something\nelse we’d have no cause to keep pushing on. Everyone…was a slave to\nsomething.”\n\n\n“I’m simply an accident. Why take it all so seriously?”\n\n\n“All those moments will be lost in time, like tears in the rain.”\n\n\n“What do you do from morning to night? I endure myself”\n\n\n“Man stands face to face with the irrational. He feels within him his longing\nfor happiness and for reason. The absurd is born of this confrontation between\nthe human need and the unreasonable silence of the world.”\n\n\n“After\nhaving struggled madly to solve all problems, after having suffered on\nthe heights of despair, in the supreme hour of revelation, you will\nfind that the only answer, the only reality, is silence.”\n\n\n“Everything that is formulated becomes more tolerable.”\n\n\n“Every man is happy until happiness is suddenly a goal”\n\n\n“The truth is, you are already worthy of being loved. You don’t need to be\nconvinced of your self-worth by taking on society’s demands and living up to\nits expectations. You already are a precious being and deserve to be loved and\ncared for. Look inside and see if you can find the child within you, still\nshaking with anxiety because of his father. Send the energy of loving-kindness\nto that inner child, and look at him compassionately. How difficult it must\nhave been, coping with your father’s rage alone, trying to protect your\nsiblings, without even your mother to help you.”\n\n\n“I sometimes believe my mom caused me to have ADHD or at least she worsened it.\nI can see a lot of correlations, like my “laziness” (her words) coming from her\nlack of parenting and holding me accountable in a normal way. Or the way that I\nzone out a lot / have issues focusing being because of a list of reasons\nincluding dissociation to protect myself or being use to being unheard. It’s\nkind of hard to explain it all, but you get the point hopefully.”\n\n\n“I woke up yesterday at noon. but for today”\n\n\n“Life always waits for some crisis to occur before revealing itself at its most\nbrilliant.”\n\n\n“Shower your child with attention, and make her feel secure in your love.  This\nway she won’t grow up starved for other people’s acknowledgment.”\n\n\n“Special moments are not separate from our everyday lives.  When you make use\nof something special, it makes the moment special.”\n\n\n“Are you looking to move onwards and upwards or run away? Look before you leap,\nyou don’t want to appear the fool do you?”\n\n\n“Lengthy deliberation often leads to a terrible decision. If you think and\nworry too much before doing something, your boat goes to the mountain instead\nof the ocean.”\n\n\n“We are so good at creating something from nothing that we make our own lives\nfantasy and give ourselves a hard time. Color dodge the shit out of your life.\nOnly you can.”\n\n\n“No one is going to break the cycle until they accept who they are, and what\nthey are, and what they’ve done, and they forgive themselves. You can’t move\nforward unless you forgive yourself. A lot of people don’t know that. So they\nlive in guilt and shame for so long.”\n\n\n“Nature does not hurry yet everything is accomplished”\n\n\n“O archon seeker, do you now understand? She and her wisdom have long been\nfound by you. Along your journey we were in every flower and blade of grass,\nevery ray of sparkling sun, and every breath of dancing wind. So long as you\ncontinue to think and ponder, well be wherever you go.”\n\n\n“Fate will only ever show you the beginning of a journey. It is up to you for\nforge your own ending.”\n\n\n“Analogies are wonderful tools. They let you use existing knowledge to\nunderstand unfamiliar things.”\n\n\n“I believe that the Archons revelations are never more than vague hints.\nAnything more specific is beyond the reach of mere mortals.”\n\n\n“If you trust your instincts and overcome your fears, the sun will surely rise”\n\n\n“Knowledge always comes at a price.”\n\n\n“Tomorrow will come. Everyone assumes this as common knowledge, but the only\nway you can know for sure is if you experience tomorrow. How many ‘today’s’ has\nit been? Is it possible that today will be followed by yesterday? Does tomorrow\ntruly exist as anything beyond a made up concept?”\n\n\n“That’s why it makes no sense to waste your energy thinking about things you\nwill learn tomorrow.”\n\n\n“Dreams are fantastical, complex and full of imagination. Peoples brains are\nmost active when they’re dreaming. In other words, the dreams are rich bundles\nof human wisdom.”\n\n\n“There will always be frustrations in life, but I know that the point of living\nis to not leave behind any regrets.”\n\n\n“When the north wind blows, some dandelion seeds are blown across the sea to\nInazuma, while others get swept away to Liyue.”\n\n\n“Jeht: I don’t have family traditions, and I don’t even remember my mother’s\nface. I only have you, and you only have me. There is no large tree and no\nbranches that tie us together.  Jeht: You might have left the desert long ago,\nand this might be my first time seeing it for myself, but I have this feeling\nthat we are like grains of sand here in the desert.  Jeht: No family, no\nobjective… But what we are is pretty free. Haha!  Paimon: Jeht…  Jebrael:\nThe desert holds only the past. It has no future. If what lies beneath the sand\nand wind can be described as the truth, then I can only say that it perhaps\nisn’t something that will bring joy to everyone.  Jebrael: I gave up the path\nof seclusion, not because it’s a mistaken path, but because I found a better\npath. Jeht, I hope you’ll be able to take that bright path.  Jeht: I don’t\nreally understand you. If the path of sand isn’t what you desire, then why\naccept Tirzad’s commission? Don’t tell me it’s because you want to take a look\nat your past.  Jeht: I think you just can’t forget the desert and the hot winds\nthat sweep it.  Jeht: The strange thing is that I have no memories of the\ndesert, but coming here makes me feel like I’ve returned home all the same.\nJeht: I love the sand, just like I love Paimon and Benben.”\n\n\n“O stars high above the wasteland; O nightingales weary from the day; It’s time\nto take off the crown of roses; Cleanse yourself with wine made from grapes.\nSleep, close your eyes; Yon golden slumber summons thee, wandering sand; Drink\nnot that bitter salt water; For the sorrows of tomorrow have gone away.”\n\n\n“Underestimate the sand and you’ll pay with your life.”\n\n\n“The door is protecting something very important, and so much time has passed\nthat it has almost forgotten how it used to open.”\n\n\n“Coming to the desert is like returning home, and in the end, I’m a grain of\nsand. I was born here, and I shall be buried here as well.”\n\n\n“Ufairah and I were like a bird and a fish, but Al-Ahmar’s secrets allowed our\nfates to intertwine. Now, you and I, people from different lands, are again\nhere for those very same secrets.”\n\n\n“A man chained by hatred cannot raise a daughter.”\n\n\n“There was the Door to which I found no Key; There was the Veil through which I\nmight not see: Some little talk awhile of Me and Thee There was—and then no\nmore of Thee and Me.” “Each Morn a thousand Roses brings, you say: Yes, but\nwhere leaves the Rose of Yesterday?”\n\n\n“Agree to disagree. But, your appearance just made this a whole lot more\ninteresting.”\n\n\n“Every journey has its end. Don’t rush.”\n\n\n“Some say a few are chosen and the rest are dregs; But I say we humans have our\nhumanity. We will defy this world with a power from beyond.”\n\n\n“Then, the threads of all fate will be yours to reweave…”\n\n\n“Beauty is a waste when the beholder has no taste.”\n\n\n“Nobody ever feels alone when they set off a firework”\n\n\n“There will always be those who dare to brave the lightning’s glow”.\n\n\n“Even if an antique is priceless, the happiness it brings lasts for only the\nmoment you obtain it.”\n\n\n“I know, I know, you can’t get me out of your mind. But you really don’t need\nto call my name all the time.”\n\n\n“Never stop searching, even for a brief flash of light”\n\n\n“We have always… had enough time”\n\n\n“People believe whatever they want to believe. Some things they do not\nsee,simply because they do not wish to look.\n\n\n“To endure hardship you must prepare for hardship”\n\n\n“What you might not realize is that all too often… people have far more to lose\nby chasing their dreams”\n\n\n“A heart made of stone is a heart nonetheless”\n\n\n“If you’re not willing to communicate, then the problem just sits there. If you\njust keep staring at it without doing anything, eventually you’ll watch every\nlast opportunity to resolve it slip away before your eyes”\n\n\n“Where our legs cannot take us, maybe our tools can. And when tools fail us,\nperhaps wings can carry us instead.”\n\n\n“The spirit soars the mountains high, while the body rests as the world goes\nby…  By wave and storm I hunt for fish, by wind and snow I slay evil…”\n\n\n“Let this be a lesson to those who yesterday said, “I’ll do it tomorrow.”\n\n\n“As a Guuji, there’s one thing I know very well: People believe whatever they\nwant to believe. Some things you do not see, simply because you do not wish to\nlook. And so… it falls to me to place the truth before your eyes, in all its\nugliness.”\n\n\n“There are no coincidences in the world. Everything is the fruit of seeds\nplanted long ago.Just like your appearance in that tavern.Time is just waiting\nfor those seeds to sprout”.\n\n\n“I’m all for work-life balance, but I think this is pushing it.”\n\n\n“For those that live too long, the friends of days gone by and scenes from\ntheir adventures live on in their memories. As such i have no regrets in\nmeeting you, friend. Should the day ever come that we are not together, you\nwill continue to shine like gold in my memories.”\n\n\n“You stand upon your tomb, though you know it not! ”\n\n\n“When shall we meet again after this parting? For life is like the morning\ndew.”\n\n\n“What is more pure and free than than wish of a child?”\n\n\n“No matter what the days may bring,whichever roads we choose to take.While this\noath remains observed,each of us remains the same…”\n\n\n“One is simply not partial to the tedium of social interaction, and wished to\nfind some peace and quiet.”\n\n\n“Even I cannot avoid it. But there is something I understand better than most:\nWhen the door opens, it is time to leave. The greater the power, the greater\nthe danger erosion may bring about. The millennia may come and go, but even a\nstone may tire.”\n\n\n“People who look for the flaws in others, expose theirs quickes”\n\n\n“Because maybe it doesn’t matter so much if something’s real or not. Maybe\nmagic and awesomeness are what make something worth believing. Why should\nsomeone else get to take that away from you?”\n\n\n“Cast your fear of injury by the wayside and fight with all your might. I too\ndid this during the Archon war”\n\n\n“If saving you is a sin, I’ll gladly become a sinner”\n\n\n“To be alive is to seek, to set foot in every place that eye can see.”\n\n\n“In teyvat, the stars in the sky will always have a place for you”\n\n\n“Someday even bedrock returns to dust”\n\n\n“For dawn to come, there must be those willing to pierce the darkness with\ntheir light.”\n\n\n“When one’s fervent ambition burns brightly, the gods will cast their gaze upon\nyou. Some ambitions have the power to heal wounds. to bring victory. to inspire\nhope. But some ambitions outlive their masters, long after the soul ascends.\nThey remain as they were in the beginning. Burning bright and true for all\neternity.”\n\n\n“A blade embraces it’s duty, as a jeweller cherished their gems”\n\n\n“A non-believer going into a church never ends well”\n\n\n“You’ve been with me for some time now. Have you learned to observe? Then\nobserve me. Observe me right down to my very core.”\n\n\n“Hard work is all there is to the craft”\n\n\n“People show you whatever side of themselves they want you to see.”\n\n\n“I hear the voice of fate, speaking my name in humble supplication…”\n\n\n“Gamblers always place their bets on the next dice roll… But the bankers\nalways have the last laugh, and they never touch a single die.”\n\n\n“Before demanding too many miracles from the gods, first consider if you are\nwilling to pay the price they ask”\n\n\n“This scenery is wonderful… Surely enough to convince anyone to become a\nwanderer.”\n\n\n“My memory has all but faded completely… but I will always remember how she\ntoo, loved these flowers”\n\n\n“To be blind to the beauty that poets adore is to view the world from the ocean\nfloor.”\n\n\n“The Crash of a Spear brought billowing dust. The mountains and waters made way\nat the sound. The sight of a dragon bestowed with a touch. The promise of\nrainwater blessing the ground.”\n\n\n“If only you could make one exception…things could have been easier for you.”\n\n\n“A foolish man complains of a hole in his pocket, I wise man uses it to scratch\nhis balls.”\n\n\n“For how could a god, who never once resisted, even till the end, nurse hatred\nfor her people in her heart?”\n\n\n“Sorry to also have you shoulder the grievances of the world, since you could\nendure my bitter cold, you must have desire to burn?”\n\n\n“The power of water is its ability to take any shape.”\n\n\n“And thus another spark of divinity departs from Liyue. My legacy shall now be\nleft for those who come after to debate.”\n\n\n“The fate that brings people together is not a cord so easily cut.”\n\n\n“He” is unhappy. “He” is displeased with the disturbance caused by “words of\nthe tone,” which drown out the wisdom of the “words of the heart”. The “words\nof the tongue” are like pouring rain. They shake the eardrums and disrupt the\nheart. “The words of the heart” are like fog. They deep in naturally,\nenshrouding.. do not use your tongue do not use your ears, listen with your\nheart”.”\n\n\n“There are so many disappointments to face. Some are huge, some are small, some\nare what they are.  Some are deep wounds that change the course of your life.\nThey become the scar that everyone sees, no matter how much you try to hide it.\nSome are so small, there’s no understanding why we give them any time at all,\nbut we keep giving them center stage. Some are disappointing. We accept them\nfor what they are and hope good comes out it. Once the ice cream hits the\npavement, all we can do is hope the ants enjoy the treat”\n\n\n“I have lived places where black lives didn’t matter. I have lived places where\npoor lives didn’t matter. I have lived places where military lives didn’t\nmatter.  Those places will never live in me.”\n\n\n“When a child is bored, read them a book about far away places. Plant a dream\nof a peace filled world”\n\n\n“I’m often asked how I did it, how I got through the hard times. I wish there\nwas a check list of things to help others get through their darkness, but\nthere’s not. Sometimes you have to shuffle your feet to keep your balance.\nSometimes you have to take bold steps to get over a crevasse. Sometimes you\njust have to stand still and let the raging water pass in front of you. There’s\nnot a right answer, or even one answer. Just keep moving forward to find your\nlight.”\n\n\n“Darkness may not always be evident, but it doesn’t mean it’s not there. But\nthere is, also, still hope to shed light on those places.”\n\n\n“Impatient with actions, Patience with results.”\n\n\n“Past meets present, heritage becomes legacy, long into the future may we\nthrive.”\n\n\n“Aranishat, you should admit defeat. Don’t be like a stubborn twig that refuses\nto bend against strong gusts. You will break just like it.”\n\n\n“The unseen and uncatchable that slips past the moon’s gaze and encourages the\ngrowth of sprouts”\n\n\n“Hmm, the children of the forest aren’t distinguished by age.”\n\n\n“Zhongli: I ended an era with my own two hands. I have always wondered how I\nshould… remember that which I ended.  Zhongli: History records, but history\nmay be changed. This incident proved that. Time is a mighty force, and\nhistories twist in its flow…  Zhongli: I need to find a better way of\nrecording history in order to engrave its truth.  Zhongli: Stone carvings were\none such ancient method. But unchanging stone, immovable earth, even one such\nas myself… Someday, we may all disappear.  Paimon: Zhongli…  Zhongli:\nTherefore, I thought of you, Traveler.  Zhongli: You are one who crosses the\ncelestial atlas, and who passes through countless worlds. If our history is\nengraved in your memory, it will one day accompany you into another world.\nZhongli: As long as a Traveler like you is able to record what happened, then a\nbackup of sorts will exist for times and tides of Teyvat.”\n\n\n“Life may exist in all kinds of unfathomable forms and in all manner of\nunthinkable environments. Mysterious, yet tenacious… Perhaps this is what\nmakes life so special.”\n\n\n“To those who seek shall just reward be given in proportion to their wisdom and\nwork.”\n\n\n“Yae Miko: The path of the swordmaster is filled with twists and turns. It is\nno small undertaking to pursue the position of greatest swordmaster in the\nworld.  Yae Miko: It requires one to take their sword firmly in both hands and\ncut down the hopes and dreams of others… even those of one’s closest\ncompanions.  Yae Miko: Only a deep commitment to his ambition to become the\nbest made it possible for him to rise above the pain of these encounters — to\nfocus on the way ahead.  Yae Miko: When that ambition disappeared, he began to\ndoubt himself. As he battled his growing anxiety… he slowly descended into\nthe state you see him in now.”\n\n\n“The wind that blows from afar carries fresh life to these shores…”\n\n\n“How else can you catch a thief except by being familiar with how a thief\nthieves?”\n\n\n“Only those who have traveled understand the meaning of a journey. I hope\nshe’ll find what she’s been looking for…”\n\n\n“The essence of the Amenoma Art is to have the patience to move mountains and\nunrelenting willpower”\n\n\n“Then again, the memories of ore can shift with the passage of time and the\nchanging of the environment”\n\n\n“He who bears the weight of memory is destined to shoulder the burden of truth.\nAs it ought to be.”\n\n\n“In the wilderness, snow falls on a spring day. In an instant it will melt.\nEven where it is fleeting and leaves no trace. Even where it will never fall\nagain…”\n\n\n” Kun Jun: Curious how swords and daggers are blind, yet their creators see so\nmuch. Perhaps empathy is mankind’s proudest achievement after all?  Zhongli:\nAzhdaha. I am no longer the Geo Archon.  Kun Jun: …I can sense it.  Zhongli:\nToday I am just an ordinary citizen of Liyue.  Kun Jun: Even you met such a\nfate… Let’s get the difficult part out of the way. I cannot guarantee that I\nwon’t be awoken a second time.  Zhongli: No matter. If that day comes to pass,\nLiyue must prepare itself to face you.  Kun Jun: And how will Liyue fare\nwithout Rex Lapis?  Zhongli: Even without a god above, this remains a nation of\nmen. I was once their god, I ought to be here to witness their rise and fall.\nKun Jun: All life is shaped and then ground away by the endless flow of time.\nYou were always the strongest among us, yet it would seem that even you have\nbeen eroded…  Kun Jun: That’s unimportant… fate is ordained by heaven. Even\nif our mission had already concluded, it would be cowardly not to strike out on\nthe road of departure.  Kun Jun: You may live forever, doomed to a\nlonely existence… yet even this is temporary. When you reach the end\nof time, those people, those past and future relationships\npredetermined by fate… They will be waiting for you.  Zhongli: I do\nnot pretend to match your rhetoric when it comes to the subject of a\nlife long-lived. I fear that the life of an elemental being is longer\nthan any in this world.”\n\n\n“The greater the power, the greater the danger erosion may bring about. The\nmillennia may come and go, but even a stone may tire.”\n\n\n“But as long as you firmly believe that you are on the right path… everything\nhas meaning.”\n\n\n“A fleeting moment, a thousand years in the mortal world. The rocks feel it,\nand so too does the earth and the god”\n\n\n“Nachtigal: I think you’re being too stubborn. There’s always a choice in life.\nNachtigal: I mean, just look at Bonifaz and I. We weren’t born as traveling\nmerchants, and no one in our families has even been to Sumeru…  Nachtigal:\nBut we’re still here with the northern wind at our backs. We don’t regret our\nchoice at all. Seeing the boundless world unfold before our eyes fills us with\na sense of freedom.”\n\n\n“The desert holds only the past. It has no future. If what lies beneath the\nsand and wind can be described as the truth, then I can only say that it\nperhaps isn’t something that will bring joy to everyone.”\n\n\n“Venti: “Fill up the barrels and store them away, Then wait, wait for a windier\nday.  Wax the bottles, seal them tight, For the south wind that soothes, for\nthe north wind that bites.” “How does this fine wine taste to the tongue?  As\n‘Mondstadt’ to the ear: like a sweet dream of freedom.  And what are the fruits\nthat went into the brew?  An explorer’s courage, a love tender and true.” “A\ndefender’s will, strong as yesteryear, Joining the thousand winds in a song of\ngood cheer, Turning sour into sweet, bitter notes fade away, As we wait, wait\nfor a windier day.” “Pray tell, what treasure does this barrel hold?  ‘Tis\nwheat’s greatest triumph, the true liquid gold.  As it flows from the\nkeg, what sound drifts by?  Wind chimes in the boundless, immemorial\nsky.” “We raise up our glasses, and voices in song, As we wait, wait\nfor the wind to sing along.  Where do we turn once the thousand winds\ntake flight?  To the tales of the lyre, to the sweet dream of\ntonight.”\n\n\n“You know, no two waves are ever the same…”\n\n\n“The sun nurtures many good things, but it can’t do anything about the problems\nlurking in the shadows.”\n\n\n“The ancient writers once emphasized the virtues of “edification through\nimmersion” — that is to say that human temperament and character can be\naffected by being steeped in aromas and lovely scents.”\n\n\n“Dear young master, you still have a long life ahead of you. There shall be\nplenty of opportunities to drink — in the future, that is.”\n\n\n“The flesh resides in society, while the heart yearns for the natural world…\nSuch has been the way of the Kaedeharas for many generations.”\n\n\n“To me, what is past is gone. Everything in the world is guided by its own\nrules, and as for people… we can never relive the past.”\n\n\n“If you ever grow tired of this tedious life, just drop everything and go off\non a journey, see the world. Remember, Kazuha, don’t let yourself get tied down\nin life.”\n\n\n“I yearn to hear the song of nightingale, my patient ears ready to attend. A\nveil of mist obscures the western skies, into its midst a silver moon\ndescends.”\n\n\n“Life is a long journey, and that’s why I must travel far and wide.”\n\n\n“Ambition is our power in its rawest form. We cannot live without it.”\n\n\n“Old things often carry around some form of regret.”\n\n\n“But that’s okay. Partings produce reunions, if not at home then in a distant\nland.”\n\n\n“When the heart is clear, the world is too. And when the heart is unladen, the\nsame is true.”\n\n\n“Only when you witness my whole story does it become truly consigned to\nhistory.”\n\n\n“What really matters in life is not how strict we are with ourselves, but the\nconnections we make along the way. There’s no future for those who linger on\nthe past.”\n\n\n“I often travel during storms Which means my eyes are often blinded by the rain\nMany times, I couldn’t even see what was right in front of me One day, I\nfinally reached the top of the mountain I looked out with the clouds beneath my\nfeet And only the gentle breeze murmuring in my ears The highest mountain is a\nclear and enlightened heart Here, there is no self, no hatred, no regrets, and\nno desires Let’s embark on a journey, for I am the breeze We will meet again,\nno matter how far along the road Life has just begun, and maybe the whole world\ncan be my home”\n\n\n“I wander like the autumn leaves that float to the mountains and seas afar.”\n\n\n“If an astrologist thinks that their arts can solve all problems, they will be\nforsaken by the starry ether.  Mona: Their divinations will lose the power to\nguide, and will not be able to pierce the fog of the unknown before them.\nPrinciples are principles.”\n\n\n“Haste is indeed a normal part of life, yes… but as we often put it, even a\nraging river must sometimes flow peacefully through a serene stream.”\n\n\n“The forest will remember. No good thing will ever fade away, and all\nsuffering will come to nourish something beautiful…”\n\n\n“The battlefield is a treacherous place. Every opportunity you take, you put\neverything on the line for. If you fear sacrifice and failure, you can never\nbe victorious.”\n\n\n“Ningguang: Breakfast sets the tone for the rest of the day. You can’t\ncompromise on it.  Ningguang: If you wake up to the same monotonous meal each\nday, you will start to feel fatigued even before you start working.”\n\n\n“Times flies and the good years slip away easily.”\n\n\n“Sorry, sorry! It’s just that good conversation can be as fleeting as\nfireworks, sometimes, you know? So when I’m in the mood and I’ve got a lot to\nsay, I just have to get it all out there in one go and leave no regrets!”\n\n\n“Yeah, no matter how close you and your friends are, there’s always going to\nbe some distance after being separated for a long time. But as soon as the\nfireworks lit up the sky, it’d instantly take us right back to our childhood,\nand we’d be chatting away like in the old days.”\n\n\n“Raiden Shogun: My form is a symbol of supreme majesty, in which has been\nvested power over all the realm. It is the cohesive embodiment of all that\nconstitutes the “Raiden Shogun.” Raiden Shogun: It inherits Ei’s pain — the\npain of inevitable loss that comes as she moves forward. So too does it inherit\nher determination to reach eternity.  Raiden Shogun: Every action undertaken is\nfor the sake of resisting erosion.  Raiden Shogun: Determination, courage,\nlove, hatred… All of these will be degraded and distorted by the\nincessant flow of time.  Raiden Shogun: Only rules shall remain\nconstant for eternity.”\n\n\n“Change will come to Inazuma, and with it, new possibilities. This will take\ntime, but eventually, the future will bring healing to the scars of the past.”\n\n\n“The people’s sacrifice has always caused me immense pain, but in dwelling on\nthe tragedy, I overlooked their splendor… The grief blinded me to how\nbrightly they shone in their final moments.”\n\n\n“Jean: Don’t worry about it too much. The more flustered you become, the less\nlikely you are to find it.  Jean: Pay attention to what you see in your\nperipheral vision, and you might just stumble upon what you’re looking for.”\n\n\n“Stop cooping yourself up in your cabin. Get some of that sea breeze, it’ll do\nyou some good.”\n\n\n“When people see the object of their dreams, how many are really able to\ncontrol their desire and follow the contract…?”\n\n\n“Drifting seeds are destined to flourish on nice soil… same for Nara! One\nday, the wind in Nara’s hearts will stop, halting their steps.”\n\n\n“Don’t worry, no memories will be lost! The forest will remember, just like the\nrivers emptying into the sea, before being turned into the rain splashing down\nunto the earth.”\n\n\n“The adepti leave the human world, find somewhere to go be a hermit, and then\nthey research and invent all these amazing things…”\n\n\n“Yep! Work hard, play hard, and rest even harder. The two of you may need even\nmore rest than you’d expect!”\n\n\n“Eternity stretches things out over a long time. But each moment within it\nbecomes all the more fragile.”\n\n\n“A glutton, this Paimon is. A bite on the mushroom today, another bite\ntomorrow, and a hundred days later, there shall be no Paimon left in Paimon,\nbut only mushrooms.”\n\n\n“A helping hand must see things to the end.”\n\n\n“May no regrets linger in the night bygone. May all shadows fade ere the dawn\nto come.”\n\n\n“Xamaran: …Ignorance might be a blessing, and knowledge might bring forth\ncalamity…  Xamaran: …Only that which graced with the ambrosia of knowledge\ncan flourish into the strong…  Xamaran: …Only that which has been tested\nwith the desolation of wandering can brave the wilderness…”\n\n\n“Faith doesn’t ask for anything in return though, does it?”\n\n\n“Dainsleif: It’s just my opinion, but a word of advice: Always be on your guard\nwhen around gods.  Dainsleif: You shouldn’t place too much trust in them. But\nat the same time, don’t go too far in the opposite direction… Don’t go trying\nto overthrow them, or hunt them down.”\n\n\n“The ocean is good company… I hope we can all find some peace and happiness\nwhile we’re here.”\n\n\n“Reply: “Instant gratification has no place in the aesthetics of a wicked\ndragon! First comes expectation, and then comes patience! Once a dragon’s\nyearning for golden dreams has reached its peak…” Reply: “…Ah, then would\nthe taste of those dreams not be splendid beyond compare?”\n\n\n” Venti: It is people’s shared will that brings them onto the same page. And\nsurely, it is the wind of freedom that brought us together.  Venti: It comes\nfrom the end of the journey, the edge of the world, the depths of our heart. It\nis ceaseless.”\n\n\n“Who was it that stroked your bloodied, determined visage By stream flowing\nsmall By boulder standing large” Venti: “Who was it that embraced your weary\nyet noble soul In dreams deep In skies soaring” Venti: “Dear friend I am\nleading you by the hand Into the night where lanterns shine bright” Venti: To\ntell you a tale of freedom and dreams The tale of where this festival begins”\n\n\n“Those who live by the sword know to listen to the voice of their blade.\n…Hehe, she’s nagging me to get on with it again.”\n\n\n“Warriors make friends best when their swords clash, rather than when they talk\naround a table.”\n\n\n“Truly fascinating. The harder they try to silence the situation, the greater\nthe chaos that erupts…”\n\n\n“Pale flame, lay waste to this frozen shell and witness my suffering. In fires\nof sin and retribution, your soul will be incinerated!”\n\n\n“In the world of researchers, there’s nothing more painful than writing a\npaper. If there is, it’ll be getting stuck writing a paper.”\n\n\n“But as a wandering samurai, I find meaning in traveling and the sprawling\nbeauty of nature that lies along the way, while still retaining the “warrior\nway” in my heart.”\n\n\n“The part of your journey that lies after the storm may well prove to be the\nmost arduous.”\n\n\n“Arana: Arana made little Nara sleep and dream, then removed the bad stuff in\ntheir dreams. So when they grow, just like saplings growing into big trees,\nthey won’t become bad.  Arana: Otherwise, little Nara would be afraid. If\nalways afraid, they would believe in the power of “fear,” and grow up to be\nscary bad Nara.”\n\n\n“The forest will remember. No good thing ever fades away. All pains will become\nthe nourishment of something beautiful…”\n\n\n“Press forward and sing, even if the path is filled with flame…”\n\n\n“There are those who no longer dream and thus can no longer see us, those who\nhave accepted fate and stopped searching for the way forward, and those who\nwallow in fear and pain. Their hearts have hardened, unable to get help from\nthese flowers.”\n\n\n“Arama: The vegetation that once covered it had all died and been reduced to\ngrains of sand. Just as Marana whispered: All good things must vanish.  Arama:\nBut you should know that as dark as night is, the stars still shine and the sun\nwill still rise. Death longs to dominate all, but life will not fade.  Arama:\nJust like the yellow leaves that fall down shall nourish Cuihua Trees, their\nfruits shall sustain Sumpter Beasts, foxes, and forest boars, while they shall\nfeed Rishboland Tigers in turn. And so, the forest always teems with life.\nArama: Once, the land wat status s polluted with toxic blood. Like Nara,\nAranara too thought that was the end of the world, but now even the memory of\nit is gone.  The forest is even more luxuriant than it was in the time of\nGreater Lord Rukkhadevata.  Arama: In the end, all that remains is beautiful.\nThose who part will come to meet again in Sarva.  Arama: One day, our dreams\nand memories will intertwine and blossom on the numberless branches in Sarva.\nWouldn’t that be great?”\n\n\n“I believe in you. Like the clouds know they have to release rain, the rain\nknows that it must fall onto the ground. Meeting all of you made me realize…\nthat the time has finally come. I’m very happy.”\n\n\n“Alhaitham: The Akademiya firmly believes that all human actions can be\nexplained through logic.  Alhaitham: By sorting and analyzing entered data, the\nAkasha can derive behavioral logic, and predict the actions of those who fit an\nexisting logic model.  Alhaitham: However, at the risk of sounding like an\nadvocate for fallacies, can everyone truly be considered “logical” at all\ntimes?  Alhaitham: Emotions are a part of our behavioral logic. But can you\nguarantee that every experience of the same joy or pain would be equally\nintense? How can our feelings and opinions be predictable down to the letter in\nevery single instance?”\n\n\n“But this also shows that humanity’s worship of gods is a combination of\nblasphemy and exaltation. It’s truly laughable.”\n\n\n“Just think about a sheet of paper… By itself, it holds no meaning. The\ncontent recorded on it is what gives it value.”\n\n\n“Humans are a species that can only find bliss in ignorance.”\n\n\n“That is the so-called “pride” of a scholar. If someone questions their\nacademic facility, they will instantly feign understanding to keep up\nappearances.”\n\n\n“You all can only see the world in your mind, the one you think you know.”\n\n\n“Nahida: To me, everything we perceive in this world, everything we learn, and\neverything that happens to us is considered knowledge.  Nahida: And if it’s a\nform of knowledge, then it can be understood.  Nahida: However, only fate is\nabout that which has yet to occur, so it has always drawn my curiosity.\nNahida: So to me, “fate” is the ultimate knowledge.  Nahida: That’s also why I\nlove observing humans and all the things that happen to them. It all brings me\ngreat satisfaction.  Nahida: And now, at long last, I’m not just an observer\nanymore.  Nahida: I will personally experience my own fate, with you by my\nside. Hehe, isn’t this such a wonderfully exciting thing?”\n\n\n“Don’t worry. The growth of wisdom is like that of a plant — you only need to\nwait quietly for the flower to bloom.”\n\n\n“We all nestle under the great tree of wisdom, peering out to perceive the\nworld.\nFrom the earth, and from the rain, we perceive its wonders until we become a\nwhite bird to perch atop a branch…\nAnd finally snap off the most important leaf.\nOnce upon a time, I alone dreamed in this world.\nIn my dreams, everybody would also dream after they fell asleep.\nWild and wonderful thoughts would emerge from their minds.\nSome tumbled to the ground, and others floated to the sky.\nConnecting all things in the world into one dazzling net.\nAmongst the plethora of worlds were numerous smaller worlds.\nAll of fate finding within the tapestry their brilliant glow.\nI gradually understood that these indescribable and constantly changing things\nAre the most profound things in the world.\nOnly they can completely repel the madness.\nOnly dreams can awaken consciousness from the deepest darkness.\nI’m the one who posed this question, yet also the one who sought a solution\nSaving the world with the dreams of the people used to be my answer.\nAnd now you’ve also found your own answer.\nAnd I shall return all the dreams to the people.\nGoodbye, people of Sumeru.\nMay you be blessed tonight with the sweetest of dreams.”\n\n\n“As a scholar, I respect all possibilities. This has always been my principle\nand is an essential trait as an experimenter.”\n\n\n“The stars have always guided caravans, thieves, soldiers, and travelers who\nget lost in the night. They lead those in the dark out of trouble and back to\nsafety.”\n\n\n” Yae Miko: The bane of our existence: “writer’s block.” It’s your arch-nemesis\nfor life, appearing without warning and inflicting a pain worse than death upon\nthe writer. They sell their souls just to get their muse back.  Paimon:\nSounds awful.  Yae Miko: When this happens, the best thing you can do\nis have a bite to eat and take a proper break.”\n\n\n“Sigh. I learned most of my socializing from books that were beyond my grade\nlevel and as a result had difficulty connecting with my peers.”\n\n\n“Yeah autism makes me not know how to respond to social prompts and adhd\nquickly makes connections out of things so I usually respond with something\nwitty or funny or fun because of it.”\n\n\n“The road to hell is paved with good intentions.”\n\n\n“Nothing vast enters the life of mortals without a curse”\n\n\n“If you are not paying for the product, then you are the product.”\n\n\n“There are only two industries that call their customers users - illegal drugs\nand software”\n\n\n“Researchers are like children, always re-discovering things that are already\nknown and make a big deal out of it.”\n\n\n“Are you the kind of guy who is so hung up on being nice he never really thinks\nabout other people”\n\n\n“Like did you ever tell anyone how you felt did you ever ask them not to do\nsomething did you explain what was wrong and try to help resolve it?  No\noffense but it kind of seems like you always just expected people to know how\nyou felt and to treat you right just because you were nice.  Another brief\npause filled the space between them I mean I just don’t like arguing or being\nmean or confrontational for no reason especially when people should kind of\nalready know what’s right.  Yeah but how does anyone know of a problem if the\nproblem is hidden from them and why would anyone want to solve it if it\nbenefits them and the other person doesn’t seem to care.  I don’t think\nniceness is always kind because kindness is not quietness, submissiveness or\nself-surrender.  I think it’s the willingness to confront and deal with others\nand issues honestly and fairly for everyone’s benefit even when it’s difficult\nor uncomfortable for you”\n\n\n“To be autistic with ADHD is a huge contradiction. What one part of me wants\nthe other cannot bear. Solitude, separation, quiet and stability, community,\nconnection, busyness, and change. I have to try to find a way to balance the\nright amount of each or I’ll be on the path to falling apart again. ”\n\n\n“I don’t think niceness is always kind”\n\n\n“Enthusiasm can become self sabotage”\n\n\n“The only gods out here are the odds”\n\n\n“The muse is shy. She wants to be seen but not stared at. If you expect too\nmuch, its like a date and she just wont show up.”\n\n\n“It’s said that the gods favor fools as they were more entertaining to watch.”\n\n\n“You can’t spell self assurance without ass”\n\n\n“We give too much attention to the glorified few claiming what they’ve done\nhave changed the world. There’s some truth to that. But there are many many\nothers, quietly, doing their best in their capacity to make the world a better\nplace, asking very little in return. So at this holiday season, I’d like to say\n\n\nthank you. When building a machine learning model, we can rely on a few\nstrong features, but an ensemble of weak learners can be more performant and\nrobust, but each one of them don’t necessarily get the credits they deserve.”\n\n\n“Being enthusiastic is worth 25 IQ points.”\n\n\n“Always demand a deadline. A deadline weeds out the extraneous and the\nordinary. It prevents you from trying to make it perfect, so you have to make\nit different. Different is better.”\n\n\n“Don’t be afraid to ask a question that may sound stupid because 99% of the\ntime everyone else is thinking of the same question and is too embarrassed to\nask it.”\n\n\n“A worthy goal for a year is to learn enough about a subject so that you can’t\nbelieve how ignorant you were a year earlier.”\n\n\n“Pros are just amateurs who know how to gracefully recover from their mistakes.\n\n\n“Extraordinary claims should require extraordinary evidence to be believed.”\n\n\n“Don’t take it personally when someone turns you down. Assume they are like\nyou: busy, occupied, distracted. Try again later. It’s amazing how often a\nsecond try works.”\n\n\n“The purpose of a habit is to remove that action from self-negotiation. You no\nlonger expend energy deciding whether to do it. You just do it. Good habits can\nrange from telling the truth, to flossing.”\n\n\n“Promptness is a sign of respect.”\n\n\n“To make something good, just do it. To make something great, just re-do it,\nre-do it, re-do it. The secret to making fine things is in remaking them.”\n\n\n“If you are looking for something in your house, and you finally find it, when\nyou’re done with it, don’t put it back where you found it. Put it back where\nyou first looked for it.”\n\n\n“Separate the processes of creation from improving. You can’t write and edit,\nor sculpt and polish, or make and analyze at the same time. If you do, the\neditor stops the creator. While you invent, don’t select. While you sketch,\ndon’t inspect. While you write the first draft, don’t reflect. At the start,\nthe creator mind must be unleashed from judgement.”\n\n\n“You are what you do. Not what you say, not what you believe, not how you vote,\nbut what you spend your time on.”\n\n\n“Be prepared: When you are 90% done any large project (a house, a film, an\nevent, an app) the rest of the myriad details will take a second 90% to\ncomplete.”\n\n\n“Before you are old, attend as many funerals as you can bear, and listen.\nNobody talks about the departed’s achievements. The only thing people will\nremember is what kind of person you were while you were achieving.”\n\n\n“For every dollar you spend purchasing something substantial, expect to pay a\ndollar in repairs, maintenance, or disposal by the end of its life.”\n\n\n“Anything real begins with the fiction of what could be. Imagination is\ntherefore the most potent force in the universe, and a skill you can get better\nat. It’s the one skill in life that benefits from ignoring what everyone else\nknows.”\n\n\n“On vacation go to the most remote place on your itinerary first, bypassing the\ncities. You’ll maximize the shock of otherness in the remote, and then later\nyou’ll welcome the familiar comforts of a city on the way back.”\n\n\n“When you get an invitation to do something in the future, ask yourself: would\nyou accept this if it was scheduled for tomorrow? Not too many promises will\npass that immediacy filter.”\n\n\n“Art is in what you leave out.”\n\n\n“Rule of 7 in research. You can find out anything if you are willing to go\nseven levels. If the first source you ask doesn’t know, ask them who you should\nask next, and so on down the line. If you are willing to go to the 7th source,\nyou’ll almost always get your answer.”\n\n\n“Most really amazing or great things are done by people doing them for the\nfirst time.”\n\n\n“The universe is conspiring behind your back to make you a success. This will\nbe much easier to do if you embrace this pronoia.”\n\n\n“It’s not an apology if it comes with an excuse. It is not a compliment if it\ncomes with a request”\n\n\n“Only imperfect beings can make art because art begins in what is broken.”\n\n\n“If someone is trying to convince you it’s not a pyramid scheme, it’s a pyramid\nscheme.”\n\n\n“The reward for good work is more work.”\n\n\n“The foundation of maturity: Just because it’s not your fault doesn’t mean it’s\nnot your responsibility.”\n\n\n“You are only as young as the last time you changed your mind.”\n\n\n“The worst evils in history have always been committed by those who truly\nbelieved they were combating evil. Beware of combating evil.”\n\n\n“Don’t loan money to a friend unless you are ready to make it a gift”\n\n\n“If you can’t tell what you desperately need, it’s probably sleep.”\n\n\n“When playing Monopoly, spend all you have to buy, barter, or trade for the\nOrange properties. Don’t bother with Utilities.”\n\n\n“If you borrow something, try to return it in better shape than you received\nit. Clean it, sharpen it, fill it up”\n\n\n“To quiet a crowd or a drunk, just whisper.”\n\n\n“You are given the gift of life in order to discover what your gift in life is.\nYou will complete your mission when you figure out what your mission is. This\nis not a paradox. This is the way.”\n\n\n“Don’t treat people as bad as they are. Treat them as good as you are.”\n\n\n“We are not bodies that temporarily have souls. We are souls that temporarily\nhave bodies.”\n\n\n“If your goal does not have a schedule, it is a dream.”\n\n\n“The greatest breakthroughs are missed because they look like hard work.”\n\n\n“People can’t remember more than 3 points from a speech.”\n\n\n“Many backward steps are made by standing still.”\n\n\n“You don’t marry a person, you marry a family.”\n\n\n“When making something, always get a few extras — extra material, extra parts,\nextra space, extra finishes. The extras serve as backups for mistakes, reduce\nstress, and fill your inventory for the future. They are the cheapest\ninsurance.”\n\n\n“To combat an adversary, become their friend.”\n\n\n“Take one simple thing — almost anything — but take it extremely seriously, as\nif it was the only thing in the world, or maybe the entire world is in it — and\nby taking it seriously you’ll light up the sky.”\n\n\n“Advice like these are not laws. They are like hats. If one doesn’t fit, try\nanother.”\n\n\n“Painting is complete as a distraction. I know of nothing which, without\nexhausting the body, more entirely absorbs the mind.”\n\n\n“This beginning with Audacity, or being thrown into the middle of it, is\nalready a very great part of the art of painting.”\n\n\n“Painting a picture is like trying to fight a battle.”\n\n\n“Painting is the same kind of problem as unfolding a long, sustained\ninterlocked argument… It is a proposition commanded by a single unity of\nconception.”\n\n\n“Go out into the sunlight and be happy with what you see.”\n\n\n“We cannot aspire to masterpieces. We may content ourselves with a joy ride in\na paint-box. And for this Audacity is the only ticket.”\n\n\n“The profoundest eternal questions are met only with a boundless and eternal\nsilence.”\n\n\n“These things happen, honey. I’ll tell you what I do. I have a little cry, then\nI pick myself up, dust myself off and keep going. The show must go on!”\n\n\n“When you try to be nice to everyone, you’re not being nice to anyone”\n\n\n“Zhongli was corroded by time. Pain and suffering he saw and maybe even did\ngave him wisdom.  Cloud Retainer was most likely hardy and stubborn, but she\ntoo became corroded over time, becoming more like stone, not revealing how she\nfeels to anyone.  Madame Ping always tried to see the beauty in things and\nloved her ways. But over time she realized everything is beautiful in its own\nright. Her wisdom is that she learned to cherish every moment.”\n\n\n“We think of human life as like a lantern that’s lit one minute and\nextinguished the next.”\n\n\n“In the perpetual meantime of a sheltered eternity, most are content to live\nand not to dream. But in the hidden corners where the Gods’ gaze does not fall,\nthere are those who dream of dreaming. Some say a few are chosen and the rest\nare dregs, but I say we humans have our humanity. We will defy this world with\na power from beyond.”\n\n\n“Though flowers bloom yet always fade, but people go yet return another day. I\njust knew that you’d be back to see me before long, hehe.”\n\n\n“Truth be told, the older you get, the more time you have on your hands, and\nthe more you start contemplating the past… I do wonder how my old friends\nback in the mountains are getting on..”\n\n\n“For us desert people, this is an inescapable fate. We come from the soil and\nreturn to the sand. It is a simple matter of being born and dying.”\n\n\n“Pitiful child… How can one envisage paradise’s shape without ever having\nfeasted one’s eyes upon it?”\n\n\n“O stars high above the wasteland, O nightingales weary from the day, It’s\ntime to take off your crowns of roses, Cleanse yourself with wine made from\ngrapes.  Sleep, sleep.  The eternal oasis welcomes the lonely wanderer, And\nhere the crisp springs flow, And the memories are forever sweet.  Sleep, sleep.\nYon golden slumber summons thee, wandering sand, Drink not that bitter salt\nwater, For the sorrows of tomorrow have gone away.”\n\n\n“A little get-together between friends, sipping the finest tea, and watching\nlanterns float into the sky. Bidding farewell to the past, and embracing the\npresent with joy.”\n\n\n“The same truth will sound different coming from different people. As more bear\nwitness to a story, feelings and interpretations expand in variety too.”\n\n\n“Madame Ping: Times change, and the music enjoyed by the youngsters of today\nis, no doubt, very different from the tunes I was accustomed to in my youth.\nNevertheless, all fine things in life can be appreciated.”\n\n\n“The sky you yearn for is yet another abyss”\n\n\n“Sometimes the only way to experience the beauty of things is to think of them\nin a beautiful way.”\n\n\n“Only when we hold our breath and try to keep all the oxygen in, do we\nsuffocate.”\n\n\n“Do not seek to follow in the footsteps of the wise; seek what they sought.”\n\n\n“The truth doesn’t change according to your ability to stomach it.”\n\n\n“If you tidy up in one shot, rather than little by little, you can dramatically\nchange your mind-set”\n\n\n“Don’t aim for perfection. Start off slowly and discard just one item a day.”\nWhat lovely words to ease the hearts of those who lack confidence in their\nability to tidy.”\n\n\n“Many people get the urge to clean up when under pressure, such as just before\nan exam. But this urge doesn’t occur because they want to clean their room. It\noccurs because they need to put “something else” in order. Their brain is\nactually clamoring to study, but when it notices the cluttered space, the focus\nswitches to “I need to clean up my room.” The fact that the tidying urge rarely\ncontinues once the crisis is over proves my theory. Once the exam has ended,\nthe passion poured into cleaning the previous night dissipates and life returns\nto normal. All thought of tidying is wiped from the person’s mind. Why? Because\nthe problem faced—that is, the need to study for the exam—has been “tidied\naway.”\n\n\n“When a room becomes cluttered, the cause is more than just physical. Visible\nmess helps distract us from the true source of the disorder. The act of\ncluttering is really an instinctive reflex that draws our attention away from\nthe heart of an issue.”\n\n\n“I have a habit of trying to categorize everything, probably because I have\nspent so much time pondering how to organize”\n\n\n“I came to the conclusion that it makes far more sense to categorize people by\ntheir actions rather than by some generalized personality trait.”\n\n\n“It’s easy to get rid of things when there is an obvious reason for doing so.\nIt’s much more difficult when there is no compelling reason”\n\n\n“focusing solely on throwing things away can only bring unhappiness. Why?\nBecause we should be choosing what we want to keep, not what we want to get rid\nof.”\n\n\n“Repetition and wasted effort can kill motivation, and therefore it must be\navoided.”\n\n\n“The process of deciding what to keep and what to discard will go much more\nsmoothly if you begin with items that are easier to make decisions about. As\nyou gradually work toward the harder categories, you will be honing your\ndecision-making skills. Clothes are the easiest because their rarity value is\nextremely low. Photographs and letters, on the other hand, not only have a high\nsentimental value but also are one of a kind; therefore, they should be left\nuntil last. This is true for photographs, in particular, because they tend to\nturn up at random while sorting through other categories and in the most\nunexpected places, such as between books and papers. The best sequence is this:\nclothes first, then books, papers, komono (miscellany), and lastly, mementos.”\n\n\n“There’s nothing wrong with tidying. However, it’s extremely stressful for\nparents to see what their children discard. The sheer volume of the pile can\nmake parents anxious about whether their children can survive on what’s left.\nIn addition, despite knowing that they should rejoice at their child’s\nindependence and maturity, parents can find it very painful to see clothes,\ntoys, and mementos from the past on the rubbish heap, especially if they are\nthings they gave to their child. Keeping your garbage out of sight is\nconsiderate. It also protects your family from acquiring more than they need or\ncan enjoy.”\n\n\n“The urge to point out someone else’s failure to tidy is usually a sign that\nyou are neglecting to take care of your own space.”\n\n\n“There are two reasons why younger sisters tend to collect clothes they don’t\nreally like. One is that it’s hard to get rid of something received from\nfamily. The other is that they don’t really know what they like, which makes it\nhard to decide whether they should part with it. Because they receive so much\nclothing from others, they don’t really need to shop and therefore they have\nless opportunity to develop the instinct for what really inspires joy.”\n\n\n“You’re right. There is still a little kid inside me, trembling with anxiety,\nunable to be loved. And he is pleading with me not to ignore him anymore. All\nthis time, I made myself too busy worrying about the opinions of others while\nsuppressing the inner wound from the past. I need to believe that I am worthy\nof love for who I am.”\n\n\n“Even if you never achieve anything big and significant, to me, your existence\nalone is already enough.”\n\n\n“use them whenever you get the chance. Special moments are not separate from\nour everyday lives. When you make use of something special, it makes the moment\nspecial.”\n\n\n“People sometimes express their longing through hate.”\n\n\n“Lengthy deliberation often leads to a terrible decision.” If you think and\nworry too much before doing something, “your boat goes to the mountain instead\nof the ocean.” Now and then it is necessary”\n\n\n“Being alone makes the world pause for a moment and helps to restore harmony.”\n\n\n“When someone you love is in pain, the most meaningful gift you can give is\nyour kind presence.”\n\n\n“We live longer now not because we do not get sick, but because we have learned\nto manage our illness.”\n\n\n“Some people come into our lives and quickly go. Some stay for a while and\nleave footprints on our hearts, and we are never, ever the same.”\n\n\n“The greatest gift that parents can give their child is to be happy themselves.\nIf the parents are happy, then the child can grow up into a happy and confident\nadult. But if the parents are not happy, then the child can feel worthless—\nunable to make his parents happy no matter what. *”\n\n\n“Look back and see if you deluded yourself into believing that being obsessed\nwith your children was a sacrifice. And consider whether your “sacrifice” did\nnot rob your children of the opportunity to learn for themselves”\n\n\n“Children want to admire their parents. You won’t win their admiration by being\noverprotective.”\n\n\n“Do not lose your grip on the reins of your own life and allow yourself to be\ndragged around by someone else.”\n\n\n“If we combine painful memories, the need for attention, and pride, the\nrelationship can easily be ruined.”\n\n\n“Love needs no reason other than love itself”\n\n\n“When your self-esteem hits rock bottom, say to yourself: “To my family and\nclose friends, I’m just as precious as I’ve always been. I’m still capable of\ndoing good in the world; a few people who don’t really know me don’t get to\ndecide what I’m worth. In time, I believe I’ll meet different people who will\nvalue me and my abilities.”\n\n\n“Though it comes from a good place, doing what you think someone needs can be\nthe seed of wanting to control them, to make them a certain way to please\nyourself.”\n\n\n“With a little planning, you can continue to enjoy your life while looking\nafter someone close to you. Sacrificing yourself completely won’t be good in\nthe long run,”\n\n\n“Change will last longer when it’s not forced but when it comes about because\nthey have been convinced of its need.”\n\n\n“If we think of the child as a stranger, we focus on the inconvenience to\nourselves, but if we think of the child as a family member, we become\nmerciful,”\n\n\n“If you take home a cat and care for it, even one that’s been abandoned and is\ndirty, it won’t be long before it becomes the cutest cat in the world.”\n\n\n“MAYBE YOU’VE HEARD it said that each time someone embraces you warmly, your\nlife is extended by one more day?”\n\n\n“Though I am lacking in many ways, I want to be a person who can bring some\nsmall comfort to people, who can give them courage, like a ray of warm\nsunshine. If there is someone who needs a hug from me, I will do it willingly,\ngladly, and as often as they need.”\n\n\n“Because I have experienced pain, I am able to embrace the pain of others.\nBecause I have made mistakes, I am able to forgive others their mistakes. May\nmy suffering become the seed of compassion”\n\n\n“There are those who love you for who you are, and there are those who love you\nfor what you do. There is no change in the love of those who love you for who\nyou are, even when you make a mistake or fail. Such people are your\ntrue friends and family.”\n\n\n“When someone is tired, bring them a cup of herbal tea and just leave them be.”\n\n\n“Words can become the seed of reality.”\n\n\n“To a student monk, books are like the bread we eat or the air we breathe.”\n\n\n“I wish to be generous to the younger generation like they were to me.”\n\n\n“Listening openly, patiently, and attentively is one of the most significant\nexpressions of love.”\n\n\n“I think this has to do with the fact that we want someone to listen to what we\nhave to say, even if that someone is the impersonal online world.”\n\n\n“Children are eager to show off their scars because they like to receive loving\nattention from others.”\n\n\n“When we think we already know someone, we stop making an effort to know them\nbetter. When we do not know someone, we make an effort to get to know them.\nLove is the state of not knowing, and of wanting to know more.”\n\n\n“I saw an advertisement saying, “People are like heaters.” Our presence can\nwarm each other. May you be a heater for someone today.”\n\n\n“Big world, some weirdos!”\n\n\n“All that pointless anguish over such a simple task; I could have done it\nwithout getting so worked up.”\n\n\n“WHAT DISTRESSES US IS LESS the circumstances we find ourselves in and more the\nenergy we expend in resisting them. Once we actually do the work, we are often\nsurprised that it was not as hard as we imagined it to be. But when we resist,\nwe become preoccupied by an endless cycle of negative thoughts, and in turn\nfeel harried and stressed.”\n\n\n“A person’s behavior might not be motivated by any particular thought or\nfeeling,”\n\n\n“In this fast-paced world, where so much gets done immediately, when you have\nto wait long enough for anticipation to build, the moment with that person will\nbe very special.”\n\n\n“While love lets the other person be, obsession wants control.”\n\n\n“In other words, a feeling of disappointment is like a warning light, telling\nme that if I don’t do something about it, the relationship could fail.”\n\n\n“Mastering more skills means that you will have a smoother road than the\nothers”\n\n\n“So, Lang Ga and Ge both agreed on the theory that you should keep it going\neven when being yelled at like hell. You should grasp the opportunity and try\nto learn as much as possible. It’s best to absorb all your master’s skills!”\n\n\n“When you want to learn things from someone, you must acquire many qualities,\nand having no sense of shame must be the most important quality of yours.”\n\n\n“Old Ke said that every stone had its own story, and even the most excellent\nstoneware crafter might not be able to understand all stones’ stories.”\n\n\n“After you catch something, you have to examine it first to see if it’s ill.\nFor example, that animal we caught earlier had uncolored eyes and bald spots in\nhis fur. If you cut it open, you would smell a funny smell from its meat. Even\nif the prey lived, it wouldn’t be alive for long. Any warrior would share its\nfate if he ate its meat.”\n\n\n“There was a saying, that the monkey reigns in the mountains when the tiger is\nabsent.”\n\n\n“All earth returns to the mansion while all water belongs to the gully. Insects\ndo not labor, by the blessing of nature……”\n\n\n“Altering people’s minds should always start with kids”\n\n\n“Shao Xuan had heard from someone that the earliest form of humans showing\ntheir feelings was through body movements and dancing gestures. Dance and moves\nwere also a certain language of people expressing their passions and desires.”\n\n\n“They say that genius is an infinite capacity for taking pains,”\n\n\n“One’s ideas must be as broad as Nature if they are to interpret Nature,”\n\n\n“I ought to know by this time that when a fact appears to be opposed to a long\ntrain of deductions, it invariably proves to be capable of bearing some other\ninterpretation.”\n\n\n“It’s interesting how a random event can change our lives in ways that would be\nimpossible to imagine, isn’t it?”\n\n\n“After Susie had been diagnosed, that excitement had dulled, but he’d soldiered\non, somehow managing to continue to be all things to all people—to their\ndaughter, to her, to the other kids and parents. The question of what he’d left\nfor himself sometimes kept her awake at night.”\n\n\n“Sweet or not, it’s water from home”\n\n\n“When all phenomena are reduced to truth they follow a single pattern; Like the\nTathagatha reaching nirvana under the two trees.”\n\n\n“If you try to ask about the dhyana Chapter 8 99 Journey to the West Or\ninvestigate the innumerable You will waste your life and achieve nothing.”\n\n\n“Polishing bricks to make mirrors, Or piling up snow to turn it into grain−−\nHowever many years have you wasted like that? A hair can contain an ocean, A\nmustard−seed can hold a mountain, And the golden Kasyapa only smiles. When you\nare awakened you will surpass the Ten Stages and the Three Vehicles, And stop\nthe four kinds of birth and the six types of reincarnation.”\n\n\n“The meditating heart shines like the moon in a thousand rivers; The true\nnature embraces ten thousand miles of sky.”\n\n\n“If you want to have a future, don’t do anything with no future in it?”\n\n\n“Brother Li,” said Zhang Shao, “it seems to me that people who struggle for\nfame kill themselves for it; those who compete for profit die for it; those who\naccept honors sleep with a tiger in their arms; and those who receive imperial\nfavours walk around with snakes in their sleeves. Taking all in all, we are\nmuch better off living free among our clear waters and blue hills: we delight\nin our poverty and follow our destinies.”\n\n\n“An axe well honed on rock is sharper than a spear.”\n\n\n“He who does good in secret can always prolong his life; Heaven looks after the\none who asks no pity.”\n\n\n“There is an Indian story about a grain of salt that wanted to know just how\nsalty the ocean is, so it jumped in and became one with the water of the\nocean.”\n\n\n“This is, because that is. This is not, because that is not. This is like this,\nbecause that is like that.”\n\n\n“The wealth of one society is made of the poverty of the other.”\n\n\n“It was just a computer. If you didn’t keep that thought firmly in your mind it\nwas too easy to start thinking of it as human, and that was the first step\ntoward forgetting.”\n\n\n“He had created in passion, and passion isn’t sane. If it were, nobody would\never have children. After all, while the outcome of that passion might be the\ndoctor who cures a dreaded disease, it might also be the tyrant who despoils a\ncontinent or the criminal who murders for pleasure. In the grip of that passion\nno one could know and few bothered to care. They cared only about the passion,\nwere driven by it and it alone, and if it drove them to ruin it would not\nmatter; they would follow it again, into death for themselves and everybody\naround them if that was where it led. Because passion isn’t sane.”\n\n\n“The Buddha is silent on that, and his silence implies that you’ll have to find\nout for yourself. He uses a negative definition so that the mind cannot make it\ninto something to believe in or into a superhuman accomplishment, a goal that\nis impossible for you to attain.”\n\n\n“The word God has become a closed concept. The moment the word is uttered, a\nmental image is created, no longer, perhaps, of an old man with a white beard,\nbut still a mental representation of someone or something outside you, and,\nyes, almost inevitably a male someone or something.”\n\n\n“The compulsive thinker, which means almost everyone, lives in a state of\napparent separateness, in an insanely complex world of continuous problems and\nconflict, a world that reflects the ever- increasing fragmentation of the\nmind.”\n\n\n“A belief may be comforting. Only through your own experience, however, does it\nbecome liberating.”\n\n\n“The beginning of freedom is the realization that you are not the possessing\nentity the thinker. Knowing this enables you to observe the entity. The moment\nyou start watching the thinker, a higher level of consciousness becomes\nactivated”\n\n\n“One day you may catch yourself smiling at the voice in your head, as you would\nsmile at the antics of a child. This means that you no longer take the content\nof your mind all that seriously, as your sense of self does not depend on it.”\n\n\n“Every time you create a gap in the stream of mind, the light of your\nconsciousness grows stronger.”\n\n\n“Thinking and consciousness are not synonymous. Thinking is only a small aspect\nof consciousness. Thought cannot exist without consciousness, but consciousness\ndoes not need thought.”\n\n\n“An emotion usually represents an amplified and energized thought pattern, and\nbecause of its often overpowering energetic charge, it is not easy initially to\nstay present enough to be able to watch it. It wants to take you over, and it\nusually succeeds unless there is enough presence in you.”\n\n\n“Basically, all emotions are modifications of one primordial, undifferentiated\nemotion that has its origin in the loss of awareness of who you are beyond name\nand form. Because of its undifferentiated nature, it is hard to find a name\nthat precisely describes this emotion. “Fear” comes close, but apart from a\ncontinuous sense of threat, it also includes a deep sense of abandonment and\nincompleteness. It may be best to use a term that is as undifferentiated as\nthat basic emotion and simply call it “pain.” One of the main tasks of the mind\nis to fight or remove that emotional pain, which is one of the reasons for its\nincessant activity, but all it can ever achieve is to cover it up temporarily.\nIn fact, the harder the mind struggles to get rid of the pain, the greater the\npain. The mind can never find the solution, nor can it afford to allow you to\nfind the solution, because it is itself an intrinsic part of the “problem.”\n\n\n“Pleasure is always derived from something outside you, whereas joy arises from\nwithin. The very thing that gives you pleasure today will give you pain\ntomorrow, or it will leave you, so its absence will give you pain. And what is\noften referred to as love may be pleasurable and exciting for a while, but it\nis an addictive clinging, an extremely needy condition that can turn into its\nopposite at the flick of a switch.”\n\n\n“If we address stories as archaeological sites, and dust through their layers\nwith meticulous care, we find at some level there is always a doorway. A\ndividing point between here and there, us and them, mundane and magical. It is\nat the moments when the doors open, when things flow between the worlds, that\nstories happen.”\n\n\n“I say “amateur” only because it was fashionable for wealthy men to refer to\ntheir passions in this dismissive way, with a little flick of their fingers, as\nif admitting to a profession other than moneymaking might sully their\nreputations.”\n\n\n“On the third day, my room became a cell, which became a cage, which became a\ncoffin, and I discovered the very deepest fear that swam through my heart like\neels in undersea caves: to be locked away, trapped and alone.”\n\n\n“His expression as he surveyed me made me think of old-timey illustrations of\nGod: severely paternal, bestowing the kind of love that weighs and measures\nbefore it finds you worthy. His eyes were stones, pressing down. “You are going\nto mind your place and be a good girl.”\n\n\n“It’s stupid to think things like that. It just gives you this hollow, achy\nfeeling between your ribs, like you’re homesick even though you’re already\nhome, and you can’t read your magazine anymore because the words are all warped\nand watery-looking.”\n\n\n“Words and their meanings have weight in the world of matter, shaping and\nreshaping realities through a most ancient alchemy. Even my own writings—so\ndamnably powerless—may have just enough power to reach the right person and to\ntell the right truth, and change the nature of things.”\n\n\n“Second, my long years of research have taught me that all stories, even the\nmeanest folktales, matter. They are artifacts and palimpsests, riddles and\nhistories. They are the red threads that we may follow out of the labyrinth.”\n\n\n“You see, doors are many things: fissures and cracks, ways between, mysteries\nand borders. But more than anything else, doors are change. When things slip\nthrough them, no matter how small or brief, change trails them like porpoises\nfollowing a ship’s wake. The change had already taken hold of Adelaide Lee, and\nshe could not turn away.”\n\n\n“His books have been translated into 59 languages and published in 150\ncountries. He is also the recipient of numerous prestigious international\nawards, among them the Crystal Award by the World Economic Forum, France’s\nChevalier de l’Ordre National de la Légion d’Honneur”\n\n\n“What man of you, having an hundred sheep, if he lose one of them doth not\nleave the ninety and nine in the wilderness, and go after that which is lost,\nuntil he find it”\n\n\n“Buying more had failed to make them happier. In fact, it was entrapping them,\nand they needed to find a new relationship to their possessions, usually by\nthrowing most of them out.”\n\n\n“As the architect Pier Vittorio Aureli writes, the “less is more” attitude can\nbe a form of capitalist exploitation, encouraging workers to produce more while\ngetting by with less, creating more profit for their bosses at the cost of\ntheir own quality of life.”\n\n\n“I began thinking of this universal feeling as the longing for less. It’s an\nabstract, almost nostalgic desire, a pull toward a different, simpler world.\nNot past nor future, neither utopian nor dystopian, this more authentic world\nis always just beyond our current existence in a place we can never quite\nreach. Maybe the longing for less is the constant shadow of humanity’s\nself-doubt: What if we were better off without everything we’ve gained in\nmodern society?”\n\n\n“The minimalist blogger Joshua Becker, an evangelical Christian and author of\nThe More of Less, published in 2016, proposes Jesus as the original minimalist.\nWhen he instructed a rich man to “sell everything you own and give it away to\nthe poor,” the commandment wasn’t about self-sacrifice, according to Becker. It\nmeant that the rich man would be happier without the possessions, so giving\nthem away was a net gain—a kind of minimalist prosperity gospel.”\n\n\n“Minimalism is thus a kind of last resort. When we can’t control our material\nsecurity or life path, the only possibility left is to lower our expectations\nto the point where they’re easier to achieve, which could mean living in a\ntrain car, or a camper van.”\n\n\n“We like to think that we can do without, rough it to prove that we’re not so\nsoft or bound to the past.”\n\n\n“That’s the whole meaning of life,9 isn’t it—just trying to find a place to put\nyour stuff.”)”\n\n\n“Minimalism’s lack of a coherent history is in part due to its nature—it\ninstinctively tends to erase its own background, as if starting anew in each\niteration. If its practitioners admitted to being referential or reviving a\npast tradition, they wouldn’t seem so radically minimal after all.”\n\n\n“Material simplicity will thus likely be manifest in consumption styles that\nare less ascetic … and more aesthetic,” Elgin wrote—in other words, not Spartan\nbut stylized.”\n\n\n“He predicted the social media era’s obsession with curated authenticity, the\nkind that we see displayed on Instagram accounts: “Each person will consider\nwhether his or her level and pattern of consumption fits, with grace and\nintegrity,”\n\n\n“We take pride in the small details that we have actually chosen from our\nlimited options, which might make us feel better about not being able to change\nour circumstances as a whole.”\n\n\n“When workers are separated from the products of their labor and compensated by\nan hourly wage, they can’t find satisfaction in their jobs or the remainder of\nfamily life. Thus they turn to acquiring capital as the only form of\nself-fulfillment. We work only to accumulate stuff and in turn the accumulated\nstuff dominates us, further distancing us from non-commodified things like\nrelationships, joy, and community.”\n\n\n“Labor “is therefore not the satisfaction of a need, but only a means for\nsatisfying needs external to it,”\n\n\n“The less you are,12 the less you express your own life, the more you have,”\nMarx argued, “the greater is your alienated life, the greater is the store of\nyour estranged being.”\n\n\n“Stuff is therefore the enemy of happiness, and not just because it’s crowding\nyour apartment, but because it’s part of this larger alienating system.”\n\n\n“The need for simplicity, taken to an extreme, can wipe function away\nentirely.”\n\n\n“We’re taking advantage of a maximalist assemblage. Just because something\nlooks simple doesn’t mean it is; the aesthetics of simplicity cloak artifice or\neven unsustainable excess.”\n\n\n“Because there’s an infinite amount of things we can now see or know, there are\nalso an infinite number of ways we can discover that we don’t measure up, that\nwe’re not good enough, that things aren’t as great as they could be. And this\nrips us apart inside.”\n\n\n“Wanting positive experience is a negative experience; accepting negative\nexperience is a positive experience.”\n\n\n“The lake was silent for some time. Finally, it said: “I weep for Narcissus,\nbut I never noticed that Narcissus was beautiful. I weep because, each time he\nknelt beside my banks, I could”\n\n\n“I weep for Narcissus, but I never noticed that Narcissus was beautiful. I weep\nbecause, each time he knelt beside my banks, I could see, in the depths of his\neyes, my own beauty reflected.”\n\n\n“And he knew that shepherds, like seamen and like traveling salesmen, always\nfound a town where there was someone who could make them forget the joys of\ncarefree wandering.”\n\n\n“But ever since he had been a child, he had wanted to know the world, and this\nwas much more important to him than knowing God and learning about man’s sins”\n\n\n“When someone sees the same people every day, as had happened with him at the\nseminary, they wind up becoming a part of that person’s life. And then they\nwant the person to change.”\n\n\n“Everyone seems to have a clear idea of how other people should lead their\nlives, but none about his or her own”\n\n\n“What’s the world’s greatest lie?” the boy asked, completely surprised. “It’s\nthis: that at a certain point in our lives, we lose control of what’s happening\nto us, and our lives become controlled by fate. That’s the world’s greatest\nlie.”\n\n\n“there is one great truth on this planet: whoever you are, or whatever it is\nthat you do, when you really want something, it’s because that desire\noriginated in the soul of the universe. It’s your mission on earth.”\n\n\n“The Soul of the World is nourished by people’s happiness. And also by\nunhappiness, envy, and jealousy. To realize one’s Personal Legend is a person’s\nonly real obligation. All things are one. “And, when you want something, all\nthe universe conspires in helping you to achieve it.”\n\n\n“he’s an old man, he’s going to spend a month in Africa. He never realized\nthat”\n\n\n“He never realized that people are capable, at any time in their lives, of\ndoing what they dream of”\n\n\n“People learn, early in their lives, what is their reason for being,” said the\nold man, with a certain bitterness. “Maybe that’s why they give up on it so\nearly, too. But that’s the way it is.”\n\n\n“Treasure is uncovered by the force of flowing water, and it is buried by the\nsame currents”\n\n\n“and for gold and adventure—and for the Pyramids. The boy felt jealous of the\nfreedom of the wind, and saw that he could have the same freedom. There was\nnothing to hold him back except himself.”\n\n\n“Because there is a force that wants you to realize your Personal Legend; it\nwhets your appetite with a taste of success.”\n\n\n“Don’t forget that everything you deal with is only one thing and nothing else.\nAnd don’t forget the language of omens. And, above all, don’t forget to follow\nyour Personal Legend through to its conclusion.”\n\n\n“You cannot trust a man if you don’t know his house.‘”\n\n\n“I can give you,’ said the wisest of wise men. ‘The secret of happiness is to\nsee all the marvels of the world, and”\n\n\n“The secret of happiness is to see all the marvels of the world, and never to\nforget the drops of oil on the spoon.‘”\n\n\n“A shepherd may like to travel, but he should never forget about his sheep.”\n\n\n“I’m like everyone else—I see the world in terms of what I would like to see\nhappen, not what actually does.”\n\n\n“He had learned that there were certain things one shouldn’t ask about, so as\nnot to flee from one’s own Personal Legend”\n\n\n“This candy merchant isn’t making candy so that later he can travel or marry a\nshopkeeper’s daughter. He’s doing it because it’s what he wants to do,”\n\n\n“But the sheep had taught him something even more important: that there was a\nlanguage in the world that everyone understood, a language the boy had used\nthroughout the time that he was trying to improve things at the shop. It was\nthe language of enthusiasm, of things accomplished with love and purpose, and\nas part of a search for something believed in and desired.”\n\n\n“He still had some doubts about the decision he had made. But he was able to\nunderstand one thing: making a decision was only the beginning of things. When\nsomeone makes a decision, he is really diving into a strong current that will\ncarry him to places he had never dreamed of when he first made the decision.”\n\n\n“The closer one gets to realizing his Personal Legend, the more that Personal\nLegend becomes his true reason for being, thought the boy.”\n\n\n“I’ve crossed these sands many times,” said one of the camel drivers one night.\n\n\n“But the desert is so huge, and the horizons so distant, that they make a\nperson feel small, and as if he should remain silent.”\n\n\n“The boy was beginning to understand that intuition is really a sudden\nimmersion of the soul into the universal current of life, where the histories\nof all people are connected, and we are able to know everything, because it’s\nall written there.”\n\n\n“We are afraid of losing what we have, whether it’s our life or our possessions\nand property. But this fear evaporates when we understand that our life stories\nand the history of the world were written by the same hand.”\n\n\n“Everything on earth is being continuously transformed, because the earth is\nalive … and it has a soul. We are part of that soul, so we rarely recognize\nthat it is working for us. But in the crystal shop you probably realized that\neven the glasses were collaborating in your success.”\n\n\n“Because I don’t live in either my past or my future. I’m interested only in\nthe present. If you can concentrate always on the present, you’ll be a happy\nman. You’ll see that there is life in the desert, that there are stars in the\nheavens, and that tribesmen fight because they are part of the human race. Life\nwill be a party for you, a grand festival, because life is the moment we’re\nliving right now.”\n\n\n“Because people become fascinated with pictures and words, and wind up\nforgetting the Language of the World.”\n\n\n“If he pushed forward impulsively, he would fail to see the signs and omens\nleft by God along his path.”\n\n\n“Until then, he had considered the omens to be things of this world. Like\neating or sleeping, or like seeking love or finding a job. He had never thought\nof them in terms of a language used by God to indicate what he should do.”\n\n\n“Don’t be impatient,” he repeated to himself. “It’s like the camel driver said:\n‘Eat when it’s time to eat. And move along when it’s time to move along.‘”\n\n\n“The dunes are changed by the wind, but the desert never changes.”\n\n\n“Isn’t wine prohibited here?” the boy asked “It’s not what enters men’s mouths\nthat’s evil,” said the alchemist. “It’s what comes out of their mouths that\nis.”\n\n\n“Remember that wherever your heart is, there you will find your treasure.\nYou’ve got to find the treasure, so that everything you have learned along the\nway can make sense.”\n\n\n“Camels are traitorous: they walk thousands of paces and never seem to tire.\nThen suddenly, they kneel and die. But horses tire bit by bit. You always know\nhow much you can ask of them, and when it is that they are about to die.”\n\n\n“You must understand that love never keeps a man from pursuing his Personal\nLegend. If he abandons that pursuit, it’s because it wasn’t true love … the\nlove that speaks the Language of the World.”\n\n\n“Men dream more about coming home than about leaving,”\n\n\n“what one finds is made of pure matter, it will never spoil. And one can always\ncome back. If what you had found was only”\n\n\n“If what one finds is made of pure matter, it will never spoil. And one can\nalways come back. If what you had found was only a moment of light, like the\nexplosion of a star, you would find nothing on your return.”\n\n\n“There is only one way to learn,” the alchemist answered. “It’s through action.\nEverything you need to know you have learned through your journey.”\n\n\n“The desert will give you an understanding of the world; in fact, anything on\nthe face of the earth will do that. You don’t even have to understand the\ndesert: all you have to do is contemplate a simple grain of sand, and you will\nsee in it all the marvels of creation”\n\n\n“Listen to your heart. It knows all things, because it came from the Soul of\nthe World, and it will one day return there.”\n\n\n“Why do we have to listen to our hearts?” the boy asked, when they had made\ncamp that day. “Because, wherever your heart is, that is where you’ll find your\ntreasure.”\n\n\n“My heart is a traitor,” the boy said to the alchemist, when they had paused to\nrest the horses. “It doesn’t want me to go on.” “That makes sense,” the\nalchemist answered. “Naturally it’s afraid that, in pursuing your dream, you\nmight lose everything you’ve won.”\n\n\n“Well, then, why should I listen to my heart?” “Because you will never again be\nable to keep it quiet. Even if you pretend not to have heard what it tells you,\nit will always be there inside you, repeating to you what you’re thinking about\nlife and about the world.”\n\n\n“You will never be able to escape from your heart. So it’s better to listen to\nwhat it has to say. That way, you’ll never have to fear an unanticipated blow.”\n\n\n“He lost his fear, and forgot about his need to go back to the oasis, because,\none afternoon, his heart told him that it was happy. “Even though I complain\nsometimes,” it said, “it’s because I’m the heart of a person, and people’s\nhearts are that way. People are afraid to pursue their most important dreams,\nbecause they feel that they don’t deserve them, or that they’ll be unable to\nachieve them. We, their hearts, become fearful just thinking of loved ones who\ngo away forever, or of moments that could have been good but weren’t, or of\ntreasures that might have been found but were forever hidden in the sands.\nBecause, when these things happen, we suffer terribly.”\n\n\n“Tell your heart that the fear of suffering is worse than the suffering itself.\nAnd that no heart has ever suffered when it goes in search of its dreams,\nbecause every second of the search is a second’s encounter with God and with\neternity.”\n\n\n“Every second of the search is an encounter with God,” the boy told his heart.\n\n\n“When I have been truly searching for my treasure, every day has been luminous,\nbecause I’ve known that every hour was a part of the dream that I would find\nit. When I have been truly searching for my treasure, I’ve discovered things\nalong the way that I never would have seen had I not had the courage to try\nthings that”\n\n\n“Because a grain of sand is a moment of creation, and the universe has taken\nmillions of years to create it. “Everyone on earth has a treasure that awaits\nhim,” his heart said. “We, people’s hearts, seldom say much about those\ntreasures, because people no longer want to go in search of them. We speak of\nthem only to children. Later, we simply let life proceed, in its own direction,\ntoward its own fate. But, unfortunately, very few follow the path laid out for\nthem—the path to their Personal Legends, and to happiness. Most people see the\nworld as a threatening place, and, because they do, the world turns out,\nindeed, to be a threatening place.”\n\n\n“So, we, their hearts, speak more and more softly. We never stop speaking out,\nbut we begin to hope that our words won’t be heard: we don’t want people to\nsuffer because they don’t follow their hearts.” “Why don’t people’s hearts tell\nthem to continue to follow their dreams?” the boy asked the alchemist. “Because\nthat’s what makes a heart suffer most, and hearts don’t like to suffer.”\n\n\n“What you still need to know is this: before a dream is realized, the Soul of\nthe World tests everything that was learned along the way. It does this not\nbecause it is evil, but so that we can, in addition to realizing our dreams,\nmaster the lessons we’ve learned as we’ve moved toward that dream. That’s the\npoint at which most people give up. It’s the point at which, as we say in the\nlanguage of the desert, one ‘dies of thirst just when the palm trees have\nappeared on the horizon.‘”\n\n\n“When you possess great treasures within you, and try to tell others of them,\nseldom are you believed.”\n\n\n“Does a man’s heart always help him?” the boy asked the alchemist. “Mostly just\nthe hearts of those who are trying to realize their Personal Legends. But they\ndo help children, drunkards, and the elderly, too.” “Does that mean that I’ll\nnever run into danger?” “It means only that the heart does what it\ncan,” the alchemist said.”\n\n\n“Trust in your heart, but never forget that you’re in the desert. When men are\nat war with one another, the Soul of the World can hear the screams of battle.\nNo one fails to suffer the consequences of everything under the sun.”\n\n\n“And then there were the others, who were interested only in gold. They never\nfound the secret. They forgot that lead, copper, and iron have their own\nPersonal Legends to fulfill. And anyone who interferes with the Personal Legend\nof another thing never will discover his own.”\n\n\n“The sea has lived on in this shell, because that’s its Personal Legend. And it\nwill never cease doing so until the desert is once again covered by water.”\n\n\n“Don’t give in to your fears,” said the alchemist, in a strangely gentle voice.\n\n\n“If you do, you won’t be able to talk to your heart.”\n\n\n“If a person is living out his Personal Legend, he knows everything he needs to\nknow. There is only one thing that makes a dream impossible to achieve: the\nfear of failure.”\n\n\n“Usually the threat of death makes people a lot more aware of their lives.”\n\n\n“What is love?” the desert asked. “Love is the falcon’s flight over your sands.\nBecause for him, you are a green field, from which he always returns with game.\nHe knows your rocks, your dunes, and your mountains, and you are generous to\nhim.” “The falcon’s beak carries bits of me, myself,” the desert said. “For\nyears, I care for his game, feeding it with the little water that I have, and\nthen I show him where the game is. And, one day, as I enjoy the fact that his\ngame thrives on my surface, the falcon dives out of the sky, and takes away\nwhat I’ve created.” “But that’s why you created the game in the first place,”\nthe boy answered. “To nourish the falcon. And the falcon then nourishes man.\nAnd, eventually, man will nourish your sands, where the game will once again\nflourish. That’s how the world goes.” “So is that what love is?” “Yes, that’s\nwhat love is. It’s what makes the game become the falcon, the falcon become\nman, and man, in his turn, the desert. It’s what turns lead into gold, and\nmakes the gold return to the earth.”\n\n\n“The wind has many names. In that part of the world, it was called the sirocco,\nbecause it brought moisture from the oceans to the east. In the distant land\nthe boy came from, they called it the levanter, because they believed that it\nbrought with it the sands of the desert, and the screams of the Moorish wars.\nPerhaps, in the places beyond the pastures where his sheep lived, men thought\nthat the wind came from Andalusia. But, actually, the wind came from no place\nat all, nor did it go to any place; that’s why it was stronger than the desert.\nSomeone might one day plant trees in the desert, and even raise sheep there,\nbut never would they harness the wind.”\n\n\n“You can’t be the wind,” the wind said. “We’re two very different things.”\n“That’s not true,” the boy said. “I learned the alchemist’s secrets in my\ntravels. I have inside me the winds, the deserts, the oceans, the stars, and\neverything created in the universe. We were all made by the same hand, and we\nhave the same soul. I want to be like you, able to reach every corner of the\nworld, cross the seas, blow away the sands that cover my treasure, and carry\nthe voice of the woman I love.”\n\n\n“When you are loved, you can do anything in creation. When you are loved,\nthere’s no need at all to understand what’s happening, because everything\nhappens within you, and even men can turn themselves into the wind. As long as\nthe wind helps, of course.”\n\n\n“This is why alchemy exists,” the boy said. “So that everyone will search for\nhis treasure, find it, and then want to be better than he was in his former\nlife. Lead will play its role until the world has no further need for lead; and\nthen lead will have to turn itself into gold.”\n\n\n“That’s what alchemists do. They show that, when we strive to become better\nthan we are, everything around us becomes better, too.”\n\n\n“A current of love rushed from his heart, and the boy began to pray. It was a\nprayer that he had never said before, because it was a prayer without words or\npleas. His prayer didn’t give thanks for his sheep having found new pastures;\nit didn’t ask that the boy be able to sell more crystal; and it didn’t beseech\nthat the woman he had met continue to await his return. In the silence, the boy\nunderstood that the desert, the wind, and the sun were also trying to\nunderstand the signs written by the hand, and were seeking to follow their\npaths, and to understand what had been written on a single emerald. He saw that\nomens were scattered throughout the earth and in space, and that there was no\nreason or significance attached to their appearance; he could see that not the\ndeserts, nor the winds, nor the sun, nor people knew why they had been created.\nBut that the hand had a reason for all of this, and that only the hand could\nperform miracles, or transform the sea into a desert … or a man into the\nwind. Because only the hand understood that it was a larger design that had\nmoved the universe to the point at which six days of creation had evolved into\na Master Work. The boy reached through to the Soul of the World, and saw that\nit was a part of the Soul of God. And he saw that the Soul of God was his own\nsoul. And that he, a boy, could perform miracles.”\n\n\n“But this payment goes well beyond my generosity,” the monk responded. “Don’t\nsay that again. Life might be listening, and give you less the next time.”\n\n\n“Everything that happens once can never happen again. But everything that\nhappens twice will surely happen a third time.”\n\n\n“No matter what he does, every person on earth plays a central role in the\nhistory of the world. And normally he doesn’t know it.”\n\n\n“But here he was, at the point of finding his treasure, and he reminded himself\nthat no project is completed until its objective has been achieved”\n\n\n“While I was fighting, I heard other people speaking in the name of freedom,\nand the more they defended this unique right, the more enslaved they seemed to\nbe to their parents’ wishes, to a marriage in which they had promised to stay\nwith the other person “for the rest of their lives,” to the bathroom scales, to\ntheir diet, to half-finished projects, to lovers to whom they were incapable of\nsaying “No” or “It’s over,” to weekends when they were obliged to have lunch\nwith people they didn’t even like. Slaves to luxury, to the appearance of\nluxury, to the appearance of the appearance of luxury. Slaves to a life they\nhad not chosen, but which they had decided to live because someone had managed\nto convince them that it was all for the best. And so their identical days and\nnights passed, days and nights in which adventure was just a word in a book or\nan image on the television that was always on, and whenever a door opened, they\nwould say: “I’m not interested. I’m not in the mood.”\n\n\n“We want to think of families as safe havens in a heartless world and of our\nown country as populated by enlightened, civilized people. We prefer to believe\nthat cruelty occurs only in faraway places like Darfur or the Congo. It is hard\nenough for observers to bear witness to pain. Is it any wonder, then, that the\ntraumatized individuals themselves cannot tolerate remembering it and that they\noften resort to using drugs, alcohol, or self-mutilation to block out their\nunbearable knowledge?”\n\n\n“After you have experienced something so unspeakable, how do you learn to trust\nyourself or anyone else again? Or, conversely, how can you surrender to an\nintimate relationship after you have been brutally violated?”\n\n\n“It’s hard enough to face the suffering that has been inflicted by others, but\ndeep down many traumatized people are even more haunted by the shame they feel\nabout what they themselves did or did not do under the circumstances. They\ndespise themselves for how terrified, dependent, excited, or enraged they\nfelt.”\n\n\n“Imagination is absolutely critical to the quality of our lives. Our\nimagination enables us to leave our routine everyday existence by fantasizing\nabout travel, food, sex, falling in love, or having the last word—all the\nthings that make life interesting. Imagination gives us the opportunity to\nenvision new possibilities—it is an essential launchpad for making our hopes\ncome true. It fires our creativity, relieves our boredom, alleviates our pain,\nenhances our pleasure, and enriches our most intimate relationships.”\n\n\n“Without imagination there is no hope, no chance to envision a better future,\nno place to go, no goal to reach.”\n\n\n“After trauma the world becomes sharply divided between those who know and\nthose who don’t.”\n\n\n“We have learned that trauma is not just an event that took place sometime in\nthe past; it is also the imprint left by that experience on mind, brain, and\nbody. This imprint has ongoing consequences for how the human organism manages\nto survive in the present.”\n\n\n“We have discovered that helping victims of trauma find the words to describe\nwhat has happened to them is profoundly meaningful, but usually it is not\nenough.”\n\n\n“For real change to take place, the body needs to learn that the danger has\npassed and to live in the reality of the present.”\n\n\n“The greater the doubt, the greater the awakening; the smaller the doubt, the\nsmaller the awakening. No doubt, no awakening.”\n\n\n“Semrad taught us that most human suffering is related to love and loss and\nthat the job of therapists is to help people “acknowledge, experience, and\nbear” the reality of life—with all its pleasures and heartbreak.”\n\n\n“Scared animals return home, regardless of whether home is safe or\nfrightening.”\n\n\n“At this point, just as with drug addiction, we start to crave the activity and\nexperience withdrawal when it’s not available. In the long run people become\nmore preoccupied with the pain of withdrawal than the activity itself”\n\n\n“We concluded that Beecher’s speculation that “strong emotions can block pain”\nwas the result of the release of morphinelike substances manufactured in the\nbrain. This suggested that for many traumatized people, reexposure to stress\nmight provide a similar relief from anxiety.”\n\n\n“Manipulating a monkey into a lower position in the dominance hierarchy made\nhis serotonin drop, while chemically enhancing”\n\n\n“The social environment interacts with brain chemistry. Manipulating a monkey\ninto a lower position in the dominance hierarchy made his serotonin drop, while\nchemically enhancing serotonin elevated the rank of former subordinates.”\n\n\n“After conducting numerous studies of medications for PTSD, I have come to\nrealize that psychiatric medications have a serious downside, as they may\ndeflect attention from dealing with the underlying issues. The brain-disease\nmodel takes control over people’s fate out of their own hands and puts doctors\nand insurance companies in charge of fixing their problems.”\n\n\n“The brain-disease model overlooks four fundamental truths: (1) our capacity to\ndestroy one another is matched by our capacity to heal one another. Restoring\nrelationships and community is central to restoring well-being; (2) language\ngives us the power to change ourselves and others by communicating our\nexperiences, helping us to define what we know, and finding a common sense of\nmeaning; (3) we have the ability to regulate our own physiology, including some\nof the so-called involuntary functions of the body and brain, through such\nbasic activities as breathing, moving, and touching; and (4) we can change\nsocial conditions to create environments in which children and adults can feel\nsafe and where they can thrive.”\n\n\n“You must make her hell-bent on being with you. Remember, the easier something\nis to get, the less precious it will be in the end”\n\n\n“I just—’ The thing with mental turmoil is that so many things that make you\nfeel better in the short term make you feel worse in the long term. You\ndistract yourself,”\n\n\n“The thing with mental turmoil is that so many things that make you feel better\nin the short term make you feel worse in the long term. You distract yourself,\nwhen what you really need is to know yourself.”\n\n\n“In the early days of my first experience of panic the only things I had taken\naway were booze and cigarettes and strong coffees. Now, though, years later, I\nrealised that a more general overload was the problem. A life overload.”\n\n\n“As Nicholas Kristof pointed out in a 2017 New York Times article, ‘if just\nabout the worst thing that can happen is for a parent to lose a child, that’s\nabout half as likely as it was in 1990.‘”\n\n\n“It sometimes feels as if we have temporarily solved the problem of scarcity\nand replaced it with the problem of excess.”\n\n\n“I want to know if one of the reasons I sometimes feel like I am on the brink\nof a breakdown is partly because the world sometimes seems on the brink of a\nbreakdown.”\n\n\n“I am petrified of where my mind can go, because I know where it has already\nbeen.”\n\n\n“Anxiety, to quote the philosopher Søren Kierkegaard, may be the ‘dizziness of\nfreedom’, but all this freedom of choice really is a miracle.”\n\n\n“We don’t need another world. Everything we need is here, if we give up\nthinking we need everything.”\n\n\n“He who fears he shall suffer, already suffers what he fears.‘”\n\n\n“Ours is a world of nuclear giants and ethical infants. We know more about\nkilling than we know about living.‘”\n\n\n“Sex isn’t really what sells. What sells is fear.”\n\n\n“The news unconsciously mimics the way fear operates – focusing on the worst\nthings, catastrophising, listening to an endless, repetitive stream of\ninformation on the same worrying topic. So, it can be hard to tell these days\nwhere your anxiety disorder ends and where actual news begins.”\n\n\n“We seldom realise, for example, that our most private thoughts and emotions\nare not actually our own. For we think in terms of languages and images which\nwe did not invent, but which were given to us by our society.‘”\n\n\n“But this change – even within the last four millennia – is not a smooth,\nstraight upward line. It is the kind of steepening curve that would intimidate\na professional skateboarder. Change may be a constant, but the rate of change\nis not.”\n\n\n“Then there are other serious psychological concerns. To be constantly\npresenting ourselves, and packaging ourselves, like potatoes pretending to be\ncrisps.”\n\n\n“So, modern life is, basically, slowly killing the planet. Small wonder that\nsuch toxic societies can damage us, too.”\n\n\n“The whole of consumerism is based on us wanting the next thing rather than the\npresent thing we already have. This is an almost perfect recipe for\nunhappiness.”\n\n\n“To see the act of learning as something not for its own sake but because of\nwhat it will get you reduces the wonder of humanity.”\n\n\n“You will be happy when people like you. You will be happy when more people\nlike you. You will be happy when everyone likes you. You will be happy when\npeople dream of you.”\n\n\n“MAYBE HAPPINESS IS not about us, as individuals. Maybe it is not something\nthat arrives into us. Maybe happiness is felt heading out, not in.”\n\n\n“People might be encouraged to feel inadequate, but they don’t have to, as soon\nas they realise that the feeling is separate from the thing they are worried\nabout.”\n\n\n“Just as being overly anxious about money can paradoxically result in\ncompulsive spending, so worrying about our bodies is no guarantee we’ll have\nbetter bodies.”\n\n\n“In some areas, in some kind of distorted idea of equality, we seem to be\ntrying to make everyone equally anxious, rather than equally free.”\n\n\n“In nature,’ wrote Alice Walker, ‘nothing is perfect and everything is perfect.\nTrees can be contorted, bent in weird ways, and they’re still beautiful.‘”\n\n\n“Why don’t you do what I do? Let it wash all over you. Allow yourself just to\nbe as you are. Just be.”\n\n\n“Everybody dies,’ wrote Nora Ephron. ‘There’s nothing you can do about it.\nWhether or not you eat six almonds a day.‘”\n\n\n“Reframe your idea of beauty. Be a rebel against marketing. Look forward to\nbeing the wise elder. Be the complex elegance of a melting candle. Be a map\nwith 10,000 roads. Be the orange at sunset that outclasses the pink of sunrise.\nBe the self that dares to be true.”\n\n\n“No one has ever found a Neolithic cave painting of someone waking up stressed\nbecause they slept through their alarm and missed their nine o’clock management\nmeeting.”\n\n\n“having access to information gives you one kind of freedom at the expense of\nanother.”\n\n\n“When the ability to check something turns into the compulsion to do so, we\noften find ourselves craving the time before, when there was no ability to\ncheck in the first place.”\n\n\n“For instance, in 2016, physicists in Germany built a clock so accurate that it\nwon’t lose or gain a second for 15 billion years. German physicists now have no\nexcuse for being late for anything ever again.”\n\n\n“We are too aware of numerical time and not aware enough of natural time.\nPeople”\n\n\n“People for thousands of years may have woken up at seven in the morning. The\ndifference with these last few centuries is that now we are waking up because\nit is seven in the morning.”\n\n\n“We often find ourselves wishing for more hours in the day, but that wouldn’t\nhelp anything. The problem, clearly, isn’t that we have a shortage of time.\nIt’s more that we have an overload of everything else.”\n\n\n“We have multiplied everything, but we are still individual selves. There”\n\n\n“To enjoy life, we might have to stop thinking about what we will never be able\nto read and watch and say and do, and start to think of how to enjoy the world\nwithin our boundaries. To live on a human scale. To focus on the few things we\ncan do, rather than the millions of things we can’t. To not crave parallel\nlives. To find a smaller mathematics. To be a proud and singular one. An\nindivisible prime.”\n\n\n“The world’s brain is a common but fitting metaphor. We are the nerve cells of\nthe world’s brain, transmitting ourselves to all the other nerve cells. Sending\nthe overload back and forth. Overloaded neurons on a nervous planet. Ready to\ncrash.”\n\n\n“And things are happening too quickly for us to take stock of it all. Certainly\nquicker than in Tolstoy’s time. All this falling out. All this information. All\nthis technological connection. The world’s brain is a common but fitting\nmetaphor. We are the nerve cells of the world’s brain, transmitting ourselves\nto all the other nerve cells. Sending the overload back and forth. Overloaded\nneurons on a nervous planet. Ready to crash.”\n\n\n“The Internet is the first thing that humanity has built that humanity doesn’t\nunderstand, the largest experiment in anarchy that we have ever had.‘”\n\n\n“LIFE OVERLOAD IS a feeling that partly stems from how contracted and\nconcentrated the world seems to have become. The human world has sped up and\nhas effectively shrunk, too.”\n\n\n“The trouble is that if we are plugged in to a vast nervous system, our\nhappiness – and misery – is more collective than ever. The group’s emotions\nbecome our own.”\n\n\n“The whole internet is one step removed from the physical world. The most\npowerful aspects of the internet are mirrors of the offline world, but\nreplications of the external world aren’t the actual external world. It is the\nreal internet, but that’s all it can be.”\n\n\n“Don’t be steered towards being a caricature of yourself.”\n\n\n“Be a mystery, not a demographic. Be someone a computer could never quite know.\nKeep empathy alive. Break patterns. Resist robotic tendencies. Stay human.”\n\n\n“I used to think social media was harmless. I used to think I was on it because\nI enjoyed it. But then I was still on it even when I wasn’t enjoying it. I\nremembered that feeling. It was the feeling you get at three in the morning in\na bar after your friends have gone home.”\n\n\n“The internet age encourages choice and comparison, but don’t do this to\nyourself. ‘Comparison is the thief of joy,’ said Theodore Roosevelt. You are\nyou. The past is the past. The only way to make a better life is from inside\nthe present. To focus on regret does nothing but turn that very present into\nanother thing you will wish you did differently. Accept your own reality. Be\nhuman enough to make mistakes. Be human enough not to dread the future. Be\nhuman enough to be, well, enough.”\n\n\n“Accepting where you are in life makes it so much easier to be happy for other\npeople without feeling terrible about yourself.”\n\n\n“We would do well to remember that this feeling we have these days – that each\nyear is worse than the one previously – is partly just that: a feeling. We are\nincreasingly plugged in to the ongoing travesties and horrors of world news and\nso the effect is depressing. It’s a global sinking feeling. And the real worry\nis that all the increased fears we feel in themselves risk making the world\nworse.”\n\n\n“It is like someone who is ill with a compulsive disorder continually\nunderlining their fears – staying indoors, or washing their hands 200 times a\nday. They are actually doing more to hurt themselves, in the name of protecting\nthemselves. But this time the disorder isn’t individual. It is social. It is\nglobal.”\n\n\n“Shock may be an unpleasant thing for an individual or a society to experience,\nbut it can be a useful political tool.”\n\n\n“Naomi Klein coined the term the ‘shock doctrine’ to describe the cynical\ntactic of systematically using ‘the public’s disorientation following a\ncollective shock’ for corporate or political gain.”\n\n\n“We don’t go into a state of shock when something big and bad happens,’ she\nsays. ‘It has to be something big and bad that we do not yet understand.‘”\n\n\n“I remember once, during depression, staring up at a clear sky of stars. The\nwonder of the universe. At the bottom of the pit, I always had to force myself\nto find the beauty, the goodness, the love, however hard it was. It was hard to\ndo. But I had to try. Change doesn’t just happen by focusing on the place you\nwant to escape. It happens by focusing on where you want to reach.\nBoost the good guys, don’t just knock the bad guys. Find the hope that\nis already here and help it grow.”\n\n\n“The medium isn’t just the message, it’s the emotional intensity of that\nmessage.”\n\n\n“Realise the world is not as violent as it feels. Many writers on this\nsubject – such as the famed cognitive psychologist Steven Pinker – have pointed\nout that, despite all its horrors, society is less violent than it used to be.”\n\n\n“Some people talk to animals. Not many listen though. That’s the problem.‘”\n\n\n“Cynicism was a luxury for the non-suicidal.”\n\n\n“One thing mental illness taught me is that progress is a matter of acceptance.\nOnly by accepting a situation can you change it. You have to learn not to be\nshocked by the shock. Not to be in a state of panic about the panic. To change\nwhat you can change and not get frustrated by what you can’t.”\n\n\n“There is no panacea, or utopia, there is just love and kindness and trying,\namid the chaos, to make things better where we can. And to keep our minds wide,\nwide open in a world that often wants to close them.”\n\n\n“sleep has traditionally been an enemy of consumerism. We can’t shop in our\nsleep. We can’t work or earn or post to Instagram in our sleep. Very few\ncompanies – beyond bed manufacturers and duvet sellers and makers of black-out\nblinds – have actually made money from our sleep.”\n\n\n“And now, at this later stage of capitalism, sleep has become seen not just as\nsomething that slows work down, but as an actual business rival. The chief\nexecutive of Netflix, Reed Hastings,”\n\n\n“We live in 24-hour societies but not 24-hour bodies.”\n\n\n“EVEN WHEN THE world is not overtly terrifying us, the speed and pace and\ndistraction of modern existence can be a kind of mental assault that is hard to\nidentify. Sometimes life just seems too complicated, too dehumanising, and we\nlose sight of what matters.”\n\n\n“Accentuating the things that make you feel good, cutting back the things that\nmake you feel bad, and letting people feel truly connected to the world around\nthem.”\n\n\n“That is the biggest paradox, I think, about the modern world. We are all\nconnected to each other but we often feel shut out.”\n\n\n“Because often identifying a problem, being mindful of it, becomes the solution\nitself.”\n\n\n“individualism has replaced collectivism and community. We have face-to-face\nconversations less and less, and more interactions with avatars.”\n\n\n“The more stimulation we have, the easier it is to feel bored.”\n\n\n“She thought the cure to misery was to ‘decorate one’s inner house so richly\nthat one is content there, glad to welcome anyone who wants to come and stay,\nbut happy all the same when one is inevitably alone’.”\n\n\n“1.Don’t feel you always have to be there. In the not-so olden days of letters\nand landlines, contacting someone was slow and unreliable and an effort. In the\nage of WhatsApp and Messenger it’s free and easy and instant. The flipside of\nthis ease is that we are expected to be there. To pick up the phone. To get\nback to the text. To answer the email. To update our social media. But we can\nchoose not to feel that obligation. We can sometimes just let them wait. We can\nrisk our social media getting stale.”\n\n\n“There is no final checking of your phone. Think of all the times you checked\nyour phone yesterday. Did you really need to so often?”\n\n\n“Because there is no end to the uncertainty. There is no final checking of your\nphone.”\n\n\n“Illness has a lot to teach wellness.”\n\n\n“When it comes to our minds, awareness is very often the solution itself.”\n\n\n“This was an already familiar tactic of mine: trying to distract myself from\none torment by finding another.”\n\n\n“In a world of a million distractions you are still left with only one mind.”\n\n\n“I SO WISH I could explain something to my younger self. I wish I could tell\nmyself that it wasn’t all me. I wish I could say that there were things I could\ndo. Because my anxiety, my depression, wasn’t just there. Illness, like injury,\noften has context.”\n\n\n“When I fall into a frantic or despairing state of mind, full of unwelcome\nthoughts that can’t slow down, it is often the result of a series, a sequence\nof things. When I do too much, think too much, absorb too much, eat too badly,\nsleep too little, work too hard, get too frazzled by life, there it is. A\nrepetitive strain injury of the mind.”\n\n\n“6.Don’t grab life by the throat. ‘Life should be touched, not strangled,’”\n\n\n“save lives. But, as C.S. Lewis once put it, ‘The frequent attempt to conceal\nmental pain increases the burden: it is easier to say “My tooth is aching” than\nto say “My heart is broken”.‘”\n\n\n“this collision between one’s image of oneself and what one actually is is\nalways very painful and there are two things you can do about it, you can meet\nthe collision head-on and try and become what you really are or you can retreat\nand try to remain what you thought you were, which is a fantasy, in which you\nwill certainly perish.‘”\n\n\n“Because there is, I suppose, a clear self doing the asking. But when I was ill\nthese weren’t simply abstract concerns. These were desperate mysteries to\nsolve, as though my life depended on it. Because my life did depend on it. The\nfeeling of me-ness had gone – it had been crowded out – and I felt like I could\nbecome trapped in the infinite I, silently floating in panic, with nowhere to\nland.”\n\n\n“Panic is there to help us. As it is for many other animals, panic is our mind\nand body telling us to do something. Fight or flight. Run from the predator or\nfight the predator.”\n\n\n“People who have never had a period of living with anxiety and panic don’t\nunderstand that the realness of you is an actual feeling that you can lose.\nPeople take it for granted. You don’t get up in the morning and think, as you\nspread peanut butter onto your toast, ‘Ah, good, my sense of self is still\nintact, and the world is still real, I can now get on with my day.’ It’s just\nthere. Until it isn’t. Until you are in the cereal aisle, feeling inexplicable\nterror.”\n\n\n“Within a feeling of derealisation, I still knew I was me. I just didn’t feel I\nwas me. It is a feeling of disintegration. Like a sand sculpture crumbling\naway. And there is a paradox about this sensation. Because it feels like both\nan extreme intensity of self and a nothingness of self. A feeling of no return,\nas if you have suddenly lost something that you didn’t know you had to look\nafter, and that the thing you had to look after was you.”\n\n\n“And in unnatural settings, when your anxiety is raw enough, you can feel\nunnatural, too. You can feel as removed from yourself as a packet of toilet\nroll is removed from a tree.”\n\n\n“It helps to know I am just a caveman in a world that has arrived faster than\nour minds and bodies expected.”\n\n\n“Nothing I was worried about would fundamentally change anything. I would still\nbe able to walk the dog. I would still be able to look at the sea. I would\nstill be able to spend time with the people I love. The anxiety retreated, like\na criminal under the spotlight of an investigation.”\n\n\n“Perhaps when we find ourselves wanting everything it is because we are\ndangerously near to wanting nothing.‘”\n\n\n“WE ARE BEING sold unhappiness, because unhappiness is where the money is. Much\nof what is sold to us is the idea that we could be better than who we are if we\ntried to become something else.”\n\n\n“Life isn’t a play. Don’t rehearse yourself. Be yourself.”\n\n\n“8.If you’re feeling bad about yourself, stay away from Instagram.”\n\n\n“9.Remember no one else is ever worried about what your face looks like.”\n\n\n“So we have to be careful of our wants and watch that they don’t cause too many\nholes inside us, otherwise happiness will drip through us like water through a\nleaky bucket. The moment we want is the moment we are dissatisfied. The more we\nwant, the more we will drip ourselves away.”\n\n\n“I have learned that however strong the craving gets the guilt afterwards will\nbe stronger.”\n\n\n“That’s the problem with mental illness. It’s easy not to judge people for\nhaving an illness; it’s a lot harder not to judge people for how the illness\noccasionally causes them to behave. Because people can’t see the reasons.”\n\n\n“An evening of heaven in a glass doesn’t outweigh a month of hell in a cage.”\n\n\n“If the whole planet is having a kind of collective breakdown, then unhealthy\nbehaviour fits right in. When normality becomes madness, the only way to find\nsanity is by daring to be different. Or daring to be the you that exists beyond\nall the physical clutter and mind debris of modern existence.”\n\n\n“Even when the tide of society is pulling us in one direction it has to be\npossible – if that direction makes and keeps us unhappy – to learn how to swim\nanother way. To swim towards the truth of ourselves, a truth our distractions\nmight be hiding. Our very lives might depend on it.”\n\n\n“How many young college graduates have taken demanding jobs in high-powered\nfirms, vowing that they will work hard to earn money that will enable them to\nretire and pursue their real interests when they are thirty-five? But by the\ntime they reach that age, they have large mortgages, children to school, houses\nin the suburbs that necessitate at least two cars per family, and a sense that\nlife is not worth living without really good wine and expensive holidays\nabroad. What are they supposed to do, go back to digging up roots? No, they\ndouble their efforts and keep slaving away.‘”\n\n\n“I want to say, in all seriousness, that a great deal of harm is being done in\nthe modern world by belief in the virtuousness of work, and that the road to\nhappiness and prosperity lies in an organised diminution of work.‘”\n\n\n“As our schoolchildren are also discovering, all this testing and evaluating\nmakes us stress about the future rather than be comfortable with the present.”\n\n\n“Work culture can lead to low self-esteem. We are encouraged to believe that\nsuccess is the result of hard work, that it is down to the individual. So, it\nis no surprise that when we feel as if we are failing – which is almost\ncontinually in an aspirational culture that thrives on raising the bar of our\nhappiness – we take it personally. And think it is down to ourselves. We aren’t\nencouraged to see the context.”\n\n\n“We like to work. It gives us purpose. But work can also be bad for physical\nhealth.”\n\n\n“It is hard to challenge our cultural obsession with work. Politicians and\nbusiness leaders keep up the idea of relentless work as a moral virtue. They\ntalk with misty-eyed sentiment and a dose of sycophancy about ‘decent ordinary\nworking people’ and ‘hard-working families’. We accept the five-day working\nweek as if it was a law of nature. We are often made to feel guilty when we\naren’t working. We say to ourselves, like Benjamin Franklin did, that ‘time is\nmoney’, forgetting that money is also luck. A lot of people who work very long\nhours have far less money than people who have never worked in their life.”\n\n\n“and if it is what we can do about it. How much pressure are we actually\nputting on ourselves, simply because the way we work makes us feel continually\nbehind? Like life is a race that we are losing? And in our struggle to keep up\nwe don’t dare to stop and think what might be good for us.”\n\n\n“Aim not to get more stuff done. Aim to have less stuff to do. Be a work\nminimalist. Minimalism is about doing more with less. So much of working life\nseems to be about doing less with more. Activity isn’t always the same as\nachievement.”\n\n\n“One of the symptoms of an approaching nervous breakdown is the belief that\none’s work is terribly important.‘”\n\n\n“Progress,’ wrote C.S. Lewis, ‘means getting nearer to the place you want to\nbe. And if you have taken a wrong turning, then to go forward does not get you\nany nearer.‘”\n\n\n“In a world that can get too much, a world where we are running out of mind\nspace, fictional worlds are essential.”\n\n\n“For me, reading was never an antisocial activity. It was deeply social. It was\nthe most profound kind of socialising there was. A deep connection to the\nimagination of another human being. A way to connect without the many filters\nsociety normally demands.”\n\n\n“Reading isn’t important because it helps to get you a job. It’s important\nbecause it gives you room to exist beyond the reality you’re given. It is how\nhumans merge. How minds connect. Dreams. Empathy. Understanding. Escape.\nReading is love in action.”\n\n\n“I used to want to lose myself in the most intense experiences, as if life was\nsimply a tequila to be slammed. But most of life can’t be lived like this. To\nhave a chance of lasting happiness, you have to calm down. You have to just be\nit as well as just do it.”\n\n\n“We crowd our lives with activity because in the West we often feel happiness\nand satisfaction are achieved by acquisition, by ‘seizing’ the day, or by going\nout and ‘grabbing’ life by the horns. We might sometimes do better to replace\nlife as something to be grabbed at, or reached for, with something we already\nhave. If we clear out the mental clutter we can surely enjoy it more.”\n\n\n“Even world news seemed like a background irrelevance when you were sitting in\nan intensive care unit hearing the wails of grief coming from beyond a thin\nhospital curtain as the patient in the next bed passes away.”\n\n\n“I am trying now, when my life gets too packed with unnecessary stressful junk,\nto remember that room in the hospital. Where patients were thankful just to\nlook at the view out of a window. Some sunshine and sycamore trees. And where\nlife, on its own, was everything.”\n\n\n“human to ask a turtle questions. So, when depression slugs over me I close my\neyes and enter the bank of good days and think of sunshine and laughter and\nturtles. And I try to remember how possible the impossible can sometimes be.”\n\n\n“Anyway, if you really want to know, the advice I would give is stop it.’ ‘Stop\nwhat?’ ‘It. The rushing after nothing. Humans seem in such a rush to escape\nwhere they are. Why? Is it the air? Does it not hold you up well enough? Maybe\nyou need more time in the sea. I would say: stop it. Don’t just take your time,\nbe your time. Move fast or slow, but be aware you will always take yourself\nwith you. Be happy to paddle in the water of existence.‘”\n\n\n“Look at my head. It’s tiny. My brain-to-body-mass ratio is embarrassing. But\nit doesn’t matter, you see. If you take life carefully, you can focus. You can\nbe how you need to be. You can have an amphibious approach to life. You can be\nat one with the rhythms of the whole earth. The wet and the dry. You can tune\nin to the wind and the water. You can tune in to yourself. It’s rather\nwonderful, you know, being a turtle.’ ‘I”\n\n\n“When looking at the sky, all our 21st-century worries can be placed in their\ncosmic context. The sky is bigger than emails and deadlines and mortgages and\ninternet trolls. It is bigger than our minds, and their illnesses. It is bigger\nthan names and nations and dates and clocks. All of our earthly concerns are\nquite transient when compared to the sky. Through our lives, throughout every\nchapter of human history, the sky has always been the sky.”\n\n\n“The world affects us, but it isn’t quite us. There is a space inside us that\nis independent to what we see and where we are. This means we can feel pain\namid external beauty and peace. But the flipside is that we can feel calm in a\nworld of fear. We can cultivate a calmness inside us, one that lives and grows,\nand gets us through.”\n\n\n“The story is never just the words. It is also the reading of them. And that”\n\n\n“LIFE CAN SOMETIMES feel like an overproduced song, with a cacophony of a\nhundred instruments playing all at once. Sometimes the song sounds better\nstripped back to just a guitar and a voice. Sometimes, when a song has too much\nhappening, it’s hard to hear the song at all. And like that overcrowded song\nwe, too, can feel a bit lost.”\n\n\n“There is only one corner of the universe you can be certain of improving, and\nthat’s your own self.‘”\n\n\n“Nature does not hurry, yet everything is accomplished.‘”\n\n\n“Don’t try to pin yourself down. Don’t try to understand, once and for all, who\nyou are. As the philosopher Alan Watts said, ‘trying to define yourself is like\ntrying to bite your own teeth’.”\n\n\n“There is no future. Planning for the future is just planning for another\npresent in which you will be planning for the future.”\n\n\n“No one can make you feel inferior without your consent.‘”\n\n\n“And I did, like lots of people, get happy, fleetingly, at each career goal I\nset myself, but my mind quickly got used to the previous achievement and found\na new goal. So, the more I got, the more I needed to get in order to stay\nlevel.”\n\n\n“The more ‘success’ you get, the easier it is to be disappointed by not getting\nthings. The only difference is that now no one feels sorry for you.”\n\n\n“Simplify your life. Take away what doesn’t need to be there.”\n\n\n“The thing is to free one’s self,’ wrote Virginia Woolf, struggling with the\ntask. ‘To let it find its dimensions, not be impeded.‘”\n\n\n“Everything special about humans – our capacity for love and art and friendship\nand stories and all the rest – is not a product of modern life, it is a product\nof being a human. And so, while we can’t disentangle ourselves from the\ntransient and frantic stress of modern life, we can place an ear next to our\nhuman self (or soul, if you’d rather) and listen to the quiet stillness of\nbeing. And realise that we don’t need to distract ourselves from ourselves.\nEverything we need is right here. Everything we are is enough. We don’t need\nthe bigger boat to deal with the invisible sharks around us. We are the bigger\nboat. The brain, as Emily Dickinson put it, is bigger than the sky. And by\nnoticing how modern life makes us feel, by allowing that reality and by being\nbroad-minded enough to change when change is healthy, we can engage with this\nbeautiful world without being worried it will steal who we are.”\n\n\n“For after all,’ wrote the poet Henry Wadsworth Longfellow, ‘the best thing one\ncan do when it is raining is let it rain.‘”\n\n\n“overwhelm isn’t having too much to do; it’s not knowing where to start.”\n\n\n“Maybe you are like me—an overachiever, a people pleaser, a perfecter—so you\nbelieve success has been defined by what you do, not who you are. You fill your\ndays in pursuit of this illusion of success, just as I did, but deep down you\nfeel there must be a better way. And you are so right.”\n\n\n“we falsely believe that we need to be busy, that we are supposed to fill our\ndays.”\n\n\n“When we try to do too much, we overfill our plates with a multitude of tiny\ntasks and chores. We check a hundred things off our to-do lists, but when we\nslip into bed at night and our heads hit the pillow, we think, Why didn’t I get\nmore done?”\n\n\n“That’s why productivity may have failed you in the past—it’s the struggle to\nmake your life fit the system when, in fact, it should be the system that fits\nyour life. You can customize your productivity so that your life and your\npriorities are at the center.”\n\n\n“But too many of us tie our self-worth to our busyness. Stress and overwhelm\nare badges of honor declaring our worthiness. We falsely believe that if we are\nnot busy, we are failing. In the pursuit of finding balance, we try to do\neverything, but the more we do, the less we succeed.”\n\n\n“Creating an extraordinary life for ourselves requires moving away from\nbalance, because when we lean into a priority—when we give time to the most\nimportant things—we have to take that time away from something else. We cannot\ngive equal time to all the tasks on our lists.”\n\n\n“In chasing this illusion of balance, we end up creating a life that feels\nbusy—not meaningful. We have to be willing to go out of balance. We need to be\nwilling not to do everything. That’s the real magic.”\n\n\n“Ask any kindergartner what they are good at, and you’ll need to sit through a\nlaundry list of topics: art, running, painting, climbing trees, eating potato\nchips—seriously, five-year-olds think they are amazing at everything! But wait\nten years and ask the very same child, and she’ll think of almost nothing; at\nbest you’ll maybe hear one or two things she believes she excels in. What\nhappens to us in this space of time? How do we lose our belief in ourselves?\nWe’ve allowed the world to define us and reinforce these limiting beliefs, but\nit’s time to break through.”\n\n\n“Sometimes it’s just a limiting belief, the idea that you simply cannot do\nsomething, that restricts you.”\n\n\n“Too often we hand over the reins, allowing others to imprison us with their\nown agendas and urgent fires that need putting out. We think we don’t have\ncontrol over how our day runs, but we do. We’ve simply forgotten that we have\nthe ability to choose to spend time on our own priorities.”\n\n\n“It’s not reality that makes us feel stuck; it’s the lens we use to view the\nworld. Maybe you are tired of trying because it feels like it just doesn’t seem\nto matter. I’ve felt this way too. There are times when we all just want to\ncrawl back in bed and throw the covers over our heads because we are so\noverwhelmed with the chaotic rush of our days. We can lose sight of who we are\ndeep inside and what is most important to us. We are so busy struggling and\nfighting to keep our heads above the proverbial water that we seem to forget we\ncan choose to tread water for a moment. We can allow ourselves a deep breath\nand time to scan the horizon—we can choose to swim to calmer waters. When we\ngift ourselves with the ability to step back and choose, something powerful\nbegins to happen. We strengthen our internal locus of control3. In other words,\nwe remember we have the ability to influence our own destiny instead of\nallowing the current to push us wherever it wants. People with a strong\ninternal locus of control believe they have the freedom and ability to make\ntheir own choices and determine what happens to them. Because of that, they are\nsignificantly happier and more motivated. Psychologists have found that an\n\n\n“internal locus of control has been linked with academic success … higher\nself-motivation and social maturity … lower incidences of stress and\ndepression … and longer life span.” We want to strengthen our internal\nlocus of control and begin to understand that we have choices. BUT I REALLY\nDON’T HAVE ANY CONTROL If you’re still saying, “That isn’t true for me; I don’t\nhave any choices in my day.” I hear you. You have a strict boss, an overbearing\nfamily member, an overly regimented schedule, a special-needs child, or\nsomething similar. Right? I met Rhonda when I was speaking at a workshop event”\n\n\n“Are you choosing to spend your time being busy, or are you choosing to focus\nyour day on what matters most?”\n\n\n“When I look at this limited time I have, it reminds me that we all have\nseasons we live through. Seasons when our lives are hard and seasons where life\ncomes easy, but in the scheme of 100 years, those seasons are a mere fraction\nof the time we have.”\n\n\n“Seasons pass, life ebbs and flows, but our priorities are what anchor us.”\n\n\n“Your happiness isn’t defined by others, it is defined by you and the daily\nchoices you make. Living a life centered on your priorities is making a choice\nto be happy, and it’s okay to choose happy.”\n\n\n“am not afraid of storms, for I am learning how to sail my ship.”\n\n\n“When we live our life using our North Star, we take ownership of our legacy.”\n\n\n“If the mat was not straight, the Master would not sit.”\n\n\n“Still, its nature could be understood, and those who cared the most about it,\nand the life from which it was inseparable, understood it best”\n\n\n“Life itself, when understood and utilized for what it is, is sweet”\n\n\n“The essence of the principle of the Uncarved Block is that things in their\noriginal simplicity contain their own natural power, power that is easily\nspoiled and lost when that simplicity is changed.”\n\n\n“All that we are is a result of what we have thought.”\n\n\n“Nothing can prevent your picture from coming into concrete form except the\nsame power which gave it birth—yourself.”\n\n\n“Whatever the mind … can conceive it can achieve.”\n\n\n“Inside relationships it’s important to first understand who’s coming into the\nrelationship, and not just your partner. You need to understand yourself\nfirst.”\n\n\n“We’ve got a thousand different diagnoses and diseases out there. They’re just\nthe weak link. They’re all the result of one thing: stress. If you put enough\nstress on the chain and you put enough stress on the system, then one of the\nlinks breaks.”\n\n\n“I always say that incurable means “curable from within.”\n\n\n“Man becomes what he thinks about.”\n\n\n“What you resist persists.”\n\n\n“So many times people say to me, “Well, James, I have to be informed.” Maybe\nyou have to be informed, but you don’t have to be inundated.”\n\n\n“Energy flows where attention goes.”\n\n\n“The essence of this law is that you must think abundance; see abundance, feel\nabundance, believe abundance. Let no thought of limitation enter your mind.”\n\n\n“A person who sets his or her mind on the dark side of life, who lives over and\nover the misfortunes and disappointments of the past, prays for similar\nmisfortunes and disappointments in the future. If you will see nothing but ill\nluck in the future, you are praying for such ill luck and will surely get it.”\n\n\n“Whether you think you can or think you can’t, either way you are right.”\n\n\n“So your purpose is what you say it is. Your mission is the mission you give\nyourself. Your life will be what you create it as, and no one will stand in\njudgment of it, now or ever.”\n\n\n“can never be all the people I want and live all the lives I want. I can never\ntrain myself in all the skills I want. And why do I want? I want to live and\nfeel all the shades, tones and variations of mental and physical experience\npossible in my life. Sylvia Plath”\n\n\n“I can never be all the people I want and live all the lives I want. I can\nnever train myself in all the skills I want. And why do I want? I want to live\nand feel all the shades, tones and variations of mental and physical experience\npossible in my life.”\n\n\n“Master, I understand that the body is the basis. A good body is like uncut\njade, it can be cut into many good things. And a weak body is like impure jade,\neven with excellent craftsmanship, nothing good can come from it.‘”\n\n\n“If a craftsman is to do a good job, he shall need good tools.”\n\n\n“Meteors — even though their lives are short, they radiate the brightest\nlights. They are dazzling. As for human, not even a Xiantian expert can live\nlonger than several hundred years. Instead of living my whole life in\nmediocrity, why don’t I learn from the meteors? In my limited life, I’ll burn\nmy passion of life and stimulate the blood in my veins, making my life radiate\nthe most dazzling light. Only in this way will I die with no regret.‘”\n\n\n“Generally, people who are alone are independent and self-reliant. This is\nbecause if a person is usually alone they will naturally ponder. Pondering\nabout life, pondering about their own values, the more they ponder, the more\nthoroughly they can understand things.”\n\n\n“Perhaps I’ve been wrong since the beginning. A person can’t always live for\nother people. If I live like that, not only will I be tired, father will be put\nunder great pressure as well”\n\n\n“Alright, living in the world, why do I have to be afraid of something?\nMoreover, I’ve been living the wrong way. I mustn’t live for other people. That\nkind of life has tired both me and father out. I must live for myself. Father,\nbig brother, 2nd brother, I’ll pursue my own life!”\n\n\n“This is just like how watched flowers never bloom but an unattended willow\ngrows.”\n\n\n“You have to be true to your feelings no matter what happens later. To survive\nin this world, you’ll have to restrain yourself on many occasions, but if you\nrestrain yourself too much, something that makes you feel regretful for the\nrest of your life can happen. Remember … sometimes, you have to be true to\nyourself even if you’ll die from this.”\n\n\n“The weak let their ideals control their actions, but the strong control their\nideals with their actions.’ If not for this saying supporting my inner world\nlike a column at the bottom of my heart, perhaps … I would have collapsed long\nago due to being unable to stand acting against my will.”\n\n\n“The outside world, we are looking at only a small part that interests us.”\n\n\n“The world we see is not the entire universe but a limited one that the mind\ncares about. However, to our minds, that small world is the entire universe.\nOur reality is not the infinitely stretching cosmos but the small part we\nchoose to focus on. Reality exists because our minds exist. Without the mind,\nthere would be no universe.”\n\n\n“But then I realize it isn’t the outside world that is a whirlwind; it is only\nmy mind. The world has never complained about how busy it is”\n\n\n“We feel unhappy not just because something bad has happened, but also because\nof the swirling thoughts about what happened.”\n\n\n“Too many choices make people unhappy.”\n\n\n“Those who don’t believe in magic will never find it.”\n\n\n“How could you forgive until you’re forgiven?\nSparks fly upward, you’re learning the hustle\nYou’re learning that everyone in a room is in love with their own tongue”\n\n\n“Still searching for trouble\nYou wanted a tempt, but not a tempter\nWhere’d they find you?\nOut looking for home that you don’t remember\nWhere did youth go?”\n\n\n“Your twenties are for spending hours and hours pretending\nWe have plans and we have places we should visit\nBut everybody knows your twenties are for wasting time”\n\n\n“I don’t wanna take the world for granted\nWhile I’m still trying to understand it”\n\n\n“This world makes me dizzy, how’d we get so busy?\nNo one tries to take the time it takes to turn your\nLove into a love or friends into a family”\n\n\n“Patience is fatal but pain is not weak”\n\n\n“Long nights to quiet my mind\nWith empty conversations\nDestroyed the life in my eyes\nI swear I’m gonna change, yeah\nI won’t be a casualty\nA product of my agony\nBreak my bloodline\nFeel the pain and then kill it twice\nI won’t be a casualty\nNo, I won’t go down like that”\n\n\n“They say one is too many\nWhen enough is enough”\n\n\n“Manifestation isn’t about trying, it is about being. You don’t try to be what\nyou want. You choose and embody immanently what it is you want to be.”\n\n\n“17, and we got a dream to have a family\nA house, and everything in between\nAnd then, oh, suddenly, we turned 23\nAnd now we got pressure for taking our life more seriously”\n\n\n“Is anybody there?\nDoes anybody care\nWhat I’m feeling?\nI wanna disappear\nSo nobody can hear\nMe when I’m screamin’\n‘Cause I could use a hand sometimes\nYeah, I could use a hand sometimes\nThey say pain is an illusion\nThis is just a bruise\nAnd you are just confused\nBut I am only human\nI could use a hand sometimes\nI am only human”\n\n\n“Let’s make this fleeting moment last forever\nSo, tell me what you’re waiting for?\nI’m gonna keep it frozen here forever\nThere’s no regretting anymore\nIt’s worth the wait, even so far away\nI’m making the night mine until the day I die\nNo lights to brake when you’re hanging by fate\nYou know what it feels like when you’re dancing blind\nAll alone, just the beat inside my soul\nTake me home, where my dreams are made of gold\nIn the zone where the beat is uncontrolled\nI know what it feels like\nCome on make me feel alive”\n\n\n“We were born ready, ready to be free\nChasin’ every thrill we could see\nWith our eyes steady, waking to the dream\nAching to be thrown in the ring\nIf nothing comes easy as long as we’re breathing\nWe’ll go all the way or go home\nWe were born ready, wherever it leads\nWhat we have is all we need\n‘Cause if it’s fast or slow\nAll I really know is I’m gonna enjoy the ride\nAnd if it’s hard or soft before we get off\nI’m gonna enjoy the ride, enjoy the ride”\n\n\n“When I’m three feet from the edge\nWill I break before I bend?\nI’m only human, ashes to dust\nMaking a mess of us\nWhere the words fall from your lips\nTo save this sinking ship\nGive me a sign to keep my heart beating\nThrow me a line in over my head”\n\n\n“I can hear the sound of a heartbeat before it goes out\nWon’t ever leave my memory of bloodshed all around\nAnd I can see a tear on my father’s face before it falls out\nOh, my enemy, how could I have ever let you down? Oh\nWhen all these trees saw us grow\nCut our teeth and make our bones right here\nWe’d play with shields made of stone\nShare our dreams and sit our thrones\nBe still, ‘cause I see smoke up ahead and I got steel in my hands\nWe will return like warriors, I swear, that we’ll find glory up ahead\nTell me\nWhere is my home?\nI don’t recognize the faces anymore, no\nWhere is my friend?\nThe one I’ve known since I was only just a kid\nI think it’s time to say goodbye\nGoodbye, goodbye”\n\n\n“Cause there’s no one to love you\nWhen you build your walls\nToo high”\n\n\n“He’d trade his guns for love\nBut he’s caught in the crossfire\nAnd he keeps wakin’ up\nBut it’s not to the sound of birds\nThe tyranny\nThe violent streets\nDeprived of all that we’re blessed with\nAnd we can’t get enough, no\nHeaven, if you sent us down\nSo we can build a playground\nFor the sinners\nTo play as saints\nYou’d be so proud of what we made\nI hope you got some beds around\n‘Cause you’re the only refuge now\nFor every mother\nEvery child\nEvery brother\nThat’s caught in the crossfire\nThat’s caught in the crossfire\nI’d trade my luck to know\nWhy he’s caught in the crossfire\nAnd I’m here wakin’ up\nTo the sun and the sound of birds\nSociety’s anxiety\nDeprived of all that we’re blessed with\nWe just can’t get enough, no\nHeaven, if you sent us down\nSo we can build a playground\nFor the sinners\nTo play as saints\nYou’d be so proud of what we made\nI hope you got some beds around\n‘Cause you’re the only refuge now”\n\n\n“Feeling lost in life means that you’re totally free to go anywhere you want”\n\n\n“For years I thought I was looking for the meaning of life, one of my biggest\nrevelations is when I realized I was actually seeking the meaning of\nsuffering.”\n\n\n“In your ignorance you want to immortalize your body because you think that is\nyou. Seek to immortalize your mind. Then you may have whatever body you wish,\nwhen you wish.”\n\n\n“Our feelings arise because we see pictures as extensions of the real world.\nPictures that affect us strongly use structural principles based on the way we\nhave to react in the real world in order to survive. As soon as you understand\nthese principles, you will understand why pictures have such specific emotional\neffects. You will understand how pictures work.”\n\n\n“Pictures are usually read as though there is an invisible, emotional horizon\nline stretching across the middle of the space and dividing it into top and\nbottom.”\n\n\n“Cause it’s not too late, it’s not too late I, I see the hope in your heart And\nsometimes you lose and sometimes you’re shooting Broken arrows in the dark We\nhave to tear down walls that live in your heart To find someone you call home\nNow you see me for me and my beautiful scars So take my hand, don’t let go”\n\n\n“For when joy passes its climax we are bound to revert to anger, and when anger\npasses its climax we always revert to joy, because in both cases we are off\nbalance.”\n\n\n“Set your will on one aim, And be equal to the gods.”\n\n\n“What common knowledge knows is shallow”\n\n\n“The utmost in speech is to be rid of speech, the utmost doing is Doing\nNothing”\n\n\n“When a man’s inner integrity is not firm, something oozes\nfrom his body and\nbecomes an aura, which outside him presses\non the hearts of others; it makes\nother men honour him more\nthan his elders and betters, and gets him into\ndifficulties.\nThe only motive of an innkeeper is to sell his rice and soup,\nand\nincrease his earnings; his profits are meagre, the considerations\nwhich sway him have little weight. If men with so little to gain\nfrom me\nvalue me so highly as a customer, will it not be even\nworse with the lord of\nten thousand chariots, who has worn out his body and drained his knowledge in\nstate affairs?”\n\n\n“If you act nobly and banish from your mind the thought that you are noble,\nwhere can you go and not be loved?”\n\n\n“Why should the place where you lived be different from your own palace, or the\nplace of our excursion different from your own park? Your Majesty feels at home\nwith the permanent, is suspicious of the sudden and temporary. But can one\nalways measure how far and how fast a scene may alter and turn into something\nelse?”\n\n\n“With rank high enough to distinguish you, and more property than you need, you\nare too far above other men. Dreaming at night that you are a slave, reverting\nfrom ease to toil, is fortune righting itself. Can you reasonably expect to\nhave it both ways, dreaming as well as awake?”\n\n\n“What I mistake for ecstacy is simply the abscence of grief”\n\n\n“That funny feeling is the emptiness of everything that is supposed to fill the\ngaps. When all the gaps are filled with plastic, it still looks empty and\ncracked. We are fed too big a plate of nothing but empty calories, and the\nfunny feeling is being hungry with a full stomach.”\n\n\n“People cry, not because they are weak. It is because they’ve been strong for\ntoo long”\n\n\n“Sometimes what we want is simpler and closer than we think”\n\n\n“She filled my head with dreams, telling me I could become anything I wanted. I\nbelieved her so much I thought I could be white.”\n\n\n“Part of you is obsessed with the mission to save your parents, while the other\npart is filled with anger and resentment because you know you were stunted by\nparents who were so helpless or dysfunctional that they could not be real\nparents to you”\n\n\n“I don’t care what baggage they dragged over the ocean. They have no right to\nmake me carry it the rest of my life”\n\n\n“Your true self can survive with just freedom and will. Your true self is\nfiercer, braver, purer, infinitely more adaptable, and at home with nature than\nyou ever dreamed possible”\n\n\n“As I began to love myself I found that anguish and emotional suffering are\nonly warning signs that I was living against my own truth. Today, I know, this\nis Authenticity”\n\n\n“Children who are respected learn respect. Children who are cared for learn to\ncare for those weaker than themselves. Children who are loved for what they are\ncannot learn intolerance. In an environment such as this, they will develop\ntheir own ideals, which can be nothing other than humane, since they grew out\nof the experience of love.”\n\n\n“You are allowed to take up space. Own who you are and what you want for\nyourself. Stop downplaying the things you care about, the hopes you have. Own\nyour passions, your thoughts, your perceptions. Own your fire. Stop putting\nyour worth in the hands of others; stop letting them decide your value. Own\nsaying no, saying yes. Own your mood, your feelings. Own your plans, your path,\nyour success”\n\n\n“When interacting with others, practise simply relaxing in their presence. Try\nto experiment with not doing anything, saying anything, or impressing anyone.\nYou are not asked to advise, be useful, or advise anyone. Even when others are\nexpressing distress, you can practice ‘just’ listening and offering your quiet\npresence. This will help you gradually realize that your mere existence is\nsufficient and that ”doing” has its limits. ”\n\n\n“I’m not so weird to me.”\n\n\n“One of the most pertinent lessons of being a highly sensitive and intense\nperson is learning to shrug your shoulders when you feel ostracised by the\nworld, rejected by a friend, or abandoned by a lover.  We can start by\nacknowledging and accepting who we are to gain such strength.  Your trait as an\natypical, sensitive, and intense person is a gift.  High functioning autism:\nYou did nothing to get it, nor can you get rid of it.  It was given to you at\nbirth. You are wired that way.  You love deeply, but you may not be able to say\nit.  You see beauty where others don’t, but you may not have a pal who\nappreciates it alongside you.  You have a unique sense of humor, and those who\nunderstand it are delighted by your presence.  “A normal day” for you is a\nroller coaster ride for others. You have a million shades of emotions, nuanced\nobservations, and complex thoughts at any given hour.  Your thoughts are deep\nand complex. When you read a book, listen to a song, or watch a movie, what you\nsee or hear are not merely images or sounds. But rather multi-layered,\ninterwoven meanings and existential questions. Your mind has the ability to\ntravel a million miles even if you sit still.  You may not know it yourself;\nothers certainly don’t see it, but your heart breaks when you see the world’s\npain.  You love fiercely— not just humans but nature, science, the arts, a\ndiscipline, and the world.  As a child, you were not afraid to show it. But\nwhen you realize how much your passion threatens others, you learned to hide.\nHow much do we adjust our intensity and oddity to match the world’s frequency?\nAny intense and sensitive misfits across time and space ask this question.  If\nyou tell the truth, others become frightened.  You are relentlessly giving, but\nnot everyone can reciprocate.  You may be dismissed and judged when you try to\nreveal the real you, speak your mind, and express your true feelings.  People\nmay blatantly say you are dramatic, arrogant, and extreme. Or, they quietly\nretreat and passively punish you.  It takes incredible courage to stand up for\nyourself and be who you are.  But Here is a crucial piece of wisdom for the\nneuro-atypical gifted soul that you are: Once you accept yourself, including\neverything that comes with high functioning autism, other people’s judgment,\ncriticism, and rejection… will hurt less.  If you can wholeheartedly accept\nyour intensity and drive for what others see as odd, you will find it easier to\nnavigate the world.  Your interpersonal fragility will decrease because you no\nlonger depend on other people’s love, acceptance, praise, approval, and\nappreciation.  In the past, if a friend went silent, you might immediately\nthink you had done “too much.” You would wonder if you had said too much,\nrevealed too much, acted too extremely, or acted in a way that elicited their\njudgment. You might have fallen into the trap of self-blame and shame. You\nmight blame yourself for acting the way you did.  You might be unable to do\nanything else while you anxiously await their responses. You might wonder how\nyou can edit your words and change your personality so that you will not be\nrejected again. If you had experienced family trauma in your childhood, intense\nfeelings could arise. You might feel abandoned by the world and betrayed by\nthose who are supposed to love you. You sink into a deep ditch of silent anger\nand hopelessness.  In some therapies, you are told to eradicate the thoughts\nand feelings because they are ‘irrational’ and that you are ‘catastrophizing’.\nBut intellectually, knowing the rejection may or may not be true doesn’t mean\nyour nervous system can calm down. To the hurt inner child within you, even the\nmere chance that someone would dislike you can feel like the world is\ncollapsing. Thus, rather than rationalizing and arguing with ourselves, the\nultimate strategy is to parent yourself so well that you will no longer be\ndependent on other people’s love and approval.  Here is a new path that will\nbring you more peace, freedom, and joy— the way of unconditional self-love.\nHaving it, you no longer hinge on your peace and sanity on your other people’s\ntimely responses to you, what they think of you, or whether or not they like\nyou.  Learn to be yourself – intense, sensitive, quietly empathetic, restless,\ncurious, and passionate.  Do not pretend to know less than you do, do not hide\nthe extent of your true feelings, and do not put yourself down before others\nsay something . Make a joke even if it is not understood, cry when you feel the\nurge, and laugh out loud when you want to.  Call when you want to, be warm if\nyou want to, and speak your mind without over-editing every word.  Express your\nstrong opinions and respect others, tell them you are offended when you are,\nand it is okay to express your needs even you are not asking others to meet\nthem.  Authenticity is your natural ‘filter’.  If someone loves the above, they\nare your person.  If not, you need not bow to their preferences and lovingly\nrelease them to find someone better suited to be their friend.  As long as you\nact honestly, the outcome is best for all. ” “Feel in your body the deep\nknowledge that every animal, tree, and flower is your friend. You are anything\nbut alone.  Look up into the sky and talk to ‘God’, the Buddha, Allah, any\nhigher power that you believe in, or a departed loved one, knowing that they\nare forever in your heart and that love never ends.  Build a personal library\nand have virtual conversations with brilliant and like-minded souls near and\nfar.  Create a piece of art or music that expresses your most profound truth,\nand know that the moment someone resonates with your work is a deep spiritual\nconnection.  There are ways to take such good care of yourself that growing old\nmeans sage-ing up.  Know yourself so well that you say no to that which does\nnot excite your heart.  Give yourself permission to receive nourishment and\nknow that you are worth it.  Be so gentle, so loving, and compassionate with\nyourself that you feel safe in your own presence.  Be so encouraging and loving\nthat you do not believe in mistakes, only in learning.  Accept the imperfection\nof friendships; enjoy what is there and let go of what is not.  Mourn the ideal\nparents you never had, but vow to be the best parents you can be, even to\nyourself.  Sylvia Plath did say the best way to get what you want is to be who\nyou are.  So, starting today, when something has not gone the way you expected,\nwhen you feel judged, rejected, or abandoned, can you learn to be on your own\nside?  Shrug your shoulders.  Give yourself a big hug.  Then move on and Rock\non.”\n\n\n“My feelings are too loud for words and too shy for the world.”\n\n\n“There might not have been any explicit trauma, but on a level deep inside, the\nparentified child did not feel welcome in the world.”\n\n\n“We may look like we are loved based on what can externally be seen, yet inside\nwe feel like orphans”\n\n\n“Adulthood is an attempt to become the antithesis of the wounded child within\nus”\n\n\n“And as the ancient saying went, it is easy to dodge the spear in the open,\nbut hard to avoid a stab in the dark.”\n\n\n“Xiaochun…” he said softly. “You’re right. As long as we’re alive, there are\nendless possibilities. But just because we die doesn’t mean that our hopes\nand dreams die with us! “I can’t speak for anyone else, but I can say that\nwhen I look at all of these graves, I’m absolutely certain… that these\ndisciples who died in battle did it because of the Dao that existed in their\nhearts!”\n\n\n“You often found it easier just to adapt to their ideas of what was best for\nyou rather than trying to figure it out for yourself. But in all this\nadapting to your society’s attempts to make you fit in, and in your own\nattempt to find less trouble, you have unwittingly relinquished your most\nbasic foundation: your total and absolute freedom to create.”\n\n\n“That is the optimal creative vantage point: To stand on the brink of what is\ncoming, feeling eager, optimistic anticipation—with no feeling of impatience,\ndoubt, or unworthiness hindering the receiving of it—that is the Science of\nDeliberate Creation at its best.”\n\n\n“You cannot desire something, predominantly focus on the absence of it, and\nthen expect to receive it,”\n\n\n“You would not walk into a brightly lit room and look for the “dark switch.”\nIn other words, you would not expect to find a switch that would flood an\ninky darkness into the room to cover the brightness of the light—you would\nfind a switch that would resist the light, for in the absence of the light there is darkness.”\n\n\n“In a world that often seems too crowded or busy to notice beautiful things\nor make meaningful connections, there is still room for each of us to grow in\nthe ways we were meant to.”\n\n\n“Conforming to convention is emptiness,” replied Meng Hao. “Yielding to and complying with the Heavens is well and good. Unending persistence is fine, too. However, I cannot choose either of those.”\n\n\n“It is hard for me to resent my parents, although I envy them their naivete.”\n\n\n“When Love cast me out, it was Cruelty who took pity upon me”\n\n\n“When you care for yourself first, the world will also find you worthy of care.”\n\n\n“But because of the weight of the secrets, we become more humble and understanding.”\n\n\n“Why should your life be destroyed by the easy criticism of those who do not know you or care about you?””\n\n\n“Just as a mother looks at her child with love, look at your own suffering with compassion. You will soon feel that you are not alone. There is a soft inner core of love and caring at the heart of every suffering. You are not thrown into this world alone.”\n\n\n“The greatest gift that parents can give their child is to be happy themselves. If the parents are happy, then the child can grow up into a happy and confident adult. But if the parents are not happy, then the child can feel worthless— unable to make his parents happy no matter what.”\n\n\n“The reason adolescents don’t listen to their parents and stubbornly try to have their own way is that they are learning to be independent. It is normal, so don’t worry too much.”\n\n\n“Do your best to give your children someone to look up to.”\n\n\n“It is nearly impossible for a son or daughter to change a parent’s personality, values, or behavior. Even if children consider their parents problematic in some way, they have neither the right nor the responsibility to change them”\n\n\n“If you assume that, since you’ve been together for so long, you should be able to read each other’s minds, there are so many things you will fail to understand about each other.”\n\n\n“When someone is showing his temper, it could be because he wants us to hear about his current situation and empathize. Rather than arguing, try to understand his deeper needs”\n\n\n“If we think of the child as a stranger, we focus on the inconvenience to ourselves, but if we think of the child as a family member, we become merciful, wondering whether the child is uncomfortable or in pain”\n\n\n“If you give something your full attention, whatever it is, and examine it closely, it will come to attract your interest and care.”\n\n\n“My interest in philosophy started where I’d imagine it starts for most people: a dissatisfaction with what I had been told about life combined with a curiosity about what I had not.”\n\n\n“My desire for knowledge is intermittent; but my desire to commune with the spirit of the universe, to be intoxicated with the fumes, call it, of that divine nectar, to bear my head through atmospheres and over heights”\n\n\n“My desire for knowledge is intermittent; but my desire to commune with the spirit of the universe, to be intoxicated with the fumes, call it, of that divine nectar, to bear my head through atmospheres and over heights unknown to my feet, is perennial and constant.”\n\n\n“the experience and effects of concrete knowledge can be fleeting, but the wonder found in the spirit of the unknown can be constant and enduring”\n\n\n“Nobody ever figures out what life is all about, and it doesn’t matter. Explore the world. Nearly everything is really interesting if you go into it deeply enough,”\n\n\n“Through our eyes, the universe is perceiving itself. Through our ears, the universe is listening to its harmonies. We are the witnesses through which the universe becomes”\n\n\n“Through our eyes, the universe is perceiving itself. Through our ears, the universe is listening to its harmonies. We are the witnesses through which the universe becomes conscious of its glory, of its magnificence,”\n\n\n“As we experience more of life, and we are continually disappointed by our optimism’s inability to align with the real conditions of the world, our optimism is beaten further and further into submission.”\n\n\n“healthy dose of pessimism is necessary in our ability to adequately deal with this life. It helps us mitigate our expectations and serves as padding that protects us from life’s constant attempts to beat our spirit out of us. Pessimism counterbalances the ridiculously overly optimistic expectations of the culture we live in and helps us adapt out of the deeply detached, unrealistic perspective that we likely formed as children. It reminds us that things won’t always go our way or always be that nice, but rather, things will go wrong a lot, but despite this, we can still be ok.”\n\n\n“To be completely optimistic about the rules always working in our favor would be foolish. However, to be completely pessimistic about the game as a whole simply because the rules don’t always work in our favor would be equally foolish.”\n\n\n“the dirt of life, it is up to us to plant the seeds, watch the flowers grow, and enjoy their beauty, even in spite of the fact that we know that they will die.”\n\n\n“In the dirt of life, it is up to us to plant the seeds, watch the flowers grow, and enjoy their beauty, even in spite of the fact that we know that they will die.”\n\n\n“Perhaps all reality is a prison and time is its guard”\n\n\n“Perhaps what we should and only can do is to try to enjoy the process of playing with the blocks of philosophy like children playing with toy blocks for no reason other than the curiosity and fun of it; not because in the end the blocks will provide something that stays up forever, but because we inevitably will take the blocks down, put them away for a little while, and then play with them again on another day, in a different way.”\n\n\n“Kōans did not spring up out of lack of thought or contemplation, but rather, out of a specific sort of contemplation: a self-referential thinking that denies its ability to be a single, concrete, and universal thought that answers or understands what might exist beyond itself. Zen and the lesson of the kōans suggest that we should flow with life, ask questions, contemplate them, but not become tricked by any singular idea or answer that might tempt us into a final resolution”\n\n\n“Just like how the center of a tornado is calm with little to no motion, despite it being surrounded by a coil of rapid, violent wind, we can live in the center of the tornado of knowing and unknowing and still remain calm and at ease.”\n\n\n“it takes no more than a sick stomach or migraine to realize just how heavy this corporeal weight is, how stricken and limited by it we are. We are stuck inside the body, captives to it, subject to its faulty and fragile mechanisms that do and will break, keeping us bound in space according to its condition—until it finally turns itself off, and us with it.”\n\n\n“We can only think through the mind, and we can only think in the way our mind thinks.”\n\n\n“It is a demonstration of humanity’s overzealous ego and anthropocentrism to think that so long as no other humans tell them what to do, they are free.”\n\n\n“Freedom can be manifested only in the void of beliefs, in the absence of axioms, and only where the laws have no more authority than a hypothesis,”\n\n\n“Like the desire for perfect, unending happiness, the desire for complete and absolute freedom is impossible. But like happiness, it is, in its true, ultimate form, a state that comes and goes, unattainable in the ideal but attainable in the moment—in the moments when we surrender to the complete unified image of being, when we cease trying to square circles and placate everything that contests us, when we stop trying to escape what cannot be escaped. It is the classic lesson: the Chinese finger trap, the Tao in Taoism, Nirvana in Buddhism, the silence of Wittgenstein; the harder one tries, the harder one flails, the more entrenched one becomes.”\n\n\n“for you can only be free when even the desire of seeking freedom becomes a harness to you, and when you cease to speak of freedom as a goal and a fulfilment.”\n\n\n“In truth that which you call freedom is the strongest of these chains, though its links glitter in the sun and dazzle your eyes.”\n\n\n“When we persist with the belief that things outside of ourselves or things in the future will provide us with a form of ultimate happiness, we exchange the real moments of our lives for ones that do not exist. We become dependent on things outside of ourselves that we cannot control, and we endlessly run on a treadmill of unceasing desire.”\n\n\n“We can and should engage our nature to progress and pursue bigger, faster, better, and more interesting things, but we should ensure that in our pursuits, we are intentional about what we are doing so as to ensure that we are not being careless with our time and wasting our experience of life”\n\n\n“No matter what task we undertake, we will do it wastefully if we assume that anything beyond the task itself will provide anything better than the experience of focus and presence in the task.”\n\n\n“Until we have begun to go without them, we fail to realize how unnecessary many things are. We’ve been using them not because we needed them but because we had them”\n\n\n“It is now that we must find time and it is now that we must find happiness if it is either that we are seeking, because if we do not focus the lens through which we view life right now, everything we see from this moment forward will remain out of focus”\n\n\n“Starting from birth, we seemingly run, if not sprint, through life, racing out of every moment, unsatisfied with what life is and constantly looking to the future for what life could be if we could just obtain something more or different. Our cultures overwhelm us with the reinforcement of this idea, convincing us that our duty is to achieve, buy, own, and live perfect, unaffected lives. This delusion, however, frenzies us with an anxiety that we are then told, by culture, that we can rid ourselves of if we just achieve a few more things, make a little more money, be a little more popular, and buy a little more stuff, creating an endless feedback loop of unsatisfied hunger. If we cave into this, we surrender our life, we give up our self.”\n\n\n“We don’t have much, if any, control over what happens to us, how people see and treat us, or what happens because of what we do, and in the big picture, none of it really matters all that much anyway. And so, we must define our happiness not by what we own or achieve, not by how others see us, not by some bigger picture of life, but by how we think and see our self and live our own life through what we deem virtuous and relevant”\n\n\n“But of course, after a point, worrying about the future, the unknown, and the potential for things to go wrong is nothing but a useless handicap”\n\n\n“Once one has done everything that is rationally and realistically preventative, one should work to revert their attention back to the present, leaving all additional concern about the future for the future.”\n\n\n“But worrying about what one cannot know nor control in the future has no value to either, and comes at the cost of the present.”\n\n\n“We are more often frightened than hurt; and we suffer more from imagination than from reality.”\n\n\n“Man is not worried by real problems so much as by his imagined anxieties about real problems.”\n\n\n“There is a spectrum of human horrors, some far worse and more trying than others. In some cases, it is largely improbable to recover in the true sense of the term. But even if this is true, and one is worried about these sorts of horrible things happening, then again, they haven’t happened yet.”\n\n\n“Human history is carved through trenches. We dip in and out of oscillating hardships, founded or unfounded. We are plagued by plagues and hatred and conflict and mortal fragility. But if we are fortunate enough to worry about something that is potentially not survivable happening to us as opposed to trying to survive something that already has, it is perhaps worth trying to be ok while we still are.”\n\n\n“Men are thrifty in guarding their private property, but as soon as it comes to wasting time, they are most extravagant with the one commodity for which it’s respectable to be greedy,”\n\n\n“Seneca thought that because the present is so brief and immaterial, we mostly struggle to properly perceive and value”\n\n\n“Unlike the majority of possessions in life, you can’t see, hold, or truly know time, making it incomprehensibly slippery and abstract. Seneca thought that because the present is so brief and immaterial, we mostly struggle to properly perceive and value it”\n\n\n“It is inevitable that life will be not just very short but very miserable for those who acquire by great toil what they must keep by greater toil”\n\n\n“It seems that, in this, the worthiest use of time is in some sense spending it on reflecting on time itself. By considering, or at least pondering time and how to best use it, one is paradoxically using it well.”\n\n\n“Wasted time or well-spent time is all the same when viewed from a sufficient distance, and it is only the individual who can examine, consider, and determine the best way to balance and claim their time in each moment. And, of course, all anyone can ever do is try their best.”\n\n\n“What makes the sad song that I listen to when I’m in my worst of moods work is that it validates my feelings and transmutes them rather than denies them.”\n\n\n“We are perhaps the only stop on this evolutionary train that is outside the tunnel of darkness, able to take the material of everything and make it into something beautiful or helpful or interesting, to understand and create the meaning of meaning itself. And to do so just because we can, because the universe, for some reason, gave us a blank page to write on.”\n\n\n“As an assessment of the nature of reality, he would describe the Will as a sort of malevolent force that we, as individual selves, become victims of in its process of continuation, deceived by our own mind and body to go against our fundamental interests and yearnings in order to carry it out. Since the Will has no aim or purpose other than its perpetual continuation, then the Will can never be satisfied. And since we are expressions of it, neither can we. Thus, we are driven to consume beings, things, ideas, goals, circumstances, and all the rest, constantly hoping that we will feel satisfaction or happiness as a result, while constantly being left in the wake of each achievement unsatisfied.”\n\n\n“The safest way of not being very miserable is not to expect to be very happy,” he wrote.”\n\n\n“There can be no turning against the Will if the Will is doing the turning.”\n\n\n“We are merely born into a crazy, sad, violent reality with a mind and body that are often all in conspiracy against us.”\n\n\n“There are no facts, only interpretations,”\n\n\n“If we have our own why in life, we shall get along with almost any how,”\n\n\n“My formula for greatness in a human being is amor fati: that one wants nothing to be different, not forward, not backward, not in all eternity. Not merely bear what is necessary, still less conceal it … but love it.”\n\n\n“But in this reality, the one we must live, there was no option to have done differently, and there is no other way for things to go. Every decision you’ve made was the best and only decision you could’ve made at the time with the information you had and the state of mind you were in. And every condition of life that either these decisions led to or that are fundamental to life in general, you have no control over and cannot change.”\n\n\n“I want to learn more and more to see as beautiful what is necessary in things; then I shall be one of those who make things”\n\n\n“I want to learn more and more to see as beautiful what is necessary in things; then I shall be one of those who make things beautiful. Amor fati: let that be my love henceforth! I do not want to wage war against what is ugly. I do not want to accuse; I do not even want to accuse those who accuse. Looking away shall be my only negation”\n\n\n“Ultimately, the question may not be how much you love your life right now, but how much you could and how. And perhaps sometimes the only way to experience the beauty of things is to think about things in a beautiful way.”\n\n\n“What’s scarier than an opponent who smiles while being beaten?”\n\n\n“Even in writing about the futility and meaninglessness of life and its endeavors, the power of the creative process can, in some sense, save the writer from the very content of their own work, paradoxically making the futility and meaninglessness of life that they discuss somewhat less futile and meaningless.”\n\n\n“When all the current reasons—moral, esthetic, religious, social, and so on—no longer guide one’s life, how can one sustain life without succumbing to nothingness? Only by a connection with the absurd, by love of absolute uselessness, loving something which does not have substance but which simulates an illusion of life. I live because the mountains do not laugh and the worms do not sing.”\n\n\n“Everything that is formulated becomes more tolerable”\n\n\n“Cursed with the gift of consciousness, we are all inescapably forced into a beautiful confrontation of the void and the absurd inevitability of creating meaning and somethingness out of it.”\n\n\n“In other words, one of the greatest writers and thinkers of the century lived his life with his work buried in some drawer, aware, unaware, or indifferent to the fact that he was sitting on some the most significant works in modern history. He was, in the eyes of his father, an inadequate disappointment—and yet, in the eyes of history, he is an immensely important individual. One can only wonder how many individuals like Kafka have walked and continue to walk this earth, completely disconnected or restricted from ever seeing who they really are or could be. How many Kafkas have lived and died without ever sharing their voice with the world; voices that would have changed it forever?”\n\n\n“How many people never know who they’ll be after they’re gone?”\n\n\n“Perhaps in this, Kafka is suggesting that the struggle to find solace and understanding is both inescapable and impossible. As conscious, rational beings, we fight against the absurdity, trying to resolve the discrepancy between us and the universe. But ironically, we only serve to perpetuate the very struggle we are trying to resolve by trying to resolve the unresolvable. And in this sense, on some level, we almost want the struggle.”\n\n\n“Don’t bend; don’t water it down; don’t try to make it logical; don’t edit your own soul according to the fashion. Rather, follow your most intense obsessions mercilessly”\n\n\n“I think we ought to read only the kind of books that wound or stab us. If the book we’re reading doesn’t wake us up with a blow to the head, what are we reading for? So that it will make us happy … Good Lord, we would be happy precisely if we had no books. Kafka’s”\n\n\n“Existence precedes essence.””\n\n\n“life exists for itself. Beyond itself, it is intrinsically meaningless.”\n\n\n“We each have our little flickers of time here. No one else will ever know much, if anything, of what it’s like to be who we are. And for the most part, no one will ever really care. Our life is ultimately our life, and so long as we are not harming others in the process, we must create a life of our own meaning, determining our own objects of importance, committing to their pursuit, and reaping the significance and wonder of life along the way.”\n\n\n“However, perhaps it is less about getting a potential course of life right and more about attempting to do so with self-honesty and virtue—to live a life that can be looked back on with the knowledge that some of our decisions were perhaps wrong in their effects but right in their intention not to sell ourselves short.”\n\n\n“At any street corner, the feeling of absurdity can strike any man in the face.”\n\n\n“Does such a thing as ‘the fatal flaw,’ that showy dark crack running down the middle of a life, exist outside literature? I used to think it didn’t. Now I think it does. And I think that mine is this: a morbid longing for the picturesque at all costs.”\n\n\n“‘Perhaps in certain ways it is a helpful construct in talking about”\n\n\n“‘Psychology is a terrible word.’ He agreed vigorously. ‘Yes, it is terrible, isn’t it?’ he said, but with an expression that indicated that he thought it rather tasteless of me even to use it. ‘Perhaps in certain ways it is a helpful construct in talking about a certain kind of mind. The country people who live around me are fascinating because their lives are so closely bound to fate that they really are predestined.”\n\n\n“Why does that obstinate little voice in our heads torment us so?’ he said, looking round the table. ‘Could it be because it reminds us that we are alive, of our mortality, of our individual souls – which, after all, we are too afraid to surrender but yet make us feel more miserable than any other thing? But isn’t it also pain that often makes us most aware of self? It is a terrible thing to learn as a child that one is a being separate from all the world, that no one and no thing hurts along with one’s burned tongues and skinned knees, that one’s aches and pains are all one’s own. Even more terrible, as we grow older, to learn that no person, no matter how beloved, can ever truly understand us. Our own selves make us most unhappy, and that’s why we’re so anxious to lose them, don’t you think?”\n\n\n“‘Aristotle says in the Poetics,’ said Henry, ‘that objects such as corpses, painful to view in themselves, can become delightful to contemplate in a work of art.‘”\n\n\n“‘Death is the mother of beauty,’ said Henry. ‘And what is beauty?’ ‘Terror,’ ‘Well said,’ said Julian. ‘Beauty is rarely soft or consolatory. Quite the contrary. Genuine beauty is always quite alarming.‘”\n\n\n“‘We don’t like to admit it,’ said Julian, ‘but the idea of losing control is one that fascinates controlled people such as ourselves more than almost anything. All truly civilized people – the ancients no less than us – have civilized themselves through the willful repression of the old, animal self. Are we, in this room, really very different from the Greeks or the Romans? Obsessed with duty, piety, loyalty, sacrifice? All those things which are to modern tastes so chilling?‘”\n\n\n“‘And it’s a temptation for any intelligent person, and especially for perfectionists such as the ancients and ourselves, to try to murder the primitive, emotive, appetitive self. But that is a mistake.’ ‘Why?’ said Francis, leaning slightly forward. Julian arched an eyebrow; his long, wise nose gave his profile a forward tilt, like an Etruscan in a bas-relief. ‘Because it is dangerous to ignore the existence of the irrational. The more cultivated a person is, the more intelligent, the more repressed, then the more he needs some method of channeling the primitive impulses he’s worked so hard to subdue. Otherwise those powerful old forces will mass and strengthen until they are violent enough to break free, more violent for the delay, often strong enough to sweep the will away entirely.”\n\n\n“Pragma tists are often strangely superstitious.”\n\n\n“‘Do you remember what we were speaking of earlier, of how bloody, terrible things are sometimes the most beautiful?’ he said. ‘It’s a very Greek idea, and a very profound one. Beauty is terror. Whatever we call beautiful, we quiver before it. And what could be more terrifying and beautiful, to souls like the Greeks or our own, than to lose control completely? To throw off the chains of being for an instant, to shatter the accident of our mortal selves?”\n\n\n“Beauty is terror. We want to be devoured by it, to hide ourselves in that fire which refines us.‘”\n\n\n“The chronological sorting of memories is an interesting business.”\n\n\n“Grown children (an oxymoron, I realize) veer instinctively to extremes; the young scholar is much more a pedant than his older counterpart.”\n\n\n“I suppose there is a certain crucial interval in everyone’s life when character is fixed forever;”\n\n\n“Sometimes, when there’s been an accident and reality is too sudden and strange to comprehend, the surreal will take over. Action slows to a dreamlike glide, frame by frame; the motion of a hand, a sentence spoken, fills an eternity. Little things – a cricket on a stem, the veined branches on a leaf- are magnified, brought from the background in achingly clear focus.”\n\n\n“It seemed my whole life was composed of these disjointed fractions of time, hanging around in one public place and then another, as if I were waiting for trains that never came.”\n\n\n“But, like the Invisible Man in H. G. Wells, I discovered that my gift had its price, which took the form of, in my case as in his, a sort of mental darkness. It seemed that people failed to meet my eye, made as if to walk through me; my superstitions began to transform themselves into something like mania.”\n\n\n“drink tonight?’ One likes to think there’s something in it, that old platitude amor vincit omnia. But if I’ve learned one thing in my short sad life, it is that that particular platitude is a lie. Love doesn’t conquer everything. And whoever thinks it does is a fool.”\n\n\n“‘I suppose that when anyone accustomed to working with the mind is faced with a straightforward action, there’s a tendency to embellish, to make it overly clever. On paper there’s a certain symmetry. Now that I’m faced with the prospect of executing it I realize how hideously complicated it is.‘”\n\n\n“‘I never realized, you know, how much we rely on appearances,’”\n\n\n“When the snow finally melted it went as quickly as it had come.”\n\n\n“‘About a Hindu saint being able to slay a thousand on the battlefield and it not being a sin unless he felt remorse.‘”\n\n\n“Alas, poor gentleman, He look’d not like the ruins of his youth But like the ruins of those ruins.”\n\n\n“You must know, a truly sincere heart is always lustrous!””\n\n\n“Punishing one’s self for something one can no longer alter…that is nothing more than being made a fool of by fate! The only one who rules over myself…is myself!””\n\n\n“Live neither in the entanglements of outer things, nor in inner feelings of emptiness.”\n\n\n“When you try to stop activity by passivity your very effort fills you with activity.”\n\n\n“The more you talk and think about it, the further astray you wander from the truth.”\n\n\n“To return to the root is to find meaning, but to pursue appearances is to miss the source.”\n\n\n“Do not search for the truth; only cease to cherish opinions.”\n\n\n“A person’s true nature is only revealed when he’s pushed right to the edge, am I right? Now that their bellies are full, and they are feeling all nice and comfy, they are acting like a bunch of spoiled a.s.sholes. I don’t like guys like Kang Seok, but that b.a.s.t.a.r.d’s opinions aren’t half wrong.” ”….” “You continue being nice to them, they’ll eventually end up thinking that it’s their birthright or something. Well, in any case…. Don’t ever trust those two stinking b*tches, okay?”\n\n\n“He told himself, ‘you find yourself in a such a favorable position, so much better than compared to other people, yet is this all you can do?”\n\n\n“When he was still addicted to gambling, he was constantly on edge. The victim mentality took center stage in his heart and caused him to choke up over nothing important; often, he’d get defensive and angry even if he was in the wrong.”\n\n\n“Therefore whatever you desire for men to do to you, you shall also do to them; for this is the law and the prophets.” “Is that…. the Golden Rule?”\n\n\n“Hiya~. And I heard that when a girl bears a grudge, even snow will fall in the middle of a Summer”\n\n\n“There will be other expeditions in the future. I pray that you don’t bet everything on this one.”\n\n\n“Money and fame? Of course, they sounded nice. However, none of them compared to his own self-worth he had regained after such a struggle.”\n\n\n“Goodwill with goodwill, and malice with malice… I’ve learned a lot during this expedition.” “I agree. After all, humans aren’t the only race who possess intelligence.”\n\n\n“There is no right or wrong in matters of survival. In this world, whether you are a righteous man or of a wicked persuasion, you have to gather under one banner and pool your resources to survive. That is the case, even now.”\n\n\n“However, didn’t Gula say it? The future was not that easy to change. That he’d have to go through unimaginable trials and tribulations. That he needed to exceed his own limits”\n\n\n“Wait. Even if you don’t do anything, there will be people who curse you. The more famous you get, the more hate you’ll receive. Some people will even resent you. That’s not the end of it. There will be a ton of people who are going to try to use you, even if you didn’t do anything wrong.”\n\n\n“Just look at celebrities. Sure, some of them might deserve the hate they receive, but there are a lot more who don’t. Do you know why haters leave mean comments or attack them on their social media? It’s simple. Because they’re unhappy, because they want attention, because they’re bored, because they don’t like the way someone looks, because they just want to argue, because they’re jealous. There are countless reasons.”\n\n\n“What heals a wounded heart isn’t time or medicine. It’s sincerity”\n\n\n“Humans all have a limit to their vessel. The same goes for ghosts.”\n\n\n“I think life is like the four seasons. When spring pa.s.ses, summer comes. When summer leaves, autumn comes knocking. And when autumn departs, winter enters.”\n\n\n“In the four seasons of life, spring won’t come just by waiting.” ”….” “You have to endure the bitter cold and struggle to break through the frozen earth. Only then can you see the light of day and welcome spring.”\n\n\n“The moment the youth realized that he wasn’t special, the only thing he could do was put in painstaking, bloodcurdling effort.”\n\n\n“You’re right. It is hard. After all, you have to unite people who aren’t like you and people who aren’t like each other.”\n\n\n“They might think I’m just being c.o.c.ky.” “Only if you introduce yourself arrogantly. Depending on your att.i.tude or the situation, things might have gone in a different direction.”\n\n\n“Words have different weights depending on who says them. The words of a famous, authoritative person are different than the words of a nameless brat.” Kazuki muttered endlessly. “And fame is the strongest card in your possession. What’s wrong about using something you’ve built up fair and square”\n\n\n“You need to know your value a bit more,”\n\n\n“You should find a style that suits your nature. You know, wear the clothes that fit you.”\n\n\n“He who wishes to wear the crown, bear its weight.”\n\n\n“The bigger your goals are, the bigger burden you must face.”\n\n\n“A leader is not someone who is placed in that position by someone else. A leader is someone who wishes to become a leader himself.”\n\n\n“People tend to get hasty when they aren’t able to finish everything in time.”\n\n\n“In front of profit, justification changes depending on the situation and power at hand”\n\n\n“One could never predict everything in life, and life was bound to be full of ups and downs, but now that he had experienced this irony of fate, he couldn’t help but feel bewildered.”\n\n\n“One could never predict everything in life, and life was bound to be full of ups and downs, but now that he had experienced this irony of fate, he couldn’t help but feel bewildered. On the other hand, he felt a bit creeped out. Ian told him about this. That an action he considered insignificant might cause huge waves.”\n\n\n“Rather than struggling to overcome this emotion, he accepted it fully to get used to it”\n\n\n“You lose a war when you get scared.””\n\n\n“Life isn’t a game you can see the ending of after clicking on one or two choices, is it?”\n\n\n“Because you’re so uncomfortable with getting anything from my parents that they’re too careful with doing anything for you”\n\n\n“Enduring an injustice makes you a person, but enduring a loss makes you a pushover”\n\n\n“A world where one only pursues their own freedom and success, throwing aside all morals and responsibilities. A world poisoned by self-indulgence.””\n\n\n“New wine has to be brewed in a new keg”\n\n\n“‘make the body learn if the brain can’t understand’.”\n\n\n“To grow, one must first face their flaws.”\n\n\n“That was just how human psychology worked. After hogging a jar of honey for a long time, they wouldn’t feel comfortable sharing it with someone else.”\n\n\n“Seeing it once is better than hearing it a thousand times.”\n\n\n“There is no such thing as an eternal war. A war will end. Whether that be in a constructive way or a destructive way.”\n\n\n“There’s no rule saying that battles have to be conducted with weapons. You can use words, money, or even law, religion, pen, and other things as well.””\n\n\n“Do you know what the most important thing is when a scammer is preparing to strike?” Seol Jihu raised one of his eyebrows. Kim Hannah continued without batting an eye. “It’s simple — to make sure the person getting scammed doesn’t know he’s getting scammed.” ”….” “Only when the scam is successful would they realize, ‘Ah, that was a scam.’”\n\n\n“When apologizing for a mistake, one should not be overly dignified, but one should also not lower oneself more than necessary”\n\n\n“When apologizing for a mistake, one should not be overly dignified, but one should also not lower oneself more than necessary either.”\n\n\n“Unless there was someone to say harsh words and correct mistakes, a child would grow up without knowing right and wrong.”\n\n\n“—You’re a terrifying man if you said that intentionally. But if you said that sincerely, then you’re an even more terrifying person.”\n\n\n“I’ve learned a lot this time and found a lot of things I have to keep learning.” Seol Jihu candidly spoke. Hao Win peered at him from across the crystal. —…That’s your scariest trait. It was an unexpected remark. —You’re a terrifying man if you said that intentionally. But if you said that sincerely, then you’re an even more terrifying person. “Huh?” —People change as their accomplishments stack and their position grows higher. They begin to think, ‘I’ve earned this much. I’ve achieved all these things. What would you know?’ They naturally begin to get full of themselves. He crossed his arms and continued in a tired voice. —It’s not easy to keep your initial resolve. I’m no exception to this. Seol”\n\n\n“She remembered the saying, three men can speak a tiger into existence. Even a lie would seem real if enough people said it.”\n\n\n“There was a saying that people would confuse goodwill for privilege if it went on for too long”\n\n\n“I can’t do something that goes against the law, but I also can’t stand by and watch other people do things that go against the law. Being an unmoving spectator isn’t a crime, but it is injustice.”\n\n\n“Being talented does not mean being smart or intelligent. It also has nothing to do with how they normally act.”\n\n\n“As someone once said, it was foolish to resent someone for their inborn talent; rather, one should try to take one step every day for ten, twenty years. Then one day, they would meet the person they always wanted to become.”\n\n\n“Just like how a genius had their own path, an ordinary person had their own path.”\n\n\n“I thought I could do it somehow, but… Sometimes I get so tired like today…”\n\n\n“I thought I’d be fine…” He smacked his lips before continuing. “I thought I could do it somehow, but… Sometimes I get so tired like today…”\n\n\n“it was inevitable for amazing things to become familiar and new things to become worn-out. Such was the natural order of the world.”\n\n\n“I just was, actually. The past despair is what makes the present peace all the more precious, isn’t it?” Phi Sora flinched. She raised her head again”\n\n\n“Sure. Perhaps you really were trash like you say. I won’t say the past is past. Wrongdoings of the past are wrongdoings nonetheless. But even if you committed a terrible sin, depending on whether you take that opportunity to learn from your mistakes or remain the same, you can either be recycled or become a waste.””\n\n\n“But laying one’s weakness out in the open takes a lot of courage.”\n\n\n“Words, especially in philosophy, aren’t used to logically explain that one plus one is two. No matter how nice something sounds, you need to ruminate over it and interpret it in such a way that it benefits you personally.”\n\n\n“Words, especially in philosophy, aren’t used to logically explain that one plus one is two. No matter how nice something sounds, you need to ruminate over it and interpret it in such a way that it benefits you personally. Doubt is the origin of wisdom. Isn’t that what Descartes said?”\n\n\n“Humans are not born for the sake of existing. Humans exist first. They decide on the meaning of life and their own values afterward. Through their own choice.””\n\n\n“Existentialism emphasizes the freedom of choice and the consequence of that choice. Depending on what you choose to do and how you choose to take responsibility, you can decide what life you will lead and what death you will meet.””\n\n\n“In other words, human beings are not trapped by destiny. They are existences capable of pioneering their own fate. They can decide for themselves by choosing and taking responsibility.””\n\n\n“There are countless people in this world. Naturally, countless fates are intertwined with each other in incomprehensible ways. Kind of like the stars in the night sky.””\n\n\n“Looking at the rising sun, Seol Jihu vowed internally. To become a sun that gives off the light on its own. To become a star that can share its light to other people. Both in Paradise and on Earth.”\n\n\n“Those Who Meet Eventually Bid Farewell While Those Who Have Parted Eventually Meet Again”\n\n\n“Look. There’s a limit to improving your skills through simple repet.i.tive training. Even spear techniques like Thrust, Strike, and Cut have their limits. And of course, mindless repet.i.tion is even less effective for physical body skills like Intuition.””\n\n\n“Stop a.s.suming that if you keep trying, it will somehow work. Don’t you have a brain? You’ve walked this path more than a thousand times already.””\n\n\n“If your destination is far away, you need to think about getting there step”\n\n\n“Every flow has its ebbs, all that’s fair must fade.”\n\n\n“You’re an engineering major, right?” “Yes.” “I think sometimes you just think too hard. When you solve a problem, you expect each step to be logical and precise, like math.””\n\n\n“A soft answer turns away wrath. Sometimes the same word changes meaning depending on the context. You say one thing, and others will interpret it in hundreds of different ways.”\n\n\n“By lost, I mean that we momentarily lose touch with ourselves and with the full extent of our possibilities. Instead, we fall into a robotlike way of seeing and thinking and doing. In those moments, we break contact with what is deepest in ourselves and affords us perhaps our greatest opportunities for creativity, learning, and growing. If we are not careful, those clouded moments can stretch out and become most of our lives.”\n\n\n“We pay a high price for this mistaken and unexamined assumption, for our almost willful ignoring of the richness of our present moments. The fallout accumulates silently, coloring our lives without our knowing it or being able to do something about it. We may never quite be where we actually are, never quite touch the fullness of our possibilities. Instead, we lock ourselves into a personal fiction that we already know who we are, that we know where we are and where we are going, that we know what is happening—all the while remaining enshrouded in thoughts, fantasies, and impulses, mostly about the past and about the future, about what we want and like, and what we fear and don’t like, which spin out continuously, veiling our direction and the very ground we are standing on.”\n\n\n“Only that day dawns to which we are awake.”\n\n\n“understanding the nature of life and mind. Intelligence is the door to freedom and alert attention is the mother”\n\n\n“Intelligence is the door to freedom and alert attention is the mother of intelligence.”\n\n\n“expectations. So, in meditation practice, the best way to get somewhere is to let go of trying to get anywhere at all.”\n\n\n“If your mind isn’t clouded by unnecessary things, This is the best season of your life.”\n\n\n“mistakes were often as revealing as the right answers.”\n\n\n“The truly correct proof is one that strikes a harmonious balance between strength and flexibility. There are plenty of proofs that are technically correct but are messy and inelegant or counterintuitive. But it’s not something you can put into words—explaining why a formula is beautiful is like trying to explain why the stars are beautiful.”\n\n\n“It was just a little puzzle,” he would say, “a game”; and his tone sounded more sad than modest. “The person who made the problem already knew the answer. Solving a problem for which you know there’s an answer is like climbing a mountain with a guide, along a trail someone else has laid. In mathematics, the truth is somewhere out there in a place no one knows, beyond all the beaten paths. And it’s not always at the top of the mountain. It might be in a crack on the smoothest cliff or somewhere deep in the valley.”\n\n\n“A problem has a rhythm of its own, just like a piece of music,” the Professor said. “Once you get the rhythm, you get the sense of the problem as a whole, and you can see where the traps might be waiting.”\n\n\n“Math has proven the existence of God, because it is absolute and without contradiction; but the devil must exist as well, because we cannot prove it.”\n\n\n“Eternal truths are ultimately invisible, and you won’t find them in material things or natural phenomena, or even in human emotions. Mathematics, however, can illuminate them, can give them expression—in fact, nothing can prevent it from doing so.”\n\n\n“So you think that zero was there waiting for us when humans came into being, like the flowers and the stars? You should have more respect for human progress. We made the zero, through great pain and struggle.”\n\n\n“Attention is the beginning of devotion”\n\n\n“Sometimes the things you hate are the things you need”\n\n\n“The absurd does not liberate; it binds.”\n\n\n“THE WISDOM OF LIFE CONSISTS IN THE ELIMINATION OF NONESSENTIALS.”\n\n\n“In this example is the basic value proposition of Essentialism: only once you give yourself permission to stop trying to do it all, to stop saying yes to everyone, can you make your highest contribution towards the things that really matter.”\n\n\n“It took courage, as it always does, to eliminate the nonessential.”\n\n\n“The way of the Essentialist means living by design, not by default.”\n\n\n“the pursuit of success can be a catalyst for failure.”\n\n\n“the more choices we are forced to make, the more the quality of our decisions deteriorates.”\n\n\n“It is not just the number of choices that has increased exponentially, it is also the strength and number of outside influences on our decisions that has increased.”\n\n\n“Instead of reacting to the social pressures pulling you to go in a million directions, you will learn a way to reduce, simplify, and focus on what is absolutely essential by eliminating everything else.”\n\n\n“If I didn’t already own this, how much would I spend to buy it?”\n\n\n“One paradox of Essentialism is that Essentialists actually explore more options than their Nonessentialist counterparts.”\n\n\n“Remember, when we forfeit our right to choose, someone else will choose for us.”\n\n\n“There is tremendous freedom in learning that we can eliminate the nonessentials, that we are no longer controlled by other people’s agendas, and that we get to choose.”\n\n\n“That’s when I realized that in sacrificing my power to choose I had made a choice—a bad one.”\n\n\n“The ability to choose cannot be taken away or even given away—it can only be forgotten.”\n\n\n“My first act of free will shall be to believe in free will.”\n\n\n“MOST OF WHAT EXISTS IN THE UNIVERSE—OUR ACTIONS, AND ALL OTHER FORCES, RESOURCES, AND IDEAS—HAS LITTLE VALUE AND YIELDS LITTLE RESULT; ON THE OTHER HAND, A FEW THINGS WORK FANTASTICALLY WELL AND HAVE TREMENDOUS IMPACT.”\n\n\n“the Law of the Vital Few.”2 His observation was that you could massively improve the quality of a product by resolving a tiny fraction of the problems.”\n\n\n“You cannot overestimate the unimportance of practically everything.”9”\n\n\n“You have to look at every opportunity and say, ‘Well, no … I’m sorry. We’re not going to do a thousand different things that really won’t contribute much to the end result we are trying to achieve.’ ”\n\n\n“A strategic position is not sustainable unless there are trade-offs with other positions.”\n\n\n“I didn’t start out with the goal of devoting all of myself to my job. It crept in over time. Each year that went by, slight modifications became the new normal. First I spent a half-hour on Sunday organizing my e-mail, to-do list, and calendar to make Monday morning easier. Then I was working a few hours on Sunday, then all day. My boundaries slipped away until work was all that was left.”\n\n\n“to be acted upon. As economist Thomas Sowell wrote: “There are no solutions. There are only trade-offs.”7 Jim”\n\n\n“Trade-offs are not something to be ignored or decried. They are something to be embraced and made deliberately, strategically, and thoughtfully.”\n\n\n“Ironically, in a Nonessentialist culture these things—space, listening, playing, sleeping, and selecting—can be seen as trivial distractions. At best they are considered nice to have. At worst they are derided as evidence of weakness and wastefulness”\n\n\n“If somebody can’t make the meeting because of too much going on, that tells me either we’re doing something inefficiently or we need to hire more people.”\n\n\n“For some reason there is a false association with the word focus. As with choice, people tend to think of focus as a thing. Yes, focus is something we have. But focus is also something we do.”\n\n\n“Of course, nobody likes to be bored. But by abolishing any chance of being bored we have also lost the time we used to have to think and process.”\n\n\n“WHERE IS THE KNOWLEDGE WE HAVE LOST IN INFORMATION?”\n\n\n“As someone once said to me, the faintest pencil is better than the strongest memory.”\n\n\n“A LITTLE NONSENSE NOW AND THEN, IS CHERISHED BY THE WISEST MEN.”\n\n\n“We have sold ourselves into a fast-food model of education, and it’s impoverishing our spirit and our energies as much as fast food is depleting our physical bodies.… Imagination is the source of every form of human achievement. And it’s the one thing that I believe we are systematically jeopardizing in the way we educate our children and ourselves.”2 In this he is correct. This idea that play is trivial stays with us as we reach adulthood and only becomes more ingrained as we enter the workplace. Sadly, not only do far too few companies and organizations foster play; many unintentionally undermine it. True, some companies and executives give lip service to the value of play in sparking creativity, yet most still fail to create the kind of playful culture that sparks true exploration. None of this should surprise us. Modern corporations were born out of the Industrial Revolution, when their entire reason for being was to achieve efficiency in the mass production of goods. Furthermore, these early managers looked to the military—a rather less-than-playful entity—for their inspiration (indeed, the language of the military is still strong in corporations today; we still often talk of employees being on the front lines, and the word company itself is a term for a military unit). While the industrial era is long behind us, those mores, structures, and systems continue to pervade most modern organizations. Play, which I would define as anything we do simply for the joy of doing rather than as a means to an end—whether”\n\n\n“When I examine myself and my methods of thought, I come to the conclusion that the gift of fantasy has meant more to me than my talent for absorbing positive knowledge.”\n\n\n“Pushing oneself to the limit is easy! The real challenge for the person who thrives on challenges is not to work hard.”\n\n\n“If you think you are so tough you can do anything I have a challenge for you. If you really want to do something hard: say no to an opportunity so you can take a nap.”\n\n\n“If the answer isn’t a definite yes then it should be a no.”\n\n\n“So why is it so hard in the moment to dare to choose what is essential over what is nonessential?”\n\n\n“So why is it so hard in the moment to dare to choose what is essential over what is nonessential? One simple answer is we are unclear about what is essential. When this happens we become defenseless.”\n\n\n“Nonessentialists say yes because of feelings of social awkwardness and pressure. They say yes automatically, without thinking, often in pursuit of the rush one gets from having pleased someone. But Essentialists know that after the rush comes the pang of regret. They know they will soon feel bullied and resentful—both at the other person and at themselves.”\n\n\n“When people ask us to do something, we can confuse the request with our relationship with them. Sometimes they seem so interconnected, we forget that denying the request is not the same as denying the person.”\n\n\n“But part of living the way of the Essentialist is realizing respect is far more valuable than popularity in the long run.”\n\n\n“Saying no is its own leadership capability. It is not just a peripheral skill. As with any ability, we start with limited experience.”\n\n\n“Saying no is its own leadership capability. It is not just a peripheral skill. As with any ability, we start with limited experience. We are novices at “no.”\n\n\n“HALF OF THE TROUBLES OF THIS LIFE CAN BE TRACED TO SAYING YES TOO QUICKLY AND NOT SAYING NO SOON ENOUGH.”\n\n\n“nobody in the history of the world has washed their rental car!”\n\n\n“Abandoning a project that you’ve invested a lot in feels like you’ve wasted everything, and waste is something we’re told to avoid,”\n\n\n“It’s natural not to want to let go of what we wasted on a bad choice, but when we don’t, we doom ourselves to keep wasting even more.”\n\n\n“Instead of trying to budget your time on the basis of existing commitments, assume that all bets are off. All previous commitments are gone. Then begin from scratch, asking which you would add today.”\n\n\n“In a reverse pilot you test whether removing an initiative or activity will have any negative consequences.”\n\n\n“In other words, a good film editor makes it hard not to see what’s important because she eliminates everything but the elements that absolutely need to be there.”\n\n\n“It’s true that doing less can be harder, both in art and in life.”\n\n\n“The best surgeon is not the one who makes the most incisions; similarly, the best editors can sometimes be the least intrusive, the most restrained.”\n\n\n“NO IS A COMPLETE SENTENCE.”\n\n\n“But what most people don’t realize is that the problem is not just that the boundaries have been blurred; it’s that the boundary of work has edged insidiously into family territory. It is hard to imagine executives in most companies who would be comfortable with employees bringing their children to work on Monday morning, yet they seem to have no problem expecting their employees to come into the office or to work on a project on a Saturday or Sunday.”\n\n\n“Boundaries are a little like the walls of a sandcastle. The second we let one fall over, the rest of them come crashing down.”\n\n\n“Remember, forcing these people to solve their own problems is equally beneficial for you and for them.”\n\n\n“when we don’t set clear boundaries in our lives we can end up imprisoned by the limits others have set for us.”\n\n\n“The only thing we can expect (with any great certainty) is the unexpected. Therefore, we can either wait for the moment and react to it or we can prepare. We can create a buffer.”\n\n\n“Instead of trying to improve every aspect of the facility he needs to identify the “Herbie”: the part of the process that is slower relative to every other part of the plant”\n\n\n“Being good with a hammer, the Nonessentialist thinks everything is a nail. Thus he applies more and more pressure, but this ends up only adding more friction and frustration. Indeed, in some situations the harder you push on someone the harder he or she will push back.”\n\n\n“When we don’t know what we’re really trying to achieve, all change is arbitrary.”\n\n\n“Removing obstacles does not have to be hard or take a superhuman effort. Instead, we can start small. It’s kind of like dislodging a boulder at the top of a hill. All it takes is a small shove, then momentum will naturally build.”\n\n\n“Each time a young person was recognized and commended for doing something good, he or she was that much more motivated to continue doing good until, eventually, doing good became natural and effortless.”\n\n\n“of all forms of human motivation the most effective one is progress.”\n\n\n“It is the process Pixar uses on their movies. Instead of starting with a script, they start with storyboards—or what have been described as the comic book version of a movie. They try ideas out and see what works. They do this in small cycles hundreds of times. Then they put out a movie to small groups of people to give them advance feedback.”\n\n\n“We don’t actually finish our films, we release them.”\n\n\n“ROUTINE, IN AN INTELLIGENT MAN, IS A SIGN OF AMBITION.”\n\n\n“This power of a routine grows out of our brain’s ability to take over entirely until the process becomes fully unconscious.”\n\n\n“There is a difference between losing and being beaten. Being beaten means they are better than you. They are faster, stronger, and more talented.” To Larry, losing means something else. It means you lost focus. It means you didn’t concentrate on what was essential.”\n\n\n“Multitasking itself is not the enemy of Essentialism; pretending we can “multifocus” is.”\n\n\n“Suppose you are drinking a cup of tea. When you hold your cup, you may like to breathe in, to bring your mind back to your body, and you become fully present. And when you are truly there, something else is also there—life, represented by the cup of tea. In that moment you are real, and the cup of tea is real. You are not lost in the past, in the future, in your projects, in your worries. You are free from all of these afflictions. And in that state of being free, you enjoy your tea. That is the moment of happiness, and of peace.” Pay attention through the day”\n\n\n“BEWARE THE BARRENNESS OF A BUSY LIFE.”\n\n\n“Every choice we make to pursue the essential and eliminate the nonessential builds on itself, making that choice more and more habitual until it becomes virtually second nature.”\n\n\n“I continue to discover almost daily that I can do less and less—in order to contribute more.”\n\n\n“If one’s life is simple, contentment has to come. Simplicity is extremely important for happiness.”\n\n\n“The course of history is determined not by battles, by sieges, or usurpations, but by the actions of the individual. The strongest city, the largest army is, at its most basic level, a collection of individuals. Their decisions, their passions, their foolishness, and their dreams shape the years to come. If there is any lesson to be learned from history, it is that all too often the fate of armies, of cities, of entire realms rests upon the actions of one person. In that dire moment of uncertainty, that person’s decision, good or bad, right or wrong, big or small, can unwittingly change the world. But history can be quite the slattern. One never knows who that person is, where he might be, or what decision he might make. It is almost enough to make me believe in Destiny.”\n\n\n“I think men who lust for power are capable of almost anything.”\n\n\n“When you reach my age, Amara, people show themselves to you very clearly. They write their intentions and beliefs through their actions, their lies.”\n\n\n“But the power to shake mountains does little good if the knife is already buried in one’s throat”\n\n\n“There are some people who will never understand what loyalty means. They could tell you what it was, of course, but they will never know. They will never see it from the inside. They couldn’t imagine a world where something like that was real.”\n\n\n“Courage.” Tavi sighed. “As near as I can figure it, all courage gets you is more of a beating than if you’d run away.”\n\n\n“There’s two kinds of bad men in the world. I mean, there’s all kinds of ways for a man to go bad, but when you get right down to it, there’s only about two kinds of men who will hurt others with forethought. Premeditation. Men that don’t figure there’s anyone else alive who matters but them. And men who figure that there’s something that matters more than anyone’s life. Even their own.” He shook his head. “First one is common enough. Petty, small. They’re everywhere. People who just don’t give a scorched crow about anyone else. Mostly, the bad they do doesn’t amount to much. “The second kind is like your patriserus. People who hold something dear above their own lives, above anyone else’s. They’ll fight to protect it and kill to protect it, and the whole time they’ll be thinking to themselves that it has to be done. That it’s the right thing to do.” Bernard glanced up at her and said, “Dangerous those. Very dangerous.”\n\n\n“Sometimes,” Bernard rumbled, “the only smart thing to do is nothing. Sometimes you just have to be still and see how events begin to unfold before you move. Be patient.”\n\n\n“If the beginning of wisdom is in realizing that one knows nothing, then the beginning of understanding is in realizing that all things exist in accord with a single truth: large things are made of smaller things. Drops of ink are shaped into letters, letters form words, words form sentences, and sentences combine to express thought. So it is with the growth of plants that spring from seeds, as well as with walls built of many stones. So it is with mankind, as the customs and traditions of our progenitors blend together to form the foundation for our own cities, history, and way of life. Be they dead stone, living flesh, or rolling sea; be they idle times or events of world-shattering proportion, market days, or desperate battles, to this law, all things hold: Large things are made from small things. Significance is cumulative—but not always obvious.”\n\n\n“Don’t struggle to heal your wounds. Just pour time into your heart and wait”\n\n\n“you’ve been unable to change a bad situation, even after many attempts, you should”\n\n\n“If you’ve been unable to change a bad situation, even after many attempts, you should change how you look at the situation”\n\n\n“Awareness is inherently pure, like the open sky. Stress, irritation, and anger can temporarily cloud the sky, but they can never pollute it.”\n\n\n“The martial path was lonely. It was a source of great happiness if one found a confidant.”\n\n\n“The Azure Yang Lord’s uninhibited life was filled with ups and down. Having felt both extreme happiness and sadness, he knew how difficult it was to earn happiness. If he could make this moment last forever, that would be a blissful matter… However, be it the Azure Yang Lord or the ancient Great Empress, they knew that these days were not going to last.”\n\n\n“Warriors had to suffer hardship and experience numerous life and death encounters. If they were not careful, they would lose their lives. They had to restrain their desires and endure lonely decades of reclusive training. Wasn’t all this for them to lead a free life and do as they pleased when their power succeeded on reaching unprecedented heights!? Everything in the world depended on one’s preference. To do as the heart pleased!”\n\n\n“The ways of the world are full of vicissitudes, and in it, there is the grief at separation and joy in union, the suffering of life and death. No matter how thick a history book is, it would not be able to record everything down. However, it is such infinite matters of the past that can pass by with a finger snap. In one’s old age, while looking back at the past, only then would you feel like everything was ephemeral.”\n\n\n“The further he walked, the tinier he felt.”\n\n\n“Against the vast cosmos, the lives of people were like ants. It was like how bacteria did not know what it meant for the first and last day of the month, or how mole crickets did not know the seasons. What was a mortal’s life to the Universe? Yi Yun did not want to remain like a speck of dust in the Universe.”\n\n\n“In a warrior’s world, we may have the power to cause great destruction, have long lifespans, and are able to lead extravagant lives, but the pressure is intense. Be it life-and-death trials or breakthroughs that require great risks, or the sense of urgency of being killed by someone else at any moment in time, it forces us to continuously forge ahead.” “The accumulation of all these pressure naturally need an opportunity for it to be released.”\n\n\n“Before one’s heart reduced to mediocrity, Great Dao lies within one’s heart!”\n\n\n“Those without sufficient strength but can bear disgrace and a heavy burden are people who I appreciate as well.”\n\n\n“Time is like a flowing river. Everything and anything can be washed away by that river.”\n\n\n“Everyone had their own time, but time treated everyone equally.”\n\n\n“Were destruction and finality the end? Matter in the Universe could not remain eternal. They would eventually proceed towards destruction, including the Universe. But what happened after destruction? Could it be that all that was left was eternal ‘nothingness’? Everything in the world underwent birth and death. A drop of water could evaporate and become part of a cloud before condensing into new drops of rain. Plants would wither, but the fruits that they bore could give rise to seeds. Mortals would die of age, but babies would grow up into adults. Stars could be destroyed, but new stars would eventually be born… Everything in the world underwent a cycle, so it definitely included the Universe. The destruction of the Universe was the beginnings of a new Universe. It was just that the cycle was immensely long that it was beyond the imagination of mere mortals. New Universe… Yi Yun suddenly seemed to realize something. What were the beginnings of the Universe? Dao begets One, One begets Two… Before Yin-Yang and Space-time was Chaos! Rebirth after destruction was Chaos and Major Destruction respectively. They were both two Great Dao of Supremacy! Why were there two Great Dao of Supremacy? They might be like Yin-Yang, Space-time, Water-Fire, just two sides of the Universe. The two could supplement each other and not a single one was dispensable.”\n\n\n“But if a warrior does not have any aspirations, he won’t be able to go far either.”\n\n\n“To what end do we practice martial arts for? If we can’t depend on the sword, so what about death? One’s true richness of heart is not to be annihilated!”\n\n\n“Helping a man once gains gratitude, but not helping that man again gains hatred.”\n\n\n“grinding a chopper will not hold up the work of cutting firewood”\n\n\n“As the saying goes, he who gets other’s kindness but does not repay is not a gentleman.”\n\n\n“History changes erratically. Time is enough to wipe away many things.”\n\n\n“Chaos in the world leads to the suffering of all life, but it is something some people are willing to see. Heroes are born in difficult times.”\n\n\n“Humans were warlike, to begin with. It was common among humans to fight unceasingly for their interests. Furthermore, there were some perverse people who wanted revenge on the world. Such people were even more dangerous. There was no limit to what they would do. Yi Yun knew that there was evil deeply rooted in human nature. However, he was not a person who would bemoan the state of the universe and mankind, much less the kind who would question life after seeing the ugly nature of humans and eventually decide to destroy the entire world in a crazy fit. He was only Yi Yun, an ordinary person that pursued the martial path, wanting his life to escape the cycle of samsara.”\n\n\n“When one postures to impress a girl, his failure becomes all the more foolish.”\n\n\n“When warriors took lives or destroyed an item, or even shattered a world, they could not avoid violence. Only time could turn everything into dust in an infinitely calm manner simply by passing. It was silent from beginning to end. Time was the most unique power of destruction. Yi”\n\n\n“A man who loses position and influence may be subjected to much indignity. It’s all fated…”\n\n\n“He who understands the times is a wise man.”\n\n\n“Yi Yun refused to believe that someone would forgo their lives for a master who had not won the hearts of people.”\n\n\n“We warriors have been cultivating our entire lives. What was it for? It’s not to enjoy riches or sex. Nor is it about being placed on a pedestal while enslaving those beneath us! We cultivate to fight against our fate! We do not wish to to let Samsara run its course, so we cultivate for years! Our martial heritage has been passed down for millions of years, billions of years, and will be so for trillions of years! It’s not for us to kneel down to beg for mercy after gaining mastery. With the backbone broken, there’s no way one can straighten one’s back.”\n\n\n“The Heavenly Dao’s evolution is unending. Creation and Destruction are interchangeable, forming a complete circle. Be it life or a universe, they have formation, existence, disintegration, and emptiness; birth, aging, illness, and death. You wanted to devour the Heavenly Dao, robbing and changing the fate of the universe. You wanted to empower yourself by initiating the deaths of millions; hence, you were destined to be abandoned by the Heavenly Dao.”\n\n\n“Legends were typically like this. They would not be eclipsed with the passage of time, but instead, they became histories that one tried to live up to.”\n\n\n“Life, from lifeforms as large as towering trees to bugs as small as ants, the reason for all they do is for the continuation of their bloodline.”\n\n\n“In this Sinkhole that was filled with despair, a bunch of seniors were forging ahead bearing the burdens, so as to hold up the sky for the young. Although these seniors had their backs hunched from the pressures of time, to the point of not being able to bear the weight any further, that was already sufficient, no matter how tiny that piece of sky they held up was. They had already done everything they could have done.”\n\n\n“The crests and troughs of life are all but ephemeral mists. How nice if reunion and separation were fixed in place of that first meeting. A thousand years pass and happiness a certainty, but how much was sadness in exchange?”\n\n\n“Every artist has thousands of bad drawings in them and the only way to get rid of them is to draw them out. - Chuck Jones”\n\n\n“I spent years trying to behave “appropriately” so that other people would accept me, because underneath I felt like my true self was unworthy and underserving. Once I unravelled this ball I realized that I was just as worthy and deserving as anyone else, and I could start being myself—my true self.”\n\n\n“Realize that 9 times out of 10, when you’re worried about what other people think - it’s a projection. You’re projecting your own fears and your own internalized self-judgment onto other people. You’re pinning on them what you yourself think. So when we take responsibility for letting go of other people’s judgments we empower ourselves to stop being harsh and judgmental with ourselves too. Because ultimately they go hand-in-hand.”\n\n\n“Often when we get lost in worrying about what everyone else thinks it’s because we’re existing in a state of perpetual comparison. We look at what everyone else is doing and think that unless we’re doing similarly we failed”\n\n\n“For me I think the lack of “off leash” non-self-regulation time also ties directly into the “revenge bedtime procrastination” phenom. I often find myself doing practically nothing or the mindless scrolling even though I am physically and mentally tired and WANT to sleep, but my brain is like “I HAVENT HAD A RUN AROUND TODAY WE MUST STAY UP”\n\n\n“The absurd does not liberate; it binds.”\n\n\n“The wisdom of life consists in the elimination of nonessentials.”\n\n\n“In this example is the basic value proposition of Essentialism: only once you give yourself permission to stop trying to do it all, to stop saying yes to everyone, can you make your highest contribution towards the things that really matter.”\n\n\n“It took courage, as it always does, to eliminate the nonessential.”\n\n\n“The way of the Essentialist means living by design, not by default.”\n\n\n“the pursuit of success can be a catalyst for failure.”\n\n\n“the more choices we are forced to make, the more the quality of our decisions deteriorates.”\n\n\n“It is not just the number of choices that has increased exponentially, it is also the strength and number of outside influences on our decisions that has increased.”\n\n\n“Instead of reacting to the social pressures pulling you to go in a million directions, you will learn a way to reduce, simplify, and focus on what is absolutely essential by eliminating everything else.”\n\n\n“If I didn’t already own this, how much would I spend to buy it?”\n\n\n“One paradox of Essentialism is that Essentialists actually explore more options than their Nonessentialist counterparts.”\n\n\n“Remember, when we forfeit our right to choose, someone else will choose for us.”\n\n\n“There is tremendous freedom in learning that we can eliminate the nonessentials, that we are no longer controlled by other people’s agendas, and that we get to choose.”\n\n\n“That’s when I realized that in sacrificing my power to choose I had made a choice—a bad one.”\n\n\n“The ability to choose cannot be taken away or even given away—it can only be forgotten.”\n\n\n“My first act of free will shall be to believe in free will.”\n\n\n“Most of what exists in the universe—our actions, and all other forces, resources, and ideas—has little value and yields little result; on the other hand, a few things work fantastically well and have tremendous impact.”\n\n\n“the Law of the Vital Few.  His observation was that you could massively improve the quality of a product by resolving a tiny fraction of the problems.”\n\n\n“You cannot overestimate the unimportance of practically everything.\n\n\n“You have to look at every opportunity and say, ‘Well, no … I’m sorry. We’re not going to do a thousand different things that really won’t contribute much to the end result we are trying to achieve.’ ”\n\n\n“A strategic position is not sustainable unless there are trade-offs with other positions.”\n\n\n“I didn’t start out with the goal of devoting all of myself to my job. It crept in over time. Each year that went by, slight modifications became the new normal. First I spent a half-hour on Sunday organizing my e-mail, to-do list, and calendar to make Monday morning easier. Then I was working a few hours on Sunday, then all day. My boundaries slipped away until work was all that was left.”\n\n\n“to be acted upon. As economist Thomas Sowell wrote: “There are no solutions. There are only trade-offs.”7 Jim”\n\n\n“Trade-offs are not something to be ignored or decried. They are something to be embraced and made deliberately, strategically, and thoughtfully.”\n\n\n“Ironically, in a Nonessentialist culture these things—space, listening, playing, sleeping, and selecting—can be seen as trivial distractions. At best they are considered nice to have.”\n\n\n“Ironically, in a Nonessentialist culture these things—space, listening, playing, sleeping, and selecting—can be seen as trivial distractions. At best they are considered nice to have. At worst they are derided as evidence of weakness and wastefulness”\n\n\n“If somebody can’t make the meeting because of too much going on, that tells me either we’re doing something inefficiently or we need to hire more people.”\n\n\n“For some reason there is a false association with the word focus. As with choice, people tend to think of focus as a thing. Yes, focus is something we have. But focus is also something”\n\n\n“For some reason there is a false association with the word focus. As with choice, people tend to think of focus as a thing. Yes, focus is something we have. But focus is also something we do.”\n\n\n“Of course, nobody likes to be bored. But by abolishing any chance of being bored we have also lost the time we used to have to think and process.”\n\n\n“Where is the knowledge we have lost in information?”\n\n\n“As someone once said to me, the faintest pencil is better than the strongest memory.”\n\n\n“A little nonsense now and then, is cherished by the wisest men.”\n\n\n“We have sold ourselves into a fast-food model of education, and it’s impoverishing our spirit and our energies as much as fast food is depleting our physical bodies.… Imagination is the source of every form of human achievement. And it’s the one thing that I believe we are systematically jeopardizing in the way we educate our children and ourselves.”2 In this he is correct. This idea that play is trivial stays with us as we reach adulthood and only becomes more ingrained as we enter the workplace. Sadly, not only do far too few companies and organizations foster play; many unintentionally undermine it. True, some companies and executives give lip service to the value of play in sparking creativity, yet most still fail to create the kind of playful culture that sparks true exploration. None of this should surprise us. Modern corporations were born out of the Industrial Revolution, when their entire reason for being was to achieve efficiency in the mass production of goods. Furthermore, these early managers looked to the military—a rather less-than-playful entity—for their inspiration (indeed, the language of the military is still strong in corporations today; we still often talk of employees being on the front lines, and the word company itself is a term for a military unit). While the industrial era is long behind us, those mores, structures, and systems continue to pervade most modern organizations. Play, which I would define as anything we do simply for the joy of doing rather than as a means to an end—whether”\n\n\n“When I examine myself and my methods of thought, I come to the conclusion that the gift of fantasy has meant more to me than my talent for absorbing positive knowledge.”\n\n\n“Pushing oneself to the limit is easy! The real challenge for the person who thrives on challenges is not to work hard.”\n\n\n“If you think you are so tough you can do anything I have a challenge for you. If you really want to do something hard: say no to an opportunity so you can take a nap.”\n\n\n“If the answer isn’t a definite yes then it should be a no.”\n\n\n“So why is it so hard in the moment to dare to choose what is essential over what is nonessential?”\n\n\n“So why is it so hard in the moment to dare to choose what is essential over what is nonessential? One simple answer is we are unclear about what is essential. When this happens we become defenseless.”\n\n\n“Nonessentialists say yes because of feelings of social awkwardness and pressure. They say yes automatically, without thinking, often in pursuit of the rush one gets from having pleased someone. But Essentialists know that after the rush comes the pang of regret. They know they will soon feel bullied and resentful—both at the other person and at themselves.”\n\n\n“When people ask us to do something, we can confuse the request with our relationship with them. Sometimes they seem so interconnected, we forget that denying the request is not the same as denying the person.”\n\n\n“But part of living the way of the Essentialist is realizing respect is far more valuable than popularity in the long run.”\n\n\n“Saying no is its own leadership capability. It is not just a peripheral skill. As with any ability, we start with limited experience. We are novices at “no.”\n\n\n“Half of the troubles of this life can be traced to saying yes too quickly and not saying no soon enough.”\n\n\n“nobody in the history of the world has washed their rental car!”\n\n\n“Abandoning a project that you’ve invested a lot in feels like you’ve wasted everything, and waste is something we’re told to avoid,”\n\n\n“It’s natural not to want to let go of what we wasted on a bad choice, but when we don’t, we doom ourselves to keep wasting even more.”\n\n\n“Instead of trying to budget your time on the basis of existing commitments, assume that all bets are off. All previous commitments are gone. Then begin from scratch, asking which you would add today.”\n\n\n“In a reverse pilot you test whether removing an initiative or activity will have any negative consequences.”\n\n\n“In other words, a good film editor makes it hard not to see what’s important because she eliminates everything but the elements that absolutely need to be there.”\n\n\n“It’s true that doing less can be harder, both in art and in life.”\n\n\n“The best surgeon is not the one who makes the most incisions; similarly, the best editors can sometimes be the least intrusive, the most restrained.”\n\n\n“No is a complete sentence.”\n\n\n“But what most people don’t realize is that the problem is not just that the boundaries have been blurred; it’s that the boundary of work has edged insidiously into family territory. It is hard to imagine executives in most companies who would be comfortable with employees bringing their children to work on Monday morning, yet they seem to have no problem expecting their employees to come into the office or to work on a project on a Saturday or Sunday.”\n\n\n“Boundaries are a little like the walls of a sandcastle. The second we let one fall over, the rest of them come crashing down.”\n\n\n“Remember, forcing these people to solve their own problems is equally beneficial for you and for them.”\n\n\n“When we don’t set clear boundaries in our lives we can end up imprisoned by the limits others have set for us.”\n\n\n“The only thing we can expect (with any great certainty) is the unexpected. Therefore, we can either wait for the moment and react to it or we can prepare. We can create a buffer.”\n\n\n“Instead of trying to improve every aspect of the facility he needs to identify the “Herbie”: the part of the process that is slower relative to every other part of the plant”\n\n\n“Being good with a hammer, the Nonessentialist thinks everything is a nail. Thus he applies more and more pressure, but this ends up only adding more friction and frustration. Indeed, in some situations the harder you push on someone the harder he or she will push back.”\n\n\n“When we don’t know what we’re really trying to achieve, all change is arbitrary.”\n\n\n“Removing obstacles does not have to be hard or take a superhuman effort. Instead, we can start small. It’s kind of like dislodging a boulder at the top of a hill. All it takes is a small shove, then momentum will naturally build.”\n\n\n“Each time a young person was recognized and commended for doing something good, he or she was that much more motivated to continue doing good until, eventually, doing good became natural and effortless.”\n\n\n“of all forms of human motivation the most effective one is progress.”\n\n\n“It is the process Pixar uses on their movies. Instead of starting with a script, they start with storyboards—or what have been described as the comic book version of a movie. They try ideas out and see what works. They do this in small cycles hundreds of times. Then they put out a movie to small groups of people to give them advance feedback.”\n\n\n“We don’t actually finish our films, we release them.”\n\n\n“Routine, in an intelligent man, is a sign of ambition.”\n\n\n“This power of a routine grows out of our brain’s ability to take over entirely until the process becomes fully unconscious.”\n\n\n“There is a difference between losing and being beaten. Being beaten means they are better than you. They are faster, stronger, and more talented.” To Larry, losing means something else. It means you lost focus. It means you didn’t concentrate on what was essential.”\n\n\n“Multitasking itself is not the enemy of Essentialism; pretending we can “multifocus” is.”\n\n\n“Suppose you are drinking a cup of tea. When you hold your cup, you may like to breathe in, to bring your mind back to your body, and you become fully present. And when you are truly there, something else is also there—life, represented by the cup of tea. In that moment you are real, and the cup of tea is real. You are not lost in the past, in the future, in your projects, in your worries. You are free from all of these afflictions. And in that state of being free, you enjoy your tea. That is the moment of happiness, and of peace.” Pay attention through the day”\n\n\n“Beware the barrenness of a busy life.”\n\n\n“Every choice we make to pursue the essential and eliminate the nonessential builds on itself, making that choice more and more habitual until it becomes virtually second nature.”\n\n\n“I continue to discover almost daily that I can do less and less—in order to contribute more.”\n\n\n“If one’s life is simple, contentment has to come. Simplicity is extremely important for happiness.”\n\n\n“The course of history is determined not by battles, by sieges, or usurpations, but by the actions of the individual. The strongest city, the largest army is, at its most basic level, a collection of individuals. Their decisions, their passions, their foolishness, and their dreams shape the years to come. If there is any lesson to be learned from history, it is that all too often the fate of armies, of cities, of entire realms rests upon the actions of one person. In that dire moment of uncertainty, that person’s decision, good or bad, right or wrong, big or small, can unwittingly change the world. But history can be quite the slattern. One never knows who that person is, where he might be, or what decision he might make. It is almost enough to make me believe in Destiny.”\n\n\n“I think men who lust for power are capable of almost anything.”\n\n\n“When you reach my age, Amara, people show themselves to you very clearly. They write their intentions and beliefs through their actions, their lies.”\n\n\n“But the power to shake mountains does little good if the knife is already buried in one’s throat”\n\n\n“There are some people who will never understand what loyalty means. They could tell you what it was, of course, but they will never know. They will never see it from the inside. They couldn’t imagine a world where something like that was real.”\n\n\n“Courage.” Tavi sighed. “As near as I can figure it, all courage gets you is more of a beating than if you’d run away.”\n\n\n“There’s two kinds of bad men in the world. I mean, there’s all kinds of ways for a man to go bad, but when you get right down to it, there’s only about two kinds of men who will hurt others with forethought. Premeditation. Men that don’t figure there’s anyone else alive who matters but them. And men who figure that there’s something that matters more than anyone’s life. Even their own.” He shook his head. “First one is common enough. Petty, small. They’re everywhere. People who just don’t give a scorched crow about anyone else. Mostly, the bad they do doesn’t amount to much. “The second kind is like your patriserus. People who hold something dear above their own lives, above anyone else’s. They’ll fight to protect it and kill to protect it, and the whole time they’ll be thinking to themselves that it has to be done. That it’s the right thing to do.” Bernard glanced up at her and said, “Dangerous those. Very dangerous.”\n\n\n“Sometimes,” Bernard rumbled, “the only smart thing to do is nothing. Sometimes you just have to be still and see how events begin to unfold before you move. Be patient.”\n\n\n“If the beginning of wisdom is in realizing that one knows nothing, then the beginning of understanding is in realizing that all things exist in accord with a single truth: large things are made of smaller things. Drops of ink are shaped into letters, letters form words, words form sentences, and sentences combine to express thought. So it is with the growth of plants that spring from seeds, as well as with walls built of many stones. So it is with mankind, as the customs and traditions of our progenitors blend together to form the foundation for our own cities, history, and way of life. Be they dead stone, living flesh, or rolling sea; be they idle times or events of world-shattering proportion, market days, or desperate battles, to this law, all things hold: Large things are made from small things. Significance is cumulative—but not always obvious.”\n\n\n“Don’t struggle to heal your wounds. Just pour time into your heart and wait”\n\n\n“If you’ve been unable to change a bad situation, even after many attempts, you should change how you look at the situation”\n\n\n“Awareness is inherently pure, like the open sky. Stress, irritation, and anger can temporarily cloud the sky, but they can never pollute it.”\n\n\n“Having critics means what you’re doing is getting people’s attention”\n\n\n“To be happy, it’s not necessary to expend great effort so we get somewhere else. Instead, relax into the present moment while finding humor in your life. With humor, life becomes light and leisurely. And laughter always brings people to experience openness and joy”\n\n\n“Humor opens closed hearts.”\n\n\n“as frequently as you can as a family. Although we see our family every day, we don’t really get to be with”\n\n\n“Although we see our family every day, we don’t really get to be with one another. A change in environment can do wonders and can bring families closer. A good family trip can prevent divorce. What makes music beautiful is the distance between one note and another. What makes speech eloquent is the appropriate pause between words. From time to time we should take a breath and notice the silence between sounds. When you have to make an important decision,”\n\n\n“The world will keep turning even without you. Let go of the idea that your way is the only way, that you are the only one who can make it happen”\n\n\n“Therefore, much like a mirror reflects what is before it without judgment or identification with the image, simply reflect the negative emotion—let’s say it’s anger—and watch dispassionately. You will see the anger slowly changing shape, either revealing a deeper layer of emotion or disappearing on its own. If another layer of emotion is revealing itself, attend to it the way you did with your anger.”\n\n\n“pure attention without judgment is not only the highest form of human intelligence, but also the expression of love”\n\n\n“No person can be found Who has been, is, or will be Only criticized Or only praised.”\n\n\n“When someone criticizes another, you might think he deserves it. But if you look more closely, you’ll see that the critic is complaining because he did not get his way.”\n\n\n“As we resist, we are in constant motion trying to adjust, and yet we still remain unhappy about what is”\n\n\n“When you cannot control even your own mind, what makes you think you can control others?”\n\n\n“Whether it is an object, a thought, or a feeling, if it has emerged out of emptiness, it will soon change its form and eventually retreat back to emptiness. Seekers in search of the eternal Truth must look beyond its impermanent nature and become aware of “that” which knows impermanence”\n\n\n“Spirituality must be practiced not just in solitude but also among people. Open up to people around you and feel connected. This is the true challenge of spiritual practice.”\n\n\n“Feelings are often born from a matrix of conditions beyond your control. Just like you can’t control the weather, or your boss’s mood, you can’t control the feelings in your body. They are just passing through, like clouds in the sky. They, too, dissipate on their own. But if you take them too seriously and start internalizing them as part of your identity, then you will resuscitate them every time you think about the past. Remember that you are neither your feelings nor the story your mind tells about you to make sense of them. You are the vast silence that knows of their emergence and their disappearance”\n\n\n“Of all the words that pour out of our mouths every day, how many are really ours, and how many are borrowed from others?”\n\n\n“Everything in this universe is evanescent. Because it is evanescent, it is also precious. Spend this precious moment wisely and beautifully. The mind cannot have two thoughts at once. See if you can think two thoughts at exactly the same time. Well? Is it possible? We can be consumed by anger for a long time without realizing we have been angry. Similarly, we are easily lost in thought without knowing we have been thinking. Even when we are awake we are no different from a sleepwalker. We do things without the awareness of doing them. Just because our eyes are open does not mean we are awake. Being awake means that”\n\n\n“When I gaze upon water, I become water. When I look at a flower, I become the flower. The flower riding on the water, yippee!”\n\n\n“A long time ago, there was only one mind, which became bored by being alone for so long. So it decided to split into two. But since the two knew they were originally one, playing together was not much fun— as if playing both sides of a chess game. So the two minds agreed to forget where they came from; they pretended not to know each other. As time passed, they also forgot about their agreement. They forgot they were actually one and the same. This is the condition of our existence. We forget that we are originally from one mind.”\n\n\n“Life is like theater. You are assigned a role. If you don’t like the role, keep in mind that you have the power to re-create the role you want”\n\n\n“something I should have known all along. When we’re first given a job, especially one we’ve been working toward for a long time, it’s easy to become overly enthusiastic, as we are eager to prove ourselves. But in our excitement, we make the mistake of equating our own eagerness with effectiveness”\n\n\n“When we’re first given a job, especially one we’ve been working toward for a long time, it’s easy to become overly enthusiastic, as we are eager to prove ourselves. But in our excitement, we make the mistake of equating our own eagerness with effectiveness. Getting the job done well is more important than one’s feelings of doing a good job. It takes wisdom to discern that these two are not always related. In some cases, one’s zealous efforts can get in the way of achieving the desired outcome, especially if one is unable to see the needs of the others working toward it together”\n\n\n“The world notices your efforts more quickly than you think”\n\n\n“It is important that you work hard, but don’t be enamored of the feeling of working hard. If you are drunk on that feeling, then you care less about the actual work than about how you appear to others to be working hard.”\n\n\n“A person does not live the way he says he would. He lives the way he has been living”\n\n\n“A vow to help others can summon immense energy from within.”\n\n\n“When you are making a decision, try to assess how many people it will benefit. If it satisfies only your ego and unnecessarily hurts many, then it is the wrong decision”\n\n\n“Isn’t it better to be happy together than to be right alone?”\n\n\n“One lesson of maturity is that we should not take our thoughts too seriously, and must learn to curb our ego and see the bigger picture.”\n\n\n“Criticism without a solution is merely an inflation of the critic’s ego.”\n\n\n“When you ask a question and there is no response, then that is the answer.”\n\n\n“Don’t try to make it perfect. Instead, make it interesting!”\n\n\n“After mastering eighteen levels of kung fu, you can hurt someone with the flick of a finger. But if you go on to master all thirty-six levels, you choose to retreat when the weak foolishly come to fight”\n\n\n“From this experience I realized that the art of maintaining a good relationship can be compared to sitting by a fireplace. If we sit too close for too long, we become hot and possibly burned. If we sit too far away, we cannot feel the warmth. Similarly, no matter how well we get along with someone, if we stick too close without building in some personal space, we soon feel trapped and burned out; it is easy to take the relationship for granted and feel resentful about not having enough privacy and independence. On the other hand, if we put in too little effort to stay in touch with friends and family, we can’t feel the warmth of their love. Striking a balance is key”\n\n\n“cup was full. Perplexed, Maeng demanded to know what he was doing. “You seem to know that too much tea will ruin the floor,” the master answered, “but how do you not know that too much knowledge will ruin one’s character?”\n\n\n“If you lower your head, you won’t bump into trouble.”\n\n\n“Do not expect others to follow your way. When things always go your way, it is easy to become arrogant.”\n\n\n“The end of a sushi roll, with the filling sticking out, is often tastier than a piece sliced neatly from the middle. Someone slick and well-put-together can come across as cold and alienating, while an average guy without pretense is more genuine and attractive”\n\n\n“People say hurtful things because they themselves have been hurt.”\n\n\n“If you wish to communicate effectively with others, better to describe what you are feeling rather than go on the offensive. For instance, say, “I am very sad to hear that,” not, “Why do you always make me sad?”\n\n\n“When you criticize someone, see if you are doing so out of envy. Your criticism reveals more about yourself than you realize. Even if you are correct, people still may find you unappealing. If you wish to communicate effectively with others, better to describe what you are feeling rather than go on the offensive. For instance, say, “I am very sad to hear that,” not, “Why do you always make me sad?” You”\n\n\n“You want people to hear you rather than have to defend themselves from you”\n\n\n“When blinded by anger, we make choices we later regret. Leaving the room before the bridge is burned is a sign of maturity”\n\n\n“The best revenge is love”\n\n\n“Paper wrapped around incense smells of incense, and string binding fish smells of fish.”\n\n\n“The best way to hide your wealth is to give it away. If you are generous with your wealth, the money that would have disappeared sooner or later”\n\n\n“Whether we like it or not, we are all connected, and it is unthinkable to be happy all by oneself.”\n\n\n“Inside of us there is a steep mountain of fear and a deep river of grief. But there is also the compassionate eye witnessing your inner landscape. May you find your inner witness, the source of freedom and healing.”\n\n\n“Does the person you hate deserve to be carried around in your heart?”\n\n\n“Humans are like mirrors: We reflect each other.”\n\n\n“When you lower yourself, the world elevates you. When you elevate yourself, the world lowers you”\n\n\n“When you keep clashing with someone, it may be the world’s way of asking you to look closely at yourself. When you don’t like someone, try to figure out what it is you don’t like; see whether you have a similar flaw within yourself. The flaw that you immediately notice in someone you meet is probably a flaw of yours, too. If you didn’t have it, you wouldn’t have noticed it so quickly.”\n\n\n“For spiritual practitioners, relationships are the final test. Even if you have awakened to your enlightened nature, there is still further to go in your spiritual journey if you’re not living harmoniously with others”\n\n\n“Demonstrations of love are small, compared with the great thing that is hidden behind them”\n\n\n“Love is like an uninvited guest. Love will come when it wants to. Love will leave when you ask more of it”\n\n\n“If you look for love, in pursuit of what it can give you, it will hide itself. If you ask love to arrive because you are now ready, it will skip your door. Love is like an uninvited guest. Love will come when it wants to. Love will leave when you ask more of it. If you attempt to find a love that meets certain criteria, your new love may also make certain demands of you. Drop your demands quickly when love knocks on your door. Love is warm and freeing. It is innocent, like a child without a hidden agenda. We”\n\n\n“We can determine how close we are to someone by asking, “Can I act like a little kid in front of that person?” When we love someone, we feel like a little kid in our heart”\n\n\n“Why can’t I give her a gift and tell her I love her?” Your words and gifts will mean more to her when she is ready. Love her, not your feelings.”\n\n\n“Love needs to be balanced. If you like him more than he likes you, give him time and space to catch up. It is important to hold back your emotions when your feelings are not in balance with his.”\n\n\n“I want you to know that I love your ordinariness, because I, too, am ordinary. The truth is, we are all ordinary”\n\n\n“No matter how famous or beautiful one is, no matter how much money or power one has, no matter how many wonderful accomplishments one has had, we all have our share of setbacks, heartbreak, and loss. We have to face challenges we have no control over. Loneliness and the fear of death will accompany us to our final days. Everyone is on the same treacherous journey of life’s tainted glory.”\n\n\n“When we are in love, we like to do nice things for the one we love. But it is equally important to refrain from doing unnecessary things. We often overlook that part”\n\n\n“We like to get involved in other people’s business, thinking we are doing so for them. We offer unsolicited help and interfere with their lives. We take away their power and make them feel incapable. This stems from our desire for control and recognition. It has little to do with love”\n\n\n“Moreover, when I meet someone new, do I make an effort to see who he is beneath his social markers? Or am I reducing people to their background and failing to see who they really are? I am reminded again that anyone, including those young boys, can be our spiritual teacher if we are willing to open our hearts to them.”\n\n\n“Life is like a slice of pizza. It looks delicious in an advertisement, but when we actually have it, it is not as good as we imagined. If you envy someone’s life, remember the pizza in the ad. It always looks better than it is.”\n\n\n“Always go with your first choice if you can afford it. It is better than a life filled with regrets.”\n\n\n“It makes sense that Scandinavia should be famous for furniture design, since people in a cold climate spend more time inside their homes. Similarly, Italy is renowned for designer apparel; it makes sense that people in a warm climate should pay more attention to how they appear outdoors. Where you live shapes you. Do you live in a place conducive to the pursuit of your dreams”\n\n\n“The biggest obstacle to learning is pretending to know even when you don’t. It is better to admit you don’t know something; if you pretend, you have to act as if you knew all along. It is easier to learn when you set aside your pride and are honest. The compassionate gaze of the wounded soul is”\n\n\n“if we are brutally honest with ourselves, most things we do for others are in fact for ourselves. We pray for the well-being of our family because we need them to be around. We shed tears when our partner dies because of the impending loneliness. We sacrifice for our children in the hope that they will grow up the way we want. Unless we become enlightened like the Buddha or Jesus, it is difficult to abandon our deeprooted preoccupation with ourselves.”\n\n\n“Stop worrying about what others think and just do what your heart wishes. Do not crowd your mind with “what ifs.” Uncomplicate your life and own up to your desires. Only when you are happy can you help to make the world a happier place”\n\n\n“A majestic tree is the first to be cut down and used for lumber, whereas a modest one lives on. Likewise, a real master conceals his virtue and never boasts of his excellence”\n\n\n“If someone shares his problems with you, don’t feel the need to have the solutions. Just listen sincerely. This is often more helpful.”\n\n\n“A foolish person thinks, “I already know that.” He keeps anything new from coming into his mind. A wise person thinks, “I don’t know the whole story.” She opens herself up to even greater wisdom. An ordinary person mainly notices particular things he likes or dislikes. A wise person notices both the whole and the particulars. When you share your problems with your friends, you do not expect them to have the solutions. You are just grateful they are there for you and willing to listen. If someone shares his problems with you, don’t feel the need to have the solutions. Just listen sincerely. This is often more helpful. When”\n\n\n“When you share your problems with your friends, you do not expect them to have the solutions. You are just grateful they are there for you and willing to listen. If someone shares his problems with you, don’t feel the need to have the solutions. Just listen sincerely. This is often more helpful. When I look deeply within myself, I realize what it is that I really want from others: attentive ears that listen to what I am saying, kind words that acknowledge my existence and worth, gentle eyes that accept my flaws and insecurities. I resolve to be that person for those around me”\n\n\n“When our greed is awakened, we are cheated”\n\n\n“The reward for someone who works hard is more work.”\n\n\n“If we’re quick to grant a favor, then people quickly forget their gratitude. If we grant a favor with several conditions, then people express immense gratitude”\n\n\n“According to the Lotus Sutra, the Buddha makes the prophesy about his disciples because he has a supernatural ability to foresee when they would achieve the final stage of buddhahood. But I do not think they automatically attained enlightenment because they received the Buddha’s prophesy; I think it had a lot to do with the Buddha’s faith in them, which motivated them to work harder to accomplish what their teacher predicted. Like Ms. Lee’s words to me, the Buddha’s trusting words and his loving gaze transformed the lives of his five hundred disciples. One word of encouragement, said with kindness and hope, can change a person’s future, the way it did for the five hundred disciples and for me”\n\n\n“No matter how famous, powerful, or rich some people are, they are not very different from anyone else. We long for deep connection and unconditional acceptance. We have the same insecurities and need for approval. There is no reason to feel inferior”\n\n\n“A restaurant specializing in a few good dishes is more likely to develop a good reputation than one with a lengthy menu.”\n\n\n“When it comes to learning a new skill, there are two kinds of people. One kind prefers to first study the typewriter, while the other starts by pounding on the keys. One kind likes to first master the grammar of a foreign language, while the other learns in the trenches, using body language if they must. Generally speaking, the second type tends to learn faster than the first, because the latter is not afraid of making mistakes. There is no such thing as being completely prepared. Life is an adventure, through which we learn and mature. Of course, we must consider all our options carefully. But if we wait for 100 percent certainty, then it is often too late.”\n\n\n“The college you graduated from is not that important. The life you have chosen to live after college is.”\n\n\n“When you look for a job, try to find out how long a company’s employees stay at the company. This is more important than the size of the company or the salary offered. If people keep leaving, then that says a lot.”\n\n\n“Dedication to one’s job should not be measured by how late one works or how often one forgoes a vacation but by how effectively one works and what kind of contribution one makes to the business.”\n\n\n“The vaguest and least effective statement: “I will have whatever.”\n\n\n“Those who have not realized their True Self live like the blind, unintentionally scratching someone else’s leg. If you would like to scratch your own leg, first awaken to your True Self.”\n\n\n“Some may think that life in such a community is repressed, strict, and difficult, but that is not the case. A monastic life is characterized by simple beauty and unexpected joy. Monks find happiness in things that may seem trivial to those who pursue the material trappings of success. Watching the seasons change—the blossoming of the magnolias, the dazzling fall foliage, the first snowfall—brings indescribable joy and gratitude. A simple meal made with fresh ingredients from the nearby mountains is a source of great contentment. Because our monastic brothers are our friends, teachers, and family, we are never lonely”\n\n\n“If our faith can be shaken from merely learning about a different tradition, then that faith is not worth keeping”\n\n\n“when the essence is forgotten, ritual takes over.”\n\n\n“He who knows only one religion knows none.”\n\n\n“A spiritual leader is a finger pointing at the moon. If the finger attempts to become the moon, this can lead to a grave sin.”\n\n\n“Above all, please understand that what makes you feel tense and awkward is not the spirituality itself but the pressure from your family to conform. You may resent their coerciveness and self-righteousness”\n\n\n“Above all, please understand that what makes you feel tense and awkward is not the spirituality itself but the pressure from your family to conform. You may resent their coerciveness and self-righteousness. You may feel their spiritual path is strange and unorthodox.”\n\n\n“In the beginning, our prayer takes the shape of, “Please grant me this, please grant me that,” and then develops into, “Thank you for everything,” and then matures into, “I want to resemble you.” Eventually it transcends language”\n\n\n“then try to pray this way as well— “Enlarge my heart to hold and accept the things I cannot.” Do not bargain with God, Buddha, or any divine being to give you what you want in exchange for material offerings. If you do not know how to solve a problem in your life, give prayer a try. As you bring your attention inward and sincerely seek an answer, something sacred within you unlocks the door of inner wisdom. If you are desperately looking to meet someone special, send your prayer out to the universe. The universe is an amazing matchmaker. Monastics can pray for many years because their prayers of happiness for others make them happy. As I prepare to officiate at my friend’s wedding, I become joyous. For unenlightened people, not every day is a good day, because they feel happy only when things happen the way they want them to. For enlightened people, every single day is a good day, because they feel free knowing that nothing can take away their wisdom. When an unenlightened person does good, he tries to leave his mark. When an enlightened person does good, he leaves no marks. The holier a person is, the more likely it is that she describes herself as a sinner. This is because she doesn’t lie to herself. ”\n\n\n“The saints are what they are not because their sanctity makes them admirable to others but because the gift of sainthood makes it possible for them to admire everybody else.”\n\n\n“Everything is impermanent, including the world’s most comfortable posture.”\n\n\n“If you begin to believe what others say about you, they will begin to control you. Not everything that appears in your mind is true. Do not let someone else’s opinion rule your life.”\n\n\n“If children do not receive enough attention, psychological problems often emerge.”\n\n\n“If I like myself, it is easy for me to like people around me. But if I am unhappy with myself, it is easy to feel unhappy with those around me. May you become your own biggest fan!”\n\n\n“The absurd is essentially a divorce. It lies in neither of the elements compared; it is born of their confrontation.”\n\n\n“neither the human nor the universe are necessarily absurd on their own, but rather, their relationship is absurd. As humans, we exist with an innate desire for meaning, reason, and order, but yet, we simultaneously exist in a universe that appears to lack all of the above. So far as we can tell, the universe is completely indifferent. Thus, what we want and expect from the universe is fundamentally in contradiction with what we get. In this conflict, the absurdity of the human experience is found.”\n\n\n“Unlike anything else in the known universe, we are able to consciously observe, consider, reason, and act. As a result of our unique abilities, we ask ourselves why. We desperately try to find the answer. We get increasingly clever in our attempts, yet every time, just like the rock in The Myth of Sisyphus, at some point, we return to the bottom of the hill, leaving us to start anew.”\n\n\n“The struggle itself towards the heights is enough to fill a man’s heart.”\n\n\n“Everyone is experiencing the same dissonance of being, living their own story through the nauseating rollercoaster of ups and downs to nowhere. It is both banal and almost hollow to say that we should be compassionate and kind to others because of this. Every first-grade child knows this in fewer words. The notion of togetherness and compassion is sold to us in soda and fast-food commercials as well as nearly every Disney movie. It is so commodified, clichéd, and obvious that it’s hard to even take seriously the idea of needing to consider it further. Life isn’t a soda commercial nor a Disney movie, and it isn’t anywhere near as clean as a first-grade child could likely even imagine. There is a world filled with malevolence and anger and greed and impatience and all the rest. But for this very reason—because compassion is so trite and yet still seemingly so hard and absent—it is perhaps all the more essential and rational to give it serious focus and effort.”\n\n\n“There is but one thing that seems to have any positive effect against such absurdity: a sort of compassion for the whole. We must try to remember that everyone is in it, and everyone is flailing against it for the same reasons as everyone else. Thus, within limits perhaps, everyone deserves such consideration and compassion.”\n\n\n“Man can do what he wills but he cannot will what he wills.”\n\n\n“We see the world not as it is, but as we are,”\n\n\n“In all cases, for the most part, knowing what the real color of things are (metaphorically speaking) is perhaps minimally relevant to living and thriving as an individual and as a species. Perhaps what matters more is that we can agree and disagree on subjective things sufficiently well enough, cordially enough, and often enough. And it seems as though that in order to do so, if such a feat is possible, the prerequisite is a willingness to embrace often being wrong.”\n\n\n“We so often take personally what the world does without us in mind.”\n\n\n“Those who are often angered reveal themselves to be a strange sort of optimist, still in denial of the tragedies of this life and the death of their youthful innocence—the belief that life can be what it can’t.”\n\n\n“A theory in psychology known as appraisal theory, initially developed by psychologist Magda Arnold, suggests that our emotional responses are in large part created by our conscious evaluations of events—how we view, interpret, and label stimuli rather than the stimuli themselves. In other words, in between our primary experience of an event and our emotional experience of an event, there is a filtering process that occurs through and is based on our cognitive faculties. In this space, how we think based on our experiences, perceptions, views, and values determines what we feel.”\n\n\n“For the stoics, events in the world are objective and neutral, and our qualitative emotional experiences are merely a product of the narratives we tell ourselves. “It doesn’t hurt me unless I interpret it’s happening as harmful to me. I can choose not to,” wrote Marcus Aurelius.”\n\n\n“Rather, if we realize that the world has not singled us out, that most people are good people trying their best, that ignorance is far more often behind the curtain and not malice, that our emotions are not the result of being made victims by others but by us not taking ownership of them ourselves, that life is inherently difficult and suffering is fundamental to everyone, we can perhaps more accurately evaluate if what we are angered by is worthy of being angry about, and how.”\n\n\n“Knowing how to know and be yourself versus actually knowing and being it is like knowing the mechanics of how to surf on paper and then going out into the ocean and encountering an actual wave.”\n\n\n“Of course, it feels very nice to be liked. In certain cases, it is essential. Our wellbeing depends to some degree on the quality of our social relations, which require us to be sufficiently liked by at least some people some amount of the time.”\n\n\n“In infancy and early childhood, we are inadvertently conditioned by our parents, teachers, and other people we encounter to feel like we are the center of all attention, the most important thing in the world and beyond. Everything we do is relevant, consequential, or impressive. When we walk for the first time, it’s the most important thing that could have happened in the world that day. If we draw on the wall, it’s the worst thing. But of course, the world did not care about either of those things that day at all. As we age and are further socialized into the world, we slowly but surely realize that the world mostly has never and will never care about what just about anyone does, including us. We are not really at the center of anything at all, not even our own minds. We are not important in any grand sense. No one really cares.”\n\n\n“In his famous twentieth-century play, No Exit, Jean-Paul Sartre wrote, “You remember all we were told about the torture-chambers, the fire and brimstone, the “burning marl.” Old wives’ tales! There’s no need for red-hot pokers. Hell is other people!”\n\n\n“Even on the highest throne in the world, we are still sitting on our ass,”\n\n\n“I am not who you think I am; I am not who I think I am; I am who I think you think I am,”\n\n\n“I have sometimes thought that, in order to be a good minister, it was necessary to leave the ministry,”\n\n\n“No man,” he wrote, “can antedate his experience or guess what faculty or feeling a new object shall unlock, any more than he can draw today the face of a person whom he shall see tomorrow for the first time.” In other words, no one can know what life will be like tomorrow nor what such life may cause one to think or feel. However, one must move with it and live according to the present now.”\n\n\n“Arguably, great artists and writers aren’t popular because they say something no one has thought of or experienced before, but because they say something that most of us have but weren’t sure we were right in doing so. Emerson”\n\n\n“Perhaps so long as one authentically stands in their own position of confusion and limitation, they have still remained in accordance with their own relative truth and greatness, and the notion of self-reliance holds steady.”\n\n\n“One’s shadow does not disappear by looking away from it. In the same way that one cannot literally evade the shadow of their body by outrunning it, there is no move or evasive tactic that separates the individual from their psychological shadow. The danger, rather, is in the attempt to do so—the ignorance and denial of it. Jung wrote: Good does not”\n\n\n“One must know of a problem to be able to fix it, and it is an act of healing to admit that one is sick.”\n\n\n“man is a worm and food for worms. This is the paradox: he is out of nature and hopelessly in it; he is dual, up in the stars and yet housed in a heart-pumping, breath-gasping body that once belonged to a fish and still carries the gill-marks to prove it.”\n\n\n“What does it mean to be a self-conscious animal? The idea is ludicrous, if it is not monstrous. It means to know that one is food for worms. This is the terror: to have emerged from nothing, to have a name, consciousness of self, deep inner feelings, an excruciating inner yearning for life and self-expression and with all this yet to die. It seems like a hoax.”\n\n\n“One must be careful to not make the singularness of their shot at existence a pressure to get it all right—to do all the right things and think all the right thoughts and feel all the right feelings.”\n\n\n“The point is quite the opposite; you will mostly do a lot of the wrong things, think a lot of the wrong thoughts, and feel a lot of the wrong feelings. But precisely because this is your one shot at life, this must be ok. You are driving blind through the most impossibly complex, strange maze that you know ends in a head-on collision with a wall. What use is getting more upset or guilty about feeling upset or guilty in an existence that set you up? Of course, this is far easier said than done, but perhaps in true, deep contemplations of one’s mortality, at least on occasion, this reminder can sometimes serve more as a sedative and not merely a stimulant.”\n\n\n“It is strange and rather terrifying to consider that we can be something for now and nothing forever—but perhaps it is only because of the fact that we are nothing forever that we can be something for now.”\n\n\n“We might never know what nothing is until we know nothing at all. And even then, we might not. We are all free to imagine nothing however we like. Because if nothingness is in fact the source and destiny that connects us all, then perhaps, through nothing, anything is possible.”\n\n\n“If we are to be fully human and fully alive and aware, it seems that we must be willing to suffer for our pleasures.”\n\n\n“the more one tries to remove or escape the negative experience of life, the more negative it becomes. Rather, the more one faces it willingly and intentionally, the stronger and more equipped one becomes—the more meaningful and positive the pain and hardship can be made to feel .”\n\n\n“However, beyond this area of exceptional misery, there still exists a realm of suffering and unhappiness entrenched in human life that appears to be unshakeable, even when one’s circumstances are relatively good. This realm can draw the healthy, decent, prosperous person to self-hatred and self-sabotage, to addiction and suicide. It is the realm of misery in the background of any and every moment that should be enjoyed simply and happily but isn’t. It is the mental pain that is specific to no one but applicable to everyone.”\n\n\n“The person who depends on their ability to accomplish away the struggle, sadness, and uncertainty of life, could accomplish the whole world just to be met with a disappointment so intense that it would destroy whatever is left of them. Perhaps, then, our quality of life is not found in the heights of our happiness or pleasures but in how we choose to consider and look at what surrounds these extremes, how we attempt to create a life of personal intention, meaning, and decency, and justify the inevitable lows rather than always trying to escape them.”\n\n\n“every exhalation, there is a breath to come so long as we keep breathing. In every moment of hardship, pain, confusion, or weakness, there is a story taking place filled with the potential for triumph and vitality worthy of tears bursting with wonder and fondness for life, so long as we keep moving. So long as there is life in us, there is the rare and exclusively human”\n\n\n“In every exhalation, there is a breath to come so long as we keep breathing. In every moment of hardship, pain, confusion, or weakness, there is a story taking place filled with the potential for triumph and vitality worthy of tears bursting with wonder and fondness for life, so long as we keep moving. So long as there is life in us, there is the rare and exclusively human opportunity to take this chaotic, thrashing existence, this strange nothingness, and make it something. There is the chance to connect and love and help, to feel and think and live, to experience the cosmic everything. And at some point, perhaps that is enough.”\n\n\n“When you get the shit kicked out of you long enough … you will have a tendency to say what you really mean,” said”\n\n\n“Too many writers write for the wrong reasons. They want to get famous or they want to get rich or they want to get laid by the girls with the bluebells in their hair … When everything works best, it’s not because you chose writing, but because writing chose you. It’s when you’re mad with it. When it’s stuffed in your ears, nostrils, under your finger nails. It’s when there’s no hope but that.”\n\n\n“There are degrees of madness, and the madder you are the more obvious it will be to other people. Most of my life I have hidden my madness within myself but it is there,”\n\n\n“Men are so necessarily mad, that not to be mad would amount to another form of madness,”\n\n\n“Like a wild elephant, the untamed mind can inflict enormous damage on ourselves and those around us.”\n\n\n“One traditional Tibetan doctor whom I know once commented on people living in the West, “From the perspective of Tibetan medicine, you are all suffering from nervous disorders. But given how ill you are, you are coping remarkably well!”\n\n\n“Physicians don’t heal abrasions, and surgeons don’t mend bone fractures. Instead, they do whatever they can to allow the body to heal itself—by keeping the wound clean, setting the broken bone, and so on.”\n\n\n“The reason we don’t devote more time to balancing our minds is that we are betting our lives that we can find the happiness we seek by chasing fleeting pleasures. Psychologists have called this the hedonic treadmill,11 and the first step to escaping from this exhausting grind is to seek a vision of genuine happiness that draws on our own, largely untapped inner resources.”\n\n\n“During the path of cultivation, he would meet enemies one by one. If he wanted to improve, he would have to constantly look towards the future and review the past. Only then, he would be able to improve slowly.”\n\n\n“When there are too many debts, one stops to worry.‘”\n\n\n“The martial path was lonely. It was a source of great happiness if one found a confidant.”\n\n\n“The Azure Yang Lord’s uninhibited life was filled with ups and down. Having felt both extreme happiness and sadness, he knew how difficult it was to earn happiness. If he could make this moment last forever, that would be a blissful matter… However, be it the Azure Yang Lord or the ancient Great Empress, they knew that these days were not going to last.”\n\n\n“Warriors had to suffer hardship and experience numerous life and death encounters. If they were not careful, they would lose their lives. They had to restrain their desires and endure lonely decades of reclusive training. Wasn’t all this for them to lead a free life and do as they pleased when their power succeeded on reaching unprecedented heights!? Everything in the world depended on one’s preference. To do as the heart pleased!”\n\n\n“The ways of the world are full of vicissitudes, and in it, there is the grief at separation and joy in union, the suffering of life and death. No matter how thick a history book is, it would not be able to record everything down. However, it is such infinite matters of the past that can pass by with a finger snap. In one’s old age, while looking back at the past, only then would you feel like everything was ephemeral.”\n\n\n“The further he walked, the tinier he felt.”\n\n\n“Against the vast cosmos, the lives of people were like ants. It was like how bacteria did not know what it meant for the first and last day of the month, or how mole crickets did not know the seasons. What was a mortal’s life to the Universe? Yi Yun did not want to remain like a speck of dust in the Universe.”\n\n\n“In a warrior’s world, we may have the power to cause great destruction, have long lifespans, and are able to lead extravagant lives, but the pressure is intense. Be it life-and-death trials or breakthroughs that require great risks, or the sense of urgency of being killed by someone else at any moment in time, it forces us to continuously forge ahead.” “The accumulation of all these pressure naturally need an opportunity for it to be released.”\n\n\n“Before one’s heart reduced to mediocrity, Great Dao lies within one’s heart!”\n\n\n“Those without sufficient strength but can bear disgrace and a heavy burden are people who I appreciate as well.”\n\n\n“Everyone had their own time, but time treated everyone equally.”\n\n\n“Were destruction and finality the end? Matter in the Universe could not remain eternal. They would eventually proceed towards destruction, including the Universe. But what happened after destruction? Could it be that all that was left was eternal ‘nothingness’? Everything in the world underwent birth and death. A drop of water could evaporate and become part of a cloud before condensing into new drops of rain. Plants would wither, but the fruits that they bore could give rise to seeds. Mortals would die of age, but babies would grow up into adults. Stars could be destroyed, but new stars would eventually be born… Everything in the world underwent a cycle, so it definitely included the Universe. The destruction of the Universe was the beginnings of a new Universe. It was just that the cycle was immensely long that it was beyond the imagination of mere mortals. New Universe… Yi Yun suddenly seemed to realize something. What were the beginnings of the Universe? Dao begets One, One begets Two… Before Yin-Yang and Space-time was Chaos! Rebirth after destruction was Chaos and Major Destruction respectively. They were both two Great Dao of Supremacy! Why were there two Great Dao of Supremacy? They might be like Yin-Yang, Space-time, Water-Fire, just two sides of the Universe. The two could supplement each other and not a single one was dispensable.”\n\n\n“But if a warrior does not have any aspirations, he won’t be able to go far either.”\n\n\n“To what end do we practice martial arts for? If we can’t depend on the sword, so what about death? One’s true richness of heart is not to be annihilated!”\n\n\n“Helping a man once gains gratitude, but not helping that man again gains hatred.”\n\n\n“grinding a chopper will not hold up the work of cutting firewood”\n\n\n“As the saying goes, he who gets other’s kindness but does not repay is not a gentleman.”\n\n\n“speaking.”History changes erratically. Time is enough to wipe away many things.”\n\n\n“Chaos in the world leads to the suffering of all life, but it is something some people are willing to see. Heroes are born in difficult times.”\n\n\n“Humans were warlike, to begin with. It was common among humans to fight unceasingly for their interests. Furthermore, there were some perverse people who wanted revenge on the world. Such people were even more dangerous. There was no limit to what they would do. Yi Yun knew that there was evil deeply rooted in human nature. However, he was not a person who would bemoan the state of the universe and mankind, much less the kind who would question life after seeing the ugly nature of humans and eventually decide to destroy the entire world in a crazy fit. He was only Yi Yun, an ordinary person that pursued the martial path, wanting his life to escape the cycle of samsara.”\n\n\n“When one postures to impress a girl, his failure becomes all the more foolish.”\n\n\n“When warriors took lives or destroyed an item, or even shattered a world, they could not avoid violence. Only time could turn everything into dust in an infinitely calm manner simply by passing. It was silent from beginning to end. Time was the most unique power of destruction. Yi”\n\n\n“A man who loses position and influence may be subjected to much indignity. It’s all fated…”\n\n\n“He who understands the times is a wise man.”\n\n\n“Yi Yun refused to believe that someone would forgo their lives for a master who had not won the hearts of people.”\n\n\n“We warriors have been cultivating our entire lives. What was it for? It’s not to enjoy riches or sex. Nor is it about being placed on a pedestal while enslaving those beneath us! We cultivate to fight against our fate! We do not wish to to let Samsara run its course, so we cultivate for years! Our martial heritage has been passed down for millions of years, billions of years, and will be so for trillions of years! It’s not for us to kneel down to beg for mercy after gaining mastery. With the backbone broken, there’s no way one can straighten one’s back.”\n\n\n“The Heavenly Dao’s evolution is unending. Creation and Destruction are interchangeable, forming a complete circle. Be it life or a universe, they have formation, existence, disintegration, and emptiness; birth, aging, illness, and death. You wanted to devour the Heavenly Dao, robbing and changing the fate of the universe. You wanted to empower yourself by initiating the deaths of millions; hence, you were destined to be abandoned by the Heavenly Dao.”\n\n\n“Legends were typically like this. They would not be eclipsed with the passage of time, but instead, they became histories that one tried to live up to.”\n\n\n“Life, from lifeforms as large as towering trees to bugs as small as ants, the reason for all they do is for the continuation of their bloodline.”\n\n\n“In this Sinkhole that was filled with despair, a bunch of seniors were forging ahead bearing the burdens, so as to hold up the sky for the young. Although these seniors had their backs hunched from the pressures of time, to the point of not being able to bear the weight any further, that was already sufficient, no matter how tiny that piece of sky they held up was. They had already done everything they could have done.”\n\n\n“The crests and troughs of life are all but ephemeral mists. How nice if reunion and separation were fixed in place of that first meeting. A thousand years pass and happiness a certainty, but how much was sadness in exchange?”\n\n\n“Imagination comes from limitations. When you can do anything it gets debilitating”\n\n\n“Every storm runs out of rain”\n\n\n“Writing is nature’s way of letting you know how sloppy your thinking is”\n\n\n“Faith is a gift, not a reward”\n\n\n“Where there is a will, there grows a forest”\n\n\n“It’s frustrating navigating my feelings because they are all self-imposed - the only person I can get angry at is, well, myself. ”\n\n\n“The Heavens and Earth will not move; neither will the stone turn in the flowing river. There is still a long journey ahead of us. It’s just as you have always said. There’s no need to force yourself to do anything. We just hope Teacher can return safely; that is our only request.”\n\n\n“You can try to run from the sun as hard as you can, but you can never outrun the sunlight”\n\n\n“A sect’s decline is not due to the external pressures but due to the internal disputes and lack of unity”\n\n\n“Literature lasts for eternity,\n\n\nA Martial artist retires at death.”\n\n\n“When things are moving so fast, it’s good to remember that sometimes you’ll waste a lot of effort or get scooped. The best thing you can do is to just accept it and keep going on your process. You’re not alone in this one.”\n\n\n“Am I the creator if I simply role the dice until I win the lottery?”\n\n\n“If you write a piece of code and it seems simple to you, but others think it is complex, then it is complex.”\n\n\n“Ironically, there never seems to be enough time to do something as idle as contemplate the very nature of time.”\n\n\n“In work, he writes, time is horizontal, a pattern of forward-leaning labor time punctuated by little gaps of rest that simply refresh us for more work. For Pieper, those little gaps are not leisure. True leisure, instead, exists on a “vertical” axis of time, one whose totality cuts through or negates the entire dimension of workaday time, “run[ning] at right angles to work.” If such moments happen to refresh us for work, that is merely secondary.”\n\n\n“As long as slowness is invoked merely to make the machine of capitalism run faster, it risks being a cosmetic fix, another little gap on the horizontal plane of work time.”\n\n\n“What first appears to be a wish for more time may turn out to be just one part of a simple, yet vast, desire for autonomy, meaning, and purpose.”\n\n\n“There is a lonely absurdity in the idea of racing against the clock at the end of time, as evidenced in a headline by the parody site Reductress: “Woman Waiting for Evidence That World Will Still Exist in 2050 Before She Starts Working Toward Goals.”\n\n\n“What I find in chronos is not comfort but dread and nihilism, a form of time that bears down on me, on others, relentlessly. Here, my actions don’t matter. The world worsens as assuredly as my hair is graying, and the future is something to get over with. In contrast, what I find in kairos is a lifeline, a sliver of the audacity to imagine something different. Hope and desire, after all, can exist only on the differential between today and an undetermined tomorrow.”\n\n\n“Tomorrow was growing raw out of the husk of today, and in it, I’d be different. All of us would.”\n\n\n“How much someone’s time is valued is not measured simply by a wage, but by who does what kind of work and whose temporality has to line up with whose, whether that means rushing or waiting or both.”\n\n\n“The lava moves, and it’s not because of us.”\n\n\n“The future is always over the horizon, and to be alive is to be in transit. For a few minutes, a sunrise collects all that ineffable bittersweetness into a single burning point.”\n\n\n“In turn, looking for kairos while living largely in chronos puts you in that difficult gray area between personal agency and structural limits, an area long explored by social theorists but also simply experienced by anyone negotiating life in a social world.”\n\n\n“Sometimes the best muse is the thing you’re so afraid of you almost cannot speak it.”\n\n\n“Landes suggests that a crucial deviation happened with the development of Christian canonical hours, particularly under the sixth-century Rule of Saint Benedict. The Rule, which subsequently spread to other orders, specified seven times during the day when Benedictine monks should pray, as well as an eighth in the middle of the night. Determining that “idleness is the enemy of the soul,” the Rule also described punishments for monks who failed to hurry sufficiently upon the signal for work or prayer.”\n\n\n“A clock hour was meant to be an hour, no matter where or what the season, just as a man-hour would be expected to be an hour, no matter who the man. This was as useful for regulating labor as it was for conquering land.”\n\n\n“Clocks arrived as tools of domination.”\n\n\n“On a larger scale, they graded native populations as being more or less “progressed” into modernity based on how removed their systems of time seemed from nature—a”\n\n\n“As would be the case in many different contexts going forward, the science of recording labor days was inextricable from the project of intensifying them.”\n\n\n“none of us who toil for our daily bread are free. At one time…we were chattel slaves; today we are, one and all, white and black, wage slaves.”\n\n\n“When the relationship of time to literal money is expressed as a natural fact, it obscures the political relationship between the seller of time and its buyer. This may seem obvious, but if time is money, it is so in a way that’s different for a worker than for an employer. For the worker, time is a certain amount of money—the wage. But the buyer, or employer, hires a worker to create surplus value; this excess is what defines productivity under capitalism. From an employer’s point of view, purchased time could always yield more money.”\n\n\n“The only reward for working faster is more work.”\n\n\n“In this way, Taylorism rendered labor more abstract and fungible, hastening a process that has often been referred to as “de-skilling.” Among other things, this deepened the divide between how different kinds of time are valued. As Braverman puts it, “Every step in the labor process is divorced, so far as possible, from special knowledge of training and reduced to simple labor. Meanwhile, the relatively few persons for whom special knowledge and training are reserved are freed so far as possible from the obligations of simple labor. In this way a structure is given to all labor processes that at its extremes polarizes those whose time is infinitely valuable and those whose time is worth almost nothing.”\n\n\n“The more fragmented and minutely timeable work becomes, the more meaningless it becomes.”\n\n\n“A psychopath would make a terrible content moderator.”\n\n\n“At Cognizant, where humans keep coming in for work despite the conditions, one worker tells Newton that they are merely “bodies in seats.”\n\n\n“The tragedy of fungible labor time lies first in its historical association with coercion, exploitation, and the imagining of people as machines. Time is the punitive dimension in which the wage worker is both measured and squeezed. But beyond that, an overemphasis on fungible time upholds an impoverished view of what time and labor are in the first place.”\n\n\n“Just because you’re going forward doesn’t mean I’m going backward.”\n\n\n“As a form of Protestantism, seventeeth-century Puritanism invited introspection and constant evaluation of the self against a high moral standard, a practice that included the use of daily journals where self-observation and measurement could take place.”\n\n\n“A feeling of time pressure can result from constantly having to switch tasks or coordinate with external factors. Here,”\n\n\n“Self-help has generally promised to revolutionize your life, not the social or economic hierarchy—and you can’t really blame anyone for not fulfilling a promise they never made. At the same time, even seemingly practical self-help can read as an invitation to find a niche in a brutal world and wait for the storm to pass you over.”\n\n\n“If time management is not simply an issue of numerical hours but of some people having more control over their time than others, then the most realistic and expansive version of time management has to be collective:”\n\n\n“Excess work and performance escalate into auto-exploitation.”\n\n\n“Trained to set her sights on infinity, she never experiences the feeling of having actually reached a goal and, instead, exhibits the “auto-aggression” of the master and mastered rolled into one. She is forever “jumping over [her] own shadow,” frustrated at the impossible gap between what is and what could be.”\n\n\n“Social comparison is probably as old as time, but to compare a wide range of people using the same grades, you have to be able to turn those people into data and decide what you’re optimizing for.”\n\n\n“If, in Taylorism, the measurement of work was an attempt to intensify it, then, in eugenics, the measurement of people was an attempt to “mold” them in a specific direction, a mechanistic combination of Mendelian genetics and social Darwinism.”\n\n\n“this is “a terrible time to have a midlife crisis.” But advice for winning the rat race assumes that you’re running”\n\n\n“But advice for winning the rat race assumes that you’re running in it, rather than peeling away from a vanishing dream.”\n\n\n“Slow living is now ‘for sale’ and approaches a consumerist lifestyle mostly for middle-class metropolitan dwellers—the majority of whom are probably far from holding transformative, progressivist or even socialist agendas. Arguably, many would admit that ‘it all needs to slow down,’ but such slowness would then be, more often than not, consumed, and consumed privately.”\n\n\n“It’s just that, as the experience economy expands to include commodified notions of things like slowness, community, authenticity, and “nature”—all while income inequality yawns wider and the signs of climate change intensify—I feel the panic of watching possible exits blocked. I keep wanting to do something instead of consume the experience of it. But seeking new ways of being, I find only new ways of spending.”\n\n\n“Whether conspicuous, compensatory, or both, consumption has long had a relationship to leisure, which can make leisure a strange kind of circumscribed freedom.”\n\n\n“Sociologists have observed that once assembly-line jobs made it difficult to see how well or hard someone had worked, what became visible instead was how much someone was able to consume. This consumption, in turn, became the new way to signal how hard one had worked.”\n\n\n“What makes you unique?” is a standard interview question. As a result, what once looked like leisure so easily becomes the arena both of the eternal self-upgrade and the search for some uniqueness to exploit. Marketing advice formerly given to companies—for example to “find your niche”—is now applicable to individuals during every moment of the day.”\n\n\n“true leisure requires the kind of emptiness in which you remember the fact of your own aliveness.”\n\n\n“Leisure is not refreshment-for-work but something completely different that exists for its own sake.”\n\n\n“If I exhibited the leisure mindset while in line for groceries, it was at least in part because I wasn’t worried about paying for them.”\n\n\n“Pieper’s definition of leisure emphasizes wholeness; it is “when a man is at one with himself, when he acquiesces in his own being.”\n\n\n“Heartbreak did not make me love the birds less; it did not make the ocean less beautiful; it only suffused my seeing them with a deep desire for things to be different.”\n\n\n“momentary pattern of light. Perhaps this is precisely what Pieper meant with his “vertical” time—maybe it is vertical not just in that it’s the opposite of horizontal, but also in that it reaches deep into the recesses of history even as it stretches up toward an infinite and utopian ideal.”\n\n\n“Rest is not some cute lil luxury item you grant to yourself as an extra treat after you’ve worked like a machine and are now burned out,” Hersey tweeted in October 2020. “Rest is our path to liberation. A portal for healing. A right.”\n\n\n“Hersey uses social media for work but is critical of the way it encourages a grind culture with historical roots in capitalism and white supremacy.”\n\n\n“introduced”—for example, indentured or wage labor. Whereas”\n\n\n“Whereas the source of boredom for the leisure class was free time, the source of boredom for everyone else was work, and working people had no problem deciding what to do with whatever leisure time they were allotted.”\n\n\n“In its least useful form, the concept of leisure time reflects an undignified process: working to buy the temporary experience of freedom and then faithfully breathing air in the little gaps that are allowed in the horizontal plane of work. Rest and recreation are applied like maintenance, the leisure machine to the feeding machine.”\n\n\n“At its most useful, however, leisure time is an interim means of questioning the bounds of the work that surrounds it. Like a stent in a culture that can’t stand what looks like emptiness, it might provide that vertical crack in the horizontal scale of work and not-work—that critical pause during which the worker wonders why she works so much, where collective grief is processed, and where the edges of something new start to become visible.”\n\n\n“What songs are audible when the wind stops? What has been kept alive in the time snatched from work and sheltered from ongoing destruction—what moments of recognition, what ways of relating, what other imagined worlds, what other selves? What other kinds of time?”\n\n\n“In his 2021 special, Inside, Bo Burnham deadpans that “the outside world, the non-digital world, is merely a theatrical space in which one stages and records content for the much more real, much more vital, digital space. One should only engage with the outside world as one engages with a coal mine. Suit up, gather what is needed, and return to the surface.”\n\n\n“For Bergson, time was duration—something creating, developing, and somewhat mysterious, as opposed to abstract and measurable. According to him, all our problems conceiving of the true nature of time stemmed from wanting to imagine discrete moments sitting side by side in space. He further noted that this “space” was not concrete environmental space, but something purely conceptual: Think of that green-on-black grid that sometimes shows up in the virtual nonspace of sci-fi movies, and think of moments in this kind of time as cubes existing in that space. (This conception also provided the grounds for the concept of fungible time I mention in chapters 1 and 2.) Bergson thought that our predisposition toward thinking of time in these kinds of spatial terms came from our experience manipulating inert matter; we wanted to see time in the same way, as something we could cut up, stack, and move around.”\n\n\n“Time as expressed in these processes—which Bergson explains using what he calls the élan vital (usually translated as “vital impetus” or “life force”)—is not an abstract quantity to be counted up and measured. Instead, it’s another irreversible turn of the kaleidoscope, something driving division, reproduction, growth, decay, and complexity. The old adage “You never step in the same river twice” also speaks quite well to what Bergson is describing, especially if you go on to consider the shifts in evolution of the riverbank, the canyon that the river may be slowly carving, and maybe even the cellular processes in your foot.”\n\n\n“Meanwhile, as you stand there thinking about it, the live edge of the lava is moving forward into the future, which is imminent in every present moment but also contains the history of everything that happened before. Another example would be a seed that has fallen from one individual in a generation of plants and that contains the instructions for a future plant.”\n\n\n“Clock time is not the only form of time reckoning we experience, but it is certainly primary in how many of us think about the “stuff” of time. And it was an allegiance to clock time that allowed colonists, anthropologists, and contemporary Western observers in general to view non-Western and indigenous cultures as being without, or outside, time.”\n\n\n“We don’t have a word for nonlinear in our languages because nobody would consider traveling, thinking, or talking in a straight line in the first place. The winding path is just how a path is, and therefore it needs no name.”\n\n\n“by every subsequent wave. Look again at the pebbles. Make no mistake: They are neither signs nor symbols of time. No—they really are two things at once: seafloor from the last ice age, and future sand.”\n\n\n“Resting here gives us a very different sense of being “on time.” Rather than avatars passing through an empty calendar square, we are actually on top of the material outcome of processes that span millions of years into both the past and the future. Suddenly, everything we look at is suffused with concrete time: not just the pebbles, crags, and cliffs, but also the fog’s slow movement to the south; each wave’s unrepeatable expression of tides and wind; the frenetic activity of the beach flies; the dispersion of air and water through our bodies; and even the chemicals flashing across our synapses as we think these very thoughts. They, too, will never repeat, and they, too, make the world anew.”\n\n\n“ROCKS WILL TEACH you the inseparability of time and space.”\n\n\n“I see that the events of the past are still present….This impression is a glimpse not of timelessness but timefulness, an acute consciousness of how the world is made by—indeed, made of—time.”\n\n\n“Newtonian time is the kind of time that can be measured, bought, and sold. Wage work requires us to see time as “stuff” divorced from bodies and environmental context.”\n\n\n“There is no inherent reason for a season to be any length of time, much less of four equal, mutually exclusive lengths. Until relatively recently, the naming and recognition of seasons or seasonal entities was an indicator of some action to be taken: collecting, hunting, harvesting.”\n\n\n“If, as Deloria puts it, each place exhibits a “personality,” then it is made out of as much when as who: a string of overlapping developments like the tracks of a song. This song sounds slightly different in each place:”\n\n\n“invisibility is part of the very nature of habit.”\n\n\n“In the translator’s afterword to a 2010 edition of An Attempt at Exhausting a Place in Paris, Marc Lowenthal emphasizes the “attempt” in Perec’s toc: true\ntitle, writing that “time, unarrestable, works against [Perec’s] project….Every bus that passes, every person who walks by, every object, thing, and event—everything that happens and that does not happen ultimately serves no other function than that of so many chronometers, so many signals, methods, and clues for marking time, for eroding permanence.”\n\n\n“The Birds Are Not on Lockdown, and More People Are Watching Them,”\n\n\n“it might “give us peace and calm to see that even though our rhythm is interrupted, there is a larger rhythm that continues to go on.”\n\n\n“Most living entities and systems on this planet obviously do not live by the Western human clock (though some, like the crows who memorize a city’s daily garbage truck route, do of course adapt to the timing of human activities). To watch a brown creeper as it inches up and down, peering into crevices and extracting bugs with its little dentist beak, is thus a way of catching a ride out of the grid and toward a time sense so different that it is barely imaginable to us.”\n\n\n“The literal tree in front of you is encoding time and change at this literal moment.”\n\n\n“This exercise of unfreezing something in time is not hard to do. If you want to see time that isn’t fungible, just pick a point in space—a branch, a yard, a sidewalk square, a webcam—and simply keep watch. A story is being written there. Like the larger and larger wind patterns on Windy.com, this story is inseparable from the story of all life, even yours. This story is, finally, the signature of “it”: the restless, unstoppable, constantly overturning thing that makes it all go.”\n\n\n“The world, just like the architecture of a city, becomes a patchwork of outcomes from different weeks, decades, and centuries, all of it being built upon and eroded—pushing, trickling, and winging forward into the unknown.”\n\n\n“In a display case, a thing becomes only a facsimile of itself, like a drum hung on the gallery wall,” Kimmerer writes. “A drum becomes authentic when human hand meets wood and hide. Only then do they fulfill its intention.”\n\n\n“Having specifically studied how mosses “decide” to grow on a rock, Kimmerer knows that the mosses that grow on rocks are “inordinately resistant to domestication.”\n\n\n“Owning diminishes the innate sovereignty of the thing,”\n\n\n“To see something in time is to allow that it has a life and to allow that this life entails more than the mechanistic cause-and-effect of a Newtonian world. In this way of thinking, mosses “decide” which rocks to live on, and even rocks have lives.”\n\n\n“The specifically disorienting part, for me, was when it showed Canada geese passing over New York City. Seen as part of a journey the geese had taken for thousands of years, the skyline looked suddenly alien to me; “New York” became an odd conglomeration of hard shapes and protrusions along a particular riverbank. The city existed for the geese, too, but they read it differently, perhaps as a signpost on a path of other signposts that may have included other rivers. Their flight path tied these places together into one big calendar. As”\n\n\n“THE word experience has a common origin with experiment. To experience something is to be present for it, to be the responsive co-creator of something that is happening—like the ducks and geese who make migration happen by sensing the weather and deciding when to leave.”\n\n\n“Experience isn’t merely the best teacher; it’s the only teacher. If she’s learned anything raising Jax, it’s that there are no shortcuts; if you want to create the common sense that comes from twenty years of being in the world, you need to devote twenty years to the task. You can’t assemble an equivalent collection of heuristics in less time; experience is algorithmically incompressible.”\n\n\n“The real paradox is a mind that conceives of the world as inert but that may come to see itself as bound to the same laws of determinism as everything else—in a way, the ultimate self-own.”\n\n\n“Freedom is choice, and choice is scattered throughout the universe, pushing forward and acting upon what would constrain it.”\n\n\n“For Bergson, the everyday experience of learning and recognition demonstrates both the freshness of each moment and the irreversibility of time. He describes walking through a familiar town where the buildings don’t seem to change. But as he thinks back to the first time he ever observed those buildings, a comparison emerges that momentarily unfreezes the world: “It seems that these objects, continually perceived by me and constantly impressing themselves on my mind, have ended by borrowing from me something of my own conscious existence; like myself they have lived, and like myself they have grown old. This is not a mere illusion: for if today’s impression were absolutely identical with that of yesterday, what difference would there be between perceiving and recognizing, between learning and remembering?”\n\n\n“Like rocks pushing up out of the depths and the water that wears them down; like browned and ripened buckeye fruits falling off the tree and rolling down the hill; like poetry, which strains the boundaries of an ossified language; or like Bergson’s cascading rocket that can never be arrested—the co-creation events of our lives do not play out in an external, homogenous time. They are the stuff of time itself.”\n\n\n“the future can cease to look like an abstract horizon toward which your abstract ego plods in its lonely container of a body. Instead, “it,” that irrepressible force that drives this moment into the next, is a thing that is speaking back to you always—even and especially from unexpected places. The task for many of us is to learn once more how to hear.”\n\n\n“it is one thing to acknowledge the past and future losses that follow from what has occurred; it is another to truly see history and the future proceeding with the same grim amorality as the video playhead, where nothing is driving it except itself. In failing to recognize the agency of both human and nonhuman actors, such a view makes struggle and contingency invisible and produces nihilism, nostalgia, and ultimately paralysis.”\n\n\n“some relationships arguably end in the first place because partners have stopped seeing each other in time, one partner having replaced the living, changing other with a static image that can impart no surprises, only a comforting presence.”\n\n\n“a Westerner’s attempt to arrive at the idea of how things are “supposed to be” is usually fraught, because it doesn’t take into account who is doing the supposing.”\n\n\n“that?…Mountain time and city time appear to be bifocal. Even with geology functioning at such remarkably short intervals,”\n\n\n“A super-event in 1934? In 1938? In 1969? In 1978? Who is going to remember that?…Mountain time and city time appear to be bifocal. Even with geology functioning at such remarkably short intervals, people have ample time to forget it.”\n\n\n“Fire was part of a reciprocal responsibility between one subject (humans) and another (land).”\n\n\n“The land is not really the place (separate from ourselves) where we act out the drama of our isolate destinies. It is not a means of survival, a setting for our affairs….It is rather a part of our being, dynamic, significant, real. It is our self.”\n\n\n“To think deterministically is to take things for granted, both forward and backward in time. Just as I misunderstood the forested mountains as a child, projecting them into a supposedly uniform past, the concept of the Anthropocene has the potential to make the outcomes of specific actions by specific people seem like a natural and inevitable condition.”\n\n\n“Human beings were rendered economic machines that seek to maximize their share of sparse natural resources. The inscription of a bio-evolutionary and thus inevitable impulse behind the ascent of Western Man—“we all want to grab more resources, Europeans just did it better than everyone else”—came to vindicate capitalism, white supremacy, and imperial expansion. The West invented Man and projected Him onto the past as natural and timeless, rather than historical and cultural.”\n\n\n“The story of Enlightenment Man teaches me an all-too-common truth: that the people who stand to gain the most from determinism (in others) are typically the people doing the determining.”\n\n\n“We just sell the cigarettes; you’re the ones smoking them.”\n\n\n“In the meantime, energy companies’ emphasis on consumption is disingenuous. This rhetoric echoes Big Tobacco’s effort to portray itself as a neutral purveyor of what consumers just can’t seem to help but demand. In other words, We just sell the cigarettes; you’re the ones smoking them. A framing like this one portrays climate change as solely”\n\n\n“A framing like this one portrays climate change as solely “our” fault, where the “our” is an aggregate of consumers who should attend to their carbon footprint calculators. All the while, as Aronoff writes, “every shred of evidence suggests the [energy] industry is moving full speed ahead in the opposite direction, pushing more exploration and more production as temperatures rise, seas swell, and fires burn.” One smoky day while I was writing this chapter, a Wells Fargo ATM asked me if I wanted to donate to help with the wildfires. I stared back at the screen. Wells Fargo is one of the largest funders of fossil fuels, having invested $198 billion into the coal, oil, and gas industry in the four years following the Paris Agreement. Just as the industry of individual time management resells the idea of time as money to the isolated bootstrapper, energy companies sell the idea of the carbon footprint to conceal larger and more significant avenues of change. These include both technological and political tools to which we already have access. For Klein, Aronoff, and others, some of those tools would be public regulation and oversight—things like the Green New Deal—and standing up to the global trade agreements that favor the suicidal time horizon of energy companies. Indeed, Klein has an entire chapter toc: true\ntitled “Planning and Banning.” Klein acknowledges that this is an uphill battle in the United States, where both planning and banning are currently decried as government overreach. Nevertheless, she writes, “we should be clear about the nature of the challenge: it is not that ‘we’ are broke or that we lack options. It is that our political class is utterly unwilling to go where the money is (unless it’s for a campaign contribution), and the corporate class is dead set against paying its fair share.” For her part, Aronoff takes great pains throughout her book to remind us that the hill in the uphill battle is historically specific: “In positing all of human existence as an endless striving toward market society, neoliberals had to erase not just the possibility of a future but all memory of a past when humans managed to organize themselves in other ways. The kinds of tools needed to navigate out of the climate crisis—things like public ownership, full employment, or even just tough regulations—have receded into memory.” Aronoff is talking mostly about policies of the New Deal era, before a globalized economy took hold and the perception of government regulation soured in a neoliberal atmosphere. But one could extend this notion of political amnesia even further back, as an echo of what serynada describes: rewriting the history of Man as an economic machine. Again, purgatory is enervating. Like a fog machine spewing a priori dystopia, energy companies are still selling their certain future, still designing targets and portraying us as drifting helplessly toward them. I think back on my nightmares, about how the future looks there. Who wrote that scenario? — A PIER JUTS out from the promenade into that uncompromising ocean. The moment we move past the”\n\n\n“Without suppressing grief, there has to be a different way of thinking about time than the one in which”\n\n\n“Rather, to the nihilist who cannot imagine the future, I am highlighting a perspective that has survived, and continues to survive, the long-ago end of the world. There are many people and places that could accept neither Enlightenment Man’s march of progress nor the billiard ball declinism of the Anthropocene—because that narrative was inherently premised upon their destruction, commodification, and relegation to a state of nonbeing. For those people and places, the historical past can never be an object of nostalgia, and the future has always been in jeopardy. If you don’t want to kick the can down the road, look to those who never recognized the road in the first place. BACK ON THE seawall, there is a circle of five wooden posts that looks like a miniature Stonehenge, that most iconic of calendrical tools.”\n\n\n“The land is chief, man is its servant.”\n\n\n“NEVER TURN YOUR BACK ON THE OCEAN. That sign always puts me in my place. It reminds me that the beach is not an amenity for humans—that I can be there, but I’d better learn the laws of the ocean if I want to stay alive.”\n\n\n“I grew up on a false plateau I took for infinity.”\n\n\n“Observing that the Greek word apokalypsis meant “through the concealed,” Washuta writes that “apocalypse has very little to do with the end of the world and everything to do with vision that sees the hidden, that dismantles the screen.”\n\n\n“We live according to the sun, not the clock.”\n\n\n“Cynicism and nihilism will make you dry up, like soil compacted by neglect and abuse.”\n\n\n“As Marx writes in Capital, “Après moi, le déluge [After me, the flood] is the watchword of every capitalist and of every capitalist nation.”\n\n\n“Language is dynamic, unruly, always splintering. It has to be, because in order to use it, we take words and constructions we never chose and make them do what we—as collectives, however big or small—want them to do.”\n\n\n“Just because a language is imposed, it doesn’t mean it can be controlled; and just because it’s spoken, it doesn’t mean it’s been internalized.”\n\n\n“An inside joke makes a new inside, a new center.”\n\n\n“Time was never a specific minute, but rather spaces of time, like early morning, just afternoon, or just before midnight. The real meaning of Indian time comes from…nake nula waun yelo, a phrase in traditional songs that means ‘I’m ready for whatever, any place, always prepared.’ ”\n\n\n“But if, just for a moment, we leave behind historically and culturally specific notions of clock-based punctuality and time as money, then Filipino time actually doesn’t appear to be a problem at all. If you and everyone you know are on it, then it’s just time.”\n\n\n“Speaking a language is a way of participating in the making, preservation, and evolving of worlds.”\n\n\n“Study is what you do with other people. It’s talking and walking around with other people, working, dancing, suffering, some irreducible convergence of all three, held under the name of speculative practice….The point of calling it “study” is to mark that the incessant and irreversible intellectuality of these activities is already present.”\n\n\n“In this partitioned, soundproof, PCB lined office jungle, truly the worst fate is to believe in your boss’s dream, to strive for the company good.”\n\n\n“Mould writes that, in either case, creativity is not actually creative, because it merely “produces more of the same form of society.” If it makes progress, it is the progress of capitalist logic into ever-more-minute corners of our daily lives, making what Braverman calls “the universal market” even more universal.”\n\n\n“Why are individuals expected to be “resilient” when corporations are not?”\n\n\n“The demand for less work might be made “not so that we can have, do, or be what we already want, do, or are, but because it might allow us to consider and experiment with different kinds of lives, with wanting, doing, and being otherwise.”\n\n\n“What does one do when one finds oneself marking time on the job? One develops a lot of cynicism, apathy, and anger to which there is no outlet.”\n\n\n“To keep walking is to keep living, to keep inquiring, and to keep hoping.”\n\n\n“This simple gesture, and the story of the beans, made me realize how broken my mental mechanisms were for thinking about anything beyond the transactional exchange.”\n\n\n“Time can have many rhythms, and rhythms can take on many meanings.”\n\n\n“Tempo and intensity surround us at every level: we know that a birthday tomorrow can feel like an eternity to a little child whilst a birthday one year ago can seem like only yesterday to an old person. The dormant period of winter is followed by a burst of growth in spring….’Our’ social time as it emerges from common usage is inseparable from the rhythms of the earth. Complexity reigns supreme.”\n\n\n“Saying it meant that you could take time and give time, but also that you could plant time and grow more of it and that there were different varieties of time. It meant that all your time grew out of someone else’s time, maybe out of something someone planted long ago. It meant that time was not the currency of a zero-sum game and that, sometimes, the best way for me to get more time would be to give it to you, and the best way for you to get some would be to give it back to me. If time were not a commodity, then time, our time, would not be as scarce as it seemed just a moment ago. Together, we could have all the time in the world.”\n\n\n“In “Why Time Management Is Ruining Our Lives,” Oliver Burkeman observes that keeping a detailed log of your time use, in an effort to save time or spend it more wisely, ironically “heightens your awareness of the minutes ticking by, then lost forever.” Whether on the level of minutes or of life stages and benchmarks, the more you stare at time, the more cruelly it seems to slip through your fingers.”\n\n\n“the product offered by a capitalist version of wellness is “the means to remake oneself into an ever more perfect self-correcting machine capable of setting goals and moving toward them with smooth determination.”\n\n\n“How long does it take, or should it take, for a body to move through the world, the forty-plus-hour work week, the demands of caregiving for ailing parents, the daily commute of the body with its changing needs over the life span—a pregnant body, an aging one, a body in recovery after a bad injury?” Hendren asks. “Is the clock of industrial time built for bodies at all?”\n\n\n“It’s actually okay to be on a spectrum of reality. It means that there are times when it’s juicier, there are times when it’s drier, there’s times when I’m gonna be tired, there’s times when I’m going to have a lot of energy. It’s actually part of being alive. It is being alive.”\n\n\n“Too worn out to grasp, and forced to sit back, the tired and resigned person finds that something else floods in: the world, in all its detail, its constantly acting and infinitely dispersed agents, and its minute-by-minute changes.”\n\n\n“deep tiredness loosens the strictures of identity. Things flicker, twinkle, and vibrate at the edges.”\n\n\n“Maybe “the point” isn’t to live more, in the literal sense of a longer or more productive life, but rather, to be more alive in any given moment—a movement outward and across, rather than shooting forward on a narrow, lonely track.”\n\n\n“Disability highlights something that is true for all of us: No matter how independent and fit we may feel, we are not simply alive but, rather, kept alive—against odds that some people are nonetheless privileged enough to ignore.”\n\n\n“People do not spring up from the soil like mushrooms,” she writes. “People need to be cared for and nurtured throughout their lives by other people.”\n\n\n“For me, death is when I can no longer engage with the world around me; when I can no longer take anything in and, therefore, can no longer connect.”\n\n\n“what’s the use of building your body if you can’t build your mind?‘”\n\n\n“It is, therefore, a severe and grave misinterpretation of man to deal with him as if he were a closed system.”\n\n\n“A lack of “direct access” to the mental states of those selves makes us less prone to see them as having evolving inner lives.”\n\n\n“No matter how optimized, healthy, and productive I am, I simply will not become more or better forever, which means there are things I will never do and never be. Just like this book, which could have been anything when I started it, my life will take some paths and not others—and then it will end, the thread pulled out of the ball, with no witch to indulge me by taking it back. Realizing that I cannot be everything is in one sense incredibly freeing: It means I am not responsible for being everything. Yet the fact that life ends, for anyone who enjoys being alive and in the world, is also inherently sorrowful.”\n\n\n“I feel alive if I’m not alone in the air, but embraced by it. I feel alive when someone’s eyes light up, and mine do too. I feel alive if I can look at a deer and see it looking back at me; if, when geese speak, it sounds like language; if, when I walk on the ground, I feel it pushing back against me.”\n\n\n“No one is responsible for an emergence; no one can glory in it, since it always occurs in the interstice.”\n\n\n“But when I understand that this glass is already broken, every minute with it is precious.”\n\n\n“With nothing but distance between you and your destination, it may as well have already taken place. It’s as though you had an amazing set of binoculars that let you see something far away in such detail that you didn’t actually need to go there. Let’s just get it over with, says the heartbroken subject, unable to enjoy her already-broken cup.”\n\n\n“You’re turning time into space, he would say. You’re imagining empty blocks of time stretching out in front of you, mentally crossing that distance toward the thing you think has already happened, instead of admitting the creative aspect of time that is ever evolving and shifting, each second heaving the world—and you—through the crust of the present and into the future.”\n\n\n“Could it be that the opposite of looking assuredly through binoculars at a flat space would be the perspective you get when rounding a mountain trail—one where, even though you know where you are, things look different at every turn?”\n\n\n“Though my episodic memory goes back only so far, my existence is explained by older things: my mother’s immigration, a war whose exigencies threw my grandparents together, and the fish swimming off the coast of Estancia, on the eastern tip of Iloilo. The people who fished there have something to do with me, just as I continue to have something to do with them.”\n\n\n“Kinship moves in cycles, the land moves in seasonal cycles, the sky moves in stellar cycles, and time is so bound up in those things that it is not even a separate concept from space. We experience time in a very different way from people immersed in flat schedules and story-less surfaces. In our spheres of existence, time does not go in a straight line, and it is as tangible as the ground we stand on.”\n\n\n“Compared to chronos, kairos sounds like the domain of those wayfarers who knew that time is inseparable from space and that every place-moment demands close attention, lest you miss your opportunity. It’s not that you can’t plan, but that the time in the plan doesn’t appear flat, dead, inert. Instead, in the “meantime,” you wait with your ear to the ground for patterns of vibration that will never repeat themselves. Faced with flatness, you look for an opening. When it comes, you take it, and you don’t look back.”\n\n\n“You can make the same mistake in the opposite direction, forgetting that the future will contain many such moments of doubt—or even neglecting to notice when you’re in one.”\n\n\n“The past would crush you with tradition, and the future would crush you with determinism. Hence the importance and fragility of the “gap” (another term for “non-time”) in the toc: true\ntitle of Arendt’s preface, “The Gap between Past and Future.” To live in the gap between past and future is quite simply the human condition, even if culturally dominant and politically convenient views of time, history, and the future obscure it from us. Looking mournfully to the future in which something new can never happen, we can’t see ourselves standing in the gap, the only place where anything new is capable of happening. It makes me wonder if one meaning of “having time” is to halve time—to make a cut in chronos and hold the past and the future apart as much as hope will allow.[*5] — EVERY PIECE OF writing is a time capsule. It assembles fragments of its own world and sends them onward to a reader who exists in a different one, not just in space but also in time. Even writing privately in a journal presupposes a future self who will be reading it—and a future at all. In the case of this book, I cannot know what has happened between the time I am writing this and the time in which you are encountering it. But I can tell you that I am living in a moment of doubt. Perhaps you are, too. That evening when I saw the indistinguishable figure, I had been headed to the place where the road ends, a designated “natural area” called Raab’s Lagoon. There, after the pavement turns to grass, you pass under alder and fir trees and come to a bench dedicated to a man who died in 2016. If you keep going, the pathway juts out into the water, part of an artificial barrier between the waters of Quartermaster Harbor and the smaller lagoon. Across a small breach through which the harbor water flows, the barrier continues on until it hits the other side of the lagoon. The first time I visited, the water in the breach didn’t seem to be moving in any particular direction. It was high tide, though I didn’t know that at the time; having just arrived, I thought the area always looked like that. Over the course of a few weeks, I inevitably became familiar with the tides, because Quartermaster Harbor was right outside the door of my room. When the tide was high, you could hear the water plopping and the plastic canoe docks banging against the wooden posts, something I started to call “the song of the dock.” When the tide was lower, white-winged scoters, migratory diving ducks with a surreal flourish of a white feather under their eyes, would appear in loose flocks and dive for the mussels at the bottom. When the tide was all the way out, the mussel shells were maximally revealed, and both people”\n\n\n“The mind always wants to categorize and compare,”\n\n\n“A belief may be comforting. Only through your own experience, however, does it become liberating.”\n\n\n“So the single most vital step on your journey toward enlightenment is this: learn to disidentify from your mind. Every time you create a gap in the stream of mind, the light of your consciousness grows stronger.”\n\n\n“Emotion arises at the place where mind and body meet.”\n\n\n“If you really want to know your mind, the body will always give you a truthful reflection, so look at the emotion or rather feel it in your body. If there is an apparent conflict between them, the thought will be the lie, the emotion will be the truth.”\n\n\n“doorway into Being. ¤ An emotion usually represents an amplified and energized thought pattern, and because of its often overpowering energetic charge, it is not easy initially to stay present enough to be able to watch it. It wants to take you over, and it usually succeeds unless there is enough presence in you.”\n\n\n“Basically, all emotions are modifications of one primordial, undifferentiated emotion that has its origin in the loss of awareness of who you are beyond name and form. Because of its undifferentiated nature, it is hard to find a name that precisely describes this emotion. “Fear” comes close, but apart from a continuous sense of threat, it also includes a deep sense of abandonment and incompleteness.”\n\n\n“the harder the mind struggles to get rid of the pain, the greater the pain.”\n\n\n“Pleasure is always derived from something outside you, whereas joy arises from within.”\n\n\n“Even when the sky is heavily overcast, the sun hasn’t disappeared. It’s still there on the other side of the clouds.”\n\n\n“All cravings are the mind seeking salvation or fulfillment in external things and in the future as a substitute for the joy of Being.”\n\n\n“Pain is inevitable as long as you are identified with your mind, which is to say as long as you are unconscious, spiritually speaking.”\n\n\n“The pain that you create now is always some form of nonacceptance, some form of unconscious resistance to what is.”\n\n\n“In other words, the more you are identified with your mind, the more you suffer.”\n\n\n“Yes, we need the mind as well as time to function in this world, but there comes a point where they take over our lives, and this is where dysfunction, pain, and sorrow set in.”\n\n\n“The accumulation of time in the collective and individual human mind also holds a vast amount of residual pain from the past.”\n\n\n“Always say “yes” to the present moment. What could be more futile, more insane, than to create inner resistance to something that already”\n\n\n“What could be more futile, more insane, than to create inner resistance to something that already is?”\n\n\n“By watching the mechanics of the mind, you step out of its resistance patterns, and you can then allow the present moment to be.”\n\n\n“Accept then act. Whatever the present moment contains, accept it as if you had chosen it. Always work with it, not against it. Make it your friend and ally, not your enemy. This will miraculously transform your whole life.”\n\n\n“Pain can only feed on pain. Pain cannot feed on joy. It finds it quite indigestible.”\n\n\n“Once the pain-body has taken you over, you want more pain. You become a victim or a perpetrator. You want to inflict pain, or you want to suffer pain, or both. There isn’t really much difference between the two.”\n\n\n“Where there is anger, there is always pain underneath.”\n\n\n“your own light will quickly grow stronger. When a log that has only just started to burn is placed next to one that is burning fiercely, and after”\n\n\n“When a log that has only just started to burn is placed next to one that is burning fiercely, and after a while they are separated again, the first log will be burning with much greater intensity. After all, it is the same fire.”\n\n\n“an emotion is the body’s reaction to your mind.”\n\n\n“To the ego, death is always just around the corner.”\n\n\n“Power over others is weakness disguised as strength. True power is within, and it is available to you now.”\n\n\n“The secret of life is to “die before you die” and find that there is no death.”\n\n\n“Studying the complexities of the mind may make you a good psychologist, but doing so won’t take you beyond the mind, just as the study of madness isn’t enough to create sanity.”\n\n\n“The ego’s needs are endless. It feels vulnerable and threatened and so lives in a state of fear and want.”\n\n\n“When you are present, you can allow the mind to be as it is without getting entangled in it. The mind in itself is not dysfunctional. It is a wonderful tool. Dysfunction sets in when you seek your self in it and mistake it for who you are. It then becomes the egoic mind and takes over your whole life.”\n\n\n“The eternal present is the space within which your whole life unfolds, the one factor that remains constant. Life is now. There was never a time when your life was not now, nor will there ever be.”\n\n\n“What you think of as the past is a memory trace, stored in the mind, of a former Now. When you remember the past, you reactivate a memory trace and you do so now. The future is an imagined Now, a projection of the mind.”\n\n\n“Time is what keeps the light from reaching us. There is no greater obstacle to God than time.”\n\n\n“The mind cannot know the tree. It can only know facts or information about the tree. My mind cannot know you, only labels, judgments, facts, and opinions about you. Being alone knows directly.”\n\n\n“heavy burden of psychological time. If you set yourself a goal and work toward it, you are using clock time. You are aware of where you want to go, but you honor and give your fullest attention to the”\n\n\n“If you set yourself a goal and work toward it, you are using clock time. You are aware of where you want to go, but you honor and give your fullest attention to the step that you are taking at this moment. If you then become excessively focused on the goal, perhaps because you are seeking happiness, fulfillment, or a more complete sense of self in it, the Now is no longer honored. It becomes reduced to a mere stepping stone to the future, with no intrinsic value. Clock time then turns into psychological time. Your life’s journey is no longer an adventure, just an obsessive need to arrive, to attain, to “make it.” You no longer see or smell the flowers by the wayside either, nor are you aware of the beauty and the miracle of life that unfolds all around you when you are present in the Now.”\n\n\n“belief in a future heaven creates a present hell.”\n\n\n“Guilt, regret, resentment, grievances, sadness, bitterness, and all forms of nonforgiveness are caused by too much past, and not enough presence.”\n\n\n“If all your problems or perceived causes of suffering or unhappiness were miraculously removed for you today, but you had not become more present, more conscious, you would soon find yourself with a similar set of problems or causes of suffering, like a shadow that follows you wherever you go.”\n\n\n“There is no salvation in time. You cannot be free in the future. Presence is the key to freedom, so you can only be free now.”\n\n\n“Hope is what keeps you going, but hope keeps you focused on the future, and this continued focus perpetuates your denial of the Now and therefore your unhappiness.”\n\n\n“The mind unconsciously loves problems because they give you an identity of sorts.”\n\n\n“Problem” means that you are dwelling on a situation mentally without there being a true intention or possibility of taking action now and that you are unconsciously making it part of your sense of self. You become so overwhelmed by your life situation that you lose your sense of life, of Being. Or you are carrying in your mind the insane burden of a hundred things that you will or may have to do in the future instead of focusing your attention on the one thing that you can do now.”\n\n\n“If you cannot be present even in normal circumstances, such as when you are sitting alone in a room, walking in the woods, or listening to someone, then you certainly won’t be able to stay conscious when something “goes wrong” or you are faced with difficult people or situations, with loss or the threat of loss. You will be taken over by a reaction, which ultimately is always some form of fear, and pulled into deep unconsciousness. Those challenges are your tests.”\n\n\n“When you learn to be the witness of your thoughts and emotions, which is an essential part of being present, you may be surprised when you first become aware of the background “static” of ordinary unconsciousness and realize how rarely, if ever, you are truly at ease within yourself. On the level of your thinking, you will find a great deal of resistance in the form of judgment, discontent, and mental projection away from the Now. On the emotional level, there will be an undercurrent of unease, tension, boredom, or nervousness. Both are aspects of the mind in its habitual resistance mode.”\n\n\n“Unhappiness spreads more easily than a physical disease.”\n\n\n“Once you realize that a certain kind of food makes you sick, would you carry on eating that food and keep asserting that it is okay to be sick?”\n\n\n“When you complain, you make yourself into a victim. When you speak out, you are in your power.”\n\n\n“is there something that you “should” be doing but are not doing it? Get up and do it now. Alternatively, completely accept your inactivity, laziness, or passivity at this moment, if that is your choice. Go into it fully. Enjoy it. Be as lazy or inactive as you can. If you go into it fully and consciously, you will soon come out of it. Or maybe you won’t. Either way, there is no inner conflict, no resistance, no negativity.”\n\n\n“Stress is caused by being “here” but wanting to be “there,” or being in the present but wanting to be in the future. Its a split that tears you apart inside.”\n\n\n“Waiting is a state of mind. Basically, it means that you want the future; you don’t want the present. You don’t want what you’ve got, and you want what you haven’t got.”\n\n\n“So give up waiting as a state of mind. When you catch yourself slipping into waiting … snap out of it. Come into the present moment.”\n\n\n“Your outer journey may contain a million steps; your inner journey only has one: the step you are taking right now.”\n\n\n“gain the world and lose your soul,” as Jesus puts it. Ultimately, of course, every outer purpose is doomed to “fail” sooner or later, simply because it is subject to the law of impermanence of all things.”\n\n\n“Ultimately, of course, every outer purpose is doomed to “fail” sooner or later, simply because it is subject to the law of impermanence of all things. The sooner you realize that your outer purpose cannot give you lasting fulfillment, the better. When you have seen the limitations of your outer purpose, you give up your unrealistic expectation that it should make you happy, and you make it subservient to your inner purpose.”\n\n\n“The past cannot survive in your presence. It can only survive in your absence.”\n\n\n“What do you mean by “rooted within yourself”? It means to inhabit your body fully. To always have some of your attention in the inner energy field of your body. To feel the body from within, so to speak. Body awareness keeps you present.”\n\n\n“If a fish is born in your aquarium and you call it John, write out a birth certificate, tell him about his family history, and two minutes later he gets eaten by another fish thats tragic. But ifs only tragic because you projected a separate self where there was none. You got hold of a fraction of a dynamic process, a molecular dance, and made a separate entity out of it.”\n\n\n“Consciousness takes on the disguise of forms until they reach such complexity that it completely loses itself in them.”\n\n\n“The teacher and the taught together create the teaching.”\n\n\n“Even if there is noise, there is always some silence underneath and in between the sounds. Listening to the silence immediately creates stillness inside you.”\n\n\n“You can study and talk about honey for as long as you like, but you won’t really know it until you taste it. After you have tasted it, the word becomes less important to you. You won’t be attached to it anymore.”\n\n\n“An image, no matter how beautiful or powerful, is already defined in form, so there is less scope for penetrating more deeply.”\n\n\n“The light of their consciousness was not yet strong enough to make friends with their animal nature, to allow it to be and even enjoy that aspect of themselves let alone to go deeply into it to find the divine hidden within it, the reality within the illusion. So they did what they had to do. They began to disassociate from their body. They now saw themselves as having a body, rather than just being it.”\n\n\n“Forgiveness is to offer no resistance to life to allow life to live through you.”\n\n\n“Before you enter the temple, forgive.”\n\n\n“In any thought activity, make it a habit to go back and forth every few minutes or so between thinking and an inner kind of listening, an inner stillness. We could say. don’t just think with your head, think with your whole body.”\n\n\n“opening up. Every sound is born out of silence, dies back into silence, and during its life span is surrounded by silence. Silence enables the sound to be.”\n\n\n“Every sound is born out of silence, dies back into silence, and during its life span is surrounded by silence. Silence enables the sound to be. It is an intrinsic but unmanifested part 0fevery sound, every musical note, every song, every word.”\n\n\n“Everybody pays attention to the things in space, but who pays attention to space itself?”\n\n\n“Once you have a theory, ifs not too hard to find evidence to substantiate it, at least until some other theory comes along.”\n\n\n“At least two points of reference are needed for distance and space to come into being. Space comes into being the moment the One becomes two, and as “two” become the “ten thousand things,” as Lao Tse calls the manifested world, space becomes more and more vast. So world and space arise simultaneously.”\n\n\n“If there were no illusion, there would be no enlightenment.”\n\n\n“You see time as the means to salvation, whereas in truth it is the greatest obstacle to salvation.”\n\n\n“The positive already contains within itself the as yet unmanifested negative. Both are in fact different aspects of the same dysfunction.”\n\n\n“The reason why the romantic love relationship is such an intense and universally sought-after experience is that it seems to offer liberation from a deep-seated state of fear, need, lack, and incompleteness that is part of the human condition in its unredeemed and unenlightened state.”\n\n\n“Every addiction starts with pain and ends with pain.”\n\n\n“To disidentify from thinking is to be the silent watcher of your thoughts and behavior, especially the repetitive patterns of your mind and the roles played by the ego.”\n\n\n“The bond that connects you with that person is the same bond that connects you with the person sitting next to you on a bus, or with a bird, a tree, a flower. Only the degree of intensity with which it is felt differs.”\n\n\n“when you know there is disharmony and you hold that “knowing,” through your knowing a new factor has come in, and the disharmony cannot remain unchanged.”\n\n\n“You cannot transform yourself, and you certainly cannot transform your partner or anybody else. All you can do is create a space for transformation to happen, for grace and love to enter.”\n\n\n“Judgment is either to confuse someone’s unconscious behavior with who they are or to project your own unconsciousness onto another person and mistake that for who they are.”\n\n\n“It is not easy to live with an enlightened person, or rather it is so easy that the ego finds it extremely threatening.”\n\n\n“As a general rule, the major obstacle for men tends to be the thinking mind, and the major obstacle for women the pain-body, although in certain individual cases the opposite may be true, and in others the two factors may be equal.”\n\n\n“if you are trapped in a nightmare you will probably be more strongly motivated to awaken than someone who is just caught in the ups and downs of an ordinary dream.”\n\n\n“A victim identity is the belief that the past is more powerful than the present, which is the opposite of the truth.”\n\n\n“In Being, male and female are one. Your form may continue to have certain needs, but Being has none. It is already complete and whole. If those needs are met, that is beautiful, but whether or not they are met makes no difference to your deep inner state. So it is perfectly possible for an enlightened person, if the need for the male or female polarity is not met, to feel a sense of lack or incompleteness on the outer level of his or her being, yet at the same time be totally complete, fulfilled, and at peace within. In”\n\n\n“Acute unhappiness can be a great awakener.”\n\n\n“Happiness depends on conditions being perceived as positive; inner peace does not.”\n\n\n“For example, when a loved one has just died, or you feel your own death approaching, you cannot be happy. It is impossible. But you can be at peace. There may be sadness and tears, but provided that you have relinquished resistance, underneath the sadness you will feel a deep serenity, a stillness, a sacred presence. This is the emanation of Being, this is inner peace, the good that has no opposite.”\n\n\n“Accept whatever comes to you woven in the pattern of your destiny, for what could more aptly fit your needs?”\n\n\n“You cannot have an argument with a fully conscious person.”\n\n\n“No one who is at one with himself can even conceive of conflict,”\n\n\n“Growth is usually considered positive, but nothing can grow forever.”\n\n\n“The cyclical nature of the universe is closely linked with the impermanence of all things and situations.”\n\n\n“The happiness that is derived from some secondary source is never very deep.”\n\n\n“Have you ever seen an unhappy flower or a stressed oak tree? Have you come across a depressed dolphin, a frog that has a problem with self-esteem, a cat that cannot relax, or a bird that carries hatred and resentment? The only animals that may occasionally experience something akin to negativity or show signs of neurotic behavior are those that live in close contact with humans and so link into the human mind and its insanity.”\n\n\n“When you have reached a certain degree of presence, you don’t need negativity anymore to tell you what is needed in your life situation. But as long as negativity is there, use it. Use it as a kind of signal that reminds you to be more present.”\n\n\n“Even the slightest irritation is significant and needs to be acknowledged and looked at; otherwise, there will be a cumulative build-up of unobserved reactions.”\n\n\n“As an alternative to dropping a negative reaction, you can make it disappear by imagining yourself becoming transparent to the external cause of the reaction.”\n\n\n“When you accept what is, every piece of meat every moment is the best. That is enlightenment.”\n\n\n“You don’t resist change by mentally clinging to any situation. Your inner peace does not depend on it. You abide in Being unchanging, timeless, deathless and you are no longer dependent for fulfillment or happiness on the outer world of constantly fluctuating forms. You can enjoy them, play with them, create new forms, appreciate the beauty of it all. But there will be no need to attach yourself to any of it.”\n\n\n“Compassion is the awareness of a deep bond between yourself and all creatures.”\n\n\n“Surrender is a purely inner phenomenon. It does not mean that on the outer level you cannot take action and change the situation.”\n\n\n”¤ If you find your life situation unsatisfactory or even intolerable, it is only by surrendering first that you can break the unconscious resistance pattern”\n\n\n“If you find your life situation unsatisfactory or even intolerable, it is only by surrendering first that you can break the unconscious resistance pattern that perpetuates that situation.”\n\n\n“Surrender is perfectly compatible with taking action, initiating change or achieving goals. But in the surrendered state a totally different energy, a different quality, flows into your doing.”\n\n\n“Look at the lilies, how they grow; they neither toil nor spin.”\n\n\n“Any action you take may not bear fruit immediately. Until it does do not resist what is. If there is no action you can take, and you cannot remove yourself from the situation either, then use the situation to make you go more deeply into surrender, more deeply into the Now, more deeply into Being. When you enter this timeless dimension of the present, change often comes about in strange ways without the need for a great deal of doing on your part. Life becomes helpful and cooperative.”\n\n\n“In Taoism, there is a term called wu wei, which is usually translated as “actionless activity” or “sitting quietly doing nothing.” In ancient China, this was regarded as one of the highest achievements or virtues. It is radically different from inactivity in the ordinary state of consciousness, or rather unconsciousness, which stems from fear, inertia, or indecision. The real “doing nothing” implies inner nonresistance and intense alertness. On the other hand, if action is required, you will no longer react from your conditioned mind, but you will respond to the situation out of your conscious presence. In that state, your mind is free of concepts, including the concept of nonviolence. So who can predict what you will do?”\n\n\n“Withdraw time from the illness. Do not give it any past or future. Let it force you into intense present-moment awareness and see what happens.”\n\n\n“The condition that is labeled “illness” has nothing to do with who you truly are.”\n\n\n“If you cannot accept what is outside, then accept what is inside. If you cannot accept the external condition, accept the internal condition. This means: Do not resist the pain. Allow it to be there. Surrender to the grief, despair, fear, loneliness, or whatever form the suffering takes. Witness it without labeling it mentally. Embrace it. Then see how the miracle of surrender transmutes deep suffering into deep peace.”\n\n\n“Suffering does not diminish in intensity when you make it unconscious.”\n\n\n“The mind, conditioned as it is by the past, always seeks to re- create what it knows and is familiar with. Even if it is painful, at least it is familiar. The mind always adheres to the known. The unknown is dangerous because it has no control over it. Thats why the mind dislikes and ignores the present moment. Present-moment awareness creates a gap not only in the stream of mind but also in the past-future continuum. Nothing truly new and creative can come into this world except through that gap, that clear space of infinite possibility.”\n\n\n“What comes to mind when we ask “Who am I?” consists of those things we have been paying attention to over the years. The same goes for our impressions of other people. The reality that appears to us is not so much what’s out there as it is those aspects of the world we have focused on.”\n\n\n“Before we can develop attentional stability, we first need to learn to relax.”\n\n\n“We are all aware of the way the body heals itself. Physicians don’t heal abrasions, and surgeons don’t mend bone fractures. Instead, they do whatever they can to allow the body to heal itself—by keeping the wound clean, setting the broken bone, and so on. These are so common that it’s easy to lose sight of the extraordinary nature of the body’s own healing power.”\n\n\n“When a stream is polluted, one may try to add antidotes to the toxins in the water, hoping such additives will neutralize the damage. But the more straightforward and sensible approach is simply to stop the flow of contamination into the stream. When this is done, over time the flow of the water through soil, stones, and vegetation can purify the stream completely.”\n\n\n“Genuine happiness is a symptom of a balanced, healthy mind, just as a sense of physical well-being is a sign of a healthy body.”\n\n\n“We do not exist independently from others, so our well-being cannot arise independently of others either.”\n\n\n“The reason we don’t devote more time to balancing our minds is that we are betting our lives that we can find the happiness we seek by chasing fleeting pleasures.”\n\n\n“However busy we may be, or think we are, no one is paying us enough to have demands on our minds every single moment of the day.”\n\n\n“In the seen there is only the seen; in the heard, there is only the heard; in the sensed, there is only the sensed; in the mentally perceived, there is only the mentally perceived.”\n\n\n“to divide your attention, consider your priorities. If something’s worth doing, it’s worth doing well, and if something’s not worth doing, it’s not worth doing at all.”\n\n\n“When you start to experience the inner calm, simplicity, and quietude of shamatha practice, you may become attached to this state of mind, and that can result in apathetic indifference to those around you and the world at large. You’ve got your own quiet space of serenity, and you may not want to be disturbed. The worthy venture of meditative training becomes derailed when it results in such complacency; it can become little more than a substitute for Prozac or Valium. The real aim of this practice is to cultivate mental balance that results in genuine happiness, and indifference to others is not a sign of genuine happiness or mental health.”\n\n\n“The quality of our lives reflects the ways we have cultivated our minds until now.”\n\n\n“Virtually anything may catalyze unhappiness, but its true source is always in the mind.”\n\n\n“Solitary meditation doesn’t cause mental imbalances, but uncovers them. Boredom may set in, especially when the mind succumbs to laxity, and restlessness often comes in the wake of excitation. With perseverance you can move beyond these imbalances and begin to discover the well-being that arises from a balanced mind. But this requires courage to face your own inner demons and persist in the practice despite the emotional upheavals that are bound to occur in the course of this training.”\n\n\n“those who have accustomed themselves to having few desires and contentment can find joy in solitude, whereas those who have not found such equilibrium are bound either to sink into laxity and depression or to float up into excitation and restlessness.”\n\n\n“What you make doesn’t have to be witnessed, recorded, sold, or encased in glass for it to be a work of art.”\n\n\n“by the mere fact of being alive, we are active participants in the ongoing process of creation.”\n\n\n“Attuned choice by attuned choice, your entire life is a form of self-expression. You exist as a creative being in a creative universe. A singular work of art.”\n\n\n“The taste and beauty are in the eye of the beholder.”\n\n\n“If you have an idea you’re excited about and you don’t bring it to life, it’s not uncommon for the idea to find its voice through another maker. This isn’t because the other artist stole your idea, but because the idea’s time has come.”\n\n\n“The best artists tend to be the ones with the most sensitive antennae to draw in the energy resonating at a particular moment. Many great artists first develop sensitive antennae not to create art but to protect themselves. They have to protect themselves because everything hurts more. They feel everything more deeply.”\n\n\n“Artists who are able to continually create great works throughout their lives often manage to preserve these childlike qualities. Practicing a way of being that allows you to see the world through uncorrupted, innocent eyes can free you to act in concert with the universe’s timetable.”\n\n\n“Clouds never truly disappear. They change form. They turn into rain and become part of the ocean, and then evaporate and return to being clouds. The same is true of art.”\n\n\n“As soon as you label an aspect of Source, you’re no longer noticing, you’re studying.”\n\n\n“Analysis is a secondary function. The awareness happens first as a pure connection with the object of your attention. If something strikes me as interesting or beautiful, first I live that experience. Only afterward might I attempt to understand it.”\n\n\n“The universe is only as large as our perception of it.”\n\n\n“doesn’t fit easily within the limits of our belief system. The more raw data we can take in, and the”\n\n\n“The more raw data we can take in, and the less we shape it, the closer we get to nature.”\n\n\n“One can think of the creative act as taking the sum of our vessel’s contents as potential material, selecting for elements that seem useful or significant in the moment, and re-presenting them. This is Source drawn through us and made into books, movies, buildings, paintings, meals, businesses—whatever projects we embark on. If we choose to share what we make, our work can recirculate and become source material for others.”\n\n\n“What we create allows us to share glimpses of an inner landscape, one that is beyond our understanding. Art is our portal to the unseen world.”\n\n\n“The world of reason can be narrow and filled with dead ends, while a spiritual viewpoint is limitless and invites fantastic possibilities. The unseen world is boundless.”\n\n\n“The practice of spirituality is a way of looking at a world where you’re not alone. There are deeper meanings behind the surface. The energy around you can be harnessed to elevate your work. You are part of something much larger than can be explained—a world of immense possibilities.”\n\n\n“If a piece of work, a fragment of consciousness, or an element of nature is somehow allowing us to access something bigger, that is its spiritual component made manifest. It awards us a glimpse of the unseen.”\n\n\n“If we aren’t looking for clues, they’ll pass by without us ever knowing. Notice connections and consider where they lead.”\n\n\n“the universe is nudging you with little reminders that it’s on your side and wants to provide everything you need to complete your mission.”\n\n\n“Look for what you notice but no one else sees.”\n\n\n“Awareness needs constant refreshing. If it becomes a habit, even a good habit, it will need to be reinvented again and again. Until one day, you notice that you are always in the practice of awareness, at all times, in all places, living your life in a state of constant openness to receiving.”\n\n\n“Living life as an artist is a practice. You are either engaging in the practice or you’re not.”\n\n\n“The real work of the artist is a way of being in the world.”\n\n\n“Because there’s an endless amount of data available to us and we have a limited bandwidth to conserve, we might consider carefully curating the quality of what we allow in.”\n\n\n“Of all the great works that we can experience, nature is the most absolute and enduring. We can witness it change through the seasons. We”\n\n\n“We don’t have to understand nature to appreciate it. This is true of all things. Simply be aware of moments when your breath”\n\n\n“We don’t have to understand nature to appreciate it. This is true of all things. Simply be aware of moments when your breath gets taken away by something of great beauty.”\n\n\n“Nature transcends our tendencies to label and classify, to reduce and limit.”\n\n\n“It is said the ocean provides a closer reflection of who we are than any mirror.”\n\n\n“Even if an element seems static, whether a work of art in a museum or an everyday object in a kitchen, when we look at it deeply, we can see a newness. We recognize aspects unnoticed before. Reread the same book over and over, and we’ll likely find new themes, undercurrents, details, and connections.”\n\n\n“You can’t step into the same stream”\n\n\n“Our inner world is every bit as interesting, beautiful, and surprising as nature itself. It is, after all, born of nature.”\n\n\n“Ultimately, it doesn’t make a difference whether your content originates on the inside or the outside. If a beautiful thought or phrase comes to mind, or if you see a beautiful sunset, one’s not better than the other. Both are equally beautiful in different ways. It’s helpful to consider there are always more options available to us than we might realize.”\n\n\n“It isn’t uncommon, out of the gibberish, for a story to unfold or key phrases to appear.”\n\n\n“sleep. Memories can also be thought of as dreamlike. They’re more a romantic”\n\n\n“Memories can also be thought of as dreamlike. They’re more a romantic story than a faithful document of a life event.”\n\n\n“Tomorrow presents another opportunity for awareness, but it’s never an opportunity for the same awareness.”\n\n\n“It helps to realize that it’s better to follow the universe than those around you.”\n\n\n“Self-doubt lives in all of us. And while we may wish it gone, it is there to serve”\n\n\n“Flaws are human, and the attraction of art is the humanity held in it. If we were machinelike, the art wouldn’t resonate. It would be soulless. With life comes pain, insecurity, and fear.”\n\n\n“If a creator is so afraid of judgment that they’re unable to move forward, it might be that the desire to share the work isn’t as strong as the desire to protect themselves. Perhaps art isn’t their role. Their temperament might serve a different pursuit. This path is not for everyone. Adversity is part of the process.”\n\n\n“One of the reasons so many great artists die of overdoses early in their lives is because they’re using drugs to numb a very painful existence. The reason it’s painful is the reason they became artists in the first place: their incredible sensitivity.”\n\n\n“If you see tremendous beauty or tremendous pain where other people see little or nothing at all, you’re confronted with big feelings all the time. These emotions can be confusing and overwhelming. When those around you don’t see what you see and feel what you feel, this can lead to a sense of isolation and a general feeling of not belonging, of otherness.”\n\n\n“All art is a work in progress.”\n\n\n“some things are too important to be taken seriously.”\n\n\n“the Buddhist concept of papancha, which translates as preponderance of thoughts. This speaks to the mind’s tendency to respond to our experiences with an avalanche of mental chatter.”\n\n\n“The imperfections you’re tempted to fix might prove to be what make the work great. And sometimes not.”\n\n\n“Distraction is one of the best tools available to the artist when used skillfully. In some cases, it’s the only way to get where we are going.”\n\n\n“Nothing begins with us. The more we pay attention, the more we begin to realize that all the work we ever do is a collaboration.”\n\n\n“The inspired-artist aspect of your self may be in conflict with the craftsperson aspect, disappointed that the craftsperson is unable to create the physical embodiment of the inspired artist’s vision.”\n\n\n“A painting is just a painting until you put a frame on it and hang it on the wall, then it’s called art. What’s considered art is simply an agreement. And none of it is true.”\n\n\n“A completed project is only made up of our intention and our experiments around it. Remove intention and all that’s left is the ornamental shell.”\n\n\n“Not all projects take time, but they do take a lifetime.”\n\n\n“Most creators think of themselves as the conductor of the orchestra. If we zoom out of our small view of reality, we function more as an instrumentalist in a much larger symphony the universe is orchestrating. We may not have a great understanding of what this magnum opus is because we only see the small part we play.”\n\n\n“Similarly, the total output of human creativity, in all its kaleidoscopic breadth, pieces together the fabric forming our culture. The underlying intention of our work is the aspect allowing it to fit neatly into this fabric. Rarely if ever do we know the grand intention, yet if we surrender to the creative impulse, our singular piece of the puzzle takes its proper shape.”\n\n\n“Intention is all there is. The work is just a reminder.”\n\n\n“Rules, by their nature, are limitations.”\n\n\n“Rules direct us to average behaviors. If we’re aiming to create works that are exceptional, most rules don’t apply. Average is nothing to aspire to. The goal is not to fit in. If anything, it’s to amplify the differences, what doesn’t fit, the special characteristics unique to how you see the world. Instead of sounding like others, value your own voice. Develop it. Cherish it.”\n\n\n“The reason to make art is to innovate and self-express, show something new, share what’s inside, and communicate your singular perspective.”\n\n\n“In the beginning, we approach our craft with a template of what’s come before.”\n\n\n“As soon as you use a label to describe what you’re working on, there’s a temptation to conform to its rules.”\n\n\n“Often, the most innovative ideas come from those who master the rules to such a degree that they can see past them or from those who never learned them at all.”\n\n\n“Rules obeyed unconsciously are far stronger than the ones set on purpose.”\n\n\n“Holding every rule as breakable is a healthy way to live as an artist. It loosens constraints that promote a predictable sameness in our working methods.”\n\n\n“Once you have a new framework, some elements of your older process may find their way back into the work, and that’s okay.”\n\n\n“For any rules you accept of what you can and cannot do as an artist … of what your voice is and isn’t … of what’s required to do the work and what you don’t need … it would be worthwhile to try the opposite.”\n\n\n“Think of a rule as an imbalance.”\n\n\n“Though to say that we listen with the ears, or the mind, might be a misconception. We listen with the whole body, our whole self.”\n\n\n“Many of us experience life as if we’re taking it in through a pair of headphones. We strip away the full register. We hear information, but don’t detect the subtler vibrations of feeling in the body.”\n\n\n“If it’s music you’re listening to, consider closing your eyes. You may find yourself getting lost in the experience. When the piece ends, you might be surprised by where you find yourself. You’ve been transported to another place. The place where the music lives.”\n\n\n“Formulating an opinion is not listening. Neither is preparing a response, or defending our position or attacking another’s. To listen impatiently is to hear nothing at all.”\n\n\n“Listening is suspending disbelief.”\n\n\n“More often than not, there are no right answers, just different perspectives.”\n\n\n“Many of our beliefs were learned before we had a choice in what we were taught. Some of them might go back generations and may no longer apply. Some may never have applied.”\n\n\n“The lottery winner isn’t ultimately happy after their sudden change of fortune. The home built hastily rarely survives the first storm. The single-sentence summary of a book or news event is no substitute for the full story.”\n\n\n“Re-reading even a well-understood paragraph or page can be revelatory.”\n\n\n“Our continual quest for efficiency discourages looking too deeply.”\n\n\n“Impatience is an argument with reality.”\n\n\n“When it comes to the creative process, patience is accepting that the majority of the work we do is out of our control. We can’t force greatness to happen. All we can do is invite it in and await it actively. Not anxiously, as this might scare it off. Simply in a state of continual welcoming.”\n\n\n“If we remove time from the equation of a work’s development, what we’re left with is patience.”\n\n\n“Even the masterpieces that have been produced on tight timelines are the sum of decades spent patiently laboring on other works.”\n\n\n“What was it that allowed a machine to devise a move no one steeped in the game had ever made in thousands of years of play? It wasn’t necessarily its intelligence. It was the fact that the machine learned the game from scratch, with no coach, no human intervention, no lessons based on an expert’s past experience. The AI followed the fixed rules, not the millennia of accepted cultural norms attached to them. It didn’t take into account the three-thousand-year-old traditions and conventions of Go. It didn’t accept the narrative of how to properly play this game. It wasn’t held back by limiting beliefs.”\n\n\n“One Go expert commented, “After humanity spent thousands of years improving our tactics, computers tell us that humans are completely wrong … I would go as far as to say not a single human has touched the edge of the truth of Go.”\n\n\n“To see what no human has seen before, to know what no human has known before, to create as no human has created before, it may be necessary to see as if through eyes that have never seen, know through a mind that has never thought, create with hands that have never been trained.”\n\n\n“Did the computer win because it knew more than the grandmaster or because it knew less?”\n\n\n“Experience provides wisdom to draw from, but it tempers the power of naivete.”\n\n\n“The more ingrained your adopted approach, the harder it is to see past it.”\n\n\n“Just as an infant is selfish, they’re protective of their art in a way that’s not always cooperative.”\n\n\n“A child has no set of premises it relies on to make sense of the world. It may serve you to do the same. Any label you assume before sitting down to create, even one as foundational as sculptor, rapper, author, or entrepreneur, could be doing more harm than good. Strip away the labels. Now how do you see the world?”\n\n\n“If you spent your whole life living near the ocean, your experience of it would almost certainly be less dramatic.”\n\n\n“As artists, we aim to live in a way in which we see the extraordinary hidden in the seemingly mundane. Then challenge ourselves to share what we see in a way that allows others a glimpse of this remarkable beauty.”\n\n\n“For the lungs to draw in air, they must first be emptied. For the mind to draw inspiration, it wants space to welcome the new. The universe seeks balance. Through this absence, you are inviting energy in.”\n\n\n“To vary your inspiration, consider varying your inputs. Turn the sound off to watch a film, listen to the same song on repeat, read only the first word of each sentence in a short story, arrange stones by size or color, learn to lucid dream.”\n\n\n“The work yielded may not be used in the current project, but it may be of use another time. Or it may not. The task of the artist is simply to recognize the transmission and stay with it in gratitude, until it truly runs its course.”\n\n\n“In terms of priority, inspiration comes first. You come next. The audience comes last.”\n\n\n“A full, imperfect version is generally more helpful than a seemingly perfect fragment.”\n\n\n“Wooden often said the only person you’re ever competing against is yourself. The rest is out of your control.”\n\n\n“If you set a routine that is oppressive, you’ll likely find excuses not to show up. It’s in the interest of your art to create an easily achievable schedule to start with.”\n\n\n“As if catching fish, we walk to the water, bait the hook, cast the line, and patiently wait. We cannot control the fish, only the presence of our line.”\n\n\n“The work reveals itself as you go.”\n\n\n“Not every seed must grow. But it may be there is a right time for each one. If a seed does not seem to be developing or responding, consider storing it rather than discarding it. In nature, some seeds lie dormant in anticipation of the season most conducive to their growth. This is true of art as well. There are ideas whose time has not yet come. Or perhaps their time has come, but you are not yet ready to engage with them. Other times, developing a different seed may shed light on a dormant one.”\n\n\n“As we lose enthusiasm, we often continue to labor on a seed, believing that the work has to turn out for the better because we’ve invested so much time in it. If the energy continues to drop, it does not necessarily mean that the seed is bad. We just may not have found the right experiment for it. Perhaps we need to step away for a time and shift perspective.”\n\n\n“Excitement tends to be the best barometer for selecting which seeds to focus on. When something interesting starts to come together, it arouses delight. It’s an energizing feeling of wanting more. A feeling of leaning forward. Follow that energy.”\n\n\n“There is a gap between imagination and reality. An idea might seem brilliant in our mind. But once employed, it might not work at all. Another might seem dreary at first. Then, upon execution, it might be exactly what’s called for. To dismiss an idea because it”\n\n\n“Descriptions do not do ideas justice.”\n\n\n“Art may only exist, and the artist may only evolve, by completing the work.”\n\n\n“While crafting, make deadlines for your own motivation, not necessarily to be shared with others unless it helps with accountability. Once the Craft phase is nearing an end, then we might start thinking in terms of fixed deadlines.”\n\n\n“Art is a reflection of the artist’s inner and outer world during the period of creation. Extending the period complicates the artist’s ability to capture a state of being. The result can be a loss of connection and enthusiasm for the work over time.”\n\n\n“To avoid demo-itis, there is a simple technique. Unless actively working to make something better, avoid listening to it, reading it, playing it, looking at it, or showing it to friends. Work as far forward as you can while crafting and then step away, without repetitively consuming the unfinished work. By not accepting the work-in-progress as the standard version, we leave room for growth, change, and development to continue.”\n\n\n“We mistake the fantasy version of the work in our minds for what the actual work has the possibility to become. There may indeed be times when our mental conception of a piece translates almost directly into the physical realm. At other times, it’s an unrealistic idealized version. And sometimes, our vision for the work is a goal to work toward, and in the process we come to learn we’ll reach a new and unexpected destination.”\n\n\n“Falling short of grander visions might actually put the work exactly where it wants to be.”\n\n\n“Artists allow us to see what we are unable to see, but somehow already know.”\n\n\n“The reason we create art isn’t with the intention of making something useful for someone else. We create to express who we are. Who we are and where we are on our journey.”\n\n\n“A point of view is different from having a point.”\n\n\n“It’s impossible to imitate another artist’s point of view. We can only swim in the same waters. So feel free to copy the works that inspire you on the road to finding your own voice. It’s a time-tested tradition.”\n\n\n“Whatever the situation, if a task is challenging to accomplish, there’s often a way to design the surroundings to naturally encourage the performance you’re striving for.”\n\n\n“We interrogate ourselves when we offer our work up to others.”\n\n\n“If someone chooses to share feedback, listen to understand the person, not the work. People will tell you more about themselves than about the art when giving feedback. We each see a unique world.”\n\n\n“Art doesn’t get made on the clock. But it can get finished on the clock.”\n\n\n“When the last chapter is about to end, we may create excuses to put off the completion of the work.”\n\n\n“Hanging on to your work is like spending years writing the same entry in a diary. Moments and opportunities are lost. The next works are robbed of being brought to life.”\n\n\n“In an environment where nothing is permanent, we produce static artifacts. Mementos of spirit. We hope they’ll live forever, holding resonance through each passing decade. Some might, many won’t. It’s impossible to know. We can only keep building.”\n\n\n“If the mind creates a world that is limited, where we think we don’t have enough worthwhile ideas or material, we will not see the inspiration the universe is providing.”\n\n\n“Finishers might benefit from taking more time in the early phases. Writing beyond the minimum requirement, experimenting with other materials, considerations, and perspectives. Allowing themselves space for improvisation and surprise in the process. Experimenters might benefit from taking an aspect of the work through to completion. It might be a drawing, a song,or the chapter of a book. Even making one foundational decision from which to build can help.”\n\n\n“As artists, we get to create a new set of rules each and every time we play. After careful consideration, we may choose to break them in the midst of a project if a discovery impels us.”\n\n\n“We create our art so we may inhabit it ourselves.”\n\n\n“And any story beyond “I want to make the best thing I can make, whatever it is” are all undermining forces in the quest for greatness.”\n\n\n“It’s not uncommon to long for outward success, hopeful it will fill a void inside ourselves. Some imagine achievement as a remedy to fix or heal a sense of not being enough.”\n\n\n“If you are living in the belief that success will cure your pain, when the treatment comes and doesn’t work, it can lead to hopelessness. A depression can accompany the realization that what you’ve spent most of your life chasing hasn’t fixed your insecurities and vulnerabilities. More likely, with the stakes and consequences now higher, it has only amplified the pressure. And we are never taught how to handle this epic disappointment.”\n\n\n“Art has the power to snap us out of our transfixion, open our minds to what’s possible, and reconnect with the eternal energy that moves through all things.”\n\n\n“The ecstatic is our compass, pointing to our true north. It arises genuinely in the process of creation. You’re working and struggling, and suddenly you notice a shift. A revelation. A small tweak is made, a new angle is revealed, and it takes your breath away.”\n\n\n“So little was needed to make the leap from mediocrity to greatness. The leap can’t always be understood, but when it happens, it’s clear and enlivening.”\n\n\n“Many artists come to realize long after their work is released that it was actually a shockingly vulnerable and cryptic form of public confession.”\n\n\n“When we don’t have context, new ideas appear foreign or awkward.”\n\n\n“Be aware of strong responses. If you’re immediately turned off by an experience, it’s worth examining why. Powerful reactions often indicate deeper wells of meaning. And perhaps by exploring them, you’ll be led to the next step on your creative path.”\n\n\n“Art is about the maker. Its aim: to be an expression of who we are. This makes competition absurd.”\n\n\n“comparison is the thief of joy.”\n\n\n“No system exists that can rank which work is most reflective of the maker. Great art is an invitation, calling to creators everywhere to strive for still higher and deeper levels.”\n\n\n“Perfection is finally obtained not when there is no longer anything to add, but when there’s no longer anything to take away.”\n\n\n“From a distance, what can we know to be true?”\n\n\n“We live in a mysterious world full of uncertainties. And we regularly make assumptions to explain them. Coming to terms with the complexity of our human experience allows us to exit our natural state of confusion. To survive. Generally our explanations are guesses. These vague hypotheticals become fixed in our minds as fact. We are interpretation machines, and this process of labeling and detaching is efficient but not accurate. We are the unreliable narrators of our own experience.”\n\n\n“Each artist works with their own balance of strengths and weaknesses. And there is no rule that more praiseworthy strengths or romanticized self-destruction equals better art. Expressing yourself is all that matters.”\n\n\n“We will never know a work’s true meaning. It’s helpful to remember that there are forces at work beyond our comprehension. Let’s make art, and let others make the stories.”\n\n\n“If any distractions come along during that period, don’t ignore them or focus on them. Don’t give them any energy at all. Let them pass, like clouds parting around a mountain.”\n\n\n“As artists, we are on a continual quest to get closer to the universe by getting closer to self. Moving ever nearer to the point where we can no longer tell where one begins and the other ends. We’re on a distant metaphysical journey from the here to the now. It’s helpful to work as if the project you’re engaged in is bigger than you.”\n\n\n“When you acknowledge a weakness, always consider how it could either be removed or improved before discarding the entire piece.”\n\n\n“Many of us are taught to create through sheer will. If we choose surrender, the ideas that want to come through us will not be blocked.”\n\n\n“For the artist, whose job is testing possibilities, success is as much ruling out a solution as finding one that works.”\n\n\n“In the process of experimentation, we allow ourselves to make mistakes, to go too far, to go even further, to be inept. There is no failure, as every step we take is necessary to reach our destination, including the missteps. Each experiment is valuable in its own way if we learn something from it. Even if we can’t comprehend its worth, we are still practicing our craft, moving ever so much closer to mastery.”\n\n\n“Humanity breathes in mistakes.”\n\n\n“Making great art may not always require great effort, but without it, you’ll never know.”\n\n\n“Creativity is something you are, not only something you do.”\n\n\n“Quality isn’t based on the amount of time invested.”\n\n\n“If you sit down to write with no preparation or forethought, you might bypass the conscious mind and draw from the unconscious. You may find that what emerges holds a charge that cannot be duplicated through rational means.”\n\n\n“If we were to learn anything, it would be to free ourselves from any beliefs or baggage or dogma that gets in the way of us acting according to our true nature. The closer we get to a childlike state of free self-expression, the purer our test and the better our art.”\n\n\n“Once a work is complete, no amount of testing can guarantee we’ve made the best possible version. These qualities are not measurable. We test to identify which is the best version from the options at hand.”\n\n\n“When the work has five mistakes, it’s not yet completed. When it has eight mistakes, it might be.”\n\n\n“If we like what we are creating, we don’t have to know why. Sometimes the reasons are obvious, sometimes not. And they can change over time. It could be good for any of a thousand different reasons. When we’re making things we love, our mission is accomplished. There’s nothing at all to figure out.”\n\n\n“Holding your work hostage to meaning is a limitation.”\n\n\n“is far more powerful than our plans”\n\n\n“Art is far more powerful than our plans for it.”\n\n\n“Art is above and beyond judgment. It either speaks to you or it doesn’t. The artist’s only responsibility is to the work itself. There are no other requirements. You’re free to create what you will.”\n\n\n“Whether you have a powerful passion or a tortured compulsion, neither makes the art any better or worse. If you are able to choose between these paths, consider selecting the more sustainable one. An artist earns the toc: true\ntitle simply through self-expression, as they work in their own way at their own pace.”\n\n\n“Established artists generally draw from their personal experience and recommend the solutions that worked for them. These tend to be specific to their journey, not yours. It’s worth remembering that their way is not the way.”\n\n\n“The passive element of practice is as important as the active one.”\n\n\n“I’m both a professor and student, because if you’re no longer a student, you don’t have the right to call yourself a professor.”\n\n\n“If you feel unable to hit a note or faithfully paint an image, it’s helpful to remember that the challenge is not that you can’t do it, but that you haven’t done it yet. Avoid thinking in impossibilities.”\n\n\n“Having the knowledge won’t hurt the work. How you use the knowledge may. You have new tools. You don’t have to use them.”\n\n\n“If we train ourselves to step away from the work, to truly detach from it, to distract ourselves completely, to dive fully into something else … After being away for a long enough period of time, when we come back, we just may be able to see it as if for the first time.”\n\n\n“A way to practice keeping a clean slate is to avoid looking at the work too often. If you finish a section or come to a sticking point, consider putting the project away and not engaging with it for a period of time. Let it sit for a minute, a week, or longer, while you go get lost.”\n\n\n“The context changes the content.”\n\n\n“The social norms of any time and place are another contextual box that art lives in.”\n\n\n“When a piece isn’t living up to your expectations, consider changing the context. Look past the principle element, examine the variables around it. Play with different combinations. Place it next to other works. Surprise yourself.”\n\n\n“If the work is thrilling one day and isn’t for a long while after, you may have experienced a false indicator. When the moments of joy seem like a distant memory and the work feels like an obligation to a past idea, this could mean you’ve either gone too far or that particular seed wasn’t actually ready to germinate yet.”\n\n\n“Every artist creates a dynamic history. A living museum of finished objects. One work after another. Begun, completed, released. Begun, completed, released. Over and over again. Each a time stamp commemorating a moment of passage. A moment filled with energy, now forever embodied in a work of art.”\n\n\n“Within every artist, there’s a child emptying a box of crayons onto the floor, searching for just the right color to draw the sky.”\n\n\n“If you’re looking for the work to support you, you may be asking too much of it. We create in service to art, not for what we can get from art.”\n\n\n“Creativity is contagious.”\n\n\n“If asked to participate in a fellow creator’s project, proceed delicately.”\n\n\n“Sometimes the most valuable touch a collaborator can have is no touch at all.”\n\n\n“Believing an idea is best because it’s ours is an error of inexperience. The ego demands personal authorship, inflating itself at the expense of the art. It can reject new methods that appear counterintuitive and protect familiar ones.”\n\n\n“Great decisions aren’t made in a spirit of sacrifice. They’re made by the mutual recognition of the best solution available.”\n\n\n“The more clinical the feedback, the better it will be received.”\n\n\n“Our ego can perceive assistance as interference.”\n\n\n“It helps to keep in mind that language is an imperfect means of communication.”\n\n\n“We like to think of ourselves as consistent, rational beings, possessing certain attributes and not others. Yet a person who is completely consistent, who possesses no contradictions, comes across as less real. Wooden. Plastic.”\n\n\n“Art goes deeper than thought. Deeper than the stories about yourself. It breaks through inner walls and accesses what’s behind. If we get out of the way and let the art do its work, it may yield the sincerity we seek. And sincerity may look nothing like we expected.”\n\n\n“The editor’s role is to gather and sift. Amplifying what’s vital and whittling away the excess. Culling the work down to the best version of itself.”\n\n\n“The editor is required to set ego aside. Ego pridefully attaches to individual elements of a work. The editor’s role is to remain unattached and see beyond these passions to find unity and balance. Talented artists who are unskilled editors can”\n\n\n“The editor is required to set ego aside. Ego pridefully attaches to individual elements of a work. The editor’s role is to remain unattached and see beyond these passions to find unity and balance. Talented artists who are unskilled editors can do subpar work and fail to live up to their gift’s promise.”\n\n\n“As we move closer to the completion of a project, it can be helpful to drastically cut the work back to only what’s necessary, to conduct a ruthless edit.”\n\n\n“Making the simple complicated is commonplace,” Charles Mingus once said. “Making the complicated simple, awesomely simple, that’s creativity.”\n\n\n“Art is a reverberation of an impermanent life.”\n\n\n“Just as each small stroke on a canvas can’t step aside to see the whole painting, we’re unable to take in the great whole of relationships and counterbalance that surrounds us in all directions.”\n\n\n“The magic is not in the analyzing or the understanding. The magic lives in the wonder of what we do not know.”\n\n\n“The widespread use of chopsticks shaped the development of Chinese cuisine—the reason there’s so much preparatory chopping involved in making a Chinese meal is because dishes had to consist of bite-size pieces that could easily be grasped with chopsticks, since  nothing was sliced at the table.”\n\n\n“As she fingered the pamphlets, I realized it was new to her, this idea of vegetarianism not as a mark of poverty but a conscious lifestyle choice, and coming from my place of privilege, I hadn’t understood her wariness about it.”\n\n\n“The golden rule of beauraucracy, give me a problem, and Im willing to spend any amount of someone elses money to solve it”\n\n\n“the less money the higher the expectations”\n\n\n“Life feels more romantic when life is a performance when youre mimicking what that romantic scene is in the film that you saw.”\n\n\n“It’s harder to see than to paint.”\n\n\n“When you’re a people-pleaser, you unconsciously wear a facade of niceness that hides your true feelings from your family, colleagues, friends, lovers — essentially giving up your needs for the sake of everyone else’s. For years I thought that niceness was one of my best, most pure qualities. Only recently have I realized that this was how I tried to protect myself and, in fact, was an attempt to control what other people thought of me.”\n\n\n“People-pleasing had so fundamentally shaped my relationship to myself and was a deep layer that had kept me from living authentically for the vast majority of my life. I’d been playing a role instead of being a person.”\n\n\n“Do you dedicate more time to other people than yourself? Have you neglected self-care, because you’re just too busy taking care of others? If the answer is yes, then it’s time to re-evaluate your priorities.”\n\n\n“The more action you take, the more you want to take action”\n\n\n“When you add value to people’s lives, they are eager to share your story with everyone”\n\n\n“It seems evident that very few people can simply sit still. Children spin in circles until they collapse with dizziness.”\n\n\n“Pretend its like eating the last candy you will ever eat in your life”\n\n\n“ather than just accepting, what I learned is that you have to also appreciate what’s right in front of you including the simple things. It’s the small details that actually matter.”\n\n\n“Creative people do not have the gift of novelty, they have the gift of being able to turn large amounts of chaotic information into order”\n\n\n“Growing up we learn feeling words from adults who simultaneously shame us for using them.”\n\n\n“Young man, don’t be too rampant. The road of life is still very long”\n\n\n“Some would say. “All problems that could be solved with money are not problems.” Clearly those who say that are all rich people.”\n\n\n“The greatness of any artform cannot be in the technique, as important as technique is. Nobody loves a song because the singer hit all the notes correctly. They love it because the melodies and rhythms and interactions of the instruments give them a feeling. It’s the same with imagery. Good composers know how to create an image that haunts, or entertains, or somehow emotionally involves a viewer, and that happens on a primal level through compositional choices. And even though I believe it is a talent, it is realized through practice and training. As with music. Musicians and artists both compose. The good ones move our emotions”\n\n\n“A wound can only begin healing when it’s noticed and attended to.”\n\n\n“History only seems simple, unified and natural when we forget about all the voices that go unheard.”\n\n\n“People are strange when you’re a stranger\nFaces look ugly when you’re alone\nWomen seem wicked when you’re unwanted\nStreets are uneven when you’re down\nWhen you’re strange faces come out of the rain\nWhen you’re strange no one remembers your name”\n\n\n“Kant saw humanity as a race mature enough to leave home but not mature enough to know how to live well alone”\n\n\n“The world is but a stage.\nWhy cry, when you can laugh instead?\nFor laughter is humanity’s preserve.\nLaugh it all off, fret not,\nLet’s just enjoy the moment.”\n\n\n“Dread is anxiety on steroids,”\n\n\n“For us vertebrates, the core of the stress-response is built around the fact that your muscles are going to work like crazy.”\n\n\n“The stress response cycle needs to complete, and just eliminating the stressor isn’t enough to do that.”\n\n\n“This is the upside-down world we live in: in most situations in the modern, post-industrial West, the stress itself will kill you faster than the stressor will—unless you do something to complete the stress response cycle.”\n\n\n“Just don’t forget that these survival strategies do not deal with the stress itself. They postpone your body’s need to complete the cycle; they don’t replace it.”\n\n\n“If anxiety starts, it ends.” “It just ends?” “Yeah. If you let it, it just ends.”\n\n\n“Wellness is not a state of being, but a state of action.”\n\n\n“When something feels uncomfortable, you’re probably doing something that creates more and better progress than if it were easy.”\n\n\n“Or, as Douglas Adams’s character Dirk Gently puts it, “I rarely end up where I was intending to go, but often I end up somewhere that I needed to be”\n\n\n“Part of recovering from a loss is turning toward your grief with kindness and compassion, as well as completing the cycle of stress brought on by failure. But another part is recognizing failing’s unintended positive outcomes.”\n\n\n“Heck no. Sometimes you need to close the door on the world and allow yourself to feel comfortable and safe—as long as it’s not the only thing you’re doing. Think of it as a short-term survival strategy. You also need a plan and a sense of what value there is in the struggle.”\n\n\n“When you paint the dingiest wall in a room, it just makes the other walls look dingier.”\n\n\n“It’s normal for change to be difficult. Sometimes it gets worse before it gets better. Sometimes a solution to one problem creates another. Sometimes there’s not enough organization and positive attitude in the world to save a marriage. Sometimes—as Julie would eventually find—what it takes to save a marriage is saving yourself.”\n\n\n“We have been taught that letting go of a goal is the same as failing. We share stories of people overcoming the odds to achieve remarkable things in the face of great resistance, which is inspiring. But these stories too often imply that we are the controllers of our destinies—as if we control the amount of nuts and seeds in a particular patch of forest. If we “fail” to achieve a goal, it’s because there is something wrong with us. We didn’t fight hard enough. We didn’t “believe.”\n\n\n“But there is a deep, wide chasm between us and the realization of those possibilities. Our default action in the face of that chasm is to do whatever it takes to get to the other side, and keep on doing it, no matter what, until we get there. But then we get exhausted and we wonder if we can accomplish any of the things we hope for, without destroying ourselves in the process.”\n\n\n“That freedom comes when we have abundance enough and safety enough to let go of what is broken and reach for something new.”\n\n\n“You can chart the progress of women in America by the things Disney heroines sing about in their “I Want” songs.”\n\n\n“Meaning is the feeling that you “matter in some larger sense. Lives may be experienced as meaningful when they are felt to have significance beyond the trivial or momentary, to have purpose, or to have a coherence that transcends chaos.”\n\n\n“But rarely is meaning something that we find at the end of a long, hard journey. For most of us, meaning is what sustains us on the long, hard journey, no matter what we find at the end. Meaning is not found; it is made.”\n\n\n“When an airplane bounces into a sudden pocket of turbulence, you grab the arms of your seat, as if by holding your seat, you can hold the plane steady. You, of course, know it doesn’t work that way, but your hands don’t. They will hold on to anything they can reach, and the very fact of holding on makes the turbulence more tolerable.”\n\n\n“We are not our own worst enemy. Nor is the enemy the other people in the game. The enemy is the game itself, which tries to convince us that it’s not the enemy.”\n\n\n“Self-care” is, indeed, selfish because it uses personal resources to promote a giver’s well-being, rather than someone else’s.”\n\n\n“In so many ways, most of us tend to ignore or forget about advantages we’ve received, but remember the obstacles we’ve overcome, because the struggle against the obstacles requires more effort and energy than the easy parts.”\n\n\n“Just because the road looks flat doesn’t mean it is. Just because you can’t see the ocean doesn’t mean it’s not there. You can infer the landscape by looking at the shapes of the people who grew in those environments. Instead of wondering why they aren’t thriving on the level playing field, imagine how the field can be changed to allow everyone to thrive.”\n\n\n“The truth will set you free, but first it will piss you off.”\n\n\n“All your body requires of you is that you turn toward it with kindness and compassion, with nonjudgment and plain-vanilla acceptance of all your contradictory emotions, beliefs, and longings.”\n\n\n“Everyone is the new hotness. You are the new hotness. So is she. So are they. So are we.”\n\n\n“Many of us have grown into world-class ignorers of our own needs, just as we were taught to be. We don’t even notice that we’re ignoring our needs. Our bodies are sending us all kinds of signals, but we live from the neck up, only attending to the noise in our heads and shutting out the noise coming from the other 95 percent of our internal experience.”\n\n\n“Contact with another person is a basic biological need; loneliness is a form of starvation.”\n\n\n“We’re made of energy. The nature of energy is to be shared, to spread, to connect one thing to another. Sharing space with other people means that our energy influences theirs, and theirs influences ours. It’s physics. And psychology. And unavoidable. And amazing.”\n\n\n“people tend to take better care of themselves when they’re in a high-quality relationship. In other words, our “self-care” is facilitated by the ways we care for and are cared for by someone else.”\n\n\n“Rage gives you strength and energy and the urge to fight, and sharing that energy in the Bubble changes it from something potentially dangerous to something safe and potentially transformative.”\n\n\n“The pleasure of synchronized movement is built into our biology, and it’s a powerful tool to access your greatest well-being.”\n\n\n“When it’s inconvenient, it’s probably doing the most for you.”\n\n\n“What makes you stronger is whatever happens to you after you survive the thing that didn’t kill you. What makes you stronger is rest.”\n\n\n“You don’t have to set yourself on fire to keep other people warm.”\n\n\n“Caring for myself is not self-indulgence, it is self-preservation, and that is an act of political warfare.”\n\n\n“Mental rest is not idleness; it is the time necessary for your brain to process the world.”\n\n\n“Boredom is the discomfort you experience when your brain is in active-attention mode, but can’t latch on to anything to attend”\n\n\n“Everybody knows a muscle that isn’t used will atrophy. We all know a muscle that is worked constantly, without rest, will grow fatigued and eventually fail in exhaustion. And we all know a muscle that gets worked and rested and worked and rested will grow stronger.”\n\n\n“Our whole body, including our brain, is working hard as we sleep, to accomplish life-preserving tasks that can be best achieved when we’re not around to interfere.”\n\n\n“If you’ve dealt with the stressors but haven’t dealt with the stress itself, your brain won’t let you rest. It will constantly scan for the lion that’s about to”\n\n\n“If you’ve dealt with the stressors but haven’t dealt with the stress itself, your brain won’t let you rest. It will constantly scan for the lion that’s about to come after you, so when you try to go to sleep, your brain won’t let you fall asleep, or it will wake you up over and over, checking for that lion. Complete the cycle, so your brain can transition into rest.”\n\n\n“I don’t want a doctor who’s been awake for twenty hours; I don’t want a lawyer who bills more than twelve hours a day—I know how sloppy work gets when somebody is fatigued—and you shouldn’t want an engineer who isn’t sleeping seven hours a night. Your work is crap if your brain isn’t rested.”\n\n\n“Hi, rage. I know our family raised us to believe we didn’t matter unless we were perfect, and perfect means we never stop working, and it’s right to be angry that we didn’t get the warm, unconditional acceptance every child is born deserving. Let’s treat ourselves as we wanted to be treated, granting ourselves permission to be human.”\n\n\n“Mine is more like a teenage version: the smart, quiet, yet sad and downtrodden girl who always sat in the back of class and no one talked to….When something goes wrong, I can hear her ‘told you so’ voice in the back of my mind.”\n\n\n“This uncomfortable, fragile part of ourselves serves a very important function. She grew inside us, to manage the chasm between who we are and who Human Giver Syndrome expects us to be. She is the part of us that has the impossible, tormenting task of bridging the unbridgeable chasm between us and this “expected-us.” It’s a form of torture, like Sisyphus rolling a rock up a hill only to have it roll back down each time. She’s forever oscillating from rage to helpless despair.”\n\n\n“Listen to her stories—never forgetting she’s a madwoman. Remind her that you are the grown-up, the homeowner, or the teacher, and she can trust you to maintain the attic so that she always has a safe place to stay. Thank her for the hard work she has done to help you survive.”\n\n\n“Guilt is, ‘I made a mistake.’ Shame is, ‘I am a mistake.’ ” With guilt, as opposed to shame, there is at least a pretense that one day you might deserve to participate fully in the human experience. With shame, your core self is judged.”\n\n\n“Are we really working toward our goals only because we’ll torture ourselves if we stop, so that as soon as we put down the whip we’ll sink into eternal apathy? Of course not. In fact, it’s the opposite: We only whip ourselves because our goals matter so much that we’re willing to suffer this self-inflicted pain if that’s what it takes. And we believe that because we’ve always done it that way, it must be why we’ve accomplished as much as we have.”\n\n\n“When people with depression try to be self-reassuring, their brains respond with threat activation.12 In fact, fear of compassion for self is linked to fear of compassion from others. That means that somewhere inside them, they believe that if they’re isolated, that’s good; isolation protects others from their real, core badness.”\n\n\n“And she realized “Perfect Julie” was just a defense she had constructed, to protect her real madwoman—who wasn’t a woman at all, but a little girl. This little girl was sensitive and afraid of rejection. She loved books and theater. She put on “Perfect Julie” the way a little girl might put on her mother’s shoes and lipstick, playing pretend. She wore “adulting” as a costume. It had been a game at first, like playing house, back when she was Diana’s age. But as Julie had gotten older, the Perfect Julie costume became necessary to disguise the fact that she was, underneath it all, just a girl who didn’t want to make anyone mad.”\n\n\n“Beating ourselves up results in pain, obviously, so at the same time that we’re beating ourselves up, we’re looking for ways to manage that pain, to make it bearable.”\n\n\n“Whichever metaphor you prefer, self-compassion isn’t always a comfortable or peaceful experience, but it does help us grow mightier.”\n\n\n“lot of us have a quiet little voice worrying that we’ll get up in that corporate office and have no idea what we’re actually doing. As a person with a hobby, you’re not ready for all of that now, and it’s difficult to imagine what it will feel like and how ready you could be after you go through the process of growing. The difficulty of imagining ourselves with the knowledge, expertise, and strengths we will gain in the future can stop us entirely from moving toward that future.”\n\n\n“It’s really strange when we’re doing our best, and our best falls short of what the world expects from us. When we can turn toward that strangeness with observational distance, then we are best enabled to be the change we want to see in the world.”\n\n\n“Being grateful for good things doesn’t erase the difficult things”\n\n\n“The cure for burnout is not “self-care”; it is all of us caring for one another. So we’ll say it one more time: Trust your body. Be kind to yourself. You are enough, just as you are right now. Your joy matters. Please tell everyone you know.”\n\n\n“Humans are not built to do big things alone; we are built to do them together.”\n\n\n“even if I felt like a social pariah in my classes, at least I would have a better vocabulary than these philistines”\n\n\n“I realized that there was some prestige in being smart, or at least appearing smart. Sounding smart was not the same social Teflon as being good-looking or athletic or funny, but hell, if someone could give me some props for being good at school, I would take nerd props over no props at all.”\n\n\n“They belonged in a way that I never could, and their regard for me was sweet and sour. How Asian.”\n\n\n“But in the course of reading great books, something happened. My reading molded me, the tool hammering its hand into shape. By some miracle—and by miracle, I mean great teachers—I pushed past the shallowness and stupidity of my own motivations. I fell in love with the actual literature and the actual ideas of great literature.”\n\n\n“The medium has no depth, but the content does.”\n\n\n“If you catch me in my off-guard moments, I’ll tell you that at some points in my life, I wanted to be white. It’s not a proud feeling, but it’s not a feeling that comes from the shame of being brown. It’s a tired feeling. Tired of the crushing racism. Tired of not belonging. It’s the exhaustion from fighting for your right to exist.”\n\n\n“Faith. Knowledge. Doubt. They weaved in and out of our lives with a baroque intricacy, a background fugue to our stumblings on the stage.”\n\n\n“What do you have control over? And what is beyond your control? As Camus’s protagonist, Dr. Rieux offers an answer: when the world is coming apart, you do your job.”\n\n\n“That was the delusion. We weren’t like everybody else. I now had the menace of knowing, and it infected everything I did. I was reminded of it constantly in ways large and small: my parents’ wobbly accents as my English became arrow-straight, the long and confusing searches in the grocery stores for simple items, the stares from strangers at the mall. These reminders that my family was not a normal American family—that we didn’t look like the rest of our town, that we were from somewhere else—wove into my very fabric a need to belong, a need that was a glittering and slippery yarn. I would never be able to untangle it from who I was and who I wanted to be, and it seemed that if I tugged on this thread, everything would unravel and leave me exposed.”\n\n\n“Bà Ngoại’s ire wasn’t the anger of personal damages but the anger of being shamed, a singular dishonor that she and my parents bore heavily. If our elders felt that we kids had done something to embarrass them or to cause them to lose face, our punishment was administered as if the entire town were watching and judging them as parents. Puritanical in its purity and unflinching in its deliverance. The severity of our punishment was commensurate to their own perception of their parenting.”\n\n\n“Bà Ngoại, my grandmother. Gentle one moment, violent in the next. Violence was sometimes kindness. Sometimes it was love. Sometimes it was rage. But it was everywhere, always.”\n\n\n“But if I allowed myself to be harmed by words, I was showing them that I belonged at least by virtue of understanding their language. And all I wanted was to belong.”\n\n\n“Kids covet things. They see something that other kids have, and they want that thing. They see a cool T-shirt with Darth Vader on it, and they want that shirt, too. Soon they want the house, the hair, the skin color.”\n\n\n“As an adult, I can explain and even understand where his anger came from (PTSD as a refugee, his own abuse as a child, the cycle of abuse that can perpetuate itself in a culture that equated obedient children with great parenting). As a second grader, I knew this violence as my only reality. If I spilled something, disobeyed, did something too quickly or too slowly.”\n\n\n“The wish for different parents fuels the archetypal fairy tales about evil stepmothers and children left in the woods. These fairy tales pivot around the wish that our parents, irascible and imperfect, aren’t even our real parents, that a fairy godmother will reveal to us our true royal bloodline or magical lineage. Whether you’re Harry Potter or Luke Skywalker or Cinderella, the fantasy is that the adults who are raising you aren’t even your real parents, that your real parents are kinder and magical.”\n\n\n“My past was worse than my present, and if my present indicated my future, I could live with that.”\n\n\n“As an adult, I’ve been able to understand that my father was not as trapped by his past as I thought he was. He was often violent and angry, but now I can look back and see that he tried to do fun things from time to time, things that didn’t fit into the narrow, cartoonish image that I formed of him.”\n\n\n“But even if the past is unchangeable, maybe our perspective of the past can change. And maybe the way we see past events can change, and if that can change, maybe the past event itself does change—not in action or outcome but in purpose and intent.”\n\n\n“How could I explain to Lou that we were symbols? That some people would never be able to see us as just people? That we were symbols of a painful and confusing war? Symbols of the refugees they saw on TV? Symbols of what they were afraid of? Symbols of the people who had shot at them and killed their friends, brothers, and sons? Symbols of whatever they wanted to see?”\n\n\n“I did learn a lesson that night. In the void of their departure, I learned to appear greater than I was. My brother needed me to be more than his nine-year-old brother. I learned that even if I didn’t have a clue about driving or having a job or being a parent, the symbolism of being a parent—the basic act of making toast and washing plates—was enough for Lou. That was the power of the symbol.”\n\n\n“Kids don’t have a euphemistic or subtle way of speaking about life.”\n\n\n“Symbolism in our waking Jungian dream was a two-way mirror. We were symbols for our American neighbors, but our neighbors—with their polished cars, grand homes, backyard swing sets—they symbolized something for us, too. They glittered as goals, mirages toward which we endlessly stumbled.”\n\n\n“Did I think that I could upend their expectations without any resistance? Of course, I was technically the same kid, but I had gotten the message: I couldn’t try too hard to change or fit in, especially if it involved something as dramatic as changing my name. That was too much. Too symbolic. Too real.”\n\n\n“Jung says that the process of becoming an individual begins with a wounding of the ego, and mine was undeniably wounded.”\n\n\n“But if literature moves you deeply, does it matter where it comes from? Does it matter that it’s trashy or lowbrow? Isn’t that emotional connection one of the purposes of art? To make you feel—really feel—emotions? To resonate with your life? And perhaps, in that connection, to introduce you to a world that lies beyond your own perspective, the utopia beyond your myopia?”\n\n\n“Emma Bovary’s books made her yearn for a dreamworld of emotional dramas and searing affairs. They seeded a delusion in her that grew quickly and blinded her, choking out the light except for the glow of fictitious hope. She was not ready for the quiet boredom of a married woman’s life, and the yearning that flourished inside her made her own life repulsive to her. Could art do that? Could it make you long and loathe at the same time?”\n\n\n“It would be decades before I diagnosed the lump of alienation, dual consciousness, and self-hatred, but it was already growing quickly, bilious and caustic. I only saw myself as the piece that did not fit in the puzzle.”\n\n\n“Our imaginations, our self-reflection, were circumscribed by what we saw, limited and funneled into someone else’s view of who we were. We saw ourselves as others saw us, and when we were in the media (which was not frequent), our stereotypes were reinforced in the books we read, the movies we saw, the things our friends said to us.”\n\n\n“With my mother, I lacked the words to tell her what I needed. With my father, I lacked the trust to tell him, the trust that he wouldn’t respond with violence or disappointment, the trust that he could give me heartfelt advice, that he could see the olive branch I was extending to him if I shared an intimate and personal experience with him. I didn’t believe that he would be anyone different from who he had been (no matter how much I wanted or needed him to be different).”\n\n\n“In Vietnamese, the word for country and the word for water are the same: nước. Context obviously makes it clear if you’re talking about the former or the latter, but in Vietnamese, your country is not the terra firma or the nationality; it’s the water. The waters that feed the soil. The waters that lap your shores. If you ask someone where they’re from, you’re asking them literally from what waters do they come.”\n\n\n“Old water. New water. Old country. New country. Aqua vitae. Giver of life. Destroyer of memories.”\n\n\n“My perceived need to read changed, slowly and surprisingly, into a desire to read—a desire that I didn’t fight.”\n\n\n“Their kindness was confusing for me, and it was easier for me to play the simple role of a teenager being angry with his parents. It was easier for me to fixate on the cool new stereo and not to think about why I got a cool new stereo from my parents. In the midst of my own changes, I wasn’t able to change my perspective on my mom and dad, to consider that my parents might be changing. Maybe they wanted to change the story of parents being the antagonists in their children’s lives. We didn’t have the script for that scene. I missed the cues for the dialogue, and the curtain closed on another moment of disconnection.”\n\n\n“Hatred required calories that I didn’t have.”\n\n\n“Punk rock had paved a way for me through my first two years of high school, but I was beginning to realize that maybe anarchy and nihilism were not a blueprint for building a future. Punk rock was an explosive for detonating the present so that I could rebuild my future from the rubble.”\n\n\n“It is a terrible thing for a man to find out suddenly that all his life he has been speaking nothing but the truth.”\n\n\n“This was the real lesson I learned from Malcolm X, the one I had been avoiding, and like Malcolm, I had to evolve my own thinking. But for me, that meant confronting a hideous truth about who the racists were. This was the hardest thing he had written about. It would be a long time before I could begin to understand how big racism was and how it affected me, but I had to take the first step, to acknowledge the reality of life, to tackle the hardest truth if I wanted to fight it. We all were the racists.”\n\n\n“In retrospect, I realized how my parents draped the hopes of college and greatness upon me, their eldest born. They snuffed out their own ambitions, exchanging their dreams for their children’s, hoping that our lives would be full of possibilities and free from devastation. My vague but lustrous potential would be built upon the heavy shards of their broken futures.”\n\n\n“My aspirations for greatness were the antithesis of punk, and I knew it. And for the first time I realized that the most punk thing for me to do was to be who I was without pretention or preamble or grandiose posturing. I had read it in Nietzsche but didn’t know what it really meant: become who you are. But becoming who you are required throwing away who you thought you were, unloading expectations both internal and external.”\n\n\n“I just never understood the logic behind buying souvenirs. After all, it’s the memory we want, not the souvenir, and once the memory goes, what meaning does the souvenir have? That’s like keeping training wheels on your bicycle. I think most of us spend far too much time looking back at our past anyway and not worrying enough about the future without an attic full of old prom dresses, dried corsages, books, toys you don’t play with anymore and photo albums of pictures we had thought were long gone.”\n\n\n“I don’t need souvenirs, and I don’t want souvenirs. None of you need souvenirs either. As long as you laugh, won’t you carry all the good times with you? And as long as you cry, won’t you carry all the bad times with you? So wherever we all end up and wherever we are all going, I know we don’t need souvenirs because laughter will always be laughter, and tears will always be tears.”\n\n\n“Oh, I don’t know about that. It was okay. I messed it up a little from my nerves, I guess. I feel … I feel weirdly let down. So much hype around everything, you know?” I reflexively shrugged at Molly’s compliment, deflecting praise as I always did because I was so unaccustomed to hearing earnest accolades. Any kind remark made me feel vulnerable, forced me to confront myself and my purported excellence, a task that felt impossible and undeserved.”\n\n\n“Teenagers and truck drivers: We’re all on our way to somewhere else. We’re all sitting at these booths, knowing that we’re not at our final destination. This is not the last stop for any of us. We’re all en route to far-off places, but our arrival? Who knows? Our arrival will be dictated by things we both can and can’t control, unforeseen things. I’m going to college to study English and art, but who knows if that’ll happen? Conor’s joined the army, but what if he gets killed in Iraq? What about you or my brother? What’s next? And we might get to our destination directly, but maybe it’ll be in a roundabout, crazy way.”\n\n\n“In a world that often seems too crowded or busy to notice beautiful things or make meaningful connections, there is still room for each of us to grow in the ways we were meant to.”\n\n\n“Whenever you meditate, bear in mind the phrase “without distraction and without grasping,” and put this into practice.”\n\n\n“If you fill a gourd with just a little water and shake it, it makes a lot of noise. But if you fill it to the brim and shake it, it makes no sound.”\n\n\n“Let your mind be a gracious host in the midst of unruly guests.”\n\n\n“Like Einstein’s theory that physical space is warped by bodies of matter within it, it sometimes feels as if the space of awareness is warped by the contents of the mind.”\n\n\n“At times, when we become fixated on something, our minds seem to become very small. Trivial issues loom up in our awareness as if they were very large and important. In reality, they haven’t become large. Our minds have become small. The experienced magnitude of the contents of the mind is relative to the spaciousness of the mind.”\n\n\n“Let the space of your mind be emotionally neutral, like physical space, which could not care less whether bullets or hummingbirds streak through it.”\n\n\n“What we observe is not nature in itself but nature exposed to our method of questioning.”\n\n\n“As an analogy, consider a researcher who measured only the vibrations created by the musical instruments as an orchestra played Beethoven’s Pastoral Symphony. He would find that before anyone heard any music, the instruments vibrated in specific ways, so he might very well conclude that those vibrations are the sole cause of the symphony. What he will have left out is the role of the composer, the conductor, the skills and emotional states of the musicians, the audience, and so on. While he’s right that the vibrations of the instruments played a critical role in producing the music, his eliminative approach has blinded him to a myriad of other influences, and he will be oblivious of the fact that many people can compose and play tunes in their minds, with no vibrating musical instruments exerting any causal role.”\n\n\n“All phenomena are preceded by the mind. When the mind is comprehended, all phenomena are comprehended. By bringing the mind under control, all things are brought under control.”\n\n\n“‘The mind that is established in equipoise comes to know reality as it is.‘”\n\n\n“One of the ways of stopping science would be only to do experiments in the region where you know the law. But experimenters search most diligently, and with the greatest effort, in exactly those places where it seems most likely that we can prove our theories wrong. In other words, we are trying to prove ourselves wrong as quickly as possible, because only in that way can we find progress.”\n\n\n“a person whose mind is distracted lives between the fangs of mental afflictions.”\n\n\n“The success of the McDonald’s model suggests that many people have come to prefer a world in which there are few surprises.”\n\n\n“‘If you want to be happy, you mustn’t fear the following truths but confront them head-on: one, that we are always unhappy, and that our sadness, suffering and fear have good reasons for existing. Two, that there is no real way to separate these feelings completely from ourselves.‘”\n\n\n“I’ve always thought that art is about moving hearts and minds. Art has given me faith: faith that today may not have been perfect but was still a pretty good day, or faith that even after a long day of being depressed, I can still burst into laughter over something very small.”\n\n\n“I’ve also realised that revealing my darkness is just as natural a thing to do as revealing my light.”\n\n\n“I was very serious about friendships when I was little, like most children. But after being bullied in elementary school and middle school, I think by the time I reached high school I’d developed a fear of straying from the herd, and was nervous about friendships in general. That fear was reflected in my romantic relationships, and I decided not to expect too much from friends or friendships anymore. Psychiatrist: I see. Do you find your”\n\n\n“Perhaps you’re co-dependent on your work as well. When you get good results, your worth is realised and you relax, but that satisfaction doesn’t last long – that’s the problem. It’s like you’re running inside a hamster wheel.”\n\n\n“Sometimes the best thing to do with people who would never listen to you in the first place is to avoid them altogether.”\n\n\n“If it doesn’t make you feel good, don’t go out of your way to do it.”\n\n\n“I’d like you not to give too much credit to what people say about you. The moment you set out to be more empathic is the moment it becomes a chore. That would result in your empathy decreasing, if anything. It’s good not to fake interest in things you’re not interested in.”\n\n\n“Because there’s really no end to worrying once you set your mind to it. If you shift your perspective from their past to your present, you can start perceiving your personal experiences in a more positive manner. From ‘How sad they didn’t realise this’ to ‘How lucky it is that I realise this.‘”\n\n\n“It’s like with your guilt. You want to strangle someone, and then you automatically feel guilty about having had that thought. Your own anger turns you into a guilty person. There’s a desire to punish yourself, shall we say. You have this superego that exerts control over you, a superego built not only from your own experiences but cobbled together from all sorts of things that you admire, creating an idealised version of yourself. But that idealised version of yourself is, in the end, only an ideal. It’s not who you actually are. You keep failing to meet that ideal in the real world, and then you punish yourself. If you have a strict superego, the act of being punished eventually becomes gratifying. For example, if you’re suspicious of the love you’re receiving, and so act out until your partner lashes out and leaves you, you feel relief. You eventually become controlled more by imaginary outside forces than anything that is actually you.”\n\n\n“when your life satisfaction falls, it’s natural to retreat into primitive measures. And eating and sleeping happen to be our most instinctive base measures.”\n\n\n“I’m needlessly harsh towards myself, so I need comforting, someone who is on my side.”\n\n\n“I’m very good at immersing myself emotionally, and I’m very empathic; I also feel pressured to be empathic, which means whenever someone would share an experience with me, I’d find myself lying and saying I’d been through the same thing. I would lie to make others laugh or to get attention, while simultaneously chastising myself for lying.”\n\n\n“We often lie when our cognitive abilities become impaired for whatever reason. Like when we’re drunk, for instance. You know how our memory or judgement falters after a few drinks, right? We subconsciously start lying to fill in the blanks. How many times have you seen drunk people insist they’re not drunk? We also find ourselves announcing things that have nothing to do with the context.”\n\n\n“Psychiatrist: Why are you so aware of all the hardships others are going through? Me: (Realisation hits.) You’re right. Wouldn’t it be more natural for me not to know? Psychiatrist: So, complain. Let others know how hard things are for you. Me: I wouldn’t know what to say. Psychiatrist: Observe how other people are saying it. They’re saying they’re having a hard time – that’s how you know, so clearly, that they are. But I think you’re the kind of person who would ask someone who wasn’t having a hard time if they were having a hard time. Me: (I burst into tears at this point.) You mean I was just pretending to be kind all this time? Psychiatrist: You are kind. There’s nothing you can do about that. Me: But I don’t think it’s kindness, I think it’s just … being pathetic. Psychiatrist: You’re attempting to silence your own complaints by thinking, At least I’m better off than them. And the world is full of so much suffering that it’s the easiest thing to find people who are having a harder time than you are. But once you do, you then insist on taking the extra step of berating yourself: How could I have been so blind to that person’s hardships until now?”\n\n\n“Psychiatrist: I feel like you’re not very interested in yourself. Me: Even when I keep a diary of my feelings? Psychiatrist: Is that not more of a record of yourself in the third person?”\n\n\n“When you’re having a hard time, it’s natural to feel like you’re having the hardest time in the world. And it’s not selfish to feel that way. Just because certain conditions in your life are relatively better, it doesn’t mean you’re better off in general”\n\n\n“It’s not the pills that make people addicted to them.”\n\n\n“It’s just your opinion. It’s not like there’s any right or wrong to it. Of course, others may have their own expectations, or you may feel pressured to sound impressive in your critiques because of your studies and your job. But the moment you think to yourself, Well, this is the way I am, and what can you do about it, you’ll feel much freer. Me: Oh. Just the thought of”\n\n\n“Forgetfulness can be liberating, you know.”\n\n\n“You put a lot of stock in what other people think. It’s because your satisfaction with yourself is so low. But your life is your life, your body is your body – and you have responsibility over it. Right now, you don’t process the input that comes to you through a mechanism of rationality or mediation, you go straight to the extreme. Self-surveillance isn’t necessarily a bad thing, but there is so much you can do with the input, such as rationalising or finding a different way to think about things – but you only do one thing with it. There can be so many reasons for something, but you’re so focused on the result of it that you don’t see the reasons. You keep focusing on, I’m sad, I want to cry, I’m angry, which only amplifies these emotions.”\n\n\n“I think you need to spatially separate your work and your rest. If you were stressed at work, you ought to be relaxing when you’re home,”\n\n\n“Your mood is extremely important. It determines how you interpret the random events of life.”\n\n\n“It’s just a hobby after all. You mustn’t let your hobby become stressful. But I hope if you don’t do it isn’t out of fear.”\n\n\n“Usually before a dream becomes reality, we tend to think we’ll wish for nothing else if only the dream is realised. Imagine how you’d feel if you always remembered that your dream has already been fulfilled. Everything that comes after would be like a lovely bonus. When you feel envious of something, try to imagine how you would look to your twenty-year-old self.”\n\n\n“But the you of the present is looking at your life and past as if you’re a failure. When in truth, from the perspective of a younger you, you’re the very picture of success.”\n\n\n“when my face looked like a bleak black-and-white film still in the mirror – could that girl have imagined she would become me one day?”\n\n\n“I’ve worked hard to get here. And now I make a living doing what I enjoy. I’ve no anxieties about whether this is the right path for me. All I want is to get better at it. That’s enough for me – why did I torture myself by comparing myself to someone else? If twenty-year-old me met me today, she would cry with joy. And that’s enough for me.”\n\n\n“more you sacrifice, the more you’ll begin to expect a payback.”\n\n\n“You keep obsessively holding yourself to these idealised standards, forcing yourself to fit them. It’s another way, among many, for you to keep punishing yourself.”\n\n\n“You try to hide your obsession because you are aware of it, I mean.”\n\n\n“Your self-esteem determines how you feel about the sincerity of others.”\n\n\n“You have sympathy for social minorities. Perhaps that comes from seeing yourself as disadvantaged?”\n\n\n“We are so bad at mourning in our society. Maybe it’s a failure of respect.”\n\n\n“I think it’s good to experience complete solitude in an unfamiliar environment. You’re not hitting rock bottom right now. When we’re sinking in water, it can be a relief to feel the ground beneath our feet, the rock bottom, because we know we can kick against it to rise again. But if you can’t feel the ground in life, the fear can be overwhelming. So maybe it’s good to find your rock bottom.”\n\n\n“Mother clearly hated how she had passed on this part of herself to us, which was why she was always angry at our faults.”\n\n\n“Accepting your burdens and putting them down isn’t an occasional posture; it’s something you need to practise for the rest of your life.”\n\n\n“Books never tire of me. And in time they present a solution, quietly waiting until I am fully healed.”\n\n\n“We always put modifiers in front of ourselves, and I’m no exception to that.”\n\n\n“to expect someone to always be a certain way or consistently do a certain thing can be a huge burden on them.”\n\n\n“When life becomes something one just lives through, when the demands of survival take up all of our time and effort, leaving no strength for any other demands, and when time rushes by drying up or rotting whatever we have had to neglect, expecting someone to carry on being the same is truly too much of a burden.”\n\n\n“There are days when I wish I were numb, when I’m desperate to feel nothing. I want to be simple and cold and totally without feeling. Empathy has a large presence in my life, and it can cast a very long shadow. I can be watching a television drama or a movie, listening to a song or looking at a photograph, listening to someone’s story or writing my own, and my heart and mood will sink. Like a punctum they pierce me without context, a feeling I am very used to now and tired of.”\n\n\n“Life is as messy as a bag whose owner never cleans it out. You have no idea when you might reach in and pull out a piece of old trash, and you’re afraid someone is going to look through your bag someday. Maybe your ‘baggage’ is like an old bag, too. You toss it around any which way, not caring how worn it gets or where it lands, and no one notices. You can’t afford a new bag so you carefully and painstakingly hold it so the rough patches don’t show.”\n\n\n“Changing your mindset takes some work, but the beauty is when you begin to understand there’s a huge difference between being busy and being productive. This is something so many of us struggle with because we falsely believe that we need to be busy, that we are supposed to fill our days.”\n\n\n“Balance is just one of the stories we tell ourselves. We all have a library of folklore filled with stories about ourselves that we believe: we are supposed to be a certain way, have a certain job, or live a certain life.”\n\n\n“I quieted the stories in my head and reset my expectations to make them realistic for my life. I’m not going to say I’ve completely gotten rid of the guilt, but I feel so much better because I changed my way of thinking.”\n\n\n“Ask any kindergartner what they are good at, and you’ll need to sit through a laundry list of topics: art, running, painting, climbing trees, eating potato chips—seriously, five-year-olds think they are amazing at everything! But wait ten years and ask the very same child, and she’ll think of almost nothing; at best you’ll maybe hear one or two things she believes she excels in. What happens to us in this space of time? How do we lose our belief in ourselves? We’ve allowed the world to define us and reinforce these limiting beliefs, but it’s time to break through.”\n\n\n“Sometimes we have to let go of our old stories.”\n\n\n“Because so many of us live in a state of either/or, we tend to push aside other things we really want to do. Far too many of us have pushed aside our aspirations because we believe we don’t have time or don’t have the right to pursue them.”\n\n\n“Too often we hand over the reins, allowing others to imprison us with their own agendas and urgent fires that need putting out. We think we don’t have control over how our day runs, but we do. We’ve simply forgotten that we have the ability to choose to spend time on our own priorities.”\n\n\n“Have you ever experienced that feeling of having no control over your day? As if your world is so rigid and made up of so many rules you don’t really get to choose the life you live? That, my friend, is learned helplessness.”\n\n\n“It’s not reality2 that makes us feel stuck; it’s the lens we use to view the world.”\n\n\n“We want to believe our kids need us, and sometimes in the busy rush of our everyday life, we forget they are capable of being more independent.”\n\n\n“We all have these invisible choices, don’t we?”\n\n\n“Have you ever watched a squirrel aiming to get something she wants? Perched in a tree, tail twitching, she sees a bird feeder and is drawn to it. The homeowners, though, are smart, and they’ve added all kinds of obstacles to make it “squirrel-proof.” Does our squirrel take a look, decide she has no chance of getting to the seeds, and toss in the towel? Absolutely not. A squirrel will attack the problem from all angles, testing and pushing the boundaries of what she knows she can and cannot do, until she sits triumphantly atop that feeder with a belly full of birdseed.”\n\n\n“Finding choices isn’t only possible, it’s essential to thrive. You just have to start actively looking for them—that’s a choice in and of itself.”\n\n\n“It’s possible for your future to look brighter, for you to focus on the things that are important to you. But to do that, your priorities have to take priority. It’s possible to have a job that makes you happy and to spend time on the things you really want.”\n\n\n“Our North Star is a combination of our mission, vision statement, and core values. Each one answers the question of who you are at your heart. The mission statement tells us what we are doing now, the vision statement tells of where we want to be, and the core values tell us how these can be defined through our actions. Like pieces of a puzzle, they come together to create the completed picture of why we make the choices we do. They become the North Star we need to guide us and help us navigate through decisions.”\n\n\n“Human beings are works in progress2 that mistakenly think they are finished. The person you are right now is transient, as fleeting and as temporary as all the people you’ve ever been. The one constant in our lives is change.”\n\n\n“Your mission statement isn’t about your job itself—it’s about what your job does and why you do it.”\n\n\n“Remembering that I’ll be dead soon15 is the most important tool I’ve ever encountered to help me make the big choices in life.” We don’t have to wait until we receive bad news from the doctor or read our obituary in the paper. We can begin to make those big choices now, using our North Star to help guide us.”\n\n\n“Don’t spend time beating on a wall, hoping to transform it into a door.”\n\n\n“For too long, I had no idea where to spend my time or how to spend my energy. I wasn’t productive—I was simply running around being busy, filling my days but not my soul.”\n\n\n“It seems like this abundance of information should make life easier, but when we are bombarded with so much of it, the paradox is that decision making becomes more difficult. This is when the feeling of overwhelm begins to settle in and we simply don’t know where to start.”\n\n\n“We have to cut in order to really grow and flourish. I know this seems counterintuitive, but think of a garden: Do you plant the flowers one on top of another? Do you squeeze so many in that there is no room? Or do you allow each plant to have space—space to receive the rain and the sun, space to spread their leaves and grow? That’s what we need: space to allow ourselves to focus. The only way to have that space is to actively create it for ourselves. We need boundaries.”\n\n\n“Perfectionism is just fear in fancy shoes and a mink coat.”\n\n\n“Remember, efficiency is doing a lot of work; effectiveness is doing the important work. Quality wins every time. And yes, we want to use less energy and time, but not at the expense of quality. Sometimes we are so caught up in deadlines, we don’t realize that the processes we believe make us faster are working against us.”\n\n\n“Writing on paper deepens the relationship between the information and your brain, and it creates the ability for you to see your bread crumbs to help uncover patterns. It allows you to see the bigger picture, which can sometimes feel abstract—it helps you uncover what’s important, which is where you really want to spend your time.”\n\n\n“No matter which bowl I choose, I will end up eating the amount of ice cream that fills it. My idea of how much ice cream I need expands to the size of the bowl I have. Time works in exactly the same way.”\n\n\n“We don’t realize that without failure we wouldn’t be as successful as we are. Our shortcomings and mistakes are all part of our path.”\n\n\n“Instead of saying, “I don’t have time,” try saying, “It’s not a priority,” and see how that feels.”\n\n\n“Overwhelm isn’t having too much to do; it’s not knowing where to start. Our long checklist doesn’t show us where to start. Instead, it confuses us more, spinning us in circles as we feverishly scan our tasks, wondering how we will possibly get it all done. Yes, it makes us feel busy, but it doesn’t make us productive”\n\n\n“You see, dopamine doesn’t distinguish between important and unimportant; it just knows that crossing items off our lists feels good.”\n\n\n“if you prioritize the important tasks, you get to a place where you don’t have any urgent tasks.”\n\n\n“Oftentimes we feel that something is important because we believe it’s something we are supposed to do—even if it’s not something we really want. These tasks are so deeply entwined with our stories and our need for perfection that we don’t even realize it. We feel tied to the obligation, and we lose sight of why we are even doing the task in the first place.”\n\n\n“Are we allowing our stories to dictate our days?”\n\n\n“A good plan includes the three Rs—record, reward, and redirect.”\n\n\n“You see, traditions are systems—they take the thinking out of tasks. Routines and rituals do that for us, too, but on a daily basis—they help streamline our days and make it easier for us to enjoy each day.”\n\n\n“Decision fatigue loves laundry stress; they’re best friends.”\n\n\n“How did the river go from intimidating to entertaining? All it took was structuring our run. We took some time to create a plan, and suddenly the crushing power of the river didn’t seem so out of control. We owned the river that day, and it felt good.”\n\n\n“One of the biggest mistakes I see people make is planning out the entire week in one sitting,”\n\n\n“It’s amazing the deep connections we can make when we strip away everything else.”\n\n\n“As long as you can start9, you are all right.”\n\n\n“important to count the marbles in our jars.”\n\n\n“The world will see you the way you see you, and treat you the way you treat yourself.”\n\n\n“We’ve all experienced that vague sense of dissatisfaction with our day. Time passes in a blur as we sit at our desks without really paying attention to the tasks or even the people around us. We punch the clock, getting work done, but when the day closes we feel like we accomplished nothing. This is why we feel unsatisfied when we lay our heads on our pillows, why we wonder where the day has gone.”\n\n\n“Harmony can be found in the 168 hours we have each week, but so many people choose to focus—almost hawklike—on just the 24 hours of each day. Twenty-four hours is such a tiny snapshot of the whole picture, literally one-seventh of our week. And yet each day is treated as if it stands alone, so there’s a tendency to look at this tiny snapshot as our chance at achieving this mythical balance.”\n\n\n“Any given 24 hours might not be balanced, but the 168-hour week as a whole can be.”\n\n\n“We have a tendency to beat ourselves up and to notice only the things we didn’t do well, when in reality we are doing much better than we think.”\n\n\n“It’s all about effectiveness over efficiency.”\n\n\n“Sharks glide through the salty water at the top of the food chain, but they are burdened with the constant task of movement. For sharks to breathe, oxygen-rich water must continually flow over their gills. Their fins act like the wings of a fighter jet, giving them lift. If a shark stops moving, it will sink to the sandy bottom of the ocean floor and suffocate. Sharks are predators in constant motion, which has led scientists for years to wonder, If sharks can never stop moving, how do they sleep?”\n\n\n“We have the time, but the idea of intentionally creating space for this unstructured time feels uncomfortable. It feels silly because we are grown-ups and we don’t think we need recess. But we do. Whitespace is essential for our own well-being.”\n\n\n“the beauty of acknowledging our stories is that we have the ability to rewrite our own endings.”\n\n\n“We are taught by society—by our upbringing—to be givers. We give, we give, we give, and we feel guilty taking.”\n\n\n“To bring out the best in others7, I first have to bring out the best in me. I cannot give what I do not have.”\n\n\n“We have a thousand words for busy13 but no single word for the true opposite—at least not a positive one. There isn’t an English word for slowing down and savoring time.”\n\n\n“We all want to be acknowledged. We falsely believe we have to be everywhere in order to be seen. I think we all worry about being forgotten. We all want to make our mark on this world.”\n\n\n“Committing to nothing means you’re distracted by everything.”\n\n\n“Sometimes yes is the very best word.”\n\n\n“every time we say yes, we say no to something else.”\n\n\n“You may not control all the events that happen to you, but you can decide not to be reduced by them.”\n\n\n“Our brain can overlook countless items in our surroundings, but once our brain takes notice of something it considers significant (in my case, pregnant women), it starts to pull those occurrences out of the background noise. Because of our selective attention,1 it feels as if they are appearing again and again in our world. Really, the truth is, those things were there all along; we just didn’t take notice. It’s our mindset kicking in.”\n\n\n“While I love the crystal-clear waters of the beaches, it does limit me from seeing the mountains and rivers.”\n\n\n“If I spent all my time in the mountains,3 I would miss diving with sea turtles, but if I only swam the warm waters of the Caribbean, I’d never get to see the sun set on the mountains.”\n\n\n“The world has arrived at an age of cheap complex devices of great reliability; and something is bound to come of it. -VANNEVAR BUSH”\n\n\n“Very few tools transform their culture.”\n\n\n“Humans often anthropomorphize the objects they use, especially when they become fond of their interaction with those objects.”\n\n\n“As with most media from which things are built, whether the thing is a cathedral, a bacterium, a sonnet, a fugue or a word processor, architecture dominates material. To understand clay is not to understand the pot. What a pot is all about can be appreciated better by understanding the creators and users of the pot and their need both to inform the material with their meaning and to extract meaning from the form.”\n\n\n“You’re doing this because that’s the dream,” he said. “Don’t mess with my dream, and I’ll like you.”\n\n\n“You know, we don’t grow most of the food we eat. We wear clothes other people make. We speak a language that other people developed. We use a mathematics that other people evolved  I mean, we’re constantly taking things. It’s a wonderful, ecstatic feeling to create something that puts it back in the pool of human experience and knowledge.”\n\n\n“Crusades are not completed in a day, even in a year.”\n\n\n“improve its abilities to wrestle problems into submission. Something to augment human powers. That was the word he used, augment. The other word he would come to use was crusade. Engelbart was embarking on a crusade to augment human capabilities by applying new technologies and developing ways to interact with that technology. He ultimately would realize, and even surpass, what Vannevar Bush had written in his terribly important yet unappreciated essay in the Atlantic. Crusades are not completed in a day, even in a year.”\n\n\n“The territory you see through the augmented window in your new vehicle is not the normal landscape of plains and trees and oceans, but an informationscape in which the features are words, numbers, graphs, images, concepts, paragraphs, arguments, relationships, formulas, diagrams, proofs, bodies of literature and schools of criticism.” We now have a term for this informationscape: cyberspace.”\n\n\n“The tablet become a page become a screen become a world, a virtual world. Its depths increase with every image and word or number, with every addition, every contribution, of fact or thought. Its corridors form wherever electricity runs with intelligence. Its chambers bloom wherever data gathers and is stored . ”\n\n\n“His vision was at the mercy of those he inspired.”\n\n\n“intimately. He found himself reading Marshall McLuhan’s Understanding”\n\n\n“The medium is the message.”\n\n\n“The computer is a medium! I had always thought of it as a tool, perhaps a vehicle-a much weaker conception. What McLuhan was saying is that if the personal computer is truly a new medium then the very use of it would actually change the thought patterns of an entire civilization.”\n\n\n“The Alan Kay style of virtual designing, which he continued long after visualizing the Dynabook, consists of creating imaginative abstractions of what can be, going through the motions of gathering a team to build the thing, and discovering important new techniques and innovations in the process. The real product is the body of ideas that circulate from the vision.”\n\n\n“Metaphor, it turns out, is the key to making computers comprehensible.”\n\n\n“if it didn’t hit the streets, it wasn’t worth doing. Ideas were useless if they didn’t get out there.”\n\n\n“people are more important than computers, and that computer systems should be designed to alleviate human frailties, rather than have the human succumb to the needs of the machine”\n\n\n“How can you believe any criticism when everything you do turns to”\n\n\n“How can you believe any criticism when everything you do turns to gold?”\n\n\n“The proper lesson from all this was that personal computer companies are just as well off letting others produce great software.”\n\n\n“when you start looking at a problem and it seems really simple, with simple solutions, you don’t really understand the complexity of the problem. Your solutions are way over-simplified. Then you get into the problem, and you see that it’s really complicated, and you come up with all these convoluted solutions. That’s sort of the middle, and that’s where most people stop, and the solutions tend to work for a while. But the really great person will keep on going and find the key, the underlying principle of the problem. And come up with an elegant, really beautiful solution that works.”\n\n\n“A lot of times people don’t do great things because great things really aren’t expected of them and because nobody really demands they try and nobody says, ‘Hey, that’s the culture here, to do great things”\n\n\n“What did I learn from Steve Jobs?” he repeated. “That ignorance [of what you can’t do] is great. We learned to keep on trying and trying. We weren’t the best, but we tried the hardest. We were just a bunch oflucky nerds.”\n\n\n“Real artists ship.”\n\n\n“When you’re trying to spread a religion you have to be pretty strict at first. After you get them converted, you can relax,”\n\n\n“Those in charge of the marketplace regarded computing as a rite of passage, a sort of hazing. Only by acquiring knowledge in this needlessly arcane system could one gain admittance to the society of adepts. It was not a joyous society, but one of stiffupper-lips.”\n\n\n“What I didn’t understand was that most people didn’t get to make their own decisions.”\n\n\n“Breakthroughs like PageMaker have two sorts of effects. The first is to increase the ease and reduce the cost of performing previously expensive, time-consuming tasks. The second, and possibly more significant, is to empower people who otherwise could never afford to do the task in the first place.”\n\n\n“When the going gets weird, the weird turn pro.”\n\n\n“What’s the difference between Apple and Boy Scouts? The Boy Scouts have adult supervision.”\n\n\n“There was a time, I know, when I conducted much of the same sorts of business that I currently engage in, without requiring a machine that makes more calculations in a morning’s work than all the combined arithmetical operations of humanity performed by hand, over the span of recorded history.”\n\n\n“Still, I am hard pressed for proof that, for all its magic, Macintosh has enabled me to be more productive. I feel that it has, with every inch of my being. But after my recent fiasco with On Location and Word, I sometimes question whether this is an illusion.”\n\n\n“This gap between accepted reality (computers make us more productive) and the quantifiable result (they don’t), has come to be known as the Productivity Paradox. A true puzzler: If computers enable us to get so much work done, in a much shorter period of time  why can’t we measure it? Where did the productivity go?”\n\n\n“But maybe productivity is not the main benefit from computers. As its designers understood, the point of Macintosh was not to prod you into piling up x more reams of paper, but to change the way you interact with information, to empower you to manipulate information with confidence, to augment your creative powers, and to change the very way you think.”\n\n\n“Something happens to companies when they get to be a few billion dollars,” he said. “They sort of turn into vanilla companies. They add a lot of layers of management. They get really into process rather than result, rather than products. They lose touch with their customers. Their soul goes away.”\n\n\n“Macintosh tells people as they use it, ‘You don’t have to take things too seriously.‘”\n\n\n“Each new purchase brought its small dopamine rush that faded as soon as the thing was out of its box and taking up space.”\n\n\n“Minimalism was a brand to identify with as much as a way of coping with mess.”\n\n\n“Every advertisement for a new product implied that you should dislike what you already had.”\n\n\n“There was really nothing wrong with our lives at all.”\n\n\n“Minimalism also seemed sometimes to be a form of individualism, an excuse to put yourself first by thinking, I shouldn’t have to deal with this person, place, or thing because it doesn’t fit within my worldview.”\n\n\n“It makes sense that millennials embrace minimalism. My generation has never had a healthy relationship with material stability. There are always too few resources at hand or too much competition for what’s left, a scenario that’s engulfing not just one age group but a wider swath of people every year. Even as the traditional economy falls apart, we’re awash in social media noise and new platforms competing for our attention, labor, and cash. Stability is no longer the default.”\n\n\n“Maybe the longing for less is the constant shadow of humanity’s self-doubt: What if we were better off without everything we’ve gained in modern society? If the trappings of civilization leave us so dissatisfied, then maybe their absence is preferable, and we should”\n\n\n“Maybe the longing for less is the constant shadow of humanity’s self-doubt: What if we were better off without everything we’ve gained in modern society? If the trappings of civilization leave us so dissatisfied, then maybe their absence is preferable, and we should abandon them in order to seek some deeper truth.”\n\n\n“We should not believe the lack of silver and gold to be proof of the simple life.”\n\n\n“Your bedroom might be cleaner, but the world stays bad.”\n\n\n“Minimalism is thus a kind of last resort. When we can’t control our material security or life path, the only possibility left is to lower our expectations to the point where they’re easier to achieve, which could mean living in a train car, or a camper van.”\n\n\n“We like to think that we can do without, rough it to prove that we’re not so soft or bound to the past.”\n\n\n“If we don’t establish our identities with the volume of things we consume, then we feel more attuned to the way we consume them and the careful decisions we make between one thing and another. It’s a species of the narcissism of small differences. We take pride in the small details that we have actually chosen from our limited options, which might make us feel better about not being able to change our circumstances as a whole.”\n\n\n“Consumerism causes a kind of alienation, in the Marxist sense: When workers are separated from the products of their labor and compensated by an hourly wage, they can’t find satisfaction in their jobs or the remainder of family life. Thus they turn to acquiring capital as the only form of self-fulfillment. We work only to accumulate stuff and in turn the accumulated stuff dominates us, further distancing us from non-commodified things like relationships, joy, and community. Labor “is therefore not the satisfaction of a need, but only a means for satisfying needs external to it,” Karl Marx wrote in 1844.”\n\n\n“Minimalism is a perfect fit because it allows for just enough character to make a space interesting but not too much. The rest gets smoothed over into blankness.”\n\n\n“When a word or style spreads everywhere, it tends to lose its original meaning.”\n\n\n“The veneer of minimalist style becomes like an organic food label, expensive green juices, or complex skin treatments being sold as a “no-makeup” look. It’s another class-dependent way of feeling better about yourself by buying a product, as Spartan as the product might be. It takes a lot of money to look this simple.”\n\n\n“It was better to go without a couch than buy one that wasn’t perfect. That commitment to taste might be rarified, but it probably didn’t endear Jobs to his family, who might have preferred a place to sit.”\n\n\n“The need for simplicity, taken to an extreme, can wipe function away entirely.”\n\n\n“Minimalist design encourages us to forget everything a product relies on and imagine, in this example, that the internet consists of carefully shaped glass and steel alone.”\n\n\n“For the sake of narrative it’s always tempting to link a biographical cause to an artist’s work, like a stem to a flower.”\n\n\n“Curation by definition is not an original act.”\n\n\n“Minimalism can be oppressive. The style can make you feel like you don’t belong in a space unless you conform to it, as in upscale cafés or severe hotel lobbies.”\n\n\n“(The problem with being both a critic and an artist is that you’ll probably like work that resembles yours.)”\n\n\n“It isn’t necessary for a work to have a lot of things to look at, to compare, to analyze one by one, to contemplate,” Judd wrote in 1964. “The thing as a whole, its quality as a whole, is what is interesting.”\n\n\n“Writing about emptiness is difficult because words document presence. As soon as you point to something in writing,”\n\n\n“Writing about emptiness is difficult because words document presence. As soon as you point to something in writing, it’s there, even if what you point to is empty floor.”\n\n\n“You have to know what to look for, even if you’re looking for absence.”\n\n\n“According to Minimalist principles, we have to fight the need to anthropomorphize or impose a metaphorical meaning on the installation. The boxes do not symbolize anything.”\n\n\n“This is Minimalism’s most powerful and frightening insight. It has nothing to do with the aesthetic cues associated with lowercase-m minimalism, the consumer products, interior decoration, the curated items of clothing. Minimalism doesn’t need to look good. It tries to make us understand that the sense of artistic beauty humanity has built up over millennia—the varieties of colors, stories recounted, and bodies represented—is also an artificial creation, not as inevitable as we think it is.”\n\n\n“A definition of art finally occurred to me. Art is everything at once.”\n\n\n“you don’t act, someone will decide everything”\n\n\n“Art becomes retail surprisingly quickly.”\n\n\n“Our heads are victims of the prevailing clutter as much as our spaces.”\n\n\n“Silence can be a kind of nothingness, an erasure of the world in favor of a more manageable blankness, an absense of perception. We use silence to paper over experiences we don’t like, creating a blockade between ourselves and sensation. It’s a natual response to the excess of information that we confront every day in the form of emails, texts, advertising, and noise.”\n\n\n“As the prestige of language falls, that of silence rises.” Sontag’s evocation”\n\n\n“One good thing about extreme simplicity to the point of boredom is that it makes you focus on whatever is available to you,”\n\n\n“The simplest tune in the world can become grating just the same over hundreds of listens, the way even the most elegant, seamless design gets boring if you see it everywhere.”\n\n\n“all art is made from preexisting material, and any change that one makes is a creative act.”\n\n\n“The Generic City is what is left after large sections of urban life crossed over to cyberspace.” Instead of marking the walls with graffiti, we type on our screens. We post photos of things on Instagram instead of creating them for ourselves. We end up in a desiccated malaise: According to Koolhaas, ambience inspires only “weak and distended sensation, few and far between emotions, discreet and mysterious like a large space lit by a bed lamp.”\n\n\n“To escape the ambience—to feel anything—we have to be willing to risk hearing something unpleasant and being taken out of our familiar comfort zones. We need to recapture the awe and the surprise of silence.”\n\n\n“In Zen they say: If something is boring after two minutes, try it for four. If still boring, try it for eight, sixteen, thirty-two, and so on. Eventually one discovers that it’s not boring at all but very interesting,”\n\n\n“applause. As Cage wrote, “The”\n\n\n“The best purpose is no purpose at all.”\n\n\n“Conversation strives toward silence,“23 Walter Benjamin wrote, “and the listener is really the silent partner.”\n\n\n“What we require is silence; but what silence requires is that I go on talking,”\n\n\n“Minimalism can often lead to a stultifying sameness as everything becomes as simple as possible—the elegant, ambient, blank style that I’ve described. It whitewashes both literally and metaphorically, at times privileging the Westernized, sanitized versions of external influences while deemphasizing their origins. Minimalism’s sources get rebranded as high-minded art made by solo geniuses instead of the products of a globalized culture, even if the artists themselves readily acknowledged their debts.”\n\n\n“Minimalism as it appears in the West is inherently oppositional, posing itself against something, as a departure from a current state—cleanliness against mess, absence against presence, and silence against noise.”\n\n\n“minimalism itself is not a homogenous thing. It’s the combination of what might at first seem to be opposites, the way light is inextricable from shadow.”\n\n\n“We look all around ourselves for instructions on how to live only to be confronted with the basic unknowability of the world. And so we turn to some new mode of control, such as minimalism, only to be infected with the suspicion that it, too, is unreal, a map to no territory.”\n\n\n“Anyone who explains this or that, yes and no, is himself the man of yes and no.” To echo the ending of so many of the monk’s analyses: When you understand that, you will understand nothing.”\n\n\n“Mono no aware is the beauty of transience, the way a falling leaf or sunlight gilding the edge of a rock at the end of the day can incite a sudden gut-punch awareness that life is evanescent.”\n\n\n“The Buddhist acceptance of ephemerality didn’t necessarily kill desire but made it all the more intense by giving the Heians a taste for ephemerality itself; they pursued the most beautiful evanescence possible.”\n\n\n“If life was so arbitrary then one may as well embrace the contradictions and the possibilities.”\n\n\n“The desire that everything be just right, matched with everything else around it in a unified whole, leads easily to intolerance.”\n\n\n“Man first of all exists, encounters himself,24 surges up in the world—and defines himself afterwards,”\n\n\n“To do is to be.”\n\n\n“In the midst of existence, most living things deny time. They grow and reproduce in order to fight the inevitable. Life strives to be permanent, though it cannot be. Even the slow natural decay of a flower in the ground is a consequence of this struggle to survive as long as possible.”\n\n\n“It’s not about consuming the right things or throwing out the wrong; it’s about challenging your deepest beliefs in an attempt to engage with things as they are, to not shy away from reality or its lack of answers. To believe or commit too strongly to one particular way of seeing or being is to miss out on all the other possibilities and to allow yourself to be defined too much by one thing.”\n\n\n“No wonder that in the twenty-first century, when so many feel modernity has failed the West—that our civilization has come close to destroying itself and our lifestyles appear gaudy and pointless—absence is appealing once more. Embracing it reflects the need for a new way of thinking as well as consuming, one that makes a virtue of incompleteness and irresolution. Minimalism is”\n\n\n“Languages, it seemed, did not only sprout in continuity like new branches from the same tree from where they started, but were like different trees that happened to be neighbours stretching their branches, touching each other and sharing structure.”\n\n\n“The first slave languages to appear were pidgins—stripped down, unstable codes made up on the spot. It was just adults throwing new words together—words they heard from the white people who owned the estates. But it was children, with the genetic ability to pick up a first language out of all the talk they hear, who pieced together the pidgin words and made them into creoles that could do everything natural languages did. When parents, and indeed a whole community, is reduced to connecting through a pidgin, that pidgin becomes the only input the children get for working out their first language. Fortunately, children are able to take this raw material and impose a regular structure on it, with rules for grammar and syntax and a standardized vocabulary, turning it into a creole. A creole, according to this model, is simply a pidgin that has—due to the innate ability of young children—evolved into a native language and, in the process, fleshed out and become stable. Creole languages were like evolution happening before our eyes.”\n\n\n“A story always starts before it can be told.”\n\n\n“TODAY, INDIAN AMERICAN FAMILIES like ours represent an American success story. But it is easy to forget that, long before they called us “the good immigrants,” they called us “the bad immigrants.”\n\n\n“For much of their history, Canada and America barred Asians from entry. In 1882, America enacted the Chinese Exclusion Act, the first significant race-based immigration ban in the country’s history. America later extended the ban to all of Asia. Canada passed a similar set of laws, and both countries curbed citizenship, land, and other rights for Asian laborers already within their borders. While America’s racial segregation was more explicit, both countries shared a commitment to building a white nation. That changed during the Cold War, when America wanted to promote itself as a liberal democracy capable of leading the world. Politicians reversed decades of discriminatory policy, reinventing America as a melting pot.”\n\n\n“In India’s highly stratified society, middle- and upper-class Indians from dominant castes typically access the best schools and jobs that feed into opportunities in America, which favor immigrants who bring specialized skills in tech and science. The result: an American diasporic community that is roughly nine times more educated than Indians in India. These conditions enabled Indian families like ours—families that had been thrice-filtered and stratified—to prosper like few other immigrant groups have ever done in America.”\n\n\n“I was, as historian Vijay Prashad observed in The Karma of Brown Folk, “unaware of how we are used as a weapon by those whom we ourselves fear and yet emulate.”\n\n\n“When we had nothing to throw back at the slurs thrown at us, when we had to silently swallow the humiliation of knowing that we were inferior in our own country, Yush and I found solace in the idea that success was part of our destiny. The belief that we were exceptional protected us. Until it didn’t. Because stories designed to uphold hierarchies protect only one group—those at the very top.”\n\n\n“Myths imbue the ordinary and mundane with celestial meaning. But this is also what makes them so dangerous: They do not reveal truths. Rather, they obscure any part of our realities that do not conform to the fantastical narrative”\n\n\n“It is not hard to see how the myth reinforces America’s existing social and racial order, then, seducing its adherents with the promise of belonging in a country where their position remains tenuous and their acceptance is always in question. Rather than fostering solidarity over the ways in which white America disenfranchises those who look unlike them, the myth sows division among Asian ethnic communities. The myth encourages those at the top of the economic ladder to reinforce it, pushing those at the bottom further down. The privilege of the few sets constraints upon the many. The myth erases the legacy of racial exclusion from America’s collective consciousness while perpetuating racial exclusion. The myth creates cognitive dissonance and then tells us that this dissonance does not exist. The myth splits our psyches, then calls this violence peace. The myth forces our minds to forget that which our bodies cannot: that belonging is always conditional.”\n\n\n“The problems between you and me began when I started trying to create context around things that were meant to be forgotten. Our problems began when I started searching for a way to explain everything that felt so inexplicable. Our problems began when I was expected to shrink myself, as you had been forced to do, but instead I insisted on expanding.”\n\n\n“The world we live in, which demands perfection and achievement, teaches us we cannot love ourselves as we are. The myth teaches us to think greatness always resides outside us instead of within”\n\n\n“I used to think that memories followed a straight line, starting at one point and ending at another, held together by the backbone of the strong linear narrative of cause and effect that takes each of us from birth to death. Now I think of memories as haphazard blots of ink in a Rorschach test that we assemble along the spine of the story we are told about who we are. If given enough space, time, and support, we can arrange the memories along a story that we write for ourselves, extracting new meaning from events experienced one way and later understood as another.”\n\n\n“I had underestimated the power and the depth of that desire and how the force of that current swept up the rest of us.”\n\n\n“There was a time when my outspokenness brought us together instead of tearing us apart. There was a time when speaking my mind was received not as a threat but as an act of love.”\n\n\n“I didn’t know how to get the girls in my class to see me as special or good, but I learned that winning over adults was easy.”\n\n\n“Even though my friends were not always nice to me, I felt good about myself because I excelled in school.”\n\n\n“Now I wonder who we could have been if we saw our ethnicity not as something to manipulate into belonging in white America but as an opportunity to understand why we were treated differently in the first place.”\n\n\n“Until then, I thought adults always told the truth. I thought that the rules adults enforced existed to keep us safe, and I thought that adults followed all the rules that they made us follow. I saw this as the distinction between childhood and adulthood: Kids didn’t know the rules for how to behave, and being an adult meant following the rules well. I wondered, then, if sometimes adults told us things that they thought were good for us because they wanted us to behave one way, but they acted another, because that was more convenient for them. This struck me as the most unfair, wrong, unjust thing in the world.”\n\n\n“I do not know what India represented to Papa, but I suspect he carried nostalgia for a place that never existed, a utopia created by the frozen impressions and desires of a nine-year-old boy who moved to a white country that shunned him.”\n\n\n“In the forgotten history of this influential board game, I recognize the arc of my own obscured cultural past. I see a deep self-knowledge abandoned and forgotten, replaced with a story that asserts the power of the very people who ensured our history’s erasure, then marketed it back to us as our truth. I wonder now how this shaped his psyche and spirit: When, even in his own country, his people’s stories did not matter, he was forced to study his oppressor’s greatness, and learned to deny his own.”\n\n\n“In Dadaji’s story, I see how Papa’s ancestors grasped for security by seeking educational attainment, specifically through math and science, and by learning the ways of the white man. In Papa’s lineage, I see people who strove to ascend to feel secure. It is under these auspices that Dadaji rose from poverty and that Papa rose from a lower-class upbringing in Canada to the upper-middle class as an adult. Papa followed this common immigrant road map and imprinted the map upon Yush and me. But along the way, we forgot that this is not necessarily who we are—it is who we felt pressured to become.”\n\n\n“A deeply sensitive, wounded brown boy grew into a deeply sensitive, wounded brown man who sought to gain respect, status, and security by embracing the only role that embraced him: that of the high-achieving Indian kid.”\n\n\n“The difference in treatment between son and daughter would ripple through generations, one learning entoc: true\ntitlement, the other learning injustice. One sibling would lean into nostalgia for lost culture to justify his behavior, while the other would struggle to reclaim her lost culture, observing how tradition was so often invoked to evade accountability and prevent change.”\n\n\n“I saw that on the page, my words mattered as much as those of someone like Shel Silverstein. The page could not ignore me or treat me differently just because I was small or dark-skinned or a girl. On the page, I could show people things that I had trouble showing them any other way.”\n\n\n“Creativity poured out of me as a natural expression, touching everything that I did.”\n\n\n“It’s not that I wanted to be white, Mummy. I loved my bronze skin. There was no food better than your jeera-and-haldi-spiced aloo gobi. I felt glamorous in the deep-blush silver-lined lehenga that Naniji and Nanaji sent me from India. But I yearned for the freedom that I associated with whiteness. I felt like a simile, my personhood contextualized by whatever popular image I conjured in the minds of others—usually, Apu from The Simpsons. “Thank you, come again!” kids joked, asking if my parents owned a Kwik-E-Mart. They wondered out loud why my hands were brown on one side and white on the other and wanted to know where I was really from. I was envious that white people didn’t have to liken themselves to something else in order to be understood. They could appear as they wanted to appear, without question or comparison.”\n\n\n“AS I STROVE TO meet Papa’s expectations, the joy I had once taken from my hobbies faded. Reading, writing, and even painting started to feel rote and mechanical. The fact that things I loved could seem tedious scared me. I read books to extract facts and words, guzzling them like Papa’s nutrient-rich Ensure. I doubt that you or Papa noticed my creativity clamping shut, because I barely noticed. I continued to paint and draw—only now I did it to impress the hypothetical white Harvard admissions officers of Papa’s imagination.”\n\n\n“If I could answer Papa’s question today, I would say this: Art kept my spirit alive. Expressing myself, whether by drawing, writing, or dancing, was an assertion of my existence that enabled me to connect to something deeper than simply what I was expected to produce in the world. Later, when I felt too blocked to create, consuming art broke my sense of isolation and helped me see parts of myself in work created by others. When I forgot who I was, creating art helped me find my way back. Art was my entry point to learning how to love myself. Now I feel sad that Papa might not know what it means to connect with oneself or with someone else in this way.”\n\n\n“But my treatment of you wasn’t simply mimicry, either. It was a clumsy expression of anger over how mother was raising daughter to learn that to be good is to betray oneself, to forever contort oneself to fit into impossible, contradictory expectations of womanhood that felt stifling.”\n\n\n“Papa wanted me to remain invisible. But from what I could tell, dating relied upon being seen, noticed, and chosen. America, too, sat high up on a stage, forcing the rest of the world to behold its spectacle. Being seen—well, that was about the most American thing I could think of. Unfortunately, I was seen as little more than the mule”\n\n\n“As I began to love my body, I understood myself beyond my capacity for academic performance. I saw that I was so much more than what others perceived me to be.”\n\n\n“Papa’s anger was not new, but this was the first time that I did not see any part of his rage toward me as justifiable. And it was the first time that I recall you piling on, berating me the way that he did. Your words hurt far more than what Papa said. After he was done yelling at me, you yelled for another hour. You said hurtful things, things that sounded like what Papa would say. I knew that you didn’t mean any of it, but I didn’t understand where your anger came from. From you, hateful words sounded clunky and unnatural. It was as if you needed a place to express your anger, and this was the only forum available. I think Papa’s words to me gave you a template for your rage.”\n\n\n“He could have shown Yush and me how to love ourselves in the face of whiteness. But he could not teach us what he did not know himself.”\n\n\n“The truth is, I had no idea if Drew could have appreciated me for who I really was. I never gave him the chance. I couldn’t accept myself, so how could I let Drew?”\n\n\n“I think I know why you busted me: My defiance hurt you. You had become a casualty in the war between Papa and me. You worked so hard to keep the peace at home. And here I was, poking the bull to get him to charge. My antics made a mockery of all that you had put up with to provide stability. I wonder now if you perceived my behavior as a personal rejection, not just of your efforts as a mother but of my duty within the family. Everyone had a role to play, and everyone continued to play their part, except for me. It is true that I took out my anger in ways that were hurtful. I spat in the face of your sacrifice, and for that, I am sorry.”\n\n\n“I resented that whenever I succeeded, Papa credited himself and his Indian values, but when I failed, that failure was uniquely mine, a product of my Americanness. I felt so ashamed that I was—as I put it then—“bad at being Indian.” At the time I thought I was abdicating my identity. Now I see that I was asserting”\n\n\n“resented that whenever I succeeded, Papa credited himself and his Indian values, but when I failed, that failure was uniquely mine, a product of my Americanness. I felt so ashamed that I was—as I put it then—“bad at being Indian.” At the time I thought I was abdicating my identity. Now I see that I was asserting it.”\n\n\n“Yush’s love taught me that a fight does not have to become a war that ends in the total annihilation of another.”\n\n\n“I viewed Yush’s talent in science and math as a gift and wished that, like him, I was naturally skilled in the exact ways that capitalism rewards. I never thought about the toll that his success took on him or what kind of pressure he must have felt to maintain it. I thought Yush—whose name means “glory” in Sanskrit—was blessed. Now I know that he was cursed.”\n\n\n“Papa said that he valued achievement, but it seemed like he was willing to bend the rules to appear that way—even if that involved cheating.”\n\n\n“I thought Papa was in pain because I had been such a bad student in high school. I thought that if I became a Good Indian Girl, our relationship would improve and he would be at peace. I want you to remember that I tried to be that daughter, Mummy. And I want you to know that trying taught me that achieving perfection would not have kept our family whole.”\n\n\n“was nervous about my best friends seeing my family life so intimately, and I was nervous about what Papa might find when seeing me with these white friends up close. I felt a dissonance that I could not yet articulate, a tear in what W.E.B. Du Bois called the “double-consciousness” of race, not knowing how to meld my distinct identities. I understood these two versions of myself as Indian—my home self—and American—my outside self. I was anxious that either I would not be Indian enough for Papa or I would not be American enough for Nancy and Marin. Ultimately, I’d be outed as a fraud by everyone I cared about.”\n\n\n“EVERYWHERE I WENT, PEOPLE saw me as Indian. But India was the only place in the world I felt American. The specter of India loomed so large over my life, yet my entire actual lived experience with a vast country of over one billion people and hundreds of spoken tongues was probably no more than about three months, total. I do not know how long we stayed when you took me as a baby, when I learned the Hindi that I can still speak in choppy waves.”\n\n\n“Your dad would hate to have me as a daughter,” she said. “Why?” I asked. “Because I would tell him exactly what I think, and I don’t think he’d like that.” Then she asked me, “Why do you act like a child when you’re with your dad?” “What do you mean?” I said, getting defensive. I received her question as judgment from a white woman who didn’t understand anything about the stress of navigating life with two different identities. Nancy had crossed a line that she wasn’t supposed to cross. “You pretend to not know things that you know. You ask him questions when you already know the answer to them. You’re smart, but around your dad, you play dumb.” The truth of Nancy’s words hit something deep inside me, a place that I had numbed. In that moment, I felt humiliated—but also recognized. Something I had put to sleep had been awakened, and I could not ignore it.”\n\n\n“At the time, I scolded Yush for passing on an opportunity for success that I would never have, a chance to be among the truly elite. But Yush was happier at home. I think Yush would have been happy with a simple life. What I think he didn’t feel sure about was whether, if he chose that simple life, he’d still be loved and respected.”\n\n\n“None of us knew then that what Yush dealt with was not an anomaly but a tragically common symptom of the pressures he faced. We didn’t know that Asian American college students are more likely to deal with suicidal thoughts and attempt suicide than white students—straddling multiple cultures, experiencing racism, and living up to narrow expectations of achievement exerts extreme stress on the mind and body. To navigate those pressures, Yush and I learned to repress our feelings and forge onward,”\n\n\n“There was no way for us to talk about any of this, because we did not know these problems even existed. We found out about a problem like most families do, when it became so big that it exploded in front of us and we could no longer avoid dealing with it. And we dealt with it the way most families do: quickly and quietly. We swept up the mess, put things back as best we could, and continued to live in the same way, as if nothing had ever happened. We didn’t know that by trying to forget, we were more deeply committing ourselves to the very circumstances and problems that had caused the explosion in the first place. We didn’t know that we were teaching Yush not to resolve his pain but to find more-creative ways to hide it. Now I wonder what decisions Yush would later have made if he had been encouraged to talk about his mental health, rather than feel pressured to stay quiet.”\n\n\n“But as our family struggled to find some sense of normalcy, I began to question the idea of normal. Yush’s best friend, who also lived with depression, said, “This is a dumb analogy, but it sort of fits. It’s like in Men in Black. If you don’t believe in aliens, you walk around like everything is normal. But once you become aware of depression and how it lies to your mind, it’s like you know about the aliens. You can’t go back to the way you used to think, and you can’t believe how uninformed you were.” It was not the most eloquent analogy, but it captured my sentiments. For the first time in my life, I began to wonder what else I had failed to see because I had blocked it from view.”\n\n\n“Writing demands conviction. But the last thing that I could put on my page was me—not that I even really knew who that was. I prided myself on my ability to be a chameleon: to change myself to reflect what others needed of me when they needed it. This, I thought, was the quality that made me unique—that was what made me me.”\n\n\n“I had originally listened to Buaji because I respected her. But that summer, I began to see how owning my reactions and behavior liberated me. By not responding to Papa’s tantrums, I took myself out of them entirely, to the point where I could recognize the absurdity of his actions. I started to understand the directionality of the movement, how it traveled from one source and then stopped at another. His outbursts were a pattern—not an anomaly. I didn’t understand what caused his anger, but I was no longer willing to submit to the repetitive cycle.”\n\n\n“Questions are dangerous. Questions lead to dissent.”\n\n\n“In our family, loving someone meant rescuing them or letting yourself be rescued by them. But when I needed help to cope with work and the chaos at home, no one in our family had the capacity to support me, because everyone was dealing with their own, bigger problems. As I rescued myself, I wondered if that could be a form of love, too: the ability to take care of oneself well enough to not require saving.”\n\n\n“I would not be able to write or paint anything authentic for years, plagued by that question. I would struggle to believe that anything I created could ever really matter, because it didn’t seem to matter to the people who were supposed to love me the most.”\n\n\n“In a capitalist society, the measure of wellness isn’t a person’s actual health or happiness but how far one can rise or how much wealth one can accumulate. Somebody seen as “unwell” is unable to produce and to achieve.”\n\n\n“I had spent a good portion of my life feeling cheated out of the exceptional family I was told I had. I believed that if I held the same values and followed the rules laid before me, I could make that perfect family materialize. When that didn’t happen, and most of the people around me doubled down on that message—to suggest that we were not happy because I was not adhering to the rules closely enough—I felt like I was losing my mind. Now I had learned that the secret of having a happy family was pretending to be perfect. I felt robbed. I didn’t want to do that, and I couldn’t accept that as the answer. I didn’t understand how no one else in our family felt the same level of indignation that I did over living a lie.”\n\n\n“But wasn’t society a little ill, too, for normalizing the idea that the rules didn’t apply to him because of his status?”\n\n\n“NO ONE IN OUR family had died, yet I felt an emotional death. My grief wasn’t just about losing my relationship with you. The rickety bridge to my cultural identity had also collapsed. I had mostly participated in Indian culture through family events, like Diwali celebrations or weddings, where I could wear lehengas and speak unsteady Hindi among relatives. As I distanced myself from family, without many friends with whom to create my own traditions, I withdrew from all things Indian. I had once prided myself on being a devoted daughter, but I was ashamed that I could no longer call myself that, either.”\n\n\n“I’d been raised to believe that comfort was the result of hard work or innate intellect, but I was starting to understand that fulfillment of these basic human needs was tied to a person’s body, bloodline, and the origins of their birth. Papa’s wealth had made me feel entoc: true\ntitled to a level of security that no one is owed or guaranteed. I had a simplistic understanding of the world and how it worked because it worked well enough for me, and it was only when it stopped working for me that I began to think about the ways in which it failed to work for others.”\n\n\n“I began to think of success not as a job toc: true\ntitle, wealth, prestige, or social network but as the ability to be myself in the world. To know that, as a woman who had been taught that I needed to serve a man to be complete, I could instead build a life for myself that I loved, and that I could sustain that life by myself. I hoped that, maybe if you saw me live this way, you might choose to come back to me.”\n\n\n“At the very moment I started seeing myself as an equal and asked for the same opportunities and rights as my mostly white peers, I was cut down to size, put in my place, reminded that I should be grateful to be allowed among them at all. I saw that I was not valued for my perspectives, only for what I could produce.”\n\n\n“MUMMY, I HAD WANTED to think that fame and wealth—conventional notions of success—didn’t matter to me anymore. But they did. I didn’t get approval from you or Papa or Yush, and the desire to be validated was so deep in me that now I sought it on an even larger stage: the whole world, demanding that everyone look at the very thing that no one in our home could acknowledge—my perspective. But now I could see that, while the world loved what I did, it still didn’t love me. I didn’t know what to do with my ugly desire for validation or the world’s ugly response to it.”\n\n\n“It’s a strange thing to miss someone who is right there. When I talk to you is when I miss you the most, because I am confronted by what I cannot have.”\n\n\n“WE HAD EACH BEEN raised to believe that every unknown could be resolved through willpower and intellect, a message reinforced by America’s rigid conception of who we are supposed to be. The truth is, society doesn’t raise people to aspire to be kind or compassionate or happy. It pressures adults to achieve and accomplish. It teaches people that what matters more than their character or how they treat others or how they feel about themselves is how much money they can hoard, who they know, how famous they can get, and how much power they wield over others. Emotions have no basis in this framework. They are a nuisance, a hindrance, a distraction, a weakness.”\n\n\n“I HAD THOUGHT OF love as a taut chain with a tight clasp that carried our weight as we clutched one another, no matter what dragged any of us down. I had believed that when I love someone, I should hold on regardless of what else I have to give up in order to keep them. The more one gives up, the greater the love, I thought. To love someone well was to perform perfection for them, and to be loved well was for them to perform perfection for me. But that is not true love. That is self-abandonment masquerading as love.”\n\n\n“I had once thought that I came from a line of Gods, and I had punished myself for failing to be Godlike. But we were not Gods, and I was not the avatar for our family’s unraveling. I was just another product of inherited trauma, unresolved grief, and reactive survival mechanisms, like everyone else who came before me. We were mortals who felt ashamed when we failed to appear omnipotent. Now I see that my job was to release my ancestors from this burden, to allow those who come next the freedom to be ordinary.”\n\n\n“Language was always the companion of empire, and as such, together they begin, grow, and flourish. And later, together, they fall.”\n\n\n“He buried his past life, not because it was so terrible but because abandoning it was the only way to survive.”\n\n\n“‘Every language is complex in its own way. Latin just happens to work its complexity into the shape of the word.”\n\n\n“Words and phrases you think are carved into your bones can disappear in no time.‘”\n\n\n“A lie was not a lie if it was never uttered; questions that were never asked did not need answers.”\n\n\n“‘Whenever the English see me, they try to determine what kind of story they know me from,’ Ramy said. ‘Either I’m a dirty thieving lascar, or I’m a servant in some nabob’s house. And I realized in Yorkshire that it’s easier if they think I’m a Mughal prince.‘”\n\n\n“If they’re going to tell stories about you, use it to your advantage.”\n\n\n“The English are never going to think I’m posh, but if I fit into their fantasy, then they’ll at least think I’m royalty.‘”\n\n\n“Travel sounds fun until you realize what you really want is to stay at home with a cup of tea and a stack of books by a warm fire.‘”\n\n\n“‘Translation, from time immemorial, has been the facilitator of peace. Translation makes possible communication, which in turn makes possible the kind of diplomacy, trade, and cooperation between foreign peoples that brings wealth and prosperity to all.”\n\n\n“‘That’s the thing about secret societies,’ said Griffin. ‘They’re easy to romanticize. You think it’s this long courting process – that you’ll be inducted, shown a whole new world, shown all the levers and people at play. If you’ve formed your only impression of secret societies from novels and penny dreadfuls, then you might expect rituals and passwords and secret meetings in abandoned warehouses.”\n\n\n“‘Language does not exist as a nomenclature for a set of universal concepts,’”\n\n\n“So you see, translators do not so much deliver a message as they rewrite the original. And herein lies the difficulty – rewriting is still writing, and writing always reflects the author’s ideology and biases.”\n\n\n“He was a child starved of affection, which he now had in abundance – and was it so wrong for him to cling to what he had?”\n\n\n“‘What makes the English superior is guns. Guns, and the willingness to use them on innocent people.‘”\n\n\n“Unpretty women were so much easier to deal with in some ways—you didn’t have to face the pain of their probable unattainability.”\n\n\n“For so accustomed are we to electric lights that the sight of a naked bulb beneath an ordinary milk glass shade seems simpler and more natural than any gratuitous attempt to hide”\n\n\n“For so accustomed are we to electric lights that the sight of a naked bulb beneath an ordinary milk glass shade seems simpler and more natural than any gratuitous attempt to hide it.”\n\n\n“The cleanliness of what can be seen only calls up the more clearly thoughts of what cannot be seen.”\n\n\n“An insignificant little piece of writing equipment, when one thinks of it, has had a vast, almost boundless, influence on our culture.”\n\n\n“And so we distort the arts themselves to curry favor for them with the machines.”\n\n\n“In making for ourselves a place to live, we first spread a parasol to throw a shadow on the earth, and in the pale light of the shadow we put together a house.”\n\n\n“If the roof of a Japanese house is a parasol, the roof of a Western house is no more than a cap, with as small a visor as possible so as to allow the sunlight to penetrate directly beneath the eaves.”\n\n\n“A light room would no doubt have been more convenient for us, too, than a dark room. The quality that we call beauty, however, must always grow from the realities of life, and our ancestors, forced to live in dark rooms, presently came to discover beauty in shadows, ultimately to guide shadows towards beauty’s ends.”\n\n\n“This was the genius of our ancestors, that by cutting off the light from this empty space they imparted to the world of shadows that formed there a quality of mystery and depth superior to that of any wall painting or ornament. The technique seems simple, but was by no means so simply achieved. We”\n\n\n“This was the genius of our ancestors, that by cutting off the light from this empty space they imparted to the world of shadows that formed there a quality of mystery and depth superior to that of any wall painting or ornament. The technique seems simple, but was by no means so simply achieved. We can imagine with little difficulty what extraordinary pains were taken with each invisible detail—the placement of the window in the shelving recess, the depth of the crossbeam, the height of the threshold. But for me the most exquisite touch is the pale white glow of the shoji in the study bay; I need only pause before it and I forget the passage of time.”\n\n\n“But I see why in ancient times statues of the Buddha were gilt with gold and why gold leaf covered the walls of the homes of the nobility. Modem man, in his well-lit house, knows nothing of the beauty of gold; but those who lived in the dark houses of the past were not merely captivated by its beauty, they also knew its practical value; for gold, in these dim rooms, must have served the function of a reflector. Their use of gold leaf and gold dust was not mere extravagance. Its reflective properties were put to use as a source of illumination.”\n\n\n“Beautiful though such a face may be, it is after all made up; it has nothing of the immediate beauty of the flesh.”\n\n\n“The unseen for us does not exist. The person who insists upon seeing her ugliness, like the person who would shine a hundred-candlepower light upon the picture alcove, drives away whatever beauty may reside there.”\n\n\n“As even this trifle suggests, pitch darkness has always occupied our fantasies, while in the West even ghosts are as clear as glass.”\n\n\n“we Orientals tend to seek our satisfactions in whatever surroundings we happen to find ourselves, to content ourselves with things as they are; and so darkness causes us no discontent, we resign ourselves to it as inevitable. If light is scarce then light is scarce; we will immerse ourselves in the darkness and there discover its own particular beauty. But the progressive Westerner is determined always to better his lot. From candle to oil lamp, oil lamp to gaslight, gaslight to electric light—his quest for a brighter light never ceases, he spares no pains to eradicate even the minutest shadow.”\n\n\n“The older we get the more we seem to think that everything was better in the past.”\n\n\n“Yet of this I am convinced, that the conveniences of modern culture cater exclusively to youth, and that the times grow increasingly inconsiderate of old people.”\n\n\n“One of the oldest and most deeply ingrained of Japanese attitudes to literary style holds that too obvious a structure is contrivance, that too orderly an exposition falsifies the ruminations of the heart, that the truest representation of the searching mind is just to “follow the brush.”\n\n\n“‘We’ll keep making the wrong decisions,’ a friend told me, ‘and we’ll keep enjoying the consequences.‘”\n\n\n“When a person meets misfortune, even drinking water would get stuck in the teeth.”\n\n\n“He envied the disciples of the larger sects. No matter what spell, it required self-comprehension after it reached a certain level. But learning from the experiences of the people that had studied it before them would greatly decrease the detours that they had to take.”\n\n\n“Work and relax, that was the right way.”\n\n\n“Do within the limits, and depend on the heavens.”\n\n\n“Behind the most beautiful and exquisite things, was the most dullness and loneliness.”\n\n\n“The joy after surviving a calamity made everything seem so beautiful.”\n\n\n“To cultivate the sword, what cannot be lost is purity. But the five elements birth and defeat each other, and are difficult to make pure.”\n\n\n“Ha ha, cultivation is long and difficult. If there isn’t a strong and resolute heart, how can one achieve the path?”\n\n\n“the existing official wasn’t as good as the existing manager.”\n\n\n“A clever companion was much better than a stupid one.”\n\n\n“Anything that required study, it had to be built from the ground up, there were no tricks.”\n\n\n“All kinds of comprehension, it wasn’t a building in the sky. Without a solid base, even if you understood it, you could not produce it.”\n\n\n“Cultivation, it was to have a steady heart, and not waver!”\n\n\n“In any case, the barefoot weren’t afraid then those wearing shoes.”\n\n\n“Stop trying to convince everyone that you suck, focus on the positives“\n\n\n“As you get better, the goalposts get further away”\n\n\n“Your eye for mistakes grows as your skills do”\n\n\n“Sticky notes are a great way of telling you what you failed to do”\n\n\n“Relationships are about mutual giving — not about just avoiding everything you don’t both like equally.”\n\n\n“People, generally speaking, want outcomes. They don’t care how it’s done or what is used to get there. No one goes to the hardware store to buy a drill. They go there to buy holes.”\n\n\n“What’s the point of living your life with your hands and feet tied? You should create a sky shaking, earth shattering way for yourself!”\n\n\n“According to the history books, most of the falls of any ancient emperor were caused by their suspicions of their ministers; their suspicions made them ill and therefore led to their death”\n\n\n“The waves in the back always push the ones in the front in the Yangtze river, so of course each generation has to be stronger than the next”\n\n\n“The hardest part is not learning how to use the tools but know what to do with them”\n\n\n“The universe will never give you peace in something you were not meant to settle in”\n\n\n“When I was a teen, I’d self harm, but now I just occasionally inconvenience myself in different ways when I feel bad or guilty about something I’ve said or done.”\n\n\n“When we’re in a dangerous situation as a child, we’re not allowed to get angry because the abusers are dangerous. So we turn our anger back at ourselves. The abusers contribute to this on purpose, making us feel like we had some control over the situation when we did not. That’s what abusers do.”\n\n\n“You’re hurting yourself in an attempt to soothe yourself. And because you had abusive parents who were probably prone to punishment, you’re doing that to yourself. You are imitating them, what they did to you.”\n\n\n“They see me ho’ing, they hatin”\n\n\n“Thousands of miles fly by in a dream and thousands of years like a racing steed”\n\n\n“Taking rejections personally is the biggest enemy of a creative professional. Don’t let your work become your whole identity.”\n\n\n“This is your job, and as long as you’re being paid for it, just let it go.”\n\n\n“What horrifies me most is the idea of being\nuseless: well-educated, brilliantly promising, and\nfading out info an idifierent middle age.”\n\n\n“we have kitchens to clean because our bellies are full. we have beds to make because we did not have to sleep on floors. our laundry is piling because we have so many clothes to choose from and so many ways to make them dirty. our arms are full because our hearts are\nfull. celebrate the messy. celebrate every little thing that needs to be done. life is too short and too beautiful to let the little things slip away.”\n\n\n“It’s easy to say I wish my partner did more for me. It’s hard to do more for your partner”\n\n\n“If you stop and think about it, your life is a lot longer as an old guy than as a kid.”\n\n\n“To her, reuniting with her Aranara friend is the “result.” But recapturing her past joy and belief in herself is the “process,” and that’s where our help is needed.I’m guessing that her sudden illness made her feel like she may not be able to realize her dreams anymore. It also made it difficult for her to hold onto the happy memories and dreams she had.Everyone has their own imagination. After some chance coincidence, she met an “Aranara.”So as long as we help her rediscover that same feeling she once had, her “Aranara” will return naturally.Oh, so that’s what you’re thinking…Adults only want to believe in objective reality. In doing so, they may unintentionally do harm to the innocent fantasies of children.But I think there are ways to get even subjective things back.”\n\n\n“I’m so happy… I almost forgot how it feels to be this happy…You know, at first, it felt like my world had shrunk down to a tiny space.But as long as I continue moving forward, new sights will always appear in front of me, and my world will keep expanding before my eyes.Even though I don’t know how I did it, I know I have you all to thank.Oh, that’s not important. What’s important is that you’ve remembered the joy of going through the world.Our memories don’t just symbolize our past. They can also shine a light on our future.Once you find the hope in your heart again, that happiness will come back to you.”\n\n\n“Dreams themselves may be imaginary, but they’re also experiences that can never be relived or replicated.If we were sticklers about truth and fiction, we would’ve missed out on so much beauty and emotion.”\n\n\n“That might sound silly to you… Like, why do you have to be happy just because it is?\nBut that’s the magic of a smile. If you don’t believe me, try it. Look at it a few times every day, and you’ll understand.”\n\n\n“People use fireworks to remember their most precious memories, and these memories sparkle and shine each time the fireworks fly.\nIn other words, fireworks symbolize the past.\nAnd shooting stars make people think of wishes because wishes carry people’s brilliant hopes and expectations for the future.\nOne represents the past, and the other the future. They both bloom in the sky, but have completely different meanings behind them.”\n\n\n“Underneath their hard shell is often a vulnerable human, who fails to provide you with nurture because they never received it themselves. Their dysfunctional behaviours are rooted in a painful past. To these parents, their fundamental experience in life is that of being groundless. From a young age, they did not feel protected or guided but were ‘thrown into’ a precarious and scary world. They had to survive challenges, protect themselves and seek direction in the world while their parents remained weak or absent. At some point in their life, perhaps on an unconscious level, they decided they would make better parents to themselves than their real parents could. They took over the role and became the powerful figure they had been searching for. Since their own sense of invincibility is the only thing that they have ever been able to count on, they fiercely protect it with all they have. This is why they demand compliance from others to reinforce their authority and are extremely defensive and reactive to anything that threatens their sense of control. ”\n\n\n“One has never found oneself lacking in basic comforts. On the contrary, it is the gesture that one values above all else. So long as you’ve shown proper respect and consideration, the quantity or quality of the gift is but a trivial matter.”\n\n\n“A name is but a simple label we carry with us on our journey through the world. Why would one be offended by such a trivial matter?”\n\n\n“That is not to say that your words paint an inaccurate picture. One has always lived by a single ideal: eschew all action and abide by no rule. One does as one pleases and speaks as one pleases. Others may critique or praise as they see fit, yet one places little weight in such judgments.”\n\n\n“When dwelling between mountain and forest, away from the struggles and troubles of the mortal world, a mortal form is hardly the most fitting of choices.”\n\n\n“‘Tis a truth most evident: One always recognizes one’s own… no matter what form they may take.”\n\n\n“When it is time for one’s progeny to leave the nest, it is the responsibility of an elder to let them fly free. Yet when your wings grow weary and the night grows dark, just know that you always have a place to which to return. Tis a refuge referred to by many a name in mortal writing: “Home,” “nest,” “haven”… Whatever its denomination may be, its essence remains quite unchanged. One speaks, of course, of a place not unlike one’s own abode. One’s disciples are free to come and go as they wish, yet the door remains forever open to those who wish to return… One rather hopes you count yourself among them.”\n\n\n“Tears are a necessary part of maturation. Sometimes, there is scarcely a better vehicle to wash away the toll of stress and misery. Now that the issue has been resolved, you should also take a moment to relax. Give yourself some time to rest, take a nap if you must. One will wake you in due time.”\n\n\n“Every individual must find their own path to enlightenment. So long as one retains a pureness of spirit, one’s dietary proclivities are quite irrelevant.”\n\n\n“Not unlike the ocean tides, so too shall the movement of people ebb and flow. From turmoil to peace, enlightenment to aspiration — human society possesses limitless potential. In another thousand years, the scene we witness here may change in ways that are impossible for either of us to imagine.”\n\n\n“I’m a self-sufficient adult and abandonment no longer means the end of my life.”\n\n\n“It can be both aggravating and overwhelming when your parents repeatedly raise groundless fears, make false claims or subscribe to conspiracy theories. Perhaps you have realised by now that no amount of reassurance will ease their worries, but you may still be tempted to challenge their thinking or try to eradicate their fears with logic. Besides being futile, these efforts will likely backfire as your parents’ fears are real to them. In fact, these are the pillars of their existence. For many years, they have relied on a rigid and absolute way of feeling safe in the world. They hold onto their defensive system so tightly because if they don’t, their sense of self will crumble. Therefore, the more you try to challenge their views, the more defensiveness and pushback you will face from them. To have a productive interaction, try not to ridicule, tease or undermine their paranoia, or convince them that they may be wrong. Instead, allow them to have their say. If they try to force you to agree with them, it is within your rights to remain firm and honest. You do not have to share their beliefs, but you can validate their feelings, or remain non-reactive. On the other hand, bear in mind that when your parents are in a fearful place, their cognitive abilities will tend to regress, and they may not be capable of abstract thinking or logical reasoning. Therefore, when you speak, try to be clear, explicit and non-metaphorical, to reduce the chances of misinterpretation.”\n\n\n“It’s amazing how a little tomorrow can make up for a whole lot of yesterday”\n\n\n“To survive the lack of or inconsistent parenting input, we learned to withdraw our needs from others and even ourselves.”\n\n\n“We fear the thing we want the most.If our experience had taught us that being angry would lead to someone deserting us, or that our sadness was a burden, it makes sense that we default to hiding our feelings. We learn to shut off our emotions – first to others, then to ourselves, to prevent potential rejection or exile from the family and community”\n\n\n“As our need for love has been frustrated, we construct a facade of pretending not to have any needs, and eventually, we start to believe we really don’t need love. Then, we feel our lives to be flat and numb. To make up for our inner emptiness, we try to establish our values through ‘doing’ rather than ‘being’. We might be high-achievers in the professional arenas or appear successful, independent, and self-contained, but deep down our battle with perfectionism, shame and loneliness keep us away from living life fully. Our loneliness is perpetuated as we continue to live with a facade, rather than letting others see our raw, unedited self.”\n\n\n“When you have a grasp on eternity your eyes won’t ever see the battle or the lost people that hurt you. You will see a beautiful story of hope, in every character.”\n\n\n“We feel that we are not a big enough container for our dreams, and we are so used to the disquiet longings in solitude that the idea of getting what we want terrifies us.”\n\n\n“Instead of being fixated on the wrong that had been done to me— the obsession to return to innocence— I began to see my heartbreaks as necessary. Without them, I would never have known true belonging, which is inclusive of exile, not in spite of it.”\n\n\n“It may seem paradoxical at first glance, but the answer to healing from defensive non-attachment is actually to affirm our ultimate autonomy and resilience. We push away good things in life because deep down, we worry that we would not survive losses and heartbreaks. If we know we are strong enough to go through grieve, disappointment and heartbreaks, then placing our trust in someone’s hand would become much less threatening. To melt away our armour, we ought to feel safe and grounded within ourselves. We could allow ourselves to graduate from a child-like way of need, into a mature, grounded way of relating. As an adult, the basis of our courage to trust and to love does not lie in the hands of others but our strengths. It is not that we have the blind faith that others will not hurt, disappointment or betray us, but we trust that we could grieve, digest the disaster, and bounce back from it. Unlike the helpless child we once were, we are more resourceful, resilient and adaptable than we think we are. We do not have to fear dependency, for we are never truly dependent on another. We are both dependent and independent— and when the time comes, we can summon the strength that is needed to adapt. As children, we need from others utmost reliability, consistency and availability. As adults, we rely on our ability to self-contain and self-soothe. Unlike a child, we know that people can break promises, withhold their love, and change the way they act. But rather than counting on others to create a haven for us, we do that for ourselves. We no longer need our partner to guess our needs, fulfil our desires or stand up for us, but we can assert ourselves to the world. We may also become aware of our deprived needs as a child and the longings, and become our own best parents. We no longer live in fear of ‘being dropped’ as a baby would be; we stand on our own two feet. Rather than being pulled by insatiable hunger, we simply appreciate the love, attention and respect offered by others when they are freely given. Then, our understanding of relationships becomes much more nuanced We do not need absolute safety and certainty, we can hold the paradoxes of trust and disappointment, separation and attachment, and find our ways in the flux of life.”\n\n\n“To love at all is to be vulnerable. Love anything, and your heart will be wrung and possibly broken. If you want to make sure of keeping it intact you must give it to no one, not even an animal. Wrap it carefully round with hobbies and little luxuries; avoid all entanglements. Lock it up safe in the casket or coffin of your selfishness. But in that casket, safe, dark, motionless, airless, it will change. It will not be broken; it will become unbreakable, impenetrable, irredeemable. To love is to be vulnerable.”\n\n\n“He is terribly afraid of dying because he hasn’t yet lived.”\n\n\n“We can easily forgive a child who is afraid of the dark; the real tragedy of life is when men are afraid of the light.”\n\n\n“In Hyper-criticalness, you have designed your life to meet very high standards about most things. These might have been standards you have internalized from other sources like your parents or competitive schooling, but they now feel like ‘yours’. You constantly feel like you must be doing something, producing something, and achieving something. You struggle to slow down or relax. In order to meet your own standards, you may sacrifice health, leisure, and relationships. You are perfectionistic and would notice the smallest thing that does not align with the bigger picture. You might also be preoccupied with speed and efficiency and feel anxious if you think you might be wasting time. You may impose a lot of rules, moral standards, and ‘shoulds’ on yourself and others. Even you know being critical of others is detrimental to your relationship, you cannot help yourself.”\n\n\n“Things ain’t really been sweet for me, I lost that piece of me, I don’t think that’s peace for me, that might not be meant for me, my heart not something they need and your love not something I need, tough love when you come around me, I’m the bad guy if that’s what they need”\n\n\n“Are you looking to move onwards and upwards or run away? Look before you leap, you don’t want to appear the fool do you?”\n\n\n“The suffering manifested in your life is you taking on a portion of the collective darkness, you do not deserve this suffering, this is the same as a firefighter going to fight the flames\nIts even harder if you dont remember you are a firefighter and you find yourself amidst the flames, if you can face the pain in your life with positivity, with light, you are helping this entire planet, every single human being is being helped by that, does this make sense?”\n\n\n“You are the Universe in the way that a wave is to the ocean. You are of the ocean but a wave.”\n\n\n“We seldom see where the chapters of our lives begin and end until we are gifted the benediction of hindsight. So I’m having my adventures and moving further on the path so I can get that hindsight.”\n\n\n“Something I remind myself often is that rejection is simply a redirection.”\n\n\n“Dreams are like paper kites, with them do our hopes take flight, sailing high above the clouds, they yearn for something more profound,,yet try we may and try we might, a deeper truth waits in plain sight, though we hang our hopes in skies abound, many joys lie on the ground.”\n\n\n“Humans have nothing but unknowns. Even if they act like they know everything, that’s surely a lie. That’s why there’s no way but to spend your whole life learning it. There’s plenty of wisdom you won’t find in a book, and I agree with your opinion, Lyle.”\n\n\n“Humans, you know. Someone who wasn’t standing out up to now, when you leave work to them, you see they can suddenly accomplish it? Yeah, that happens. Up to that point, they thought it was fine if they didn’t do anything. But when work’s left to them, they suddenly feel no one but them can do it. That type.”\n\n\n“Lyle, don’t think everyone’s the same as you. Without thinking of the consequences, there will surely be some who’ll attack you just because you look like you have money. Make sure you’re firm with monetary exchanges. There were lots of times when I had to give out rewards, and when you end up in that position, it’s easy to understand. If you’re not reliable in such fields, it will affect your credibility.”\n\n\n“Don’t just assume that everyone around you is a wise guy like you.”\n\n\n“Even here in the Underworld, everybody—even monsters—needed a little attention once in a while.”\n\n\n“For some reason sugar and caffeine always calmed down my hyperactive brain.”\n\n\n“Percy, the hardest part about being a god is that you must often act indirectly, especially when it comes to your own children. If we were to intervene every time our children had a problem … well, that would only create more problems and more resentment. But I believe if you give it some thought, you will see that Poseidon has been paying attention to you. He has answered your prayers.”\n\n\n“My point is you heroes never change. You accuse us gods of being vain. You should look at yourselves. You take what you want, use whoever you have to, and then you betray everyone around you.”\n\n\n“The most dangerous flaws are those which are good in moderation”\n\n\n“You see, in times of trouble, even gods can lose faith. They start putting their trust in the wrong things. They stop looking at the big picture and start being selfish. But I’m the goddess of marriage, you see. I’m used to perseverance. You have to rise above the squabbling and chaos, and keep believing. You have to always keep your goals in mind.”\n\n\n“Maybe that’s why monsters fade,” I said. “Maybe it’s not about what the mortals believe. Maybe it’s because you give up on yourself.”\n\n\n“It isn’t easy being a brilliant inventor,” Hephaestus rumbled. “Always alone. Always misunderstood. Easy to turn bitter, make horrible mistakes. People are more difficult to work with than machines. And when you break a person, he can’t be fixed.”\n\n\n“I don’t know if Daedalus will help you, lad, but don’t judge someone until you’ve stood at his forge and worked with his hammer, eh?”\n\n\n“A real artist must be good at many things.”\n\n\n“I picked up Pandora’s jar. The spirit of Hope fluttered inside, trying to warm the cold container. “Hestia,” I said, “I give this to you as an offering.” The goddess tilted her head. “I am the least of the gods. Why would you trust me with this?” “You’re the last Olympian,” I said. “And the most important.” “And why is that, Percy Jackson?” “Because Hope survives best at the hearth”\n\n\n“there are times when you have to light one fire to put out another. There are no drugs that will make you immune to stress or to pain, or that will by themselves magically solve your life’s problems or promote healing. It will take conscious effort on your part to move in a direction of healing, inner peace, and well-being. This means learning to work with the very stress and pain that are causing you to suffer.”\n\n\n“You can’t sail straight into the wind, and if you only know how to sail with the wind at your back, you will only go where the wind blows you. But if you know how to use the wind’s energy and are patient, you can sometimes get where you want to go. You can still be in control.”\n\n\n“To a great extent, our ability to influence our circumstances depends on how we see things. Our beliefs about ourselves and about our own capabilities as well as how we see the world and the forces at play in it all affect what we will find possible. How we see things affects how much energy we have for doing things and our choices about where to channel what energy we do have.”\n\n\n“Some of our biggest stresses actually come from our reactions to the smallest, most insignificant events when they threaten our sense of control in one way or another: the car breaking down just when you have someplace important to go, your children not listening to you for the tenth time in as many minutes, long lines at the supermarket checkout.”\n\n\n“As is so often the case, the public hero that others admire can leave quite a trail of private hurt in his wake.”\n\n\n“life is always in flux, that everything we think is permanent is actually only temporary and constantly changing. This includes our ideas, our opinions, our relationships, our jobs, our possessions, our creations, our bodies, everything.”\n\n\n“Each person, without exception, has a unique story that gives meaning and coherence to that person’s perception of his or her life, illness, and pain, and what he or she believes is possible.”\n\n\n“You can observe a lot by just watching.”\n\n\n“a map is not the territory it portrays. ”\n\n\n“Much of the time you may get away with being only partially conscious like this. At least it seems that way. But what you are missing is more important than you realize. If you are only partially conscious over a period of years, if you habitually run through your moments without being fully in them, you may miss some of the most precious experiences of your life, such as connecting with the people you love, or with sunsets or the crisp morning air.”\n\n\n“In reality we are being driven by our likes and dislikes, totally unaware of the tyranny of our own thoughts and the self-destructive behaviors they often result in.”\n\n\n“The means and the end of meditation are really the same.”\n\n\n“Patience can be a particularly helpful quality to invoke when the mind is agitated. It can help us to accept this wandering tendency of the mind while reminding us that we don’t have to get caught up in its travels. Practicing patience reminds us that we don’t have to fill up our moments with activity and with more thinking in order for them to be rich. In fact, it helps us to remember that quite the opposite is true. To be patient is simply to be completely open to each moment, accepting it in its fullness, knowing that, like the butterfly, things can only unfold in their own time.”\n\n\n“open, “beginner’s” mind allows us to be receptive to new possibilities and prevents us from getting stuck in the rut of our own expertise, which often thinks it knows more than it does. No moment is the same as any other. Each is unique and contains unique possibilities. Beginner’s mind reminds us of this simple truth.”\n\n\n“Perhaps it’s time to acknowledge that escaping into the woods as a response to a typical, innocently asked Monday-morning question is not a viable option. Instead, I will have to respond.”\n\n\n“Goodness me, the social faux pas were I to respond honestly!”\n\n\n“After all, if they truly wanted me to do something, wouldn’t they ask, rather than just hint in such a vague way? The whole thing is so fraught with uncertainty: even if I had spotted the implication, it’s likely I’d then spend a frantic few moments second-guessing myself in a panic – ‘But what if they don’t mean that and I end up looking presumptuous?’ – by which time they’ve probably given up and closed the window themselves.”\n\n\n“By responding with a ‘Fine, thanks’, I will have lied twice. Once with the ‘fine’ – my weekend was awful and I’m still feeling terrible about it (hence the huge coffee) – and second with the ‘thanks’ – I’ve nothing to be thankful for here: they’ve just forced me to lie about my feelings when I would rather have said nothing at all.”\n\n\n“It’s a process I’m very familiar with, whereby I consider every possible likely outcome that I can imagine, and try to figure out how I’ll cope with it should it come to pass, rather like when Doctor Strange visits millions of future timelines searching for the one where the Avengers win the day.”\n\n\n“We tend to slide from crisis to crisis when talking to people, and behind it all is a brain whirring over the potential problems, analysing every facet.”\n\n\n“The fact is, there’s no discernible logic to turn-taking in neurotypical conversation. It just happens, and it happens fairly well, with interactions only occasionally going wrong. If”\n\n\n“The problem is that the earnest zeal with which we approach our favourite things is very rarely matched by the neurotypical listener. I’ve often pondered whether neurotypical people are even capable of the same intense level of interest in a topic.”\n\n\n“We may struggle to identify others’ emotions (and our own, truth be told), but I wager most autistic people will eventually notice the boredom on the face of the person we’re sharing with – usually because it’s paired with them telling us to shut the hell up. This hurts. We love our interests and much of the time see them as one of the only topics worth discussing – I mean, surely it’s better than small talk? – and talking about them is so intensely joyful, as well as being cathartic and stress-relieving.”\n\n\n“It always feels to me that the neurotypical world puts arbitrary limits on how passionate one is ‘allowed’ to be about a subject. Crossing this line is a social faux pas that ranks somewhere around telling inappropriate jokes at a funeral, but it isn’t based on anything real or important.”\n\n\n“though autistic people may struggle to understand neurotypical viewpoints, neurotypicals have just as much difficulty understanding us. The difference is that we’re hyper-aware of our struggle and go out of our way to compensate for the difference, while you lot (with the greatest respect) don’t seem to have a bloody clue.”\n\n\n“There’s something about eye contact in Western society that’s peculiar, at least to my autistic eyes. It seems to be held in such high regard for something so fleeting and ambiguous, and the general rule of thumb appears to be that eye contact equals trustworthiness. Far be it from me to question such a bizarre belief (as if liars are incapable of eye contact, and that’s their singular weakness …); it’s enough to note that there are many good reasons why eye contact may be impossible at any given moment, and that placing such high value upon it might therefore be rather ill-advised.”\n\n\n“Masking is something that almost all autistic people will learn to do at some point in their life. It often begins in childhood when we realise that something is apparently ‘wrong’ with us. We notice that our social skills don’t seem to cut it, that we’re frequently at a loss to understand what’s going on, and that our attempts to make and keep friends are clumsier or less successful than our peers. We learn that the depth of our interests and the way we express that passion is unacceptable to everyone else, and that our sensory sensitivity annoys people who don’t seem to ever want to understand it. Frequently, we’re mistreated because of all or some of these things; sometimes we’re bullied; occasionally even abused. In this apparently life-or-death situation, it becomes clear to us that we’re going to have to adapt, and so, usually by ourselves and with very little assistance from anyone else, we learn to mask. We learn to adopt a kind of persona – based on all the things our extremely observant brains have noticed in other people – in order to please those around us so they stop bullying us or causing us harm.”\n\n\n“Autistic people frequently report using elements of personalities they observe – a friend, perhaps, or even a favourite fictional character – as building blocks for their mask, almost as if we’re constructing them out of LEGO bricks, and I can certainly empathise with this. I would study (still do, though at least now I’m more aware of what I’m doing) personalities with the care of a collector, trying things on like a shopper in the market for a new pair of jeans.”\n\n\n“Often – and I did this myself, without being aware of it – we instinctively relax a little on finding out we’re autistic. There’s a strange catharsis in finding out you’re neurodivergent, a kind of epiphany that there’s a reason why things are the way they are. As a result, we exhale for the first time in years, lean into our autistic traits a little, let our mask slip and … well, we pay the price almost immediately. We learn very quickly that our unmasked selves are simply not welcome and so we hurriedly fix our masks back on – nail them firm for fear of them dislodging – and realise we’ll never be free to be ourselves. I think of all the concepts elaborated in this book,”\n\n\n“(it’s amazing how cruel a person can be to themselves when trying to explain behaviour caused by a disability they don’t know they have).”\n\n\n“Autistic people must be allowed to self-isolate in order to recuperate.”\n\n\n“Phone calls are like entertainment radio: dead air is a crime.”\n\n\n“Autistic people regularly report feeling like ‘emotional sponges’ – upon walking into a room where some kind of conflict has taken place, we’ll so frequently immediately take on board all of that negative emotion (some might call it the ‘vibe’ or ‘atmosphere’) that it can result in our having to sit down, or even bursting into tears.”\n\n\n“Many autistic people report having what we’ve collectively labelled ‘hyper-empathy’ – a kind of extreme set of emotional responses to people and animals in dangerous or upsetting situations. I mentioned my strong response to seeing children in distress earlier, but it’s by no means limited to that. Other interesting features of this type of autistic empathy might include extreme empathy (and I mean extreme) towards animals.”\n\n\n“For now, the myth of autistic introversion endures, and sociable autistic people continue to be tarred by that brush, from school age all the way up to retirement. Autistic children at school, spending every lunch and breaktime alone hiding in the library or in some undiscovered nook under the stairs, are not seen as sources for concern. ‘It’s OK,’ teachers may reason, ‘they’re autistic.’ As such they fly low and under the radar, their intense loneliness eventually metastasising into something more dangerous – depression.”\n\n\n“we seek to replicate success when we experience it, right down to the minutiae.”\n\n\n“Everyone has a fairly standard morning routine of course – the classic ‘shit, shower, shave’ is testament to that, though why anyone would shave after a shower is beyond me. But how many of these people would fear their entire day is going to come crashing down if they accidentally or by necessity swap around the order a little, or, worse, miss something out altogether? If I miss a segment of my morning ritual – for example, the bit where I sit down on the living-room sofa with a coffee and check Twitter groggily – then I’ll feel intensely uncomfortable for hours after, a similar feeling to knowing you’ve left the gas hob on and you’re two hours from home, a kind of jumpy, extreme anxiety based on a fear that something truly terrible will ensue from your own foolishness.”\n\n\n“It takes enormous amounts of time and energy to switch focus as an autistic person. I’ve likened it in the past to turning circles for vehicles. Neurotypicals are able to switch tasks as easily as a car can make a U-turn. Autistic people, on the other hand, seem to make U-turns at the same pace as an ocean liner, requiring huge amounts of patience.”\n\n\n“An autistic person will have their issues around changing tasks and changing focus, but will eventually manage these on their own terms (with care and a peaceful setting, ideally). Interrupt that slow, gentle process with an external question, demand or event (a partner asking for something, a phone ringing, a knock at the door), however, and all hell breaks loose internally. That slow process, the cruise liner turning about – as painstaking as carefully untangling Christmas lights in early December – is suddenly broken. Our hyperfocus is disturbed and our mood follows a predictable path. The result is likely to be anger, irritation, despair or actual pain; the social relationship with the person making the demand is tarnished and possibly broken, and the cycle continues towards desperately trying to avoid that situation ever, ever happening again. PDA as we know it is born.”\n\n\n“I suppose this is a big part of the problem: we autistic people find little workarounds, tricks and bespoke solutions to our personal difficulties. Yet these are often ever-so-slightly odd, meaning that when neurotypical people spot us in the act they may assume that we’re up to no good, in that peculiarly pessimistic way that they have.”\n\n\n“Beyond the need for a place to hide and recuperate, special interests enable us to focus our brains in a strangely pleasurable way. When the constant demand of looking at the ‘big picture’ in life gets too much and too boring, the ability to tweak our brain’s lens to precisely focus our laser-like attention on something specific is a wonderful feeling, rather like it’s allowing our brain to do what’s natural for it, rather than expecting it to cope with the wide view, which can feel so … false.”\n\n\n“We approach the world like laser beams, I suppose, rather than wider car headlights or floodlights, with everything within the narrow focus of our attention drilled down into its very depths. And this”\n\n\n“Autism is called an ‘invisible disability’, and sometimes I think that term might be more literal than we realise.”\n\n\n“Because of how poorly the school was accommodating my particular neurodivergence, I was forced to rebel and lie and dissemble, despite the fact that doing so went against every law-abiding, if clumsy, bone in my body. Why couldn’t they allow me to play sports that weren’t team games – like badminton or tennis – or ones that were more ordered and structured, like athletics? Why was it always football and rugby, for goodness’s sake?”\n\n\n“No one ever asked me if I really wanted to do it; but then, I didn’t ask myself either.”\n\n\n“Authority and hierarchy are social constructs and, as I’ve been at pains to point out, autistic people have our own, different, culture that doesn’t seem to include it.”\n\n\n“And so, like an archaeologist drawing conclusions about the past from the evidence of the present day, I can say with some certainty that I must have realised that I had to hide my stims at some point. Why else would they be so subtle, so ‘inoffensive’?”\n\n\n“Autistic people often seem to have a very deep and strong sense of what’s right, what’s reasonable and what’s fair. This is not to say that we’re unerring moral arbiters; after all, our sense of what’s fair may be affected by any number of factors, such as privilege, experience, upbringing and so on, and therefore not match others’ opinions and values. Nevertheless, the strength of feeling and conviction is likely to be a feature for many autistic people.”\n\n\n“In my experience, autistic people are good pattern spotters and often excellent at working out the rules of any given situation. This is what makes us so good at masking, after all. We figure out the rules, and then we play the game. The problem is that figuring out the rules doesn’t prepare you for how to handle those who decide to cheat.”\n\n\n“I find that when my stress levels reach a particular point, my voice begins to falter as a tool, becoming less reliable and less focused, and I begin to lose my vocabulary and grammar. The spoken word is by no means guaranteed in the autistic community, and we don’t deserve to be overlooked as a result of this.”\n\n\n“When the ears of the student are ready to hear, then cometh the lips to fill them with wisdom.”\n\n\n“The lips of Wisdom are closed, except to the ears of Understanding.”\n\n\n“Everything is Dual; everything has poles; everything has its pair of opposites; like and unlike are the same; opposites are identical in nature, but different in degree; extremes meet; all truths are but half-truths; all paradoxes may be reconciled.”\n\n\n“Everything flows, out and in; everything has its tides; all things rise and fall; the pendulum-swing manifests in everything; the measure of the swing to the right is the measure of the swing to the left; rhythm compensates”\n\n\n“To the pure, all things are pure; to the base, all things are base.”\n\n\n“Nothing endures but Change.”\n\n\n“And still more presumptuous are those who attempt to ascribe to THE ALL the personality, qualities, properties, characteristics and attributes of themselves, ascribing to THE ALL the human emotions, feelings, and characteristics, even down to the pettiest qualities of mankind, such as jealousy, susceptibility to flattery and praise, desire for offerings and worship, and all the other survivals from the days of the childhood of the race. Such ideas are not worthy of grown men and women, and are rapidly being discarded.”\n\n\n“Nothing can rise higher than its source—nothing is evolved unless it is involved—nothing manifests in the effect, unless it is in the cause.”\n\n\n“But do not yield to the temptation which, as The Kybalion states, overcomes the half-wise and which causes them to be hypnotized by the apparent unreality of things, the consequence being that they wander about like dream-people dwelling in a world of dreams, ignoring the practical work and life of man, the end being that “they are broken against the rocks and torn asunder by the elements, by reason of their folly.”\n\n\n“Transmutation, not presumptuous denial, is the weapon of the Master.”\n\n\n“And, in the degree that Man realizes the existence of the Indwelling Spirit immanent within his being, so will he rise in the spiritual scale of life. This is what spiritual development means—the recognition, realization, and manifestation of the Spirit within us.”\n\n\n“I must create a system or be enslaved by another man’s; I will not reason and compare: my business is to create.”\n\n\n“Disney had reinterpreted Christianity for mass culture.”\n\n\n“In numerous ways Disney struck what may be the very fundament of entertainment: the promise of a perfect world that conforms to our wishes.”\n\n\n“In an idealized world where wish fulfillment prevailed, Disney had consistently concretized the ideal and provided the pleasure of things made simple and pure the way one imagined they should be, or at least the way one imagined they should be from childhood.”\n\n\n“Whether in his movies or in his theme parks, Disney always promised a fantasy in which one could exercise the privileges of childhood—privileges he never abandoned in his own life. This will to power also explained why animation was his preferred medium. In animation one took the inanimate and brought it to life, or the illusion of life. In animation one could exercise the power of a god.”\n\n\n“Disney’s best animations—Snow White and the Seven Dwarfs, Pinocchio, Bambi, and Dumbo—were archetypal expressions of this idea. In large measure, they were about the process of a child making his or her claim upon the world, about the process of overcoming obstacles to become whatever he or she wanted to be.”\n\n\n“‘In this life you only have a set number of chances. If you grab them when they appear, you’ll succeed. If you don’t, you’ll be doomed to a life of mediocrity.‘”\n\n\n“Isn’t it enough to see that a garden is beautiful without having to believe that there are fairies at the bottom of it too?”\n\n\n“I mean, what’s the use of our sitting up half the night arguing that there may or may not be a God if this machine only goes and gives you his bleeding phone number the next morning?”\n\n\n“Perhaps I’m old and tired,” he continued, “but I always think that the chances of finding out what really is going on are so absurdly remote that the only thing to do is to say hang the sense of it and just keep yourself occupied. Look at me: I design coastlines. I got an award for Norway.”\n\n\n“The History of every major Galactic Civilization tends to pass through three distinct and recognizable phases, those of Survival, Inquiry and Sophistication, otherwise known as the How, Why and Where phases. “For instance, the first phase is characterized by the question How can we eat? the second by the question Why do we eat? and the third by the question Where shall we have lunch?”\n\n\n“if life is going to exist in a Universe of this size, then the one thing it cannot afford to have is a sense of proportion.”\n\n\n“The trouble with most forms of transport, he thought, is basically that not one of them is worth all the bother.”\n\n\n“Life was, in short, ridiculously easy and for a while at least they were able to cope with the problems of aimlessness and isolation by deciding to ignore them. When the craving for company became too great they would know where to find it, but for the moment they were happy to feel that the Golgafrinchans were hundreds of miles behind them.”\n\n\n“History is never altered you see, it just fits together like a jigsaw. Funny old thing, life, isn’t it?”\n\n\n“Sometimes, all it takes is taking a leap of faith and enjoying the adventure.”\n\n\n“Elden Ring’s story will still have its special place in my heart. The whole story about this world is a story of gods, greater beings so above the little mortals they pretend to protect, but the end if the game is precisely about the opposite, those small beings each having their own wishes and desires, and deciding what the world should be instead of those eldritch entities. You can rule by yourself an imperfect world, after the greater will is forced to renounce his control over life or death to keep itself here. A scorned woman can choose her own god of undeath, to change the place of the tarnished as oppressed to rulers. The mad prisoner can take his revenge upon the world, by cursing all things so the greater being lose all control over the souls of this land. A silent prophet, with no name for himself, can repare the golden order by himself at the cost of his life, when a goddess like Marika was never capable of it. You and Ranni, the demigoddess spurning the control of the greater will, can together take the gods out of this land to travel far away, and let the Lands Between decide its own fate. Or you can make true the wish of the burning eyed believers, and scorch the whole world to return to the origin of life itself, the ultimate price to shatter the greater will invasion.”\n\n\n“The game make you think it’s a story about gods, Marika’s bloodline, heroes of forgotten time, but it’s in fact a story of how those small nobodies spurned by grace that are the Tarnished will have the last words, at the end of a long journey.”\n\n\n“Not putting your heart and mind into sword practice makes one merely a slave of the sword”\n\n\n“King Calm Sea once told me”—Fairy Meng looked at Meng Chuan and continued—“that cultivation requires one to follow one’s intuition and follow what one likes the most before forging forward. You will go further and further. When you look back after a few decades, you would’ve far surpassed your former self. I’ll share this saying with you as well. Follow your heart and proceed down what you like the most.”\n\n\n“Qi will only move when the mind moves. If the mind does not move, then Qi will also remain still!”\n\n\n“Okay, stop getting downcast. If you can’t figure things out at the moment, stop mulling over it and think about it again when your head is clearer. Like the old people say, even if the sky collapses, there’s nothing to be afraid of. If worst comes to worst, may it be.”\n\n\n“To elevate yourself as a sheepdog amongst a herd of sheeps, you have to understand the rules set by the shepherd. Only if you can understand the rules, can you become a shepherd. If you can’t do either, then your only fate is to toil all your life in a sheep pen. If the shepherd is cold, your wool will be shaved for him. This is called selflessness. If the shepherd is hungry, you shall lay on the butcher block and scream your devotion for his cause”\n\n\n“Three millennia of recorded history, fraught with power, lust, and greed. Ninety thousand miles of meditation, only to return to gardening, wine and poetry”\n\n\n“A man does not mature into pragmatism; he simply accepts reality more readily. ”\n\n\n“If love lives on sharing and chains you like fetters\nThen hate, more than anything, depends on freedom.”\nLove and hate coiling and twisting together. Tell me, what can I do to save you?\nIf love is spilt milk, Then who can protect whom?\nFor whom will my heart wait? Tell me, what can I do to save you?\nIf passion is a fatal poison,\nThen who can protect whom? How can I make this love immortal?”\n\n\n“The maturity of a man ought not to come from his own pragmatism, but rather from his ability to accept this trait shown in others. To become gentle, gentle toward others, to let go of grudges and to look at the world with a pair of sympathetic eyes and a compassionate heart. It was not about how many epigrams one could spew out to indicate that they had reached a higher level of wisdom, or about one’s ability to convince others with impressive speech, but rather about the habit of blaming others less and understanding them more. The true sign of maturity was tolerance and the absence of animosity… Those from the top did not compete, whereas those at the bottom fought for every inch. A man of virtue does not show it, whereas those purporting it will flaunt constantly.”\n\n\n“the freight of friendship sails for the distance, and the solidarity between brothers outlives heaven and earth…”\n\n\n“He had learnt that the most important trait of maturity was to be able to joke about everyone, including himself.”\n\n\n“Being a guardian is all about protecting the way, seeing the way and focusing on it. If Heaven’s Laws shows you the way, then what is there to doubt, even when facing death? Good luck”\n\n\n“When it comes to helping others, it is the thought that matters. Kindness, no matter how insignificant it can seem at the moment, can often steer one’s life into another, and usually better direction.”\n\n\n“Though bones and minds were chained\nYou cut down the thorns again and again\nYour pride will not be slain\nFrom the heart to the veins”\n\n\n“Our journey begins anew. Life passes like a fleeting rain, eventually merging with the ground. May our next encounter be… under a clear sky.”\n\n\n“Our existence is fleeting as dawn’s dew, destined for oblivion. On the still waters of oblivion, I guide the wandering souls. ”\n\n\n“I weep for the departed… Stream forth…\nThe gleam of old blades\nRestore this lost memory…\nApply, your color.”\n\n\n“Everyone has a past. But for some, their past is a silent abyss filled with those who drowned in it”\n\n\n“Do not go searching for what doesn’t want to be found. For in order to not be found, what doesn’t want to be seen will go through extreme measures to make sure that all who go searching… will wind up not being found.”\n\n\n“The sun sets above the blue mountain, the autumn moon with the wind of spring. The morning is fine like hair and night is like snow, whether you succeed or fail when you look back there’s nothing left.”\n\n\n“Painting is just that. Taking what looks shit, look less shit until it suddenly starts to look good.”\n\n"},"KB/random-dump":{"title":"Random Dump","links":[],"tags":["temp"],"content":"Random Dump"},"KB/regularize":{"title":"regularize","links":["KB/AdaIn","KB/Adam","KB/Batch-Normalization","KB/DeepNorm","KB/Dropout","KB/Effects-of-Regularization","KB/Fine-Tuning-Based-Pruning","KB/Global-Gradient-Magnitude-Based-Pruning","KB/Global-Magnitude-Based-Pruning","KB/He-Initialization","KB/Label-Smoothing","KB/Layer-Normalization","KB/Layerwise-Gradient-Magnitude-Based-Pruning","KB/Layerwise-Magnitude-Based-Pruning","KB/LeCun-Init","KB/Leaky-Relu","KB/Learning-Rate-Range-Test","KB/Lp-Regularization","KB/Mixup","KB/Modality-Dropout","KB/No-bias-decay","KB/Optimizers","KB/Orthogonal-Initialization","KB/Pruning","KB/Random-Pruning","KB/Regularization-Term","KB/Regularization","KB/Scheduling","KB/Scoring-Pruning-Approaches","KB/Structure-Based-Pruning","KB/Tuning-Model-Flexibility","KB/VariationalRecurrent-Dropout","KB/Weight-Decay-Vs-L2-Regularization","KB/Xavier-Initialization"],"tags":["anchor"],"content":"\nAdaIn\nAdam\nBatch Normalization\nDeepNorm\nDropout\nEffects of Regularization\nFine Tuning Based Pruning\nGlobal Gradient Magnitude Based Pruning\nGlobal Magnitude Based Pruning\nHe Initialization\nLabel Smoothing\nLayer Normalization\nLayerwise Gradient Magnitude Based Pruning\nLayerwise Magnitude Based Pruning\nLeCun Init\nLeaky Relu\nLearning Rate Range Test\nLp Regularization\nMixup\nModality Dropout\nNo bias decay\nOptimizers\nOrthogonal Initialization\nPruning\nRandom Pruning\nRegularization Term\nRegularization\nScheduling\nScoring Pruning Approaches\nStructure Based Pruning\nTuning Model Flexibility\nVariationalRecurrent Dropout\nWeight Decay Vs L2 Regularization\nXavier Initialization\n"},"KB/residual-flows":{"title":"residual flows","links":["KB/Res-Net","KB/Banach-fixed-point-theorem","lipschitz-constant"],"tags":["architecture"],"content":"Residual Flows\n \n\ninspiration from Res Net\nuse Banach fixed point theorem\n\nInverting Residual Network Layers\n\nform h&#039; = h + f[h, \\phi] , if we ensure that f[h, \\phi] is a contraction mapping (Banach fixed point theorem)\nlipschitz constant must be less than 1\nAssuming that the slope of the activation functions is not greater than one, this is equivalent to ensuring the largest eigenvalue of each weight matrix \\Omega must be less than 1\n\n"},"KB/robotics":{"title":"robotics","links":["KB/Action-Component","KB/Active-Compliant-Robot","KB/Actuator","KB/Affordance-Detection-Task-Specific","KB/Anthropomorphic","KB/Articulated-Manipulator","KB/Articulation","KB/Assembly-Robot","KB/Average-Number-of-Stored-Instances-per-Category","KB/Bag-of-Words-robotics","KB/Base-Link","KB/Burn-in","KB/Carousel","KB/Centrifugal-Force","KB/Circular-Motion-Type","KB/Clamp","KB/Clamping","KB/Compliant-Robot","KB/Contact-Sensor","KB/Continuous-Path","KB/Cyclo-Drive","KB/Cylindrical-Topology","KB/Degrees-of-Freedom","KB/Direct-drive","KB/Drop-Delivery","KB/Dual-memory-Approach","KB/Enabling-Device","KB/End-effector","KB/Endpoint","KB/Ensemble-of-Shape-Functions","KB/Eye-to-hand-System","KB/Familar-Object-Grasping-Object-Viiew-recog","KB/Fine-grained-Object-Recognition","KB/Forgetting","KB/Forward-Kinematic-Solution","KB/Forward-Kinematics","KB/GGCNN","KB/GRConvNet","KB/Gantry-Robot","KB/Global-Classification-Accuracy","KB/Grasp-Point-Detection","KB/Gravity-Loading","KB/Gripper","KB/Hand-Guiding","KB/Harmonic-Drive","KB/Harness","KB/Heightmaps-Kinesthetic","KB/Humanoid-Vision-Engine","KB/Inductive-Sensor","KB/Instance-based-Learning","KB/Instruction-Cycle","KB/Inverse-Kinematics","KB/Inverse-Reinforcement-Learning","KB/Iterative-Closest-Point","KB/Joint-Interpolated-Motion","KB/Joint-Motion-Type","KB/Joint-Space","KB/Joint-Velocity","KB/Kalman-Filter","KB/Kinesthetic-Teaching","KB/Ladle-Gripper","KB/Learning-Component","KB/Learning-to-Detect-Grasp-Affordance","KB/Linear-Interpolated-Motion","KB/Load-Cycle-Time","KB/Local-Descriptor","KB/Local-Reference-Frame","KB/Local-LDA-Object-Representation","KB/MVCNN","KB/MVGrasp","KB/Magnetic-Detectors","KB/Manipulator","KB/Material-Processing-Robot","KB/Mirror-Shift-Function","KB/Mode-Switch","KB/MoreMVCNN","KB/Neural-Radiance-Field","KB/Occlusion","KB/Opportunistic-Learning","KB/Optical-Encoder","KB/Optical-Proximity-Sensors","KB/OrthographicNet","KB/Palletizing","KB/Parallel-Shift-Function","KB/Particle-Filter","KB/Pendant-Teaching","KB/Perception-Component","KB/Perceptual-Messages","KB/Phases-of-Simulated-User-Experiments","KB/Pick-and-Place-Cycle","KB/Pinch-Points","KB/Point-to-Point","KB/Polynomial-Trajectories","KB/Power-and-Force-Limiting-(PFL)","KB/Presence-sensing-Safeguarding-Device","KB/Prismatic-Joint","KB/Programmable-Logical-Controller-(PLC)","KB/Proximity-Sensor","KB/Pulse-Coordinates","KB/Quadratic-Potential-Field","KB/Quasi-static-Clamping","KB/RANSAC","KB/Reasoning-Component","KB/Revolute-Joint","KB/Risk-Mitigation","KB/Robot-Range-Limit-Monitoring","KB/Robotic-Joints","KB/Roll","KB/Rotary-Joint","KB/Rotary-Vector-Drive-(RV)","KB/Safeguard","KB/Safety-Integrity-Level","KB/Safety-Logic-Circuit","KB/Semantic-Data","KB/Sense-Plan-Act-Model","KB/Sensory-Feedback","KB/Servo-Control","KB/Servo-Motor","KB/Servo-Pack","KB/Servo-controlled-Robot","KB/Servo-system","KB/Shock-Detection-Function","KB/Singularity","KB/Softlimit-Setting-Function","KB/Spline-Motion-Type","KB/Spline","KB/TSDF","KB/Task-(endeffector)-Space-Vs-Joint-Space","KB/Teach-Lock","KB/The-Repulsive-Potential","KB/Through-beam","KB/Time-Measuring-Function","KB/Trajectory-Planning","KB/Transducer","KB/Trapezoidal-Trajectory","KB/Unet-Grasping","KB/Vacuum-Cup-Hand","KB/Viewpoint-Feature-Histogram","KB/Visual-Servo-System","KB/Volumetric-Grasping-Network","KB/Work-Envelope","KB/Wrist","KB/Yaw"],"tags":["anchor"],"content":"\nAction Component\nActive Compliant Robot\nActuator\nAffordance Detection Task Specific\nAnthropomorphic\nArticulated Manipulator\nArticulation\nAssembly Robot\nAverage Number of Stored Instances per Category\nBag of Words robotics\nBase Link\nBurn-in\nCarousel\nCentrifugal Force\nCircular Motion Type\nClamp\nClamping\nCompliant Robot\nContact Sensor\nContinuous Path\nCyclo Drive\nCylindrical Topology\nDegrees of Freedom\nDirect-drive\nDrop Delivery\nDual-memory Approach\nEnabling Device\nEnd-effector\nEndpoint\nEnsemble of Shape Functions\nEye-to-hand System\nFamilar Object Grasping Object Viiew recog\nFine-grained Object Recognition\nForgetting\nForward Kinematic Solution\nForward Kinematics\nGGCNN\nGRConvNet\nGantry Robot\nGlobal Classification Accuracy\nGrasp Point Detection\nGravity Loading\nGripper\nHand Guiding\nHarmonic Drive\nHarness\nHeightmaps Kinesthetic\nHumanoid Vision Engine\nInductive Sensor\nInstance-based Learning\nInstruction Cycle\nInverse Kinematics\nInverse Reinforcement Learning\nIterative Closest Point\nJoint Interpolated Motion\nJoint Motion Type\nJoint Space\nJoint Velocity\nKalman Filter\nKinesthetic Teaching\nLadle Gripper\nLearning Component\nLearning to Detect Grasp Affordance\nLinear Interpolated Motion\nLoad Cycle Time\nLocal Descriptor\nLocal Reference Frame\nLocal-LDA Object Representation\nMVCNN\nMVGrasp\nMagnetic Detectors\nManipulator\nMaterial Processing Robot\nMirror Shift Function\nMode Switch\nMoreMVCNN\nNeural Radiance Field\nOcclusion\nOpportunistic Learning\nOptical Encoder\nOptical Proximity Sensors\nOrthographicNet\nPalletizing\nParallel Shift Function\nParticle Filter\nPendant Teaching\nPerception Component\nPerceptual Messages\nPhases of Simulated User Experiments\nPick and Place Cycle\nPinch Points\nPoint-to-Point\nPolynomial Trajectories\nPower and Force Limiting (PFL)\nPresence-sensing Safeguarding Device\nPrismatic Joint\nProgrammable Logical Controller (PLC)\nProximity Sensor\nPulse Coordinates\nQuadratic Potential Field\nQuasi-static Clamping\nRANSAC\nReasoning Component\nRevolute Joint\nRisk Mitigation\nRobot Range Limit Monitoring\nRobotic Joints\nRoll\nRotary Joint\nRotary Vector Drive (RV)\nSafeguard\nSafety Integrity Level\nSafety Logic Circuit\nSemantic Data\nSense-Plan-Act Model\nSensory Feedback\nServo Control\nServo Motor\nServo Pack\nServo-controlled Robot\nServo-system\nShock Detection Function\nSingularity\nSoftlimit Setting Function\nSpline Motion Type\nSpline\nTSDF\nTask (endeffector) Space Vs Joint Space\nTeach Lock\nThe Repulsive Potential\nThrough-beam\nTime Measuring Function\nTrajectory Planning\nTransducer\nTrapezoidal Trajectory\nUnet Grasping\nVacuum Cup Hand\nViewpoint Feature Histogram\nVisual Servo System\nVolumetric Grasping Network\nWork Envelope\nWrist\nYaw\n"},"KB/sacred-values":{"title":"sacred values","links":[],"tags":["ethics"],"content":"Sacred Values\n\nmorally forbids the commitment of certain actions regardless of consequences\n"},"KB/surjection":{"title":"surjection","links":["images/c0666f21cb878173e5817cb49d1bc596_MD5.jpeg"],"tags":["algebra"],"content":"Surjection\n \n\nOpen: Pasted image 20241119180504.png\n\n"},"KB/swap-dominance":{"title":"swap-dominance","links":[],"tags":["ethics"],"content":"Swap-dominance\n\nwhen ranking alternatives to form a model of ethical preferences\nWhen new decisions need to be made, the summarized model is used to compute a collective decision that results in the best possible outcome\n"},"KB/textless-lib":{"title":"textless-lib","links":["KB/Speech-Resynthesis"],"tags":["deeplearning"],"content":"Textless-lib\n\ntextless-lib: a Library for Textless Spoken Language Processing\nTextless spoken language processing research aims to extend the applicability of standard NLP toolset onto spoken language and languages with few or no textual resources\nPyTorch\nspeaker probing, (ii) Speech Resynthesis and compression, and (iii) speech continuation.\n"},"KB/todo":{"title":"todo","links":["KB/Graphs","Tag-Pages/loss"],"tags":["temp"],"content":"ToDO\n\nGraphs for every loss function and it’s derivative\nreflow gradient descent document\ngithub.com/kennethleungty/MLOps-Specialization-Notes\n"},"KB/treecoverSegmentation":{"title":"Tree Cover Segmentation","links":["KB/PointNet++","Tag-Pages/loss","KB/Focal-Loss","Watershed","KB/Unet"],"tags":["application"],"content":"Tree Cover Segmentation\n\ntreecover segmentation PointNet++\n\nData collected from above\nNormalization : height, xy\nRotation\nJiggling ??\nLabeling\n\n Segmentation algorithm. Canopy hide model\nWeighted loss + Focal Loss\n\n\n\n\n\n2d Methods\n\nWatershed + Unet\n\n\n\n\\Theta is just clippingpng]\nThe sqrt makes it a little smoother\n\n\n\nRef\n\nMax Freudenberg - Gottingen Uni Germany\nAdrian Stroker - Gottingen Uni Germany\n"},"KB/trolley-scenario":{"title":"trolley scenario","links":[],"tags":["ethics"],"content":"Trolley Scenario\n\nmaking a participant actively cause harm to an innocent bystander by pushing him on to the train track in order to save the lives of five people\n"},"KB/usermodel":{"title":"usermodel","links":["KB/Algebra-Cognitive-Tutor","KB/Andes","KB/AutoTutor","KB/Coarse-grained-assessment","KB/Collaborative-Recommender","KB/Content-Based-Recommender","KB/DT-Tutor","KB/Declarative-memory","KB/Effects-of-Contextual-Cues-on-Inferring-and-Remembering-Meanings-of-New-Word","KB/Extensions-to-SlimStampen","KB/Eye-Tracking","KB/Filter-Bubble-Problem","KB/Final-Paper-User-Models","KB/Fine-Grained-assesment","KB/GOMS","KB/Game-Based-Learning","KB/Gamification","KB/Gaze-position","KB/Grey-sheep-problem","KB/Group-Modeling-Approach","KB/Help-Abuse","KB/Help-Refusal","KB/IRT","KB/Ideas-for-Fact-Learning","KB/Individual-Modeling","KB/Knowledge-Component","KB/Latent-Semantic-Analysis","KB/Learning-Event","KB/Learning-L2-German-Vocabulary-Through-Reading","KB/Macroadaptation","KB/Mastery-learning","KB/Modeling-Driver-Behavior-with-Cognitive-Architecture","KB/Modeling-Transfer","KB/Predicting-Student-learning-Curve","KB/Pupil-Dilation","KB/Ramp-up-problem","KB/Rational-inference","KB/Recommender-System","KB/SQL-Tutor","KB/Satisficing-Heuristic","KB/Second-Language-Vocabulary-Learning-,-The-role-of-context--versus-translation","KB/Serious-Games","KB/Sherlock","KB/SlimStampen","KB/Steve","KB/The-Behavior-of-Tutoring-Systems","KB/The-Effect-of-Three-Consecutive-Context-Sentences-on-EFL-Vocabulary-Learning","KB/Tutor","KB/plan-recognition-problem"],"tags":["anchor"],"content":"\nAlgebra Cognitive Tutor\nAndes\nAutoTutor\nCoarse-grained assessment\nCollaborative Recommender\nContent Based Recommender\nDT Tutor\nDeclarative memory\nEffects of Contextual Cues on Inferring and Remembering Meanings of New Word\nExtensions to SlimStampen\nEye Tracking\nFilter Bubble Problem\nFinal Paper User Models\nFine Grained assesment\nGOMS\nGame Based Learning\nGamification\nGaze position\nGrey sheep problem\nGroup Modeling Approach\nHelp Abuse\nHelp Refusal\nIRT\nIdeas for Fact Learning\nIndividual Modeling\nKnowledge Component\nLatent Semantic Analysis\nLearning Event\nLearning L2 German Vocabulary Through Reading\nMacroadaptation\nMastery learning\nModeling Driver Behavior with Cognitive Architecture\nModeling Transfer\nPredicting Student learning Curve\nPupil Dilation\nRamp up problem\nRational inference\nRecommender System\nSQL-Tutor\nSatisficing Heuristic\nSecond Language Vocabulary Learning , The role of context  versus translation\nSerious Games\nSherlock\nSlimStampen\nSteve\nThe Behavior of Tutoring Systems\nThe Effect of Three Consecutive Context Sentences on EFL Vocabulary-Learning\nTutor\nplan recognition problem\n"},"KB/visualization":{"title":"visualization","links":["KB/1D-piecewise-linear-interpolation","KB/Area-Minimization","KB/Asymptotic-Decider","KB/Average-Filter","KB/Back-To-Front-Raycasting","KB/Barycentric-Interpolation","KB/Bend-Minimization","KB/Bilinear-Interpolation","KB/Change-Blindness","KB/Characteristics-of-Visual-Variables","KB/Classification-Ray-Casting","KB/Clutter-In-Visualisation","KB/Color-Compositing","KB/Color-Spaces","KB/ColorMap","KB/Complex-Geometry","KB/Contour","KB/Conv-Based-Noise-Reduction","KB/Countouring-with-Transparency","KB/Critical-Points","KB/Cross-Minimization","KB/Cross-angle-Maximization","KB/Cuboids","KB/Curl-And-Vorticity","KB/Cylinders","KB/Data-Structures","KB/Diffusion-Tensor","KB/Divergence","KB/Divide-Oriented","KB/Early-Ray-Termination","KB/Eigenvector","KB/Ellipsoids","KB/Entourage-Plot","KB/Euler-Integration","KB/Eulerian-Grid","KB/Filtering","KB/Finite-Differences","KB/First-order-integration","KB/Force-Directed-Graph-Layout","KB/Fractional-Anisotropy","KB/Front-to-Back-Raycasting","KB/Gaussian-Filter","KB/Gestalt-Laws","KB/Glyphs","KB/Graphs","KB/Grids","KB/H3-View","KB/Height-Plots","KB/Helmholtz-Theorem","KB/Hierarchial-Refinement","KB/Hierarchical-Edge-Bundling","KB/High-pass-filter","KB/HyperStreamlines","KB/ICA-Noise-Removal","KB/Inattentional-Blindness","KB/Inceptionism","KB/Indirect-Volume-Visualization","KB/Information-Visualization","KB/Integral-Lines","KB/Interpolation","KB/Intuitive-Color-spaces","KB/Isoline","KB/Isosurface","KB/Lagrangian-Coherent-Structure","KB/Lagrangian-Grid","KB/Laplacian-Grid-Smoothing","KB/Length-Optimization","KB/Line-Integral-Convolution","KB/Mapping-to-Geometry","KB/Marching-Cubes","KB/Marching-Squares","KB/Marching-Tetrahedra","KB/Mean-Diffusivity","KB/Median-Filter","KB/Mesh-Smoothing","KB/Mesh-refinement","KB/Midpoint-Decider","KB/Midpoint-Method","KB/Node-Link-Diagram","KB/Noise-Suppression","KB/Notch-filter","KB/Oblique-Slicing","KB/Opacity-Correction","KB/Orthogonal-Slicing","KB/Parallel-Coordinate-Plots","KB/Particle-Visualization","KB/Pathlines","KB/Perception","KB/Perceptually-Uniform","KB/Phong-Lighting","KB/Post-Classification","KB/Postattentive-Amnesia","KB/Pre-Classification","KB/Pre-Integrated-Volume-Rendering","KB/Radial-Plot","KB/Raycasting","KB/Region-Growing","KB/Runge-Kutta","KB/Sampling-Ray-Casting","KB/Scalar-Color-Coding","KB/Shading","KB/Shepard-Interpolation","KB/Slice-Based-Volume-Rendering","KB/Spectrogram","KB/Stream-Ribbons","KB/Stream-Surfaces","KB/Streamline-Stopping-Criterion","KB/Streamlines","KB/SuperQuadrics","KB/Symmetries-Node-Link","KB/Time-Dependant-Vector-Field","KB/Transfer-Function","KB/Treemap","KB/Visual-Associative","KB/Visual-Encoding","KB/Visual-Length","KB/Visual-Ordered","KB/Visual-Quantitative","KB/Visual-Selective","KB/Visualization-Of-Layers","KB/Volume-Rendering-Equation","KB/Volume-Visualization","KB/Volumetric-Illumination","KB/Voxel-Projection"],"tags":["anchor"],"content":"\n1D piecewise linear interpolation\nArea Minimization\nAsymptotic Decider\nAverage Filter\nBack To Front Raycasting\nBarycentric Interpolation\nBend Minimization\nBilinear Interpolation\nChange Blindness\nCharacteristics of Visual Variables\nClassification Ray Casting\nClutter In Visualisation\nColor Compositing\nColor Spaces\nColorMap\nComplex Geometry\nContour\nConv Based Noise Reduction\nCountouring with Transparency\nCritical Points\nCross Minimization\nCross angle Maximization\nCuboids\nCurl And Vorticity\nCylinders\nData Structures\nDiffusion Tensor\nDivergence\nDivide Oriented\nEarly Ray Termination\nEigenvector\nEllipsoids\nEntourage Plot\nEuler Integration\nEulerian Grid\nFiltering\nFinite Differences\nFirst order integration\nForce Directed Graph Layout\nFractional Anisotropy\nFront to Back Raycasting\nGaussian Filter\nGestalt Laws\nGlyphs\nGraphs\nGrids\nH3 View\nHeight Plots\nHelmholtz Theorem\nHierarchial Refinement\nHierarchical Edge Bundling\nHigh pass filter\nHyperStreamlines\nICA Noise Removal\nInattentional Blindness\nInceptionism\nIndirect Volume Visualization\nInformation Visualization\nIntegral Lines\nInterpolation\nIntuitive Color spaces\nIsoline\nIsosurface\nLagrangian Coherent Structure\nLagrangian Grid\nLaplacian Grid Smoothing\nLength Optimization\nLine Integral Convolution\nMapping to Geometry\nMarching Cubes\nMarching Squares\nMarching Tetrahedra\nMean Diffusivity\nMedian Filter\nMesh Smoothing\nMesh refinement\nMidpoint Decider\nMidpoint Method\nNode Link Diagram\nNoise Suppression\nNotch filter\nOblique Slicing\nOpacity Correction\nOrthogonal Slicing\nParallel Coordinate Plots\nParticle Visualization\nPathlines\nPerception\nPerceptually Uniform\nPhong Lighting\nPost Classification\nPostattentive Amnesia\nPre Classification\nPre Integrated Volume Rendering\nRadial Plot\nRaycasting\nRegion Growing\nRunge Kutta\nSampling Ray Casting\nScalar Color Coding\nShading\nShepard Interpolation\nSlice Based Volume Rendering\nSpectrogram\nStream Ribbons\nStream Surfaces\nStreamline Stopping Criterion\nStreamlines\nSuperQuadrics\nSymmetries Node Link\nTime Dependant Vector Field\nTransfer Function\nTreemap\nVisual Associative\nVisual Encoding\nVisual Length\nVisual Ordered\nVisual Quantitative\nVisual Selective\nVisualization Of Layers\nVolume Rendering Equation\nVolume Visualization\nVolumetric Illumination\nVoxel Projection\n"},"KB/wave2vec":{"title":"wave2vec","links":[],"tags":["architecture"],"content":"wave2vec\n\nwav2vec: Unsupervised Pre-training for Speech Recognition\nReducing the need for manually annotated data is important for developing systems that understand non-English languages, particularly those with limited existing training sets of transcribed speech\nfirst application of unsupervised pre-training to [speech recognition](speech recognition.md) using a fully convolutional model that learns representations of raw, unlabeled audio\ntrained on large amounts of unlabeled audio data and the resulting representations are then used to improve acoustic model training\npre-train a simple multi-layer convolutional neural network optimized via a noise contrastive binary classification task\nlearn the difference between original speech examples and modified versions, often repeating this task hundreds of times for each second of audio, and predicting the correct audio milliseconds into the future\nbeats traditional ASR systems that rely solely on transcribed audio\nexperiments on WSJ reduce WER of a strong character-based log-mel filterbank baseline\nmore data for pre-training improves performance and that this approach not only improves resource-poor setups, but also settings where all WSJ training data is used\n"},"ToDOs":{"title":"ToDO Today","links":["Book-Notes/Understanding-Deep-Learning/"],"tags":["daily"],"content":"ToDO\n \nTODAY\nOpenML\n\nread gnn and make notes, sources\nreach out to roshan for gpu endpoints\nAuto-automlb :\n\ndebug anneal dataset for Randomforest\n\n\n\nEverything Else\nGeneral List\nOpenML\n\nreach out to people for openml\n\nLaurens Bliek\nMitko Veta (bio, also on BOOST project)\nNami Sunami (or other data stewards at TU/e)\n\n\nLook at data upload/curation software from students\nBuild a meta-feature (evaluation) engine in Python (from java)\nreading chapters from index\nspeed up ai search\nrewrite data uploading logic\n\nDone\nEverything Else"},"index":{"title":"Subhaditya's KB","links":["tags","KB","Articles","Book-Notes"],"tags":[],"content":"Go To\n\nTags\nKB\nArticles\nBook Notes\n\nAbout This Blog\n\nThis is my little knowledge base\nIf there is something you are looking for, just type it into the search bar\n\nOf course, since this is not google, it is not a one stop shop\nIn essence, it will have something on things that I learn\n\n\nHow to go about finding things?\n\nScroll the sidebar and pick something you like. Click the links inside it to go about. Choose your own adventure.\n\n\nOther things? Drop me an email :)\n\nLinks\n\nMy Github\nBlogs\nMedium → More blogs\nMy Linkedin\nMy Art\nEmail\n"}}