---
title: BERT
tags: architecture
---

# BERT
- Bidirectional Encoder rep from transformers
- Uses [Token Embedding](Token%20Embedding.md)
- [[Self supervised]]
- Masked language modeling, next sentence prediction
- ![im](assets/Pasted%20Image%2020220307183916.png)
- [CLS] : start of classification task, [SEP] between sentences, [MASK] : masked token
















