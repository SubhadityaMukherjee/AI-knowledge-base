---
title: Embedding
---

# Embedding
- More complex than 1 hot
- Lookup table is an example. 
	- $$token\_embedding(i) = gather(W, i)$$
- Say vocabulary is (the cat walks)
	- Embedding vector v that will be learnt
	- Values like : $v_{the}$, $v_{cat}$, $v_{walks}$






























## Backlinks

> - [ELMO](ELMO.md)
>   - context-sensitive word embeddings using the [Long Short Term Memory (LSTM)](Long%20Short%20Term%20Memory%20(LSTM).md)-based [[Embedding]] from Language Models (ELMo) architecture
>    
> - [Self Attention](Self Attention.md)
>   - Assign every word t in the vocabular an [[Embedding]]
>    
> - [CvT](CvT.md)
>   - a hierarchy of Transformers containing a new convolutional token [[Embedding]]
>    
> - [Transformer](Transformer.md)
>   - [[Embedding]]

_Backlinks last generated 2022-06-20 17:57:49_
