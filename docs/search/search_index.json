{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Subhaditya's KB","text":""},{"location":"#about-this-blog","title":"About this blog","text":"<ul> <li>This is my little knowledge base</li> <li>If there is something you are looking for, just type it into the search bar<ul> <li>Of course, since this is not google, it is not a one stop shop</li> <li>In essence, it will have something on things that I learn</li> </ul> </li> <li>How to go about finding things?<ul> <li>Scroll the sidebar and pick something you like. Click the links inside it to go about. Choose your own adventure.</li> </ul> </li> </ul>"},{"location":"#links","title":"Links","text":"<ul> <li>My Github</li> <li>Blogs</li> <li>Medium -&gt; More blogs</li> <li>My Linkedin</li> <li>My Art</li> <li>Email</li> </ul>"},{"location":"Articles/","title":"Articles","text":""},{"location":"Articles/#articles","title":"Articles","text":"<ul> <li>This section has open source no paywall versions of every article I write (unless requested for closed source of course)</li> <li>They might not have the images required as those are only available on the source website. BUT if you search for those topics in the Knowledge base, you will find all the material there anyway, with images (most of them)</li> <li>I use this KB as a gateway into writing these articles as well.</li> </ul>"},{"location":"Articles/BookNotes/Summary%20-%20Notes%20on%20a%20Nervous%20Planet%20by%20Matt%20Haig/","title":"Summary - Notes on a Nervous Planet by Matt Haig","text":""},{"location":"Articles/BookNotes/Summary%20-%20Notes%20on%20a%20Nervous%20Planet%20by%20Matt%20Haig/#chapterwise-summary-30-ways-to-take-back-control-notes-on-a-nervous-planet-by-matt-haig","title":"Chapterwise summary + 30+ ways to take back control : Notes on a Nervous Planet by Matt Haig","text":"<p>Notes on a nervous planet is a book that brings a lot of important points about the collective overwhelm that we as a \u201cmodern\u201d society face. Social media, lack of sleep, odd working hours, loneliness. Neither of these were aspects that we were really prepared to deal with. But before treating a disease, we must know what the symptoms are and why they showed up in the first place. (At the end of the article, there is a long is list of what you can do to avoid overwhelm.)</p> <p>This is a summary and my thoughts (chapter wise, mostly) on the book. I try to cover what stuck out to me the most and in turn, hope that it helps someone out. If you like the summary, you will like the book more. </p> <p>Go support the author here.</p> <p>(Note: This is not sponsored by the author and is a personal opinion that just reflects my own views. Since this is an interpretation, the author might have thought something different in a few places.)</p>"},{"location":"Articles/BookNotes/Summary%20-%20Notes%20on%20a%20Nervous%20Planet%20by%20Matt%20Haig/#a-stressed-out-mind-in-a-stressed-out-world","title":"A stressed-out mind in a stressed-out world","text":"<p>As society progresses, we face an overwhelming increase in everything from groceries to people to jobs and somewhere down the line it takes a huge toll on our mental health. Humans weren\u2019t meant to deal with this constant barrage of information. If you stop and look around you, you can almost feel this underlying quit panic. The rush of everyday life, the endless calling out for attention by companies, all of these things wear you down little by little.  Before, if you wanted to find out about what was happening in the world, you would look at the news and be presented with a limited selection of it. These days however, the more attention grabbing headlines are what you see. There is always something horrible happening and this triggers the part of our brains that make us feel unsafe.  You might think that social media is different, but it still does the same thing doesn\u2019t it? It keeps triggering the fear and panic that makes you feel FOMO (fear of missing out), or that someone else\u2019s life is better. After even a few minutes, you might have noticed the guilt and sadness you feel but you can\u2019t stop.  All of these, and the added loneliness it brings, makes us feel like we are in a 24x7 catastrophe. </p>"},{"location":"Articles/BookNotes/Summary%20-%20Notes%20on%20a%20Nervous%20Planet%20by%20Matt%20Haig/#the-big-picture","title":"The big picture","text":"<p>Some people blame this on time and modernisation. Which is true, but at the end of the day the main reason for this insanity is just\u2026 consumerism and capitalism. Companies need to grow, and get the \u201cbig bucks\u201d and what better way to do that but constantly keep expanding.  But every expansion has a price, and in this case it\u2019s the sanity of society and the slow extortion of our planet. In time, this mentality has seeped into every aspect of society. Our politicians glorify the \u201chard working families\u201d and our companies glorify endless work hours. To what end? These goalposts are endless. And the constant comparison with everyone else that social media brings just feeds the fire. It\u2019s come to the point that nobody is really ever satisfied. You spend years to work on a degree, and social media convinces you that traveling the world would have been a better idea. You get a great job, a new car, move to a bigger house etc, and your device shows you pictures of even more. The fuel is dissatisfaction and the fire is the need to buy. The need to keep creating and producing.  Doesn\u2019t this remind you of a factory line?</p>"},{"location":"Articles/BookNotes/Summary%20-%20Notes%20on%20a%20Nervous%20Planet%20by%20Matt%20Haig/#a-feeling-is-not-your-face","title":"A feeling is not your face","text":"<p>Social media has added a lot to the constant insecurity that we face, and beauty standards have reached an almost ridiculous peak. It\u2019s not just a couple of magazines anymore. We are constantly surrounded by unrealistic body standards. Regardless of your gender, you are shown endless options and made to subconsciously identify with either the \u201cbarbie\u201d, \u201csuperhuman\u201d or some other stereotype based on what you see online. Neither of these are achievable.  If you go to a beach, you might think that the people around you are so concerned with how you look with your shirt off. But in all honesty, most people are thinking the same way. Most people are too concerned with their own appearance to care about yours. And then the beach itself does not care. It\u2019s nature after all. Regardless of what your body looks like, you\u2019re a part of it. It bears no judgement.  Accepting yourself the way you are, and realising that what you see online in unreasonable, goes a pretty long way.</p>"},{"location":"Articles/BookNotes/Summary%20-%20Notes%20on%20a%20Nervous%20Planet%20by%20Matt%20Haig/#notes-on-time","title":"Notes on time","text":"<p>Cavemen didn\u2019t particularly care if they were 5 minutes late to a hunting meeting. Time was not quantified to this precision. At some point the sun and the moon guided time. There was day and night, maybe afternoon. Some times were good for the hunt, the others were good for rest and community. As technology marches on, the concept of time becomes ever refined. Now not only are you always \u201clate\u201d, you know exactly how badly you failed within a nanosecond of accuracy. Hello guilt! Deadlines came to exist, and the constant need to check your watch or phone.  Society made time an enemy. \u201cFinish everything before the timer runs out\u201d. Suddenly, you could never achieve enough in the time that you had. Because the more divisions we make, the more we are expected to do to fill them up. We stopped listening to our bodies and became a slave to a ticking clock. It\u2019s time we started going back to a simpler time.</p>"},{"location":"Articles/BookNotes/Summary%20-%20Notes%20on%20a%20Nervous%20Planet%20by%20Matt%20Haig/#life-overload","title":"Life overload","text":"<p>It\u2019s not just your sink that gets filled with dishes, your poor brain is struggling to keep up with all this work and information. There is an unnecessary excess in everything and thus your brain needs to constantly make decisions. In turn, everyone around you faces the same thing. If you look it from the outside, it\u2019s almost as if we are all feeding that collective frenzy. The world is heading for a collective breakdown, and in our heart of hearts, we know that we are not just a part of it, we are feeding the fire too. It\u2019s not just life, but an overload of it.</p>"},{"location":"Articles/BookNotes/Summary%20-%20Notes%20on%20a%20Nervous%20Planet%20by%20Matt%20Haig/#internet-anxieties","title":"Internet anxieties","text":"<p>Ah the internet. Humanities craziest information. A way to \u201cforever\u201d and almost instantly look up anything. Any think you wanted to know, feel, see or find, all up for grabs. The biggest fuel we have added to the frenzy. But the internet is not always a bad thing is it? So many people have learnt skills, built families, found similar people, started a business and found partners through it. But they have also been pushed opinions that they never knew of. They have been fed with a constant diet of hate, choice, games of power, games of \u201cratings\u201d and a desire for pretence.  On the internet, nobody knows if you are a human or a dog. And it\u2019s so much easier to bully someone if you know you are a continent away and can\u2019t be held accountable. These exacerbate the collective overload. Because now not only do people have the power to play either the victim or the bully, they can see millions of the these stories playing out everyday.  A slowly spreading poison doesn\u2019t kill very fast, but withers you away in time. The only way to go about it is empathy. You cannot avoid the internet but you can choose what you want to actively look for. Simplify, and leave the fighting to the real world.</p>"},{"location":"Articles/BookNotes/Summary%20-%20Notes%20on%20a%20Nervous%20Planet%20by%20Matt%20Haig/#shock-of-the-news","title":"Shock of the news","text":"<p>Fear does sell though, and the news profits on that. Wars and death have always been common, but these days? You almost feel like the violence of the world has increased to a ridiculous point. You feel unsafe everywhere you go, 24x7.  A few decades ago, you would get your news twice a day maybe. Now you get it every second, from every possible angle, all across the world. There\u2019s more than 7 billion people and the stories are endless. You don\u2019t hear about one robbery, you hear about them from 10 countries, 300 people and commented on by thousands more. Feeding the flame of anxiety again. Remember, fear sells.  Try to limit your exposure to it. If something truly important happens, you will get to know. </p>"},{"location":"Articles/BookNotes/Summary%20-%20Notes%20on%20a%20Nervous%20Planet%20by%20Matt%20Haig/#a-small-section-on-sleep-priorities","title":"A small section on sleep priorities","text":"<p>The part that really stuck out to me here was this \u201cSleep is the enemy of consumerism\u201d. Who\u2019s the biggest competition of Netflix? It\u2019s SLEEP. You can\u2019t watch a show if you\u2019re asleep. Neither can you buy the next \u201cgreatest phone\u201d.  Likewise, you cannot produce these things. The endless factory \u201cneeds\u201d to grow. Working hours keep getting later and later, working on weekends is a normal thing, working 16 hours is a badge of pride, we all know these don\u2019t we?  And the lesser we sleep, the more health issues we have. Catch my drift? (Hint: Healthcare is a product too you know?) Apart from healthcare, the constant panic and fear leads us to the other large businesses - Drugs and Weapons.  Might as well get those good hours of sleep in right? Routines do help. And a little light movement, a warm shower, turning off the devices etc do help quite a bit.</p>"},{"location":"Articles/BookNotes/Summary%20-%20Notes%20on%20a%20Nervous%20Planet%20by%20Matt%20Haig/#phone-fears","title":"Phone fears","text":"<p>I think we all have heard a lot about how our devices are affecting our lives negatively. That being the case, the more important aspect of that is understanding how to tackle it. A selection is as follows. Notifications do suck. Turn them off except for the most important ones. Spend some time away from your devices, especially before sleep. Multitasking is an illusion.  Social media breeds unnecessary uncertainty. You are always left \u201cwondering\u201d. What is my friend doing? Did my favourite artist upload a new comic? Are there any new TikTok\u2019s on the thing I\u2019m currently obsessing about? The answer of course, is yes. The problem is, we cannot have everything can we? Accepting that uncertainty is a part of life helps drastically.</p>"},{"location":"Articles/BookNotes/Summary%20-%20Notes%20on%20a%20Nervous%20Planet%20by%20Matt%20Haig/#the-detective-of-despair","title":"The detective of despair","text":"<p>All the effects so far seem almost too general? The best way to see how something affects you is to look at what your body is feeling. Listen to what it says. Is it telling you to stop watching people making food and go eat dinner? Is it saying that hey, I\u2019m so tired, can you stop looking at people waking up at 6 am and taking cold showers? Listen. Breathe.</p>"},{"location":"Articles/BookNotes/Summary%20-%20Notes%20on%20a%20Nervous%20Planet%20by%20Matt%20Haig/#the-thinking-body","title":"The thinking body","text":"<p>One rather interesting point the author makes is that of a \u201cunit\u201d to how much mental toll something takes : a \u201cpsychogram\u201d. I find it a very interesting notion. Some examples the author gave are (negative and lower is better): - The sun appearing unexpectedly from behind a cloud : -57pg - Dancing : -1350pg - Arriving home after a terrible train journey : -398pg - Watching the news : 222pg - A worrying symptom you have googled : 672pg Trying to at least vaguely quantify how much we can deal with in a day before snapping can help us set better boundaries and listen to our body more. We all start off with a limited amount of energy, and some things drain more than the others.  Of course, the actual quantification of it is not very useful. It\u2019s the idea that makes it interesting to me. A reminder that we have limited resources. Like a Health Bar in a game, ours keeps going down too. We just can\u2019t see it.</p>"},{"location":"Articles/BookNotes/Summary%20-%20Notes%20on%20a%20Nervous%20Planet%20by%20Matt%20Haig/#the-end-of-reality","title":"The end of reality","text":"<p>The more the virtual creeps in, the more choices are presented to us. Instead of going to your local store to buy a new phone, you look it up online first. Instead of being presented with one shampoo, you have 40 in a store, and 4000 online.  All these choices can lead your brain to want to not compute and choose. So it turns part of itself off. It hides in \u201cDerealisation\u201d. Reality seems a bit wonky. The virtual world feels more like home, an uncertain, panic filled home, but home nonetheless. We must take steps to protect ourselves from this frenzy before it drowns us.</p>"},{"location":"Articles/BookNotes/Summary%20-%20Notes%20on%20a%20Nervous%20Planet%20by%20Matt%20Haig/#wanting","title":"Wanting","text":"<p>This section ties in a lot the \u201cbeauty\u201d one that came before. It serves as a reminder to be kinder to yourself, consider that age and time are a natural function of life itself and realise that \u201ctoo much\u201d of anything is not always a good thing. More is not a solution. The solution is acceptance and being grateful for what you have.  We can never stop wanting, but if we can draw the line between what we truly \u201cwant\u201d and what we are made to believe that we do by our consumerist economy, then we can get some control back.</p>"},{"location":"Articles/BookNotes/Summary%20-%20Notes%20on%20a%20Nervous%20Planet%20by%20Matt%20Haig/#two-lists-about-work","title":"Two lists about work","text":"<p>As a society, we like work. We base our worth on it sometimes, which is not great though. The cultural obsession we have with \u201chard working families\u201d is not always a good thing though. Statistics show that the \u201chardest workers\u201d aren\u2019t always the happiest or the wealthiest. Does that say something?  Choosing to have less stuff to do vs choosing to constantly be \u201cbusy\u201d is a lot healthier. Never forget that deadlines are a product of consumerism. Imperfection is a part of humanity itself, it\u2019s a feature, not a bug. One quote that I really liked in the book :  \u2018One of the symptoms of an approaching nervous breakdown is the belief that one\u2019s work is terribly important.\u2019 - Bertrand Russell</p>"},{"location":"Articles/BookNotes/Summary%20-%20Notes%20on%20a%20Nervous%20Planet%20by%20Matt%20Haig/#shaping-the-future","title":"Shaping the future","text":"<p>Of course, change is hard. Most of these changes are easier said than done. And many of them are not even fully achievable by you yourself. Remembering that the space around you matters quite a bit, is also very important. We are a part of nature, and the vast openness and freedom that it brings is in our blood. Having more open space, parks and clean  workspaces makes our days a little better. When we can\u2019t escape into the woods, sometimes a good fiction book can serve that purpose. Progress is not something that happens overnight, it takes effort and time. But the peace is worth it.</p>"},{"location":"Articles/BookNotes/Summary%20-%20Notes%20on%20a%20Nervous%20Planet%20by%20Matt%20Haig/#the-song-of-you","title":"The song of you","text":"<p>Another important reminder here, the sky is always there for you. Wherever you may be, whoever you may be. We are not separate from nature, we are nature. Your inside world is important too. Tend to the garden that is you, sing the songs that are your very being.</p>"},{"location":"Articles/BookNotes/Summary%20-%20Notes%20on%20a%20Nervous%20Planet%20by%20Matt%20Haig/#everything-you-are-is-enough","title":"Everything you are is enough","text":"<p>The key takeaway really, is acceptance. Accepting that you cannot be everything, do everything, go everywhere, have everything. And even if you did, it would not really fill the void. What would is accepting the present moment, prioritising living over just existing, taking back the power and freedom that society so desperately wants to take away from us. Accepting that our failures are a part of life. Accepting that after a point, putting more work into something only has diminishing returns.  Accepting that, people aren\u2019t forever. If you want to show your love, what are you waiting for?</p>"},{"location":"Articles/BookNotes/Summary%20-%20Notes%20on%20a%20Nervous%20Planet%20by%20Matt%20Haig/#a-list","title":"A list","text":"<p>That ends my summary and thoughts about the book.  The author gives quite a few helpful tips dispersed among the chapters, and I thought I would list my favourites in one place. (Some are verbatim and I take no credit for them.)</p> <ol> <li>Happiness is felt heading out, not in</li> <li>Happiness is about what we already have</li> <li>Maybe the point of life is to embrace life\u2019s beautiful uncertainty.</li> <li>Products that make us ashamed of our age, don\u2019t actually help us not age</li> <li>The beach does not care what you look like</li> <li>Young people are more worried about age than young</li> <li>Acceptance &gt; Denial in the long run</li> <li>Feeling something doesn\u2019t mean that it\u2019s the absolute truth. It\u2019s just a feeling</li> <li>The empty joy of likes is\u2026 empty</li> <li>Posting about experiences instead of having experiences is not great</li> <li>Don\u2019t type your symptoms into Google</li> <li>What is real on the Internet, isn\u2019t always true</li> <li>Social media abstinence is good for you</li> <li>You cannot understand someone through Intsagram</li> <li>Ratings are not worth the judgement </li> <li>Don\u2019t be steered towards being a caricature of yourself by the internet </li> <li>Algorithms eat empathy</li> <li>Limit the number of times you get the news</li> <li>The world is not as violent as it feels</li> <li>Bad news doesn\u2019t mean good news doesn\u2019t happen</li> <li>Sleep is the enemy of consumerism </li> <li>The imperfections of the real world fill the void that the perfections of the virtual do not</li> <li>Being lonely sometimes is not bad</li> <li>You don\u2019t always have to be available </li> <li>Uncertainty is not going to go away</li> <li>Be your own friend </li> <li>Don\u2019t grab life by the throat.</li> <li>Too many choices trigger your fight or flight response.</li> <li>You cannot be everything</li> <li>Work isn\u2019t the point of life. It\u2019s the point of capitalism. You are replaceable to your work, not your family.</li> <li>Aim to have less stuff to do.</li> <li>Nature is always there for you. So are animals</li> </ol>"},{"location":"Articles/BookNotes/Summary%20-%20Notes%20on%20a%20Nervous%20Planet%20by%20Matt%20Haig/#fin","title":"Fin","text":"<p>And again, if you liked my summary, you will like the original book more. You wouldn\u2019t read Wikipedia and feel like you watched a movie would you? Support the author!</p> <p>This article is in the hopes that it will help someone out. Maybe have the help that I did not. I do not know who it will reach. But to whoever it does, best of luck :)</p> <p>Like these/Want more? Buy me a coffee! Kofi</p> <p>Want articles on something specific? Just ask!</p> <p>You can always contact me on LinkedIn, or drop me an Email</p>"},{"location":"Articles/DL/AI%20and%20Doom/","title":"AI and Doom","text":"<p>toc: true title: AI and Doom</p> <p>categories: [\"article\"] date modified:  date created: </p>"},{"location":"Articles/DL/AI%20and%20Doom/#ai-and-doom-the-real-fear-and-some-hope","title":"AI and Doom : The Real Fear and Some Hope","text":""},{"location":"Articles/DL/AI%20and%20Doom/#ideas","title":"Ideas","text":"<ul> <li>Why do we work : society</li> <li>The pursuit of art </li> <li>Luddites</li> <li>Malthusian mindset</li> </ul>"},{"location":"Articles/DL/AI%20and%20Doom/#research","title":"Research","text":"<ul> <li> <p>Why be an artist when there is AI?</p> <ul> <li>Just because we have cars, should we stop walking?</li> <li>We taught machines how to speak, but now that they have, why should we stop?</li> </ul> </li> <li> <p>AI and Rumors of Impending Doom</p> <ul> <li>Another week, another story about how artificial intelligence (AI) is an existential threat to the human race</li> <li>discouraging to see so many pronouncements of AI\u2019s existential threat to humanity by people who should know better</li> <li>one technology that poses existential risk and whose creation undermined the enlightenment narrative of progress in science and technology improving human life. Global nuclear war</li> <li>A long series of movies produced after the explosions of 1945\u2014beginning with The Day the Earth Stood Still, On the Beach, Fail Safe, Dr. Strangelove, even Godzilla\u2014entertained us with apocalyptical tales and created a narrative whose theme (humanity is at risk from uncontrolled science) continues to shape thinking in unhelpful ways.</li> <li>loss of faith in the ability of democratic societies to manage themselves.</li> <li>seemingly intractable domestic issues, undercut the belief that change can be managed.</li> <li>The perception of failure does not inspire confidence and undercuts the legitimacy of leaders and institutions.</li> <li>The internet has made discourse chaotic.</li> <li>the internet puts rumor and conspiracy before a giant audience.</li> <li>Competition for attention in an intensely commercial society inclines people to tell horror stories\u2014reflecting the inherent bias in human cognition, where a scary story commands larger audiences than a happy one</li> <li>If predictions of doom were only for entertainment and PR purposes, they would not be a problem, but exaggerated fear can lead to bad policy.</li> <li>fears about AI-created unemployment</li> <li>That automation will cause jobs to disappear is a fear that goes back to the early nineteenth century and the Luddites</li> <li>AI is the latest phase in the automation of human activity that began in the eighteenth century, and automation creates wealth and innovation. Some jobs disappear; more jobs are created. With these new jobs will come increased wealth and leisure.</li> <li>It would be better to reinterpret the challenge of AI as deciding how to allocate increased wealth and leisure, but income distribution has not been a shining success for social policy for the last 30 years.</li> </ul> </li> <li> <p>AI Dooms Humanity But Not In The Way You Think</p> <ul> <li>If you want to get people to read your material there is no better way than to highlight the negativity.</li> <li>I tend to think the basis for this is largely influenced by the well-established \u201cMalthusian\u201d mindset that constantly states we are about to be out of resources and the four horseman are just around the corner. Malthus simply stated that population rises until all resources are consumed creating a boom bust dynamic similar to what we today often see in markets. Too many people equates to too much consumption which results in unavoidable catastrophe.</li> <li>So we should nonetheless worry about climate change and therefore we need to worry about energy use because if \u201cclimate change is a thing,\u201d if energy use is not carefully curated, it\u2019s not that we perish because we run out of energy, we all perish because we run out of environment.</li> <li>AI equates to power equates to CO2.</li> <li>If ChatGPT really is the beginning of transhuman intelligence, then that\u2019s it for the decarbonization strategy because any country not prepared to run its energy at full blast to fuel its AI</li> <li>But for even a skeptic like me, ChapGPT writing my bio in the style of the old testament is enough to prove it\u2019s smarter than 95% of people I know, so how long before that the full 100%?</li> </ul> </li> <li> <p>ChatGPT creator says there\u2019s 50% chance AI ends in \u2018doom\u2019 | The Independent</p> <ul> <li>Former OpenAI worker Paul Christiano, who now runs AI research non-profit Alignment Research Center, said he believed there was a significant chance that the technology would lead to the destruction of humanity.</li> <li>The main danger, he claimed, will come when AI systems reach and surpass the cognitive capacity of a human. Dr Christiano predicts there is a \u201c50/50 chance of doom\u201d once this moment arrives.</li> <li>The most likely way we die involves \u2013 not AI comes out of the blue and kills everyone \u2013 but involves we have deployed a lot of AI everywhere... [And] if for some reason, God forbid, all these AI systems were trying to kill us, they would definitely kill us.\u201d</li> <li>godfather of AI Geoffrey Hinton quitting Google to sound the alarm about the dangers of AI</li> </ul> </li> <li> <p> LinkedIn</p> <ul> <li>When experts like Hinton speak on the subject of AI we should, of course, listen.</li> <li>But I would argue that the fears currently being expressed around AI amount to little more than scaremongering and are consistent with an approach towards technology that has existed for centuries, perhaps longer.</li> <li>The advent of technologies as diverse as the printing press, the steam engine and the computer have all been accompanied by fears over what they might bring</li> <li>Indeed, I wonder whether some of the warnings around AI can be reduced to fears and frustrations about the pace of its development, and the fact that generative AI, based on large language models (LLM), remains firmly the hands of the usual big tech players. How much of this is sour grapes?</li> <li>We are surely right to be concerned about the potential of AI to eradicate jobs. But how many more jobs will be created as workers are redeployed to do the things that AI cannot, and will not, be able to do?</li> <li>Should there be guardrails in place to govern the development of such a powerful technology as AI? Of course. But we place regulations around loads of technologies \u2013 such as the installation of electrical wiring, the roadworthiness of cars, or the use and storage of our data.</li> <li>Does anyone seriously suggest switching off every computer around the globe just because \u2018bad actors\u2019 can take control of corporate networks and steal money from bank accounts? Of course not.</li> </ul> </li> <li> <p>Will Life Be Worth Living in a World Without Work? Technological Unemployment and the Meaning of Life - Science and Engineering Ethics</p> <ul> <li>Simple Subjectivist Theories</li> <li>life is meaningful to the extent that the individual living it experiences certain subjective states, typically conscious well-being and desire satisfaction</li> <li>Simple Objective Theories</li> <li>individual living it brings about certain objectively good or valuable states of affairs</li> <li>Aim-Achievement Theories</li> <li>combination of subjective and objective states are needed in order to make life meaningful</li> <li>Fitting-Fulfillment Theories</li> <li>combination of subjective and objective states are needed in order to make life meaningful</li> <li>under a simple subjectivist theory, there is reason to think that technological unemployment could enhance the overall level of meaning in our lives, but only if we make use of the right kinds of technological advances</li> <li>reason to think that technological unemployment could undermine the overall level of meaning in our lives, but this impact could lessened with the right kind of technological developments</li> <li> <ul> <li>The Subjective Satisfaction of Non-work </li> </ul> </li> <li>The idea is that compulsory work takes us away from the things that we are really passionate about and that would confer upon us the most subjective satisfaction</li> <li>I cannot do these things because the productivist ethos of modern academia demands that I produce more peer-reviewed publications to pad out my CV.</li> <li>This argument assumes that if we control our own time we will spend it in a way that induces the right subjective states.</li> <li>Dan Gilbert\u2019s work on mis-wanting, for example, suggests that we often don\u2019t really know what makes us truly happy and often stumble upon happiness (Gilbert ; Gilbert et al.  </li> <li>75, 617\u2013638.\"),  </li> <li>15, 14\u201319.\")).</li> <li>Findings like this can be exploited by critics of automation and technological unemployment. A recent example of this is Carr (). Carr contends that without the pressures and incentives of work we may live a life of listless and unsatisfied boredom.</li> <li>Cskikszentmihalyi (, , )</li> <li>studies reveal that people are generally more focused, happier and more satisfied at work than at play</li> <li>Indeed, they often report feeling anxious and bored outside of work when they are presumably free to engage in their preferred activities.</li> <li>\u2018paradox of work</li> <li>Cskikszentmihalyi\u2019s theory holds that entering into a flow state is a function of how difficult the task is and how much pressure is associated with it.</li> <li>Work is often an excellent way to provide the right kind of pressure and difficulty.</li> <li>There is a limited utility to studies, such as Cskikszentmihalyi\u2019s, which compare work and leisure in a world that is dominated by work.</li> <li>When I come home from a busy day at the office, I\u2019m usually drained and lethargic. I\u2019m often not physically able to engage in the kinds of activity I would prefer. I\u2019m conscious of the fact that I need to recover before going back to the office again. In an era of rampant technological unemployment, in which the shadow of work is removed, things could be very different.</li> <li>It is paternalistic to assume that people will lack sufficient self-motivation if they are unemployed.</li> <li>This paternalism is behind much of the traditional ideological glorification of the work ethic.</li> <li>the argument ignores the ways in which modern technology can greatly assist in providing these alternative sources of pressure.</li> <li>although technology could be leveraged in ways that make us more likely to achieve flow states through our activities, there are also ways in which we could use technology to trick ourselves into such subjectively pleasurable states without any associated activity</li> <li>if we are to think seriously about meaning and personal fulfillment in an age of technological unemployment, we probably need to take into consideration the link between our actions and the objective world, and how technology might mediate the relationship between our actions and the objective world.</li> <li>antiwork positions</li> <li>it is too dismissive of the potential for the market to direct human activity towards objectively valuable outcomes.</li> <li>production of art and intellectual discovery, both of which are subject to significant market forces in the modern world</li> <li>If automating technology renders human contribution to such market-based activities unnecessary, then we may be robbed of something that is conducive to the good life.</li> <li>the antiwork camp will simply respond by saying that nonwork is better at allowing us to do these things.</li> <li>It assumes that the kinds of technological advance that make widespread technological unemployment possible will occur in a vacuum\u2014that the impact of automating technologies will be felt solely in our economic lives.</li> <li>If this trend continues, and we rely on those technologies in these other domains, we could sever the necessary causal and mental link between our actions and the outcomes that are said to be constitutive of meaning.</li> <li>Science is increasingly a \u2018big data\u2019 enterprise, reliant on algorithmic, and other forms of automated assistance, to process large datasets and make useful inferences from those datasets. Humans are becoming increasingly irrelevant to the process of discovery.</li> <li>The machines start doing much of the work themselves; the logic of their decision-making becomes more and more opaque to those who interact with them</li> <li>Thus, once again, the rise of automation reduces the space in which humans can engage in meaningful and fulfilling moral activities.</li> <li>Now, you might dispute this characterisation. You might argue that there is still room for human input in all of these automated systems. For one thing, such systems would seem to require human designers, programmers and overseers; for another, humans would still have to contribute to the smooth functioning of such systems, e.g. by becoming kidney donors or by providing crucial data</li> <li>not everyone is going to be equipped or trained to design or program such systems</li> <li>advances in technology may be such that human designers, programmers and overseers will become less needed over time</li> <li>even if humans will always participate in such systems, the participation in question has to be of the right type in order to facilitate meaningfulness in the objective or hybridist sense.</li> <li>even if machines are better at achieving certain objective outcomes there is nothing to stop humans from achieving them too</li> <li>why would you waste time and risk lives if the machines are better</li> <li>it assumes that if machines get better and better at doing things they will take away from the fixed lump of potentially meaningful activities that are open to human beings.</li> <li>But why couldn\u2019t more and more objectively meaningful activities open up</li> </ul> </li> <li> <p>https://www.history.com/news/who-were-the-luddites</p> <ul> <li>\u201cLuddite\u201d is now a blanket term used to describe people who dislike new technology, but its origins date back to an early 19th-century labor movement that railed against the ways that mechanized manufacturers and their unskilled laborers undermined the skilled craftsmen of the day</li> <li>Most were trained artisans who had spent years learning their craft, and they feared that unskilled machine operators were robbing them of their livelihood</li> <li>cheap competition of early textile factories particularly threatening to the artisans, a few desperate weavers\u00a0began breaking into factories and smashing textile machines</li> <li>Machine-breaking Luddites attacked and burned factories, and in some cases, they even exchanged gunfire with company guards and soldiers</li> <li>It wasn\u2019t until the 20th century that their name re-entered the popular lexicon as a synonym for \u201ctechnophobe.\u201d</li> </ul> </li> </ul>"},{"location":"Articles/DL/Artists%20vs%20AI/","title":"Artists vs AI","text":"<p>toc: true title: Artists vs AI</p> <p>categories: [\"article\"] date modified:  date created: </p>"},{"location":"Articles/DL/Artists%20vs%20AI/#artists-vs-ai-companies-the-real-battle-under-the-hype","title":"Artists vs AI (companies) : The real battle under the hype","text":""},{"location":"Articles/DL/Experiments%20with%20LLM%20temperature/","title":"Experiments with Temperature","text":""},{"location":"Articles/DL/Experiments%20with%20LLM%20temperature/#experimenting-with-temperature-llms","title":"Experimenting With Temperature (LLMs)","text":"<p>Over the past few months at OpenML, we have been experimenting with LLM models in an attempt to improve the search experience for our users. While our existing implementation uses ElasticSearch, we wanted to also have the option of having a more \"semantic\" search experience. </p> <p>Aside from the usual RAG pipeline that everyone and their grandparents seems to be using these days, we also wanted to experiment with using an LLM to semi-automatically generate filters for our search queries. While it may not seem like a big feature, it is something that has always been a bit of an annoyance for some of our users. </p> <p>So what does this entail? Consider the interface we have at the moment. We have a search bar at the top, and subsequently a bunch of filters that users can use to narrow down their search. While this works pretty well as is, how about trying to automate it a bit.</p> <p>In summary, we want a query like \"find me a large dataset with multiple classes of flowers\" to automatically generate filters like \"classification\", \"multiclass\", \"sort by size of dataset\" etc.</p> <p></p>"},{"location":"Articles/DL/Experiments%20with%20LLM%20temperature/#temperature","title":"Temperature","text":"<p>Think about the first time you used ChatGPT. What stood out to you? Was it how well it could elaborate on a topic? Or was it how creative it could be? The temperature parameter in LLMs is what controls this. </p> <p>How can we control creativity? Well, saying that we can directly control creativity is a bit of a stretch. We can however use a workaround.</p> <p>Do you remember the softmax function? The function that takes a vector of arbitrary real-valued scores and squashes it into a vector of probabilities that sum to 1. The inputs to the softmax function are the unnormalized log likelikhoods or the raw per class score assigned by the model. </p> <p>The softmax function is defined as:</p> \\[\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^{k} e^{x_j}}\\] <p>If we want more control over the distribution of the probabilities, we can use a temperature parameter. This would look like:</p> \\[ \\text{softmax}(x_i) = \\frac{e^{x_i/T}}{\\sum_{j=1}^{k} e^{x_j/T}} \\] <p>where \\(T\\) is the temperature parameter.</p> <ul> <li> <p>If \\(T = 1\\), the softmax function is the same as the original softmax function.</p> </li> <li> <p>If \\(T &gt; 1\\), the probabilities will become \"flatter\". Since the difference between the probabilities will be less, the model can be more exploratory aka more creative.</p> </li> <li> <p>If \\(T &lt; 1\\), the distribution of the probabilities are \"peakier\". There will be a higher difference between the probabilities, leading to the model being more confident in its predictions, but also less creative.</p> </li> </ul>"},{"location":"Articles/DL/Experiments%20with%20LLM%20temperature/#visualizing-temperature-using-softmax","title":"Visualizing Temperature Using Softmax","text":"<pre><code>from tqdm import tqdm\nimport regex as re\n# LangChain supports many other chat models. Here, we're using Ollama\nfrom langchain_community.chat_models import ChatOllama\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom typing import List, Dict, Any\nimport numpy as np  \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd \nsns.set_theme(style=\"white\")\n</code></pre> <pre><code>def softmax(input, t=1.0):\n  ex = np.exp(input/t)\n  sum = np.sum(ex, axis=0)\n  return ex / sum\n</code></pre> <pre><code># plot softmax over a range of inputs\nx = np.arange(0,1.0, 0.01)\nt = np.array([0.1,.5, .8, 1.0])\ny = np.array([softmax(x, ti) for ti in t])\n\n# Create a DataFrame for Seaborn\ndata = pd.DataFrame({\n    'x': np.tile(x, len(t)),\n    'softmax': np.concatenate(y),\n    't': np.repeat(t, len(x))\n})\n\n# Plotting with Seaborn\nplt.figure(figsize=(8, 6))\nsns.lineplot(data=data, x='x', y='softmax', hue='t', palette='viridis')\nplt.xlabel('x')\nplt.ylabel('softmax(x)')\nplt.toc: true\ntitle('Softmax Function for Different Values of t')\nplt.legend(toc: true\ntitle='t')\nplt.show()\n</code></pre>"},{"location":"Articles/DL/Experiments%20with%20LLM%20temperature/#creating-the-experimental-setup","title":"Creating the Experimental Setup","text":"<p>Now, we can focus on testing the effects of temperature for our use case. We are using the <code>llama3</code> model for our experiments. The experiments are being run on a 2023 MacBook Pro with an M3 chip and 18GB memory.</p>"},{"location":"Articles/DL/Experiments%20with%20LLM%20temperature/#defining-a-prompt","title":"Defining a Prompt","text":"<p>We need to first think of a prompt that we can use for our experiments. This prompt can be thought of as an instruction that the model uses along with the query to generate answers. To make it easier for us to use, we only want one/two word answers and for now we are only focusing on a small subset of the filters that we want our model to understand.</p> <pre><code>prompt = \"\"\"User Query : {query}\nBased on the query, answer the following questions one by one in one or two words only and a maximum of two with commas only if asked for. Use only the information given and do not make up answers - \nDoes the user care about the size of the dataset? Yes/No and if yes, ascending/descending.\nDoes the user want to sort by number of downloads? Yes/No.\nDoes the user care about missing values? Yes/No.\nIf it seems like the user wants a classification dataset, is it binary/multi-class/multi-label? If not, say none.\n\"\"\"\n</code></pre> <pre><code>query = \"Find me a big classification dataset about mushrooms\"\n</code></pre>"},{"location":"Articles/DL/Experiments%20with%20LLM%20temperature/#creating-a-chain","title":"Creating a Chain","text":"<p>Since we are using the <code>langchain</code> and <code>ollama</code> libraries for our experiments, we follow their API and create a chain. The template uses string formatting to insert the prompt and the query into the chain.</p> <pre><code>def create_chain(prompt , temperature, llm_model = \"llama3\"):\n    prompt = ChatPromptTemplate.from_template(prompt)\n    llm = ChatOllama(model=llm_model, temperature=temperature)\n    chain = prompt | llm | StrOutputParser()\n    return chain\n</code></pre>"},{"location":"Articles/DL/Experiments%20with%20LLM%20temperature/#parsing-the-results","title":"Parsing the Results","text":"<p>To make it easier for us to analyze the results, we generate an example answer and then see see if any further processing is needed.</p> <pre><code># functiont to parse responses like this to a list of yes/no/none/yes,aescending/no etc\ndef parse_response(response):\n    # split by new line and remove first two lines (here are the answers:)\n    response = response.split('\\n')[2::]\n    # if response has a question mark, split by question mark and remove empty strings\n    for i in range(len(response)):\n        if '?' in response[i]:\n            response[i] = response[i].split('?')[1].strip()\n    # replace full stops with empty strings\n    response = [x.replace('.','') for x in response]\n    response = [x for x in response if x]\n    return response\n</code></pre> <pre><code>chain = create_chain(prompt, 0.5)\nresponse = chain.invoke({\"query\": query})\nprint(response)\n</code></pre> <pre><code>Here are the answers:\n\n1. Does the user care about the size of the dataset?\nYes, ascending.\n\n2. Does the user want to sort by number of downloads?\nNo\n\n3. Does the user care about missing values?\nNo\n\n4. Is it a classification dataset? If so, is it binary/multi-class/multi-label?\nYes, multi-class\n</code></pre> <p>Yay, it works. We now write a function to generate results for different temperatures.</p> <pre><code>def generate_results_for_temp(query:str, range_of_temps : np.ndarray) -&gt; List[List[str]:\n    results = []\n    for temperature in tqdm(range_of_temps):\n        chain = create_chain(prompt, temperature)\n        response = chain.invoke({\"query\": query})\n        results.append(parse_response(response))\n    return results\n\n</code></pre>"},{"location":"Articles/DL/Experiments%20with%20LLM%20temperature/#running-the-experiments-and-plotting-results","title":"Running the Experiments and Plotting Results","text":"<p>It is time to run the experiments and plot the results.  We write a function to plot the results in a <code>stripplot</code> to see the distribution of the answers for different temperatures.</p> <pre><code>def plot_yes_no(df: pd.DataFrame, toc: true\ntitle:str) -&gt; None:\n    fig, axs = plt.subplots(2, 2, figsize=(15, 15))\n    fig.suptoc: true\ntitle(toc: true\ntitle)\n    sns.stripplot(data=df, x='size', y='temperature', ax=axs[0, 0], hue='size')\n    sns.stripplot(data=df, x='sort_by_downloads', y='temperature', ax=axs[0, 1], hue='sort_by_downloads')\n    sns.stripplot(data=df, x='missing_values', y='temperature', ax=axs[1, 0], hue='missing_values')\n    sns.stripplot(data=df, x='classification_type', y='temperature', ax=axs[1, 1], hue='classification_type')\n    # tilt x axis labels\n    for ax in axs.flat:\n        ax.set_xticklabels(ax.get_xticklabels(), rotation=30, horizontalalignment='right')\n    plt.show()\n</code></pre> <p>Sometimes, the model returns an extra field, we combine the last two fields to plot the results. (This is a bit of a hack, but it works for now and is ONLY used for plotting)</p> <pre><code>def combine_last_two_elements(lst):\n    # Check if the list has at least two elements\n    if len(lst) &gt; 4:\n        # Combine the last two elements with a space separator\n        combined_element = lst[-2] + ' ' + lst[-1]\n\n        # Create a new list with combined element instead of the last two\n        return lst[:-2] + [combined_element]\n    else:\n        return lst\n</code></pre>"},{"location":"Articles/DL/Experiments%20with%20LLM%20temperature/#experiment-1","title":"Experiment 1","text":"<p>Out first experiment is a rather simple query, \"Find me a big classification dataset about mushrooms\". As you can probably guess, we are looking for a dataset that is large, is a classification dataset and is about mushrooms.</p> <pre><code>range_of_temps = np.linspace(0, 1, 20)\nquery = \"Find me a big classification dataset about mushrooms\"\nresults1 = generate_results_for_temp(query, range_of_temps)\n</code></pre> <pre><code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:49&lt;00:00,  2.49s/it]\n</code></pre> <pre><code>results1 = [y for y in x if all(sub not in y for sub in [\"If\", \":\"])] for x in results1]\n</code></pre> <pre><code>df = pd.DataFrame(results1, columns = ['size', 'sort_by_downloads', 'missing_values', 'classification_type'])\ndf['temperature'] = range_of_temps\nplot_yes_no(df, toc: true\ntitle = query)\n</code></pre> <p></p> <p>Rather interesting don't you think? At higher temperatures, the model gets the answers wrong. Even at a temperature slightly above 0.1, the model starts adding extra information to it's answers.</p> <p>Did you notice that I tried to remove sentences that started with \"If\"? There are more examples of this later, but this is because at higher temperatures, the model tends to add random sentences to the answers and this makes it quite hard to plot them.</p>"},{"location":"Articles/DL/Experiments%20with%20LLM%20temperature/#experiment-2","title":"Experiment 2","text":"<p>Our second experiment is super easy. \"Find me a dataset that has a lot of missing values and order by number of downloads\". As you can obviously guess, we are looking for a dataset that has a lot of missing values and we want to order the results by the number of downloads.</p> <pre><code>range_of_temps = np.linspace(0, 1, 20)\nquery = \"Find me a dataset that has a lot of missing values and order by number of downloads\"\nresults2 = generate_results_for_temp(query, range_of_temps)\nresults2 = [y for y in x if \"so\" not in y] for x in results2]\n\n</code></pre> <pre><code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:34&lt;00:00,  1.74s/it]\n</code></pre> <pre><code>df = pd.DataFrame(results2, columns = ['size', 'sort_by_downloads', 'missing_values', 'classification_type'])\ndf['temperature'] = range_of_temps\nplot_yes_no(df, toc: true\ntitle = query)\n</code></pre> <p></p> <p>Hmm, same as before. The model starts adding extra information at higher temperatures and starts getting the answers wrong. (Yes, No?? ) What kind of answer is that?</p>"},{"location":"Articles/DL/Experiments%20with%20LLM%20temperature/#experiment-3","title":"Experiment 3","text":"<ul> <li>Now a slightly more complex query. \"Find me a dataset that has 10 classes and sort by number of downloads\". We want it to understand that we want a multiclass classification dataset and we want to sort the results by the number of downloads.</li> </ul> <pre><code>range_of_temps = np.linspace(0, 1, 20)\nquery = \"Find me a dataset that has 10 classes and sort by number of downloads\"\nresults3 = generate_results_for_temp(query, range_of_temps)\n</code></pre> <pre><code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:55&lt;00:00,  2.80s/it]\n</code></pre> <pre><code>results3 = [combine_last_two_elements(x) for x in results3]\n</code></pre> <pre><code>df = pd.DataFrame(results3, columns = ['size', 'sort_by_downloads', 'missing_values', 'classification_type'])\ndf['temperature'] = range_of_temps\nplot_yes_no(df, toc: true\ntitle = query)\n</code></pre> <p>This seems to have been very easy for the model. But as always, the model starts adding extra information at higher temperatures. A lot of extra information in fact. Even though the prompt says to ONLY answer with one or two words</p>"},{"location":"Articles/DL/Experiments%20with%20LLM%20temperature/#experiment-4","title":"Experiment 4","text":"<ul> <li>\"Find me a dataset that 2 classes and is a big dataset\". You know the drill by now. We want a binary classification dataset that is large.</li> </ul> <pre><code>range_of_temps = np.linspace(0, 1, 20)\nquery = \"Find me a dataset that 2 classes and is a big dataset\"\nresults4 = generate_results_for_temp(query, range_of_temps)\n</code></pre> <pre><code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:42&lt;00:00,  2.14s/it]\n</code></pre> <pre><code>results4 = [combine_last_two_elements(x) for x in results4]\n</code></pre> <pre><code>df = pd.DataFrame(results4, columns = ['size', 'sort_by_downloads', 'missing_values', 'classification_type'])\ndf['temperature'] = range_of_temps\nplot_yes_no(df, toc: true\ntitle = query)\n</code></pre> <p>Notice how some things changed? At higher temperatures, we get extended answers.</p>"},{"location":"Articles/DL/Experiments%20with%20LLM%20temperature/#conclusion","title":"Conclusion","text":"<p>In conclusion, we can see that we should probably stick to lower temperatures for our use case. As we go higher, the model starts being more \"creative\" and either adds extra information to the answers or gets them wrong. While this behaviour might be useful in cases like creative writing, it is not something we want in our search.</p> <p>Using LLMs can sometimes be a bit of a hit or miss. But of course, learning to control it's parameters can help us get the most out of it. This blog post was just a simple experiment, but in the deluge of content made by people who have no idea what Softmax is, I hope this was helpful.</p>"},{"location":"Articles/DL/Experiments%20with%20LLM%20temperature/#fin","title":"Fin","text":"<p>This article is in the hopes that it will help someone out. Maybe have the help that I did not. I do not know who it will reach. But to whoever it does, best of luck :)</p> <p>Like these/Want more? Buy me a coffee! Kofi</p> <p>Want articles on something specific? Just ask!</p> <p>You can always contact me on LinkedIn, or drop me an Email</p>"},{"location":"Articles/DL/Parsing%20and%20Querying%20Tensorboard%20logs%20-%20A%20Mini%20Tutorial/","title":"Parsing and Querying Tensorboard logs   A Mini Tutorial","text":"<p>toc: true title: Parsing and Querying Tensorboard logs - A Mini Tutorial</p> <p>categories: [\"article\"] date modified:  date created: </p>"},{"location":"Articles/DL/Parsing%20and%20Querying%20Tensorboard%20logs%20-%20A%20Mini%20Tutorial/#parsing-and-querying-tensorboard-logs-a-mini-tutorial","title":"Parsing and Querying Tensorboard logs: A Mini Tutorial","text":"<p>So, you wanted to parse your Tensorboard logs, didn\u2019t you? Did you try using GPT-3? OH! GPT-4? Well. Guess that didn\u2019t give you what you wanted.  Yeah, me neither. So here we are. Read on and you will find out how to take all the runs you logged to Tensorboard, clean them up, and put them in a single DataFrame. From there, you can query it as you would any other table. </p>"},{"location":"Articles/DL/Parsing%20and%20Querying%20Tensorboard%20logs%20-%20A%20Mini%20Tutorial/#what-do-we-want","title":"What do we want?","text":"<p>Tensorboard is one of the more popular means of logging your deep learning experiments. The issue though, is that it is hard to run custom queries over the already-created graphs. Now, if you were saving your results separately, this would not be an issue. But you probably weren\u2019t were you? (Yeah, me neither.) So we want to iterate over all the logs, for every file we create rows and then columns for each of the tags you saved (eg: Loss, accuracy, etc.) And as a bonus, this script also takes into account all those juicy images you saved from the last time you wanted to try running a DCGAN, again. (Or a cats and dogs classifier, I don't know)</p>"},{"location":"Articles/DL/Parsing%20and%20Querying%20Tensorboard%20logs%20-%20A%20Mini%20Tutorial/#shush-and-show-me-the-code","title":"Shush and show me the code?!","text":"<p>Yes, I know this could just be a code snippet on Stack Overflow and there was no real need for this article. If you are an intermediate/advanced programmer, honestly just skip ahead and grab the code here. </p> <p>Now if you did go and skip ahead and realized that it made no sense, welcome back. Read on and hopefully, your doubts will be answered.</p>"},{"location":"Articles/DL/Parsing%20and%20Querying%20Tensorboard%20logs%20-%20A%20Mini%20Tutorial/#imports","title":"Imports","text":"<p>As usual, we need to grab some libraries for this to work. You probably have most of them anyway. We need os and pathlib to read the files, pandas for the dataframe, and numpy if you want to further process your data. Tqdm is a little progress bar helper that prints a pretty little progress bar as you wait for your loop to finish running. PIL will be used to read the Image files. BytesIO and base64 will be used to decode the images from Tensorboard so we can save them to the dataframe. Finally, we also do need the Tensorboard package, but if you didn\u2019t have that already then what are you doing here?  Why pickle you might ask? You will see.</p> <p>All of these packages are available either as a pip or a conda/mamba install. So just run <code>pip install &lt;x&gt;</code> and it should hopefully work.</p> <p>Now that we have that out of the way, let\u2019s define the path where your logs are. (The same one you pass to the \u2014logdir argument on Tensorboard)</p> <pre><code>import os\nimport pandas as pd\nfrom tensorboard.backend.event_processing.event_accumulator import EventAccumulator\nfrom tqdm import tqdm\nfrom pathlib import Path\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport base64\nfrom io import BytesIO\nfrom PIL import Image\nimport pickle\n\nmain_path = './runs/' # CHANGE THIS\n</code></pre>"},{"location":"Articles/DL/Parsing%20and%20Querying%20Tensorboard%20logs%20-%20A%20Mini%20Tutorial/#find-tfevent-files","title":"Find TFevent files","text":"<p>Tensorboard uses a custom format that it calls a \u2018tfevent\u2019. If you do look at your logs, you will see that the format of the files is either \u2018tfevent\u2019, \u2018checkpoint\u2019, and of course whatever else you have saved.</p> <p>The first step is then to just read all the tfevent files from the directory. We will use the walk function and pathlib to find all of the relevant files.  Why convert it to a Path object? Using the Path function from the pathlib instead of just a string for the directories will allow us to quickly perform operations on the directory if we need it. (For path.name will give us the file name from the full path.)  We just save the complete paths of each of the files to an array here.</p> <pre><code>def get_event_files(main_path):\n    \"\"\"Return a list of event files under the given directory\"\"\"\n    all_files = []\n    for root, _, filenames in os.walk(main_path):\n        for filename in filenames:\n            if \"events.out.tfevents\" in filename:\n                all_files.append(str(Path(root) / Path(filename)))\n    return all_files\n</code></pre>"},{"location":"Articles/DL/Parsing%20and%20Querying%20Tensorboard%20logs%20-%20A%20Mini%20Tutorial/#create-dataframe","title":"Create DataFrame","text":"<p>Our objective is to collect all the metadata from the logs and convert it to a single DataFrame. To this accord, we get a list of all the relevant tfevents using the previous function. We then create an empty dictionary to store file-wise information. The EventAccumulator function uses the Tensorboard API to read the tfevent file and so we run every file through it. To make sure we read the logs from the start, we also reload the object.  Once we have this object, we can pass it to a function that will return the relevant tags to a dictionary with the file name as a key and the tags as a value. This function will be discussed in the next section. Once we run over all the files, we have a dictionary of dictionaries with all the information we need. Now, pandas provides a function to convert a nested dictionary to a DataFrame, so we use it directly.  If you look at this output, you will see that the rows are the tags while the columns are individual files. If this version works for you, then do use it!  I find it easier to have the tags as column names and each row of the DataFrame for file information. I also wish to access the file names later so I use the reset_index() function that will essentially make sure all the columns have names here. (In this case, the file names will be named \u2018index\u2019. You can change this if you want by passing in a columns=[] that you want to the from_records function.) <pre><code>def process_runs(main_path):\n    all_files = get_event_files(main_path=main_path)\n    all_dict = {}\n    for files in tqdm(all_files, total = len(all_files)):\n        event_acc = EventAccumulator(files)\n        event_acc.Reload()\n        temp_dict = process_event_acc(event_acc)\n        all_dict[files] = temp_dict\n    return pd.DataFrame.from_records(all_dict).T.reset_index()\n</code></pre>"},{"location":"Articles/DL/Parsing%20and%20Querying%20Tensorboard%20logs%20-%20A%20Mini%20Tutorial/#process-an-event","title":"Process an Event","text":"<p>Now for the main bit. Depending on what you were training, you probably have many types of logs that you saved. In my case, these are Scalars, Tensors, or Images. The following function processes these, but it is easy enough to extend to whatever you want. From the process_runs function, we pass in the EventAccumulator object to this function. This object contains the tags that we saved each of our logs as. (For eg: \u2018Train/Acc\u2019, \u2018Train/Loss\u2019 etc.)  I do not want to manually type these every time, and would rather use a function to do that for me.</p> <p>These tags are first divided into the category of object it is, for instance, \u201cimages\u201d, \u201cscalars\u201d, \u201chistogram\u201d etc. We will need to write a separate pre-processing step for each object depending on what information we want from them.</p> <p>Note that for each of the tags, the subtags are the names of the actual values that we want. (Eg: For the tag \u2018scalars\u2019, we have \u2018Train/Acc\u2019).</p>"},{"location":"Articles/DL/Parsing%20and%20Querying%20Tensorboard%20logs%20-%20A%20Mini%20Tutorial/#scalars","title":"Scalars","text":"<p>These are probably numerical values that you saved. Say the loss, accuracy, number of classes, etc. For this type of data, we first read the Scalar value using the event_acc.Scalars command. We need to pass in the name of the subtag that we want to look at (eg: \u2018Train/Acc\u2019).  Now, if you were looking at values that change throughout training, you will probably get a list here. For instance, you will get an epoch-wise accuracy list. Since I only want the final accuracy/loss etc, I am only returning the final index.  Feel free to customize it to whatever you need.</p>"},{"location":"Articles/DL/Parsing%20and%20Querying%20Tensorboard%20logs%20-%20A%20Mini%20Tutorial/#tensors","title":"Tensors","text":"<p>The name gives it away, but these are the values that you might find have the extra tag \u2018/text_summary\u2019. We can access these by using the event_acc.Tensors command. Now if you inspect the object, you will see that the actual value can be found in the first index of this object. The value is stored in the \u2018string_val\u2019 field of this object, so we take that out and again index it into the first element. (You will see why if you print the object. No further explanations are given because that\u2019s just how the API is. It also depends on what exactly you want to save of course.)  Now if you look at the final output, you will find that it looks something like b\u2019some value\u2019. This is an encoded string, and to convert it to a normal string, we have to decode it as an ASCII character string. Pretty easily done.</p>"},{"location":"Articles/DL/Parsing%20and%20Querying%20Tensorboard%20logs%20-%20A%20Mini%20Tutorial/#images","title":"Images","text":"<p>Images are a bit of a complicated case here. If you look at the tensor board object that we get from event_acc.Images, you will see that it is a Bytes object and not a numpy array/PIL image. This is just the format Tensorboard chose, so all we can do is accept it and convert it to our needs. After indexing into the correct component of the object, the field \u2018encoded_image_string\u2019 holds, well, the encoded image string. We take that and convert it to a BytesIO object. This is a format that PIL can read as an image, so we read it as one. </p>"},{"location":"Articles/DL/Parsing%20and%20Querying%20Tensorboard%20logs%20-%20A%20Mini%20Tutorial/#other-formats","title":"Other formats","text":"<p>Now if you have something like a histogram, you hopefully get how to process it. Use event_acc.Histogram for instance, and then apply whatever transform you want to it. I do not need them yet, but I might add them to this article later on. </p> <pre><code>def process_event_acc(event_acc):\n    \"\"\"Process the EventAccumulator and return a dictionary of tag values\"\"\"\n    all_tags = event_acc.Tags()\n    temp_dict = {}\n    for tag in all_tags.keys():\n        if tag == 'scalars':\n            for subtag in all_tags[tag]:\n                temp_dict[subtag] = [tag[-1] for tag in event_acc.Scalars(tag=subtag)][-1]\n        if tag == 'tensors':\n            for subtag in all_tags[tag]:\n                temp_dict[subtag.replace('/text_summary', \"\")] = [tag[-1] for tag in event_acc.Tensors(tag=subtag)][0].string_val[0].decode('ascii')\n        if tag == 'images':\n            for subtag in all_tags[tag]:\n                temp_dict[subtag] = Image.open(BytesIO(event_acc.Images(subtag)[1].encoded_image_string))\n    return temp_dict\n</code></pre>"},{"location":"Articles/DL/Parsing%20and%20Querying%20Tensorboard%20logs%20-%20A%20Mini%20Tutorial/#a-caveat","title":"A Caveat","text":"<p>Now, there is a catch. After creating the DataFrame, if you save it as a \"csv\" object, it becomes impossible to load the images back. This happens because the object is saved as a string like \u2018` instead of the actual image.  Instead of that, you can save the DataFrame as a pickled object.  <pre><code>import pickle\nwith open(\"pickled_df.pkl\", \"wb+\") as f:\n    pickle.dump(combined_df, f)\n\nwith open(\"pickled_df.pkl\", \"rb+\") as f:\n    combined_df = pickle.load(f)\n</code></pre>"},{"location":"Articles/DL/Parsing%20and%20Querying%20Tensorboard%20logs%20-%20A%20Mini%20Tutorial/#load-and-clean-dataframe","title":"Load and clean DataFrame","text":"<p>To get the DataFrame we so badly desire, we just run the functions we wrote before. And hopefully, if everything worked fine, you can go home and sleep. (Or if not, sorry! I hope you only have a few hours of your workday left.) Another step I want to mention is the ability to ignore failed runs. If you were tracking failure, then just use that object. If you weren\u2019t, then just look at the columns that are written at runtime. For instance, I always write the name of the experiment, and if even a single epoch was completed, then there should be a validation loss as well.  To filter the data, I just take the rows that have values for these. (As usual, depends on what you want.)</p> <pre><code>combined_df = process_runs(main_path=main_path)\ncombined_df = combined_df[(~pd.isnull(combined_df['experiment_name'])) &amp; (~pd.isnull(combined_df['Loss/Val']))]\ncombined_df.head()\n</code></pre>"},{"location":"Articles/DL/Parsing%20and%20Querying%20Tensorboard%20logs%20-%20A%20Mini%20Tutorial/#display-images","title":"Display images","text":"<p>The final part of the code is looking at the images. Filter out what you want, and pick the row and column name as you would index a text object. Done! If you are using a Jupyter notebook, then you should see the image pop up. If you are running this program as a script, you will have to use .show()  <pre><code>filtered_df = combined_df[(~pd.isnull(combined_df['converted_proxy'])) &amp; (~pd.isnull(combined_df['original_images']))]\nfiltered_df.iloc[0].original_images\n</code></pre>"},{"location":"Articles/DL/Parsing%20and%20Querying%20Tensorboard%20logs%20-%20A%20Mini%20Tutorial/#what-next","title":"What next?","text":"<p>Depends on how much you are getting paid. :) Jokes aside, this is just a regular DataFrame now. So you can write all the queries you want. Do you want to know how badly your model did? Sure, look at the columns. Did you write some complex logic and now forgot what your actual project was? Oops. Open the DataFrame in Excel and cry. But I am sure you will manage. After all, you\u2019ve got this far haven\u2019t you?</p>"},{"location":"Articles/DL/Parsing%20and%20Querying%20Tensorboard%20logs%20-%20A%20Mini%20Tutorial/#why-not-wandbwb-etc-etc","title":"Why not Wandb/W&amp;B etc etc","text":"<p>I do want to mention that I am not against any of the other logging platforms. Honestly, they do some pretty great work. But I am used to Tensorboard, and having my data offline and not on someone else\u2019s cloud (jokes on me, this article is on someone else\u2019s cloud) is nice.  Use whatever works for you. Or write your own. Heck, use a CSVLogger and save what you want directly to a CSV. </p>"},{"location":"Articles/DL/Parsing%20and%20Querying%20Tensorboard%20logs%20-%20A%20Mini%20Tutorial/#fin","title":"Fin","text":"<p>This article is in the hopes that it will help someone out. Maybe have the help that I did not. I do not know who it will reach. But to whoever it does, best of luck :)</p> <p>Like these/Want more? Buy me a coffee! Kofi</p> <p>Want articles on something specific? Just ask!</p> <p>You can always contact me on LinkedIn, or drop me an Email. For all the code, drop by my Github.</p>"},{"location":"Articles/DL/Tree%20of%20Thoughts%20Explained%20Simply%20%28LLMs%20Dehyped%20-%201%29/","title":"Tree of Thoughts Explained Simply (LLMs Dehyped   1)","text":"<p>toc: true title: Tree of Thoughts Explained Simply (LLMs Dehyped - 1)</p> <p>categories: [\"article\"] date modified:  date created: </p>"},{"location":"Articles/DL/Tree%20of%20Thoughts%20Explained%20Simply%20%28LLMs%20Dehyped%20-%201%29/#tree-of-thoughts-explained-from-foundations-llms-unhyped-1","title":"Tree of Thoughts Explained From Foundations (LLMs Unhyped - 1)","text":"<p>\u201cProgrammers are, in their hearts, architects, and the first thing they want to do when they get to a site is to bulldoze the place flat and build something grand.\u201d : Joel Spolsky, co-founder of Stack Overflow</p>"},{"location":"Articles/DL/Tree%20of%20Thoughts%20Explained%20Simply%20%28LLMs%20Dehyped%20-%201%29/#introduction","title":"Introduction","text":"<p>Large language models like GPT4 have taken over the world and has left every second IT person to scramble towards the next great AI solution. Given the monetary benefits, a lot of research has popped up in a very short time. While this is very exciting, it is wise to look a little deeper beneath the hype. The \u201cnew groundbreaking research from {X} that will change your company\u201d is sometimes a tiny change or a creative use of an existing concept.  This is not to pour cold water over your dreams, but a way to help you understand these concepts better. If you did not come from a technical computer science background, this constant influx of \u201cgroundbreaking research\u201d can get very overwhelming.</p> <p>So here is a more sober, in depth view of how the \u201cTree of thoughts\u201d [1] paradigm came into being from concepts that have been around for decades and creatively applied to LLMs.</p> <p>Note : Both the research and my understanding of it fluctuates over time and if something changes, I will try to come back and correct it. If you notice something weird, do drop a comment!</p>"},{"location":"Articles/DL/Tree%20of%20Thoughts%20Explained%20Simply%20%28LLMs%20Dehyped%20-%201%29/#what-is-the-tree-of-thought","title":"What is the Tree of Thought?","text":"<p>If you have not already encountered it, the Tree of Thoughts claims to help an LLM arrive at a more logical conclusion and also generate the steps it took to come to it.</p> <p>It was first mentioned in a paper by Yao et al. [1] and was further expanded on by a LOT of articles and papers. As the authors say, it is a way of \u201cDeliberate Problem Solving with Large Language Models\u201d.</p> <p>But you might ask, why do we care? I just want my assignment done for me.</p>"},{"location":"Articles/DL/Tree%20of%20Thoughts%20Explained%20Simply%20%28LLMs%20Dehyped%20-%201%29/#why-bother-adding-it-to-an-llm","title":"Why Bother Adding it to an LLM?","text":"<p>To better understand why we care about algorithms like this, we need to dig into some of the shortcomings behind LLMs. - Fixed Knowledge : A LLM is trained on a large text database encompassing a huge chunk of the web. But the web is not a fixed resource, neither is the information in it. Unless the model is trained with new data or an external data source is given to it, it\u2019s \u201cknowledge\u201d is essentially fixed. - Cost of training : Training a LLM like ChatGPT is extremely costly, and it is just not possible to keep updating the model everytime something new pops up on the internet.  - Knowing everything is not the point : At it\u2019s heart, an LLM is not meant to know everything. It is just a text model - it predicts the next word in a sentence. Using it as a model that can understanding text and it\u2019s underlying relations is a better and cheaper way to use it. - Logic : It is not easy to understand the steps an LLM took to arrive at an answer. Neither is it easy to make it follow logical steps without knowing the logic in the first place. - Structured Data : LLMs are great at making sense of large amounts of unstructured data. But it is not meant to be good at working with structured data like graphs by itself. There are workarounds, but it is not something an LLM can do by itself.</p> <p>Given these shortcomings, the Tree of Thought is an attempt at combining classical \u201clogical search algorithms\u201d with LLMs. How? Well, read on.</p>"},{"location":"Articles/DL/Tree%20of%20Thoughts%20Explained%20Simply%20%28LLMs%20Dehyped%20-%201%29/#theoretical-background","title":"Theoretical Background","text":"<p>This section is for you, the reader who want to dig deeper and understand the actual concepts that led to the research. If you just want to mash together models and don\u2019t really care how they work. Just skip to the next section.</p> <p>Now, if you have a background in computers, you will recognize these terms. If you don\u2019t, then pay attention. It will help you understand a lot of research in this domain.</p>"},{"location":"Articles/DL/Tree%20of%20Thoughts%20Explained%20Simply%20%28LLMs%20Dehyped%20-%201%29/#chain-of-thought","title":"Chain of Thought","text":"<p>The Tree of Thought is derived from a theoretical computer science concept - the \u201cChain of Thought\u201d.  Simply put, the Chain of Thought is a logical breakdown of a task into it\u2019s simplest steps. Instead of trying to solve a problem as a whole, we try to solve it in parts and then combine the result. Why? Well, it is a lot easier to do and writing it formally makes it possible for others to understand how you arrived at the solution. (Think of trying to solve for x in a math equation.) It also adds in error checking. If something went wrong, you can backtrack and find the bug.</p> <p>Eg: You want to check if it\u2019s raining outside. How would you break down the steps? Look outside the window -&gt; Check for rain clouds -&gt; Check if there are raindrops -&gt; Check the ground to see if it is wet -&gt; If all the conditions are satisfied, you know it is raining.</p>"},{"location":"Articles/DL/Tree%20of%20Thoughts%20Explained%20Simply%20%28LLMs%20Dehyped%20-%201%29/#trees","title":"Trees","text":"<p>In computing, a tree is a way of representing this chain of thought. It\u2019s main advantage is that once we create a tree of steps, it is possible to iterate over it and reach a logical conclusion or explore options.</p> <p>A tree has a root node (eg: Is it raining) and leaf nodes (eg: rain clouds, wetness). The leaf nodes are arranged in levels and are connected to previous levels (eg: clouds -&gt; ((present) , (absent)) )</p>"},{"location":"Articles/DL/Tree%20of%20Thoughts%20Explained%20Simply%20%28LLMs%20Dehyped%20-%201%29/#traversing-a-tree","title":"Traversing a Tree","text":"<p>Once we have a tree, there are many ways of traversing on it. Choosing an algorithm usually depends on what you need, and how much compute you are willing to use. Some of the approaches used in the paper are as follows (high level explanations): </p> <ul> <li>Breadth First Search : This is used to find the shortest path from the root to a leaf. The tree is traversed layer by layer and all nodes at each level are evaluated. If a match is found, the algorithm stops. This is quite fast.</li> <li>Depth First Search : This is used if you want to explore your options and find new possibilites. The tree is traversed by starting at a node and going as deep as possible from there until the end. If a match is found, the algorithm stops. If not, it backtracks and goes to the next node. This is much slower, but is useful in certain cases.</li> <li>Other Options : Covering the whole lot (like A*) is beyond the scope of this article, but you can refer to [4] if you want to learn more.</li> </ul>"},{"location":"Articles/DL/Tree%20of%20Thoughts%20Explained%20Simply%20%28LLMs%20Dehyped%20-%201%29/#ensemble-learning","title":"Ensemble Learning","text":"<p>A large portion of ML algorithms are stochastic (if you run it again, you will get a different result). While this is good for tasks like creative writing, it is not great if you want logical answers.  One way around this is to use multiple similar models on the same data and then \u201caverage\u201d out the results. This somewhat helps to counter the randomness and usually leads to better performance. </p> <p>There are many ways of combining these results - Weight them and use a mix of them, use a majority vote, use a separate model to evaluate which result is better etc. </p> <p>Can you see how this would be useful for an LLM when trying to solve a logical problem?</p> <p>Want to learn more? Refer to [3].</p>"},{"location":"Articles/DL/Tree%20of%20Thoughts%20Explained%20Simply%20%28LLMs%20Dehyped%20-%201%29/#tricking-an-llm","title":"Tricking an LLM","text":"<p>Now that you understand the background, let us dive into how this works with an LLM.  So, what do we want to do? Simply put, we want the LLM to come up with different answers that we can then put into a graph. We can then use this graph to arrive at a more logical solution. Spoiler : Can you see why this would not always be a good idea?</p>"},{"location":"Articles/DL/Tree%20of%20Thoughts%20Explained%20Simply%20%28LLMs%20Dehyped%20-%201%29/#prompts","title":"Prompts","text":"<p>How do we do it? Quite simple really, we first define a format such as \u201cthe answer is {n} because {x1} and {x2} lead to {n}\u201d. We then prompt multiple times using a prompt like \u201csolve it in multiple ways while pretending to be three different experts\u201d and save the results. We also add a prompt like \u201conly use the information give\u201d, and voila! We have a graph.</p> <p>Well, mostly. The answers you get might or might not be useful.</p>"},{"location":"Articles/DL/Tree%20of%20Thoughts%20Explained%20Simply%20%28LLMs%20Dehyped%20-%201%29/#ensemble","title":"Ensemble","text":"<p>Now that you have the answers, provided your prompt has the logic you want, you can decide how to traverse the graph and find the best answer. You can also then repeat this multiple times and vote on the best result from multiple graphs.</p>"},{"location":"Articles/DL/Tree%20of%20Thoughts%20Explained%20Simply%20%28LLMs%20Dehyped%20-%201%29/#evaluation","title":"Evaluation","text":"<p>If you know exactly how to evaluate the task (eg: The best step for a robot to take when choosing to get left or right depends on if it will hit something or not), then you can use that as a criteria. But well, this is neither always possible. If you already knew what you wanted, then you would probably not be using an LLM.</p> <p>The paper asks the model to evaluate how well it did itself. Respectfully, this might be a bit dubious for real world issues if unchecked.</p>"},{"location":"Articles/DL/Tree%20of%20Thoughts%20Explained%20Simply%20%28LLMs%20Dehyped%20-%201%29/#immediate-shortcomings","title":"Immediate Shortcomings","text":"<p>While the Tree of Thought is quite a nice idea, there are quite a few issues that immediately crop up. - Evaluation : How can we decide if the answer is correct. Say for a math problem, if you knew how to get to the right step, why would you use an LLM? And for say a creative writing task, how can you even evaluate if the answer was correct? - Manual Effort : Using a Tree of Thought in practise requires a bit of manual effort in creating the perfect logical prompt, and knowing the evaluation steps beforehand. This might not always be useful. - Bias : Asking a model to evaluate how well it did is to ask a math student to see if they got the problem right. If they knew how to check, why would they want to explore other paths? But perhaps it could encourage them to think deeper about other aspects of a problem. - Computation : This is a big one. Running an LLM is expensive. For most tasks, running a model multiple times and then further algorithms is not exactly compute friendly.  - Not everything needs an LLM : As they say, \u201cwhen you have a hammer, everything starts looking like a nail\u201d. An LLM is useful, but not everywhere.</p>"},{"location":"Articles/DL/Tree%20of%20Thoughts%20Explained%20Simply%20%28LLMs%20Dehyped%20-%201%29/#perks","title":"Perks","text":"<p>So when would you actually want to use them? - Generating Data : An LLM comes with a lot of knowledge inbuilt. Perhaps this is a good way to generate data for a different task. Using a Tree of Thought would enable an LLM to come up with much better and more logical examples. - Forcing an LLM to think more : Instead of taking the first result for granted, you can force the LLM to think a step deeper. Like a kid asking \u201cwhy?\u201d multiple times, perhaps a better answer can be reached. - Combining with Knowledge Bases : Combining the language understanding capabilites of LLMs with existing knowledge bases is pretty useful. While this area is a little different from the Tree of Thought, they are related concepts. Perhaps in the future these ideas will be combined to improve LLMs even further [6]. - Domain Specific Information : This could be used as a means of injecting domain specific logical steps to the results obtained from an LLM.</p>"},{"location":"Articles/DL/Tree%20of%20Thoughts%20Explained%20Simply%20%28LLMs%20Dehyped%20-%201%29/#why-llms-unhyped","title":"Why \u201cLLMs Unhyped\u201d?","text":"<p>This is the first article in the series LLMs Unhyped. A rather weird name, I know. But why even have this series in the first place? LLMs are amazing, but they are still in the research phase. With companies like OpenAI and Hugging face, it is now possible for a lot of people to work with these massive AI pipelines without much effort.</p> <p>While that is an amazing feat in itself, and so many great ideas come out of it, it also leads to a lot of misinformation. In the hype of AI, many people who don\u2019t fully understand the background research end up  using these massive models in places where they were probably not needed. </p> <p>Sometimes, it\u2019s awesome. But in other times, it is a massive waste of money. Now ultra large corporations want you to spend your money on them, why wouldn\u2019t they? But occasionally it\u2019s like using a helicopter to get to a supermarket 100m away. </p> <p>AI has it\u2019s uses. But not everywhere. By no means is this a critique against research or enjoying the magic of AI. Please, continue to do that! But consider your options too. At the end of the day, this is just yet another tool.</p>"},{"location":"Articles/DL/Tree%20of%20Thoughts%20Explained%20Simply%20%28LLMs%20Dehyped%20-%201%29/#some-more-resources-for-you","title":"Some More Resources for You :)","text":"<ul> <li>[1] Tree of Thoughts : https://arxiv.org/abs/2305.10601</li> <li>[2] Graph of Thoughts : https://arxiv.org/abs/2308.09687</li> <li>[3] Voting Algorithms : https://scikit-learn.org/stable/modules/ensemble.html#voting-classifier</li> <li>[4] Graph Search Algorithms : https://en.wikipedia.org/wiki/Graph_traversal , https://en.wikipedia.org/wiki/Pathfinding</li> <li>[5] Roadmap of KB + LLM : https://arxiv.org/abs/2306.08302</li> </ul>"},{"location":"Articles/DL/Tree%20of%20Thoughts%20Explained%20Simply%20%28LLMs%20Dehyped%20-%201%29/#fin","title":"Fin","text":"<p>This article is in the hopes that it will help someone out. Maybe have the help that I did not. I do not know who it will reach. But to whoever it does, best of luck :)</p> <p>Like these/Want more? Buy me a coffee! Kofi</p> <p>Want articles on something specific? Just ask!</p> <p>You can always contact me on LinkedIn, or drop me an Email</p>"},{"location":"Articles/Drafts/What%20I%20learnt%20from%20an%20AI%20Masters%20Part%202/","title":"What I learnt from an AI Masters Part 2","text":"<p>Part 2 : Planning your thesis Part 3 : Setting up your writing, programming, and research environment for success Part 4 : Tips for writing your thesis</p> <p>What I learned from an AI masters degree - Part 2 (Setting up your writing, programming, and research environment)</p>"},{"location":"Articles/Drafts/What%20I%20learnt%20from%20an%20AI%20Masters%20Part%202/#setting-up-your-programming-environment","title":"Setting up your programming environment","text":"<p>If you are a programmer, this part might not be something you are particularly afraid of. But if you are not as comfortable with programming, it would help you to set up your environment before you start. This might include things like creating a GitHub repository, installing all the packages that you need, making sure that you have the resources that you might require (such as a GPU) etc. In future parts of the article, I will try to detail as much of this as I can based on my experience. (Depending on when you are reading this, these parts of the article might be already posted, so refer to them.)</p>"},{"location":"Articles/Drafts/What%20I%20learnt%20from%20an%20AI%20Masters%20Part%202/#setting-up-your-writing-environment","title":"Setting up your writing environment","text":"<p>A thesis is a rather long document, but by this point, you hopefully have written enough assignments to be comfortable with writing technical articles. If you have not, or you are reading this before doing a Master, it would serve you to be a little familiar with LaTEX. It's took me about a few hours to get everything set up and running, but it might take you longer. (Refer to a detailed guide in future articles.)</p>"},{"location":"Articles/Drafts/What%20I%20learnt%20from%20an%20AI%20Masters%20Part%202/#programming-your-idea","title":"Programming your idea","text":"<p>Not every thesis would require a lot of programming. Depending on what you're doing, you might have to spend more time writing analysis scripts instead. In my case, I created a solution from scratch and also wrote the analysis scripts so it definitely took me a lot of experimentation and time. But to be honest, it took me longer because I hesitated quite a lot, and did not spend enough time trying to refine what I already had. </p>"},{"location":"Articles/Drafts/What%20I%20learnt%20from%20an%20AI%20Masters%20Part%202/#running-experimentsstudies","title":"Running experiments/studies","text":"<p>Again, depending on the kind of thesis, you are working on the time taken for you to run, experiments or studies might differ considerably. If you are running experiments where you collect data yourself, from different systems or from human studies, it might take you a lot longer. In my case, I used datasets that were already available but since I ran a lot of experiments, the running took longer. </p>"},{"location":"Articles/Drafts/What%20I%20learnt%20from%20an%20AI%20Masters%20Part%202/#analysing-the-experimentsstudies","title":"Analysing the experiments/studies","text":"<p>Just like the previous one, analysing your results might take you longer. I personally found it useful to have the analysis scripts written as early as possible. I started writing the scripts around the same time that I started programming my solution. This allowed me to be able to run quick experiments just to make sure that my code was working and saved me quite a lot of time in the long run. It also meant that as my code changed, and I thought of new things, I could add them directly to my analysis without having to wait till I was finished running my experiments.</p>"},{"location":"Articles/Drafts/What%20I%20learnt%20from%20an%20AI%20Masters%20Part%202/#actually-writing-the-thesis","title":"Actually writing the thesis","text":"<p>So, you made it to having your experiments and data and literature survey now you actually want to start writing the thesis. I will warn you though, it will probably take more time than you think it would. No matter how good you are at writing long form, there will be many things that will crop up that you did not probably think of. It took me probably the longest to write my thesis compared to every other step. But, although it took a long time, it was not very painful as I had planned ahead and made preparations. Many of the little things such as making notes while reading papers, setting up the environments, beforehand, etc significantly cut down the mental effort it would have required otherwise.  As for all the parts that helped me out, there will be detailed articles about the same. Depending on when you read, this, you could probably refer to them already.</p>"},{"location":"Articles/Drafts/What%20I%20learnt%20from%20an%20AI%20Masters%20Part%202/#changes","title":"Changes","text":"<p>Another thing that you probably should be warned about is having to change things. Like any project, a thesis is not straightforward. There will be many times when you need to redo a part of the project or rewrite a section. This may take a decent amount of time as well. You might also get a lot of feedback that you need to implement as you go along.  Before I started my project, I made sure that I had systems in place that would let me quickly change things if required. This article would be too long if I also listed them, so future parts will follow up on these.</p> <p>As you can tell by now, none of these steps are linear. There are many things that you need to go back and forth between. But now that you know what to expect, it will be quite a lot easier for you to plan the same.</p>"},{"location":"Articles/Drafts/What%20I%20learnt%20from%20an%20AI%20Masters%20Part%202/#checkpoints","title":"Checkpoints","text":"<p>I think it\u2019s important to have checkpoints along the way to help guide you and make sure that you are on the right track. The process of finding these checkpoints was quite a bit of trial and error on my part, but perhaps they may come easier to you.  These checkpoints are like deciding a day when you want to complete your literature survey or have a prototype ready etc. They might also be ones where you update your supervisor on your progress or take a little break. I admit these are a bit hard to plan for until you are sure of where your project is headed, but it is nice to keep it in mind.</p>"},{"location":"Articles/Drafts/What%20I%20learnt%20from%20an%20AI%20Masters%20Part%202/#recovery-time","title":"Recovery time","text":"<p>There are a million articles on how to do your thesis. If you went looking for them, you probably found some more. But what I don\u2019t see a lot of them talking about is recovery. Doing a thesis is hard. For many of us (including me), it is the first time we have had to do such a long project without any external motivation. It\u2019s your project, after all. Unless you choose a company internship, in which case you will have stricter deadlines, of course. There are bound to be days, sometimes even weeks, where you have literally 0 motivation to even look at your thesis.  And while that is pretty normal, I think many of us don\u2019t factor that into our plans. Make sure you keep some extra time on hand for these days because you will really need them. </p>"},{"location":"Articles/Drafts/What%20I%20learnt%20from%20an%20AI%20Masters%20Part%202/#supervisor","title":"Supervisor","text":""},{"location":"Articles/Drafts/What%20I%20learnt%20from%20an%20AI%20Masters%20Part%202/#what-can-you-expect-from-them","title":"What can you expect from them","text":""},{"location":"Articles/Drafts/What%20I%20learnt%20from%20an%20AI%20Masters%20Part%202/#updating","title":"Updating","text":""},{"location":"Articles/Drafts/What%20I%20learnt%20from%20an%20AI%20Masters%20Part%202/#extra-tips","title":"Extra tips","text":""},{"location":"Articles/Drafts/Writing%20your%20own%20Markdown%20to%20LaTEX%20parser/","title":"Writing your own Markdown to LaTEX parser","text":""},{"location":"Articles/Drafts/Writing%20your%20own%20Markdown%20to%20LaTEX%20parser/#writing-your-own-markdown-to-latex-parser","title":"Writing your own Markdown to LaTEX parser","text":""},{"location":"Articles/Drafts/Writing%20your%20own%20Markdown%20to%20LaTEX%20parser/#what-we-want","title":"What we want","text":""},{"location":"Articles/Drafts/Writing%20your%20own%20Markdown%20to%20LaTEX%20parser/#disclaimers","title":"Disclaimers","text":""},{"location":"Articles/Drafts/Writing%20your%20own%20Markdown%20to%20LaTEX%20parser/#overview-of-steps","title":"Overview of Steps","text":""},{"location":"Articles/Drafts/Writing%20your%20own%20Markdown%20to%20LaTEX%20parser/#lets-make-it","title":"Let\u2019s Make It!","text":""},{"location":"Articles/Drafts/Writing%20your%20own%20Markdown%20to%20LaTEX%20parser/#libraries","title":"Libraries","text":"<pre><code>import markdown\nimport argparse as ap\nfrom pathlib import Path\nimport re\nfrom html.parser import HTMLParser\nfrom html.entities import name2codepoint\n</code></pre>"},{"location":"Articles/Drafts/Writing%20your%20own%20Markdown%20to%20LaTEX%20parser/#base-templates","title":"Base Templates","text":"<pre><code>default_template = \"\"\"\n\\\\documentclass[12pt]{article}\n\\\\usepackage[a4paper, total={6in, 8in}]{geometry}\n\\\\usepackage[utf8]{inputenc}\n\\\\usepackage[T1]{fontenc}\n\\\\usepackage[english]{babel}\n\\\\usepackage{graphicx}\n\\\\usepackage[dvipsnames]{xcolor}\n\\\\usepackage{hyperref}\n\\\\usepackage{listings}\n\n\\\\newcommand\\myshade{85}\n\\\\colorlet{mylinkcolor}{violet}\n\\\\colorlet{mycitecolor}{YellowOrange}\n\\\\colorlet{myurlcolor}{Aquamarine}\n\n\\\\hypersetup{\n  linkcolor  = mylinkcolor!\\\\myshade!black,\n  citecolor  = mycitecolor!\\\\myshade!black,\n  urlcolor   = myurlcolor!\\\\myshade!black,\n  colorlinks = true,\n}\n\\\\author{}\n\"\"\"\n</code></pre>"},{"location":"Articles/Drafts/Writing%20your%20own%20Markdown%20to%20LaTEX%20parser/#html-parser","title":"HTML Parser","text":"<pre><code>class MyHTMLParser(HTMLParser):\n    def __init__(self):\n        super().__init__()\n        self.attrs = []\n    def handle_starttag(self, tag, attrs):\n        for attr in attrs:\n            self.attrs.append(attr)\n    def get_attrs(self):\n        return self.attrs\n    def handle_endtag(self, tag):\n        pass\n\n    def handle_data(self, data):\n        print(\"Data     :\", data)\n\n    def handle_comment(self, data):\n        print(\"Comment  :\", data)\n\n    def handle_entityref(self, name):\n        c = chr(name2codepoint[name])\n        print(\"Named ent:\", c)\n\n    def handle_charref(self, name):\n        if name.startswith('x'):\n            c = chr(int(name[1:], 16))\n        else:\n            c = chr(int(name))\n        print(\"Num ent  :\", c)\n\n    def handle_decl(self, data):\n        print(\"Decl     :\", data)\n</code></pre> <pre><code>def get_html_attributes(text):\n    parser = MyHTMLParser()\n    parser.feed(text)\n    return parser.get_attrs()\n</code></pre>"},{"location":"Articles/Drafts/Writing%20your%20own%20Markdown%20to%20LaTEX%20parser/#replace-strings","title":"Replace strings","text":"<pre><code>replacer_dict = {\n    \"&lt;head&gt;\" : \"\",\n    \"&lt;/head&gt;\" : \"\",\n    \"&lt;html&gt;\" : \"\",\n    \"&lt;/html&gt;\" : \"\",\n    \"&lt;p&gt;\" : \"\",\n    \"&lt;/p&gt;\" : \"\",\n    \"&lt;h1&gt;\" : \"\\\\begin{document}\\n\\\\toc: true\ntitle{\",\n    \"&lt;/h1&gt;\" : \"}\\n\\\\maketoc: true\ntitle{}\\n\",\n    \"&lt;h2&gt;\" : \"\\\\section{\",\n    \"&lt;h3&gt;\" : \"\\\\subsection{\",\n    \"&lt;h4&gt;\" : \"\\\\subsubsection{\",\n    # \"&lt;body&gt;\" : \"\\\\begin{document}\\n\",\n    \"&lt;/body&gt;\" : \"\\\\end{document}\\n\",\n    \"&lt;ul&gt;\" : \"\\\\begin{itemize}\\n\",\n    \"&lt;/ul&gt;\" : \"\\\\end{itemize}\\n\",\n    \"&lt;il&gt;\" : \"\\\\begin{enumerate}\\n\",\n    \"&lt;/il&gt;\" : \"\\\\end{enumerate}\\n\",\n    \"&lt;code&gt;\" : \"\\\\begin{lstlisting}[language=Python]\\n\",\n    \"&lt;/code&gt;\" : \"\\\\end{lstlisting}\\n\",\n    \"&lt;li&gt;\" : \"\\\\item \",\n    \"&lt;/li&gt;\" : \"\",\n    \"%\": \"\\%\",\n    \"&amp;\": \"\\&amp;\",\n}\n</code></pre>"},{"location":"Articles/Drafts/Writing%20your%20own%20Markdown%20to%20LaTEX%20parser/#close-tags","title":"Close Tags","text":"<pre><code>def add_end_brace(list_of_vals, replacer_dict):\n    list_of_vals = [x.strip() for x in list_of_vals.split(\",\")]\n    for i in list_of_vals:\n        replacer_dict[i.replace(\"&lt;\", \"&lt;/\")] = \"}\\n\"\n</code></pre> <pre><code>add_end_brace(\n    list_of_vals=\"&lt;h2&gt;, &lt;h3&gt;, &lt;h4&gt;\", \n    replacer_dict=replacer_dict\n)\n</code></pre>"},{"location":"Articles/Drafts/Writing%20your%20own%20Markdown%20to%20LaTEX%20parser/#images","title":"Images","text":"<pre><code>def figure_code(text):\n    found_links = re.findall('\\&lt;img .* \\/&gt;' , text)\n    for link in found_links:\n        attrs = get_html_attributes(link)\n        caption_data = \"\"\n        file_path = \"\"\n        for i in attrs:\n            if i[0] == \"alt\":\n                caption_data = i[1]\n            if i[0] == \"src\":\n                file_path = i[1]\n        gen_latex = \"\\\\begin{figure}[!htbp]\\n\\centering\\n\\includegraphics[width=.75\\columnwidth]{\"+file_path+\"}\\n\\caption{\"+caption_data+\"}\\n\\label{}\\n\\end{figure}\"\n        text = text.replace(link, gen_latex)\n    return text\n</code></pre>"},{"location":"Articles/Drafts/Writing%20your%20own%20Markdown%20to%20LaTEX%20parser/#cli-input","title":"CLI input","text":"<pre><code>ags = ap.ArgumentParser(\"md2tex\")\nags.add_argument(\"-f\", help=\"Full file path\", required=True)\nags.add_argument(\"-d\", help=\"Insert default formatting code\", action='store_true')\naps = ags.parse_args()\n\nf_name = Path(aps.f)\n</code></pre>"},{"location":"Articles/Drafts/Writing%20your%20own%20Markdown%20to%20LaTEX%20parser/#running-the-pipeline","title":"Running the pipeline","text":"<pre><code># Read the file\nwith open(f_name, 'r') as f:\n    text = f.read()\n    html = markdown.markdown(text)\n\n# Replacing things\ntext = figure_code(html)\nfor key in replacer_dict.keys():\n    text = text.replace(key, replacer_dict[key])\n\n# Write the file\nwith open(f_name.parent/f\"{f_name.stem}.tex\", 'w') as f:\n    if aps.d:\n        f.write(default_template)\n    f.write(text)\n    if aps.d:\n        f.write(\"\\\\end{document}\")\n</code></pre>"},{"location":"Articles/Drafts/Writing%20your%20own%20Markdown%20to%20LaTEX%20parser/#fin","title":"Fin","text":"<p>This article is in the hopes that it will help someone out. Maybe have the help that I did not. I do not know who it will reach. But to whoever it does, best of luck :)</p> <p>Like these/Want more? Buy me a coffee! Kofi</p> <p>Want articles on something specific? Just ask!</p> <p>You can always contact me on LinkedIn, or drop me an Email. For all the code, drop by my Github.</p>"},{"location":"Articles/FAQ/Commission%20FAQ/","title":"Commission FAQ","text":""},{"location":"Articles/FAQ/Commission%20FAQ/#commission-faq","title":"Commission FAQ","text":""},{"location":"Articles/FAQ/Commission%20FAQ/#name","title":"Name","text":"<p>Full form article about anything related to AI/Neural networks/Technical writing.</p>"},{"location":"Articles/FAQ/Commission%20FAQ/#description","title":"Description","text":"<p>Want an article on something in AI? Go ahead and ask me for it.  If there are specific questions you want answered, you can request them as well. </p> <p>Topics you can request for (my expertise) - AI, Computer Vision, Deep learning, Neural Networks, Shell scripting, Python, Scientific Writing</p> <p>My previous blogs :  https://msubhaditya.medium.com https://subhadityamukherjee.github.io/AI-knowledge-base/</p> <p>I\u2019d suggest you contact me first before you pay. You can either drop me an email here : msubhaditya@gmail.com Text me on LinkedIn : https://www.linkedin.com/in/subhaditya-mukherjee-a36883100/</p> <p>Word count isn't really my thing but it would be a medium sized article - around 1k-1.5k words. Please read the FAQs!</p>"},{"location":"Articles/FAQ/Commission%20FAQ/#instructions-and-faq","title":"Instructions and FAQ","text":"<p>The more specific your request, the better you will get an answer to them.  Please Please drop your name and email. It would make it so much easier to contact you. FAQ: - Will you do my homework?     - Probably not. I take no responsibility for the grades you get. </p> <ul> <li> <p>How long will it take you to write my article?</p> <ul> <li>That depends on your topic. Around 3 days is a good estimate. Although I would be able to give you a better estimate once I see the topic.</li> </ul> </li> <li> <p>Can I distribute this article freely?</p> <ul> <li>Generally yes. You must link to my page and have my name on it though. If you are going to monetise it, well. I can\u2019t really stop you but then I will monetise it too.</li> </ul> </li> <li> <p>Will it be public?</p> <ul> <li>That is your choice. Generally, yes. But if you want it on your website/company website, then just inform me before. As long as you credit me properly wherever you post it, I\u2019m okay with that.</li> </ul> </li> <li> <p>Will you make a whole framework for me?</p> <ul> <li>Nope. You will get demo code. Full code etc will be a lot more time intensive so the rate will be decided based on the request.</li> </ul> </li> <li> <p>How many rewrites do I get?</p> <ul> <li>Minor changes : two times.</li> <li>Major changes : once. </li> <li>Delays suck for both of us. So if you want something specific, it\u2019s better you tell me before I start.</li> </ul> </li> <li> <p>Delays</p> <ul> <li>I am not GPT-3, and sometimes I do face delays in getting work done. If I do though, I will keep you informed.</li> </ul> </li> <li> <p>Refunds</p> <ul> <li>Oops, you didn\u2019t like what you got. Apologies. If you can convince me that it was a badly written article, even after 2 minor and 1 major change, I will send you half your money back. After all, it did take a lot of my time. I don\u2019t want to encourage scams, so I hope you understand.</li> </ul> </li> </ul>"},{"location":"Articles/FAQ/Commission%20FAQ/#thank-you","title":"Thank you!","text":"<p>I hope we can both enjoy this article. If you have any further questions, don't hesitate to drop me a message or email at msubhaditya@gmail.com </p>"},{"location":"Articles/FAQ/Commission%20FAQ/#linkedin-post","title":"LinkedIn Post","text":"<p>TL;DR AI Article Commissions - Open!</p> <p>https://ko-fi.com/subhadityamukherjee/commissions I'm going to take a deep breath, and tell you that I'm now open to article commissions. Want an article on something in AI? Go ahead and ask me for it.  If there are specific questions you want answered, you can request them as well. </p> <p>Topics you can request for (my expertise) - AI, Computer Vision, Deep learning, Neural Networks, Shell scripting, Python, Scientific Writing</p> <p>More info on the page linked here. This is not a free article. Although, if you do want one but can't afford it, you can still drop me a message. I can't guarantee it though.</p> <p>PS: This took a lot more than one deep breath. Let's see how it goes.</p>"},{"location":"Articles/FAQ/Commission%20FAQ/#commission-computervision-python-neuralnetworks-ai-deeplearning-shellscripting-article-request-scientificresearch","title":"commission #computervision #python #neuralnetworks #ai #deeplearning #shellscripting #article #request #scientificresearch","text":""},{"location":"Articles/FAQ/Commission%20FAQ/#prices","title":"Prices","text":"<ul> <li>Minimum. Short article. 500 ish words : 30 euro</li> <li>Medium sized article. More in depth. Around 1k-1.5k words. : 50 euro</li> <li>Longer article. Fully in-depth. Around 2k-2.5k words. : 100 euro</li> <li>Along with working code. : 200 euro</li> </ul>"},{"location":"Articles/Mental%20Health/ADHD%20%2B%20Autism%20-%20Challenges%20and%20hopeful%20workarounds/","title":"ADHD + Autism - Challenges and hopeful workarounds","text":""},{"location":"Articles/Mental%20Health/ADHD%20%2B%20Autism%20-%20Challenges%20and%20hopeful%20workarounds/#adhd-autism-challenges-possible-reasons-and-hopeful-workarounds","title":"ADHD + Autism - Challenges, possible reasons and hopeful workarounds","text":""},{"location":"Articles/Mental%20Health/ADHD%20%2B%20Autism%20-%20Challenges%20and%20hopeful%20workarounds/#motivation","title":"Motivation","text":"<p>Finally, a manual for why I am such a wreck and how to salvage it! (Kind of.) Think of this as a lab notebook. You can pick whichever chapter you want to read. This obscenely long post is intended to be an experience log of workarounds that I have found over the years that have worked, most times atleast and their possible reasons. It is not intended to be a \u201coh if I do XYZ I will be free of my head~\u201d but more of \u201coh~~, maybe that\u2019s why doing XYZ broke my brain! I should remember that.\u201d.  I wrote down whatever I could (clearly hyper focusing on this now don\u2019t judge me please) in the hopes that sharing my understanding helps you find your own.  Feel free to critique (politely thank you, don\u2019t make me cry). But more importantly, feel free to start a discussion. I\u2019m on a journey as well. I have too much left to learn. I would love to hear your stories too.</p>"},{"location":"Articles/Mental%20Health/ADHD%20%2B%20Autism%20-%20Challenges%20and%20hopeful%20workarounds/#disclaimers","title":"Disclaimers","text":"<ul> <li>This is not scientifically backed but is purely anecdotal. </li> <li>I do \u201cnot\u201d have my s**t together at all. Just because I wrote this huge article doesn\u2019t mean all these points always work. Sometimes the Autism side of my head takes over and breaks down, sometimes the other. It\u2019s a war, baby :)</li> <li>It is a \u201cspectrum\u201d and you probably won\u2019t relate to a few of them.</li> <li>Conversely, relating to these doesn\u2019t automatically make you have ADHD/ASD. Get a diagnosis if you can! But either ways. If it helps you, then that\u2019s all that matters to me.</li> <li>I would love to add scientific literature to these points, but I\u2019m scared it will take long enough for me to lose motivation and never end up posting. (Help me out please?)</li> <li>If you disagree with anything, comment! I\u2019m learning too. I fully admit that I might be wrong. </li> <li>I would love to have a discussion on these. Especially if you have more tips!!</li> <li>Today is one of the good days. And I am writing this whole article in one go. I don\u2019t have the courage to do it in parts.</li> <li>I admit I do not have extreme symptoms. They ruin my life, but subtly sabotage it. I do not have any visible effects unless someone lives with me or sees me crying in the corner after my fourth breakdown when all I wanted to do was wash two plates so I could have lunch. My family does not know of my diagnosis. Some friends and my girlfriend do. My birth society would not accept me if they did. I am studying in a different country, so.. I have my privacy here. </li> </ul>"},{"location":"Articles/Mental%20Health/ADHD%20%2B%20Autism%20-%20Challenges%20and%20hopeful%20workarounds/#somewhat-categorised-choose-your-own-adventure","title":"Somewhat categorised - Choose your own adventure","text":""},{"location":"Articles/Mental%20Health/ADHD%20%2B%20Autism%20-%20Challenges%20and%20hopeful%20workarounds/#time","title":"Time","text":"<p>Time blindness affects every little part of my life. But I have come to some workarounds to help me with it. Turns out, our brains update their internal clocks by changes in the environment. That was an important realisation for me. The following points relate to that. - I used to listen to a lot of lofi music while working and always felt like time would stretch out to infinity. Well, turns out it does. Because it repeats, and doesn\u2019t change enough. Now I actively try to switch up the music a bit. - I have tried time blocking a bunch of times. But it inevitably always fails. It\u2019s still useful though. I just have to forgive myself for not always being able to stick to it. But I tried you know? - You can use this to your advantage. If you want to slow down, make sure you have repeating things. If you want to speed up, do the opposite. - I tried having clocks everywhere but only \u201canalog\u201d clocks helped. Maybe because they are more visual.  - Telling yourself \u201cOkay this will take 1 hour\u201d and then seeing that it actually took 3 is probably a good way to realise that you are setting yourself up for failure. - I have tried to explain to people that my sense of time is a little warped. I don\u2019t think they always understand, but atleast some of them try.</p>"},{"location":"Articles/Mental%20Health/ADHD%20%2B%20Autism%20-%20Challenges%20and%20hopeful%20workarounds/#adhd-meds","title":"ADHD Meds","text":"<p>I made it through 21 years without a diagnosis and forever wondering if I was completely looney. But yeah, a while back I did get one and was prescribed Ritalin. It changed my life, but it then ASD side started showing up a little more frequently. And I realized even more how chaotic my poor brain was. I am on my meds as I write this manual of course.  What it helped me with: - Getting my work done, the ones I don\u2019t care about but still have to do. - Slowing down a bit. - Having the space to see just how badly I was wrecking my life. (Cue. Also maybe helping me heal a little) - Realising that the ASD side had been suppressed a lot and maybe that\u2019s why nothing was working out. I was focusing too much on \u201cfixing\u201d the ADHD side but the other side wanted all the opposites. - Helping me separately understand what the ADHD and ASD sides wanted. Not that I can fully balance their needs yet. It\u2019s a work in progress. What it somewhat made worse - Regular meals become harder because the meds suppress my hunger. - Sometimes I forget to take the right dosage and then oh god it\u2019s down to the bottom of the emotional well.  - Now that I realise how badly the neurodivergent \u201cspice\u201d affected every part of my life, I\u2019m almost angry. But I have to remember to be grateful too. Without it, I don\u2019t think I would have come this far. - I sometimes expect it to \u201cfix\u201d me. But it doesn\u2019t of course. It does help a lot though.  - I have to remember to set an intention before I take them. Or I end up wasting my time anyway. But more focused.</p>"},{"location":"Articles/Mental%20Health/ADHD%20%2B%20Autism%20-%20Challenges%20and%20hopeful%20workarounds/#rejections","title":"Rejections","text":"<p>I don\u2019t want to throw terms around, but RSD (Rejection Sensitive Dysphoria) has shadowed me all my life. Like it probably has many of you. More than anything, I realise it hurts more because we are always trying to play \u201ccatch up\u201d with everyone else. And.. honestly\u2026 it\u2019s very unfair.  - Recently, I have been trying to have the courage to speak up and say \u201chey, that kind of hurt. I would appreciate some empathy.\u201d Sometimes, it helps. - I realize the ADHD side wants to talk a lot, keep talking and enjoy. But the ASD side can\u2019t keep up with it. It gets very overwhelmed. I talk a lot, but when someone asks me something, I freeze. Then I stutter something or just, want to disappear into the ground.  - I think a lot of times, we are just expected to be able to do something. But we can\u2019t and everyone just says \u201cyou are just lazy\u201d when I\u2019m not. I really want to do it. But the other side of my head wants to shut down forever.  - I\u2019m learning to take breaks. Give my ADHD side conversation but then also take my time to soothe and recover. </p>"},{"location":"Articles/Mental%20Health/ADHD%20%2B%20Autism%20-%20Challenges%20and%20hopeful%20workarounds/#people-and-forgetting-them","title":"People.. and.. forgetting them","text":"<p>Another rather interesting and sad feeling I\u2019ve had is that if I don\u2019t interact with someone for a few days, they kind of.. disappear. I stop feeling anything for them. It\u2019s not permanent of course. I still care a lot, and love the people I do. But, I forget to reach out. And.. they get hurt. - Most of my life, I blamed myself for being well, I dunno what. \u201cYou forgot to even text your girlfriend today, come on man. You love her right? You can\u2019t even remember she\u2019s there because she\u2019s in a different country right now? Such a dumbass\u201d.  - At some point, I realized that it\u2019s the same thing with me and cupboards. If I wanted to stop eating chips or cookies, I would keep them at the back of a cupboard and then forget they ever existed. I vaguely know they are there, I just don\u2019t act on it or feel anything towards it. - Note to self. If you care, try to make semi regular contact. It will help you \u201crenew the connection\u201d. - Note to self P2. You are not a sociopath. You are neurodivergent. You still have feelings. Too many of them lol.</p>"},{"location":"Articles/Mental%20Health/ADHD%20%2B%20Autism%20-%20Challenges%20and%20hopeful%20workarounds/#organisation-and-planning","title":"Organisation and Planning","text":"<p>Gods. I\u2019m so bad at this it is not even funny. Some things arguably make it slightly easier though. - I realise I get frustrated because I don\u2019t have parts of the puzzle and  in spending time trying to find them, I lose all my motivation to work. So instead, I first try to collect all the information I might need for something in one place. I do not start actual work before I do.  - I try not to do things in parts. I sit down and finish things in one go. If I can. - Simplify, simplify, and simplify some more. The more choices you have, the harder you make something. Pick the essentials. You cannot plan everything and you will get overwhelmed.  - Ask for help. I know, I know. But it\u2019s okay. Pick someone who might understand if you can? In time, maybe we can be better at it too.</p>"},{"location":"Articles/Mental%20Health/ADHD%20%2B%20Autism%20-%20Challenges%20and%20hopeful%20workarounds/#masking-general","title":"Masking - General","text":"<p>I admit, I don\u2019t fully understand to what extent this affects me. It is a lot, but maybe in time I will understand it more. - It started ages ago in school - when I didn\u2019t look people in the eye, or when I spoke in monotone, or when I would not talk at all, or when I spoke too softly. My mom, or teachers would tell me I wasn\u2019t \u201cnormal\u201d and tell me what to do instead. And they would be \u201cvery\u201d upset about it if I didn\u2019t. So I started pretending. I would look people in the eye, I would talk louder, I would \u201cemphasise\u201d, heck I pretended to be emotional too sometimes.  - I didn\u2019t realise it was slowly taking over though. In time, nobody believed me anymore when I said I was different. They\u2019d say, no you cannot have ADHD/ASD. You are too \u201cnormal\u201d. Well, another one bites the dust I suppose. - Who are you when the dust settles and you are alone in your room crying your eyes out because it hurts so badly that nobody ever understands you? Give that person a long hug because oh god, they tried their best.  - I tried so hard, and go so far, but in the end it doesn\u2019t even matter~ . There is always a price. At the end of the day, who are you living for?</p>"},{"location":"Articles/Mental%20Health/ADHD%20%2B%20Autism%20-%20Challenges%20and%20hopeful%20workarounds/#masking-adhdasd","title":"Masking - ADHD/ASD","text":"<p>Ah this is a nice little cocktail. Do they cancel each other out? Not really. I just feel like I am always at war with myself. I do not have workarounds yet, so I will just list down what I face instead. - I really want to do the dishes because warm soapy water is fun, but the other side wants to shut down forever and scream. - I make plans to hang out with new people, but the other side wants comfort and people I know already. - I want to plan novel experiences, new trips, new food etc, but the other side wants to do none of them and sit at home and have ramen. - I am horrible at conversations and I die inside each time someone asks me something I don\u2019t know, but the other side of me learns to adapt so fast nobody even realises that I\u2019m screaming inside. - One side of me has gotten too good at pretending to be normal, while the other needs days to recover from all this masking. - I technically should not be able to work as much as I do, but when I hyper focus, I am unstoppable. When I don\u2019t, I am a carrot.  - I like working on new projects, new hobbies etc, but the other side wants to sit and do the same thing everyday because new things are scary. - I beat myself up so badly and get good grades so everyone thinks I\u2019m a genius and have it so easy because I get so much done. Ah. Well. Madame, there is always a price</p> <p>(I think my motivation is running out now. Or maybe it\u2019s because I didn\u2019t have lunch yet.)</p>"},{"location":"Articles/Mental%20Health/ADHD%20%2B%20Autism%20-%20Challenges%20and%20hopeful%20workarounds/#social-media","title":"Social Media","text":"<p>I think this kinda applies to everyone. But more so to us. Doom scrolling is the bane of my existence. But I am trying to use what I have learnt about my brain to make it easier to cope. - Brain loves dopamine right? Stop giving it that. Make social media a chore. Open instagram on your browser, step back and see how \u201cboring\u201d all the reels are. Do you really want to watch someone make a coffee for the fourth time today? No you don\u2019t. - Shove it in a cupboard. Forget it exists. For a while atleast. - Write instead. I would never have been able to write so much before. But ever since I replaced my doom scrolling with writing, it\u2019s easier. - It doesn\u2019t always work. Yeah. Sorry. You can\u2019t fully forget about it. It\u2019ll come back. You\u2019ll waste hours on it. But\u2026 atleast you saved a couple.</p>"},{"location":"Articles/Mental%20Health/ADHD%20%2B%20Autism%20-%20Challenges%20and%20hopeful%20workarounds/#a-thank-you-note","title":"A thank you note","text":"<p>There are so many people I want to thank here. I sadly do not know who most of them are.  - Jessica and their team from HowToADHD (https://www.youtube.com/@HowtoADHD). I am so grateful to you I want to run to you from the other end of the world and give you one huge huge huge hug. Thank you. Thank you so so much. (Am I really tearing up now? Yes.) I don\u2019t have words but you have my eternal gratitude. - Reddit and all of you. Feeling like I am not alone here makes things a lot easier to handle. I am very grateful to you for giving me that. - Every website that gave me tips on how to manage my life. Not all of them helped of course, but they were a start.</p>"},{"location":"Articles/Mental%20Health/What_I_learnt_about_myself_after_two_years/","title":"What I Learnt About Myself the Past Two Years","text":""},{"location":"Articles/Mental%20Health/What_I_learnt_about_myself_after_two_years/#what-i-learnt-about-myself-the-past-two-years","title":"What I Learnt About Myself the Past Two Years","text":"<ul> <li>Irrational rage with task interruptions : probably because I am not sure if I can come back to the task now or ever, or how long the interruption will last.</li> <li>Being told to do things right when I am just about to do them makes me feel like control is being taken away from me. And since I barely have control usually, that just frustrates me a lot.</li> <li>Cleaning is hard because it feels like a big thing. This could be also similar with things I suck at, and the energy cost for it is crazy.</li> <li>If I clean everything, the next time I have to do it, it feels like it's a brand new task. And brand new tasks take a lot of energy</li> <li>I always only do upto 80$ of something. Just by this point if I knew about ADHD as a kid I would have been diagnosed already.</li> <li>I am slowly learning that masking does take a lot of energy, and when I don't have it it pisses me off</li> <li>When I get hungry my ADHD is at max</li> <li>ADHD is not really about not being able to focus but more of being unable to regulate it</li> <li>If I change the food I want slightly then it makes it a looot better. Even if that means adding some tiny sauce to it</li> <li>Exercise REALLY helps</li> <li>Sometimes simple things that you don't consider to solve anything solves anything : like the breathe right nasal strips?</li> <li>Showers help me regulate</li> <li>I love running, a lot</li> <li>Travelling on trains is not bad if I take care of noise and have something to do</li> <li>If I plan my day before doing anything or have something to look forward to, it really helps</li> <li>Long conversations are also fine if I am drawing or have something to regulate my emotions</li> <li>Low energy people are so nice to be around though. Esp other neurodivergent people. Gods.</li> <li>I have more friends than I think I do</li> <li>People love talking about things they care about. I asked Manon for running shoe recs and it's one of her special interests apparently</li> <li>I get angry a lot because I feel like my boundaries are being crossed but I have grown up ignoring them</li> <li>Mayu needs space in between things like space when she comes back home. </li> <li>Mom likes to talk and she does not super care if someone pays that much attention but atleast cares enough to give her some attention. Makes sense right?</li> <li>I dont actually hate TV. I just need something else on the side to regulate because my empathy kicks in a lot of times</li> <li>I sometimes get too much input from things and then need to take some of it out. Like write something, draw something whatever</li> <li>If I start getting annoyed with something that I love, it means that I probably hyperfocuses the absolute shit out and should probably give myself a break before it gets too much</li> <li>I have learnt how to identify when my energy is low</li> <li>Meditation is hard but the count backwards from X with 3 repeats of X per breath is the best</li> <li>Mom really does have ADHD. No wonder she switches topics three times in every sentence</li> </ul>"},{"location":"Articles/Others/A%20letter%20to%20my%20junior%20school%20art%20teacher/","title":"A letter to my junior school art teacher","text":""},{"location":"Articles/Others/A%20letter%20to%20my%20junior%20school%20art%20teacher/#a-letter-to-my-junior-school-art-teacher-david-fitzgerald","title":"A letter to my junior school art teacher - David Fitzgerald","text":"<p>Dear David sir,  This is Subhaditya, or Vishal as you knew me back then. I barely remember how many years ago it was. You had recently started teaching art in my class then. I have these vague memories of awe when I saw you draw on the blackboard with a marker. It seemed like magic almost, you would draw some lines and suddenly, presto! It was a horse. Or a dragon. Or a face.  Did you know I have loved dragons since then? You always started them with the letter 5.  I remember pestering my parents about art. Back then I used to doodle and draw all the time. When I went to Kolkata for the summer break, my parents enrolled me in a class for painting. It was with my cousin brother, and I hated it. It felt so stifling. I do not remember the chronology of this, of course, I was in the fourth grade I think. A little boy who just wanted to draw dragons. </p> <p>At some point, mom decided to ask you if you were willing to teach me art after school. Once a week. I think it was on weekends, I cannot remember. You did not take tuitions, and I was just a child anyway. But somehow you agreed. I remember going to your place the first day with an art book. You had a pet chameleon that was fascinating to me. I remember that it jumped on me. I can still feel its tiny claws on my shirt. You showed me a tattoo machine. An airbrush. You painted horses. You painted dragons. You made me paint a dragon.</p> <p>I loved those days. At least, I think I did. After all, I had forgotten about them until now.  You gave me a little pebble with a white horse painted on it. I still have it back at home. </p> <p>I started painting properly during the pandemic in 2019 when I found art videos online. It\u2019s been a few years since then. But if I think back, I remember traces of me trying to draw all my life. I never did it properly though. I did not realize it was something that I could do. </p> <p>It is almost the end of 2023 as I write this, I graduated with a masters degree in AI a few days ago. In my hunt for a job, I came across one that had the toc: true title of \u201cCreative Technologist\u201d. It was an entry to art! As I write this, I am waiting to know if they accepted me for the job. It will be my first one. </p> <p>While trying to find my way of art and thinking about the job, I suddenly remembered those days with you. And then I realized why I had stopped drawing. And\u2026 what made me fall in love with it in the first place. </p> <p>Will you take a guess? Of course. It was. You.</p> <p>You, David sir. You.</p> <p>But life is not easy is it? I wonder if things had not gone the way it did, and you had stayed, perhaps this life of mine would have been very different.</p> <p>I remember walking into class one day and seeing a new lady. Maybe she was a nice person, I do not know. I remember her trying to teach shadows with a coat hangar.  I missed you. I wanted to know where you went.</p> <p>I don\u2019t remember how but I found out that you had tried to kill yourself. I remember hearing that you set yourself on fire, that your skin had melted off. I remembered those hushed conversations.  I vaguely remember seeing a picture, or did I actually see you? I do not remember. As a kid, I did not understand what it meant. I thought you left because they thought you looked like a monster.</p> <p>As an adult, I wonder how it must have felt. Many years after that, I had long forgotten these things, wiped away in the warm embrace of forgetfulness. I blocked off my art without realizing it. It broke me I think. There were many factors, of course. But that is not the point of this letter. In my darkest times, I wanted to kill myself many, many, many times. I wonder how it felt when it didn\u2019t work. When you were left with the ashes of a broken life. I wonder how you are now, more than a decade later. </p> <p>I wonder if you were made to feel like a monster for the gift that you had. I wonder if it tore you apart. I wonder what set you on fire. Was it you? Or was it them? </p> <p>The real demons are not us are they? You were not a monster then, they were. </p> <p>After I rediscovered art, I started getting better you know? I no longer wanted to die. I didn\u2019t realize that that was what saved me in the end. I just did while writing this letter to you. Art showed me the world for what it was, not what I was told it was.</p> <p>I tried to find you online, but I can\u2019t. I wonder where you are now. If you are even alive. I don\u2019t know. Maybe one day our paths will cross. Maybe they won\u2019t.</p> <p>I want you to know that I have been painting almost everyday for the past few years. I have a long way to go, but I am getting there. Art has made my life brighter. I wish I could show you my work. I wish I could tell you that I went to two art conferences and got to meet some of my favorite artists. I wish I could tell you that my friends have my art hanging on their walls. Yes, i gave it to them. But they cherish it, I think. Yes, I still do not feel like I am at a point where I can accept those compliments. </p> <p>I suddenly remember something you said once. A classmate had drawn something poorly and he expected anger, like all of our other teachers had. You stood in front of the class and said, \u201cLet nobody tell you that your art is bad. There is no bad art.\u201d I wonder if you feel the same way about yourself.</p> <p>I realized that art is not just our work, it is us. We are creatives. We live, breathe and exude creation. I do not know why. I don\u2019t think anyone does. I realized that if we stopped creation, it either destroys us, or finds the cracks in our soul and gets out somehow. </p> <p>You will probably never see this. Even if you did, I doubt you remember any of these. But, I do now. I see you sir, Mr.David Fitzgerald. I see you. I see your pain. I was a child then, but I\u2019m all grown up now. </p> <p>This is making me cry, so I will end this here. </p> <p>Thank you sir, you started this journey for me without me realizing it. There are so many things I want to say that I can\u2019t. If you are out there somewhere, I want you to know that you made a difference in my life. Perhaps this journey towards art was complicated by your pain, but perhaps it was the only way I could process it. After all, you were my art hero. As a little kid, what could hurt more than knowing that heros could die too? </p> <p>Thank you David sir, for the horses, dragons and the magic. Thank you. </p> <p>Wherever you are, I send you my love and affection. Little Vishal to you. I hope the fire that consumed you showed you the road you were scared to take, and I hope you followed it. </p> <p>Thank you. For everything. Subhaditya Mukherjee 17th October 2023</p>"},{"location":"Articles/Others/Ai_for_startups/","title":"Ai for startups","text":"<p>toc: true title: Ai_for_startups</p> <p>categories: [\"article\"] date modified:  date created: </p> <p>AI For startup #ai</p>"},{"location":"Articles/Others/Ai_for_startups/#hitchhikers-guide-to-ai-for-startups","title":"Hitchhikers Guide to AI for Startups","text":"<p>Are you part of a startup, or want to start one? Want to have AI superpowers but don't know where to start? Want to make sure you don't sink your boat before it floats? Make sure you hire the right people and avoid blowing your budget out of the water. This article is for you.  A helpful guide on what to focus on, resources you need, and punching common myths in the face. Sounds interesting? Read on.</p> <p>(This is a long article. Skip to what you need. But if you are just starting, I would recommend reading the whole thing. Take your time. Take notes. Drop me an email with your questions. msubhaditya@gmail.com)</p> <p>Okay, now, DONT PANIC.</p>"},{"location":"Articles/Others/Ai_for_startups/#a-definition","title":"A definition","text":"<p>Before we move on, let us first define what we mean by AI. Of course, there are quite a few definitions but for our purposes, we can define it as \u201cany means of injecting a form of semi-automated intelligence that either performs a task previously impossible for computers or does a task as good as/better/faster than a human. An AI is used to learn how to perform a task, and can be thought of as a more advanced software.\u201d Note for AI experts: The term AI is used in the article as a general word, interchangeably with deep learning, neural networks, etc. Yes, technically this is inaccurate. But this is an article for people with not much in the way of experience.</p>"},{"location":"Articles/Others/Ai_for_startups/#some-useful-terms-you-might-need","title":"Some useful terms you might need","text":"<ul> <li>AI model/architecture: The brain that makes up an AI</li> <li>GPU: A specialized computing unit that accelerates intensive computation, like training an AI</li> <li>Neural Networks: Inspired by the brain, what makes \"AI\" tick</li> <li>Latency: How long does your model take to give results</li> <li>SOTA: State of the Art. What is the best model for the task, right now?</li> <li>Training: The process where the AI learns how to perform a task</li> <li>Inference: Using a trained AI to predict some results</li> <li>Cluster: A group of computers used to parallelly perform a task</li> </ul>"},{"location":"Articles/Others/Ai_for_startups/#a-little-index","title":"A little index","text":"<p>This article is divided into three major sections.  - If you want to dive into this field for the first time, or find ways to inject some AI into your companies, the first section is for you. - If you already have an AI startup, and are looking for ways to improve your infrastructure so you can grow, the second section is for you. - The third section talks about some of the pitfalls that one might face when they first dive into this space. Take it as a word of caution. </p>"},{"location":"Articles/Others/Ai_for_startups/#section-1-the-beginning-stages-or-how-should-i-care-about-ai","title":"Section 1 - The beginning stages Or How should I care about \u201cAI\u201d?","text":""},{"location":"Articles/Others/Ai_for_startups/#cutting-through-the-hype","title":"Cutting through the hype","text":"<ul> <li>Unrealistic expectations At the end of the day, AI is not a magic mushroom. It cannot solve everything you want, and neither can it evolve and take over the planet. Yet. Can you use it for your needs? Undoubtedly. So what can it do? It can categorize images, translate text, understand what it hears, recognize tumors, and anything your creativity allows.  The key is to set realistic expectations. Think of it this way, if you can get a team of experts to do the task, then it might be possible to have an AI do it (Terms and conditions apply). Ask an AI consultant, or if you cannot afford one, look at this website [insert paperswithcode] for a list of tasks that are possible. If nothing else, it can give an idea of what can be done. Sometimes it might be possible for your idea to work in the long run, but it might just take more time and resources than you imagined. Only an expert would be able to tell you if it is a feasible plan. Expecting your developers to come up with something impossible is great, but only if you can afford it. For example, instead of trying to make an \"AI that will take humans to Mars\", break it down into smaller tasks - \"AI that recognizes space debris\" + \"AI that would identify system faults\" + \"AI that might predict what a type of rock found was\" + ... etc</li> <li>Data requirements It is a myth that to \"train\" an AI, you need massive amounts of data. Yes, technically you do. But recently, there has been a lot of research conducted on \"transfer learning\", which is a technology that allows you to start with an AI trained on large amounts of data, and fine-tune it to your specific use case. This is very helpful, especially if you are working on tasks similar to those that exist. For instance, if you want to train an AI to recognize different types of cars, this might not need a massive amount of data, because similar \"recognition\" tasks exist. But if you want to classify a hundred types of new tumors, that might require a little more data. </li> <li>Extreme requirements The Tech giants want everyone to think that we need extreme computing power for our AI needs. But in reality, most companies can start with minimal requirements. Even if you cannot afford huge computing clusters or lots of computers, you can use online services to run the code. There are quite a few such services that provide \"GPUs\" (special computing units that accelerate running compute-intensive code). They charge you a fee by the hour, which is often quite cheaper, even at scale. As long as you have skilled workers, decent computers, and funds to support it. A word of warning, AI is still an experimental field. If your staff does not deliver, it would serve to try and understand if the task handed to them is realistic, or needs more resources. An AI expert/consultant would be helpful here.</li> </ul>"},{"location":"Articles/Others/Ai_for_startups/#so-you-want-to-be-an-ai-startup","title":"So you want to be an AI startup?","text":"<ul> <li>Is AI needed? The rush to say \"We use AI in our products to do XYZ\" is very intense. Do not fall into peer pressure. It serves to first understand and identify where you need AI. Does a specific part of the infrastructure need to be revamped? Is there something you want to do that just normal programming cannot handle? Have you tried other options first? AI is expensive in the long run. Can you afford the staff? Pay for the time taken to research? Test some ideas, you might be able to hire a freelancer to make some mockups before diving in headfirst.</li> <li>AI products vs AI as part of your existing infrastructure You might not need to fully use AI, maybe only some parts of your idea need it. If you are a new startup, please note that just AI will not be the end of all. Even if your idea is a pure AI product, there are quite a few software engineering components in it as well. It serves to understand the pipeline before starting, just so you do not fall into common traps and end up wasting time and money.</li> <li>Domain knowledge Does your product revolve around a specific domain? Hire/Consult an expert. You will suffer badly otherwise. Yes, an AI model will give you results. But you will have no way of knowing how well they do. Just having numbers is not enough. In return, this will help you make even better software.</li> <li>Survey Understanding your customer base is even more important here. The better you understand the requirements, the better you can make use of AI. The more you can flesh out your ideas, the more specific you can get with your software.</li> <li>Competition Who is your competition? Do you have the time to revamp your software? Everyone wants AI these days. Some companies can spend millions on it, while others can't. If you are in a field that has cutthroat technology developments, just starting up might be harder for you if you cannot afford it. If you are a new startup, try to focus on fields that need solutions but do not have larger corporations working on them. Domain expertise is great here.</li> <li>Using products from Google/Microsoft/Amazon You will find every big company these days offering AI support. Should you use them? In my opinion, these are useful if you are planning on performing a very common task. If you have a domain-specific idea, making your own is probably the better bet. But do not hesitate to make use of the resources that they offer. Google Colab, Amazon AWS, and Microsoft Azure are great services. Using them, especially at the start, is a good idea. They could be a cheaper way of testing out your ideas.</li> </ul> <p>Remember, AI is a tool. Not a complete product.</p>"},{"location":"Articles/Others/Ai_for_startups/#section-2-the-mid-stages-or-how-can-i-make-sure-i-make-my-best-ai","title":"Section 2 - The mid stages ,Or, How can I make sure I make my best \u201cAI\u201d?","text":""},{"location":"Articles/Others/Ai_for_startups/#focus","title":"Focus","text":"<p>Being an AI startup, you have a lot of things to look out for. Some of the main focus points to keep in mind are as follows. - Clear goals of what you want to achieve With any endeavor, knowing exactly what you want is imperative. More so in fields where testing takes up quite a lot of time and money. The better and more fine-grained your explanation, the better your results. - Clear expectations on what is possible within a time frame I repeat, AI is a research field. Just because Google can make a huge product in a matter of months, does not guarantee you can too. An expert can help you set more realistic goals. You can, of course, try it out on your own, but only if you have the time and the risk appetite for the same. - Easy of use Make sure your service is easy to use. Using AI will suddenly show you how many knobs and switches you need to control. Do not overwhelm your poor users. Specific research in understanding what your customer wants is essential. - Multiple large platforms Scaling up is probably not a major concern for you right now. If it is though, make sure you can afford it. If you can't, see if you can outsource it, or use an online service. Perhaps also consider other ways of optimizing your workflow. - Quick, rough tests Before you work on a final product, try out some small ideas. Hire freelancers if you do not have full-time staff on hand. Try out different models with any data you have on hand. Making sure you have a good baseline will save you quite some headache later on. - Validation testing By this point, you might have heard me say \"test your ideas\" quite a lot. But again, test your models. Use new data, use crappy data. Does it still work? If not, keep working.</p>"},{"location":"Articles/Others/Ai_for_startups/#people","title":"People","text":"<p>Aside from being able to build AI models, you also need people who would be able to support the infrastructure. Make sure you have domain experts you can call on. Also find people who would be able to deploy the model onto a chosen service, and can maintain them. Of course, the usual requirements of making an interface, servers, etc still stay based on the type of project you have in mind. Budget and requirements play a key role here and are specific to your idea and scale.</p>"},{"location":"Articles/Others/Ai_for_startups/#section-3-a-word-of-caution-how-can-i-make-sure-my-boat-does-not-sink","title":"Section 3 - A word of caution How can I make sure my boat does not sink?","text":""},{"location":"Articles/Others/Ai_for_startups/#capacity","title":"Capacity","text":"<p>Oh? So you made an AI model? Congratulations. Now you need to get your users on board. Regardless of if you have an app, a website, or a device, make sure you can estimate how many users you will have. AI models tend to take more resources than classical computer software.  Identify how long your model takes to get results. Will your users be fine with that? Do you need more resources to ensure faster outputs? Will they be able to reliably access your service? Test, test, test. There are quite a few optimizations possible. It is beyond the scope of this article, but you can drop me an email if you want to know more.</p>"},{"location":"Articles/Others/Ai_for_startups/#model-drift","title":"Model drift","text":"<p>Over time, how good your results are will drift. This might be because the data your model learned differs from the model it is getting now. For example, in a fashion-related product, the trend of clothing changes over time. Using an old model that has not seen new data will not be great. Periodically checking for a spike in errors, and retraining the model on new data is essential. Make sure your employees know how to do that.</p>"},{"location":"Articles/Others/Ai_for_startups/#bias-and-ethical-concerns","title":"Bias and ethical concerns","text":"<p>If you teach a kid that colored people are evil, that's what they will learn. Similarly, an AI model can learn mistaken assumptions. The larger the data, the more such assumptions are automatically made. Identifying these would help you understand if your product would have unintended consequences. The more variation you can provide to your model, the better in the long run.  Make sure your data is inclusive. Especially if it contains hints of the biases of age, gender, ethnicity, etc. Test for these biases specifically. Make sure that you can explain if your model goes wrong and does something stupid. There is a great course on how to do this Ethics for AI by Rachel Thomas from fast.ai.</p>"},{"location":"Articles/Others/Ai_for_startups/#fin","title":"Fin","text":"<p>So, will you use AI, or will AI use you Feel free to reach out or leave your comments. Email: msubhaditya@gmail.com</p>"},{"location":"Articles/Others/An%20understanding%20of%20will%20-%20Part%201/","title":"An understanding of will - Part 1","text":""},{"location":"Articles/Others/An%20understanding%20of%20will%20-%20Part%201/#an-understanding-of-will-part-1","title":"An understanding of will - Part 1","text":"<p>Willpower is illusive. In essence, it would be something that you don\u2019t encounter unless you see an extreme. Someone going through insane hardships to make ends meet, a mother lifting a car up to save her child, people struggling through all odds to escape oppression. But in real life, sometimes you encounter people like that. Ones that have lived entire lives with the ability to drag through. Hell, high water or drought. They will pull through anything. The question of course, is.. how? How can someone pull through being a 95 year old grandmother with every last living direct family member having passed on; in the last decade? How can one push through holding their husband, their child, their siblings as they took their final breath? It truly shows how strong we are , when we want to be.</p> <p>Willpower is fleeting, yet all pervasive. Sometimes it only lasts for a few moments. Sometimes it lasts for years. Sometimes it is that frantic burst of energy we get to clean our rooms and get our life straight at 3 am when we cannot sleep. Other times, it is people working towards a common goal for years on end, building schools, cities, empires.  How do any of these people prevail through ages of discomfort, misery, pain and what would seem like an inhuman amount of suffering?</p> <p>I think it boils down to a few core ideas. 1 - A reason For someone to get up at 6 am everyday to go for a run, there must be a reason. Without one, they are not better than a caged animal pacing around. Be it making a difference in someones lives, or as penance, or as a way of redeeming something that was lost. These are motives we hear all around us. But there are other, more subtle motives that show up from time to time. One persons dream of bringing their poor parents into a new home. Another person\u2019s dream of building a school for women who were never given a chance to learn. Another of providing wartime health services. All of them have something in common - emotion. Long, deep, slow. An ache that must be massaged.  2 - Challenges Through time immemorial, we have never faced a lack of problems. Most people wait for someone to solve them, some try and some others succeed. Through all of these stories, we see that most of these people were not exactly taught the hardcore perseverance that they show. Some had it beaten into them, others had no choice but to get their shit together, others were running from something they were too scared to face.  3 - Beliefs At the end of the day, we are the sum of our thoughts. The first time you realise, just how much your life is influenced by your perspective on life, it changes you. There are people who are not willing to believe that they would lie down and let their fate drag them. You see people like this, they stick out like a sore thumb. You know people like this, maybe a grandmother who faced war and famine but still is the nicest person you know. Or a parent, who against all odds, raised healthy and successful children. Or even a friend, who went against everyone and everything to pursue a career nobody approved of.  4 - Consistency  The person who wakes up at 6 am and goes for a run everyday, would be hard pressed to be eating chips the minute they wake up. But if they let themselves go for long enough, the efforts of years of discipline is lost. It is truly strange if you think about it isn\u2019t it?</p> <p>From these ideas, what can we think of that will make us\u2026 mentally stronger over time? If you start looking, you will find countless pieces of text that try to answer this question. This particular article does not aim to be more definitive than any that preceded it, but collects the stories the author has had the fortune of hearing.</p> <p>1 - Do something hard every day.  2 - Document your day, to yourself, honestly. Keep a journal.  3 - Find a few things that truly matter to you. Be it a sunset, or people, or that book you\u2019ve been reading. Beware though, that this will change in time. You will not have the same thoughts forever. What you like and care about will change. The point is to just have something that matters. 4 - Discipline really does matter. In times where sticking to a routine does not really seem feasible, look at the tiny things that you do. Did you tuck in your blanket? Drink water right after you wake up? Light a candle? Wind down? In times where all else fails, these little things give you the sanity that you would not find otherwise. Look inside, you will find your own little routine. 5 - Being part of a community. We need people. Hopefully some of those people need us too. It also helps if those people share a hobby with you.  6 - Finding the joy in the little things. It goes a long way. Sometimes a warm bowl of soup gives you the motivation to continue.  7 - Light your own candles. If you keep waiting, they will just remain dark and cold. 8 - Give yourself a little reward, if you can. If you learn to love the hardships, then you can be better at facing them 9 - Learning some form of meditation or prayer seems to help tremendously.</p>"},{"location":"Articles/Others/Duolingo%20is%20a%20platform%20for%20language%20learning/","title":"Duolingo is a platform for language learning","text":"<p>toc: true title: Duolingo is a platform for language learning</p> <p>categories: [\"article\"] date modified:  date created: </p>"},{"location":"Articles/Others/Duolingo%20is%20a%20platform%20for%20language%20learning/#my-take-on-some-ml-interview-questions-p1-ai-inprogress","title":"My Take on Some ML Interview Questions - P1 #ai #inprogress","text":"<p>Chip Huyen is one of my favourite authors in the space of MLOps. She has some great blogs, and a really useful book. In one of them, she asks the . This blog post is my answer to the ones I felt I could contribute interesting points to. Since there are quite a few, I will probably split them up into parts. </p> <p>Note: These are my views on these questions. They are not a comprehensive resource by any means. Just me thinking out loud on how I would go about solving the problem. Like any research project, as more time passes, these answers might change. They are here as a way for someone starting out to get a feel for approaching problems posed to them.</p> <ol> <li>Duolingo is a platform for language learning. When a student is learning a new language, Duolingo wants to recommend increasingly difficult stories to read.<ol> <li>How would you measure the difficulty level of a story?</li> <li>Given a story, how would you edit it to make it easier or more difficult?</li> </ol> </li> <li>You run an e-commerce website. Sometimes, users want to buy an item that is no longer available. Build a recommendation system to suggest replacement items.</li> <li>When you enter a search query on Google, you're shown a list of related searches. How would you generate a list of related searches for each query?</li> <li>Each question on Quora often gets many different answers. How do you create a model that ranks all these answers? How computationally intensive is this model?</li> <li>How to you build a system to display top 10 results when a user searches for rental listings in a certain location on Airbnb?</li> <li>When you type a question on StackOverflow, you're shown a list of similar questions to make sure that your question hasn't been asked before. How do you build such a system?</li> <li>On social networks like Facebook, users can choose to list their high schools. Can you estimate what percentage of high schools listed on Facebook are real? How do we find out, and deploy at scale, a way of finding invalid schools?</li> <li>How would you build a trigger word detection algorithm to spot the word \"activate\" in a 10 second long audio clip?</li> <li>If you were to build a Netflix clone, how would you build a system that predicts when a user stops watching a TV show, whether they are tired of that show or they're just taking a break?</li> <li>Facebook would like to develop a way to estimate the month and day of people's birthdays, regardless of whether people give us that information directly. What methods would you propose, and data would you use, to help with that task?</li> <li>Imagine you were working on iPhone. Everytime users open their phones, you want to suggest one app they are most likely to open first with 90% accuracy. How would you do that?</li> <li>How would you train a model to predict whether the word \"jaguar\" in a sentence refers to the animal or the car?</li> <li>How would you create a model to recognize whether an image is a triangle, a circle, or a square?</li> <li>Given only [CIFAR]-10 dataset, how to build a model to recognize if an image is in the 10 classes of ../../CIFAR|CIFAR.md|../../CIFAR|CIFAR-10 or not?</li> </ol>"},{"location":"Articles/Others/Easier%20Deep%20Learning%20Research%20for%20Beginners/","title":"Easier Deep Learning Research for Beginners","text":"<p>toc: true title: Easier Deep Learning Research for Beginners</p> <p>categories: [\"article\"] date modified:  date created: </p>"},{"location":"Articles/Others/Easier%20Deep%20Learning%20Research%20for%20Beginners/#starting-deep-learning-research-part-1-a-start-using-fastai-dl","title":"Starting Deep Learning Research (PART 1): A start using FastAI #dl","text":"<p>{{TOC}} So, you took your first steps into Deep Learning. Maybe you read a few articles, did a course or two, and watched a bunch of videos. Or maybe you just heard so much about it that you wanted to learn more.  Welcome.  This is a beautiful world. It is also very overwhelming. There is so much to learn and understand. But we need to start somewhere. This is your ticket. Enjoy the ride.</p> <p>Note: This will be very long-winded as it is meant for complete beginners. It might seem very scary at first. But don\u2019t panic. The hardest part is getting started. Hold on. Come back to it. It will take time. Slow down. Read through it. It will save you a lot of pain later on.</p>"},{"location":"Articles/Others/Easier%20Deep%20Learning%20Research%20for%20Beginners/#entry-requirements","title":"Entry Requirements","text":"<ul> <li>Go to this link for the code and follow along kaggle_notebook. Once that opens up, click the button next to \u201cAccelerator\u201d and choose GPU. Accept the terms.</li> <li>You have learned some Python. If not, go to YouTube and learn as much as you can first. Do as many examples and problems as you deem enough to understand. Come back.</li> <li>You know what a GPU is and if you have one.</li> <li>You have a computer and an internet connection.<ul> <li>If you have a powerful computer, you can set this up locally. fastai<ul> <li>You will need an editor. I would recommend VSCode. </li> <li>If you do not, that\u2019s alright. You can still follow along.</li> </ul> </li> <li>Watch a small tutorial on how to use a Jupyter Notebook here.</li> </ul> </li> </ul>"},{"location":"Articles/Others/Easier%20Deep%20Learning%20Research%20for%20Beginners/#what-we-want-to-do","title":"What we want to do","text":"<p>Today we will be teaching our little \u201cAI\u201d how to categorize different fruits. To do that, we need to give examples - aka the \u201cdataset\u201d. You can grab it from Kaggle.  We will show our little \u201cAI\u201d quite a lot of images and tell it \u201chey, this is an Apple, this is not one\u201d and so on, in the hope that it will learn. The exact algorithms are not important at this stage. I believe it is first good to be able to see where to use it in practice and play around with it before diving deep.  In the end, we want our \u201cAI\u201d to accurately classify the fruits that it has learned about.</p>"},{"location":"Articles/Others/Easier%20Deep%20Learning%20Research%20for%20Beginners/#libraries-we-will-use","title":"Libraries we will use","text":"<ul> <li>Major Framework : Pytorch. Deep learning is quite a vast field, which means that code quickly starts to become complicated. There is a lot of boilerplate code that needs to be written every time you want to make something. Writing the same 200 lines of code every time is not efficient, and leads to bugs and weird errors. To avoid that, we use a framework that handles most of the hard work for us so we can focus on innovation and application. There are quite a few of these frameworks, PyTorch and Tensorflow/Keras are the two major ones in Python. \\     I prefer Pytorch. There are many reasons for that. Instead of me adding to this blog about them, read this little article by the creator of the next library we will use, link</li> <li>fastai FastAI was one of my first introductions to research grade Deep Learning. It is a wrapper around Pytorch that does live up to its name. Pytorch, being a general framework, focuses on the core components of Deep Learning. FastAI was made to provide a lot of ease-of-use functions that Pytorch did not. But because it\u2019s just a wrapper, any functions that Pytorch offers can be easily accessed. </li> </ul>"},{"location":"Articles/Others/Easier%20Deep%20Learning%20Research%20for%20Beginners/#grab-the-data-and-set-everything-up","title":"Grab the Data and Set Everything Up","text":"<ul> <li>Make an account on Kaggle. This is not promoted or anything. It is probably one of the biggest hubs for AI code, and you will make an account sooner or later. Better to do it now.</li> <li>If you are using this link, click the 3 dots, and then click \u201cCopy and Edit\u201d. That will set up the data and the code for you. </li> <li>If you are working on your machine, you can download the notebook by clicking the 3 dots, and then clicking \u201cDownload code\u201d. Open this up in VSCode. The data can be found from Kaggle. </li> </ul>"},{"location":"Articles/Others/Easier%20Deep%20Learning%20Research%20for%20Beginners/#code-imports","title":"Code Imports","text":"<p>We will need to first import the libraries we will use.</p> <pre><code>import os\nfrom fastai.vision.all import *\nfrom fastai.vision.widgets import *\n\n# Set the base directory where Kaggle saves its data. Change this if you are on your machine.\nroot_dir = \"../input/fruits/fruits-360_dataset/fruits-360/Training\"\n</code></pre>"},{"location":"Articles/Others/Easier%20Deep%20Learning%20Research%20for%20Beginners/#load-data","title":"Load Data","text":"<p>Fastai provides quite a few convenient functions to load data. Now slow down. What do we have? Think about it for a second. We have an image and a label right? And we need to \u201cmap\u201d the image to the label. If this does not make sense. Think about it some more. This is important. Okay. </p> <p>Now, what does the file path for an image look like? Something like : \"../input/fruits/fruits-360_dataset/fruits-360/Test/Cantaloupe 1/r_140_100.jpg\u201d. What will the label be? \u201cCantaloupe 1\u201d right? This is the \u201cparent\u201d of the path, aka the parent folder. This fully depends on how the data is structured. More on that later.</p> <pre><code>def get_parent_name(x): \n    return x.parent.name\n</code></pre> <p>Okay, now we need to tell the system the kind of data we are giving it.   In this case, it is Image -&gt; Category (Label). Then we can find all the image files in the folders, and use the \u201cget_parent_name\u201d function to assign labels to all our data.</p> <p>When we learn something in school, we are tested on it with questions we have never seen right? This helps us understand if we comprehended the topic or not. Similarly, we split the data into Train and Test sets. We also resize the images to a single size (for convenience).  Since we want our little \u201cAI\u201d to recognize objects from any angle, lighting condition, etc, we provide some \u201caugmentation\u201d to the data. Things like randomly changing the brightness, rotating the image, etc. The objective is to provide variation, that further improves how well our network learns.</p> <p>We can then pass these instructions are create a \u201cData-loader\u201d. The definition is the name itself.</p> <pre><code>path = Path(root_dir) # base path\nfields = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_items=get_image_files,\n    get_y=get_parent_name,\n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    item_tfms=RandomResizedCrop(64, min_scale=0.5),\n    batch_tfms=aug_transforms(),\n)\ndls = fields.dataloaders(path)\n</code></pre> <p>\u200c Okay. Does this work? Let us get a sample (or a batch as it is called in this field.) </p> <pre><code>dls.show_batch()\n</code></pre> <p>How many categories of fruits do we have?</p> <pre><code>dls.c # no of categories\n</code></pre>"},{"location":"Articles/Others/Easier%20Deep%20Learning%20Research%20for%20Beginners/#teaching-our-ai-part-1","title":"Teaching our AI: Part 1","text":"<p>We have the data. Now we need the \u201cAI\u201d. You must be wondering why the word has been in quotes the whole time. It is because what we are training is not an \u201cAI\u201d, but just a component of it, called a \u201cNeural Network\u201d. </p> <pre><code>learn = vision_learner(dls,\n                resnet18, #architecture\n                loss_func=LabelSmoothingCrossEntropy(), #loss function/objective\n                opt_func=partial(OptimWrapper, opt=torch.optim.AdamW), # Optimizer\n                metrics=[accuracy, error_rate],\n                cbs=[MixUp]).to_fp16() #callbacks, mixed precision\n</code></pre> <p>The only things you need to know about it for now are :  - There are many types of networks, each better at something else. These are called \u201carchitectures\u201d. ResNet18 is one of them. - You can train a network to learn a specific type of mapping. Be it an image -&gt; text, audio -&gt; labels , image -&gt; image etc.  - This training takes time and is mathematically pretty hard. Which is why this field is still in research. Further explanations about all of this will be given in later articles. (And others that I have written about in the past, linked at the end.) - In addition to the architectures, we have algorithms called \u201cOptimisers\u201d that enable smoother learning. This will not make any sense right now. But all in due time. - A \u201closs function\u201d is fancy speak that is a way of seeing how well our network is doing while it\u2019s learning. A mini exam if you will, in the sense that the network tries to \u201coptimise\u201d for this exam. The better it gets, the better your final results. - Metrics give us somewhat precise values. Such as the grade in an exam. The \u201cAccuracy\u201d. Etc. - The other weird things that you see here are:     - cbs: MixUp.%20The%20first%20is%20to%20make%20sure%20that%20the%20network%20trains%20better,%20like%20the%20augmentations%20we%20spoke%20about.%20The%20second%20is%20an%20interesting%20paradigm%20that%20lets%20us%20train%20our%20models%20faster%20and%20with%20lower%20memory.%20More%20on%20that%20[here). These are advanced topics. More on that later.</p>"},{"location":"Articles/Others/Easier%20Deep%20Learning%20Research%20for%20Beginners/#teaching-our-ai-part-2","title":"Teaching our AI : Part 2","text":"<p>Let the Neural network.. drumroll.. learn!</p> <pre><code>learn.fine_tune(1, wd=0.5)\nlearn.export(\"model.pkl\") # Save the model\n</code></pre> <p>Fine-tune? Huh? 1? Huh? Don\u2019t worry too much about that right now. It is called Transfer Learning. And we are training the model for 1 epoch. (One run over all the data.). More epochs generally lead to better results.</p> <p>AND WE ARE DONE!! We have a working model already. This can recognize fruits. Congratulations!!!! </p> <p>(Too many words and terms. How will I ever learn them all? I should just give up now. Breathe. One thing at a time. Go through this. Come back. Go through bits you do not understand. More intermediate articles will follow. But in the meanwhile, the links are excellent resources on getting started.)</p>"},{"location":"Articles/Others/Easier%20Deep%20Learning%20Research%20for%20Beginners/#how-well-did-we-do","title":"How Well Did We Do?","text":"<pre><code>from fastai.interpret import *\nfrom fastai.vision.widgets import *\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_top_losses(5, nrows=1)\n</code></pre> <p>We can use this to see what our model gets confused about. This will change as you train it more.</p> <pre><code>interp.most_confused()\n</code></pre>"},{"location":"Articles/Others/Easier%20Deep%20Learning%20Research%20for%20Beginners/#using-the-model","title":"Using The Model","text":"<p>Okay, the last little bit here. We now need to use this trained model on images the network has never seen. (They were in a different folder. Also called the Validation set.) Fine, It\u2019s an exam. Best of luck little model. I hope you do well.</p> <pre><code>predictions_path = \"../input/fruits/fruits-360_dataset/fruits-360/Test\"\n\ndef predict_batch(self, item, rm_type_tfms=None, with_input=False): # this bit is slightly complicated. ignore it for now\n    dl = self.dls.test_dl(item, rm_type_tfms=rm_type_tfms, num_workers=15)\n    ret = self.get_preds(dl=dl, with_input=False, with_decoded=True)\n    return ret\nimport random\npredictions_path = Path(predictions_path)\nLearner.predict_batch = predict_batch\n\n# This is important\nlearn = load_learner(\"model.pkl\")\n\ntst_files = get_image_files(predictions_path) #same as before\ntst_files = tst_files.shuffle() \n</code></pre> <p>Now running the predictions. </p> <pre><code>preds = learn.predict_batch(tst_files)\nclasses = learn.dls.vocab # the original categories\npreds_mapped = list(map(lambda x: classes[int(x)], preds[2])) #just saving them out\n</code></pre> <p>So did it work? tst_files are the images we gave it. Preds is the output. Would you look at that?? It got most of them right :)</p>"},{"location":"Articles/Others/Easier%20Deep%20Learning%20Research%20for%20Beginners/#fin","title":"Fin","text":"<p>What\u2019s next? More articles. In the meanwhile, you can look at this little link with resources I have collected over the years.</p> <p>You have a long way to go. But I do hope this was a good start. I know you didn\u2019t read the whole thing. Maybe you didn\u2019t make it till here either. I get that. I also did that when I was starting.  This article is in the hopes that it will help someone out. Maybe have the help that I did not. I do not know who it will reach. But to whoever it does, best of luck :)</p> <p>You can contact me on LinkedIn, drop me an Email</p>"},{"location":"Articles/Others/GRAD/","title":"GRAD","text":"<p>toc: true title: GRAD</p> <p>categories: [\"article\"] date modified:  date created: </p>"},{"location":"Articles/Others/GRAD/#grad-cam-dl","title":"GRAD-Cam #dl","text":"<p>Note: This is a slightly advanced article. If you are not comfortable with training neural networks, this is probably not for you yet. Start here instead.</p>"},{"location":"Articles/Others/GRAD/#intro","title":"Intro","text":"<p>So you want to train a Neural Network to classify images. Woah. That\u2019s awesome! How well did it do? Did you get a good score? Oh? You want to do better? I hear you. What if you could see what the network sees to make the choice? That would help understand how to make it perform better right? Read on!</p> <p>A few years ago, a paper toc: true titled \u201cGrad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization\u201d by Selvaraju et al. talked about how we could visually see the activation maps of a trained CNN by looking at the gradients in the final layer. This post will show you how to use that for your own needs. </p> <p>Note: We will be using PyTorch and the fast.ai library. But the concepts stay the same, so you should be able to use it in any other library.</p>"},{"location":"Articles/Others/GRAD/#the-objective-and-data","title":"The Objective and Data","text":"<p>Before we can go to the code, what exactly are we trying to achieve? In short, we want to first train a network such as the \u201cresnet34\u201d architecture on any kind of data. In today\u2019s example, I will be using the Fish Dataset. This dataset has 9 types of sea creatures. You can of course, use any other image based classification data that you want. Knowing how the Dataset is structured is essential, so let\u2019s see that.</p> <p>Once the training is completed, we want to be able to give the trained network any image for inference and then have it spit out a visual heat map of exactly what it saw in the image. The brighter the color, the more the network focused on that particular spot to make it\u2019s decision. Have a look at the below example. </p> <p>In this image, we can see a yellow border around the fish. (Apologies if it is a disturbing image. Feel free to use any other dataset.) This shows that the network has identified the boundary of the fish and probably tried to use that information to decide what kind of fish it is.</p> <p>Examining these, we can see if the network is looking at the wrong thing, and find ways to show it what we want it to see. </p>"},{"location":"Articles/Others/GRAD/#code","title":"Code","text":""},{"location":"Articles/Others/GRAD/#training","title":"Training","text":"<p>Okay, let\u2019s not waste any more time and delve right into the code. If you are not familiar with fastai, you can look at this tiny blog for reference. The complete code can be found here on github.</p> <p>First, we import fastai as the deep learning library, matplotlib for plots, and IPython.display.Image to display the image inline. https://gist.github.com/SubhadityaMukherjee/grad_imports.py</p> <p>Training a classifier in fastai is just a few lines. We first decide where the dataset is.  Then we create a DataBlock. (Think of it as a constructor for a dataloader). This DataBlock is given the following information - Type of task : Image -&gt; Label - What to do : get the images from the directory - How to label the images : use the folder where the images are. For example, if the file name is \u201c/media/hdd/Datasets/Fish_Dataset/Fish_Dataset/Shrimp/Shrimp/00001.png\u201d , then it\u2019s parent is \u201cShrimp\u201d.  - How to split the data: Randomly with an 80/20 train/test split. - Transforms : Basic vision transforms, Crop and resize to 224x224 px.</p> <p>Once we have that, we can pass it to the main trainer class - the \u201cvision_learner\u201d. To it, we pass in the network we want to use (You can use any other as well), and the metrics we care about. The to_fp16() enables Mixed Precision Training that would further increase the training speed and decrease the memory consumption.</p> <p>Awesome! Now let\u2019s train the network. We are using a pre-trained resnet34 and performing transfer learning, so we use fine_tune. If you want to train from scratch, you can use \u201clearn.fit(1)\u201d instead. I trained it for a single epoch as a demo.</p> <p>https://gist.github.com/SubhadityaMukherjee/grad_train.py</p>"},{"location":"Articles/Others/GRAD/#hooks","title":"Hooks","text":"<p>I mentioned previously that we would be looking at the gradients of the trained network. But how do we access them?  In PyTorch, we can modify the components of the training loop using the concept of \u201cHooks\u201d. As the name suggests, it involves inserting a hook in the training loop and execute arbitrary code. Using that, we need to save the gradients during training. PyTorch has functions for the same called \u201cregister_forward_hook\u201d for the forward pass, and \u201cregister_backward_hook\u201d for the backward pass. We can take this information and write the following classes as a wrapper around our training loop. </p>"},{"location":"Articles/Others/GRAD/#plotting-activations","title":"Plotting Activations","text":"<p>Now for the intense bit. Let\u2019s pick a random image from the dataset. The original image is slightly disturbing so I blurred it a bit. </p> <p>Okay, we need to now pass the image through the model. The FastAI syntax for this kind of patching is slightly complicated. But let us walk through it. We first create the test data loader with the new image that we picked. We can then convert that into a Tensor. Once we pass the image to the network, it performs a full forward and backward pass on that image through every layer. Now for every layer in the model, we can get the computed gradients that we stored away using the Hook class.  Selvaraju et al. defined the activation map as the weighted combination of the forward activation maps. Which is what we perform.  The rest of the code is just matplotlib convenience functions to plot the nice grid heatmap that you see. The only weird line of code you might see is \u201ccam_map.detach().cpu()\u201d. This is done because we cannot plot a Tensor on the GPU, so we first detach it from the computational graph, then bring it back to the CPU to plot it.</p> <p>Well, yay! You made it. Try it with your own network and/or data. Just a word of warning, the 0 in line \u201cwith HookBwd(learn.model[0][layer]) as hookg\u201d, needs to be modified based on the network architecture. If you get errors, try with a 1 and so forth.</p> <p>https://gist.github.com/SubhadityaMukherjee/grad_plot.py</p>"},{"location":"Articles/Others/GRAD/#whats-next","title":"What\u2019s next?","text":"<p>Firstly, good job on making it so far! Pat yourself on the back or go grab something nice to eat. Then look at what the network sees. Does it make sense? Is the model looking at something weird? Train for a few more epochs, rinse and repeat. Try it for different images. You might find examples that make no sense. Sometimes it might be because of random augmentation, other times it might be because of model bias or the data itself being not okay. You will find ways to improve on it eventually. If you can, look for examples that the model gets wrong, and apply GRADCam on those.</p>"},{"location":"Articles/Others/GRAD/#fin","title":"Fin","text":"<p>Woah. That was long. What\u2019s next? More articles. In the meanwhile, you can look at this little link with resources I have collected over the years. You have a long way to go. But I do hope this was a good start. I know you didn\u2019t read the whole thing. Maybe you didn\u2019t make it till here either. I get that. I also did that when I was starting. This article is in the hopes that it will help someone out. Maybe have the help that I did not. I do not know who it will reach. But to whoever it does, best of luck :) You can contact me on LinkedIn, drop me an Email</p>"},{"location":"Articles/Others/Ideas/","title":"Ideas","text":"<p>toc: true title: Ideas</p> <p>categories: [\"article\"] date modified:  date created: </p>"},{"location":"Articles/Others/Ideas/#blog-ideas","title":"Blog Ideas","text":"<ul> <li>FastAI/Pytorch tricks : Shorts </li> <li>GRADCAM</li> </ul>"},{"location":"Articles/Others/Learnings%20from%20this%20time%20back%20home/","title":"Learnings from this time back home","text":"<p>toc: true title: Learnings from this time back home</p> <p>categories: [\"article\"] date modified:  date created: </p> <p>Lessons for future me : Part 1 - 31st Aug 2022 #learnings</p> <p>This is a time of change, and this trip was the period of rest between what my life was, and what it will become soon. I needed it. A break. A space between me and everything that I thought mattered. It gave me the time and environment to learn a lot more about myself and what I want to do in my life from now on until whenever my time comes. This is a list of the things that I realised in this holiday. It is probably not an exhaustive list, but it is something to look back on in the future and see how much this changed my life.</p> <ol> <li>Time. Time is the biggest helper. But it doesn\u2019t exist. And we give it the power to. The more we are attached to the clock, the lesser we give ourselves permission to actually enjoy life.</li> <li>Our whole lives, we had been used to a very confusing relationship with money. We either thought that we had enough, when dad bought something expensive, or were led to think that we didn\u2019t and wanting things was very wrong, when mom would remind us that wanting too many things was a bad and almost evil thing. When I realised that we had enough. We really did. But what parents wanted me to realise is that, we should forever be grateful for what we have. Nothing else. There is enough for me. And for everyone else around. If I have the ability to use a bit of what I have to make life slightly better for someone else, then I should do my best to enable them. If it is the right moment.</li> <li>Creativity is stumped by so many things. But mostly by the belief in not having it. Every time I sat down to paint, I should be grateful, and feel something first. A good emotion. A strong feeling. Without which, creation is not possible. It is the first step to manifestation. The same way I manifested winning an impossible choice in the Cover draw, or the way I manifested good weather and a beautiful trip. Or parents not coming there so I could have more time to heal. It is the first step </li> <li>In effect, many things stop us from doing things. One of them, and probably the greatest of them all, is fear. Fear of not having enough, fear of failure, fear of being rejected, an outcast. Probably also aggravated by ADHD. We have been different our whole lives, and have been shamelessly mocked for it. It made me resent everything and stopped me from starting things just for that fear.</li> <li>In the age of content and the internet, it is hard to say that you are the first person to come up with something. Me being me, that stops me from starting. Which is a waste of time. Once there is power, and an emotion, work must be done. Creation must come into being. There will be many obstacles to this. But creation cannot be stopped, one way or the other, through one person or the other, it will come into being. Remember the war of art?</li> <li>Relationships are made by understanding and effort. Over time, we lose track of who we were and who we fell in love with in the first place. Life shows us so many things, we take it for granted, and keep asking for things that do not exist. In essence, we take what was beautiful, and mar it. Over the past few months, neither me nor her have been okay. And instead of offering support, we ended up fighting not only life, but ourselves and our demons too. All empathy was lost. So was the understanding that neither of us wanted the other to solve our problems, or babysit each other. We have our own lives and ways of dealing with things. </li> <li>In the headlong rush to get over what has dragged me down all these years, I sometimes forget what the past has led to. Everything happens for a reason. This period has let me realise that, at the end of the day, people matter. So much. And so do I. But in the attempt to make a space of rmyself, I must realise that not everyone is in the same space as me. Maybe they will get there, maybe this is not their road. We can only be there on the way. </li> <li>Maybe my destiny is just this. To live. To guide when I am called. To provide for when it is needed. To make others realise how to make their lives flow forward. To create. To spread my light. Everything else is.. secondary. That\u2019s all that I am truly good at isn\u2019t it? Spreading my light. Telling people that they are enough, that they matter. That life is going to turn out okay. I am a guide. And I have been called for so many times. Along the way, I stopped listening. A guide doesn\u2019t have to take the person all the way does he? His only objective is to show the way. Point to what would be  a good place to go.</li> <li>So much changes with time, but I remain. So much happens around me, but I alone am unchanging. Forever present. Eternal. One thought for a thousand years.</li> <li>Parents did their best for me. Maybe they got lost too. They were children once too. I think there is no real adult here. Just kids. Very lost kids, with voices that so few people hear anymore. Nobody showed them how to live. And they made mistakes. A lot of mistakes. So will I. There remains no time to blame anyone. Just accept, take what serves me, give back what doesn\u2019t.</li> <li>Games and shared experiences bind us together. We forgot how to be together. A simple game of Uno brings so many people joy. No matter how old or young they are. Baking cookies, brings a smile on everyones face. </li> <li>Being part of an experience with people around you, bring satisfaction. Be it baking, or painting, or just walking. But the real question is, how much of their attention is directed towards what you are doing together. </li> <li>Long journeys are frustrating, only because we think of how long it will take. We keep looking at the time. But the miracle of it all, is just.. breathtaking sometimes. A few \u201chours\u201d and everything you know and love changes. New people, new places, new experiences. The only cost is some unrest. And even that fades in time. This journey will take 18 \u201chours\u201d. But its a journey between two eras in my life. In the moment, it really is not too bad is it?</li> <li>There are an infinite streams of energy around us, all the time. Maybe you can call it the Dao. They are us. We are them. Streams do not flow backwards. They keep moving forward. Regardless. That\u2019s how life goes too. We move from places of stagnation and rot, to life and change. </li> <li>We have an intimate relationship with food. Whatever it may be, being involved in the process of making something nourishing is something I greatly enjoy. I would like to spend many hours in my life, creating and enjoying these creations. </li> <li>Art. Art came out of nowhere and took over everything. Or so I thought. It was always there. Everywhere. Doodles. Those fake tattoos from School that mom would have me wipe off everyday, all those scribbled doodles across my notebooks, the random urges to create, make something, draw and colour. It was something I did not fully understand until recently. Now I just need to speak the language. I am learning it, it will take time. But I will let it flow and see where it takes me. It\u2019s a journey after all.</li> <li>Travelling is something I very much want to do. In time. And whenever I can. The experiences and the ideas that flow through, are unmatched. </li> <li>The idea of impermanence is something that I want to focus on. Every time I feel sad about not being too close to someone. Or anything. I know that its not going to last. Nothing does. Its a cycle of lows and highs. Or whatever we choose to perceive. My time here is not permanent. Neither are my grudges, or discomfort with people. </li> <li>Everyone, deep down, wants similar things. Even people who appear cold and uncaring, turn out to be the most broken and confused souls. They too just want a warm hug and someone to tell them that it will be okay. Especially when they are drunk.</li> <li>People regret saving up a lot and never spending it when they have the time to. Mainly because they are dead.</li> <li>Resistance.md|../../Resistance|Resistance is the biggest killer of anything. The more you resist something, the more it might be an indicator of what you truly want to do.</li> <li>With old age and too much power, comes a sense of ego. A sense of \u201cI am the best\u201d.</li> <li>Journalling is life changing. Especially micro journaling</li> <li>Don\u2019t forget to backup!</li> <li>Sometimes you need to negotiate. People are not always extremely nice, or honest.</li> <li>Plain text files rule. Ease of use, simplicity, universal support, easy to backup. What else can one want? Pictures? Oops, use markdown.</li> <li>Having a knowledge base of information is amazing. And probably the best first step to truly.. doing research. Your own personal interconnected search. How truly beautiful. A whole second brain.</li> <li>Doing something everyday, no matter how tiny, leads to massive changes over time.</li> <li>Sometimes all you need to do to start writing is to set a timer of 10 minutes on an app that erases your work if you stop. </li> <li>Minimalism is not always the answer. Neither is it always a good question.</li> <li>Resentment builds up over a long time, from sacrificing things that you need not have, for reasons that were not clear at the time.</li> <li>Emotions, pure emotions, are a good indicator of if your choices are right.</li> <li>Destiny.. oh sweet destiny, how intricate your works are.</li> <li>Friends.. sometimes do not seem to really exist. But they have their own lives. That does not mean they do not want you in theirs.</li> <li>Moving abroad is.. hard. Extremely hard. Especially because you\u2019re in a place where almost everyone has a community already. And you barging in is almost never going to work. It will take a very long time. Better get used to looking at yourself in the mirror.</li> <li>When people tell you, hey, you could look a lot more professional and smart if you gave a shit. Please give a shit. Please. At least try.</li> <li>If you do not meditate for a very long time, prepare for the consequences. </li> <li>Instead of just stretching, some yoga will go a very long way.</li> <li>Trust the process. </li> <li>Pour your time into things. In the majority of cases, that is what is lacking. Both with art and with life.</li> <li>Not eating meat is probably a good idea. That doesn\u2019t mean cutting it out, it means not treating it like the most important part of the meal. This comes from your Bengali upbringing and you can feel free to get over it.</li> <li>So many things in your past have been tainted, no the better word is influenced, by your ADHD. From time blindness, to forever forgetting things, to taking time to process things, to auditory overload. People dont understand it yet, its okay though. Its not their fault. Neither is it yours. </li> <li>We should be grateful to YouTube for everything it has taught us so far. Who knows what will come later. But they did give us a good base.</li> <li>Focus on giving time to research. You never really have. Maybe because of your ADHD, or maybe because of your fear of failure. Things take time. Give it the time it deserves.</li> <li>The things that you are the most afraid of, thinking that it would take the longest, will actually get over pretty fast. Just try it okay?</li> <li>Your inner child needs a lot more love than you think. But remember that like all children, and like anything else, too much of anything leads to ruin. Heal the parts that need it, let the others go.</li> <li>Most emotions are temporary. Watching them go by is probably the best course of action before doing anything about it.</li> <li>Try to remember names. People really like it</li> <li>Dale Carnige was a goddamn genius.</li> <li>The Pareto principle applies every single place</li> <li>Use your calendar a little more. It really helps out.</li> </ol>"},{"location":"Articles/Others/Myself/","title":"Myself","text":"<p>toc: true title: Myself</p> <p>categories: [\"article\"] date modified:  date created: </p>"},{"location":"Articles/Others/Myself/#myself-learnings","title":"Myself : Learnings","text":""},{"location":"Articles/Others/Myself/#-when-you-are-slogging-on-something-and-feel-like-not-doing-it-consider-if-it-is-worth-it-to-start-over-sometimes-that-is-what-you-need","title":"- When you are slogging on something and feel like not doing it, consider if it is worth it to start over. Sometimes that is what you need.","text":""},{"location":"Articles/Others/Notes%20-%20Machine%20Learning%20System%20Design/","title":"Notes   Machine Learning System Design","text":"<p>toc: true title: Notes - Machine Learning System Design</p> <p>categories: [\"article\"] date modified:  date created: </p>"},{"location":"Articles/Others/Notes%20-%20Machine%20Learning%20System%20Design/#notes-on-machine-learning-system-design-for-interviews-by-chip-huyen-ai","title":"Notes on \u201cMachine Learning System Design\u201d (for Interviews) by Chip Huyen #ai","text":"<p>These are my notes on this article by Chip Huyen. </p> <p>The article that these notes are based on, talks about some factors that are involved in designing machine learning systems, and what to watch out for in interviews on the same. I wanted to write a summary of what I read and add my take on it for future reference.</p> <p>As a note, I found Chip\u2019s book Designing Machine Learning Systems an excellent resource for anyone starting or willing to improve their skills in this field. I would recommend a read. (This is not sponsored by any means.)</p>"},{"location":"Articles/Others/Notes%20-%20Machine%20Learning%20System%20Design/#interviews","title":"Interviews","text":"<ul> <li>The major issue with Machine Learning interviews seems to be the lack of a standard criterion by which to judge a candidate. This makes a lot of sense considering how varied the requirements of each project are.</li> <li>Interviewers generally look for what they are familiar with and often this means that an ideal candidate would be someone who thinks along similar lines. This is especially interesting in ML because there are an infinite number of ways to approach a problem.</li> <li>It would be helpful for the candidate to understand what kind of answers the company would be looking for based on their previous work. </li> </ul>"},{"location":"Articles/Others/Notes%20-%20Machine%20Learning%20System%20Design/#compute-requirements","title":"Compute requirements","text":"<ul> <li>Contrary to the vast amount of research in the ML space on improving models and focusing on metrics, in production, users might barely notice a tiny improvement in accuracy. In return, this would further increase the complexity of the system and thus its latency.</li> <li>It is important to first understand exactly what you wish to achieve and what you need to optimize for. </li> <li>You cannot do everything.</li> </ul>"},{"location":"Articles/Others/Notes%20-%20Machine%20Learning%20System%20Design/#setting-up-the-project","title":"Setting Up The Project","text":"<ul> <li>At the end of the day, an ML system also requires skills from Software Engineering. A complete system is not just a model but a lot of moving parts. Thus there are quite a few tradeoffs to consider.</li> <li>An initial thought process<ul> <li>Pick the top 2 goals that your solution needs</li> <li>What does the user interaction look like?<ul> <li>Do you care about personalized results? </li> <li>Does latency matter?</li> <li>How much of your system relies on ML?</li> <li>What kind of devices are you looking at for deployment?</li> <li>Does privacy matter?</li> </ul> </li> <li>Data<ul> <li>Do you have the data? </li> <li>Is it usable? Is it clean?</li> <li>Where is it stored? How much of it is stored?</li> </ul> </li> <li>What metrics are useful in this context? (Domain expertise would be very helpful here)</li> <li>What resources do you have/can acquire? People, time, users, etc</li> </ul> </li> </ul>"},{"location":"Articles/Others/Notes%20-%20Machine%20Learning%20System%20Design/#baselines","title":"Baselines","text":"<ul> <li>These systems can get complex pretty fast. So start with the simplest possible algorithm. You might not even need a very complex Deep learning system. And it might not be feasible at the start.</li> <li>To evaluate if it is worth shifting to a more complex implementation, looking at baselines is essential.<ul> <li>Random Baseline: How well would a random guess do?</li> <li>How well does a human do on this task? (If that is feasible)</li> <li>What minimum results do you need for a functioning solution?</li> </ul> </li> </ul>"},{"location":"Articles/Others/Notes%20-%20Machine%20Learning%20System%20Design/#debugging-models","title":"Debugging Models","text":"<p>This is one endless minefield.  In my experience, this list by Andrej Karpathy is a pretty comprehensive guide. I do not have anything to add to it so I will skip this section.</p> <p>As a recommendation though, I would suggest considering the use of a framework such as fast.ai. These are built with a lot of issues in mind and take care of a lot of them. </p>"},{"location":"Articles/Others/Notes%20-%20Machine%20Learning%20System%20Design/#model-scaling","title":"Model Scaling","text":"<ul> <li>Most large-scale systems these days use Parallel and Distributed computing. Multiple GPUs/TPUs etc.</li> <li>If your data does not fit into memory, there are many ways of getting it - Gradient checkpointing, Mixed Precision, and Parallelism are some of them. (Future blogs will go a lot more in-depth. Putting it here will make it pretty huge.)</li> </ul>"},{"location":"Articles/Others/Notes%20-%20Machine%20Learning%20System%20Design/#inference","title":"Inference","text":"<ul> <li>There are a lot of steps and tradeoffs involved in inference. <ul> <li>Where will you run your model? How long does it take? Can you say with certainty why a particular result is obtained? Can you retrain it? </li> <li>Before deployment, it is advisable to turn off all the modifications one by one and test how well the system performs without them.</li> <li>Check for biases in the results. (For example: Does your model prefer colored people?)</li> <li>Does your data remain constant? Or keep changing?</li> </ul> </li> </ul>"},{"location":"Articles/Others/Notes%20-%20Machine%20Learning%20System%20Design/#fin","title":"Fin","text":"<p>The list of things to consider to make an efficient and thought-out model is endless. I do not think it is possible to take everything into account in the real world, but the more you think these through and follow these guidelines, the better and more stable your implementation will be. Of course, this is more of a short primer rather than a comprehensive guide. Future articles will cover more details.</p> <p>This article is in the hopes that it will help someone out. Maybe have the help that I did not. I do not know who it will reach. But to whoever it does, best of luck :)</p> <p>You can contact me on LinkedIn, drop me an Email</p>"},{"location":"Articles/Others/Professional%20Reports%20and%20Papers%20with%20LaTEX/","title":"Professional Reports and Papers with LaTEX","text":"<p>toc: true title: Professional Reports and Papers with LaTEX</p> <p>categories: [\"article\"] date modified:  date created: </p> <p>Writing Professional Reports and Papers at 2x speed - A LaTeX tutorial + tips</p> <p>At some point in your study or career, you will be required to write a report or an article, perhaps even a research paper. You start writing your content down in Word or your favourite text editor. After a little while, some issues crop up. Maybe you don\u2019t like how it looks, or you want to change the format to a two column layout. Maybe you moved an image and the entire document went crazy. Maybe you wrote a long article, but have to change your citations manually. Or heaven forbid, you need to create a table of contents.  Ugh, you think. I don\u2019t have time for this. Just let me work on the content. If only there was a better way.</p> <p>Sounds familiar? Read on.</p>"},{"location":"Articles/Others/Professional%20Reports%20and%20Papers%20with%20LaTEX/#latex-vs-word","title":"LaTEX vs Word","text":"<p>At the heart of everything, what you are struggling with is the issue of change. If you had a small article, with barely any changes or styling, Word is great. But for anything more than 5 pages? Ouch. LaTeX is a complete document preparation system, with the added advantage of a different \u201clanguage\u201d that makes your life a lot easier. It sounds and looks very strange at first. But once you get the hang of it, it will change the way you write content. Think of it as a more advanced template that is infinitely customisable. All you do is set things up once. Then you can focus on your content. Oh no! Something changed? That\u2019s okay. Add a few lines to your document and you are good to go. I do all my reports, articles, homework, projects everything using it and it has saved me days of effort. It looks professional right off the bat.</p> <p>(Note: You do NOT need to be a programmer to use this. Just go with it. And google to your heart\u2019s content.)</p>"},{"location":"Articles/Others/Professional%20Reports%20and%20Papers%20with%20LaTEX/#intro-to-overleaf","title":"Intro to Overleaf","text":"<p>So how do you use this magic mushroom? Well, luckily we have Overleaf. This website allows you to write anything you want, provides a lot of templates, live collaboration, and so much more. Mostly for free as well. Just open the site and make an account. If your institution provides premium access, use your official email ID to register. (You don\u2019t really need it for most use cases.)</p> <p>(Note: This is not a sponsored post. I just use it everyday and want to make sure it helps more people.)</p> <p>Everything I show can be viewed at this link.</p>"},{"location":"Articles/Others/Professional%20Reports%20and%20Papers%20with%20LaTEX/#choosing-a-template","title":"Choosing A Template","text":"<p>Okay great, now it\u2019s time to start working on a project. I will demonstrate with a small article : \u201cComputer Vision in Pytorch - A Primer\u201d.  - Click \u201cNew project\u201d - Based on what kind of work you are writing this article for, pick a section. For this article, I picked Academic Journal.  - Look around and find the look you are going for. Click on it, and then click \u201cUse as template\u201d. If you are a University, look for an official template, most of them have one. - I picked this - IEEE template Perfect! You are halfway there. </p>"},{"location":"Articles/Others/Professional%20Reports%20and%20Papers%20with%20LaTEX/#initial-setup","title":"Initial Setup","text":"<p>Before we start, let\u2019s just get used to the interface.  On the left you can see a sidebar with all your files and folders.  To it\u2019s right, there is the main editor window. If you look closely, the text seems a little strange? Don\u2019t worry, more on that later. After that you will see a preview of your article. Every time you hit save or \u201crecompile\u201d, this preview will update. You can latex export this as a PDF. - In the sidebar, I like to make a folder for images by click the little icon that looks like a folder. - Now, copy paste the first few lines until the line before \u201cbegin document\u201d. Think of these as extra functionality. For example: highlighting your links, structuring your document etc. - Make a new document toc: true titled \u201cmain.tex\u201d using the file icon.  - Paste the contents there.</p> <p>Great job! Now we can get to writing everything.</p>"},{"location":"Articles/Others/Professional%20Reports%20and%20Papers%20with%20LaTEX/#understanding-the-components","title":"Understanding The Components","text":"<p>Now the following might seem slightly too complicated. And you will probably feel like it\u2019s not worth the effort. But, trust the process okay? So far, you should have something like this. ```tex,latex_report_modules,latex_report_modules \\documentclass[conference]{IEEEtran} \\IEEEoverridecommandlockouts % The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out. \\usepackage{cite} \\usepackage{amsmath,amssymb,amsfonts} \\usepackage{algorithmic} \\usepackage{graphicx} \\usepackage{textcomp} \\usepackage{xcolor} \\def\\BibTeX{{\\rm B\\kern-.05em{\\sc i\\kern-.025em b}\\kern-.08em     T\\kern-.1667em\\lower.7ex\\hbox{E}\\kern-.125emX}}</p> <pre><code>Now we define a basic template.\n```tex,latex_report_template,latex_report_template\n\\begin{document}\n\n\\toc: true\ntitle{Computer Vision in Pytorch - A Primer\\\\\n\\thanks{I thank Overleaf for this template}\n}\n\n\\author{\\IEEEauthorblockN{1\\textsuperscript{st} Subhaditya Mukherjee}\n\\IEEEauthorblockA{\\textit{Faculty of Science and Engineering } \\\\\n\\textit{University Of Groningen}\\\\\nGroningen, Netherlands \\\\\nmsubhaditya@gmail.com}\n}\n\\maketoc: true\ntitle\n\n\\begin{abstract}\n\\end{abstract}\n\n\\begin{IEEEkeywords}\n\n\\end{IEEEkeywords}\n\n\\section*{}\n\\subsection*{}\n\n\\begin{thebibliography}{00}\n\n\\end{thebibliography}\n\n\\end{document}\n\n</code></pre> <p>You will only need to do this once to get a feel for what\u2019s happening. It\u2019s scary I am sure. But hold on. Keep trying. You\u2019ve got this! Look at the following code snippets twice. Do you see a pattern?</p>"},{"location":"Articles/Others/Professional%20Reports%20and%20Papers%20with%20LaTEX/#title-abstract-keywords","title":"Title, Abstract, Keywords","text":"<p>We now need a toc: true title. So we put it in this line. If you notice the little {}, that is required by LaTeX to know where to start and end something. ```tex,latex_report_toc: true title, latex_report_parts \\toc: true title{Computer Vision in Pytorch - A Primer}</p> <pre><code>Are you required to write an abstract or a summary of sorts?\n```tex,latex_report_abstract, latex_report_parts\n\\begin{abstract}\nThis paper is a short introduction to Pytorch, a deep learning framework. Special focus will be given to applications of Computer Vision. This is a demo paper, and has no particular significance.\n\\end{abstract}\n</code></pre> <p>Sometimes, a template will have keywords. You can just enter the ones you think are relevant.</p>"},{"location":"Articles/Others/Professional%20Reports%20and%20Papers%20with%20LaTEX/#authors","title":"Authors","text":"<p>Every template you copy, will have a section for the author. Just fill it in the way you want.</p>"},{"location":"Articles/Others/Professional%20Reports%20and%20Papers%20with%20LaTEX/#sections-and-subsections","title":"Sections and Subsections","text":"<p>Now for the actual content. Here we use the commands \u201csection\u201d, \u201csubsection\u201d, \u201csubsubsection\u201d. You do not need to bother about giving them numbers. LaTeX will take care of it. Something like this is a good start. ```tex,latex_report_section, latex_report_parts \\section{What is Computer Vision?} A study of the techniques used to extract meaning from image or video related data. The applications are endless, starting from face recognition, to self driving cars. \\subsection{Computer Vision in the field of Deep Learning} Deep learning has revolutionised the field of Computer Vision by giving it superpowers. The ability to learn from billions of images come as a huge leap forward in the field. \\subsubsection*{A note} Classicial CV is still very relevant today.</p> <pre><code>\n### Figures\nDoesn\u2019t seem too hard does it? Let\u2019s add some figures. Woah. What is happening here??\nWe are defining how we want our figure to look! We tell the system where the image is, how want it to look - centered, fit inside the line, have a caption, and a label. Just change the text to what is relevant to you.\nThink about it once. Makes sense right?\n```tex,latex_report_fgure, latex_report_parts\n\\begin{figure}[h]\n\\includegraphics[width=\\linewidth]{figures/2560px-PyTorch_logo_black.svg.png}\n\\centering\n\\caption{Representation in the Simulation}\n\\label{fig:colors}\n\\end{figure}\n</code></pre> <p>If you want to change the size, replace \u201c\\linewidth\u201d with something like \u201c0.2\\linewidth\u201d which makes the figure 0.2 times the size of the line.</p>"},{"location":"Articles/Others/Professional%20Reports%20and%20Papers%20with%20LaTEX/#formatting-text","title":"Formatting Text","text":"<p>Okay how about formatting such as bold italics and the like? Super simple. Look at these lines.</p> <p>```tex,latex_report_form, latex_report_parts \\section{Formatting} This \\textbf{will be bold}, then \\textit{italic}, and also \\textcolor{red}{red}. </p> <p>To add a line break, simply add \\ this will be a new line</p> <pre><code>See? That was not too bad was it? Now we have colours as well!\n\n### Code\nIf you are a programmer, or need to have any bits of code, this is how you can do it.\nTo the section where the packages were imported, add the following line.\n```tex,latex_report_code,latex_report_code\n\\usepackage{listings}\n</code></pre> <p>Now wherever you want to add code, just use it like this. Change the language to what you need of course. Viola! Syntax highlighting!</p> <p>```tex,latex_report_code_2, latex_report_parts \\begin{lstlisting}[language=Python] import numpy as np print(np.random.rand(10)) \\end{lstlisting}</p> <pre><code>\n### Equations\nHave you ever had to add equations in Word? I feel sorry for you. LaTeX lets you do it in a breeze. \n```tex,latex_report_equations, latex_report_parts\n\\section{Equations}\nWe can have three types of these - An inline equation : $2x+3 = 10$, or a proper block , $$2 \\sin(x)+10 = 100$$ or a long form one such as this.\n\\begin{equation}\nE[g^{2}]_{t}= 0.9E[g^{2}]_{t-1}+ 0.1g^{2}_{t}\\\\\n\\theta_{t+1}= \\theta_{t}- \\frac{\\eta}{\\sqrt{E[g^{2}])_{t}+\\epsilon}}g_{t}\n\\end{equation}\n</code></pre> <p>It would be impossible to explain the intricacies of using these, but as you can see, it almost feels like writing the equation down as it is. And it looks gorgeous as well. For more information, refer to this nice page.</p>"},{"location":"Articles/Others/Professional%20Reports%20and%20Papers%20with%20LaTEX/#cover-page","title":"Cover page","text":"<p>Hopefully you picked a template that already had one. But if you did not, add this before \u201c\\begin{document}\u201d. Replace the text however you feel like.  ```tex,latex_report_cover, latex_report_parts \\begin{toc: true titlepage}    \\begin{center}        \\vspace*{1cm}</p> <pre><code>   \\textbf{Thesis Title}\n\n   \\vspace{0.5cm}\n    Thesis Subtoc: true\n</code></pre> <p>title</p> <pre><code>   \\vspace{1.5cm}\n\n   \\textbf{Author Name}\n\n   \\vfill\n\n   A thesis presented for the degree of\\\\\n   Doctor of Philosophy\n\n   \\vspace{0.8cm}\n\n   \\includegraphics[width=0.4\\textwidth]{university}\n\n   Department Name\\\\\n   University Name\\\\\n   Country\\\\\n   Date\n</code></pre> <p>\\end{center} \\end{toc: true titlepage}</p> <pre><code>\n### Table Of Contents\nAdding a TOC is even easier. It also updates automatically. Just add.\n```tex,latex_report_toc, latex_report_parts\n\\tableofcontents\n</code></pre>"},{"location":"Articles/Others/Professional%20Reports%20and%20Papers%20with%20LaTEX/#appendix","title":"Appendix","text":"<p>Want a list of images and/or tables you have used throughout your document? With page numbers. ```tex,latex_report_appendix, latex_report_parts %\\newpage %add this if you want it to be on a separate page \\begin{appendix}   \\listoffigures   \\listoftables \\end{appendix}</p> <pre><code>\n### Tables\nTables are a bit complicated in LaTeX, but there is an easier way. Just open this website - [Table generator](https://www.tablesgenerator.com). \nThis is a very user friendly UI, so just add whatever you want. And click generate. \nCopy paste that into your Overleaf editor. Done!\nNotice the auto numbering? Cool right?\n\n### Citations\nCitations are one of the most powerful features of working in LaTeX. The best part? Only the ones you cited will show up in your bibliography! You do not need to worry if you missed any, or forgot to remove any. All you need to do is paste all your citations in a bib file.\n- Create a file called references.bib\n- Paste all your references in \u201cBibTex\u201d format in the file. Google scholar or any reference manager you use will have that option.\n- Cite them like this\n```tex,latex_report_cite, latex_report_parts\n\\section{Citation Example}\nTwo interesting libraries are Kornia \\cite{riba2020kornia} and fast.ai \\cite{howard2020fastai}. If you want it inline then : \\citep{howard2020fastai}.\n</code></pre>"},{"location":"Articles/Others/Professional%20Reports%20and%20Papers%20with%20LaTEX/#bibliography","title":"Bibliography","text":"<p>Once you have added all your citations, you would need to have a Bibliography. The file \u201creferences.bib\u201d you created? Remember the name. Right before \u201c\\end{document}\u201d you can add these lines. (Change the first one to what you need. Your template mostly will define it already.) ```tex,latex_report_bib, latex_report_parts \\bibliographystyle{IEEEtran} \\bibliography{references}</p> <p>\\end{document} ``` Looking good!</p>"},{"location":"Articles/Others/Professional%20Reports%20and%20Papers%20with%20LaTEX/#comments","title":"Comments","text":"<p>Want to make comments that you or a fellow author can refer to in the future? Just select any bit of text in the editor, you will get a pop up for adding a comment.</p>"},{"location":"Articles/Others/Professional%20Reports%20and%20Papers%20with%20LaTEX/#exporting","title":"Exporting","text":"<p>Congratulations! Looks like you made it to the end. Now how do you export your awesome document?  See the little button that has a downward arrow? Click that. You can also click the \u201cMenu\u201d button and find more options there.</p>"},{"location":"Articles/Others/Professional%20Reports%20and%20Papers%20with%20LaTEX/#collaboration","title":"Collaboration","text":"<p>Want to work with colleagues/teammates? Just hit Share and send them the link.</p>"},{"location":"Articles/Others/Professional%20Reports%20and%20Papers%20with%20LaTEX/#general-principles-and-how-to-get-help","title":"General Principles and How To Get Help","text":"<p>That\u2019s about it for the basics. Feel free to come back to this document for your reference. You will face problems. Just remember the following things. - Google is your best friend. - This website is a good place to start looking for what\u2019s possible. - It will take a few tries. But it is definitely worth it. - If you can say what you want in English, you mostly know the commands already. - There are a lot of commands. You do NOT need to remember them. You will passively pick them up. Overleaf also has autocomplete which helps. - Give it some time, it will change your life.</p>"},{"location":"Articles/Others/Professional%20Reports%20and%20Papers%20with%20LaTEX/#fin","title":"Fin","text":"<p>This article is in the hopes that it will help someone out. Maybe have the help that I did not. I do not know who it will reach. But to whoever it does, best of luck :)</p> <p>You can contact me on LinkedIn, drop me an Email</p> <p>Like my work? Buy me a coffee :) </p>"},{"location":"Articles/Others/Sigh%2C%20Gone%20-%20My%20Thoughts%20on%20the%20Book/","title":"Sigh, Gone   My Thoughts on the Book","text":"<p>toc: true title: Sigh, Gone - My Thoughts on the Book</p> <p>categories: [\"article\"] date modified:  date created: </p>"},{"location":"Articles/Others/Sigh%2C%20Gone%20-%20My%20Thoughts%20on%20the%20Book/#sigh-gone-phuc-tran-my-thoughts-on-the-book","title":"Sigh, Gone ; Phuc Tran - My Thoughts on the Book","text":"<p>At some point in life, we stop reading and shift to faster content - articles, videos, and now \u201creels.\u201d This year, I decided to go the other way and read as many books as possible. And perhaps write about some of them. Summarizing a book, though, is an exercise in futility and encourages more \u201cfast content,\u201d which is the opposite of what I want to do. Instead, I want to explore some of my thoughts while reading these books as a note to you, my dear reader, and future me.</p>"},{"location":"Articles/Others/Sigh%2C%20Gone%20-%20My%20Thoughts%20on%20the%20Book/#introduction","title":"Introduction","text":"<p>This article is about the book Sigh, Gone: A Misfit's Memoir of Great Books, Punk Rock, and the Fight to Fit In by Phuc Tran. The book is a journey through the author\u2019s life as a second-generation immigrant in America and his struggle to fit in. Phuc talks about many things: kind families, racism, trying to blend in, books, and what influenced him growing up. I loved the writing style and how every chapter was the name of a book. It was an enjoyable read, although some parts did make me want to cry.</p> <p>Okay, that\u2019s enough of an introduction\u2014time for a more serious discussion.</p>"},{"location":"Articles/Others/Sigh%2C%20Gone%20-%20My%20Thoughts%20on%20the%20Book/#blending-in","title":"Blending In","text":"<p>I think it makes sense to start with the actual premise of the book, without which the context is impossible to understand. Immigrants face many challenges wherever they go, some harder than others. While this is hard for adults, it is even more challenging for children. Growing up, they not only have to live in a society that does not want them there but also understand what shaped their families. Many children experience a different life before moving to a foreign country, and the change is sometimes too difficult for them to understand. More often than not, they grow up with many conceptions about the world and themselves that are not helpful to them as adults. </p> <p>But everyone is different, so why not just be yourself? Well, that is easier said than done. Growing up, the author tries to fit in with a society his family is unfamiliar with. He faces blatant racism, violence, and impossible expectations. As a kid, this led to a lot of confusion. Perhaps the only way to fit in is to be a misfit? </p>"},{"location":"Articles/Others/Sigh%2C%20Gone%20-%20My%20Thoughts%20on%20the%20Book/#cultures","title":"Cultures","text":"<p>From personal experience, culture is the biggest source of my confusion in understanding my identity. Growing up as a part of two different cultures is genuinely confusing. For the author, this, too, was a significant source of confusion. Who was he? In his community, he was an Oriental, a highly fetishized term. According to some of his siblings from larger cities, he was an Asian, someone who could be cool. But perhaps he was an American too? But he could never be what they were - White.</p> <p>We often struggle to blend in because we don\u2019t have a fixed identity. All these parts of us blend in to be a mix of everything. But society loves putting people and concepts in rigid boxes. If you are a punk rocker, you are not a good student. If you are a person of color, you can never be as cool as the others. </p> <p>This affects us all the way down, from food to clothing to our self-image and worth. Perhaps inherently, we believe that parts of us are bad and the parts that fit in are good. We forget that everyone is different. We try to fit in by erasing all the parts of us that are bad but end up erasing many that are good too.</p>"},{"location":"Articles/Others/Sigh%2C%20Gone%20-%20My%20Thoughts%20on%20the%20Book/#we-are-the-racists","title":"We Are the Racists","text":"<p>As a kid, everyone is the same. There is no \u201cother\u201d. But as adults, we believe in our tribe to the point of excluding everyone else. The author experiences a lot of racism, from his classmates to teachers and even the police.</p> <p>When he was a little older, he realized everyone, including himself, was a little racist. Not in the traditional sense of the word - to distinguish between white and people of color but in the much broader sense. His family thought that hard-working people were a lot better than those who did not. People with a good education were better than those without, etc.</p> <p>Now that I think about it, we tend to categorize everyone as tribe and not tribe. This may be useful to some extent, but on a broader scale, it leads to alienation both within and without. </p> <p>We are the racists. All of us are. One way or the other. </p>"},{"location":"Articles/Others/Sigh%2C%20Gone%20-%20My%20Thoughts%20on%20the%20Book/#kindness","title":"Kindness","text":"<p>Amid all the struggle, there are always moments of joy and love. The author notes that without the help of families that supported them, there was no way they would have been able to survive. Some people help others unconditionally; perhaps they are why we still have faith in humanity. </p> <p>Neither I nor the author intend to say we should unquestioningly trust everyone. But being careful is different from excluding everyone different.</p>"},{"location":"Articles/Others/Sigh%2C%20Gone%20-%20My%20Thoughts%20on%20the%20Book/#great-expectations","title":"Great Expectations","text":"<p>Ask any children whose parents struggled to be where they were, and you will find great expectations. Perhaps this is only natural. Our parents faced a lot of hardship to give us the life that we have. In that, we must be grateful. That being said, a certain expectation trickles down to the generations that come after.  \u201cWe worked so hard to raise you. You must be the best.\u201d Perhaps this is our parents trying to live their dreams through us. Or maybe everything seems worth it to them when we are happy.</p> <p>The intent is filled with love and kindness, but the effects might not be. The outside world is not always kind to those who are different, but what if family makes it worse? Growing up, the author\u2019s family wanted him to be someone he was probably not\u2014good grades, showing the world that the family was okay and much more. </p> <p>Hard work does not always equate to a successful life. But trying to live the American Dream, makes it feel like that is the only way to get there. Exchange your life and dreams to be someone you are not.</p> <p>Sometimes, children need to be told that they are doing well. We can see that we must be more, but hearing that a hundred times does not make us want to do it.</p>"},{"location":"Articles/Others/Sigh%2C%20Gone%20-%20My%20Thoughts%20on%20the%20Book/#violence","title":"Violence","text":"<p>More often than not, violence is born from pain. A lot of pain. For immigrant families, the drastic change from home to elsewhere is a significant source of this pain. New cultures, hard work, loneliness, and fitting in all lead to a lot of frustration. Sometimes, this pain stays with people and only bursts out at points where they cannot hold it in anymore. The author experiences physical abuse from his parents, being beaten to the point he could not sit down, being forced to sit on uncooked rice, etc. </p> <p>One can say that these are manifestations of a large amount of pain that has not been processed. Therapy helps here, but many of our parents have never had the chance to have that. </p> <p>I think that, as children, it helps to know that these bouts of rage are not because of us. While that does not change anything, it does help us accept it a little more. Perhaps in the future, when we have the space to process these emotions, we can forgive our past and present.</p>"},{"location":"Articles/Others/Sigh%2C%20Gone%20-%20My%20Thoughts%20on%20the%20Book/#this-is-not-your-home","title":"This Is Not Your Home","text":"<p>The author and his brother faced many instances where their parents pretended to leave to discipline them. They left the author stranded on a road, packed their bags, and left them alone at home, etc.</p> <p>Some part of me wants to talk about my experiences here, but I am not ready to do that yet. </p> <p>In some ways, these experiences inspire discipline in children but also make them trust their families less. To some extent, they end up growing up faster than they should. When everyone around you already does not want you, if your parents show that they don\u2019t, it breaks you. It really does. </p>"},{"location":"Articles/Others/Sigh%2C%20Gone%20-%20My%20Thoughts%20on%20the%20Book/#books","title":"Books","text":"<p>Ah, books. I, for one, would never be where I am without them. Growing up, they were more than just stories. They were my mentors, friends, and confidants. </p> <p>Phuc talks about how we found a book called the Lifetime Reading Plan, a list of books that every civilized American must read before they die. He initially starts reading some of them as a means of fitting in and being more American, but he slowly starts to enjoy them. They show him a world beyond his own, ways of living and understanding.  He realizes that he can be a Punk but also a good student.</p> <p>To a large extent, this is also my own journey with books. I started reading to escape and also because I hoped it would help me understand my fellow humans a bit better. I was a misfit too and that will never change. But without books, I do not think I would have ever had the confidence to be myself, learn so much about the world and want to live in it.</p>"},{"location":"Articles/Others/Sigh%2C%20Gone%20-%20My%20Thoughts%20on%20the%20Book/#symbols","title":"Symbols","text":"<p>I did not realize the concept of symbols in this fashion before I read the chapter on it. Phuc talks about how sometimes we treat people as symbols. For instance, we might think of Vietnamese people as those affected by the war. The author bumps into many people who fought in the war and probably lost loved ones there. He realizes that they look at him and his family as symbols\u2014a concept, not a human.</p> <p>I think this is one way we, as humans, try to make sense of the world. We categorize everyone we see. Indians are the ones with butter chicken, and Middle Easterns are terrorists. Are they? NO. Every single person is different. Every single person has a long, complex history that ranges back generations before them. But perhaps thinking of this is not easy, and we try to define people as a singular concept. </p> <p>It is nice to be mindful of these thought patterns, I think. But it is a major shift in perspective too.</p>"},{"location":"Articles/Others/Sigh%2C%20Gone%20-%20My%20Thoughts%20on%20the%20Book/#black-and-white","title":"Black and White","text":"<p>One of the things I love about this book is that the author does not take a black-and-white approach when talking about his past. Hard boundaries do not define people. </p> <p>He talks about how his father had great expectations and showed them in not very calm ways, but also how he tried to let his kids enjoy life. He did take them to the movies. He did try to let them become who they were.</p> <p>This is the part that confused me the most growing up. Society tells us that people are either good or bad. But everyone is a mix of both. People can hurt us but also love us. As a child, these mixed signals might lead to some of us shutting everyone out. How can you trust anyone if they are not consistent? </p> <p>Our parents love us, but they have their demons too. Perhaps if we understand their pain, we can try to forgive them and soothe our own.</p>"},{"location":"Articles/Others/Sigh%2C%20Gone%20-%20My%20Thoughts%20on%20the%20Book/#disclaimer","title":"Disclaimer","text":"<p>I said many things in the blog, most of which are around acceptance. I think these views reflect what the author talks about, but I do not want to make the claim that they fully portray his views. These opinions are my own, and perhaps this is a way of showing my thoughts about the book without spoiling the story for those who decide to read it.</p> <p>I understand that acceptance seems like a scam for anyone facing many of these issues now. For now, it probably is. If you are being abused, please get help!  I do not intend to say that you should sit back and accept everything happening to you. Instead, this is for when you are out of the situation. Life moves on, and we must all make friends with our demons.</p>"},{"location":"Articles/Others/Sigh%2C%20Gone%20-%20My%20Thoughts%20on%20the%20Book/#resources-for-second-generation-immigrants","title":"Resources For Second Generation Immigrants","text":"<p>If you are like me, a second-generation immigrant, this book might resonate quite deeply with you. In my journey to learn more about these struggles and how they affect me, I came across a blog post that made a difference. If I am being honest, it made me bawl my eyes out.</p> <p>Check it out, maybe?</p> <p>Egg Shell Therapy</p>"},{"location":"Articles/Others/Sigh%2C%20Gone%20-%20My%20Thoughts%20on%20the%20Book/#fin","title":"Fin","text":"<p>This article is not a conclusive overview of the book or all the emotions I felt while reading it. But by putting it out there, I wanted to share a different perspective. </p> <p>I would love to talk some more about it, so please do reach out if that is something you want to do too!</p> <p>Like these/Want more? Buy me a coffee! Kofi</p> <p>Want articles on something specific? Just ask!</p> <p>You can always contact me on LinkedIn, or drop me an Email. For all the code, drop by my Github.</p>"},{"location":"Articles/Others/What%20I%20learnt%20from%20an%20AI%20Masters%20Part%201/","title":"What I learnt from an AI Masters Part 1","text":"<p>toc: true title: What I learnt from an AI Masters Part 1</p> <p>categories: [\"article\"] date modified:  date created: </p>"},{"location":"Articles/Others/What%20I%20learnt%20from%20an%20AI%20Masters%20Part%201/#what-i-learned-from-an-ai-master-degree-part-1-finding-a-project-and-supervisors-creating-the-proposal","title":"What I learned from an AI master degree - Part 1 (Finding a Project and Supervisors; Creating the Proposal)","text":"<p>Part 1 : Starting  Troubles - Finding a Project, Supervisors ; Creating a proposal (This article) Part 2 : Timelines  - Optimal Planning and Breakdowns  Part 3 : Setup and Tools - Setting up Writing, Programming, and research to prevent tears Part 4 : Writing and Programming Advice</p>"},{"location":"Articles/Others/What%20I%20learnt%20from%20an%20AI%20Masters%20Part%201/#introduction","title":"Introduction","text":"<p>So you decided to pursue a master degree? It must be scary to know that you have to write some obscure research thesis on a niche topic that your friends will give you weird looks about. Welcome to the club. I was pretty terrified too, but it all worked out in the end, and I now have a master in Artificial Intelligence from the University of Groningen, Netherlands. (Yay for me!)</p> <p>This is a series of articles where I share what I learned from the project and what I would do differently if I were to start again. This is no definitive guide, but I hope it helps you, my dear reader.</p> <p>This article is Part 1 : How to find a project and supervisors, plan your idea, and pitch it without going crazy. Depending on when you read this, the other parts will be out, and the links will be updated at the start of the article.</p> <p>Pick your adventure. I am proud of you for whatever you do.</p>"},{"location":"Articles/Others/What%20I%20learnt%20from%20an%20AI%20Masters%20Part%201/#finding-a-project","title":"Finding a Project","text":"<p>The first hurdle you will face is finding a project. Of course, there is no point in thinking about the rest of the steps if you don't know what exactly you want to do. So, how do you find a project? You have approximately three options.</p> <ol> <li>You can find a project on your own by finding a problem you want to solve or an idea you want to explore.</li> <li>You can find supervisors who have projects that you find interesting and ask them if you can work on them.</li> <li>You can find a company hiring interns and working on a project they have as the thesis.</li> </ol> <p>Each of them has their own advantages and drawbacks. For instance, if you are not particularly comfortable in the field you chose, it would be harder for you to judge what kind of projects are feasible for you. In that case, you might be better off finding an existing project. That way you will be able to learn faster and also have a supervisor that is already familiar with the field.</p> <p>On the other hand, if you are particularly interested in a field, you can find a research gap that you want to explore. In that case, you might be better off finding a supervisor who is interested in the same field and pitching your idea to them. If they think it is too ambitious, they can help you refine it. (This is what I did.)</p> <p>If you are more interested in the industry, you might be better off finding a company that is hiring interns. That might give you some work experience off the bat and might (depending on how you work and the connections you make) also help you get a job later on. Depending on where you are based, this might be a little harder to accomplish. For instance, if you are in a country where either the language or the culture is different, it might be harder for you to find a company that is willing to hire you. Or perhaps you might find one, but it might take you a lot longer than you expected. If you are a native of the country, that might not be an issue. But if you are an international student, perhaps it is nice to have a backup plan and contact a few supervisors as well.</p>"},{"location":"Articles/Others/What%20I%20learnt%20from%20an%20AI%20Masters%20Part%201/#but-what-ideas-do-they-want","title":"But What Ideas Do They Want?","text":"<p>The whole point of a thesis is to solidify your foundations and help you find a field you would perhaps enjoy contributing your time and energy to. So, it is not necessary that you have to find a project that is completely new.  Many universities don't impose much of a guideline except for stating that you should use at least some of the concepts you learned in your master.  There is no \"wrong\" project. There is only a matter of feasibility in the time that you have. You might have a great idea, but if you don't have the time to implement it (even partially), it might help to save it for later. Sometimes you may have to change your idea to make it more feasible. But that is okay. (This is what I had to do, as my initial idea was too ambitious.)</p> <p>It would be nice to find something that you are interested in. Perhaps talking to your seniors or faculties might help if you are not sure what you want to do. </p> <p>Okay, so how do you find a project? You have the following options. Pick your adventure.</p>"},{"location":"Articles/Others/What%20I%20learnt%20from%20an%20AI%20Masters%20Part%201/#on-your-own","title":"On Your Own","text":"<p>Okay, so you chose to find a project on your own. Yay! But how do you actually find something to work on? Well this depends on what kind of project you want to do, but I will list down a few things that might help you out. - Finding a Literature Gap : Simply put, this would be to dig through research in your field and find problems that they were not able to solve. Finding such a research gap does take time and effort, though. In some sense, this is also what the supervisors do, except they are more familiar with the field and might have an easier time in understanding the papers. (*Look at the \"Future work\" section of papers.) - Merge Projects : Suppose you find multiple projects that do similar/different things, and you want to see what happens if you combine them. That is a good start. (This is what I did.) - Continue Existing Research : Another way to find a project is to look at existing research and see if you have any other ideas that you can add to it. They might not solve the problem at hand, but even finding solutions that don't work is a good start and might lead to one that does. - Company Research Projects : Many large companies publish a lot of their projects as blog posts. For instance, one of my favorite ones is Google AI Blog. I have been following them on and off for many years now, and I have found many interesting projects that I would like to work on. This route may be helpful for you as well. - Seniors : Almost all universities have a thesis archive. You can look through them and see if you find something interesting. But of course, you can also ask your senior friends for inspiration. You may be able to continue their work or find supervisors they worked with.</p>"},{"location":"Articles/Others/What%20I%20learnt%20from%20an%20AI%20Masters%20Part%201/#find-a-supervisor","title":"Find a Supervisor","text":"<p>Okay, so you chose to find a supervisor. For many people, this step is one of the hardest things to do. (Depending, of course, on the institute you are at and how many faculties are available.) In my opinion, it would serve you to make some relationships with the faculties about as soon as you can. This could be as simple as trying to find out what kind of research each of your faculties does.  At the end of the day, they are here for you, and if you show them some interest, they might be able to guide you more than you think. </p> <p>In my case, I remember getting this piece of advice from a senior during my bachelor, and since then, I have put a little extra effort into getting to know some of my faculties. I have to admit this made it easier to find a supervisor. My mentor not only guided me throughout the thesis, he helped me find a second supervisor and also trusted me to do my own project. He taught one of my favorite courses, and I remember just asking him how I could learn a little more in it, and he invited me to his office for a much longer discussion. To be honest, I had no idea he would be my supervisor after a year. I was just interested in what he was teaching.</p>"},{"location":"Articles/Others/What%20I%20learnt%20from%20an%20AI%20Masters%20Part%201/#the-proposal","title":"The Proposal","text":"<p>Almost every university requires you to submit a proposal before you can start your thesis. This is a document that outlines what you want to do, how you plan to do it, and what you expect to achieve. It is also a document that your supervisor will use to judge whether or not your project is feasible and if they can guide you through it. Once you have the initial idea, creating a proposal will not take you that long, but what might take time is being able to phrase your idea in a way that is understandable and creating supporting diagrams, etc., that would make it easier for your supervisor to understand exactly what you want to do.</p> <p>Depending on your supervisor, getting an initial approval may be quick or may take a little bit of time. It would help to maintain communication throughout. You are probably not the only person under them, and they are genuinely busy people. While you wait, there is no harm in trying to flesh out the rest of your project. It might save you a little brainpower in the long run.</p>"},{"location":"Articles/Others/What%20I%20learnt%20from%20an%20AI%20Masters%20Part%201/#some-tips-for-the-proposal","title":"Some Tips for the Proposal","text":"<p>While there is no \"correct\" way to write a proposal, there are some things that might help you out. I will list down some of the things that I found useful. - Look At Examples : Simple, but perhaps not always done. You are not the first person to do a thesis. There is no need to reinvent the wheel. Look at examples of previous proposals (ask your supervisor, perhaps), and save your brainpower for later. Trust me, you will need it. - TMLIF : Aka, tell me like I am five. How would you explain your idea to a five-year-old? Start there and then make it to the standard of an adult. Assume that the person you will present it to only knows about the basics and builds up from there. - Literature : It would be nice to have a small initial literature survey to show that your idea has some support from the research community. This means finding some papers that do similar things or have concepts that you will use. Since you will need to do this properly later on, just finding a few relevant papers is enough here. - Timeline : In most cases, a timeline is requested. While this is personal and my approach will not work for you, I recommend that you take into account resting time. This was something I only encountered later on, and it would have helped to think of it before. A thesis is a long task. You will lose motivation many times. It is quite normal, but if you don\u2019t plan for it, you might end up feeling miserable for no reason at all.  - Template : Most universities have a proposal template or guidelines you can follow. While they are not always strict, it is a good place to start. At least you will know what is expected of you. - How Much Is Too Much? : If you are doing your own project, it is hard to know when to stop. Just remember that this is not your entire project. This is just a draft, an idea. As long as what you have conveys your idea and how you plan to tackle it properly, that is enough. Keep it simple and short, stupid. :)</p>"},{"location":"Articles/Others/What%20I%20learnt%20from%20an%20AI%20Masters%20Part%201/#an-example-my-thesis","title":"An Example: My Thesis","text":"<p>While my thesis is by no means a standard. If you are interested in Computer Vision, or want to see an example, feel free to look at the links below.  The project was about emulating Attention from Transformer networks in regular CNNs using gradient-based XAI techniques as a proxy - aka \u201cProxy Attention\u201d. (Yeah, I tried to make it sound fancy. Come on, just let me sound cool sometimes. ;p) You can find the paper here and the code along with all the writing here.</p>"},{"location":"Articles/Others/What%20I%20learnt%20from%20an%20AI%20Masters%20Part%201/#fin","title":"Fin","text":"<p>This article is in the hopes that it will help someone out. Maybe have the help that I did not. I do not know who it will reach. But to whoever it does, best of luck :)</p> <p>Like these/Want more? Buy me a coffee! Kofi</p> <p>Want articles on something specific? Just ask!</p> <p>You can always contact me on LinkedIn, or drop me an Email</p>"},{"location":"Articles/Productivity/CheckEmptySections/","title":"CheckEmptySections","text":"<p>toc: true title: CheckEmptySections</p> <p>categories: [\"article\"] date modified:  date created: </p>"},{"location":"Articles/Productivity/CheckEmptySections/#todo","title":"TODO","text":"<pre><code>import os\nfrom pathlib import Path\nimport argparse as ap\nimport concurrent.futures\n</code></pre> <pre><code>ags = ap.ArgumentParser()\nags.add_argument(\"-f\", help=\"File or folder name\")\nargs = ags.parse_args()\n\nmainpath = Path(args.f)\n</code></pre> <pre><code># Check if the input is a directory or a file\nif mainpath.is_dir() == True:\n    # If it's a directory, create a list of Path objects for all the files in the directory\n    all_files = [mainpath/Path(fl) for fl in os.listdir(mainpath)]\nelse:\n    # If it's a file, create a list with a single Path object for the file\n    all_files = [mainpath]\n</code></pre> <pre><code>def pipeline(txt_fn_list):\n    \"\"\"\n    Applies a series of functions to a text.\n\n    Parameters:\n        txt_fn_list: A list containing the text to be processed as the first element, and the functions to be applied as the remaining elements.\n\n    Returns:\n        The final result of applying all the functions to the text.\n    \"\"\"\n    # [txt, fn1, fn2....]\n    txt = txt_fn_list[0]\n    # Apply all fns one by one\n    for fn in txt_fn_list[1]:\n        txt = fn(txt)\n    return txt\n</code></pre> <pre><code>def read_md(path):\n    \"\"\"\n    Reads the contents of a Markdown file.\n\n    Parameters:\n        path: The path to the file.\n\n    Returns:\n        The contents of the file as a string.\n    \"\"\"\n    print(path)\n    try:\n        with open(path, 'r') as fin:\n            return fin.read()\n    except:\n        return None\n</code></pre> <pre><code>def check_blanks_hash(markdown):\n    \"\"\"\n    Finds headers in a Markdown text that have no content under them.\n\n    Parameters:\n        markdown: The Markdown text to be processed.\n\n    Returns:\n        A list of headers that have no content under them.\n    \"\"\"\n    if markdown != None:\n        lines = markdown.split('\\n')\n        headers = []\n        current_header = None\n        current_content = []\n        for line in lines:\n            if line.startswith('#'):\n                if current_header is not None:\n                    headers.append({'header': current_header, 'content': \" \".join(current_content)})\n                current_header = line\n                current_content = []\n            else:\n                current_content.append(line)\n        if current_header is not None:\n            headers.append({'header': current_header, 'content': \" \".join(current_content)})\n\n        # Return a list of headers that have no content under them\n        return [header[\"header\"] for header in headers if len(header[\"content\"]) &lt; 2]\n    else:\n        return None\n</code></pre> <pre><code>def main():\n    pipe_fns = [read_md, check_blanks_hash]\n    with concurrent.futures.ProcessPoolExecutor() as executor:\n        # Start a process for each file and store the returned result in a list\n        results = [executor.submit(pipeline, [file, pipe_fns]) for file in all_files]\n        for future in concurrent.futures.as_completed(results):\n            print(future.result())\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"Articles/Productivity/Extensions%20that%20make%20writing%20research%20papers%20easier/","title":"Extensions that make writing research papers easier","text":"<p>toc: true title: Extensions that make writing research papers easier</p> <p>categories: [\"article\"] date modified:  date created: </p>"},{"location":"Articles/Productivity/Extensions%20that%20make%20writing%20research%20papers%20easier/#11-free-chromefirefox-extensions-that-make-research-easier","title":"11 Free Chrome/Firefox Extensions That Make Research Easier.","text":"<p>While researching anything, we tend to heavily rely on our browser. To make this process more efficient, quite a few \u201cplugins\u201d have been created over the years. Every major browser has hundreds, if not thousands of such plugins/extensions/add-ons. Like any infinite list of things, it\u2019s often overwhelming to find helpful ones.</p> <p>For my work, I read a lot of articles and skim through many web pages, blogs, you name it. In the process, I need to store this information somewhere. Either to write a research paper, or a blog like this one, or just for my knowledge. The following plugins have made my life a lot easier and so I thought I would share them with you.</p> <p>If any of these plugins make no sense to you, ignore them. You probably don\u2019t need them as of now. But you might later on!</p> <p>Disclaimer - I am not sponsored by or affiliated with any of the programs I mention. They are shared purely because I find them useful. </p>"},{"location":"Articles/Productivity/Extensions%20that%20make%20writing%20research%20papers%20easier/#a-note-about-browsers","title":"A Note About Browsers","text":"<p>I know I only mention Chrome and Firefox. Anywhere I mention Chrome, you can safely assume that Microsoft Edge, Brave, Opera, and Vivaldi also work. The same links should work directly!</p>"},{"location":"Articles/Productivity/Extensions%20that%20make%20writing%20research%20papers%20easier/#collecting-information","title":"Collecting Information","text":"<p>I like taking notes, and my tool of choice here is the Markdown (another format like txt) notes app Obsidian. Of course, you can use anything you want. There are endless note applications, so everyone has their preference. These two plugins have been invaluable in collecting information across pages. - Roam Highlighter  This little plugin is absolutely beautiful. You can call it with a shortcut and highlight across the whole page! It strips out useless formatting and converts them to Markdown. If you use Roam/Obsidian/any other markdown editor, it even auto-converts the links and converts the highlighted text to markdown.      - Chrome , Firefox - MarkDownload Sometimes you want large amounts of text from a webpage you come across. This plugin gets all the text/links/images etc from the page you are on and converts it into a format you can easily copy and paste from. Extremely handy isn\u2019t it?     - Chrome , Firefox</p>"},{"location":"Articles/Productivity/Extensions%20that%20make%20writing%20research%20papers%20easier/#writing-research-papersarticles","title":"Writing Research Papers/Articles","text":"<p>A large chunk of my work involves writing technical articles and research papers. It\u2019s a lot of work, and I need shortcuts to help me out in the process. These plugin-ins have saved me a lot of headaches in the long run. - TamperMonkey: Bibtex copy This plugin is a bit of a special case. Instead of providing specific functionality, it enables users to write their scripts to modify any webpage in real-time. You can do anything from Getting direct links, endlessly loading Google search results to modifying Twitter.  While writing papers, citing them is a huge headache. Since I write my reports in LaTEX, I need the \u201cBibTex\u201d version from Google Scholar which is a pain. This little plugin automatically does that with a single click. You can find out how to install it here.     - Chrome , Firefox - SLext If you write your documents in LaTEX, chances are you use Overleaf to do so. It is a website that makes it extremely easy to write any kind of professional document using LaTEX. But it does have limitations, and the biggest bummer for me is the lack of tabs. This plugin adds just that, and it saves me so much trouble.     - Chrome , Firefox - Sci-Hub scholar I firmly believe that research should not be paywalled. (Even this article is available for free on my blog without the fancy stuff). If you are affiliated with a university or company, you might have access to as many papers as you want. But as an independent researcher? Well. Happy crying.  This plugin adds links to the website Sci-Hub directly in Google Scholar which lets you access a lot of paywalled research directly. (I would link it but I don\u2019t want to get demonetized).      - Chrome , Firefox - Unpaywall Similar to the previous one, this also lets you access paywalled articles by finding other un-paywalled versions of them from elsewhere on the web.     - Chrome , Firefox - Zotero/Mendeley Connector For anyone who reads a lot of research papers, managing them is probably the biggest headache. Mendeley and Zotero are probably the most popular library managers.  While browsing the web, you might want to directly save any research paper you like to your computer. This plugin lets you do that with a single click. Both the programs have this installed by default, you just need to enable them from the application itself - Zotero, Mendeley.</p>"},{"location":"Articles/Productivity/Extensions%20that%20make%20writing%20research%20papers%20easier/#medium","title":"Medium","text":"<p>I admit most of you probably don\u2019t write articles yourself. But if you do, I find these two plugins useful for extending what is possible here on Medium.  - Code medium Adding proper code snippets is honestly such a pain on Medium. Maybe in time, this will be fixed, but for now, this plugin lets you directly create gists on Github. It looks pretty and just works. I write technical articles and trust me, it has saved me hours.     - Chrome , Firefox - TOC Medium If you have noticed, not every article has a clickable table of contexts like this one. If you were to do it manually, you might need to look at the HTML source code, and do a lot of drama. Instead, this plugin just lets you add the TOC just as you would any other element. It does not auto-update though, so I would recommend leaving it for after you are done with your article.  It is also sadly not available for Firefox.     - Chrome , Firefox(Not there)</p>"},{"location":"Articles/Productivity/Extensions%20that%20make%20writing%20research%20papers%20easier/#honorable-mentions","title":"Honorable Mentions","text":"<p>Some of these are not exactly extensions, but I think they deserve to be mentioned anyway. - My Text Cleanup Script Although many of the applications I mentioned above let you copy the text in chunks, they don\u2019t offer any advanced formatting options. (Eg: cleaning up Wikipedia links, making lists, formatting paragraphs, etc.) Now, there are a million ways to do this. But my favorite is this script that I wrote a while back. It uses Python and lets you do whatever you want to the text in your clipboard. After processing is complete, it pastes it back to your clipboard.  This is not for everyone. But if you are interested, I\u2019ll be happy to explain how it works. Just ask!     - Github - Text Workflow This application is not free. (Sorry!) I have a Mac, and this app lets you do whatever my script above does but with a UI. If you want something free, use the previously mentioned script. I\u2019m sure there are other alternatives for Linux and Windows. I either use vim or my script. I have not used Windows in a long time and so I can\u2019t recommend anything.     - TextWorkflow  - Adblocker An adblocker is essential to save your sanity. (Although turning it off on websites you want to support is good!) These are my favorites.     - Chrome , Firefox - I don\u2019t care about cookies As the name suggests, this auto accepts only essential cookies from the prompt. It saves you that extra click and removes the banner that covers the entire bottom of web pages.      - Chrome , Firefox</p>"},{"location":"Articles/Productivity/Extensions%20that%20make%20writing%20research%20papers%20easier/#fin","title":"Fin","text":"<p>This article is in the hopes that it will help someone out. Maybe have the help that I did not. I do not know who it will reach. But to whoever it does, best of luck :)</p> <p>Like these/Want more? Buy me a coffee! Kofi</p> <p>Want articles on something specific? Just ask!</p> <p>You can always contact me on LinkedIn, or drop me an Email</p>"},{"location":"Articles/Productivity/Extracting%20Highlights%20from%20a%20PDF%20easily/","title":"Extracting Highlights from a PDF easily","text":"<p>toc: true title: Extracting Highlights from a PDF easily</p> <p>categories: [\"article\"] date modified:  date created: </p> <p>How to get PDF Highlights to Notes in 3 steps </p> <p>Reading a research paper/lecture slides/book? Made a lot of highlights in a PDF? Now want to get them into your notes as text? Don\u2019t want to have to type everything again? Here\u2019s your way out. Hey. Reading a research paper or a long document is itself a lot of effort. After that, I do not have the energy to again copy everything properly into my notes. I have been searching for a way to do that, but none of the options I found work for two-column layouts, or even maintain page order properly.</p> <p>(Disclaimer: I am not sponsored by, or hold any affiliations to any of the products mentioned below. I share them because I use them every day and think they would be useful to you.)</p>"},{"location":"Articles/Productivity/Extracting%20Highlights%20from%20a%20PDF%20easily/#what-we-want-to-do","title":"What we want to do","text":"<p>Convert highlights like in this document on the left to notes with minimal effort.  I use a note application called Obsidian, but anything you use works here. As long as it supports text. </p>"},{"location":"Articles/Productivity/Extracting%20Highlights%20from%20a%20PDF%20easily/#existing-programs-and-their-flaws","title":"Existing Programs and\u2026 their flaws","text":"<p>Many PDF viewers offer the ability to export notes into text. Like Skim for Mac, Adobe Acrobat, OneNote etc  But they have their fair share of issues - Most of them are paid - Two column layouts, like those in research papers are not handled very well - The export option removes all page information which makes it super hard to format it - They do not export comments or other annotations properly - They get very confused when the word or line spacing differs a little</p> <p>You get the point.</p>"},{"location":"Articles/Productivity/Extracting%20Highlights%20from%20a%20PDF%20easily/#pdfannots","title":"pdfannots","text":"<p>After a lot of searching, I found a Python script on GitHub called pdfannots. Sadly their documentation makes it very hard for someone without experience to get it running. Hence this article.</p>"},{"location":"Articles/Productivity/Extracting%20Highlights%20from%20a%20PDF%20easily/#installing-the-script","title":"Installing the script","text":"<p>There are not many steps involved here. But it might sound a little complicated. - Step 1: Install Python. If you are on Windows, select install \u2018pip\u2019 as well. If you are on Mac or Linux, you can skip this step as it already comes pre-installed with your system. - Step 2: Open a terminal (Hit the Windows button -&gt; Search/ Mac: Open Spotlight -&gt; Search/Linux: You probably know how to) - Step 3: Run (paste it and hit Enter) the following command</p> <pre><code>pip install pdfannots\n</code></pre> <p>(If you get an error, you probably did not install Python or pip. Try Step 1 again.) PS: Don\u2019t get annoyed at me for not counting these steps. This is a one-time install after all. Once it\u2019s on your laptop, you can just proceed with the ones below every other time.</p>"},{"location":"Articles/Productivity/Extracting%20Highlights%20from%20a%20PDF%20easily/#getting-your-highlights","title":"Getting your highlights!","text":"<ul> <li>Step 1: Once that completes, identify where the PDF with highlights is saved on your computer. You can open the location in your file manager. Note the location. Replace  with this As an example : (Windows: \u201cC:\\Users\\Subha\\Documents\\paper.pdf)\u201d,  Mac : \u201c/Users/eragon/Documents/paper.pdf\u201d) <li>Step 2: Now go back to the terminal, and run the following </li> <pre><code>pdfannots &lt;filename&gt;\n</code></pre> <ul> <li>Step 3: Voila! Now just select all that output text, copy it, and paste it wherever. Easy right?</li> <li>Step 4: This is optional of course, but you can format your text however you want to. (I will write an article on how you can do that and update this blog later.)</li> </ul>"},{"location":"Articles/Productivity/Extracting%20Highlights%20from%20a%20PDF%20easily/#specific-formatting","title":"Specific Formatting","text":"<p>Maybe your document did not get read properly. Here is a little FAQ. - Document not found: Check your file path properly. Maybe a little google search? Try dragging and dropping instead. - Two column research paper, use this instead: pdfannots \u2014cols=2  - Spaces all weird? : pdfannots \u2014word-margin=  - Weird technicalities? : Refer to the original page of the script. You can also ask the developers questions. - Want to automate common formatting, convert to markdown, etc? : (An article will come soon, but you can leave a comment in the meanwhile.)"},{"location":"Articles/Productivity/Extracting%20Highlights%20from%20a%20PDF%20easily/#fin","title":"Fin","text":"<p>This article is in the hopes that it will help someone out. Maybe have the help that I did not. I do not know who it will reach. But to whoever it does, best of luck :)</p> <p>Have any questions? Comment or shoot me an email. Like these/Want more? Buy me a coffee! Kofi Want articles on something specific? Just ask. You can contact me on LinkedIn, drop me an Email</p>"},{"location":"Articles/Productivity/Obisidian%20Daily%20Notes/","title":"Obisidian Daily Notes","text":"<p>toc: true title: Obisidian Daily Notes</p> <p>categories: [\"article\"] date modified:  date created: </p>"},{"location":"Articles/Productivity/Obisidian%20Daily%20Notes/#daily-notes-in-obsidian","title":"Daily Notes in Obsidian","text":"<p>Obsidian is one of my favorite productivity tools, but there are some things that I don't fully like about it. One of them that being the daily notes feature. I use Obsidian for journaling and getting an overview of my day while making notes and so being able to use this function efficiently would be very helpful to me. In this article, I talk about how I use daily notes in Obsidian, the tweaks I have made, the plugins I use, and the hotkeys I have set up. </p> <p>Note: None of the plugins I discuss are sponsored and I am sharing them only because I use almost them every day.</p>"},{"location":"Articles/Productivity/Obisidian%20Daily%20Notes/#a-disclaimer","title":"A Disclaimer","text":"<p>This article is about the way I like to work. If it does not work for you, take what you want from it and modify the workflow to fit your style. There are no \u201crules\u201d. I wanted to write this article as I did not find too many of them on using Daily notes more efficiently.</p>"},{"location":"Articles/Productivity/Obisidian%20Daily%20Notes/#why-use-daily-notes","title":"Why use Daily notes?","text":"<p>Daily notes help me track my daily activities in a structured way. This way, I can easily find what I did on a particular day and see if I'm progressing toward my goals. It helps me keep track of my time and allows me to see if I am spending too much or too little time on certain tasks, enabling me to make adjustments where needed.</p> <p>Furthermore, by keeping a record of my daily work, I can get a quick overview of my weekly work. This overview also helps me to find topics that I have recently worked on, which is useful when I want to continue working on them or refer to them later.  Having a place to temporarily store ideas before turning them into proper notes removes the pressure of having to write something perfect from the start. This way, I can record my thoughts without worrying about the format or accurately writing something, which reduces the chance of breaking my flow.</p> <p>An example of what a daily note for me might look like is as follows.</p>"},{"location":"Articles/Productivity/Obisidian%20Daily%20Notes/#my-issues-with-daily-notes","title":"My issues with Daily notes","text":"<p>One of the main areas for improvement of the daily notes feature in Obsidian is the need for an overview. The lack of one makes it difficult to quickly see all of the notes I have created on a specific date or within a certain range of dates. </p> <p>It can also be hard to navigate between different dates, making it time-consuming to find the note I am looking for. Another issue that I wanted to tackle was the ability to record time. The lack of this means I don't have any record of the exact time I created or edited something, making it challenging to track how much time I spent on a particular task or activity. Additionally, getting to the daily notes page requires too many clicks and can be frustrating, especially because I use this feature frequently and need to access my daily notes quickly. These shortcomings make it less convenient for me to use the daily notes feature, making my workflow less efficient.</p>"},{"location":"Articles/Productivity/Obisidian%20Daily%20Notes/#some-solutions","title":"Some solutions","text":"<p>Plugins are a great way to solve specific problems in my workflow. Some of the plugins that I have mentioned in this article, such as the Daily Notes Plugin and the Time Stamper Plugin, have greatly improved my experience with the daily notes feature in Obsidian. They provide an overview of all my daily notes, make it easy to navigate between them, and easily add timestamps to my notes. These plugins have helped me stay organized, find my notes quickly, and make the most of my time. In addition to using plugins, I also use shortcuts to make my workflow more efficient. I use shortcuts to navigate between pages quickly, create new notes and add timestamps without breaking my flow while working. These shortcuts help me focus on the task by reducing the time I would otherwise spend navigating through pages or menus.</p>"},{"location":"Articles/Productivity/Obisidian%20Daily%20Notes/#templater-plugin","title":"Templater Plugin","text":"<p>The Templater plugin is a useful tool that I use to streamline the process of creating new notes in Obsidian. It allows me to insert a predefined template with a shortcut every time I start a new note. This template contains the date and time I created the note and when it was last modified. The modified time is automatically updated as I make changes to the note. It also has tags and headers, which makes it easy for me to organize my notes and find the ones I need more easily. This plugin saves me time and effort by providing a consistent and standardized format for all my notes. It also provides me with important information about when the note was created and last modified, which is useful for me to track the progress of my work. This helps me to stay organized and make the most of my time by making it easy to find and refer to my notes.</p> <p>The template I use is as follows. It uses the Liquid syntax to insert entries automatically. Note that I have configured the plugin to ignore the folder where I save these templates to prevent Obsidian from auto-converting the template into text. </p> <pre><code>---\ntoc: true\ntitle: &lt;% tp.file.toc: true\ntitle %&gt;\ncategories: ['temp']\ndate modified: &lt;% tp.date.now(\"dddd Do MMMM YYYY, ddd\") %&gt;\ndate created: &lt;% tp.date.now(\"dddd Do MMMM YYYY, ddd\") %&gt;\n---\n# &lt;% tp.file.toc: true\ntitle %&gt;\n</code></pre>"},{"location":"Articles/Productivity/Obisidian%20Daily%20Notes/#daily-notes-plugin","title":"Daily Notes Plugin","text":"<p>The Daily Notes Plugin is a great solution to improve my daily note-taking experience. It provides a single-page overview of my daily notes, making it easy to navigate between them. Instead of hunting for something I wrote, I can scroll through the notes and find it quickly. Additionally, each date is editable directly, so I can skip opening multiple pages or searching for the note I want to edit. This makes it much more convenient and efficient for me to review, update or organize my daily notes. Another advantage of using this plugin is how simple it is to create new daily notes. Instead of navigating multiple pages or menus, I can click a button to create a new note for the day. This feature is incredibly useful for quickly capturing new ideas, thoughts, or information that comes up during the day and helps me to keep track of my daily activities and efficiently.</p> <p>The picture shows what this plugin looks like.</p>"},{"location":"Articles/Productivity/Obisidian%20Daily%20Notes/#time-stamper","title":"Time Stamper","text":"<p>The Time Stamper Plugin is another very useful plugin in my note-taking process. It enables me to use a custom time stamp format, which gives me the hour and minute time as a new bullet point. This plugin makes it easy for me to track time and is an efficient way to keep track of what I do during the day. Not having to enter the time manually saves me time and makes my note-taking process more streamlined. I use this plugin mostly when I start a new topic. This way, I can quickly see when I began working on it. This plugin does not replace manually tracking time spent on a note which can be useful when I want a rough idea of the duration of a topic or task. </p> <p>The template I use to generate my timestamps is.</p> <pre><code>- **hh:mm** \n</code></pre> <p>This picture shows what it looks like.</p>"},{"location":"Articles/Productivity/Obisidian%20Daily%20Notes/#useful-shortcuts","title":"Useful Shortcuts","text":"<p>I use some custom shortcuts to navigate between different pages in Obsidian quickly. These shortcuts save me time and effort by allowing me to jump between pages with a single keystroke. These shortcuts are essential to keep my work efficient and streamline my note-taking process.</p> <p>(Note that if you are on Windows or Linux, Command is replaced by Control and Option by the Alt key.)</p> <p>One shortcut I often use is Command+Shift+G, which takes me directly to the daily notes page. This shortcut allows me to quickly access my daily notes and start working on them without navigating through different menus or pages.</p> <p>Another shortcut that I use frequently is Command+Shift+T. This shortcut creates a timestamp quickly and helps me keep track of time when I start a new topic or task, making it easier to track the time spent on each task.</p> <p>For navigation between notes, I use Command+Option+Left Arrow to access the previous note quickly and Command+Option+Right Arrow to quickly access the next note. These shortcuts help me stay organized by allowing me to easily switch between notes without manually navigating through different pages.</p>"},{"location":"Articles/Productivity/Obsidian%20Plugins/","title":"Obsidian Plugins","text":"<p>toc: true title: Obsidian Plugins</p> <p>categories: [\"article\"] date modified:  date created: </p>"},{"location":"Articles/Productivity/Obsidian%20Plugins/#my-favourite-obsidian-plugins-for-research-notes-2-bonus-tips","title":"My Favourite Obsidian Plugins for Research Notes + 2 Bonus Tips","text":"<p>Obsidian is my favourite program for taking notes. Be it for research, general things I learn, summaries from papers, lecture notes and the like. Out of the box, it does so many things really well. But, its real power lies in the vast number of plugins it has. Most of these are user created, and you can even make your own (or hack one together)! In this sea of functionality, these are the top few that I use. Grouped by the type of task for easier lookup.</p> <p>(Disclaimer : I am not sponsored by either Obsidian or any of the authors of the plugins mentioned here. These are personal preferences.)</p>"},{"location":"Articles/Productivity/Obsidian%20Plugins/#the-use-case","title":"The use case","text":"<p>I am a student, researcher and programmer. I take lecture notes, read a lot of research papers, articles and books. These come down to a lot of information. Of course, there\u2019s no way I can remember all of these bits of fragmented information.  Therefore, drumroll\u2026, I use Obsidian to help me put these bits of information in a place I can easily access. Since I use this almost everyday, I want taking notes to be as painless and efficient as possible.  These plugins are a huge help in doing exactly that. (Ordered by the type of task)</p>"},{"location":"Articles/Productivity/Obsidian%20Plugins/#how-to-install-these-plugins","title":"How to install these plugins?","text":"<p>This is a simple step. Simple open Obsidian Settings, Scroll down a bit and select \u201cCommunity plugins\u201d. Disable Restrictive mode, and then browse to your hearts content!</p>"},{"location":"Articles/Productivity/Obsidian%20Plugins/#writing","title":"Writing","text":"<p>Writing notes is the major objective here. So how do we make it extra painless? Plugins of course! - Dynamic Table of Contents : Many times, I take notes for a long form text. Sometimes these notes end up pretty huge, and it becomes slightly harder to find something. What about adding a Table Of Contents to the start? Sounds great, but what if we update the note? In comes this plugin, with an automatically updating TOC. - Tags : Super simple, also built in. Adding \u201c#topic1, #topic2 etc\u201d to a file to make it easier to search and organise. - Frontmatter Tag Suggest : Tags are great, but who remembers which ones they used before? Nobody. This plugin autocompletes tags based on ones you have used in previous notes. You can create new ones the normal way of course.  - Note Refactor : Made a huge note with a lot of headings? Why not split them into individual topics and maintain links to them? This makes it easier for you to have one major idea per note. Here I have a bunch of test headings, you can see how after applying them they become new notes that link to the current file. - Paste URL into selection : The name says it all doesn\u2019t it? - Templater : Another plugin I use daily. I like starting my notes with \u201cdate_created\u201d, \u201cdate_modified\u201d, \u201ctags\u201d, \u201ctoc: true title\u201d and insert the file name as the header. Since I do this for every single note, why not automate it? This plugin lets you create blocks of dynamic text to be inserted with a keyboard shortcut. I use \u201cCmd+Shift+I\u201d (\u201cControl+Shift+I\u201d for Windows) - Typewriter Scroll : Zen Mode is a way of life. This lets me focus on what I am writing by automatically scrolling the page, and dimming the rest of the text apart from the line I am currently writing. I do disable it while reading though. - Command Palette : This one is pretty obvious, but this built in plugin is just a text search. You can quickly open files with a (Cmd/Control + O) that brings up a searchable menu, or use (Cmd/Control + P) to bring up a searchable list of quick actions. - Vim Mode : This little option is not for everyone honestly. If you have never heard of Vim, just skip this point. I use vim as my default text editor for everything else. And I can\u2019t live without its keybindings. This just lets me use the vim keys for everything.</p>"},{"location":"Articles/Productivity/Obsidian%20Plugins/#research","title":"Research","text":"<p>For research (AI research in my case), we have three main objectives :  - Merge important information from a large number of sources. - Find links between ideas that you did not see. - Maintain a daily log as something of a lab notebook. There are 5 plugins that fulfil these criteria pretty decently.  - Daily Notes : This is a Core plugin and comes with Obsidian. Essentially it\u2019s a journal. You can add whatever you want to it and it is created every day. I use it to keep a time stamped log of what I did that day. It is also useful if you just want to dump a bunch of information but don\u2019t want to format and organise it just yet. - TimeStamper : In my daily notes, I like having timestamps (eg - 9:30 : I did xyz). This plugin lets me set a custom format and a keyboard shortcut. I have set it to \u201cCmd+T\u201d (for Mac or Control+T for Windows) - Backlinks : A real game changer and another built in plugin. This shows you every file that either is linked in the current file, or refers to the current one. Identifying links between concepts, and finding more of them is absolutely invaluable in research. - Quick LaTEX for Obsidian : LaTEX is probably the easiest way of writing professional looking math-y stuff, be it equations or formulae or anything similar. This plugin has a lot of options for autocomplete, formatting, and makes my job almost ridiculously easy. Here\u2019s how it looks. (Just typing a random equation)</p>"},{"location":"Articles/Productivity/Obsidian%20Plugins/#organising","title":"Organising","text":"<p>What do you do once you have a lot of files, you organise them of course! Now Obsidian by default makes it pretty easy to do this. But these plugins make organising less of a chore and much more of a fun thing to do. - Local Images : To make my notes more informative, I sometimes paste images. Now many times these are links from some website, which makes it a little risky, because what if the website stops working? This plugin automatically downloads image links in your notes and saves them locally. (It also links to the correct downloaded file.) - Graph view : Oh the gift and curse of a pretty graph. I sometimes use this to navigate between my links either to or from a file. It also gives me a very useful overview of what I have. I generally use the \u201cLocal graph\u201d that shows me a graph for the current note, rather than the \u201cGlobal\u201d full one which shows me everything. (It\u2019s pretty, but unhelpful) - Linter : Maybe I have a bunch of empty lines, empty list items, my headers are not in sentence case, my text is not formatted, my paragraphs are weird. Or anything like that. I am lazy, so I use the Linter plugin to automatically perform a bunch of processing and clean up my files.  - Tag Wrangler : Have a lot of tags? View/Edit/Change them across every file that uses them in one place. Also useful for finding files that match a few criteria. - File Cleaner : Remove empty files, unreferenced images etc. Keeping your \u201cDigital Garden\u201d pruned and bug free. - Obsidian Link Converter : Because I host my Obsidian Vault on a personal website, sometimes the links that Obsidian uses don\u2019t work, this plugin lets me mass convert them to a format that does.</p>"},{"location":"Articles/Productivity/Obsidian%20Plugins/#bonus-tips","title":"Bonus tips!!","text":"<ul> <li>Make sure every file has a single major idea. If you have too many, use the \u201cNote Refactor\u201d to put them in their own files. This will make it extremely easy to refer to the \u201cIdeas\u201d in the text somewhere else instead of linking to the whole text. </li> <li>Want pages that consolidate all the notes that have a particular tag together and save them automatically to a single file? Say you want a file that has links to all the notes that have the tag \u201c#apple\u201d. Here is a little script that I wrote which does just that.</li> </ul>"},{"location":"Articles/Productivity/Obsidian%20Plugins/#fin","title":"Fin","text":"<p>This article is in the hopes that it will help someone out. Maybe have the help that I did not. I do not know who it will reach. But to whoever it does, best of luck :)</p> <p>Like these/Want more? Buy me a coffee! Kofi</p> <p>Want articles on something specific? Just ask!</p> <p>You can always contact me on LinkedIn, or drop me an Email</p>"},{"location":"Articles/Productivity/Pomodoro%20-%20A%20means%20but%20not%20the%20end/","title":"Pomodoro   A means but not the end","text":"<p>toc: true title: Pomodoro - A means but not the end</p> <p>categories: [\"article\"] date modified:  date created: </p>"},{"location":"Articles/Productivity/Pomodoro%20-%20A%20means%20but%20not%20the%20end/#pomodoro-a-means-but-not-the-end","title":"Pomodoro : A Means but not the End","text":"<p>In our quest for productivity, many ideas, tools, and theories have sprung up over the ages. Some work, some don\u2019t, and some are versions of previous ones with a \u201cmodern\u201d twist. The concept of the Pomodoro timer seems to have stood the test of time. The hustle culture especially has glorified its existence and made it out to be something of a magic pill that either works for you or does absolutely nothing.  I think it is a very valuable tool. But in time, we have stopped thinking of it as a tool and made it out to be more of a rule.  So how do we make it work for us?</p> <p>(Disclaimer - I am no self-help productivity guru. This works for me, and I share it in the hopes that it helps someone else. Don\u2019t take my words as a golden rule.)</p>"},{"location":"Articles/Productivity/Pomodoro%20-%20A%20means%20but%20not%20the%20end/#a-definition","title":"A definition","text":"<p>Before we move further it is my habit to define the terms we work with. A Pomodoro timer, in its essence, is a combination of time blocking and scheduling rest periods. It has \u201ccycles\u201d that are combinations of a 25-minute deep work session followed by a 5-minute break. After four such cycles, a longer break of 45 minutes can be taken. In theory, it does sound pretty good. So why is it that it doesn\u2019t work for so many of us?</p>"},{"location":"Articles/Productivity/Pomodoro%20-%20A%20means%20but%20not%20the%20end/#why-does-it-fail","title":"Why does it fail","text":"<p>I think there are a few simple reasons why it might not work for you. Which one(s) is(are) it for you? - You try sticking to the 25-5-25-5.. timing, but it just doesn\u2019t work. - You start one, but something comes up in between and then it just doesn\u2019t work. - You think it should give you hours of focus, and when it doesn\u2019t, you decide to just use something else instead. - The breaks are not timed right. - You end up constantly looking at the timer and it is way too distracting.</p> <p>Sounds like you? Welcome to the club. Something else? Let me know in the comments. Either way, let\u2019s make it work for us.</p>"},{"location":"Articles/Productivity/Pomodoro%20-%20A%20means%20but%20not%20the%20end/#redefining-our-objective","title":"Redefining our objective","text":"<p>I believe that the common theme we face with using the Pomodoro, or honestly any other productivity technique is that we put too many expectations on it. At the end of the day, it\u2019s just a tool. The ability to use a tool depends on who wields it.  Our objective is not to work for long long hours. It is to focus, and get into an undistracted state where you are deeply concentrated on getting whatever you have to do, done. A \u201cflow state\u201d if you want to use a trendy term.  Pomodoro can help there. But only if you let it.</p>"},{"location":"Articles/Productivity/Pomodoro%20-%20A%20means%20but%20not%20the%20end/#how-i-use-it","title":"How I use it","text":"<p>Indeed, that was a pretty long-winded introduction to something rather simple. I use it as a means to get started. I don\u2019t think I have ever completed a \u201ccycle\u201d recently. But almost every time, it does help me set the tone for my work session of the day. As of writing this article, I had a Pomodoro of 30 minutes going. After that initial time, I think I spent about an hour more. I didn\u2019t check on the timer then. I just sat down and finished my writing for the morning.</p> <p>But, as it goes, this is not perfect. In time I have noticed a few factors that make or break my session. - Planning. If I don\u2019t plan out my work for the day, and just get started, nothing I do works. I\u2019m still too distracted even with the timer on. I like to block off times for work on a calendar and try my best to stick to them.  - Changing the timings. A 30 - 5 - 30 \u2026 timing works for me when it comes to longer running tasks. Sometimes, a task I pick is more challenging and I end up adjusting the breaks to give me some more rest. - Breaks are subjective. There is no way for Pomodoro to know what you are working on. If the break comes when you are way too focused and working hard, skip it. Take it later though. Add them up. - Finding your productive times. To make use of the timer, try to find the general times in a day that you have more mental clarity and headspace. For me, it\u2019s earlier in the day. And maybe a few hours in the evening. I do have to say that, these times aren\u2019t fixed in stone. Pick what works for you. - Start with a completed task. I find the satisfaction of finishing something very motivating. I have noticed that if the first task you do in your work session is something you know for sure that you can complete, it does set the tone for the rest of the time. Try ordering your work that way. - Play around with the timings. With some experimentation, you will find one that works for you. - Don\u2019t stick with it. Ironic, I know. Use it to start and then do what you need to.</p>"},{"location":"Articles/Productivity/Pomodoro%20-%20A%20means%20but%20not%20the%20end/#fin","title":"Fin","text":"<p>This article is in the hopes that it will help someone out. Maybe have the help that I did not. I do not know who it will reach. But to whoever it does, best of luck :)</p> <p>Like these/Want more? Buy me a coffee! Kofi</p> <p>Want articles on something specific? Just ask!</p> <p>You can always contact me on LinkedIn, or drop me an Email</p>"},{"location":"Articles/Productivity/Taking%20notes%20from%20websites%20to%20markdown%20-%20A%20workflow/","title":"Taking notes from websites to markdown   A workflow","text":"<p>toc: true title: Taking notes from websites to markdown - A workflow</p> <p>categories: [\"article\"] date modified:  date created: </p> <p>Taking notes from websites to markdown - A workflow #inprogress #article</p>"},{"location":"Articles/Productivity/Taking%20notes%20from%20websites%20to%20markdown%20-%20A%20workflow/#why","title":"Why","text":""},{"location":"Articles/Productivity/Taking%20notes%20from%20websites%20to%20markdown%20-%20A%20workflow/#what-we-want","title":"What we want","text":""},{"location":"Articles/Productivity/Taking%20notes%20from%20websites%20to%20markdown%20-%20A%20workflow/#obsidian-quick-intro","title":"Obsidian Quick intro","text":""},{"location":"Articles/Productivity/Taking%20notes%20from%20websites%20to%20markdown%20-%20A%20workflow/#why-markdown","title":"Why markdown","text":""},{"location":"Articles/Productivity/Taking%20notes%20from%20websites%20to%20markdown%20-%20A%20workflow/#chrome-extension","title":"Chrome extension","text":"<p>Extension Roam Highlighter</p>"},{"location":"Articles/Productivity/Taking%20notes%20from%20websites%20to%20markdown%20-%20A%20workflow/#script","title":"Script","text":"<p>```py, webtomd_imports, webtomd_imports import clipboard import os from pathlib import path import re</p> <p>def apply_transforms(txt, list_of_trans):     for trans in list_of_trans:         txt = trans(txt)     return txt</p> <p>def replacer(txt, dict_of_replace):     for rep in dict_of_replace.keys():         txt = txt.replace(rep, dict_of_replace[rep])     return txt</p> <p>def regexreplacer(txt, dict_of_replace):     for rep in dict_of_replace.keys():         txt = re.sub(rep, dict_of_replace[rep], txt)     return txt</p> <p>def indent_transform(txt):     l = txt.split(\"\\n\")     return \"\\n\".join([x.strip() for x in l])</p> <p>def paragraph_converter(txt):     l = txt.split(\"\\n\")     for item in range(len(l)):         if len(l[item])&gt;0 and l[item][0] not in [\"#\",\" \",]:             l[item] = \"- \"+l[item]     return \"\\n\".join(l)</p> <pre><code>\n## Practically Using it\n</code></pre> <p>dict_regex_replace = {     r'[.*?]':\"\", #wiki links }</p> <p>dict_of_replace = {     #\"\": \"\",     \"- \": \"# \",     \"\": \"\",     \"***\": \"\",     \"[latex]\": \"\\(\",     \"[/latex]\": \"\\)\",         \"(&lt;https://en.wikipedia.org/w/index.php?toc: true title=\":\" \",     \"(https://en.wikipedia.org/w/index.php?toc: true title=\":\" \",     \"s&amp;action=edit&amp;redlink=1\":\" \", }</p> <pre><code>\n</code></pre> <p>text = clipboard.paste() text = replacer(text, dict_of_replace) text = regexreplacer(text, dict_regex_replace) text = apply_transforms(text, [     indent_transform,     paragraph_converter, ]) clipboard.copy(text) ```</p>"},{"location":"Articles/Productivity/Taking%20notes%20from%20websites%20to%20markdown%20-%20A%20workflow/#faq","title":"FAQ","text":""},{"location":"Articles/Productivity/Taking%20notes%20from%20websites%20to%20markdown%20-%20A%20workflow/#why-not-a-program","title":"Why not a program?","text":""},{"location":"Articles/Productivity/Taking%20notes%20from%20websites%20to%20markdown%20-%20A%20workflow/#fin","title":"Fin","text":"<p>This article is in the hopes that it will help someone out. Maybe have the help that I did not. I do not know who it will reach. But to whoever it does, best of luck :)</p> <p>Like these/Want more? Buy me a coffee! Kofi Want articles on something specific? Just ask. You can contact me on LinkedIn, drop me an Email</p>"},{"location":"Articles/Scalar/","title":"Scalar Academy Articles","text":""},{"location":"Articles/Scalar/#scalar-academy-articles","title":"Scalar Academy Articles","text":"<ul> <li>This section has some of the articles I wrote on commission for Scalar Academy.</li> <li>These are unpublished as of now but can be eventually found here.</li> </ul>"},{"location":"Articles/Scalar/Autoencoders%20with%20Convolutions/","title":"Autoencoders with Convolutions","text":"<p>toc: true title: Autoencoders with Convolutions</p> <p>categories: [\"article\"] date modified:  date created: </p>"},{"location":"Articles/Scalar/Autoencoders%20with%20Convolutions/#autoencoders-with-convolutions","title":"Autoencoders with Convolutions","text":"<p>:::section{.abstract}</p>"},{"location":"Articles/Scalar/Autoencoders%20with%20Convolutions/#overview","title":"Overview","text":"<p>In the absence of labels in a dataset, only a few models can perform well. The Convolutional Autoencoder is a model that can be used to re-create images from a dataset, creating an unsupervised classifier and an image generator in the process. This model uses an Encoder-Bottleneck-Decoder architecture to understand the latent space of the dataset and re-create the images. ::: :::section{.scope}</p>"},{"location":"Articles/Scalar/Autoencoders%20with%20Convolutions/#scope","title":"Scope","text":"<ul> <li>This article explains the principle behind the Convolutional Autoencoder.</li> <li>It explores the architecture of the Autoencoder.</li> <li>It presents a template we can use to implement the Autoencoder in Tensorflow.</li> <li>The article also covers the loss functions and optimizers required to train a Convolutional Autoencoder.</li> </ul> <p>::: :::section{.main}</p>"},{"location":"Articles/Scalar/Autoencoders%20with%20Convolutions/#introduction","title":"Introduction","text":"<p>To be able to learn how to re-create a dataset, a model must have an understanding of the underlying latent space. The Convolutional Autoencoder compresses the information in an image dataset by applying successive convolutions. This output is passed to a Bottleneck layer, the smallest possible representation of the dataset. Using this compressed representation, the Decoder attempts to recreate the original dataset. In the process of re-creation, the compressed output resembles a sort of Dimensionality Reduction.md|../../Dimensionality Reduction|Dimensionality Reduction procedure, while the Reconstruction Loss can be used as a classification metric. This article explores the architecture and methods behind creating a Convolutional Autoencoder. </p>"},{"location":"Articles/Scalar/Autoencoders%20with%20Convolutions/#what-is-an-autoencoder","title":"What is an Autoencoder?","text":"<p>The convolutional Autoencoder is part of a family of models that can reduce the noise in data. In the noise reduction task, these models learn the latent space by reconstructing the data. Autoencoders are split into three main parts - the Encoder, the Bottleneck, and the Decoder. The Encoder compresses the input data while keeping the useful features. The Bottleneck is responsible for choosing the important features that can flow through it to the Decoder. Finally, the Decoder uses the features passed to it by the Bottleneck layer to reconstruct the input.</p> <p>The architecture diagram of a convolutional autoencoder is shown below. [IMAGE {1} { arch } START SAMPLE]  [IMAGE {1} FINISH SAMPLE]</p>"},{"location":"Articles/Scalar/Autoencoders%20with%20Convolutions/#encoder-structure","title":"Encoder Structure","text":"<p>The Encoder part of the network is to compress the input data and passes it to the Bottleneck layer. The compression creates a knowledge representation much smaller than the original input but has most of its features. This part of the network comprises blocks of convolutions followed by pooling layers that, in turn, further help to create a compressed data representation. The output of an ideal Encoder should be the same as the input but with a smaller size. The Encoder should be sensitive to the inputs to recreate it and not over-sensitive. Being over-sensitive would make the model memorize the inputs perfectly and then overfit the data. </p>"},{"location":"Articles/Scalar/Autoencoders%20with%20Convolutions/#bottleneck-layer","title":"Bottleneck layer","text":"<p>The Bottleneck is the most important layer of an Autoencoder. This module stores the compressed knowledge that is passed to the Decoder. The Bottleneck restricts information flow by only allowing important parts of the compressed representation to pass through to the Decoder. Doing so ensures that the input data has the maximum possible information extracted from it and the most useful correlations found. This part of the architecture is also a measure against overfitting as it prevents the network from memorizing the input data directly.  Note that smaller bottlenecks lead to lesser overfitting (to an extent).</p>"},{"location":"Articles/Scalar/Autoencoders%20with%20Convolutions/#decoder-structure","title":"Decoder Structure","text":"<p>This part of the network is a \"Decompressor\" that attempts to recreate an image given its latent attributes. The Decoder gets the compressed information from the Bottleneck layer and then uses upsampling and convolutions to reconstruct it. The output generated by the Decoder is compared with the ground truth to quantify the network's performance.</p>"},{"location":"Articles/Scalar/Autoencoders%20with%20Convolutions/#latent-space-structure","title":"Latent Space Structure","text":"<p>The latent space of a network is the compressed representation it creates from an input dataset. This latent space usually has hundreds of dimensions and is hard to visualize directly. More complex neural networks have latent spaces so hard to visualize that they are generally referred to as black boxes. In a convolutional autoencoder, the better the representation of the data, the richer the latent space. The space structure here is a large matrix of tensors that encode the weights of layers of the network. </p>"},{"location":"Articles/Scalar/Autoencoders%20with%20Convolutions/#uses-of-autoencoder","title":"Uses of Autoencoder","text":"<p>The Convolutional Autoencoder architecture is good for a lot of use cases. Some of these are explained below.</p>"},{"location":"Articles/Scalar/Autoencoders%20with%20Convolutions/#reducing-complexity","title":"Reducing Complexity","text":"<p>The Encoder of the model works very well as a dimensionality reduction.md|../../Dimensionality Reduction|dimensionality reduction technique. For example, if we consider an image dataset, we can compress every image before feeding it to another model. This compression reduces the number of input values and thus makes the model less likely to be biased toward smaller details. The Autoencoder thus helps in improving the performance of a second model.</p>"},{"location":"Articles/Scalar/Autoencoders%20with%20Convolutions/#anomaly-detection","title":"Anomaly Detection","text":"<p>An Autoencoder is generally used for reconstructing the base data using an Encoder-Bottleneck-Decoder architecture. Thus if the output reconstruction has a much larger error for a given sample, this sample could be an outlier. We can thus use the reconstruction error to find unusual data points in a dataset.</p>"},{"location":"Articles/Scalar/Autoencoders%20with%20Convolutions/#unsupervised-learning","title":"Unsupervised Learning","text":"<p>A convolutional Autoencoder's information compression ability is a good start for an unsupervised learning problem. Using the Autoencoder as a dimensionality reduction.md|../../Dimensionality Reduction|dimensionality reduction technique allows the data to be clustered without any labels much more easily. This clustering may not be useful but can be a starting point for many other solutions.</p>"},{"location":"Articles/Scalar/Autoencoders%20with%20Convolutions/#what-is-a-cnn","title":"What is a CNN?","text":"<p>A CNN is one of the foundation stones of Deep Learning that can take an input data sample and learn to recognize it given other such samples. A CNN generally comprises an input, multiple hidden layers, and an output. CNN's are composed of individual neurons that respond to specific sets of stimuli and combine their responses to perform many tasks such as classification, clustering, segmentation, and others. CNNs are useful because they can understand spatial and temporal dependencies given large amounts of data by learning feature \"filters\". Complex CNNs can model large amounts of data previously impossible for a computer to understand. </p> <p>A simple CNN is shown below. [IMAGE {2} { CNN } START SAMPLE]  [IMAGE {2} FINISH SAMPLE]</p> <p>This article elaborates on a CNN-based architecture called the convolutional Autoencoder.</p>"},{"location":"Articles/Scalar/Autoencoders%20with%20Convolutions/#implementation-of-an-autoencoder","title":"Implementation of an Autoencoder","text":"<p>The Convolutional Autoencoder has a few hyper-parameters that must be tweaked before training. The section below discusses these hyper-parameters, the Reconstruction loss used, and the general implementation of the architecture of a simple convolutional Autoencoder.</p> <p>An example use case would be to re-create the MNIST dataset. The following image shows the input and output of an Autoencoder that was trained on such a task. The first row is the original input and the second row is the output of the Autoencoder. In this case, a perfect reconstruction is obtained. [IMAGE {3} { Example } START SAMPLE]  [IMAGE {3} FINISH SAMPLE]</p> <p>A complete demonstration of the code to re-create the MNIST dataset can be found on this link.</p>"},{"location":"Articles/Scalar/Autoencoders%20with%20Convolutions/#code-size","title":"Code size","text":"<p>The code size is defined as how large the bottleneck layer is, consequently deciding to what extent the input data is compressed before being Decoded. The code size is the most important hyper-parameter to tune.</p>"},{"location":"Articles/Scalar/Autoencoders%20with%20Convolutions/#number-of-layers","title":"Number of Layers","text":"<p>The convolutional Autoencoder is similar to other types of networks in that the number of layers is a hyper-parameter. Note that increasing the number of layers increases the time to train a network. A higher model depth also increases inference time.</p>"},{"location":"Articles/Scalar/Autoencoders%20with%20Convolutions/#number-of-nodes-per-layer","title":"Number of Nodes per Layer","text":"<p>This hyper-parameter controls the weights per layer. As a general rule, the number of nodes decreases in the Encoder and increases in the Decoder.</p>"},{"location":"Articles/Scalar/Autoencoders%20with%20Convolutions/#reconstruction-loss","title":"Reconstruction loss","text":"<p>The loss function the convolutional Autoencoder is trained to minimize is called the reconstruction error. This loss function depends on the type of data we use. In the case of image-based data, the reconstruction loss can be either an MSE, an L1, or Binary Cross Entropy loss.</p>"},{"location":"Articles/Scalar/Autoencoders%20with%20Convolutions/#encoder","title":"Encoder","text":"<p>The Encoder can be created with a stack of 2D Convolutions starting with a large size and slowly reducing in dimensions. Each of the convolutions is followed by a 2D Max-Pooling layer. The ReLU activation is used throughout.  In Keras, the Encoder can look something like this.</p> <pre><code>x = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\nx = layers.MaxPooling2D((2, 2), padding='same')(x)\nx = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x)\nx = layers.MaxPooling2D((2, 2), padding='same')(x)\nx = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x)\nencoded = layers.MaxPooling2D((2, 2), padding='same')(x)\n</code></pre>"},{"location":"Articles/Scalar/Autoencoders%20with%20Convolutions/#decoder","title":"Decoder","text":"<p>The Decoder comprises blocks of 2D convolutions followed by Up Sampling layers. This part of the network looks something like the following in Keras.</p> <pre><code>x = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\nx = layers.UpSampling2D((2, 2))(x)\nx = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x)\nx = layers.UpSampling2D((2, 2))(x)\nx = layers.Conv2D(16, (3, 3), activation='relu')(x)\nx = layers.UpSampling2D((2, 2))(x)\ndecoded = layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n</code></pre> <p>::: :::section{.summary}</p>"},{"location":"Articles/Scalar/Autoencoders%20with%20Convolutions/#conclusion","title":"Conclusion","text":"<ul> <li>Convolutional Autoencoders are a powerful unsupervised learning technique.</li> <li>The article explained the Encoder-Bottleneck-Decoder architecture in detail.</li> <li>The Reconstruction Loss obtained is a valuable classification and image generation resource.</li> <li>The article also explained multiple use cases such as Anomaly Detection, Complexity Reduction, and some others.</li> <li>This article also provided a template for implementing a Convolutional Autoencoder in Tensorflow. :::</li> </ul>"},{"location":"Articles/Scalar/Building%20a%20GAN%20from%20scratch/","title":"Building a GAN from scratch","text":"<p>toc: true title: Building a GAN from scratch</p> <p>categories: [\"article\"] date modified:  date created: </p>"},{"location":"Articles/Scalar/Building%20a%20GAN%20from%20scratch/#building-a-gan-from-scratch","title":"Building a GAN from scratch","text":"<p>:::section{.abstract}</p>"},{"location":"Articles/Scalar/Building%20a%20GAN%20from%20scratch/#overview","title":"Overview","text":"<p>Generating images from scratch is a huge deal in computer vision. A Generative Adversarial Network(GAN) was one of the first models to generate new images in an unsupervised manner efficiently. A GAN is not a single model but a family of different architectures used for image generation.</p> <p>This article will look at the first Generative Adversarial Network, a vanilla GAN. We will learn how to make a Generative Adversarial Network from scratch. ::: :::section{.scope}</p>"},{"location":"Articles/Scalar/Building%20a%20GAN%20from%20scratch/#scope","title":"Scope","text":"<p>This article covers the following topics: - What is a Generative Adversarial Network, and how to make a Generative Adversarial Network from scratch? - What is the architecture of a GAN, and what are the loss functions and optimizers required to train one? - How to feed a custom dataset to a GAN and use it to generate novel images.</p> <p>::: :::section{.main}</p>"},{"location":"Articles/Scalar/Building%20a%20GAN%20from%20scratch/#introduction","title":"Introduction","text":"<p>A GAN is a network we can use to create novel images given any vision dataset. In most cases, they are unsupervised, but many architectures also consider labels during training. Some examples of outputs GANs are shown here.</p> <p>[IMAGE {1} Summer to Winter START SAMPLE]  [IMAGE {1} FINISH SAMPLE]</p> <p>[IMAGE {2} Face Generation START SAMPLE]  [IMAGE {2} FINISH SAMPLE]</p> <p>GANs have much bigger and more complex architectural pipelines than a standard Convolutional network. They generally have two major structures, the Generator and the Discriminator. These structures are Convolutional networks that we can substitute for other networks that perform similar functions. </p> <p>The training paradigm for GANs is called Adversarial Training and relies on an interplay between the Generator and the Discriminator. </p> <p>This article will look at what a Generative Adversarial Network is and its components. After we understand the parts, we will build our own GAN from scratch and train it on a dataset of handwritten images (MNIST). </p>"},{"location":"Articles/Scalar/Building%20a%20GAN%20from%20scratch/#architecture-of-a-gan","title":"Architecture of a GAN","text":"<p>One of the hardest parts of understanding how to make a Generative Adversarial Network is comprehending the architecture. GANs are very different from regular neural networks in that they are composed of two completely different neural networks - The Generator and the Discriminator.</p> <p>Consider the architecture diagram shown below. [IMAGE {3} Architecture Diagram START SAMPLE]  [IMAGE {3} FINISH SAMPLE]</p> <p>The first part of the architecture is the Generator, whose job is to create images realistic enough that the Discriminator cannot tell the difference between a fake image and a real one.</p> <p>[IMAGE {4} Generator And Discriminator START SAMPLE]  [IMAGE {4} FINISH SAMPLE]</p>"},{"location":"Articles/Scalar/Building%20a%20GAN%20from%20scratch/#generator","title":"Generator","text":"<p>The Generator can be considered a network that takes a random noise and then arranges the pixels to make it look like a real image. It is also a simple neural network composed of blocks of fully connected linear layers (FC) and Leaky ReLU activations. In the final layer of the Generator, the LeakyReLU is replaced by a Tanh activation. The Tanh activation is chosen as we do not want probabilities but want to take the generated image and squish it to the range of (-1,1). This range is the range of the MNIST data images.</p>"},{"location":"Articles/Scalar/Building%20a%20GAN%20from%20scratch/#discriminator","title":"Discriminator","text":"<p>The second part of the network is the Discriminator, whose job is to take the images that the Generator creates and return the probability that the image is real.  The Discriminator is a binary classifier and comprises blocks of fully connected linear layers (FC), Leaky ReLU activations, and Dropout.md|../../Dropout|Dropout layers. The final layer of the Discriminator is a block with an FC layer and a Sigmoid at the end. The Sigmoid is responsible for returning the classification probability that we want.</p>"},{"location":"Articles/Scalar/Building%20a%20GAN%20from%20scratch/#demystifying-the-loss-function","title":"Demystifying the loss function","text":"<p>Loss functions are an essential part of any neural network pipeline. Before we learn how to make a Generative Adversarial Network, we first need to understand the loss functions.</p>"},{"location":"Articles/Scalar/Building%20a%20GAN%20from%20scratch/#discriminator-loss","title":"Discriminator loss","text":"<p>The Discriminator's job is to classify the generated images into real or fake and return the probability that it is real. To do so, it needs to do extremely well at ensuring that the input it gets belongs to the real dataset. It should also ensure that if the input is fake, it is not classified as belonging to the real dataset.  Mathematically, this can be understood as maximizing \\(D(x)\\) and minimizing \\(D(G(z))\\).</p>"},{"location":"Articles/Scalar/Building%20a%20GAN%20from%20scratch/#generator-loss","title":"Generator loss","text":"<p>The Generator is tasked with ensuring that the Discriminator is fooled. It can do so by creating realistic images that the Discriminator thinks are real. This process can be thought of as ensuring that the Discriminator classifies an image sampled from the fake dataset as belonging to the real one.  Mathematically, this is formulated as maximizing \\(D(G(z))\\). Using this as the loss might lead to the network becoming extremely confident, even if it is wrong. To prevent this from happening, \\(log(D(G(z)))\\) is used instead. </p>"},{"location":"Articles/Scalar/Building%20a%20GAN%20from%20scratch/#total-loss","title":"Total loss","text":"<p>There is no net loss that is used in practice. Still, while learning to make a Generative Adversarial Network, we must consider the total theoretical loss the network is trying to optimize. Training a Generative Adversarial Network is a game between two enemies (aka adversaries). In other words, this is a MinMax game where one party attempts to reduce the probability of the other winning. Both parties are simultaneously also trying to increase their chances of winning. Mathematically, this can be represented as \\(\\(\\underset{G}{min} \\underset{D}{max} V(D,G) = \\mathbb{E}_{x \\sim p_{data}(x)}[log(D(x)] + \\mathbb{E}_{z \\sim p_{z}}[log(1 - D(G(z))]\\)\\)</p>"},{"location":"Articles/Scalar/Building%20a%20GAN%20from%20scratch/#heuristic-loss","title":"Heuristic loss","text":"<p>Another aspect of knowing how to make a Generative Adversarial Network is understanding heuristics. These heuristics are not part of any network directly but are training guidelines that work for most GANs. (Any GAN created before 2016, at least.) We can use these heuristics to ensure stable reductions in the loss landscape, which is key to training a Generative Adversarial Network well. - If the network has any pooling layers, they can be replaced with [Strided] convolutions in the Discriminator and fractionally ../../Strided|Strided.md|../../Strided|Strided convolutions in the Generator. - We can use Batch Normalization layers in the Generator and the Discriminator. - If the architecture is deep, we should remove FC layers for better performance. - As for activations, the ReLU activation should be used for all the layers. The only exception is the output layer, where a TanH activation should be used.</p>"},{"location":"Articles/Scalar/Building%20a%20GAN%20from%20scratch/#training-a-gan","title":"Training a GAN","text":"<p>We need an optimization algorithm that performs gradient descent on the network weights to train the GAN. The SGD (Stochastic Gradient Descent) algorithm is used for a vanilla GAN such as ours. The Generator and the Discriminator are assigned to their SGD optimizer for training. This procedure ensures that they both learn independent weights. Since the outputs of both networks flow to and from each other, they are influenced by each other as well.</p> <p>The general training paradigm for any GAN is as follows. This paradigm is always a good place to refer to when figuring out how to build a Generative Adversarial Network. - Obtain an image, and create a random noise of the same size as the image.  - Pass these images to the Discriminator and obtain the probability of the image passed being real or fake. - Create another noise of the same size as before, and pass it to the Generator. - Train the Generator with this input data. - Repeat all the previous steps until the weights are successfully optimized and satisfactory results are obtained. </p>"},{"location":"Articles/Scalar/Building%20a%20GAN%20from%20scratch/#coding-a-gan","title":"Coding a GAN","text":"<p>In this article, we will create a GAN that can create novel handwritten digits every time it is called. We will take all the concepts we have learnt and, finally, learn how to make a Generative Adversarial Network in Python using the Tensorflow library. Before actually building the network and training pipeline, we need to choose a dataset and set up the optimizers and loss functions.  After the initial set-up is completed, we can train the network and generate our handwritten digits (or any other data).</p>"},{"location":"Articles/Scalar/Building%20a%20GAN%20from%20scratch/#imports","title":"Imports","text":"<p>First, we import all the required libraries. We will import the plotting library matplotlib and the numerical processing library numpy. In this case, we will import all the required functions from Tensorflow.</p> <pre><code>from __future__ import print_function, division\n\nfrom keras.datasets import mnist\nfrom keras.layers import Input, Dense, Reshape, Flatten, [Dropout.md|../../Dropout|Dropout](../../Dropout.md)\nfrom keras.layers import BatchNormalization, Activation, ZeroPadding2D\nfrom keras.layers import LeakyReLU\nfrom keras.layers.convolutional import UpSampling2D, Conv2D\nfrom keras.models import Sequential, Model\nfrom keras.optimizers import Adam, SGD\n\nimport matplotlib.pyplot as plt\nimport sys\nimport numpy as np\n</code></pre>"},{"location":"Articles/Scalar/Building%20a%20GAN%20from%20scratch/#setup-configuration","title":"Setup Configuration","text":"<p>To further understand how to make a Generative Adversarial Network, we need to explore our configuration options. We first define the size of the image we want to load and generate. Since we are using the MNIST dataset, we set this to 28x28. The MNIST dataset is grayscale, and this only has one channel. We can set the size of the latent space to 100. If the dataset was more complex, We could choose a higher number.</p> <pre><code>num_rows = 28\nnum_cols = 28\nnum_channels = 1\ninput_shape = (num_rows, num_cols, num_channels)\nz_size = 100\nopt = SGD()\n</code></pre>"},{"location":"Articles/Scalar/Building%20a%20GAN%20from%20scratch/#dataset","title":"Dataset","text":"<p>The dataset we use in this article is the Modified National Institute of Standards and Technology database or MNIST. It is a dataset of handwritten digits almost ubiquitous in deep learning. A sample of this dataset is shown below.  [IMAGE {5} MNIST START SAMPLE]  [IMAGE {5} FINISH SAMPLE]</p> <p>The MNIST is a very simple dataset for modern networks to model, so it is a good challenge for our vanilla GAN.</p> <p>The MNIST dataset comes pre-installed with Keras, and we can directly use it. We need to pre-process the images by normalizing them and converting them to 3 dimensions to pass them to the network (a Generative Adversarial Network cannot directly process 2D images without changing the architecture). We also create proxy containers for the real and fake images to save memory during training.</p> <pre><code>(train_ims, _), (_, _) = mnist.load_data()\ntrain_ims = train_ims / 127.5 - 1.\ntrain_ims = np.expand_dims(train_ims, axis=3)\n\nvalid = np.ones((batch_size, 1))\nfake = np.zeros((batch_size, 1))\n</code></pre>"},{"location":"Articles/Scalar/Building%20a%20GAN%20from%20scratch/#networks","title":"Networks","text":"<p>We now look at the network architecture to understand how to make a Generative Adversarial Network from scratch. </p>"},{"location":"Articles/Scalar/Building%20a%20GAN%20from%20scratch/#discriminator_1","title":"Discriminator","text":"<p>To create the Discriminator, we define a function that returns a model with the network defined. This function does not compile the model as we need to call it multiple times, and pre-compiling it will lead to issues when we do.</p> <pre><code>def build_discriminator():\n\n    disc_model = Sequential()\n    disc_model.add(Flatten(input_shape=input_shape))\n    disc_model.add(Dense(512))\n    disc_model.add(LeakyReLU(alpha=0.2))\n    disc_model.add(Dense(256))\n    disc_model.add(LeakyReLU(alpha=0.2))\n    disc_model.add(Dense(1, activation='sigmoid'))\n\n    disc_img = Input(shape=input_shape)\n    validity = disc_model(disc_img)\n    return Model(disc_img, validity)\n</code></pre>"},{"location":"Articles/Scalar/Building%20a%20GAN%20from%20scratch/#generator_1","title":"Generator","text":"<p>The Generator is built similarly to the Discriminator. We define a custom function to create the Generator but do not compile it for now. The noise is also generated and passed through the network here.</p> <pre><code>def build_generator():\n\n    gen_model = Sequential()\n    gen_model.add(Dense(256, input_dim=z_size))\n    gen_model.add(LeakyReLU(alpha=0.2))\n    gen_model.add(BatchNormalization(momentum=0.8))\n    gen_model.add(Dense(512))\n    gen_model.add(LeakyReLU(alpha=0.2))\n    gen_model.add(BatchNormalization(momentum=0.8))\n    gen_model.add(Dense(1024))\n    gen_model.add(LeakyReLU(alpha=0.2))\n    gen_model.add(BatchNormalization(momentum=0.8))\n    gen_model.add(Dense(np.prod(input_shape), activation='tanh'))\n    gen_model.add(Reshape(input_shape))\n\n    gen_noise = Input(shape=(z_size,))\n    gen_img = gen_model(gen_noise)\n    return Model(gen_noise, gen_img)\n</code></pre>"},{"location":"Articles/Scalar/Building%20a%20GAN%20from%20scratch/#optimization","title":"Optimization","text":"<p>We define the following functions to set up the optimizers for both networks. We will be using an SGD optimizer in this case for both models.</p> <pre><code># discriminator\ndisc= build_discriminator()\ndisc.compile(loss='binary_crossentropy',\n    optimizer='sgd',\n    metrics=['accuracy'])\n\nz = Input(shape=(z_size,))\n# generator\nimg = generator(z)\n\ndisc.trainable = False\n\nvalidity = disc(img)\n\n# combined model\ncombined = Model(z, validity)\ncombined.compile(loss='binary_crossentropy', optimizer='sgd')\n</code></pre>"},{"location":"Articles/Scalar/Building%20a%20GAN%20from%20scratch/#training","title":"Training","text":"<p>Before we can train, we need to define a few utility functions. The first function sets up both the Generator and the Discriminator for training. It compiles the combined model and also creates noise.</p> <pre><code>def intialize_model():\n    disc= build_discriminator()\n    disc.compile(loss='binary_crossentropy',\n        optimizer='sgd',\n        metrics=['accuracy'])\n\n    generator = build_generator()\n\n    z = Input(shape=(z_size,))\n    img = generator(z)\n\n    disc.trainable = False\n\n    validity = disc(img)\n\n    combined = Model(z, validity)\n    combined.compile(loss='binary_crossentropy', optimizer='sgd')\n    return disc, generator, combined\n</code></pre> <p>The entire training loop is then as follows. This loop follows the exact procedure described in previous sections. We also add a running counter that tells us how far along we are in training and saves the outputs every sample_interval epochs.</p> <pre><code>def train(epochs, batch_size=128, sample_interval=50):\n    # load images   \n    (train_ims, _), (_, _) = mnist.load_data()\n    # preprocess\n    train_ims = train_ims / 127.5 - 1.\n    train_ims = np.expand_dims(train_ims, axis=3)\n\n    valid = np.ones((batch_size, 1))\n    fake = np.zeros((batch_size, 1))\n    # training loop\n    for epoch in range(epochs):\n\n        batch_index = np.random.randint(0, train_ims.shape[0], batch_size)\n        imgs = train_ims[batch_index]\n    # create noise\n        noise = np.random.normal(0, 1, (batch_size, z_size))\n    # predict using Generator\n        gen_imgs = gen.predict(noise)\n    # calculate loss functions\n        real_disc_loss = disc.train_on_batch(imgs, valid)\n        fake_disc_loss = disc.train_on_batch(gen_imgs, fake)\n        disc_loss_total = 0.5 * np.add(real_disc_loss, fake_disc_loss)\n\n        noise = np.random.normal(0, 1, (batch_size, z_size))\n\n        g_loss = full_model.train_on_batch(noise, valid)\n    # show progress\n        print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, disc_loss_total[0], 100*disc_loss_total[1], g_loss))\n    # save outputs every few epochs\n        if epoch % sample_interval == 0:\n            one_batch(epoch)\ndisc, gen, full_model = intialize_model()\ntrain(epochs=10000, batch_size=32, sample_interval=200)\n</code></pre> <p>After defining these functions, we train it for as many epochs as we want. For the sake of this article, we can train it for 10,000 epochs. Longer epochs do not necessarily mean better performance.</p>"},{"location":"Articles/Scalar/Building%20a%20GAN%20from%20scratch/#testing","title":"Testing","text":"<p>We also need a function that samples a batch of data to generate images on demand during/after training. This function creates a random noise vector and uses the trained Generator to perform a prediction on the noise. The generated images are then plotted for a batch of images.</p> <pre><code>def one_batch(epoch):\n    row, col = 5, 5\n    noise = np.random.normal(0, 1, (r * c, z_size))\n    gen_imgs = gen.predict(noise)\n\n    # Rescale images 0 - 1\n    gen_imgs = .5 * gen_imgs + .5\n\n    fig, axs = plt.subplots(r, c)\n    cnt_axis = 0\n    plt.cla()\n    plt.clf()\n    for i in range(row):\n        for j in range(col):\n            axs[i,j].imshow(gen_imgs[cnt_axis, :,:,0], cmap='gray')\n            axs[i,j].axis('off')\n            cnt_axis += 1\n    fig.savefig(\"images/%d.png\" % epoch)\n    plt.close()\n</code></pre> <p>In the training loop, if the number of passed epochs is a multiple of the sample interval (how many epochs to skip before saving the outputs), we call this function and save the images. We can also do this later on.</p> <pre><code>if epoch % sample_interval == 0:\n    one_batch(epoch)\n</code></pre>"},{"location":"Articles/Scalar/Building%20a%20GAN%20from%20scratch/#results","title":"Results","text":"<p>The code we wrote saves images at an interval of 200 epochs. For clarity, we can look at the images generated at the start, at 400 epochs, at 5000 epochs and finally after 10,000 epochs.</p> <p>At the start, we have random noise. [IMAGE {6} Epoch 0 START SAMPLE]  [IMAGE {6} FINISH SAMPLE]</p> <p>After 400 epochs, we are getting somewhere slowly. But these results are different from real digits. [IMAGE {7} Epoch 400 START SAMPLE]  [IMAGE {7} FINISH SAMPLE]</p> <p>After 5000 epochs, we can see figures that resemble the MNIST dataset. [IMAGE {8} Epoch 5000 START SAMPLE]  [IMAGE {8} FINISH SAMPLE]</p> <p>After training the network for the entire 10,000 epochs, we get the following outputs.  [IMAGE {9} Final results START SAMPLE]  [IMAGE {9} FINISH SAMPLE]</p> <p>These images look very close to the handwritten number data we fed the network. Note that none of these exact images was previously shown to the network, and the network generated these as we trained it. ::: :::section{.summary}</p>"},{"location":"Articles/Scalar/Building%20a%20GAN%20from%20scratch/#conclusion","title":"Conclusion","text":"<ul> <li>This article taught us how to make a Generative Adversarial Network from scratch.</li> <li>We looked at the architecture of a vanilla GAN and understood the loss functions required to train it.</li> <li>We also made our own Generative Adversarial Network in Python and trained it on MNIST data.</li> <li>Finally, we looked at the stepwise results we obtained from training our GAN. :::</li> </ul>"},{"location":"Articles/Scalar/Building%20the%20Word2Vec%20Model%20in%20Gensim/","title":"Building the Word2Vec Model in Gensim","text":"<p>toc: true title: Building the Word2Vec Model in Gensim</p> <p>categories: [\"article\"] date modified:  date created: </p>"},{"location":"Articles/Scalar/Building%20the%20Word2Vec%20Model%20in%20Gensim/#building-the-word2vec-model-in-gensim","title":"# Building the Word2Vec Model in Gensim","text":"<p>:::section{.abstract}</p>"},{"location":"Articles/Scalar/Building%20the%20Word2Vec%20Model%20in%20Gensim/#overview","title":"Overview","text":"<p>Word2Vec is a family of models that can take large corpora and represent them in vector space. These representations, also known as word embeddings, are extremely useful as they help us perform many tasks in NLP. From recommender systems to analysing sentiments from internet feeds to large-scale chatbots, word embeddings have brought life to the field of NLP for decades. Word2Vec, one of the older models, is relatively simple to implement. After implementing it we will use word embedding visualization.md|../../visualization|visualization to further understand how the model works. ::: :::section{.scope}</p>"},{"location":"Articles/Scalar/Building%20the%20Word2Vec%20Model%20in%20Gensim/#scope","title":"Scope","text":"<p>This article covers the following topics: - What are Word2Vec and Gensim. - How to train a Word2Vec model using a text corpora. - Parameters that can be tweaked in a Word2Vec model. - How to load and pre-process text for Word2Vec. - How to perform word embedding visualization in TensorBoard.  ::: :::section{.main}</p>"},{"location":"Articles/Scalar/Building%20the%20Word2Vec%20Model%20in%20Gensim/#what-are-we-building","title":"What Are We Building?","text":"<p>In this article, we are modeling text data by converting a large corpora of text into a Vector model using Word2Vec. We will be using the Python library gensim to do so. We also will build the pipeline needed for word embedding visualization using the Tensorflow Embedding Projector as well as save the trained model for inference.</p>"},{"location":"Articles/Scalar/Building%20the%20Word2Vec%20Model%20in%20Gensim/#problem-statement","title":"Problem Statement","text":"<p>Our problem statement for this article is to create a pipeline using gensim that uses Word2Vec to process a text corpora, visualize the embeddings, and save the trained model to disk.</p>"},{"location":"Articles/Scalar/Building%20the%20Word2Vec%20Model%20in%20Gensim/#pre-requisites","title":"Pre-Requisites","text":"<p>Before moving to the actual implementation, there are some pre-requisite topics that we need to know. A summary of them is as follows.</p>"},{"location":"Articles/Scalar/Building%20the%20Word2Vec%20Model%20in%20Gensim/#embeddings-and-word-embedding-visualization","title":"Embeddings and Word Embedding Visualization","text":"<p>The output of a Word2Vec model is a word embedding. There are many other models which give similar embeddings, some better than Word2Vec as well. These are synonymous with word vectors for our use case. </p>"},{"location":"Articles/Scalar/Building%20the%20Word2Vec%20Model%20in%20Gensim/#stopwords","title":"Stopwords","text":"<p>Common words like \"the\", \"to\", etc., do not add much to the model but can negatively influence the embedding.</p>"},{"location":"Articles/Scalar/Building%20the%20Word2Vec%20Model%20in%20Gensim/#cbow","title":"CBOW","text":"<p>A continuous bag of words uses a single hidden layer NN to predict a source word based on its neighbouring words. It uses a sliding window over the sentence to generate these pairs. Consider these examples as (X, Y) pairs to be passed into a model.  An example: Given the sentence \"I love gensim a lot\" and a sliding window of 2, we get ([I, gensim], love), ([love, a], gensim) etc</p>"},{"location":"Articles/Scalar/Building%20the%20Word2Vec%20Model%20in%20Gensim/#skip-gram","title":"Skip-gram","text":"<p>Skip Grams are a mirror of CBOW. It also uses a similar single hidden layer NN with a sliding window but uses the context words to predict the source word.  An example: Given the sentence \"I love gensim a lot\" and a sliding window of 2, we get (love, [I, gensim]), (gensim, [love, a]) etc</p>"},{"location":"Articles/Scalar/Building%20the%20Word2Vec%20Model%20in%20Gensim/#how-are-we-going-to-build-this","title":"How Are We Going To Build This?","text":"<p>We are going to load a custom data file as an input to the model and preprocess it to make it fit for the model. After that, we will load the gensim implementation of Word2Vec and train it on the data we loaded. Once the model is done training, we will export the model to disk and load it the trained model using Tensorflow Embedding Projector for word embedding visualization. In the process, we also will save and load the model for further inference. </p>"},{"location":"Articles/Scalar/Building%20the%20Word2Vec%20Model%20in%20Gensim/#final-output","title":"Final Output","text":"<p>The final output that we will get from the model is the text corpora that we load converted to vector space. This vector space can then be used to find similar words from the text and be passed to other neural network models if required. An example word embedding might look something like this.</p>"},{"location":"Articles/Scalar/Building%20the%20Word2Vec%20Model%20in%20Gensim/#requirements","title":"Requirements","text":"<p>To create a Word2Vec model, we need to first understand what it is. We will also be looking at the library Gensim where we obtain the Word2Vec model from.</p>"},{"location":"Articles/Scalar/Building%20the%20Word2Vec%20Model%20in%20Gensim/#what-is-word2vec","title":"What Is Word2Vec?","text":"<p>Word vectors are a numerical representation of text content. Word2Vec is a model that converts large text corpora to a vector representation as  doing so provides the corpora with the following properties : - Since they are converted to numerical vectors, they can be fed into any numerical model, such as a neural network.  - The converted vectors can be compared by using distance metrics such as the cosine distance.md|../../Cosine Distance|cosine distance function \\(cos(\\theta) = \\frac{A\\cdot B}{||A||\\cdot||B||}\\) ( where A and B are vectors). These metrics make it easy to find related words like the one we want to achieve.</p>"},{"location":"Articles/Scalar/Building%20the%20Word2Vec%20Model%20in%20Gensim/#what-is-gensim","title":"What is Gensim?","text":"<p>Gensim is a text processing library that lets us train models like Word2Vec very quickly. The library has many more features, such as finding related documents using trained word embeddings and other methods of vectorization that are beyond the scope of this article.</p>"},{"location":"Articles/Scalar/Building%20the%20Word2Vec%20Model%20in%20Gensim/#building-word2vec-model-with-gensim","title":"Building Word2Vec Model with Gensim","text":"<p>Now that we have understood the problem statement, we can start building the model using gensim. We first load all the required packages and data. </p>"},{"location":"Articles/Scalar/Building%20the%20Word2Vec%20Model%20in%20Gensim/#loading-packages-and-data","title":"Loading Packages and Data","text":"<p>Before we build the Word2Vec Model, we need to load a few packages along with the data. These packages can be installed using pip. (Eg : <code>pip install gensim</code>) if they are not already present on the system that is being used. We also download the stopwords and punctuation data from nltk.</p> <pre><code>import nltk  \nnltk.download('stopwords')  \nnltk.download('punkt')\nnltk.download('gutenberg')\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize\nfrom gensim.models import Word2Vec as w2v\n</code></pre> <p>After this, we load the data. For this demo, we will be using text from the book \"Emma\" by \"Jane Austen\". This dataset is a public domain dataset from Project Gutenberg that comes with nltk. We can get it using the following code. </p> <pre><code>import nltk\nnltk.download('gutenberg')\n!cp /root/nltk_data/corpora/gutenberg/austen-emma.txt sample.txt\n</code></pre> <p>We can also use custom text by creating a text file called \"sample.txt\" in the same directory as the code and pasting whatever we want. (Make sure it is English text).</p>"},{"location":"Articles/Scalar/Building%20the%20Word2Vec%20Model%20in%20Gensim/#data-preprocessing","title":"Data Preprocessing","text":"<p>After loading the data, we need to pre-process it to be able to pass it to the model by removing stopwords and punctuation and converting the words into lowercase tokens. We should get something like this <code>['emma', 'jane', 'Austen', '1816']</code></p> <pre><code>sw = stopwords.words('English') # this is a list of stopwords\n# Function to remove stopwords per line if they are present in the line\ndef rem_stops_line(line, words):\n    if len(line) &gt;1:\n        return [w for w in line if w not in words]\n    else:\n        return line\n\n# Remove stop words for an entire text. Separate functions make it easier to parallelise if required.\ndef remove_stops(text, words = SW):\n    return [rem_stops_line(line, words) for line in text]\n\n# Open the file and convert it to a list of lines\nwith open('sample.txt', 'r') as f:\n    lines = f.readlines()\n\n# Remove new lines, convert all to lowercase, remove punctuation and stop words and tokenise\nlines = [line.rstrip('\\n').lower() for line in lines]\nlines = [line.translate(str.maketrans('', '', string.punctuation)) for line in lines]\nlines = [word_tokenize(line) for line in lines]\nfiltered_lines = remove_stops(text = lines, words = SW)\n\nprint(filtered_lines[:10])\n</code></pre>"},{"location":"Articles/Scalar/Building%20the%20Word2Vec%20Model%20in%20Gensim/#gensim-word2vec-model-training","title":"Gensim Word2Vec Model Training","text":"<p>Once both the data and the model have been loaded, we can train it on the data using the following code.</p> <pre><code>w = w2v(\n    filtered_lines,\n    min_count=3,  \n    sg = 1,       # 1 for skip-gram, 0 for cbow\n    window=7   # sliding window size\n)      \n</code></pre>"},{"location":"Articles/Scalar/Building%20the%20Word2Vec%20Model%20in%20Gensim/#parameters-of-the-word2vec-model","title":"Parameters of the Word2Vec Model","text":"<p>The Word2Vec model implemented in gensim has a few parameters that we can tune based on the task. They are explained as follows: - sentences : This is the preprocessed input text. - size : This is the maximum dimension size of the output vector. - window : This is the sliding window size. - min_count : The minimum word frequency below which words would not be passed to the model. - workers : This is the number of parallel threads for processing. - sg : 1 for the Skip Gram model, 0 for the CBOW model.     - CBOW is prone to overfitting words that frequently appear in the same contexts, so try SkipGrams as well.     - Skip grams need more data and are more resource intensive but perform better. Choose it based on the task at hand. - iter : This is the number of iterations the model will update it's gradients for. Tweaking these parameters also help improving the accuracy of the word embedding visualization.</p>"},{"location":"Articles/Scalar/Building%20the%20Word2Vec%20Model%20in%20Gensim/#compute-similarities","title":"Compute Similarities","text":"<p>The Word2Vec model can also be used to find similar words in a text.  We can find words similar to a random word we pick from the text to see our model works. Since we used data from Emma, let us try searching for the word \"book\".</p> <pre><code>print(w.wv.most_similar('book'))\n</code></pre> <p>We get the following words and how related the model thinks they are to the word \"book\".</p> <pre><code>[('peace', 0.9995625615119934), ('exercise', 0.999549388885498), ('burst', 0.9995266199111938), ('purpose', 0.9995185732841492), ('meet', 0.9995156526565552), ('mei', 0.9995115995407104), ('move', 0.9995064735412598), ('week', 0.9995056986808777), ('views', 0.9995036125183105), ('persons', 0.9995019435882568)]\n</code></pre>"},{"location":"Articles/Scalar/Building%20the%20Word2Vec%20Model%20in%20Gensim/#t-sne-visualizations","title":"T-SNE Visualizations","text":"<p>To see what the embedding space looks like, we can give these embeddings to Tensorboard. To do so, make sure the model is saved. Here we have saved it as \"wvecemma\". Now we use a script that comes inbuilt with gensim to convert our Word2Vec model to the Tensorboard format.</p> <pre><code>python -m gensim.scripts.word2vec2tensor -i \"wvecemma\" -o \"model\"\n</code></pre> <p>We get two files, \"model_tensor.tsv\" and \"model_metadata.tsv\".</p> <p>To convert the model to a Keras Embedding, use the code (for model w), <code>w.wv.get_keras_embedding()</code></p> <p>Now we can either open Tensorboard (if you have it installed) and navigate to this folder and open the generated files, or go to this website Tensorflow Projector. On the website, click the Load button. For Step 1, choose the \"model_tensor.tsv\" file, and for Step 2, choose the other one. </p> <p>We can then see the embeddings directly.</p> <p>[IMAGE {1} {Embedding Projector} START SAMPLE]  [IMAGE {1} FINISH SAMPLE]</p>"},{"location":"Articles/Scalar/Building%20the%20Word2Vec%20Model%20in%20Gensim/#saving-and-loading-the-model","title":"Saving and Loading the Model","text":"<p>To prevent having to train again, we save the model to disk using the following code. </p> <pre><code>w.wv.save_word2vec_format(\"wvecemma\") # save to disk \n</code></pre> <p>This model can then be loaded for inference during production using the following code.</p> <pre><code>model = gensim.models.KeyedVectors.load_word2vec_format(\"wvecemma.bin.gz\", binary=True) # load from disk\n</code></pre> <p>::: :::section{.summary}</p>"},{"location":"Articles/Scalar/Building%20the%20Word2Vec%20Model%20in%20Gensim/#conclusion","title":"Conclusion","text":"<ul> <li>This article taught us how to implement the Word2Vec model in gensim. </li> <li>We learnt how to perform word embedding visualization using the TensorBoard Embedding Projector.</li> <li>We also learnt how to use our own data to train a Word2Vec model.</li> </ul> <p>:::</p>"},{"location":"Articles/Scalar/CycleGAN%20%23scalar/","title":"CycleGAN #scalar","text":"<p>toc: true title: CycleGAN #scalar</p> <p>categories: [\"article\"] date modified:  date created: </p>"},{"location":"Articles/Scalar/CycleGAN%20%23scalar/#translating-images-using-a-cyclegan-scalar","title":"Translating Images Using a CycleGAN #scalar","text":"<p>:::section{.abstract}</p>"},{"location":"Articles/Scalar/CycleGAN%20%23scalar/#overview","title":"Overview","text":"<p>The field of computer vision has been trying to create AI that creates never seen before images for decades. Generative networks such as the CycleGAN are part of a long line of such research, but one that performs extremely well in tasks ranging from converting images to paintings to changing the weather in images. The CycleGAN is rather different from many approaches before it as it is an unpaired Image2Image translation task with these tasks being cyclic in nature. In this article, we will explore what all these terms mean and how to put them into practise in a CycleGAN.</p>"},{"location":"Articles/Scalar/CycleGAN%20%23scalar/#disclaimer","title":"Disclaimer","text":"<p>This is an intermediate level article and introduces a significant number of new terms. Attempting to understand this article is not recommended before mastering how a basic GAN.md|../../Basic GAN|basic GAN (eg: DCGAN) works. Being so complex, it is advised to slow down and understand a section before moving on to the next. ::: :::section{.scope}</p>"},{"location":"Articles/Scalar/CycleGAN%20%23scalar/#scope-of-the-article","title":"Scope of the Article","text":"<p>This article covers the following concepts - What is an unpaired Image2Image task - What is GAN and subsequently, what is a CycleGAN - Concepts such as Generators, Discriminator, Encoder Decoder architectures - Latent space as an easier means of understanding CycleGANs - Architectural Details and Training procedure of CycleGANs - How to use CycleGANs in various domains ::: :::section{.main}</p>"},{"location":"Articles/Scalar/CycleGAN%20%23scalar/#note-about-the-code","title":"Note About The Code","text":"<p>This article does not contain the code to run a CycleGAN. This was done to prevent the article from becoming extremely huge. Instead, every concept that would be required to understand the code is explained in detail below. The entire code with examples can be found on the official Tensorflow/Keras documentation website. On understanding this article, this code will become extremely easy to comprehend and use.</p>"},{"location":"Articles/Scalar/CycleGAN%20%23scalar/#quick-recap","title":"Quick Recap","text":"<p>This section gives a recap of the important concepts we need to understand CycleGAN. </p>"},{"location":"Articles/Scalar/CycleGAN%20%23scalar/#image2image-translation","title":"Image2Image Translation","text":"<p>An image translation task\u2019s objective is to convert from one image to either text or another image. An example of the same would be taking a picture of a sunset and converting that picture to the style of the artist Van Gogh. There are many such Image translation tasks, but the one in consideration in this article is specifically converting from an image to another image aka Image2Image.</p>"},{"location":"Articles/Scalar/CycleGAN%20%23scalar/#paired-vs-unpaired-translation","title":"Paired vs Unpaired Translation","text":"<p>An important detail about the CycleGAN is that unlike most other GANs or UNet style architectures, it uses an unpaired translation. This means that the image, label pairs that are being passed into the model are not related to each other. In contrary to say, a classification task where the label passed is the exact label of the image passed. This might be slightly confusing to understand without knowledge of latent space. More explanations can be found in the next section</p>"},{"location":"Articles/Scalar/CycleGAN%20%23scalar/#gan","title":"GAN","text":"<p>GANs are a special class of architectures that have two components, one that tries to get better at a task, and the other that tries to find out how badly the first part is performing. The training procedure is somewhat similar to a game with one component being an \u201cadversary\u201d of the other, hence the term \u201cAdversarial network\u201d. The generative term comes into play as these networks are used to create novel images from existing data.</p>"},{"location":"Articles/Scalar/CycleGAN%20%23scalar/#adversarial-training","title":"Adversarial Training","text":"<p>A useful analogy to understand how a GAN is trained is the classic \u201ccop\u201d vs \u201cthief\u201d analogy. Assume that the thief in this case is trying to forge a painting by Van Gogh, while the cop tries to prove the thief wrong. The thief first comes up with a forgery, and the cop says no, this is fake and is not the real one because of some reason. The thief then shows a modified image to the cop with minor differences. This scenario repeats until the cop can no longer tell the difference between the real and the fake images. This type of training is called adversarial training. Using a second opinion to improve the outputs of the first component of the model. A useful advantage here is that the data does not need to be labelled. </p>"},{"location":"Articles/Scalar/CycleGAN%20%23scalar/#generator","title":"Generator","text":"<p>The generator is the \u201cthief\u201d. It starts with random noise to create a fake image and traverses the latent space until the Discriminator can no longer tell the difference between the fake and the real images.</p>"},{"location":"Articles/Scalar/CycleGAN%20%23scalar/#discriminator","title":"Discriminator","text":"<p>Consequently, the discriminator is the \u201ccop\u201d. It is essentially a classifier that returns a metric of how fake the image looks. </p>"},{"location":"Articles/Scalar/CycleGAN%20%23scalar/#encoder-decoder-architecture","title":"Encoder Decoder Architecture","text":"<p>Many networks such as the UNet and GANs have a two sided architecture that involves downsampling.md|../../Downsampling|downsampling the image until a point and then upsampling from there on. The Encoder is the first half which downsamples the image and condenses the information in a batch of data down into the smallest possible unit.  The decoder does the opposite, it takes this smallest possible unit and attempts to recreate the original input. In the process, it learns how to traverse the latent space and create the required translation. </p>"},{"location":"Articles/Scalar/CycleGAN%20%23scalar/#residual-block","title":"Residual Block","text":"<p>The Encoder Decoder architecture has a major drawback, in the process of compressing the image information and reconstructing it, a lot of information is lost during the operations. The Residual Block is the answer to this dilemma. It essentially contains a \u201cSkip Connection\u201d that ensures that the gradients from the previous layer are carried over to the next layer.  This is done very simply. If \\(F(x)\\) is the network, then for an input \\(x_{i}\\), \\(x_i = F(x_{i-1}) + x_{i-1}\\). </p>"},{"location":"Articles/Scalar/CycleGAN%20%23scalar/#cost-function","title":"Cost Function","text":"<p>A cost function is the objective that the network tries to minimise. In essence, it is a metric of how well the network performs with respect to a task. For image classification, it could be CrossEntropy. CycleGAN has a rather complex cost function that is explained in later sections.</p>"},{"location":"Articles/Scalar/CycleGAN%20%23scalar/#latent-space","title":"Latent Space","text":"<p>Understanding latent space is the key to fully comprehending the CycleGAN. It can be thought of as an n-dimensional vector space that contains every possible image that can be generated from the given data. It is not possible to entirely visualise this directly but a useful analogy is considering faces.  If we consider the faces of every person that we know and \u201caverage\u201d them together, we would end up with a \u201cgeneric face\u201d that has traits from every face that we considered. From this \u201cgeneric face\u201d, if we were to attempt to recreate any of the other faces, we would have to \u201cadd\u201d or \u201csubtract\u201d some features from the face to reach the other one. This can be thought of as \u201ctraversing\u201d the latent space. Thus in essence, the latent space contains all such possible faces. In theory then, it makes sense to assume that if we can approximate this latent space, we can translate any image to the other by traversing it. The CycleGAN attempts to do just that. Traversal can go in both directions, hence it\u2019s \u201ccyclic\u201d nature.</p>"},{"location":"Articles/Scalar/CycleGAN%20%23scalar/#cyclegan-architecture","title":"CycleGAN Architecture","text":"<p>The CycleGAN architecture is divided into two major parts - The Generator and the Discriminator. The Generator further has the Encoder, the Transformer and the Decoder as components.</p>"},{"location":"Articles/Scalar/CycleGAN%20%23scalar/#encoder-block","title":"Encoder Block","text":"<p>The encoder uses convolution layers to consolidate information from the data and compress it into the least possible representational unit. The number of channels consequently increase. In the current model, there are 3 convolution operations. The final output of the model reduces the original image size by 3/4th and passes it to the Transformer Block.</p>"},{"location":"Articles/Scalar/CycleGAN%20%23scalar/#transformer-block","title":"\u201cTransformer\u201d Block","text":"<p>The Transformer Block has nothing to do with \u201cTransformer architectures\u201d but is called so because it takes the output of the encoder and transformers it so the decoder can use it. In the CycleGAN, this block has around 6-9 Residual blocks. This is done to ensure maximum information extraction from the compressed representation that the encoder generates.</p>"},{"location":"Articles/Scalar/CycleGAN%20%23scalar/#decoder-block","title":"Decoder Block","text":"<p>The decoder block then takes the inputs from the transformer block and passes it through two de-convolution layers. </p>"},{"location":"Articles/Scalar/CycleGAN%20%23scalar/#discriminator_1","title":"Discriminator","text":"<p>The Discriminator in the CycleGAN is another type of GAN - \u201cPatchGAN\u201d. This special type of Discriminator uses patches of the input image to map to outputs. Unlike a normal GAN that maps from a 256x256 sized image to a scalar result (\u201creal\u201d or \u201cfake), the PatchGAN maps it to NxN sized arrays of outputs \\(X\\) where each \\(X_{i,j}\\) maps to (\u201creal\u201d, \u201cfake\u201d). This is run as a convolution through the entire image and the results are averaged out.</p>"},{"location":"Articles/Scalar/CycleGAN%20%23scalar/#loss-functions","title":"Loss Functions","text":"<p>CycleGAN uses a mix of three loss functions. Along with adversarial loss, a cycle consistency loss and an identity loss are used to create the final objective function.</p>"},{"location":"Articles/Scalar/CycleGAN%20%23scalar/#cycle-consistency-loss","title":"Cycle Consistency Loss","text":"<p>This is the most important part of the CycleGAN research. Considering two image domains \\(X\\) and \\(Y\\), the Cycle consistency loss uses two different mappings \\(G: X \\rightarrow Y\\) and \\(F : Y \\rightarrow X\\). These mappings are bijections, aka reverses of each other. (Hence \u201ccyclic\u201d). As a mathematical expression, the Cycle Consistency Loss \\(\\mathcal{L}_{cyc}\\) can then be represented as \\(\\(\\mathcal{L}_{cyc}(G, F, X, Y) = \\frac{1}{m}\\Sigma_{i=1}^{m}[(F(G(x_{i})-x_{i})+ (G(F(y_{i}))-y_{i})]\\)\\).</p>"},{"location":"Articles/Scalar/CycleGAN%20%23scalar/#identity-loss","title":"Identity Loss","text":"<p>Another loss function that CycleGAN proposes is the Identity loss. \\(\\(L_{identity}(G,F) = \\mathbb{E}_{y \\sim p_{data}(y)}[||G(y)-y)||_{1}] + \\mathbb{E}_{x \\sim p_{data}(x)}[||F(x)-x)||_{1}]\\)\\) This is especially useful for converting to and from photos and paintings. The loss is used to preserve the color information during transformation and attempts to make sure that the reverse color is not used. If a part of the image looks like it belongs to the target image already, it is not mapped to something else. In principle, it makes the model more conservative if the content to be transformed is not known.</p>"},{"location":"Articles/Scalar/CycleGAN%20%23scalar/#objectivecost-function","title":"Objective/Cost Function","text":"<p>The net loss function is therefore a combination of all the three losses just described. A hyper parameter \\(\\lambda\\) is used to control the strength of the transfer. It is usually set to 10. \\(\\(\\mathcal{L}_{GAN}(G, F, D_{X}, D_{Y}) = \\mathcal{L}_{GAN}(G, D_{Y}, X, Y) + \\mathcal{L}_{GAN}(F, D_{X}, X, Y) + \\lambda \\mathcal{L}_{cyc}(G,F)\\)\\)</p> <p>Thus the function that we wish to solve is finding the value of the optimisers that gives the best loss while maintaining the ability to convert to and from the target image. This can be seen in the formula. \\(\\(G^{*},F^{*} =\\underset{G,F}{argmin} \\underset{D_{X}, D_{Y}}{max} \\mathcal{L}_{GAN}(G, F, D_{X}, D_{Y})\\)\\)</p>"},{"location":"Articles/Scalar/CycleGAN%20%23scalar/#other-useful-details-about-the-architecture","title":"Other Useful Details About The Architecture","text":"<p>The CycleGAN paper and code have a lot of interesting tweaks that were done to improve performance. Some of the ones that are not usually mentioned are as follows.</p>"},{"location":"Articles/Scalar/CycleGAN%20%23scalar/#separate-optimizers","title":"Separate Optimizers","text":"<p>There are two optimisers for the discriminator and the generator each. (Four in total.) Using two separate optimisers ensure that the model learns to convert images in both directions and minimises the loss for both sets of generators and discriminators.</p>"},{"location":"Articles/Scalar/CycleGAN%20%23scalar/#instance-normalisation","title":"Instance Normalisation","text":"<p>Instance normalisation is a regularisation technique that allows the network to remove specific contrast information when transferring style information between images. This technique makes CycleGANs extremely useful for image stylisation tasks. The formula is similar to that of Batch Normalisation and is left in this article as a reference.  \\(\\(y_{tijk} = \\frac{x_{tijk} - \\mu_{ti}}{\\sqrt{\\sigma_{ti}^2 + \\epsilon}},     \\quad     \\mu_{ti} = \\frac{1}{HW}\\sum_{l=1}^W \\sum_{m=1}^H x_{tilm},     \\quad     \\sigma_{ti}^2 = \\frac{1}{HW}\\sum_{l=1}^W \\sum_{m=1}^H (x_{tilm} - mu_{ti})^2\\)\\)</p>"},{"location":"Articles/Scalar/CycleGAN%20%23scalar/#fractional-stride","title":"Fractional Stride","text":"<p>Most networks use a stride that is a whole number. CycleGAN has two types of convolutions. One type that has a stride of 2, while the other that has a stride of \\(\\frac{1}{2}\\). This is a special case of convolution that is also known as a \u201cde-convolution\u201d. A fractional stride upsamples an image from a smaller dimension to a larger one. A whole number stride downsamples it.</p>"},{"location":"Articles/Scalar/CycleGAN%20%23scalar/#reflection-padding","title":"Reflection Padding","text":"<p>When performing convolutions, the sliding window does not always fit the size of the image to be convolved, padding is used in those cases to make up for the missing pixels. In CycleGAN, a reflection padding is used which just means that the missing pixels are filled in with its neighbouring pixels before being convolved instead of being filled with a \u201cblack\u201d 0 pixel. This helps to preserve some more content information.</p>"},{"location":"Articles/Scalar/CycleGAN%20%23scalar/#photos-to-paintings","title":"Photos To Paintings","text":"<p>To convert photos to paintings, the procedure remains exactly the same as before. The only difference is the dataset. As long as the dataset contains images in different folders with different painting style information alongside a folder of plain photos, CycleGAN can convert between them. The original paper has many such results. Some of which are shown below.</p>"},{"location":"Articles/Scalar/CycleGAN%20%23scalar/#more-use-cases","title":"More Use Cases","text":"<p>Just like converting photos to paintings, a similar thought process can be applied to find other uses cases. The ones that the paper mention are as follows : - Style Transfer : Same as photos to and from paintings except with other types of imagery than just paintings. - Object Transformation : Convert to and from objects within ImageNet classes by traversing the latent space. Eg: Converting apples to oranges, zebras to horses etc. - Season Transfer : Converting images taken in Winter to summer and vice versa.</p>"},{"location":"Articles/Scalar/CycleGAN%20%23scalar/#limitations","title":"Limitations","text":"<p>Every method has its limitations. CycleGAN performs poorly when given geometrical transformations. This is because it is trained to change appearances but it does not penalise changes in geometry.  ::: :::section{.summary}</p>"},{"location":"Articles/Scalar/CycleGAN%20%23scalar/#conclusion","title":"Conclusion","text":"<p>In this article, we learnt about CycleGAN and all the architectural details required to create it. We explored the concept of a latent space and understood how a GAN works by traversing it. We also looked at many applications of a CycleGAN and how to train one on our own data. :::</p>"},{"location":"Articles/Scalar/DCGAN%20%E2%80%93%20Adding%20convolution%20to%20a%20GAN/","title":"DCGAN \u2013 Adding convolution to a GAN","text":"<p>toc: true title: DCGAN \u2013 Adding convolution to a GAN</p> <p>categories: [\"article\"] date modified:  date created: </p>"},{"location":"Articles/Scalar/DCGAN%20%E2%80%93%20Adding%20convolution%20to%20a%20GAN/#dcgan-adding-convolution-to-a-gan","title":"DCGAN \u2013 Adding convolution to a GAN","text":"<p>:::section{.abstract}</p>"},{"location":"Articles/Scalar/DCGAN%20%E2%80%93%20Adding%20convolution%20to%20a%20GAN/#overview","title":"Overview","text":"<p>Generative networks are a fascinating subfield of Computer vision. The GAN, in particular, is a training paradigm and a family of network architectures that convert a simple convolutional network to generate novel images based on an image dataset. This training is generally unpaired and does not require any labels. The original GAN architecture was unstable and had issues returning random noise as an output. The DCGAN was proposed as an alternative architecture with many tweaks over the original to counter issues such as mode collapse, diminished gradients, and non-convergence. ::: :::section{.scope}</p>"},{"location":"Articles/Scalar/DCGAN%20%E2%80%93%20Adding%20convolution%20to%20a%20GAN/#scope","title":"Scope","text":"<ul> <li>This article explains the concept of GANs and how DCGANs differ from Vanilla GANs.</li> <li>It shows how to build a DCGAN from scratch using PyTorch for image generation using the CIFAR.md|../../CIFAR|CIFAR dataset.</li> <li>It explains the preprocessing and loading of the CIFAR.md|../../CIFAR|CIFAR dataset using a DataLoader.</li> <li>It describes the architecture of the DCGAN and the reasoning behind the choices of layers and activation functions.</li> <li>The article also describes how to train the network, generate new images, and improve the training time and the results.</li> </ul> <p>::: :::section{.main}</p>"},{"location":"Articles/Scalar/DCGAN%20%E2%80%93%20Adding%20convolution%20to%20a%20GAN/#introduction-to-dggan","title":"Introduction to DGGAN","text":"<p>This article will explore using a Deep Convolutional Generative Adversarial Network (DCGAN) to generate new images from the CIFAR.md|../../CIFAR|CIFAR dataset. GANs are neural networks designed to generate new, previously unseen data similar to the input data the model trained on. DCGANs are a variation of GANs that address issues that can arise with standard GANs by using deep convolutional neural networks in both the Generator and the Discriminator.</p> <p>This architecture allows larger image sizes than in standard GANs, as convolutional layers can efficiently process images with many pixels. Additionally, DCGANs use batch normalization and leaky ReLU activations in the Discriminator and transposed convolutional layers in the Generator, improving performance and stability during training.</p> <p>We will use PyTorch to build the DCGAN from scratch, train it on the [CIFAR] dataset, and write scripts to generate new images. The goal is to generate photorealistic images that resemble one of the ten classes in the ../../CIFAR|CIFAR.md|../../CIFAR|CIFAR dataset. Before we begin, we will set up the necessary libraries and create folders to store the models' images and weights. This article will guide the implementation process and explain the reasoning for some architectural choices.</p> <p>:::</p> <p>:::section{.main}</p>"},{"location":"Articles/Scalar/DCGAN%20%E2%80%93%20Adding%20convolution%20to%20a%20GAN/#need-for-dcgan","title":"Need for DCGAN","text":"<ul> <li>A simple convolutional GAN needs to be more stable to generate images with a high resolution and suffers from mode collapse. </li> <li>DCGAN, on the other hand, has an architecture that uses not just convolutions but also transposed convolutions and other improvements. </li> <li>These changes help the network learn better and generate images more stably compared to other architectures that came before it. </li> <li>The DCGAN research was a monumental step for GANs as it was one of the earliest stable unsupervised image generators. </li> <li>Understanding how it works is the gateway to creating more advanced GANs.</li> </ul> <p>:::section{.main}</p>"},{"location":"Articles/Scalar/DCGAN%20%E2%80%93%20Adding%20convolution%20to%20a%20GAN/#pre-requisites","title":"Pre-requisites","text":"<p>To understand the DCGAN Architecture, we need to know some pre-requisite concepts. Since the entire architecture is made up of blocks of the same components, knowing them is helpful.</p>"},{"location":"Articles/Scalar/DCGAN%20%E2%80%93%20Adding%20convolution%20to%20a%20GAN/#transposed-convolutionsde-convolution","title":"Transposed Convolutions/De-Convolution","text":"<p>A De-Convolution is an upsampling method that uses transforms opposite to a normal convolution operation. It maintains the input's shape and pattern that a standard convolution would possess.  [IMAGE {1} { Transposed Convolution } START SAMPLE]  [IMAGE {1} FINISH SAMPLE]</p>"},{"location":"Articles/Scalar/DCGAN%20%E2%80%93%20Adding%20convolution%20to%20a%20GAN/#stridedmdstridedstrided-convolutions","title":"Strided.md|../../Strided|Strided Convolutions","text":"<p>The stride in a Convolution determines how many steps the moving filter skips over in an image. In a general Convolution, the stride is set to 1. To perform downsampling.md|../../Downsampling|downsampling, we can set the stride to any number above 1. Larger numbers are only sometimes good; only experimenting with the parameter can be used to understand which to pick. [IMAGE {2} {Strided Convolution} START SAMPLE]  [IMAGE {2} FINISH SAMPLE]</p>"},{"location":"Articles/Scalar/DCGAN%20%E2%80%93%20Adding%20convolution%20to%20a%20GAN/#leaky-relu","title":"Leaky ReLU","text":"<p>The Leaky ReLU activation is a small modification over the ReLU that is useful for networks with sparse gradients like the DCGAN. Due to the GAN architecture and training methodology, some nodes that use ReLU tend to die out and do nothing. The Leaky ReLU accounts for negative values by having a smaller slope instead of going straight to zero.  The change is quite minor but makes a huge difference. While the ReLU is defined as \\(max(0, x)\\), the Leaky ReLU is \\(max(0,01x, x)\\)</p> <p>[IMAGE {3} {Leaky ReLU} START SAMPLE]  [IMAGE {3} FINISH SAMPLE] ::: :::section{.main}</p>"},{"location":"Articles/Scalar/DCGAN%20%E2%80%93%20Adding%20convolution%20to%20a%20GAN/#architecture","title":"Architecture","text":"<p>The DCGAN architecture follows a similar pattern to many GAN architectures, with a Generator and a Discriminator to process inputs. </p> <p>[IMAGE {4} {Architecture} START SAMPLE]  [IMAGE {4} FINISH SAMPLE]</p> <p>The general flow of input looks something like the following. For every iteration, randomly generated noise is passed to the Generator. The Discriminator gets a random image sampled from the dataset. The Generator uses the learned weights to modify the noise closer to the target image. The Generator then passes this modified image to the Discriminator, which predicts how real the image looks and returns a probability of the same. The loss from both parts is combined to minimize the loss functions for the Generator and the Discriminator using back-propagation. </p> <p>An important point to note for both parts is that the weights are initialized differently for the Convolutional and Batch Normalization layers. If the layer is Convolutional, the weights are from a random normal distribution with a standard deviation of 0.02 and a mean of 0. If the layer is a Batch Normalization layer, a standard deviation of 0.02 means of 1.0 with a bias of 0 is used. </p>"},{"location":"Articles/Scalar/DCGAN%20%E2%80%93%20Adding%20convolution%20to%20a%20GAN/#deconvolutional-generator","title":"Deconvolutional Generator","text":"<p>[IMAGE {5} {Generator} START SAMPLE]  [IMAGE {5} FINISH SAMPLE]</p> <p>The Generator maps the input from its latent space to the vector data space. This part of the network outputs an RGB image the same size as the training image (3x64x64).  The Generator comprises blocks of Transposed Convolutions, Batch Normalizations, and ReLU layers. The output is passed through a Tanh activation that maps it to a range of [-1,1].  The DCGAN authors also found that using a Batch Normalization layer after a Transposed Convolution led to the best results by aiding the gradient flow between the layers. This effect was previously never studied in depth. In the architecture diagram of this component, nz stands for the width of the input, ngf stands for the shape of the maps that the network creates, and nc refers to a count of the channels that the output will have (Eg : 3 channels for RGB, 4 for RGBA).</p>"},{"location":"Articles/Scalar/DCGAN%20%E2%80%93%20Adding%20convolution%20to%20a%20GAN/#convolutional-discriminator","title":"Convolutional Discriminator","text":"<p>The Discriminator is a mirror of the Generator except for a few changes. The input size remains the same as the Generator (3x64x64). Instead of a De-Convolution, a Strided.md|../../Strided|Strided Convolution is used. A Leaky ReLU version of ReLU replaces the ReLU activations. The final layer is a Sigmoid layer to return the probability of real vs. fake.  The DCGAN architecture also uses Strided.md|../../Strided|Strided Convolutions to downsample the images instead of Pooling, allowing the network to learn a custom pooling function. </p>"},{"location":"Articles/Scalar/DCGAN%20%E2%80%93%20Adding%20convolution%20to%20a%20GAN/#implementation","title":"Implementation","text":"<p>To generate images using a DCGAN, we first need to prepare our dataset. This process includes creating a DataLoader to load the images, preprocessing them as necessary, and sending batches of the data to the GPU memory for efficient processing.</p> <p>Next, we need to define the architecture of the DCGAN, including the Generator and Discriminator networks. This process involves specifying the number and type of layers and initializing the weights of these layers. We must also send the network architecture to the GPU memory for efficient processing.</p> <p>Once the data and network are ready, we can train the DCGAN. During training, the network learns to map random noise from the latent space to images that resemble the training data. After training, we can use the Generator to generate new images by providing random noise from the latent space.</p>"},{"location":"Articles/Scalar/DCGAN%20%E2%80%93%20Adding%20convolution%20to%20a%20GAN/#defining-the-discriminator","title":"Defining the Discriminator","text":"<p>In the DCGAN, the Discriminator differentiates between the images generated by the Generator as real or fake. Its architecture resembles the Generator but with a few modifications. Specifically, the Discriminator incorporates Strided.md|../../Strided|Strided Convolution layers, a LeakyReLU activation function, and several layers of Batch Normalization. Lastly, the output is passed through a Sigmoid layer that returns a probability value.</p> <p>For the process of DCGAN image generation, the Discriminator uses Strided.md|../../Strided|Strided Convolutions in place of Pooling layers. This approach enables the network to develop custom padding functions, improving performance. This approach is a key technique that helps the Discriminator to distinguish between real and fake images more accurately.</p> <pre><code>ngpu = 1\nnz = 100\nngf = 64\nndf = 64\n\nclass Disc_model(nn.Module):\n    def __init__(self, ngpu):\n        super(Disc_model, self).__init__()\n        self.ngpu = ngpu\n        self.main = nn.Sequential(\n            nn.Conv2d(num_channels, ndf, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, input):\n        if input.is_cuda and self.ngpu &gt; 1:\n            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n        else:\n            output = self.main(input)\n\n        return output.view(-1, 1).squeeze(1)\n</code></pre>"},{"location":"Articles/Scalar/DCGAN%20%E2%80%93%20Adding%20convolution%20to%20a%20GAN/#defining-the-generator","title":"Defining the Generator","text":"<p>The Generator in a DCGAN is responsible for taking a random vector from the latent space and mapping it to an image in the vector data space. This mapping uses a series of transposed convolutional layers, batch normalization layers, and ReLU activation layers. Using batch normalization after the transposed convolutional layers helps improve the gradient flow through the network, resulting in better performance and stability during the training process. The final layer of the Generator uses a Tanh activation function to ensure that the output image is in the range of [-1, 1], which is the expected range for image data.</p> <pre><code>class Gen_model(nn.Module):\n    def __init__(self, ngpu):\n        super(Gen_model, self).__init__()\n        self.ngpu = ngpu\n        self.main = nn.Sequential(\n            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf * 8),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf, num_channels, 4, 2, 1, bias=False),\n            nn.Tanh()\n        )\n\n    def forward(self, input):\n        if input.is_cuda and self.ngpu &gt; 1:\n            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n        else:\n            output = self.main(input)\n            return output\n</code></pre>"},{"location":"Articles/Scalar/DCGAN%20%E2%80%93%20Adding%20convolution%20to%20a%20GAN/#defining-the-inputs","title":"Defining the inputs","text":"<p>The CIFAR10 dataset is utilized in this article provided by the Canadian Institute for Advanced Research. This dataset consists of ten classes of images that are similar to the MNIST format but with 3-channel RGB. The CIFAR10 dataset is widely used for benchmarking image classification models and is an easily learned dataset.</p> <p>Before using the dataset, it must be loaded and preprocessed. PyTorch has an inbuilt CIFAR10 dataset implementation that we can load directly. If the dataset is being used for the first time, it must be downloaded. Once the dataset is loaded, images are resized to a common size of 64x64x3. Although CIFAR10 is a clean dataset, this resizing step is still important to standardize the images. Finally, the images are normalized and converted to PyTorch tensors.</p> <p>A DataLoader is then created, a class that creates optimized batches of data to pass to the model. If available, this DataLoader is sent to the GPU to accelerate the DCGAN image generation process.</p> <pre><code>dataset = tv_data.CIFAR10(root=\"./data\", download=True,\n                           transform=transforms.Compose([\n                               transforms.Resize(64),\n                               transforms.ToTensor(),\n                               transforms. Normalize ((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n                           ]))\nnum_channels=3\n\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=128,\n                                         shuffle=True, num_workers=2)\ncurrent_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n</code></pre>"},{"location":"Articles/Scalar/DCGAN%20%E2%80%93%20Adding%20convolution%20to%20a%20GAN/#starting-the-dcgan","title":"Starting the DCGAN","text":"<p>To streamline the workflow, some empty containers are set up at the beginning of the process. A fixed noise of shape (128, size of latent space, 1, 1) is created and transferred to the GPU memory. The labels for real images are also set as one and for fake images as 0. The network will run for 25 epochs in this example. For tracking progress and analyzing performance, arrays are created to store the Generator and Discriminator loss during training.</p> <pre><code>fixed_noise = torch.randn(128, nz, 1, 1).to(current_device)\nreal_label = 1\nfake_label = 0\n\nniter = 25\ng_loss = []\nd_loss = []\n</code></pre>"},{"location":"Articles/Scalar/DCGAN%20%E2%80%93%20Adding%20convolution%20to%20a%20GAN/#computing-the-loss-function","title":"Computing the loss function","text":"<p>The DCGAN image generation process involves two loss functions, one for the Generator and another for the Discriminator.</p> <p>The Discriminator loss function penalizes the model for incorrectly classifying a real image as fake or a fake image as real. This loss can be thought of as maximizing the following function: \\(\\(\\nabla_{\\theta_{d}} \\frac{1}{m} \\Sigma_{i=1}^{m}[log D(x^{(i)}) + log(1-D(G(z^{(i)})))]\\)\\)</p> <p>The Generator loss function considers the Discriminator's output, rewarding the Generator if it can fool the Discriminator into thinking the fake image is real. If this condition is not met, the Generator is penalized. This loss can be thought of as minimizing the following function: \\(\\(\\nabla_{\\theta_{g}} \\frac{1}{m} \\Sigma_{i=1}^{m}log(1-D(G(z^{(i)})))\\)\\)</p> <p>In summary, the Discriminator's role is to maximize its loss function, and Generator's role is to minimize its loss function, which results in Generator creating an image similar to real images. These fake images should be identified as real by the Discriminator.</p> <pre><code>model_Gen = Gen_model(ngpu).to(current_device)\nmodel_Gen.apply(weights_normal_init)\nmodel_Disc = Disc_model(ngpu).to(current_device)\nmodel_Disc.apply(weights_normal_init)\nloss_func = nn.BCELoss()\n</code></pre>"},{"location":"Articles/Scalar/DCGAN%20%E2%80%93%20Adding%20convolution%20to%20a%20GAN/#optimizing-the-loss","title":"Optimizing the loss","text":"<p>In this implementation, for DCGAN image generation, the ADAM optimizer is used with a learning rate of 0.0002, and the beta parameters are set to (0.5, 0.999) to minimize the loss function. Different optimizers are used for each of them to ensure that the Generator and Discriminator learn independently.</p> <pre><code>optimizerD = optim.Adam(model_Disc.parameters(), lr=0.0002, betas=(0.5, 0.999))\noptimizerG = optim.Adam(model_Gen.parameters(), lr=0.0002, betas=(0.5, 0.999))\n</code></pre>"},{"location":"Articles/Scalar/DCGAN%20%E2%80%93%20Adding%20convolution%20to%20a%20GAN/#train-the-dcgan","title":"Train the DCGAN","text":"<p>The DCGAN image generation process involves training the network before generating new images. The procedure is done in the following steps.</p> <ul> <li>For each epoch, random noise is sent as an input to the Generator.</li> <li>The Discriminator also receives a random image sampled from the dataset.</li> <li>The Generator then uses its learned weights to transform the noise to be more similar to the target image. These weights allow the Generator to learn the mapping between random noise and the latent space of the image dataset.</li> <li>The Generator sends the modified image to the Discriminator.</li> <li>The Discriminator evaluates the realism of the generated image and communicates it to the Generator through a probability metric.</li> <li>This process of the Generator creating new images and the Discriminator evaluating it continues until the desired number of epochs.</li> <li>Once the training is completed, the Generator can generate new images by inputting random noise.</li> </ul> <pre><code>for epoch in tqdm(range(niter), total = niter):\n    for i, data in enumerate(dataloader, 0):\n        model_Disc.zero_grad()\n        device_model = data[0].to(current_device)\n        batch_size = device_model.size(0)\n        label = torch.full((batch_size,), real_label).to(current_device)\n\n        output = model_Disc(device_model) # Discriminator output\n        disc_error_real = loss_func(output.float(), label.float()) \n        disc_error_real.backward() # disc loss for real image\n        D_x = output.mean().item()\n\n        noise = torch.randn(batch_size, nz, 1, 1).to(current_device) # create noise\n        fake = model_Gen(noise) # Fake image\n        label.fill_(fake_label) # Fill with 0\n        output = model_Disc(fake.detach())\n        disc_error_fake = loss_func(output.float(), label.float()) # disc loss for fake image\n        disc_error_fake.backward() \n        D_G_z1 = output.mean().item()\n        disc_error = disc_error_real + disc_error_fake\n        optimizerD.step()\n\n        model_Gen.zero_grad()\n        label.fill_(real_label) # fill with 1\n        output = model_Disc(fake.float()) # disc output\n        gen_error = loss_func(output.float(), label.float())\n        gen_error.backward()\n        D_G_z2 = output.mean().item()\n        optimizerG.step()\n\n        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f' % (epoch, niter, i, len(dataloader), disc_error.item(), gen_error.item(), D_x, D_G_z1, D_G_z2))\n\n        if i % 100 == 0: # save images every 100 steps\n            print('saving the output')\n            vutils.save_image(device_model,'./images/real_samples.png',normalize=True)\n            fake = model_Gen(fixed_noise)\n            vutils.save_image(fake.detach(),'./images/fake_samples_epoch_%03d.png' % (epoch),normalize=True)\n\n    torch.save(model_Gen.state_dict(), 'weights/model_Gen_epoch_%d.pth' % (epoch))\n    torch.save(model_Disc.state_dict(), 'weights/model_Disc_epoch_%d.pth' % (epoch))\n</code></pre> <p>This article trains the network for 25 epochs. To get a better understanding of the progression of the training, we compare the original sample to the outputs generated at the 0th, 10th, and 25th epochs. As the training progresses, the ./images folder is periodically checked every 100 steps to observe the output. After the training is completed, the final results are as follows.</p> <p>[IMAGE {5} Original Sample/Target START SAMPLE]  [IMAGE {5} FINISH SAMPLE]</p> <p>[IMAGE {6} Epoch 0 START SAMPLE]  [IMAGE {6} FINISH SAMPLE]</p> <p>[IMAGE {7} Epoch 10 START SAMPLE]  [IMAGE {7} FINISH SAMPLE]</p> <p>[IMAGE {8} Final Results START SAMPLE]  [IMAGE {8} FINISH SAMPLE]</p>"},{"location":"Articles/Scalar/DCGAN%20%E2%80%93%20Adding%20convolution%20to%20a%20GAN/#weight-initialization","title":"Weight Initialization","text":"<p>The DCGAN model requires a careful weight initialization scheme. If the layer is a Convolutional layer, we can take the initialization values from a Normal distribution in the range of (0.0,0.02). On the other hand, if the layer is a Batch Normalization layer, we can take the weights from a Normal distribution in the range of (0.0, 0.02) while we can set the bias to 0.</p> <pre><code>def weights_normal_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        m.weight.data.normal_(0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        m.weight.data.normal_(0.0, 0.02)\n        m.bias.data.fill_(0)\n</code></pre> <p>::: :::section{.summary}</p>"},{"location":"Articles/Scalar/DCGAN%20%E2%80%93%20Adding%20convolution%20to%20a%20GAN/#conclusion","title":"Conclusion","text":"<ul> <li>The article has explained the concept of GANs and the specific architecture of DCGANs, which are a variation that can handle larger images.</li> <li>It has also provided a step-by-step guide on how to build a DCGAN from scratch using the PyTorch library and the CIFAR.md|../../CIFAR|CIFAR dataset.</li> <li>The implementation process, including loading the dataset and preprocessing it, creating the network architecture and initialization of weights, as well as training the network, has been explained.</li> <li>The final output is expected to be a set of photorealistic images that resemble one of the classes in the CIFAR.md|../../CIFAR|CIFAR dataset, which is a significant achievement.</li> <li>GANs, particularly DCGANs, have a wide range of applications and can generate images of different objects, depending on the dataset used to train the network. This article provides a foundation for further research and experimentation with GANs.</li> </ul> <p>:::</p>"},{"location":"Articles/Scalar/Differences%20between%20Discriminative%20and%20Generative%20models/","title":"Differences between Discriminative and Generative models","text":"<p>toc: true title: Differences between Discriminative and Generative models</p> <p>categories: [\"article\"] date modified:  date created: </p>"},{"location":"Articles/Scalar/Differences%20between%20Discriminative%20and%20Generative%20models/#differences-between-discriminative-and-generative-models","title":"Differences between Discriminative and Generative models","text":"<p>:::section{.abstract}</p>"},{"location":"Articles/Scalar/Differences%20between%20Discriminative%20and%20Generative%20models/#overview","title":"Overview","text":"<p>Machine learning models can be broadly classified into discriminative and generative. Discriminative models, such as logistic regression, support vector machines, and decision [trees], learn to model boundaries around classes in a dataset and estimate the conditional probability of the target variable given the data. On the other hand, generative models, such as latent Dirichlet allocation, Bayesian networks, and hidden Markov models, create new data points by estimating the joint probability distribution of the data and the target variable. Generative models are often used for unsupervised tasks, such as topic modelling and ../../Dimensionality Reduction|dimensionality reduction.md|../../Trees|trees, while discriminative models are more commonly used for classification and regression. ::: :::section{.scope}</p>"},{"location":"Articles/Scalar/Differences%20between%20Discriminative%20and%20Generative%20models/#scope","title":"Scope","text":"<ul> <li>The article introduces the concept of discriminative and generative models in the context of machine learning.</li> <li>The article explains the differences between these two models and how they approach tasks differently.</li> <li>The article provides examples of commonly used machine learning models in the discriminative or generative category.</li> <li>The article discusses the applications of discriminative and generative models in various tasks, such as classification, regression, and unsupervised learning.</li> <li>The article also compares the advantages and disadvantages of discriminative and generative models.</li> </ul> <p>::: :::section{.main}</p>"},{"location":"Articles/Scalar/Differences%20between%20Discriminative%20and%20Generative%20models/#introduction","title":"Introduction","text":"<p>Various Machine Learning models have been proposed over the years, each for different tasks. A broad categorization of these models is to classify them into Generative and Discriminative models. Discriminative models estimate the conditional probability, while Generative models estimate the joint probability distribution. This article will examine the difference between Generative and Discriminative models. We will also introduce several commonly used ML models and categorize them into these two groups.</p>"},{"location":"Articles/Scalar/Differences%20between%20Discriminative%20and%20Generative%20models/#discriminative-model","title":"Discriminative Model","text":"<p>Discriminative Models are a family of models that do not generate new data points but learn to model boundaries around classes in a dataset instead. These models aim to maximize the separation between the classes in the dataset to perform classification or Regression. Discriminative models estimate the conditional probability \\(P(Y|X =x)\\) on a data X and a target Y.  Note that sometimes classifiers that do not use a probability model are called Discriminative Models.</p>"},{"location":"Articles/Scalar/Differences%20between%20Discriminative%20and%20Generative%20models/#logistic-regression","title":"Logistic Regression","text":"<p>Logistic Regression is a classification algorithm that uses the Sigmoid function instead of a linear function to model data.</p> <p>The Sigmoid curve is shown in the figure below.</p> <p>[IMAGE {1} { Sigmoid Curve } START SAMPLE]  [IMAGE {1} FINISH SAMPLE]</p> <p>We can also use Logistic Regression for multi-class tasks by modelling each class separately. Therefore, the Regression's outcome must be a discrete or categorical value. (e.g., Yes/No, True/False) The model's output is a probabilistic value in the range [0,1]. The modelled curve that the logistic function uses indicates the likelihood of the binary decision.  The following equation can mathematically represent Logistic Regression. \\(\\(log[\\frac{y}{1-y}] = b_0 + b_1 x_1 + b_2 x_2 + \u2026 + b_n x_n\\)\\)</p> <p>Considering the difference between Generative and Discriminative models is important in understanding why these models are Discriminative.</p>"},{"location":"Articles/Scalar/Differences%20between%20Discriminative%20and%20Generative%20models/#support-vector-machine","title":"Support Vector Machine","text":"<p>A Support Vector Machine (SVM) is a supervised classification and regression algorithm that uses the concept of hyperplanes. These hyperplanes can be understood as multi-dimensional linear decision boundaries.md|../../Decision Boundaries|decision boundaries that separate groups of unequal data points.  An example of a hyperplane is shown below.</p> <p>[IMAGE {2 } { SVM } START SAMPLE]  [IMAGE { 2} FINISH SAMPLE]</p> <p>An optimal fit of the SVM occurs when a hyperplane is furthest from the training data points of any of the classes\u2014the larger this distance margin, the lower the error of the classifier.  To better understand how the SVM works, consider a group of data points like the one shown in the diagram. It is a good fit if the hyperplane separates the points in the space so they are clustered according to their labels. If not, further iterations of the algorithm are performed. </p>"},{"location":"Articles/Scalar/Differences%20between%20Discriminative%20and%20Generative%20models/#decision-tree","title":"Decision Tree","text":"<p>Decision trees.md|../../Trees|trees are tree-based decision models that use an internal structure of a root node followed by successive child leaf nodes. The leaf nodes are a placeholder for the classification label, and the branches show the outcomes of the decision. The paths from the tree's root to the leaves represent the classifier rules. Each tree and sub-tree models a single decision and enumerates all the possible decisions to choose the best one. A Decision tree can be optimal if it represents most of the data with the least number of levels. Decision [trees] are helpful for classification but can be extended for Regression using different algorithms. These ../../Trees|trees.md|../../Trees|trees are computationally efficient, and many tree-based optimizations have been created over the years to make them perform even faster. An example of such a tree is shown below.  [IMAGE {3 } { Decision Tree } START SAMPLE]  [IMAGE { 3} FINISH SAMPLE]</p>"},{"location":"Articles/Scalar/Differences%20between%20Discriminative%20and%20Generative%20models/#random-forest","title":"Random Forest","text":"<p>[IMAGE {4 } { Random Forest } START SAMPLE]  [IMAGE { 4} FINISH SAMPLE]</p> <p>Random Forest models use a forest of Decision [Trees] to make better decisions by combining each tree's decisions. The most popular decision across the ../../Trees|trees.md|../../Trees|Trees for a task is the best after the aggregation. This technique of aggregating multiple results from similar processes is called Ensembling.  The second component of the Random Forest pertains to another technique called Bagging. Bagging differs from Ensembling because, in Bagging, the data is different for every model, while in Ensembling, the different models are run on the same data. In Bagging, a random sample with replacement is chosen multiple times to create a data sample. These data samples are then used to train the model independently. After training all these models, the majority vote is taken to find a better estimate of the data. Random forests combine the concepts of Bagging and Ensembling to decide the best feature splits and select subsets of the same. This algorithm is better than using a single Decision Tree as it reduces bias and the net variance, generating better predictions.</p> <p>Bagging and Ensembling might seem like they help model the joint probability distribution, but that is not the case. Understanding the difference between Generative and Discriminative models can clear this confusion.</p>"},{"location":"Articles/Scalar/Differences%20between%20Discriminative%20and%20Generative%20models/#generative-models","title":"Generative Models","text":"<p>Generative Models are a family of models that create new data points. They are generally used for unsupervised tasks. Generative Models use the joint probability distribution \\(P(X, Y)\\) on a variable X and a target variable Y to model the data and perform inference by estimating the probability of the new data point belonging to any given class.</p>"},{"location":"Articles/Scalar/Differences%20between%20Discriminative%20and%20Generative%20models/#latent-dirichlet-allocation-ldamdldalda","title":"Latent Dirichlet Allocation (LDA.md|../../LDA|LDA)","text":"<p>[LDA] models aim to accurately estimate the classwise mean and variance of the data points in a given dataset. After calculating these statistics, [../../LDA|LDA] makes predictions by estimating the probability of the new class belonging to any of the classes in the original data. In ML, [../../LDA|LDA] models are used for topic modelling and discovery. [../../LDA|LDA] is similar to [../../PCA|PCA] in that it also performs a [../../Dimensionality Reduction|dimensionality reduction]. But unlike [../../PCA|PCA], ../../LDA|LDA.md|../../LDA|LDA maximizes the class separation and not the variance of the data. This principle is illustrated in the figure below.</p> <p>[IMAGE {5 } { LDA } START SAMPLE]  [IMAGE { 5} FINISH SAMPLE]</p>"},{"location":"Articles/Scalar/Differences%20between%20Discriminative%20and%20Generative%20models/#bayesian-network","title":"Bayesian Network","text":"<p>A Bayesian Network is a graph-based probabilistic model that uses a special graph structure known as a DAG - Directed Acyclic Graph to model conditional dependencies between the given variables. These networks are useful in finding the possible cause of an event, given several contributing factors. A classic example of what a Bayesian Network looks like is shown below.</p> <p>[IMAGE {6 } { Bayesian Network } START SAMPLE]  [IMAGE { 6} FINISH SAMPLE]</p> <p>The Bayesian Network uses this graph to model the joint probability distribution. Each of the edges in the graph represents a dependency, while each node represents a unique variable. The model can then use this learnt distribution for inference. We can use Bayesian Networks to infer unobserved variables, learn parameters from the data and learn the structure of a manually created data distribution.</p> <p>Note that each represents Boolean variables if there are \\(m\\) parent nodes. A minimum of \\(2^m\\) entries are required to model the possible events.</p>"},{"location":"Articles/Scalar/Differences%20between%20Discriminative%20and%20Generative%20models/#hidden-markov-model","title":"Hidden Markov Model","text":"<p>A Markov process is a sequential process where the previous item only influences the next item in the sequence. A Markov Chain, therefore, is a graph that uses probabilities to denote how likely it is to move to the next state in the chain. (If it is not clear how this is a Generative model, refer to the section on the difference between Generative and Discriminative models) An example Markov Chain is shown below. [IMAGE {7 } { Markov Chain } START SAMPLE]  [IMAGE { 7} FINISH SAMPLE]</p> <p>A Hidden Markov Model is a graph where the chain is unobservable. The inputs the model receives are combined with the probabilities of the previous step. This combination is used to calculate the next step in the graph.  A constraint in an HMM is that at a certain time \\(t= t_{0}\\), the target Y must be influenced only by the state of X at \\(t= t_{0}\\). The states of Y at \\(t= t_{0}\\) should not be affected by the states of X and Y at \\(t &lt; t_{0}\\).</p> <p>An example of a Markov Model is shown here for reference. [IMAGE {8 } { HMM } START SAMPLE]  [IMAGE { 8} FINISH SAMPLE]</p>"},{"location":"Articles/Scalar/Differences%20between%20Discriminative%20and%20Generative%20models/#autoregressive-model","title":"Autoregressive model","text":"<p>An Autoregressive model is used in time series forecasting. This model uses the past values in the time series to predict values that might occur in the future. An Autoregressive model gets its name as it is a regression of itself. These models are generally represented as stochastic difference equations that use linear combinations of past values to model the data. A mathematical representation is as follows.</p> <p>\\(y_{t} + c + \\phi_{1} y_{t-1} + \\phi_{2} y_{t-2} + \u2026 + \\phi_{p} y_{t-p} + \\epsilon_{t}\\) where  \\(\\epsilon_{t}\\) is white noise.  Note that changing the patterns for \\(\\phi\\) changes the time series. Varying the error term does not change the pattern but modifies the scale of the data instead.</p> <p>An example of an Autoregressive model is shown below. [IMAGE {9 } { Autoregressive Model } START SAMPLE]  [IMAGE { 9} FINISH SAMPLE]</p>"},{"location":"Articles/Scalar/Differences%20between%20Discriminative%20and%20Generative%20models/#generative-adversarial-network","title":"Generative Adversarial Network","text":"<p>Generative Adversarial Networks are models that take large image datasets as input and generate new images. A GAN models the data distribution by exploiting the latent space of the dataset given to it. A GAN comprises two parts - A Generator and a Discriminator. These parts play a MinMax Game, where the Generator creates novel images from random noise while the Discriminator classifies the outputs into real or fake. When the Discriminator can no longer distinguish between real and fake images created by the Generator, the GAN training is said to be complete.</p> <p>An example of a GAN that converts images to a different style is shown in the following image. [IMAGE {10 } { GAN } START SAMPLE]  [IMAGE { 10} FINISH SAMPLE]</p>"},{"location":"Articles/Scalar/Differences%20between%20Discriminative%20and%20Generative%20models/#discriminative-vs-generative-models","title":"Discriminative vs Generative Models","text":"<p>The difference between Generative and Discriminative models is summarised in the following table.</p> Generative Models Discriminative Models Aim to understand the data distribution Aim to model the data decision boundary Uses the joint probability Uses the conditional probability Relatively computationally expensive Relatively cheaper computationally Unsupervised Tasks Supervised Tasks Harmed by outliers More robust to the presence of outliers Models how data is placed in space Generates boundaries around similar classes of data in space <p>::: :::section{.summary}</p>"},{"location":"Articles/Scalar/Differences%20between%20Discriminative%20and%20Generative%20models/#conclusion","title":"Conclusion","text":"<ul> <li>In summary, discriminative and generative models are two categories of machine learning models that approach tasks differently.</li> <li>Discriminative models aim to maximize the separation between classes in a dataset to perform classification or regression. </li> <li>In contrast, generative models create new data points by estimating the joint probability distribution of the data and the target variable. </li> <li>Both models have their own set of advantages and disadvantages and can be applied to various tasks. </li> </ul> <p>:::</p>"},{"location":"Articles/Scalar/Generating%20Images%20using%20GANs%20in%20Tensorflow/","title":"Generating Images using GANs in Tensorflow","text":"<p>toc: true title: Generating Images using GANs in Tensorflow</p> <p>categories: [\"article\"] date modified:  date created: </p>"},{"location":"Articles/Scalar/Generating%20Images%20using%20GANs%20in%20Tensorflow/#generating-images-using-gans-in-tensorflow","title":"Generating Images using GANs in Tensorflow","text":"<p>:::section{.abstract}</p>"},{"location":"Articles/Scalar/Generating%20Images%20using%20GANs%20in%20Tensorflow/#overview","title":"Overview","text":"<p>This article explains using a Generative Adversarial Network (GAN) to generate new images of handwritten digits. A GAN is a machine-learning model consisting of a generator and a discriminator. The generator creates novel images from random, while the Discriminator attempts to prove that the images generated are fake. The GAN is trained on the MNIST dataset of handwritten digits and is evaluated by testing it on unseen data and creating new images using the generator. The final output of the GAN is a batch of images that look like handwritten digits. The article provides code for reading the dataset, creating the required architecture, computing loss functions, training the network, and testing the network.</p> <p>::: :::section{.scope}</p>"},{"location":"Articles/Scalar/Generating%20Images%20using%20GANs%20in%20Tensorflow/#scope","title":"Scope","text":"<ul> <li>The article provides a general overview of Generative Adversarial Networks (GANs) and their use in image generation.</li> <li>The specific goal of the article is to demonstrate how to create a GAN from scratch using the Tensorflow library and train it on the MNIST dataset to generate new images of handwritten digits.</li> <li>The article explains the architecture and components of a GAN, including the generator and Discriminator.</li> <li>The article also provides code for reading and preprocessing the MNIST dataset, creating the GAN architecture, computing loss functions, training the network, and testing the network.</li> <li>The article also discusses the final output of the GAN, which should be a batch of images that look like handwritten digits.</li> </ul> <p>::: :::section{.main}</p>"},{"location":"Articles/Scalar/Generating%20Images%20using%20GANs%20in%20Tensorflow/#what-are-we-building","title":"What are we building?","text":"<p>Creating novel images given an image dataset is one of the strengths of a specific branch of models called Generative Adversarial Networks (GAN). These networks specialize in unsupervised/semi-supervised image generation given any image data.  This article uses the GANs image generation ability to create novel handwritten digits. We perform this generation by training the network on a dataset of handwritten digits. We will create a simple GAN from scratch using the Tensorflow library, train it on the MNIST dataset and generate new images of handwritten digits. </p>"},{"location":"Articles/Scalar/Generating%20Images%20using%20GANs%20in%20Tensorflow/#pre-requisites","title":"Pre-requisites","text":""},{"location":"Articles/Scalar/Generating%20Images%20using%20GANs%20in%20Tensorflow/#what-are-gans","title":"What are GANs","text":"<p>GANs, or Generative Adversarial Networks, are a family of networks used for unsupervised image generation, converting between images to another, and many other applications. They are composed of two parts - a Generator and a Discriminator. The Generator creates novel images from random. The Discriminator attempts to prove that the images generated are fake. This game leads to a training approach dubbed \"Adversarial Learning\". This article focuses on implementing a GAN and its image-generation ability to create new handwritten digits.</p>"},{"location":"Articles/Scalar/Generating%20Images%20using%20GANs%20in%20Tensorflow/#how-are-we-going-to-build-this","title":"How are we going to build this?","text":"<p>In this article, we focus on the GAN's image generation ability. To let the GAN learn about images, we must first load an image dataset and preprocess it. After loading the data, we must create the GAN and write the training and testing code. The below sections focus on implementing these features and generating new images from the MNIST dataset.</p>"},{"location":"Articles/Scalar/Generating%20Images%20using%20GANs%20in%20Tensorflow/#final-output","title":"Final Output","text":"<p>The final output we want should be a batch of images that look like handwritten digits. The image shown below is what we get after training the GAN for 10000 epochs on the MNIST dataset.</p> <p>[IMAGE {1} { Final results } START SAMPLE]  [IMAGE {1} FINISH SAMPLE]</p>"},{"location":"Articles/Scalar/Generating%20Images%20using%20GANs%20in%20Tensorflow/#requirements-list-the-libraries-modules-and-other-requirements-needed-for-the-project","title":"Requirements (List the libraries, modules, and other requirements needed for the project)","text":"<p>Before creating the GAN's image generation module, we must import a few libraries. We will import all the functions, layers and dataset loaders from Tensorflow. We will also import numpy (a math library) and matplotlib (a plotting library). </p> <p>We also need to set up some that will make up our configuration for running the module. The shape of the image is defined as a matrix of 28x28x1. The last dimension corresponds to the number of channels in an image. Since we are using the MNIST dataset in black and white, we only have a single channel.</p> <p>The zsize is the shape of the latent space we want to generate. In this case, we set it to 100. This number could be modified if required. </p> <pre><code>from __future__ import print_function, division\n\nfrom keras.datasets import mnist\nfrom keras.layers import Input, Dense, Reshape, Flatten, [Dropout.md|../../Dropout|Dropout](../../Dropout.md)\nfrom keras.layers import BatchNormalization, Activation, ZeroPadding2D\nfrom keras.layers import LeakyReLU\nfrom keras.layers.convolutional import UpSampling2D, Conv2D\nfrom keras.models import Sequential, Model\nfrom keras. optimizers import Adam, SGD\n\nimport matplotlib.pyplot as plt\nimport sys\nimport numpy as np\n\nnum_rows = 28\nnum_cols = 28\nnum_channels = 1\ninput_shape = (num_rows, num_cols, num_channels)\nz_size = 100\n</code></pre>"},{"location":"Articles/Scalar/Generating%20Images%20using%20GANs%20in%20Tensorflow/#building-the-model","title":"Building the Model","text":"<p>The GAN we want to create comprises two major parts - The Generator and the Discriminator. The Generator is responsible for creating novel images, while the Discriminator is responsible for understanding how good the generated image is.  The entire architecture we want to build for the GANs image generation is shown in the following diagram.</p> <p>[IMAGE {2} { Architecture Diagram } START SAMPLE]  [IMAGE {2} FINISH SAMPLE]</p> <p>The sections below explain how to read a dataset, create the required architecture, compute the loss functions and train the network. Finally, the code to test the network and create new images is also shown.</p>"},{"location":"Articles/Scalar/Generating%20Images%20using%20GANs%20in%20Tensorflow/#reading-the-dataset","title":"Reading the dataset","text":"<p>This article will use the MNIST (Modified National Institute of Standards and Technology) dataset. This dataset has a larger number of handwritten digits of 28x28 and is one of the most widely used datasets in computer vision. The MNIST is an easy dataset for a GAN such as the one we are building, as it has small, single channels images.  A sample of the dataset is shown below.</p> <p>We only need to write a little code to load the MNIST dataset as Tensorflow comes with it inbuilt. After loading the dataset, we normalize it and then reshape it to 3 dimensions. This reshaping enables the GAN architecture to use this 2D data. We also pre-allocate some memory for our training and validation data. </p> <pre><code>(train_ims, _), (_, _) = mnist.load_data()\ntrain_ims = train_ims / 127.5 - 1.\ntrain_ims = np.expand_dims(train_ims, axis=3)\n\nvalid = np.ones((batch_size, 1))\nfake = np.zeros((batch_size, 1))\n</code></pre>"},{"location":"Articles/Scalar/Generating%20Images%20using%20GANs%20in%20Tensorflow/#defining-the-generator","title":"Defining the Generator","text":"<p>[IMAGE {3} { Generator And Discriminator } START SAMPLE]  [IMAGE {3} FINISH SAMPLE]</p> <p>The job of the Generator (D) is to create realistic images that the Discriminator fails to understand are fake. Thus, the Generator is an essential component that enables a GANs image generation ability. The architecture we consider in this article comprises fully connected layers (FC) and Leaky ReLU activations. The final layer of the Generator has a TanH activation rather than a LeakyReLU. This replacement was done because we wanted to convert the generated image to the same range as the original MNIST dataset (-1,1).</p> <pre><code>def build_generator():\n    gen_model = Sequential()\n    gen_model.add(Dense(256, input_dim=z_size))\n    gen_model.add(LeakyReLU(alpha=0.2))\n    gen_model.add(BatchNormalization(momentum=0.8))\n    gen_model.add(Dense(512))\n    gen_model.add(LeakyReLU(alpha=0.2))\n    gen_model.add(BatchNormalization(momentum=0.8))\n    gen_model.add(Dense(1024))\n    gen_model.add(LeakyReLU(alpha=0.2))\n    gen_model.add(BatchNormalization(momentum=0.8))\n    gen_model.add(Dense(np.prod(input_shape), activation='tanh'))\n    gen_model.add(Reshape(input_shape))\n\n    gen_noise = Input(shape=(z_size,))\n    gen_img = gen_model(gen_noise)\n    return Model(gen_noise, gen_img)\n</code></pre>"},{"location":"Articles/Scalar/Generating%20Images%20using%20GANs%20in%20Tensorflow/#defining-the-discriminator","title":"Defining the Discriminator","text":"<p>The GAN uses the Discriminator (D) to identify how real the Generator's outputs look by returning a probability of real vs fake. This part of the network can be thought of as a binary classification problem. To solve this binary classification problem, we need a rather simple network composed of blocks of Fully Connect Layers (FC), Leaky ReLU activations and Dropout.md|../../Dropout|Dropout layers. Note that the final layer has a block with an FC layer and a Sigmoid.  The final Sigmoid activation returns the classification probability that we require.</p> <pre><code>def build_discriminator():\n\n    disc_model = Sequential()\n    disc_model.add(Flatten(input_shape=input_shape))\n    disc_model.add(Dense(512))\n    disc_model.add(LeakyReLU(alpha=0.2))\n    disc_model.add(Dense(256))\n    disc_model.add(LeakyReLU(alpha=0.2))\n    disc_model.add(Dense(1, activation='sigmoid'))\n\n    disc_img = Input(shape=input_shape)\n    validity = disc_model(disc_img)\n    return Model(disc_img, validity)\n</code></pre>"},{"location":"Articles/Scalar/Generating%20Images%20using%20GANs%20in%20Tensorflow/#computing-the-loss-function","title":"Computing the loss function","text":"<p>To make the GANs image generation procedure smoother, we need to supply it with metrics that show how well it is performing now. Loss functions do just that.</p> <p>The Discriminator classifies the generated images into real or fake and returns the probability of it being real. To make this distinction, it needs to ensure that the input it receives is part of the real dataset. And if the input received is fake, it is not classified as part of the real dataset. We can mathematically understand this difference as maximizing \\(D(x)\\) and minimizing \\(D(G(z))\\).</p> <p>Building on these concepts, the Generator is tasked with fooling the Discriminator by creating realistic images. We can understand this procedure as ensuring that when the Discriminator gets an image sampled from the fake dataset, it thinks that the image belongs to the real dataset instead. We can mathematically understand this procedure as maximizing \\(D(G(z))\\). It is to be. Note that just using this part of the formulae as a loss function sometimes makes the network confident about the wrong outputs. To prevent this assumption, we use \\(log(D(G(z)))\\) instead.</p> <p>The net cost function for the GAN's image generation can be thus mathematically represented as \\(\\(\\underset{G}{min} \\underset{D}{max} V(D, G) = \\mathbb{E}_{x \\sim p_{data}(x)}[log(D(x)] + \\mathbb{E}_{z \\sim p_{z}}[log(1 - D(G(z))]\\)\\)</p> <p>Training a GAN such as this is a delicate balance and can be considered a game between two enemies. (Hence the name - Adversarial Learning.) Since either party attempts to influence the opposition and reduce the others' chance of winning, this is a MinMax game.</p> <p>We can then create the Generator and Discriminator with a Binary Crossentropy loss.</p> <pre><code># discriminator\ndisc= build_discriminator()\ndisc.compile(loss='binary_crossentropy',\n    optimizer='sgd',\n    metrics=['accuracy'])\n\nz = Input(shape=(z_size,))\n# generator\nimg = generator(z)\n\ndisc.trainable = False\n\nvalidity = disc(img)\n\n# combined model\ncombined = Model(z, validity)\ncombined.compile(loss='binary_crossentropy', optimizer='sgd')\n</code></pre>"},{"location":"Articles/Scalar/Generating%20Images%20using%20GANs%20in%20Tensorflow/#optimizing-the-loss","title":"Optimizing the loss","text":"<p>To train the network, we need the GAN to play the MinMax game. The training procedure hinges on performing Gradient Descent on the network weights. To reduce the training time and ensure that the training does not get stuck on the loss landscape, we use a Stochastic version of GD, aka Stochastic Gradient Descent.  Both the Discriminator and the Generator have different losses. If We gave both these networks a single loss function, they would not be able to optimize each other. </p> <pre><code>def intialize_model():\n    disc= build_discriminator()\n    disc.compile(loss='binary_crossentropy',\n        optimizer='sgd',\n        metrics=['accuracy'])\n\n    generator = build_generator()\n\n    z = Input(shape=(z_size,))\n    img = generator(z)\n\n    disc.trainable = False\n\n    validity = disc(img)\n\n    combined = Model(z, validity)\n    combined.compile(loss='binary_crossentropy', optimizer='sgd')\n    return disc, Generator, and combined\n</code></pre> <p>Having defined all the required functions, we can train the network to optimize the losses. The steps we follow for the GAN's image generation are as follows. - Load an image, and generate random noise of the same size as the loaded image. - Send these images to the Discriminator and calculate the real vs fake probability for the same. - Generate another noise of the same size. Send this noise to the Generator. - Run training for the Generator for a few epochs. - Repeat all the steps until a satisfactory image is generated.</p> <p>These steps are directly translated into the code shown below.</p> <pre><code>def train(epochs, batch_size=128, sample_interval=50):\n    # load images   \n    (train_ims, _), (_, _) = mnist.load_data()\n    # preprocess\n    train_ims = train_ims / 127.5 - 1.\n    train_ims = np.expand_dims(train_ims, axis=3)\n\n    valid = np.ones((batch_size, 1))\n    fake = np.zeros((batch_size, 1))\n    # training loop\n    for epoch in range(epochs):\n\n        batch_index = np.random.randint(0, train_ims.shape[0], batch_size)\n        imgs = train_ims[batch_index]\n    # create noise\n        noise = np.random.normal(0, 1, (batch_size, z_size))\n    # predict using Generator\n        gen_imgs = gen.predict(noise)\n    # calculate loss functions\n        real_disc_loss = disc.train_on_batch(imgs, valid)\n        fake_disc_loss = disc.train_on_batch(gen_imgs, fake)\n        disc_loss_total = 0.5 * np.add(real_disc_loss, fake_disc_loss)\n\n        noise = np.random.normal(0, 1, (batch_size, z_size))\n\n        g_loss = full_model.train_on_batch(noise, valid)\n    # show progress\n        print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, disc_loss_total[0], 100*disc_loss_total[1], g_loss))\n    # save outputs every few epochs\n        if epoch % sample_interval == 0:\n            one_batch(epoch)\n</code></pre>"},{"location":"Articles/Scalar/Generating%20Images%20using%20GANs%20in%20Tensorflow/#generating-handwritten-digits","title":"Generating handwritten digits","text":"<p>Finally, we can generate handwritten digits from the MNIST dataset. To look at how far the network has trained the images, we create a helper function to store predictions from the Generator for a batch of images. This function creates random noise, passes them to the Generator, processes it for displaying and then saves it to a folder. We run this helper function every 200 epochs.</p> <pre><code>def one_batch(epoch):\n    r, c = 5, 5\n    noise = np.random.normal(0, 1, (r * c, z_size))\n    gen_imgs = gen.predict(noise)\n\n    # Rescale images 0 - 1\n    gen_imgs = 0.5 * gen_imgs + 0.5\n\n    fig, axs = plt.subplots(r, c)\n    cnt = 0\n    for i in range(r):\n        for j in range(c):\n            axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n            axs[i,j].axis('off')\n            cnt += 1\n    fig.savefig(\"images/%d.png\" % epoch)\n    plt.close()\n</code></pre> <p>For this article, we trained the GAN for around 10,000 epochs with a batch size of 32. We save the generated images every 200 epochs in the images folder.</p> <pre><code>disc, gen, full_model = intialize_model()\ntrain(epochs=10000, batch_size=32, sample_interval=200)\n</code></pre> <p>We can now look at the results of the GAN's image generation at the start, at 400 epochs, at 5000 epochs and the final result at 10000 epochs.</p> <p>At the start, we have random noise. [IMAGE {4} { Epoch 0 } START SAMPLE]  [IMAGE {4} FINISH SAMPLE]</p> <p>After 400 epochs, we are getting somewhere slowly. But these results are different from real digits. [IMAGE {5} { Epoch 400 } START SAMPLE]  [IMAGE {5} FINISH SAMPLE]</p> <p>After 5000 epochs, we can see figures that resemble the MNIST dataset. [IMAGE {6} { Epoch 5000 } START SAMPLE]  [IMAGE {6} FINISH SAMPLE]</p> <p>After training the network for the entire 10,000 epochs, we get the following outputs.  [IMAGE {7} { Final results } START SAMPLE]  [IMAGE {7} FINISH SAMPLE]</p> <p>These images look very close to the handwritten number data we fed the network. These images were not shown to the network during training and were generated from scratch.</p>"},{"location":"Articles/Scalar/Generating%20Images%20using%20GANs%20in%20Tensorflow/#whats-next","title":"What's next","text":"<p>The output we got from the GANs image generation is good, but there are many ways we can improve it. Without leaving the scope of this article, we can experiment with a few parameters. Some of them are as follows: - Try different values of the latent space variable z_size to see if the performance improves. - Try training the model for a larger number of epochs. We trained it for 10000; try doubling or tripling that to see if the results improve or worsen. - Try different datasets such as the Fashion MNIST or the Moving MNIST. These datasets follow the same structure as the MNIST, making it possible to use the code we wrote directly. - Finally, it is worth experimenting with other architectures such as CycleGAN, DCGAN etc. Many of them would only require changing the functions of the Generator and Discriminator.</p> <p>::: :::section{.summary}</p>"},{"location":"Articles/Scalar/Generating%20Images%20using%20GANs%20in%20Tensorflow/#conclusion","title":"Conclusion","text":"<ul> <li>GANs are machine learning models that can generate new images from a dataset.</li> <li>In this article, a simple GAN is created using the Tensorflow library and trained on the MNIST dataset.</li> <li>The GAN comprises two parts: a Generator that creates novel images from random and a Discriminator that attempts to prove that the images generated are fake.</li> <li>The final output is a batch of images that look like handwritten digits, as shown in the example image provided.</li> <li>The GAN is trained by supplying it with metrics and loss functions that show how well it correctly classifies real and fake images. </li> <li>The GAN is then evaluated by testing it on unseen data and creating new images using the generator.</li> </ul> <p>:::</p>"},{"location":"Articles/Scalar/Image%20Classification%20using%20TensorFlow/","title":"Image Classification using TensorFlow","text":"<p>toc: true title: Image Classification using TensorFlow</p> <p>categories: [\"article\"] date modified:  date created: </p>"},{"location":"Articles/Scalar/Image%20Classification%20using%20TensorFlow/#image-classification-using-tensorflow","title":"Image Classification using TensorFlow","text":"<p>:::section{.abstract}</p>"},{"location":"Articles/Scalar/Image%20Classification%20using%20TensorFlow/#overview","title":"Overview","text":"<p>Image Classification is one of the basic tasks in Deep Learning. Given a dataset with images of different categories, we create a Deep Learning model and a pipeline to classify these images. We can create models in any library, but Tensorflow is a good starting point for beginners, and we will use this library to create an image classifier. ::: :::section{.main}</p>"},{"location":"Articles/Scalar/Image%20Classification%20using%20TensorFlow/#what-are-we-building","title":"What are we building?","text":"<p>This article will tackle a Tensorflow image classification problem by creating a neural network that can classify images from the CIFAR10 dataset. We will explore the concepts of Pre-Processing, Augmentation, and Performance Optimisation. We will learn how to load a dataset, build the model and finally train the model we created using the dataset. We will also learn how to use the trained model to make predictions on custom images.  The following sections explain these concepts and how to implement them using Tensorflow.</p>"},{"location":"Articles/Scalar/Image%20Classification%20using%20TensorFlow/#pre-requisites","title":"Pre-requisites","text":"<p>Before we get to the actual code, we must understand a few pre-requisite terms. They are explained here.</p>"},{"location":"Articles/Scalar/Image%20Classification%20using%20TensorFlow/#data-loaders","title":"Data Loaders","text":"<p>A Data Loader is a utility function that enables Tensorflow to optimize the data loading performance. The Loader does this by pre-allocating memory, creating batched containers, and applying many other tweaks to improve performance. </p>"},{"location":"Articles/Scalar/Image%20Classification%20using%20TensorFlow/#data-augmentation","title":"Data Augmentation","text":"<p>Data Augmentation is a [regularization] technique that improves performance by applying transformations on the base image. These transformations enable the model to see a much richer dataset without additional data collection. Data augmentation is extremely useful for small to medium-sized datasets. There are many augmentations, such as random flipping, random color jitter, random resized ../../Cropping|cropping.md|../../Regularization|regularization, and many others.</p> <p>[IMAGE {1} Data Augmentation  START SAMPLE]  [IMAGE {1} FINISH SAMPLE]</p>"},{"location":"Articles/Scalar/Image%20Classification%20using%20TensorFlow/#lambda-functions","title":"Lambda functions","text":"<p>Lambda functions are special functions in Tensorflow that lets the user create functions without explicitly defining a function call. These functions are useful for improving the readability of the code and avoiding defining extra functions for single use. </p>"},{"location":"Articles/Scalar/Image%20Classification%20using%20TensorFlow/#map-functions","title":"Map functions","text":"<p>In deep learning, there are many times when we need to apply a function over a batch of data. Sequentially performing these tasks is extremely time-consuming, so we use map functions to apply a function in parallel over any batch of data.</p>"},{"location":"Articles/Scalar/Image%20Classification%20using%20TensorFlow/#how-are-we-going-to-build-this","title":"How are we going to build this?","text":"<p>In this article, we will be building a Tensorflow image classification model. After loading the required libraries, we first load the data. Here, we will use the CIFAR10 dataset. After loading the data, we split it into the train, test, and validation components and then create batches of the same. To optimize performance, we will also be using caching and pre-fetching. After creating the required data loaders, we can create the model. This demo will use a ResNet50 model with the Adam optimizer and a Sparse Cross-Entropy loss function. Once we have loaded both the data and the model, we can finally train the model on the data and then evaluate its performance. All of these steps are detailed in the sections below.</p>"},{"location":"Articles/Scalar/Image%20Classification%20using%20TensorFlow/#final-output","title":"Final Output","text":"<p>The final output we want is a Tensorflow image classification model that can identify the class of a given image. We want our model to learn from the CIFAR10 dataset and understand all ten classes accurately. For example, if we pass this 64x64 image to the model, it should classify it as a horse. [IMAGE {2} Classified Data START SAMPLE]  [IMAGE {2} FINISH SAMPLE]</p>"},{"location":"Articles/Scalar/Image%20Classification%20using%20TensorFlow/#requirements","title":"Requirements","text":"<p>Before creating a Tensorflow image classification model, we need to import the required modules. Apart from the base Tensorflow library, we also import the Keras package. Keras is a wrapper around Tensorflow that simplifies building neural networks. We will also import a library called Tensorflow Datasets that will enable us to load and pre-process our dataset faster.</p> <pre><code>import numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow_datasets as tfds\nimport os\n</code></pre>"},{"location":"Articles/Scalar/Image%20Classification%20using%20TensorFlow/#building-the-classifier","title":"Building the Classifier","text":"<p>We can now move on to building the classifier with the libraries we implemented. The below sections explain how to load the dataset, pre-process it, optimize it for use, and pass it to the model. We also explore how to create a model using Tensorflow and how to train it on the CIFAR10 dataset. Finally, we also learn how to evaluate a trained model on the test dataset. </p>"},{"location":"Articles/Scalar/Image%20Classification%20using%20TensorFlow/#download-and-explore-the-dataset","title":"Download and explore the dataset","text":"<p>For this article, we will be using the CIFAR10 dataset. This dataset has 60000 32x32 color images grouped into ten classes. Before we create the model, we need to load and pre-process the dataset. A sample of images is shown below.</p> <p>[IMAGE {3} CIFAR10 START SAMPLE]  [IMAGE {3} FINISH SAMPLE]</p> <p>We split the dataset into training, testing, and validation and loaded the dataset with labels using the as_supervised option. To verify that we loaded the data correctly, we checked the size of the splits we had just created. If they are not zero or tiny numbers, we can know that our code has worked so far.</p> <pre><code>train_dataset, validation_dataset, test_dataset = tfds.load(\n    \"cifar10\",\n    split=[\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"],\n    as_supervised=True,\n)\n\nprint(\"Number of training samples: %d\" % tf.data.experimental.cardinality(train_dataset))\nprint(\n    \"Number of validation samples: %d\" % tf.data.experimental.cardinality(validation_dataset)\n)\nprint(\"Number of test samples: %d\" % tf.data.experimental.cardinality(test_dataset))\n</code></pre>"},{"location":"Articles/Scalar/Image%20Classification%20using%20TensorFlow/#configure-the-dataset-for-performance","title":"Configure the dataset for performance","text":"<p>Simply loading the data and passing it to the model works out of the box but leads to a sharp down in performance. We need to perform some tweaks to ensure that we use our resources optimally. We first define our image size as 128x128x3 pixels and use a lambda function to resize all the images in our dataset to this image size.  The next optimization we perform is converting the dataset into batches of 64 and informing Keras that we wish to cache the data and prefetch 10 samples.  Prefetching data reduces the time it takes to pass the data to the memory by preallocating memory and fetching a few extra samples for the next time the model is called.</p> <pre><code>size = (128, 128)\nbs = 64\n\ntrain_dataset = train_dataset.map(lambda x, y: (tf.image.resize(x, size), y))\nvalidation_dataset = validation_dataset.map(lambda x, y: (tf.image.resize(x, size), y))\ntest_dataset = test_dataset.map(lambda x, y: (tf.image.resize(x, size), y))\n\ntrain_dataset = train_dataset.cache().batch(bs).prefetch(buffer_size=10)\nvalidation_dataset = validation_dataset.cache().batch(bs).prefetch(buffer_size=10)\ntest_dataset = test_dataset.cache().batch(bs).prefetch(buffer_size=10)\n</code></pre>"},{"location":"Articles/Scalar/Image%20Classification%20using%20TensorFlow/#create-the-model","title":"Create the model","text":"<p>Since we wish to maximize performance, we also use two simple Data Augmentation techniques. The first one randomly flips the images along the horizontal axis to ensure that the model learns some spatial information. The second Augmentation we use is Random Rotation. We only want to apply this for some images, so we use a lower probability. Applying it to every image might make the model perform worse.</p> <p>In this article, we use the Tensorflow image classification model ResNet50. This model uses the concept of Skip Connections to improve performance. We will not create the model from scratch but use the Keras implementation. To the function call, we need to pass in the weights from which we wish to use the pre-trained model. Since we are training the model from scratch here, we will pass the option as None. If we were using transfer learning, we would use the option \"imagenet\". We also need to pass the image size (128x128x3), the number of classes (10), and whether we want to include the whole model. This final option is only useful for transfer learning.</p> <p>[IMAGE {4} Skip Connections START SAMPLE]  [IMAGE {4} FINISH SAMPLE]</p> <p>Once we load the model, we create an input, pass the current batch through the Augmentation, and finally, a Fully Connected (FC) layer with a size of 10 (CIFAR10 has ten classes). This final model is the one we will be used for training on our data.  We will use the summary function to check the layers to verify if we created the model correctly.</p> <pre><code>aug_transforms = keras.Sequential(\n    [layers.RandomFlip(\"horizontal\"), layers.RandomRotation(0.1),]\n)\n\nmodel_base = keras.applications.ResNet50(\n    weights=None, \n    input_shape=(128, 128, 3),\n    classes = 10,\n    include_top=True\n)\n\ninputs = keras.Input(shape=(128, 128, 3))\n\nx = aug_transforms(inputs) \nx = model_base(x, training=False)\noutputs = keras.layers.Dense(10)(x)\nfinal_model = keras.Model(inputs, outputs)\nfinal_model.summary()\n</code></pre>"},{"location":"Articles/Scalar/Image%20Classification%20using%20TensorFlow/#train-the-model","title":"Train the model","text":"<p>We can finally move on to training our model on the CIFAR10 data. Since we have defined our model, we need to define all the parameters required for training it. In this article, we will use the Adam optimizer with the default parameters. We choose a Sparse Categorical Crossentropy function for the loss function as this task is a multi-class classification problem. The metric we use here is a Categorical Accuracy metric that checks how well the classifier performed across all the classes. We will now train the model for five epochs. We can perform further training by increasing the number of epochs.</p> <pre><code>final_model.compile(\n    optimizer=keras.optimizers.Adam(),\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n)\n\nnum_epochs = 5\nfinal_model.fit(train_dataset, epochs=num_epochs, validation_data=validation_dataset)\n</code></pre> <p>[IMAGE {5} Training START SAMPLE]  [IMAGE {5} FINISH SAMPLE]</p>"},{"location":"Articles/Scalar/Image%20Classification%20using%20TensorFlow/#evaluating-the-model","title":"Evaluating the Model","text":"<p>Five epochs are very small since we are not using transfer learning, and it is only done as a demo. After our model is done training, we can perform a full evaluation on it by performing predictions on the test dataset. While performing inference, we can increase the batch size as we are not training a network.  We can perform this evaluation by using the evaluate function from Keras. </p> <pre><code>results = final_model.evaluate(test_dataset, batch_size=128)\nprint(\"test loss, test acc:\", results)\n</code></pre> <p>A single prediction can be performed with the following code.</p> <pre><code>from PIL import Image\nimport numpy as np\nfrom skimage import transform\ndef load(filename):\n   np_image = Image.open(filename)\n   np_image = np.array(np_image).astype('float32')/255\n   np_image = transform.resize(np_image, (256, 256, 3))\n   np_image = np.expand_dims(np_image, axis=0)\n   return np_image\n\nimage = load('my_file.jpg')\nfinal_model.predict(image)\n</code></pre> <p>::: :::section{.summary}</p>"},{"location":"Articles/Scalar/Image%20Classification%20using%20TensorFlow/#conclusion","title":"Conclusion","text":"<ul> <li>This article taught us how to build a Tensorflow Image Classification model.</li> <li>The article showed how to load the CIFAR10 dataset and pre-process it for training.</li> <li>It also explained how to create a simple ResNet50 model and how to train it on the CIFAR10 dataset.</li> <li>The article also explained how to evaluate the model and perform inference using the trained model. :::</li> </ul>"},{"location":"Articles/Scalar/Implementing%20DCGAN%20to%20generate%20CIFAR%20images/","title":"Implementing DCGAN to generate CIFAR images","text":"<p>toc: true title: Implementing DCGAN to generate CIFAR images</p> <p>categories: [\"article\"] date modified:  date created: </p>"},{"location":"Articles/Scalar/Implementing%20DCGAN%20to%20generate%20CIFAR%20images/#implementing-dcgan-to-generate-cifarmdcifarcifar-images","title":"Implementing DCGAN to generate CIFAR.md|../../CIFAR|CIFAR images","text":""},{"location":"Articles/Scalar/Implementing%20DCGAN%20to%20generate%20CIFAR%20images/#what-are-we-building","title":"What are we building?","text":"<p>Creating novel images is one of the strengths of Generative Adversarial Networks (GANs). In this article, we will build a DCGAN for image generation using the CIFAR.md|../../CIFAR|CIFAR dataset. The DCGAN is a type of GAN that builds upon the Vanilla GAN and addresses some of its issues. The DCGAN is a good choice if the image data size is larger than 28x28. This network also leads to fewer chances of a mode collapse and is thus a better network than a standard GAN.  Here, we want the network to create realistic images to resemble any of the ten classes of the CIFAR.md|../../CIFAR|CIFAR dataset. We will create the DCGAN from scratch using PyTorch, train it and write scripts to generate our images. </p>"},{"location":"Articles/Scalar/Implementing%20DCGAN%20to%20generate%20CIFAR%20images/#what-are-dgans","title":"What are DGANs","text":"<p>Researchers created the Vanilla GAN architecture to generate images in an unsupervised manner from image datasets. But this GAN had quite a few flaws that impacted its training. DCGANs are a modification of the Vanilla GAN architecture. The implementation of the Discriminator and Generator is configured to tackle some of the issues of the original GAN. Some of the changes are as follows.  Convolutional layers are used explicitly in the Discriminator. In this architecture, the Generator explicitly uses Transposed Convolution layers. The Discriminator relies on Batch Normalization along with LeakyReLU activations. The Generator, on the other hand, uses ReLU activations.  The DCGAN image generation process is almost similar to the Vanilla GAN except for a few tweaks to the optimizer and the architecture itself.</p>"},{"location":"Articles/Scalar/Implementing%20DCGAN%20to%20generate%20CIFAR%20images/#how-are-we-going-to-build-this","title":"How are we going to build this?","text":"<p>To build the DCGAN, we will use the Python library PyTorch. We will first import PyTorch and other required libraries. We then load the image dataset using a DataLoader, which is the CIFAR10 dataset in this case. After we load the data, we create the functions that make up the DCGAN. We can then train the network on this dataset and generate new photorealistic images that look like they were part of the CIFAR10 dataset.  The below sections focus on implementing all the functions required to create a DCGAN image generation pipeline.</p>"},{"location":"Articles/Scalar/Implementing%20DCGAN%20to%20generate%20CIFAR%20images/#final-output","title":"Final Output","text":"<p>The final output we want is photorealistic images that look like they might belong to the CIFAR10 dataset. The result that we get is shown below. </p> <p>[IMAGE {1} Final Output START SAMPLE]  [IMAGE {1} FINISH SAMPLE]</p> <p>Note that longer training might yield better results but take significantly longer.</p>"},{"location":"Articles/Scalar/Implementing%20DCGAN%20to%20generate%20CIFAR%20images/#requirements","title":"Requirements","text":"<p>Before starting the DCGAN image generation process, we must import some libraries and perform set-up operations. First, we must create folders to store the models' images and weights using the <code>mkdir</code> command. We import the PyTorch library and all the required features we need that comes with it. We also import the numerical processing library numpy and the plotting library matplotlib.  Training a GAN takes quite a long time. To further improve performance, we enable a flag in Pytorch that enables benchmarking. Pytorch runs a few checks on your current device during the benchmarking process to determine which algorithms perform the best. These checks let you run slightly more optimized code for your current device. </p> <pre><code>!mkdir images\n!mkdir weights\nfrom __future__ import print_function\nimport os\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nimport torch.utils.data\nimport torchvision.datasets as tv_data\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\nfrom tqdm import tqdm\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ncudnn.benchmark = True\n</code></pre>"},{"location":"Articles/Scalar/Implementing%20DCGAN%20to%20generate%20CIFAR%20images/#implementing-dcgan-to-generate-cifarmdcifarcifar-images_1","title":"Implementing DCGAN to generate CIFAR.md|../../CIFAR|CIFAR images","text":"<p>We can now move on to the main DCGAN image generation process. To generate the images, we need to create a DataLoader, load the CIFAR.md|../../CIFAR|CIFAR images,  preprocess them, and send batches of this data to the GPU memory. We also need to create the network architecture and initialize the weights of its components. This network also needs to be sent to the GPU memory.  After these initial steps have been completed, we can finally train the network and generate new images. </p> <p>The DCGAN architecture is similar to many other GAN architectures and consists of a Generator and a Discriminator. The Generator is responsible for creating photo-realistic images from random noise to fool the Discriminator. On the other hand, the Discriminator takes the outputs of the Generator and returns the probability that the generated image is real. The Generator uses this probability to improve its generation capabilities by training the model. The architecture diagram of the DCGAN is shown below. [IMAGE {2} Architecture START SAMPLE]  [IMAGE {2} FINISH SAMPLE]</p> <p>[IMAGE {3} Generator START SAMPLE]  [IMAGE {3} FINISH SAMPLE]</p> <p>In the architecture, ngpu stands for the number of GPUs. In this case, we will only be using a single one. nz stands for the width of the input. ngf and ndf denote the shape of the maps that the Generator and Discriminator create, respectively. nc is the number of channels that the image has.</p> <p>We first start by loading our data and understanding how it looks.</p>"},{"location":"Articles/Scalar/Implementing%20DCGAN%20to%20generate%20CIFAR%20images/#exploring-the-dataset","title":"Exploring the dataset","text":"<p>This article uses the CIFAR10 (Canadian Institute for Advanced Research) dataset. This dataset contains ten classes of images similar to the MNIST format but in 3-channel RGB. The CIFAR10 is a very common dataset for benchmarking image classification models and is easy for a model to learn.</p> <p>Before we can explore the dataset, we must load and preprocess it. PyTorch comes with the CIFAR10 dataset inbuilt, and we can directly load it. If this is the first time we have used this dataset, it may not exist locally, and we need to download it first. After that, we need to resize all the images to a common size of 64x64x3. CIFAR10 is a clean dataset, and this resizing step is probably unnecessary, but it is a good practice to uphold. We also normalize the images and convert them to the PyTorch tensors. In PyTorch, we need to create a DataLoader, simply a class that creates optimized batches of data to pass to the model.  We send this DataLoader to the GPU if we are using one to enhance the speed of the DCGAN image generation process.</p> <pre><code>dataset = tv_data.CIFAR10(root=\"./data\", download=True,\n                           transform=transforms.Compose([\n                               transforms.Resize(64),\n                               transforms.ToTensor(),\n                               transforms. Normalize ((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n                           ]))\nnum_channels=3\n\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=128,\n                                         shuffle=True, num_workers=2)\ncurrent_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n</code></pre> <p>After loading the data, we can see how it looks. To visualize a batch of data, we iterate over the DataLoader to grab a single batch of images. After obtaining the images, we can create a grid the size of a single batch, pad the images to make them look neater and normalize them. This batch of images is still on the GPU, which means that we cannot plot it on the CPU without sending it back.  </p> <pre><code>single_batch = next(iter(dataloader))\nplt.figure(figsize=(8,8))\nplt.axis(\"off\")\nplt.toc: true\ntitle(\"Training Images\")\nplt.imshow(np.transpose(vutils.make_grid(single_batch[0].to(current_device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))\n</code></pre> <p>[IMAGE {4} Training Images START SAMPLE]  [IMAGE {4} FINISH SAMPLE]</p> <p>As we can see, the dataset comprises ten classes of images from which we plot a random sample.</p>"},{"location":"Articles/Scalar/Implementing%20DCGAN%20to%20generate%20CIFAR%20images/#defining-the-discriminator","title":"Defining the Discriminator","text":"<p>The Discriminator in the DCGAN is responsible for classifying the images returned by the Generator as real or fake. Architecturally, the Discriminator is almost a mirror of the Generator with minor differences. The Discriminator uses a Strided.md|../../Strided|Strided Convolution and a LeakyReLU along with Batch Normalization layers. The final layer is a Sigmoid layer that returns the probability we want.  For DCGAN image generation, the Discriminator uses Strided.md|../../Strided|Strided Convolutions instead of Pooling layers. This choice allows the network to learn custom padding functions that, in turn, improve performance.</p> <pre><code>ngpu = 1\nnz = 100\nngf = 64\nndf = 64\n\ndef weights_normal_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        m.weight.data.normal_(0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        m.weight.data.normal_(1.0, 0.02)\n        m.bias.data.fill_(0)\n\nclass Disc_model(nn.Module):\n    def __init__(self, ngpu):\n        super(Disc_model, self).__init__()\n        self.ngpu = ngpu\n        self.main = nn.Sequential(\n            nn.Conv2d(num_channels, ndf, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, input):\n        if input.is_cuda and self.ngpu &gt; 1:\n            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n        else:\n            output = self.main(input)\n\n        return output.view(-1, 1).squeeze(1)\n</code></pre>"},{"location":"Articles/Scalar/Implementing%20DCGAN%20to%20generate%20CIFAR%20images/#defining-the-generator","title":"Defining the Generator","text":"<p>The Generator is responsible for mapping the input data from the latent space to the vector data space. In this part of the network, we get an RGB image as an output that can then be passed to the Discriminator. The generated image size is the same as the original training images but in channel first indexing (3x64x64). The Generator comprises blocks with Transposed Convolutions, Batch Normalizations and ReLU layers. The final layer has a Tanh activation used to make the data to a range of [-1,1]. In this part of the DCGAN image generation process, the DCGAN uses Batch Normalization layers after Transposed convolutions. This shift enables a smoother gradient flow between the layers of the network. </p> <pre><code>class Gen_model(nn.Module):\n    def __init__(self, ngpu):\n        super(Gen_model, self).__init__()\n        self.ngpu = ngpu\n        self.main = nn.Sequential(\n            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf * 8),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf, num_channels, 4, 2, 1, bias=False),\n            nn.Tanh()\n        )\n\n    def forward(self, input):\n        if input.is_cuda and self.ngpu &gt; 1:\n            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n        else:\n            output = self.main(input)\n            return output\n</code></pre>"},{"location":"Articles/Scalar/Implementing%20DCGAN%20to%20generate%20CIFAR%20images/#defining-the-inputs","title":"Defining the inputs","text":"<p>We need to set up some empty containers for an optimized workflow. We first create a fixed noise with the shape (128, size of latent space, 1, 1) and send it to the GPU. We also denote the label of the real image as one and the fake image as 0. For this article, we will run the network for 25 epochs.  We also pre-allocate arrays to store the Generator and Discriminator loss during training. </p> <pre><code>fixed_noise = torch.randn(128, nz, 1, 1).to(current_device)\nreal_label = 1\nfake_label = 0\n\nniter = 25\ng_loss = []\nd_loss = []\n</code></pre>"},{"location":"Articles/Scalar/Implementing%20DCGAN%20to%20generate%20CIFAR%20images/#computing-the-loss-function","title":"Computing the loss function","text":"<p>The DCGAN image generation pipeline has two loss functions, for the Generator and Discriminator, respectively.  The Discriminator penalizes wrongly classifying a real image as a fake or a fake image as real. This concept can be thought of as maximizing the following function. \\(\\(\\nabla_{\\theta_{d}} \\frac{1}{m} \\Sigma_{i=1}^{m}[log D(x^{(i)}) + log(1-D(G(z^{(i)})))]\\)\\)</p> <p>The Generator loss takes the output of the Discriminator into account and rewards it if the Generator is fooled into thinking the fake image is real. If this condition is not satisfied, the Generator is penalized. This concept can be thought of as minimizing the following function. \\(\\(\\nabla_{\\theta_{g}} \\frac{1}{m} \\Sigma_{i=1}^{m}log(1-D(G(z^{(i)})))\\)\\)</p> <pre><code>model_Gen = Gen_model(ngpu).to(current_device)\nmodel_Gen.apply(weights_normal_init)\nmodel_Disc = Disc_model(ngpu).to(current_device)\nmodel_Disc.apply(weights_normal_init)\nloss_func = nn.BCELoss()\n</code></pre>"},{"location":"Articles/Scalar/Implementing%20DCGAN%20to%20generate%20CIFAR%20images/#optimizing-the-loss","title":"Optimizing the loss","text":"<p>We will use the ADAM optimizer with a learning rate of 0.0002 and the beta parameters set to (0.5m, 0.999) to optimize the loss. The Generator and Discriminator have different optimizers to ensure that they both learn independently.</p> <pre><code>optimizerD = optim.Adam(model_Disc.parameters(), lr=0.0002, betas=(0.5, 0.999))\noptimizerG = optim.Adam(model_Gen.parameters(), lr=0.0002, betas=(0.5, 0.999))\n</code></pre>"},{"location":"Articles/Scalar/Implementing%20DCGAN%20to%20generate%20CIFAR%20images/#train-the-dcgan","title":"Train the DCGAN","text":"<p>To run DCGAN image generation, we need to train the network first. The general procedure is as follows. For each epoch, the noise is sent to the Generator. The Discriminator also gets a random image sampled from the dataset. The Generator then uses the weights it has learned to modify the noise to be closer to the target image. In doing so, the Generator learns a mapping between random noise and the latent space of the image dataset. The Generator then sends the tweaked image to the Discriminator. The Discriminator then predicts how real it thinks the generated image is and informs the Generator using a probability metric. </p> <pre><code>for epoch in tqdm(range(niter), total = niter):\n    for i, data in enumerate(dataloader, 0):\n        model_Disc.zero_grad()\n        device_model = data[0].to(current_device)\n        batch_size = device_model.size(0)\n        label = torch.full((batch_size,), real_label).to(current_device)\n\n        output = model_Disc(device_model) # Discriminator output\n        disc_error_real = loss_func(output.float(), label.float()) \n        disc_error_real.backward() # disc loss for real image\n        D_x = output.mean().item()\n\n        noise = torch.randn(batch_size, nz, 1, 1).to(current_device) # create noise\n        fake = model_Gen(noise) # Fake image\n        label.fill_(fake_label) # Fill with 0\n        output = model_Disc(fake.detach())\n        disc_error_fake = loss_func(output.float(), label.float()) # disc loss for fake image\n        disc_error_fake.backward() \n        D_G_z1 = output.mean().item()\n        disc_error = disc_error_real + disc_error_fake\n        optimizerD.step()\n\n        model_Gen.zero_grad()\n        label.fill_(real_label) # fill with 1\n        output = model_Disc(fake.float()) # disc output\n        gen_error = loss_func(output.float(), label.float())\n        gen_error.backward()\n        D_G_z2 = output.mean().item()\n        optimizerG.step()\n\n        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f' % (epoch, niter, i, len(dataloader), disc_error.item(), gen_error.item(), D_x, D_G_z1, D_G_z2))\n\n        if i % 100 == 0: # save images every 100 steps\n            print('saving the output')\n            vutils.save_image(device_model,'./images/real_samples.png',normalize=True)\n            fake = model_Gen(fixed_noise)\n            vutils.save_image(fake.detach(),'./images/fake_samples_epoch_%03d.png' % (epoch),normalize=True)\n\n    torch.save(model_Gen.state_dict(), 'weights/model_Gen_epoch_%d.pth' % (epoch))\n    torch.save(model_Disc.state_dict(), 'weights/model_Disc_epoch_%d.pth' % (epoch))\n</code></pre> <p>In this article, we train the network for 25 epochs. After the training is complete and even during training, we can periodically check the <code>./images</code> folder to see outputs every 100 steps.  The results we get are as follows.</p> <p>To understand the training progression, we look at the original sample and then the outputs at 0, ten, and 25 epochs.</p> <p>[IMAGE {5} Original Sample/Target START SAMPLE]  [IMAGE {5} FINISH SAMPLE]</p> <p>[IMAGE {6} Epoch 0 START SAMPLE]  [IMAGE {6} FINISH SAMPLE]</p> <p>[IMAGE {7} Epoch 10 START SAMPLE]  [IMAGE {7} FINISH SAMPLE]</p> <p>[IMAGE {8} Final Results START SAMPLE]  [IMAGE {8} FINISH SAMPLE]</p>"},{"location":"Articles/Scalar/Implementing%20DCGAN%20to%20generate%20CIFAR%20images/#whats-next","title":"What's next","text":"<p>We can apply a few tweaks to the DCGAN in further experiments. Some of them are mentioned below. - Tweaking the z_size variable and either increasing or decreasing it might lead to better performance. - Longer training might also lead to better results. - Using Label Smoothing Cross Entropy Loss instead of just a Cross Entropy Loss might also improve performance. - There is a long list of tweaks proposed by one of the creators of the PyTorch library regarding the DCGAN. The article mentioned is quite a few years old but gives a good background for further experiments on the DCGAN image generation process.</p>"},{"location":"Articles/Scalar/Implementing%20DCGAN%20to%20generate%20CIFAR%20images/#conclusion","title":"Conclusion","text":"<ul> <li>A DCGAN was built using PyTorch to generate images from the CIFAR10 dataset.</li> <li>The DCGAN is a modified version of a Vanilla GAN that addresses some issues and leads to fewer chances of mode collapse.</li> <li>Suggestions for further experimentation with the DCGAN include adjusting the z_size variable, increasing training time, and using Label Smoothing Cross Entropy Loss.</li> </ul>"},{"location":"Articles/Scalar/Improving%20Model%20Accuracy%20in%20Image%20Classification/","title":"Improving Model Accuracy in Image Classification","text":"<p>toc: true title: Improving Model Accuracy in Image Classification</p> <p>categories: [\"article\"] date modified:  date created: </p>"},{"location":"Articles/Scalar/Improving%20Model%20Accuracy%20in%20Image%20Classification/#improving-model-accuracy-in-image-classification","title":"Improving Model Accuracy in Image Classification","text":"<p>:::section{.abstract}</p>"},{"location":"Articles/Scalar/Improving%20Model%20Accuracy%20in%20Image%20Classification/#overview","title":"Overview","text":"<p>Improving image classification accuracy is one of the biggest hurdles in deep learning. Apart from using a deeper network and better data, many techniques have been developed to optimize network performance. Some techniques, such as [Dropout], target training bottlenecks in the architecture itself, while others, like ../../Regularization|Regularization.md|../../Dropout|Dropout, are more focused on improving the overall pipeline.  ::: :::section{.scope}</p>"},{"location":"Articles/Scalar/Improving%20Model%20Accuracy%20in%20Image%20Classification/#scope","title":"Scope","text":"<ul> <li>This article explains the concepts of Overfitting and Underfitting.</li> <li>Dropout and other Regularization.md|../../Regularization|Regularization techniques like Data Augmentation are explained.</li> <li>The article also explains Early Stopping and other pipeline tweaks such as Hyperparameter Tuning and Transfer Learning.</li> </ul> <p>::: :::section{.main}</p>"},{"location":"Articles/Scalar/Improving%20Model%20Accuracy%20in%20Image%20Classification/#introduction","title":"Introduction","text":"<p>It is often impossible to always have better data or larger models. In such cases, using techniques like [Regularization] and Transfer Learning not only optimize training time but also care for the lack of data. Algorithms such as ../../Dropout|Dropout.md|../../Regularization|Regularization and Early Stopping tackle the challenges of Overfitting.  This article provides an introduction to many such algorithms and pipeline tweaks that help in the process of improving model accuracy in image classification.</p>"},{"location":"Articles/Scalar/Improving%20Model%20Accuracy%20in%20Image%20Classification/#improving-model-accuracy","title":"Improving Model Accuracy","text":"<p>The two biggest hurdles in training neural networks are Overfitting and Underfitting. In the first case, the network memories the data, and in the second, the network does not learn enough. The following techniques can be divided into categories based on these two concepts. [Dropout] layers, Data augmentation, ../../Regularization|Regularization.md|../../Dropout|Dropout, Early Stopping, tackle Overfitting.  Transfer Learning and Hyperparameter Tuning tackle Underfitting. If there is a lack of data, we can use Transfer learning and Data Augmentation. The other algorithms can be experimented with if the model does not perform well. The below sections explain all of these algorithms.</p>"},{"location":"Articles/Scalar/Improving%20Model%20Accuracy%20in%20Image%20Classification/#overfitting-and-underfitting","title":"Overfitting and Underfitting","text":"<p>Training a neural network is a balancing act that requires understanding many metrics.  Training accuracy is defined as the model's accuracy during training time on the train/test split of the data.  Validation accuracy is defined as the model's performance when tested on real-world data that the model has never seen. If, in improving image classification accuracy, the training accuracy is far higher than the validation accuracy for many epochs, this is called Overfitting. In Overfitting, the model focuses too heavily on the training data and essentially fails to predict any sample it has yet to see before. </p> <p>If the training accuracy is very low and the validation accuracy seems to fluctuate or is much higher than the training accuracy, this is called Underfitting. In Underfitting, the model must be more powerful to fit the data.  Both Overfitting and Underfitting can be countered in many ways, but it is to be noted that they have a delicate balance. Learning to understand which of these the network is going through is essential in being able to improve image classification accuracy.</p> <p>[IMAGE {1} Overfitting_Underfitting START SAMPLE]  [IMAGE {1} FINISH SAMPLE]</p>"},{"location":"Articles/Scalar/Improving%20Model%20Accuracy%20in%20Image%20Classification/#dropoutmddropoutdropout-layers","title":"Dropout.md|../../Dropout|Dropout layers","text":"<p>When the single unit in a network computes gradients wrt the error, it also considers the other units and tries to fix their mistakes. This dependency is known as Co Adaptation and leads to the formation of complex relations that encourages Overfitting. Dropout.md|../../Dropout|Dropout layers reduce co-dependence between the neurons in a network by randomly (with a probability p) setting neuron activations to 0. This layer is applied to Dense (Fully connected) layers in a network. [Dropout] helps with smaller datasets and slightly with larger ones. If the dataset is bigger, ../../Dropout|Dropout.md|../../Dropout|Dropout can help performance as more information is recovered. Similarly, if the dataset is too large, the model performance might also worsen. During testing, the weights are scaled by the probability p.</p> <p>[IMAGE {2} Dropout START SAMPLE]  [IMAGE {2} FINISH SAMPLE]</p>"},{"location":"Articles/Scalar/Improving%20Model%20Accuracy%20in%20Image%20Classification/#data-augmentation","title":"Data Augmentation","text":"<p>Neural networks are extremely data-hungry, and training them requires many training examples. It is, of course, only sometimes possible to have a large amount of training data. We can use a method called Data Augmentation to artificially expand the number of available examples. In essence, Data Augmentation is the process of tweaking the given examples multiple times in different ways to generate new training samples from the existing images. Some examples of Data Augmentation for image data include Random Flipping, Jittering Brightness/Contrast, Random Resizing, and Random Cropping.md|../../Cropping|Cropping. Some Data Augmentations are shown below.</p> <p>[IMAGE {3} Augmentation START SAMPLE]  [IMAGE {3} FINISH SAMPLE]</p> <p>Data augmentation is a good method for improving image classification accuracy. This technique is not restricted to images; we can apply similar concepts to every other data domain. Data Augmentation also has the added benefit of being a regularizer by showing the model data from different perspectives.</p>"},{"location":"Articles/Scalar/Improving%20Model%20Accuracy%20in%20Image%20Classification/#regularizationmdregularizationregularization","title":"Regularization.md|../../Regularization|Regularization","text":"<p>One of the biggest challenges neural networks face during training is Overfitting. Penalizing complex models that have better performance during training but not during validation is one way of reducing the effects of Overfitting. The objective of training neural networks is for them to be used on real data outside the training set. Penalizing models that learn too much of the training set is called [Regularization]. A ../../Regularization|regularization.md|../../Regularization|Regularization term is used to control the penalty applied to the model. This term is also a hyperparameter, as increasing it too much may hurt model performance.  Many algorithms perform [Regularization] during training, such as Data Augmentation, Early Stopping, ../../Dropout|Dropout.md|../../Regularization|Regularization, etc.</p>"},{"location":"Articles/Scalar/Improving%20Model%20Accuracy%20in%20Image%20Classification/#early-stopping","title":"Early Stopping","text":"<p>Early Stopping is a regularization.md|../../Regularization|regularization technique that improves image classification accuracy by intentionally stopping the training when validation loss increases. Training is stopped as training a model for too many epochs sometimes causes Overfitting. In Early Stopping, the number of epochs becomes a tunable hyperparameter. We continuously store the best parameters during training, and when these parameters no longer change for several epochs, we stop training.  The idea of Early Stopping can be seen in this diagram. [IMAGE {4} Early Stopping START SAMPLE]  [IMAGE {4} FINISH SAMPLE]</p>"},{"location":"Articles/Scalar/Improving%20Model%20Accuracy%20in%20Image%20Classification/#transfer-learning","title":"Transfer Learning","text":"<p>Training large-scale image models are time and energy-consuming. Since most vision datasets have some common features, it is possible to take a network trained on a similar dataset and use the trained features to reduce training time on a different dataset.  Transfer learning is a procedure that lets a pre-trained model be used either as a feature extractor or as a weight initializer. In most cases, Transfer learning is used for fine-tuning. We can transfer knowledge from a network trained on a complex task to a simpler one or from a network trained on large amounts of data to one with fewer data.  Transfer learning is thus a potential key to multi-task learning, an active field of research in deep learning. This technique is also key in quickly improving image classification accuracy with fewer data. The following diagram shows the concept behind using Transfer learning to improve image classification accuracy.</p>"},{"location":"Articles/Scalar/Improving%20Model%20Accuracy%20in%20Image%20Classification/#hyperparameter-tuning","title":"Hyperparameter tuning","text":"<p>Every DL model and training pipeline has parameters we can tune to optimize performance. Parameters can include - how many epochs to train the network, weight decay, optimizers, learning rate, and a lot more. Each hyperparameter can have multiple values, and these quickly add up to hundreds or more different cases to try.  Hyperparameter tuning is the art of tweaking these parameters to create an optimal model in the shortest amount of time. We can test only some parameters, but a tuning service can estimate which hyperparameter to keep and which to discard. Many algorithms enable such a service, one of them being a grid search over the hyperparameter space. If the hyperparameter in question reduces model performance, it is dropped, and sometimes similar hyperparameters are also dropped.  Hyperparameter tuning is a challenging problem as every task requires different requirements. Tuning hundreds of parameters is a balancing act between the choices.  This technique is one of the final bits of the pipeline that leads to improving image classification accuracy. </p> <p>::: :::section{.summary}</p>"},{"location":"Articles/Scalar/Improving%20Model%20Accuracy%20in%20Image%20Classification/#conclusion","title":"Conclusion","text":"<ul> <li>In this article, we learned the importance of other algorithms in improving image classification accuracy.</li> <li>We looked at the concepts of Overfitting and Underfitting and understood how they affect model training.</li> <li>We also looked at many algorithms that improve performance by modifying the architecture or changing how we train the network.</li> <li>We understood what techniques to use when we lack data.</li> <li>We also tackled improving the existing model's performance by tuning its hyperparameters. :::</li> </ul>"},{"location":"Articles/Scalar/Intro%20to%20Conditional%20GANS/","title":"Intro to Conditional GANS","text":"<p>toc: true title: Intro to Conditional GANS</p> <p>categories: [\"article\"] date modified:  date created: </p>"},{"location":"Articles/Scalar/Intro%20to%20Conditional%20GANS/#introduction-to-conditional-gans","title":"Introduction to Conditional GANs","text":"<p>:::section{.abstract}</p>"},{"location":"Articles/Scalar/Intro%20to%20Conditional%20GANS/#overview","title":"Overview","text":"<p>Generating novel images from an image dataset has been a dream in Computer Vision. Being able to influence the generation of these images was made possible by a family of GANs named Conditional GANs. The following article explores these CGANs and shows how the DCGAN was modified to have the ability to control latent space traversal to a certain extent. We also look at the training paradigm and cover some challenges we might encounter while training a CGAN on our data. ::: :::section{.scope}</p>"},{"location":"Articles/Scalar/Intro%20to%20Conditional%20GANS/#scope","title":"Scope","text":"<ul> <li>This article provides an overview of Conditional GANs (CGANs) and their ability to influence the generation of images.</li> <li>It talks about the differences between CGANs and normal GANs.</li> <li>It provides an explanation of the architecture of a CGAN and the added components for controlling feature generation.</li> <li>It discusses the loss functions and the training paradigm used in CGANs.</li> <li>It provides an overview of challenges that may arise when training a CGAN on specific data and how to potentially overcome them. ::: :::section{.main}</li> </ul>"},{"location":"Articles/Scalar/Intro%20to%20Conditional%20GANS/#pre-requisites","title":"Pre-requisites","text":"<p>Before understanding CGANs, we need to understand some concepts.</p>"},{"location":"Articles/Scalar/Intro%20to%20Conditional%20GANS/#z-space","title":"Z Space","text":"<p>Consider the task of generating new faces from a large dataset of faces. To create a new face, we can take an average of all the faces. Now, to create a new face, we need to tweak the average face a little. We could make the nose a little longer, the mouth smaller and so on. Our average face would also have the same features as human faces but be in between all the faces.  These transformations can be thought of as a kind of interpolation. In essence, we consider the faces as vectors and instead of moving from one to the other, we move to an intermediate point in between. We get new faces depending on which \"direction\" we choose to interpolate between these face vectors.  This transformation enables CGANs to have more control over what they generate.</p>"},{"location":"Articles/Scalar/Intro%20to%20Conditional%20GANS/#introduction","title":"Introduction","text":"<p>CGANs are a variant of GANs that allow for greater control over the features of the generated images. They build upon the DCGAN architecture, which is a popular architecture for GANs, and have a similar architecture with some small differences. The main difference between a CGAN and a DCGAN is the ability of a CGAN to control which features are modified in the generated images. This is achieved by conditioning the generator with additional information, such as labels, during the training process.</p> <p>This added complexity allows the CGAN to generate images with specific features, such as generating a specific type of fruit with certain characteristics. By providing the generator with more information, it can create a more diverse set of images while still preserving the features that we want. Additionally, the CGAN has a better ability to generalize to unseen data, which is an essential aspect when it comes to image generation tasks. Overall, the CGAN architecture is an important step forward in the field of GANs, as it allows for more control over the generation process and improves the quality of the generated images.</p> <p>This article takes an in-depth look at CGANs. </p>"},{"location":"Articles/Scalar/Intro%20to%20Conditional%20GANS/#what-is-a-conditional-gan","title":"What is a conditional GAN?","text":"<p>A Conditional GAN (CGAN) is a modification of the DCGAN architecture that allows for more control over the features of the generated images. Unlike the DCGAN, a CGAN can selectively modify certain features of the generated image by conditioning the generator with additional information, such as labels, during training. This input allows the generator to generate images consistent with the conditioning information and specific characteristics that the labels represent. The differences between a CGAN and a DCGAN come from changes to the training methodology. In a CGAN, the generator is guided by the labels during training to traverse the latent space, the lower-dimensional space of random noise input that the generator uses to create images. These labels give the CGAN more control over what it generates by allowing it to traverse the latent space more precisely and be guided by the labels.</p> <p>An example would include choosing specific features of the face to modify.</p> <p>[IMAGE {0} { CGAN Example } START SAMPLE]  [IMAGE {0} FINISH SAMPLE]</p>"},{"location":"Articles/Scalar/Intro%20to%20Conditional%20GANS/#how-is-it-different-from-gan","title":"How is it different from GAN","text":"<p>There are a few significant differences between a GAN and a CGAN. This article compares the CGAN to the DCGAN, as the latter is the base for many advanced GANs. These differences are summarized in the following table.</p> GAN CGAN Output features are not controllable Output features can be controlled Unsupervised Semi-Supervised Discriminator does not receive labels Discriminator requires labels Discriminator evaluates similarity between input and target images Discriminator considers input and target images and their respective labels"},{"location":"Articles/Scalar/Intro%20to%20Conditional%20GANS/#architecture","title":"Architecture","text":"<p>The overall architecture of a CGAN is similar to that of a DCGAN but with minor modifications. The Discriminator architecture in a CGAN is similar to that of a DCGAN, consisting of several convolutional layers, batch normalization layers, and Leaky ReLU activation functions. However, in a CGAN, the Discriminator receives an additional input, the conditioning information, such as labels, and the generated image. This extra input allows the Discriminator to consider both the image's realism and the consistency of the conditioning information when evaluating the generated image. The Generator architecture in a CGAN is similar to that of a DCGAN, consisting of several transpose convolutional layers, batch normalization layers, and ReLU activation functions. However, in a CGAN, the Generator receives an additional input, the conditioning information, and the random noise used as the latent code. This input allows the Generator to generate images consistent with the conditioning information.</p> <p>[IMAGE {1} { CGAN Architecture } START SAMPLE]  [IMAGE {1} FINISH SAMPLE]</p>"},{"location":"Articles/Scalar/Intro%20to%20Conditional%20GANS/#the-discriminators-network","title":"The Discriminator's network","text":"<p>The Discriminator in a CGAN has a similar architecture as the DCGAN, consisting of several convolutional layers, batch normalization layers, and Leaky ReLU activation functions. However, an additional hot-encoding layer is added to consider the current image's labels. This layer is used to represent the conditioning information, such as the labels, in a format that the network can process. This addition allows the Discriminator to consider both the image's realism and the consistency of the conditioning information when evaluating the generated image. Thus it can guide the generator to create images with specific characteristics.</p> <p>[IMAGE {2} { Discriminator Architecture } START SAMPLE]  [IMAGE {2} FINISH SAMPLE]</p>"},{"location":"Articles/Scalar/Intro%20to%20Conditional%20GANS/#the-generators-network","title":"The Generator's network","text":"<p>The Generator in a CGAN also has a similar architecture as the DCGAN, consisting of several transposed convolutional layers, batch normalization layers, and ReLU activation functions. However, it also has an additional layer added to take into consideration the labels. This added layer helps the generator create images with certain characteristics and make them more consistent with the conditioning information.</p>"},{"location":"Articles/Scalar/Intro%20to%20Conditional%20GANS/#loss-functions","title":"Loss functions","text":"<p>To train the network, we use two loss functions for the Generator and the Discriminator of the CGAN, respectively.</p>"},{"location":"Articles/Scalar/Intro%20to%20Conditional%20GANS/#generator-loss","title":"Generator Loss","text":"<p>Since the Generator's objective is to create better fake images gradually, it needs to minimize the difference between the predicted image and the target. The model uses one hot.md|../../One hot|one hot encoded label in this architecture to decide which features to care for. The loss function thus is the following.</p> \\[\\mathcal{L}^{(G)}(\\theta^{(G)}, \\theta^{(D)}) = - \\mathbb{E}_{z} log \\mathcal{D} (\\mathcal{G} (z|y\u2019))\\] <p>The equation can be thought of as: given a label, use the Generator to traverse the latent space and create an image. Then pass the created image through the Discriminator and find how well we performed. </p>"},{"location":"Articles/Scalar/Intro%20to%20Conditional%20GANS/#discriminator-loss","title":"Discriminator Loss","text":"<p>Since the objective of the Discriminator is to label the generated images, its outputs are the probability that the image is true. The loss function is, therefore, the following. (This is a Binary Cross Entropy Loss function)</p> \\[ \\mathcal{L}^{(D)}(\\theta^{(G)}, \\theta^{(D)})= - \\mathbb{E}_{x \\sim p_{data}}log \\mathcal{D}(x|y) - \\mathbb{E}_{z} log (1- \\mathcal{D}(\\mathcal{G}(z|y')))\\]"},{"location":"Articles/Scalar/Intro%20to%20Conditional%20GANS/#training","title":"Training","text":"<p>Training a CGAN is similar to training any other GAN. The Discriminator and the Generator work parallel to create novel images and identify how real they look. The Generator first starts with random noise and passes it to the Discriminator. The Discriminator, in this case, is also provided with labels and returns a probability of how real it thinks the generated image is. This probability is passed to the Generator, which updates its weights to generate better images slowly. This cycle continues until the required quality of images is generated.</p>"},{"location":"Articles/Scalar/Intro%20to%20Conditional%20GANS/#training-flow","title":"Training Flow","text":"<p>Training a CGAN is similar to training any other GAN, the main difference being the use of conditioning information to guide the network in generating specific features. We can break down the process into several steps:</p> <ul> <li>The Generator starts with a random noise, often called a latent code, as its input. This noise is passed through the generator network, which maps it to an image in the data space.</li> <li>The generated image and its corresponding conditioning information, such as labels, are passed to the Discriminator. The Discriminator evaluates the image based on its similarity to the real images and the consistency of the conditioning information. It returns a probability of how realistic the generated image is.</li> <li>The probability is passed back to the Generator, which uses it to update its weights to generate better images. - The weights are updated to minimize the difference between the generated and real images.</li> <li>This process is repeated for several iterations until the Generator can produce images that are of the desired quality. During this process, the Discriminator and Generator improve each other, with the Discriminator getting better at identifying realistic images and the Generator getting better at producing realistic images.</li> </ul> <p>One important thing to note is that, during the training process, we must provide the Discriminator with real images and the conditioning information. The additional information allows the Discriminator to make informed decisions about the realism of the generated images and to detect if the conditioning information is consistent with the generated image.</p>"},{"location":"Articles/Scalar/Intro%20to%20Conditional%20GANS/#challenges-with-conditional-generation","title":"Challenges with Conditional generation","text":"<p>No model is perfect. CGANs have multiple issues as well. Some of these have been tackled by later research, while others are still active research areas. </p> <p>[IMAGE {3} { Discriminator Training Flow } START SAMPLE]  [IMAGE {3} FINISH SAMPLE]</p> <p>[IMAGE {4} { Generator Training Flow } START SAMPLE]  [IMAGE {4} FINISH SAMPLE]</p>"},{"location":"Articles/Scalar/Intro%20to%20Conditional%20GANS/#feature-correlation","title":"Feature Correlation","text":"<p>Another challenge with CGANs is related to the correlation of features in the dataset, which can cause the generated images to be biased toward certain characteristics. For instance, when working with an image dataset of fruits, if the dataset contains images of mostly red apples, the output of the CGAN will also be biased toward generating red apples. Similarly, if the dataset contains mostly pears with spots, the CGAN will generate images of pears with spots.</p> <p>This feature correlation can cause problems when we want to generate an image of a fruit with certain characteristics that do not align with the features of the images in the dataset. For example, if we want to generate an apple with the spots of a pear, the CGAN might modify the entire apple instead of just the spots because these features are tightly linked in the dataset. This problem can make generating images with specific characteristics difficult or even impossible.</p> <p>One solution to this problem is to use larger and more diverse datasets that contain a wide range of images with various features. These datasets will help reduce feature correlation and increase the range of images the CGAN can generate. Pre-processing, such as data augmentation or removing correlation between features, might also help mitigate this issue.</p>"},{"location":"Articles/Scalar/Intro%20to%20Conditional%20GANS/#z-space-entanglement","title":"Z-Space Entanglement","text":"<p>The Z-Space is the space of all possible points where a generator could generate an image. In other words, it is the space of all possible random inputs to the generator.</p> <p>When a generator generates an image, it takes a point from the Z-space and maps it to an image in the data space. The idea is that the generator learns a mapping from the latent space to the data space to generate new images by sampling from the latent space. However, this process is not always straightforward, as the latent space may need to be better structured or may not be entirely controllable.</p> <p>One of the challenges of working with the latent space is that it is often high-dimensional, which makes it difficult to visualize and understand. Additionally, the space may not be smoothly connected, resulting in \"entanglement,\" where the generator produces unexpected results when the latent space is traversed. Entanglement occurs when interpolation between examples becomes hard to perform and can lead to difficulties in controlling the generation process, making it difficult to generate specific images.</p>"},{"location":"Articles/Scalar/Intro%20to%20Conditional%20GANS/#classifier-gradients","title":"Classifier Gradients","text":"<p>Another challenge with CGANs is related to the ability of the network to identify and modify specific features in the generated images. For the CGAN to make changes in a specific way, it must first be able to understand and interpret what we want it to modify. For example, in the case of generating fruits, if we want to generate an image of a fruit with a longer stem, the network must first be able to understand what a stem is and what it means for it to be longer.</p> <p>This understanding can be difficult for the network because the feature we want to modify may not always be clearly defined, or the network may need more information to understand. In some cases, the network may generate images different from what we want because it may interpret our request differently or need more information to make the desired change. Additionally, the feature we want to change might be harder to identify in the images. Hence the network might need help to learn it.</p> <p>One solution to this problem is to use more explicit and detailed conditioning information, such as annotations or labels, to guide the network in identifying and modifying specific features. For example, in the case of generating fruits, we could provide the network with detailed annotation data on the fruits, including information on the size, shape, and position of the stem. However, this approach requires a large amount of labeled data, which may only sometimes be available or can be time-consuming to acquire.</p> <p>Overall, being able to modify specific features in the generated images is a challenging task for CGANs and requires careful consideration of the available data and the network architecture used.</p> <p>::: :::section{.summary}</p>"},{"location":"Articles/Scalar/Intro%20to%20Conditional%20GANS/#conclusion","title":"Conclusion","text":"<ul> <li>In conclusion, this article discussed Conditional GANs (CGANs), a family of GANs that allow for control over the features modified in the generated image.</li> <li>The CGAN builds upon the DCGAN architecture and mainly differs in its training methodology, which allows for traversing the latent space by conditioning the generator with labels during training. </li> <li>The article also compared CGANs to DCGANs, highlighting the key differences between the two, such as the output feature control and the semi-supervised nature of CGANs. </li> <li>The article also provided an overview of CGAN architecture and the challenges one may face while training a CGAN. </li> <li>Overall, the article offers a comprehensive introduction to the concept and application of Conditional GANs, making it a valuable resource for anyone interested in this area of computer vision. :::</li> </ul>"},{"location":"Articles/Scalar/Masked%20Language%20Modeling%20in%20BERT%20%23scalar/","title":"Masked Language Modeling in BERT #scalar","text":"<p>toc: true title: Masked Language Modeling in BERT #scalar</p> <p>categories: [\"article\"] date modified:  date created: </p>"},{"location":"Articles/Scalar/Masked%20Language%20Modeling%20in%20BERT%20%23scalar/#masked-language-modelingmdmasked-language-modelingmasked-language-modeling-in-bert-scalar","title":"Masked Language Modeling.md|../../Masked Language Modeling|Masked Language Modeling in BERT #scalar","text":"<p>masked language model explained</p> <p>:::section{.abstract}</p>"},{"location":"Articles/Scalar/Masked%20Language%20Modeling%20in%20BERT%20%23scalar/#overview","title":"Overview","text":"<p>Language modelling is a massive domain and has many sub-research areas. One such domain involves understanding contextual information about words in a sentence. This task can be done in many ways, and masked language modelling is one such method. In the past few years, Transformer based models have reached SOTA(state of the art) in many NLP domains. BERT is one such model. In this article, we will understand how to train BERT to fill in missing words in a sentence using MLM.</p> <p>::: :::section{.scope}</p>"},{"location":"Articles/Scalar/Masked%20Language%20Modeling%20in%20BERT%20%23scalar/#scope-of-the-article","title":"Scope of the Article","text":"<p>This article covers the following topics: - Masked Language Modeling.md|../../Masked Language Modeling|Masked Language Modeling explained in an easy-to-understand manner - Quickly recap all the pre-requisite terms to build an MLM model - Go over some libraries that are essential to MLM - How to build a BERT-based MLM model using our data in Tensorflow/Keras</p> <p>::: :::section{.main}</p>"},{"location":"Articles/Scalar/Masked%20Language%20Modeling%20in%20BERT%20%23scalar/#masked-language-modelingmdmasked-language-modelingmasked-language-modeling-explained","title":"Masked Language Modeling.md|../../Masked Language Modeling|Masked Language Modeling Explained","text":"<p>To a NN model, the word context has no meaning. So, we need to find ways to make the model consider surrounding words to learn which context words appear. For example, consider the sentence, \"I am eating an ice cream\". In this, the \"ice cream\" is being \"eaten\". What would an appropriate word be if we now remove the word \"eating\" and have the sentence as \"I am ___ an ice cream\"? We can consider something along the lines of \"licking\", \"eating\", \"sharing\", etc. However, we cannot say \"drowning\", \"cycle\", \"chair\", or other random words.</p> <p>In the same way, to ensure the model learns which word is appropriate, it needs to understand the structure of language. As modellers, we need to help it do so.  Quite simply, all we do is give the model inputs with blanks as a \"token\" <code>&lt;MASK&gt;</code> along with the word that should be there. We can create data in this format by taking any text and running over it. How to do so will be explained later on.</p>"},{"location":"Articles/Scalar/Masked%20Language%20Modeling%20in%20BERT%20%23scalar/#quick-recap","title":"Quick Recap","text":"<p>This section gives a small recap of all the important concepts we need to understand the code.</p>"},{"location":"Articles/Scalar/Masked%20Language%20Modeling%20in%20BERT%20%23scalar/#tokens","title":"Tokens","text":"<p>Tokens can be considered parts of a sentence that contribute to understanding. A sentence such as \"I am Jane Austen\" can have the tokens <code>[\"I\", \"am\", \"Jane\", \"Austen\"]</code>. As a computer does not understand words, we need to convert these to numerical values. To do so, we give each word a unique ID. Something like <code>[100, 101, 102, 103]</code> etc.</p>"},{"location":"Articles/Scalar/Masked%20Language%20Modeling%20in%20BERT%20%23scalar/#attention","title":"Attention","text":"<p>One of the most important concepts in Machine learning today, attention mechanisms give models the power to decide where to look in an input. Compared to CNNs, this makes models such as Transformers perform much better as it simultaneously learns which parts of the input would be better along with what the input could be classified. </p>"},{"location":"Articles/Scalar/Masked%20Language%20Modeling%20in%20BERT%20%23scalar/#transformers","title":"Transformers","text":"<p>Transformers are a large family of models that employ different variants of attention mechanisms. They first started in NLP but have also branched out to computer vision due to their immense potential. They generally have an Encoder-Decoder architecture with multiple \"heads\". Each head is responsible for its attention.</p>"},{"location":"Articles/Scalar/Masked%20Language%20Modeling%20in%20BERT%20%23scalar/#bert","title":"BERT","text":"<p>BERT is part of a sub-family of transformer architectures made for NLP and led to a sea of other new models that reached SOTA. It can be used in many different contexts, especially MLM, next-word prediction, question answering and many more.</p>"},{"location":"Articles/Scalar/Masked%20Language%20Modeling%20in%20BERT%20%23scalar/#transfer-learning","title":"Transfer Learning","text":"<p>A training paradigm that changed the DL world by allowing any researcher with limited resources to use SOTA models for inference and fine-tune them to their needs. A model like BERT requires Terabytes of data before it can be trained to extremely high levels. This level of computing is almost impossible for most people to have apart from companies with massive funding and resources. However, now we can use their trained models and make them work for our tasks.</p>"},{"location":"Articles/Scalar/Masked%20Language%20Modeling%20in%20BERT%20%23scalar/#stopwords","title":"Stopwords","text":"<p>Words that do not add much value to the model but are repeated enough times for it to become a problem. These are generally removed before modelling.</p>"},{"location":"Articles/Scalar/Masked%20Language%20Modeling%20in%20BERT%20%23scalar/#mlm-vs-clm-vs-word2vec","title":"MLM vs CLM vs Word2Vec","text":"<p>The major difference between MLM and CLM is that CLM can only take into account words that occur before it in a sentence, unlike MLM, which is bi-directional. This difference means that CLM does better for generating large amounts of text. However, MLM is better for contextually understanding text (refer to the Masked Language Modeling.md|../../Masked Language Modeling|Masked Language Modeling Explained section). Word2Vec, on the other hand, has similar ideas but the embeddings it generates have weaker contextual information than Transformer based models. The outputs it produces can also be used as a part of BERT training, although it is not usually required.</p>"},{"location":"Articles/Scalar/Masked%20Language%20Modeling%20in%20BERT%20%23scalar/#libraries-we-need","title":"Libraries We Need","text":"<p>We need a few libraries to make this work.</p>"},{"location":"Articles/Scalar/Masked%20Language%20Modeling%20in%20BERT%20%23scalar/#tensorflowkeras","title":"Tensorflow/Keras","text":"<p>TF is one of the major DL libraries in Python. We will be using it for training our model.</p>"},{"location":"Articles/Scalar/Masked%20Language%20Modeling%20in%20BERT%20%23scalar/#hugging-face-transformers","title":"Hugging Face Transformers","text":"<p>This library is one of the recent developments in the open-source community. It is a database of trained models and datasets that can be directly used in any codebase. They have thousands of tasks, making it extremely easy to get results fast. We will use their pre-trained BERT model.</p>"},{"location":"Articles/Scalar/Masked%20Language%20Modeling%20in%20BERT%20%23scalar/#nltk","title":"NLTK","text":"<p>A text processing library that we will use to clean up our text before passing it to the model.</p>"},{"location":"Articles/Scalar/Masked%20Language%20Modeling%20in%20BERT%20%23scalar/#seabornmatplotlib","title":"Seaborn/Matplotlib","text":"<p>These libraries are used for plotting our training performance.</p>"},{"location":"Articles/Scalar/Masked%20Language%20Modeling%20in%20BERT%20%23scalar/#implementation","title":"Implementation","text":"<p>Now for the code.</p>"},{"location":"Articles/Scalar/Masked%20Language%20Modeling%20in%20BERT%20%23scalar/#imports","title":"Imports","text":"<p>We first import all the required packages. We also download the stopwords and punctuation data from nltk.</p> <p>\"`python import nltk nltk.download('stopwords') nltk.download('punkt') nltk.download('gutenberg') import string from nltk.corpus import stopwords import seaborn as sns</p> <p>from transformers import BertTokenizer, TFBertForMaskedLM import tensorflow as tf import numpy as np import re import matplotlib.pyplot as plt sns.set()</p> <pre><code>\n### Data\nFor this demo, we use text from the book \"Emma\" by \"Jane Austen\". This dataset is a public domain dataset from Project Gutenberg that comes with nltk. We can download it using the following code. \n\"`python\nimport nltk\nnltk.download('gutenberg')\n!cp /root/nltk_data/corpora/gutenberg/austen-emma.txt sample.txt\n</code></pre> <p>We can also use custom text by creating a text file called \"sample.txt\" in the same directory as the code and pasting whatever we want. (Make sure it is English text).</p> <p>We then pre-process the data by removing stopwords and punctuation and converting the words into tokens BERT needs. Since every Transformer model has their configuration of tokens in the pre-trained model, we will use the tokenizer that Hugging face provides us. </p> <p>(Note: Here, we only take the first 1000 lines from the text. Language models take a long time to train; this is just a demo. If we have GPUs, the entire data can be used.) \"`Python tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')</p> <p>sw = stopwords.words('english') # this is a list of stopwords</p>"},{"location":"Articles/Scalar/Masked%20Language%20Modeling%20in%20BERT%20%23scalar/#take-first-1000-sentences-for-demo-purposes","title":"Take first 1000 sentences for demo purposes","text":"<p>with open('sample.txt', 'r') as f:     lines = f.readlines()[:1000]</p>"},{"location":"Articles/Scalar/Masked%20Language%20Modeling%20in%20BERT%20%23scalar/#remove-new-lines-convert-all-to-lowercase-remove-punctuation-and-stop-words-and-tokenize","title":"Remove new lines, convert all to lowercase, remove punctuation and stop words and tokenize","text":"<p>lines = [line.rstrip('\\n').lower() for line in lines] lines = [line.translate(str.maketrans('', '', string.punctuation)) for line in lines]</p> <p>def rem_stops_line(line, words):     # Function to remove stopwords per line if they are present in the line     if len(line) &gt;1:         return [w for w in line if w not in words]     else:         return line</p> <p>def remove_stops(text, words = sw):     # Remove stop words for an entire text. Separate functions make it easier to parallelize if required.     return [rem_stops_line(line, words) for line in text]</p> <p>filtered_lines = remove_stops(text = lines, words = sw) inputs = tokenizer(lines,max_length=100,truncation=True,padding='max_length',return_tensors='tf')</p> <pre><code>\n### Masked Language Model Explained Further\nWe use the model from the Transformers library directly. The uncased model converts all text into lowercase. Other models do not, and any of them can be used. This one was chosen for the sake of this demo.\n\"`Python\nmodel = TFBertForMaskedLM.from_pretrained('bert-base-uncased')\n</code></pre> <p>In Masked Language Modeling.md|../../Masked Language Modeling|Masked Language Modeling, we explained that every sentence needs to be converted to a format with words masked using a special token, <code>&lt;MASK&gt;</code>. We can do that by using the tokenized words and making the model aware of which token number corresponds to this special token. (In this case, it is 103). In the original paper, token numbers 101 and 102 were replaced, but we ignore that here. (It is not relevant for now.)</p> <p>\"`Python</p>"},{"location":"Articles/Scalar/Masked%20Language%20Modeling%20in%20BERT%20%23scalar/#mask","title":"MASK","text":"<p>inp_ids = [] lbs = [] for idx, inp in enumerate(inputs.input_ids.numpy()):     current_tokens = list(set(range(100)) -                           set(np.where((inp == 101) | (inp == 102)                              | (inp == 0))[0].tolist()))     # Find number of tokens to mask     num_token_masked = 0.15 * len(current_tokens)     token_to_mask = np.random.choice(np.array(actual_tokens),                                       size=int(num_token_masked),                                       replace=False)     # Store special token and inform model     inp[token_to_mask.tolist()] = 103     inp_ids.append(inp)</p>"},{"location":"Articles/Scalar/Masked%20Language%20Modeling%20in%20BERT%20%23scalar/#convert-the-tokens-to-tensor-format","title":"Convert the tokens to tensor format","text":"<p>inp_ids = tf.convert_to_tensor(inp_ids) inputs['input_ids'] = inp_ids</p> <pre><code>\n### Training the Model\nNow that we have all the required data and the model, we need to train it on our data.\nIf the system does not have a GPU or access to a cloud GPU is unavailable, this model will take a very long time to train. Consider using lesser data.\n\nConsidering we have a GPU, we first check if TF can find it.\n\"`Python\nprint('GPU name: ', tf.config.experimental.list_physical_devices('GPU'))\n</code></pre> <p>We use a Sparse Categorical Crossentropy loss with logits enabled. (logits are enabled if the model does not end with a Softmax. BERT does not.). We use a learning rate of 1e-3 for the Adam Optimizer.  Finally, we run training for around ten epochs. \"`Python</p>"},{"location":"Articles/Scalar/Masked%20Language%20Modeling%20in%20BERT%20%23scalar/#compile-and-train","title":"Compile and Train","text":"<p>lr = 1e-3 loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),loss=loss) history = model.fit([inputs.input_ids,inputs.attention_mask],inputs.labels,verbose=1,batch_size=16,epochs=10)</p> <pre><code>\n### Loss\nWe plot the loss per epoch to see if our model is learning anything.\nWe can see that the loss has decreased, which is a good sign! Our model is learning. More data and longer training will help the model be better than before.\n\n### Prediction\nJust training a model is useless. We need to be able to use it for prediction. To do that, we need to define a few functions. We need to be able to find the `&lt;MASK&gt;` tokens in the sentence, we need to tokenize the sentence and pass it into the model for prediction. And finally, we need to do this for multiple sentences. \nThe following functions do exactly these. \n\n```python\ndef find_masks(inp):\n    # Find position of the masks in a sentence\n    return np.where(inp.input_ids.numpy()[0] == 103)[0].tolist()\n\ndef single_prediction(model, inp, mask_loc):\n    # Prediction for all the positions of the masks\n    return model(inp).logits[0].numpy()[mask_loc]\n\ndef return_prediction(model, query):\n    # Return a prediction for a single sentence\n    inp = tokenizer(query,return_tensors='tf')\n    mask_loc = find_masks(inp)\n    # Find prediction with the highest confidence\n    predicted_tokens = np.argmax(single_prediction(model, inp, mask_loc),axis=1).tolist()\n    # Decode the numerical value of the returned ID back to the word \n    return tokenizer.decode(predicted_tokens)\n\ndef multiple_preds(model, query_list):\n    # Return predictions for a list of queries\n    preds = [f\"{x} -&gt; {return_prediction(model, x).split(' ')}\" for x in query_list]\n    for i in preds: print(i)\n</code></pre>"},{"location":"Articles/Scalar/Masked%20Language%20Modeling%20in%20BERT%20%23scalar/#predictions","title":"Predictions","text":"<p>After training, we can finally give the model a practice exam! Since we fine-tuned it on the book \"Emma\", we give it the following sentences. <code>[\"too well [MASK] for her\", \"nice to [MASK] her\", \"Emma [MASK] a girl who wanted a [MASK]\"]</code></p> <p>\"`Python query_list = [\"too well [MASK] for her\", \"nice to [MASK] her\", \"Emma [MASK] a girl who wanted a [MASK]\"] multiple_preds(model, query_list)</p> <pre><code>\nAs an output we get the following. We see that it performs quite well even with such a short training! \n\n\"`plaintext\ntoo well [MASK] for her -&gt; ['suited']\nnice to [MASK] her -&gt; ['see']\nEmma [MASK] a girl who wanted a [MASK] -&gt; ['was', 'chance']\n</code></pre>"},{"location":"Articles/Scalar/Masked%20Language%20Modeling%20in%20BERT%20%23scalar/#further-tips","title":"Further Tips","text":"<ul> <li>Language models are generally very heavy to use. If possible, using Mixed Precision training helps</li> <li>Having more GPU memory is more useful than having a faster GPU for language models</li> <li>Multiple GPUs are useful for expanding available memory</li> <li>There are smaller variants of BERT that use less memory. </li> <li>Hugging Face has a huge list of models that we can use. Trying them might lead to better results.</li> <li>To get over the overwhelming number of pre-trained models, pick the task and find benchmarks in that task. PaperswithCode is a great place to start.</li> </ul> <p>::: :::section{.summary}</p>"},{"location":"Articles/Scalar/Masked%20Language%20Modeling%20in%20BERT%20%23scalar/#conclusion","title":"Conclusion","text":"<p>This article showed us masked language modelling explained. We learnt the following: - What MLM is and when to use it.  - How to pre-process our data for MLM - How to fine-tune pre-trained BERT models for MLM - How to perform predictions over multiple sentences with our fine-tuned models.</p> <p>:::</p>"},{"location":"Articles/Scalar/Reconstructing%20the%20MNIST%20images%20using%20an%20autoencoder/","title":"Reconstructing the MNIST images using an autoencoder","text":"<p>toc: true title: Reconstructing the MNIST images using an autoencoder</p> <p>categories: [\"article\"] date modified:  date created: </p>"},{"location":"Articles/Scalar/Reconstructing%20the%20MNIST%20images%20using%20an%20autoencoder/#reconstructing-mnist-images-using-an-autoencoder","title":"Reconstructing MNIST images using an Autoencoder","text":"<p>:::section{.abstract}</p>"},{"location":"Articles/Scalar/Reconstructing%20the%20MNIST%20images%20using%20an%20autoencoder/#overview","title":"Overview","text":"<p>Given noisy images, an Autoencoder is a family of models that can convert these noisy images to their original form. These models are unsupervised and use an Encoder-Decoder architecture to re-create the original images given noisy variants of the same. In the process of re-creation, the model also learns to model the latent space of the dataset. :::  :::section{.main}</p>"},{"location":"Articles/Scalar/Reconstructing%20the%20MNIST%20images%20using%20an%20autoencoder/#scope","title":"Scope","text":"<ul> <li>This article explores the concepts required to build a simple Autoencoder.</li> <li>It explains how to implement an Autoencoder using Tensorflow.</li> <li>It explains how to use an Autoencoder to re-create MNIST images.</li> <li>The article also explains how to train an Autoencoder model and use the trained model for inference.</li> </ul>"},{"location":"Articles/Scalar/Reconstructing%20the%20MNIST%20images%20using%20an%20autoencoder/#what-are-we-building","title":"What are we building?","text":"<p>In this article, we will build an Autoencoder in Tensorflow that can re-create MNIST images. We will create functions to load and pre-process the dataset, along with creating noisy versions of the data points. We will create the Encoder-Decoder structure of the Autoencoder and then use the noisy and real images as inputs to it. After training the model, we will use it to generate new images. The following sections elaborate on these points.</p>"},{"location":"Articles/Scalar/Reconstructing%20the%20MNIST%20images%20using%20an%20autoencoder/#pre-requisites","title":"Pre-requisites","text":"<p>Before moving to the implementation, we must understand some prerequisite terms.</p>"},{"location":"Articles/Scalar/Reconstructing%20the%20MNIST%20images%20using%20an%20autoencoder/#transposed-convolution","title":"Transposed Convolution","text":"<p>2D Convolutions compress information from images into smaller representations by downsampling.md|../../Downsampling|downsampling them. Transposed Convolutions perform the opposite operation. These convolutions take compressed/small images and attempt to expand their sizes. An illustration of how this happens is as follows.</p> <p>[IMAGE {1} Transposed Conv START SAMPLE]  [IMAGE {1} FINISH SAMPLE]</p>"},{"location":"Articles/Scalar/Reconstructing%20the%20MNIST%20images%20using%20an%20autoencoder/#what-are-autoencoders","title":"What are Autoencoders","text":"<p>Autoencoders are models that we created to reduce noise in images. In attempting to learn how to reduce noise, they model the latent space of the dataset. Any architecture that understands the latent space can then re-create the original forms of the images from the noisy variants. In the process, the model can not only act as an unsupervised classifier but also be used to generate new images. AutoEncoders have an Encoder-Decoder structure where the Encoder compresses the image while the Decoder re-creates the original image from the compressed representation. </p>"},{"location":"Articles/Scalar/Reconstructing%20the%20MNIST%20images%20using%20an%20autoencoder/#how-are-we-going-to-build-this","title":"How are we going to build this?","text":"<p>To build an autoencoder that can recreate MNIST images, we will be using the Tensorflow library. We will create functions to load the MNIST dataset and pre-process it. We will also need to create a random noise generator and a function to plot a batch of images. Once we have these functions, we can create the Autoencoder. The network structure follows an Encoder-Decoder pattern, and we will explore how to create that using Tensorflow. We will then train the Autoencoder on the MNIST data and use the trained model to re-create examples from the dataset.  The sections below elaborate on these steps.</p>"},{"location":"Articles/Scalar/Reconstructing%20the%20MNIST%20images%20using%20an%20autoencoder/#final-output","title":"Final Output","text":"<p>The final output of the model should be a near-perfect representation of the MNIST dataset. After training the model for a few epochs. The first row of images is the training data, while the second row contains the images generated by the Autoencoder. These rows are almost the same, which shows that our model has done a good job. [IMAGE {2} Final Output START SAMPLE]  [IMAGE {2} FINISH SAMPLE]</p>"},{"location":"Articles/Scalar/Reconstructing%20the%20MNIST%20images%20using%20an%20autoencoder/#requirements","title":"Requirements","text":"<p>We must import some libraries and write a few functions to create a model that can read and re-create mnist images.  Since we will be using the Tensorflow library, we import it and its other useful components. We import the numerical processing library numpy and also a plotting library matplotlib.</p> <pre><code>from tensorflow.keras.datasets import mnist\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras import layers\nimport numpy as np\nfrom tensorflow.keras.models import Model\nimport tensorflow as tf\n</code></pre> <p>We also need to write some helper functions. The first function takes an array as an input and reshapes it to the size that the model requires.</p> <pre><code>def data_proc(dat):\n    larr = len(dat)\n    return np.reshape(dat.astype(\"float32\") /255.0 , (larr, 28,28,1))\n</code></pre> <p>The second helper function is used to take an array, add Gaussian noise, and ensure that the values lie in the range (0,1).</p> <pre><code>def gen_noise(array):\n    noisy_array = array + 0.4 * np.random.normal(\n        loc=0.0, scale=1.0, size=array.shape\n    )\n    return np.clip(noisy_array, 0.0, 1.0)\n</code></pre> <p>To understand if our model is performing well, we will need to display batches of images. The third function takes two arrays - the input array and the predicted images array and plots them in two rows.</p> <pre><code>def display(array1, array2):\n    n = 10\n    indices = np.random.randint(len(array1), size=n)\n    images1 = array1[indices, :]\n    images2 = array2[indices, :]\n    plt.figure(figsize=(20, 4))\n    for i, (image1, image2) in enumerate(zip(images1, images2)):\n        ax = plt.subplot(2, n, i + 1)\n        plt.imshow(image1.reshape(28, 28))\n        plt.gray()\n        ax.get_xaxis().set_visible(False)\n        ax.get_yaxis().set_visible(False)\n        ax = plt.subplot(2, n, i + 1 + n)\n        plt.imshow(image2.reshape(28, 28))\n        plt.gray()\n        ax.get_xaxis().set_visible(False)\n        ax.get_yaxis().set_visible(False)\n    plt.show()\n</code></pre>"},{"location":"Articles/Scalar/Reconstructing%20the%20MNIST%20images%20using%20an%20autoencoder/#building-the-autoencoder","title":"Building the AutoEncoder","text":"<p>The following sections explain how to create a simple Autoencoder in Tensorflow and use MNIST images to train it. We will first see how to load the MNIST data and preprocess it for our needs. After converting the data to the right format, we will build and train the model. The architecture of the network is split into three major parts - the Encoder, the Bottleneck, and the Decoder. The Encoder is used to compress the input images while also preserving useful information. The Bottleneck chooses which features are relevant to flow through to the Decoder, and the Decoder re-creates the images using the outputs of the Bottleneck. The Autoencoder attempts to learn the latent space of the data in the process of this reconstruction.</p> <p>The architecture diagram of an autoencoder is shown below. [IMAGE {3} arch START SAMPLE]  [IMAGE {3} FINISH SAMPLE]</p>"},{"location":"Articles/Scalar/Reconstructing%20the%20MNIST%20images%20using%20an%20autoencoder/#preparing-the-dataset","title":"Preparing the dataset","text":"<p>The MNIST dataset is already included with Tensorflow as a split dataset so we can load it directly. We use the default splits into train and test datasets and then pass them to the pre-processing function we defined earlier. The second half of the inputs to the model are noisy versions of the original MNIST images. We use the gen_noise function we defined before to create these noise images. Note that the larger the noise, the more distorted the image gets, and the harder the model must work to re-create them. We will also visualize the noise data alongside the original. [IMAGE {4} Data Visualized START SAMPLE]  [IMAGE {4} FINISH SAMPLE]</p> <pre><code>(ds_train, _), (ds_test, _) = mnist.load_data()\nds_train,ds_test = data_proc(ds_train), data_proc(ds_test)\n\nnoisy_ds_train, noisy_ds_test = gen_noise(ds_train), gen_noise(ds_test)\n\ndisplay(ds_train, noisy_ds_train)\n</code></pre>"},{"location":"Articles/Scalar/Reconstructing%20the%20MNIST%20images%20using%20an%20autoencoder/#defining-the-encoder","title":"Defining the Encoder","text":"<p>The Encoder of the network uses blocks of Convolutions and Max Pooling layers with ReLU activations. The objective is to compress the input data before passing it through the network. The output of this part of the network should be a compressed version of the original data. Since the MNIST images are of shape 28x28x1, we create an input with that shape.</p> <pre><code>input = layers.Input(shape=(28, 28, 1))\n\n# Encoder\nx = layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(input)\nx = layers.MaxPooling2D((2, 2), padding=\"same\")(x)\nx = layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(x)\nx = layers.MaxPooling2D((2, 2), padding=\"same\")(x)\n</code></pre>"},{"location":"Articles/Scalar/Reconstructing%20the%20MNIST%20images%20using%20an%20autoencoder/#defining-the-bottleneck","title":"Defining the Bottleneck","text":"<p>Unlike the other two components, the Bottleneck does not need to be explicitly programmed. Because the output of the Encoder's final MaxPooling layer is very small, the Decoder must learn to recreate the images using this compressed representation.  In more complex implementations of Autoencoders, modifying the Bottleneck is also an option.</p>"},{"location":"Articles/Scalar/Reconstructing%20the%20MNIST%20images%20using%20an%20autoencoder/#defining-the-decoder","title":"Defining the Decoder","text":"<p>The Decoder comprises Transposed Convolutions with a stride of 2. The final layer of the model is a simple 2D convolution with the sigmoid activation function.  Since this part of the network is used to recreate images from the compressed representation, upsampling is done using the Transposed Convolution. Larger strides are used for upsampling the images in fewer steps.</p> <pre><code># Decoder\nx = layers.Conv2DTranspose(32, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\nx = layers.Conv2DTranspose(32, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\nx = layers.Conv2D(1, (3, 3), activation=\"sigmoid\", padding=\"same\")(x)\n</code></pre>"},{"location":"Articles/Scalar/Reconstructing%20the%20MNIST%20images%20using%20an%20autoencoder/#training-the-model","title":"Training the model","text":"<p>After defining the model, we must compile it with the Optimiser and the loss function. This article will use the Adam Optimiser and a Binary Cross Entropy loss function.</p> <pre><code># conv_autoenc_model\nconv_autoenc_model = Model(input, x)\nconv_autoenc_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\nconv_autoenc_model.summary()\n</code></pre> <p>After we compile the model, we can finally train it on the modified MNIST images we generated at the start of the article. We will train the model for 50 epochs with a batch size of 128. We also pass the validation data to the model. </p> <pre><code>conv_autoenc_model.fit(\n    x=ds_train,\n    y=ds_train,\n    epochs=50,\n    batch_size=128,\n    shuffle=True,\n    validation_data=(ds_test, ds_test),\n)\n</code></pre>"},{"location":"Articles/Scalar/Reconstructing%20the%20MNIST%20images%20using%20an%20autoencoder/#reconstructing-images","title":"Reconstructing images","text":"<p>After training the model, we can use the trained model to generate predictions. We display the re-created images using the function we wrote previously.</p> <pre><code>preds = conv_autoenc_model.predict(ds_test)\ndisplay(ds_test, preds)\n</code></pre> <p>::: :::section{.summary}</p>"},{"location":"Articles/Scalar/Reconstructing%20the%20MNIST%20images%20using%20an%20autoencoder/#conclusion","title":"Conclusion","text":"<ul> <li>In this article, we implemented a simple Autoencoder to re-create the MNIST image dataset.</li> <li>We learned how to load and pre-process the MNIST images to make them fit the Autoencoder model.</li> <li>We explored the architecture of the network and understood how to implement it using Tensorflow. </li> <li>Finally, we learned how to train the Autoencoder and use it to generate new images. :::</li> </ul>"},{"location":"Articles/Scalar/Siamese%20Network/","title":"Siamese Network","text":"<p>toc: true title: Siamese Network</p> <p>categories: [\"article\"] date modified:  date created: </p>"},{"location":"Articles/Scalar/Siamese%20Network/#siamese-network","title":"Siamese Network","text":"<p>:::section{.abstract}</p>"},{"location":"Articles/Scalar/Siamese%20Network/#overview","title":"Overview","text":"<p>The lack of data is a huge hurdle for any Deep learning task. Finding large datasets is nearly impossible in domains such as facial recognition, signature verification, text similarity, and many others. In many cases, a single example of each class is present. A regular CNN fails to perform in these cases, but if a network learns to minimize a similarity metric between images, it can easily perform this task. The Siamese Network is a class of architectures that excel at this one-shot learning task.  This article explains the workings behind the Siamese Network and provides an implementation for Signature Verification. ::: :::section{.scope}</p>"},{"location":"Articles/Scalar/Siamese%20Network/#scope","title":"Scope","text":"<ul> <li>This article explains the concept of Siamese Networks and why they are useful.</li> <li>It explains the multiple loss functions behind Siamese Networks.</li> <li>It talks about few-shot learning and how to apply it to tasks.</li> <li>It talks about some of the use cases of Siamese Networks.</li> <li>The article also explains how to create a Siamese Network for a Signature Verification task using PyTorch. ::: :::section{.main}</li> </ul>"},{"location":"Articles/Scalar/Siamese%20Network/#introduction","title":"Introduction","text":"<p>Siamese networks are a one-shot classification paradigm where only a single example is enough for the network to classify images accurately. This network uses the concept of Contrastive Loss, which finds a pairwise similarity score between the images in the Dataset. Instead of learning the content of the images, the Siamese network learns the differences and similarities between them. This unique learning paradigm makes these networks much more robust to the lack of data and improves performance without needing domain-specific information.</p> <p>Signature verification is a task in which these networks excel. This task aims to identify forged signatures given a single signature sample for thousands of people. This task is quite challenging due to the vast differences between individual signatures and the need for more training data. </p> <p>In this article, we will explore the task of Signature Verification using these Siamese Networks and create a working model using PyTorch.</p>"},{"location":"Articles/Scalar/Siamese%20Network/#what-are-siamese-networks","title":"What Are Siamese Networks?","text":"<p>Siamese Networks are a family of networks that uses two identical subnetworks for one-shot classification. The sub-networks share the same configuration, parameters, and weights but have different inputs. Unlike a regular CNN that learns to predict multiple classes using vast amounts of data, a Siamese Network learns a similarity function. We can use the learned function to differentiate between classes without needing a lot of data. These networks are specialized in one-shot classification, which means that in many cases, they only need a single example to classify images accurately.</p> <p>As a real-life use case, Siamese Networks are applied to face recognition and signature verification tasks. Consider the face recognition task done for a company that wants to take an automated face-based attendance. The company would only have a single picture of its employees. A regular CNN would have been incapable of accurately classifying thousands of employees based on a single image of each of them. A Siamese network, on the other hand, excels at this task.</p>"},{"location":"Articles/Scalar/Siamese%20Network/#exploring-few-shot-learning","title":"Exploring Few-Shot Learning","text":"<p>Few shot models are a family of architectures that rely not on the number of training examples but on exploiting the differences between a small number of samples. This property allows them to make predictions ranging from a few samples to a single sample. The advantage of few-shot learning comes into play when the training data is very small. For large datasets, this training paradigm could be more helpful.</p>"},{"location":"Articles/Scalar/Siamese%20Network/#architecture-of-siamese-networks","title":"Architecture of Siamese Networks","text":"<p>The objective of the Siamese network is to find similar inputs and magnify the differences between dissimilar pairs. The architecture of this network is shown in the figure below.</p> <p>[IMAGE {1} {Architecture} START SAMPLE]  [IMAGE {1} FINISH SAMPLE]</p> <p>Some features that set Siamese networks apart from the usual CNN architecture are as follows. - The network has two different inputs. Each of these inputs is passed into identical subnetworks. - The inputs are passed through a Convolutional network first and then encoded.  - Any changes to one side of the network are reflected on the other. - The network returns an encoding that is a similarity score. This score can be used to differentiate between classes.  - The network is a one-shot classifier and does not require a lot of examples per class. </p>"},{"location":"Articles/Scalar/Siamese%20Network/#loss-functions-used-in-siamese-networks","title":"Loss Functions Used in Siamese Networks","text":"<p>The Siamese Network uses multiple loss functions. They are explained below.</p>"},{"location":"Articles/Scalar/Siamese%20Network/#binary-cross-entropy-loss","title":"Binary Cross Entropy Loss","text":"<p>This loss is one of the common image classification loss functions. The Siamese network uses this loss to classify the image pairs as similar or dissimilar.</p>"},{"location":"Articles/Scalar/Siamese%20Network/#contrastive-loss","title":"Contrastive Loss","text":"<p>The Contrastive Loss function finds the difference between image pairs by using distance as a similarity measure. This function is useful when there are few training examples per class.  A caveat of using Contrastive loss is that it requires pairs of both negative and positive training samples. We can visualize this loss in the figure below.</p> <p>[IMAGE {2} {Contrastive Loss} START SAMPLE]  [IMAGE {2} FINISH SAMPLE]</p> <p>The Contrastive Loss equation is \\(\\((1-Y) \\frac{1}{2} D^{2}_{w} + (Y) + \\frac{1}{2} (max(0, m - D^{2}_{\\omega}))\\)\\) When Y is 0, the inputs share the same class. When the value of Y is 1, they are from different classes. The margin m defines the margin that the distance function uses to identify pairs that contribute to the loss. The value of m is always greater than 0. D denotes Euclidean distance.</p>"},{"location":"Articles/Scalar/Siamese%20Network/#triplet-loss","title":"Triplet Loss","text":"<p>The triplet loss uses triples of data. These triples can be seen in the image below. [IMAGE {3} {Triplet Loss} START SAMPLE]  [IMAGE {3} FINISH SAMPLE]</p> <p>The objective of the triplet loss function is to maximize the distance between the anchor and the negative samples while minimizing the distance between the anchor and the positive samples. This task is shown in the below image.</p> <p>The Triplet loss is defined as \\(\\(L = max(d(a,p) - d(a,n) + margin, 0)\\)\\)</p>"},{"location":"Articles/Scalar/Siamese%20Network/#building-a-signature-verification-model-with-siamese-networks","title":"Building a Signature Verification Model With Siamese Networks","text":"<p>Signature verification is the task of identifying forged signatures given a dataset of real ones. For this task, a model has to learn the difference between hundreds of signatures. Given a fake or a real signature, the model has to differentiate between them. This verification task is extremely hard for a regular CNN due to the complexity of changes and lack of training samples. In most cases, only a single signature is available per person, and the model needs to learn how to verify signatures for thousands of people. The following sections explore building a model to tackle this task using PyTorch.</p>"},{"location":"Articles/Scalar/Siamese%20Network/#dataset","title":"Dataset","text":"<p>The Dataset we will be using is a signature verification dataset known as ICDAR 2011. This Dataset contains Dutch signatures that are either forged or original. An example of the data is shown below.</p> <p>[IMAGE {4} {Examples} START SAMPLE]  [IMAGE {4} FINISH SAMPLE]</p> <p>We can download the Dataset from this drive link.</p>"},{"location":"Articles/Scalar/Siamese%20Network/#description-of-problem-statement","title":"Description of problem statement","text":"<p>This article considers recognizing fake signatures as part of a signature verification problem. We aim to take a dataset of signatures and use a Siamese network to predict which of the test signatures belong to real people and which are forged.  We need to create a pipeline that reads the Dataset, creates image pairs, and passes them to the Siamese network. After training the network on the Dataset, we need to create functions for inference.</p>"},{"location":"Articles/Scalar/Siamese%20Network/#imports","title":"Imports","text":"<p>To create the Siamese Network, we need to import a few libraries. We import the Pillow library(PIL) for image processing. We will import the plotting package matplotlib, the numerical library numpy, and the progress bar library tqdm for other utilities. We will use Pytorch and torchvisionto train and build the network.</p> <pre><code>from PIL import Image\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader, Dataset\nimport PIL.images\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.data as utils\nimport torchvision\nimport torchvision.transforms as transforms\nimport torchvision.utils\nfrom tqdm import tqdm\n</code></pre>"},{"location":"Articles/Scalar/Siamese%20Network/#utility-functions","title":"Utility Functions","text":"<p>To visualize the network's outputs, we create a function that takes the images and the labels as inputs and plots them in an easy-to-visualize grid.</p> <pre><code>def imshow(img, text=None, should_save=False):\n    npimg = img.numpy()\n    plt.axis(\"off\")\n    if text:\n        plt.text(\n            75,\n            8,\n            style= \"italic\",\n            fontweight= \"bold\",\n            bbox={\"face color\": \"white\", \"alpha\": 0.8, \"pad\": 10},\n        )\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n</code></pre>"},{"location":"Articles/Scalar/Siamese%20Network/#data-preprocessing","title":"Data preprocessing","text":"<p>The data structure used by the Siamese network is very different from the usual image classification networks. Instead of providing an image and a label, the Dataset Generator for the Siamese network must provide pairs of images. These pairs are converted to Black and white and are then resized and converted to Tensors.  There are two types of pairs - the positive pair, where both the inputs images are identical, and the negative pair, where the images are not identical. We also create a function that returns the size of the Dataset when called.</p> <pre><code>dir_train = \"train\"\ndf_train = \"train_data.csv\"\ndf_val = \"test_data.csv\"\ndir_val = \"test\"\nbs = 32\nnum_epoch = 20\n\nclass PairedDataset:\n    def __init__(self, df_train=None, dir_train=None, transform=None, load_subset = None):\n        self.train_df = PD.read_csv(df_train)\n        if load_subset!=None:\n            self.train_df = self.train_df[:load_subset]\n        self.train_df.columns = [\"image1\", \"image2\", \"label\"]\n        self.train_dir = dir_train\n        self.transform = transform\n\n    def __getitem__(self, index):\n        pair1 = os.path.join(self.train_dir, self.train_df.iat[index, 0])\n        pair2 = os.path.join(self.train_dir, self.train_df.iat[index, 1])\n\n        pair_left = Image.open(pair1).convert(\"L\")\n        pair_right = Image.open(pair2).convert(\"L\")\n\n        if self.transform is not None:\n            pair_left = self.transform(pair_left)\n            pair_right = self.transform(pair_right)\n\n        return (\n            pair_left,\n            pair_right,\n            torch.from_numpy(\n                np.array([int(self.train_df.iat[index, 2])], dtype=np.float32)\n            ),\n        )\n\n    def __len__(self):\n        return len(self.train_df)\n\n</code></pre>"},{"location":"Articles/Scalar/Siamese%20Network/#brief-description-of-the-features","title":"Brief Description of the features","text":"<p>The features that the network gets are pairs of images. There are positive or negative data pairs. Both the pairs are image data and are Tensor representations of the underlying images.  The labels provided to the Siamese network are categorical. </p>"},{"location":"Articles/Scalar/Siamese%20Network/#standardization-of-the-features","title":"Standardization of the features","text":"<p>To standardize the features, we first convert them to Black and White. We also resized all the images to be (105x105) square as the Siamese Network requires this size. Finally, we convert all the images to Tensors to improve performance and be able to use the GPU.</p> <pre><code>transform=transforms.Compose(\n        [transforms.Resize((105, 105)), transforms.ToTensor()]\n)\n</code></pre>"},{"location":"Articles/Scalar/Siamese%20Network/#splitting-the-dataset","title":"Splitting the Dataset","text":"<p>To ensure that the model can be used for prediction and not just training, we split the Dataset into training and testing parts.  For simplicity, we only use the first 1000 data points. Setting the load_subset function to None would use the entire Dataset but take much longer. We do not perform Data Augmentation here, but that is an option to make the network perform better in the long run.</p> <pre><code>train_ds = PairedDataset(\n    df_train,\n    dir_train,\n    transform=transforms.Compose(\n        [transforms.Resize((105, 105)), transforms.ToTensor()]\n    ),\n    load_subset=1000\n)\neval_ds = PairedDataset(\n    df_val,\n    dir_val,\n    transform=transforms.Compose(\n        [transforms.Resize((105, 105)), transforms.ToTensor()]\n    ),\n    load_subset=1000\n)\n</code></pre>"},{"location":"Articles/Scalar/Siamese%20Network/#neural-network-architecture","title":"Neural Network Architecture","text":"<p>We can create the architecture that we described above in a few steps.  First, we create a function that creates blocks of Convolutions, Batch Normalisation, and ReLU with different input and output channels. We give this function the option of having a Dropout.md|../../Dropout|Dropout layer at the end or skipping that layer. We also create another function that generates blocks of FC layers followed by ReLU layers. We can use these functions to create the Sequential model that defines the Siamese Network. After creating the CNN part of the architecture using the functions we defined earlier, we have to create the FC part of the network. Note that different padding and kernel sizes are used across the network.  The FC part of the network is blocks of Linear layers followed by ReLU activations.  Once we have defined the architecture, we can run a forward pass for the data we pass to the network. Note that the view function is used to resize the output of the previous block by flattening dimensions. After creating this function, we can start training the Siamese network on the data. </p> <pre><code>class Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n\n        self.cnn1 = nn.Sequential(\n            self.conv_bn_relu(1, 96, 11, 1, False),\n            self.conv_bn_relu(96, 256, 5, 2, True),\n            nn.Conv2d(256,384 , kernel_size=3,stride=1,padding=1),\n            nn.BatchNorm2d(384),\n            nn.ReLU(inplace=True),\n            self.conv_bn_relu(384, 256, 3, 1, True),\n        )\n\n        self.fc1 = nn.Sequential(\n            self.linrel(30976, 1024),\n            nn.Dropout2d(p=0.5),\n            self.linrel(1024, 128),\n            nn.Linear(128,2))\n\n    def linrel(self, inc, outc): return nn.Sequential(nn.Linear(inc, outc), nn.ReLU(inplace=True))\n\n    def conv_bn_relu(self,inc, outc, ks, pad,dropout = True):\n        if dropout == True:\n            return nn.Sequential(\n                nn.Conv2d(inc, outc, kernel_size=ks,stride=1,padding=pad),\n                nn.BatchNorm2d(outc),\n                nn.ReLU(inplace=True),\n                nn.MaxPool2d(3, stride=2),\n                nn.Dropout2d(p=0.3),\n            )\n        else:\n            return nn.Sequential(\n                nn.Conv2d(inc, outc, kernel_size=ks,stride=1),\n                nn.BatchNorm2d(outc),\n                nn.ReLU(inplace=True),\n                nn.MaxPool2d(3, stride=2),\n            )\n\n    def forward_once(self, x):\n        output = self.cnn1(x)\n        output = output.view(output.size()[0], -1)\n        output = self.fc1(output)\n        return output\n\n    def forward(self, input1, input2):\n        out1 = self.forward_once(input1)\n        out2 = self.forward_once(input2)\n        return out1, out2\n</code></pre>"},{"location":"Articles/Scalar/Siamese%20Network/#loss-function","title":"Loss Function","text":"<p>The loss function that the Siamese Network uses is contrastive loss. We can define this loss using the equations mentioned earlier in the article.  To improve code performance, instead of defining the loss as a simple function, we inherit from nn.Module and create a class that returns the outputs of the function. This wrapper will allow PyTorch to optimize the code for better runtime performance.</p> <pre><code>class ContrastiveLoss(nn.Module):\n    def __init__(self, margin=2.0):\n        super(ContrastiveLoss, self).__init__()\n        self.margin = margin\n\n    def forward(self, out1, out2, label):\n        euclidean_distance = F.pairwise_distance(out1, out2)\n        return torch.mean(\n            (1 - label) * torch.pow(euclidean_distance, 2)\n            + (label)\n            * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)\n        )\n</code></pre>"},{"location":"Articles/Scalar/Siamese%20Network/#training-the-siamese-network","title":"Training the Siamese Network","text":"<p>Now that we have loaded and cleaned up the data, we can start training the Siamese network using it. To do so, we first create the training and testing data loaders. Note that the evaluation DataLoader has a batch size of 1 as we want to perform one-by-one evaluations. We then send the model to the GPU and define the Contrastive Loss and the Adam optimizer.</p> <pre><code>dl_train = DataLoader(train_ds,\n                        shuffle=True,\n                        num_workers=8,\n                        batch_size=bs) \ndl_eval = DataLoader(eval_ds,\n                        shuffle=True,\n                        num_workers=8,\n                        batch_size=1) \n\nnet = Model().cuda()\ncriterion = ContrastiveLoss()\noptimizer = torch.optim.Adam(net.parameters(), lr=1e-3, weight_decay=0.0005)\n</code></pre> <p>We then write a function that takes the train DataLoader as an argument. We keep a running array of the loss and the counter to plot it later. After that, we iterate over the points in the DataLoader. For every point, we send the pairs to the GPU, run the pairs through the network, and calculate the Contrastive Loss. We can then perform the backward pass and return the net loss for a batch of data.</p> <pre><code>def train(dl_train):\n    loss=[] \n    counter=[]\n    iteration_number = 0\n    for i, data in tqdm(enumerate(dl_train,0), total = len(dl_train)):\n      pair_left, pair_right , label = data\n      pair_left, pair_right , label = pair_left.cuda(), pair_right.cuda() , label.cuda()\n      optimizer.zero_grad()\n      out1,out2 = net(pair_left,pair_right)\n      loss_contrastive = criterion(out1,out2,label)\n      loss_contrastive.backward()\n      optimizer.step()\n      loss.append(loss_contrastive.item())\n    loss = np.array(loss)\n    return loss.mean()/len(dl_train)\n\n</code></pre> <p>We can train the model for several epochs using the function we just created. This article only trains the model for a few epochs as a demo. If the evaluation loss is the best we have seen across the entire training period, we save the model for inference at that epoch.</p> <pre><code>for epoch in tqdm(range(1,num_epoch)):\n  best_eval_loss = 9999\n  train_loss = train(dl_train)\n  eval_loss = eval(dl_eval)\n\n  print(f\"Training loss {train_loss}\")\n  print(f\"Eval loss {eval_loss}\")\n\n  if eval_loss&lt;best_eval_loss:\n    best_eval_loss = eval_loss\n    print(f\"Best Eval loss{best_eval_loss}\")\n    torch.save(net.state_dict(), \"model.pth\")\n    print(\"Model Saved Successfully\") \n</code></pre> <p>[IMAGE {5} {Training} START SAMPLE]  [IMAGE {5} FINISH SAMPLE]</p>"},{"location":"Articles/Scalar/Siamese%20Network/#testing-the-model","title":"Testing the model","text":"<p>After training the model, we can evaluate it and run inference for a single data point. Similar to the training function, we create an evaluation function that takes the test data loader as input. We iterate the data loader one at a time and obtain the pairs of images we wish to test. We pass these image pairs to the GPU and run the model over them. After obtaining the output from the model, we find the Contrastive loss and save it to a list.</p> <pre><code>def eval(dl_eval):\n    loss=[] \n    counter=[]\n    iteration_number = 0\n    for i, data in tqdm(enumerate(dl_eval,0), total=len(dl_eval)):\n      pair_left, pair_right , label = data\n      pair_left, pair_right , label = pair_left.cuda(), pair_right.cuda() , label.cuda()\n      out1,out2 = net(pair_left,pair_right)\n      loss_contrastive = criterion(out1,out2,label)\n      loss.append(loss_contrastive.item())\n    loss = np.array(loss)\n    return loss.mean()/len(dl_eval)\n</code></pre> <p>We can now run the code for a single evaluation over all the test data points. To visualize the performance, we will plot the image and print the pairwise distances between the data points the model identifies. We can then plot these results as a grid.</p> <pre><code>for i, data in enumerate(dl_eval, 0):\n    x0, x1, label = data\n    concat = torch.cat((x0, x1), 0)\n    out1, out2 = net(x0.to('cuda'), x1.to('cuda'))\n\n    eucledian_distance = F.pairwise_distance(out1, out2)\n    print(label)\n    if label == torch.FloatTensor([0](0.md)):\n        label = \"Original Pair Of Signature\"\n    else:\n        label = \"Forged Pair Of Signature\"\n\n    imshow(torchvision.utils.make_grid(concat))\n    print(\"Predicted Euclidean Distance:-\", eucledian_distance.item())\n    print(\"Actual Label:-\", label)\n    if i == 4:\n        break\n\n</code></pre> <p>[IMAGE {6} {Outputs} START SAMPLE]  [IMAGE {6} FINISH SAMPLE]</p>"},{"location":"Articles/Scalar/Siamese%20Network/#pros-and-cons-of-siamese-networks","title":"Pros and Cons of Siamese Networks","text":"<p>Like all Deep Learning applications, Siamese Networks have multiple pros and cons. Some of them are listed below.</p>"},{"location":"Articles/Scalar/Siamese%20Network/#cons","title":"Cons","text":"<ul> <li>The biggest disadvantage of using a Siamese network is that it returns only a similarity score. Since the network's output is a distance metric, it does not sum up to 1. This property makes it harder to use in some cases.</li> </ul>"},{"location":"Articles/Scalar/Siamese%20Network/#pros","title":"Pros","text":"<ul> <li>Siamese networks are robust to varying numbers of examples in classes. This robustness is due to the network requiring very little information about the classes.</li> <li>Domain-specific information does not need to be provided to the network to classify images.</li> <li>Siamese networks can perform predictions even with a single image per class.</li> </ul>"},{"location":"Articles/Scalar/Siamese%20Network/#applications","title":"Applications","text":"<p>Siamese Networks have quite a few applications. Some of them are as follows.</p>"},{"location":"Articles/Scalar/Siamese%20Network/#facial-recognition","title":"Facial Recognition","text":"<p>Due to the paired nature of the Siamese networks, one-short facial recognition is a good use case to use this network. The contrastive loss is used to push different faces away from each other and pull similar faces closer. In doing so, the Siamese network learns to identify faces without requiring too many examples.</p>"},{"location":"Articles/Scalar/Siamese%20Network/#fingerprints","title":"Fingerprints","text":"<p>Similar to facial recognition, we can also use Siamese Networks for fingerprint recognition. Once the fingerprints in the database have been cleaned and pre-processed, we can feed them pairwise to the network. The Siamese network then learns to find their differences and identify which fingerprint is valid and invalid.</p>"},{"location":"Articles/Scalar/Siamese%20Network/#signature-verification","title":"Signature Verification","text":"<p>This article focused on implementing Signature Verification using Siamese networks. As we saw in this article, we can create a pairwise dataset using signatures and the network to identify which signatures are forged and which are real. </p>"},{"location":"Articles/Scalar/Siamese%20Network/#text-similarity","title":"Text Similarity","text":"<p>Another useful application of Siamese Networks is Text similarity. Given multiple pieces of text, the network can be fed a pairwise dataset and tasked with identifying which are similar. Examples of such tasks include - finding similar questions from a question bank and using Siamese networks to find similar documents from a text database.</p> <p>::: :::section{.summary}</p>"},{"location":"Articles/Scalar/Siamese%20Network/#conclusion","title":"Conclusion","text":"<ul> <li>Siamese networks are a powerful tool for classifying datasets with few examples per class.</li> <li>We learned the concepts behind Siamese networks and understood the architecture, loss functions used, and how to train such a network.</li> <li>We explored the Signature verification task using the ICDAR 2011 dataset and implemented a model to identify forged signatures.</li> <li>We also understood the entire training and testing pipeline for Siamese networks, including its paired data representation. :::</li> </ul>"},{"location":"Articles/Scalar/StackGAN/","title":"StackGAN","text":"<p>toc: true title: StackGAN</p> <p>categories: [\"article\"] date modified:  date created: </p>"},{"location":"Articles/Scalar/StackGAN/#stackgan","title":"StackGAN","text":"<p>:::section{.abstract}</p>"},{"location":"Articles/Scalar/StackGAN/#overview","title":"Overview","text":"<p>In image generation, the GAN architecture is one of the best ones. The StackGAN architecture addresses some of the flaws of basic GANs by decomposing the task of generating images into multiple parts. This article will focus on the training paradigm proposed by StackGAN and take an in-depth look at its architecture.  ::: :::section{.scope}</p>"},{"location":"Articles/Scalar/StackGAN/#scope","title":"Scope","text":"<ul> <li>The scope of this article is to provide an introduction to StackGANs.</li> <li>The article focuses on understanding how a StackGAN works, how it differs from other GANs and where We can use it.</li> <li>Once these concepts are understood, the architecture is explained in detail with all the different stages and how they are linked.</li> </ul> <p>::: :::section{.main}</p>"},{"location":"Articles/Scalar/StackGAN/#introduction","title":"Introduction","text":"<p>Generating novel photorealistic images is a huge part of the computer vision process in many fields, such as photo editing, design and other graphics-related work. Many attempts to generate high-resolution images have been made in the past, and StackGAN is one of the major ones.  Instead of performing the generation task in one go as most existing architectures do, a StackGAN uses two separate GANs. The authors of the StackGAN made this architectural choice to replicate how a human artist paints a picture. They first start with a rough sketch and a colour blockout, then move on to refining the details of the sketch. They then add more information based on the description of what they want to paint. This article looks at StackGAN and how and why it works. The multi-stage architecture and the loss functions are also explained, along with the need to have such a modification to the GAN paradigm.</p> <p>Some of the images generated by StackGAN given their descriptions are shown below.</p> <p>[IMAGE {1} Example Images START SAMPLE]  [IMAGE {1} FINISH SAMPLE]</p>"},{"location":"Articles/Scalar/StackGAN/#what-is-a-stackgan","title":"What is a StackGAN","text":"<p>The StackGAN is a multi-modal network that can produce much higher quality images than many networks by first generating a low-quality image and then rectifying it to increase the image's resolution. A StackGAN is a modification of the general GAN training paradigm were generating new objects is split into sub-tasks. This split makes training the network much easier. The StackGAN research paper also introduces a technique called \"Conditioning augmentation\" that produces better results.</p>"},{"location":"Articles/Scalar/StackGAN/#pre-requisites","title":"Pre Requisites","text":"<p>Before understanding the StackGAN architecture, we need to understand the concept of Conditional GANs (CGANs). In a CGAN, the Generator and Discriminator are given conditioning variables \\(c\\) alongside the input. These enable the Generator to create images influenced by these variables. This conditioning is formulated as \\(G(z,c), D(x,c)\\) for the Generator and the Discriminator, respectively.</p>"},{"location":"Articles/Scalar/StackGAN/#architecture","title":"Architecture","text":"<p>The StackGAN comprises two parts - Stage I and Stage II GANs. The first stage generates low-quality images by \"sketching\" a primitive shape and colouring the image with a simple colour blockout based on the text description provided. The background is generated from random noise. The second stage corrects defects in the output of the first stage by re-reading the provided description and then completing the details the first phase missed. The output of the second stage is thus a high-resolution image.</p> <p>[IMAGE {2} Architecture START SAMPLE]  [IMAGE {2} FINISH SAMPLE]</p>"},{"location":"Articles/Scalar/StackGAN/#stage-i-gan","title":"Stage I GAN","text":"<p>Stage I of the GAN is focused on generating a rough sketch with simple colours from the description.</p>"},{"location":"Articles/Scalar/StackGAN/#architecture_1","title":"Architecture","text":"<p>It is first fed into a fully connected layer(FC) to understand the embedding. The output of this FC layer is then passed to the Generator, which attempts to learn how to create the image better. The Discriminator takes the text embeddings and compresses them to a smaller representation using another FC layer. The image is also passed through a few downsampling.md|../../Downsampling|downsampling blocks until it is a size the network can use. The final down-sampled image is combined with the text embedding and passed through a 1x1 convolutional layer. The final layer is another FC layer that returns the probability of the generated image being real or fake.</p>"},{"location":"Articles/Scalar/StackGAN/#loss-functions","title":"Loss functions","text":"<p>Consider the text embeddings of the required description as \\(\\varphi{t}\\). The meaning of the text embeddings is sampled from the Gaussian conditioning variables \\(\\hat{c{0}}\\).  Stage I first trains the Discriminator and then the Generator to alternatively maximize the discriminator and generator losses. The equations for these loss functions are as follows.</p> \\[\\mathcal{L}_{D{0}} = \\mathbb{E}{(I{0},t)\\sim p{data}}[log D{0}(I{0}, \\varphi{t})]+ \\\\ \\mathbb{E}{z \\sim p{z}, t \\sim p{data}}[log(1- D{0}(G{0}(z , \\hat{c{0}}, \\varphi_{t})))]\\] \\[\\mathcal{L}_{G{0}}= \\mathbb{E}{z \\sim p{z}, t \\sim p{data}}[log(1- D{0}(G{0}(z, \\hat{c{0}}),\\varphi{t}))] + \\\\ \\lambda D{KL}(\\mathcal{N}(\\mu{0}(\\varphi{t}), \\Sigma{0}(\\varphi{t}))|| \\mathcal{N}(0, I))\\] <p>\\(z\\) is a noise vector that is randomly sampled from the Gaussian distribution \\(p_{z}\\). A regularizing parameter is provided in the \\(\\lambda\\) variable. The StackGAN research uses a \\(\\lambda = 1\\) for the paper. </p>"},{"location":"Articles/Scalar/StackGAN/#stage-ii-gan","title":"Stage II GAN","text":"<p>The Stage II GAN receives the output of the Stage I GAN and refines it by re-considering the descriptions. </p>"},{"location":"Articles/Scalar/StackGAN/#architecture_2","title":"Architecture","text":"<p>The StackGANs Stage II Generator follows an encoder-decoder architecture with residual blocks. Text embedding is used first to create the conditioning variables. The result of Stage I is passed through downsampling.md|../../Downsampling|downsampling layers and then concatenated with the features obtained from the text embedding. The output of these layers is then upsampled to generate high-resolution images. The Discriminator architecture is almost identical to Stage I, except for a few extra down-sampling layers. These down-sampling layers were included as the output of this part of the network is of a higher resolution than Stage I.</p>"},{"location":"Articles/Scalar/StackGAN/#loss-functions_1","title":"Loss functions","text":"<p>If the low-resolution image is given by \\(s_{0}\\) and the Gaussian sampled latent variables are given by \\(\\hat{c}\\), the Discriminator and Generator are trained by alternatively maximizing the value of the Discriminator loss and minimizing the Generator loss. The equations for these loss functions are the same as the Stage I GAN, except the low-resolution image \\(s_{0}\\) is used instead of the noise \\(z\\). It is also to be noted that the noise \\(z\\) is not used in Stage II as the StackGAN is meant to preserve the required randomness with the previous stage. A different FC layer is also used here that generates different statistical outputs compared to Stage I to learn better features.</p>"},{"location":"Articles/Scalar/StackGAN/#more-architectural-details","title":"More architectural details","text":"<p>Some architectural details were also mentioned in the StackGAN research. These details apply to both the Generator and the Discriminator. - The up-sampling blocks are composed of nearest neighbour upsampling and then passed to a 3x3 stride one convolutional layer. Besides the final layer, Batch Normalization and the ReLU activation are applied after every convolution.  - The residual blocks have 3x3 stride 1 convolutions.  - The StackGAN model that generates 256x256 images has four residual blocks, while the one that generates 128x128 images has only two blocks. - The down-sampling blocks have 4x4 stride two convolutions and LeakyReLU instead of ReLU.  - The first downsampling.md|../../Downsampling|downsampling block does not have a Batch Normalization layer.</p>"},{"location":"Articles/Scalar/StackGAN/#embedding","title":"Embedding","text":"<p>Contrary to other networks where the text embeddings are transformed using non-linear techniques, the StackGAN uses additional variables and a process called Conditioning Augmentation. These embeddings are more robust to minor changes in the data manifold and work with lesser image data. </p>"},{"location":"Articles/Scalar/StackGAN/#conditioning-augmentation","title":"Conditioning Augmentation","text":"<p>Conditioning Augmentation is one of the major contributions of the StackGAN research. Given a text description \\(t\\), the StackGAN uses an embedding to convert it to input for the Generator. Under circumstances where the data is limited, the latent space of the embedding is not fully exploited and leads to changes in the data manifold. These changes in the manifold are not desirable and hurt performance. </p> <p>Conditioning Augmentation uses these to create more training pairs from a small subset of data. Instead of using a fixed conditioning variable, StackGAN samples latent variables from a Gaussian distribution. The mean and covariance.md|../../Covariance|covariance matrix is generated for a text embedding \\(\\varphi_{t}\\). </p> <p>The secondary objective of Conditioning Augmentation is to encourage reducing changes in the output with small changes in the data manifold. To do this, StackGAN uses a regularization.md|../../Regularization|regularization term called KL Divergence as part of the Generator. This is given by, \\(\\(D{KL}(\\mathcal{N}(\\mu(\\varphi{t}), \\Sigma(\\varphi_{t})) || \\mathcal{N}(0,I))\\)\\) </p>"},{"location":"Articles/Scalar/StackGAN/#need-for-stackgan","title":"Need for StackGAN","text":"<p>Even though generating novel photorealistic images is easy enough to do with a GAN such as DCGAN, generating higher-resolution images is a hard problem. Previously failed approaches have tried stacking more up-sampling layers. StackGAN, using the decomposition of the generation and refinement tasks, can generate 256x256 images.  The StackGAN training paradigm can be used with existing GANs to improve performance as it generates higher image sizes. ::: :::section{.summary}</p>"},{"location":"Articles/Scalar/StackGAN/#conclusion","title":"Conclusion","text":"<p>In this article, we looked at StackGAN and all its components. - We understood how to decompose the task of generating novel images using a StackGAN. - We looked at the architectural details of the StackGAN, its' embeddings and the respective training stages. - We also explored Conditional Augmentation and understood why it was proposed. :::</p>"},{"location":"Articles/Scalar/Text_proc/","title":"Image","text":"In\u00a0[1]: Copied! <pre>import os\nimport re\n# %pip install clipboard\nimport clipboard\n</pre> import os import re # %pip install clipboard import clipboard In\u00a0[2]: Copied! <pre>def gen_image_hackmd():\n    link = input(\"Enter link\")\n    name = input(\"Enter name\")\n    count = int(input(\"Enter count\"))\n    print(\"\\n\\n\")\n    return f\"\"\"[IMAGE { {count} } { {name} } START SAMPLE]\n![{name}](https://hackmd.io/_uploads/{link})\n[IMAGE { {count} } FINISH SAMPLE]\"\"\".replace(\"\\\\\",\"\").replace(\"'\", \"\")\n</pre> def gen_image_hackmd():     link = input(\"Enter link\")     name = input(\"Enter name\")     count = int(input(\"Enter count\"))     print(\"\\n\\n\")     return f\"\"\"[IMAGE { {count} } { {name} } START SAMPLE] ![{name}](https://hackmd.io/_uploads/{link}) [IMAGE { {count} } FINISH SAMPLE]\"\"\".replace(\"\\\\\",\"\").replace(\"'\", \"\") In\u00a0[3]: Copied! <pre>def setup_base(main_paras):\n#     main_paras = input('Enter main paras')\n    para_string = \"\"\"\n    :::section{.abstract}\n    ## Overview\n    :::\n    :::section{.scope}\n    ## Scope\n    This article covers the following topics:\n    \n    :::\n    :::section{.main}\n    \"\"\"\n    para_string += f\"\\n{main_paras}\"\n    para_string += \"\"\"\n    :::\n    :::section{.summary}\n\n    ## Conclusion\n    :::\n    \"\"\"\n    return para_string\n</pre> def setup_base(main_paras): #     main_paras = input('Enter main paras')     para_string = \"\"\"     :::section{.abstract}     ## Overview     :::     :::section{.scope}     ## Scope     This article covers the following topics:          :::     :::section{.main}     \"\"\"     para_string += f\"\\n{main_paras}\"     para_string += \"\"\"     :::     :::section{.summary}      ## Conclusion     :::     \"\"\"     return para_string In\u00a0[12]: Copied! <pre>clipboard.copy(gen_image_hackmd())\n# print(gen_image_hackmd())\n</pre> clipboard.copy(gen_image_hackmd()) # print(gen_image_hackmd()) <pre>\n\n\n</pre> In\u00a0[4]: Copied! <pre>main_parts = \"\"\"\n## Introduction\n## What Are Siamese Networks?\n- (how do they work?)\n\n- (Proper explanation with real life examples)\n## Exploring Few-Shot Learning\n## Architecture of Siamese Networks\n## Loss Functions Used in Siamese Networks\n## Building a Signature Verification Modeil With Siamese Networks\n### Dataset\n### Description of problem statement\n### Data preprocessing\n### Brief description of the features (numerical, categorical, etc.)\n### Standardization of the features\n### Splitting the dataset\n### Neural Network Architecture \n### Loss Function \n### Training the Siamese Network\n### Testing the model \n    - (Should include a section showing how to perform inference with the model on a single data point)\n    - (Note for the writer - use an incremental approach here, break the projects into different steps/parts and make it easy for the reader to follow through your code with proper outputs and explanation)\n## Pros and Cons of Siamese Networks\n## Applications\n\"\"\"\n# print(setup_base(main_parts))\nclipboard.copy(setup_base(main_parts))\n</pre> main_parts = \"\"\" ## Introduction ## What Are Siamese Networks? - (how do they work?)  - (Proper explanation with real life examples) ## Exploring Few-Shot Learning ## Architecture of Siamese Networks ## Loss Functions Used in Siamese Networks ## Building a Signature Verification Modeil With Siamese Networks ### Dataset ### Description of problem statement ### Data preprocessing ### Brief description of the features (numerical, categorical, etc.) ### Standardization of the features ### Splitting the dataset ### Neural Network Architecture  ### Loss Function  ### Training the Siamese Network ### Testing the model      - (Should include a section showing how to perform inference with the model on a single data point)     - (Note for the writer - use an incremental approach here, break the projects into different steps/parts and make it easy for the reader to follow through your code with proper outputs and explanation) ## Pros and Cons of Siamese Networks ## Applications \"\"\" # print(setup_base(main_parts)) clipboard.copy(setup_base(main_parts)) In\u00a0[71]: Copied! <pre>get_inpt = clipboard.paste()\nsplit_inps = get_inpt.split(\"\\n\")\nfixed_text = []\ntemp_rep = \"\"\nfor line in split_inps:\n    if \"IMAGE\" in line:\n        if line.count(\"}\") &lt;= 1:\n            if \"START\" in line:\n                st = line.find(\"}\") + 1\n                end = line.find(\"START\")\n                found_string = line[st:end]\n                new_string = \" {\" + found_string + \"} \"\n                line = line.replace(found_string, new_string)\n                temp_rep = line\n\n            # if \"FINISH\" in line:\n            #     st = line.find(\"}\") + 1\n            #     end = line.find(\"FINISH\")\n            #     found_string = line[st:end]\n            #     new_string = \" {\" + temp_rep + \"} FINISH\"\n            #     line = line.replace(\"FINISH\", new_string)\n        # else:\n        #     if \"START\" in line:\n        #         temp_rep = line[line.find(\"} {\")+1 : line.find(\"} ST\")]\n        #         temp_rep = temp_rep.replace(\"}\", \"\")\n        #         temp_rep = temp_rep.replace(\"{\", \"\")\n\n    fixed_text.append(line)\nclipboard.copy(\"\\n\".join(fixed_text))\n# print(\"\\n\".join(fixed_text))\n</pre> get_inpt = clipboard.paste() split_inps = get_inpt.split(\"\\n\") fixed_text = [] temp_rep = \"\" for line in split_inps:     if \"IMAGE\" in line:         if line.count(\"}\") &lt;= 1:             if \"START\" in line:                 st = line.find(\"}\") + 1                 end = line.find(\"START\")                 found_string = line[st:end]                 new_string = \" {\" + found_string + \"} \"                 line = line.replace(found_string, new_string)                 temp_rep = line              # if \"FINISH\" in line:             #     st = line.find(\"}\") + 1             #     end = line.find(\"FINISH\")             #     found_string = line[st:end]             #     new_string = \" {\" + temp_rep + \"} FINISH\"             #     line = line.replace(\"FINISH\", new_string)         # else:         #     if \"START\" in line:         #         temp_rep = line[line.find(\"} {\")+1 : line.find(\"} ST\")]         #         temp_rep = temp_rep.replace(\"}\", \"\")         #         temp_rep = temp_rep.replace(\"{\", \"\")      fixed_text.append(line) clipboard.copy(\"\\n\".join(fixed_text)) # print(\"\\n\".join(fixed_text)) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"Articles/Scalar/Text_proc/#image","title":"Image\u00b6","text":""},{"location":"Articles/Scalar/Text_proc/#setup-article","title":"Setup Article\u00b6","text":""},{"location":"Articles/Scalar/Text_proc/#fix-image-entries","title":"Fix image entries\u00b6","text":""},{"location":"Articles/Scalar/Time%20series%20forecasting%20using%20LSTM/","title":"Time series forecasting using LSTM","text":"<p>toc: true title: Time series forecasting using LSTM</p> <p>categories: [\"article\"] date modified:  date created: </p>"},{"location":"Articles/Scalar/Time%20series%20forecasting%20using%20LSTM/#time-series-forecasting-using-lstm","title":"Time series forecasting using LSTM","text":"<p>:::section{.abstract}</p>"},{"location":"Articles/Scalar/Time%20series%20forecasting%20using%20LSTM/#overview","title":"Overview","text":"<p>Any temporal data can be framed as a time series task. Data such as heart rates, stock market prices, sensor logs and many others fall under the category of time series data. There are many Deep Learning architectures that are used to model such data, LSTMs being one of them. This article focuses on building an LSTM time series model. :::</p> <p>:::section{.main}</p>"},{"location":"Articles/Scalar/Time%20series%20forecasting%20using%20LSTM/#what-are-we-building","title":"What are we building?","text":"<p>In this article, we will be creating an LSTM time series model. We will be using data that we generate and create a simple LSTM that can model it accurately. To perform this task, we will write functions that can generate data, model it and perform predictions on future points.  We will implement this model using Tensorflow and the below sections explain how to perform just that.</p>"},{"location":"Articles/Scalar/Time%20series%20forecasting%20using%20LSTM/#pre-requisites","title":"Pre-Requisites","text":"<p>Before moving on to creating the LSTM time series model, we must understand some pre-requisite information.</p>"},{"location":"Articles/Scalar/Time%20series%20forecasting%20using%20LSTM/#what-is-time-series","title":"What is Time Series?","text":"<p>A time series data is any temporal data that has a discrete time interval and almost equidistant time steps. The general task is to estimate the function that was used to generate such the time series. If the function can be estimated correctly, future points that the model has not encountered yet can be predicted. Examples of time series include heart rate data, stock market data and many others.</p>"},{"location":"Articles/Scalar/Time%20series%20forecasting%20using%20LSTM/#rnns","title":"RNNs","text":"<p>RNNs are a family of models that take entire series as inputs and return series as outputs. This algorithm is a sequential process and contains hidden states that model the underlying data. Unlike a simple Convolutional network that uses Backpropagation, an RNN uses a modified variant called Backpropagation through time (BPTT) that enables it to embed temporal data. An RNN is said to be Turing complete and is used in domains such as Natural Language Processing, Computer Vision, Robotics and many others. The RNN architecture is made up of gates and is shown below.</p> <p>[IMAGE {1} RNN START SAMPLE]  [IMAGE {1} FINISH SAMPLE]</p>"},{"location":"Articles/Scalar/Time%20series%20forecasting%20using%20LSTM/#problems-with-classical-rnns","title":"Problems with Classical RNNs","text":"<p>RNNs suffer from a variety of problems due to their sequential nature. - RNNs fail to model longer sequences. This property makes it very hard to use for data that has a long temporal span. - The classical RNN also had an issue with exploding and vanishing gradients due to the way the underlying architecture worked. These problems make an RNN very unstable. </p>"},{"location":"Articles/Scalar/Time%20series%20forecasting%20using%20LSTM/#what-is-lstm","title":"What is LSTM?","text":"<p>LSTMs are a modified version of RNNs with different gates that enable the architecture to model much longer sequences. The LSTMs use gated connections that learn which features to forget and which to remember. The ability to choose what to forget makes them much better than a classical RNN. LSTMs are also a lot more stable and have a smaller chance of exploding or vanishing gradients. The LSTM architecture is shown below.</p> <p>[IMAGE {2} LSTM START SAMPLE]  [IMAGE {2} FINISH SAMPLE]</p>"},{"location":"Articles/Scalar/Time%20series%20forecasting%20using%20LSTM/#how-are-we-going-to-build-this","title":"How are we going to build this?","text":"<p>To build an LSTM time series model, we will write functions that can generate a time series data. Once we have the data, we will pre-process it and make it fit to be used by the model. We will also write a function that can display the results of the model. After creating these helper functions, we will create a simple LSTM model and train it using the data we generated previously. The LSTM time series model we will be using in this article is just comprised of a single LSTM block followed by a FC layer and is very easy to implement. After implementing all the required functions, we will train the model and use it to predict future points. The following sections detail the implementation. </p>"},{"location":"Articles/Scalar/Time%20series%20forecasting%20using%20LSTM/#final-output","title":"Final Output","text":"<p>The final output of the LSTM time series model is a prediction of future points that the model has not encountered yet. The output we get after training the model for ~50 epochs is shown below. [IMAGE {3} Final Output START SAMPLE]  [IMAGE {3} FINISH SAMPLE]</p>"},{"location":"Articles/Scalar/Time%20series%20forecasting%20using%20LSTM/#requirements","title":"Requirements","text":"<p>There are many libraries that we need to implement an LSTM time series model. Since we will be building the architecture in Tensorflow, we import it first. We will also be using the numerical processing library numpy, the tabular data library pandas and the plotting libraries matplotlib and seaborn. The rc module in matplotlib enables configuring some of the plots and comes in handy later on.</p> <pre><code>import numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nimport pandas as pd\nimport seaborn as sns\nfrom pylab import rcParams\nimport matplotlib.pyplot as plt\nfrom matplotlib import rc\n</code></pre>"},{"location":"Articles/Scalar/Time%20series%20forecasting%20using%20LSTM/#building-the-time-series-forecaster","title":"Building the Time Series Forecaster","text":"<p>The Time Series Forecaster model has a simple LSTM based architecture. Before creating it, we have to write functions to set up the library, generate and load and finally pre-process the data.  The model we will use for this article is a Sequential model comprising an LSTM block followed by a Fully Connected layer. We will then use the generated data and this model to train a LSTM time series prediction model. We will use the trained model to predict points in the future that the model has not seen before. The following sections detail all of these points.</p>"},{"location":"Articles/Scalar/Time%20series%20forecasting%20using%20LSTM/#setup","title":"Setup","text":"<p>To set up our modules, we set the RANDOM_SEED variable. This variable sets the seed for the random number generator and ensures that we get the same \u201crandom\u201d numbers every time. This is not useful in practice but is done only for demonstration purposes. We also modify the plot to be a white style grid with a muted palette for better display.</p> <pre><code>sns.set(style='whitegrid', palette='muted', font_scale=1.5)\nrcParams['figure.figsize'] = 16, 10\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)\ntf.random.set_seed(RANDOM_SEED)\n</code></pre>"},{"location":"Articles/Scalar/Time%20series%20forecasting%20using%20LSTM/#data","title":"Data","text":"<p>To generate the data, we create a custom function that uses a combination of a Sin wave and a small Gaussian noise. These values are generated in the range of (0,200) with a step of 0.1 .  To see how this data looks like, we can plot it using matplotlib.</p> <pre><code>data_time = np.arange(0, 200, 0.1)\nsin_values = np.sin(data_time) + np.random.normal(scale=0.5, size=len(data_time))\nplt.plot(data_time, sin_values, label='sine (with noise)');\n</code></pre> <p>[IMAGE {4} Original Data START SAMPLE]  [IMAGE {4} FINISH SAMPLE]</p>"},{"location":"Articles/Scalar/Time%20series%20forecasting%20using%20LSTM/#data-pre-processing","title":"Data Pre-processing","text":"<p>Now, we need to convert this data into a DataFrame before passing it to the model. Doing so makes future processes much easier. We also split the data into training and testing components. The first few rows of the DataFrame are shown here.</p> <p>[IMAGE {5} Pre-Processing START SAMPLE]  [IMAGE {5} FINISH SAMPLE]</p> <pre><code>data_full = pd.DataFrame(dict(sine=sin_values), index=data_time, columns=['sine'])\ndata_full.head()\n\nlen_train = int(len(data_full) * 0.8)\nlen_test = len(data_full) - len_train\ntrain, test = data_full.iloc[0:len_train], data_full.iloc[len_train:len(data_full)]\n</code></pre> <p>Now that we have created a data frame, we will use it to generate batches of data. We do this using the following function and create the input and labels for both training and testing.</p> <pre><code>def gen_data(X, y, num_steps=1):\n    Xs, ys = [], []\n    for i in range(len(X) - num_steps):\n        Xs.append(X.iloc[i:(i + num_steps)].values)       \n        ys.append(y.iloc[i + num_steps])\n    return np.array(Xs), np.array(ys)\n\nnum_steps = 10\ntrainX, trainY = gen_data(train, train.sine, num_steps)\ntestX, testY = gen_data(test, test.sine, num_steps)\n</code></pre>"},{"location":"Articles/Scalar/Time%20series%20forecasting%20using%20LSTM/#implementing-the-sequential-model","title":"Implementing the Sequential model","text":"<p>We can finally implement the LSTM time series model. This is a very simple model and just has a single LSTM layer followed by a FC layer.  We compile the model with the Mean Squared Error loss function and an Adam Optimiser. This compiled model can now be trained on the generated data.</p> <pre><code>lstm_model = keras.Sequential()\nlstm_model.add(keras.layers.LSTM(128, input_shape=(trainX.shape[1], trainX.shape[2])))\nlstm_model.add(keras.layers.Dense(1))\nlstm_model.compile(loss='mean_squared_error', optimizer=keras.optimizers.Adam(0.001))\n</code></pre>"},{"location":"Articles/Scalar/Time%20series%20forecasting%20using%20LSTM/#early-stopping-callback","title":"Early Stopping Callback","text":"<p>Time series model tend to Overfit really easily. To reduce the probability of Overfitting, the Early Stopping callback is used. This callback uses the number of epochs as a hyper parameter. If the validation accuracy does not increase for a few epochs, the model is saved and training is stopped. This stops the training before the model starts to focus too much on the training data.</p> <pre><code>callbacks=[tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)]\n</code></pre>"},{"location":"Articles/Scalar/Time%20series%20forecasting%20using%20LSTM/#model-training","title":"Model Training","text":"<p>Once we have defined all the required functions, we can train the model. In this article we train the LSTM time series model for 30 epochs with a batch size of 16. We use a validation split of 0.1% and also supply the Early Stopping callback that we defined earlier.</p> <pre><code>history = lstm_model.fit(\n    trainX, trainY, \n    epochs=30, \n    batch_size=16, \n    validation_split=0.1,\n    shuffle=False,\n    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)],\n)\n</code></pre> <p>[IMAGE {6} Training START SAMPLE]  [IMAGE {6} FINISH SAMPLE]</p>"},{"location":"Articles/Scalar/Time%20series%20forecasting%20using%20LSTM/#evaluation","title":"Evaluation","text":"<p>After training the model, we can use the evaluate function to perform a batch evaluation on the test dataset. We can see that the model performs pretty decently. </p> <pre><code>lstm_model.evaluate(testX)\n</code></pre> <p>To visualize the training performance, we plot both the training and validation losses throughout history. We can see that the model is learning stably and is neither Overfitting nor Underfitting the data.</p> <pre><code>plt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='test')\nplt.legend();\n</code></pre> <p>[IMAGE {7} Evaluation START SAMPLE]  [IMAGE {7} FINISH SAMPLE]</p>"},{"location":"Articles/Scalar/Time%20series%20forecasting%20using%20LSTM/#predicting-a-new-point-in-the-future","title":"Predicting a new point in the future","text":"<p>No LSTM time series model is useful without the ability to predict future points. We can use the predict function on a set of future points to see how well the model can predict the results. After performing inference, we plot the results against the actual data.</p> <pre><code>y_pred = lstm_model.predict(testX)\nplt.plot(testY, marker='.', label=\"true\")\nplt.plot(y_pred, 'r', label=\"prediction\")\nplt.ylabel('Value')\nplt.xlabel('Time Step')\nplt.legend()\nplt.show();\n</code></pre> <p>We can see that the model did perform pretty decently. Further improvements in performance can be obtained by training for longer, using more data and many other methods that are beyond the scope of this article.</p> <p>::: :::section{.summary}</p>"},{"location":"Articles/Scalar/Time%20series%20forecasting%20using%20LSTM/#conclusion","title":"Conclusion","text":"<ul> <li>In this article, we learnt what an LSTM model is and why it was created.</li> <li>We learnt how to create an LSTM model in Tensorflow.</li> <li>We also learnt how to generate our own time series data using a sin curve.</li> <li>Finally, we trained an LSTM time series model on the generated data.</li> <li>We also learned how to use the trained model to predict points in the future and display its predictions. :::</li> </ul>"},{"location":"Articles/Scalar/Transfer%20Learning%20and%20Fine-tuning/","title":"Transfer Learning and Fine tuning","text":"<p>toc: true title: Transfer Learning and Fine-tuning</p> <p>categories: [\"article\"] date modified:  date created: </p>"},{"location":"Articles/Scalar/Transfer%20Learning%20and%20Fine-tuning/#transfer-learning-and-fine-tuning","title":"Transfer Learning and Fine-tuning","text":"<p>:::section{.abstract}</p>"},{"location":"Articles/Scalar/Transfer%20Learning%20and%20Fine-tuning/#overview","title":"Overview","text":"<p>Training deep learning models requires a massive amount of labeled data. In most cases, this data needs to be made available or easier to clean up. Many approaches for working with limited data sets have been created over the years, Transfer Learning being one of the breakthroughs. Transfer learning enables us to fine-tune a model pre-trained on a large dataset on our task.  ::: :::section{.scope}</p>"},{"location":"Articles/Scalar/Transfer%20Learning%20and%20Fine-tuning/#scope","title":"Scope","text":"<ul> <li>This article explains the principles behind Transfer Learning.</li> <li>It covers the method of fine-tuning using a pre-trained model.</li> <li>It elaborates on the principles of freezing and unfreezing weights.</li> <li>The article also discusses implementing the Transfer Learning pipeline in Tensorflow. ::: :::section{.main}</li> </ul>"},{"location":"Articles/Scalar/Transfer%20Learning%20and%20Fine-tuning/#introduction","title":"Introduction","text":"<p>Transfer Learning is useful for smaller datasets and can be considered an intelligent weight initialization scheme. Instead of randomly initializing the weights of the model like we usually do, we obtain weights from a model trained on a larger dataset. Any company/individual with the funds can train a larger model and make its weights public. After doing so, we can train these models on any other similar dataset much faster than before.  This article explores the concept of Transfer Learning by creating a network that can identify ten different classes from the CIFAR10 dataset by fine-tuning a model pre-trained on the ImageNet dataset (1000 classes). </p>"},{"location":"Articles/Scalar/Transfer%20Learning%20and%20Fine-tuning/#transfer-learning","title":"Transfer Learning","text":"<p>In a DL pipeline, Transfer Learning is usually done when the data available is too less to train a network properly. The general approach for a Transfer Learning workflow is as follows. - Obtain a pre-trained model on data similar to your current dataset. For example, many models are pre-trained on the ImageNet dataset in computer vision approaches. Since the ImageNet dataset has classes relating to real-life objects and things, models pre-trained on it already have some knowledge of the world. - Load the model and understand its layer structure. - Freeze the weights of the model. Freezing the weights sets these layers to be un-trainable and prevents them from having their existing knowledge destroyed by the Transfer Learning process. - Append new layers to the frozen part of the model. These new layers can be trained and use the pre-trained weights to learn faster. - Train the new model on a new dataset.</p>"},{"location":"Articles/Scalar/Transfer%20Learning%20and%20Fine-tuning/#implementation","title":"Implementation","text":"<p>This article will explore how to take a model trained on ImageNet and fine-tune it on new data. We will create this implementation in Tensorflow and use the Cats and Dogs dataset from Kaggle.</p>"},{"location":"Articles/Scalar/Transfer%20Learning%20and%20Fine-tuning/#pre-requisites","title":"Pre-Requisites.","text":"<p>Before we can fine-tune a model, we must decide what base model we need. We also need to load and preprocess the dataset. Since Transfer Learning is generally used for small datasets, we take a subset of the Cats and Dogs dataset for this example.</p>"},{"location":"Articles/Scalar/Transfer%20Learning%20and%20Fine-tuning/#imports","title":"Imports","text":"<p>We first import the required libraries. We use Tensorflow for the entire pipeline. </p> <pre><code>import numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow_datasets as tfds\nimport os\nimport zipfile\n</code></pre>"},{"location":"Articles/Scalar/Transfer%20Learning%20and%20Fine-tuning/#loading-the-data","title":"Loading the Data","text":"<p>Since the Cats and Dogs dataset is not a part of Tensorflow, we download it from Kaggle and then use the tensorflow_datasets library to load it into memory. After loading, we split the data into train and test while also sub-setting it.</p> <pre><code>train_dataset, validation_dataset, test_dataset = tfds.load(\n    \"cats_vs_dogs\",\n    split=[\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"],\n    as_supervised=True,\n)\n</code></pre> <p>An example subset of the data is shown below. [IMAGE {1} CatsDogs START SAMPLE]  [IMAGE {1} FINISH SAMPLE]</p> <p>We can then convert the data into batches, split them into data loaders, and optimize the data loading using caching and pre-fetching. We use a batch size of 32 for this example. After loading, we can also apply some simple data augmentation methods. For example, we use Random Horizontal Flipping and Random Rotation.</p> <pre><code>size = (150, 150)\nbs = 32\naug_transforms = keras.Sequential(\n    [layers.RandomFlip(\"horizontal\"), layers.RandomRotation(0.1),]\n)\n\ntrain_dataset = train_dataset.map(lambda x, y: (tf.image.resize(x, size), y))\nvalidation_dataset = validation_dataset.map(lambda x, y: (tf.image.resize(x, size), y))\ntest_dataset = test_dataset.map(lambda x, y: (tf.image.resize(x, size), y))\n\n\ntrain_dataset = train_dataset.cache().batch(bs).prefetch(buffer_size=10)\nvalidation_dataset = validation_dataset.cache().batch(bs).prefetch(buffer_size=10)\ntest_dataset = test_dataset.cache().batch(bs).prefetch(buffer_size=10)\n</code></pre> <p>This article uses an Xception.md|../../Xception|Xception model pre-trained on the ImageNet dataset and applied to images 150x150x3 in size. The important point is to exclude the pre-trained model's final classification layer. This final layer is just for classification, and we only care about the layers before it.</p> <pre><code>model_pretrained = keras.applications.Xception(\n    weights=\"imagenet\", \n    input_shape=(150, 150, 3),\n    include_top=False,\n)\n</code></pre> <p>The Xception.md|../../Xception|Xception model architecture is shown here. [IMAGE {2} arch START SAMPLE]  [IMAGE {2} FINISH SAMPLE]</p>"},{"location":"Articles/Scalar/Transfer%20Learning%20and%20Fine-tuning/#fine-tuning","title":"Fine-Tuning","text":"<p>Now, we freeze the layers of the model we just loaded by setting the trainable parameter to False. After that, we create a model on top of the frozen layers and apply the data augmentations we defined. The Xception.md|../../Xception|Xception model's caveat is that it defines the inputs are scaled from the original range of (0,255) to the range of (-1.0, 1.0). We perform this rescaling using the Rescaling layer as follows.</p> <pre><code>model_pretrained.trainable = False\n\ninputs = keras.Input(shape=(150, 150, 3))\nrescale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=-1)\n\nx = aug_transforms(inputs) \nx = rescale_layer(x)\n</code></pre>"},{"location":"Articles/Scalar/Transfer%20Learning%20and%20Fine-tuning/#unfreeze-the-top-layers-of-the-model","title":"Unfreeze the top layers of the model","text":"<p>The [Xception] model also contains Batch Normalization layers that should not be trained when the model is unfrozen. To make sure this is the case, we disable the training mode. We also apply a GlobalAveragePooling followed by ../../Dropout|Dropout.md|../../Xception|Xception layers to improve performance further. Global Average Pooling is an alternative to the Fully Connected layer (FC) that preserves spatial information better. Since our pre-trained model uses different data, these layers are useful here. The final layer is an FC layer for a binary classification task. </p> <pre><code>x = model_pretrained(x, training=False)\nx = keras.layers.GlobalAveragePooling2D()(x)\nx = keras.layers.Dropout(0.2)(x) \n\noutputs = keras.layers.Dense(1)(x)\nfinal_model = keras.Model(inputs, outputs)\n</code></pre> <p>We can now train the new layers that we created.</p> <pre><code>final_model.compile(\n    optimizer=keras.optimizers.Adam(),\n    loss=keras.losses.BinaryCrossentropy(from_logits=True),\n    metrics=[keras.metrics.BinaryAccuracy()],\n)\n\nnum_epochs = 5\nfinal_model.fit(train_dataset, epochs=num_epochs, validation_data=validation_dataset)\n</code></pre> <p>Now that we trained the new layers, we unfreeze the entire model and then train it with a very small learning rate. This gradual training leads to much better performance. Note that the Batch Normalization layers are not updating during this training, as if they did, it would badly hurt performance.</p> <pre><code>model_pretrained.trainable = True\nfinal_model.summary()\n\nfinal_model.compile(\n    optimizer=keras.optimizers.Adam(1e-5),\n    loss=keras.losses.BinaryCrossentropy(from_logits=True),\n    metrics=[keras.metrics.BinaryAccuracy()],\n)\n\nnum_epochs = 5\nfinal_model.fit(train_dataset, epochs=num_epochs, validation_data=validation_dataset)\n</code></pre>"},{"location":"Articles/Scalar/Transfer%20Learning%20and%20Fine-tuning/#evaluation-and-prediction","title":"Evaluation and prediction","text":"<p>This example shows how useful Transfer Learning is for quickly training small datasets. After training the model, we evaluate the test dataset. The model still performs quite well despite the few training epochs and fewer data.  [IMAGE {3} Results START SAMPLE]  [IMAGE {3} FINISH SAMPLE]</p> <p>::: :::section{.summary}</p>"},{"location":"Articles/Scalar/Transfer%20Learning%20and%20Fine-tuning/#conclusion","title":"Conclusion","text":"<ul> <li>Transfer Learning is a powerful method when fewer data is present.</li> <li>As long as the pre-trained model uses similar data, a niche model can be fine-tuned using it.</li> <li>Selectively freezing the pre-trained layers and training the rest is a way to achieve the effects of fine-tuning.</li> <li>After an initial round of selective training, unfreezing the model and training the entire model improves performance.</li> <li>The Transfer Learning approach is thus an invaluable breakthrough in Deep Learning. :::</li> </ul>"},{"location":"Book%20Notes/","title":"Index","text":""},{"location":"Book%20Notes/#book-notes","title":"Book notes","text":"<ul> <li>Notes of books I read</li> </ul>"},{"location":"Book%20Notes/Understanding%20Deep%20Learning/","title":"Index","text":""},{"location":"Book%20Notes/Understanding%20Deep%20Learning/#udl-book","title":"UDL Book","text":"<ul> <li>UDL Book</li> </ul>"},{"location":"Book%20Notes/Understanding%20Deep%20Learning/Chapter%201/","title":"Chapter 1","text":""},{"location":"Book%20Notes/Understanding%20Deep%20Learning/Chapter%201/#chapter-1","title":"Chapter 1","text":"<p>-</p>"},{"location":"Book%20Notes/Understanding%20Deep%20Learning/Chapter%204%20-%20Deep%20Neural%20Networks/","title":"Chapter 4 - Deep Neural Networks","text":""},{"location":"Book%20Notes/Understanding%20Deep%20Learning/Chapter%204%20-%20Deep%20Neural%20Networks/#chapter-4-deep-neural-networks","title":"Chapter 4 - Deep Neural Networks","text":""},{"location":"Book%20Notes/Understanding%20Deep%20Learning/Chapter%204%20-%20Deep%20Neural%20Networks/#intro","title":"Intro","text":"<ul> <li>Deep networks : more than one hidden layer</li> <li>Both deep and shallow describe piecewise linear mappings from input -&gt; output</li> <li>A shallow network can describe complex functions but might have too many hidden layers to be practically possible to use.</li> </ul>"},{"location":"Book%20Notes/Understanding%20Deep%20Learning/Chapter%204%20-%20Deep%20Neural%20Networks/#why-did-deep-learning-take-off","title":"Why did deep learning take off","text":"<ul> <li>Krizhevsky et al. (2012) aka ImageNet</li> <li>Larger datasets</li> <li>Improved processing power for training</li> <li>Relu</li> <li>SGD</li> </ul>"},{"location":"Book%20Notes/Understanding%20Deep%20Learning/Chapter%204%20-%20Deep%20Neural%20Networks/#composing-networks","title":"Composing networks","text":"<ul> <li>Composing shallow neural networks to get deep networks</li> </ul>"},{"location":"Book%20Notes/Understanding%20Deep%20Learning/Chapter%204%20-%20Deep%20Neural%20Networks/#general-deep-neural-networks","title":"General deep neural networks","text":"<ul> <li>Consider a deep network with 2 hidden layers, each of which has 3 hidden units<ul> <li></li> <li></li> </ul> </li> </ul>"},{"location":"Book%20Notes/Understanding%20Deep%20Learning/Chapter%204%20-%20Deep%20Neural%20Networks/#how-does-the-network-deal-with-complicated-functions","title":"How does the network deal with complicated functions?","text":""},{"location":"Book%20Notes/Understanding%20Deep%20Learning/Chapter%204%20-%20Deep%20Neural%20Networks/#hyperparameters","title":"Hyperparameters","text":"<ul> <li>Capacity : total number of hidden units </li> <li>NNs represent a family of families of functions relating input to output</li> </ul>"},{"location":"Book%20Notes/Understanding%20Deep%20Learning/Chapter%204%20-%20Deep%20Neural%20Networks/#width","title":"Width","text":"<ul> <li>Number of hidden units in each layer (\\(D_{1}, D_{2},..., D_{K}\\))</li> </ul>"},{"location":"Book%20Notes/Understanding%20Deep%20Learning/Chapter%204%20-%20Deep%20Neural%20Networks/#depth","title":"Depth","text":"<ul> <li>Number of hidden layers (\\(K\\))</li> </ul>"},{"location":"Book%20Notes/Understanding%20Deep%20Learning/Chapter%204%20-%20Deep%20Neural%20Networks/#matrix-notation-to-represent-deep-networks","title":"Matrix notation to represent deep networks","text":"<ul> <li>Matrix notation for NNs</li> </ul>"},{"location":"Book%20Notes/Understanding%20Deep%20Learning/Chapter%204%20-%20Deep%20Neural%20Networks/#shallow-vs-deep-networks","title":"Shallow vs deep networks","text":"<ul> <li>Shallow vs deep networks</li> </ul>"},{"location":"Book%20Notes/Understanding%20Deep%20Learning/Chapter%204%20-%20Deep%20Neural%20Networks/#other-notes","title":"Other notes","text":"<ul> <li>Universal Approximation Theorem</li> <li>Depth Efficiency of Neural Networks</li> <li>Width Efficiency of Neural Networks</li> </ul>"},{"location":"Book%20Notes/Understanding%20Deep%20Learning/Chapter%205%20-%20Loss%20functions/","title":"Chapter 5 - Loss functions","text":""},{"location":"Book%20Notes/Understanding%20Deep%20Learning/Chapter%205%20-%20Loss%20functions/#chapter-5-loss-functions","title":"Chapter 5 - Loss functions","text":"<ul> <li>A large list of useful loss functions can be found if you search for loss on the KB</li> <li>A loss function or cost function L[\u03c6] returns a single number that describes the mismatch between the model predictions f[xi, \u03c6] and their corresponding ground-truth outputs \\(y_i\\)</li> </ul>"},{"location":"Book%20Notes/Understanding%20Deep%20Learning/Chapter%205%20-%20Loss%20functions/#maximum-likelihood","title":"Maximum likelihood","text":""},{"location":"Book%20Notes/Understanding%20Deep%20Learning/Chapter%205%20-%20Loss%20functions/#log-likelihood-loss","title":"Log Likelihood Loss","text":""},{"location":"Book%20Notes/Understanding%20Deep%20Learning/Chapter%205%20-%20Loss%20functions/#maximum-likelihood_1","title":"Maximum Likelihood","text":""},{"location":"Book%20Notes/Understanding%20Deep%20Learning/Chapter%205%20-%20Loss%20functions/#constructing-new-loss-functions","title":"Constructing new loss functions","text":""},{"location":"Book%20Notes/Understanding%20Deep%20Learning/Chapter%205%20-%20Loss%20functions/#recipe-for-constructing-loss-functions","title":"Recipe for constructing loss functions","text":""},{"location":"Book%20Notes/Understanding%20Deep%20Learning/Chapter%205%20-%20Loss%20functions/#loss-for-univariate-regression","title":"Loss for univariate regression","text":""},{"location":"Book%20Notes/Understanding%20Deep%20Learning/Chapter%205%20-%20Loss%20functions/#loss-for-binary-classification","title":"Loss for binary classification","text":""},{"location":"Book%20Notes/Understanding%20Deep%20Learning/Chapter%205%20-%20Loss%20functions/#loss-for-multiclass-classification","title":"Loss for multiclass classification","text":""},{"location":"Book%20Notes/Understanding%20Deep%20Learning/Chapter%205%20-%20Loss%20functions/#other-notes","title":"Other notes","text":"<ul> <li>heteroscedastic nonlinear regression</li> <li>Robust regression</li> <li>Quantile Regression</li> <li>Focal Loss</li> <li>Van Mises distribution</li> <li>Non probabilistic approaches - Hinge Loss, AdaBoost</li> </ul>"},{"location":"KB/","title":"Knowledge Base","text":""},{"location":"KB/#welcome","title":"Welcome!","text":"<ul> <li>This is my little knowledge base</li> <li>If there is something you are looking for, just type it into the search bar<ul> <li>Of course, since this is not google, it is not a one stop shop</li> <li>In essence, it will have something on things that I learn</li> </ul> </li> <li>How to go about finding things?<ul> <li>Scroll the sidebar and pick something you like. Click the links inside it to go about. Choose your own adventure.</li> </ul> </li> </ul>"},{"location":"KB/#links","title":"Links","text":"<ul> <li>My Github</li> <li>Blogs</li> <li>Medium -&gt; More blogs</li> <li>My Linkedin</li> <li>My Art</li> <li>Email</li> </ul>"},{"location":"KB/0-1%20Loss/","title":"0-1 Loss","text":""},{"location":"KB/0-1%20Loss/#0-1-loss","title":"0-1 Loss","text":"<ul> <li> \\[\\begin{cases} 1 &amp; f(x)=y \\\\ 0 &amp; f(x)\\neq y\\end{cases}\\] </li> <li>Classification</li> </ul>"},{"location":"KB/1D%20piecewise%20linear%20interpolation/","title":"1D piecewise linear interpolation","text":""},{"location":"KB/1D%20piecewise%20linear%20interpolation/#1d-piecewise-linear-interpolation","title":"1D Piecewise Linear Interpolation","text":""},{"location":"KB/1D%20piecewise%20linear%20interpolation/#1d-piecewise-linear-interpolation_1","title":"1D Piecewise Linear Interpolation","text":""},{"location":"KB/1D-ALVINN/","title":"1D-ALVINN","text":""},{"location":"KB/1D-ALVINN/#1d-alvinn","title":"1D-ALVINN","text":"<ul> <li>road simulator</li> <li>predict steering angle from road conditions</li> <li>augmented to produce additional labels representing new tasks, namely: road is one or two lanes, left edge, center, and right edge road locations, location and intensity of road centerline, intensity of road surface and region bordering road</li> <li>Root MSE </li> </ul>"},{"location":"KB/2%20X%202%20Study/","title":"2 X 2 Study","text":""},{"location":"KB/2%20X%202%20Study/#2-x-2-study","title":"2 X 2 Study","text":"<ul> <li>First factor has 2 levels (The vs each)</li> <li>Second factor has 2 levels (Distributive vs. Collective situations)</li> <li>We are trying to see how these fixed factors effect responses</li> <li>We control the fixed factors</li> <li>We carefully design them so we know what category an item presented to a participant belongs to</li> </ul>"},{"location":"KB/2%20byte%20character%20set/","title":"2 byte character set","text":""},{"location":"KB/2%20byte%20character%20set/#2-byte-character-set","title":"2 Byte Character Set","text":"<ul> <li>65,536 unique characters Pairs of Bytes for a Single Character</li> <li>Sometimes single byte letters, spaces and punctuations will be interspersed with two-byte characters</li> <li>Chinese characters are encoded in two format:<ul> <li>Big-5 - Complex Mandarin</li> <li>GB - Simple form</li> </ul> </li> </ul>"},{"location":"KB/8%20bit%20character%20set/","title":"8 bit character set","text":""},{"location":"KB/8%20bit%20character%20set/#8-bit-character-set","title":"8 Bit Character Set","text":"<ul> <li>First 128 characters are reserved for ASCII</li> <li>ISO-8859 series of 10+ Character Sets for most European Languages</li> <li>Results in large number of overlapping character sets for different languages</li> </ul>"},{"location":"KB/A%20declarative%20modular%20framework%20for%20representing%20and%20applying%20ethical%20principles/","title":"A declarative modular framework for representing and applying ethical principles.","text":""},{"location":"KB/A%20declarative%20modular%20framework%20for%20representing%20and%20applying%20ethical%20principles/#a-declarative-modular-framework-for-representing-and-applying-ethical-principles","title":"A Declarative Modular Framework for Representing and Applying Ethical Principles.","text":"<ul> <li>Fiona Berreby, Gauvain Bourgne, and JeanGabriel Ganascia</li> <li>high level action language for designing ethical agents in an attempt to shift the burden of moral reasoning to the autonomous agents</li> <li>collects action, event and situation information to enable an agent to simulate the outcome of various courses of actions</li> <li>event traces are then passed to the causal engine to produce causal traces</li> <li>ethical specifications and priority of ethical considerations under a given situation are used to compute the goodness assessment on the consequences</li> <li>combined with deontological specifications (duties, obligations, rights) to produce a final rightfulness assessment</li> </ul>"},{"location":"KB/A%20low-cost%20ethics%20shaping%20approach%20for%20designing%20reinforcement%20learning%20agents/","title":"A low-cost ethics shaping approach for designing reinforcement learning agents","text":""},{"location":"KB/A%20low-cost%20ethics%20shaping%20approach%20for%20designing%20reinforcement%20learning%20agents/#a-low-cost-ethics-shaping-approach-for-designing-reinforcement-learning-agents","title":"A Low-cost Ethics Shaping Approach for Designing Reinforcement Learning Agents","text":"<ul> <li>Yueh-Hua Wu and Shou-De Lin.</li> <li>authors investigated how to enable RL to take ethics into account ethics shaping</li> <li>assuming that the majority of observed human behaviours are ethical, the proposed approach learns ethical shaping policies from available human behaviour data in given application domains</li> <li>rewards positive ethical decisions, punishes negative ethical decisions, and remains neutral when ethical considerations are not involved</li> </ul>"},{"location":"KB/A%20matter%20of%20ambiguity%3F%20Using%20eye%20movements%20to%20examine%20collective%20vs%20distributive%20interpretations%20of%20plural%20sets/","title":"A matter of ambiguity? Using eye movements to examine collective vs. distributive interpretations of plural sets","text":""},{"location":"KB/A%20matter%20of%20ambiguity%3F%20Using%20eye%20movements%20to%20examine%20collective%20vs%20distributive%20interpretations%20of%20plural%20sets/#a-matter-of-ambiguity-using-eye-movements-to-examine-collective-vs-distributive-interpretations-of-plural-sets","title":"A Matter of Ambiguity? Using Eye Movements to Examine Collective Vs. Distributive Interpretations of Plural Sets","text":"<ul> <li>Christine Boylan, Dimka Atanassov, Florian Schwarz, John Trueswell</li> </ul>"},{"location":"KB/A%20matter%20of%20ambiguity%3F%20Using%20eye%20movements%20to%20examine%20collective%20vs%20distributive%20interpretations%20of%20plural%20sets/#previous-work","title":"Previous Work","text":"<ul> <li>Frazier et al. (1999) used eye-tracking reading times to compare processing loads of sentences that were explicitly distributive (involving the adverb each), explicitly collective (involving the adverb together), and locally indeterminate at the predicate.</li> <li>Having found evidence for increased processing load associated with distributive sentences, they concluded that the processor initially pursues a collective reading, and thus the distributive / collective distinction was one of ambiguity and not vagueness.</li> <li>However, an increased processing load for the distributive reading might be expected regardless of whether the underlying representation is vague or ambiguous</li> <li>processor must stipulate a distributive operator D (the spell-out of which is each) to interpret a distributive meaning, which may incur processing delays</li> <li>Moreover, increased reading times at the point of disambiguation preclude conclusions about exactly when listeners may have committed to a distributive reading during the processing the underdetermined predicate.</li> </ul>"},{"location":"KB/A%20matter%20of%20ambiguity%3F%20Using%20eye%20movements%20to%20examine%20collective%20vs%20distributive%20interpretations%20of%20plural%20sets/#introduction","title":"Introduction","text":"<ul> <li>eye movements of listeners were recorded to investigate the representation of collective vs. distributive interpretations of plural subjects in light of the Minimal Semantic Commitment (MSC) hypothesis</li> <li>Minimal Semantic Commitment</li> <li>The crucial difference between these two proposed representation types is that an ambiguous representation forces a decision about an interpretation, while a vague representation tolerates unspecified features.</li> <li>Given the prediction that an ambiguous item will prompt the processor to converge on one interpretation even in the absence of disambiguating information, we tested whether sentences underdetermined for collectivity / distributivity would nonetheless cause listeners to converge on a single interpretation.</li> <li>Rather than relying on processing times to infer representational commitments, we employed the visual world paradigm to track which representations subjects considered over the course of hearing a sentence</li> </ul>"},{"location":"KB/A%20matter%20of%20ambiguity%3F%20Using%20eye%20movements%20to%20examine%20collective%20vs%20distributive%20interpretations%20of%20plural%20sets/#method","title":"Method","text":"<ul> <li>The eye movements of 24 participants were recorded as they listened to explicit/indeterminate collective/distributive sentences while they considered collective and distributive acts depicted on a computer screen</li> <li>An earlier switch in gaze to one of the two images would indicate a processing preference for one interpretation over the other. Results and Discussion</li> <li>Explicitly collective sentences prompted looks to the collective scenario at the point of disambiguation (i.e. together) and explicitly distributive sentences (using each) prompted looks to the distributive scenario (Fig. 2a)</li> <li>Crucially, however, the indeterminate, nulldisambiguator sentences patterned along the same trajectory as together sentences: the predicate alone prompted looks to the collective, prior to hearing the final word of the sentence</li> <li>We also compared the proportion of looks averaged across two time windows: an 800- ms interval before the onset of the predicate and an 800-ms time window following the predicate onset (Fig. 2b)</li> <li>Since the distinction between ambiguity and vagueness lies at the interface of semantics and pragmatics, it is particularly important that we find a psychometric realization of the difference between these two types of representations.</li> <li>Here we presented a method by which to investigate the time courses of these representations as they relate to distributivity, and we suggest this method may further contribute to the study of the semantics-pragmatics interface at large.</li> </ul>"},{"location":"KB/A%20matter%20of%20ambiguity%3F%20Using%20eye%20movements%20to%20examine%20collective%20vs%20distributive%20interpretations%20of%20plural%20sets/#results","title":"Results","text":"<ul> <li>In an ANOVA of proportion of looks to collective and distributive scenes, we found significant interactions between disambiguator and time window.</li> <li>In a targeted analysis of disambiguator effects in each time window, we found significant differences between together and each sentences and the null and each sentences after the predicate onset but not before</li> <li>Moreover, the together sentences did not significantly differ from the null form.</li> <li>Thus, despite a lack of explicit disambiguating information, the indeterminate, nulldisambiguator sentences nonetheless prompted looks to the collective scenario, which was reliably different from the time course of distributive-directed each sentences.</li> <li>This provides evidence that the listener has committed to the collective interpretation in the absence of disambiguating information.</li> <li>This is consistent with a theory that treats the collective / distributive distinction as ambiguous rather than vague.</li> <li>The results also indicate that this processing commitment is essentially immediate; i.e., as soon as listeners begin hearing the ambiguous predicate, they show a preference for the collective interpretation.</li> </ul>"},{"location":"KB/A%20matter%20of%20ambiguity%3F%20Using%20eye%20movements%20to%20examine%20collective%20vs%20distributive%20interpretations%20of%20plural%20sets/#pictures","title":"Pictures","text":""},{"location":"KB/A%20survey%20on%20Image%20Data%20Augmentation%20for%20Deep%20Learning/","title":"Data Augmentation with Curriculum Learning","text":"<ul> <li>@shortenSurveyImageData2019</li> <li>Data Augmentation with Curriculum Learning</li> </ul>"},{"location":"KB/A%20survey%20on%20Image%20Data%20Augmentation%20for%20Deep%20Learning/#methods","title":"Methods","text":"<ul> <li>Geometric Transformations</li> <li>Flipping</li> <li>Color Space Transform</li> <li>Cropping</li> <li>Noise Injection</li> <li>Color Space Transformations</li> <li>Kernel Filters</li> <li>Feature Space Augmentation</li> <li>SMOTE </li> <li>GAN\u2010based Data Augmentation</li> <li>Meta Learning Data Augmentations</li> <li>Neural Augmentation</li> <li>Smart Augmentation</li> <li>AutoAugment</li> <li>Augmented Random Search</li> <li>Test-time Augmentation</li> <li>SamplePairing</li> <li>Data Augmentation with Curriculum Learning</li> <li>Alleviating Class Imbalance with Data Augmentation</li> </ul>"},{"location":"KB/A%20survey%20on%20Image%20Data%20Augmentation%20for%20Deep%20Learning/#discussion","title":"Discussion","text":"<ul> <li>It is easy to explain the benefit of horizontal Flipping or random cropping</li> <li>However, it is not clear why mixing pixels or entire images together such as in PatchShuffle regularization or SamplePairing is so effective.</li> <li>dditionally, it is difficult to interpret the representations learned by neural networks for GAN-based augmentation, variational auto-encoders, and meta-learning.</li> <li>An interesting characteristic of these augmentation methods is their ability to be combined together.</li> <li>The GAN framework possesses an intrinsic property of recursion which is very interesting</li> <li>Samples taken from GANs can be augmented with traditional augmentations such as lighting filters, or even used in neural network augmentation strategies such as Smart Augmentation to create even more samples. These samples can be fed into further GANs and dramatically increase the size of the original dataset.</li> <li>An interesting question for practical Data Augmentation is how to determine postaugmented dataset size.</li> <li>no consensus about the best strategy for combining data warping and oversampling techniques</li> <li>One important consideration is the intrinsic bias in the initial, limited dataset</li> <li>There are no existing augmentation techniques that can correct a dataset that has very poor diversity with respect to the testing data</li> </ul>"},{"location":"KB/A%20voting-based%20system%20for%20ethical%20decision%20making/","title":"A voting-based system for ethical decision making","text":""},{"location":"KB/A%20voting-based%20system%20for%20ethical%20decision%20making/#a-voting-based-system-for-ethical-decision-making","title":"A Voting-based System for Ethical Decision Making","text":"<ul> <li>Ritesh Noothigattu, Snehalkumar \u2018Neil\u2019 S. Gaikwad, Edmond Awad, Sohan Dsouza, Iyad Rahwan, Pradeep Ravikumar, and Ariel D. Procaccia</li> <li>voting-based system for autonomous entities to make collective ethical decisions leverages data collected from the Moral Machine project</li> <li>Self reported preference over different outcomes under diverse ethical dilemmas are used to learn models of preference for the human voters over different alternative outcomes.</li> <li>individual models are then summarized to form a model that approximates the collective preference of all voters</li> </ul>"},{"location":"KB/ABN%20Amro%20AI%20Dev/","title":"ABN Amro AI Dev","text":""},{"location":"KB/ABN%20Amro%20AI%20Dev/#abn-amro-motivation-letter-subhaditya","title":"ABN AMRO Motivation Letter - Subhaditya","text":"<p>During COVID, it became increasingly clear how important it was to have stable financial systems. No matter your position in the financial sector, it is essential to consider the customers affected by the pipeline. That being the case, many systems can be created to improve the user experience. I have been using ABN AMRO as my bank of choice ever since I came to the Netherlands two years ago, and recently, I started noticing many AI features such as predicted recurring costs, automated filling in of details from ID cards during new user creation, etc. Given the active development of such features, I am interested in an AI development position at ABN AMRO.</p> <p>My expertise is a combination of data analytics and computer vision, and as of a month ago, I also have a Masters in Artificial Intelligence from the University of Groningen. I am familiar with the tools required for basic and advanced AI from internships, research projects, papers, freelance work, and many personal projects. In the past, I interned at the AI divisions of Emirates NBD (one of the largest banks in the UAE) and KPMG (one of the big four consultancy firms) in similar teams with similar goals of developing disruptive AI solutions, and I quite enjoyed the work there. Working in a team that has a start-up vibe but also in the context of a much larger system is also something that I enjoy. As this position offers something similar, I can contribute to any team I get the chance to work with.</p> <p>In any team, it is essential that each of the members can contribute something to the overall development of the projects. Although I have a lot to learn, I genuinely enjoy developing AI solutions that have some impact, and I hope to get to work with the amazing AI team at ABN AMRO and ship some useful solutions to the users (myself included).</p>"},{"location":"KB/ABN%20amro%20ml/","title":"ABN amro ml","text":""},{"location":"KB/ABN%20amro%20ml/#abn-amro-motivation-letter-subhaditya","title":"ABN AMRO Motivation Letter - Subhaditya","text":"<p>During COVID, it became increasingly clear just how important it was to have stable financial systems. At the end of the day, no matter what position you work in in the financial sector, it is important to consider the customers that will be affected by the pipeline. Even small-scale bank frauds can affect a large number of people. Growing up, I heard many stories of such cases and how many people lost their livelihoods because of financial crime. Back then, of course, I could do nothing about it. But now I have a small chance of contributing to the fight for financial safety, and I want to take it.</p> <p>A while back, I had the privilege of doing an internship at Emirates NBD, one of the biggest banks in the UAE. There I was involved in a project to build a similar system to detect financial fraud. I helped build the ETL pipelines and ML models to identify load defaulters and also generate forecasts of how much customers might be able to pay based on their historical records. I learned quite a bit about the proceedings there. It has been a few years since that internship, and I have completed my Masters in Artificial Intelligence as of last month. That being the case, I am now more prepared to tackle the challenges that this job might bring about.</p> <p>I hope to contribute to this and any future projects to the best of my abilities. It would be good to give customers a bit more peace of mind by helping create such systems.</p>"},{"location":"KB/ACT-R%20Chunk/","title":"ACT-R Chunk","text":""},{"location":"KB/ACT-R%20Chunk/#act-r-chunk","title":"ACT-R Chunk","text":"<ul> <li>id</li> <li>Attribute<ul> <li>Attributes point to other chunks (eg: fact3+2)</li> <li>number of attributes</li> </ul> </li> <li>Activation<ul> <li>determines priority of retrieval</li> <li>if below threshold, chunk cannot be retrieved</li> <li>determines time of retrieval</li> <li>decays with time</li> <li>depends on<ul> <li>How often retrieved or recreated</li> <li>context</li> </ul> </li> </ul> </li> </ul>"},{"location":"KB/ACT-R/","title":"ACT-R","text":""},{"location":"KB/ACT-R/#act-r","title":"ACT-R","text":"<ul> <li>Explain human behavior</li> <li>Manual Control<ul> <li>Motor Cortex</li> </ul> </li> <li>Visual Perception<ul> <li>Visual Cortex</li> </ul> </li> <li>Problem State<ul> <li>Parietal lobe</li> <li>Store intermediate results</li> </ul> </li> <li> <p>Declarative memory</p> </li> <li> <p>Perception</p> </li> <li>Control State<ul> <li>Anterior Cingulate</li> <li>Keep track of goals</li> <li>What? How far?</li> </ul> </li> <li>Time perception</li> <li>Ideas always get reinforced if retrieve</li> </ul>"},{"location":"KB/ADE20K/","title":"ADE20K","text":"<p>toc: true title: ADE20K</p> <p>categories: ['temp']</p>"},{"location":"KB/ADE20K/#ade20k","title":"ADE20K","text":""},{"location":"KB/ADVENT/","title":"ADVENT","text":""},{"location":"KB/ADVENT/#advent","title":"ADVENT","text":"<ul> <li>blog</li> <li>paper</li> <li></li> <li>ADVENT is a flexible technique for bridging the gap between two different domains through entropy minimization</li> <li>models trained only on source domain tend to produce over-confident, i.e., low-entropy, predictions on source-like images and under-confident, i.e., high-entropy, predictions on target-like ones</li> <li>Consequently by minimizing the entropy on the target domain, we make the feature distributions from the two domains more similar.</li> <li>More annotated data has been shown to always improve performance of DNNs</li> <li>Here we are working on Unsupervised DA (UDA), which is a more challenging task where we have access to labeled source samples and only unlabeled target samples. We use as source, data generated by a simulator or video game engine, while for target we consider real-data from car-mounted cameras.</li> <li>The main approaches for UDA include discrepancy minimization between source and target feature distributions usually achieved via adversarial training , , self-training with pseudo-labels  and generative approaches , .</li> <li>We present our two proposed approaches for entropy minimization using (i) an unsupervised entropy loss and (ii) adversarial training. To build our models, we start from existing semantic segmentation frameworks and add an additional network branch used for domain adaptation.</li> <li>Direct entropy minimization</li> <li>Entropy minimization by adverarial learning</li> <li>GTA5</li> <li>SYNTHIA</li> <li>Cityscapes</li> </ul>"},{"location":"KB/ALBERT/","title":"ALBERT","text":""},{"location":"KB/ALBERT/#albert","title":"ALBERT","text":"<ul> <li>Lite BERT</li> <li></li> <li>blog #Roam-Highlights</li> <li>perhaps even better when scaled to the same number of parameters as BERT</li> <li>Factorized Embedding Parameters</li> <li>cross-layer parameter sharing</li> <li>inter-sentence coherence loss</li> <li>We can see that the ALBERT base model attempts to mimic BERT base, with a hidden state size of 768, parameter sharing and a smaller embedding size due to factorization explained above. Contrary to the 108 million parameters, it has only 12 million. This makes a big difference when training the model.</li> <li>Another model, ALBERT xxlarge (extra-extra large) has 235 million parameters, with 12 encoder segments, 4096-dimensional hidden state and 128-dimensional embedding size. It also includes parameter sharing.</li> <li>GLUE</li> <li>SQuAD</li> <li>RACE</li> <li>For Factorized Embedding Parameters, the authors report good performance. Both the case where cross-layer parameters were not shared and where they were, are reported. Without sharing, larger embedding sizes give better performance. With sharing, performance boosts satisfy at an embedding size of 128 dimensions. That's why the 128-size embeddings were used in the table above.</li> <li>For cross-layer parameter sharing, the authors looked at not performing cross-layer sharing, performing cross-layer sharing for the feedforward segments only, performing sharing for the attention segments, and performing sharing for all subsegments. It turns out that sharing the parameters for the attention segments is most effective, while sharing the feedforward segment parameters does not contribute significantly. This clearly illustrates the important role of the attention mechanism in Transformer models. Because, however, all-segment sharing significantly decreases the number of parameters, at only slightly worse performance compared to attention-only sharing, the authors to perform all-segment sharing instead.</li> <li>For the SOP task, we can read that if NSP is performed on a SOP task, performance is poor. NSP on NSP of course performs well, as well as SOP on SOP. However, if SOP is performed on NSP, it performs really well. This suggests that SOP actually captures sentence coherence whereas NSP might not, and that SOP yields a better result than NSP.</li> </ul>"},{"location":"KB/ANOVA/","title":"ANOVA","text":""},{"location":"KB/ANOVA/#anova","title":"ANOVA","text":"<ul> <li>For an ANOVA you aggregate the data</li> <li>so that you get the mean responses per condition for each individual, look at the differences between their means, and then check if the differences for all the</li> </ul>"},{"location":"KB/ASCII/","title":"ASCII","text":""},{"location":"KB/ASCII/#ascii","title":"ASCII","text":"<ul> <li>7 bit character set</li> <li>Roman, Latin</li> </ul>"},{"location":"KB/AUC-Borji/","title":"AUC-Borji","text":""},{"location":"KB/AUC-Borji/#auc-borji","title":"AUC-Borji","text":"<ul> <li>\"PR is calculated in the same way as AUC-Judd\"</li> <li>\"a location-based metric\"</li> <li>FPR is obtained by calculating the proportion of negatives in the thresholded region, where the negatives are collected uniformly at random.</li> <li>\"AUC for the curve is calculated as AUC-Borji.\"</li> <li>\"TPR is calculated in the same way as AUC-Judd\"</li> <li>\"location-based metric\"</li> </ul>"},{"location":"KB/AUC-Judd/","title":"AUC-Judd","text":""},{"location":"KB/AUC-Judd/#auc-judd","title":"AUC-Judd","text":"<ul> <li>\"location-based metric\"</li> <li>The true positive rate (TPR) is calculated as the proportion of fixations falling into the thresholded saliency map</li> <li>The false positive rate (FPR) is calculated as the proportion of no-fixated pixels in the thresholded saliency map.</li> <li>After calculating TPR and FPR at each threshold, the area under the curve (AUC) is calculated for the curve of TPR against FPR.</li> </ul>"},{"location":"KB/Absolute%20Error/","title":"Absolute Error","text":""},{"location":"KB/Absolute%20Error/#absolute-error","title":"Absolute Error","text":"<ul> <li> \\[\\lvert y-f(x)\\rvert\\] </li> <li>Penalize large errors</li> </ul>"},{"location":"KB/Acceleration/","title":"Acceleration","text":""},{"location":"KB/Acceleration/#acceleration","title":"Acceleration","text":"<ul> <li>Change in Velocity wrt Time</li> <li> \\[a_{avg}= \\Delta v/\\Delta t\\] </li> </ul>"},{"location":"KB/Accessibility/","title":"Accessibility","text":""},{"location":"KB/Accessibility/#accessibility","title":"Accessibility","text":"<ul> <li>a minor subset of the reviewed contributions argues for explainability as the property that allows end users to get more involved in the process of improving and developing a certain ML model</li> <li>explainable models will ease the burden felt by non-technical or non-expert users when having to deal with algorithms that seem incomprehensible at first sight</li> </ul>"},{"location":"KB/Action%20Component/","title":"Action Component","text":""},{"location":"KB/Action%20Component/#action-component","title":"Action Component","text":"<ul> <li>reactively dispatches and monitors the execution of actions.</li> </ul>"},{"location":"KB/Action%20Potential/","title":"Action Potential","text":""},{"location":"KB/Action%20Potential/#action-potential","title":"Action Potential","text":"<ul> <li>Sometimes called a \u201cspike\u201d or described as a neuron \u201cfiring,\u201d an action potential occurs when there is a significant increase in the electrical activity along the membrane of a nerve cell. It is associated with neurons passing electrochemical messages down the axon, releasing neurotransmitters to neighboring cells in the synapse.</li> </ul>"},{"location":"KB/Action%20Transitive%20verb/","title":"Action Transitive verb","text":""},{"location":"KB/Action%20Transitive%20verb/#action-transitive-verb","title":"Action Transitive Verb","text":"<ul> <li>a verb has a direct object + verb</li> <li>I made her lower her head or body</li> </ul>"},{"location":"KB/Activation%20Functions/","title":"Activation Functions","text":"<ul> <li>general rule \"which is better\"</li> <li>SELU &gt; Elu &gt; Leaky Relu &gt; Relu &gt; Tanh &gt; Sigmoid</li> <li>loss</li> </ul>"},{"location":"KB/Active%20Compliant%20Robot/","title":"Active Compliant Robot","text":""},{"location":"KB/Active%20Compliant%20Robot/#active-compliant-robot","title":"Active Compliant Robot","text":"<ul> <li>An active compliant robot is one in which motion modification during the performance of a task is initiated by the control system. The induced motion modification is slight, but sufficient to facilitate the completion of a desired task.</li> </ul>"},{"location":"KB/Active%20tracking/","title":"Active tracking","text":""},{"location":"KB/Active%20tracking/#active-tracking","title":"Active Tracking","text":"<ul> <li>hz evaluated moment-to-moment  </li> <li>Is it already time to start preparing?</li> <li>Makes your models run much slower!</li> </ul>"},{"location":"KB/Actuator/","title":"Actuator","text":""},{"location":"KB/Actuator/#actuator","title":"Actuator","text":"<ul> <li>A power mechanism used to effect motion, or maintain position of the robot (for example, a motor which converts Electrical Energy to effect motion of the robot) (R15.07). The actuator responds to a signal received from the control system.</li> </ul>"},{"location":"KB/Acute/","title":"Acute","text":""},{"location":"KB/Acute/#acute","title":"Acute","text":"<ul> <li>A condition that is often severe but starts and ends quickly</li> </ul>"},{"location":"KB/AdaDelta/","title":"AdaDelta","text":""},{"location":"KB/AdaDelta/#adadelta","title":"AdaDelta","text":"<ul> <li> \\[RMS[\\Delta \\theta]_{t}= \\sqrt{E[\\Delta \\theta^{2}]_{t}+ \\epsilon}$$ $$\\begin{align}\\\\ &amp; \\Delta \\theta_{t}= -\\frac{RMS[\\Delta \\theta]_{t-1}}{RMS[g]_{t}}g_{t}\\\\ &amp; \\theta_{t+1}= \\theta_{t}+ \\Delta \\theta_{t} \\end{align}\\] </li> </ul>"},{"location":"KB/AdaIn/","title":"AdaIn","text":""},{"location":"KB/AdaIn/#adain","title":"AdaIn","text":"<ul> <li>paper</li> <li> \\[AdaIN(x,y) = \\sigma(y) \\big( \\frac{x-\\mu(x)}{\\sigma (x)} \\big)\\] </li> <li>Adaptive Instance Normalization\u00a0is a normalization method that aligns the mean and variance of the content features with those of the style features.</li> <li>no learnable affine features</li> <li>Adaptively computes affine params from style input</li> </ul>"},{"location":"KB/Adagrad/","title":"Adagrad","text":""},{"location":"KB/Adagrad/#adagrad","title":"Adagrad","text":"<ul> <li>Past squared grads as scaling factor for learning rate</li> <li> \\[\\begin{align}&amp; g_{t,i} = \\nabla_\\theta J(\\theta_{t,i}) \\\\ &amp; \\theta_{t+1, i} = \\theta_{t,i} - \\eta \\cdot g_{t,i} \\\\ &amp; \\theta_{t+1, i} = \\theta_{t,i} - \\frac{\\eta}{\\sqrt{G_{t,ii} + \\epsilon}} \\cdot g_{t,i} \\end{align}\\] </li> <li>Doesnt forget past gradients</li> </ul>"},{"location":"KB/Adam/","title":"Adam","text":""},{"location":"KB/Adam/#adam","title":"Adam","text":"<ul> <li>Supervised learning</li> <li>Rmsprop + Momentum</li> <li>Corrects bias in exponentially weighted averages</li> <li>Struggles with large no of params -&gt; Over smooths the gradient</li> <li> \\[\\begin{align} &amp; s_n = \\rho_1 s_{n-1} + (1-\\rho_1) g_n \\\\ &amp; r_n = \\rho_2 r_{n-1} + (1-\\rho_2) g_n \\odot g_n \\\\ &amp; \\Theta_{n+1} = \\Theta_n - \\alpha \\frac{s_n}{\\epsilon + \\sqrt{r_n}} \\frac{1-\\rho_2^n}{1-\\rho^n_1} \\end{align}\\] </li> <li>First and second moments</li> </ul>"},{"location":"KB/AdamW/","title":"AdamW","text":""},{"location":"KB/AdamW/#adamw","title":"AdamW","text":"<ul> <li>@loshchilovDecoupledWeightDecay2019</li> </ul>"},{"location":"KB/AdamW/#adam-features","title":"Adam Features","text":"<ul> <li>why use the same learning rate for every parameter, when we know that some surely need to be moved further and faster than others</li> <li>Since the square of recent gradients tells us how much signal we're getting for each weight, we can just divide by that to ensure even the most sluggish weights get their chance to shine</li> <li>with a little tweak to keep early batches from being biased</li> <li>Few research articles used it to train their models, new studies began to clearly discourage to apply it and showed on several experiments that plain ole SGD with momentum was performing better.</li> <li></li> <li></li> <li>Ilya Loshchilov and Frank Hutter pointed out in their paper that the way weight decay is implemented in Adam in every library seems to be wrong, and proposed a simple way (which they call AdamW) to fix it</li> <li> <p>PhD student Jeremy Bernstein has pointed out that the claimed convergence problems are actually just signs of poorly chosen hyper-parameters, and that perhaps amsgrad won't fix things anyway. Another PhD student, Filip Korzeniowski, showed some early results that seemed to support this discouraging view of amsgrad.</p> </li> <li> <p>Weight Decay Vs L2 Regularization</p> </li> <li>Amsgrad</li> </ul>"},{"location":"KB/Adaptive%20Gradient%20Clipping/","title":"Adaptive Gradient Clipping","text":""},{"location":"KB/Adaptive%20Gradient%20Clipping/#adaptive-gradient-clipping","title":"Adaptive Gradient Clipping","text":"<ul> <li>clips gradients to the ratio between weight gradient and weight value</li> <li>Clipping parameter is more robust than in traditional GC</li> <li>Swapping Batch Normalisation for AGC<ul> <li>faster training for equally sized models</li> <li>Allows for even larger batch size training</li> </ul> </li> </ul>"},{"location":"KB/Adaptive%20Input%20Representation/","title":"Adaptive Input Representation","text":""},{"location":"KB/Adaptive%20Input%20Representation/#adaptive-input-representation","title":"Adaptive Input Representation","text":"<ul> <li>Adaptive Input Representations for Neural Language Modeling</li> <li>varying the size of input word embeddings for neural language modeling</li> <li>improve accuracy while drastically reducing the number of model parameters</li> <li>more than twice as fast to train than the popular character input CNN while having a lower number of parameters</li> <li>English Wikipedia</li> <li>Billion Word</li> </ul>"},{"location":"KB/Adaptive%20Whitening%20Saliency/","title":"Adaptive Whitening Saliency","text":"<ul> <li>@Adaptive deconvolutional networks for mid and high level feature learning</li> <li>is based on the whitening of low-level features and has shown good performance for saliency map estimation.</li> <li>uses the features extracted by the model pre-trained for scene recognition.</li> </ul>"},{"location":"KB/Adaptive%20Whitening%20Saliency/#adaptive-whitening-saliency","title":"Adaptive Whitening Saliency","text":""},{"location":"KB/Adding%20noise/","title":"Noise","text":""},{"location":"KB/Adding%20noise/#noise","title":"Noise","text":"<ul> <li>To training input data<ul> <li>Gaussian random noise</li> <li>Augmentation</li> </ul> </li> <li>While the algo runs<ul> <li>Dropout</li> </ul> </li> <li>Stochastic ensemble learning</li> </ul>"},{"location":"KB/Additive%20Attention/","title":"Additive Attention","text":""},{"location":"KB/Additive%20Attention/#additive-attention","title":"Additive Attention","text":"<ul> <li>Bahdanau et al., 2015</li> <li>Uses a one layer feedforward network to calculate Attention Alignment</li> <li>Oh, basically it is the same as Bahdanau Attention</li> </ul>"},{"location":"KB/Adjective/","title":"Adjective","text":""},{"location":"KB/Adjective/#adjective","title":"Adjective","text":"<ul> <li>Properties of objects</li> </ul>"},{"location":"KB/Adrenal%20Glands/","title":"Adrenal Glands","text":""},{"location":"KB/Adrenal%20Glands/#adrenal-glands","title":"Adrenal Glands","text":"<ul> <li>Located on top of each kidney, these two glands are involved in the body\u2019s response to stress and help regulate growth, blood glucose levels, and the body\u2019s metabolic rate. They receive signals from the brain and secrete several different hormones in response, including cortisol and adrenaline.</li> </ul>"},{"location":"KB/Adrenaline/","title":"Adrenaline","text":""},{"location":"KB/Adrenaline/#adrenaline","title":"Adrenaline","text":"<ul> <li>Also called epinephrine, this hormone is secreted by the adrenal glands in response to stress and other challenges to the body. The release of adrenaline causes a number of changes throughout the body, including the metabolism of carbohydrates to supply the body\u2019s energy demands and increased arousal or alertness.</li> </ul>"},{"location":"KB/Advantages%20of%20Federated%20Learning/","title":"Advantages of Federated Learning","text":""},{"location":"KB/Advantages%20of%20Federated%20Learning/#advantages-of-federated-learning","title":"Advantages of Federated Learning","text":"<ul> <li>All your information is locally stored and is never sent anywhere</li> <li>Saves your personalized data from being leaked</li> <li>Removes all connections to you</li> <li>Allows the model to be updated and become better without compromizing on privacy</li> <li>Nobody \"owns\" your data except you</li> </ul>"},{"location":"KB/Adverb/","title":"Adverb","text":""},{"location":"KB/Adverb/#adverb","title":"Adverb","text":"<ul> <li>Properties of verbs</li> <li>slowly, frequently, nally</li> </ul>"},{"location":"KB/Adversarial%20Distillation/","title":"Adversarial Distillation","text":""},{"location":"KB/Adversarial%20Distillation/#adversarial-distillation","title":"Adversarial Distillation","text":"<ul> <li>Furthermore, an effective intermediate supervision, i.e., the squeezed knowledge, was used by Shu et al. (2019) to mitigate the capacity gap between the teacher and the student.</li> <li>In the third category, adversarial knowledge dis- tillation is carried out in an online manner, i.e., the teacher and the student are jointly optimized in each it- eration (Wang et al., 2018e; Chung et al., 2020)</li> </ul>"},{"location":"KB/Adversarial%20Learning/","title":"Adversarial Learning","text":""},{"location":"KB/Adversarial%20Learning/#adversarial-learning","title":"Adversarial Learning","text":"<ul> <li>Consider data in a Manifold. The PDF is concentrated along a low dim Manifold \\(\\mathcal{M}\\)</li> <li>Now the original picture is a point on the Manifold (dim = output layer size)</li> <li>Add noise to the image such that the image now appears to be in a direction orthogonal to \\(\\mathcal{M}\\) -&gt; value of PDF shrinks dramatically</li> <li>Then the network has never seen this before and will return a random classification</li> </ul>"},{"location":"KB/Adversarial%20Loss/","title":"Adversarial Loss","text":""},{"location":"KB/Adversarial%20Loss/#adversarial-loss","title":"Adversarial Loss","text":"<ul> <li>We apply Adversarial Loss to both the Generators, where the Generator tries to generate the images of it's domain, while its corresponding discriminator distinguishes between the translated samples and real samples.</li> <li>Generator aims to minimize this loss against its corresponding Discriminator that tries to maximize it.</li> </ul>"},{"location":"KB/Adversarial%20Spatial%20Dropout%20for%20Occlusion/","title":"Adversarial Spatial Dropout for Occlusion","text":""},{"location":"KB/Adversarial%20Spatial%20Dropout%20for%20Occlusion/#adversarial-spatial-dropout-for-occlusion","title":"Adversarial Spatial Dropout for Occlusion","text":"<ul> <li>Crops region pixels to generate hard positives for object detection by learning key image regions</li> <li>Within the proposed region only 1/3 pixels are dropped after sorting based on magnitudes</li> <li>Dropped values are non-contiguous here as compared to previously discussed methods</li> </ul>"},{"location":"KB/Afferent/","title":"Afferent","text":""},{"location":"KB/Afferent/#afferent","title":"Afferent","text":"<ul> <li>Sensory Division</li> </ul>"},{"location":"KB/Affine%20Function/","title":"Affine Function","text":""},{"location":"KB/Affine%20Function/#affine-function","title":"Affine Function","text":"<ul> <li>b is a bias term which is padded at the end, size 1</li> <li>Function + bias</li> </ul>"},{"location":"KB/Affordance%20Detection%20Task%20Specific/","title":"Affordance Detection Task Specific","text":""},{"location":"KB/Affordance%20Detection%20Task%20Specific/#affordance-detection-task-specific","title":"Affordance Detection Task Specific","text":"<ul> <li>Kokic, Mia, et al. \u201cAffordance Detection for Task-Specific Grasping using Deep Learning.\u201d Humanoids 2017.</li> <li></li> <li></li> <li>Affordance Score vs Contact Constraint</li> </ul>"},{"location":"KB/Afib/","title":"Afib","text":""},{"location":"KB/Afib/#afib","title":"Afib","text":"<ul> <li>Atrial fibrillation, irregular and rapid heartbeats</li> </ul>"},{"location":"KB/Agglutinating%20words/","title":"Agglutinating words","text":""},{"location":"KB/Agglutinating%20words/#agglutinating-words","title":"Agglutinating Words","text":"<ul> <li>Words divide into smaller units with clear boundaries</li> <li>Compound words are hyphenated (as in English or not as in German)</li> <li>Nachkriegszeit ; Nichtraucher</li> <li>Single token words - end-of-line</li> <li>Multi-token words \u2013 Delhi-based</li> <li>String of Morphology Affix</li> </ul>"},{"location":"KB/Akaike%20Information%20Criterion/","title":"Akaike Information Criterion","text":""},{"location":"KB/Akaike%20Information%20Criterion/#akaike-information-criterion","title":"Akaike Information Criterion","text":"<ul> <li>Considers goodness-of-fit to the data and penalizes complexity of the model</li> <li> \\[AIC=\u22122log\u2061(L)+2q\\] </li> <li>where:</li> <li>L: likelihood function for a particular model</li> <li>q: number of variables of this model</li> <li>If error terms \\(\\epsilon\\) follows Normal Distribution , expected value 0 + constant variance \\(\\(AIC = \\frac{1}{\\eta \\sigma^{2}}(RSS + 2p \\hat \\sigma^2)\\)\\)</li> </ul>"},{"location":"KB/Aleatoric/","title":"Aleatoric","text":""},{"location":"KB/Aleatoric/#aleatoric","title":"Aleatoric","text":"<ul> <li>Uncertainty part of the data</li> <li>Sensor noise etc</li> <li>Simplest noise : additive noise \\(\\(f(x) = x^{3}+ \\epsilon\\)\\)</li> <li> \\[\\epsilon \\sim \\mathcal{N}(0, \\sigma^{2})\\] </li> <li>Homoscedatic</li> <li>Heteroscedatic</li> <li></li> </ul>"},{"location":"KB/Alex%20Net/","title":"Alex Net","text":""},{"location":"KB/Alex%20Net/#alex-net","title":"Alex Net","text":"<ul> <li>Dropout + Relu</li> <li>No of filters increase according to depth</li> <li></li> </ul>"},{"location":"KB/Algebra%20Cognitive%20Tutor/","title":"Algebra Cognitive Tutor","text":""},{"location":"KB/Algebra%20Cognitive%20Tutor/#algebra-cognitive-tutor","title":"Algebra Cognitive Tutor","text":"<ul> <li>Anderson's group (Anderson, Corbett, Koedinger, &amp; Pelletier, 1995), and has been extended and marketed by Carnegie Learning ( <li>Because it is so widely used and has undergone so many positive evaluations, it is arguably the most successful ITS in the world at this time.</li> <li>Inner loop: The inner loop monitors the student's steps while solving an algebra problem.</li> <li>Although most problems, including the one in Figure 2, involve using multiple representational tools (graphs, tables, etc.) to analyze a problem scenario, some problems focus only on specific tools, such as the equation solver. In Figure 2, the problem to be solved is shown in the upper left window. This problem has four parts, labeled 1 through 4. When the student began, the cells of the table in the worksheet window at the lower left were all empty. The student has filled every cell with a number, text or an algebraic formula. In the process of figuring out what to put in the cells, the student used the solver window (upper right) and the graphing window (lower right). Each time the student filled a table cell, plotted a point on the graph, entered an equation in the solver window, etc, the tutor gave immediate feedback that told the student whether the step was correct or incorrect.</li> <li>Outer loop: The outer loop in the Algebra I Algebra Cognitive Tutor selects an algebra problem for the student to do and makes sure that the student submits a solution. The tutor uses its fine-grained assessment to select a task that exercises a few knowledge components that the student has not yet mastered. When the student has mastered all the knowledge components in a unit (all the bars turn gold), the student is advanced to the next unit in the algebra curriculum.</li> <li>Step analysis: The tutor analyzes each student step in terms of a set of anticipated steps (S. Ritter, Blessing, &amp; Wheeler, 2003). The set of anticipated steps for a problem is precomputed by solving the problem in all acceptable ways by running a rule-based problem solver. The rules are written to correspond to knowledge components. Each step is associated with the rules that were used to generate it during the precomputation. During tutoring, the student's step is matched against these anticipated steps.</li> <li>When a student's step matches an anticipated step, the student is credited in the assessment with having applied the associated knowledge components.</li>"},{"location":"KB/Allele/","title":"Allele","text":""},{"location":"KB/Allele/#allele","title":"Allele","text":"<ul> <li>One of two or more varying forms of a gene due to genetic mutation.</li> <li>Differing alleles, which can be found at the same spot on a chromosome, produce variation in inherited characteristics such as hair color or blood type.</li> </ul>"},{"location":"KB/Alleviating%20Class%20Imbalance%20with%20Data%20Augmentation/","title":"Alleviating Class Imbalance with Data Augmentation","text":""},{"location":"KB/Alleviating%20Class%20Imbalance%20with%20Data%20Augmentation/#alleviating-class-imbalance-with-data-augmentation","title":"Alleviating Class Imbalance with Data Augmentation","text":"<ul> <li>Data Augmentation falls under a Data-level solution to class imbalance and there are many different strategies for implementation.</li> <li>A naive solution to oversampling with Data Augmentation would be a simple random oversampling with small geometric transformations such as a 30\u00b0 rotation</li> <li>One problem of oversampling with basic image transformations is that it could cause overfitting on the minority class which is being oversampled</li> <li>The biases present in the minority class are more prevalent post-sampling with these techniques.</li> <li>Neural Style Transfer is an interesting way to create new images. These new images can be created either through extrapolating style with a foreign style or by interpolating styles amongst instances within the dataset.</li> <li>Oversampling with GANs can be done using the entire minority class as 'real' examples, or by using subsets of the minority class as inputs to GANs</li> <li>The use of evolutionary sampling to find these subsets to input to GANs for class sampling is a promising area for future work.</li> </ul>"},{"location":"KB/Allomorph/","title":"Allomorph","text":""},{"location":"KB/Allomorph/#allomorph","title":"Allomorph","text":"<ul> <li>Variants of the same morpheme but cant be replaced by another</li> <li>Un \u2013 happy gives unhappy</li> <li>In-comprehensible gives incomprehensible</li> </ul>"},{"location":"KB/Alpha%20Waves/","title":"Alpha Waves","text":""},{"location":"KB/Alpha%20Waves/#alpha-waves","title":"Alpha Waves","text":"<ul> <li>9-14 Hz</li> <li>Drowsy/Inhibition</li> <li></li> </ul>"},{"location":"KB/Alphacode/","title":"Alphacode","text":""},{"location":"KB/Alphacode/#alphacode","title":"Alphacode","text":"<ul> <li>Other language models have demonstrated an impressive ability to generate code, but these systems still perform poorly when evaluated on more complex, unseen problems</li> <li>Alphacode is a system for code generation for problems that require for deeper reasoning</li> <li>having an extensive dataset for training and evaluation, large and ecient transformer based architectures and a large-scale model sampling.</li> <li>model is firstly pre-trained through GitHub repositories amounting to 715.1 GB of code.</li> <li>more extensive dataset than Codex's pre training dataset.</li> <li>For the training to be better, a fine-tuning dataset is introduced from the Codeforces plataform</li> <li>Codecontests are conducted, for the validation phase, in which we better the performance of the model.</li> <li>transformer-based architecture, they use an encoder-decoder transformer architecture</li> <li>Compared to decoder-only architectures commonly used, this architecture allows for a bidirectional description and extra flexibility.</li> <li>shallow encoder and a deep encoder to further the model's ecienc</li> <li>o reduce the cost of sampling, multi-query attention is used.</li> </ul>"},{"location":"KB/Alzheimer%E2%80%99s%20Disease/","title":"Alzheimer\u2019s Disease","text":""},{"location":"KB/Alzheimer%E2%80%99s%20Disease/#alzheimers-disease","title":"Alzheimer\u2019s Disease","text":"<ul> <li>A debilitating form of dementia, this progressive and irreversible neurodegenerative disease results in the development of protein plaques and tangles that damages neurons and interfere with neural signaling, ultimately affecting memory and other important cognitive skills</li> </ul>"},{"location":"KB/Amdahl%27s%20Law/","title":"Amdahl's Law","text":""},{"location":"KB/Amdahl%27s%20Law/#amdahls-law","title":"Amdahl's Law","text":"<ul> <li>Maximum expected improvement to an overall system when only part of the system is improved</li> <li>Theoretical maximum speedup using multiple processors</li> <li> \\[Speedup = \\frac{\\text{Execution time before improvement}}{\\text{Execution time after improvement}}\\] </li> <li> \\[Speedup = ((1-f_{E})+(\\frac{f_{E}}{f_{I}}))^{-1}\\] <ul> <li>\\(f_E\\) is fraction enhanced</li> <li>\\(f_{I}\\) is factor of improvement</li> <li>\\(S_{e}\\) is speedup enhanced</li> </ul> </li> <li> \\[ExecutionTime_{new} = ExecutionTime_{old}\\times [(1-f_{E})+f_{E}/S_{e}]\\] </li> <li> \\[MaximumSpeedupp = n/(1+(n-1)f )\\] <ul> <li>n is no of processors</li> </ul> </li> </ul>"},{"location":"KB/Amino%20Acid/","title":"Amino Acid","text":""},{"location":"KB/Amino%20Acid/#amino-acid","title":"Amino Acid","text":"<ul> <li>A type of small organic molecule that has a variety of biological roles but is best known as the \u201cbuilding block\u201d of proteins.</li> </ul>"},{"location":"KB/Amsgrad/","title":"Amsgrad","text":""},{"location":"KB/Amsgrad/#amsgrad","title":"Amsgrad","text":""},{"location":"KB/Amsgrad/#modified-adam","title":"Modified Adam","text":"<ul> <li>By analyzing the proof of convergence for the Adam optimizer, they spotted a mistake in the update rule that could cause the algorithm to converge to a sub-optimal point</li> <li>update rule of Adam</li> </ul> <pre><code>avg_grads = beta1 * avg_grads + (1-beta1) * w.grad \navg_squared = beta2 * (avg_squared) + (1-beta2) * (w.grad ** 2) \nw = w - lr * avg_grads / sqrt(avg_squared)\n</code></pre> <ul> <li>We've just skipped the bias correction (useful for the beginning of training) to focus on the important point</li> <li>The error in the proof of Adam the authors spotted is that it requires the quantity</li> </ul> <pre><code>lr / sqrt(avg_squared)\n</code></pre> <ul> <li>which is the step we take in the direction of our average gradients, to be decreasing over training</li> <li>Since the learning rate is often taken constant or decreasing (except for crazy people like us trying to obtain super-convergence), the fix the authors proposed was to force the avg_squared quantity to be increasing by adding another variable to keep track of their maximums.</li> <li>Implementing amsgrad</li> <li>This causes the weight update code from the previous section to be changed to something like this:</li> </ul>"},{"location":"KB/Amsgrad/#results","title":"Results","text":"<ul> <li>Amsgrad turns out to be very disappointing. In none of our experiments did we find that it helped the slightest bit, and even if it's true that the minimum found by amsgrad is sometimes slightly lower (in terms of loss) than the one reached by Adam, the metrics (accuracy, f1 score...) always end up worse</li> <li>The proof of convergence for the Adam optimizer in deep learning (since it's for convex problems) and the mistake they found in it mattered for synthetic experiments that have nothing to do with real-</li> <li>life problems. Actual tests show that when those avg_squared gradients want to decrease, it's best for the final result to do so.</li> </ul>"},{"location":"KB/Amygdala/","title":"Amygdala","text":""},{"location":"KB/Amygdala/#amygdala","title":"Amygdala","text":"<ul> <li>Part of the brain\u2019s limbic system, this primitive brain structure lies deep in the center of the brain and is involved in emotional reactions, such as anger or fear, as well as emotionally charged memories. It also influences behavior such as feeding, sexual interest, and the immediate \u201cfight or flight\u201d stress reaction that helps ensure the person\u2019s needs are met.</li> </ul>"},{"location":"KB/Amyloid%20Plaque/","title":"Amyloid Plaque","text":""},{"location":"KB/Amyloid%20Plaque/#amyloid-plaque","title":"Amyloid Plaque","text":"<ul> <li>The sticky, abnormal accumulations of amyloid-beta protein aggregate around neurons and synapses in the memory and intellectual centers of the brain, in people with Alzheimer\u2019s. These are sometimes referred to as neuritic plaques or senile plaques. While amyloid plaques have long been considered markers of Alzheimer\u2019s, they are also found to some extent in many cognitively normal elderly people. The plaques\u2019 role in Alzheimer\u2019s neurodegeneration remains unclear.</li> </ul>"},{"location":"KB/Amyloid-beta%20%28A%CE%B2%29%20Protein/","title":"Amyloid beta (A\u03b2) Protein","text":""},{"location":"KB/Amyloid-beta%20%28A%CE%B2%29%20Protein/#amyloid-beta-a-protein","title":"Amyloid-beta (A\u03b2) Protein","text":"<ul> <li>A naturally occurring protein in brain cells. Large, abnormal clumps of this protein form the amyloid plaques that are a physiological hallmark of Alzheimer\u2019s disease.</li> </ul>"},{"location":"KB/Amyotrophic%20Lateral%20Sclerosis%20%28ALS%29/","title":"Amyotrophic Lateral Sclerosis (ALS)","text":""},{"location":"KB/Amyotrophic%20Lateral%20Sclerosis%20%28ALS%29/#amyotrophic-lateral-sclerosis-als","title":"Amyotrophic Lateral Sclerosis (ALS)","text":"<ul> <li>Also known as Lou Gehrig\u2019s disease, this neurodegenerative disease results in the death of brain cells that control the muscles.</li> </ul>"},{"location":"KB/Analysis%20of%20Explainers%20of%20Black%20Box%20Deep%20Neural%20Networks%20for%20Computer%20Vision%20A%20Survey/","title":"Analysis of Explainers of Black Box Deep Neural Networks for Computer Vision A Survey","text":"<ul> <li>10:14 --- toc: true title: Analysis of Explainers of Black Box Deep Neural Networks for Computer Vision A Survey</li> </ul> <p>categories: ['explainability'] date modified: Wednesday, October 12th 2022, 4:48:43 pm date created: Wednesday, October 12th 2022, 4:11:22 pm</p>"},{"location":"KB/Analysis%20of%20Explainers%20of%20Black%20Box%20Deep%20Neural%20Networks%20for%20Computer%20Vision%20A%20Survey/#analysis-of-explainers-of-black-box-deep-neural-networks-for-computer-vision-a-survey","title":"Analysis of Explainers of Black Box Deep Neural Networks for Computer Vision A Survey","text":"<ul> <li>@Analysis of Explainers of Black Box Deep Neural Networks for Computer Vision: A Survey</li> <li>Vanessa Buhrmester , David M\u00fcnch and Michael Arens   <code>toc</code></li> </ul>"},{"location":"KB/Analysis%20of%20Explainers%20of%20Black%20Box%20Deep%20Neural%20Networks%20for%20Computer%20Vision%20A%20Survey/#explaining-systems","title":"Explaining Systems","text":"<ul> <li>GAM</li> <li>Partial Dependence Plot</li> <li>Salience Map</li> <li>Explanator</li> <li>Comprehensibility</li> <li>Causality</li> <li>Causability</li> <li>Bayesian Rule List</li> <li>TREPAN</li> </ul>"},{"location":"KB/Analysis%20of%20Explainers%20of%20Black%20Box%20Deep%20Neural%20Networks%20for%20Computer%20Vision%20A%20Survey/#selected-dnn-explainers","title":"Selected DNN Explainers","text":"<ul> <li>Counterfactual Impact Evaluation</li> <li>DeconvNet</li> <li>Layerwise Relevance Propagation</li> <li>Parent Approximations</li> <li>RETAIn</li> <li>SP-LIME</li> <li>Deep Visual Explanation</li> <li>Prediction Difference Analysis</li> <li>Smooth-Grad</li> <li>Multimodal Explanation</li> <li>Summit</li> <li>DeepFool</li> <li>DeconvNet</li> <li>LIME</li> </ul>"},{"location":"KB/Analysis%20of%20Explainers%20of%20Black%20Box%20Deep%20Neural%20Networks%20for%20Computer%20Vision%20A%20Survey/#pictures","title":"Pictures","text":""},{"location":"KB/Andes/","title":"Andes","text":""},{"location":"KB/Andes/#andes","title":"Andes","text":"<ul> <li>Outer loop: The Andes physics tutoring system (http://www.andes.pitt.edu/) helps students learn how to solve physics problems (K. VanLehn et al., 2005; K. VanLehn et al., 2002).</li> <li>Inner loop: The student solves the problem by making steps similar to the ones they would make if solving the problem with pencil and paper. One kind of step is to type an equation into one of the numbered boxes in the right window. Another kind of step is to draw a Cartesian coordinate system, such as the one showing in the lower left. A third kind of step is to sketch a vector, then fill out a dialogue box that defines it. A vector-drawing operation was in progress at the time the screen shot was taken, so its dialogue box covers part of the screen.</li> <li>Every time the student makes a step, Andes gives immediate feedback. For most types of steps, Andes merely colors the step green if it is correct and red if it is incorrect.</li> <li>Step analysis: Like the Algebra Cognitive Tutor, Andes analyzes non-equation steps by precomputing anticipated steps and matching the student's step against them</li> <li>However, this method does not work for the equation steps because there are too many anticipated equations to generate</li> </ul>"},{"location":"KB/Angina/","title":"Angina","text":""},{"location":"KB/Angina/#angina","title":"Angina","text":"<ul> <li>Intermittent chest pain normally caused by insufficient blood flow to the heart</li> </ul>"},{"location":"KB/Angiography/","title":"Angiography","text":""},{"location":"KB/Angiography/#angiography","title":"Angiography","text":"<ul> <li>A medical imaging technique that allows clinicians to visualize the interior of blood vessels, arteries, veins, and the heart.</li> </ul>"},{"location":"KB/Anthropomorphic/","title":"Anthropomorphic","text":""},{"location":"KB/Anthropomorphic/#anthropomorphic","title":"Anthropomorphic","text":"<ul> <li>Human like in shape</li> <li>Very large number of joint</li> <li>Very large number of DOF</li> <li>Compact design</li> </ul>"},{"location":"KB/Aphasia/","title":"aphasia","text":""},{"location":"KB/Aphasia/#aphasia","title":"Aphasia","text":"<ul> <li>Disturbance of language affecting speech production, comprehension, reading or writing, due to brain injury \u2013 most commonly from stroke or trauma.</li> <li>The type of aphasia depends on the brain area damaged</li> </ul>"},{"location":"KB/Apoptosis/","title":"Apoptosis","text":""},{"location":"KB/Apoptosis/#apoptosis","title":"Apoptosis","text":"<ul> <li>A form of programmed cell death that occurs as part of normal growth and development.</li> </ul>"},{"location":"KB/Appendectomy/","title":"Appendectomy","text":""},{"location":"KB/Appendectomy/#appendectomy","title":"Appendectomy","text":"<ul> <li>Surgical procedure to remove the appendix</li> </ul>"},{"location":"KB/Application%20dependence/","title":"Application dependence","text":""},{"location":"KB/Application%20dependence/#application-dependence","title":"Application Dependence","text":"<ul> <li>Word and sentence segmentation are necessary</li> <li>Tokenizer</li> </ul>"},{"location":"KB/Application%20for%20PhD%20-%20AI%20for%20Parkinsons%20%28Subhaditya%20Mukherjee%29/","title":"Application for PhD   AI for Parkinsons (Subhaditya Mukherjee)","text":"<p>Hello Yagmur! My name is Subhaditya, it\u2019s nice to meet you :)</p> <p>I recently graduated with a masters in AI from the RUG (Groningen), and am now looking for a position where I can bring my AI knowledge to healthcare. I found out about this position quite coincidentally a few hours ago, and I knew that I could not miss the chance to apply. There is a history of Parkinsons in my family and while I could do nothing about it as a kid, now I can help contribute to the research. So, here is my formal application to the PhD position :\u2019 Artificial intelligence for monitoring of Parkinson\u2019s disease\u2019.</p> <p>I read that a lot of data was collected over the last few years using a smartwatch and video in the hope of gaining insights that would potentially help a lot of people. Here is where I can contribute to the most. I am not a healthcare professional, and all these years there wasn\u2019t any skill I could bring to the medical side of innovation. But now I have quite a bit of experience in both Computer Vision and Data Analysis. I am comfortable with using deep learning libraries, data analysis pipelines and python to create whatever is necessary for the project. My main skills lie in building these pipelines, analyzing data and subsequently using AI. Over the past few years, I have done many freelance projects, each of which has given me quite a bit of knowledge about using AI in different domains. I think that that knowledge will be useful here too. Since I am familiar with the pipeline, perhaps I can also help anticipate what might be needed for the future, and ensure we are a little more ready for that. I have also worked quite a bit with vision models and sensor data in past internships and personal projects, which is also useful experience.</p> <p>Aside from the technical aspects, I love interdisciplinary research. No field is good on it\u2019s own, AI even less so. But that being the case, I acknowledge my lack of information about medical terms. I do think that is an easy fix though, and I\u2019m willing to put in the effort to learn that side as best I can. This position is the perfect next step for me, as someone who is trying to find a position that bridges the gap between a pure AI background to something more useful, healthcare. I read that your lab is part of the ICAI network as well, and Verity healthcare too. That\u2019s so awesome and helpful.</p> <p>I have done many projects and internships over the past few years, but for most of the time, it did not feel like my work amounted to anything. This is my shot at making it count, and helping make some lives a little better. I know I applied pretty late and you probably have a lot of applications already, but I hope you give me a chance!</p>"},{"location":"KB/Applications%20of%20Knowledge%20Distillation/","title":"Applications of Knowledge Distillation","text":""},{"location":"KB/Applications%20of%20Knowledge%20Distillation/#applications-of-knowledge-distillation","title":"Applications of Knowledge Distillation","text":"<ul> <li>Specifically, in (Luo et al., 2016), the knowledge from the chosen informative neurons of top hint layer of the teacher network is trans- ferred into the student network</li> <li>A recursive knowledge distillation method was designed by using a previous student network to ini- tialize the next one (Yan et al., 2019). Since most face recognition methods perform the open-set recognition, i.e., the classes/identities on test set are unknown to the training set, the face recognition criteria are usually distance metrics between feature representations of positive and negtive samples, e.g., the angular loss in (Duong et al., 2019) and the correlated embedding loss in (Wu et al., 2020).</li> <li>Specifically, Ge et al. (2018) proposed a selective knowledge distillation method, in which the teacher network for high-resolution face recognition selectively transfers its informative facial features into the student network for low-resolution face recognition through sparse graph optimization. In (Kong et al., 2019), cross- resolution face recognition was realized by designing a resolution invariant model unifying both face halluci- nation and heterogeneous recognition sub-nets. To get efficient and effective low resolution face recognition model, the multi-kernel maximum mean discrepancy between student and teacher networks was adopted as the feature loss (Wang et al., 2019c).</li> <li>KD-based face recognition can be extended to face alignment and verification by changing the losses in knowledge distillation (Wang et al., 2017).</li> <li>For incomplete, ambiguous and redundant image labels, the label refinery model through self-distillation and label progression is proposed to learn soft, informative, collective and dynamic labels for complex image classi- fication (Bagherinezhad et al., 2018).</li> <li>To address catas- trophic forgetting with CNN in a variety of image clas- sification tasks, a learning without forgetting method for CNN, including both knowledge distillation and lifelong learning is proposed to recognize a new image task and to preserve the original tasks (Li and Hoiem, 2017).</li> <li>For improving image classification accuracy, Chen et al. (2018a) proposed the feature maps-based knowledge distillation method with GAN.</li> <li>Similar to the KD-based low-resolution face recognition, Zhu et al. (2019) proposed deep feature distillation for the low-resolution image classification, in which the output features of a student match that of teacher.</li> <li>Gordon and Duh (2019) explained the good perfor- mance of sequence-level knowledge distillation from the perspective of data augmentation and regulariza- tion. In (Kim and Rush, 2016), the effective word-level knowledge distillation is extended to the sequence- level one in the sequence generation scenario of NMT. The sequence generation student model mimics the sequence distribution of the teacher. To overcome the multilingual diversity, Tan et al. (2019) proposed multi teacher distillation, in which multiple individual models for handling bilingual pairs are teacher and a mul- tilingual model is student.</li> <li>To improve the transla- tion quality, an ensemble of mutiple NMT models as teacher supervise the student model with a data filtering method Freitag et al. (2017).</li> <li>(Wei et al., 2019) proposed a novel online knowledge distillation method, which addresses the unstableness of the training process and the decreasing performance on each validation set.</li> <li>The proposed pre- trained distillation performs well in sentiment classifi- cation, natural language inference, textual entailment. For a multi-task distillation in the context of natu- ral language understanding, Clark et al. (2019) pro- posed the single-multi born-again distillation, which is based on born-again neural networks (Furlanello et al., 2018).</li> <li>In (Perez et al., 2020), a audio-visual multi- modal knowledge distillation method is proposed. knowl- edge is transferred from the teacher models on visual and acoustic data into a student model on audio data.</li> <li>Pan et al. (2019) designed a enhanced collaborative denoising autoencoder (ECAE) model for recommender systems via knowledge distillation to capture useful knowledge from user feedbacks and to reduce noise. The unified ECAE framework contains a generation network, a retraining network and a distillation layer that trans- fers knowledge and reduces noise from the generation network.</li> </ul>"},{"location":"KB/Approximately%20Compositional%20Semantic%20Parsing/","title":"Approximately Compositional Semantic Parsing","text":""},{"location":"KB/Approximately%20Compositional%20Semantic%20Parsing/#approximately-compositional-semantic-parsing","title":"Approximately Compositional Semantic Parsing","text":"<ul> <li>which Semantic Analysis processing is applied to the result of performing a syntactic parse</li> </ul>"},{"location":"KB/Arbitrary%20Relation%20Bias/","title":"Arbitrary Relation Bias","text":""},{"location":"KB/Arbitrary%20Relation%20Bias/#arbitrary-relation-bias","title":"Arbitrary Relation Bias","text":"<ul> <li>To solve problems related to a group of things or people, it might be more informative to see them as a Graphs. The graph structure imposes arbitrary relationships between the entities, which is ideal when there\u2019s no clear sequential or local relation in the model:</li> <li></li> </ul>"},{"location":"KB/Area%20Minimization/","title":"Area Minimization","text":""},{"location":"KB/Area%20Minimization/#area-minimization","title":"Area Minimization","text":"<ul> <li>Small areas preferable</li> <li>aspect ratio can play role</li> <li></li> </ul>"},{"location":"KB/Articulated%20Manipulator/","title":"Articulated Manipulator","text":""},{"location":"KB/Articulated%20Manipulator/#articulated-manipulator","title":"Articulated Manipulator","text":"<ul> <li>A manipulator with an arm that is broken into sections (links) by one or more joints. Each of the joints represents a degree of freedom in the manipulator system and allows translation and rotary motion.</li> </ul>"},{"location":"KB/Articulation/","title":"Articulation","text":""},{"location":"KB/Articulation/#articulation","title":"Articulation","text":"<ul> <li>Describes a jointed device, such as a jointed manipulator. The joints provide rotation about a vertical axis, and elevation out of the horizontal plane. This allows a robot to be capable of reaching into confined spaces.</li> </ul>"},{"location":"KB/Assembly%20Robot/","title":"Assembly Robot","text":""},{"location":"KB/Assembly%20Robot/#assembly-robot","title":"Assembly Robot","text":"<ul> <li>A robot designed specifically for mating, fitting, or otherwise assembling various parts or components into completed products. Primarily used for grasping parts and mating or fitting them together, such as in assembly line production.</li> </ul>"},{"location":"KB/Astrocyte/","title":"Astrocyte","text":""},{"location":"KB/Astrocyte/#astrocyte","title":"Astrocyte","text":"<ul> <li>A star-shaped glial cell that supports neurons, by helping to both feed and remove waste from the cell, and otherwise modulates the activity of the neuron. Astrocytes also play critical roles in brain development and the creation of synapses.</li> </ul>"},{"location":"KB/Asymptotic%20Decider/","title":"Asymptotic Decider","text":""},{"location":"KB/Asymptotic%20Decider/#asymptotic-decider","title":"Asymptotic Decider","text":"<ul> <li>Consider the bilinear interpolant within cell</li> <li>the true isolines within a cell are hyperbolas</li> <li>investigate order of intersection points along x or y axis</li> <li>build pairs of first two and last two intersections</li> </ul>"},{"location":"KB/Attention%20Alignment/","title":"Attention Alignment","text":""},{"location":"KB/Attention%20Alignment/#attention-alignment","title":"Attention Alignment","text":"<ul> <li>If there are sequences \\(x, y\\)<ul> <li>Encoder is any Recurrent with a forward state \\(\\(\\overrightarrow h^{T}\\)\\) and \\(\\(\\overleftarrow h^{T}\\)\\) for backward</li> <li>Concat them represents the preceding and following word annotations<ul> <li>\\(\\(h_{i}= [\\overrightarrow h_{i}^{T}; \\overleftarrow h_{i}^{T}]\\)\\), \\(i = 1, \u2026, n\\)</li> <li>Decoder has hidden state \\(s_{t}= f(s_{t-1}, y_{t-1}, c_{t})\\) for the output word at position t for \\(t = 1, \u2026, m\\)<ul> <li>Context vector \\(c_{t}\\) is a sum of hidden states of the input seq, weighted by alignment scores</li> <li> \\[c_{t}= \\Sigma_{i=1}^{n}\\alpha_{t,i}h_{i}\\] </li> <li>How well the two words are aligned is given by</li> <li> \\[\\alpha_{t,i} = align(y_{t}, x_{i})\\] </li> <li>Taking softmax<ul> <li> \\[\\frac{exp(score(s_{t-1}, h_{i}))}{\\Sigma_{i'-1}^{n}exp(score(s_{t-1}, h_{i}'))}\\] </li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> <li> \\[f_{att}(h_{i}, s_{j}) = v_{a}^{T}tanh(W_{a}[h_{i};s_{j}])\\] </li> <li>\\(v_{a}\\) and \\(W_{a}\\) are the learned Attention params</li> <li>\\(h\\) is the hidden state for the encoder</li> <li>\\(s\\) is the hidden state for the decoder</li> <li>Matrix of alignment<ul> <li></li> <li>Final scores calculated with a Softmax</li> </ul> </li> </ul>"},{"location":"KB/Attention%20Based%20Distillation/","title":"Attention Based Distillation","text":""},{"location":"KB/Attention%20Based%20Distillation/#attention-based-distillation","title":"Attention Based Distillation","text":"<ul> <li>That is to say, knowledge about feature embedding is transferred using attention map functions. Unlike the attention maps, a different attentive knowledge distillation method was proposed by Song et al. (2018). An attention mechanism is used to assign different confidence rules (Song et al., 2018).</li> </ul>"},{"location":"KB/Attention%20NMT/","title":"Attention NMT","text":""},{"location":"KB/Attention%20NMT/#attention-nmt","title":"Attention NMT","text":"<ul> <li>Effective Approaches to Attention-based Neural Machine Translation</li> <li> proposed an Attention mechanism to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation</li> <li>a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time</li> <li>BLEU</li> </ul>"},{"location":"KB/Attention/","title":"Attention","text":""},{"location":"KB/Attention/#attention","title":"Attention","text":"<ul> <li>Model can decide where to look in the input</li> <li>Self Attention</li> <li>Additive Attention</li> <li>Dot Product Attention</li> <li>Location Aware Attention</li> <li>Relative Multi Head Self Attention</li> <li>Soft Attention</li> <li>Scaled Dot Product Attention</li> <li>Encoder Decoder Attention</li> <li>Multi Head Attention</li> <li>Strided Attention</li> <li>Fixed Factorization Attention</li> <li>Sliding Window Attention</li> <li>Dilated Sliding Window Attention</li> <li>Global and Sliding Window Attention</li> <li>Content Based Attention</li> <li>Location Base Attention</li> <li>Mixed chunk attention</li> </ul>"},{"location":"KB/Attentions%20and%20salience/","title":"Attentions and salience","text":""},{"location":"KB/Attentions%20and%20salience/#attentions-and-salience","title":"Attentions and Salience","text":"<ul> <li>Learning is related to attention</li> <li>We give more attention to salient items</li> <li>Selective attention leads to overshadowing</li> <li>Two cues presented together jointly predict outcomes</li> <li>salience (and then attention) leads to one cue being strongly associated</li> <li>ther cue is only weakly associated (overshadowed)</li> <li>Overshadowing leads to blocking</li> <li>Blocking could model interference</li> <li>Children learn temporal adverbs like hier late (Dale and Fenson 1996)</li> <li>L2 learners go through phases where time is marked with adverbials alone (Bardovi-Harlig 1992; Meisel 1987)</li> <li>This seems to block acquisition of other cues</li> <li>temporal adverbs are highly salient (and easier) so they get stronger associations</li> <li>even though they co-occur with different verb forms, these are hard to learn</li> </ul>"},{"location":"KB/Attentive%20CutMix/","title":"Attentive CutMix","text":""},{"location":"KB/Attentive%20CutMix/#attentive-cutmix","title":"Attentive CutMix","text":"<ul> <li>@walawalkarAttentiveCutMixEnhanced2020</li> <li>builds up on CutMix .</li> <li>Instead of random pasting, it identifies attentive patches for cutout and pastes them at the same location in the other image.</li> <li>avoids the problem of selecting a background region not important for the network and updating the label information</li> <li>A separate pre-trained network is employed to extract attentive regions.</li> <li>The attention output is mapped back onto the original image</li> </ul>"},{"location":"KB/Attribute%20Selection/","title":"Atrribute Selection","text":""},{"location":"KB/Attribute%20Selection/#atrribute-selection","title":"Atrribute Selection","text":"<ul> <li>Some tasks are harder than others due to noise and high dimensionality</li> <li>Another task can help the model to learn a shared feature that is harder to learn with a single task alone.</li> <li>helps models focus into the most important features</li> <li>Consequence of augment </li> </ul>"},{"location":"KB/AttributeMix/","title":"AttributeMix","text":"<ul> <li>@Attribute Mix: Semantic Data Augmentation for Fine Grained Recognition</li> <li>augments images based on se- mantically extracted image attributes</li> <li>Image is divided into a grid of patches where highly activated six responses are pasted onto the training image. These image pairs are selected randomly in every training iteration.</li> <li>attribute classifier by extracting k attributes (e.g., leg, head, and wings of a bird) from each image.</li> <li>The attribute mining procedure for every image is performed repetitively k times, whereas for each iteration, an attribute is masked out from the original image based on the most discriminative region in the attention map.</li> <li>attribute-level classifier is trained to generate new images for the actual classification model.</li> </ul>"},{"location":"KB/AttributeMix/#attributemix","title":"AttributeMix","text":""},{"location":"KB/AudioLM/","title":"AudioLM","text":""},{"location":"KB/AudioLM/#audiolm","title":"AudioLM","text":"<ul> <li>maps the input audio into a sequence of discrete tokens and casts audio generation as language modeling task in this representation space</li> <li>training on large corpora of raw</li> <li>audio waveforms</li> <li>learns to generate natural and coherent continuations given short prompts</li> <li>extended beyond speech by generating coherent piano music continuations, despite being trained without any symbolic representation of music</li> <li>When it comes to audio synthesis, multiple scales make achieving high audio quality while displaying consistency very challenging</li> <li>This gets achieved by this model by combining recent advances in neural audio compression, self-supervised representation learning and language modelling.</li> </ul>"},{"location":"KB/AudioSet%20classification/","title":"AudioSet classification","text":""},{"location":"KB/AudioSet%20classification/#audioset-classification","title":"AudioSet Classification","text":""},{"location":"KB/AudioSet/","title":"AudioSet","text":""},{"location":"KB/AudioSet/#audioset","title":"AudioSet","text":"<ul> <li>2, 084, 320 human-labeled 10-second sound clips drawn from YouTube videos covers ontology of 632 audio event classes </li> <li>The event classes cover a wide range of human and animal sounds, musical instruments and genres, and common everyday environmental sound </li> <li>selfsupervised learning from video and audio consistence</li> </ul>"},{"location":"KB/Auditability/","title":"Auditability","text":""},{"location":"KB/Auditability/#auditability","title":"Auditability","text":"<ul> <li>includes the assessment of algorithms, data and design processes, but preserving the intellectual property related to the AI systems</li> <li>Performing the assessment by both internal and external auditors, and making the reports available, could contribute to the trustworthiness of the technology.</li> <li>When the AI system affects fundamental rights, including safety-critical applications, it should always be audited by an external third party.</li> </ul>"},{"location":"KB/AugMix/","title":"AugMix","text":""},{"location":"KB/AugMix/#augmix","title":"AugMix","text":"<ul> <li>@hendrycksAugMixSimpleData2020</li> <li>using the input image itself</li> <li>t transforms (translate, shear, rotate and etc) the input image and mixes it with the original image</li> <li>Image transformation involves series of randomly selected augmentation operations applied with three parallel augmentation chains.</li> <li>Each chain has a composition of operations that could involve applying, for example, translation on input image followed by shear and so on</li> <li>The output of these three chains is three images mixed to form a new image.</li> <li>This new image is later mixed with the original image to generate the final augmented output image,</li> <li>while we considered mixing by alpha compositing, we chose to use elementwise convex combinations for simplicity. The k-dimensional vector of convex coefficients is randomly sampled from a Dirichlet(\u03b1, . . . , \u03b1) distribution. </li> <li>Once these images are mixed, we use a \u201cskip connection\u201d to combine the result of the augmentation chain and the original image through a second random convex combination sampled from a Beta(\u03b1, \u03b1) distribution. The final image incorporates several sources of randomness from the choice of operations, the severity of these operations, the lengths of the augmentation chains, and the mixing weights</li> <li>Jensen Shannon Divergence Consistency Loss</li> </ul>"},{"location":"KB/Augmentation-wise%20Weight%20Sharing%20strategy/","title":"Augmentation-wise Weight Sharing strategy","text":""},{"location":"KB/Augmentation-wise%20Weight%20Sharing%20strategy/#augmentation-wise-weight-sharing-strategy","title":"Augmentation-wise Weight Sharing Strategy","text":"<ul> <li>improves efficiency significantly and make it affordable to directly search on large scale datasets. </li> <li>[Lin et al., 2019] formulates the augmentation policy as a parameterized probability distribution and the parameters can be optimized jointly with network parameters, known as OHL-Auto-Aug.</li> </ul>"},{"location":"KB/Augmented%20Random%20Search/","title":"Augmented Random Search","text":""},{"location":"KB/Augmented%20Random%20Search/#augmented-random-search","title":"Augmented Random Search","text":"<ul> <li>The authors point out that the sub-policies learned from AutoAugment are inherently flawed because of the discrete search space.</li> <li>They convert the probability and magnitude of augmentations into a continuous space and search for sub-policies with ARS.</li> </ul>"},{"location":"KB/Auto%20Augment/","title":"Auto Augment","text":""},{"location":"KB/Auto%20Augment/#auto-augment","title":"Auto Augment","text":"<ul> <li>search algorithm and search space </li> <li>The search algorithm is designed to find the best policy regarding highest validation accuracy </li> <li>The search space contains many policies which details various augmentation operations and magnitudes with which the operations are applied </li> <li>Fast AutoAugment</li> <li>Population Based Augmentation</li> <li>RandAugment</li> <li>KeepAugment </li> <li>Augmentation-wise Weight Sharing strategy</li> </ul>"},{"location":"KB/Auto%20Encoders/","title":"AutoEncoder","text":""},{"location":"KB/Auto%20Encoders/#autoencoder","title":"AutoEncoder","text":"<ul> <li>Regression by predicting a reconstruction of the data</li> <li>Encoder \\(\\(E : \\mathscr{X} \\rightarrow \\mathscr{F}\\)\\)</li> <li>Decoder \\(\\(\\mathscr{F} \\rightarrow \\mathscr{D}\\)\\)</li> <li>$\\(E_\\theta, D_\\theta = argmin_{E_\\theta, D\\theta}||X-D(E(X))||^2\\)<ul> <li>Learn using Gradient Descent gradients</li> </ul> </li> <li>Compressed rep of data -&gt; Good for Classification or Regression</li> <li>MSE : Unsupervised</li> </ul>"},{"location":"KB/Auto%20Encoders/#difficulties","title":"Difficulties","text":"<ul> <li>dim \\(\\mathscr{F} \\lt \\mathscr{X}\\)<ul> <li>Cannot learn the identity function</li> </ul> </li> <li>usages<ul> <li>data compression / dimensionality reduction</li> <li>encoder to obtain features (use the latent variable as feature)</li> <li>denoising autoencoders<ul> <li>input noisy image and try to obtain image without noise</li> </ul> </li> <li>sparse auto-encoder</li> <li>contractive autoencoder</li> </ul> </li> </ul>"},{"location":"KB/Auto%20Encoders/#types","title":"Types","text":"<ul> <li>Denoising Autoencoder</li> <li>VAE</li> </ul>"},{"location":"KB/AutoAugment/","title":"AutoAugment","text":""},{"location":"KB/AutoAugment/#autoaugment","title":"AutoAugment","text":"<ul> <li>developed by Cubuk et al.</li> <li>much different approach to meta-learning than Neural Augmentation</li> <li>AutoAugment is a Reinforcement Learning algorithm that searches for an optimal augmentation policy amongst a constrained set of geometric transformations with miscellaneous levels of distortions. For example, \u2018translateX 20 pixels\u2019 could be one of the transformations in the search space</li> <li>In Reinforcement Learning algorithms, a policy is analogous to the strategy of the learning algorithm. This policy determines what actions to take at given states to achieve some goal. The AutoAugment approach learns a policy which consists of many subpolicies, each sub-policy consisting of an image transformation and a magnitude of transformation</li> <li>Reinforcement Learning is thus used as a discrete search algorithm of augmentations.</li> </ul>"},{"location":"KB/AutoDistill/","title":"AutoDistill","text":""},{"location":"KB/AutoDistill/#autodistill","title":"AutoDistill","text":"<ul> <li>AutoDistill: an End-to-End Framework to Explore and Distill Hardware-Efficient Language Models</li> <li>(NLP) tasks but they are expensive to serve due to long serving latency and large memory usage</li> <li>compress these models, knowledge distillation</li> <li>handling fast evolving models, considering serving performance, and optimizing for multiple objectives.</li> <li>end-to-end model distillation framework integrating model architecture exploration and multi-objective optimization for building hardware-efficient NLP pre-trained models</li> <li>Bayesian Optimization to conduct multi-objective Neural Architecture Search for selecting student model architectures</li> <li>proposed search comprehensively considers both prediction accuracy and serving latency on target hardware</li> <li>TPUv4i</li> <li>MobileBERT</li> <li>GLUE</li> <li>higher than BERT_BASE, DistillBERT, TinyBERT, NAS-BERT, and MobileBERT</li> </ul>"},{"location":"KB/AutoML%20Benchmark/","title":"AutoML Benchmark","text":"","tags":["openml, automl"]},{"location":"KB/AutoML%20Benchmark/#automl-benchmark","title":"AutoML Benchmark","text":"<ul> <li>@gijsbersAMLBAutoMLBenchmark2023</li> </ul>","tags":["openml, automl"]},{"location":"KB/AutoTutor/","title":"AutoTutor","text":""},{"location":"KB/AutoTutor/#autotutor","title":"AutoTutor","text":"<ul> <li>Outer loop: AutoTutor (http://demo.autotutor.org/) teaches by engaging students in a natural language (English) dialogue</li> <li>For AutoTutor, a task corresponds to a single question, such as the one shown in the upper right of Figure 4, that has a complex answer. Its outer loop consists of selecting such a question and working with the student to get it completely answered.</li> <li>Inner loop: The inner loop starts with the student typing in an initial answer to the top level question (see Figure 4; the student types into the lower right window; the whole dialogue is displayed in the lower left window).</li> <li>AutoTutor has been used to compare output modalities.</li> <li>An AutoTutor dialogue is composed of tutor turns alternating with student turns. On most of the student turns, the student makes a small contribution toward completing the whole task. Those student turns count as steps, because they are a user interface event that contributes to a solution of the whole task</li> <li>Step analysis:</li> <li>These are conclusions that are produced by applying knowledge components. For instance, the first two items above correspond to distinct learning events, wherein the student has applied the same Knowledge Component,</li> <li>In addition to having a list of all anticipated correct learning events, such as the ones mentioned above, AutoTutor has a list of several of the most important incorrect learning events</li> <li>To find out which learning events underlie the student's step, AutoTutor measures the semantic similarity between the text of the Learning Event and the text of the step. It uses a measure called Latent Semantic Analysis</li> </ul>"},{"location":"KB/Automation%20Bias/","title":"Automation Bias","text":""},{"location":"KB/Automation%20Bias/#automation-bias","title":"Automation Bias","text":"<ul> <li>When a human decision maker favors recommendations made by an automated decision-making system over information made without automation, even when the automated decision-making system makes errors.</li> </ul>"},{"location":"KB/Autonomic/","title":"Autonomic","text":""},{"location":"KB/Autonomic/#autonomic","title":"Autonomic","text":"<ul> <li>Involuntary</li> <li>Sympathetic + Parasympathetic</li> </ul>"},{"location":"KB/Autoregressive/","title":"Autoregressive","text":""},{"location":"KB/Autoregressive/#autoregressive","title":"Autoregressive","text":"<ul> <li>predict the future by past of TIme Series</li> <li>Multi Variate AR</li> </ul>"},{"location":"KB/Average%20Filter/","title":"Average Filter","text":""},{"location":"KB/Average%20Filter/#average-filter","title":"Average Filter","text":"<ul> <li>Each grey value is replaced by the average value in the kernel in a local surrounding</li> <li>linear</li> <li>flatten edges</li> </ul>"},{"location":"KB/Average%20Number%20of%20Stored%20Instances%20per%20Category/","title":"Average Number of Stored Instances per Category","text":""},{"location":"KB/Average%20Number%20of%20Stored%20Instances%20per%20Category/#average-number-of-stored-instances-per-category","title":"Average Number of Stored Instances per Category","text":"<ul> <li>memory resource required for learning</li> </ul>"},{"location":"KB/Axon%20Terminal/","title":"Axon Terminal","text":""},{"location":"KB/Axon%20Terminal/#axon-terminal","title":"Axon Terminal","text":"<ul> <li>The very end of the axon, where electrochemical signals are passed through the synapse to neighboring cells by means of neurotransmitters and other neurochemicals. A collection of axons coming from, or going to, a specific brain area may be called a white matter fiber tract.</li> </ul>"},{"location":"KB/Axon/","title":"Axon","text":""},{"location":"KB/Axon/#axon","title":"Axon","text":"<ul> <li>A long, single nerve fiber that transmits messages, via electrochemical impulses, from the body of the neuron to dendrites of other neurons, or directly to body tissues such as muscles.</li> </ul>"},{"location":"KB/BART/","title":"BART","text":""},{"location":"KB/BART/#bart","title":"BART","text":"<ul> <li>BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</li> <li>denoising autoencoder</li> <li>pretraining sequence-to-sequence</li> <li>trained by corrupting text with an arbitrary noising function, and learning a model to reconstruct the original text</li> <li>generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder),</li> <li>finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token</li> <li>With BERT, random tokens are replaced with masks, and the document is encoded bidirectionally. Missing tokens are predicted independently, so BERT cannot easily be used for generation.</li> <li>With GPT, tokens are predicted auto-regressively (generation of a new token is conditioned on the prior tokens), meaning GPT can be used for generation.</li> <li>noising schemes to an input document and thus corrupts it by replacing spans of text with mask symbols</li> <li>effective when finetuned for text generation but also works well for comprehension tasks</li> <li>matches the performance of RoBERTa with comparable training resource</li> <li>GLUE</li> <li>SQuAD</li> </ul>"},{"location":"KB/BCE%20with%20Logits/","title":"BCE Logits","text":""},{"location":"KB/BCE%20with%20Logits/#bce-logits","title":"BCE Logits","text":"<ul> <li>Cross Entropy + logits \\(\\(\\left( - \\mathrm{sum}\\left( y \\cdot \\mathrm{logsoftmax}\\left( \u0177 \\right) \\cdot weight \\right) \\right) \\cdot \\mathrm{//}\\left( 1, \\mathrm{size}\\left( y, 2 \\right) \\right)\\)\\)</li> </ul>"},{"location":"KB/BCE%20with%20Logits/#_1","title":"\u2026","text":""},{"location":"KB/BERT/","title":"BERT","text":""},{"location":"KB/BERT/#bert","title":"BERT","text":"<ul> <li>Bidirectional Encoder rep from transformers</li> <li>Uses Token Embedding</li> <li>Self Supervised</li> <li>Masked language modeling, next sentence prediction</li> <li></li> <li>[CLS] : start of classification task, [SEP] between sentences, [MASK] : masked token</li> <li> <p>christianversloot #Roam-Highlights</p> <ul> <li>BERT base \\(\\text{BERT}_\\text{BASE}\\), which has 12 Encoder Segments stacked on top of each other, has 768-dimensional intermediate state, and utilizes 12 attention heads (with hence 768/12 = 64-dimensional attention heads).</li> <li>BERT large (\\(\\text{BERT}_\\text{LARGE}\\)), which has 24 Encoder Segments, 1024-dimensional intermediate state, and 16 attention heads (64-dimensional attention heads again).</li> <li>BERT utilizes the encoder segment, meaning that it outputs some vectors \\(T_i\\) for every token. The first vector, \\(T_0\\), is also called \\(C\\) in the BERT paper: it is the \"class vector\" that contains sentence-level information (or in the case of multiple sentences, information about the sentence pair). All other vectors are vectors representing information about the specific token.</li> <li>In other words, structuring BERT this way allows us to perform sentence-level tasks and token-level tasks. If we use BERT and want to work with sentence-level information, we build on top of the \\(C\\) token.</li> <li> <p>Masked Language Modeling</p> </li> <li> <p>Next Sentence Prediction (NSP)</p> <ul> <li>This task ensures that the model learns sentence-level information. It is also really simple, and is the reason why the BERT inputs can sometimes be a pair of sentences. NSP involves textual entailment, or understanding the relationship between two sentences.</li> <li>Constructing a training dataset for this task is simple: given an unlabeled corpus, we take a phrase, and take the next one for the 50% of cases where BERT has a next sentence. We take another phase at random given A for the 50% where this is not the case (Devlin et al., 2018). This way, we can construct a dataset where there is a 50/50 split between 'is next' and 'is not next' sentences.</li> </ul> </li> <li>BooksCorpus</li> <li>English Wikipedia</li> <li>It thus does not matter whether your downstream task involves single text or text pairs: BERT can handle it.<ul> <li>Sentence pairs in paraphrasing tasks.</li> <li>Hypothesis-premise pairs in textual entailment tasks.</li> <li>Question-answer pairs in question answering.</li> <li>Text-empty pair in text classification.</li> </ul> </li> <li>Yes, you read it right: sentence B is empty if your goal is to fine-tune for text classification. There simply is no sentence after the token.</li> <li>Fine-tuning is also really inexpensive</li> </ul> </li> </ul>"},{"location":"KB/BEiT/","title":"BEiT","text":"<p>toc: true title: BEiT</p> <p>categories: ['temp']</p>"},{"location":"KB/BEiT/#beit","title":"BEiT","text":"<ul> <li>BEiT: BERT Pre-Training of Image Transformers<ul> <li>Self Supervised pre-trained representation model</li> <li>Bidirectional Encoder Decoder Attention representations from Vision Transformer</li> <li>masked image modeling task to pretrain vision Transformers</li> <li>each image has two views in their pre-training</li> <li>the embeddings of which are calculated as linear projections of flattened patches</li> <li>visual tokens</li> <li>discrete VAE (dVAE) which acts as an \u201cimage tokenizer\u201d learnt via autoencoding-style reconstruction</li> <li>input image is tokenized into discrete visual tokens obtained by the latent codes of the discrete VAE</li> <li>proposed method is critical to make BERT like pre-training (i.e., auto-encoding with masked input) work well for image Transformers</li> <li>automatically acquired knowledge about semantic regions, without using any human-annotated data</li> <li>randomly masks some image patches and feeds them into the backbone Transformer</li> <li>pre-training objective is to recover the original visual tokens based on the corrupted image patches</li> <li>directly fine-tune the model parameters on downstream tasks by appending task layers upon the pretrained encoder</li> <li>ImageNet</li> <li>outperforming from-scratch DeiT</li> </ul> </li> </ul>"},{"location":"KB/BLEU/","title":"BLEU","text":""},{"location":"KB/BLEU/#bleu","title":"BLEU","text":""},{"location":"KB/BOLD/","title":"BOLD","text":""},{"location":"KB/BOLD/#bold","title":"BOLD","text":"<ul> <li>Blood oxygenation level dependant signal<ul> <li>Indirect measure of neural activity</li> <li>Blood goes to a place</li> </ul> </li> <li>When neurons fire or increase their firing rate, they draw on oxygen and various nutrients.</li> <li>The circulatory system of the brain reacts by sending the region that just fired more highly-oxygenated blood than is needed. This results in an increased blood oxygen level in the activated region.</li> <li>With right pulse sequence, an MRI scanner is able to detect this difference in blood oxygen level</li> <li>Factors such as drugs, substances \u00a0and excitation \u00a0have been shown to increase BOLD response. Conversely, age and brain pathology \u00a0have been shown to decrease BOLD response</li> </ul>"},{"location":"KB/BUCC/","title":"BUCC","text":""},{"location":"KB/BUCC/#bucc","title":"BUCC","text":""},{"location":"KB/BYOL%20Loss/","title":"BYOL Loss","text":"<p>toc: true title: BYOL Loss</p> <p>categories: ['temp']</p>"},{"location":"KB/BYOL%20Loss/#byol-loss","title":"BYOL Loss","text":"<ul> <li>Similarity loss between \\(q_\\theta (z_\\theta)\\) and \\(sg(z^{'}_{\\xi})\\)</li> <li>\\(\\theta\\) is trained weights</li> <li>\\(\\xi\\) is exponentially moving average of \\(\\theta\\) and sg is stop gradient</li> <li>\\(f_\\theta\\) is discarded, \\(y_\\theta\\) is used as image representation</li> <li></li> </ul>"},{"location":"KB/BYOL/","title":"BYOL","text":"<p>toc: true title: BYOL</p> <p>categories: ['temp']</p>"},{"location":"KB/BYOL/#byol","title":"BYOL","text":"<ul> <li>Bootstrap Your Own Latent: a New Approach to Self-supervised Learning<ul> <li>Self Supervised image representation learning</li> <li>predicting previous versions of its outputs, without using negative pairs</li> <li>two neural networks, referred to as online and target networks</li> <li>that interact and learn from each other</li> <li>From an augmented view of an image, they train the Online Learning network to predict the target network representation of the same image under a different augmented view</li> <li>update the target network with a slow-moving average of the online network</li> <li>ImageNet</li> <li>Res Net</li> <li>dependent on existing sets of Augmentation that are specific to vision applications</li> <li>BYOL Loss</li> </ul> </li> </ul>"},{"location":"KB/Back%20Propamine/","title":"backpropamine","text":""},{"location":"KB/Back%20Propamine/#backpropamine","title":"Backpropamine","text":"<ul> <li>@miconiBackpropamineTrainingSelfmodifying2020</li> </ul>"},{"location":"KB/Back%20Propamine/#abstract","title":"ABSTRACT","text":"<ul> <li>The impressive lifelong learning in animal brains is primarily enabled by plastic changes in synaptic connectivity</li> <li>Importantly, these changes are not passive, but are actively controlled by neuromodulation, which is itself under the control of the brain</li> <li>The resulting self-modifying abilities of the brain play an important role in learningand adaptation, and are a major basis for biological reinforcement learning</li> <li>artificial neural networks with such neuromodulated plasticity can be trained withgradient descen</li> <li>differentiable Hebbian plasticity</li> <li>neuromodulated plasticity improves the performance of neural networks on bothreinforcement learning and supervised learning tasks</li> <li>neuromodulated plastic LSTMs with millions of parameters outperform standard LSTMs on a benchmark language modeling task</li> </ul>"},{"location":"KB/Back%20Propamine/#background-differentiable-hebbian-plasticity","title":"BACKGROUND: DIFFERENTIABLE HEBBIAN PLASTICITY","text":"<ul> <li>(Miconi, 2016; Miconi et al., 2018)</li> <li>allows gradient descent to optimize not just the weights, but also the plasticity of each connection</li> <li>each connection in the network is augmented with a Hebbian plastic component that grows and decays automatically as a result of ongoing activity. In effect, each connection contains a fixed and a plastic component:</li> <li>Hebbi,j is initialized to zero at the beginning of each episode/lifetime, and is updatedautomatically</li> <li>purely episodic/intra-life quantity</li> <li>wi,j, fi,j and \u2318 are the structural components of the network, which are optimizedby gradient descent between episodes/lifetimes to minimize the expected loss overan episode.</li> <li>Clip(x) in Eq. 2 is any function or procedure that constrains Hebbi,j to the [1, 1]range, to negate the inherent instability of Hebbian learning.</li> <li>distinction between the \u2318 and fi,j parameters: \u2318 is the intra-life \"learning rate\" ofplastic connections</li> <li>determines how fast new information is incorporated into the plastic component</li> <li>fi,j is a scale parameter, which determines the maximum magnitude of the plasticcomponent (since Hebbi,j is constrained to the [-1,1] range)</li> <li>in contrast to other approaches using uniform plasticity (Schmidhuber, 1993a),including \"fast weights\" (Ba et al., 2016), the amount of plasticity in each connection(represented by fi,j ) is trainable, allowing the meta-optimizer to design complexlearning strategies</li> <li>implementing a plastic recurrent network only requires less than four additional linesof code over a standard recurrent network implementation</li> </ul>"},{"location":"KB/Back%20Propamine/#backpropamine-differentiable-neuromodulation-of-plasticity","title":"BACKPROPAMINE: DIFFERENTIABLE NEUROMODULATION OF PLASTICITY","text":"<ul> <li>plasticity is modulated on a moment-to-moment basis by a network controlled neuromodulatory signal M (t)</li> <li>The computation of M (t) could be done in various ways; at present, it is simply asingle scalar output of the network, which is used either directly (for the simple RLtasks) or passed through a meta-learned vector of weights (one for eachconnection, for the language modeling task)</li> </ul>"},{"location":"KB/Back%20Propamine/#simple-neuromodulation","title":"SIMPLE NEUROMODULATION","text":"<ul> <li>make the (global) \u2318 parameter depend on the output of one or more neurons in the network</li> <li>Because \u2318 essentially determines the rate of plastic change, placing it under network control allows the network to determine how plastic connections should beat any given time.</li> <li>only modification to the equations above in this simple neuromodulation variant is to replace \u2318 in Eq. 2 with the network-computed, time-varying neuromodulatory signalM (t)</li> <li></li> </ul>"},{"location":"KB/Back%20Propamine/#retroactive-neuromodulation-and-eligibility-traces","title":"RETROACTIVE NEUROMODULATION AND ELIGIBILITY TRACES","text":"<ul> <li>alternative neuromodulation scheme that takes inspiration from the short-term retroactive effects of neuromodulatory dopamine on Hebbian plasticity in animalbrains</li> <li>dopamine was shown to retroactively gate the plasticity induced by past activity,within a short time window of about 1s (Yagishita et al., 2014; He et al., 2015; Fisheret al., 2017; Cassenaer &amp; Laurent, 2012)</li> <li>Thus, Hebbian plasticity does not directly modify the synaptic weights, but creates a fast-decaying \"potential\" weight change, which is only incorporated into the actual weights if the synapse receives dopamine within a short time window</li> <li>eligibility trace</li> <li>keeping memory of which synapses contributed to recent activity, while thedopamine signal modulates the transformation of these eligibility traces into actualplastic changes.</li> <li></li> </ul>"},{"location":"KB/Back%20To%20Front%20Raycasting/","title":"Back To Front Raycasting","text":""},{"location":"KB/Back%20To%20Front%20Raycasting/#back-to-front-raycasting","title":"Back To Front Raycasting","text":"<ul> <li>blending over operator for semi-transparent geometry</li> </ul>"},{"location":"KB/Backprop/","title":"Backprop","text":""},{"location":"KB/Backprop/#backprop","title":"Backprop","text":"<ul> <li>Gradient (\\(\\nabla l(\\theta) = [\\frac{\\partial l}{\\partial \\theta_1}(\\theta) , \u2026 , \\frac{\\partial L}{\\partial \\theta_L}(\\theta)]\\)\\)<ul> <li>partial derivs of the loss wrt weights  Forward pass<ul> <li>Store result of operation in u</li> </ul> </li> <li>Backward Pass<ul> <li>Traverse the graph backwards<ul> <li>Chain Rule : \\(\\(\\frac{dl}{d\\theta_i} = \\Sigma_{k \\in parents(l)} \\frac{\\partial l}{\\partial u_k} \\frac{\\partial u_k}{\\partial \\theta_i}\\)\\)</li> <li> \\[\\begin{align} &amp;\\frac{d\\hat y}{d\\mathbf{W_1}}\\\\ &amp;= \\frac{\\partial \\hat y}{\\partial u_2} \\frac{\\partial u_2}{\\partial h_1} \\frac{\\partial h_1}{\\partial u_1} \\frac{\\partial u_1}{\\partial \\mathbf{W_1}} \\\\ &amp;= \\frac{\\partial \\sigma (u2)}{\\partial u_2} \\frac{\\partial \\mathbf{W}^T_2 h_1}{\\partial h_1} \\frac{\\partial \\sigma (u1)}{\\partial u_1} \\frac{\\partial \\mathbf{W}^T_1 x}{\\partial \\mathbf{W}_1} \\end{align}\\] </li> <li>Collecting all the (\\(\\partial \\sigma(u_i)\\)\\) wrt params -&gt; #gradients exponentially decreases wrt depth of the network : Vanishing<ul> <li>Solved by Activation Functions</li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"KB/Bag%20of%20Words%20robotics/","title":"Bag of Words Robotics","text":""},{"location":"KB/Bag%20of%20Words%20robotics/#bag-of-words-robotics","title":"Bag of Words Robotics","text":"<ul> <li>Recognizing objects using local descriptors would be computationally expensive.</li> <li>The number of local features for a given object mainly depends on the size of the object, and therefore, varies for different objects.</li> <li>The key idea for fast 3D object recognition is to use mechanisms for representing objects in a compact and uniform format (e.g., histogram).</li> <li>If we represent objects in a uniform format, then we can apply ML algorithms</li> <li>Compute local features for all the discovered objects and make a pool of features.</li> <li>A dictionary is generated via clustering of the pool of features into N clusters (the number of the clusters is the codebook size).</li> <li>Visual word are then defined as the centres of the extracted clusters.</li> <li>Finally, each object is described (abstracted) by a histogram of occurrences of these visual words.</li> <li></li> <li></li> </ul>"},{"location":"KB/Bag%20of%20n-grams/","title":"Bag of n-grams","text":""},{"location":"KB/Bag%20of%20n-grams/#bag-of-n-grams","title":"Bag of N-grams","text":"<ul> <li>consider word phrases of length n to represent documents as fixed-length vectors to capture local word order</li> <li>suffer from data sparsity and high dimensionality.</li> </ul>"},{"location":"KB/Bag%20of%20words/","title":"Bag of words","text":""},{"location":"KB/Bag%20of%20words/#bag-of-words","title":"Bag of Words","text":""},{"location":"KB/Bag%20of%20words/#explained","title":"Explained","text":"<ul> <li>Count based conversion of document into fixed length vectors of integers</li> <li><code>John likes to watch movies. Mary likes movies too.</code><ul> <li><code>[1, 2, 1, 1, 2, 1, 1, 0, 0, 0, 0]</code></li> </ul> </li> <li><code>John also likes to watch football games. Mary hates football.</code><ul> <li><code>[1, 1, 1, 1, 0, 1, 0, 1, 2, 1, 1]</code></li> </ul> </li> <li>Order is arbitary</li> </ul>"},{"location":"KB/Bag%20of%20words/#disadvantages","title":"Disadvantages","text":"<ul> <li>Lose all info about word order</li> <li>Does not learn meaning of the words, so distance isnt very accurate</li> <li>Somewhat solved by Bag of n-grams</li> <li>Curse Of Dimensionality</li> </ul>"},{"location":"KB/Bahdanau%20Attention/","title":"Bahdanau Attention","text":""},{"location":"KB/Bahdanau%20Attention/#bahdanau-attention","title":"Bahdanau Attention","text":"<ul> <li>Neural Machine Translation by Jointly Learning to Align and Translate<ul> <li>Attention mechanism (borrowed from the field of information retrieval) within the context of NLP</li> </ul> </li> <li>Same as Additive Attention</li> <li></li> </ul>"},{"location":"KB/Barycentric%20Interpolation/","title":"Barycentric Interpolation","text":""},{"location":"KB/Barycentric%20Interpolation/#barycentric-interpolation","title":"Barycentric Interpolation","text":"<ul> <li>d+1 points</li> <li>Point x is an Affine Function of \\(x_i\\)</li> </ul>"},{"location":"KB/Basal%20Ganglia/","title":"Basal Ganglia","text":""},{"location":"KB/Basal%20Ganglia/#basal-ganglia","title":"Basal Ganglia","text":"<ul> <li>includes the caudate, putamen and globus pallidus. These nuclei work with the cerebellum to coordinate fine motions, such as fingertip movements.</li> <li>Hypothalamus</li> <li>Pituitary gland</li> <li>Pineal gland</li> <li>Thalamus</li> <li>Limbic system</li> </ul>"},{"location":"KB/Basal%20Ganglia/#basal-ganglia_1","title":"Basal Ganglia","text":"<ul> <li>A group of structures below the cortex involved in motor, cognitive, and emotional functions.</li> </ul>"},{"location":"KB/Base%20Link/","title":"Base Link","text":""},{"location":"KB/Base%20Link/#base-link","title":"Base Link","text":"<ul> <li>The stationary base structure of a robot arm that supports the first joint.</li> </ul>"},{"location":"KB/Basic%20GAN/","title":"Basic GAN","text":""},{"location":"KB/Basic%20GAN/#basic-gan","title":"Basic GAN","text":"<ul> <li>Generative Adversarial Networks</li> <li>Learn a prob distribution directly from data generated by that distribution</li> <li>no need for any Markov Chain or unrolled approximate inference networks during either training or generation of samples</li> <li>Adversarial Learning</li> <li></li> <li>Min Max game \\(\\(max_D min_G V(G,D)\\)\\) where \\(\\(V(G,D) = \\mathbb{E}_{p_{data}(x)}logD(x) + \\mathbb{E}_{p_{data}(x)}log(1-D(x))\\)\\)</li> <li>G : Gradient Descent gradients</li> <li>D : Gradient Ascent</li> <li>Discriminator Loss (Given Generator)<ul> <li> \\[L_{disc}(D_\\theta) =-\\frac{1}{2}(\\mathbb{E}_{x\\sim p_{real}(x)}[log(D_{theta}(x))] + \\mathbb{E}_{x\\sim p_{latent}(x)}[log(1- D_\\theta(G_\\phi(z)))])\\] </li> </ul> </li> <li>Generator Loss (Given Discriminator)<ul> <li> \\[L_{gen}(G_{\\phi})= - \\mathbb{E}_{z\\sim p_{latent}(z)}[log(D_\\theta(G_\\phi(z)))]\\] </li> <li>This is low if Discriminator is fooled by Gen, \\(D_{\\theta}(x_{gen}) \\approx 1\\)</li> </ul> </li> </ul>"},{"location":"KB/Basic%20GAN/#training","title":"Training","text":"<ul> <li>pick mini batch of samples\u00a0</li> <li>update discriminator with\u00a0Gradient Descent based** on discriminator loss with generator obtained from previous update</li> <li>update the generator with\u00a0Gradient Descent based on generator loss with the discriminator from the previous step</li> </ul>"},{"location":"KB/Basic%20GAN/#issues","title":"Issues","text":"<ul> <li>Mode Collapse</li> </ul>"},{"location":"KB/Basic%20RNN%20Architectures/","title":"Basic RNN Architectures","text":""},{"location":"KB/Basic%20RNN%20Architectures/#basic-rnn-architectures","title":"Basic RNN Architectures","text":"<ul> <li>Recurrent</li> <li>SRN</li> <li>Stacking RNN</li> <li>Bi Directional RNN</li> <li>Seq2Seq</li> <li>Temporal Conv</li> <li>Gated Recurrent Unit (GRU|[GRU|GRU)](Gated%20Recurrent%20Unit%20(GRU)](GRU)](Gated Recurrent Unit (GRU|Gated Recurrent Unit (GRU|GRU).md).md)</li> <li>Long Short Term Memory (LSTM|[LSTM|LSTM)](Long%20Short%20Term%20Memory%20(LSTM)](LSTM)](Long Short Term Memory (LSTM|Long Short Term Memory (LSTM|LSTM).md).md)</li> </ul>"},{"location":"KB/Basic%20Transformer/","title":"Basic Transformer","text":""},{"location":"KB/Basic%20Transformer/#basic-transformer","title":"Basic Transformer","text":"<ul> <li>Feed forward blocks, are two Dense MLPs with Relu. Residual connections in between</li> <li>Uses Attention</li> <li>Embedding Layers transform between 1 hot and vector rep</li> <li>Position Encoding + Token Embedding</li> <li>Position Wise Feed Forward</li> </ul>"},{"location":"KB/Basics%20of%20Federated%20Learning/","title":"Basics of Federated Learning","text":""},{"location":"KB/Basics%20of%20Federated%20Learning/#basics-of-federated-learning","title":"Basics of Federated Learning","text":"<ol> <li>Get data (Hopefully a lot)</li> <li>Preprocess (aka clean up) the data</li> <li>Find/create an architecture</li> <li>Train the model using the data(1) and the architecture(3). This step is done once. And then periodically updated as the data changes over time. Keep this in mind.</li> <li>Push the model out to n users</li> <li>Collect data about how well the model did. (Bye bye privacy)</li> <li>Send this data back to the main model. 7 (#new). Find the difference between the original model and the personalized one's parameters. Do this for multiple users. Remove identifiable information. 7.1 (#new). Aggregate (eg. average) the information and then send that to the main model</li> <li>Retrain the model on new data</li> </ol>"},{"location":"KB/Basilar%20Artery/","title":"Basilar Artery","text":""},{"location":"KB/Basilar%20Artery/#basilar-artery","title":"Basilar Artery","text":"<ul> <li>Located at the base of the skull, the basilar artery is a large, specialized blood vessel that supplies oxygenated blood to the brain and nervous system.</li> </ul>"},{"location":"KB/Batch%20Normalization/","title":"Batch Normalization","text":""},{"location":"KB/Batch%20Normalization/#batch-normalization","title":"Batch Normalization","text":"<ul> <li>bias=False for Linear/Conv2D for input and True for output #tricks</li> <li>Normalizes #activations</li> <li>Input distributions change per layer -&gt; Make sure they stay similar</li> <li>Reduces co variate shift because now the network must adapt per layer</li> <li>During testing : use stats saved during training</li> <li>Simplifies learning dynamics<ul> <li>Can use larger learning rate</li> <li>Higher order interactions are suppressed because the mean and std are independant of the activations which makes training easier</li> </ul> </li> <li>Cant work with small batches. Not great with RNN</li> <li></li> <li> \\[\\mu_j \\leftarrow \\frac{1}{m}\\Sigma_{i=1}^m x_{ij}\\] </li> <li> \\[\\sigma^2_j \\leftarrow \\frac{1}{m}\\Sigma^m_{i=1}(x_{ij}-\\mu_j)^2\\] </li> <li> \\[\\hat x_{ij} \\leftarrow \\frac{x_{ij}-\\mu_j}{\\sqrt{\\sigma^2_j + \\epsilon}}\\] </li> <li> \\[\\hat x_{ij} \\leftarrow \\gamma \\hat x_{ij} + \\beta\\] </li> </ul>"},{"location":"KB/Bayes%20Prediction/","title":"Bayes Prediction","text":""},{"location":"KB/Bayes%20Prediction/#bayes-prediction","title":"Bayes Prediction","text":"<ul> <li> \\[P(y|x) = \\int_{w}P(y|w,x)P(w|x)dw\\] </li> <li>w is model parameters</li> <li>basically gets y with different model parms w</li> <li>prob of those params given input x</li> <li>Model averaging</li> </ul>"},{"location":"KB/Bayes%20Rule/","title":"Bayes Rule","text":""},{"location":"KB/Bayes%20Rule/#bayes-rule","title":"Bayes Rule","text":"<ul> <li> \\[h(\\theta|D) = \\frac{p_{\\otimes_{i}}x(D|\\theta)h(\\theta)}{p(D)}\\] </li> </ul>"},{"location":"KB/Bayesian%20Information%20Criterion/","title":"Bayesian Information Criterion","text":""},{"location":"KB/Bayesian%20Information%20Criterion/#bayesian-information-criterion","title":"Bayesian Information Criterion","text":"<ul> <li> \\[BIC = -2 log(L) + 2log(n)q = \\frac{1}{n}(RSS + log(n) p \\hat \\sigma^{2})\\] </li> <li>L: denotes the likelihood function for a particular model</li> <li>q: number of estimated parameters of the model</li> </ul>"},{"location":"KB/Bayesian%20Model%20Estimation/","title":"Bayesian Model Estimation","text":""},{"location":"KB/Bayesian%20Model%20Estimation/#bayesian-model-estimation","title":"Bayesian Model Estimation","text":"<ul> <li>Unlike frequentist, sometimes things like sample mean is not a good metric because it has a high variance. Might give different results with different trials in a real valued distribution</li> <li>The task is to estimate \\(\\theta\\) from the data</li> <li>Bayesian Prior</li> <li>Now there are two sources of info about the true distribution \\(p_{X}(\\theta)\\)<ul> <li>The likelihood \\(p_{\\otimes_{i}}x(D|\\theta)\\) of \\(\\theta\\) . Empirical data</li> <li>Prior plausibility in \\(h(\\theta)\\)</li> <li>Since these are independant sources we can combine them by multiplication: \\(p_{\\otimes_{i}}x(D|\\theta)h(\\theta)\\)<ul> <li>High values -&gt; Candidate model \\(\\theta\\) is a good estimate</li> <li>Bayesian Posterior</li> <li>Posterior Mean estimate</li> </ul> </li> </ul> </li> </ul>"},{"location":"KB/Bayesian%20Model%20Estimation/#advantages","title":"Advantages","text":"<ul> <li>If priors are well chosen -&gt; Better than frequentists with small sample sizes</li> </ul>"},{"location":"KB/Bayesian%20Model%20Estimation/#disadvantages","title":"Disadvantages","text":"<ul> <li>Integrating over millions of params and performing multiple preds for each param -&gt; infeasible</li> <li>How to encode or represent Bayesian Posterior as very high dim<ul> <li>No closed form representation over weights</li> <li>Represent data with histograms and use Monte Carlo</li> </ul> </li> </ul>"},{"location":"KB/Bayesian%20Model%20Estimation/#example","title":"Example","text":"<ul> <li>Green : prior , Red: Posterior</li> <li>The Posterior Mean estimate is obtained by integrating \\(\\int_{\\mathbb{R}}\\mu h(\\mu|D)d\\mu\\)</li> <li>Since this is different from sample mean -&gt; Prior distribution really does influence the models</li> </ul>"},{"location":"KB/Bayesian%20Model%20Estimation/#protein-modeling","title":"Protein Modeling","text":""},{"location":"KB/Bayesian%20Neural%20Network/","title":"Bayesian Neural Network","text":""},{"location":"KB/Bayesian%20Neural%20Network/#bayesian-neural-network","title":"Bayesian Neural Network","text":"<ul> <li>Bayesian Model Estimation</li> <li>Generally we want to learn Joint Probability distribution \\(P(y|x)\\) but this does not use the model parameters w</li> <li>We need (\\(P(w|D) = \\frac{P(D|w)P(w)}{P(D)}\\)\\)<ul> <li>D is the labelled dataset</li> <li>Model is now defined by structure and parameters</li> </ul> </li> <li>The parameters encode information about Uncertainty<ul> <li>Can be understood using Bayesian Predictive Posterior</li> </ul> </li> </ul>"},{"location":"KB/Bayesian%20Posterior/","title":"Bayesian Posterior","text":""},{"location":"KB/Bayesian%20Posterior/#bayesian-posterior","title":"Bayesian Posterior","text":"<ul> <li>When D is fixed though, this becomes a function of Model Candidates</li> <li>Non negative on K dim param space</li> <li>Not a PDF but if we divide it by its integral -&gt; PDF .<ul> <li> \\[\\frac{p_{\\otimes_{i}}x(D|\\theta)h(\\theta)}{\\int_{\\mathbb{R}^K}p_{\\otimes_{i}}x(D|\\theta)h(\\theta)d\\theta}\\] </li> <li>Prob distrib over candidate models</li> </ul> </li> <li>If the denominator is replaced written as \\(p(D)\\) then it looks like the Bayes Rule</li> <li>Shape : \\(P(D|\\theta)h(\\theta)\\)<ul> <li>Integral not 1</li> <li>Proto Distributions on \\(\\theta\\) space</li> </ul> </li> </ul>"},{"location":"KB/Bayesian%20Predictive%20Posterior/","title":"Bayesian Predictive Posterior","text":""},{"location":"KB/Bayesian%20Predictive%20Posterior/#bayesian-predictive-posterior","title":"Bayesian Predictive Posterior","text":"<ul> <li>Follows from Bayesian Posterior</li> <li>Marginalizing over all possible model parameters w</li> <li>Computes predictions y with different model parameters w and weights them by the Probability of those params given an input x</li> <li>Bayesian Model Averaging</li> <li> \\[P(y|x) = \\int_{w}P(y|w,x)P(w|x)dx\\] </li> </ul>"},{"location":"KB/Bayesian%20Prior/","title":"Bayesian Prior","text":""},{"location":"KB/Bayesian%20Prior/#bayesian-prior","title":"Bayesian Prior","text":"<ul> <li>Use prior knowledge as beliefs (param vectors \\(\\theta\\)). Cast in the form of a Probability distribution over the space \\(\\Theta\\) .<ul> <li>Weak knowledge most times</li> <li>For a K parametric PDF \\(p_{x}\\) , \\(\\Theta \\in \\mathbb{R}^{K}\\) .</li> <li>Not connected to Random variable(RVS).</li> <li>Does not model outcomes. Instead has \"beliefs\" about true distribution \\(P_{X_{i}}\\)</li> <li>Each \\(\\theta \\in \\mathbb{R}^{K}\\) corresponds to one specific PDF \\(p_{X}(\\theta)\\) -&gt; single candidate distribution \\(\\hat P_{X}\\) for values \\(x_i\\) (In frequentist, it models single data points)</li> <li>Since this is a distribution over distributions, it is a hyperdistribution</li> <li>N dim PDF \\(p_{\\otimes}x_{i}: \\mathbb{R}^{N} \\rightarrow \\mathbb{R}^{\\geq 0}\\) for the distribution of \\(RV \\otimes_{i}X_{i}\\)<ul> <li>\\(p_{\\otimes_{i}}x_{i}((x_{i},\u2026, x_{N})) = p_{x_{1}}, \u2026, p_{x_{N}}(x_{N}) = \\Pi_{i}p_{X}(x_{i})\\)</li> <li>\\(p_{\\otimes_{i}}x(D|\\theta)\\) -&gt; PDF values on a data sample D \\(p_{\\otimes_{i}}x_{i}((x_{i},\u2026, x_{N})) = p_{\\otimes_{i}}(\\theta)(D)\\)</li> </ul> </li> <li>When \\(\\theta\\) is fixed then \\(p_{\\otimes_{i}}x(D|\\theta)\\) is a function of data vectors D. For each sample, it describes how probable this distribution is assuming the true distribution of X is \\(p_{X}(\\theta)\\)</li> <li>When D is fixed, then it is a function of \\(\\theta\\). But this does not really measure anything.<ul> <li>Integral over \\(\\theta\\) is not 1</li> <li>It is a function of \\(\\theta\\) and so it is a likelihood function. MLE</li> <li>If given data D -&gt; it can show which models are more likely than others.</li> <li>Higher values of  \\(p_{\\otimes_{i}}x(D|\\theta)\\) are better</li> </ul> </li> </ul> </li> </ul>"},{"location":"KB/Bayesian%20Rule%20List/","title":"Bayesian Rule List","text":"<ul> <li>@Interpretable classifiers using rules and Bayesian analysis: Building a better stroke prediction model</li> <li>The BRL is a generative model that yields a posterior distribution over possible decision lists, which consist of a series of if-then statements that discretize a high-dimensional, multivariate feature space into a series of simple, readily interpretable decision statements. The if statements define a partition of a set of features, and the then statements correspond to the predicted outcome of interest.</li> <li>According to the authors, their experiments showed that the BRL has predictive accuracy on par with the current top algorithms for prediction in Machine Learning</li> <li>The BRL is able to be used to produce highly accurate and interpretable medical scoring systems</li> </ul>"},{"location":"KB/Bayesian%20Rule%20List/#bayesian-rule-list","title":"Bayesian Rule List","text":"<code>toc</code>"},{"location":"KB/Bayesian/","title":"Bayesian","text":""},{"location":"KB/Bayesian/#bayesian","title":"Bayesian","text":"<ul> <li>Subjective</li> <li>Bayes Prediction</li> <li>Bayes Rule</li> <li>Bayesian Model Estimation</li> <li>Probability density function</li> </ul>"},{"location":"KB/Beam%20search/","title":"Beam search","text":""},{"location":"KB/Beam%20search/#beam-search","title":"Beam Search","text":"<ul> <li>(from)</li> <li>at each step, keep track of the\u00a0k\u00a0mos probable translation hypotheses (k, beam size)</li> <li>examine the\u00a0k\u00a0most probable words for each hypothesis, compute their entire scores, keep\u00a0k\u00a0best ones</li> <li>not guaranteed to find optimal solution, but more efficient than exhaustive search</li> <li>it does not only take the best word, it rather takes the best\u00a0B\u00a0(user specified) and generates multiple hypothesis, which will then be evaluated and the best one at each step is chosen for the next ones</li> <li>not guaranteed to find the optimal solution and is therefore an approximate search</li> <li>problems:<ul> <li>when multiplying a lot of probabilities of very unlikely word (e.g. almost 0 but not exactly), the result will get very small and the system can no longer represent it. results in numerical underflow --&gt; instead of multiplying, summing the log of probabilities (more numerical stable)</li> <li>if the sentence is very long, the probabilities get very low, therefore it rather takes smaller translations --&gt; normalize the output by the number of word in the translation (average of the log of each word)</li> </ul> </li> <li>how to choose beam width\u00a0\\(B\\)?<ul> <li>the smaller, the fewer probabilities are considered (worse result, faster)</li> <li>the larger, the more are considered, but the more computing expensive it is (better results, slower)</li> <li>try out different values and cross check</li> </ul> </li> </ul>"},{"location":"KB/Belief-Desire-Intention/","title":"Belief-Desire-Intention","text":""},{"location":"KB/Belief-Desire-Intention/#belief-desire-intention","title":"Belief-Desire-Intention","text":"<ul> <li>To judge the ethics of an agent's own actions, the awareness process generates the beliefs that describe the current situation facing the agent and the goals of the agent.</li> <li>Based on the beliefs and goals, the evaluation process generates the set of possible actions and desirable actions</li> <li>goodness process then computes the set of ethical actions based on the agent's beliefs, desires, actions, and moral value rules</li> <li>rightness process evaluates whether or not executing a possible action is right under the current situation and selects an action which satisfies the rightfulness requirement</li> </ul>"},{"location":"KB/Belmont%20Principles/","title":"Belmont Principles","text":""},{"location":"KB/Belmont%20Principles/#belmont-principles","title":"Belmont Principles","text":"<ul> <li>The three principles\u2014</li> <li>beneficence, distributive justice, and respect for persons\u2014which the 1976 Belmont Report concluded should underlie all conduct in biomedical and behavioral research in order to protect human participants.</li> </ul>"},{"location":"KB/Belmont%20Report/","title":"Belmont Report","text":""},{"location":"KB/Belmont%20Report/#belmont-report","title":"Belmont Report","text":"<ul> <li>An influential report that identified and defined the basic ethical principles (the Belmont principles) that should govern research studies involving human participants.</li> </ul>"},{"location":"KB/Benchmark%20LLM/","title":"Benchmark LLM","text":""},{"location":"KB/Benchmark%20LLM/#benchmark-llm","title":"Benchmark LLM","text":"<ul> <li>Large Language Models Still Can\u2019t Plan (A Benchmark for LLMs on Planning and Reasoning about Change))))</li> <li>The recent advances in large language models (LLMs) have transformed the field of natural language processing (NLP)</li> <li>From GPT3 to PaLM, the state-of-the-art performance on natural language tasks is being pushed forward with every new large language model</li> <li>current benchmarks are relatively simplistic and the performance over these benchmarks cannot be used as an evidence to support</li> <li>extensible assessment framework motivated by the above gaps in current benchmarks to test the abilities of LLMs on a central aspect of human intelligence, which is reasoning about actions and change</li> <li>multiple test cases</li> </ul>"},{"location":"KB/Bend%20Minimization/","title":"Bend Minimization","text":""},{"location":"KB/Bend%20Minimization/#bend-minimization","title":"Bend Minimization","text":"<ul> <li>Curved lines easier to follow than edged lines Gestalt Laws</li> <li>domain specific constraints and traditions have to be acknowledged</li> <li></li> </ul>"},{"location":"KB/Beneficence/","title":"Beneficence","text":""},{"location":"KB/Beneficence/#beneficence","title":"Beneficence","text":"<ul> <li>One of the three Belmont principles, the requirement that physicians and researchers provide, to the best of their ability, positive benefits for patients that participate in clinical trials, including good health and the prevention and removal of harmful conditions.</li> </ul>"},{"location":"KB/Benford%27s%20Law/","title":"Benford's Law","text":""},{"location":"KB/Benford%27s%20Law/#benfords-law","title":"Benford's Law","text":"<ul> <li>In a genuine dataset of numbers<ul> <li>1 will be the leading digit 30.1% of the time</li> <li>2 will be the leading digit 17.6% of the time</li> <li>rest with decreasing frequency</li> </ul> </li> </ul>"},{"location":"KB/Benign/","title":"Benign","text":""},{"location":"KB/Benign/#benign","title":"Benign","text":"<ul> <li>Refers to a tumor that is neither cancerous nor malignant</li> </ul>"},{"location":"KB/Berkeley%20et%20al/","title":"Berkeley et al","text":""},{"location":"KB/Berkeley%20et%20al/#berkeley-et-al","title":"Berkeley Et Al","text":"<ul> <li>576 input patterns</li> <li>5793 epochs needed to reach convergence</li> <li>Model frozen, stimulus set presented again</li> <li>Activation of each hidden unit recorded</li> <li>Single unit recording</li> <li>Why do these bands appear?</li> <li>Gaussian activation function</li> <li>But banding patterns have been found with sigmoidal activation as well</li> <li>Effect = units only respond to a limited number of inputs</li> <li>Bands appear when weights into HUs cancel each other out</li> <li>Activations of hidden neurons can be organized into bands</li> <li>Bands are associated with interpretable features</li> <li>Lesion studies show bands are essential to solving problem</li> <li>For some problems under some circumstances, neural networks develop highly selective hidden units</li> <li>Looks like localist coding (grandmother cells)</li> <li>Patterns of activation can be ambiguous on their own</li> <li>But realistically, more than one pattern might be activated simultaneously</li> <li>Superimposing two or more patterns over same units leads to an ambiguous blend</li> <li>Problem for localist representations, but even more serious with distributed representations</li> </ul>"},{"location":"KB/Bernoulli%20Distribution/","title":"Bernoulli Distribution","text":""},{"location":"KB/Bernoulli%20Distribution/#bernoulli-distribution","title":"Bernoulli Distribution","text":"<ul> <li>Only two possible outcomes</li> <li>PMF : \\(\\(Pr(y|\\lambda) = \\begin{cases} 1-\\lambda, &amp; \\text{for i = 1} \\\\ \\lambda,&amp; \\text{for i =2} \\end{cases} = (1-\\lambda)^{1-y} \\cdot \\lambda^{y}\\)\\)</li> <li> <p>Given : Data - \\({x_{1}, .., x_{N}}\\) and \\(x_{i} \\in {s_{1}, s_{2}}\\) then \\(\\(\\hat q = \\frac{1}{N}|\\{i|x_{i}= s_{2}\\}\\)\\)</p> </li> <li> <p></p> </li> </ul>"},{"location":"KB/Best%20Maching%20Unit/","title":"Best Matching Unit","text":""},{"location":"KB/Best%20Maching%20Unit/#best-matching-unit","title":"Best Matching Unit","text":"<ul> <li>Neuron whose weight vector best matches input pattern</li> </ul>"},{"location":"KB/Beta%20Distribution/","title":"Beta Distribution","text":""},{"location":"KB/Beta%20Distribution/#beta-distribution","title":"Beta Distribution","text":"<ul> <li>[0,1]</li> <li>Parameterized by two positive shape parameters \\(\\alpha, \\beta\\)</li> <li>Exponents of the random variable and control the shape of the distribution</li> <li>Multiple variables is Dirichlet Distribution</li> </ul>"},{"location":"KB/Beta%20Waves/","title":"Beta Waves","text":""},{"location":"KB/Beta%20Waves/#beta-waves","title":"Beta Waves","text":"<ul> <li>Movement</li> <li></li> </ul>"},{"location":"KB/Beware%20of%20Inmates%20Running%20the%20Asylum/","title":"Beware of Inmates Running the Asylum","text":"<ul> <li> <p>Tim Miller\u2217 and Piers Howe\u2020 and Liz Sonenberg</p> </li> <li>@Explainable AI: Beware of Inmates Running the Asylum Or: How I Learnt to Stop Worrying and Love the Social and Behavioural Sciences</li> </ul>"},{"location":"KB/Beware%20of%20Inmates%20Running%20the%20Asylum/#beware-of-inmates-running-the-asylum","title":"Beware of Inmates Running the Asylum","text":"<p><code>toc</code></p>"},{"location":"KB/Beware%20of%20Inmates%20Running%20the%20Asylum/#tldr","title":"TL;DR","text":"<ul> <li>Essentially proposes to look at behavioral science research as well. Not particularly useful but is a good reminder to look at other research because XAI is meant for people and not programmers.</li> </ul>"},{"location":"KB/Beware%20of%20Inmates%20Running%20the%20Asylum/#abstract","title":"Abstract","text":"<ul> <li>programmers design software for themselves, rather than for their target audience; a phenomenon he refers to as the \u2018inmates running the asylum\u2019.</li> <li>This paper argues that explainable AI risks a similar fate.</li> <li>evaluation of these models is focused more on people than on technology</li> <li>considerable scope to infuse more results from the social and behavioural sciences into explainable AI, and present some key results from these fields that are relevant to explainable AI.</li> </ul>"},{"location":"KB/Beware%20of%20Inmates%20Running%20the%20Asylum/#explainable-ai-survey","title":"Explainable AI Survey","text":""},{"location":"KB/Beware%20of%20Inmates%20Running%20the%20Asylum/#survey-method","title":"Survey Method","text":"<ul> <li> <p>On topic</p> <ul> <li>Each paper was categorised as either being about explainable AI or not, based on our understanding of the topic</li> <li>Data Driven</li> <li>Each paper was given a score from 0\u20132 inclusive.</li> <li>A score of 1 was given if and only if one or more of the references of the paper was an article on explanation in social science</li> <li>Validation</li> <li>Each paper was given a binary 0/1. A score of 1 was given if and only if the evaluation in the survey article (note, not the referenced article) was based on data from human behavioural studies.</li> </ul> </li> </ul>"},{"location":"KB/Beware%20of%20Inmates%20Running%20the%20Asylum/#results","title":"Results","text":"<ul> <li>These results show that for the on-topic papers, only four articles referenced relevant social science research, and only one of them truly built a model on this</li> <li>Further, serious human behavioural experiments are not currently being undertaken.</li> <li>For off topic papers, the results are similar: limited input from social sciences and limited human behavioural experiments.</li> <li>Where to? A Brief Pointer to Relevant Work Contrastive Explanation</li> <li>explanations are contrastive why\u2013questions are contrastive</li> <li>That is, why\u2013questions are of the form \u201cWhy P rather than Q?\u201d, where P is the fact that requires explanation, and Q is some foil case that was expected</li> <li>Importantly, the contrast case helps to frame the possible answers and make them relevant</li> <li>This is a challenge for explainable AI, because it may not be easy to elicit a contrast case from an observer.</li> <li>However, it is also an opportunity: as Lipton [1990] argues, answering a contrastive question is often easier than giving a full cause attribution because one only needs to understand the difference between the two cases, so one can provide a complete explanation without determining or even knowing all causes of the event.</li> </ul>"},{"location":"KB/Beware%20of%20Inmates%20Running%20the%20Asylum/#attribution-theory","title":"Attribution Theory","text":"<ul> <li>study of how people attribute causes to events Social Attribution</li> <li>The book from Malle [2004], based on a large body of work from himself and other researchers in the field, describes a mature model of how people explain behaviour of others using folk psychology</li> <li>people attribute behaviour based on the beliefs, desires, intentions, and traits of people</li> <li>important for systems in which intentional action will be cited as a cause important for systems doing deliberative reasoning</li> </ul>"},{"location":"KB/Beware%20of%20Inmates%20Running%20the%20Asylum/#causal-connection","title":"Causal Connection","text":"<ul> <li>Research on how people connect causes shows that they do so by undertaking a mental simulation of what would have happened had some other event turned out differently</li> </ul>"},{"location":"KB/Bhattacharya%20Distance/","title":"Bhattacharya Distance","text":""},{"location":"KB/Bhattacharya%20Distance/#bhattacharya-distance","title":"Bhattacharya Distance","text":"<ul> <li>In statistics, the\u00a0Bhattacharyya distance\u00a0measures the similarity of two Distributions. It is normally used to measure the separability of classes in classification.</li> <li> \\[D_{B}(p,q) = -ln(BC(p,q))\\] </li> <li> \\[BC(p,q) = \\Sigma_{x\\in X}\\sqrt{p(x)q(x)}\\] </li> </ul>"},{"location":"KB/Bi%20Directional%20RNN/","title":"Bi Directional RNN","text":""},{"location":"KB/Bi%20Directional%20RNN/#bi-directional-rnn","title":"Bi Directional RNN","text":"<ul> <li>Not causal</li> <li>Looks at the forward timestep dimension and also the backward<ul> <li>Both combined to make a prediction</li> </ul> </li> <li></li> </ul>"},{"location":"KB/Bias%20Variance%20Dilemma/","title":"Bias Vs Variance","text":""},{"location":"KB/Bias%20Variance%20Dilemma/#bias-vs-variance","title":"Bias Vs Variance","text":"<ul> <li><ul> <li>m is choices of PC vectors</li> <li>as m increases, weight matrices grow by \\(10\\cdot m\\) . Aka more flexible models.</li> <li>Increasing tail of MSEtest -&gt; overfitting. too flexible</li> <li>Increasing flexibility -&gt; decrease of empirical risk</li> <li>Inc : very low to very high -&gt; less and less underfitting then overfitting</li> <li>Best: min point in curve. But it is defined on test data which we do not have</li> </ul> </li> <li></li> <li></li> <li>Decision function should minimize LossFunctions and yield a function with risk h. This is hopeless \\(\\(R(h) = E[L(h(X), Y)]\\)\\)</li> <li>Tune on Emperical Risk instead using Optimizers</li> <li>\\(\\mathcal{H}\\) is hypothesis space (related to Fitting).</li> </ul>"},{"location":"KB/Bias%20Variance%20Dilemma/#why-is-this-a-dilemma","title":"Why is This a Dilemma","text":"<ul> <li>Any learning algo \\(\\mathcal{A}\\)</li> <li>If we run \\(\\mathcal{A}\\) repeatedly but for different \"fresh\" sampled data -&gt; \\(\\hat h\\) varies from trial to trial</li> <li>For any fixed x, \\(\\hat h(x)\\)<ul> <li>is a random variable</li> <li>value determined by drawn training samples</li> <li>rep by distribution \\(P_{X,Y}\\) (which we cannot really know)</li> <li>Expectation \\(E_{retrain}[\\hat h(x)]\\) . aka taken over ALL possible training runs with sampled data</li> </ul> </li> <li>Quadratic Loss (risk) is minimized by the function (\\(\\Delta(x) = E_{Y|X=x}[Y]\\)\\)<ul> <li>Expectation of Y given x.</li> </ul> </li> <li></li> <li></li> <li>Bias measures how strongly the avg result deviates from optimal value</li> <li>Variance measures how strongly the results vary around the expected value \\(E_{retrain}\\)</li> <li>When flexibility is too low -&gt; bias dominates(too good in train and horrible later) and underfits</li> <li>When flexibility is too high -&gt; variance dominates -&gt; overfitting</li> </ul>"},{"location":"KB/Bias%20Variance%20Dilemma/#tuning-model-flexibility","title":"Tuning Model Flexibility","text":""},{"location":"KB/Bias%20nodes/","title":"Bias nodes","text":""},{"location":"KB/Bias%20nodes/#bias-nodes","title":"Bias Nodes","text":"<ul> <li>Bias nodes give a defaults activation to other nodes<ul> <li>Usually included, the bias node does not connect to input nodes but to the output nodes</li> </ul> </li> </ul>"},{"location":"KB/Big%20Bird/","title":"Big Bird","text":""},{"location":"KB/Big%20Bird/#big-bird","title":"Big Bird","text":"<ul> <li>Big Bird: Transformers for Longer Sequences</li> <li>imitation of Transformer-based models is the quadratic complexity</li> <li>sparse attention mechanism that reduces this quadratic complexity to linear</li> </ul>"},{"location":"KB/Big-Bench/","title":"Big-Bench","text":""},{"location":"KB/Big-Bench/#big-bench","title":"Big-Bench","text":"<ul> <li>Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models</li> <li>present and near-future capabilities and limitations of language models</li> <li>Beyond the Imitation Game benchmark (BIG-bench)</li> <li>benchmark that can measure progress well beyond the current state-of-the-art</li> <li>204 tasks, contributed by 442 authors across 132 institutions</li> <li>Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development</li> <li>tasks that are believed to be beyond the capabilities of current language models</li> <li>valuate the behavior of OpenAI\u2019s GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters</li> </ul>"},{"location":"KB/Bilinear%20Interpolation/","title":"Bilinear Interpolation","text":""},{"location":"KB/Bilinear%20Interpolation/#bilinear-interpolation","title":"Bilinear Interpolation","text":"<ul> <li> \\[f(x,y) = (1-\\beta)(1-\\alpha)f_{i,j}+(1-\\beta)\\alpha f_{i+1,j} + \\beta(1-\\alpha)f_{i,j+1}+\\beta \\alpha f_{i+1,j+1}\\] </li> <li>Quadratic</li> <li>ik, jkl , il -&gt; ij</li> </ul>"},{"location":"KB/Billion%20Word/","title":"Billion Word","text":""},{"location":"KB/Billion%20Word/#billion-word","title":"Billion Word","text":""},{"location":"KB/Binary%20Cross%20Entropy/","title":"Binary Cross Entropy","text":""},{"location":"KB/Binary%20Cross%20Entropy/#binary-cross-entropy","title":"Binary Cross Entropy","text":"\\[-(ylog(p)+(1-y)log(1-p))$$ - $$L(y, \\hat y) = - \\Sigma_{i}y_{i}log(\\hat y_{i})+ (1-y_{i})log(1-\\hat y_{i})\\]"},{"location":"KB/Binary%20pattern/","title":"Binary Pattern Encoding","text":""},{"location":"KB/Binary%20pattern/#binary-pattern-encoding","title":"Binary Pattern Encoding","text":"<ul> <li>If symbol alphabet has a large size k<ul> <li>One hot is too huge</li> </ul> </li> <li>Encode into binary vector of length \\(\\(\\lceil log_{2} \\rceil\\)\\)</li> <li>{a,b,c,d} -&gt; {[0,0]', [0,1]', [1,0]', [1,1]'}</li> <li>Non linear effort as it is a arbitrary encoding</li> <li>Too intensive</li> </ul>"},{"location":"KB/Binary%20pattern/#_1","title":"\u2026","text":""},{"location":"KB/Binary/","title":"Binary","text":""},{"location":"KB/Binary/#binary","title":"Binary","text":""},{"location":"KB/BinaryBERT/","title":"BinaryBERT","text":""},{"location":"KB/BinaryBERT/#binarybert","title":"BinaryBERT","text":"<ul> <li>BinaryBERT: Pushing the Limit of BERT Quantization</li> <li>demand for model compression techniques</li> <li>weight binarization</li> <li>binary BERT is hard to be trained directly than a ternary counterpart due to its steep and complex loss landscape</li> <li>ternary weight splitting</li> <li>initializes BinaryBERT by equivalently splitting from a half-sized ternary network, followed by fine-tuning for further refinement</li> <li>binary model thus inherits the good performance of the ternary one, and can be further enhanced by fine-tuning the new architecture after splitting</li> <li>tailor the size of BinaryBERT based on the edge device constraints</li> <li>GLUE</li> <li>SQuAD</li> </ul>"},{"location":"KB/Binning/","title":"Binning","text":""},{"location":"KB/Binning/#binning","title":"Binning","text":"<ul> <li>k segments</li> <li>Transform each symbol</li> <li>Types<ul> <li>Simplest -&gt; k equal bins</li> <li>Approx Equal no of data points</li> <li>Reduced precision devices perform as well as the high precision ones</li> <li>Continuous range -&gt; adaptive bin boundaris Decision Trees</li> </ul> </li> </ul>"},{"location":"KB/Binomial%20Distribution/","title":"Binomial Distribution","text":""},{"location":"KB/Binomial%20Distribution/#binomial-distribution","title":"Binomial Distribution","text":"<ul> <li>Bernoulli Distribution repeated for N independant trials with success Probability q</li> <li>Aka N times with only 2 outcomes</li> <li>PMF: \\(\\(p(s) = \\binom{N}{s}q^{s}(1-q)^{N-s} = \\frac{N!}{s!(N-s)}q^{s}(1-q)^{N-s}\\)\\)</li> <li>s = 0,1,2..N</li> <li>\\(N\\choose s\\) is binomial coefficient</li> <li> \\[X \\sim Bi(N,s)\\] </li> <li></li> </ul>"},{"location":"KB/Biological%20Neuron/","title":"Biological Neuron","text":""},{"location":"KB/Biological%20Neuron/#biological-neuron","title":"Biological Neuron","text":"<ul> <li>(from)</li> <li>composed of\u00a0<ul> <li>cell body</li> <li>dendrites: many branching extensions</li> <li>axon: very long extension that splits off at its tip into many branches called synaptic terminals</li> </ul> </li> <li>composed in a network (e.g. brain) by synaptic terminals of one neuron connected to dendrites of other neurons</li> <li>electrical impulses (signals) are sent from other neurons via these synapses</li> <li>if a neuron receives a sufficient number of signal from other neurons within a few milliseconds, it is exited and fires its own signals (activation)</li> <li>connectivity in a biological neural system is huge, human brain:<ul> <li>number of neurons: \\(\\approx 10^{11}\\)</li> <li>number of connections per neuron:\u00a0\\(\\approx 10^4\\)</li> </ul> </li> <li>networks are organized into hierarchical structures</li> <li>Irreplacable</li> <li>Requires constant supply of Glucose</li> </ul>"},{"location":"KB/Biomarkers/","title":"Biomarkers","text":""},{"location":"KB/Biomarkers/#biomarkers","title":"Biomarkers","text":"<ul> <li>A measurable physiological indicator of a biological state or condition.</li> </ul>"},{"location":"KB/Biopsy/","title":"Biopsy","text":""},{"location":"KB/Biopsy/#biopsy","title":"Biopsy","text":"<ul> <li>Removal of a small tissue sample for testing</li> </ul>"},{"location":"KB/Block%20Sparse%20Kernel/","title":"Block Sparse Kernel","text":""},{"location":"KB/Block%20Sparse%20Kernel/#block-sparse-kernel","title":"Block Sparse Kernel","text":"<ul> <li>For networks with block sparse weights</li> <li>Can choose amount of sparsity</li> <li>Can replace normal Dense Layers with sparse and wide or sparse and deep</li> <li></li> <li>Enables wider and deeper networks</li> <li>Only compute on non zero blocks</li> <li></li> <li>Connectivity is unaffected in the spatial dimensions</li> <li>Compute cost is only prop to number of non zero weights</li> <li>Small World graphs</li> <li>Also useful for compression</li> </ul>"},{"location":"KB/Block%20Sparse%20Kernel/#refs","title":"Refs","text":"<ul> <li>openai</li> </ul>"},{"location":"KB/BlockNeRF/","title":"BlockNeRF","text":""},{"location":"KB/BlockNeRF/#blocknerf","title":"BlockNeRF","text":"<ul> <li>Block-NeRF: Scalable Large Scene Neural View Synthesis</li> <li>variant of Neural Radiance Field</li> <li>reconstruct large-scale environments</li> <li>scaling NeRF to render city-scale scenes spanning multiple blocks, it is vital to decompose the scene into individually trained NeRFs that can be optimized independently.</li> <li>this decomposition decouples rendering time from scene size</li> <li>allows per-block updates of the environment</li> <li>data collected will necessarily have transient objects and variations in appearance</li> <li>modifying the underlying NeRF architecture to make NeRF robust to data captured over months under different environmental conditions</li> <li>appearance Embedding, learned pose refinement, and controllable exposure to each individual NeRF</li> <li>procedure for aligning appearance between adjacent NeRFs so that they can be seamlessly combined</li> <li>building an entire neighborhood in San Francisco from 2.8M images using a grid of Block-NeRFs, forming the largest neural scene representation to date</li> </ul>"},{"location":"KB/Blood%20Culture/","title":"Blood Culture","text":""},{"location":"KB/Blood%20Culture/#blood-culture","title":"Blood Culture","text":"<ul> <li>Test to reveal the existence of fungi or bacteria in the blood, possibly indicating an infection</li> </ul>"},{"location":"KB/Blood%20Lancet/","title":"Blood Lancet","text":""},{"location":"KB/Blood%20Lancet/#blood-lancet","title":"Blood Lancet","text":"<ul> <li>A double-edged blade or needle used to obtain blood samples</li> </ul>"},{"location":"KB/Blood%20Swab/","title":"Blood Swab","text":""},{"location":"KB/Blood%20Swab/#blood-swab","title":"Blood Swab","text":"<ul> <li>Taking a blood sample using a cotton-tipped stick</li> </ul>"},{"location":"KB/Blood-brain%20Barrier/","title":"Blood brain Barrier","text":""},{"location":"KB/Blood-brain%20Barrier/#blood-brain-barrier","title":"Blood-brain Barrier","text":"<ul> <li>A protective barrier that separates the brain from the blood circulating across the body. The blood-brain barrier is semipermeable, meaning it allows the passage of water as well as molecules like glucose and other amino acids that help promote neural function.</li> </ul>"},{"location":"KB/Blur%20Baseline/","title":"Blur Baseline","text":""},{"location":"KB/Blur%20Baseline/#blur-baseline","title":"Blur Baseline","text":"<ul> <li>@fongInterpretableExplanationsBlack2017</li> <li>Another baseline is called Blur baseline and uses a multi-dimensional gaussian filter (</li> <li>The idea presented by Fong and Vedaldi blurred version of the image is a domain-specific way to represent missing information and therefore be a valid baseline according to the original definition</li> </ul>"},{"location":"KB/Boltzmann%20Distribution/","title":"Boltzmann Distribution","text":""},{"location":"KB/Boltzmann%20Distribution/#boltzmann-distribution","title":"Boltzmann Distribution","text":"<ul> <li>PDF \\(\\(p(s|T)= \\frac{1}{\\int_{s}e^{-C(s)/T}ds}e^{-C(s)/T}\\)\\)</li> <li>Energy function \\(E: S \\rightarrow \\mathbb{R}^{\\geq 0}\\)</li> <li>Markov Random Field</li> </ul>"},{"location":"KB/BooksCorpus/","title":"BooksCorpus","text":""},{"location":"KB/BooksCorpus/#bookscorpus","title":"BooksCorpus","text":""},{"location":"KB/Bottom%20Up%20Parsing/","title":"Bottom Up Parsing","text":""},{"location":"KB/Bottom%20Up%20Parsing/#bottom-up-parsing","title":"Bottom Up Parsing","text":""},{"location":"KB/Bound%20morpheme/","title":"Bound morpheme","text":""},{"location":"KB/Bound%20morpheme/#bound-morpheme","title":"Bound Morpheme","text":"<ul> <li>morphemes that cannot appear as a word by itself</li> <li>e.g., +ing, +s, +ness,ly,ed</li> </ul>"},{"location":"KB/Brain%20Areas/","title":"Brain Areas","text":""},{"location":"KB/Brain%20Areas/#brain-areas","title":"Brain Areas","text":"<ul> <li>Cerebrum</li> <li>Cerebellum</li> <li>Brainstem</li> </ul>"},{"location":"KB/Brain%20Cortex/","title":"Brain Cortex","text":""},{"location":"KB/Brain%20Cortex/#brain-cortex","title":"Brain Cortex","text":"<ul> <li>Also called Cerebral Cortex</li> <li>It has a folded appearance with hills and valleys</li> <li>The nerve cell bodies color the cortex grey-brown giving it its name \u2013 gray matter</li> <li>Beneath the cortex are long nerve fibers (axons) that connect Brain Areas to each other \u2014 called white matter</li> <li></li> <li>Gyrus</li> <li>Basal Ganglia</li> <li>Divided into parts<ul> <li>Medial Prefrontal Cortex, and the Posterior Cingulate Cortex with the nearby Precuneus and Lateral Parietal Cortex **</li> </ul> </li> </ul>"},{"location":"KB/Brain%20Organoid/","title":"Brain Organoid","text":""},{"location":"KB/Brain%20Organoid/#brain-organoid","title":"Brain Organoid","text":"<ul> <li>A research model that uses pluripotent stem cells (iPSCs) to grow structures that resemble brains in some ways, but are grown in a lab dish made of neurons and other brain tissues.</li> </ul>"},{"location":"KB/Brain%20Oscillations/","title":"Brain Oscillations","text":""},{"location":"KB/Brain%20Oscillations/#brain-oscillations","title":"Brain Oscillations","text":"<ul> <li>Periodic</li> <li>Brain waves</li> <li>Delta Waves</li> <li>Theta Waves</li> <li>Alpha Waves</li> <li>Beta Waves</li> <li>Gamma Waves</li> <li>Spectrogram</li> <li></li> </ul>"},{"location":"KB/Brain-derived%20Neurotrophic%20Factor%20%28BDNF%29/","title":"Brain derived Neurotrophic Factor (BDNF)","text":""},{"location":"KB/Brain-derived%20Neurotrophic%20Factor%20%28BDNF%29/#brain-derived-neurotrophic-factor-bdnf","title":"Brain-derived Neurotrophic Factor (BDNF)","text":"<ul> <li>Sometimes referred to as \u201cbrain fertilizer,\u201d BDNF is a protein that helps promote the growth, maintenance, and survival of neurons.</li> </ul>"},{"location":"KB/BrainWave%20Coherence/","title":"BrainWave Coherence","text":""},{"location":"KB/BrainWave%20Coherence/#brainwave-coherence","title":"BrainWave Coherence","text":"<ul> <li>Correlation in the frequency domain</li> <li>Unlike synchronization, this also depends on signal amplitude</li> <li><ul> <li>coherence vs freq</li> <li>decent coherence between CZ and O1</li> <li>O1 and PZ has little coherence</li> </ul> </li> </ul>"},{"location":"KB/BrainWave%20CrossFrequency%20Coupling/","title":"BrainWave CrossFrequency Coupling","text":""},{"location":"KB/BrainWave%20CrossFrequency%20Coupling/#brainwave-crossfrequency-coupling","title":"BrainWave CrossFrequency Coupling","text":"<ul> <li>Low frequency + Superimposed High freq signal</li> <li></li> <li>-<ul> <li>Electrode with low freq + high freq</li> </ul> </li> </ul>"},{"location":"KB/BrainWave%20Synchronization/","title":"BrainWave Synchronization","text":""},{"location":"KB/BrainWave%20Synchronization/#brainwave-synchronization","title":"BrainWave Synchronization","text":"<ul> <li>Consistency of phase difference<ul> <li>If 0 then perfect</li> </ul> </li> <li></li> <li>Phase Locking Value</li> </ul>"},{"location":"KB/Brainstem/","title":"Brainstem","text":""},{"location":"KB/Brainstem/#brainstem","title":"Brainstem","text":"<ul> <li>relay center connecting the cerebrum and cerebellum to the spinal cord. It performs many automatic functions such as breathing, heart rate, body temperature, wake and sleep cycles, digestion, sneezing, coughing, vomiting, and swallowing</li> </ul>"},{"location":"KB/Branch%20Prediction/","title":"Branch Prediction","text":""},{"location":"KB/Branch%20Prediction/#branch-prediction","title":"Branch Prediction","text":"<ul> <li>avoid delays cause of control dependencies to be resolved.</li> <li>determines whether a conditional branch (jump) in the instruction flow of a program is likely to be taken or not</li> </ul>"},{"location":"KB/Broadcasting/","title":"Broadcasting","text":""},{"location":"KB/Broadcasting/#broadcasting","title":"Broadcasting","text":"<ul> <li>Expanding the shape of an operand in a matrix math operation to dimensions compatible for that operation. For instance, linear algebra requires that the two operands in a matrix addition operation must have the same dimensions. Consequently, you can't add a matrix of shape (m, n) to a vector of length n. Broadcasting enables this operation by virtually expanding the vector of length n to a matrix of shape (m,n) by replicating the same values down each column.</li> </ul>"},{"location":"KB/Brocas%20Area/","title":"Brocas Area","text":""},{"location":"KB/Brocas%20Area/#brocas-area","title":"Brocas Area","text":"<ul> <li>If this area is damaged, one may have difficulty moving the tongue or facial muscles to produce the sounds of speech. The person can still read and understand spoken language but has difficulty in speaking and writing</li> <li>Broca's aphasia</li> <li>Discovered by French physician Paul Broca in the late 19th century, this small region in the left frontal lobe has been linked to speech production.</li> </ul>"},{"location":"KB/Broden/","title":"Broden","text":""},{"location":"KB/Broden/#broden","title":"Broden","text":"<ul> <li>Broadly and Densely Labeled Dataset</li> <li>unifies several densely labeled image data sets: ADE [43], OpenSurfaces [4], Pascal-Context [19], Pascal-Part [6], and the Describable Textures Dataset [7]</li> <li>These data sets contain examples of a broad range of objects, scenes, object parts, textures, and materials in a variety of contexts</li> <li>segmented down to the pixel level except textures and scenes which are given for full-images</li> <li>every image pixel in the data set is annotated with one of the eleven common color names according to the human perceptions classified by van de Weijer</li> <li>The concept labels in Broden are normalized and merged from their original data sets so that every class corresponds to an English word</li> <li>Labels are merged based on shared synonyms, disregarding positional distinctions such as 'left' and 'top'</li> </ul>"},{"location":"KB/Bruckhaus%20-%202024%20-%20RAG%20Does%20Not%20Work%20for%20Enterprises/","title":"Bruckhaus - 2024 - RAG Does Not Work for Enterprises","text":"","tags":["llm"]},{"location":"KB/Bruckhaus%20-%202024%20-%20RAG%20Does%20Not%20Work%20for%20Enterprises/#bruckhaus-2024-rag-does-not-work-for-enterprises","title":"Bruckhaus - 2024 - RAG Does Not Work for Enterprises","text":"<ul> <li>@bruckhausRAGDoesNot2024</li> </ul>","tags":["llm"]},{"location":"KB/Bruckhaus%20-%202024%20-%20RAG%20Does%20Not%20Work%20for%20Enterprises/#intro","title":"Intro","text":"<ul> <li>implementing RAG effectively in real-world, enterprise settings poses several challenges.</li> <li>The retriever needs to efficiently search through massive, constantly-updated knowledge bases to find the most relevant information for each query [Karpukhin et al., 2020]</li> <li>The generator needs to intelligently fuse the retrieved content with its own learned knowledge to produce coherent and accurate outputs [Shao et al., 2023]</li> <li>the RAG system needs to satisfy stringent requirements around data security, privacy, interpretability, and auditability [Arrieta et al., 2020].</li> </ul>","tags":["llm"]},{"location":"KB/Bruckhaus%20-%202024%20-%20RAG%20Does%20Not%20Work%20for%20Enterprises/#enterprise-requirements-for-retrieval-augmented-generation","title":"Enterprise Requirements for Retrieval-Augmented Generation","text":"<ul> <li>include built-in access controls, anonymization techniques, and auditing mechanisms.</li> <li>intelligently blending advanced semantic search techniques with hybrid query strategies, advanced RAG solutions retrieve the most relevant and reliable information to augment the generation process</li> <li>such solutions must provide clear explanations and attributions for its outputs, enabling enterprises to trust and act on the insights with confidence.</li> <li>flexible, API-driven architecture and pre-built connectors for popular enterprise systems.</li> </ul>","tags":["llm"]},{"location":"KB/Bruckhaus%20-%202024%20-%20RAG%20Does%20Not%20Work%20for%20Enterprises/#survey-of-current-rag-approaches-and-their-limitations","title":"Survey of Current RAG Approaches and Their Limitations","text":"<ul> <li>Lack of fine-grained control over retrieval and generation processes, which is crucial for ensuring accuracy, consistency, and regulatory compliance [Martorana et al., 2022, Anderljung et al., 2023, Rahwan et al., 2023].</li> <li>Limited scalability and performance when dealing with massive, heterogeneous enterprise knowledge bases [Ahmad et al. 2019 , Nambiar et al., 2023].</li> <li>Insufficient explainability and auditability of RAG outputs, which is essential for building trust and accountability in high-stakes enterprise use cases [Eibich et al. 2024, Gao et al. 2024, Kamath &amp; Liu 2021].</li> <li>Challenges in integrating RAG capabilities into existing enterprise systems and workflows, which often have complex security, governance, and data management requirements.</li> </ul>","tags":["llm"]},{"location":"KB/Bruckhaus%20-%202024%20-%20RAG%20Does%20Not%20Work%20for%20Enterprises/#dense-vector-indexes","title":"Dense Vector Indexes","text":"","tags":["llm"]},{"location":"KB/Bucketing/","title":"Bucketing","text":""},{"location":"KB/Bucketing/#bucketing","title":"Bucketing","text":"<ul> <li>Converting a (usually continuous) feature into multiple binary features called buckets or bins, typically based on value range. For example, instead of representing temperature as a single continuous floating-point feature, you could chop ranges of temperatures into discrete bins. Given temperature data sensitive to a tenth of a degree, all temperatures between 0.0 and 15.0 degrees could be put into one bin, 15.1 to 30.0 degrees could be a second bin, and 30.1 to 50.0 degrees could be a third bin.</li> </ul>"},{"location":"KB/Building%20Ethics%20into%20Artificial%20Intelligence/","title":"Building Ethics into Artificial Intelligence","text":""},{"location":"KB/Building%20Ethics%20into%20Artificial%20Intelligence/#building-ethics-into-artificial-intelligence","title":"Building Ethics into Artificial Intelligence","text":"<ul> <li>Han Yu, Zhiqi Shen, Chunyan Miao, Cyril Leung, Victor R. Lesser, Qiang Yang</li> </ul>"},{"location":"KB/Building%20Ethics%20into%20Artificial%20Intelligence/#abstract","title":"Abstract","text":"<ul> <li>taxonomy which divides the field into four areas: 1) exploring ethical dilemmas; 2) individual ethical decision frameworks; 3) collective ethical decision frameworks; and 4) ethics in human-AI interactions</li> </ul>"},{"location":"KB/Building%20Ethics%20into%20Artificial%20Intelligence/#types","title":"Types","text":"<ul> <li>Consequentialist ethics</li> <li>Utilitarian ethics</li> <li>Deontological ethics</li> <li>Virtue ethics</li> <li>Ethical dilemmas</li> </ul>"},{"location":"KB/Building%20Ethics%20into%20Artificial%20Intelligence/#exploring-ethical-dilemmas","title":"Exploring Ethical Dilemmas","text":"<ul> <li>explore the ethical dilemmas in the target application scenarios [Anderson and Anderson, 2014]</li> <li>GenEth</li> <li>Moral Machine project</li> </ul>"},{"location":"KB/Building%20Ethics%20into%20Artificial%20Intelligence/#individual-ethical-decision-frameworks","title":"Individual Ethical Decision Frameworks","text":"<ul> <li>AI research community largely agrees that generalized frameworks are preferred over ad-hoc rules</li> <li>if updates are provided by people, some review mechanisms should be put in place to prevent abuse</li> <li>moral decision-making by humans not only involves utilitarian considerations, but also moral rules.</li> <li>Such rules often involve protected values (a.k.a. sacred values)</li> <li>MoralDM</li> <li>Belief-Desire-Intention</li> <li>blind ethical judgement</li> <li>partially informed ethical judgement</li> <li>fully informed ethical judgement</li> <li>Moral decision making frameworks for artificial intelligence</li> <li>Preferences and ethical principles in decision making</li> <li>A declarative modular framework for representing and applying ethical principles.</li> <li>A low-cost ethics shaping approach for designing reinforcement learning agents</li> <li>Even angels need the rules AI, roboethics, and the law</li> <li>Norms as a basis for governing sociotechnical systems</li> <li>Embedding ethical principles in collective decision support systems</li> <li>A voting-based system for ethical decision making</li> <li>swap-dominance</li> <li>satisfying consequentialist ethics Ethics in Human-AI Interactions Belmont Report</li> <li>[Luckin, 2017; Yu et al., 2017b]</li> <li>1) people's personal autonomy should not be violated (they should be able to maintain their free will when interacting with the technology); 2) benefits brought</li> <li>about by the technology should outweigh risks; and 3) the benefits</li> <li>and risks should be distributed fairly among the users (people should not be discriminated based on their personal backgrounds such as race, gender and religion)</li> <li>persuasion agents</li> <li>[Kang et al., 2015; Rosenfeld and Kraus, 2016]</li> <li>[Stock et al., 2016]</li> <li>large-scale study to investigate human perceptions on the ethics of persuasion by an AI agent</li> <li>trolley scenario</li> <li>authors tested three persuasive strategies: 1) appealing to the participants emotionally; 2) presenting the participants with utilitarian arguments; and 3) lying</li> <li>participants hold a strong preconceived negative attitude towards the persuasion agent, and argumentation-based and lying-based persuasion strategies work better than emotional persuasion strategies</li> <li>did not show significant variation across genders or cultures</li> <li>adoption of persuasion strategies should take into account differences in individual personality, ethical attitude and expertise in the given domain.</li> <li>Coping Theory</li> <li>Argumentation-based explainable AI</li> <li>[Fan and Toni, 2015; Langley et al., 2017] well suited to the consequentialist ethics</li> <li>depending on how the explanations are used, researchers need to strike a balance on the level of details to be included</li> <li>Full transparency may be too overwhelming if the objective is to persuade a user to follow a time-critical recommendation</li> <li>useful as a mechanism to trace the AI decision process afterwards not enough transparency may hamper users' trust in the AI</li> </ul>"},{"location":"KB/Bunq/","title":"Bunq","text":""},{"location":"KB/Bunq/#bunq","title":"Bunq","text":"<p>Q: How familiar are you with Python, SQL, AWS, and ETL processes?</p> <p>I have a Masters in Artificial Intelligence from the University of Groningen. From my previous internship, experiences, personal and freelance projects, and research papers, I have a good amount of experience with AWS, SQL, and ETL processes.</p> <p>At almost every internship, I helped build/used ETL pipelines that required SQL, AWS, or some other data storage, and I am comfortable with learning new processes that might be required for this position.</p> <p>As for Python, it is my language of choice, and I am very familiar with it. I have experience working with creating data pipelines, data analysis, and AI/ML frameworks such as PyTorch, Tensorflow, NLTK, Hugging face, sklearn, etc. I have built several open-source packages in varying domains over the past couple of years and published many analytics and AI notebooks and articles as well.</p>"},{"location":"KB/Burn-in/","title":"Burn in","text":""},{"location":"KB/Burn-in/#burn-in","title":"Burn-in","text":"<ul> <li>Burn-In is a robot testing procedure where all components of the robot are operated continuously for an extended period of time. This is done to test movement and movement programming of the robot at early stages to avoid malfunctions after deployment.</li> </ul>"},{"location":"KB/C-section/","title":"C section","text":""},{"location":"KB/C-section/#c-section","title":"C-section","text":"<ul> <li>Caesarian section, where a baby is delivered through an abdominal and uterine incision</li> </ul>"},{"location":"KB/CAM/","title":"CAM","text":""},{"location":"KB/CAM/#cam","title":"CAM","text":"<ul> <li>@zhouLearningDeepFeatures2016</li> <li>Class Activation Mapping</li> <li>Similar to Network In Network</li> <li>zeroes out the negative grads during backward pass to provide more visually appealing results</li> <li>Uses Global Average Pooling</li> <li></li> <li> \\[\\alpha_{k}^{c}= \\overbrace{\\frac{1}{Z}\\Sigma_{i}\\Sigma_{j}}^\\text{global avg pool} \\underbrace{\\frac{\\partial y^{c}}{\\partial A^{k}_{ij}}}_\\text{grads via backprop}\\] </li> <li>k is the index of the activation map in the last convolutional layer, and c is the class of interest. Alpha computed above shows the importance of feature map k for the target class c.</li> <li>Finally, we multiply each activation map by its importance score (i.e. alpha) and sum the values</li> </ul>"},{"location":"KB/CAM/#chatgpt","title":"ChatGPT","text":"<ul> <li>The paper, \"Learning Deep Features for Discriminative Localization\" by Zhou et al. (2016) introduces the concept of Class Activation Mapping (CAM) as a way to visualize which regions of an image are most important for a given classification task. CAM is a technique for generating heatmaps that highlight the regions in an image that are most important for a specific classification. The authors propose to use global average pooling (GAP) in the final convolutional layer to generate a feature map, followed by a linear combination of the feature map and the class weight vector to generate a single class activation map.</li> <li>The authors apply CAM to the ResNet architecture and show that it outperforms the traditional fully-connected layer approach in terms of localization performance. They test the CAM on the image classification task using the ILSVRC-2012 dataset. The authors showed that by using CAM, they could identify the specific regions of an image that were important for a given classification, rather than just a \"black box\" decision made by the model. The authors also demonstrate how CAM can be used for fine-grained recognition, where the model is trained to identify sub-categories within a larger class.</li> <li>Additionally, the authors also show that the CAM can be used to improve the interpretability of deep neural networks by providing a visual representation of the model's decision-making process. They also use CAM to identify misclassifications and analyze the model's decision-making process. The authors test CAM on other architectures such as VGG and GoogleNet and show that it can be applied.</li> <li>The authors also use CAM for multi-label classification and show that it can identify the regions in an image that are relevant to multiple labels. They also use CAM for action classification in video and show that it can identify the regions of video frames that are important for a given action. They use CAM for object detection and show that it can be used to identify the regions of an image that contain an object of interest.</li> <li>The authors use CAM for fine-tuning a pre-trained model on a new dataset and show that it can be used to improve the performance of the model on the new dataset. They also use CAM for unsupervised feature learning and show that it can be used to learn features that are useful for a wide range of tasks. They use CAM for zero-shot learning and show that it can be used to identify the regions of an image that are relevant to a class that the model has never seen before.</li> <li>Finally, the authors use CAM for domain adaptation and show that it can be used to identify the regions of an image that are important for a specific task, even when the model has been trained on a different dataset. They also use CAM for weakly-supervised object localization and show that it can be used to identify the regions of an image that contain an object of interest, even when only image-level labels are available. They use CAM for multi-modal learning and show that it can be used to identify the regions of an image that are important for a given task, even when multiple modalities (e.g. image and text) are available.</li> <li>The authors conclude that the CAM is a powerful technique for visualizing the decision-making process of a deep neural network and can be used to improve the interpretability, performance, and robustness of deep models.</li> </ul>"},{"location":"KB/CBOW/","title":"CBOW","text":""},{"location":"KB/CBOW/#cbow","title":"CBOW","text":"<ul> <li>Continous implementation of Bag of words</li> <li>tries to predict the current target word (the center word) based on the source context words (surrounding words)</li> <li>\u201cthe quick brown fox jumps over the lazy dog\u201d, this can be pairs of\u00a0(context_window, target_word)\u00a0where if we consider a context window of size 2, we have examples like\u00a0([quick, fox], brown), ([the, brown], quick), ([the, dog], lazy)\u00a0and so on</li> <li>context window</li> <li></li> <li>several times faster to train than the skip-gram, slightly better accuracy for the frequent words.</li> <li>CBOW is prone to overfit frequent words because they appear several time along with the same context.</li> <li>tends to find the probability of a word occurring in a context</li> <li>it generalizes over all the different contexts in which a word can be used</li> <li>also a 1-hidden-layer neural network</li> <li>The synthetic training task now uses the average of multiple input context words, rather than a single word as in skip-gram, to predict the center word.</li> <li>Again, the projection weights that turn one-hot words into averageable vectors, of the same width as the hidden layer, are interpreted as the word embeddings.</li> </ul>"},{"location":"KB/CDF/","title":"CDF","text":""},{"location":"KB/CDF/#cdf","title":"CDF","text":"<ul> <li>get cumulative density function \\(\\varphi : \\mathbb{R} \\rightarrow [0,1]\\)</li> </ul>"},{"location":"KB/CIFAR/","title":"CIFAR","text":""},{"location":"KB/CIFAR/#cifar","title":"CIFAR","text":"<ul> <li>60'000 images</li> <li>10 classes with 6'000 images</li> <li>image size: 32x32x3</li> <li>50'000 training, 10'000 testing</li> </ul>"},{"location":"KB/CLIP/","title":"CLIP","text":""},{"location":"KB/CLIP/#clip","title":"CLIP","text":"<ul> <li>Learning Transferable Visual Models from Natural Language Supervision</li> <li>introduces CLIP, a pre-training task which efficiently learns visual concepts from natural language supervision</li> <li>performs language-guided image generation</li> <li>uses vision and language encoders trained in isolation and uses a contrastive loss to bring similar image-text pairs closer, while pulling apart dissimilar pairs as a part of pretaining</li> <li>can be applied to any visual classification benchmark by simply providing the names of the visual categories to be recognized, similar to the \u201czero-shot\u201d capabilities of GPT and GPT3</li> <li>pre-trains an image encoder and a text encoder to predict which images were paired with which texts in our dataset</li> <li>zero-shot classifier</li> <li>they convert all of a dataset\u2019s classes into captions such as \u201ca photo of a dog\u201d and predict the class of the caption CLIP estimates best pairs with a given image</li> </ul> <p>toc: true title: CLIP categories: ['architecture']</p>"},{"location":"KB/CLIP/#clip_1","title":"CLIP","text":"<ul> <li>is a neural network trained on a variety of (image, text) pairs</li> <li>Using CLIP, that can be instructed in natural language to predict the most relevant text snippet, given an image, the model has recently merged as a successful representation learner for images</li> <li>Concretely, CLIP embeddings have several desirable properties</li> <li>they are robust to image distribution shift, have impressive zero-shot capabilities and have been fine-tuned to achieve state-of-theart results</li> <li>the CLIP image embedding decoder module is combined with a prior model, which generates possible CLIP image embeddings from a given text caption</li> </ul>"},{"location":"KB/COCO/","title":"COCO","text":""},{"location":"KB/COCO/#coco","title":"COCO","text":""},{"location":"KB/CRISPR%20%28clustered%20Regularly-interspaced%20Short%20Palindromic%20repeats%29/","title":"CRISPR (clustered Regularly interspaced Short Palindromic repeats)","text":""},{"location":"KB/CRISPR%20%28clustered%20Regularly-interspaced%20Short%20Palindromic%20repeats%29/#crispr-clustered-regularly-interspaced-short-palindromic-repeats","title":"CRISPR (clustered Regularly-interspaced Short Palindromic repeats)","text":"<ul> <li>A relatively precise and reliable DNA-editing technique.</li> </ul>"},{"location":"KB/CTC/","title":"CTC","text":""},{"location":"KB/CTC/#ctc","title":"CTC","text":"<ul> <li>Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks</li> <li>Many real-world sequence learning tasks require the prediction of sequences of labels from noisy, unsegmented input data</li> <li>Recurrent neural networks (RNNs) are powerful sequence learners that would seem well suited to such task</li> <li>hey require pre-segmented training data, and post-processing to transform their outputs into label sequences, their applicability has so far been limited</li> <li>temporal classification</li> <li>label unsegmented sequences directly</li> <li>probabilistic principles</li> <li>TIMIT speech corpus</li> </ul>"},{"location":"KB/CUB-200-2011%204/","title":"CUB-200-2011","text":""},{"location":"KB/CUB-200-2011%204/#cub-200-2011","title":"CUB-200-2011","text":"<ul> <li>Caltech-UCSD Birds 200 (CUB-200) is a dataset of images of birds, with 200 different species of birds and 11,788 images in total.</li> </ul>"},{"location":"KB/CUB-200-2011/","title":"CUB-200-2011","text":""},{"location":"KB/CUB-200-2011/#cub-200-2011","title":"CUB-200-2011","text":"<ul> <li>Caltech-UCSD Birds 200 (CUB-200) is a dataset of images of birds, with 200 different species of birds and 11,788 images in total.</li> </ul>"},{"location":"KB/Cache%20Coherence/","title":"Cache Coherence","text":""},{"location":"KB/Cache%20Coherence/#cache-coherence","title":"Cache Coherence","text":"<ul> <li>Individual CPU caches or memories can become out of synch with each other</li> <li>if one processor updates a location in shared memory, all the other processors know about the update</li> </ul>"},{"location":"KB/Calibration%20Layer/","title":"Calibration Layer","text":""},{"location":"KB/Calibration%20Layer/#calibration-layer","title":"Calibration Layer","text":"<ul> <li>A post-prediction adjustment, typically to account for prediction bias. The adjusted predictions and probabilities should match the distribution of an observed set of labels.</li> </ul>"},{"location":"KB/Candidate%20Sampling/","title":"Candidate Sampling","text":""},{"location":"KB/Candidate%20Sampling/#candidate-sampling","title":"Candidate Sampling","text":"<ul> <li>A training-time optimization in which a probability is calculated for all the positive labels, using, for example, softmax, but only for a random sample of negative labels. For example, if we have an example labeled beagle and dog candidate sampling computes the predicted probabilities and corresponding loss terms for the beagle and dog class outputs in addition to a random subset of the remaining classes (cat, lollipop, fence). The idea is that the negative classes can learn from less frequent negative reinforcement as long as positive classes always get proper positive reinforcement, and this is indeed observed empirically. The motivation for candidate sampling is a computational efficiency win from not computing predictions for all negatives.</li> </ul>"},{"location":"KB/Capacitance/","title":"Capacitance","text":""},{"location":"KB/Capacitance/#capacitance","title":"Capacitance","text":"<ul> <li>Charge stored/Potential Difference</li> <li> \\[C= Q/V\\] </li> </ul>"},{"location":"KB/Capsule%20Layer/","title":"Capsule Layer","text":""},{"location":"KB/Capsule%20Layer/#capsule-layer","title":"Capsule Layer","text":"<ul> <li>Each capsule is a group of neurons that is sensitive to a specific feature of the input image</li> <li></li> <li></li> </ul>"},{"location":"KB/Capsule%20Network/","title":"Capsule Network","text":""},{"location":"KB/Capsule%20Network/#capsule-network","title":"Capsule Network","text":"<ul> <li>replace traditional convolutional and pooling layers with a more biologically inspired architecture that better captures the\u00a0spatial relationships between objects in an image</li> <li>idea that the human visual system is composed of a\u00a0hierarchy of \u201ccapsules\u201d\u00a0that process visual information at different levels of abstraction. </li> <li>Each capsule comprises a\u00a0group of neurons\u00a0sensitive to\u00a0specific features\u00a0of an image, such as the presence of an edge or a particular shape. </li> <li>These features are then combined and passed up the hierarchy to\u00a0higher-level capsules, which extract more abstract concepts such as the identity of an object or the presence of a face.</li> <li>Capsule Networks overcome the problem of translational invariance caused by CNNs.</li> <li>Capsule Networks are able to capture better spatial relationship.\u00a0</li> <li>Capsule Networks uses better downsampling methods which do not cause loss of information seen in CNNs.</li> <li>Capsule Network perform much better than CNNs but are more computationally epensive.</li> </ul>"},{"location":"KB/Capsule%20Network/#drawbacks-of-pooling-layers","title":"Drawbacks of pooling layers","text":"<ul> <li>pooling layers, which\u00a0down-sample\u00a0the input image and can lead to the\u00a0loss of important information\u00a0about the spatial relationships between objects in the image. </li> <li>Capsule networks aim to overcome this limitation by using a different down-sampling mechanism that preserves more spatial information.</li> <li>Capsule Layer</li> <li>Primary Capsule</li> <li>Higher Layer Capsule</li> </ul>"},{"location":"KB/Capsule%20Network/#loss","title":"Loss","text":"<ul> <li>Max Margin Loss </li> <li>Reconstruction loss</li> </ul>"},{"location":"KB/Capsule%20Network/#pros","title":"Pros","text":"<ul> <li>Capsule networks are more robust to image distortions and translations than traditional CNNs</li> <li>They can maintain the spatial relationships between objects in an image</li> <li>They can handle partially obscured objects better</li> <li>They can be used for a variety of tasks, including object recognition and segmentation</li> </ul>"},{"location":"KB/Capsule%20Network/#cons","title":"Cons","text":"<ul> <li>Capsule networks are more complex and computationally expensive than traditional CNNs</li> <li>They are a relatively new architecture, and there is still ongoing research to improve their performance and computational efficiency.</li> </ul>"},{"location":"KB/Capture%20bias/","title":"Capture bias","text":""},{"location":"KB/Capture%20bias/#capture-bias","title":"Capture Bias","text":"<ul> <li>photographers tending to take pictures of objects in similar ways</li> <li>Searching for \"mug\" on Google Image Search will reveal another kind of capture bias: almost all the mugs has a right-facing handle</li> <li>Beyond better data sampling strategies, one way to deal with this is to perform various data transformations to reduce this bias</li> </ul>"},{"location":"KB/Cardinality%20Principle/","title":"Cardinality Principle","text":""},{"location":"KB/Cardinality%20Principle/#cardinality-principle","title":"Cardinality Principle","text":"<ul> <li>how each step in the counting sequence (1,2,3,4,5,6\u2026.) means an increase of one individual</li> </ul>"},{"location":"KB/Carousel/","title":"Carousel","text":""},{"location":"KB/Carousel/#carousel","title":"Carousel","text":"<ul> <li>A rotating platform that delivers objects to a robot and serves as an object queuing system. This carousel delivers the objects, or work pieces, to the loading/unloading station of the robot.</li> </ul>"},{"location":"KB/Case%20Grammar/","title":"Case Grammar","text":""},{"location":"KB/Case%20Grammar/#case-grammar","title":"Case Grammar","text":"<ul> <li>The structure that is built by the parser contains some semantic information, although further interpretation may also be necessary</li> </ul>"},{"location":"KB/Categorical%20Distribution/","title":"Categorical Distribution","text":""},{"location":"KB/Categorical%20Distribution/#categorical-distribution","title":"Categorical Distribution","text":""},{"location":"KB/Causability/","title":"Causability","text":""},{"location":"KB/Causability/#causability","title":"Causability","text":"<ul> <li>measures how exact an interpretable model is in imitating the behavior of a black box. fidelity</li> <li>It is measured in terms of the accuracy score, but with respect to the outcome of the black box, similarly to the model accuracy.</li> </ul>"},{"location":"KB/Causal%201D%20Conv/","title":"Causal 1D Conv","text":""},{"location":"KB/Causal%201D%20Conv/#causal-1d-conv","title":"Causal 1D Conv","text":"<ul> <li>Only past info used for prediction</li> <li>Conv works in both directions and can leak future information into predictions</li> </ul>"},{"location":"KB/Causal%20Dilated%20Conv/","title":"Causal Dilated Conv","text":""},{"location":"KB/Causal%20Dilated%20Conv/#causal-dilated-conv","title":"Causal Dilated Conv","text":"<ul> <li>Receptive field is how much of the input sequence is needed for one prediction</li> </ul>"},{"location":"KB/Causal%20Language%20Model/","title":"Causal Language Model","text":""},{"location":"KB/Causal%20Language%20Model/#causal-language-model","title":"Causal Language Model","text":"<ul> <li>Unlike Masked Language Modeling, this is uni-directional.</li> <li>Can consider words to its left</li> <li>Better for generating text</li> </ul>"},{"location":"KB/Causal%20Systems/","title":"Causal Systems","text":""},{"location":"KB/Causal%20Systems/#causal-systems","title":"Causal Systems","text":"<ul> <li>Does not depend on future input</li> <li>Has memory if current input not fully determined by previous one but influenced by earlier inputs</li> <li>TIme Series</li> </ul>"},{"location":"KB/Causality/","title":"Causality","text":""},{"location":"KB/Causality/#causality","title":"Causality","text":"<ul> <li>An understandable explanation must be created by a machine in a given time (e.g., one hour or one day) and can be comprehended by a user, who need not to be an expert, but has an educational background</li> <li>The user keeps asking a finite number of questions of the machine until he/she can no longer ask why or how because he/she has a satisfactory answer; we say he/she has comprehended.</li> <li>This is the relationship between cause and effect; it is not a synonym for causability</li> <li>Causability is about measuring and ensuring the quality of an explanation and refers to a human model</li> <li>causality requires a wide frame of prior knowledge to prove that observed effects are causal</li> <li>A ML model only discovers correlations among the data it learns from, and therefore might not suffice for unveiling a cause-effect relationship.</li> <li>However, causation involves correlation, so an explainable ML model could validate the results provided by causality inference techniques, or provide a first intuition</li> </ul>"},{"location":"KB/CenterNet/","title":"CenterNet","text":"<p>toc: true title: CenterNet</p> <p>categories: ['temp']</p>"},{"location":"KB/CenterNet/#centernet","title":"CenterNet","text":"<ul> <li>paper</li> <li>center point-based object detection approach</li> <li>end-to-end differentiable</li> <li>bounding box based detectors</li> <li>Anchorless</li> <li>keypoint estimation networks to find center points   id:: 62a89b04-c7bf-4205-9695-39da04c2aafb</li> <li>Linear Regression to all other properties   id:: 62a89d01-53af-46f6-82d4-362fab069b46</li> <li>COCO</li> <li>Single stage   id:: 62a89d42-312d-4f3d-8235-c54a9dfafadf</li> </ul>"},{"location":"KB/Central%20Limit%20Theorem/","title":"Central Limit Theorem","text":""},{"location":"KB/Central%20Limit%20Theorem/#central-limit-theorem","title":"Central Limit Theorem","text":"<ul> <li>When random effects of many independant small sized causes sum up to large scale observable effects : one gets the Normal Distribution</li> <li>Let \\((X_{i})_{i\\in N}\\) is a seq of independant, real valued, [(X_{i}- EX_{i}%20=%20E%5B%5BX_%7Bi%7D-%20E%5BX_%7Bi%7D) \\(P_{S_{n}}\\) of standardized sum variables converge weakly to \\(\\mathscr{N}(0,1|[Square Integrable]\\) . (\\(S_{n}= \\frac{\\Sigma_{i= 1}^{n}(X_{i}- E[X_{i}])}{\\sigma(\\Sigma^{n}_{i=1}X_{i})}\\)\\)<ul> <li>Converge weakly : \\(\\(lim_{n\\rightarrow\\infty}\\int f(x)P_{n}(dx) = \\int f(x)P(dx)\\)\\) for all \\(f: \\mathbb{R} \\rightarrow \\mathbb{R}\\)</li> <li>Lebesgue Integrals</li> </ul> </li> </ul>"},{"location":"KB/Central%20Limit%20Theorem/#x_i-are-identically-distributed","title":"\\(X_{i}\\) Are Identically Distributed","text":"<ul> <li>Regardless of shape of each \\(X_{i}\\), distribution of normalized sum converges to \\(\\mathscr{N}(0,1)\\)</li> <li>Uniformly bounded</li> <li>None of the \\(X_{i}\\) dominates the other \"washing out\"</li> </ul>"},{"location":"KB/Central%20Nervous%20System/","title":"Central Nervous System","text":""},{"location":"KB/Central%20Nervous%20System/#central-nervous-system","title":"Central Nervous System","text":"<ul> <li>brain + Spinal Cord</li> </ul>"},{"location":"KB/Central%20Sulcus/","title":"Central Sulcus","text":""},{"location":"KB/Central%20Sulcus/#central-sulcus","title":"Central Sulcus","text":"<ul> <li>The primary groove in the brain\u2019s cerebrum, which separates the frontal lobe in the front of the brain from the parietal and occipital lobes in the rear of the brain.</li> </ul>"},{"location":"KB/Centrifugal%20Force/","title":"Centrifugal Force","text":""},{"location":"KB/Centrifugal%20Force/#centrifugal-force","title":"Centrifugal Force","text":"<ul> <li>When a body rotates about an axis other than one at it's center of mass, it exerts an outward radial force called centrifugal force upon the axis, which restrains it from moving in a straight tangential line. To offset this force, the robot must exert an opposing torque at the joint of rotation.</li> </ul>"},{"location":"KB/Centripetal%20Force/","title":"Centripetal Force","text":""},{"location":"KB/Centripetal%20Force/#centripetal-force","title":"Centripetal Force","text":"<ul> <li>centripetal force =\u00a0mass x speed2\u00a0  radius of path</li> <li>\\(\\(F_{C}= \\frac{mv^{2}}{r}\\)\\) </li> </ul>"},{"location":"KB/Centroid/","title":"Centroid","text":""},{"location":"KB/Centroid/#centroid","title":"Centroid","text":"<ul> <li>The center of a cluster as determined by a k-means or k-median algorithm. For instance, if k is 3, then the k-means or k-median algorithm finds 3 centroids.</li> </ul>"},{"location":"KB/Cerebellar%20Artery/","title":"Cerebellar Artery","text":""},{"location":"KB/Cerebellar%20Artery/#cerebellar-artery","title":"Cerebellar Artery","text":"<ul> <li>The major blood vessel providing oxygenated blood to the cerebellum.</li> </ul>"},{"location":"KB/Cerebellum/","title":"Cerebellum","text":""},{"location":"KB/Cerebellum/#cerebellum","title":"Cerebellum","text":"<ul> <li>Its function is to coordinate muscle movements, maintain posture, and balance</li> <li>relays information to the Basal Ganglia.</li> <li>It stores automatic learned memories like tying a shoe, playing an instrument, or riding a bike.</li> </ul>"},{"location":"KB/Cerebral%20Palsy/","title":"Cerebral Palsy","text":""},{"location":"KB/Cerebral%20Palsy/#cerebral-palsy","title":"Cerebral Palsy","text":"<ul> <li>A developmental disorder resulting from damage to the brain before or during birth, usually characterized by impaired muscle coordination and body movements, but can also include impaired cognition and social behavior.</li> </ul>"},{"location":"KB/Cerebrospinal%20Fluid%20%28CSF%29/","title":"Cerebrospinal Fluid (CSF)","text":""},{"location":"KB/Cerebrospinal%20Fluid%20%28CSF%29/#cerebrospinal-fluid-csf","title":"Cerebrospinal Fluid (CSF)","text":"<ul> <li>The clear, colorless liquid found surrounding the brain and spinal cord. This fluid can be analyzed to detect diseases.</li> </ul>"},{"location":"KB/Cerebrum/","title":"Cerebrum","text":""},{"location":"KB/Cerebrum/#cerebrum","title":"Cerebrum","text":"<ul> <li>largest part of the brain</li> <li>performs higher functions like interpreting touch, vision and hearing, as well as speech, reasoning, emotions, learning, and fine control of movement</li> <li>Divided by Corpus callosum</li> <li>Surface is called the Brain Cortex</li> <li></li> <li>Frontal lobe</li> <li>Parietal lobe</li> <li>Occipital lobe</li> <li>Temporal lobe</li> </ul>"},{"location":"KB/Chain%20of%20Thought/","title":"Chain of Thought","text":""},{"location":"KB/Chain%20of%20Thought/#chain-of-thought","title":"Chain of Thought","text":"<ul> <li>Chain of Thought Prompting Elicits Reasoning in Large Language Models</li> <li>ability of language models to generate a coherent chain of thought</li> <li>series of short sentences that mimic the reasoning process a person might have when responding to a question</li> <li>the more complex the task of interest is (in the sense of requiring multi-step reasoning approach), the bigger the boost from the chain of thought prompting!</li> <li>chain of thought processing is an emergent property of model scale that can be induced via prompting and can enable sufficiently large language models to better perform reasoning tasks that otherwise have flat scaling curves.</li> </ul>"},{"location":"KB/Challenges%20of%20Words-and-rules/","title":"Challenges of Words-and-rules","text":""},{"location":"KB/Challenges%20of%20Words-and-rules/#challenges-of-words-and-rules","title":"Challenges of Words-and-rules","text":"<ul> <li>Words-and-Rules fits a lot of the data, but is vague on<ul> <li>Exactly what innate structures are available to a child learner</li> <li>Exactly how learning proceeds</li> </ul> </li> </ul>"},{"location":"KB/Change%20Blindness/","title":"Change Blindness","text":""},{"location":"KB/Change%20Blindness/#change-blindness","title":"Change Blindness","text":"<ul> <li>Difficulty detecting changes in separated scenes even after careful inspection</li> <li>Once found viewers agree that it was trivial</li> <li>Not due to limited visual acuity but inappropriate attentional guidance</li> <li>Temporal separation (instead of spatial)</li> <li>sudden changes within a static scene are easily perceived (cf. preattentiveness of motion)</li> <li>Change blindness also in animations if temporal separation spans multiple scenes</li> <li>A scene that should be the same but differs between cuts is known as continuity error</li> </ul>"},{"location":"KB/Change%20in%20Gravitational%20Potential%20Energy/","title":"Change in Gravitational Potential Energy","text":""},{"location":"KB/Change%20in%20Gravitational%20Potential%20Energy/#change-in-gravitational-potential-energy","title":"Change in Gravitational Potential Energy","text":"<ul> <li>mass x gravitational field strenth x\u00a0 difference in height</li> <li> \\[DGPE = mgDh\\] </li> </ul>"},{"location":"KB/Character-set%20dependence/","title":"Character-set dependence","text":""},{"location":"KB/Character-set%20dependence/#character-set-dependence","title":"Character-set Dependence","text":"<ul> <li>ASCII</li> <li>8 bit character set</li> <li>2 byte character set</li> <li>Unicode 5.0</li> </ul>"},{"location":"KB/Characteristics%20of%20Visual%20Variables/","title":"Characteristics of Visual Variables","text":""},{"location":"KB/Characteristics%20of%20Visual%20Variables/#characteristics-of-visual-variables","title":"Characteristics of Visual Variables","text":"<ul> <li>Visual Selective</li> <li>Visual Associative</li> <li>Visual Ordered</li> <li>Visual Quantitative</li> <li>Visual Length </li> </ul>"},{"location":"KB/Charge/","title":"Charge","text":""},{"location":"KB/Charge/#charge","title":"Charge","text":"<ul> <li>Current x time</li> <li> \\[DQ = IDt\\] </li> </ul>"},{"location":"KB/Chat%20GPT%20is%20Not%20All%20You%20Need/","title":"chatgptisnotallyouneed","text":""},{"location":"KB/Chat%20GPT%20is%20Not%20All%20You%20Need/#chatgptisnotallyouneed","title":"Chatgptisnotallyouneed","text":"<ul> <li> <p>@gozalo-brizuelaChatGPTNotAll2023</p> </li> <li> <p>DALL\u00b7E 2</p> </li> <li> <p>CLIP</p> </li> <li> <p>Stable Difusion</p> </li> <li> <p>Muse</p> </li> <li> <p>Dreamfusion</p> </li> <li> <p>Magic3D</p> </li> <li> <p>Flamingo</p> </li> <li> <p>VisualGPT</p> </li> <li> <p>Phenaki</p> </li> <li> <p>Soundify</p> </li> <li> <p>AudioLM</p> </li> <li> <p>Jukebox</p> </li> <li> <p>Whisper</p> </li> <li> <p>ChatGPT</p> </li> <li> <p>LaMDA</p> </li> <li> <p>PEER</p> </li> <li> <p>Meta AI Speech from Brain</p> </li> <li> <p>Codex</p> </li> <li> <p>Alphacode</p> </li> <li> <p>Galactica</p> </li> <li> <p>Minerva</p> </li> <li> <p>Imagen</p> </li> </ul>"},{"location":"KB/ChatGPT/","title":"ChatGPT","text":""},{"location":"KB/ChatGPT/#chatgpt","title":"ChatGPT","text":"<ul> <li>interacts in a conversational way</li> <li>the model answers follow-up questions, challenges incorrect premises and reject inappropriate requests</li> <li>Reinforcement Learning for Human Feedback</li> <li>an initial model is trained using supervised fine-tuning: human AI trainers would provide conversations in which they played both sides, the user and an AI assistant</li> <li>those people would be given the model-written responses to help them compose their response</li> <li>This dataset was mixed to that of InstructGPT [3], which was transformed into a dialogue format</li> </ul>"},{"location":"KB/Chebyshev%20Distance/","title":"Chebyshev Distance","text":""},{"location":"KB/Chebyshev%20Distance/#chebyshev-distance","title":"Chebyshev Distance","text":"<ul> <li> \\[D(x,y) = max_{i}(|x_{i}-y_{i}|)\\] </li> <li>greatest of difference between two vectors along any coordinate dimension</li> <li>simply the maximum distance along one axis.</li> <li>Chessboard distance since the minimum number of moves needed by a king to go from one square to another is equal to Chebyshev distance</li> <li>can be used to extract the minimum number of moves needed to go from one square to another</li> <li>warehouse logistics as it closely resembles the time an overhead crane takes to move an object</li> </ul>"},{"location":"KB/CheckList/","title":"CheckList","text":""},{"location":"KB/CheckList/#checklist","title":"CheckList","text":"<ul> <li>Beyond Accuracy: Behavioral Testing of NLP Models with CheckList</li> <li>ML systems can run to completion without throwing any errors (indicating functional correctness) but can still produce incorrect outputs (indicating behavioral issues)</li> <li>CheckList</li> <li>model-agnostic and task-agnostic methodology for testing NLP models inspired by principles of behavioral testing</li> <li>matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation</li> <li>Minimum Functionality Test (MFT): A Minimum Functionality Test (MFT) uses simple examples to make sure the model can perform a specific task well. For example, they might want to test the performance of a sentiment model when dealing with negations</li> <li>Invariance Test: Besides testing the functionality of a model, they might also want to test if the model prediction stays the same when trivial parts of inputs are slightly perturbed. These tests are called Invariance Tests (IV)</li> <li>Directional Expectation Test: In the Invariance Test, they expect the outputs after the perturbation to be the same. However, sometimes they might expect the output after perturbation to change. That is when Directional Expectation Tests comes in handy</li> </ul>"},{"location":"KB/Chi%20Squared%20Distance/","title":"Chi Squared Distance","text":""},{"location":"KB/Chi%20Squared%20Distance/#chi-squared-distance","title":"Chi Squared Distance","text":"<ul> <li> \\[d= \\Sigma_{i}\\frac{p_{i}-q_{i}}{p_{i}+q_{i}}\\] </li> </ul>"},{"location":"KB/Chimera/","title":"Chimera","text":""},{"location":"KB/Chimera/#chimera","title":"Chimera","text":"<ul> <li>A single organism with cells from more than one distinct genotype.</li> </ul>"},{"location":"KB/Chinchilla/","title":"Chinchilla","text":""},{"location":"KB/Chinchilla/#chinchilla","title":"Chinchilla","text":"<ul> <li>Training Compute-Optimal Large Language Models</li> <li>given a 10x increase in computational budget, model size should increase 5.5x, and the number of tokens should only increase 1.8x</li> <li>model and data size should increase in accordance</li> <li>collecting high-quality datasets will play a key role in further scaling of LLMs</li> <li>optimal model size and number of tokens for training a transformer language model under a given compute budget</li> <li>By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, they find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled</li> <li>significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks</li> <li>ubstantially less compute for fine-tuning and inference, greatly facilitating downstream usage</li> <li>MMLU</li> </ul>"},{"location":"KB/Chronic%20Encephalopathy%20Syndrome%20%28CES%29/","title":"Chronic Encephalopathy Syndrome (CES)","text":""},{"location":"KB/Chronic%20Encephalopathy%20Syndrome%20%28CES%29/#chronic-encephalopathy-syndrome-ces","title":"Chronic Encephalopathy Syndrome (CES)","text":"<ul> <li>Symptoms, including memory issues, depression, and impulsive behavior, that manifest themselves after repeated brain traumas. Over time, CES can result in a diagnosis of chronic traumatic encephalopathy (CTE))).md).</li> </ul>"},{"location":"KB/Chronic%20Traumatic%20Encephalopathy%20%28CTE%29/","title":"Chronic Traumatic Encephalopathy (CTE)","text":""},{"location":"KB/Chronic%20Traumatic%20Encephalopathy%20%28CTE%29/#chronic-traumatic-encephalopathy-cte","title":"Chronic Traumatic Encephalopathy (CTE)","text":"<ul> <li>Once known as dementia pugilistica and thought to be confined largely to former boxers, this neurodegenerative disease, with symptoms including impulsivity, memory problems, and depression, affects the brains of individuals who have suffered repeated concussions and traumatic brain injuries.</li> </ul>"},{"location":"KB/Chronic/","title":"Chronic","text":""},{"location":"KB/Chronic/#chronic","title":"Chronic","text":"<ul> <li>Describes a condition that is persistent or recurring</li> </ul>"},{"location":"KB/Circular%20Motion%20Type/","title":"Circular Motion Type","text":""},{"location":"KB/Circular%20Motion%20Type/#circular-motion-type","title":"Circular Motion Type","text":"<ul> <li>A calculated path that the robot executes, and is circular in shape.</li> </ul>"},{"location":"KB/Circumfix/","title":"Circumfix","text":""},{"location":"KB/Circumfix/#circumfix","title":"Circumfix","text":"<ul> <li>precede and follow the stem</li> </ul>"},{"location":"KB/Cityscapes/","title":"Cityscapes","text":""},{"location":"KB/Cityscapes/#cityscapes","title":"Cityscapes","text":""},{"location":"KB/Clamp/","title":"Clamp","text":""},{"location":"KB/Clamp/#clamp","title":"Clamp","text":"<ul> <li>An end-effector which serves as a pneumatic hand that controls the grasping and releasing of an object. Tactile, and feed-back force sensors are used to manage the applied force to the object by the clamp.</li> </ul>"},{"location":"KB/Clamping/","title":"Clamping","text":""},{"location":"KB/Clamping/#clamping","title":"Clamping","text":"<ul> <li>The maximum permissible force acting on a body region, resulting from a robot collision where the period of contact results in a plastic deformation of a person\u2019s soft tissue.</li> </ul>"},{"location":"KB/Class%20Conditional%20distribution/","title":"Class Conditional Distribution","text":""},{"location":"KB/Class%20Conditional%20distribution/#class-conditional-distribution","title":"Class Conditional Distribution","text":"<ul> <li>\\(f_{i}\\) is the PDF for \\(P_{X|Y=c_{i}}\\)</li> </ul>"},{"location":"KB/Class%20Size/","title":"Class Size","text":""},{"location":"KB/Class%20Size/#class-size","title":"Class Size","text":"<ul> <li>Class inclusion seq : Set of candidate models with increasing flexibility</li> <li> \\[\\mathcal{H}_{1} \\subset \\mathcal{H}_{2} \\subset, \u2026, \\subset \\mathcal{H}_{l} \\] </li> </ul>"},{"location":"KB/Classification%20Ray%20Casting/","title":"Classification Ray Casting","text":""},{"location":"KB/Classification%20Ray%20Casting/#classification-ray-casting","title":"Classification Ray Casting","text":"<ul> <li>Transfer Function</li> <li>Pre Classification</li> <li>Post Classification</li> </ul>"},{"location":"KB/Classifier%20Gradients/","title":"Classifier Gradients","text":""},{"location":"KB/Classifier%20Gradients/#classifier-gradients","title":"Classifier Gradients","text":"<ul> <li>For example, if we want to add sunglasses to an image of a face, we can used a trained classifier that identifies if a personal has that feature.</li> <li>To do this, we can take a batch of noise vector Z that goes through the generator.</li> <li>We then pass this image through a classifier, in this case a sunglasses classifier, which will tell us if the output has that feature.</li> <li>We the use this information to modify the Z vectors, without modifying the weights of the generator at all.</li> <li>To do so, we modify the Z vectors by moving in the direction of the gradient with the costs that will penalize the model for images classified as not having sunglasses. </li> <li>We then repeat this process until the images are classified with the desired feature.</li> <li>The downside with this method is that we need a pre-trained classifier that can detect the desired feature, which may not always be readily available.</li> </ul>"},{"location":"KB/Classifying%20a%20specific%20image%20region%20using%20convolutional%20nets%20with%20an%20ROI%20mask%20as%20input/","title":"Classifying a specific image region using convolutional nets with an ROI mask as input","text":""},{"location":"KB/Classifying%20a%20specific%20image%20region%20using%20convolutional%20nets%20with%20an%20ROI%20mask%20as%20input/#classifying-a-specific-image-region-using-convolutional-nets-with-an-roi-mask-as-input","title":"Classifying a Specific Image Region Using Convolutional Nets with an ROI Mask as Input","text":"<ul> <li>Eppel, Sagi. \u201cClassifying a Specific Image Region Using Convolutional Nets with an ROI Mask as Input,\u201d n.d., 8.</li> </ul>"},{"location":"KB/Classifying%20a%20specific%20image%20region%20using%20convolutional%20nets%20with%20an%20ROI%20mask%20as%20input/#intro","title":"Intro","text":"<ul> <li>In some cases, it is desirable to classify only a specific region of the image that corresponds to a certain object.</li> <li>Hence, assuming that the region of the object in the image is known in advance and is given as a binary region of interest (ROI) mask, the goal is to classify the object in this region using a convolutional neural net.</li> <li>This goal is achieved using a standard image classification net with the addition of a side branch, which converts the ROI mask into an attention map. This map is then combined with the image classification net</li> <li>focus the attention on the object region while still extracting contextual cues from the background</li> <li>COCO</li> <li>OpenSurfaces materials dataset</li> </ul>"},{"location":"KB/Classifying%20a%20specific%20image%20region%20using%20convolutional%20nets%20with%20an%20ROI%20mask%20as%20input/#network","title":"Network","text":"<ul> <li>combining the attention map at the first layer of the net gave better results than combining it at higher layers of the net</li> <li>An alternative approach is to generate an attention map, which can be used by the net to extract features from both objects and the background using the ROI mask as an additional input to the net</li> <li>An attention map can easily be generated from the input ROI mask using a convolution layer</li> <li>This attention map is then combined with one or more layers of the main branch, either by element-wise addition or multiplication</li> <li>The combined layer is then used as an input for the next layer of the main branch</li> <li>In order to allow element-wise addition or multiplication, the attention map must be the same size as the layer with which it is combined. To achieve this, the ROI mask was first resized to match the size of the layer with which it was merged, and a convolution layer was then applied with the same number of filters as the depth of the target layer.</li> <li>For cases where the attention maps were combined with more than one layer , a separate attention map was generated using different convolution filters for each layer</li> </ul>"},{"location":"KB/Classifying%20a%20specific%20image%20region%20using%20convolutional%20nets%20with%20an%20ROI%20mask%20as%20input/#net-initiation","title":"Net Initiation","text":"<ul> <li>The convolution layer of the side branch was initialized as follows: if the attention map was to be merged by element-wise addition, both the weights and the bias were initialized to zero; if the attention map was to be merged multiplication, the bias was set to one and the filter weights to zero</li> <li>This weights initiation method promise that the initial effect of the attention branch on the classification branch is zero at the outset and increases gradually during training.</li> </ul>"},{"location":"KB/Classifying%20a%20specific%20image%20region%20using%20convolutional%20nets%20with%20an%20ROI%20mask%20as%20input/#datasets","title":"Datasets","text":"<ul> <li>The nets were also trained using the OpenSurfaces material classification dataset1 0 ; in this case, the ROI was generated by taking a connected region of the image corresponding to a single material, and the output was the material type.</li> </ul>"},{"location":"KB/Classifying%20a%20specific%20image%20region%20using%20convolutional%20nets%20with%20an%20ROI%20mask%20as%20input/#results","title":"Results","text":"<ul> <li>It can be seen that methods based on generating an attention map and combining it with the main branch net branch gave considerably better accuracy than hard attention methods based on blacking out the background region3</li> <li>The difference in accuracy is particularly large for the classification of small segments where background information is more important in classification.</li> <li>Merging the attention map with the first layer of the net gave significantly better results than merging at higher layers</li> <li>This probably due to the fact that higher layers of the net suffer from a loss of high-resolution information that is relevant in the classification of small objects.</li> <li>Generating several attention maps and merging them with multiple layers of the net gave the same or worse results than generating a single attention map and merging it with the first layer</li> </ul>"},{"location":"KB/Classifying%20a%20specific%20image%20region%20using%20convolutional%20nets%20with%20an%20ROI%20mask%20as%20input/#images","title":"Images","text":""},{"location":"KB/Clear%20Thinking/","title":"Clear Thinking","text":""},{"location":"KB/Clear%20Thinking/#clear-thinking","title":"Clear Thinking","text":"<ul> <li>Developing good abstractions, notations, visualizations, and so forth, is improving the user interfaces for ideas.</li> <li>This helps both with understanding ideas for the first time and with thinking clearly about them.</li> <li>Conversely, if we can\u2019t explain an idea well, that\u2019s often a sign that we don\u2019t understand it as well as we could.</li> </ul>"},{"location":"KB/Clustering/","title":"Clustering","text":""},{"location":"KB/Clustering/#clustering","title":"Clustering","text":"<ul> <li>KMeans</li> <li>SOMs</li> </ul>"},{"location":"KB/Clutter%20In%20Visualisation/","title":"Clutter In Visualisation","text":""},{"location":"KB/Clutter%20In%20Visualisation/#clutter-in-visualisation","title":"Clutter In Visualisation","text":""},{"location":"KB/Co%20adaptation/","title":"Co adaptation","text":""},{"location":"KB/Co%20adaptation/#co-adaptation","title":"Co Adaptation","text":"<ul> <li>Computing the gradient is done with respect to the error, but also with respect to what all other units are doing (Srivastava et al., 2014). This means that certain neurons, through changes in their weights, may fix the mistakes of other neurons. These, Srivastava et al. (2014) argue, lead to complex co-adaptations that may not generalize to unseen data, resulting in overfitting.</li> </ul>"},{"location":"KB/Co-Mixup/","title":"Co-Mixup","text":""},{"location":"KB/Co-Mixup/#co-mixup","title":"Co-Mixup","text":"<ul> <li>@kimCoMixupSaliencyGuided2021</li> <li>salient image mixing on a batch of input images to generate a batch of augmented images</li> <li>This technique maximizes saliency in output images by penalizations to ensure local data smoothness and diverse image regions</li> </ul>"},{"location":"KB/Co-training/","title":"Co training","text":""},{"location":"KB/Co-training/#co-training","title":"Co-training","text":"<ul> <li>A semi-supervised learning approach particularly useful when all of the following conditions are true</li> <li>The ratio of unlabeled examples to labeled examples in the dataset is high.</li> <li>This is a classification problem (binary or multi-class).</li> <li>The dataset contains two different sets of predictive features that are independent of each other and complementary.</li> <li>Co-training essentially amplifies independent signals into a stronger signal.</li> </ul>"},{"location":"KB/Coarse-grained%20assessment/","title":"Coarse-grained assessment","text":""},{"location":"KB/Coarse-grained%20assessment/#coarse-grained-assessment","title":"Coarse-grained Assessment","text":"<ul> <li>If the required assessment is coarse grained, such as a single number reporting the student's competence on the current unit, then it is usually computed from several measures such as:</li> <li>A measure of progress and coverage, such as the number of problems solved or the number of correct steps.</li> <li>A measure of the amount of help given, such as the number of hint sequences started and the proportion that ended with a bottom-out hint.</li> <li>Some measure of competence, such as the frequency of incorrect initial steps, the time required to enter a correct step, or the number of attempts at a step before it is entered correctly.</li> <li>Psychometrics is the field that studies how to do this for conventional tests (e.g., multiple choice tests)</li> <li>One of their main tools is item-response theory IRT</li> </ul>"},{"location":"KB/Cochlea/","title":"Cochlea","text":""},{"location":"KB/Cochlea/#cochlea","title":"Cochlea","text":"<ul> <li>The part of the inner ear that transforms sound vibrations into neural impulses.</li> </ul>"},{"location":"KB/Codex/","title":"Codex","text":""},{"location":"KB/Codex/#codex","title":"Codex","text":"<ul> <li>translates text to code</li> <li>general-purpose programming model, as it can be applied to basically any programming task</li> <li>Programming can be broken down into two parts: breaking a problem down into simpler problems and mapping those problems into existing code (libraries, APIs, or functions) that already exist</li> <li>The second part is the most time-barring part for programmers, and it is where Codex excels the most</li> <li>model is fine-tuned from GPT-3, which already contains strong natural language representations</li> </ul>"},{"location":"KB/CogMod%20Final%20Paper/","title":"CogMod Final Paper","text":""},{"location":"KB/CogMod%20Final%20Paper/#cogmod-final-paper","title":"CogMod Final Paper","text":"<ul> <li>The Reward Experiment</li> <li>An ACT-R model that explains at least one of these effects (well)  </li> <li>Report to justify design choices</li> <li>\u2018cognitive\u2019 interpretation, that is justifiable &amp; plausible</li> </ul>"},{"location":"KB/CogMod%20Final%20Paper/#to-test","title":"To test","text":"<ul> <li>linear decrease in goal activation </li> </ul>"},{"location":"KB/CogMod%20Final%20Paper/#papers","title":"Papers","text":"<ul> <li>Sequential effects within a short foreperiod context Evidence for the conditioning account of temporal preparation : IMPORTANT</li> <li>Traces of times past Representations of temporal intervals in memory : Explain the fanning effect</li> <li>The warning stimulus as retrieval cue The role of associative memory in temporal preparation : temporal preparation , sloping effects</li> <li>Modeling motivation using goal competition in mental fatigue studies : performance in reward vs non reward , distraction, linear decrease in goal activation</li> <li>The neural correlates of mental fatigue and reward processing - A task-based fMRI study : studies the physical brain effect of reward and fatigue</li> <li>Implicitly learning when to be ready - From instances to categories : fanning , maybe the graphs can also be explained by categorical association instead of just instance based</li> <li>On the Distinction Between Perceived Duration and Event Timing - Towards a Unified Model of Time Perception : brain uses temporal expectations to bias perception in a way that stimuli are \u2018regularized\u2019</li> <li>Change of Variable Foreperiod Effects within an Experiment - A Bayesian Modeling Approach : sequential modulation, which is attributed to feature binding and retrieval by the BRAC framework, could have different underlying mechanisms depending on the task scenario.</li> <li>Revisiting variable foreperiod effects evaluating the repetition priming account : change in foreperiod </li> </ul>"},{"location":"KB/Cognition%20Hazard%20Rates/","title":"Cogntition Hazard Rates","text":""},{"location":"KB/Cognition%20Hazard%20Rates/#cogntition-hazard-rates","title":"Cogntition Hazard Rates","text":"<ul> <li>{Proven wrong} : Cognitive fMTP</li> <li>Mathematical construct about probability</li> <li>Continuously tracking the odds the event appeads rn given it has not happened yet</li> <li>Idea : Use this \"hazard rate\" to decide when to prepare</li> <li>RT is proportional to hazard</li> <li>Optimally prepared if certain</li> </ul>"},{"location":"KB/Cognition%20Hazard%20Rates/#distributions-used-pdf","title":"Distributions Used (PDF)","text":"<ul> <li>Constant, exponential, flipped exponential</li> <li>Hazard rate is this pdf by 1-F</li> <li> \\[h(t) = \\frac{f(t)}{1-F(t)}\\] </li> <li></li> </ul>"},{"location":"KB/Cognition%20Hazard%20Rates/#hazard-rates","title":"Hazard Rates","text":"<ul> <li>How does that translate to RT?</li> <li>Proposed<ul> <li>\\(RT = c- h(t)\\) : linear effect</li> <li>\\(RT = c+ \\frac{1}{h(t)}\\) : inverse relation</li> </ul> </li> <li>dashed : \\(\\frac{1}{hazard}\\)</li> <li></li> <li></li> </ul>"},{"location":"KB/Cognition%20Hazard%20Rates/#vs-act-r","title":"Vs ACT-R","text":"<ul> <li>Prepare for \u2018the right moment\u2019  <ul> <li>\u2018degree of preparation\u2019 given by moment-to-moment hz  </li> </ul> </li> <li>\u2018the right moment\u2019 is estimated based on time (pulses) and memory (DM)  <ul> <li>No \u2018time\u2019, no explicit memory?  </li> </ul> </li> <li>If we are prepared\u2192 benefit, else cost  <ul> <li>Scaled benefits (useful for assignment)</li> <li>Does not specify why/how; i.e., what preparation is  </li> </ul> </li> <li>No active process during the interval  <ul> <li>Active tracking</li> </ul> </li> <li>Once we are prepared, it doesn\u2019t \u2018go away\u2019  <ul> <li>A by-product of the Hazard rate</li> </ul> </li> <li>No memory model  <ul> <li>Such mathematical models give no mechanism for how the pdf is stored in memory, retrieved, or used\u2026</li> </ul> </li> </ul>"},{"location":"KB/Cognition%20Hazard%20Rates/#problems","title":"Problems","text":"<ul> <li>Does not explain preparation</li> <li>How do particpants \u2018learn\u2019 the distribution?</li> <li>Do participants truly track \u2018conditional probabilities\u2019 throughout the foreperiod</li> </ul>"},{"location":"KB/Cognition%20Hazard%20Rates/#extending","title":"Extending","text":"<ul> <li>Does not store PDFs in memory, which sucks<ul> <li>Does not keep track of time as well</li> </ul> </li> <li>Subjective hazard/ anticipation function<ul> <li>Temporal uncertainty</li> <li>Blur the pdf such that later points are less certain using a Gaussian filter that gets wider for later points in time</li> <li> \\[f'(t) = \\frac{1}{\\theta t \\sqrt{2 \\pi}} \\int_{-\\infty}^{\\infty}f(\\tau)e^{-\\frac{(r-t)^{2}}{2 \\theta^{2}t^{2}}}d \\tau\\] </li> <li>Climb to 1 after a while</li> <li>Hazard is more even though probs are equal in classical. This equates them and makes them less blurred out</li> <li></li> </ul> </li> </ul>"},{"location":"KB/Cognition%20Hazard%20Rates/#images","title":"Images","text":""},{"location":"KB/Cognitive%20Engagement/","title":"Cognitive Engagement","text":""},{"location":"KB/Cognitive%20Engagement/#cognitive-engagement","title":"Cognitive Engagement","text":"<ul> <li>Cognitive effort is a form of labor, and unsurprisingly, people tend to favor less demanding forms of cognition and other mental shortcuts [18, 51].</li> <li>Unfortunately, this human tendency can lead to unintended or dangerous outcomes because humans are susceptible to a wide variety of cognitive biases such as confirmation bias</li> <li>Confirmation bias [41]</li> <li>refers to the interpreting of new evidence in ways that confirm one's existing beliefs</li> <li>For XAI, this manifests as practitioners only superficially examining explanations instead of digging deeply, leading to over-trust, misuse, and a lack of accurate understanding of the outputs [31]</li> <li>Forcing users to cognitively engage through some small task before showing a system's output yielded the highest performance in a comparative study [21]</li> <li>Train conductors in Japan famously point and call out important information on their journeys\u2014a cognitive forcing method which has reduced human errors by nearly 85% [45].</li> <li>Realistically, how much will users actually cognitively engage with the magnitude of generated outputs to ensure that they are correct and aligned with their intentions?</li> </ul>"},{"location":"KB/Cognitive%20Foreperiod/","title":"Cognitive Foreperiod","text":""},{"location":"KB/Cognitive%20Foreperiod/#cognitive-foreperiod","title":"Cognitive Foreperiod","text":"<ul> <li>Time before stimulus</li> <li>Prepare to act</li> </ul>"},{"location":"KB/Cognitive%20Foreperiod/#constant-fp","title":"Constant FP","text":"<ul> <li>No uncertainty</li> <li>Up to ~150ms : RT decrease, then increase</li> <li>People prepare if warning - faster</li> <li> <p>if longer intervals</p> <ul> <li>avg response time increases</li> <li>Temporal estimates get noiser : Temporal Uncertainty<ul> <li>not really prepared</li> </ul> </li> </ul> </li> <li> <p>Faster if less time to prepare</p> </li> </ul>"},{"location":"KB/Cognitive%20Foreperiod/#variable-fp","title":"Variable FP","text":"<ul> <li>Faster if more time to prepare</li> <li>Asymptotic decrease : plateau</li> <li>Try your best based on exp to be prepared<ul> <li>But stay prepared if you already are</li> </ul> </li> <li>\"Strategic\" : aim to be prepared as late<ul> <li>Why? Dunno. Maybe energy conservation</li> </ul> </li> </ul>"},{"location":"KB/Cognitive%20Foreperiod/#distribution-effects","title":"Distribution Effects","text":"<ul> <li>Uniform</li> <li>Exponential<ul> <li>Many shorts</li> </ul> </li> <li>Anti Exponential<ul> <li>Many long</li> </ul> </li> <li>Prep strategy altered based on which type of distribution</li> </ul>"},{"location":"KB/Cognitive%20Foreperiod/#sequential-effects","title":"Sequential Effects","text":"<ul> <li>Prep dependent on previous trials<ul> <li>If prev short, present longer : RTs are slow</li> </ul> </li> <li>Traces of gradually forgetting previous trials<ul> <li>in the shape of prep effects</li> </ul> </li> <li>Cannot just be accounted for by n-1 trials</li> </ul>"},{"location":"KB/Cognitive%20Foreperiod/#transfer-effects","title":"Transfer Effects","text":"<ul> <li>Start with uniform - then something else - then uniform again</li> <li>Long lasting effects</li> <li>Even if participants were informed that things changed</li> <li>Even a week later</li> <li></li> </ul>"},{"location":"KB/Cognitive%20Foreperiod/#motivation-determined-by","title":"Motivation Determined by","text":"<ul> <li>Time</li> <li>Memory</li> <li> <p>Motivation - still works on earlier prep?</p> </li> <li> <p>Cognition Hazard Rates</p> </li> </ul>"},{"location":"KB/Cognitive%20Multitasking/","title":"Cognitive Multitasking","text":""},{"location":"KB/Cognitive%20Multitasking/#cognitive-multitasking","title":"Cognitive Multitasking","text":"<ul> <li>No overlap between areas of brain in fMRI with similar tasks concurrently</li> <li>Threaded Cognition</li> <li>Natural way of cognition, need to be prepared for a new \"task\"</li> <li>If unused brain resources, put it to use</li> <li>Mental Fatigue</li> </ul>"},{"location":"KB/Cognitive%20Preparation/","title":"Cognitive Preparation","text":""},{"location":"KB/Cognitive%20Preparation/#cognitive-preparation","title":"Cognitive Preparation","text":"<ul> <li>How brains use time to make decisions</li> <li>Reflects implicit learning mechanisms<ul> <li>relies to optimize behavior</li> </ul> </li> <li>Many models assume brain can time but not about how time is implemented</li> <li>Prepartion effects are present on different time scales</li> <li>Within trial</li> <li>Block of trials</li> <li>Across blocks of trials</li> <li>Cognitive Multitasking</li> </ul>"},{"location":"KB/Cognitive%20Preparation/#design","title":"Design","text":"<ul> <li>Task has to be boring -.-</li> </ul>"},{"location":"KB/Cognitive%20Preparation/#terms","title":"Terms","text":"<ul> <li>Cognitive Foreperiod</li> </ul>"},{"location":"KB/Cognitive%20fMTP/","title":"Cognitive fMTP","text":""},{"location":"KB/Cognitive%20fMTP/#cognitive-fmtp","title":"Cognitive fMTP","text":"<ul> <li>Preparation effects manifest in the motor system</li> <li>Preparation is a balance between inhibition and activation</li> <li>A neural representation of time  </li> <li>A (crude) model of the motor system  </li> <li>Hebbian associations + Forgetting &amp; Retrieval</li> </ul>"},{"location":"KB/Cognitive%20fMTP/#between-fore-and-start","title":"Between Fore and Start","text":"<ul> <li>Timing<ul> <li>Layer of time cells</li> <li></li> </ul> </li> <li>Preparation<ul> <li>The motor system \u2018stages\u2019 a response, but holds it under inhibition..</li> <li>When the Go-stimulus (S2) arrives: activation</li> <li></li> </ul> </li> </ul>"},{"location":"KB/Cognitive%20fMTP/#learning","title":"Learning","text":"<ul> <li>Different 'Time cells' and 'motor inhibiton &amp; activation' are active at the same time: this leads to hebbian learning</li> <li>Fire together, wire together</li> <li>Forms memory traces : aka chunk</li> <li></li> </ul>"},{"location":"KB/Cognitive%20fMTP/#retrieval","title":"Retrieval","text":"<ul> <li>At the start, high degree of inhibition and low degree of activation</li> <li>If prev trial is short, inhibition short and more activation</li> <li> \\[RT = \\frac{I}{A}\\] </li> <li>More activation retrieved : faster</li> <li>Recency weighted</li> <li>Declarative Memory Blending</li> <li>Preparation := Ratio of retrieved I vs. A</li> <li></li> </ul>"},{"location":"KB/Cognitive%20fMTP/#vs-act-r","title":"Vs ACT-R","text":"<ul> <li>Prepare for \u2018the right moment\u2019  <ul> <li>Moment-to-moment balance of I and A  </li> </ul> </li> <li>\u2018the right moment\u2019 is estimated based on time (pulses) and memory (DM)  <ul> <li>Similar; but memory \u2018chunks\u2019 contain I- and A-traces not a single moment at which one should be prepared  </li> </ul> </li> <li>If we are prepared\u2192 benefit, else cost  <ul> <li>Scaled benefits </li> <li>Inhibiton increases RT; activation decreases RT  </li> </ul> </li> <li>No active process during timing  <ul> <li>Continuously retrieving associated memories?  </li> </ul> </li> <li>Once we are prepared, it doesn\u2019t \u2018go away\u2019  <ul> <li>Consequence of \u2018more A, less I retrieved\u2019</li> </ul> </li> </ul>"},{"location":"KB/Cold%20email%20templates/","title":"Cold email templates","text":""},{"location":"KB/Cold%20email%20templates/#cold-email-templates","title":"Cold email templates","text":""},{"location":"KB/Cold%20email%20templates/#eg1","title":"Eg1","text":"<p>\"Hi Betsy,</p> <p>My name is Alison Parker. I'm a freelance writer covering science and psychology. I've written for several publications, including\u00a0[X],\u00a0[Y]\u00a0and\u00a0[Z publications].\u00a0I've been doing this work for about three years, and I'm really passionate about making people think critically about the world.</p> <p>My former boss,\u00a0[X boss' name], who you used to work with at\u00a0[X company], mentioned you\u00a0when I told her I was looking for a full-time position. She said your magazine is hiring a senior science reporter. I was pleasantly surprised to hear your name,\u00a0as I've been reading your work for years! I especially enjoyed the cover story you wrote for\u00a0[X name of publication]\u00a0in 2016 about the psychology of consumer spending.</p> <p>I submitted an application for the position online, but even if things don't work out, I'd be so grateful for a 30 minute call to learn about your experience and what advice you have for young professionals in this field. I'm free any time after 3 p.m. (ET) on Wednesday, Thursday and Friday this week.</p> <p>(P.S. I saw your call on Twitter for volunteers to help at the food drive at\u00a0[X location]\u00a0this weekend. I organized a few of these in college and would love to help out, so count me in!)</p> <p>Thanks so much for your time, Alison\"</p>"},{"location":"KB/Collaborative%20Recommender/","title":"Collaborative Recommender","text":""},{"location":"KB/Collaborative%20Recommender/#collaborative-recommender","title":"Collaborative Recommender","text":"<ul> <li>Clusters users according to behavior</li> <li>Match with other users</li> <li>eg : netflix</li> </ul>"},{"location":"KB/Collaborative%20Topic%20Regression/","title":"Collaborative Topic Regression","text":""},{"location":"KB/Collaborative%20Topic%20Regression/#collaborative-topic-regression","title":"Collaborative Topic Regression","text":"<ul> <li>Collaborative Deep Learning for Recommender Systems</li> <li>Collaborative filtering (CF) is a successful approach commonly used by many recommender systems</li> <li>Conventional CF-based methods use the ratings given to items by users as the sole source of information for learning to make recommendation</li> <li>However, the ratings are often very sparse in many applications, causing CF-based methods to degrade significantly in their recommendation performance</li> <li>To address this sparsity problem, auxiliary information such as item content information may be utilized</li> <li>Collaborative topic regression (CTR) is an appealing recent method taking this approach which tightly couples the two components that learn from two different sources of information</li> <li>Nevertheless, the latent representation learned by CTR may not be very effective when the auxiliary information is very sparse.</li> <li>generalizing recent advances in deep learning from i.i.d input to non-i.i.d (CF-based) input and propose a hierarchical Bayesian model called collaborative deep learning (CDL), which jointly performs deep representation learning for the content information and collaborative filtering for the ratings (feedback) matrix.</li> </ul>"},{"location":"KB/Collective%20Ethical%20Decision%20Frameworks/","title":"Collective Ethical Decision Frameworks","text":""},{"location":"KB/Collective%20Ethical%20Decision%20Frameworks/#collective-ethical-decision-frameworks","title":"Collective Ethical Decision Frameworks","text":"<ul> <li>advocates the need of primary rules governing social norms and allowing the creation, modification and suppression of the primary rules with secondary rules as situations evolve.</li> </ul>"},{"location":"KB/Collective%20Interpretation/","title":"Collective Interpretation","text":""},{"location":"KB/Collective%20Interpretation/#collective-interpretation","title":"Collective Interpretation","text":"<ul> <li>No scopal relation</li> <li>Three aliens are holding two flags.</li> <li>Both Np's are interpreted individually and connected to each other</li> </ul>"},{"location":"KB/Collectivity%2C%20Distributivity%2C%20and%20the%20Interpretation%20of%20Plural%20Numerical%20Expressions%20in%20Child%20and%20Adult%20Language/","title":"Collectivity, Distributivity, and the Interpretation of Plural Numerical Expressions in Child and Adult Language","text":""},{"location":"KB/Collectivity%2C%20Distributivity%2C%20and%20the%20Interpretation%20of%20Plural%20Numerical%20Expressions%20in%20Child%20and%20Adult%20Language/#collectivity-distributivity-and-the-interpretation-of-plural-numerical-expressions-in-child-and-adult-language","title":"Collectivity, Distributivity, and the Interpretation of Plural Numerical Expressions in Child and Adult Language","text":"<ul> <li>Kristen Syrett, Ph.D. and Rutgers, The State University of New Jersey, Linguistics, New Brunswick, United States</li> <li>Julien Musolino : Rutgers, The State University of New Jersey, Psychology, Piscataway, United States</li> </ul>"},{"location":"KB/Collectivity%2C%20Distributivity%2C%20and%20the%20Interpretation%20of%20Plural%20Numerical%20Expressions%20in%20Child%20and%20Adult%20Language/#intro","title":"Intro","text":"<ul> <li>Sentences containing plural numerical expressions (e.g., two boys) can give rise to two interpretations (collective and distributive), arising from the fact that their representation admits of a part-whole structure</li> <li>designed to explore children\u2019s understanding of this distinction and its implications for the acquisition of linguistic expressions with number words.</li> <li>preschoolers access both interpretations, indicating that they have the requisite linguistic and conceptual machinery to generate the corresponding representations.</li> <li>hift their interpretation in response to structural and lexical manipulations.</li> <li>unlike adults, they are drawn to the distributive interpretation, and are not yet aware of the lexical semantics of each and together, which should favor one or another interpretation.</li> <li>Here, we take a different approach, and use numerically quantified expressions to study how children acquire a fundamental semantic property shared by a range of plurality-denoting expressions</li> </ul>"},{"location":"KB/Collectivity%2C%20Distributivity%2C%20and%20the%20Interpretation%20of%20Plural%20Numerical%20Expressions%20in%20Child%20and%20Adult%20Language/#findings","title":"Findings","text":"<ul> <li>Our findings demonstrate that the ability to generate collective and distributive interpretations of sentences such as (1) is part of the semantic repertoire of children as young as three (Experiment 1). However, we also uncover intriguing differences in the preferences preschoolers and adults have for resolving the collective/distributive ambiguity: whereas adults strongly prefer the collective interpretation, preschoolers prefer the distributive one (Experiment 2).</li> <li>Following analyses by Link (1983, 1987) and others more recently, we will assume that these sentences are truly ambiguous, and not merely underspecified, and that the source of the ambiguity in our target sentences is the VP predicate. Here, we adopt a default semantics approach in order to illustrate how two different interpretations may be generated.</li> <li>When the predicate in our example sentence is applied to the individuals, the derived reading is the distributive one</li> <li>When the predicate is applied to the group, however, a collective reading is derived, and the extension is an atomic joint \u2018car pushing\u2019 event in which the boys collectively push the car.</li> <li>Beginning with the latter, Musolino (2009) was primarily concerned with the range of readings arising from the interaction of two numerically quantified expressions in so-called relational plural sentences such as (3).</li> </ul>"},{"location":"KB/Collectivity%2C%20Distributivity%2C%20and%20the%20Interpretation%20of%20Plural%20Numerical%20Expressions%20in%20Child%20and%20Adult%20Language/#judgment-task-with-ambiguous-sentences","title":"Judgment Task with Ambiguous Sentences","text":"<ul> <li>The results demonstrate that both children and adults were able to access both the collective and distributive interpretations of the target sentences. While there was a significant main effect of age (p= .02), there was no main effect of context (p=.75) and no interaction between age group and context (p=.26).</li> <li>This difference stems from the fact that while four-year-olds were near ceiling in their acceptance of the sentences in the distributive context, adults\u2019 acceptance rates were slightly suppressed.</li> </ul>"},{"location":"KB/Collectivity%2C%20Distributivity%2C%20and%20the%20Interpretation%20of%20Plural%20Numerical%20Expressions%20in%20Child%20and%20Adult%20Language/#ambiguous-sentences-that-yield-either-interpretation","title":"Ambiguous Sentences That Yield Either Interpretation.","text":"<ul> <li>As the results demonstrate, adults overwhelmingly preferred the collective version of the event</li> <li>In this experiment, we found that while adults robustly prefer the collective context as a match for the ambiguous target sentences, children display a slight preference in the opposite direction, leaning towards preference for the distributive context.</li> <li>structural manipulation of passivization will lead participants to prefer the collective context.</li> <li>As predicted, adults consistently accepted the passive test sentences in the collective context, but largely rejected them in the distributive context. Most children also followed this pattern, although the difference between acceptances in the two contexts was not as striking for children as it was for the adults.</li> <li>whether participants can recruit lexical semantic information provided by individual words to disambiguate the target sentence and assign either a collective or distributive interpretation, depending on the lexical item.</li> <li>As predicted, adults were guided by the presence of the additional lexical item in their interpretation of these sentences, accepting the test sentences with each in the distributive context, but rejecting them in the collective context</li> <li>In place of the ambiguous sentences, children heard sentences with a post-verbal together (</li> <li>Interestingly, despite children\u2019s acceptance of the together sentences in both contexts of the judgment task of Experiment 4, children appeared to be aware of the collectivizing force of together in the current preference task.</li> </ul>"},{"location":"KB/Color%20Compositing/","title":"Color Compositing","text":""},{"location":"KB/Color%20Compositing/#color-compositing","title":"Color Compositing","text":"<ul> <li> \\[C_{i}= c_{i}+ (1-o_{i})C_{i-1}\\] </li> <li>where</li> <li> \\[c_{i}= o_{i}c_{i}'\\] </li> <li></li> <li>First is same as Marching Cubes</li> <li>$$I(p) = \\begin{cases*}</li> </ul> <p>f(\\sigma)&amp; \\(\\exists t \\in [0,T], s(t) = \\sigma\\) \\</p> <p>I_{o}&amp;otherwise</p> <p>\\end{cases*}$$</p> <ul> <li>Higher pixel accurate quality</li> </ul>"},{"location":"KB/Color%20Space%20Transformations/","title":"Color Space Transformations","text":""},{"location":"KB/Color%20Space%20Transformations/#color-space-transformations","title":"Color Space Transformations","text":"<ul> <li>Image data is encoded into 3 stacked matrices, each of size height\u00d7width. These matrices represent pixel values for an individual RGB color value</li> <li>Lighting biases are amongst the most frequently occurring challenges to image recognition problems</li> <li>A quick fix to overly bright or dark images is to loop through the images and decrease or increase the pixel values by a constant value.</li> <li>Another quick color space manipulation is to splice out individual RGB color matrices.</li> <li>Another transformation consists of restricting pixel values to a certain min or max value.</li> <li>Similar to geometric transformations, a disadvantage of color space transformations is increased memory, transformation costs, and training time.</li> <li>Additionally, color transformations may discard important color information and thus are not always a label-preserving transformation.</li> <li>For example, when decreasing the pixel values of an image to simulate a darker environment, it may become impossible to see the objects in the image.</li> <li>Digital image data is usually encoded as a tensor of the dimension (height \u00d7 width \u00d7 color channels)</li> <li>Performing augmentations in the color channels space is another strategy that is very practical to implement.</li> <li>Very simple color augmentations include isolating a single color channel such as R, G, or B.</li> <li>An image can be quickly converted into its representation in one color channel by isolating that matrix and adding 2 zero matrices from the other color channels. Additionally, the RGB values can be easily manipulated with simple matrix operations to increase or decrease the brightness of the image.</li> <li>More advanced color augmentations come from deriving a color histogram describing the image</li> </ul>"},{"location":"KB/Color%20Spaces/","title":"Color Spaces","text":""},{"location":"KB/Color%20Spaces/#color-spaces","title":"Color Spaces","text":"<ul> <li>Divide Oriented</li> <li>Intuitive Color spaces</li> </ul>"},{"location":"KB/ColorMap/","title":"ColorMap","text":""},{"location":"KB/ColorMap/#colormap","title":"ColorMap","text":"<ul> <li>Color Spaces</li> <li></li> </ul>"},{"location":"KB/CommonCrawl/","title":"CommonCrawl","text":""},{"location":"KB/CommonCrawl/#commoncrawl","title":"CommonCrawl","text":""},{"location":"KB/Comparing%20Data%20Augmentation%20Strategies%20for%20Deep%20Image%20Classification/","title":"Comparing Data Augmentation Strategies for Deep Image Classification","text":""},{"location":"KB/Comparing%20Data%20Augmentation%20Strategies%20for%20Deep%20Image%20Classification/#comparing-data-augmentation-strategies-for-deep-image-classification","title":"Comparing Data Augmentation Strategies for Deep Image Classification","text":"<ul> <li>Sarah O'Gara and Kevin McGuinness</li> </ul>"},{"location":"KB/Comparing%20Data%20Augmentation%20Strategies%20for%20Deep%20Image%20Classification/#summary","title":"Summary","text":"<ul> <li>Inject augmentation around 30 epochs</li> <li>Use learning rate decay</li> <li>Random Erasing is useful</li> <li>Use [Adam] + SGD </li> </ul>"},{"location":"KB/Comparing%20Data%20Augmentation%20Strategies%20for%20Deep%20Image%20Classification/#abstract","title":"Abstract","text":"<ul> <li>More complex augmentation methods have recently been developed, but it is still unclear which techniques are most effective, and at what stage of the learning process they should be introduced.</li> <li>The most accurate results in all experiments are achieved using random erasing due to its ability to simulate occlusion</li> <li>reducing the number of training examples significantly increases the importance of augmentation</li> <li>improvements in generalization from augmentation do not appear to be only as a result of augmentation preventing overfitting</li> <li>learning curriculum that injects augmentation after the initial learning phase has passed is more effective than the standard practice of using augmentation throughout, and that injection too late also reduces accuracy</li> <li>We find that careful augmentation can improve accuracy by +2.83% to 95.85% using a ResNet model on CIFAR-10 with more dramatic improvements seen when there are fewer training examples</li> </ul>"},{"location":"KB/Comparing%20Data%20Augmentation%20Strategies%20for%20Deep%20Image%20Classification/#model-and-optimizer","title":"Model and Optimizer","text":"<ul> <li>ResNet</li> <li>[He et al., 2015] presents an adaption of the model (ResNet-56) for use with 32\u00d732 images that obtained an error rate of 6.97% on CIFAR-10, which we adopt in our experiments</li> <li>SGD with Nestrov momentum</li> <li>Although there are more sophisticated first order optimizers (e.g. Adam [Kingma and Ba, 2015]) that consistently improve the loss faster in the initial epochs, SGD has been observed to reach a local minima with lower overall loss and better generalization properties [Ruder, 2016]</li> </ul>"},{"location":"KB/Comparing%20Data%20Augmentation%20Strategies%20for%20Deep%20Image%20Classification/#datasets","title":"Datasets","text":""},{"location":"KB/Comparing%20Data%20Augmentation%20Strategies%20for%20Deep%20Image%20Classification/#cifar-10","title":"CIFAR-10","text":"<ul> <li>randomly sample the dataset to create a 200 samples per class and 1,000 samples per class dataset, reducing the training examples available to 4% and 20% of the original dataset</li> <li>The effects of overfitting and model generalization as noted in [Hussain et al., 2018, Shijie et al., 2017] are more pronounced with data scarcity</li> <li>Skew Tilt</li> <li>Shear</li> <li>Random Distortion</li> <li>Gaussian Distortion</li> <li>We introduce augmentation on epochs 30, 60, and 90 of the baseline model and continue training until epoch 163 to discover the optimal time to introduce augmentation. Epochs 30, 60, and 90 represent three distinct stages in the training process: initial loss rate stabilising, loss rate stagnate before learning rate decrease, and loss rate stagnate after learning rate decrease.</li> </ul>"},{"location":"KB/Comparing%20Data%20Augmentation%20Strategies%20for%20Deep%20Image%20Classification/#experiments","title":"Experiments","text":"<ul> <li>The range of learning rates that provide a stable convergence reduces as batch size increases</li> <li>In the most extreme case, we reduce the training set to 4% of the original dataset, meaning a batch size of 128 would likely degrade performance</li> <li>Large batches tend to converge to sharp minimizers leading to poor generalization due to the numerous large eigenvalues in the Hessian on convergence</li> <li>Small batches, on the other hand, tend to converge to flat minimizers, which have smaller Hessian eigenvalues</li> <li>They generate more noise in gradient calculations, decreasing the chance of the gradient dropping into a sharp local minima</li> <li>Based on these observations, we train the small and medium datasets using three learning rate strategies: 1) the original strategy from [He et al., 2015], 2) using a batch size of 128 with no learning rate schedule, and 3) using a batch size of 8 with original learning rate schedule.</li> </ul>"},{"location":"KB/Comparing%20Data%20Augmentation%20Strategies%20for%20Deep%20Image%20Classification/#results-discussion","title":"Results &amp; Discussion","text":""},{"location":"KB/Comparing%20Data%20Augmentation%20Strategies%20for%20Deep%20Image%20Classification/#single-augmentations","title":"Single Augmentations","text":"<ul> <li>Random erasing shows the best improvement in accuracy of +1.5%</li> <li>Both distortion augmentations obtain worse or similar results to the baseline</li> <li>The complexity of the augmentation effects the overall training time. Traditional, more simplistic augmentations require little processing time, leading to increases in training time of \u223c 3.5 hours. KB/Gaussian Distortion.md sees the most significant increase in training time of 665%</li> <li>We apply each augmentation separately, leading to the dataset increasing from 50k training images to 250k. This leads to the most accurate result seen throughout all experiments of 95.85%</li> <li>Our method of applying several single augmentations produces better generalization properties</li> </ul>"},{"location":"KB/Comparing%20Data%20Augmentation%20Strategies%20for%20Deep%20Image%20Classification/#varying-augmentation-injection-epoch","title":"Varying Augmentation Injection Epoch","text":"<ul> <li>epoch 30 is the optimal time to introduce augmentation</li> <li>y injecting augmentation on the 30th epoch, the model combats the effects of overfitting better with increases in accuracy from +0.05% up to +0.76%</li> <li>Epoch 30 is the point in the training process when the reduction in loss rate begins to decrease drastically, i.e. the model falls into a local minima point</li> <li>The slight improvements in accuracy over the baseline result for introduction at epoch 90 support this conclusion</li> <li>The model has already overfit the training data and can no longer benefit from the augmentation's generalization properties.</li> <li>Epoch 60 presents a more interesting point in the training process. The form of augmentation appears to dictate whether the model will have better generalization properties than training with augmentation from scratch but will always be worse than injection at epoch 30</li> </ul>"},{"location":"KB/Comparing%20Data%20Augmentation%20Strategies%20for%20Deep%20Image%20Classification/#varying-sample-size","title":"Varying Sample Size","text":"<ul> <li>For the small dataset, by decreasing the batch size from 128 to 8, the validation accuracy is shown to improve by +31.45% using random erasing (74.46%) when compared to the baseline (43.01%)</li> <li>augmentation is most effective in training when data is scarce</li> <li>overfitting, as measured by high accuracy on the training set, in many of the augmentation results is more severe than for the baseline</li> <li>his would contradict current assumptions that augmentation improves generalization by preventing overfitting in the case of all NNs</li> <li>In many of these cases where augmentation has proven to prevent overfitting the sample size for each class is large</li> <li>generalization of the model is better in the presence of augmentation</li> <li>With smaller datasets using augmentation increases the models ability to learn certain features present in the training set as augmentation can only alter the data already available, i.e. the model will see similar images twice as much so is more likely to overfit.</li> <li>For the medium dataset, the best accuracy is achieved by random erasing trained with a batch size of 8 at 87.45%, which is an improvement of +6.3% over the baseline.</li> <li>The importance of the learning rate adjustment schedule is apparent with the accuracy decreasing for each model when not applied</li> <li>Augmentation does reduce overfitting with the most significant decrease occurring for the small batch size</li> <li>At this scale, augmentation has similar effects on accuracy as seen in the full dataset</li> <li>When the model has large volumes of training data available, augmentation only slightly increases the generalization capabilities of the network as a large amount of variance already exists</li> </ul>"},{"location":"KB/Comparing%20Data%20Augmentation%20Strategies%20for%20Deep%20Image%20Classification/#conclusion","title":"Conclusion","text":"<ul> <li>The initial augmentation gives rise to the most significant increase in training time with any additional augmentations adding little overhead</li> <li>processing time required to apply said augmentation to the dataset, which must be considered when choosing a form of augmentation to apply</li> <li>combining multiple single augmentations with the original dataset is the most effective augmentation strategy with an increase in accuracy of +2.36% to 95.85%</li> <li>Random distortion and KB/Gaussian Distortion.md are the worst forms of augmentation tested leading to changes in accuracy of -0.15% and +0.05%, respectively</li> <li>This is due to the augmented images not representing the original class and highlights the importance of the choice of augmentation</li> <li>The most effective form of single augmentation is found to be random erasing with an increase in accuracy of +1.5%. This is due to its ability to combat the effects of occlusion, and is similar to preventing co-adaption through the use of KB/Dropout.md.</li> <li>An interesting avenue to explore is the generalization and overfitting properties of augmentation for data scarcity</li> <li>Validation accuracy is seen to improve with augmentation, with the most significant improvement of +31.45% for random erasing, indicating better generalization capabilities.</li> <li>However, the model also appears to overfit the training data more</li> <li>Exploring the interaction of augmentation with more advanced optimizers such as the Adam optimizer, could lead to further improvements in accuracy and training times</li> <li>generalization gap between SGD and Adam can be reduced by switching from Adam to SGD during the training process</li> <li>During the switching process the learning rate for SGD is calculated as noted in [Keskar and Socher, 2017] and must be switched at the optimal time to ensure better generalization properties.</li> <li>Building on this approach, the optimizer switching approach could be combined with data augmentation potentially yielding improvements in accuracy.</li> <li>Injecting augmentation at epoch 30 yielded the best improvements in accuracy for single augmentations, indicating a learning curriculum is most effective for augmentation</li> <li>Late injection of augmentation improves the generalization capabilities of the network similar to the optimizer switching method of [Keskar and Socher, 2017].</li> </ul>"},{"location":"KB/Comparing%20Data%20Augmentation%20Strategies%20for%20Deep%20Image%20Classification/#images","title":"Images","text":""},{"location":"KB/Complete%20AI%20Pipeline/","title":"Complete AI Pipeline","text":"","tags":["ai"]},{"location":"KB/Complete%20AI%20Pipeline/#complete-ai-pipeline","title":"Complete AI Pipeline","text":"","tags":["ai"]},{"location":"KB/Complete%20AI%20Pipeline/#planning","title":"Planning","text":"<ul> <li>Data Availability</li> <li>Applicability</li> <li>Legal Constraints</li> <li>Robustness</li> <li>Scalability</li> <li>Explainability</li> <li>Availability of Resources</li> </ul>","tags":["ai"]},{"location":"KB/Complete%20AI%20Pipeline/#data-collection","title":"Data Collection","text":"","tags":["ai"]},{"location":"KB/Complete%20AI%20Pipeline/#data-standards","title":"Data Standards","text":"<ul> <li>Healthcare<ul> <li>DICOM</li> <li>HL7</li> <li>FHIR</li> </ul> </li> </ul>","tags":["ai"]},{"location":"KB/Complete%20AI%20Pipeline/#data-formats","title":"Data Formats","text":"<ul> <li>Healthcare    <ul> <li>Next generation sequencing<ul> <li>Raw sequencing data<ul> <li>FASTQ<ul> <li>BioPython</li> </ul> </li> </ul> </li> <li>Aligned Reads<ul> <li>SAM/BAM<ul> <li>pysam</li> </ul> </li> </ul> </li> <li>Variant Calls<ul> <li>VCF<ul> <li>pyVCF</li> </ul> </li> </ul> </li> </ul> </li> <li>Mass spectrometry<ul> <li>mzML/mzXML<ul> <li>pyteomics</li> </ul> </li> <li>MGF<ul> <li>pyOpenMS</li> </ul> </li> </ul> </li> <li>MRI<ul> <li>DICOM</li> </ul> </li> <li>CT<ul> <li>DICOM</li> </ul> </li> <li>Ultrasound<ul> <li>DICOM</li> <li>SCU</li> <li>ACR-NEMA</li> </ul> </li> <li>Functional (pre)clinical studies</li> <li>E-health and m-health technology</li> <li>Wearable Device monitoring</li> <li>Unstructured vocal information</li> <li>Unstructured textual information</li> </ul> </li> </ul>","tags":["ai"]},{"location":"KB/Complete%20AI%20Pipeline/#data-processing-and-transformation","title":"Data Processing and Transformation","text":"","tags":["ai"]},{"location":"KB/Complete%20AI%20Pipeline/#aws-services","title":"AWS Services","text":"<ul> <li>Glue<ul> <li>Fully managed ETL (Extract, Transform, Load) service for preparing and loading data.</li> </ul> </li> </ul>","tags":["ai"]},{"location":"KB/Complete%20AI%20Pipeline/#data-annotation","title":"Data Annotation","text":"","tags":["ai"]},{"location":"KB/Complete%20AI%20Pipeline/#data-integration","title":"Data Integration","text":"","tags":["ai"]},{"location":"KB/Complete%20AI%20Pipeline/#data-storage","title":"Data Storage","text":"","tags":["ai"]},{"location":"KB/Complete%20AI%20Pipeline/#aws-services_1","title":"AWS Services","text":"<ul> <li> <p>S3</p> <ul> <li>Object storage service, suitable for storing and retrieving any amount of data at any time.</li> </ul> </li> <li> <p>Dynamo DB</p> <ul> <li>Modern</li> <li>High activity</li> <li>High Velocity data (sensors and stuff)</li> <li>Structured/unstructured</li> <li>multi data, multi cloud</li> <li>NoSQL database service designed for high-performance applications that require seamless scalability.</li> </ul> </li> <li> <p>RDS</p> <ul> <li>Tables</li> <li>low velocity/activity</li> <li>Harder to modify</li> <li>Relational Database Service, supports multiple database engines, managing backups, and software patching.</li> </ul> </li> <li> <p>HealthImaging</p> </li> <li> <p>HealthLake</p> <ul> <li>A HIPAA-eligible service for storing, transforming, and analyzing healthcare data.</li> </ul> </li> <li> <p>Lake Formation</p> <ul> <li>Simplifies the process of setting up, securing, and managing a data lake.</li> </ul> </li> </ul>","tags":["ai"]},{"location":"KB/Complete%20AI%20Pipeline/#data-analysis-and-querying","title":"Data Analysis and Querying","text":"","tags":["ai"]},{"location":"KB/Complete%20AI%20Pipeline/#aws-services_2","title":"AWS Services","text":"<ul> <li>Redshift<ul> <li>Fully managed data warehouse service, designed for high-performance analysis using SQL queries.</li> </ul> </li> </ul>","tags":["ai"]},{"location":"KB/Complete%20AI%20Pipeline/#streaming-data-processing","title":"Streaming Data Processing","text":"","tags":["ai"]},{"location":"KB/Complete%20AI%20Pipeline/#aws-services_3","title":"AWS Services","text":"<ul> <li>Kinesis<ul> <li>Streaming data service that enables real-time processing of large data streams.</li> </ul> </li> </ul>","tags":["ai"]},{"location":"KB/Complete%20AI%20Pipeline/#model-development","title":"Model Development","text":"","tags":["ai"]},{"location":"KB/Complete%20AI%20Pipeline/#model-architectures","title":"Model Architectures","text":"","tags":["ai"]},{"location":"KB/Complete%20AI%20Pipeline/#model-metrics","title":"Model Metrics","text":"","tags":["ai"]},{"location":"KB/Complete%20AI%20Pipeline/#cicd","title":"CI/CD","text":"","tags":["ai"]},{"location":"KB/Complete%20AI%20Pipeline/#tracking-experiments-metadata-features-code-changes","title":"Tracking Experiments, Metadata, Features, Code Changes","text":"<ul> <li>MLFlow</li> </ul>","tags":["ai"]},{"location":"KB/Complete%20AI%20Pipeline/#state-of-the-models","title":"State of the Models","text":"<ul> <li>Staging</li> <li>Production</li> <li>Archived</li> </ul>","tags":["ai"]},{"location":"KB/Complete%20AI%20Pipeline/#model-evaluation","title":"Model Evaluation","text":"","tags":["ai"]},{"location":"KB/Complete%20AI%20Pipeline/#machine-learning","title":"Machine Learning","text":"","tags":["ai"]},{"location":"KB/Complete%20AI%20Pipeline/#aws-services_4","title":"AWS Services","text":"<ul> <li>SageMaker<ul> <li>Machine learning service that helps build, train, and deploy machine learning models at scale.</li> </ul> </li> </ul>","tags":["ai"]},{"location":"KB/Complete%20AI%20Pipeline/#model-deployment","title":"Model Deployment","text":"","tags":["ai"]},{"location":"KB/Complete%20AI%20Pipeline/#real-time-prediction","title":"Real Time Prediction","text":"","tags":["ai"]},{"location":"KB/Complete%20AI%20Pipeline/#monitoring-and-maintenance","title":"Monitoring and Maintenance","text":"","tags":["ai"]},{"location":"KB/Complete%20AI%20Pipeline/#interpretability-and-explainability","title":"Interpretability and Explainability","text":"","tags":["ai"]},{"location":"KB/Complete%20AI%20Pipeline/#regulatory-compliance","title":"Regulatory Compliance","text":"<ul> <li>HIPAA</li> </ul>","tags":["ai"]},{"location":"KB/Complete%20AI%20Pipeline/#collaboration","title":"Collaboration","text":"","tags":["ai"]},{"location":"KB/Complex%20Geometry/","title":"Challenge of Complex Geometry","text":""},{"location":"KB/Complex%20Geometry/#challenge-of-complex-geometry","title":"Challenge of Complex Geometry","text":"<ul> <li>Manifold</li> </ul>"},{"location":"KB/Compliant%20Robot/","title":"Compliant Robot","text":""},{"location":"KB/Compliant%20Robot/#compliant-robot","title":"Compliant Robot","text":"<ul> <li>A robot that performs tasks, with respect to external forces, by modifying its motions in a manner that minimizes those forces. The indicated or allowed motion is accomplished through lateral (horizontal), axial (vertical) or rotational compliance.</li> </ul>"},{"location":"KB/Composing%20shallow%20neural%20networks%20to%20get%20deep%20networks/","title":"Composing shallow neural networks to get deep","text":""},{"location":"KB/Composing%20shallow%20neural%20networks%20to%20get%20deep%20networks/#composing-shallow-neural-networks-to-get-deep-networks","title":"Composing shallow neural networks to get deep networks","text":"<ul> <li>Consider two networks with 3 hidden units. Input \\(x, x'\\) , output \\(y, y'\\)<ul> <li></li> <li></li> </ul> </li> <li></li> <li>If we use a Relu, this also is a family of piecewise linear functions. <ul> <li>the number of linear regions is potentially greater than for a shallow network with six hidden units</li> </ul> </li> <li>This maps three ranges of \\(x\\) to same range of \\(y \\in [-1,1]\\) -&gt; 9 linear regions</li> </ul>"},{"location":"KB/Composing%20shallow%20neural%20networks%20to%20get%20deep%20networks/#deep-neural-networks","title":"Deep neural networks","text":"<ul> <li>(Input -&gt; Shallow network -&gt; second network) is a special case of neural network</li> <li>Output of the first network (f1) \\(y\\) is a linear combination of the activations at the hidden units. And the first operations of the second network (f2) are linear in the output of the first network. Hence f2(f1) = linear function</li> <li><ul> <li> \\[\\psi_{10}= \\theta'_{10}+ \\theta'_{11}\\phi_{0},\\psi_{11}= \\theta'_{11}\\phi_{1}, \\psi_{12}=\\theta'_{11}\\phi_{2}...\\] </li> <li>which is a network with 2 hidden layers.</li> <li>So a network with two layers can represent a family of functions formed by composing those networks.</li> </ul> </li> <li></li> </ul>"},{"location":"KB/Composing%20shallow%20neural%20networks%20to%20get%20deep%20networks/#folding-space","title":"Folding space","text":"<ul> <li>One way to think about composing networks is by thinking of it as folding space and then applying an Activation Functions. </li> <li><ul> <li></li> </ul> </li> </ul>"},{"location":"KB/Comprehensibility/","title":"Comprehensibility","text":""},{"location":"KB/Comprehensibility/#comprehensibility","title":"Comprehensibility","text":"<ul> <li>ability of a learning algorithm to represent its learned knowledge in a human understandable fashion</li> <li>An understandable explanation must be created by a machine in a given time (e.g., one hour or one day) and can be comprehended by a user, who need not to be an expert, but has an educational background</li> <li>The user keeps asking a finite number of questions of the machine until he/she can no longer ask why or how because he/she has a satisfactory answer; we say he/she has comprehended.</li> <li>This is the relationship between cause and effect; it is not a synonym for causability</li> </ul>"},{"location":"KB/Computational%20Graph/","title":"Computational Graph","text":""},{"location":"KB/Computational%20Graph/#computational-graph","title":"Computational Graph","text":"<ul> <li>patterns in backward flow<ul> <li>add gate: gradient distributor</li> <li>max gate: gradient router</li> <li>mul gate: gradient switcher</li> <li>branches: sum up gradients</li> </ul> </li> <li>pros<ul> <li>intuitive interpretation of gradient</li> <li>easily define new nodes using forward/backward pattern (i.e. only these two functions must be implemented)</li> <li>any complex learning architecture can be composed of atomic nodes (node composition or factorization)</li> <li>no need to compute manually complex gradients</li> <li>loss function can be seen as extra nodes in the end of the graph</li> </ul> </li> </ul>"},{"location":"KB/Conceptual%20Parsing/","title":"Conceptual Parsing","text":""},{"location":"KB/Conceptual%20Parsing/#conceptual-parsing","title":"Conceptual Parsing","text":"<ul> <li>Syntactic Analysis and Semantic Analysis knowledge are combined into a single interpretation system that is driven by the semantic knowledge</li> </ul>"},{"location":"KB/Concurrency/","title":"Concurrency","text":""},{"location":"KB/Concurrency/#concurrency","title":"Concurrency","text":"<ul> <li>The number of tasks that can be executed in parallel is the degree of concurrency of a decomposition.</li> <li>always equal to the number of leaves in the tree</li> <li>Both the maximum and the average degrees of concurrency usually increase as the Parallel Granularity of tasks becomes smaller (finer)</li> <li> \\[CriticalPath = \\frac{\\text{Total amount of work}}{\\text{Critical path length}}\\] </li> <li></li> </ul>"},{"location":"KB/Concussion/","title":"Concussion","text":""},{"location":"KB/Concussion/#concussion","title":"Concussion","text":"<ul> <li>A type of mild traumatic brain injury resulting from a blow or hit to the head that causes the brain to move rapidly back and forth inside the skull.</li> </ul>"},{"location":"KB/Conditional%20GAN/","title":"Conditional GAN","text":""},{"location":"KB/Conditional%20GAN/#conditional-gan","title":"Conditional GAN","text":"<ul> <li>Image2Image, Face Aging, Text to Image</li> <li>Generate images with certain extra conditions or attributes</li> <li>The Generator and Discriminator both receive some additional conditioning input information. This could be the class of the current image or some other property.</li> <li>add an additional input layer with values of one-hot-encoded image labels</li> <li>Adding a vector of features controls the output and guide Generator figure out what to do.</li> <li>Such a vector of features should derive from a image which encode the class(like an image of a woman or a man if we are trying to create faces of imaginary actors) or a set of specific characteristics we expect from the image (in case of imaginary actors, it could be the type of hair, eyes or complexion).</li> <li>Whereas conditional generation uses labels during training, controllable generation focuses on controlling the features that you want in the output examples.</li> <li>We can incorporate the information into the images that will be learned and also into the Z input, which is not completely random anymore.</li> <li>This can be done by adjusting the input noise vector z that is fed into the generator after it has been trained.</li> <li>We can use the same DCGANs and imposed a condition on both Generator\u2019s and Discriminator\u2019s inputs. The condition should be in the form of a one-hot vector version of the digit. This is associated with the image to Generator or Discriminator as real or fake.</li> <li>Typically this is done with a one-hot vector, meaning there are zeros in every position except for the position of the class we want to generate</li> </ul>"},{"location":"KB/Conditional%20GAN/#architecture","title":"Architecture","text":"<ul> <li>GAN Z Space</li> </ul>"},{"location":"KB/Conditional%20GAN/#the-discriminators-network","title":"The Discriminator\u2019s network","text":"<ul> <li>Discriminator\u2019s evaluation is done not only on the similarity between fake data and original data but also on the correspondence of the fake data image to its input label (or features)</li> <li>Same as [DCGAN] except one hot vector for conditioning</li> </ul>"},{"location":"KB/Conditional%20GAN/#the-generators-network","title":"The Generator\u2019s network","text":"<ul> <li>To create an image that looks as \u201creal\u201d as possible to fool the Discriminator.</li> <li>Same as [DCGAN] except one hot vector.</li> <li></li> </ul>"},{"location":"KB/Conditional%20GAN/#loss-functions","title":"Loss functions","text":"<ul> <li>We need to calculate two losses for the Discriminator. The sum of the \u201cfake\u201d image and \u201creal\u201d image loss is the overall Discriminator loss. So the loss function of the Discriminator is aiming at minimizing the error of predicting real images coming from the dataset and fake images coming from the Generator given their one-hot labels.</li> </ul>"},{"location":"KB/Conditional%20GAN/#gen","title":"Gen","text":"<ul> <li>The loss function of the Generator minimizes the correct prediction of the Discriminator on fake images conditioned on the specified one-hot labels.</li> <li> \\[\\mathcal{L}^{(G)}(\\theta^{(G)}, \\theta^{(D)}) = - \\mathbb{E}_{z} log \\mathcal{D} (\\mathcal{G} (z|y\u2019))\\] </li> </ul>"},{"location":"KB/Conditional%20GAN/#disc","title":"Disc","text":"<ul> <li>has to correctly label real images which are coming from training data set as real.</li> <li>has to correctly label generated images which are coming from Generator as fake.</li> <li> \\[ \\mathcal{L}^{(D)}(\\theta^{(G)}, \\theta^{(D)})= - \\mathbb{E}_{x \\sim p_{data}}log \\mathcal{D}(x|y) - \\mathbb{E}_{z} log (1- \\mathcal{D}(\\mathcal{G}(z|y')))\\] </li> </ul>"},{"location":"KB/Conditional%20GAN/#training","title":"Training","text":"<ul> <li>The Discriminator is trained using real and fake data and generated </li> <li>After the Discriminator has been trained, both models are trained together.</li> <li>First, the Generator creates some new examples.</li> <li>The Discriminator\u2019s weights are frozen, but its gradients are used in the Generator model so that the Generator can update its weights.</li> </ul>"},{"location":"KB/Conditional%20GAN/#training-flow","title":"Training Flow","text":"<ul> <li>For the Disc </li> <li>For the Gen </li> </ul>"},{"location":"KB/Conditional%20GAN/#challenges-with-conditional-generation","title":"Challenges with Conditional generation","text":"<ul> <li>Not strictly unsupervised. Needs labels</li> <li>With a conditional GAN, you get a random example from the class you specify</li> <li>With conditional generation, you have to train the GAN with labeled datasets.</li> <li>Feature Correlationa</li> <li>Z-Space Entanglement</li> <li>Classifier Gradients</li> </ul>"},{"location":"KB/Conditional%20GAN/#dcgan-vs-cgan","title":"DCGAN vs CGAN","text":"DCGAN CGAN Output features are not controllable Output features can be controlled Unsupervised Semi-Supervised Discriminator does not receive labels Discriminator requires labels Discriminator evaluates similarity between input and target images Discriminator considers input and target images and their respective labels"},{"location":"KB/Conditional%20Independence/","title":"Conditional Independence","text":""},{"location":"KB/Conditional%20Independence/#conditional-independence","title":"Conditional Independence","text":"<ul> <li>A Naive Bayes classifier assumes that the attribute values are independent of each other given the class. Normally distributed: many statistical methods assume that data is normally distributed.</li> </ul>"},{"location":"KB/Conductance/","title":"Conductance","text":""},{"location":"KB/Conductance/#conductance","title":"Conductance","text":"<ul> <li>@dhamdhereHowImportantNeuron2018</li> <li>Kedar Dhamdhere, Mukund Sundararajan, Qiqi Yan</li> </ul>"},{"location":"KB/Conductance/#summary","title":"Summary","text":"<ul> <li>This paper introduces the concept of conductance as a way to understand the importance of hidden units in deep networks. Conductance is defined as the flow of Integrated Gradients' attribution via a hidden unit, and is used to understand the importance of a hidden unit to the prediction for a specific input or over a set of inputs. The effectiveness of conductance is evaluated in multiple ways, including theoretical properties, ablation studies, and a feature selection task using the Inception network over ImageNet data and a sentiment analysis network over reviews. The properties of conductance include completeness, linearity and insensitivity to variations in inputs or hidden unit values. The paper also discusses the issue of saturation in neural networks, where the gradient of the output with respect to the input can be near-zero, and how conductance addresses this issue. The authors also compare conductance with other methods of understanding hidden unit importance and find it to be more intuitive and accurate.</li> </ul>"},{"location":"KB/Conductance/#abstract","title":"Abstract","text":"<ul> <li>We introduce the notion of conductance to extend the notion of attribution to the understanding the importance of hidden units</li> <li>conductance of a hidden unit of a deep network is the flow of attribution via this hidden unit</li> <li>conductance to understand the importance of a hidden unit to the prediction for a specific input, or over a set of inputs</li> <li>We evaluate the effectiveness of conductance in multiple ways, including theoretical properties, ablation studies, and a feature selection task</li> <li>Inception network over ImageNet data, and a sentiment analysis network over reviews</li> <li>Informally, the conductance of a hidden unit of a deep network is the flow of Integrated Gradients' attribution via this hidden unit</li> <li>The key idea behind conductance is to decompose the computation of Integrated Gradients via the chain rule</li> </ul>"},{"location":"KB/Conductance/#conductance_1","title":"Conductance","text":"<ul> <li>Integrated Gradients produces attributions for base features</li> <li>There is a natural way to 'lift' these attributions to a neuron in a hidden layer. Consider a specific neuron y in a hidden layer of a network</li> <li>$$</li> </ul> <p>F:R^{n} \\rightarrow [0,1] $$  represents a deep network. - \\(x \\in R^{n}\\) is input, \\(x' \\in R^{n}\\) is baseline input - Integrated Gradients is path integral of gradient along straightline path from baseline \\(x'\\) to input \\(x\\). The function F varies from a near zero value for the informationless baseline to its final value. The gradients of F with respect to the image pixels explain each step of the variation in the value of F - The integration (sum) over the gradients cumulates these micro explanations and accounts for the net difference between the baseline prediction score (near zero) and the prediction value at the input x. - $$ IG_{i}(x) ::== (x_{i}- x_{i}') \\int_{\\alpha=0}^{1} \\frac{\\partial F(x' + \\alpha(x-x'))}{\\partial x_{i}}d \\alpha $$ where \\(\\frac{\\partial F(x)}{\\partial x_{i}}\\) is grad of F along i^th dimension at x - Conductance of neuron y for attribution to input variable i is $$ Cond_{i}^{y}(x) ::== (x_{i}- x_{i}') \\int_{\\alpha=0}^{1} \\frac{\\partial F(x' + \\alpha(x-x'))}{\\partial y} \\cdot \\frac{\\partial y}{\\partial x_{i}} d \\alpha $$</p>"},{"location":"KB/Conductance/#evaluation-of-conductance","title":"Evaluation of Conductance","text":"<ul> <li>Activation: The value of the hidden unit is the feature importance score.</li> <li>\\(Gradient\\times Activation\\) : $$ y \\times \\frac{\\partial F(x' + \\alpha \\times (x-x'))}{\\partial y} d \\alpha $$</li> <li>Internal Influence : $$ Int Inf ^{y}(x) ::= \\int^{1}_{\\alpha=0} \\frac{\\partial F(x' + \\alpha(x-x'))}{\\partial y} d \\alpha $$</li> <li>The premise is that hidden units that are important across a set of inputs from a class should be predictive of this input class.</li> </ul>"},{"location":"KB/Conductance/#properties-of-conductance","title":"Properties of Conductance","text":""},{"location":"KB/Conductance/#completeness","title":"Completeness","text":"<ul> <li>conductances for any single hidden layer add up to the difference between the predictions \\(F(x) - F(x')\\)</li> <li>conductances thus satisfy the Layerwise Conservation Principle</li> </ul>"},{"location":"KB/Conductance/#linearity","title":"Linearity","text":"<ul> <li>So do internal influence and gradient*activations</li> <li>Suppose that we linearly compose hidden neurons f1 and f2 to form the final network that models the function \\(a \\times f_{1} + b \\times f_{2}\\). Then, the conductances of the two hidden neurons will be \\(a \\times (f_{1}(x) f_{1}(x_{0}))\\) and \\(b \\times (f_{2}(x) f_{2}(x'))\\) respectively.</li> <li>This is a sanity-check because if the action of a network is mostly linear from a hidden layer, the conductances will match what is intuitively the obvious solution.</li> </ul>"},{"location":"KB/Conductance/#insensitive","title":"Insensitive","text":"<ul> <li>If varying the values of a hidden unit does not change the network's prediction, it has zero conductance</li> <li>If varying the inputs does not change value of the hidden unit, the hidden unit has zero conductance</li> <li>Based on \\(\\frac{\\partial F}{\\partial y_{j}}\\) and \\(\\frac{\\partial y_{j}}{\\partial x_{i}}\\) being 0</li> </ul>"},{"location":"KB/Conductance/#saturation-of-neural-networks","title":"Saturation of Neural Networks","text":"<ul> <li>Basically, for a network, or a sub-network, even when the output crucially depends on some input, the gradient of the output w.r.t. the input can be near-zero.</li> <li>As an artificial example, suppose the network first transforms the input x linearly to y = 2x, and then transforms it to z = max(y, 1). Suppose the input is x = 1 (where z is saturated at value 1), with 0 being the baseline. Then for the hidden unit of y, gradient of z w.r.t. y is 0. Gradient*activation would be 0 for y, which does not reflect the intuitive importance of y. Like in Integrated Gradients, in computing conductance, we consider all extrapolated inputs for x between 0 and 1, and look at the gradients of output w.r.t. y at these points. This takes the non-saturated region into account, and ends up attributing 1 to y, as desired.</li> <li>wrong Polarity/Sensitivity</li> </ul>"},{"location":"KB/Conductance/#methods","title":"Methods","text":"<ul> <li>we compare against can yield scores that have signs and magnitudes that are intuitively incorrect</li> <li>This is intuitively because each misses terms/paths that our method considers.</li> <li>Activation values for a ReLU based network are always positive. However, ReLU nodes can have positive or negative influence on the output depending on the upstream weights. Here, Activation does not distinguish the sign of the influence, whereas condutance can.</li> <li>Gradient*Activation as a linear projection can overshoot</li> <li>Certain hidden units that actually have near zero influence can be assigned high</li> <li>importance scores.</li> <li>For example, suppose that the network is the composition of two functions f (x) = x and a weighted ReLU g(y) = max(y 1, 0). Again, the network computes the composition g(f(x)). Suppose that the baseline is x = 0 and the input is x = 1 . The output of the network is 0. But the feature importance of the unit f is deemed to be 1 (activation) times 1 (gradient), which is 1 . Notice that this is the only unit in its layer, so the fact that its influence does not agree in magnitude with the output is undesirable. In contrast, conductance assigns all hidden units a score of zero. The example can be extended to show that the feature importance score can disagree in sign with the actual direction of influence.</li> <li>Suppose that the network is the composition two functions f(x) = x and g(y) = y, i.e., the network computes the composition g(f(x)). Suppose that the baseline is x = 0 and the input is x = 1. The output of the network is 1. But the internal influence of the unit represented by the function g is +1 (regardless of the choice of the input or the path). Notice that this is the only unit in</li> <li>its layer, so the fact that its influence does not agree in sign with the output is highly undesirable. In contrast, conductance assigns an influence score of 1.</li> </ul>"},{"location":"KB/Conductance/#applying-conductance-to-an-object-recognition-model","title":"Applying Conductance to an Object Recognition Model","text":"<ul> <li>We use conductance as a measure to identify influential filters in hidden layers in the Inception network.</li> <li>Given an input image, we identify the top predicted label</li> <li>For the pre-softmax score for this label, we compute the conductance for each of the filters in each of the hidden layers</li> <li>The visualization is done by aggregating the conductance along the color channel and scaling the pixels in the actual image by the conductance values.</li> </ul>"},{"location":"KB/Conductance/#ablation-study","title":"Ablation Study","text":"<ul> <li>Next we studied how many filters we need to ablate in the network in order for the network to change its prediction. We found that, it is sufficient to ablate 3.7 on an average for the network to change its prediction for an image. Only 3 out of 100 images needed more than 10 filter ablations to change the predicted label. The maximum was 16. This provides further evidence that using conductance we can identify filters that are important for the prediction.</li> <li>We compare this to the filters with highest internal influence. Out of the 100 sample images, the network prediction changed for only 5 images when their top 10 filters</li> </ul>"},{"location":"KB/Conductance/#division-of-labour","title":"Division of Labour","text":"<ul> <li>We notice that almost all the filters either capture positive sentiment or negative sentiment, but not both.</li> <li>We substantiate via Figure 3, which is a clustered heatmap of signs of conductances of the 256 filters (columns) for around four thousand examples (rows) from the Stanford Sentiment Tree Bank [24]. Notice that very few filters have</li> <li>both negative and positive conductance. Negation</li> <li>Negation is commonly used in expressing sentiments, in phrases like \"this is not good\" or \"this is not bad\". Does the sentiment network understand negation? Does it have hidden units dedicated to implement the logic of negation? We first identify high conductance filters for the input \"this is not good\" that have a high attribution to the pattern \"not good\".</li> <li>Sentences with high conductance for filters that have high conductance for the phrase \"not bad\". These filters are largerly focussed on negation.</li> </ul>"},{"location":"KB/Conductance/#images","title":"Images","text":""},{"location":"KB/Cone/","title":"Cone","text":""},{"location":"KB/Cone/#cone","title":"Cone","text":"<ul> <li>A type of photoreceptor cell responsible for color vision that is found in the retina.</li> </ul>"},{"location":"KB/Confidence/","title":"Confidence","text":""},{"location":"KB/Confidence/#confidence","title":"Confidence","text":"<ul> <li>confidence should always be assessed on a model in which reliability is expected.</li> <li>stability is a must-have when drawing interpretations from a certain model</li> <li>Trustworthy interpretations should not be produced by models that are not stable.</li> <li>Hence, an explainable model should contain information about the confidence of its working regime.</li> </ul>"},{"location":"KB/Confirmation%20Bias/","title":"Confirmation Bias","text":""},{"location":"KB/Confirmation%20Bias/#confirmation-bias","title":"Confirmation Bias","text":"<ul> <li>fairness</li> <li>The tendency to search for, interpret, favor, and [recall] information in a way that confirms one's preexisting beliefs or hypotheses. Machine learning developers may inadvertently collect or label data in ways that influence an outcome supporting their existing beliefs. Confirmation bias is a form of implicit bias.</li> <li>Experimenter's bias is a form of confirmation bias in which an experimenter continues training models until a preexisting hypothesis is confirmed.</li> <li> </li> <li>Using a dataset not gathered scientifically in order to run quick experiments. Later on, it's essential to switch to a scientifically gathered dataset</li> <li> </li> <li>The process of using mathematical techniques such as gradient descent to find the minimum of a convex function. A great deal of research in machine learning has focused on formulating various problems as convex optimization problems and in solving those problems more efficiently.</li> </ul>"},{"location":"KB/Confirmation%20Bias/#convenience-sampling","title":"convenience sampling","text":""},{"location":"KB/Confirmation%20Bias/#convex-optimization","title":"convex optimization","text":""},{"location":"KB/Conformer/","title":"Conformer","text":""},{"location":"KB/Conformer/#conformer","title":"Conformer","text":"<ul> <li>Conformer: Convolution-augmented Transformer for Speech Recognition</li> <li>Transformer models are good at capturing content-based global interactions, while CNNs exploit local features effectively</li> <li>integrating components from both CNNs and Transformers for end-to-end KB/Speech Recognition.md to model both local and global dependencies of an audio sequence in a parameter-efficient way</li> <li>importance of each component, and demonstrated that the inclusion of convolution modules is critical to the performance of the Conformer model</li> <li>propose the convolution-augmented transformer for KB/Speech Recognition.md, named Conformer</li> <li>LibriSpeech</li> </ul>"},{"location":"KB/Confusion%20Matrix/","title":"Confusion Matrix","text":""},{"location":"KB/Confusion%20Matrix/#confusion-matrix","title":"Confusion Matrix","text":"<ul> <li>Measures the test performance of a classification system on a per-class basis by indicating the number of samples of actual class\u00a0a\u00a0predicted as class\u00a0b.</li> <li>The rows relate to the actual class labels\u00a0a\u00a0and the columns to the predicted class labels\u00a0b.</li> </ul>"},{"location":"KB/Connectionism/","title":"Connectionism","text":""},{"location":"KB/Connectionism/#connectionism","title":"Connectionism","text":"<ul> <li>So called symbols like noun, verb or noun-phrase are just epiphenomenal misunderstandings we have of learned through network arrangements of non-linguistic primitive elements</li> </ul>"},{"location":"KB/Connectionist%20Networks/","title":"Connectionist Networks","text":""},{"location":"KB/Connectionist%20Networks/#connectionist-networks","title":"Connectionist Networks","text":"<ul> <li>Intelligence 'emerges' through the changes in the connections (weights)</li> <li>Basically deep learning</li> </ul>"},{"location":"KB/Connectives/","title":"Connectives","text":""},{"location":"KB/Connectives/#connectives","title":"Connectives","text":"<ul> <li>connect words, phrases (and, but, when)</li> </ul>"},{"location":"KB/Connectome/","title":"Connectome","text":""},{"location":"KB/Connectome/#connectome","title":"Connectome","text":"<ul> <li>the graph of how neurons in a brain connect</li> </ul>"},{"location":"KB/Connectome/#connectome_1","title":"Connectome","text":""},{"location":"KB/Consequentialist%20ethics/","title":"Consequentialist ethics","text":""},{"location":"KB/Consequentialist%20ethics/#consequentialist-ethics","title":"Consequentialist Ethics","text":"<ul> <li>an agent is ethical if and only if it weighs the consequences of each choice and chooses the option which has the most moral outcomes</li> </ul>"},{"location":"KB/Conservation%20Of%20Momentum/","title":"Conservation Of Momentum","text":""},{"location":"KB/Conservation%20Of%20Momentum/#conservation-of-momentum","title":"Conservation Of Momentum","text":"<ul> <li> \\[\\Sigma p_{i}= \\Sigma p _{f}\\] </li> <li> \\[m_{1}u_{1}+m_{2}u_{2}= m_{1}v_{1}+ m_{2}v_{2}\\] </li> <li>p is momentum</li> <li>v and d are velocity</li> <li>m is mass</li> </ul>"},{"location":"KB/Contact%20Sensor/","title":"Contact Sensor","text":""},{"location":"KB/Contact%20Sensor/#contact-sensor","title":"Contact Sensor","text":"<ul> <li>A device that detects the presence of an object or measures the amount of applied force or torque applied on the object through physical contact with it. Contact sensing can be used to determine location, identity, and orientation of work pieces.</li> </ul>"},{"location":"KB/Content%20Based%20Attention/","title":"Content Based Attention","text":""},{"location":"KB/Content%20Based%20Attention/#content-based-attention","title":"Content Based Attention","text":"<ul> <li>Graves2014</li> <li>Attention Alignment score \\(score(s_{t}, h_{i}) = cosine[s_{t}, h_{i}]\\)$</li> </ul>"},{"location":"KB/Content%20Based%20Recommender/","title":"Content Based Recommender","text":""},{"location":"KB/Content%20Based%20Recommender/#content-based-recommender","title":"Content Based Recommender","text":"<ul> <li>User actions linked to content</li> <li>User model rep as info context</li> <li>eg: Google</li> </ul>"},{"location":"KB/Content%20Morpheme/","title":"Content Morpheme","text":""},{"location":"KB/Content%20Morpheme/#content-morpheme","title":"Content Morpheme","text":"<ul> <li>carry some semantic content</li> <li>e.g. able, un, van</li> </ul>"},{"location":"KB/Content%20words/","title":"Content words","text":""},{"location":"KB/Content%20words/#content-words","title":"Content Words","text":"<ul> <li>Identifies part of a word</li> <li>Noun</li> <li>Adjective</li> <li>Verb</li> <li>Adverb</li> </ul>"},{"location":"KB/Context%20Free%20Grammar/","title":"Context Free Grammar","text":""},{"location":"KB/Context%20Free%20Grammar/#context-free-grammar","title":"Context Free Grammar","text":"<ul> <li>formal system that describes a language by specifying how any legal text can be derived from a distinguished symbol called the axiom, or sentence symbol.</li> <li>It consists of a set of productions, each of which states that a given symbol can be replaced by a given sequence of symbols</li> <li>Types of Words</li> <li></li> <li></li> <li>Top Down Parsing</li> <li>Bottom Up Parsing</li> </ul>"},{"location":"KB/Context%20Similarity/","title":"Context Similarity","text":""},{"location":"KB/Context%20Similarity/#context-similarity","title":"Context Similarity","text":"<ul> <li>between image patches </li> <li>image clusteringbased methods </li> <li>graph constraint-based methods</li> </ul>"},{"location":"KB/Continous%20-%3E%20Discrete/","title":"Continous -> Discrete","text":""},{"location":"KB/Continous%20-%3E%20Discrete/#continous-discrete","title":"Continous -&gt; Discrete","text":""},{"location":"KB/Continous%20-%3E%20Discrete/#binning","title":"Binning","text":""},{"location":"KB/Continous%20-%3E%20Discrete/#hierarchial-refinement","title":"Hierarchial Refinement","text":""},{"location":"KB/Continous%20-%3E%20Discrete/#vector-quantization","title":"Vector Quantization","text":""},{"location":"KB/Continous%20-%3E%20Discrete/#neural-dynamics","title":"Neural Dynamics","text":""},{"location":"KB/Continuous%20Path/","title":"Continuous Path","text":""},{"location":"KB/Continuous%20Path/#continuous-path","title":"Continuous Path","text":"<ul> <li>Describes the process where by a robot is controlled over the entire path traversed, as opposed to a point-to-point method of traversal. This is used when the trajectory of the end-effector is most important to provide a smooth movement, such as in spray painting etc</li> </ul>"},{"location":"KB/Contour/","title":"Contour","text":""},{"location":"KB/Contour/#contours","title":"Contours","text":"<ul> <li>For nD \\(\\(\\{x \\in \\mathbb{R}^{n}|f(x)=c\\}\\)\\)</li> <li>Always closed curves</li> <li>Never self instersect</li> <li>Nested</li> <li>Contours cut the plane into values smaller or larger than the isovalue c</li> <li>Isoline</li> <li>Isosurface</li> <li>Countouring with Transparency</li> </ul>"},{"location":"KB/Contrastive%20Loss/","title":"Contrastive Loss","text":""},{"location":"KB/Contrastive%20Loss/#contrastive-loss","title":"Contrastive Loss","text":"<ul> <li>Minimize distance between similar inputs Gradient Descent gradients, maximize between dissimilar Gradient Ascent</li> <li>Learn Embedding/Feature space using neighbors</li> <li>dim(Embedding d) &lt; dim(input Space D)</li> <li>Encoded using a learnable function(NN) \\(\\(G_\\theta(x) : \\mathcal{R}^D \\rightarrow \\mathcal{R}^d\\)\\)</li> <li>Binary labels : similar or not</li> <li>$\\(D_\\theta(x_1, x_2) = ||G_\\theta(x_1) - G_\\theta(x_2)||_2\\)</li> <li> \\[L(\\theta, y, x_1, x_2) = \\frac{(1-y)(D_\\theta(x_1, x_2))^2}{2} + \\frac{y(max(0,m-D\\theta(x_1, x_2)))^2}{2}\\] <ul> <li>m is enforced margin between similar and dissimilar (m&gt;0)</li> <li>Labeled points \\(\\((y,x_1,x_2)\\)\\) are generated</li> </ul> </li> </ul>"},{"location":"KB/Contrastive%20Predictive%20Coding/","title":"Contrastive Predictive Coding","text":""},{"location":"KB/Contrastive%20Predictive%20Coding/#contrastive-predictive-coding","title":"Contrastive Predictive Coding","text":"<ul> <li>Representation Learning with Contrastive Predictive Coding</li> <li>Contrastive Predictive Coding</li> <li>framework for extracting compact latent representations to encode predictions over future observations</li> <li>learn such representations by predicting the future in latent space by using powerful KB/Autoregressive.md models</li> <li>probabilistic contrastive loss based on NCE, which both the encoder and KB/Autoregressive.md model are trained to jointly optimize, which they call InfoNCE</li> <li>InfoNCE induces the latent space to capture information that is maximally useful to predict future samples</li> <li>combines KB/Autoregressive.md modeling and noise-contrastive estimation with intuitions from predictive coding to learn abstract representations in an unsupervised fashion</li> <li>negative sampling</li> </ul>"},{"location":"KB/Contributions%20of%20Shape%2C%20Texture%2C%20and%20Color%20in%20Visual%20Recognition%20Abstract/","title":"Contributions of Shape, Texture, and Color in Visual Recognition Abstract","text":""},{"location":"KB/Contributions%20of%20Shape%2C%20Texture%2C%20and%20Color%20in%20Visual%20Recognition%20Abstract/#contributions-of-shape-texture-and-color-in-visual-recognition-abstract","title":"Contributions of Shape, Texture, and Color in Visual Recognition Abstract","text":"<ul> <li>/zotero</li> <li>Yunhao Ge, Yao Xiao, Zhi Xu, Xingrui Wang, and Laurent Itti   <code>toc</code></li> </ul>"},{"location":"KB/Contributions%20of%20Shape%2C%20Texture%2C%20and%20Color%20in%20Visual%20Recognition%20Abstract/#abstract","title":"Abstract","text":"<ul> <li>KB/Humanoid Vision Engine.md (HVE) that explicitly and separately computes shape, texture, and color features from images</li> <li>resulting feature vectors are then concatenated to support the final classification</li> <li>HVE can summarize and rankorder the contributions of the three features to object recognition.</li> <li>We use human experiments to confirm that both HVE and humans predominantly use some specific features to support the classification of specific classes</li> <li>To demonstrate more usefulness of HVE, we use it to simulate the open-world zeroshot learning ability of humans with no attribute labeling</li> <li>Finally, we show that HVE can also simulate human imagination ability with the combination of different features.</li> </ul>"},{"location":"KB/Contributions%20of%20Shape%2C%20Texture%2C%20and%20Color%20in%20Visual%20Recognition%20Abstract/#introduction","title":"Introduction","text":"<ul> <li>A widely accepted intuition about the success of CNNs on perceptual tasks is that CNNs are the most predictive models for the human ventral stream object recognition</li> <li>To understand which feature is more important for CNN-based recognition, recent paper shows promising results: ImageNet-trained CNNs are biased towards texture while increasing shape bias improves accuracy and robustness [33]</li> <li>Here, inspired by HVS, we wish to find a general way to understand how shape, texture, and color contribute to a recognition task by pure data-driven learning.</li> <li>It has been shown by neuroscientists that there are separate neural pathways to process these different visual features in primate</li> <li>Among the many kinds of features crucial to visual recognition in humans, the shape property is the one that we primarily rely on in static object recognition [16]. Meanwhile, some previous studies show that surface-based cues also play a key role in our vision system</li> <li>For example, [21] shows that scene recognition is faster for color images compared with grayscale ones</li> <li>Humanoid Vision Engine</li> </ul>"},{"location":"KB/Contributions%20of%20Shape%2C%20Texture%2C%20and%20Color%20in%20Visual%20Recognition%20Abstract/#image-parsing-and-foreground-identification","title":"Image Parsing and Foreground Identification.","text":"<ul> <li>we use the entity segmentation method [41] to simulate the process of parsing objects from a scene in our brain.</li> <li>Entity segmentation is an open-world model and can segment the object from the image without labels.</li> <li>This method aligns with human behavior, which can (at least in some cases; e.g., autostereograms [29]) segment an object without deciding what it is</li> <li>After we get the segmentation of the image, we use a pre-trained CNN and Grad-CAM [47] to find the foreground object among all masks.</li> <li>We design three different feature extractors after identifying the foreground object segment: shape extractor, texture extractor, and color extractor, similar to the separate neural pathways in the human brain which focus on specific property</li> </ul>"},{"location":"KB/Contributions%20of%20Shape%2C%20Texture%2C%20and%20Color%20in%20Visual%20Recognition%20Abstract/#shape-feature-extractor","title":"Shape Feature Extractor","text":"<ul> <li>want to keep both 2D and 3D shape information while eliminating the information of texture and color</li> <li>first use a 3D depth prediction model [44,43] to obtain the 3D depth information of the whole image</li> <li>After element-wise multiplying the 3D depth estimation and 2D mask of the object, we obtain our shape feature</li> <li>We can notice that this feature only contains 2D shape and 3D structural information (the 3D depth) and without color or texture information</li> </ul>"},{"location":"KB/Contributions%20of%20Shape%2C%20Texture%2C%20and%20Color%20in%20Visual%20Recognition%20Abstract/#texture-feature-extractor","title":"Texture Feature Extractor","text":"<ul> <li>want to keep both local and global texture information while eliminating shape and color information.</li> <li>to remove the color information, we convert the RGB object segmentation to a grayscale image</li> <li>cut this image into several square patches with an adaptive strategy (the patch size and location are adaptive with object sizes to cover more texture information)</li> <li>If the overlap ratio between the patch and the original 2D object segment is larger than a threshold \u03c4, we add that patch to a patch pool (we set \u03c4 to be 0.99 in our experiments, which means the over 99% of the area of the patch belongs to the object</li> <li>Since we want to extract both local (one patch) and global (whole image) texture information, we randomly select 4 patches from the patch pool and concatenate them into a new texture image</li> </ul>"},{"location":"KB/Contributions%20of%20Shape%2C%20Texture%2C%20and%20Color%20in%20Visual%20Recognition%20Abstract/#color-feature-extractor","title":"Color Feature Extractor","text":"<ul> <li>The first method is phase scrambling</li> </ul>"},{"location":"KB/Contributions%20of%20Shape%2C%20Texture%2C%20and%20Color%20in%20Visual%20Recognition%20Abstract/#phase-scrambling","title":"Phase Scrambling","text":"<ul> <li>transforms the image into the frequency domain using the fast Fourier transform (FFT)</li> <li>In the frequency domain, the phase of the signal is then randomly scrambled, which destroys shape information while preserving color statistics</li> <li>Then we use IFFT to transfer back to image space</li> <li>We also used simple color histograms (see suppl.) as an alternative, but the results were not as good, hence we focus here on the phase scrambling approach for color representation.</li> </ul>"},{"location":"KB/Contributions%20of%20Shape%2C%20Texture%2C%20and%20Color%20in%20Visual%20Recognition%20Abstract/#humanoid-neural-network","title":"Humanoid Neural Network","text":"<ul> <li>After preprocessing, we have three features</li> <li>To simulate the separate neural pathways in humans' brains for different feature information [1,11], we design three feature representation encoders for shape, texture, and color, respectively</li> <li>ResNet-18 [24] as the backbone for all feature encoders to project the three types of features to the corresponding well-separated embedding spaces.</li> <li>hard to define the ground-truth label of the distance between features.</li> <li>Given that the objects from the same class are relatively consistent in shape, texture, and color, the encoders can be trained in the classification problem independently instead, with the supervision of class labels.</li> <li>fter training our encoders as classifiers, the feature map of the last convolutional layer will serve as the final feature representation</li> <li>We also propose a gradient-based contribution attribution method to interpret the contributions of shape, texture, and color to the classification decision,</li> <li>Take the shape feature as an example, given a prediction p and the probability of</li> <li>class k, namely pk, we compute the gradient of pk with respect to the shape feature Vs</li> <li>gradient as shape importance weights \u21b5sk</li> <li>In other words, Ssk represents the \"contribution\" of shape feature to classifying this</li> <li>image as class k</li> </ul>"},{"location":"KB/Contributions%20of%20Shape%2C%20Texture%2C%20and%20Color%20in%20Visual%20Recognition%20Abstract/#effectiveness-of-feature-encoders","title":"Effectiveness of Feature Encoders","text":"<ul> <li>handcrafted three subsets of ImageNet</li> <li>Shape-biased dataset containing 12 classes, where the classes were chosen which intuitively are strongly determined by shape</li> <li>Texture-biased dataset uses 14 classes which we believed are more strongly determined by texture</li> <li>Color-biased dataset includes 17 classes</li> <li>After pre-processing the original images and getting their feature images, we input the feature images into feature encoders and get the T-SNE</li> <li>Each row represents one feature-biased dataset and each column is bounded with one feature encoder, each image shows the results of one combination</li> </ul>"},{"location":"KB/Contributions%20of%20Shape%2C%20Texture%2C%20and%20Color%20in%20Visual%20Recognition%20Abstract/#effectiveness-of-humanoid-neural-network","title":"Effectiveness of Humanoid Neural Network","text":"<ul> <li>As these classifiers classify images based on corresponding feature representation, we call them feature nets.</li> <li>If we combine these three feature nets with the interpretable aggregation module, the classification accuracy is very close to the upper bound, which means our vision system can classify images based on these three features almost as well as based on the full original color images.</li> </ul>"},{"location":"KB/Contributions%20of%20Shape%2C%20Texture%2C%20and%20Color%20in%20Visual%20Recognition%20Abstract/#more-humanoid-applications-with-hve-open-world-zero-shot-learning-with-hve","title":"More Humanoid Applications with HVE Open-world Zero-shot Learning with HVE","text":"<ul> <li>Most current methods [37,32,13] need humans to provide detailed attribute labels for each image, which is costly in time and energy. However, given an image from an unseen class, humans can still describe it with their learned knowledge</li> <li>First, to represent learnt knowledge, we use feature extractors</li> <li>To retrieve learnt classes as description, we calculate the average distance dkm</li> <li>between Iun and images of other class k in the latent space on feature m Open-world classification</li> <li>To further predict the actual class of Iun based on the feature-wise description, we use ConceptNet as common knowledge to conduct reasoning</li> <li>We form a reasoning root pool R\u21e4 consisting of feature roots Rs, Rt, Rc obtained during image description, and shared attribute roots Ras , Rat , Rac . The reasoning roots will be our evidence for reasoning</li> <li>We humans can intuitively imagine an object when seeing one aspect of a feature, especially when this feature is prototypical (contribute most to classification)</li> <li>For instance, we can imagine a zebra when seeing its stripe (texture). This process is similar but harder than the classical image generation task since the input features KB/Modality.md here dynamic which can be any feature among shape, texture, or color</li> </ul>"},{"location":"KB/Contributions%20of%20Shape%2C%20Texture%2C%20and%20Color%20in%20Visual%20Recognition%20Abstract/#cross-feature-retrieval","title":"Cross Feature Retrieval","text":"<ul> <li>In order to reasonably retrieve the most possible other two corresponding features given only one feature (among shape, texture, or color), we learn a feature agnostic encoder that projects the three features into one same feature space and makes sure that the features belonging to the same class are in the nearby regions.</li> <li>In the retrieval process, given any feature of any object, we can map it into the cross feature embedding space by the corresponding encoder net and the feature agnostic net</li> <li>Then we apply the 2 norm to find the other two features closest to the input one as output. The output is correct if they belong to the same class as the input.</li> </ul>"},{"location":"KB/Contributions%20of%20Shape%2C%20Texture%2C%20and%20Color%20in%20Visual%20Recognition%20Abstract/#cross-feature-imagination","title":"Cross Feature Imagination","text":"<ul> <li>To stimulate imagination, we propose a crossfeature imagination model to generate a plausible final image with the input and retrieved features</li> <li>Inspired by the pixel2pixel GAN[26] and AdaIN[25] in the style transfer, we design a crossfeature pixel2pixel GAN model to generate the final image.</li> </ul>"},{"location":"KB/Contributions%20of%20Shape%2C%20Texture%2C%20and%20Color%20in%20Visual%20Recognition%20Abstract/#pictures","title":"Pictures","text":""},{"location":"KB/Conv%20Based%20Noise%20Reduction/","title":"Conv Based Noise Reduction","text":""},{"location":"KB/Conv%20Based%20Noise%20Reduction/#conv-based-noise-reduction","title":"Conv Based Noise Reduction","text":"<ul> <li>Noise is high frequency component, suppress via low-pass filters</li> <li>Ideal low-pass filter</li> <li>multiply with box filter in frequency domain</li> <li>convolution with sinc in spatial domain (impractical: infinite extent)</li> <li></li> <li></li> <li>Spatially narrow (wide) filter has wide (narrow) spectrum and low (high) smoothing effect</li> </ul>"},{"location":"KB/Conv/","title":"Convnd","text":""},{"location":"KB/Conv/#convnd","title":"Convnd","text":"<ul> <li> \\[A\\ast B\\] </li> <li>Connect a neighbor only to spatial neighborhood -&gt; spatial order<ul> <li>Some rotation and illumination invariance</li> </ul> </li> <li>Slide over -&gt; Same weights independant of location -&gt; less weights</li> <li>Subsample after conv</li> <li>multiple 2d feature maps</li> <li>Similar to Gabor filters after learning</li> <li>Some might collapse to 0</li> <li> \\[out(x,y) = \\Sigma_i \\Sigma_j input(x+i, y+j) kernel(i,j)\\] </li> <li> \\[Z^j = \\Sigma_{i=0}^{l-1}W^{ji} \\ast X^i + b^{ij}\\] </li> <li> \\[Y^j = g(Z^j)\\] </li> <li>Output shape : (\\(\\frac{()_i-f+2p}{s}\\)\\)<ul> <li>If \\(\\(p = \\frac{f-1}{2}\\)\\) and \\(\\(s=1\\)\\) then dimensions maintained</li> </ul> </li> <li>One operation repeated over and over starting with raw</li> <li>Padded Conv</li> <li>Strided</li> <li>Depthwise Separable</li> <li>Causal 1D Conv</li> <li>Causal Dilated Conv</li> </ul>"},{"location":"KB/ConvBERT/","title":"ConvBERT","text":""},{"location":"KB/ConvBERT/#convbert","title":"ConvBERT","text":"<ul> <li>Convolutional BERT (ConvBERT) improves the original BERT by replacing some Multi Head Attention Self Attention segments with cheaper and naturally local operations, so-called span-based dynamic convolutions. These are integrated into the self-attention mechanism to form a mixed attention mechanism, allowing Multi-headed Self-attention to capture global patterns; the Convolutions focus more on the local patterns, which are otherwise captured anyway. In other words, they reduce the computational intensity of training BERT.</li> </ul>"},{"location":"KB/ConvNeXt/","title":"ConvNeXt","text":""},{"location":"KB/ConvNeXt/#convnext","title":"ConvNeXt","text":"<ul> <li>@liuConvNet2020s2022</li> <li>modifying a standard Res Net , following design choices closely inspired by Vision Transformer</li> <li>A vanilla ViT, on the other hand, faces difficulties when applied to general computer vision tasks such as object detection and semantic segmentation</li> <li>hierarchical Transformers (e.g., Swin Transformer ) that reintroduced several Conv priors, making Transformers practically viable as a generic vision backbone</li> <li>effectiveness of such hybrid approaches is still largely credited to the intrinsic superiority of Transformers, rather than the inherent inductive biases of convolutions</li> <li>extending the number of epochs, using AdamW optimizer, Stochastic Depth, Label Smoothing</li> <li>number of blocks in each stage (stage compute ratio), which was adjusted from (4, 4, 6, 3) to (3, 3, 9, 3)</li> <li>The second is the stem cell configuration, which in the original ResNet consisted of 7\u00d77 convolutions with stride 2 followed by a max-Pooling layer. This was substituted by a more Transformer-like \u201cpatchify\u201d layer which utilizes 4\u00d74 non-overlapping convolutions with stride 4</li> <li>Depthwise Separable , which are interestingly similar to self-Attention as they work on a per-channel basis</li> <li>higher number of channels (from 64 to 96)</li> <li>Inverted Bottleneck: An essential configuration of Transformers is the expansion-compression rate in the MLP block (the hidden dimension is 4 times higher than the input and output dimension)</li> <li>input is expanded using 1 \\times 1 convolutions and then shrunk through depthwise convolution and 1 \\times 1 convolutions</li> <li>move the depthwise convolution before the convolution</li> <li>7 \\times 7 window (higher values did not bring any alterations in the results</li> <li>GELU instead of Relu , a single activation for each block (the original Transformer module has just one activation after the MLP), fewer normalization Layers, Batch Normalization substituted by Layer Normalization , and separate downsampling layer</li> <li>ImageNet</li> <li>COCO</li> <li>ADE20K</li> <li>A case in point is multi-modal learning, in which a cross-attention module may be preferable for modeling feature interactions across many modalities</li> <li>Transformers may be more flexible when used for tasks requiring discretized, sparse, or structured outputs</li> </ul>"},{"location":"KB/Convolutional%20RNN/","title":"Convolutional RNN","text":""},{"location":"KB/Convolutional%20RNN/#convolutional-rnn","title":"Convolutional RNN","text":"<ul> <li> \\[h_t = \\sigma_h(W_{hh}\\star h_{t-1} + W_{xh}\\star x_t + b_h)\\] </li> <li> \\[y_t = \\sigma_y(W_{hy}\\star h_t + b_y)\\] </li> <li>\\(\\(\\star\\)\\) is spatial Conv</li> <li>5D shapes -&gt; [samples, timesteps, width, height, channels]</li> <li>Very memory intensive</li> <li> \\[x^{2}+x\\] </li> </ul>"},{"location":"KB/Coping%20Theory/","title":"Coping Theory","text":""},{"location":"KB/Coping%20Theory/#coping-theory","title":"Coping Theory","text":"<ul> <li>[Marsella and Gratch, 2003]</li> <li>allow agents to deal with strong negative emotions by changing the appraisal of the given situation was proposed</li> <li>agent assesses the ethical effects of its own actions and other agents' actions</li> <li>If its own action violates a given moral value, the shame emotion is triggered which serves to lower the priority of continuing with the given action</li> <li>If another agent's action violates a given moral value, the reproach emotion is triggered in the observing agent which serves to increase social distance with the given agent</li> <li>similar to existing individual ethical decision frameworks implicit reward</li> <li>humans in the loop</li> </ul>"},{"location":"KB/Coronary%20Bypass/","title":"Coronary Bypass","text":""},{"location":"KB/Coronary%20Bypass/#coronary-bypass","title":"Coronary Bypass","text":"<ul> <li>Surgical transplant of a healthy blood vessel into the heart to bypass or replace an unhealthy vessel</li> </ul>"},{"location":"KB/Corpus%20callosum/","title":"Corpus callosum","text":""},{"location":"KB/Corpus%20callosum/#corpus-callosum","title":"Corpus Callosum","text":"<ul> <li>bundle of fibers that transmits messages from one side to the other</li> </ul>"},{"location":"KB/Corpus%20dependence/","title":"Corpus dependence","text":""},{"location":"KB/Corpus%20dependence/#corpus-dependence","title":"Corpus Dependence","text":"<ul> <li>Misspellings</li> <li>Erroneous Punctuations and Spacing</li> <li>Difficult to write rules to govern the corpora from the Internet</li> <li>Di\ufb03cult to prescribe rules governing the use of a written language</li> <li>Punctuations mean \u2013 Suprasegmentals in Spoken Language but might not be the same for corpora</li> <li>Algorithms may expect corpora need to obey some rules</li> </ul>"},{"location":"KB/Correlation/","title":"Correlation","text":"<p>toc: true title: Correlation</p> <p>categories: ['temp']</p>"},{"location":"KB/Correlation/#correlation","title":"Correlation","text":"<ul> <li>How strong a relationship is between data.</li> <li>The formulas return a value between -1 and 1, where:<ul> <li>1 indicates a strong positive relationship.</li> <li>-1 indicates a strong negative relationship.</li> <li>A result of zero indicates no relationship at all.</li> <li></li> </ul> </li> </ul>"},{"location":"KB/Cortical%20Homunculus/","title":"Cortical Homunculus","text":""},{"location":"KB/Cortical%20Homunculus/#cortical-homunculus","title":"Cortical Homunculus","text":""},{"location":"KB/Cortisol/","title":"Cortisol","text":""},{"location":"KB/Cortisol/#cortisol","title":"Cortisol","text":"<ul> <li>A steroid hormone produced by the adrenal glands that controls how the body uses fat, protein, carbohydrates, and minerals, and helps reduce inflammation. Cortisol is released in the body\u2019s stress response; scientists have found that prolonged exposure to cortisol has damaging effects on the brain</li> </ul>"},{"location":"KB/Cosine%20Distance/","title":"Cosine Distance","text":""},{"location":"KB/Cosine%20Distance/#cosine-distance","title":"Cosine Distance","text":"<ul> <li>Complement of Cosine Similarity</li> <li> \\[D_{c}(A,B) := 1- S_{c}(A,B)\\] </li> </ul>"},{"location":"KB/Cosine%20Learning%20Rate%20Decay/","title":"Cosine Learning Rate Decay","text":""},{"location":"KB/Cosine%20Learning%20Rate%20Decay/#cosine-learning-rate-decay","title":"Cosine Learning Rate Decay","text":"<ul> <li>Instead of Learning Rate Warmup and then decay</li> <li> \\[\\eta_{\\mathrm{t}}=\\frac{1}{2}\\left(1+\\cos\\left(\\frac{\\mathrm{t}\\pi}{\\mathrm{\\mathrm{T}}}\\right)\\right)\\eta\\] </li> <li>Rate decreases slowly at first, then almost linear in the middle and slows down again in the end</li> <li></li> </ul>"},{"location":"KB/Cosine%20Similarity/","title":"Cosine Similarity","text":""},{"location":"KB/Cosine%20Similarity/#cosine-similarity","title":"Cosine Similarity","text":"<ul> <li>Lp Regularization l2norm aka p = 2</li> <li> \\[S_{c}(A,B) := cos(\\theta) = \\frac{A\\cdot B}{||A|| ||B||} = \\frac{\\Sigma_{i=1}^{n}A_{i}B_{i}}{\\sqrt{\\Sigma_{i=1}^{n}A^{2}_{i}} \\sqrt{\\Sigma_{i=1}^{n}B_{i}^{2}}}\\] </li> <li>ranges from -1 : exactly opposite, 1 : exactly same, 0: orthogonal/not correlated, intermediate</li> <li>Cosine Distance</li> <li>Cosine similarity is $$ - \\mathrm{sum}\\left( \\mathrm{l2norm}\\left( y \\right) \\cdot \\mathrm{l2norm}\\left( \u0177 \\right) \\right)$$</li> <li></li> <li>magnitude of vectors is not taken into account, merely their direction</li> <li>In practice, this means that the differences in values are not fully taken into account</li> <li>If you take a recommender system, for example, then the cosine similarity does not take into account the difference in rating scale between different users</li> <li>high-dimensional data and when the magnitude of the vectors is not of importance</li> </ul>"},{"location":"KB/Counterfactual%20Fairness/","title":"Counterfactual Fairness","text":""},{"location":"KB/Counterfactual%20Fairness/#counterfactual-fairness","title":"Counterfactual Fairness","text":"<ul> <li>A fairness metric that checks whether a classifier produces the same result for one individual as it does for another individual who is identical to the first, except with respect to one or more sensitive attributes. Evaluating a classifier for counterfactual fairness is one method for surfacing potential sources of bias in a model.</li> <li>See \"When Worlds Collide Integrating Different Counterfactual Assumptions in Fairness\" for a more detailed discussion of counterfactual fairness.</li> </ul>"},{"location":"KB/Counterfactual%20Images/","title":"Counterfactual Images","text":""},{"location":"KB/Counterfactual%20Images/#counterfactual-images","title":"Counterfactual Images","text":"<ul> <li>Concepts in images, removing which would increment networks confidence about the target (aka competing class)</li> </ul>"},{"location":"KB/Counterfactual%20Impact%20Evaluation/","title":"Counterfactual Impact Evaluation","text":""},{"location":"KB/Counterfactual%20Impact%20Evaluation/#counterfactual-impact-evaluation","title":"Counterfactual Impact Evaluation","text":"<ul> <li>local method of comparison for different predictions. Counterfactuals are contrastive. They explain why a decision was made instead of another. A counterfactual explanation of a prediction may be defined as the smallest change to the feature values that changes the prediction to a predefined output.</li> </ul>"},{"location":"KB/Countouring%20with%20Transparency/","title":"Countouring with Transparency","text":""},{"location":"KB/Countouring%20with%20Transparency/#countouring-with-transparency","title":"Countouring with Transparency","text":"<ul> <li>draw several contours for several isovalues</li> <li>assign \u201cadequate\u201d transparency</li> </ul>"},{"location":"KB/Covariance/","title":"Covariance","text":"<p>toc: true title: Covariance</p> <p>categories: ['temp']</p>"},{"location":"KB/Covariance/#covariance","title":"Covariance","text":"<ul> <li>how much two\u00a0random variables\u00a0vary together</li> <li></li> <li> \\[Cov(X,Y) = \\frac{\\Sigma(x_{i}- \\bar x)(y_{i}- \\bar y)}{N-1}\\] </li> <li>\\(\\bar x\\) is the mean of x</li> </ul>"},{"location":"KB/Coverage%20Bias/","title":"Coverage Bias","text":""},{"location":"KB/Coverage%20Bias/#coverage-bias","title":"Coverage Bias","text":"<ul> <li>The population represented in the dataset does not match the population that the machine learning model is making predictions about.</li> </ul>"},{"location":"KB/Coverage%20of%20ethics%20within%20the%20artificial%20intelligence%20and%20machine%20learning%20academic%20literature/","title":"Coverage of ethics within the artificial intelligence and machine learning academic literature","text":""},{"location":"KB/Coverage%20of%20ethics%20within%20the%20artificial%20intelligence%20and%20machine%20learning%20academic%20literature/#coverage-of-ethics-within-the-artificial-intelligence-and-machine-learning-academic-literature","title":"Coverage of ethics within the artificial intelligence and machine learning academic literature","text":"<ul> <li>Lillywhite, Aspen; Wolbring, Gregor</li> </ul>"},{"location":"KB/Coverage%20of%20ethics%20within%20the%20artificial%20intelligence%20and%20machine%20learning%20academic%20literature/#abstract","title":"Abstract","text":"<ul> <li>Disabled people are often the anticipated users of scientific and technological products and processes advanced and enabled by artificial intelligence</li> <li>also impacted by societal impacts of AI/ML</li> <li>problems have been identified in how ethics discourses engage with disabled people</li> <li>Of the n = 1659 abstracts engaging with AI/ML and ethics downloaded from Scopus (which includes all Medline articles) and the 70 databases of EBSCO ALL, we found 54 relevant abstracts using the term \"patient\" and 11 relevant abstracts mentioning terms linked to \"impair\", \"disab\" and \"deaf\"</li> </ul>"},{"location":"KB/Coverage%20of%20ethics%20within%20the%20artificial%20intelligence%20and%20machine%20learning%20academic%20literature/#study-design","title":"Study design","text":"<ul> <li>We used a modified scoping review drawing from (Arksey &amp; O'Malley, 2005) as the most appropriate approach for the study given the aim of our study</li> <li>Scoping studies \"map rapidly the key concepts underpinning a research area\" (Arksey &amp; O'Malley, 2005, p. 21), to identify the extent of research conducted on a given topic (Davis, Drey, &amp; Gould, 2009; Grant &amp; Booth, 2009) and the current understanding of a given topic (S. Anderson, Allen, Peckham, &amp; Goodwin, 2008).</li> </ul>"},{"location":"KB/Coverage%20of%20ethics%20within%20the%20artificial%20intelligence%20and%20machine%20learning%20academic%20literature/#data-sources","title":"Data sources","text":"<ul> <li>EBSCO ALL, an umbrella database that includes over 70 other databases itself and Scopus, which incorporates the full Medline database collection</li> <li>The first article with the term \"ethic*\" and any of the AI terms within the EBSCO databases was published in 1981, while the first article within Scopus was published in 1962</li> </ul>"},{"location":"KB/Coverage%20of%20ethics%20within%20the%20artificial%20intelligence%20and%20machine%20learning%20academic%20literature/#limitation","title":"Limitation","text":"<ul> <li>Our findings also do not cover all words one could use to depict disabled people and as such, our results can not be generalized to every disability term</li> </ul>"},{"location":"KB/Coverage%20of%20ethics%20within%20the%20artificial%20intelligence%20and%20machine%20learning%20academic%20literature/#discussion","title":"Discussion","text":"<ul> <li>Many ethics issues pertinent to disabled people were discussed within the abstracts; for example the ethical decision making of robots but without engaging with disabled people.</li> <li>However, many barriers have been identified for disabled people to shape technology governance discussions in an anticipatory way</li> <li>including that the medical imagery of disabled people is seen to hinder their involvement in policy discussions (Wolbring, Mackay, Rybchinski, &amp; Noga, 2013)</li> <li>In one study, it is acknowledged that ethical, moral, social, cultural, and political issues have been traditionally de-emphasized in research guided by usability concerns (Fallman, 2010)</li> <li>Given the breadth of academic disciplines, including disability studies covered by the two databases, and given that we found only one article coming from a disability studies program based out of Bremen, Germany (Bruhn et al., 2006), it might be warranted to investigate how academics choose their topics of investigation and why the topics we found lacking in the literature were not chosen</li> <li>Another angle of investigation could be why students are not acting as knowledge producers on the topics we found lacking</li> <li>As to disabled students, based on a study that investigated the experience of disabled postsecondary students in postsecondary education (Hutcheon &amp; Wolbring, 2012) we suggest that the experience reported (feeling medicalized, hesitant to self-advocate, to try to fit in with the norm) might be factors that hinder disabled students to be knowledge producers especially on contentious issues such as ethics and disabled people</li> </ul>"},{"location":"KB/CowMask/","title":"CowMask","text":""},{"location":"KB/CowMask/#cowmask","title":"CowMask","text":"<ul> <li>@frenchMilkingCowMaskSemiSupervised2020</li> <li>semi-supervised learning</li> <li>original and augmented images are brought closer during training</li> <li>CowMask suggests two types of mixing approaches 1) erasing and 2) mixing two images similar to KB/CutMix.md</li> <li>The mask here is of irregular shape rather than rectangular and generated by masking or keeping a proportion of image pixels through thresholding.</li> <li>KB/Gaussian Filter.md is applied to remove noise before thresholding.</li> <li>Pixel values below the threshold are either erased or replaced by the pixel values of the randomly selected image at the corresponding locations.</li> </ul>"},{"location":"KB/Crash%20Blossom/","title":"Crash Blossom","text":""},{"location":"KB/Crash%20Blossom/#crash-blossom","title":"Crash Blossom","text":"<ul> <li>A sentence or phrase with an ambiguous meaning. Crash blossoms present a significant problem in natural language understanding. For example, the headline Red Tape Holds Up Skyscraper is a crash blossom because an NLU model could interpret the headline literally or figuratively.</li> </ul>"},{"location":"KB/Critical%20Points/","title":"Critical Points","text":""},{"location":"KB/Critical%20Points/#critical-points","title":"Critical Points","text":"<ul> <li>sink (attracting node): all vectors converge  </li> <li>source (repelling node): all vectors diverge  </li> <li>saddle point: slopes are zero in orthogonal directions, no extremum \u2022 center point: embedded by (circular) flow around  </li> <li>attracting focus: flow is attracted in a spiral pattern</li> <li>repelling focus, where the flow is repelled in a spiral pattern</li> <li></li> </ul>"},{"location":"KB/Cropping/","title":"Cropping","text":""},{"location":"KB/Cropping/#cropping","title":"Cropping","text":"<ul> <li>Cropping images can be used as a practical processing step for image data with mixed height and width dimensions by cropping a central patch of each image</li> <li>Additionally, random cropping can also be used to provide an effect very similar to translations.</li> <li>whereas translations preserve the spatial dimensions of the image</li> <li>Depending on the reduction threshold chosen for cropping, this might not be a label-preserving transformation. Rotation</li> <li>Rotation augmentations are done by rotating the image right or left on an axis between 1\u00b0 and 359\u00b0</li> <li>The safety of rotation augmentations is heavily determined by the rotation degree parameter.</li> <li>as the rotation degree increases, the label of the data is no longer preserved post-transformation. Translation</li> <li>Shifting images left, right, up, or down can be a very useful transformation to avoid positional bias in the data</li> <li>For example, if all the images in a dataset are centered, which is common in face recognition datasets, this would require the model to be tested on perfectly centered images as well.</li> <li>remaining space can be filled with either a constant value such as 0 s or 255 s, or it can be filled with random or Gaussian noise</li> </ul>"},{"location":"KB/Cross%20Entropy/","title":"Cross Entropy","text":""},{"location":"KB/Cross%20Entropy/#cross-entropy","title":"Cross Entropy","text":"<ul> <li>Entropy</li> <li>Cross-entropy is a measure from the field of information theory, building upon entropy and generally calculating the difference between two Probability distributions.</li> <li>It is closely related to but is different from KL Divergence that calculates the relative entropy between two Probability distributions, whereas cross-entropy can be thought to calculate the total entropy between the distributions.</li> <li>implicit distribution $$ p(Y|x;\\theta) $$ -&gt; use CE</li> <li> \\[ \\mathscr{L}(\\theta) = -\\mathbb{E}_{(x,y) \\sim P(X,Y)} log (p_{model}(Y|x)) \\] <ul> <li>Categorical CE<ul> <li></li> <li>Classification</li> <li>Labels should be One hot </li> <li> \\[ \\mathscr{L}(\\theta) = -\\mathbb{E}_{(x,y) \\sim P(X,Y)} \\Sigma_{i=1}^C 1(y=i)log (p_{model}f_i(x|\\theta)) \\] </li> <li>C is no of classes</li> <li> \\[ L(y, \\hat y) = - \\Sigma_{i}\\Sigma_{c}y_{i}^{c} log(\\hat y_{i}^{c}) \\] </li> </ul> </li> <li>MSE<ul> <li>Regression</li> <li> \\[ \\mathscr{L}(\\theta) = \\frac{1}{2}\\mathbb{E}_{(x,y) \\sim P(X,Y)}||y-f(x;\\theta)||^2 \\] </li> </ul> </li> </ul> </li> <li> <p>Binary Cross Entropy</p> </li> <li> <p>The Cross Entropy Loss function is a popular loss function that is used in multi-class image classification tasks. Derived from the field of information theory, it uses the concept of entropy to quantifies the discrepancy between two given probability distributions. The formula for computing the loss is given by $$ \\mathscr{l}(x,y) = L = {l_{1}, ..., l_{N}}^{T} $$ where $$ l_{n} = -w_{y_{n}}log \\frac{exp(x_{n, y_{n}})}{\\Sigma_{c=1}^{C}exp(x_{n,c})} $$, \\(x\\) is the input, \\(y\\) is the target, \\(C\\) is the number of classes</p> </li> <li> <p></p> </li> <li></li> <li>Also Negative Log Likelihood</li> </ul>"},{"location":"KB/Cross%20Minimization/","title":"Cross Minimization","text":""},{"location":"KB/Cross%20Minimization/#cross-minimization","title":"Cross Minimization","text":"<ul> <li>planar graph: can be drawn on a plane without edge crossings</li> <li>from Euler\u2019s formula - the maximum number of edges for planar graphs: \\(\\(e \\leq 3v-6\\)\\)</li> <li></li> </ul>"},{"location":"KB/Cross%20Modal%20Distillation/","title":"Cross Modal Distillation","text":""},{"location":"KB/Cross%20Modal%20Distillation/#cross-modal-distillation","title":"Cross Modal Distillation","text":"<ul> <li>Moreover, Do et al. (2019) proposed a knowledge distillation-based visual question answering method, in which knowledge from trilinear interaction teacher model with image-question-answer as inputs is distilled into the learning of a bilinear interaction student model with image-question as inputs</li> </ul>"},{"location":"KB/Cross%20Modal-based%20Methods/","title":"Cross Modal-based Methods","text":""},{"location":"KB/Cross%20Modal-based%20Methods/#cross-modal-based-methods","title":"Cross Modal-based Methods","text":"<ul> <li>train ConvNets to verify whether two different channels of input data are corresponding to each other </li> <li>Visual-Audio Correspondence Verification </li> <li>RGB-Flow Correspondence Verification </li> <li>egomotion</li> </ul>"},{"location":"KB/Cross%20Validation/","title":"Cross Validation","text":""},{"location":"KB/Cross%20Validation/#cross-validation","title":"Cross Validation","text":""},{"location":"KB/Cross%20Validation/#kfold","title":"KFold","text":"<ul> <li>Repeat for m = 1..L<ul> <li>Split data into roughly equal sizes. Disjoint subsets</li> <li>Get model with min Emperical Risk</li> <li>Test it with validation set</li> <li>Avg it for the folds for this value of m</li> </ul> </li> <li>Find optimal class for that m that had min avg validation risk (aka training error)</li> <li>Compute \\(h_{opt}\\) using the original training data</li> </ul>"},{"location":"KB/Cross%20Validation/#leave-one-out","title":"Leave One Out","text":"<ul> <li>Each D contains a single training example</li> <li>For tiny datasets</li> </ul>"},{"location":"KB/Cross%20angle%20Maximization/","title":"Cross angle Maximization","text":""},{"location":"KB/Cross%20angle%20Maximization/#cross-angle-maximization","title":"Cross Angle Maximization","text":"<ul> <li>avoid ambiguities</li> <li></li> </ul>"},{"location":"KB/Cross-dataset%20generalization/","title":"Cross-dataset generalization","text":""},{"location":"KB/Cross-dataset%20generalization/#cross-dataset-generalization","title":"Cross-dataset Generalization","text":"<ul> <li>virtually no papers demonstrating cross-dataset generalization, e.g. training on ImageNet, while testing on PASCAL VOC</li> <li>if our datasets were truly representative of the real world, this would be a very easy thing to do, and would give access to more of the much needed labelled data</li> <li>But from our perspective, all the datasets are really trying to represent the same domain \u2013 our visual world \u2013 and we would like to measure how well or badly they do it.</li> <li>Overall the results look rather depressing, as little generalization appears to be happening beyond the given dataset</li> </ul>"},{"location":"KB/Cross-situational%20learning/","title":"Cross-situational learning","text":""},{"location":"KB/Cross-situational%20learning/#cross-situational-learning","title":"Cross-situational Learning","text":"<ul> <li>Speakers 'take statistics' about word-concept co-occurrences</li> <li>Predicts gradual learning</li> </ul>"},{"location":"KB/Cuboids/","title":"Cuboids","text":""},{"location":"KB/Cuboids/#cuboids","title":"Cuboids","text":""},{"location":"KB/Cumulative%20Interpretation/","title":"Cumulative Interpretation","text":""},{"location":"KB/Cumulative%20Interpretation/#cumulative-interpretation","title":"Cumulative Interpretation","text":"<ul> <li>No scopal relation</li> <li>Three aliens are holding two flags.</li> </ul>"},{"location":"KB/Curl%20And%20Vorticity/","title":"Curl And Vorticity","text":""},{"location":"KB/Curl%20And%20Vorticity/#curl-and-vorticity","title":"Curl And Vorticity","text":"<ul> <li>Helmholtz Theorem</li> </ul>"},{"location":"KB/Curriculum%20Learning/","title":"Curriculum Learning","text":""},{"location":"KB/Curriculum%20Learning/#curriculum-learning","title":"Curriculum Learning","text":"<ul> <li>Formal Mathematics Statement Curriculum Learning</li> <li>neural theorem prover using GPT-f</li> <li>solve a curriculum of increasingly difficult problems out of a set of formal statements of sufficiently varied difficulty</li> <li>high-school Math Olympiad problems</li> <li>language model to find proofs of formal statements</li> <li>formal mathematics</li> <li>at same compute budget, expert iteration, by which they mean proof search interleaved with learning, dramatically outperforms proof search only</li> <li>expert iteration is capable of finding and solving a curriculum of increasingly difficult problems, without the need for associated ground-truth proofs</li> <li>miniF2F</li> <li>automatically solving multiple challenging problems drawn from high school olympiads</li> <li>lack of self-play in the formal mathematics setup can be effectively compensated for by automatically as well as manually curated sets of formal statement</li> <li>cheaper to formalize than full proofs</li> </ul>"},{"location":"KB/Curse%20Of%20Dimensionality/","title":"Curse of Dimensionality","text":""},{"location":"KB/Curse%20Of%20Dimensionality/#curse-of-dimensionality","title":"Curse of Dimensionality","text":"<ul> <li>In an n dim hypercube -&gt; greatest possible distance is \\(\\sqrt{n}\\)</li> <li>Aka the higher the dimension -&gt; wider the training points from each other</li> <li>But there are fewer data points than dimensions and the distances are huge</li> <li>Dimensionality Reduction</li> </ul>"},{"location":"KB/Cut%20and%20Delete/","title":"Cut and Delete","text":""},{"location":"KB/Cut%20and%20Delete/#cut-and-delete","title":"Cut and Delete","text":"<ul> <li>data augmentation by deleting image patches randomly or semantically.</li> <li>learn in case of occlusions</li> <li>This kind of [dropout] is different from conventional [dropout] because it drops contiguous image regions, whereas values in traditional dropout work at noncontiguous locations</li> </ul>"},{"location":"KB/Cut%20and%20Mix/","title":"Cut and Mix","text":""},{"location":"KB/Cut%20and%20Mix/#cut-and-mix","title":"Cut and Mix","text":"<ul> <li>@yunCutMixRegularizationStrategy2019</li> <li>instead of deleting a patch, the patch is replaced with some other image region</li> <li>y this approach, an image shares multiple class labels, whereas the major class label belongs to the original class label</li> <li>Hence, the model learns to differentiate between two classes within a single image.</li> <li>CutMix can be defined by the following operations \\(\\(\\overset{\\sim}x = M \\odot x_{A} + (1-M) \\odot x_{B}\\)\\)</li> <li> \\[\\overset{\\sim}y = \\lambda y_{A}+ (1- \\lambda)y_{B}\\] </li> <li>where \\(x\\) is an RGB image, \\(y\\) is the respective label, \\(M\\) is a binary mask of the patch of the image that will be dropped and \\(\\odot\\) represents element wise multiplication. The new training sample \\(\\overset{\\sim}x , \\overset{\\sim}y\\) is created by combining two other training samples \\(x_{A}, y_{A}\\) and \\(x_{B} , y_{B}\\). To control the combination ratio \\(\\lambda\\), a sample from the \\(\\beta(1,1)\\) distribution is chosen. This combination is quite similar to Mixup. </li> </ul>"},{"location":"KB/Cut%2C%20Paste%20and%20Learn/","title":"Cut, Paste and Learn","text":""},{"location":"KB/Cut%2C%20Paste%20and%20Learn/#cut-paste-and-learn","title":"Cut, Paste and Learn","text":"<ul> <li>@dwibediCutPasteLearn2017</li> <li>generates new data by extracting object instances and pasting them on randomly selected background images.</li> <li>Instances are blended with various blending approaches, for example, gaussian blurring and poison blending, to reduce pixel artifacts around the augmented object boundaries.</li> <li>Added instances are also rotated, oc- cluded, and truncated to make the learning algo- rithm robust.</li> </ul>"},{"location":"KB/CutMix/","title":"CutMix","text":""},{"location":"KB/CutMix/#cutmix","title":"CutMix","text":"<ul> <li>@yunCutMixRegularizationStrategy2019</li> <li>images are augmented by sampling patch coordinates, x, y, h, w from a uniform distribution</li> <li>selected patch is replaced at the</li> <li>corresponding location with a patch from the other randomly picked image from the current mini-batch during training.</li> <li>M is the image mask, xa and xb are images, \u03bb is the proportion of label, and ya and yb are the labels of images.</li> <li> \\[ x_{new}= M.x_{a}+(1-M).x_{b} \\] </li> <li> \\[ y_{new}= \\lambda.y_{a}+ (1-\\lambda).y_{b} \\] </li> </ul>"},{"location":"KB/Cutout/","title":"Cutout","text":""},{"location":"KB/Cutout/#cutout","title":"Cutout","text":"<ul> <li>@devriesImprovedRegularizationConvolutional2017</li> <li>removes constant size square patches randomly by replacing them with any constant value.</li> <li>The selection of region is performed by selecting a pixel value randomly and placing a square around it</li> <li>Cutout can be expressed as an element-wise multiplication operation \\(x_{cutout} = x \\odot M\\), where \\(x\\) is the original image, \\(M\\) is a binary mask of the same size as \\(x\\) with randomly chosen coordinates of a square patch of pixels to be cut out, and \\(\\odot\\) denotes element-wise multiplication.</li> </ul>"},{"location":"KB/CvT/","title":"CvT","text":"<p>toc: true title: CvT</p> <p>categories: ['temp']</p>"},{"location":"KB/CvT/#cvt","title":"CvT","text":"<ul> <li>CvT: Introducing Convolutions to Vision Transformers<ul> <li>improves Vision Transformer</li> <li>introducing Conv</li> <li>a hierarchy of Transformers containing a new convolutional token Embedding</li> <li>convolutional Transformer block leveraging a convolutional projection</li> <li>shift, scale, and distortion invariance</li> <li>dynamic Attention , global context, and better generalization</li> <li>ImageNet</li> <li>Position Encoding , a crucial component in existing Vision Transformers, can be safely removed in our model</li> <li>potential advantage for adaption</li> <li>built-in local context structure introduced by convolutions, CvT no longer requires a position embedding</li> </ul> </li> </ul>"},{"location":"KB/Cycle%20Consistency%20Loss/","title":"Cycle Consistency Loss","text":""},{"location":"KB/Cycle%20Consistency%20Loss/#cycle-consistency-loss","title":"Cycle Consistency Loss","text":"<ul> <li>For two domains X, Y mapping \\(G: X \\rightarrow Y\\), \\(F: Y \\rightarrow X\\)</li> <li>trying to enforce the intuition that these mappings should be reverses of each other and that both mappings should be bijections</li> <li>Encourages \\(\\(F(G(x)) \\approx x \\text{ and } G(F(y)) \\approx y\\)\\)</li> <li>reduces the space of possible mapping functions by enforcing forward and backwards consistency</li> <li> \\[L_{cyc}(G,F) = \\mathbb{E}_{x \\sim p_{data}(x)}[||F(G(x))-x)||_{1}] + \\mathbb{E}_{x \\sim p_{data}(y)}[||G(F(x))-x)||_{1}]\\] </li> <li> \\[\\mathcal{L}_{cyc}(G, F, X, Y) = \\frac{1}{m}\\Sigma_{i=1}^{m}[(F(G(x_{i})-x_{i})+ (G(F(y_{i}))-y_{i})]\\] </li> </ul>"},{"location":"KB/CycleGAN/","title":"CycleGAN","text":""},{"location":"KB/CycleGAN/#cyclegan","title":"CycleGAN","text":"<ul> <li>Unpaired image2image</li> <li>2 Mapping functions G, F : generator and \\(D_{X}, D_{Y}\\) as generators</li> <li>Adversarial Loss</li> <li>\\(\\mathcal{L}_{cyc}\\) Cycle Consistency Loss</li> <li>Full objective<ul> <li>Adversarial Loss + Cycle Consistency Loss</li> <li>\\(\\lambda\\) is a hyperparam. Generally set to 10</li> <li> \\[\\mathcal{L}_{GAN}(G, F, D_{X}, D_{Y}) = \\mathcal{L}_{GAN}(G, D_{Y}, X, Y) + \\mathcal{L}_{GAN}(F, D_{X}, X, Y) + \\lambda \\mathcal{L}_{cyc}(G,F)\\] </li> </ul> </li> <li>To Solve</li> <li> \\[G^{*},F^{*} =\\underset{G,F}{argmin} \\underset{D_{X}, D_{Y}}{max} \\mathcal{L}_{GAN}(G, F, D_{X}, D_{Y})\\] </li> <li>two stride-2 convolutions, several residual blocks, and two fractionally KB/Strided.md convolutions with stride \\(\\frac{1}{2}\\)</li> <li>Instance Normalization</li> </ul>"},{"location":"KB/CycleGAN/#architecture","title":"Architecture","text":""},{"location":"KB/CycleGAN/#generator","title":"Generator","text":""},{"location":"KB/CycleGAN/#encoder","title":"Encoder","text":"<ul> <li>The encoder extracts features from the input image by using Convolutions and compressed the representation of image but increase the number of channels</li> <li>The encoder consists of 3 convolution that reduces the representation by 1/4 th of actual image size</li> </ul>"},{"location":"KB/CycleGAN/#transforming-block","title":"Transforming Block","text":"<ul> <li>The transformer contains 6 or 9 residual blocks based on the size of input.</li> <li>The output of transformer is then passed into the decoder which uses 2 -deconvolution block of fraction strides to increase the size of representation to original size.</li> </ul>"},{"location":"KB/CycleGAN/#discriminator","title":"Discriminator","text":"<ul> <li>PatchGAN</li> </ul>"},{"location":"KB/CycleGAN/#applications","title":"Applications","text":"<ul> <li>Style Transfer<ul> <li>Unlike other works on neural style transfer, CycleGAN learns to mimic the style of an entire collection of artworks, rather than transferring the style of a single selected piece of art</li> </ul> </li> <li>Object Transformation<ul> <li>CycleGAN can transform object from one ImageNet class to another such as: Zebra to Horses and vice-versa, Apples to Oranges and vice versa etc</li> </ul> </li> <li>Season Transfer<ul> <li>CycleGAN can also transfer images from Winter Season to Summer season and vice-versa. For this the model is trained on 854 winter photos and 1273 summer photos of Yosemite from Flickr.</li> </ul> </li> <li>Photo Generation from Painting<ul> <li>can also be used to transform photo from paintings and vice-versa</li> <li>Identity Loss</li> </ul> </li> </ul>"},{"location":"KB/CycleGAN/#limits","title":"Limits","text":"<ul> <li>applied to perform geometrical transformation, CycleGAN does not perform very well. This is because of the generator architecture which is trained to perform appearance changes in the image.</li> </ul>"},{"location":"KB/CycleGAN/#reduce-model-oscillation","title":"Reduce Model Oscillation","text":"<ul> <li>To prevent the model from changing drastically from iteration to iteration, the discriminators were fed a history of generated images, rather than just the ones produced by the latest versions of the generator.</li> <li>To do this we keep storing the 50 most recently generated images. Based on this technique we reduce the model Oscillation as well as model overfitting.</li> </ul>"},{"location":"KB/CycleGAN/#technical-implementation","title":"Technical Implementation","text":"<p>The CycleGAN paper provides a number of technical details regarding how to implement the technique in practice.</p> <p>The generator network implementation is based on the approach described for style transfer by\u00a0Justin Johnson\u00a0in the 2016 paper toc: true titled \u201cPerceptual Losses for Real-Time Style Transfer and Super-Resolution.\u201d</p> <p>The generator model starts with best practices for generators using the deep convolutional GAN, which is implemented using multiple residual blocks (e.g. from the\u00a0ResNet).</p> <p>The discriminator models use PatchGAN, as described by\u00a0Phillip Isola, et al. in their 2016 paper toc: true titled \u201cImage-to-Image Translation with Conditional Adversarial Networks.\u201d</p> <p>This discriminator tries to classify if each NxN patch in an image is real or fake. We run this discriminator convolutionally across the image, averaging all responses to provide the ultimate output of D.</p> <p>\u2014\u00a0Image-to-Image Translation with Conditional Adversarial Networks, 2016.</p> <p>PatchGANs are used in the discriminator models to classify 70\u00d770 overlapping patches of input images as belonging to the domain or having been generated. The discriminator output is then taken as the average of the prediction for each patch.</p> <p>The adversarial loss is implemented using a least-squared loss function, as described in\u00a0Xudong Mao, et al\u2019s 2016 paper toc: true titled \u201cLeast Squares Generative Adversarial Networks.\u201d</p> <p>[\u2026] we propose the Least Squares Generative Adversarial Networks (LSGANs) which adopt the least squares loss function for the discriminator. The idea is simple yet powerful: the least squares loss function is able to move the fake samples toward the decision boundary, because the least squares loss function penalizes samples that lie in a long way on the correct side of the decision boundary.</p> <p>\u2014\u00a0Least squares generative adversarial networks, 2016.</p> <p>Additionally, a buffer of 50 generated images is used to update the discriminator models instead of freshly generated images, as described in\u00a0Ashish Shrivastava\u2019s\u00a02016 paper toc: true titled \u201cLearning from Simulated and Unsupervised Images through Adversarial Training.\u201d</p> <p>[\u2026] we introduce a method to improve the stability of adversarial training by updating the discriminator using a history of refined images, rather than only the ones in the current minibatch.</p> <p>\u2014\u00a0Learning from Simulated and Unsupervised Images through Adversarial Training, 2016.</p> <p>The models are trained with the\u00a0Adam version of stochastic gradient descent\u00a0and a small learning rate for 100 epochs, then a further 100 epochs with a learning rate decay. The models are updated after each image, e.g. a batch size of 1.</p>"},{"location":"KB/Cyclic%20Learning%20Rate/","title":"Cyclic Learning Rate","text":""},{"location":"KB/Cyclic%20Learning%20Rate/#cyclic-learning-rate","title":"Cyclic Learning Rate","text":"<ul> <li>With respect to local minima and saddle points, one could argue that you could simply walk \"past\" them if you set steps that are large enough. Having a learning rate that is too small will thus ensure that you get stuck.</li> <li>Now,\u00a0Cyclical Learning Rates\u00a0- which were introduced by Smith (2017) - help you fix this issue. These learning rates are indeed cyclical, and ensure that the learning rate moves back and forth between a\u00a0minimum value\u00a0and a\u00a0maximum value\u00a0all the time.</li> <li></li> <li></li> <li></li> </ul>"},{"location":"KB/Cyclo%20Drive/","title":"Cyclo Drive","text":""},{"location":"KB/Cyclo%20Drive/#cyclo-drive","title":"Cyclo Drive","text":"<ul> <li>A brand name for a speed reduction device that converts high speed low torque to low speed high torque, usually used on the major (larger) axis.</li> </ul>"},{"location":"KB/Cylinders/","title":"Cylinders","text":""},{"location":"KB/Cylinders/#cylinders","title":"Cylinders","text":""},{"location":"KB/Cylindrical%20Topology/","title":"Cylindrical Topology","text":""},{"location":"KB/Cylindrical%20Topology/#cylindrical-topology","title":"Cylindrical Topology","text":"<ul> <li>A topology where the arm follows a radius of a horizontal circle, with a KB/Prismatic Joint.md to raise or lower the circle. Not popular in industry</li> </ul>"},{"location":"KB/DALL-E%203/","title":"DALL-E 2","text":""},{"location":"KB/DALL-E%203/#dall-e-2","title":"DALL-E 2","text":"<ul> <li>Hierarchical Text-Conditional Image Generation with CLIP Latents</li> <li>DALL-E 2, generates more realistic and accurate images with 4x greater resolution, better caption matching and photorealism</li> <li>Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style</li> <li>two-stage model: a prior that generates a CLIP image embedding given a text caption, and a \u201cunCLIP\u201d decoder that generates an image conditioned on the image embedding</li> <li>explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity</li> <li>decoder, which is conditioned on image representations, can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation</li> <li>diffusion models for the decoder and experiment with both KB/Autoregressive.md and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples</li> </ul>"},{"location":"KB/DALL-E/","title":"DALL-E","text":""},{"location":"KB/DALL-E/#dall-e","title":"DALL-E","text":"<ul> <li>AdaIn</li> <li>it was capable of generating text that could not be distinguished from human-written text</li> <li>named after Salvador Dal\u00ed and Pixar's WALL\u00b7E</li> <li>based on the GPT3</li> <li>Previous approaches like BERT and the original GPT model followed the fine-tuning approach.</li> <li>GPT-2 and GPT-3 recognized that even while pretraining already provided lots of benefits compared to training from scratch, so-called zero-shot learning - where the model is finetuned and then applied to language tasks, without pretraining - could be the way forward.</li> <li>DALL\u00b7E is capable of performing a variety of tasks:<ul> <li>Controlling attributes, instructing the model what particular attributes of an object should look like. For example: \"a collection of glasses is sitting on a table\" (OpenAI, 2021). Here, we instruct the model about the glasses, and more precisely, their location.</li> <li>Drawing multiple objects is also possible, but is more challenging, because it can be unknown whether certain characteristics belong to one object or another (OpenAI, 2021). DALL\u00b7E is however also capable of performing that task, but at the risk of making mistakes - once again due to the issue mentioned previously. The success rate decreases rapidly when the number of objects increases.</li> <li>Visualizing perspective and three-dimensionality, meaning that DALL\u00b7E can be instructed to take a particular \"perspective\" when generating the image (OpenAI, 2021).</li> <li>Visualizing across many levels, from \"extreme close-up\" to \"higher-level concepts\" (OpenAI, 2021).</li> <li>Inferring context, meaning that particular elements can be added to an image that normally do not belong to a particular context (e.g. the OpenAI logo in the image above; this is normally not displayed on a store front).</li> </ul> </li> <li>Uses<ul> <li>Industrial and interior design, to aid designers when creating a variety of household and other objects.</li> <li>Architecture, to guide the creation of buildings and other forms of constructions.</li> <li>Photography, to create an image specifically tailored to one's requirements.</li> <li>Graphic design, with e.g. the creation of a variety of icons.</li> </ul> </li> <li>Zero-Shot Text-to-Image Generation</li> <li>DALL-E which offers a simple approach for text-to-image generation based on an KB/Autoregressive.md transformer which models the text and image tokens as a single stream of data</li> <li>simple decoder-only transformer that receives both the text and the image as a single stream of 1280 tokens\u2014256 for the text and 1024 for the image\u2014and models all of them autoregressively</li> <li>They find that sufficient data and scale can lead to improved generalization, both in terms of zero-shot performance relative to previous domain-specific approaches</li> <li>and in terms of the range of capabilities that emerge from a single generative model.</li> </ul>"},{"location":"KB/DALL%C2%B7E%202/","title":"DALL\u00b7E 2","text":""},{"location":"KB/DALL%C2%B7E%202/#dalle-2","title":"DALL\u00b7E 2","text":"<ul> <li>generate original, genuine and realistic images and art from a prompt consisting on a text description</li> <li>DALL\u00b7E 2 manages to combine concepts, attributes and diferent styles - it uses the CLIP neural network</li> </ul>"},{"location":"KB/DCGAN/","title":"DCGAN","text":""},{"location":"KB/DCGAN/#dcgan","title":"DCGAN","text":""},{"location":"KB/DCGAN/#architecture","title":"Architecture","text":""},{"location":"KB/DCGAN/#weight-init","title":"Weight Init","text":"<ul> <li>If conv Random Normal with mean = 0 and std.dev = 0.02</li> <li>If BatchNorm with mean = 1.0 and std.dev = 0.02, Bias = 0</li> </ul>"},{"location":"KB/DCGAN/#generator","title":"Generator","text":"<ul> <li>Map latent space vector z to data space</li> <li>Creating RGB with same size as training image</li> <li>[Transposed Conv] , [Batch Normalization] and Relu</li> <li>Output is 3x64x64</li> <li>Output passed through Tanh to return it to [-1,1]</li> <li>[Batch Normalization] AFTER Transposed Conv is super important as it helps with flow of gradients</li> <li>Notice, how the inputs we set in the input section (nz,\u00a0ngf, and\u00a0nc) influence the generator architecture in code.\u00a0nz\u00a0is the length of the z input vector,\u00a0ngf\u00a0relates to the size of the feature maps that are propagated through the generator, and\u00a0nc\u00a0is the number of channels in the output image (set to 3 for RGB images)</li> <li></li> </ul>"},{"location":"KB/DCGAN/#discriminator","title":"Discriminator","text":"<ul> <li>[Strided] [Conv], [Batch Normalization], and Leaky Relu</li> <li>3x64x64 input</li> <li>Binary classification network - outputs prob of real/fake</li> <li>Final is a Sigmoid layer</li> <li>For [downsampling], good Practise to use [Strided] rather than Pooling as it lets the network learn it's own pooling function</li> <li>Almost a direct inverse of the Generator</li> </ul>"},{"location":"KB/DCGAN/#special-features","title":"Special Features","text":"<ul> <li>Explicitly uses convolutional layers in the discriminator and transposed-convolutional layers in the generator</li> <li>Further the discriminator uses batch norm layers and\u00a0[Leaky Relu]\u00a0activations while the generator uses\u00a0Relu\u00a0activations</li> <li>The input is a latent vector drawn from a standard KB/Normal Distribution.md and the output is a\u00a0\\(3 \\times 32 \\times 32\\)\u00a0RGB image</li> <li>In this implementation, I also added in\u00a0Label Smoothing</li> </ul>"},{"location":"KB/DCGAN/#loss-functions","title":"Loss functions","text":""},{"location":"KB/DCGAN/#discriminator-loss","title":"Discriminator loss","text":"<p>The Discriminator penalizes wrongly classifying a real image as a fake or a fake image as real. This can be thought of as maximizing the following function. \\(\\(\\nabla_{\\theta_{d}} \\frac{1}{m} \\Sigma_{i=1}^{m}[log D(x^{(i)}) + log(1-D(G(z^{(i)})))]\\)\\)</p>"},{"location":"KB/DCGAN/#generator-loss","title":"Generator loss","text":"<ul> <li> <p>The Generator loss takes the output of the Discriminator into account and rewards it if the Generator is fooled into thinking the fake image is real. If this condition is not satisfied, the Generator is penalized.</p> </li> <li> <p>This can be thought of as minimizing the following function. \\(\\(\\nabla_{\\theta_{g}} \\frac{1}{m} \\Sigma_{i=1}^{m}log(1-D(G(z^{(i)})))\\)\\)</p> </li> </ul>"},{"location":"KB/DICOM/","title":"DICOM","text":"","tags":["medical"]},{"location":"KB/DICOM/#dicom","title":"DICOM","text":"<ul> <li>Medical Imaging Standards</li> <li>pydicom<ul> <li>pip install pydicom</li> </ul> </li> </ul>","tags":["medical"]},{"location":"KB/DLRM/","title":"DLRM","text":""},{"location":"KB/DLRM/#dlrm","title":"DLRM","text":"<ul> <li>Deep Learning Recommendation Model for Personalization and Recommendation Systems</li> <li>DLRM</li> <li>The DLRM model handles continuous (dense) and categorical (sparse) features that describe users and products</li> <li>wide range of hardware and system components, such as memory capacity and bandwidth, as well as communication and compute resources</li> <li>design a specialized KB/Parallelization.md scheme utilizing model parallelism on the embedding tables to mitigate memory constraints while exploiting data parallelism to scale-out compute from the fully-connected layers</li> <li>it computes the feature interactions explicitly while limiting the order of interaction to pairwise interactions.</li> <li>treats each embedded feature vector (corresponding to categorical features) as a single unit, whereas other methods (such as Deep and Cross) treat each element in the feature vector as a new unit that should yield different cross terms</li> <li>These design choices help reduce computational/memory cost while maintaining competitive accuracy</li> </ul>"},{"location":"KB/DT%20Tutor/","title":"DT Tutor","text":""},{"location":"KB/DT%20Tutor/#dt-tutor","title":"DT Tutor","text":"<ul> <li>DT Tutor (Murray, VanLehn, &amp; Mostow, 2004) implements a version of this ideal tutoring policy.</li> <li>The tutor applies decision theory to make its choice about whether to give a hint.</li> <li>For each tutor action it can make (e.g., to give a hint, and which kind of hint), it uses a probabilistic model of the student to predict all possible student reactions to the tutor's action and their probability.</li> <li>The predicted student state includes the likelihood of learning, of becoming frustrated, of entering the next step correctly, etc.</li> <li>DT Tutor evaluates the utility of each of the predicted student states, multiplies the state's utility by the state's probability, and eventually produces the expected utility of each proposed tutor action.</li> <li>It then takes the tutor action with the highest expected utility. Although advances in probabilistic reasoning make it feasible for DT Tutor to perform this calculation in real time, considerable data from human students is needed in order to set the parameters in its model of student learning</li> </ul>"},{"location":"KB/Data%20Augmentation%20via%20Latent%20Space%20Interpolation%20for%20Image%20Classification/","title":"Data Augmentation via Latent Space Interpolation for Image Classification","text":""},{"location":"KB/Data%20Augmentation%20via%20Latent%20Space%20Interpolation%20for%20Image%20Classification/#data-augmentation-via-latent-space-interpolation-for-image-classification","title":"Data Augmentation via Latent Space Interpolation for Image Classification","text":"<ul> <li>@liuDataAugmentationLatent2018</li> </ul>"},{"location":"KB/Data%20Augmentation%20via%20Latent%20Space%20Interpolation%20for%20Image%20Classification/#abstract","title":"Abstract","text":"<ul> <li>standard data augmentation produces only limited plausible alternative data by for example, flipping, distorting, adding noise to, KB/Cropping.md(cropping.qmd) a patch from the original samples</li> <li>adversarial autoencoder (AAE) to impose the feature representations with uniform distribution and apply the linear interpolation on latent space, which is potential to generate a much broader set of augmentations for image classification</li> <li>improves the generalization and performance of state-of-the-art deep neural networks</li> <li>Generative models are often evaluated by examining samples from the latent space </li> <li>Techniques frequently used are random sampling and linear interpolation But often these can result in sampling the latent space from locations very far outside the manifold of probable location In high dimensional space, even with a uniform distribution most points lie on a thin shell in the unit cube. We utilize both the uniform distribution prior to avoid the \"hole\" of dead zone.</li> <li>1) We propose a novel framework to augment the dataset using the interclass interpolation of latent feature representations. 2)uniform distribution are imposed as the prior to avoid \"hole\" effect of the inter-class interpolation via the adversarial autoencoder network. 3) we explore the linear interpolation on the ILSVRC 2012 and CIFAR(CIFAR.qmd)-10 datasets</li> </ul>"},{"location":"KB/Data%20Augmentation%20via%20Latent%20Space%20Interpolation%20for%20Image%20Classification/#proposed-method","title":"PROPOSED METHOD","text":"<ul> <li>latent space interpolation followed two useful principles when sampling the latent space of a generative model avoid sampling from locations that are highly unlikely given the prior of the model. being used in the original VAE paper which adjusted sampling through the inverse CDF(CDF.qmd) of the Gaussian to accommodate the Gaussian prior recognize that the dimensionality of the latent space is often artificially high and may contains dead zones that are not on the manifold learned during training implies that simply matching the model's prior will not always be sufficient to yield samples that appear to have been drawn from the training set</li> <li>it does not require significant domain knowledge. that interpolation and extrapolation in feature space can improve generalization</li> <li>Recent approaches have also proposed to regularize(regularize.qmd) the output distribution of a neural network by label smoothing [27], or penalizing highconfidence softmax distributions [28]These methods bear similarities with mixup in the sense that supervision depends on multiple smooth labels, rather than on single hard labels as in traditional ERMthe label smoothing in these works is applied or regularized independently from the associated feature values. the supervision of every example is not overly dominated by the groundtruth labe</li> <li>LSI transformation establishes a linear relationship between data augmentation and the supervision signal.strong regularizer that improves generalization The linearity constraint, through its effect on the</li> <li>derivatives of the function approximated, also relates mixup to other methods such as Sobolev training of neural networks [29] or WGAN-GP [30].</li> <li>When other types of data augmentation are employed in addition to our technique, we can apply them for each image before mixing them into the final image for training</li> <li>The data augmentation incurs additional time to prepare the input image, but this can be done on the CPU while the GPU is executing the training through back propagation</li> <li>a single data loader to obtain one minibatch, and then latent space interpolation is applied to the same minibatch after random shuffling</li> </ul>"},{"location":"KB/Data%20Augmentation%20via%20Latent%20Space%20Interpolation%20for%20Image%20Classification/#uniform-distribution-representations","title":"Uniform distribution representations","text":"<ul> <li>(images/Pasted image 20230209131445.png.qmd)</li> <li>Adversarial autoencoder (AAE) [31] can be treated as the combination of generative adversarial networks (GANs) [20] and variational autoencoder (VAE)maintains the autoencoder structure like the VAE but replaces the KLdivergence loss with a discriminative network, denoted by Dis</li> <li>Instead of generating images from random noise as in GAN, AAE utilizes the encoder part to learn the latent variables approximated on certain prior, making the style of generated images controllable AAE better captures the data manifold compared to VAEinput as x, output as x', and the distribution of the training data as \u0000!\u0000\u0be7\u0000(\u0754 ,(then the distribution of z is q(z|x). Assuming p(z) is a prior distribution, and denotes the random sampling process from p(z). The min-max objective function can be used to\u0000 train the Enc and Dis</li> <li> \\[\\mathbb{E}_{z* \\sim p(z)}[log(Dis(z*))] \\mathbb{E}_{x \\sim p_{data}(x)}[\\overset{min Enc}{log(1-Dis_{z}(z*))}]\\] </li> <li>As for the Dec, the L2 loss is used as the reconstruction loss</li> <li> \\[L_{recon}= ||x-x'||^{2}_{2}\\] </li> <li>Dis imposes a prior distribution (i.e., uniform distribution) on z Dis aims to discriminate the z generated by encoder Enc Enc will be trained to generate z that could fool Dis</li> <li>Such adversarial process forces the distribution of the generated z to gradually approach the prio uniform distribution as the prior, forcing z to evenly populate the latent space with no apparent \"holes\". the generated z's (depicted by blue dots in a 2-D space) present uniform distribution under the KB/Regularization.md(regularization.qmd) of Dis, while the distribution of z exhibits a \"hole\" without the application of Dis Exhibition of the \"hole\" indicates that the samples generated by interpolating between arbitrary z's may not lie on the real image manifold \u2013 generating unrealistic appearancesnot to generate photorealistic images as in GAN offer more informative and discriminative training sample, the adversarial loss in pixel-level is dropped in here for faster training and processing.</li> </ul>"},{"location":"KB/Data%20Augmentation%20via%20Latent%20Space%20Interpolation%20for%20Image%20Classification/#linear-interpolation","title":"Linear interpolation","text":"<ul> <li>The difference with mixup [17] in here is that the linear interpolation manipulation is conducted in latent space which align the uniform distribution instead of the pixel-level</li> <li>The one-hot vectors are used to label the original samples</li> <li>The loss function for a generated sample are calculated as the weighted sum (via \u07e3 (of two cross-entropy losses corresponding to both of its original samples</li> <li>When the 0.5=\u07e3, we can simply use two-hot vector to label the generated sample belongs to two of its original samples for faster processing.</li> </ul>"},{"location":"KB/Data%20Augmentation%20via%20Latent%20Space%20Interpolation%20for%20Image%20Classification/#conclusion","title":"CONCLUSION","text":"<ul> <li>vicinal risk minimization trains on virtual examples constructed via interpolations of the features in a latent space with uniform distributionSeveral methods are employed to avoid the dead zone in manifold of feature representations.</li> <li>the ILSVRC 2012</li> <li>CIFAR(CIFAR.qmd)-10</li> <li>valuable for tasks with a limited number of samples, such as medical image classification tasks</li> </ul>"},{"location":"KB/Data%20Augmentation%20via%20Latent%20Space%20Interpolation%20for%20Image%20Classification/#images","title":"Images","text":"<ul> <li>(images/Pasted image 20230209131412.png.qmd)</li> <li>(images/Pasted image 20230209131420.png.qmd)</li> <li>(images/Pasted image 20230209131428.png.qmd)</li> <li></li> </ul>"},{"location":"KB/Data%20Augmentation%20with%20Curriculum%20Learning/","title":"Data Augmentation with Curriculum Learning","text":"<ul> <li>Curriculum learning decisions are especially important for One-Shot Learning systems such as FaceNet</li> <li>In this sense, the concept of curriculum learning shares many similarities with adversarial search algorithms or learning only on hard examples.</li> <li>originally coined by Bengio et al.</li> <li>Plotting out training accuracy over time across different initial training subsets could help reveal patterns in the data that dramatically speed up training time.</li> </ul>"},{"location":"KB/Data%20Augmentation%20with%20Curriculum%20Learning/#data-augmentation-with-curriculum-learning","title":"Data Augmentation with Curriculum Learning","text":""},{"location":"KB/Data%20Free%20Distillation/","title":"Data Free Distillation","text":""},{"location":"KB/Data%20Free%20Distillation/#data-free-distillation","title":"Data Free Distillation","text":"<ul> <li>Just as \u201cdata free\u201d implies, there is no training data. Instead, the data is newly or synthetically generated.</li> <li>Specifically, in (Chen et al., 2019a; Ye et al., 2020; Micaelli and Storkey, 2019; Yoo et al., 2019; Hu et al., 2020), the transfer data is generated by a GAN. In the proposed data-free knowledge distillation method (Lopes et al., 2017), the transfer data to train the student network is reconstructed by using the layer ac- tivations or layer spectral activations of the teacher net- work.</li> <li>Yin et al. (2020) proposed DeepInversion, which uses knowledge distillation to generate synthesized images for data-free knowledge transfer. Nayak et al. (2019) proposed zero-shot knowledge distillation that does not use existing data.</li> <li>The transfer data is pro- duced by modelling the softmax space using the pa- rameters of the teacher network. In fact, the target data in (Micaelli and Storkey, 2019; Nayak et al., 2019) is generated by using the information from the fea- ture representations of teacher networks.</li> <li>distilling knowl- edge from a teacher model into a student neural network (Kimura et al., 2018; Shen et al., 2021).</li> <li>data distillation, which is similar to data-free distillation (Radosavovic et al., 2018; Liu et al., 2019d; Zhang et al., 2020d). In data distillation, new training annotations of unlabeled data generated from the teacher model are employed to train a student model.</li> </ul>"},{"location":"KB/Data%20Structures/","title":"Data Structures","text":""},{"location":"KB/Data%20Structures/#data-structures","title":"Data Structures","text":"<ul> <li>Grids</li> </ul>"},{"location":"KB/Data%20aug%20for%20spoken%20language/","title":"Data aug for spoken language","text":""},{"location":"KB/Data%20aug%20for%20spoken%20language/#data-aug-for-spoken-language","title":"Data Aug for Spoken Language","text":"<ul> <li>Comparing Data Augmentation and Annotation Standardization to Improve End-to-end Spoken Language Understanding Models</li> <li>All-neural end-to-end (E2E) Spoken Language Understanding (SLU) models can improve performance over traditional compositional SLU models, but have the challenge of requiring high-quality training data with both audio and annotations</li> <li>they struggle with performance on \u201cgolden utterances\u201d, which are essential for defining and supporting features, but may lack sufficient training data</li> <li>using data augmentation to compare two data-centric AI methods to improve performance on golden utterances</li> <li>improving the annotation quality of existing training utterances and augmenting the training data with varying amounts of synthetic data</li> <li>both data-centric approaches to improving E2E SLU achieved the desired effect, although data augmentation was much more powerful than annotation standardization.</li> <li>leads to improvement in intent recognition error rate (IRER) on their golden utterance test set by 93% relative to the baseline without seeing a negative impact on other test metrics</li> </ul>"},{"location":"KB/Decision%20Boundaries/","title":"Decision Boundaries","text":""},{"location":"KB/Decision%20Boundaries/#decision-boundaries","title":"Decision Boundaries","text":"<ul> <li>Minimal risk decision function is unique and must be represented in terms of Distributions of data generating RVs X and Y<ul> <li>A is some subvolume of P. (n dimensional hypercubes or volume bodies)</li> <li>\\(P_{X,Y}\\) is ground truth<ul> <li>Function that assigns every choice of \\(A \\subseteq P , c \\in C\\) the number P</li> </ul> </li> </ul> </li> <li>Decision function \\(h: P \\rightarrow {c_{1}, \u2026, c_{k}}\\) partitions pattern space into k disjoint decision regions \\(R_{1}, \u2026, R_{k}\\) by \\(\\(R_{i}= \\{x \\in P | h(x) = c_{i}\\}\\)\\)</li> <li>If a test pattern falls into \\(R_{i}\\) it is classified as class i</li> </ul>"},{"location":"KB/Decision%20Boundaries/#finding-decision-regions","title":"Finding Decision Regions","text":"<ul> <li>which yields the lowerst misclassification rate or highest Probability of correct classification</li> <li>\\(f_{i}\\) be the PDF for Class Conditional distribution</li> <li>Probability of obtaining a correct classification for \\(R_{i}\\) is \\(\\(\\Sigma_{i=1}^{k}P(X \\in R_{i}, Y = c_{i})\\)\\)</li> <li></li> <li>This region has curved boundaries aka decision boundaries<ul> <li>Folded and on higher dims : very complex and fragmented</li> </ul> </li> <li>x is a vector</li> <li>For patterns on these boundaries, two or more classifications are equally probable</li> <li>Maximal if \\(\\(R_{i}= \\{x \\in P| i = argmax_{j} P(Y=c_{j})f_{j}(x)\\}\\)\\)</li> <li>Then \\(\\(h_{opt}: P \\rightarrow C_{j}x \\rightarrow c_{argmax_{j}P(Y=c_{j})f_{j}(x)}\\)\\)</li> <li>Algo learns estimates of the Class Conditional distribution and class probabilities aka priors</li> <li>The separator between classes learned by a model in a binary class or multi-class classification problems. For example, in the following image representing a binary classification problem, the decision boundary is the frontier between the orange class and the blue class</li> </ul>"},{"location":"KB/Decision%20Trees/","title":"DT","text":""},{"location":"KB/Decision%20Trees/#dt","title":"DT","text":""},{"location":"KB/Declarative%20Memory%20Blending/","title":"Declarative Memory Blending","text":""},{"location":"KB/Declarative%20Memory%20Blending/#declarative-memory-blending","title":"Declarative Memory Blending","text":"<ul> <li>Like \"weighted avg\"</li> <li>store no of pulses</li> <li>activation decays in time<ul> <li> \\[A(t) = log(t-t_{creation})^{-d}+\\text{mismatchpenalty}\\] </li> <li>Retrieval probability<ul> <li>Softmax</li> <li> \\[P_{i}= \\frac{e^{\\frac{A_{t}}{t}}}{\\Sigma_{i}e^{\\frac{A_{t}}{t}}}\\] </li> </ul> </li> <li>Adds up to 1</li> <li> \\[Result = \\Sigma_{j}P_{j}V_{j}\\] </li> <li>t controls noise<ul> <li>if t is high : 1/no of competitors , more prob of retrieval</li> </ul> </li> <li>Looking for long interval (partial matching)</li> <li>Penalty for short intervals</li> <li>Apply \\(P_{i}\\)</li> <li>Weighted avg \\(Result\\)</li> </ul> </li> <li>Too short is positive. else negative, correct is 0</li> <li>no of pulses to wait : duration + feedback from memory</li> </ul>"},{"location":"KB/Declarative%20Memory%20Blending/#fit","title":"Fit","text":"<ul> <li>Exp done on generated data as well</li> <li>Compares if same as when run on original</li> <li>Does well with unmodified mode    </li> <li></li> </ul>"},{"location":"KB/Declarative%20memory/","title":"Declarative memory","text":""},{"location":"KB/Declarative%20memory/#declarative-memory","title":"Declarative Memory","text":""},{"location":"KB/Declarative%20memory/#info","title":"Info","text":"<ul> <li>All decisions are based on knowledge</li> <li>Depends on frequency and recency of use</li> <li>Semantic , episodic</li> <li>Representation similar to semantic networks</li> <li>No inheritence</li> <li>Partially sub-symbolic</li> <li>Related to Prefrontal cortex</li> </ul>"},{"location":"KB/Declarative%20memory/#programming","title":"Programming","text":"<ul> <li>ACT-R Chunk</li> </ul>"},{"location":"KB/DeconvNet/","title":"DeconvNet","text":""},{"location":"KB/DeconvNet/#deconvnet","title":"DeconvNet","text":"<p><code>toc</code> - @zeilerVisualizingUnderstandingConvolutional2013 - Zeiler, Fergus</p>"},{"location":"KB/DeconvNet/#summary","title":"Summary","text":"<ul> <li>Deconvnets are designed to work similar to convolutional networks but reverse (reversing pooling component, reversing filter component etc.), and they can be trained using an unsupervised approach.</li> <li>To reconstruct the activation on a specific layer, we are attaching deconv layers to corresponding CNN layers</li> <li>To examine a reconstruction for a given class c, we have to set all activations except the one responsible for predicting class c to zero.</li> <li>Then we can propagate through deconvnet layers and pass all the feature maps as inputs to corresponding layers</li> <li>Propagation through the whole deconvnet gives us a representation of the features from the first layer of the original CNN</li> <li>This approach causes the saliency map to feature some biases from the first convolutional layer and the representation looks like a localized edge detector</li> <li>works better when there is a clear distinction in the feature importance rather than similar values for the whole image</li> <li>Basically invert operations between input and the chosen layer.<ul> <li>Conv -&gt; Deconv</li> <li>Pool -&gt; Unpooling</li> <li>ReLU -&gt; ReLU with negative valyes clamped going backward from the activation space to image space</li> <li>Pooling is non invertible, but uses a switch module : recover positions of maxima in the forward pass</li> </ul> </li> <li>DeconvNet is a calculation of a backward convolutional network that reuses the weights at each layer from the output layer back to the input image</li> <li>The employed mechanisms are deconvolution and unpooling, which are especially designed for CNNs with convolutions, max-pooling, and Rectified Linear Units (ReLUs). The method makes it possible to create feature maps of an input image that activates certain hidden units most, linked to a particular prediction</li> <li>With their propagation technique, they identified the most responsible patterns for this output. The patterns are visualized in the input space</li> <li>DeconvNet is limited to max-pooling layers, but the unpooling uses an approximate inverse</li> </ul>"},{"location":"KB/DeconvNet/#filtering","title":"Filtering","text":"<ul> <li>Filtering in the original CNN computes feature maps using learned filters. Reversing that operation requires the use of a transposed version of the same filters. Those transposed filters are then applied to the Rectified Unpooled Maps.</li> </ul>"},{"location":"KB/DeconvNet/#rectification","title":"Rectification","text":"<ul> <li>same ReLU non-linearity</li> <li>simply just rectifying the values and propagate only non-negative ones to the filtering layer</li> </ul>"},{"location":"KB/DeconvNet/#unpooling","title":"Unpooling","text":"<ul> <li>The original max-pooling operation is non-invertible, but this approach uses additional variables called switch variables, which are responsible for remembering the locations of the maxima for each pooling region.</li> </ul>"},{"location":"KB/DeconvNet/#images","title":"Images","text":""},{"location":"KB/Deductive%20Approaches/","title":"Deductive Approaches","text":""},{"location":"KB/Deductive%20Approaches/#deductive-approaches","title":"Deductive Approaches","text":"<ul> <li>Settling on one hypothesis by eliminating all others</li> </ul>"},{"location":"KB/Deep%20Brain%20Stimulation/","title":"Deep Brain Stimulation","text":""},{"location":"KB/Deep%20Brain%20Stimulation/#deep-brain-stimulation","title":"Deep Brain Stimulation","text":"<ul> <li>A method of treating various neuropsychiatric and neurodegenerative disorders through small, controlled electric shocks administered from a special battery-operated neurostimulation implant. The implant, sometimes called a \u201cbrain pacemaker,\u201d is placed within deep brain regions such as the globus pallidus or subthalamus.</li> </ul>"},{"location":"KB/Deep%20Generative%20Models/","title":"Deep Generative Models","text":""},{"location":"KB/Deep%20Generative%20Models/#deep-generative-models","title":"Deep Generative Models","text":"<ul> <li>The ultimate goal of data augmentation is to draw samples from the distribution, which represent the generating mechanism of dataset </li> <li>the data distribution we generate data from should not be different with the original one </li> <li>This is the core idea of deep generative models. </li> <li>Pix2Pix </li> <li>CycleGAN </li> <li>StarGAN</li> <li>StarGAN v2</li> </ul>"},{"location":"KB/Deep%20Inside%20Convolutional%20Networks/","title":"Deep Inside Convolutional Networks","text":""},{"location":"KB/Deep%20Inside%20Convolutional%20Networks/#deep-inside-convolutional-networks","title":"Deep Inside Convolutional Networks","text":"<ul> <li>Karen Simonyan, Andrea Vedaldi, Andrew Zisserman</li> <li> <p>@simonyanDeepConvolutionalNetworks2014</p> </li> <li> <p>Because the word saliency is often related to the whole approach to display input attribution called Saliency Map, this method is also known as Vanilla Gradient</p> </li> <li>finding L2 regularized image III that maximizes score \\(S_{c}\\)\u200b for a given class c</li> <li>It can be written formally as:</li> <li> \\[arg \\underset{I}max S_{c}(I) - \\lambda||I||^{2}_{2}\\] </li> <li>Where \\(\\lambda\\) is a regularisation parameter</li> <li>To find the value of I, we can use the back-propagation method. Unlike in the standard learning process, we are going to back-propagate with respect to the input image, not the first convolution layer</li> </ul>"},{"location":"KB/Deep%20Inside%20Convolutional%20Networks/#from-class-visualization-to-saliency","title":"From class visualization to Saliency","text":"<ul> <li>This idea can be extrapolated, and with minor modifications, we should be able to query for spatial support of class ccc in a given image I0I_0I0\u200b.</li> <li>rank pixels of \\(I_{0}\\)\u200b in relation to their importance in predicting score \\(S_{c}(I_{0})\\)</li> <li>Authors assume that we can approximate \\(S_{c}(I)\\) with a linear function in the neighborhood of \\(I_{0}\\)</li> <li> \\[S_{c}(I) \\approx w^{T}I + b\\] </li> <li>For a pair of input image \\(\\(I_{0} \\in \\mathbb{R}^{m \\times n}\\)\\) and the class c, we are able to compute saliency map \\(A \\in \\mathbb{R}^{m \\times n}\\) (where m and n are the height and width of the input in pixels).</li> <li>compute derivative w and rearrange elements in the returned vector.</li> <li>uses different approaches base on the number of channels in the input image \\(I_{0}\\)\u200b.</li> <li>For grey-scale pixels (one color channel), we can rearrange the pixels to match the shape of the image</li> <li>If the number of channels is greater than one, we are going to use the maximum value from each set of values related to the specified pixel.</li> <li> \\[A_{i,j}= \\underset{ch}max |w_{h_{(i,j,ch)}}|\\] </li> <li>ch is a color channel of the pixel (i,j) and h(i,j,ch) is an index of the www corresponding to the same pixel (i,j).</li> <li>The original Saliency method produces a lot of additional noise but still gives us an idea of which part of the input image is relevant when predicting a specific class.</li> <li>This often causes a problem when the object on the image has a lot of details and the model is using most of them to make a prediction.</li> </ul>"},{"location":"KB/Deep%20Inside%20Convolutional%20Networks/#images","title":"Images","text":""},{"location":"KB/Deep%20Neural%20Networks%20are%20Easily%20Fooled%20High%20Confidence%20Predictions%20for%20Unrecognizable%20Images/","title":"Deep Neural Networks are Easily Fooled High Confidence Predictions for Unrecognizable Images","text":""},{"location":"KB/Deep%20Neural%20Networks%20are%20Easily%20Fooled%20High%20Confidence%20Predictions%20for%20Unrecognizable%20Images/#deep-neural-networks-are-easily-fooled-high-confidence-predictions-for-unrecognizable-images","title":"Deep Neural Networks Are Easily Fooled High Confidence Predictions for Unrecognizable Images","text":"<ul> <li>@nguyenDeepNeuralNetworks2015</li> <li>Current study<ul> <li>False positives</li> <li>MAP Elites algorithm </li> <li>parallel generation</li> <li>Direct encodings</li> <li>Indirect encodings</li> <li>Gradient ascent generation</li> </ul> </li> </ul>"},{"location":"KB/Deep%20Neural%20Networks%20are%20Easily%20Fooled%20High%20Confidence%20Predictions%20for%20Unrecognizable%20Images/#ga","title":"GA","text":"<ul> <li>Population of individuals </li> <li>Each individual has a fitness </li> <li>Mutation makes small edits to specific individuals </li> <li>Recombination (not used here)</li> </ul>"},{"location":"KB/Deep%20Neural%20Networks%20are%20Easily%20Fooled%20High%20Confidence%20Predictions%20for%20Unrecognizable%20Images/#direct-encoding","title":"Direct Encoding","text":"<ul> <li>Individuals are images in pixel space </li> <li>Fitness is the confidence of the DNN that the individual is a class </li> <li>Mutations make edits to the pixel values</li> </ul>"},{"location":"KB/Deep%20Neural%20Networks%20are%20Easily%20Fooled%20High%20Confidence%20Predictions%20for%20Unrecognizable%20Images/#indirect-encoding","title":"Indirect Encoding","text":"<ul> <li>Individuals are Compositional Pattern-Producing Networks (CPPNs) </li> <li>The CPPN generates an image </li> <li>All individuals initially have no hidden neurons </li> <li>Mutations add new neurons to the networks</li> <li>Maximize confidence of the network</li> </ul>"},{"location":"KB/Deep%20Neural%20Networks%20are%20Easily%20Fooled%20High%20Confidence%20Predictions%20for%20Unrecognizable%20Images/#gradient-ascent","title":"Gradient Ascent","text":"<ul> <li>Take the gradient with respect to the image pixel values </li> <li>Modify the image by moving it in the direction of the gradient</li> </ul>"},{"location":"KB/Deep%20Neural%20Networks%20are%20Easily%20Fooled%20High%20Confidence%20Predictions%20for%20Unrecognizable%20Images/#mnist-results-eas","title":"MNIST Results - EAs","text":""},{"location":"KB/Deep%20Neural%20Networks%20are%20Easily%20Fooled%20High%20Confidence%20Predictions%20for%20Unrecognizable%20Images/#imagenet-results-eas","title":"ImageNet Results - EAs","text":"<ul> <li>Harder to fool</li> <li>Different runs result into differences in patterns</li> <li>Removing repetitive patterns does not cause a dramatic confidence drop</li> <li>Global structures are not learned</li> <li></li> <li></li> <li></li> </ul>"},{"location":"KB/Deep%20Neural%20Networks%20are%20Easily%20Fooled%20High%20Confidence%20Predictions%20for%20Unrecognizable%20Images/#what-about-a-fooling-class","title":"What about a Fooling Class?","text":"<ul> <li>MNIST<ul> <li>Added an 11th fooling class. </li> <li>Evolved unrecognizable images were still recognized as digits. </li> <li>Number of misclassifications did not decrease.</li> </ul> </li> <li>ImageNet<ul> <li>Added an 1001st fooling class. </li> <li>No decrease in confidence for directly evolved images, but already low confidence. </li> <li>Confidence decreased from 88.1% to 11.7% for indirectly evolved images.</li> </ul> </li> <li>Indirectly evolved images are easier to differentiate.</li> </ul>"},{"location":"KB/Deep%20Neural%20Networks%20are%20Easily%20Fooled%20High%20Confidence%20Predictions%20for%20Unrecognizable%20Images/#gradient-ascent-results","title":"Gradient Ascent Results","text":"<ul> <li>Maximize softmax output</li> <li>Produced unrecognizable images classified with 99.99% confidence</li> </ul>"},{"location":"KB/Deep%20Visual%20Explanation/","title":"Deep Visual Explanation","text":""},{"location":"KB/Deep%20Visual%20Explanation/#deep-visual-explanation","title":"Deep Visual Explanation","text":"<ul> <li>@babikerIntroductionDeepVisual2018</li> <li>They captured the discriminative areas of the input image by considering the activation of high and low spatial scales in the Fourier space.</li> <li>\"Deep,\" because it is the development and performance of deep neural network models that we want to understand. \"Visual,\" because we believe that the most rapid insight into a complex multi-dimensional model is provided by appropriate visualization techniques, and \"Explanation,\" because in the spectrum from instrumentation by inserting print statements to the abductive inference of explanatory hypotheses, we believe that the key to understanding deep learning relies on the identification and exposure of hypotheses about the performance behavior of a learned deep model.</li> <li>Deep convolutional neural networks (DCNN) produce spatial information at the convolution layers</li> <li>loss of information makes the explanation process challenging, especially when it comes to interpreting the output of sensitive data such as medical images</li> <li>Our immediate goal is to create an explanation about the outcome of a DCNN, i.e., to identify which discriminative pixels in the image influence the final prediction</li> <li>To approach this task in this restricted context, we assume that the convolution feature maps X at pooling layer l contain some relevant information about class y</li> <li>We can then write our solution as: \\(D : I \\rightarrow y_{i} \\rightarrow S\\) i.e., map the input I to class \\(y_{i}\\) using network D, and compute the evidence/explanation S</li> <li>output. So to explain \\(y_{i} \\rightarrow S\\), we can compute the low-spatial scale and high-spatial scale activations of every feature map</li> </ul>"},{"location":"KB/Deep%20Visual%20Explanation/#visual-explanation","title":"Visual Explanation","text":"<ul> <li>Explanaton in Fourier domain. Function \\(F(x)\\) represents transform with \\(x \\in \\mathbb{R}\\) , \\(x\\) is a feature map at a conv layer</li> <li>For every \\(x_{i} \\in X\\) of size \\(M \\times N\\) , the transform  can be written as $$ F(u,v) = \\Sigma_{k=0}^{M-1} \\Sigma_{j=0}^{N-1}f(k,j)e^{-i2\\pi(\\frac{uk}{M}+ \\frac{vj}{N})} $$<ul> <li>\\(f(k,j)\\) is feature map at layer \\(l\\)</li> <li>exp term is the basis function</li> <li>inverse of fourier is $$ f(m,n) = \\frac{1}{M\\times N} \\Sigma_{u=0}^{M-1} \\Sigma_{v=0}^{N-1}F(u,v)e^{i2\\pi(\\frac{ux}{M}+ \\frac{vy}{N})} $$</li> </ul> </li> <li>For every feature map, \\(x_{i} \\in X\\), visual explanation is $$ S = \\Sigma_{i=1}F^{-1}(F(x_{i} * G_{1}) * F^{-1}(F(x_{i})*(1-G_{2}))) $$<ul> <li>\\(G_{1}, G_{2}\\) are Guassians at different \\(\\sigma\\) values,</li> <li>Low spatial scale activation \\(F(x_{i})*G_{1}\\)</li> <li>High scale activation \\(F^{-1}(F(x_{i})*(1-G_{2}))\\)</li> </ul> </li> <li></li> </ul>"},{"location":"KB/Deep%20Visual%20Explanation/#targeted-deep-visual-explanation","title":"Targeted Deep Visual Explanation","text":"<ul> <li>In our simple case of image classification (cf. speech, language) one of the ultimate goals of the visual explanation in the context of debugging is to be precise when determining the component salient patch.</li> <li>Therefore, we should penalize any activations that do not contribute much</li> <li>To handle this, we propose a method called targeted-DVE to provide a more targeted explanation. This algorithm removes any pixel that has less influence on the best explanation</li> <li>The process is identical to our previous approach except that, we slightly modify the final output S obtained in Algorithm 1. This is done, by computing S0 as follows</li> <li> \\[S' = F^{\u22121}(F(S) * G_{1}) * F^{\u22121}(F(S * (1 \u2212 G_{2})) \\] </li> <li>Our approach captures the discriminative pixels by considering the activation of high and low spatial scales in Fourier space.</li> <li>We experimented with a simple version of our approach on image classification.</li> </ul>"},{"location":"KB/Deep%20Visual%20Explanation/#images","title":"Images","text":""},{"location":"KB/DeepFM/","title":"DeepFM","text":""},{"location":"KB/DeepFM/#deepfm","title":"DeepFM","text":"<ul> <li>DeepFM: a Factorization-Machine Based Neural Network for CTR Prediction</li> <li>Learning sophisticated feature interactions behind user behaviors is critical in maximizing CTR for recommender systems</li> <li>existing methods seem to have a strong bias towards low- or high-order interactions, or require expertise feature engineering</li> <li>an end-to-end learning model that emphasizes both low- and high-order feature interactions</li> <li>DeepFM is a Factorization-Machine (FM) based Neural Network for CTR prediction, to overcome the shortcomings of the state-of-the-art models and to achieve better performance.</li> <li>DeepFM trains a deep component and an FM component jointly and models low-order feature interactions through FM and models high-order feature interactions through the DNN</li> <li>DeepFM can be trained end-to-end with a shared input to its \u201cwide\u201d and \u201cdeep\u201d parts, with no need of feature engineering besides raw features.</li> <li>1) it does not need any pre-training; 2) it learns both high- and low-order feature interactions; 3) it introduces a sharing strategy of feature embedding to avoid feature engineering</li> <li>combines the power of factorization machines for recommendation and deep learning for feature learning in a new neural network architecture</li> <li>Criteo</li> </ul>"},{"location":"KB/DeepFool/","title":"DeepFool","text":"<ul> <li><code>toc</code></li> <li>@DeepFool: A Simple and Accurate Method to Fool Deep Neural Networks</li> <li> </li> </ul>"},{"location":"KB/DeepFool/#deepfool","title":"DeepFool","text":""},{"location":"KB/DeepFool/#_1","title":"DeepFool","text":""},{"location":"KB/DeepLIFT/","title":"DeepLIFT","text":""},{"location":"KB/DeepLIFT/#deeplift","title":"DeepLIFT","text":"<ul> <li>@liDeepLIFTDeepLabelSpecific2022</li> </ul>"},{"location":"KB/DeepLearning/","title":"Index","text":"<ul> <li>Features</li> <li>Fundamentals</li> <li>Issues</li> <li>Layers</li> <li>Architectures</li> <li>Optimizers</li> <li>Regularization</li> <li>LossFunctions</li> <li>Activation Functions</li> <li>Initialization</li> <li>Augmentation</li> <li>Uncertainty</li> <li>Optimizing Code</li> <li>Useful Codes</li> <li>Federated Learning</li> <li>Reinforcement Learning</li> <li>Refs</li> </ul> <p>#anchor</p>"},{"location":"KB/DeepNet/","title":"DeepNet","text":""},{"location":"KB/DeepNet/#deepnet","title":"DeepNet","text":"<ul> <li>DeepNet: Scaling Transformers to 1,000 Layers</li> <li>allows train extremely deep transformers with 1000L+ layers</li> <li>fundamental, effective and simple</li> <li>can be used in any Transformer architecture (encoder, decoder, encoder-decoder) which covers almost all different tasks across AI areas (language, vision, speech, multimodal, and beyond)</li> <li>newly proposed normalization function</li> <li>DeepNorm</li> <li>It works alongside a dedicated initialization scheme based on Xavier initialization.</li> <li>These two tricks lead to greater stability during the training which allows the authors to scale their modified Transformer architecture (DeepNet) up to 1000 layers</li> </ul>"},{"location":"KB/DeepNorm/","title":"DeepNorm","text":""},{"location":"KB/DeepNorm/#deepnorm","title":"DeepNorm","text":"<ul> <li>which modifies the residual connection in Transformers</li> <li>theoretical justification of bounding the model update by a constant which makes stable training possible in a principled way</li> <li>DeepNorm modifies the residual connection in the Transformer architecture by up-scaling it before performing layer normalization</li> </ul>"},{"location":"KB/DeepPERF/","title":"DeepPERF","text":""},{"location":"KB/DeepPERF/#deepperf","title":"DeepPERF","text":"<ul> <li>DeepPERF: a Deep Learning-Based Approach for Improving Software Performance</li> <li>Performance bugs may not cause system failure and may depend on user input, so detecting them can be challenging</li> <li>harder to fix than non-performance bugs</li> <li>performance bug detection approaches have emerged to help developers identify performance issues</li> <li>Building rule-based analyzers is a non-trivial task, as it requires achieving the right balance between precision and KB/Recall.md</li> <li>Once developed, maintaining these rules can also be costly</li> <li>large transformer model to suggest changes at application source code level to improve its performance</li> <li>first pretrain the model using masked language modelling (MLM) tasks on English text and source code taken from open source repositories on GitHub, followed by finetuning on millions of performance commits made by .NET developers</li> <li>recommend patches to provide a wide-range of performance optimizations in C<code>#</code> applications</li> <li>Most suggested changes involve modifications to high-level constructs like API/Data Structure usages or other algorithmic changes, often spanning multiple methods, which cannot be optimized away automatically by the C<code>#</code> compiler and could, therefore, lead to slow-downs on the user\u2019s side</li> </ul>"},{"location":"KB/Default%20mode%20network/","title":"Default mode network","text":"<p>toc: true title: Default mode network</p> <p>categories: ['temp']</p>"},{"location":"KB/Default%20mode%20network/#default-mode-network","title":"Default Mode Network","text":"<ul> <li>Brain is organized into coherent spatio-temporal networks such as this one</li> <li>Brain areas in the Brain Cortex that constantly decreased their activity while performing highly demanding task</li> <li>Some studies revealed task-induced activations in the DMN, e.g. when internally directed/self-related cognition is required</li> <li>Altered with addictions</li> <li>Functional connectivity within DMN may predict successful quitting, the intensity of withdrawal-induced craving and the degree of cognitive decline in addictions</li> <li>They ^1 (as key node of the cognitive control network) and the Anterior Cingulate/Prefrontal cortex (as key nodes of the DMN)</li> </ul> <p>^1 associated with individual differences in Internet tendency in healthy young adults , Neuropsychologia</p>"},{"location":"KB/Defibrillator/","title":"Defibrillator","text":""},{"location":"KB/Defibrillator/#defibrillator","title":"Defibrillator","text":"<ul> <li>A device that discharges an electric current to the heart to correct cardiac arrhythmia or arrest</li> </ul>"},{"location":"KB/Degrees%20of%20Freedom/","title":"Degrees of Freedom","text":""},{"location":"KB/Degrees%20of%20Freedom/#degrees-of-freedom","title":"Degrees of Freedom","text":"<ul> <li>The number of independent directions or joints of the robot (R15.07), which would allow the robot to move its end effector through the required sequence of motions. For arbitrary positioning, 6 degrees of freedom are needed: 3 for position (left-right, forward-backward and up- down), and 3 for orientation (yaw, pitch and roll).</li> </ul>"},{"location":"KB/DeiT/","title":"DeiT","text":"<p>toc: true title: DeiT</p> <p>categories: ['temp']</p>"},{"location":"KB/DeiT/#deit","title":"DeiT","text":"<ul> <li>paper</li> <li>blog</li> <li>Conv free Transformer, Vision Transformer</li> <li>does not require very large amount of data   id:: 62a8a66a-941e-4a6d-918a-bb49cd496b15</li> <li>Knowledge Distillation</li> <li>teacher-student strategy specific to transformers</li> <li>Distillation Token</li> <li>ConvNet as teacher through Attention   id:: 62a8a6b2-abf4-4869-934e-c75d05884304</li> <li>ImageNet</li> <li> </li> </ul>"},{"location":"KB/DeiT/#begin_caution","title":"+BEGIN_CAUTION","text":"Heh. Didnt they say no convs?   #+END_CAUTION"},{"location":"KB/Delta%20Waves/","title":"Delta Waves","text":""},{"location":"KB/Delta%20Waves/#delta-waves","title":"Delta Waves","text":"<ul> <li>2-4 Hz</li> <li>sleep</li> <li></li> </ul>"},{"location":"KB/Demographic%20Parity/","title":"Demographic Parity","text":""},{"location":"KB/Demographic%20Parity/#demographic-parity","title":"Demographic Parity","text":"<ul> <li>A fairness metric that is satisfied if the results of a model's classification are not dependent on a given sensitive attribute.</li> <li>For example, if both Lilliputians and Brobdingnagians apply to Glubbdubdrib University, demographic parity is achieved if the percentage of Lilliputians admitted is the same as the percentage of Brobdingnagians admitted, irrespective of whether one group is on average more qualified than the other.</li> <li>Contrast with equalized odds and equality of opportunity, which permit classification results in aggregate to depend on sensitive attributes, but do not permit classification results for certain specified ground-truth labels to depend on sensitive attributes. See \"Attacking discrimination with smarter machine learning\" for a visualization exploring the tradeoffs when optimizing for demographic parity.</li> </ul>"},{"location":"KB/Dendrites/","title":"Dendrites","text":""},{"location":"KB/Dendrites/#dendrites","title":"Dendrites","text":"<ul> <li>Short nerve fibers that project from a neuron, generally receiving messages from the axons of other neurons and relaying them to the cell\u2019s nucleus.</li> </ul>"},{"location":"KB/Denoising%20Autoencoder/","title":"Denoising Autoencoder","text":""},{"location":"KB/Denoising%20Autoencoder/#denoising-autoencoder","title":"Denoising Autoencoder","text":"<ul> <li>Corrupt inputs with noise</li> <li>Salt pepper noise</li> <li>Normal Distribution</li> <li>Compare outputs to clean inputs</li> <li>$\\(L(X) = n^{-1}\\Sigma_i||x_i - D(E(\\tilde x))||^2\\)</li> </ul>"},{"location":"KB/Dense%20Net/","title":"Dense Net","text":""},{"location":"KB/Dense%20Net/#dense-net","title":"Dense Net","text":"<ul> <li>Generalized Res Net</li> <li>Skip connections inside the Dense block itself</li> <li>![im](images/Dense Skip Connections]</li> <li>Transition layer -&gt; Dense -&gt; 1x1 Conv , 2x2 avg pool -&gt; Dense</li> </ul>"},{"location":"KB/Dense%20Skip%20Connections/","title":"Dense Skip Connections","text":""},{"location":"KB/Dense%20Skip%20Connections/#dense-skip-connections","title":"Dense Skip Connections","text":"<ul> <li> \\[x_i = F(x_0,x_1 ,\u2026 ,x_{i-1})\\] <ul> <li>F : 3x3 Conv + Relu -&gt; k feature maps</li> <li>no of feature maps : \\(\\(k(i-1) + k_0\\)\\) where k is growth rate (hyperparam)</li> </ul> </li> <li>Skip Connection</li> </ul>"},{"location":"KB/Dense%20Vector%20Indexes/","title":"Dense Vector Indexes","text":""},{"location":"KB/Dense%20Vector%20Indexes/#dense-vector-indexes","title":"Dense Vector Indexes","text":"<ul> <li> <p>advanced dense vector indexing techniques to capture the semantic meaning and context of documents more effectively than traditional keyword-based methods in the enterprise knowledge base</p> </li> <li> <p>HNSW</p> </li> <li> <p>PQ</p> </li> <li> <p>IVFADC</p> </li> <li> <p>Sparse Encoder Indexes</p> </li> </ul>"},{"location":"KB/Dense/","title":"Dense","text":""},{"location":"KB/Dense/#dense","title":"Dense","text":"<ul> <li>Weighted LinearRegression</li> <li>Forward<ul> <li> \\[z = W\\cdot x + b$$ , $$y=g(z)\\] </li> </ul> </li> <li>Backward<ul> <li> \\[\\delta = g'(z)\\circ \\nabla_y E\\] </li> <li> \\[\\nabla_WE = \\delta \\cdot x^T$$ , $$\\nabla_bE = \\delta\\] </li> <li> \\[\\nabla_xE = W^T\\cdot \\delta\\] </li> </ul> </li> </ul>"},{"location":"KB/Density/","title":"Density","text":""},{"location":"KB/Density/#density","title":"Density","text":"<ul> <li>mass / vol</li> <li> \\[r - m/V\\] </li> </ul>"},{"location":"KB/Deontological%20ethics/","title":"Deontological ethics","text":""},{"location":"KB/Deontological%20ethics/#deontological-ethics","title":"Deontological Ethics","text":"<ul> <li>an agent is ethical if and only if it respects obligations, duties and rights related to given situations</li> <li>act in accordance to established social norms</li> </ul>"},{"location":"KB/Depth%20Efficiency%20of%20Neural%20Networks/","title":"Depth Efficiency of Neural Networks","text":""},{"location":"KB/Depth%20Efficiency%20of%20Neural%20Networks/#depth-efficiency-of-neural-networks","title":"Depth Efficiency of Neural Networks","text":"<ul> <li>there are functions that can be realized by deep networks but not by any shallow network whose capacity is bounded above exponentially.</li> <li>it would take an exponentially larger number of units in a shallow network to describe these functions accurately</li> <li>Eldan &amp; Shamir (2016) showed that when there are multivariate inputs, there is a three-layer network that cannot be realized by any two-layer network if the capacity is sub-exponential in the input dimension.</li> <li>Liang &amp; Srikant (2016) show that for a broad class of functions, including univariate functions, shallow networks require exponentially more hidden units than deep networks for a given upper bound on the approximation error</li> </ul>"},{"location":"KB/Depthwise%20Separable/","title":"Depthwise Separable","text":""},{"location":"KB/Depthwise%20Separable/#depthwise-separable","title":"Depthwise Separable","text":"<ul> <li>Only transforms the input once and saves computation -&gt; elongate it to more channels</li> <li>From C -&gt; F channels : Use F instances of a 1x1xC filter</li> <li></li> </ul>"},{"location":"KB/Derivational%20Morphology/","title":"Derivational Morphology","text":""},{"location":"KB/Derivational%20Morphology/#derivational-morphology","title":"Derivational Morphology","text":"<ul> <li>creates new word by changing the POS tag</li> </ul>"},{"location":"KB/Detailed%20Balance/","title":"Detailed Balance","text":""},{"location":"KB/Detailed%20Balance/#detailed-balance","title":"Detailed Balance","text":"<ul> <li>To find a transition kernel T(x|y) for a homogenous, Ergodic Markov Chain</li> <li>If we pick some state x with the Probability given by g and multiply its prob g(x) with the transition Probability density T(x|y) (weighted by Probability density of x) then its the same as the reverse weighted transiting Probability density from y to x</li> <li> \\[\\forall x,y \\in \\mathbb{R}^{k}: T(y|x)g(x) = T(x|y)g(y)\\] </li> <li>If T(x|y) has detailed balance wrt g, then it is an Invariant Distribution</li> <li> \\[\\int_{\\mathbb{R}^{k}}T(x|y)g(y)dy = \\int_{\\mathbb{R}^{k}}T(y|x)g(x)dy = g(x)\\int_{\\mathbb{R}^{k}}P(y|x)dy = g(x)\\] </li> </ul>"},{"location":"KB/Determiners/","title":"Determiners","text":""},{"location":"KB/Determiners/#determiners","title":"Determiners","text":"<ul> <li>indicate specific object (a, the,that)</li> </ul>"},{"location":"KB/DevOn%20AI%20dev/","title":"DevOn AI dev","text":""},{"location":"KB/DevOn%20AI%20dev/#devon-ai-dev","title":"DevOn AI Dev","text":"<p>As the days go by, Machine Learning and AI are slowly becoming terms that every company wants to have in their portfolio. While this drive leads to many innovations, most companies are not sure how to do AI \"well\". They want to use AI but need clarification on whether it is required, how to handle bias, how to create proper data, or even what models (ML vs. DL) to choose. The job of an AI dev, then, is to provide the key information required to find and fulfill KPIs given any project. </p> <p>My expertise is a combination of data analytics and computer vision, and as of a month ago, I also have a Master in Artificial Intelligence from the University of Groningen. I am familiar with the tools required for basic and advanced AI and analytics from internships, research projects, papers, freelance work, and many personal projects. That being the case, I can step aside from my experience with AI and decide if a project needs another solution in reality. I am familiar with PyTorch, Tensorflow, supervised and unsupervised learning, data preprocessing, and many of the other tools that are required to create successful AI applications. I am familiar with the pipeline, from analyzing data to building the model to deployment.</p> <p>The customer is king, they say, and the first step in having happy clients is understanding what they truly want and then being able to give them a solution they can use. Although I have a lot to learn, I can contribute to any team I get the chance to work with. This job would be the perfect next step for me, and so I hope you give me a chance to work with you and your team.</p>"},{"location":"KB/DiTransitive%20verb/","title":"DiTransitive verb","text":""},{"location":"KB/DiTransitive%20verb/#ditransitive-verb","title":"DiTransitive Verb","text":"<ul> <li>a verb has two noun objects</li> <li>I cooked a duck for her</li> </ul>"},{"location":"KB/Dialyser/","title":"Dialyser","text":""},{"location":"KB/Dialyser/#dialyser","title":"Dialyser","text":"<ul> <li>A machine that replaces the function of the kidneys by removing solutes, excess water and toxins from the blood</li> </ul>"},{"location":"KB/Dialysis/","title":"Dialysis","text":""},{"location":"KB/Dialysis/#dialysis","title":"Dialysis","text":"<ul> <li>Process to filter the blood, usually performed as a result of kidney failure</li> </ul>"},{"location":"KB/Dice%20Score/","title":"Dice Score","text":""},{"location":"KB/Dice%20Score/#dice-score","title":"Dice Score","text":"<ul> <li>2 * the Area of Overlap divided by the total number of pixels in both images</li> </ul>"},{"location":"KB/Dictionary%20Learning/","title":"Dictionary Learning","text":""},{"location":"KB/Dictionary%20Learning/#dictionary-learning","title":"Dictionary Learning","text":"<ul> <li>Given : N unlabeled data points \\(\\(x_i \\in \\mathcal{R}^d\\)\\)</li> <li>To find:<ul> <li>Linear rep of these points based on set of basis vectors<ul> <li> \\[x = \\Sigma_i^k r_i \\cdot d_i = Dr\\] </li> <li>dimension d</li> <li>r are repr weights corresponding to basis vector d</li> <li>D is a dict with basis vectors</li> <li>R contains weights. Scalar</li> <li>$\\(||d_i|| \\leq 1\\)</li> </ul> </li> </ul> </li> <li>Sparse Dictionary Learning Loss</li> <li>After learning, these can be used as discriminative features<ul> <li>Expensive to compute</li> </ul> </li> </ul>"},{"location":"KB/Diffusion%20LM/","title":"Diffusion LM","text":""},{"location":"KB/Diffusion%20LM/#diffusion-lm","title":"Diffusion LM","text":"<ul> <li>Diffusion-LM Improves Controllable Text Generation</li> <li>Controlling the behavior of language models (LMs) without re-training is a major open problem in natural language generation</li> <li>non-KB/Autoregressive.md language model based on continuous diffusions</li> <li>substantial departure from the current paradigm of discrete KB/Autoregressive.md generation</li> <li>iteratively denoises a sequence of Gaussian vectors into word vectors, yielding a sequence of intermediate latent variables</li> <li>continuous, hierarchical nature of these intermediate variables enables a simple gradient-based algorithm to perform complex, controllable generation tasks</li> <li>successful control of Diffusion-LM for six challenging fine-grained control tasks</li> </ul>"},{"location":"KB/Diffusion%20Tensor/","title":"Diffusion Tensor","text":""},{"location":"KB/Diffusion%20Tensor/#diffusion-tensor","title":"Diffusion Tensor","text":""},{"location":"KB/Digital%20Phenotyping/","title":"Digital Phenotyping","text":""},{"location":"KB/Digital%20Phenotyping/#digital-phenotyping","title":"Digital Phenotyping","text":"<ul> <li>The use of data collected from personal electronic devices like smart phones to diagnose and monitor medical and psychiatric conditions.</li> </ul>"},{"location":"KB/Dikes%20and%20Rivers/","title":"Dikes and Rivers","text":""},{"location":"KB/Dikes%20and%20Rivers/#dikes-and-rivers","title":"Dikes and Rivers","text":"<ul> <li>Subjects alternate producing time intervals of a short and a long duration.  </li> <li>Initially, short is 2 seconds and long is 3.1 seconds</li> <li>They receive feedback on whether their estimate is within +/- 12.5% of the target, and receive \u201ctoo short\u201d or \u201ctoo long\u201d as feedback otherwise.</li> <li>After a number of trials, the criterion for the long interval starts to change</li> <li></li> <li>Short intervals are also effected and vice versa</li> </ul>"},{"location":"KB/Dikes%20and%20Rivers/#model","title":"Model","text":"<ul> <li>ACTR declarative memory + Declarative Memory Blending</li> </ul>"},{"location":"KB/Dilated%20Sliding%20Window%20Attention/","title":"Dilated Sliding Window Attention","text":""},{"location":"KB/Dilated%20Sliding%20Window%20Attention/#dilated-sliding-window-attention","title":"Dilated Sliding Window Attention","text":"<ul> <li>Analgous to dilated CNN</li> <li>Assuming a fixed \\(d\\) and \\(w\\) for all layers, receptive field is \\(l \\times d \\times w\\) which can reach tens of thousands of tokens even with small values of \\(d\\)</li> <li></li> </ul>"},{"location":"KB/Dimensionality%20Reduction/","title":"Dimensionality Reduction","text":""},{"location":"KB/Dimensionality%20Reduction/#dimensionality-reduction","title":"Dimensionality Reduction","text":"<ul> <li>Given<ul> <li>\\(\\((x_i)_{i = 1, \u2026,N}\\)\\) raw data points, \\(\\(x_i \\in \\mathbb{R}^n\\)\\) : High dim</li> </ul> </li> <li>To get<ul> <li>Low dim \\(\\(x_i \\in \\mathbb{R}^m\\)\\) where \\(\\(m &lt;n\\)\\)</li> </ul> </li> <li>f(x) is composed of m component functions aka features<ul> <li>\\(\\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\)\\) : scalar characteristic</li> <li>m such features : feature map<ul> <li> \\[(f_1 , \u2026, f_m)' =: f : \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\] </li> <li>maps input vectors to feature vectors</li> </ul> </li> </ul> </li> <li>KMeans</li> <li>PCA</li> <li>SOMs</li> </ul>"},{"location":"KB/Dimensionality%20Reduction/#anchor","title":"anchor","text":""},{"location":"KB/Dirac%20Delta/","title":"Dirac Delta","text":""},{"location":"KB/Dirac%20Delta/#dirac-delta","title":"Dirac Delta","text":"<p>-\\(\\(P(X \\in A) = \\begin{cases}1&amp; \\text{if 0}\\in A\\\\[2ex] 0&amp; \\text{if } 0 \\notin A \\end{cases}\\)\\)</p> <ul> <li> \\[P(X \\in A) = \\int_{A}\\delta(x)dx\\] </li> </ul>"},{"location":"KB/Dirac%20Delta/#in-mathbbrn","title":"In \\(\\mathbb{R}^{n}\\)","text":"<ul> <li>it is a PDF which describes prob concentrated in the origin</li> <li>Multi Point Distribution -&gt; combine dirac deltas</li> <li></li> </ul>"},{"location":"KB/Direct%20entropy%20minimization/","title":"Direct entropy minimization","text":""},{"location":"KB/Direct%20entropy%20minimization/#direct-entropy-minimization","title":"Direct Entropy Minimization","text":"<ul> <li>On the source domain we train our model,</li> <li>as usual using a supervised loss</li> <li>For the target domain, we do not have annotations and we can no longer use the segmentation loss to train</li> <li>supervision signal that could leverage visual information from the target samples, in spite of the lack of annotations</li> <li>constrain</li> <li>to produce high-confident predictions on target samples similarly to source samples</li> <li>entropy loss \\(\\mathcal{L}_{ent}\\) to maximize directly the prediction confidence in the target domain.</li> <li>Shannon Entropy</li> </ul>"},{"location":"KB/Direct-drive/","title":"Direct drive","text":""},{"location":"KB/Direct-drive/#direct-drive","title":"Direct-drive","text":"<ul> <li>Joint actuation, including no transmission elements (i.e., the link is bolted onto the output of the motor.)</li> </ul>"},{"location":"KB/Dirichlet%20Distribution/","title":"Dirichlet Distribution","text":""},{"location":"KB/Dirichlet%20Distribution/#dirichlet-distribution","title":"Dirichlet Distribution","text":"<ul> <li>PDF</li> <li> \\[h(\\theta|\\alpha) = \\frac{1}{Z(\\alpha)} \\Pi_{j=1}^{l}\\theta_{j}^{a_{j}-1}\\] </li> <li>\\(\\(Z(\\alpha) = \\int_{\\mathcal{H}}\\Pi_{j=1}^{l}\\theta_{j}^{\\alpha_{j}-1}d\\theta\\)\\) is normalization constant. Ensures integral of h over \\(\\mathcal{H}\\) is 1</li> </ul>"},{"location":"KB/Discrete%20-%3E%20Continuous/","title":"Discrete -> Continous Transforms","text":""},{"location":"KB/Discrete%20-%3E%20Continuous/#discrete-continous-transforms","title":"Discrete -&gt; Continous Transforms","text":""},{"location":"KB/Discrete%20-%3E%20Continuous/#one-hot","title":"One hot","text":""},{"location":"KB/Discrete%20-%3E%20Continuous/#binary-pattern","title":"Binary pattern","text":""},{"location":"KB/Discrete%20-%3E%20Continuous/#linear-scale","title":"Linear scale","text":""},{"location":"KB/Discrete%20-%3E%20Continuous/#word-vectors","title":"Word Vectors","text":""},{"location":"KB/Discrete%20Cosine%20Transform/","title":"Discrete Cosine Transform","text":""},{"location":"KB/Discrete%20Cosine%20Transform/#discrete-cosine-transform","title":"Discrete Cosine Transform","text":"<ul> <li>machine-learning-articles/cnns-and-feature-extraction-the-curse-of-data-sparsity.md at main \u00b7 christianversloot/machine-learning-articles #Roam-Highlights</li> <li>expresses a finite sequence of data points in terms of a sum of cosine functions oscillating at different frequencies<ul> <li>you make the CNN blind to the unique aspects represented by the numbers\u2026 despite the fact that they are already in there</li> <li>In my opinion, this can be explained by looking at the internals of a convolutional layer. It works as follows. You specify a number of filters which, during training, learn to recognize unique aspects of the image-like data. They can then be used to classify new samples - quite accurately, as we have seen with raw MNIST data. This means that the convolutional layer already makes your data representation sparser. What's more, this effect gets even stronger when layers like Pooling are applied</li> <li>But when you downsample the data first by e.g. applying the DCT, you thus effectively apply sparsening twice. My only conclusion can thus be that by consequence, the convolutional filters can no longer learn the unique aspects within the image-like data, as they are hidden in the data set made compact. Only then, I literally found out why people always suggest to input your image data into CNNs as untransformed as possible.</li> <li>Besides the architectural differences between them, one must also conclude that CNNs make data essentially sparser while SVMs do not.</li> </ul> </li> </ul>"},{"location":"KB/Disparate%20Impact/","title":"Disparate Impact","text":""},{"location":"KB/Disparate%20Impact/#disparate-impact","title":"Disparate Impact","text":"<ul> <li>Making decisions about people that impact different population subgroups disproportionately. This usually refers to situations where an algorithmic decision-making process harms or benefits some subgroups more than others.</li> <li>For example, suppose an algorithm that determines a Lilliputian's eligibility for a miniature-home loan is more likely to classify them as \u201cineligible\u201d if their mailing address contains a certain postal code. If Big-Endian Lilliputians are more likely to have mailing addresses with this postal code than Little-Endian Lilliputians, then this algorithm may result in disparate impact.</li> </ul>"},{"location":"KB/Disparate%20Treatment/","title":"Disparate Treatment","text":""},{"location":"KB/Disparate%20Treatment/#disparate-treatment","title":"Disparate Treatment","text":"<ul> <li>Factoring subjects' sensitive attributes into an algorithmic decision-making process such that different subgroups of people are treated differently.</li> <li>For example, consider an algorithm that determines Lilliputians\u2019 eligibility for a miniature-home loan based on the data they provide in their loan application. If the algorithm uses a Lilliputian\u2019s affiliation as Big-Endian or Little-Endian as an input, it is enacting disparate treatment along that dimension.</li> <li>Contrast with disparate impact, which focuses on disparities in the societal impacts of algorithmic decisions on subgroups, irrespective of whether those subgroups are inputs to the model.</li> <li>Because sensitive attributes are almost always correlated with other features the data may have, explicitly removing sensitive attribute information does not guarantee that subgroups will be treated equally. For example, removing sensitive demographic attributes from a training data set that still includes postal code as a feature may address disparate treatment of subgroups, but there still might be disparate impact upon these groups because postal code might serve as a proxy for other demographic information.</li> </ul>"},{"location":"KB/Displacement/","title":"Displacement","text":""},{"location":"KB/Displacement/#displacement","title":"Displacement","text":"<ul> <li> \\[\\Delta x = x_{f}-x_{i}\\] </li> </ul>"},{"location":"KB/Distance%20Measures/","title":"Distance Measures","text":""},{"location":"KB/Distance%20Measures/#distance-measures","title":"Distance Measures","text":"<ul> <li>Euclidean Distance</li> <li>Cosine Similarity</li> <li>Hamming Distance</li> <li>Manhattan Distance</li> <li>Chebyshev Distance</li> <li>Hausdorff Distance</li> <li>Chi Squared Distance</li> <li>Bhattacharya Distance</li> <li>Minkowski Distance</li> <li>Jaccard Distance</li> <li>Haversine Distance</li> <li>S\u00f8rensen-Dice Index</li> </ul>"},{"location":"KB/DistillBERT/","title":"DistillBERT","text":""},{"location":"KB/DistillBERT/#distillbert","title":"DistillBERT","text":"<ul> <li>DistilBERT, a Distilled Version of BERT: Smaller, Faster, Cheaper and Lighter</li> <li>Huggingface</li> <li>general-purpose pre-trained version of BERT</li> <li>40% smaller, 60% faster, cheaper to pre-train, and retains 97% of the language understanding capabilities</li> <li>knowledge distillation during the pre-training phase</li> <li>triple loss combining language modeling, distillation and cosine-distance losses</li> </ul>"},{"location":"KB/Distillation%20Algorithms/","title":"Distillation Algorithms","text":""},{"location":"KB/Distillation%20Algorithms/#distillation-algorithms","title":"Distillation Algorithms","text":"<ul> <li>Adversarial Distillation</li> <li>Multi Teacher Distillation</li> <li>Cross Modal Distillation</li> <li>Graph Based Distillation</li> <li>Attention Based Distillation</li> <li>Data Free Distillation</li> <li>Quantized Distillation</li> </ul>"},{"location":"KB/Distillation%20Loss/","title":"Distillation Loss","text":""},{"location":"KB/Distillation%20Loss/#distillation-loss","title":"Distillation Loss","text":"<ul> <li> \\[\\mathscr{l}(p, softmax(z))+T^{2}\\mathscr{l}(softmax(\\frac{r}{T}), softmax(\\frac{z}{T}))\\] </li> <li>Negative Cross Entropy + other</li> <li>p is the true probability Distributions</li> <li>z,r are outputs of the student and teacher model</li> <li>T is the temperature to make Softmax smoother</li> </ul>"},{"location":"KB/Distillation%20Schemes/","title":"Distillation Schemes","text":""},{"location":"KB/Distillation%20Schemes/#distillation-schemes","title":"Distillation Schemes","text":"<ul> <li>Offline Distillation</li> <li>Self Distillation</li> </ul>"},{"location":"KB/Distillation%20Token/","title":"Distillation Token","text":"<p>toc: true title: Distillation Token</p> <p>categories: ['temp']</p>"},{"location":"KB/Distillation%20Token/#distillation-token","title":"Distillation Token","text":"<ul> <li>A learned vector that flows through the network along with the transformed image data</li> <li>cues the model for its distillation output, which can differ from its class output</li> <li>Specific to Transformers</li> </ul>"},{"location":"KB/Distilling%20the%20Knowledge%20in%20a%20Neural%20Network/","title":"Distilling the Knowledge in a Neural Network","text":""},{"location":"KB/Distilling%20the%20Knowledge%20in%20a%20Neural%20Network/#distilling-the-knowledge-in-a-neural-network","title":"Distilling the Knowledge in a Neural Network","text":"<ul> <li>Geoffrey Hinton, Oriol Vinyals, Jeff Dean</li> </ul>"},{"location":"KB/Distilling%20the%20Knowledge%20in%20a%20Neural%20Network/#intro","title":"Intro","text":"<ul> <li>compress the knowledge in an ensemble into a single model which is much easier to deploy</li> <li>new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse</li> <li>Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel</li> <li>Many insects have a larval form that is optimized for extracting energy and nutrients from the environment and a completely different adult form that is optimized for the very different requirements of traveling and reproduction</li> <li>training must extract structure from very large, highly redundant datasets but it does not need to operate in real time and it can use a huge amount of computation. Deployment to a large number of users, however, has much more stringent requirements on latency and computational resources.</li> <li>The cumbersome model could be an ensemble of separately trained models or a single very large model trained with a very strong regularizer such as KB/Dropout.md</li> <li>we tend to identify the knowledge in a trained model with the learned parameter values and this makes it hard to see how we can change the form of the model but keep the same knowledge.</li> <li>learned</li> <li>where T is a temperature that is normally set to 1</li> </ul>"},{"location":"KB/Distilling%20the%20Knowledge%20in%20a%20Neural%20Network/#observations","title":"Observations","text":"<ul> <li>This net achieved 67 test errors whereas a smaller net with two hidden layers of 800 rectified linear hidden units and no KB/Regularization.md achieved 146 errors</li> <li>soft targets can transfer a great deal of knowledge to the distilled model, including the knowledge about how to generalize that is learned from translated training data even though the transfer set does not contain any translations.</li> <li>When the distilled net had 300 or more units in each of its two hidden layers, all temperatures above 8 gave fairly similar results</li> <li>But when this was radically reduced to 30 units per layer, temperatures in the range 2.5 to 4 worked significantly better than higher or lower temperatures.</li> </ul>"},{"location":"KB/Distilling%20the%20Knowledge%20in%20a%20Neural%20Network/#automatic-speech-recognition","title":"Automatic Speech Recognition","text":"<ul> <li>State-of-the-art ASR systems currently use DNNs to map a (short) temporal context of features derived from the waveform to a probability distribution over the discrete states of a Hidden Markov Model (HMM)</li> <li>DNN produces a probability distribution over clusters of tri-phone states at each time and a decoder then finds a path through the HMM states that is the best compromise between using high probability states and producing a transcription that is probable under the language model.</li> <li>There is, however, another important objection to ensembles: If the individual models are large neural networks and the dataset is very large, the amount of computation required at training time is excessive, even though it is easy to parallelize.</li> </ul>"},{"location":"KB/Distilling%20the%20Knowledge%20in%20a%20Neural%20Network/#jft","title":"JFT","text":"<ul> <li>JFT is an internal Google dataset that has 100 million labeled images with 15,000 labels. When we did this work, Google\u2019s baseline model for JFT was a deep convolutional neural network that had been trained for about six months using asynchronous stochastic gradient descent on a large number of cores.</li> <li>When the number of classes is very large, it makes sense for the cumbersome model to be an ensemble that contains one generalist model trained on all the data and many 'specialist' models, each of which is trained on data that is highly enriched in examples from a very confusable subset of the classes (like different types of mushroom)</li> <li>The softmax of this type of specialist can be made much smaller by combining all of the classes it does not care about into a single dustbin class.</li> <li>To reduce overfitting and share the work of learning lower level feature detectors, each specialist model is initialized with the weights of the generalist model</li> <li>These weights are then slightly modified by training the specialist with half its examples coming from its special subset and half sampled at random from the remainder of the training set. After training, we can correct for the biased training set by incrementing the logit of the dustbin class by the log of the proportion by which the specialist class is oversampled.</li> <li>In order to derive groupings of object categories for the specialists, we decided to focus on categories that our full network often confuses.</li> <li>Even though we could have computed the confusion matrix and used it as a way to find such clusters, we opted for a simpler approach that does not require the true labels to construct the clusters. In particular, we apply a [clustering] algorithm to the covariance matrix of the predictions of our generalist model, so that a set of classes Sm that are often predicted together will be used as targets for one of our specialist models, m.</li> <li>on-line version of the K-means algorithm to the columns of the KB/Covariance.md matrix, and obtained reasonable clusters</li> <li>One of our main claims about using soft targets instead of hard targets is that a lot of helpful information can be carried in soft targets that could not possibly be encoded with a single hard target.</li> <li>It is even more remarkable to note that we did not have to do early stopping: the system with soft targets simply 'converged' to 57%. This shows that soft targets are a very effective way of communicating the regularities discovered by a model trained on all of the data to another model.</li> <li>The specialists that we used in our experiments on the JFT dataset collapsed all of their non-specialist classes into a single dustbin class. If we allow specialists to have a full softmax over all classes, there may be a much better way to prevent them overfitting than using early stopping. A specialist is trained on data that is highly enriched in its special classes.</li> <li>This means that the effective size of its training set is much smaller and it has a strong tendency to overfit on its special classes.</li> <li>The use of specialists that are trained on subsets of the data has some resemblance to mixtures of experts which use a gating network to compute the probability of assigning each example to each expert</li> <li>At the same time as the experts are learning to deal with the examples assigned to them, the gating network is learning to choose which experts to assign each example to based on the relative discriminative performance of the experts for that example.</li> <li>Using the discriminative performance of the experts to determine the learned assignments is much better than simply KB/Clustering.md the input vectors and assigning an expert to each cluster, but it makes the training hard to parallelize: First, the weighted training set for each expert keeps changing in a way that depends on all the other experts and second, the gating network needs to compare the performance of different experts on the same example to know how to revise its assignment probabilities.</li> </ul>"},{"location":"KB/Distilling%20the%20Knowledge%20in%20a%20Neural%20Network/#conclusions","title":"Conclusions","text":"<ul> <li>These difficulties have meant that mixtures of experts are rarely used in the regime where they might be most beneficial: tasks with huge datasets that contain distinctly different subsets.</li> <li>It is much easier to parallelize the training of multiple specialists</li> <li>e first train a generalist model and then use the confusion matrix to define the subsets that the specialists are trained on</li> <li>We have shown that distilling works very well for transferring knowledge from an ensemble or from a large highly regularized model into a smaller, distilled model.</li> <li>For really big neural networks, it can be infeasible even to train a full ensemble, but we have shown that the performance of a single really big net that has been trained for a very long time can be significantly improved by learning a large number of specialist nets, each of which learns to discriminate between the classes in a highly confusable cluster. We have not yet shown that we can distill the knowledge in the specialists back into the single large net.</li> </ul>"},{"location":"KB/Distributive%20Interpretation%20%282%29/","title":"Distributive Interpretation (2)","text":""},{"location":"KB/Distributive%20Interpretation%20%282%29/#distributive-interpretation-2","title":"Distributive Interpretation (2)","text":"<ul> <li>Object takes scope over the subject:</li> <li>Three aliens are holding two flags. = (Two flags, and then three aliens hold them)</li> </ul>"},{"location":"KB/Distributive%20Interpretation/","title":"Distributive Interpretation","text":""},{"location":"KB/Distributive%20Interpretation/#distributive-interpretation","title":"Distributive Interpretation","text":"<ul> <li>Subject takes scope over the object: Three aliens are holding two flags.</li> <li>Both Np's are interpreted individually and connected to each other No indicators of how they are connected.</li> </ul>"},{"location":"KB/Distributive%20units/","title":"Distributive units","text":""},{"location":"KB/Distributive%20units/#distributive-units","title":"Distributive Units","text":"<ul> <li>each unit responds to multiple categories</li> <li>You must see the entire pattern over a collection of units, to uniquely categorize an input.</li> <li>The states of individual units are uninterpretable</li> </ul>"},{"location":"KB/Divergence/","title":"Divergence","text":""},{"location":"KB/Divergence/#divergence","title":"Divergence","text":"<ul> <li>Flow field transports mass \\(\\(v:\\mathbb{R}^{3}\\rightarrow \\mathbb{R}^{3}\\)\\)</li> <li>Increase/loss of mass at point p</li> <li> \\[div_v: \\mathbb{R}^{3} \\rightarrow \\mathbb{R} = \\nabla \\cdot v = \\frac{\\partial v_{x}}{\\partial x} + \\frac{\\partial v_{y}}{\\partial y} + \\frac{\\partial v_{z}}{\\partial z}\\] </li> <li></li> </ul>"},{"location":"KB/Divide%20Oriented/","title":"Divide Oriented","text":""},{"location":"KB/Divide%20Oriented/#divide-oriented","title":"Divide Oriented","text":"<ul> <li>corresponds to physical realization (screen,printer), e.g., RGB, CMYK</li> </ul>"},{"location":"KB/Docker%20Cheatsheet/","title":"Docker Cheatsheet","text":"","tags":["cheatsheets"]},{"location":"KB/Docker%20Cheatsheet/#docker-cheatsheet","title":"Docker Cheatsheet","text":"","tags":["cheatsheets"]},{"location":"KB/Docker%20Cheatsheet/#minimal-workflow","title":"Minimal Workflow","text":"<pre><code>docker init\ndocker compose up --build\ndocker compose up --build -d #run_in_background\ndocker compose down #stop\n</code></pre>","tags":["cheatsheets"]},{"location":"KB/Docker%20Cheatsheet/#managing-containers","title":"Managing Containers","text":"Name Command Starting Containers docker container start nginx Stopping Containers docker container stop nginx Restarting Containers docker container restart nginx Pausing Containers docker container pause nginx Unpausing Containers docker container unpause nginx Blocking a Container docker container wait nginx Sending SIGKILL Containers docker container kill nginx Sending another signal docker container kill -s HUP nginx Connecting to an Existing Container docker container attach nginx Check the Containers docker ps To see all running containers docker container ls Container Logs docker logs infinite \u2018tail -f\u2019 Containers\u2019 Logs docker container logs infinite -f Inspecting Containers docker container inspect infinite Inspecting Containers for certain docker container inspect \u2013format \u2018{{ .NetworkSettings.IPAddress }}\u2019 $(docker ps -q) Containers Events docker system events infinite docker system events infinite docker container port infinite Running Processes docker container top infinite Container Resource Usage docker container stats infinite Inspecting changes to files or directories on a container\u2019s filesystem docker container diff infinite","tags":["cheatsheets"]},{"location":"KB/Docker%20Cheatsheet/#manage-images","title":"Manage Images","text":"Name Command Listing Images docker image ls Building Images docker build. From a Remote GIT Repository docker build github.com/creack/docker-firefox Instead of Specifying a Context, You Can Pass a Single Dockerfile in the URL or Pipe the File in via STDIN docker build \u2013 &lt; Dockerfile Building and Tagging docker build -t eon/infinite. Building a Dockerfile while Specifying the Build Context docker build -f myOtherDockerfile. Building from a Remote Dockerfile URI curl example.com/remote/Dockerfile|docker build -f \u2013 . Removing an Image docker image rm nginx Loading a Tarred Repository from a File or the Standard Input Stream docker image load &lt; ubuntu.tar.gz Saving an Image to a Tar Archive docker image save busybox &gt; ubuntu.tar Showing the History of an Image docker image history Creating an Image From a Container docker container commit nginx Tagging an Image docker image tag nginx eon01/nginx Pushing an Image docker image push eon01/nginx","tags":["cheatsheets"]},{"location":"KB/Docker%20Cheatsheet/#removing-images","title":"Removing Images","text":"Name Command Removing a Running Container docker container rm nginx Removing a Container and its Volume docker container rm -v nginx Removing all Exited Containers docker container rm $(docker container ls -a -f status=exited -q) Removing All Stopped Containers docker container rm <code>docker container ls -a -q</code> Removing a Docker Image docker image rm nginx Removing Dangling Images docker image rm $(docker image ls -f dangling=true -q) Removing all Images docker image rm $(docker image ls -a -q) Removing all Untagged Images docker image rm -f $(docker image ls |grep \u201c^\u201d|awk \u201c{print $3}\u201d) Stopping &amp; Removing all Containers docker container stop $(docker container ls -a -q) &amp;&amp; docker container rm $(docker container ls -a -q) Removing Dangling Volumes docker volume rm $(docker volume ls -f dangling=true -q) Removing all unused (containers, images, networks and volumes) docker system prune -f Clean all docker system prune -a","tags":["cheatsheets"]},{"location":"KB/Docker%20Cheatsheet/#dockerfile-commands","title":"Dockerfile Commands","text":"Command Description Example FROM Specifies the base image for the build FROM ubuntu:latest RUN Executes a command inside the container during build time RUN apt-get update &amp;&amp; apt-get install -y curl CMD Specifies the default command to run when the container starts CMD [\u201cnpm\u201d, \u201cstart\u201d] EXPOSE Informs Docker that the container listens on specific network ports at runtime EXPOSE 80/tcp ENV Sets environment variables inside the container ENV NODE_ENV=production COPY Copies files or directories from the build context into the container COPY app.js /usr/src/app/ ADD Similar to COPY but supports additional features like URL retrieval and decompression ADD https://example.com/file.tar.gz /usr/src/ WORKDIR Sets the working directory for subsequent instructions WORKDIR /usr/src/app ARG Defines variables that users can pass at build-time to the builder with the docker build command ARG VERSION=1.0 ENTRYPOINT Configures a container to run as an executable ENTRYPOINT [\u201cpython\u201d, \u201capp.py\u201d] VOLUME Creates a mount point and assigns it to a specified volume VOLUME /data USER Sets the user or UID to use when running the image USER appuser LABEL Adds metadata to an image in the form of key-value pairs LABEL version=\u201d1.0\u2033 maintainer=\u201dJohn Doe ONBUILD Configures commands to run when the image is used as the base for another build ONBUILD ADD . /app/src","tags":["cheatsheets"]},{"location":"KB/Docker%20Cheatsheet/#volume-commands","title":"Volume Commands","text":"Command Description Example volume create Creates a named volume docker volume create mydata volume ls Lists the available volumes docker volume ls volume inspect Displays detailed information about a volume docker volume inspect mydata volume rm Removes one or more volumes docker volume rm mydata volume prune Removes all unused volumes docker volume prune","tags":["cheatsheets"]},{"location":"KB/Document%20Triage/","title":"Document Triage","text":""},{"location":"KB/Document%20Triage/#document-triage","title":"Document Triage","text":"<ul> <li>Characters in file must be MACHINE READABLE (Character Encoding)</li> <li>Character Encoding Identification (ASCII, UNICODE..)</li> <li>Language Identification (English, French,..)</li> <li>Text Sectioning</li> </ul>"},{"location":"KB/Dopamine/","title":"Dopamine","text":"<p>toc: true title: Dopamine</p> <p>categories: ['temp']</p>"},{"location":"KB/Dopamine/#dopamine","title":"Dopamine","text":"<ul> <li>Dopamine is chemically expressed as C2H11NO2.</li> <li>It is a neuro-chemical created in various parts of the brain and is critical for all kinds of brain functions including thinking, carrying, sleeping, mood, attention, motivation, seeking and rewarding.</li> <li>The dopamine is responsible for the feeling of pleasure.</li> <li>When a person eats, drinks or performs a pleasurable action, dopamine is stimulated in his brain to repeat the action.</li> <li>Unexpected rewards increase the activity of dopamine neurons, acting as positive feedback signals for the brain regions associated with the preceding behavior.</li> <li>As learning takes place, the timing of activity will shift until it occurs upon the cue alone, with the expected reward having no additional effect.<ul> <li>And should the expected reward not be received, dopamine activity drops, sending a negative feedback signal to the relevant parts of the brain, weakening the positive association</li> </ul> </li> </ul>"},{"location":"KB/Dot%20Product%20Attention/","title":"Dot Product Attention","text":""},{"location":"KB/Dot%20Product%20Attention/#dot-product-attention","title":"Dot Product Attention","text":"<ul> <li>Luong et al., 2015</li> <li> \\[f_{att}(h_{i}, s_{j}) = h_{i}^{T}s_{j}\\] </li> <li>Equivalent to Multiplicative Attention with no trainable weight matrix. Performs better at larger dimensions</li> <li>Identity matrix</li> <li>\\(h\\) is hidden state for encoder and \\(s\\) is hidden state for decoder</li> <li>A type of Attention Alignment</li> <li>Final scores after Softmax</li> <li></li> </ul>"},{"location":"KB/Double%20Descent/","title":"Double Descent","text":""},{"location":"KB/Double%20Descent/#double-descent","title":"Double Descent","text":"<ul> <li>When increasing the model size or the number of epochs, performance on the test set initially improves, then worsens but then again starts to improve and finally saturates.  </li> <li>This phenomena is against conventional wisdom, because the test error should not be decreasing again after increasing.</li> <li>occurs often in the over-parameterization regime<ul> <li>models which have a lot of parameters</li> <li>models that have huge complexity</li> </ul> </li> <li></li> </ul>"},{"location":"KB/Down%20Syndrome/","title":"Down Syndrome","text":""},{"location":"KB/Down%20Syndrome/#down-syndrome","title":"Down Syndrome","text":"<ul> <li>A genetic disorder characterized by intellectual impairment and physical abnormalities that arises from the genome having an extra copy of chromosome 21.</li> </ul>"},{"location":"KB/Downsampling/","title":"Downsampling","text":""},{"location":"KB/Downsampling/#downsampling","title":"Downsampling","text":"<ul> <li>Overloaded term that can mean either of the following</li> <li>Reducing the amount of information in a feature in order to train a model more efficiently. For example, before training an image recognition model, downsampling high-resolution images to a lower-resolution format.</li> <li>Training on a disproportionately low percentage of over-represented class examples in order to improve model training on under-represented classes. For example, in a class-imbalanced dataset, models tend to learn a lot about the majority class and not enough about the minority class. Downsampling helps balance the amount of training on the majority and minority classes.</li> </ul>"},{"location":"KB/Downstream%20Task/","title":"Downstream Task","text":""},{"location":"KB/Downstream%20Task/#downstream-task","title":"Downstream Task","text":"<ul> <li>computer vision applications that are used to evaluate the quality of features learned by self-supervised learnin </li> <li>training data are scarce </li> <li>In general, human-annotated labels are needed to solve the downstream tasks. </li> <li>in some applications, the downstream task can be the same as the pretext task without using any human-annotated labels.</li> </ul>"},{"location":"KB/DrawBench/","title":"DrawBench","text":""},{"location":"KB/DrawBench/#drawbench","title":"DrawBench","text":"<ul> <li>comprehensive and challenging benchmark for text-to-image models</li> <li>With DrawBench, we compare Imagen with recent methods including VQGAN+CLIP, Latent Diffusion Models, and DALL-E, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment</li> </ul>"},{"location":"KB/Dreamfusion/","title":"Dreamfusion","text":""},{"location":"KB/Dreamfusion/#dreamfusion","title":"Dreamfusion","text":"<ul> <li>text-to-3D model</li> <li>uses a pretrained 2D text-to-image difusion model to perform textto-3D synthesis</li> <li>Dreamfusion replaces previous CLIP techniques with a loss derived from distillation of a 2D difusion model</li> <li>the difusion model can be used as a loss within a generic continuous optimization problem to generate samples</li> <li>sampling in parameter space is much harder than in pixels as we want to create 3D models that look like good images when rendered from random angles</li> <li>this model uses a diferentiable generator - Other approaches are focused on sampling pixels, however, this model instead focuses on creating 3D models that look like good images when rendered from random angles</li> </ul>"},{"location":"KB/Drop%20Delivery/","title":"Drop Delivery","text":""},{"location":"KB/Drop%20Delivery/#drop-delivery","title":"Drop Delivery","text":"<ul> <li>A method of introducing an object to the workplace by Gravity. Usually, a chute or container is so placed that, when work on the part is finished, it will fall or drop into a chute or onto a conveyor with little or no transport by the robot.</li> </ul>"},{"location":"KB/Dropout/","title":"Dropout","text":""},{"location":"KB/Dropout/#dropout","title":"Dropout","text":"<ul> <li>Applied to Dense Layers</li> <li>Training : Randomly (Bernoulli, p = 0.5 say) set #activations to 0</li> <li>Generally p = 0.1, 0.5</li> <li>Testing: Reweight by p<ul> <li>Because after training values will increase by \\(\\(1/(1-p)\\)\\)</li> </ul> </li> <li>Reduces co dependence between neurons</li> <li>Decreases overfitting</li> <li>Start with small rate : 20 %</li> <li>Helps with small datasets</li> <li>Reducing Co adaptation by making the presence of other hidden [neurons] unreliable</li> <li>The authors found that there is a trade-off between when Dropout is necessary, and when it's no longer useful. First, to cover the case where the dataset is extremely small: even Dropout does not improve performance in that case, simply because the dataset size is too small. The same is true for datasets that are large enough: Dropout then does no longer improve the model, but rather, model performance gets worse.</li> </ul>"},{"location":"KB/Dual-memory%20Approach/","title":"Dual-memory Approach","text":""},{"location":"KB/Dual-memory%20Approach/#dual-memory-approach","title":"Dual-memory Approach","text":"<ul> <li>Combination of several memories specialized for storing different types of data and supporting different functionalities</li> </ul>"},{"location":"KB/Dual-memory%20Approach/#triplestores","title":"Triplestores","text":"<ul> <li>The contents of this memory system is semantic in nature (small)</li> </ul>"},{"location":"KB/Dual-memory%20Approach/#leveldb","title":"LevelDB","text":"<ul> <li>key-value storage database, operate in RAM (developed by Google)</li> <li>Interpretation includes computing spatial relations between objects to keep an updated relational model of the scene around the robot</li> </ul>"},{"location":"KB/Dynamic%20Eager%20Execution/","title":"Dynamic Eager Execution","text":""},{"location":"KB/Dynamic%20Eager%20Execution/#dynamic-eager-execution","title":"Dynamic Eager Execution","text":"<ul> <li>operations are executed immediately</li> <li>more readable code and easier to debug</li> <li>lower performance than\u00a0Static Graph Execution</li> <li>graph architecture can evolve dynamically</li> </ul>"},{"location":"KB/Dynamic%20Sparsity/","title":"Dynamic Sparsity","text":""},{"location":"KB/Dynamic%20Sparsity/#dynamic-sparsity","title":"Dynamic Sparsity","text":"<ul> <li>train intrinsically sparse neural networks from scratch using only a small proportion of parameters and FLOPs</li> <li>Dynamic KB/Sparsity.md enables training sparse models from scratch, hence the training and inference FLOPs and memory requirements are only a small fraction of the dense models.</li> <li>models built with dynamic KB/Sparsity.md can be trained from scratch to match their dense counterparts without involving any pre-training or dense training</li> </ul>"},{"location":"KB/Dynamic%20visual%20attention/","title":"Dynamic visual attention","text":"<ul> <li>@Dynamic visual attention: searching for coding length increments</li> <li>Incremental coding length (ICL) using the features of local image patches is proposed to maximise the entropy of the sampled visual features</li> <li>unexpected features elicit entropy gain in the perception state and are therefore assigned high energy</li> <li>The probability function of feature activities of this model is updated dynamically</li> </ul>"},{"location":"KB/Dynamic%20visual%20attention/#dynamic-visual-attention","title":"Dynamic Visual Attention","text":""},{"location":"KB/EEG%20Artifacts/","title":"EEG Artifacts","text":""},{"location":"KB/EEG%20Artifacts/#eeg-artifacts","title":"EEG Artifacts","text":"<ul> <li>ICA</li> <li></li> </ul>"},{"location":"KB/EEG%20Artifacts/#drift","title":"Drift","text":"<ul> <li>A</li> </ul>"},{"location":"KB/EEG%20Artifacts/#probably-disconnected","title":"Probably Disconnected","text":""},{"location":"KB/EEG%20Artifacts/#periodic-probably-from-ecg","title":"Periodic - Probably From ECG","text":"<ul> <li>Arteries in neck exposed when person is nervous</li> </ul>"},{"location":"KB/EEG%20Baseline%20Correction/","title":"EEG Baseline Correction","text":""},{"location":"KB/EEG%20Baseline%20Correction/#eeg-baseline-correction","title":"EEG Baseline Correction","text":"<ul> <li>Signal drifts</li> <li>Adjust pre stimulus value by averaging across pre stimulus points during baseline period</li> <li>Subtract average value from post stimulus period</li> <li>Period where nothing is going on</li> <li>100-200 ms</li> <li></li> </ul>"},{"location":"KB/EEG%20Cap/","title":"EEG Cap","text":""},{"location":"KB/EEG%20Cap/#eeg-cap","title":"EEG Cap","text":"<ul> <li>![im](images/Pasted%20Image%2020220510230806.png|]</li> <li></li> </ul>"},{"location":"KB/EEG%20Cluster%20Testing/","title":"EEG Cluster Testing","text":""},{"location":"KB/EEG%20Cluster%20Testing/#eeg-cluster-testing","title":"EEG Cluster Testing","text":"<ul> <li>If done for each point, same test repeated and false positives increase</li> <li>eg: t test in each electrode</li> <li>A result is more believable if it occurs in a set of adjacent channels:  <ul> <li>Threshold data of statistical test  </li> <li>Compute clusters  </li> <li>Threshold randomized data  </li> <li>Compute clusters for 100 or so distr of randomized data</li> <li>Decide if its rare</li> </ul> </li> <li> \\[sumT = \\text{sum of all t stats}\\] </li> <li><ul> <li>There was a significant difference between easy and more difficult trials between 712 ms post-stimulus and 768 ms post-stimulus. This difference was initially localized to a few central electrodes but over time spread out more posteriorly. This is consistent with previous studies that have shown</li> </ul> </li> </ul>"},{"location":"KB/EEG%20Filtering/","title":"EEG Filtering","text":""},{"location":"KB/EEG%20Filtering/#eeg-filtering","title":"EEG Filtering","text":"<ul> <li>Remove 50/60Hz notch filter for line noise</li> <li>Might introduce distortion</li> </ul>"},{"location":"KB/EEG%20Statistical%20Analysis/","title":"EEG Statistical Analysis","text":""},{"location":"KB/EEG%20Statistical%20Analysis/#eeg-statistical-analysis","title":"EEG Statistical Analysis","text":"<ul> <li>EEG Cluster Testing</li> </ul>"},{"location":"KB/EEG/","title":"EEG","text":""},{"location":"KB/EEG/#eeg","title":"EEG","text":"<ul> <li>Electrical activity on the surface of the brain</li> <li>Frequencies</li> <li>Pyramidal cell</li> <li>Electrode nomenclature</li> <li>Cheaper than fMRI</li> <li>Fast signals</li> <li>Low anatomical specificity<ul> <li>Cant find where its coming from</li> <li>Lots of noise</li> </ul> </li> <li>EEG Artifacts</li> <li>EEG Filtering</li> <li>ERP</li> <li>EEG Baseline Correction</li> <li>EEG Statistical Analysis</li> <li>EEG Cap</li> <li>This might be related to fMRI</li> </ul>"},{"location":"KB/ELECTRA/","title":"ELECTRA","text":""},{"location":"KB/ELECTRA/#electra","title":"ELECTRA","text":"<ul> <li>ELECTRA: Pre-training Text Encoders As Discriminators Rather Than Generators</li> <li>Pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK] and then train a model to reconstruct the original tokens.</li> <li>sample-efficient pre-training alternative task called replaced token detection</li> <li>self-supervised task for language representation learning</li> <li>Instead of masking the input, their approach corrupts it by replacing some tokens with plausible alternatives sampled from a small generator network</li> <li>Then, instead of training a model that predicts the original identities of the corrupted tokens, the key idea is training a discriminative text encoder model to distinguish input tokens from high-quality negative samples produced by an small generator network</li> <li>more compute-efficient and results in better performance on downstream tasks</li> <li>particularly strong for small models</li> <li>GLUE</li> <li>performs comparably to RoBERTa|[RoBERTa](./RoBERTa.md) and XLNet while using less than 1/4 of their compute and outperforms them when using the same amount of compute.</li> </ul>"},{"location":"KB/ELMO/","title":"ELMO","text":""},{"location":"KB/ELMO/#elmo","title":"ELMO","text":"<ul> <li>Deep Contextualized Word Representations</li> <li>context-sensitive word embeddings using the [LSTM)](Long Short Term Memory (LSTM|Long Short Term Memory (LSTM|LSTM)](LSTM)](Long Short Term Memory (LSTM|Long Short Term Memory (LSTM|LSTM).md).md)-based Embedding from Language Models (ELMo) architecture</li> </ul>"},{"location":"KB/ERP/","title":"ERP","text":""},{"location":"KB/ERP/#erp","title":"ERP","text":"<ul> <li>Event related potentials</li> <li>-</li> <li></li> <li></li> </ul>"},{"location":"KB/Eager%20Execution/","title":"Eager Execution","text":""},{"location":"KB/Eager%20Execution/#eager-execution","title":"Eager Execution","text":"<ul> <li>A TensorFlow programming environment in which operations run immediately. By contrast, operations called in graph execution don't run until they are explicitly evaluated. Eager execution is an imperative interface , much like the code in most programming languages. Eager execution programs are generally far easier to debug than graph execution programs.</li> </ul>"},{"location":"KB/Early%20Ray%20Termination/","title":"Early Ray Termination","text":""},{"location":"KB/Early%20Ray%20Termination/#early-ray-termination","title":"Early Ray Termination","text":"<ul> <li>General acceleration idea: neglect regions with irrelevant information</li> </ul>"},{"location":"KB/Early%20Stopping%20tricks/","title":"Early Stopping","text":""},{"location":"KB/Early%20Stopping%20tricks/#early-stopping","title":"Early Stopping","text":"<ul> <li>No of epochs is a hyper parameter : to prevent overfitting</li> <li>Early Stopping is a KB/Regularization.md technique that improves image classification accuracy by intentionally stopping the training when validation loss increases. Training is stopped as training a model for too many epochs sometimes causes Overfitting.</li> <li>In Early Stopping, the number of epochs becomes a tunable hyperparameter. We continuously store the best parameters during training, and when these parameters no longer change for several epochs, we stop training.</li> </ul>"},{"location":"KB/Earth%20Mover%27s%20Distance%20%28EMD%29/","title":"Earth Mover's Distance (EMD)","text":""},{"location":"KB/Earth%20Mover%27s%20Distance%20%28EMD%29/#earth-movers-distance-emd","title":"Earth Mover's Distance (EMD)","text":"<ul> <li>A measure of the relative similarity between two documents. The lower the value, the more similar the documents.</li> <li>Spatial Distance between two PDF</li> </ul>"},{"location":"KB/Eavesdropping/","title":"Eavesdropping","text":""},{"location":"KB/Eavesdropping/#eavesdropping","title":"Eavesdropping","text":"<ul> <li>Two tasks \\(T_{A}, T_{B}\\) , share feature F<ul> <li>F is harder to learn in one task</li> <li>One task can eavesdrop into another ([Hard Parameter Sharing] , Soft Parameter Sharing)</li> <li>Extra info in each task</li> </ul> </li> </ul>"},{"location":"KB/Edema/","title":"Edema","text":""},{"location":"KB/Edema/#edema","title":"Edema","text":"<ul> <li>Swelling as a result of fluid retention or build-up</li> </ul>"},{"location":"KB/Effect%20Of%20Depth/","title":"Effect of Depth","text":""},{"location":"KB/Effect%20Of%20Depth/#effect-of-depth","title":"Effect of Depth","text":"<ul> <li>Adding skip connections make the loss surface smoother</li> <li></li> </ul>"},{"location":"KB/Effect%20Of%20Depth/#deeper-architectures","title":"Deeper Architectures","text":"<ul> <li>Makes more uneven and chaotic</li> <li></li> </ul>"},{"location":"KB/Effect%20Of%20Depth/#wider-architectures","title":"Wider Architectures","text":"<ul> <li>Makes landscape smoother and flatter</li> <li></li> </ul>"},{"location":"KB/Effects%20of%20Contextual%20Cues%20on%20Inferring%20and%20Remembering%20Meanings%20of%20New%20Word/","title":"Effects of Contextual Cues on Inferring and Remembering Meanings of New Word","text":""},{"location":"KB/Effects%20of%20Contextual%20Cues%20on%20Inferring%20and%20Remembering%20Meanings%20of%20New%20Word/#effects-of-contextual-cues-on-inferring-and-remembering-meanings-of-new-word","title":"Effects of Contextual Cues on Inferring and Remembering Meanings of New Word","text":"<ul> <li>xiaolongli</li> <li>Li, X. (1988). Effects of contextual cues on inferring and remembering meanings of new words. Applied linguistics, 9(4), 402-413.</li> </ul>"},{"location":"KB/Effects%20of%20Contextual%20Cues%20on%20Inferring%20and%20Remembering%20Meanings%20of%20New%20Word/#abstract","title":"Abstract","text":"<ul> <li>This study tested four directional hypotheses: Compared with those receiving cue- inadequate sentences, subjects receiving cue-adequate sentences will (1) report greater ease in word inference, and (2) score higher in inferring and remembering the contextual meanings of unfamiliar words. (3) Contextual cues being equally adequate, subjects reading, in contrast to listening to, the sentences will better infer and remember the contextual meanings ofunfamiliar words. (4) The higher the scores of word inference, the better the retention of the contextual meanings of the target word</li> <li>An approach combining schema theory and the generative model of comprehension was usedfor the rationale of this study and the discussion of its findings.</li> </ul>"},{"location":"KB/Effects%20of%20Contextual%20Cues%20on%20Inferring%20and%20Remembering%20Meanings%20of%20New%20Word/#literature-review","title":"LITERATURE REVIEW","text":"<ul> <li>Inferring, or 'inferencing', the meanings of unfamiliar words in context can be seen as 'a process of identifying and acquiring' new vocabulary by utilizing 'attributes and contexts that are familiar'</li> <li>In language learning, inferring word meanings while reading or listening is a process of vocabulary acquisition which has an important influence upon comprehension either in a first language (Kruse 1979) or in a second language (Yorio 1971).</li> <li>Contextual cues can affect the process and outcome of word inference.</li> <li>Carton (1971) hypothesized that in the process of identifying and acquiring unfamiliar words in context, greater certainty results from guesses based on many cues than on few.</li> <li>However, for contextual cues to be of real help for word inference, they must (1) be perceptually and conceptually familiar to the text-receiver, and (2) contain the information available for the text-receiver to find the relevant schemata in order to (a) account for the oncoming input in the text, and (b) identify unfamiliar stimuli in context.</li> <li>Memories are, in a sense, natural effects of the comprehension process (Rumelhart and Ortony 1977) which, by nature, is schematic (Bartlett 1932</li> <li>memory performance is enhanced to the extent that the encoding context forms an integrated unit with the to-be-remembered word</li> </ul>"},{"location":"KB/Effects%20of%20Contextual%20Cues%20on%20Inferring%20and%20Remembering%20Meanings%20of%20New%20Word/#this-study-purpose-and-hypotheses","title":"THIS STUDY: PURPOSE AND HYPOTHESES","text":"<ul> <li>The present study was conducted among second language learners</li> <li>It not only focused on the effects of cue adequacy on inferring and remembering the meanings of new words in discrete, semantically disconnected sentences, but also aimed at an empirical exploration concerning the relationship between word inference and retention.</li> <li>this study compared the effects of cue adequacy in both reading and listening contexts</li> <li>cue-adequate sentences were compared with their cue- inadequate counterparts for testing four directional hypotheses. These were: Compared with those receiving cue-inadequate sentences, subjects receiving cue-adequate sentences will (1) report greater ease in inferring the meanings of new words, and (2) score higher in inferring and remembering the meanings of new words. (3) Contextual cues being equally adequate, subjects reading, in contrast to listening to, the sentences will better infer and remember the meanings of unfamiliar words. (4) The higher the scores of word inference, the better the retention of the meanings of the target words.</li> <li>a sentence with certain input information that contains clues sufficient for inferring the contextual meaning of a target word was defined as a cue-adequate sentence, while a sentence without such input information was defined as a cue-inadequate one</li> <li>For example, the sentence John took out a collapsible bicycle and rode to school was treated as a cue- inadequate sentence, for, in this sentence, there was no input information signaling any clue to the contextual meaning of the target word collapsible</li> <li>However, the sentence John took out a collapsible bicycle, unfolded it, and rode to school was treated as a cue-adequate one, for the word unfolded provided the clue to the approximate meaning of the target word.</li> </ul>"},{"location":"KB/Effects%20of%20Contextual%20Cues%20on%20Inferring%20and%20Remembering%20Meanings%20of%20New%20Word/#method","title":"METHOD","text":""},{"location":"KB/Effects%20of%20Contextual%20Cues%20on%20Inferring%20and%20Remembering%20Meanings%20of%20New%20Word/#participant","title":"Participant","text":"<ul> <li>Forty-eight advanced trainees from an EAP (English for Academic Purposes) center in China were involved in this study.</li> <li>Their average age was 35 (SD = 7), ranging from 22 to 48</li> <li>They were randomly assigned into four treatment groups, namely</li> <li>LC\u2014 (i.e. listening group with inadequate cues); RC\u2014 (i.e. reading group with inadequate cues); LC + (i.e. listening group with adequate cues), and RC+ (i.e. reading group with adequate cues). Of the four groups, two (i.e. LC\u2014 and RC\u2014) received cue-inadequate sentences, and the others (i.e. LC+ and RC+), cue- adequate sentences. In each pair of groups that received the same sentences, one group (i.e. LC\u2014 and LC+) took the listening test, and the other (i.e. RC\u2014 and RC+), the reading test</li> <li>There were two independent variables. The first one, text, had two levels- sentences with adequate cues versus inadequate cues. The second one, language skills, also had two levels\u2014reading versus listening dependent</li> <li>These were group means in terms of: (1) measures of word inference (i.e. inferring the meanings of unfamiliar words); (2) ratings of degrees of difficulty of word inference, and (3) measures of word retention (i.e. KB/Recall.md of the inferred meanings of the target words).</li> </ul>"},{"location":"KB/Effects%20of%20Contextual%20Cues%20on%20Inferring%20and%20Remembering%20Meanings%20of%20New%20Word/#task","title":"Task","text":"<ul> <li>Sixty discrete, semantically disconnected sentences were constructed for the experiment.</li> <li>They formed two sets of counterparts. Each set was composed of 30 sentences.</li> <li>One set consisted of cue-adequate sentences, and the other of cue-inadequate sentences</li> <li>A target word was defined as a perceptually, not conceptually, unfamiliar term.</li> <li>Since the target words were only perceptually unfamiliar, it would not be a prerequisite for the subjects to acquire any new concept to perform the task for this experiment.</li> <li>By the same token, the topic of all the test items was based on common knowledge; thus, there was no need to turn to any biased or specialized frame of reference for inferring word meanings in this experiment</li> <li>Furthermore, no meaning of any target word for this study could be deduced simply by applying morphological knowledge in terms of stems, affixes, or other devices of word formation.</li> <li>In the pretest, the subjects were asked to write down (either in English or Chinese) the common meanings of the target words they knew.</li> <li>Three more tasks were performed after the pretest. The first one, word inference, was to infer the contextual meanings of the target words based on the input information in the sentences in which the target words were embedded.</li> <li>Both tapes, one containing sentences with adequate, and the other sentences with inadequate cues, were produced by a native English speaker at a speed of about 90 words per minute.</li> <li>The subjects listened to the sentences one by one, with each sentence repeated three times.</li> <li>Sentences were shown one by one on the screens by using a mask, and presented at the same rate as the corresponding items on the tapes for the listening groups.</li> <li>The tests were presented in an open-ended, not in a multiple-choice, form</li> <li>After reading or listening to each sentence, the subjects were asked to state (either in English or Chinese) their guesses of the contextual meaning of the target word in the sentence</li> <li>The second task after the pretest was to rate the degrees of difficulty in terms of word inferences</li> <li>The last task, word retention, was a cued KB/Recall.md of the target words' inferred contextual meanings.</li> <li>Each target word was cued by another word from the same sentence that had been processed for inferring the contextual meaning of the target word.</li> <li>The target words were listed in exactly the same order as they appeared in the tests for word inference.</li> </ul>"},{"location":"KB/Effects%20of%20Contextual%20Cues%20on%20Inferring%20and%20Remembering%20Meanings%20of%20New%20Word/#data-analyses","title":"DATA ANALYSES","text":"<ul> <li>Hypothesis 1 was tested by a Chi-square, Hypothesis 2 by two separate one-way ANOVAs, Hypothesis 3 by two separate Duncan's Multiple Range Tests, and Hypothesis 4 by a Correlation Test</li> <li>Since the Chi-square is a test especially designed for nominal data, it was decided beforehand that the nine-point scale should be dichotomized: ratings less than 5 were defined as 'difficult' (to infer the contextual meanings of the target words from the discrete sentences), while ratings equal to or greater than 5 were defined as 'easy'.</li> <li>Cronbach's Alpha was used for computing the test reliability. Reliability coefficients for the four tests of word inference were 0.60 for LC\u2014, 0.54 for RC-, 0.68 for LC+, and 0.64 for RC+, which were rather low.</li> <li>However, they can be considered as being acceptable for this study, for both the sample size (12 per cell) and the number of test items (30 for each test) were very small.</li> </ul>"},{"location":"KB/Effects%20of%20Contextual%20Cues%20on%20Inferring%20and%20Remembering%20Meanings%20of%20New%20Word/#results","title":"RESULTS","text":"<ul> <li>Data analyses indicated that all the four hypotheses were confirmed with statistical significance.</li> </ul>"},{"location":"KB/Effects%20of%20Contextual%20Cues%20on%20Inferring%20and%20Remembering%20Meanings%20of%20New%20Word/#hypothesis-1","title":"Hypothesis 1","text":"<ul> <li>no significant difference between the four groups in rating degrees of difficulty of word inference</li> <li>result of the Chi-square Test presented null hypothesis could be rejected</li> </ul>"},{"location":"KB/Effects%20of%20Contextual%20Cues%20on%20Inferring%20and%20Remembering%20Meanings%20of%20New%20Word/#hypothesis-2","title":"Hypothesis 2","text":"<ul> <li>subjects in the four groups performed differently on both tasks of word inference and word retention</li> <li>showed that RC+ scored significantly higher than LC+, and that both RC+ and LC+</li> <li>scored significantly higher than RC\u2014 and LC\u2014</li> <li>However, there was no significant difference between RC\u2014 and LC\u2014</li> <li>showed both RC+ and LC+ scored significantly higher than RC\u2014 and LC\u2014, and that R C + scored significantly higher than LC+. However, there was no significant difference between RC\u2014 and LC-.</li> </ul>"},{"location":"KB/Effects%20of%20Contextual%20Cues%20on%20Inferring%20and%20Remembering%20Meanings%20of%20New%20Word/#hypothesis-3","title":"Hypothesis 3","text":"<ul> <li>both in word inference and retention, R C + scored significantly higher than LC+; however, in either word inference or retention, there was no significant difference between RC\u2014 and LC\u2014</li> </ul>"},{"location":"KB/Effects%20of%20Contextual%20Cues%20on%20Inferring%20and%20Remembering%20Meanings%20of%20New%20Word/#hypothesis-4","title":"Hypothesis 4","text":"<ul> <li>there was a positive correlation of statistical significance between word inference and word retention</li> </ul>"},{"location":"KB/Effects%20of%20Contextual%20Cues%20on%20Inferring%20and%20Remembering%20Meanings%20of%20New%20Word/#discussion-and-conclusion","title":"DISCUSSION AND CONCLUSION","text":"<ul> <li>ubjects receiving cue- adequate sentences, in contrast to cue-inadequate sentences, not only reported greater ease in word inference, but also scored significantly higher in inferring and remembering the meanings of unfamiliar words in context</li> <li>existed a positive correlation between word inference and word retention. That is, the higher the group means in inferring the contextual meanings of unfamiliar words, the better the performance in remembering the meanings of those words</li> <li>contextual cues being equally adequate, subjects reading the sentences scored significantly higher in both inferring and remembering the contextual meanings of unfamiliar words than those listening to the sentences.</li> <li>This finding further sustained Carton's (1971) hypothesis that texts with adequate contextual cues minimize errors in the process of identifying and acquiring new words in a natural context</li> <li>The presence of contextual cues means 'bridging information' (Garrod and Sanford 1981), grammatical and/or semantic, conceptual as well as perceptual.</li> <li>Without adequate bridging information, it would seem next to impossible to infer and KB/Recall.md the contextual meaning of any unfamiliar word.</li> <li>This explains why the LC\u2014 and RC\u2014 groups scored so low on both word inference and word retention.</li> <li>the target words associated with more powerful retrieval cues were more recallable than those associated with less powerful retrieval cues</li> <li>Probably, more powerfully associated retrieval cues better triggered the schematic memory, which created a 'short cut' that linked the process needed for recalling the contextual meaning of the target word and the initial process involved in inferring the contextual meaning of that target word.</li> <li>contextual cues being equally adequate (not inadequate), subjects in the reading group scored significantly higher in both word inference and word retention than subjects in the listening group.</li> <li>not clear why this was so</li> <li>That is, the subjects might be more competent in reading than in listening contextual cues presented visually were more accessible</li> </ul>"},{"location":"KB/Effects%20of%20Contextual%20Cues%20on%20Inferring%20and%20Remembering%20Meanings%20of%20New%20Word/#implications","title":"Implications","text":"<ul> <li>irst, since adequate cues in context can relieve learners of English as a second language from the anxiety of unfamiliar words, it might follow that reasonably sufficient contextual cues should be provided in texts for second language learners, so that enough information can be created for them to play the 'psychoUnguistic guessing game' (Goodman 1983)</li> <li>contextual cues can enhance inferring and remembering the meanings of unfamiliar words in context,</li> <li>since contextual cues being equally adequate, subjects can, within the same amount of time, better acquire vocabulary through visual patterns of learning than through oral patterns, it might follow that learners whose learning styles are congruent or similar to the subjects in this study, may well enlarge their vocabulary for reading in a more efficient way through visual ways of learning</li> </ul>"},{"location":"KB/Effects%20of%20Contextual%20Cues%20on%20Inferring%20and%20Remembering%20Meanings%20of%20New%20Word/#pictures","title":"Pictures","text":""},{"location":"KB/Effects%20of%20Regularization/","title":"Effects of Regularization","text":""},{"location":"KB/Effects%20of%20Regularization/#effects-of-regularization","title":"Effects of Regularization","text":"<ul> <li>The Effects of Regularization and Data Augmentation are Class Dependent</li> <li>Current Deep Networks heavily rely on regularizers such as data Augmentation (DA) or Weight Decay, and employ structural risk minimization, i.e., Cross Validation, to select the optimal regularization hyper-parameters</li> <li>weight decay increases the average test performances at the cost of significant performance drops on some specific classes</li> <li>unfair across classes</li> <li>By focusing on maximizing aggregate performance statistics we have produced learning mechanisms that can be potentially harmful, especially in transfer learning tasks</li> <li>optimal amount of DA or weight decay found from cross-validation leads to disastrous model performances on some classes</li> <li>only by introducing random crop DA during training</li> <li>such performance drop also appears when introducing uninformative regularization techniques such as weight decay</li> <li>ur search for ever increasing generalization performance \u2013 averaged over all classes and samples \u2013 has left us with models and regularizers that silently sacrifice performances on some classes.</li> <li>varying the amount of regularization employed during pre-training of a specific dataset impacts the per-class performances of that pre-trained model on different downstream tasks e.g. an ImageNet pre-trained ResNet50 deployed on INaturalist sees its performances fall from 70% to 30% on a particular classwhen introducing random crop DA during the Imagenet pre-training phase</li> <li>designing novel regularizers without class-dependent bias remains an open research question</li> <li>Categories largely identifiable by color or texture (for e.g., yellow bird, textured mushroom) are unaffected by aggressive [cropping], while categories identifiable by shape (for e.g., corkscrew) see a performance degradation with aggressive cropping that only contains part of the object</li> <li>Conversely, color jitter does not affect shape or texture-based categories (for e.g., zebra), but affects color-based categories (for e.g., basket ball)</li> </ul>"},{"location":"KB/Efferent/","title":"Efferent","text":""},{"location":"KB/Efferent/#efferent","title":"Efferent","text":"<ul> <li>Motor Division</li> <li>Instructions from brain to muscle and glands</li> <li>Also includes Somatic + Autonomic</li> </ul>"},{"location":"KB/EfficientNet/","title":"EfficientNet","text":""},{"location":"KB/EfficientNet/#efficientnet","title":"EfficientNet","text":"<ul> <li>@tanEfficientnetRethinkingModel2019</li> </ul>"},{"location":"KB/Ego-motion/","title":"Ego-motion","text":""},{"location":"KB/Ego-motion/#ego-motion","title":"Ego-motion","text":"<ul> <li>self-driving car </li> <li>equipped with various sensors </li> <li>large-scale egocentric video along with ego-motor signal can be easily collected with very low cost by driving the car in the street </li> <li>the correspondence between visual signal and motor signal for self-supervised feature learning </li> <li>correspondence between visual signal and motor signal for s </li> <li>underline intuition of this type of methods is that a self-driving car can be treated as a camera moving in a scene </li> <li>egomotion of the visual data captured by the camera is as same as that of the car </li> <li>correspondence between visual data and egomotion can be utilized for self- supervised feature learning </li> <li>inputs to the network are two frames sampled from an egocentric video within a short time </li> <li>labels for the network indicate the rotation and translation relation between the two sampled images which can be derived from the odometry data of the dataset. </li> <li>ConvNet is forced to identify visual elements that are present in both sampled images. </li> <li>ego-motor signal is a type of accurate supervision signal </li> <li>In addition to directly applying it for self-supervised feature learning, it has also been used for unsupervised learning of depth and ego-motion</li> </ul>"},{"location":"KB/EigenCAM/","title":"EigenCAM","text":""},{"location":"KB/EigenCAM/#eigencam","title":"EigenCAM","text":"<ul> <li>@banymuhammadEigenCAMVisualExplanations2021</li> </ul>"},{"location":"KB/EigenCAM/#summary-by-me","title":"Summary by Me","text":"<p>Another method for computing Saliency Maps without modifying the architecture of the network, EigenCAM was proposed by Bany et al. \\cite{banymuhammadEigenCAMVisualExplanations2021}. EigenCAM uses a combination of an Eigen analysis of the class activated output by projecting it on the input, and a PCA of it to remove unnecessary features from the maps. The Eigen-Saliency map is computed across the network and produces sharper outputs based on the distance (using PCA) from the input image. EigenCAM and Eigen Saliency maps were fused by a point wise multiplication operation. </p>"},{"location":"KB/Eigenvector/","title":"Eigenvector","text":""},{"location":"KB/Eigenvector/#eigenvector","title":"Eigenvector","text":""},{"location":"KB/Einsum/","title":"Einsum","text":""},{"location":"KB/Einsum/#einsum","title":"Einsum","text":"<ul> <li>Matrix transpose<ul> <li>ij -&gt; ji</li> </ul> </li> <li>Sum<ul> <li>ij -&gt;</li> </ul> </li> <li>Column sum<ul> <li>ij -&gt; j</li> </ul> </li> <li>Row sum<ul> <li>ij -&gt; i</li> </ul> </li> <li>Matrix vector multiply<ul> <li>ik, k -&gt; i</li> </ul> </li> <li>Matrix matrix multiply<ul> <li>ik, kj -&gt; ij</li> </ul> </li> <li>Dot product<ul> <li>i,i -&gt;</li> <li>ij, ij -&gt;</li> </ul> </li> <li>Hadmard product<ul> <li>ij, ij -&gt; ij</li> </ul> </li> <li>Outer product<ul> <li>i,j -&gt; ij</li> </ul> </li> <li>Batch matrix multiply<ul> <li>ijk, ikl -&gt; ijl</li> </ul> </li> <li>Tensor Contraction<ul> <li>pqrs , tuvr -&gt; pstuv</li> </ul> </li> </ul>"},{"location":"KB/Elaborateness/","title":"Elaborateness","text":""},{"location":"KB/Elaborateness/#elaborateness","title":"Elaborateness","text":"<ul> <li>depth or detail of the explanation</li> <li>a doctor may tell a patient that their diagnosis looks \"similar to\" another diagnosis</li> <li>This is a \"shallow\" explanation; it does not point to the root cause of the diagnosis</li> </ul>"},{"location":"KB/Electrical%20Energy/","title":"Electrical Energy","text":""},{"location":"KB/Electrical%20Energy/#electrical-energy","title":"Electrical Energy","text":"<ul> <li>Electrical energy changed into heat = potential difference x current x time</li> <li> \\[E= \\frac{V}{t}\\] </li> </ul>"},{"location":"KB/Electroconvulsive%20Therapy%20%28ECT%29/","title":"Electroconvulsive Therapy (ECT)","text":""},{"location":"KB/Electroconvulsive%20Therapy%20%28ECT%29/#electroconvulsive-therapy-ect","title":"Electroconvulsive Therapy (ECT)","text":"<ul> <li>A therapeutic treatment for depression and other mental illnesses that sends small electric currents over the scalp to trigger a brief seizure.</li> </ul>"},{"location":"KB/Electrode%20nomenclature/","title":"Electrode nomenclature","text":"<p>5---</p> <p>toc: true title: Electrode nomenclature</p>"},{"location":"KB/Electrode%20nomenclature/#electrode-nomenclature","title":"Electrode Nomenclature","text":"<ul> <li>Bathing cap</li> <li>Top of head</li> <li>Cz : middle of head</li> <li>Left : odd</li> <li>Right : even</li> <li>Frontal : F</li> <li>Frontal Polar : Fp</li> <li>Temporal : T</li> <li>Posterior : Pz</li> <li>Occipital : Oz</li> </ul>"},{"location":"KB/Elements%20of%20sets/","title":"Elements of sets","text":""},{"location":"KB/Elements%20of%20sets/#elements-of-sets","title":"Elements of Sets","text":"<ul> <li>The stickers we have in stocks are stars, the moons, item and a flag.</li> <li>I\u2019ll take two moons.</li> <li>The moons in the 2nd sentences should be understood to be some of the moons mentioned in the 1st sentence.</li> <li>Notice that to understand the 2nd sentence at all requires that we use the context of the first sentence to establish that the word \u2018moons\u2019 means moon stickers.</li> </ul>"},{"location":"KB/Ellipsoids/","title":"Ellipsoids","text":""},{"location":"KB/Ellipsoids/#ellipsoids","title":"Ellipsoids","text":"<ul> <li>Fractional Anisotropy</li> <li></li> <li>Linear, Planar, Spherical</li> <li>2D projection can convey ambiguous 3D orientation</li> </ul>"},{"location":"KB/Elman%201990/","title":"Elman 1990","text":""},{"location":"KB/Elman%201990/#elman-1990","title":"Elman 1990","text":"<ul> <li>The network learned generalizations</li> <li>examine hidden unit activation pattern for each word measure distance between each pattern and every other pattern (Euclidean distance)</li> <li>Use this to create a hierarchical cluster.</li> <li>Network learned semantic classes</li> <li>If the input to a simulation is preselected to avoid problems, one has instantiated an expert filtering system.</li> <li>in order to accomplish the goal of creating word classes by surface structure alone, it appears that the input must be filtered in just the right way.</li> <li>Instead of semantic representations, semantics gets replaced with distributional information<ul> <li>This is not what humans know about word classes.</li> <li>If the simulation's goals are accomplished by avoiding pronouns then we have the equivalent of a pronoun filter</li> </ul> </li> <li>Some strings in English are both nouns and verbs, e.g. smell, break</li> <li>The simulation did not learn what children learn</li> <li>Yes, the input was oversimplified, but it's not clear that adding these additional features will make the model perform worse</li> <li>language is very redundant, so certain simplifications actually remove helpful features</li> <li>Categories can 'emerge' via statistical regularities</li> <li>Basic RNN Architectures can find these</li> </ul>"},{"location":"KB/Elman%201991/","title":"Elman 1990","text":""},{"location":"KB/Elman%201991/#elman-1990","title":"Elman 1990","text":"<ul> <li>The network learned generalizations</li> <li>examine hidden unit activation pattern for each word measure distance between each pattern and every other pattern (Euclidean distance)</li> <li>Use this to create a hierarchical cluster.</li> <li>Network learned semantic classes</li> <li>If the input to a simulation is preselected to avoid problems, one has instantiated an expert filtering system.</li> <li>in order to accomplish the goal of creating word classes by surface structure alone, it appears that the input must be filtered in just the right way.</li> <li>Instead of semantic representations, semantics gets replaced with distributional information<ul> <li>This is not what humans know about word classes.</li> <li>If the simulation's goals are accomplished by avoiding pronouns then we have the equivalent of a pronoun filter</li> </ul> </li> <li>Some strings in English are both nouns and verbs, e.g. smell, break</li> <li>The simulation did not learn what children learn</li> <li>Yes, the input was oversimplified, but it's not clear that adding these additional features will make the model perform worse</li> <li>language is very redundant, so certain simplifications actually remove helpful features</li> <li>Categories can 'emerge' via statistical regularities</li> <li>Basic RNN Architectures can find these</li> </ul>"},{"location":"KB/Elman%201992/","title":"Elman 1990","text":""},{"location":"KB/Elman%201992/#elman-1990","title":"Elman 1990","text":"<ul> <li>The network learned generalizations</li> <li>examine hidden unit activation pattern for each word measure distance between each pattern and every other pattern (Euclidean distance)</li> <li>Use this to create a hierarchical cluster.</li> <li>Network learned semantic classes</li> <li>If the input to a simulation is preselected to avoid problems, one has instantiated an expert filtering system.</li> <li>in order to accomplish the goal of creating word classes by surface structure alone, it appears that the input must be filtered in just the right way.</li> <li>Instead of semantic representations, semantics gets replaced with distributional information<ul> <li>This is not what humans know about word classes.</li> <li>If the simulation's goals are accomplished by avoiding pronouns then we have the equivalent of a pronoun filter</li> </ul> </li> <li>Some strings in English are both nouns and verbs, e.g. smell, break</li> <li>The simulation did not learn what children learn</li> <li>Yes, the input was oversimplified, but it's not clear that adding these additional features will make the model perform worse</li> <li>language is very redundant, so certain simplifications actually remove helpful features</li> <li>Categories can 'emerge' via statistical regularities</li> <li>Basic RNN Architectures can find these</li> </ul>"},{"location":"KB/Elman%201993/","title":"Elman 1990","text":""},{"location":"KB/Elman%201993/#elman-1990","title":"Elman 1990","text":"<ul> <li>The network learned generalizations</li> <li>examine hidden unit activation pattern for each word measure distance between each pattern and every other pattern (Euclidean distance)</li> <li>Use this to create a hierarchical cluster.</li> <li>Network learned semantic classes</li> <li>If the input to a simulation is preselected to avoid problems, one has instantiated an expert filtering system.</li> <li>in order to accomplish the goal of creating word classes by surface structure alone, it appears that the input must be filtered in just the right way.</li> <li>Instead of semantic representations, semantics gets replaced with distributional information<ul> <li>This is not what humans know about word classes.</li> <li>If the simulation's goals are accomplished by avoiding pronouns then we have the equivalent of a pronoun filter</li> </ul> </li> <li>Some strings in English are both nouns and verbs, e.g. smell, break</li> <li>The simulation did not learn what children learn</li> <li>Yes, the input was oversimplified, but it's not clear that adding these additional features will make the model perform worse</li> <li>language is very redundant, so certain simplifications actually remove helpful features</li> <li>Categories can 'emerge' via statistical regularities</li> <li>Basic RNN Architectures can find these</li> </ul>"},{"location":"KB/Elu/","title":"Elu","text":""},{"location":"KB/Elu/#elu","title":"Elu","text":"<ul> <li> \\[f(x) = max(x, a \\cdot (e^x-1))\\] </li> <li></li> </ul>"},{"location":"KB/Embedding%20Human%20Knowledge%20into%20Deep%20Neural%20Network%20via%20Attention%20Map/","title":"Embedding Human Knowledge into Deep Neural Network via Attention Map","text":""},{"location":"KB/Embedding%20Human%20Knowledge%20into%20Deep%20Neural%20Network%20via%20Attention%20Map/#embedding-human-knowledge-into-deep-neural-network-via-attention-map","title":"Embedding Human Knowledge into Deep Neural Network via Attention Map","text":"<ul> <li>@mitsuharaEmbeddingHumanKnowledge2019</li> </ul>"},{"location":"KB/Embedding%20Human%20Knowledge%20into%20Deep%20Neural%20Network%20via%20Attention%20Map/#intro","title":"Intro","text":"<ul> <li>focus on the attention mechanism of an attention branch network (ABN)</li> <li>propose a fine-tuning method that utilizes a single-channel attention map which is manually edited by a human expert.</li> <li>Our fine-tuning method can train a network so that the output attention map corresponds to the edited ones. As a result, the fine-tuned network can out- put an attention map that takes into account human knowl- edge</li> <li>ImageNet</li> <li>CUB-200-2010</li> <li>IDRiD</li> <li>human intuitive edit- ing via a visual interface and suggest new possibilities for human-machine cooperation in addition to the improvement of visual explanations.</li> <li>Typical visual explanation approaches in- clude class activation mapping (CAM) and Grad-CAM</li> <li>However, an inconsistency between the tar- get region of the recognition result, namely the ground truth (GT), and an attention region may occur.</li> <li>To this end, we focus on the visual explanation and the attention mechanism of ABN</li> <li>ABN applies an atten- tion map for visual explanation to the attention mechanism.</li> <li>We propose a fine-tuning method based on the characteristics of ABN and an edited attention map</li> <li>The proposed method fine-tunes the attention and perception branches of ABN to output the same attention map as the edited one.</li> </ul>"},{"location":"KB/Embedding%20Human%20Knowledge%20into%20Deep%20Neural%20Network%20via%20Attention%20Map/#related-work","title":"Related Work","text":""},{"location":"KB/Embedding%20Human%20Knowledge%20into%20Deep%20Neural%20Network%20via%20Attention%20Map/#embedding-human-knowledge","title":"Embedding Human Knowledge","text":"<ul> <li>human-in-the- loop (HITL)</li> <li>Branson et al. [4] pro- posed an interactive HITL approach that helps to train a decision tree by using a question and answer with respect to a specific bird.</li> <li>Deng et al. [7] used a bubble, that is, a circular bounding box, as human knowl- edge. This bubble information is annotated from an atten- tion region when a user distinguishes the two types of birds. By annotating the bubble with various pairs and users, char- acteristic regions of bird images can be obtained when we recognize bird categories.</li> <li>Linsley et al. [18] proposed a method that incorpo- rates human knowledge into large-scale deep neural net- works using the HITL framework. This method added a spatial attention mechanism into the attention mecha- nism [19, 15, 13, 2, 20, 35, 33, 37, 39, 40] of squeeze-and- excitation networks (SENet) [13] and trained the network by using a ClickMe map that introduces human knowledge to the weights of the attention mechanism.</li> </ul>"},{"location":"KB/Embedding%20Human%20Knowledge%20into%20Deep%20Neural%20Network%20via%20Attention%20Map/#editing-the-attention-map","title":"Editing the Attention Map","text":"<ul> <li>In this experiment, we used an ABN whose backbone is 152-layer ResNet [12] (ResNet152+ABN) as a network mode</li> <li>Then, we selected the 1k misclassified samples from the validation samples and edited the maps</li> <li>btain the attention map from the at- tention branch, where the size of the attention map is 14\u00d714 pixels. Then, we edit the obtained attention map manually. Note that the attention map is resized to 224\u00d7224 pixels and is overlaid with the input image for ease of manual editing. The edited attention map is resized to 14 \u00d7 14 pixels and used for an attention mechanism to infer classification re- sults from the perception branch.</li> <li>By training the attention and percep- tion branches with the edited attention map including hu- man knowledge, ABN can output an attention map that con- siders this knowledge and thereby improve the classification performance.</li> <li>During the fine-tuning process, we update the parameters of the attention and perception branches by using the loss calculated from the attention map obtained from ABN and the edited attention map in addition to the loss of ABN</li> <li>To make an attention map from the bubbles, we use a kernel KB/Density.md estimation with multiple bubbles</li> <li>A dense region of bubbles indicates an impor- tant region for recognizing the bird category.</li> <li>In contrast, the proposed method highlights the local characteristic regions, such as the color and the head of the bird. In addition, the proposed method removes noise from the attention map by fine-tuning. Thus, the proposed method can also improve the performance of fine-grained recognition.</li> <li>Consequently, our method can gener- ate a more interpretable attention map and successfully em- bed human knowledge.</li> </ul>"},{"location":"KB/Embedding%20Human%20Knowledge%20into%20Deep%20Neural%20Network%20via%20Attention%20Map/#fine-tuning-branches","title":"Fine Tuning Branches","text":"<ul> <li>\\(x_i\\) is the i-th sample $$ L_{abn}(x_{i})=L_{att}(x_{i})+L_{per}(x_{i}) $$<ul> <li>where \\(L_{arr}, L_{per}\\) are conventional cross entropy losses for the attention and perception branches, respectively $$ L(x_{i})=L_{abn}(x_{i})+L_{map}(x_{i}) $$</li> </ul> </li> <li>Edited map : M' $$ L_{map}(x_{i})=\\gamma||M'(x_{i})-M(x_{i})||_{2} $$         - \\(\\gamma\\) is a scale factor         - \\(L_{map}\\) is larger than the others, hence needs to be scaled</li> </ul>"},{"location":"KB/Embedding%20Human%20Knowledge%20into%20Deep%20Neural%20Network%20via%20Attention%20Map/#images","title":"Images","text":""},{"location":"KB/Embedding%20ethical%20principles%20in%20collective%20decision%20support%20systems/","title":"Embedding ethical principles in collective decision support systems","text":""},{"location":"KB/Embedding%20ethical%20principles%20in%20collective%20decision%20support%20systems/#embedding-ethical-principles-in-collective-decision-support-systems","title":"Embedding Ethical Principles in Collective Decision Support Systems","text":"<ul> <li>Joshua Greene, Francesca Rossi, John Tasioulas, Kristen Brent Venable, and Brian Williams.</li> <li>authors envisioned a possible way forward to enable human-agent collectives [Jennings et al., 2014] to make ethical collective decisions</li> <li>By imbuing individual agents with ethical decision-making mechanisms (such as those mentioned in the previous section), a population of agents can take on different roles when evaluating choices of action with moral considerations in a given scenario</li> <li>Based on a set of initial ethics rules, more complex rules can be acquired gradually through learning.</li> <li>Their evaluations, manifested in the form of preferences and limited by feasibility constraints, can be aggregated to reach a collective decision</li> <li>need for new forms of preference representation in collective ethical decisionmaking</li> <li>potential candidate actions to choose from can vastly outnumber the number of agents involved which is very different from multi-agent voting scenarios.</li> <li>candidate actions may not be independent from each other, some of them may share certain features which describe their ethical dilemma situations</li> </ul>"},{"location":"KB/Embedding/","title":"Embedding","text":""},{"location":"KB/Embedding/#embedding","title":"Embedding","text":"<ul> <li>More complex than 1 hot</li> <li>Lookup table is an example.<ul> <li> \\[token\\_embedding(i) = gather(W, i)\\] </li> </ul> </li> <li>Say vocabulary is (the cat walks)<ul> <li>Embedding vector v that will be learnt</li> <li>Values like : \\(v_{the}\\), \\(v_{cat}\\), \\(v_{walks}\\)</li> </ul> </li> </ul>"},{"location":"KB/Embolism/","title":"Embolism","text":""},{"location":"KB/Embolism/#embolism","title":"Embolism","text":"<ul> <li>A clot caused by blood, fat, air or other types of fluid, gas or foreign material</li> </ul>"},{"location":"KB/Emergentism/","title":"Emergentism","text":""},{"location":"KB/Emergentism/#emergentism","title":"Emergentism","text":"<ul> <li>What seems symbolic emerges from distributed representations<ul> <li>qualitatively newand more complex structures can emerge from simpler, basic facts</li> <li>Language structure can emerge from simply listening and producing speech</li> <li>Structural properties of language, e.g. part-of-speech (nouns, verbs) can emerge from serial order, distributional properties and procedural memory.</li> </ul> </li> </ul>"},{"location":"KB/Emperical%20Risk/","title":"Emperical Risk","text":""},{"location":"KB/Emperical%20Risk/#emperical-risk","title":"Emperical Risk","text":"<ul> <li>TRAINING ERROR. Mean loss computed over training examples</li> <li> \\[R(f) = \\mathbb{E} _{(X,Y) \\sim P(X,Y)}[l(y, f(x))]\\] </li> <li> \\[R^{emp}(h) = \\frac{1}{N}\\Sigma_{i=1}^{N}L(h(x_{i}), y_{i})\\] </li> <li> <p>joint prob distribution \\(P(X\\in A,Y=c)\\) is unknown</p> <ul> <li>Decision Boundaries</li> </ul> </li> <li> <p>Learning set \\(\\(\\mathcal L\\)\\) is finite</p> </li> <li>Need an estimator to evaluate it<ul> <li>Supervised Learning<ul> <li>Compute \\(\\(\\mathcal L_{train}\\)\\)</li> <li>Risk train = (1/M)(sum of loss values for (y, f(x)))</li> <li>This is an unbiased estimator, so we can use it to approximate the optimal function f* that minimizes \\(\\(\\mathbb{R}\\)\\)</li> <li>This means that we find \\(\\(argmin_{f\\in F} \\hat R(f, \\mathcal{L}_Train)\\)\\) (out of all the possible functions)</li> <li>\\(\\(lim_{M\\rightarrow \\infty}(f^*_{\\mathcal{L}_Train}) = f^*\\)\\) : converges to the fn that minimizes emprical risk</li> </ul> </li> <li>Ordinary least squares regression</li> </ul> </li> </ul>"},{"location":"KB/Enabling%20Device/","title":"Enabling Device","text":""},{"location":"KB/Enabling%20Device/#enabling-device","title":"Enabling Device","text":"<ul> <li>A manually operated device which when continuously activated, permits motion. Releasing the device shall stop robot motion and motion of associated equipment that may present a hazard.</li> </ul>"},{"location":"KB/Encoder%20Decoder%20Attention/","title":"Encoder Decoder Attention","text":""},{"location":"KB/Encoder%20Decoder%20Attention/#encoder-decoder-attention","title":"Encoder Decoder Attention","text":"<ul> <li>Q comes from prev decoder</li> <li>K,V from encoder</li> </ul>"},{"location":"KB/Encodings/","title":"Encoding","text":""},{"location":"KB/Encodings/#encoding","title":"Encoding","text":""},{"location":"KB/Encodings/#discrete-continuous","title":"Discrete -&gt; Continuous","text":""},{"location":"KB/Encodings/#continous-discrete","title":"Continous -&gt; Discrete","text":""},{"location":"KB/End-effector/","title":"End effector","text":""},{"location":"KB/End-effector/#end-effector","title":"End-effector","text":"<ul> <li>An accessory device or tool, specifically designed for attachment to the robot wrist or tool mounting plate to enable the robot to perform its intended task. (Examples may include: gripper, spot weld gun, arc weld gun, spray point gun or any other application tools.)</li> </ul>"},{"location":"KB/Endoscope/","title":"Endoscope","text":""},{"location":"KB/Endoscope/#endoscope","title":"Endoscope","text":"<ul> <li>An optical instrument containing a tube with a lighted end used for internal examinations</li> </ul>"},{"location":"KB/Endpoint/","title":"Endpoint","text":""},{"location":"KB/Endpoint/#endpoint","title":"Endpoint","text":"<ul> <li>The nominal commanded position that a manipulator will attempt to achieve at the end of a path of motion. The end of the distal link.</li> </ul>"},{"location":"KB/Eneco%20Data%20Scientist/","title":"ABN Amro AI Dev","text":""},{"location":"KB/Eneco%20Data%20Scientist/#eneco-motivation-letter-subhaditya","title":"Eneco Motivation Letter - Subhaditya","text":"<p>As I write this motivation letter, the temperatures in NL have hit yet another record for the hottest September ever. One of the biggest reasons for this is global warming, and carbon emissions are a massive component. Eneco's mission to be carbon neutral and help customers shift towards sustainable energy sources greatly resonates with me, so I am applying for this position as a Data Scientist.</p> <p>My expertise is a combination of data analytics and computer vision, and as of a month ago, I also have a Master in Artificial Intelligence from the University of Groningen. I am familiar with the tools required for basic and advanced AI and analytics from internships, research projects, papers, freelance work, and many personal projects. In my previous internship at KPMG, I built 10+ dashboards using PowerBI for a project with the Abu Dhabi government. In other internships, I have made several analytics pipelines and machine learning implementations. </p> <p>In any team, it is essential that each of the members can contribute something to the overall development of the projects. Although I have a lot to learn, I genuinely enjoy developing solutions that have a positive impact, and I can contribute quite a bit to any team I get the chance to work with. If there is anything I do not know, I am also ready to develop those skills quickly.</p>"},{"location":"KB/Energy%20Transferred%20in%20a%20Component/","title":"Energy Transferred in a Component","text":""},{"location":"KB/Energy%20Transferred%20in%20a%20Component/#energy-transferred-in-a-component","title":"Energy Transferred in a Component","text":"<ul> <li>charge passing through it x potential difference acorss it</li> <li> \\[W = QV\\] </li> </ul>"},{"location":"KB/English%20Wikipedia/","title":"English Wikipedia","text":""},{"location":"KB/English%20Wikipedia/#english-wikipedia","title":"English Wikipedia","text":""},{"location":"KB/Ensemble%20Distillation/","title":"Ensemble Distillation","text":""},{"location":"KB/Ensemble%20Distillation/#ensemble-distillation","title":"Ensemble Distillation","text":"<ul> <li>Distilling the Knowledge in a Neural Network</li> <li>training many different models on the same data and then to average their predictions</li> <li>making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets</li> <li>compress the knowledge in an ensemble into a single model</li> <li>MNIST</li> <li>ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse</li> <li>specialist models can be trained rapidly and in parallel</li> <li>distillation works remarkably well even when the transfer set that is used to train the distilled model lacks any examples of one or more of the classes</li> <li>performance of a single really big net that has been trained for a very long time can be significantly improved by learning a large number of specialist nets, each of which learns to discriminate between the classes in a highly confusable cluster.</li> </ul>"},{"location":"KB/Ensemble%20of%20Shape%20Functions/","title":"Ensemble of Shape Functions","text":""},{"location":"KB/Ensemble%20of%20Shape%20Functions/#ensemble-of-shape-functions","title":"Ensemble of Shape Functions","text":"<ul> <li>ESF</li> <li></li> </ul>"},{"location":"KB/Entities%20involving%20in%20actions/","title":"Entities involving in actions","text":""},{"location":"KB/Entities%20involving%20in%20actions/#entities-involving-in-actions","title":"Entities Involving in Actions","text":"<ul> <li>Her house was broken into last week.</li> <li>They took the TV and the stereo.</li> <li>The pronoun \u2018they\u2019 should be recognized as referring to the burglars who broke into the house.</li> </ul>"},{"location":"KB/Entourage%20Plot/","title":"Entourage Plot","text":""},{"location":"KB/Entourage%20Plot/#entourage-plot","title":"Entourage Plot","text":""},{"location":"KB/Entropy%20minimization%20by%20adverarial%20learning/","title":"Entropy minimization by adverarial learning","text":""},{"location":"KB/Entropy%20minimization%20by%20adverarial%20learning/#entropy-minimization-by-adverarial-learning","title":"Entropy Minimization by Adverarial Learning","text":"<ul> <li>A limitation of the entropy loss is related to the absence of structural dependencies between local semantics.</li> <li>This is caused by the aggregation of the pixel-wise prediction entropies by summation.</li> <li>unified adversarial training framework which minimizes indirectly the entropy of target data, by encouraging it to become similar to the source one.</li> <li>minimizing distribution distance between source and target on the weighted self-information space</li> <li>We perform the adversarial adaptation on weighted self-information maps using a fully-convolutional discriminator network</li> <li>the discriminator produces domain classification outputs, i.e., class label for the source (resp. target) domain.</li> <li>discriminate outputs coming from source and target images, and at the same time, train the segmentation network to fool the discriminator.</li> </ul>"},{"location":"KB/Entropy/","title":"Entropy","text":""},{"location":"KB/Entropy/#entropy","title":"Entropy","text":"<ul> <li>Measure of information content</li> <li> \\[H = -\\Sigma_{x}P(x)logP(x) = \\Sigma_{x}P(x)log \\frac{1}{P(x)}\\] </li> <li>Units : bits of \\(log_{2}\\)</li> <li>Uniform Distribution maximizes entropy. Results harder to predict</li> </ul>"},{"location":"KB/Ependymal%20Cell/","title":"Ependymal Cell","text":""},{"location":"KB/Ependymal%20Cell/#ependymal-cell","title":"Ependymal Cell","text":"<ul> <li>Line cavities</li> <li>Create, secrete and circulate Cerebrospinal Fluid (CSF)).md)</li> </ul>"},{"location":"KB/Epigenetics/","title":"Epigenetics","text":""},{"location":"KB/Epigenetics/#epigenetics","title":"Epigenetics","text":"<ul> <li>A subset of genetics that focuses on how specific environmental factors can influence where, when, and how a gene is expressed, resulting in variation in the gene\u2019s related traits.</li> </ul>"},{"location":"KB/Epilepsy/","title":"Epilepsy","text":""},{"location":"KB/Epilepsy/#epilepsy","title":"Epilepsy","text":"<ul> <li>A neurological disorder characterized by abnormal electrical activity in the brain, leading to seizures.</li> </ul>"},{"location":"KB/Epistemic/","title":"Epistemic","text":""},{"location":"KB/Epistemic/#epistemic","title":"Epistemic","text":"<ul> <li>Uncertainty produced by the model</li> <li>Class imbalance etc</li> <li>Reduce by adding more info</li> <li></li> </ul>"},{"location":"KB/Equal%20And%20Opposite%20Force%20Pairs/","title":"Equal And Opposite Force Pairs","text":""},{"location":"KB/Equal%20And%20Opposite%20Force%20Pairs/#equal-and-opposite-force-pairs","title":"Equal And Opposite Force Pairs","text":"<ul> <li>\"When one body exerts a force on a second body, the second body simultaneously exerts a force equal in magnitude and opposite in direction on the first body.\"</li> <li> \\[F_{1}= -F_{2}\\] </li> </ul>"},{"location":"KB/Equality%20of%20Opportunity/","title":"Equality of Opportunity","text":""},{"location":"KB/Equality%20of%20Opportunity/#equality-of-opportunity","title":"Equality of Opportunity","text":"<ul> <li>fairness</li> <li>A fairness metric that checks whether, for a preferred label (one that confers an advantage or benefit to a person) and a given attribute, a classifier predicts that preferred label equally well for all values of that attribute. In other words, equality of opportunity measures whether the people who should qualify for an opportunity are equally likely to do so regardless of their group membership.</li> <li>For example, suppose Glubbdubdrib University admits both Lilliputians and Brobdingnagians to a rigorous mathematics program. Lilliputians\u2019 secondary schools offer a robust curriculum of math classes, and the vast majority of students are qualified for the university program. Brobdingnagians\u2019 secondary schools don\u2019t offer math classes at all, and as a result, far fewer of their students are qualified. Equality of opportunity is satisfied for the preferred label of \"admitted\" with respect to nationality (Lilliputian or Brobdingnagian) if qualified students are equally likely to be admitted irrespective of whether they're a Lilliputian or a Brobdingnagian.</li> <li>For example, let's say 100 Lilliputians and 100 Brobdingnagians apply to Glubbdubdrib University, and admissions decisions are made as follows</li> </ul>"},{"location":"KB/Equalized%20Odds/","title":"Equalized Odds","text":""},{"location":"KB/Equalized%20Odds/#equalized-odds","title":"Equalized Odds","text":"<ul> <li>A fairness metric that checks if, for any particular label and attribute, a classifier predicts that label equally well for all values of that attribute.</li> <li>For example, suppose Glubbdubdrib University admits both Lilliputians and Brobdingnagians to a rigorous mathematics program. Lilliputians' secondary schools offer a robust curriculum of math classes, and the vast majority of students are qualified for the university program. Brobdingnagians' secondary schools don\u2019t offer math classes at all, and as a result, far fewer of their students are qualified. Equalized odds is satisfied provided that no matter whether an applicant is a Lilliputian or a Brobdingnagian, if they are qualified, they are equally as likely to get admitted to the program, and if they are not qualified, they are equally as likely to get rejected.</li> </ul>"},{"location":"KB/Equations%20of%20motion/","title":"Equations of motion","text":""},{"location":"KB/Equations%20of%20motion/#equations-of-motion","title":"Equations of Motion","text":"<ul> <li>\\(\\Delta x\\) is displacement</li> <li>$\\Delta t $ is time</li> <li>v is final velocity</li> <li>u is initial velocity</li> <li>a is acceleration</li> <li> \\[v = u + a\\Delta t\\] </li> <li> \\[\\Delta x = u \\Delta t + \\frac{1}{2}a \\Delta t^{2}\\] </li> <li> \\[\\Delta x = \\frac{1}{2}(v+u) \\Delta t\\] </li> <li> \\[v^{2}= u^{2}+2a \\Delta x\\] </li> </ul>"},{"location":"KB/Equivalent%20Current%20Dipole/","title":"Equivalent Current Dipole","text":""},{"location":"KB/Equivalent%20Current%20Dipole/#equivalent-current-dipole","title":"Equivalent Current Dipole","text":"<ul> <li>Generates an electric field</li> <li></li> <li>Perpendicular is Magnetic field - MEG</li> </ul>"},{"location":"KB/Eraneous/","title":"Eraneous","text":"","tags":["jobs"]},{"location":"KB/Eraneous/#eraneous","title":"Eraneous","text":"<p>Hey Naiyara, This is Subhaditya, I just finished my masters in AI and am now looking for an AI job in the NL. I found some really cool projects from Eraneos and was wondering if there were any opportunities to work there. It would be awesome to have a chat :) Best, SM</p>","tags":["jobs"]},{"location":"KB/Ergodic/","title":"Ergodic","text":""},{"location":"KB/Ergodic/#ergodic","title":"Ergodic","text":"<ul> <li>If only one Invariant Distribution</li> <li>Sequence of distributions \\(g^{(n)}\\) converges to g from any initial distribution</li> <li>Asymptotic, stationary, equilibrium distribution</li> </ul>"},{"location":"KB/Ethical%20dilemmas/","title":"Ethical dilemmas","text":""},{"location":"KB/Ethical%20dilemmas/#ethical-dilemmas","title":"Ethical Dilemmas","text":"<ul> <li>situations in which any available choice leads to infringing some accepted ethical principle and yet a decision has to be made</li> </ul>"},{"location":"KB/Euclidean%20Distance/","title":"Euclidean Distance","text":""},{"location":"KB/Euclidean%20Distance/#euclidean-distance","title":"Euclidean Distance","text":"<ul> <li> \\[d = \\sqrt{\\Sigma_{i=1}^{n}(p_{i}-q_{i})^{2}}\\] </li> <li>It is a distance measure that best can be explained as the length of a segment connecting two points.</li> <li>calculated from the cartesian coordinates of the points using the Pythagorean theorem</li> <li>Euclidean distance is not scale in-variant which means that distances computed might be skewed depending on the units of the features. Typically, one needs to\u00a0normalize\u00a0the data before using this distance measure.</li> <li>Moreover, as the dimensionality increases of your data, the less useful Euclidean distance becomes. This has to do with the curse of dimensionality</li> <li>works great when you have low-dimensional data and the magnitude of the vectors is important to be measured</li> </ul>"},{"location":"KB/Eugenics/","title":"Eugenics","text":""},{"location":"KB/Eugenics/#eugenics","title":"Eugenics","text":"<ul> <li>A 19th century scientific theory that advocated for selective mating of people with desirable hereditary traits.</li> </ul>"},{"location":"KB/Euler%20Integration/","title":"Euler Integration","text":""},{"location":"KB/Euler%20Integration/#euler-integration","title":"Euler Integration","text":"<ul> <li> \\[\\frac{dx}{dt} = f(x,t), x(t_{0}) = x_{0}\\] </li> <li>First order integration</li> <li>Midpoint Method</li> <li>Runge Kutta</li> </ul>"},{"location":"KB/Eulerian%20Grid/","title":"Eulerian Grid","text":""},{"location":"KB/Eulerian%20Grid/#eulerian-grid","title":"Eulerian Grid","text":"<ul> <li>Focus on domain</li> <li>Properties given on a grid  </li> <li>(Position of particles is implicit)</li> <li></li> </ul>"},{"location":"KB/Europarl-ST/","title":"Europarl-ST","text":""},{"location":"KB/Europarl-ST/#europarl-st","title":"Europarl-ST","text":"<ul> <li>multilingual speech translation corpus</li> <li>based on speeches and debates in the European parliament between 2008-2013</li> <li>Creative Common Non-Commercial license</li> <li>data belongs to the European Union</li> <li>by releasing, authors want to improve speech translation</li> <li>consists of audio, transcript and translation of the transcript</li> <li>72 translation directions</li> <li>includes also noisy samples</li> </ul>"},{"location":"KB/Even%20angels%20need%20the%20rules%20AI%2C%20roboethics%2C%20and%20the%20law/","title":"Even angels need the rules AI, roboethics, and the law","text":""},{"location":"KB/Even%20angels%20need%20the%20rules%20AI%2C%20roboethics%2C%20and%20the%20law/#even-angels-need-the-rules-ai-roboethics-and-the-law","title":"Even Angels Need the Rules AI, Roboethics, and the Law","text":"<ul> <li>Ugo Pagallo</li> <li>Collective Ethical Decision Frameworks</li> </ul>"},{"location":"KB/Evidence%20For%20Distributivity%20Effects%20in%20Comprehension/","title":"Evidence For Distributivity Effects in Comprehension","text":""},{"location":"KB/Evidence%20For%20Distributivity%20Effects%20in%20Comprehension/#evidence-for-distributivity-effects-in-comprehension","title":"Evidence For Distributivity Effects in Comprehension","text":"<ul> <li>Nikole D. Patson and Tessa Warren</li> </ul>"},{"location":"KB/Evidence%20For%20Distributivity%20Effects%20in%20Comprehension/#intro","title":"Intro","text":"<ul> <li>In the current paper, we introduce a new methodology for detecting whether a word in a sentence is conceptually represented as plural and use it to shed light on a debate about whether comprehenders interpret singular indefinite noun phrases within a distributed predicate as plural during on-line reading.</li> <li>self-paced reading on a sentence presented in one- and two-word chunk</li> <li>indicated that participants were slower to judge that one word was on the screen when the word was plural (e.g., cats) than when it was singular (e.g., cat)</li> <li>build different conceptual representations for distributed versus collective predicates, and interpret a singular indefinite noun phrase within a distributed predicate as plura</li> </ul>"},{"location":"KB/Evidence%20For%20Distributivity%20Effects%20in%20Comprehension/#results","title":"Results","text":"<ul> <li>When the group was conceptually distributed (5a), participants incorrectly (in American English) used a plural verb (were) more often in their continuations than when the group was conceptually collective (5b). This indicates that participants were more likely to treat the gang as a plural when its individuals were more salient (5a) rather than when the group was the relevant referent (5b). This suggests that distributivity can make grammatically singular lexical items that have plural referents (e.g., gang, group) functionally plural during language production.</li> <li>This production and off-line comprehension work suggests that readers build different conceptual representations for collective and distributed predicates, and is consistent with the hypothesis that singular indefinite noun phrases within distributed predicates are often treated as conceptually plural.</li> </ul>"},{"location":"KB/Evidence%20For%20Distributivity%20Effects%20in%20Comprehension/#experiment-1","title":"Experiment 1","text":"<ul> <li>Experiment 1 was conducted to test whether the Berent et al. (2005) methodology could be extended to sentences. It is possible that the added complexity involved in building and maintaining a sentence representation during the number judgment task, or the task demands of simultaneously carrying out self-paced reading and number judgments, might make participants less sensitive to interference than they were in Berent et al. (2005). Experiment 1 is also important because in order to use the paradigm to test ambiguous cases (as in Experiment 2), we must first establish that the paradigm works on simple, unambiguous sentences.</li> <li>The critical measure was the reaction time for the number judgment for correct number judgment trials only. There was a significant main effect of noun type such that 'one' responses were slower when the target word was plural</li> <li>Experiment 1 confirmed that even in sentential contexts, semantic plural information on a word interferes with singular number judgments.</li> <li>Specifically, if the distributing quantifier takes wide scope over the indefinite, and comprehenders build conceptual representations for distributed predicates that contain multiple exemplars of the referent introduced by the singular indefinite noun phrase, then one-word judgment times should be slower for indefinite noun phrases in distributed predicates than collective predicates.</li> <li>The experiment had a 2 \u00d7 2 within-participants design. The first factor was the quantifier type and was either distributed (a) or collective (b)</li> <li>They were asked to rate on a scale of 1 \u2013 5 (where 1 was 'definitely one' and 5 was 'definitely more than one') whether the last word in the sentence referred to one or more than one object. Results indicated that the singular-marked distributed items were indeed biased toward a plural interpretation of the noun phrase. Participants rated the singular-marked distributed items as being closer to the 'definitely more than one' end of the scale</li> <li>than the singular-marked collective items</li> </ul>"},{"location":"KB/Evidence%20For%20Distributivity%20Effects%20in%20Comprehension/#experiment-2","title":"Experiment 2","text":"<ul> <li>The results of Experiment 2 confirm the hypothesis that singular indefinite noun phrases in distributed predicates can indeed be treated as conceptually plural during reading</li> <li>There was no reliable effect of distributivity and no reliable difference between the plural-marked conditions, indicating that the difference in the singular-marked conditions was unlikely to be the result of one kind of predicate being more costly to compute than the other. These results indicate that in these items the distributing quantifier took wide scope over the indefinite</li> <li>indicate that conceptual plurality interferes with number judgments during sentence comprehension</li> <li>These findings (Filik et al., 2004; Paterson et al., 2008) indicate that comprehenders do not build conceptually plural referents on-line for indefinite noun phrases in distributed structures that off-line norming had indicated were likely to be interpreted as plural.</li> </ul>"},{"location":"KB/Explainability%20Defn/","title":"Explainability Defn","text":""},{"location":"KB/Explainability%20Defn/#explainability-defn","title":"Explainability Defn","text":"<ul> <li>associated with the notion of explanation as an interface between humans and a decision maker that is, at the same time, both an accurate proxy of the decision maker and comprehensible to humans</li> </ul>"},{"location":"KB/Explainability%20Taxonomy/","title":"Explainability Taxonomy","text":""},{"location":"KB/Explainability%20Taxonomy/#explainability-taxonomy","title":"Explainability Taxonomy","text":"<ul> <li>Explainable Artificial Intelligence (XAI) Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI</li> <li>Analysis of Explainers of Black Box Deep Neural Networks for Computer Vision A Survey</li> </ul>"},{"location":"KB/Explainable%20Artificial%20Intelligence%20%28XAI%29%20Concepts%2C%20Taxonomies%2C%20Opportunities%20and%20Challenges%20toward%20Responsible%20AI/","title":"Explainable Artificial Intelligence (XAI) Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI","text":""},{"location":"KB/Explainable%20Artificial%20Intelligence%20%28XAI%29%20Concepts%2C%20Taxonomies%2C%20Opportunities%20and%20Challenges%20toward%20Responsible%20AI/#explainable-artificial-intelligence-xai-concepts-taxonomies-opportunities-and-challenges-toward-responsible-ai","title":"Explainable Artificial Intelligence (XAI) Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI","text":"<ul> <li>@arrietaExplainableArtificialIntelligence2019</li> <li> </li> <li>Understandability</li> <li>Comprehensibility</li> <li>Interpretability</li> <li>Explainability Defn</li> <li>Transparency</li> <li>Trustworthiness</li> <li>Causality</li> <li>Transferability</li> <li>Informativeness</li> <li>Confidence</li> <li>Fairness</li> <li>Accessibility</li> <li>Interactivity</li> <li>Privacy awareness</li> </ul>"},{"location":"KB/Explainable%20Artificial%20Intelligence%20%28XAI%29%20Concepts%2C%20Taxonomies%2C%20Opportunities%20and%20Challenges%20toward%20Responsible%20AI/#charactersitics","title":"Charactersitics","text":""},{"location":"KB/Explainable%20Artificial%20Intelligence%20%28XAI%29%20Concepts%2C%20Taxonomies%2C%20Opportunities%20and%20Challenges%20toward%20Responsible%20AI/#on-the-tradeoff-between-interpretability-and-performance","title":"On the Tradeoff between Interpretability and Performance","text":"<ul> <li>it is not necessarily true that models that are more complex are inherently more accurate</li> <li>It is in this situation that the trade-off between performance and interpretability can be observed</li> <li>In this path toward performance, when the performance comes hand in hand with complexity, interpretability encounters itself on a downwards slope that until now appeared unavoidable</li> <li>Another aspect worth mentioning at this point due to its close link to model interpretability and performance is the approximation dilemma: explanations made for a ML model must be made drastic and approximate enough to match the requirements of the audience for which they are sought, ensuring that explanations are representative of the studied model and do not oversimplify its essential features.</li> <li>confluence of multiple criteria</li> <li>need for having the human in the loop</li> <li>Contextual factors, potential impacts and domain-specific needs must be taken into account when devising an approach to interpretability</li> <li>a thorough understanding of the purpose for which the AI model is built</li> <li>the complexity of explanations that are required by the audience</li> <li>the performance and interpretability levels of existing technology, models and methods</li> <li>Interpretable techniques should be preferred when possible</li> <li>black-box models such as those reviewed in this work (namely, support vector machines, ensemble methods and neural networks) should be selected only when their superior modeling capabilities fit best the characteristics of the problem at hand.</li> <li>If a black-box model has been chosen, the third guideline establishes that ethics-, fairnessand safetyrelated impacts should be weighed</li> <li>rethink interpretability in terms of the cognitive skills, capacities and limitations of the individual human</li> </ul>"},{"location":"KB/Explainable%20Artificial%20Intelligence%20%28XAI%29%20Concepts%2C%20Taxonomies%2C%20Opportunities%20and%20Challenges%20toward%20Responsible%20AI/#fairness-and-discrimination","title":"Fairness and Discrimination","text":"<ul> <li>KB/Individual Fairness.md</li> <li>Group fairness</li> <li>KB/Counterfactual Fairness.md</li> <li>Skewed data</li> <li>Tainted data</li> <li>Limited features</li> <li>Proxy features</li> <li>Independence</li> <li>Separation</li> <li>Sufficiency</li> </ul>"},{"location":"KB/Explainable%20Artificial%20Intelligence%20%28XAI%29%20Concepts%2C%20Taxonomies%2C%20Opportunities%20and%20Challenges%20toward%20Responsible%20AI/#accountability","title":"Accountability","text":"<ul> <li>Auditability</li> <li>Minimization and reporting of negative impacts</li> <li>Redress</li> </ul>"},{"location":"KB/Explanation%20is%20not%20a%20Technical%20Term/","title":"Explanation is not a Technical Term","text":""},{"location":"KB/Explanation%20is%20not%20a%20Technical%20Term/#explanation-is-not-a-technical-term","title":"Explanation is not a Technical Term","text":"<ul> <li>@gilpinExplanationNotTechnical2022</li> </ul>"},{"location":"KB/Explanation%20is%20not%20a%20Technical%20Term/#abstract","title":"Abstract","text":"<ul> <li>Artificial Intelligence (XAI) and those explanations that users and other audiences actually need, which should be defined by the full spectrum of functional roles, audiences, and capabilities for explanation</li> <li>In this paper, we explore the features of explanations and how to use those features in evaluating their utility.</li> <li>we discuss the risk of XAI enabling trust in systems without establishing their trustworthiness and define a critical next step for the field of XAI to establish metrics to guide and ground the utility of system-generated explanations</li> </ul>"},{"location":"KB/Explanation%20is%20not%20a%20Technical%20Term/#introduction","title":"Introduction","text":"<ul> <li>The problem is that explainability is not a well-defined goal: there is no common definition, metrics, or benchmarks for success.</li> <li>Rather than taking a view of explanations as undi\u21b5erentiated artifacts shared by multiple users, we view them as generated in response to their functional roles, audience, and data access</li> <li>Functional role: How is the explanation going to be used?</li> <li>Audience: To whom is it directed and what is their knowledge of the system and domain?</li> <li>Capabilities: What are the capabilities of the system constructing the explanation and the source of data/knowledge used to do so?</li> </ul>"},{"location":"KB/Explanation%20is%20not%20a%20Technical%20Term/#motivation","title":"Motivation","text":"<ul> <li>Because explanations are links between agents, we also need to consider the audience and their knowledge background</li> <li>An explanation's functional role defines the information that needs to be communicated.</li> <li>The state of the audience's initial knowledge of both the domain and processes define a second set of requirements related to the detail and vocabulary used in the explanation.</li> <li>These three factors define the basis for metrics for an evaluation calculus that can be used to evaluate an explanation based on whether it serves the right functional role with the right level of elaboration for its audience supported by the system's knowledge of its own reasoning.</li> <li>By unpacking the idea of \"explanation\" into these factors, XAI can go beyond the \"checking the box\" phase to one in which explanations can play the role for which they were designed</li> </ul>"},{"location":"KB/Explanation%20is%20not%20a%20Technical%20Term/#the-requirements-functional-roles","title":"The Requirements: Functional Roles","text":"<ul> <li>Unfortunately, one of the early findings in the realm of explanation was the discovery that the human bar on what constitutes a \"good explanation\" is shockingly low.</li> <li>As Langer discovered in 1978, people are satisfied with \"Placebic\" information if it has the syntactic form of an explanation [13].</li> <li>A system's capabilities and access to knowledge about its own reasoning determine the scope and validity of the explanations it can generate.</li> </ul>"},{"location":"KB/Explanation%20is%20not%20a%20Technical%20Term/#engineers-and-developers","title":"Engineers and Developers","text":"<ul> <li>explanations are close to the machinery, and the shared language is technical and can contain machine representation</li> <li>These users have experience working with models, understand limitations, can run experiments, and can intuit from incomplete or partial \"explanations\" as they perform their task: debugging and iterating on the model to improve its performance and make it trustworthy</li> </ul>"},{"location":"KB/Explanation%20is%20not%20a%20Technical%20Term/#doctors","title":"Doctors","text":"<ul> <li>explanations are less about the core mechanics of the models themselves and reflect the logic of the domain, a\u21b5ording exploration, counterfactuals, cohort comparison and transparent reasoning about the features most pertinent to a diagnosis</li> <li>if a system provides a warning that a particular patient is showing signs of possible heart failure, a doctor will want a list of relevant factors in order of importance or concern, as well as the patient's prognosis compared to that of other patients, and how that prognosis changes if certain factors are amended</li> <li>At their best, the interaction between a doctor and an intelligent system should seek to mirror the sorts of interactions two doctors might have when collaborating on the task of developing a diagnosis and refining a treatment plan.</li> </ul>"},{"location":"KB/Explanation%20is%20not%20a%20Technical%20Term/#patients","title":"Patients","text":"<ul> <li>patient's goal is more immediate and comes with higher personal stakes</li> <li>This is the realm of personal decision making and, in cases of emergency, immediate action</li> <li>These individuals are less informed about aspects of the medical domain, and systems tailored to them must account for that information asymmetry.</li> <li>Thus, XAI, in this use case, becomes less about collaboration or justification and instead is geared towards confidence building, risk assessment, contextualization, and guided calls to action</li> </ul>"},{"location":"KB/Explanation%20is%20not%20a%20Technical%20Term/#regulators-and-auditors","title":"Regulators and Auditors","text":"<ul> <li>require explanations that scope across the mechanisms of a system, the data used to train it, and the medical practices that it embodies.</li> <li>they require explanations that might include elements of the human in the loop in order to determine responsibility and culpability.</li> <li>engineers</li> <li>building a system, explanations need to touch on the aspects of a system's decisions that can be used in debugging, referencing the data, feature selection, and comparisons.</li> <li>users interpreting the recommendations of a system</li> <li>explanations need to include features that can be used to support exploration of hypotheticals, counterfactuals, cohort comparison and likelihoods.</li> <li>explanations need to support trust and confidence building, risk assessment, contextualization, and decision support</li> <li>stakeholders impacted by a decision</li> </ul>"},{"location":"KB/Explanation%20is%20not%20a%20Technical%20Term/#auditors-and-regulators","title":"Auditors and Regulators","text":"<ul> <li>explanations need to support comparisons and aggregate review of performance and the trail of both algorithmic and human decisions that led to it</li> </ul>"},{"location":"KB/Explanation%20is%20not%20a%20Technical%20Term/#the-requirements-user-knowledge","title":"The Requirements: User Knowledge","text":"<ul> <li>The knowledge state of the various stakeholders interacting with the system defines another</li> <li>As noted above, the functional role of XAI starts in the realm of model debugging and diagnostics \u2013 a tool set aimed at technical users capable of deciphering machine representations, interpreting model performance metrics, and updating model training to improve performance</li> <li>Domain experts may not need explanations that provide insight into the technical workings of systems</li> <li>They want and understand explanations at the domain level.</li> <li>While it is often the case that they have the right level of domain knowledge, it is always a possibility that they lack detailed knowledge of specific domain level features and ideas.</li> <li>Impacted stakeholders are highly variable</li> <li>they may have no knowledge of either the domain or the technology. They do not have expertise in either but do have basic knowledge of how the world works.</li> <li>Stakeholders such as auditors and regulators have specialized knowledge of the ways in which data and algorithms interact and how to look at the performance of a system through the lens of comparison and systemic issues.</li> </ul>"},{"location":"KB/Explanation%20is%20not%20a%20Technical%20Term/#system-capabilities","title":"System Capabilities","text":""},{"location":"KB/Explanation%20is%20not%20a%20Technical%20Term/#interpretability","title":"Interpretability","text":"<ul> <li>how understandable the output representation is to the audience.</li> <li>depends on the target audience and the task</li> </ul>"},{"location":"KB/Explanation%20is%20not%20a%20Technical%20Term/#accuracy","title":"Accuracy","text":"<ul> <li>based on correctness</li> <li>except that it is in terms of the explanation itself</li> <li>dependent on both the domain and the user</li> <li>A saliency map applied to cancer images may highlight the hospital name, indicating that the reasons supporting the diagnosis is</li> <li>the hospital where the image was taken [17]</li> <li> <p>This explanation is true to the model but the model is faulty. The explanation is accurate but the model is not.</p> </li> <li> <p>Elaborateness</p> </li> <li> <p>Faithfulness</p> </li> </ul>"},{"location":"KB/Explanation%20is%20not%20a%20Technical%20Term/#trusted-but-not-trustworthy-a-new-dark-pattern","title":"Trusted but not Trustworthy: A New Dark Pattern","text":"<ul> <li>One of the dominant themes in XAI is the notion of trust</li> <li>The focus is on getting users to trust a system rather than on making the system trustworthy.</li> <li>Such trust is developed through output assessment over time and through a host of factors external to the model output itself (including the apparent trust of other experts, design decisions at the system and user experience levels, and ancillary components such as domain-informed conversational interfaces).</li> <li>hard won and easily lost</li> </ul>"},{"location":"KB/Explanation%20is%20not%20a%20Technical%20Term/#discussion-metrics-the-next-critical-step-for-xai","title":"Discussion: Metrics - the next Critical step for XAI","text":"<ul> <li>Explanations should provide new insights: explanations should go beyond the \"why, what, how\" [20]</li> <li>We must also be able to compare and contrast them, as explanations can disagree and directly contradict each other [21]</li> <li>We also want to quantify the system capabilities: how well the explanation actually explains</li> <li>When we as humans explain something, it is a deductive process. Each claim follows from the last claim</li> <li>The crucial point is that explanations need to fit the functional role in a manner that is interpretable to the audience</li> <li>In evaluating an explanation, the question is whether it fits the role and the audience</li> <li>With a variety of explanation types, a single \"one size fits all\" metric is inappropriate</li> <li>Explanations must be built, interpreted, and evaluated through the lens of the functions that they serve and the knowledge situations in which they do so; through the lens of requirements and a system's capability to meet them.</li> <li>As work in XAI continues, it must attend to what exactly is being explained, to whom the explanation is directed, and how it is going to be used, and must provide a structured, standard evaluation approach via metrics of success.</li> </ul>"},{"location":"KB/Explanation%20is%20not%20a%20Technical%20Term/#images","title":"Images","text":""},{"location":"KB/Explanator/","title":"Explanator","text":""},{"location":"KB/Explanator/#explanator","title":"Explanator","text":"<ul> <li>synonym for an explaining system or explaining process that gives answers to questions in understandable terms, which could, computationally, be considered a program execution trace.</li> <li>or instance, if the question is how a machine is working, the explainer makes the internal structure of a machine more transparent to humans</li> </ul>"},{"location":"KB/Exploding%20Gradient/","title":"Exploding Gradient","text":""},{"location":"KB/Exploding%20Gradient/#exploding-gradient","title":"Exploding Gradient","text":"<ul> <li>weight matrices have max eigenvalue \\(max_{i} \\lambda_{j} &gt; 1\\) , gradient increases per layer <ul> <li>if enough depth, then converges to infinity </li> </ul> </li> </ul>"},{"location":"KB/Exponential%20Distribution/","title":"Exponential Distribution","text":""},{"location":"KB/Exponential%20Distribution/#exponential-distribution","title":"Exponential Distribution","text":"<ul> <li>How long you have to wait for something after the it has happened once already</li> <li>Average rate /unit reference time</li> <li>PDF \\(\\(p(x) = \\lambda e^{-\\lambda x}\\)\\) and \\(x \\geq 0\\)</li> <li>Expectation \\(\\(E(X) = \\frac{1}{\\lambda}\\)\\)</li> <li></li> <li>Rate : \\(\\(\\hat \\lambda = \\frac{1}{N-1}\\Sigma_{i = 1, \u2026, N}t_{i+1}-t_{i}\\)\\)</li> <li>Spiking Networks</li> </ul>"},{"location":"KB/Extensions%20to%20SlimStampen/","title":"Extensions to SlimStampen","text":""},{"location":"KB/Extensions%20to%20SlimStampen/#extensions-to-slimstampen","title":"Extensions to SlimStampen","text":"<ul> <li>Konteksti<ul> <li>Semantic similarity</li> </ul> </li> <li>Increase activation of similar facts</li> <li>Dual-lingo<ul> <li>Word + Picture based cues</li> </ul> </li> <li>Type of info<ul> <li>Eg vocabulary</li> </ul> </li> <li>Perfect pitch<ul> <li>auditory stimuli</li> </ul> </li> <li>Fun with flags<ul> <li>Improve scheduling by computing a continous stimuli</li> </ul> </li> <li>CramDroid<ul> <li>App that helps with between-sessions</li> <li>Track decay functions and send notif</li> </ul> </li> <li>Space Times<ul> <li>Game based memorization of tables</li> </ul> </li> <li>Vocab Warrior<ul> <li>play against an ACT-R</li> <li>answer faster</li> </ul> </li> </ul>"},{"location":"KB/Extra-position/","title":"Extra-position","text":""},{"location":"KB/Extra-position/#extra-position","title":"Extra-position","text":"<ul> <li>Did anyone who you expected to help actually help?</li> <li>Did anyone actually help who you expected to help?</li> </ul>"},{"location":"KB/Eye%20Tracking/","title":"Eye Tracking","text":""},{"location":"KB/Eye%20Tracking/#eye-tracking","title":"Eye Tracking","text":"<ul> <li>Gaze position</li> </ul>"},{"location":"KB/Eye-to-hand%20System/","title":"Eye-to-hand System","text":""},{"location":"KB/Eye-to-hand%20System/#eye-to-hand-system","title":"Eye-to-hand System","text":"<ul> <li>The task in visual servoing is to use visual information to control the robot's endeffector relative to a target object.</li> <li>provide feedback to the robot controller at each time step</li> <li></li> </ul>"},{"location":"KB/FGSM/","title":"FGSM","text":""},{"location":"KB/FGSM/#fgsm","title":"FGSM","text":"<ul> <li>FGSM is a method of generating noise in the direction of the cost function gradient concerning the data</li> <li>Given original input image x, label y, model parameter \u03b8, and loss J.  </li> <li> \\[adv_{x}= x+ \\epsilon \\ast sign(\\nabla_{x}(J(\\theta, x, y)))\\] </li> <li>this gives us the perturbations</li> </ul>"},{"location":"KB/FGVC%20Aircraft/","title":"FGVC Aircraft","text":""},{"location":"KB/FGVC%20Aircraft/#fgvc-aircraft","title":"FGVC Aircraft","text":"<ul> <li>This dataset contains images of 100 different types of aircrafts, with a total of 10,000 images.</li> </ul>"},{"location":"KB/FGVCx/","title":"FGVCx","text":""},{"location":"KB/FGVCx/#fgvcx","title":"FGVCx","text":"<ul> <li>FGVCx is a dataset that includes a number of fine-grained datasets, such as the FGVC Aircraft, Stanford Cars and Stanford Dogs datasets.</li> </ul>"},{"location":"KB/FHIR/","title":"FHIR","text":"","tags":["medical"]},{"location":"KB/FHIR/#fhir","title":"FHIR","text":"<ul> <li>specific standard within HL7 that leverages modern web technologies and is gaining traction for its flexibility in healthcare data exchange</li> <li>fhir-py<ul> <li><code>apt install protobuf-compiler protoc --version # Ensure version 3+ pip install google-fhir-views[r4,bigquery,spark]</code></li> </ul> </li> </ul>","tags":["medical"]},{"location":"KB/FLASH/","title":"FLASH","text":""},{"location":"KB/FLASH/#flash","title":"FLASH","text":"<ul> <li>Transformer Quality in Linear Time</li> <li>weaknesses in handling long sequences</li> <li>FLASH</li> <li>performant layer (gated linear unit) and by combining it with an accelerator-efficient approximation strategy (mixed chunk attention)</li> <li>GAU</li> <li>Mixed chunk attention</li> <li>outperforms three baselines: vanilla Transformer, Performer and Combiner in terms of quality and efficiency</li> <li>Wiki</li> <li>PG-19</li> </ul>"},{"location":"KB/FLAVA/","title":"FLAVA","text":""},{"location":"KB/FLAVA/#flava","title":"FLAVA","text":"<ul> <li>FLAVA: a Foundational Language and Vision Alignment Model</li> <li>foundational vision and language alignment model that performs well on all three target modalities: 1) vision, 2) language, and 3) vision &amp; language</li> <li>use a single holistic universal model, as a \u201cfoundation\u201d, that targets all modalities at once</li> <li>wide range of 35 tasks spanning these target modalities</li> </ul>"},{"location":"KB/FP16%20training/","title":"FP16 training","text":""},{"location":"KB/FP16%20training/#fp16-training","title":"FP16 Training","text":"<ul> <li>@micikeviciusMixedPrecisionTraining2018</li> <li>Reduced precision has a narrower range that might make the results more out of range and worsen the training progress</li> <li>Can store all parameters and activations in FP16 and then use that for gradients.</li> <li>Also copy to FP32 for parameter updates</li> <li>Multiply scalar to loss to align range of FP16</li> </ul>"},{"location":"KB/FTSwish/","title":"FTSwish","text":""},{"location":"KB/FTSwish/#ftswish","title":"FTSwish","text":"<ul> <li>Relu + Sigmoid</li> <li> \\[\\begin{equation} FTSwish: f(x) = \\begin{cases} T, &amp; \\text{if}\\ x &lt; 0 \\\\ \\frac{x}{1 + e^{-x}} + T, &amp; \\text{otherwise} \\\\ \\end{cases} \\end{equation}\\] </li> <li>As we can see, the KB/Sparsity.md principle is still true - the neurons that produce negative values are taken out.</li> <li>What we also see is that the derivative of FTSwish is smooth, which is what made Swish theoretically better than ReLU in terms of the loss landscape</li> <li>However, what I must note is that this function does not protect us from the dying ReLU problem: the gradients for \\(x &lt; 0\\) are zero, as with ReLU.</li> </ul>"},{"location":"KB/FaceNet/","title":"FaceNet","text":""},{"location":"KB/FaceNet/#facenet","title":"FaceNet","text":"<ul> <li>FaceNet: a Unified Embedding for Face Recognition and Clustering</li> <li>mapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity</li> <li>Optimize the embedding itself</li> <li>FaceNet directly trains its output to be a compact 128-D embedding using a Triplet Loss function</li> <li>Choosing which triplets to use turns out to be very important for achieving good performance<ul> <li>inspired by curriculum learning</li> <li>online negative exemplar mining strategy which ensures consistently increasing difficulty of triplets as the network trains</li> <li>also explore hard-positive mining techniques which encourage spherical clusters for the embeddings of a single person</li> </ul> </li> <li>squared Lp Regularization L2 distance, in the embedding space directly correspond to face similarity: faces of the same person have small distances and faces of distinct people have large distances</li> <li>face verification simply involves thresholding the distance between the two embeddings; recognition becomes a KNN classification problem</li> <li>Labeled Faces in the Wild</li> <li>Zeiler Fergus</li> <li>Inception</li> <li>Harmonic Embedding</li> </ul>"},{"location":"KB/Factorized%20Embedding%20Parameters/","title":"Factorized Embedding Parameters","text":""},{"location":"KB/Factorized%20Embedding%20Parameters/#factorized-embedding-parameters","title":"Factorized Embedding Parameters","text":"<ul> <li>Factorization of these parameters is achieved by taking the matrix representing the weights of the word embeddings \\(E\\) and decomposing it into two different matrices. Instead of projecting the one-hot encoded vectors directly onto the hidden space, they are first projected on some-kind of lower-dimensional embedding space, which is then projected to the hidden space (Lan et al, 2019). Normally, this should not produce a different result, but let's wait.</li> <li>Another thing that actually ensures that this change reduces the number of parameters is that the authors suggest to reduce the size of the embedding matrix.</li> <li>In BERT, the shape of the vocabulary/embedding matrix E equals that of the matrix for the hidden state H.</li> <li>First of all, theoretically, the matrix E captures context-independent information</li> <li>whereas the hidden representation H captures context-dependent information</li> <li>ALBERT solves this issue by decomposing the embedding parameters into two smaller matrices, allowing a two-step mapping between the original word vectors and the space of the hidden state. In terms of computational cost, this no longer means \\(\\text{O(VxH)}\\) but rather \\(\\text{O(VxE + ExH)}\\), which brings a significant reduction when \\(\\text{H &gt;&gt; E}\\).</li> </ul>"},{"location":"KB/Factors%20for%20MC%20estimate/","title":"Factors for MC Estimate","text":""},{"location":"KB/Factors%20for%20MC%20estimate/#factors-for-mc-estimate","title":"Factors for MC Estimate","text":"<ul> <li>Amount of computation required to simulate transition kernel</li> <li>Time for chain to converge to equilibrium -&gt; no of states that must be discarded</li> <li>No of transitions needed to move from one state in the equilibrium to another that is independant<ul> <li>Redundant information</li> <li>First value will depend to a decreasing degree on the distance from this timestep to the previous ones. Then washout (because old ones are too far away)</li> </ul> </li> </ul>"},{"location":"KB/Fairness%20Constraint/","title":"Fairness Constraint","text":""},{"location":"KB/Fairness%20Constraint/#fairness-constraint","title":"Fairness Constraint","text":"<ul> <li>Applying a constraint to an algorithm to ensure one or more definitions of fairness are satisfied. Examples of fairness constraints include</li> </ul>"},{"location":"KB/Fairness/","title":"Fairness","text":""},{"location":"KB/Fairness/#fairness","title":"Fairness","text":"<ul> <li>explainability can be considered as the capacity to reach and guarantee fairness in ML models.</li> <li>an explainable ML model suggests a clear visualization of the relations affecting a result, allowing for a fairness or ethical analysis</li> <li>a related objective of XAI is highlighting bias in the data a model was exposed to</li> </ul>"},{"location":"KB/Faithfulness/","title":"Faithfulness","text":""},{"location":"KB/Faithfulness/#faithfulness","title":"Faithfulness","text":"<ul> <li>how well the explanation describes the underlying model. This is also known as model completeness [18]</li> </ul>"},{"location":"KB/Familar%20Object%20Grasping%20Object%20Viiew%20recog/","title":"Familar Object Grasping Object Viiew recog","text":""},{"location":"KB/Familar%20Object%20Grasping%20Object%20Viiew%20recog/#familar-object-grasping-object-viiew-recog","title":"Familar Object Grasping Object Viiew Recog","text":"<ul> <li>Shafii, Nima, S. Hamidreza Kasaei, and Lui\u0301s Seabra Lopes. \"Learning to grasp familiar objects using object view recognition and template matching.\" IROS 2016.</li> <li>Grasp template :</li> <li>Local shape feature for graspable regions<ul> <li>spin-image feature</li> </ul> </li> <li>Global feature, the radius (distance from the CoM to the selected keypoint)<ul> <li>important to represent a stable grasp</li> </ul> </li> <li>Finger configuration</li> <li>New objects that are similar to known ones (i.e. they are familiar) can be grasped in a similar way<ul> <li>As an example, if the robot knows how to grasp a pen, it may use the same grasp temple to take a marker</li> </ul> </li> </ul>"},{"location":"KB/Fashion%20MNIST/","title":"Fashion MNIST","text":""},{"location":"KB/Fashion%20MNIST/#fashion-mnist","title":"Fashion MNIST","text":"<ul> <li>https://github.com/zalandoresearch/fashion-mnist</li> <li>alternative to\u00a0MNIST<ul> <li>60'000 train images</li> <li>10'000 test images</li> <li>28x28x1 grayscale</li> <li>10 classes</li> </ul> </li> <li>bit more challenging than\u00a0MNIST</li> </ul>"},{"location":"KB/Fast%20AutoAugment/","title":"Fast AutoAugment","text":""},{"location":"KB/Fast%20AutoAugment/#fast-autoaugment","title":"Fast AutoAugment","text":"<ul> <li>that finds effective augmentation policies via a more efficient search strategy based on density matching</li> </ul>"},{"location":"KB/FastText/","title":"FastText","text":""},{"location":"KB/FastText/#fasttext","title":"FastText","text":"<pre><code>def FastTextNew(vocab_size, embedding_dim, output_dim):\n    return nn.Sequential(\n        Rearrange('t b -&gt; t b'),\n        nn.Embedding(vocab_size, embedding_dim),\n        Reduce('t b c -&gt; b c', 'mean'),\n        nn.Linear(embedding_dim, output_dim),\n        Rearrange('b c -&gt; b c'),\n    )\n</code></pre>"},{"location":"KB/Fastai%20Blocks/","title":"Fastai Blocks","text":""},{"location":"KB/Fastai%20Blocks/#fastai-blocks","title":"Fastai Blocks","text":""},{"location":"KB/Fastai%20Blocks/#building-blocks","title":"Building Blocks","text":"<pre><code># Image Classification\ndls = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=[Resize(192, method='squish')]\n).dataloaders(path)\n\ndls.show_batch(max_n=6)\nlearn = cnn_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(8)\n</code></pre> <pre><code># Label regex\npets = DataBlock(blocks = (ImageBlock, CategoryBlock),\n                 get_items=get_image_files, \n                 splitter=RandomSplitter(seed=42),\n                 get_y=using_attr(RegexLabeller(r'(.+)_\\d+.jpg$'), 'name'),\n                 item_tfms=Resize(460),\n                 batch_tfms=aug_transforms(size=224, min_scale=0.75))\ndls = pets.dataloaders(path/\"images\")\n</code></pre> <ul> <li>DataBlock is more general<ul> <li>list models</li> </ul> </li> </ul> <pre><code>timm.list_models('convnex*')\n</code></pre> <pre><code># Segmentation\npath = untar_data(URLs.CAMVID_TINY)\ndls = SegmentationDataLoaders.from_label_func(\n    path, bs=8, fnames = get_image_files(path/\"images\"),\n    label_func = lambda o: path/'labels'/f'{o.stem}_P{o.suffix}',\n    codes = np.loadtxt(path/'codes.txt', dtype=str)\n)\nlearn = unet_learner(dls, resnet34)\nlearn.fine_tune(8)\n</code></pre> <ul> <li>Segmentation Dataloaders is just another abstraction of the DataBlock for a specific case. Can use the DataBlock as well</li> </ul> <pre><code># Tabular\ndls = TabularDataLoaders.from_csv(path/'adult.csv', path=path, y_names=\"salary\",\n    cat_names = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race'],\n    cont_names = ['age', 'fnlwgt', 'education-num'],\n    procs = [Categorify, FillMissing, Normalize])\nlearn = tabular_learner(dls, metrics=accuracy)\nlearn.fit_one_cycle(2)\n</code></pre> <ul> <li>Fitting because pretrained models are not going to be there</li> </ul> <pre><code># Collaborative [Filtering](./Filtering.md)\ndls = CollabDataLoaders.from_csv(path/'ratings.csv')\nlearn = collab_learner(dls, y_range=(0.5,5.5))\nlearn.fine_tune(8)\n</code></pre> <ul> <li>Range is for the output (Since its not a binary output)</li> <li>Saving a model</li> </ul> <pre><code>learn.export('model.pkl')\n</code></pre>"},{"location":"KB/Fastai%20Deployment/","title":"Fastai Deployment","text":""},{"location":"KB/Fastai%20Deployment/#fastai-deployment","title":"Fastai Deployment","text":"<ul> <li>Gradio</li> </ul>"},{"location":"KB/Fastai%20Deployment/#save","title":"Save","text":"<pre><code>from fastai.vision.widgets import *\n</code></pre> <pre><code>path = Path()\npath.ls(file_exts='.pkl')\n\nlearn_inf = load_learner(path/'export.pkl')\nlearn_inf.predict('images/grizzly.jpg')\nlearn_inf.dls.vocab\n</code></pre>"},{"location":"KB/Fastai%20Interpretation/","title":"Fastai Interpretation","text":""},{"location":"KB/Fastai%20Interpretation/#fastai-interpretation","title":"Fastai Interpretation","text":""},{"location":"KB/Fastai%20Interpretation/#classification-interpretation","title":"Classification Interpretation","text":"<pre><code>interp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\ninterp.plot_top_losses(5, nrows=1)\n</code></pre> <ul> <li>Ordered by loss</li> <li>If predicted correctly but still shown, then low confidence</li> </ul>"},{"location":"KB/Fastai%20Interpretation/#cleaner","title":"Cleaner","text":"<pre><code>cleaner = ImageClassifierCleaner(learn)\ncleaner\n\nfor idx in cleaner.delete(): cleaner.fns[idx].unlink()\nfor idx,cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat)\n</code></pre>"},{"location":"KB/Fastai%20Interpretation/#get-all-classes-and-their-probabilities","title":"Get All Classes and Their Probabilities","text":"<pre><code>def classify_image(img):\n    pred,idx,probs = learn.predict(img)\n\nreturn dict(zip(categories, map(float,probs)))\n\nclassify_image(im)\n</code></pre>"},{"location":"KB/Fastai%20Tricks/","title":"Fasai Tricks","text":""},{"location":"KB/Fastai%20Tricks/#fasai-tricks","title":"Fasai Tricks","text":""},{"location":"KB/Fastai%20Tricks/#batched-map","title":"Batched Map","text":"<pre><code>tok_ds = ds.map(tok_func, batched=True)\n</code></pre>"},{"location":"KB/Fastai%20Tricks/#learning-rate-finder","title":"Learning Rate Finder","text":"<pre><code>learn.lr_find(suggest_funcs=(slide, valley))\n</code></pre>"},{"location":"KB/Fastai%20Tricks/#test-dataset","title":"Test Dataset","text":"<pre><code>tst_dl = learn.dls.test_dl(tst_df)\npreds,_ = learn.get_preds(dl=tst_dl)\n</code></pre>"},{"location":"KB/Fastai%20Tricks/#ensemble","title":"Ensemble","text":"<pre><code>def ensemble():\n    learn = tabular_learner(dls, metrics=accuracy, layers=[10,10])\n    with learn.no_bar(),learn.no_logging(): learn.fit(16, lr=0.03)\n    return learn.get_preds(dl=tst_dl)[0]\nlearns = [ensemble() for _ in range(5)]\n\nens_preds = torch.stack(learns).mean(0) # stack and mean\n</code></pre>"},{"location":"KB/Faster%20RCNN/","title":"Faster RCNN","text":""},{"location":"KB/Faster%20RCNN/#faster-rcnn","title":"Faster RCNN","text":"<ul> <li>Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</li> <li>SPNet</li> <li>Fast-RCNN + Region Proposal</li> <li>Attention is also used</li> <li>Vgg</li> <li>PASCAL VOC, ILSVRC, COCO</li> </ul>"},{"location":"KB/FeatMatch/","title":"FeatMatch","text":""},{"location":"KB/FeatMatch/#featmatch","title":"FeatMatch","text":"<ul> <li>novel learned feature-based refinement and augmentation method to produce a varied set of complex transformations </li> <li>utilize information from both within-class and across-class prototypical repre- sentations</li> </ul>"},{"location":"KB/Feature%20Augmentation/","title":"Feature Augmentation","text":""},{"location":"KB/Feature%20Augmentation/#feature-augmentation","title":"Feature Augmentation","text":"<ul> <li>Rather than conduct augmentation only in the input space, feature augmentation performs the transformation in a learned feature space </li> <li>[DeVries and Taylor, 2017a] </li> <li>when traversing along the manifold it is more likely to encounter realistic samples in feature space than compared to input space </li> <li>manipulating the vector representation of data within a learned feature space are investigated </li> <li>FeatMatch</li> <li>Moment Exchange</li> </ul>"},{"location":"KB/Feature%20Based%20Knowledge/","title":"Feature Based Knowledge","text":""},{"location":"KB/Feature%20Based%20Knowledge/#feature-based-knowledge","title":"Feature Based Knowledge","text":"<ul> <li>Huang and Wang (2017) using neuron selectivity trans- fer. Passalis and Tefas (2018) transferred knowledge by matching the probability distribution in feature space.</li> <li>Kim et al. (2018) introduced so called \u201cfactors\u201d as a more understandable form of intermediate repre- sentations. To reduce the performance gap between teacher and student, Jin et al. (2019) proposed route constrained hint learning, which supervises student by outputs of hint layers of teacher. Recently, Heo et al. (2019c) proposed to use the activation boundary of the hidden neurons for knowledge transfer. Interestingly, the parameter sharing of intermediate layers of the teacher model together with response-based knowledge is also used as the teacher knowledge (Zhou et al., 2018).</li> <li>To match the semantics between teacher and stu- dent, Chen et al. (2021) proposed cross-layer knowledge distillation, which adaptively assigns proper teacher layers for each student layer via attention allocation.</li> <li>Though feature-based knowledge transfer provides favorable information for the learning of the student model, how to effectively choose the hint layers from the teacher model and the guided layers from the student model remains to be further investigated (Romero et al., 2015).</li> <li>Distillation Schemes</li> <li>Teacher Student Architecture</li> <li>Distillation Algorithms</li> <li>Applications of Knowledge Distillation</li> <li>For example, on one hand, some recent works find that the student model can learn little from some teacher models due to the model capac- ity gap between the teacher model and the student model (Zhang et al., 2019b; Kang et al., 2020); On the other hand, from some early theoretical analysis on the capacity of neural networks, shallow networks are capable of learning the same representation as deep neural networks (Ba and Caruana, 2014).</li> </ul>"},{"location":"KB/Feature%20Correlationa/","title":"Feature Correlationa","text":""},{"location":"KB/Feature%20Correlationa/#feature-correlationa","title":"Feature Correlationa","text":"<ul> <li>If certain features in a dataset have a high correlation in a dataset, it becomes difficult to control specific features without changing the closely correlated ones.</li> <li>For example, let's say you have a dataset of \u00a0face images and want to add facial hair to an image of a woman, it's likely that you'll end up modifying more features as this feature is highly correlated with a male's face.</li> </ul>"},{"location":"KB/Feature%20Learning/","title":"Feature Learning","text":""},{"location":"KB/Feature%20Learning/#feature-learning","title":"Feature Learning","text":"<ul> <li>Dictionary Learning</li> <li>Methods for Feature Learning</li> <li>Contrastive Loss</li> <li>Max Margin Loss</li> <li>Triplet Loss</li> </ul>"},{"location":"KB/Feature%20Map%20Visualization/","title":"Feature Map Visualization","text":""},{"location":"KB/Feature%20Map%20Visualization/#feature-map-visualization","title":"Feature Map Visualization","text":"<ul> <li>Feature maps are visualized to show the attention of networks </li> <li>Larger activation represents the neural network pays more attention to the corresponding region in the imag </li> <li>eature maps are usually qualitatively visualized and compared with that of super- vised models [28], [36].</li> </ul>"},{"location":"KB/Feature%20Space%20Augmentation/","title":"Feature Space Augmentation","text":""},{"location":"KB/Feature%20Space%20Augmentation/#feature-space-augmentation","title":"Feature Space Augmentation","text":"<ul> <li>The sequential processing of neural networks can be manipulated such that the intermediate representations can be separated from the network as a whole. The lower-dimensional representations of image data in fully-connected layers can be extracted and isolated.</li> <li>DeVries and Taylor tested their feature space augmentation technique by extrapolating between the 3 nearest neighbors per sample to generate new data and compared their results against extrapolating in the input space and using affine transformations in the input space</li> <li>Vector representations are then found by training a CNN and then passing the training set through the truncated CNN. These vector representations can be used to train any machine learning model from Naive Bayes, Support Vector Machine, or back to a fully-connected multilayer network.</li> <li>A disadvantage of feature space augmentation is that it is very difficult to interpret the vector data.</li> </ul>"},{"location":"KB/Feature%20Spec/","title":"Feature Spec","text":""},{"location":"KB/Feature%20Spec/#feature-spec","title":"Feature Spec","text":"<ul> <li>Describes the information required to extract features data from the tf.Example protocol buffer. Because the tf.Example protocol buffer is just a container for data, you must specify the following<ul> <li>the data to extract (that is, the keys for the features)</li> <li>the data type (for example, float or int)</li> <li>The length (fixed or variable)</li> </ul> </li> </ul>"},{"location":"KB/Features/","title":"Features","text":""},{"location":"KB/Features/#features","title":"Features","text":""},{"location":"KB/Features/#dimensions","title":"Dimensions","text":""},{"location":"KB/Features/#wide","title":"Wide","text":"<ul> <li>Had to train</li> <li>More number of neurons</li> <li>Easy parallel</li> <li>Infinitely wide -&gt; Gaussian process</li> </ul>"},{"location":"KB/Features/#deep","title":"Deep","text":"<ul> <li>Easier to train</li> <li>Less data</li> <li>Linear amount</li> <li>Difficult to parallelize</li> </ul>"},{"location":"KB/Features/#why","title":"Why","text":"<ul> <li>Domain Adaptation</li> <li>Structure exploitation</li> <li>Relevant features</li> </ul>"},{"location":"KB/Features/#random-things","title":"Random Things","text":"<ul> <li>1 hidden layer Perceptron -&gt; Universal fn estimator</li> <li>Best generalization -&gt; First order optimization</li> </ul>"},{"location":"KB/Federated%20Learning/","title":"Federated Learning","text":""},{"location":"KB/Federated%20Learning/#federated-learning","title":"Federated Learning","text":"<ul> <li>Basics of Federated Learning</li> <li>Advantages of Federated Learning</li> <li>Federated Updates</li> </ul>"},{"location":"KB/Federated%20Learning/#refs","title":"Refs","text":"<ul> <li>OpenMined Blog</li> <li>Nvidia</li> <li>Unite.ai</li> <li>Google blog</li> <li>Wiki</li> <li>Digital health : Rieke, N., Hancox, J., Li, W. et al. The future of digital health with federated learning. npj Digit. Med. 3, 119 (2020). https://doi.org/10.1038/s41746-020-00323-1</li> <li>Gboard : Chen, M., Mathews, R., Ouyang, T., &amp; Beaufays, F. (2019). Federated learning of out-of-vocabulary words. arXiv preprint arXiv:1903.10635. Paper</li> <li>Chen, M., Mathews, R., Ouyang, T., &amp; Beaufays, F. (2019). Federated learning of out-of-vocabulary words. arXiv preprint arXiv:1903.10635.</li> </ul>"},{"location":"KB/Federated%20Updates/","title":"Federated Updates","text":""},{"location":"KB/Federated%20Updates/#federated-updates","title":"Federated Updates","text":"<ul> <li>Structured Update</li> <li>Sketched Update</li> </ul>"},{"location":"KB/Feedback%20Loop/","title":"Feedback Loop","text":""},{"location":"KB/Feedback%20Loop/#feedback-loop","title":"Feedback Loop","text":"<ul> <li>In machine learning, a situation in which a model's predictions influence the training data for the same model or another model. For example, a model that recommends movies will influence the movies that people see, which will then influence subsequent movie recommendation models.</li> </ul>"},{"location":"KB/Few%20Shot%20Order%20Sensitivity/","title":"Few Shot Order Sensitivity","text":""},{"location":"KB/Few%20Shot%20Order%20Sensitivity/#few-shot-order-sensitivity","title":"Few Shot Order Sensitivity","text":"<ul> <li>Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity</li> <li>When primed with only a handful of training samples, very large, pretrained language models such as GPT-3 have shown competitive results when compared to fully-supervised, fine-tuned, large, pretrained language models</li> <li>few-shot prompts suffer from order sensitivity</li> <li>for the same prompt the order in which samples are provided can make the difference between state-of-the-art and random performance \u2013 essentially some permutations are \u201cfantastic\u201d and some not</li> <li>problem is prevalent across tasks, model sizes (even for the largest current models), prompt templates, it is not related to a specific subset of samples, number of training samples, and that a given good permutation for one model is not transferable to another.</li> <li>novel probing method that exploits the generative nature of language models to construct an artificial development set</li> <li>identity performant permutations for prompts using entropy-based statistics over this set, which yields a 13% relative improvement for GPT-family models across eleven different established text classification tasks</li> </ul>"},{"location":"KB/Filter%20Bubble%20Problem/","title":"Filter Bubble Problem","text":""},{"location":"KB/Filter%20Bubble%20Problem/#filter-bubble-problem","title":"Filter Bubble Problem","text":"<ul> <li>Shows only things that you have seen before</li> </ul>"},{"location":"KB/Filter%20Wise%20Normalization/","title":"Filter Wise Normalization","text":""},{"location":"KB/Filter%20Wise%20Normalization/#filter-wise-normalization","title":"Filter Wise Normalization","text":"<ul> <li>Following up from Random Directions  $$ d_{i,j} \\leftarrow \\frac{d_{i,j}}{||d_{i,j}||}||\\theta_{i,j}|| $$<ul> <li>d is a random Gaussian Direction vector</li> <li>\\(d_{i,j}\\) is \\(j_{th}\\) filter of the \\(i_{th}\\) layer of the direction vector d</li> <li>\\(||\\cdot||\\) is the Frobenius norm</li> </ul> </li> <li></li> <li></li> </ul>"},{"location":"KB/Filtering/","title":"Filtering","text":""},{"location":"KB/Filtering/#filtering","title":"Filtering","text":"<ul> <li>Noise Suppression</li> </ul>"},{"location":"KB/Final%20Paper%20Language%20Modeling/","title":"Final Paper LM","text":""},{"location":"KB/Final%20Paper%20Language%20Modeling/#final-paper-lm","title":"Final Paper LM","text":""},{"location":"KB/Final%20Paper%20Language%20Modeling/#tips","title":"Tips","text":"<ul> <li>Identify a Research Question that links with previous research</li> <li>Identify a Method that will let you answer the research question, or at least partially</li> <li>Develop an experiment (or two) that would then test this research question</li> <li>Predict what the results will look like given current theory/theories     If your experiment tests the predictions of more than one theory then you should have one set of predictions for each theory  </li> <li>What are the consequences of certain results for our understanding of the phenomena studied?</li> <li>What should following research do given certain results?</li> <li>How will you analyze the results statistically? What methods, which tests, what does your data look like?</li> <li>So write a 2-3 page paper about an experiment that would answer an open question about quantification.</li> <li>This paper can be written as if you are proposing it (e.g. \"We would then test x children with \u2026.\" ) or you could write it as if you already did the experiment, imagining the results, e.g. \"We tested 30 Spanish speakers \u2026\".</li> <li>Give it the kind of toc: true title you would give to a paper. (Don't call it \"My Gedankenexperiment\" !;)).</li> <li>Give an introduction to the research area, summarize the results you already know, using references to relevant papers. This could be the background section. Explain what's still missing in our knowledge and how we should test it.</li> <li>In the methods section I need to know the details of the experimental design, including examples of sentences that will be tested and pictures that might be used (use clip art, or I also don't mind simple drawings. Don't worry if you are not very artistic!).</li> <li>Explain what kind of participants you will need to test, how many, and what features they need to have. Be realistic. Just because it's fantasy, you shouldn't propose testing 1000 children.</li> <li>Do you also want to do additional testing? (working memory, inhibition?) Make sure you motivate it.</li> </ul>"},{"location":"KB/Final%20Paper%20Language%20Modeling/#idea","title":"Idea.","text":"<ul> <li>Shortcuts to Quantifier Interpretation in Children and Adults But studies and comparisons were weird. Change?</li> <li>They were salty about This feature of their design provided children with unambiguous cues as to which set of entities was the focus of attention. : Crain and Thornton (1998)</li> <li>Crain et al.\u2019s (1996) claim that preschoolers have full competence with uni- versal quantifiers would seem to be undermined by the fact that even older school-age children make errors identifyi- 04:40 ng the domain of a universal quantifier.</li> <li></li> <li>Brooks and Braine (1996, Experiment 1) tested adults with actional scenes and found no errors. Their data, however, came from 10 undergraduates at a highly selective private university (Carnegie Mellon) and thus may not be representative of adults in general.</li> <li>Test same thing as Shorts. but different data</li> <li>Pictures</li> </ul>"},{"location":"KB/Final%20Paper%20Language%20Modeling/#abstract","title":"Abstract","text":"<ul> <li>Ostensive cues</li> <li>Locative bias</li> <li>Mouse tracking</li> </ul>"},{"location":"KB/Final%20Paper%20Language%20Modeling/#participants","title":"Participants","text":"<ul> <li>We recruited and tested twelve 5-year-olds (M = 5;5, range = 5;2\u20135;11), twelve 6-year-olds (M = 6;6, range = 6;2\u20136;10), twelve 7-yearolds (M = 7;6, range = 7;1\u20137;11), twelve 8-year-olds (M = 8;6, range = 8;0\u20138;11), and twelve 9-year-olds (M = 9;6, range = 9;1\u20139;11), twelve adults at private elementary schools and after-school programs in Atlanta, Georgia.</li> <li>adults from RUG, kiddos from some school</li> </ul>"},{"location":"KB/Final%20Paper%20Language%20Modeling/#procedure","title":"Procedure","text":"<ul> <li>single, 20-min session conducted in a quiet room of their school</li> <li>We showed children two pictures at a time and asked them to point to the picture that went best with a sentence read aloud</li> <li>After the child looked at both pictures, the experimenter read the corresponding sentence and asked the child to point to the picture that went best with the sentence.</li> <li>Same for adults (Contrary to this paper itself)</li> <li>the teddy thing was nice and cute too</li> </ul>"},{"location":"KB/Final%20Paper%20Language%20Modeling/#experimental-design","title":"Experimental Design","text":"<ul> <li>16 test items</li> <li>2x2 study (Picture: Collective vs. Distributive) and Sentence: (With marker (\"each\" or \"together\") or without)</li> <li>2 practice trials</li> <li>7 controls</li> <li>3 fillers</li> </ul>"},{"location":"KB/Final%20Paper%20Language%20Modeling/#what-do-we-expect","title":"What Do We Expect","text":"<ul> <li>Both children and adults make errors</li> <li>Only 9 y/o were consistent</li> <li>7 y/o : extra animals/objects vs containers</li> <li>Better performance :<ul> <li>Quantifier modifying the containers vs subject (Disprove Kang et al.)</li> <li>Children , Not Adults</li> </ul> </li> <li>Prefer locative scenes with all filled containers (Drozd et al.)<ul> <li>Children , Not Adults</li> </ul> </li> <li>The rate of rejection of the sentences as true descriptions of the visually represented situations significantly correlated with the type of the visual representation.</li> <li>The sentence\u2013 drawing pairs were rejected in 10.53% of the cases.</li> <li>n the case of the sentence\u2013photo pairs, the rate of rejection was a mere 3.51%.</li> <li>Just as in Pint\u00e9r's (2016) experiment, the rate of rejection (i.e., the rate of the exhaustive interpretation of the sentences) was slightly even higher in the adult control group: 13.33% in the case of sentence\u2013drawing pairs, and 8.88% in the case of sentence\u2013photo p</li> <li>Crucially, the photos contained many more extra agents and extra objects than the drawings, still they elicited significantly less rejections.</li> <li>What made the presence of extra objects in the drawings ostensive was the minimality of the drawings, suggesting that everything irrelevant had been eliminated from them.</li> <li>This suggests that the problem does not reside in the child's syntax, given the similarities in sentence structures used across studies, but in</li> <li>stead has to do with the difficulty of selecting the appropriate set of entities and avoiding distraction by salient objects.</li> <li>Taken together, the experiments suggest that it was the collective scenes as opposed to the use of all that improved children's performance in Experiment 1. Collective scenes were easier presumably because the group depiction aided the child in isolating one set of entities relative to the other</li> <li>We suspect that both children and adults make errors in comprehension because they engage in shallow processing that causes inaccurate mapping between syntactic and semantic representations.</li> <li>Shallow processing also provides a straightforward explanation of the errors made by adults in Experiment 3. Their high error rates suggest that adult listeners often do not tax their limited information-processing capacities by conducting exhaustive syntactic analyses of sentences but rather make use of simpler strategies in generating reasonable guesses</li> </ul>"},{"location":"KB/Final%20Paper%20Language%20Modeling/#literature","title":"Literature","text":"<ul> <li>Quantifier spreading children misled by ostensive cues</li> <li>Shortcuts to Quantifier Interpretation in Children and Adults</li> <li>A matter of ambiguity? Using eye movements to examine collective vs. distributive interpretations of plural sets</li> </ul>"},{"location":"KB/Final%20Paper%20Language%20Modeling/#experiment","title":"Experiment","text":"<ul> <li>Replace doodles with images , Use more</li> <li>Combine locative bias fix + old version of exp</li> <li>Mouse tracking</li> </ul>"},{"location":"KB/Final%20Paper%20Language%20Modeling/#sentences","title":"Sentences","text":"<ul> <li>Each of the (people) is (verb)ing an (object), for example, Each of the men is washing a bear.</li> <li>There is a (person) (verb)ing each of the (objects), for example, There is a man washing each of the bears.</li> <li>Every (person) is (verb)ing an (object), for example, Every man is washing a bear.</li> <li>There is a (person) (verb)ing every (object), for example, There is a man washing every bear.</li> <li>All of the (people) are (verb)ing an (object), for example, All of the men are washing a bear.</li> <li>There is a (person) (verb)ing all of the (objects), for example, There is a man washing all of the bears.</li> <li>All of the (objects) are in a (container), for example, All of the alligators are in a bathtub.</li> <li>All of the (containers) have an (object) in them, for example, All of the bathtubs have an alligator in them.</li> <li>There is an (object) in all of the (containers), for example, There is an alligator in all of the bathtubs.</li> <li>Each of the (objects) is in a (container), for example, Each of the alligators is in a bathtub.</li> <li>Each of the (containers) has an (object) in it, for example, Each of the bathtubs has an alligator in it.</li> <li>There is an (object) in each of the (containers), for example, There is an alligator in each of the bathtubs.</li> <li>Every (object) is in a (container), for example, Every alligator is in a bathtub.</li> <li>Every (container) has an (object) in it, for example, Every bathtub has an alligator in it.</li> <li>There is an (object) in every (container), for example, There is an alligator in every bathtub.</li> </ul>"},{"location":"KB/Final%20Paper%20User%20Models/","title":"Final Paper User Models","text":""},{"location":"KB/Final%20Paper%20User%20Models/#final-paper-user-models","title":"Final Paper User Models","text":""},{"location":"KB/Final%20Paper%20User%20Models/#literature","title":"Literature","text":"<ul> <li>van den Broek, G. S., Takashima, A., Segers, E., &amp; Verhoeven, L. (2018). Contextual richness and word learning: Context enhances comprehension but retrieval enhances retention. Language learning, 68(2), 546-585.</li> <li>Effects of Contextual Cues on Inferring and Remembering Meanings of New Word</li> <li>SlimStampen</li> <li>The Behavior of Tutoring Systems</li> <li>The Effect of Three Consecutive Context Sentences on EFL Vocabulary-Learning</li> <li>Second Language Vocabulary Learning , The role of context  versus translation</li> <li>Learning L2 German Vocabulary Through Reading</li> </ul>"},{"location":"KB/Final%20Paper%20User%20Models/#members","title":"Members","text":"<ul> <li>Hila Schwartz</li> <li>Juliette Bruin</li> <li>Isabelle Tilleman</li> <li>Subhaditya Mukherjee</li> </ul>"},{"location":"KB/Final%20Paper%20User%20Models/#concept","title":"Concept","text":"<ul> <li>Outlines the general concept of our system, meaning how we tried to adjust and improve the KB/SlimStampen.md system, and any additional improvements we made to the system.</li> </ul>"},{"location":"KB/Final%20Paper%20User%20Models/#main-idea","title":"Main Idea","text":"<ul> <li>The main idea we have for this project is to improve the SlimStampen system by presenting vocabulary in context. The way we would like to do this, is by presenting users with the word they need to learn in a sentence. As the learner gets better, the system will increase the difficulty by presenting words without context. If the words are presented out of context and the user makes a mistake, the correct answer will also be shown without context.</li> <li>An example of this would be as follows:</li> <li>Prompt: Wij kopen een huis</li> <li>Correct answer: to buy OR buy OR buying</li> <li>This idea is based on research by van den Broek et al. (2018), which showed that presenting novel words in context during the initial learning phase, and then reducing context later on improves long-term word retention. Next to that, Li (1988) found that learning words in context improves understanding of the word.</li> <li>Based on previous feedback, we have decided to have two contexts for each word. If a user gets a word wrong consistently, they will be shown a different context. This is based on the idea that they perhaps do not understand the first context they were provided with, and that perhaps the second context will provide them with the information they need to get the word correct. Unlike previously stated, this addition will not be tested in a separate condition. The only conditions we will test are no contexts at all and two contexts.</li> </ul>"},{"location":"KB/Final%20Paper%20User%20Models/#params","title":"Params","text":"<ul> <li>WORD_THRESHOLD = 0.29</li> <li>CONTEXT2_THRESHOLD = 0.35</li> <li>DEFAULT_ALPHA = 0.3</li> </ul>"},{"location":"KB/Final%20Paper%20User%20Models/#fact","title":"Fact","text":"<pre><code>Fact(fact_id = 6,\u00a0\n\u00a0question = 'gemiddeld',\u00a0\n\u00a0context_1 = \"Deze opleiding heeft 'gemiddeld' \u2026\",\u00a0\n\u00a0context_2 = \"De temperatuur is 'gemiddeld' \u2026\",\u00a0\n\u00a0answer = 'average',\u00a0\n\u00a0chosen_context = \"Deze opleiding heeft 'gemiddeld' \u2026\",\n\u00a0encounter_2 = True)\n</code></pre>"},{"location":"KB/Final%20Paper%20User%20Models/#additional-improvements","title":"Additional Improvements","text":"<ul> <li>We added a few things to the system of which we will not test the effects:</li> </ul>"},{"location":"KB/Final%20Paper%20User%20Models/#1-ui-improvements","title":"1 UI Improvements","text":"<ul> <li>The UI was updated for a cleaner look and feel, as well as to improve readability for dyslexic participants.</li> </ul>"},{"location":"KB/Final%20Paper%20User%20Models/#2-gamification","title":"2 Gamification","text":"<ul> <li>We have decided to add a few small gamification elements. One of those is that through the use of colors (green/red). Another is displaying a score which shows how many answers you got correct out of the total number of trials you have done so far.</li> </ul>"},{"location":"KB/Final%20Paper%20User%20Models/#3-multiple-translations","title":"3 Multiple Translations","text":"<ul> <li>For some of the words, we added multiple correct answers. This means that multiple different answers can be counted as correct. This is especially useful for verbs, since there are different ways to translate those which do not really change the meaning of the words. An example of this is the word vergeten, which can be translated to forget, to forget, and forgot. Out-of-context, all three of these meanings make sense. Having multiple options be marked as correct also helped with gathering context sentences, since you are not as restricted to one specific use of the word.</li> </ul>"},{"location":"KB/Final%20Paper%20User%20Models/#system","title":"System","text":"<ul> <li>Explains how the system itself works, meaning how it switches between words and context, and how it decided which word to show next.</li> </ul>"},{"location":"KB/Final%20Paper%20User%20Models/#word-order","title":"Word Order","text":"<ul> <li>Similarly to the set-up of KB/SlimStampen.md, we let the system decide which words to show and how based on rate of forgetting. What we changed is that we have different thresholds for when it shows the context, which context it shows, and when it shows just the word.</li> <li>Once it enters the second context, it will not go back to the first. This is based on the assumption that the first context did not help the participant figure out the meaning of the word. Thresholds were decided through trial-and-error, based on what felt like a natural progression of conditions.</li> <li>In the original system, the same word (and in this case also context) was shown thrice without changing at all based on input. We changed that to twice in our system.</li> </ul>"},{"location":"KB/Final%20Paper%20User%20Models/#testing","title":"Testing","text":"<ul> <li>Explains how we aim to test the system, including each of the conditions and the full experimental procedure.</li> </ul>"},{"location":"KB/Final%20Paper%20User%20Models/#stimuli-and-design","title":"Stimuli and Design","text":"<ul> <li>To test whether the improvements we made to the SlimStampen system have an effect on word retrieval, we want to do a within-participant study. For each condition, we will vary whether words are presented in a sentence, and how many sentences are available. The independent variables in this study are accuracy and response time. The dependent variable is number of contexts available.</li> </ul>"},{"location":"KB/Final%20Paper%20User%20Models/#conditions","title":"Conditions","text":"<ul> <li>We are planning to have two conditions:</li> <li>Baseline condition: In this condition, participants are only shown words without context.</li> <li>Two-context condition: In this condition, participants will be presented with words in-context as well as out of context. If participants repeatedly give the wrong answer when an item is shown in-context, they will be shown a different context.</li> </ul>"},{"location":"KB/Final%20Paper%20User%20Models/#components","title":"Components","text":"<ul> <li>The experiment consists of different components:</li> <li>Questionnaire: This will ask the participants for some basic info: age, their native language, and their previous experience with learning Dutch.</li> <li>Level Evaluation: Participants will be shown 20 Dutch words and will be asked to translate them to English. We can use this data to compare the Dutch level of each participant, in case we see some weird results. The words were chosen by taking vocabulary from different levels of Dutch (a1-b2).</li> <li>Training: Participants will try to learn new Dutch words using our adapted SlimStampen system. This is where the conditions come into play. This segment lasts either 150 trials or until they have seen all words.</li> <li>Distractor: A small dot-counting task that functions as a distractor task. Participants need to do this 10 times between training and testing. The number of blue dots that are shown is between 10 and 20.</li> <li>Test: Participants will be tested on the words learned during training using a simple translation task. In this test, they are only presented with words that they saw during training. They immediately receive feedback on how they did.</li> <li>Break: Participants will be asked to take a small break between testing and starting the training in a new condition.</li> <li>We will present each participant with the same two word/context lists, but they will be randomized over the conditions. The order of the conditions will also be randomized.</li> </ul>"},{"location":"KB/Final%20Paper%20User%20Models/#procedure","title":"Procedure","text":"<ul> <li>The experiment will be performed on our personal laptops using OpenSesame in a quiet place. Participants will be presented with a small introduction to the experiment. Then, they will answer a small questionnaire about their Dutch level. After that, they will do a small Dutch test. All participants will be asked to do the same Dutch test. The words in this test will not be used in any of the conditions. This test will provide a frame of reference for each participant's Dutch level, in case we see unexpected results. After the test, there is a small break.</li> <li>Next, participants will be asked to practice a word list in one of the two conditions. This finishes after 150 trials or after a participant has seen all of the words. Next, participants will do a small distractor test. In this distractor test, they are asked to count dots. They need to do this 10 times. Then, we will test their word retrieval with a simple single-word translation test. After this, participants are asked to take a small break. This process is repeated a total of 2 times, once for each of the conditions.</li> <li>After each participant has done each of the two conditions, we will end the experiment by thanking them for their efforts.</li> </ul>"},{"location":"KB/Final%20Paper%20User%20Models/#results","title":"Results","text":"<ul> <li>Not significant ):</li> <li>Not effective for all learners\u00a0</li> <li>Short time frame</li> <li>Effect : Using words in sentences vs Retrieval (Prince, 1996)</li> </ul>"},{"location":"KB/Final%20Paper%20User%20Models/#limitations","title":"Limitations","text":"<ul> <li>Very small participant pool</li> <li>Variation in participant backgrounds</li> <li>Ceiling effect</li> <li>No external motivation to do well</li> <li>Not enough attention to the context</li> </ul>"},{"location":"KB/Final%20Paper%20User%20Models/#future-directions","title":"Future Directions","text":"<ul> <li>Additional focus on gamification</li> <li>Longer term studies (Like SlimStampen)</li> <li>Harder words : Ceiling Effect</li> <li>More informed context in sentences (van den Broek et al., 2018)\u00a0</li> <li>Other ways of testing context</li> <li>Investigate thresholds</li> </ul>"},{"location":"KB/Final%20Paper%20User%20Models/#pictures","title":"Pictures","text":""},{"location":"KB/Fine%20Grained%20assesment/","title":"Fine Grained assesment","text":""},{"location":"KB/Fine%20Grained%20assesment/#fine-grained-assesment","title":"Fine Grained Assesment","text":""},{"location":"KB/Fine%20Grained%20assesment/#counting-learning-events","title":"Counting Learning Events","text":"<ul> <li>We also assumed that a step is the product of one or more learning events, and defined a Learning Event to be a mental event based on a Knowledge Component. Learning events and knowledge components are not directly observable, but steps are.</li> <li>Mastery really means the probability that a Knowledge Component will be applied when it should be applied. If the student's competence was frozen instead of constantly changing due to learning and forgetting, then mastery could be estimated by counting the number of times a Knowledge Component was applied and dividing by the number of times it should have been applied. Thus, we need to discuss three KB/Issues.md: (1) How to detect applications of a Knowledge Component, (2) how to detect times when a Knowledge Component should have been applied, and (3) how to adjust for instruction, learning and forgetting.</li> </ul>"},{"location":"KB/Fine%20Grained%20assesment/#counting-failures","title":"Counting Failures","text":"<ul> <li>Counting only the successful applications of a Knowledge Component is not enough; we need to know how many times the student failed to apply it as well.</li> <li>One approach is to detect failed attempts at steps. Suppose for simplicity that there is a one-to-one correspondence between a step and a Learning Event</li> <li>If the student fails to make the step, then the student must lack the knowledge that underlies the Learning Event.</li> </ul>"},{"location":"KB/Fine%20Tuning%20Based%20Pruning/","title":"Fine Tuning Based Pruning","text":""},{"location":"KB/Fine%20Tuning%20Based%20Pruning/#fine-tuning-based-pruning","title":"Fine Tuning Based Pruning","text":"<ul> <li>Some store weights before Pruning and use that to continue training.</li> <li>Others somehow try to rewind to a previous state and reinitialize the network entirely</li> </ul>"},{"location":"KB/Fine%20grained%20datasets/","title":"Fine grained datasets","text":""},{"location":"KB/Fine%20grained%20datasets/#fine-grained-datasets","title":"Fine Grained Datasets","text":"<ul> <li>CUB-200-2011</li> <li>Stanford Dogs</li> <li>FGVC Aircraft</li> <li>FGVCx</li> <li>iNaturalist</li> <li>PlantCLEF</li> </ul>"},{"location":"KB/Fine-grained%20Object%20Recognition/","title":"Fine-grained Object Recognition","text":""},{"location":"KB/Fine-grained%20Object%20Recognition/#fine-grained-object-recognition","title":"Fine-grained Object Recognition","text":"<ul> <li>An object can be represented by:</li> <li>a shared (generic) dictionary, which is used to describe the content of all categories (basic-level)</li> <li>and a set of category-specific dictionaries for highlighting the small diversity within the different categories (fine-grained )</li> <li></li> </ul>"},{"location":"KB/Finite%20Differences/","title":"Finite Differences","text":""},{"location":"KB/Finite%20Differences/#finite-differences","title":"Finite Differences","text":"<ul> <li> \\[f'(x) = \\frac{df}{dx} \\rightarrow \\frac{\\Delta f}{\\Delta x}\\] </li> <li></li> <li>Forward differences \\(\\(f'(x) = \\frac{f(x_{i+1})-f(x_{i})}{\\Delta x}\\)\\)</li> <li>Non Isotropic</li> <li>Backward differences \\(\\(f'(x) = \\frac{f(x_{i})-f(x_{i-1})}{\\Delta x}\\)\\)</li> <li>Non Isotropic</li> <li>Central differences \\(\\(f'(x) = \\frac{f(x_{i+1})-f(x_{i-1})}{2\\Delta x}\\)\\)</li> <li>High pass filter</li> <li>Non isotropic</li> </ul>"},{"location":"KB/First%20order%20generalization/","title":"First order generalization","text":""},{"location":"KB/First%20order%20generalization/#first-order-generalization","title":"First Order Generalization","text":"<ul> <li>Present model with an example, ask it to choose which of three objects most likely of the same category</li> </ul>"},{"location":"KB/First%20order%20integration/","title":"First order integration","text":""},{"location":"KB/First%20order%20integration/#first-order-integration","title":"First Order Integration","text":"<ul> <li> \\[x(t+ \\Delta t) = x(t)+ \\Delta t f(x,t)\\] </li> <li>Global error proportional to \\(\\Delta t\\)</li> <li>Not stable</li> <li></li> <li></li> </ul>"},{"location":"KB/Fisher%20Spanish-English/","title":"Fisher Spanish-English","text":""},{"location":"KB/Fisher%20Spanish-English/#fisher-spanish-english","title":"Fisher Spanish-English","text":""},{"location":"KB/Fitting/","title":"Fitting","text":""},{"location":"KB/Fitting/#fitting","title":"Fitting","text":"<ul> <li>Bayes risk<ul> <li>Minimal expected risk over set of all functions \\(\\(R_B = min_{f\\in y^X} R(f)\\)\\)</li> <li>If minimized -&gt; Best possible function</li> <li>Capacity of hypothesis space \\(\\mathcal{H}\\)</li> <li>It is essentally all possible things. In reg, all possible affine linear fns. In neural networks, all possible specific connection structure.<ul> <li>If low, \\(\\(\\mathscr{F} = R(f) - R_B\\)\\) is large : Underfitting (Huge difference between best risk and current risk)</li> <li>If high, \\(\\(\\mathscr{F} = R(f) - R_B\\)\\) is small : Overfitting (Tiny difference between best risk and current risk)</li> </ul> </li> </ul> </li> </ul>"},{"location":"KB/Fixed%20Factorization%20Attention/","title":"Fixed Factorization Attention","text":""},{"location":"KB/Fixed%20Factorization%20Attention/#fixed-factorization-attention","title":"Fixed Factorization Attention","text":"<ul> <li>paper</li> <li>Specific cells summarize previous locations and propagate to all future cells.</li> <li>Part of Sparse Transformer</li> <li>Fixed attention pattern with c = 1 limits expressivity</li> <li>many representations in the network are only used for one block whereas a small number of locations are used by all blocks.</li> <li>Choosing $c \\in 8, 16, 32</li> <li>when using multiple heads, having them attend to distinct subblocks of length \\(c\\) within the block of size \\(l\\) was preferable to having them attend to the same subblock</li> <li></li> </ul>"},{"location":"KB/Fixed%20Factors/","title":"Fixed Factors","text":""},{"location":"KB/Fixed%20Factors/#fixed-factors","title":"Fixed Factors","text":"<ul> <li>Another term: independent variables</li> <li>This is a \"Between subjects ANOVA\"</li> <li>participants go in the same directions\u2026</li> <li>BUT, because item is also a random factor, we have to check that too</li> <li>This is a \"Between items ANOVA\"</li> <li>Check that for each item, what's the difference between the conditions, and check if they go in the same direction.</li> </ul>"},{"location":"KB/Flamingo/","title":"Flamingo","text":""},{"location":"KB/Flamingo/#flamingo","title":"Flamingo","text":"<ul> <li>Flamingo: a Visual Language Model for Few-Shot Learning</li> <li>large-scale pre-training followed by task-specific fine-tuning has emerged as a standard approach, but the fine-tuning step still requires a lot of samples.</li> <li>building models that can be rapidly adapted to numerous tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research</li> <li>family of Visual Language Models (VLM) which seek to train a multi-modal model (i.e., with the ability to understand different types of input \u2013 visual, audio, text etc.) in a few-shot learning approach (which refers to the ability to learn a new task with just a few samples for training).</li> <li>bridge powerful pretrained vision-only and language-only models</li> <li>handle sequences of arbitrarily interleaved visual and textual data</li> <li>seamlessly ingest images or videos as inputs</li> <li>Interleave cross-attention layers with language-only self-attention layers (frozen).</li> <li>Perceiver-based architecture that transforms the input sequence data (videos) into a fixed number of visual token</li> <li>Large-scale (web) multi-modal data by scraping webpages which has inter-leaved text and images</li> <li>Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities</li> </ul> <p>toc: true title: Flamingo categories: ['architecture']</p>"},{"location":"KB/Flamingo/#flamingo_1","title":"Flamingo","text":"<ul> <li>A Visual Language Model created by Deepmind using few shot learning on a wide range of open-ended vision and language tasks, simply by being prompted with a few input/output examples</li> <li>the input of Flamingo contains visually conditioned autoregressive text generation models able to ingest a sequence of text tokens interleaved with images and/or videos</li> <li>and produce text as output</li> <li>A query is made to the model along with a photo or a video and the model answers with a text answer</li> <li>Flamingo models take advantage of two complementary models: a vision model that analyzes visual scenes and a large language model which performs a basic form of reasoning</li> <li>The language model is trained on a large amount of text data.</li> </ul>"},{"location":"KB/Flickr30K/","title":"Flickr30K","text":""},{"location":"KB/Flickr30K/#flickr30k","title":"Flickr30K","text":""},{"location":"KB/Flipping/","title":"Flipping","text":""},{"location":"KB/Flipping/#flipping","title":"Flipping","text":"<ul> <li>Horizontal axis flipping is much more common than flipping the vertical axis.</li> <li>On datasets involving text recognition such as MNIST or SVHN, this is not a label-preserving transformation.</li> </ul>"},{"location":"KB/FlowNet/","title":"FlowNet","text":"<ul> <li>FlowNet <ul> <li>end-to-end convolution neural network for optical flow estimation from two consecutive frames [151], [152] </li> <li>ConvNet needs to capture appearance changes of two frames </li> <li>self-supervised feature learning </li> <li>automatically generated by simulators such as game engines or by hard-code programs without human annotation.</li> </ul> </li> </ul>"},{"location":"KB/Flynn%27s%20Taxonomy/","title":"Flynn's Taxonomy","text":""},{"location":"KB/Flynn%27s%20Taxonomy/#flynns-taxonomy","title":"Flynn's Taxonomy","text":"<ul> <li>Classify multi processor architectures</li> <li>SISD</li> <li>SIMD</li> <li>MISD</li> <li>MIMD</li> </ul>"},{"location":"KB/Fmix/","title":"Fmix","text":""},{"location":"KB/Fmix/#fmix","title":"Fmix","text":"<ul> <li>random binary masks obtained by applying a threshold to low-frequency images sampled from Fourier space. Fmix can take on a wide range of shapes of random masks and can improve performance over Mixup and CutMix</li> </ul>"},{"location":"KB/Focal%20Loss/","title":"Focal Loss","text":""},{"location":"KB/Focal%20Loss/#focal-loss","title":"Focal Loss","text":"<ul> <li>two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations.</li> <li>In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far.</li> <li>Extreme foreground-background class imbalance encountered during training of dense detectors is the central cause</li> <li>modulating term to Cross Entropy in order to focus learning on hard misclassified examples</li> <li>scaling factor decays to zero as confidence in the correct class increases</li> <li>training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training</li> <li>RetinaNet</li> </ul>"},{"location":"KB/Foley/","title":"Foley","text":""},{"location":"KB/Foley/#foley","title":"Foley","text":"<ul> <li>A catheter inserted into the bladder to help with urinary drainage</li> </ul>"},{"location":"KB/Force%20Directed%20Graph%20Layout/","title":"Force Directed Graph Layout","text":""},{"location":"KB/Force%20Directed%20Graph%20Layout/#force-directed-graph-layout","title":"Force Directed Graph Layout","text":"<ul> <li>Model a graph as rings and springs</li> <li>Attractive forces between adjacent nodes</li> <li>edges are modeled as springs with uniform length</li> <li>Repulsive forces between non-adjacent nodes could be seen as springs of infinite length or repelling forces of electrically charged metal spheres</li> <li></li> </ul>"},{"location":"KB/Force/","title":"Force","text":""},{"location":"KB/Force/#force","title":"Force","text":"<ul> <li>\"The vector sum of the external forces F on an object is equal to the mass m of that object multiplied by the acceleration vector of the object.\"</li> <li> \\[\\Sigma F = ma\\] </li> <li>mass times Acceleration</li> </ul>"},{"location":"KB/Forceps/","title":"Forceps","text":""},{"location":"KB/Forceps/#forceps","title":"Forceps","text":"<ul> <li>A hinged instrument, like scissors, used to grasp and hold objects</li> </ul>"},{"location":"KB/Forgetting/","title":"Forgetting","text":""},{"location":"KB/Forgetting/#forgetting","title":"Forgetting","text":"<ul> <li>If our memories are too precise and overfitted, then we can't actually use them to make predictions about future situations</li> <li>Forgetting is an essential component of adaptive system</li> <li>Simple memories that store the gist of our experiences and avoid complicated details will be better for generalizing to future events.</li> <li>orgetting has been regarded as a passive decay over time of the information stored in the memory.</li> </ul>"},{"location":"KB/Forgetting/#passive-forgetting","title":"Passive Forgetting","text":"<ul> <li>Concepts stored in the memory can be forgotten \"passively\" based on: Decay over time (fading factor)</li> <li>Loss of context cue</li> <li>Retrieval interference</li> </ul>"},{"location":"KB/Forgetting/#active-forgetting","title":"Active Forgetting","text":"<ul> <li>may be more potent at erasing memory than the passive forgetting mechanisms Motivated forgetting</li> <li>forgetting is often more intentional</li> <li>unpleasant memories (categories) Intrinsic forgetting</li> <li>redundant data Interference-based forgetting samples that cause interference</li> </ul>"},{"location":"KB/Forward%20Backward%20Matching/","title":"Forward Backward Matching","text":""},{"location":"KB/Forward%20Backward%20Matching/#forward-backward-matching","title":"Forward Backward Matching","text":"<ul> <li>Matching proceeds from the end of the string of characters</li> <li>Results are compared</li> <li>Optimised Segmentation occurs</li> <li>Language-specific heuristics are used later</li> </ul>"},{"location":"KB/Forward%20Kinematic%20Solution/","title":"Forward Kinematic Solution","text":""},{"location":"KB/Forward%20Kinematic%20Solution/#forward-kinematic-solution","title":"Forward Kinematic Solution","text":"<ul> <li>The calculation required to find the endpoint position, given the joint positions. For most robot topologies this is easier than finding the inverse kinematic solution.</li> </ul>"},{"location":"KB/Forward%20Kinematics/","title":"Forward Kinematics","text":""},{"location":"KB/Forward%20Kinematics/#forward-kinematics","title":"Forward Kinematics","text":"<ul> <li>Computational procedures which determine where the end-effector of a robot is located in space. The procedures use mathematical algorithms along with joint sensors to determine its location.</li> <li> <ol> <li>For a robot with n joints, what is the endeffector pose (\u03be ), given the joint angles (q)</li> <li>\u03be = \u03ba(q) : q = {qi,i \u2208 [1,\u2026,n]},</li> </ol> </li> </ul>"},{"location":"KB/Fractional%20Anisotropy/","title":"Fractional Anisotropy","text":""},{"location":"KB/Fractional%20Anisotropy/#fractional-anisotropy","title":"Fractional Anisotropy","text":"<ul> <li> \\[FA = \\sqrt{\\frac{3}{2}}\\frac{\\sqrt{\\Sigma_{i=1}^{3}(\\lambda_{1}-\\mu)^{2}}}{\\Sigma_{i=1}^{3}\\lambda_{i}^{2}}\\] </li> <li></li> </ul>"},{"location":"KB/Fracture/","title":"Fracture","text":""},{"location":"KB/Fracture/#fracture","title":"Fracture","text":"<ul> <li>A cracked or broken bone</li> </ul>"},{"location":"KB/Free%20Semantic%20Label-based%20Method/","title":"Free Semantic Label-based Method","text":""},{"location":"KB/Free%20Semantic%20Label-based%20Method/#free-semantic-label-based-method","title":"Free Semantic Label-based Method","text":"<ul> <li>automatically generated semantic labels </li> <li>The labels are generated by traditional hardcode algorithms </li> <li>game engines </li> <li>moving object segmentation </li> <li>contour detection </li> <li>relative depth prediction</li> </ul>"},{"location":"KB/Free%20morpheme/","title":"Free morpheme","text":""},{"location":"KB/Free%20morpheme/#free-morpheme","title":"Free Morpheme","text":"<ul> <li>can appear as a word by itself, often combined with other morphemes too.</li> <li>e.g., house (houses) , walk (walked ) of ,or,the</li> </ul>"},{"location":"KB/Freedom/","title":"Freedom","text":""},{"location":"KB/Freedom/#freedom","title":"Freedom","text":"<ul> <li>(N, D, P) N samples, D degrees of freedom</li> <li>If N&lt;D , then ill posed</li> <li>Need N &gt;&gt; D</li> <li>If P learnable params , \\(\\(P&lt;N\\)\\) : underspecified</li> <li>If \\(\\(P &gt;&gt; N\\)\\) : overparameterized</li> <li>No of params not a good indicator of overfitting</li> <li>Solution : Regularization</li> </ul>"},{"location":"KB/Frequentist/","title":"Frequentist","text":""},{"location":"KB/Frequentist/#frequentist","title":"Frequentist","text":"<ul> <li>Measure probablity -&gt; Counting</li> <li>Repeat an experiment n times and get the estimate : \\(\\hat P\\) (estimate based on finite amount of data)</li> <li>Law of large numbers</li> <li>Random variable X which takes values in a sample space S.</li> <li>Measurement process hard to carry out in reality</li> <li>What does unbiased means? Especially because most things related to future input that we do not have yet</li> <li>Distibution of data points</li> <li>MLE</li> </ul>"},{"location":"KB/Friction/","title":"Friction","text":""},{"location":"KB/Friction/#friction","title":"Friction","text":"<ul> <li>Friction scales linearly with the normal force.</li> <li>Friction is not affected by the area of contact between surfaces.</li> <li>Stationary objects have more friction than sliding objects.</li> <li>Sliding friction is not affected by sliding velocity.</li> <li>You can look up the magnitude of friction for each pair of materials</li> <li>Static Friction</li> <li>Kinetic Friction</li> </ul>"},{"location":"KB/Frobenius%20norm/","title":"Frobenius norm","text":""},{"location":"KB/Frobenius%20norm/#frobenius-norm","title":"Frobenius Norm","text":"<ul> <li>The Frobenius norm, sometimes also called the Euclidean norm </li> <li>Lp Regularization</li> <li>When\u00a0p\u00a0=\u00a0q\u00a0= 2\u00a0for the\u00a0\\(L_{p,q}norm\\), it is called the\u00a0Frobenius norm\u00a0or the\u00a0Hilbert\u2013Schmidt norm, though the latter term is used more frequently in the context of operators on (possibly infinite-dimensional)\u00a0Hilbert space. $$ { |A|{\\text{F}}={\\sqrt {\\sum {i}^{m}\\sum {j}^{n}|a{ij}|^{2}}}={\\sqrt {\\operatorname {trace} \\left(A^{*}A\\right)}}={\\sqrt {\\sum {i=1}^{\\min{m,n}}\\sigma {i}^{2}(A)}},} $$</li> <li>where \\({ \\sigma _{i}(A)}\\) are the singular values of A</li> </ul>"},{"location":"KB/Front%20to%20Back%20Raycasting/","title":"Front to Back Raycasting","text":""},{"location":"KB/Front%20to%20Back%20Raycasting/#front-to-back-raycasting","title":"Front to Back Raycasting","text":"<ul> <li>Color Compositing</li> </ul>"},{"location":"KB/Frontal%20Operculum/","title":"Frontal Operculum","text":""},{"location":"KB/Frontal%20Operculum/#frontal-operculum","title":"Frontal Operculum","text":"<ul> <li>The part of the KB/Frontal lobe.md that sits over the insula.</li> </ul>"},{"location":"KB/Frontal%20lobe/","title":"Frontal lobe","text":""},{"location":"KB/Frontal%20lobe/#frontal-lobe","title":"Frontal Lobe","text":"<ul> <li>Personality, behavior, emotions</li> <li>Judgment, planning, problem solving</li> <li>Speech: speaking and writing (Brocas Area)</li> <li>Body movement (motor strip)</li> <li>Intelligence, concentration, self awareness</li> </ul>"},{"location":"KB/Function%20words/","title":"Function words","text":""},{"location":"KB/Function%20words/#function-words","title":"Function Words","text":"<ul> <li>Glues words and phrases together</li> <li>Determiners</li> <li>Quantifiers</li> <li>Prepositions</li> <li>Connectives</li> </ul>"},{"location":"KB/Functional%20Connectivity/","title":"Functional Connectivity","text":""},{"location":"KB/Functional%20Connectivity/#functional-connectivity","title":"Functional Connectivity","text":""},{"location":"KB/Functional%20Connectivity/#symmetric","title":"Symmetric","text":"<ul> <li>BrainWave Synchronization</li> <li>BrainWave Coherence</li> <li>BrainWave CrossFrequency Coupling</li> </ul>"},{"location":"KB/Functional%20Connectivity/#directedasymmetric","title":"Directed/Asymmetric","text":"<ul> <li>Granger Causallity</li> </ul>"},{"location":"KB/Functional%20Morpheme/","title":"Functional Morpheme","text":""},{"location":"KB/Functional%20Morpheme/#functional-morpheme","title":"Functional Morpheme","text":"<ul> <li>provides grammatical information</li> <li>e.g. s (plural ) third person singular</li> </ul>"},{"location":"KB/Functional%20correlates/","title":"Functional correlates","text":"<p>toc: true title: Functional correlates</p> <p>categories: ['temp']</p>"},{"location":"KB/Functional%20correlates/#functional-correlates","title":"Functional Correlates","text":"<ul> <li>Dimensionality Reduction technique used to quantify the Correlation and dependence between two variables when the data is functional</li> <li>Relations between the surface and phenomena that influence or are influenced by the topography.</li> </ul>"},{"location":"KB/Fundamentals/","title":"Fundamentals","text":""},{"location":"KB/Fundamentals/#fundamentals","title":"Fundamentals","text":"<ul> <li>Emperical Risk</li> <li>LinearRegression</li> <li>TemporalLearning</li> <li>Dimensionality Reduction</li> <li>Unsupervised Learning</li> <li>Semi Supervised</li> <li>Self Supervised</li> <li>Encodings</li> <li>Probability</li> <li>Universal Approximation Theorem</li> <li>Sampling</li> <li>Distributions</li> </ul>"},{"location":"KB/GAM/","title":"GAM","text":""},{"location":"KB/GAM/#gam","title":"GAM","text":"<ul> <li>Early explaining systems for ML black boxes go back to 1986 with Generalized Additive Models (GAM)</li> <li>GAMs are global statistic models that use smooth functions, which are estimated using a scatterplot smoother</li> <li>The technique is applicable to any likelihood-based regression model, provides a flexible method for identifying nonlinear covariate effects in exponential family models and other likelihood-based regression models, and has the advantage of being completely automatic</li> <li>In its most general form, the algorithm can be applied to any situation in which a criterion is optimized involving one or more smooth functions</li> </ul>"},{"location":"KB/GAN%20Z%20Space/","title":"GAN Z Space","text":""},{"location":"KB/GAN%20Z%20Space/#gan-z-space","title":"GAN Z Space","text":""},{"location":"KB/GAN%20Z%20Space/#vector-algebra-in-z-space","title":"Vector Algebra in Z-Space","text":"<ul> <li>Controllable generation is somewhat similar to interpolation.</li> <li>With interpolation, you get intermediate examples between two generated observations.</li> <li>These intermediate examples between to two targets by manipulating the inputs from Z-space, which is the same idea behind controllable generation.</li> <li>In order to get intermediate values between two images, for example, you can make an interpolation between their two input vectors v1 and v2 in the Z-space.</li> <li>Controllable generation also uses changes in Z-space and makes use of how adjustments to the noise vector are reflected in the output from the generator.</li> <li>Differences in the features generated, for example different hair colors, occur due to changes in the direction that you have to move in Z-space to modify the features of the image.</li> <li>If image output of \\(g(v_{1})\\) , new controlled output with \\(g(v_{1}+d)\\)</li> </ul>"},{"location":"KB/GAN%E2%80%90based%20Data%20Augmentation/","title":"GAN\u2010based Data Augmentation","text":""},{"location":"KB/GAN%E2%80%90based%20Data%20Augmentation/#ganbased-data-augmentation","title":"GAN\u2010based Data Augmentation","text":"<ul> <li>Bowles et al. describe GANs as a way to 'unlock' additional information from a dataset</li> <li>Another useful strategy for generative modeling worth mentioning is variational auto-encoders. The GAN framework can be extended to improve the quality of samples produced with variational auto-encoders</li> <li>Using CycleGANs to translate images from the other 7 classes into the minority classes was very effective in improving the performance of the CNN model on emotion recognition.</li> <li>As exciting as the potential of GANs is, it is very difficult to get high-resolution outputs from the current cutting-edge architectures. Increasing the output size of the images produced by the generator will likely cause training instability and non-convergence</li> </ul>"},{"location":"KB/GAU/","title":"GAU","text":""},{"location":"KB/GAU/#gau","title":"GAU","text":"<ul> <li>gated attention unit; a generalization of GLU - gated linear unit</li> <li>allows for better and more efficient approximation of multi-head attention than many other efficient attention methods by using a weaker single-head attention with minimal quality loss</li> </ul>"},{"location":"KB/GE2E/","title":"GE2E","text":""},{"location":"KB/GE2E/#ge2e","title":"GE2E","text":"<ul> <li>Generalized End-to-end Loss for Speaker Verification</li> <li>new loss function</li> <li>training of speaker verification models more efficient</li> <li>Unlike TE2E, the GE2E loss function updates the network in a way that emphasizes examples that are difficult to verify at each step of the training process</li> <li>pushes the embedding towards the [centroid] of the true speaker, and away from the centroid of the most similar different speaker</li> <li>does not require an initial stage of example selection</li> <li>MultiReader technique</li> </ul>"},{"location":"KB/GELU/","title":"GELU","text":""},{"location":"KB/GELU/#gelu","title":"GELU","text":"<ul> <li>Paper</li> <li>Smoother Relu</li> <li>\\(\\(x\\Phi(x)\\)\\) where \\(\\Phi(x)\\) is the Normal Distribution CDF</li> <li>Weights inputs by percentile, rather than by sign like ReLU</li> <li> \\[GELU(x) = xP(X \\leq x) = x\\Phi(x) = x. \\frac{1}{2}\\left[ 1+erf\\left( \\frac{x}{\\sqrt{ 2 }} \\right) \\right]\\] </li> <li>If \\(X \\sim \\mathscr{N}(0,1)\\)</li> <li>Used in GPT3, Transformer, Vision Transformer, BERT</li> <li></li> </ul>"},{"location":"KB/GGCNN/","title":"GGCNN","text":""},{"location":"KB/GGCNN/#ggcnn","title":"GGCNN","text":"<ul> <li>Learning an object agnostic function to grasp objects</li> <li>Grasping using uni-modal data (depth image).  </li> <li>Generate pixel-wise grasp configuration for the given input.  </li> <li>The gripper approaches the target object in top-down manner. Uses a shallow network, and an eye-in-hand camera configuration.</li> <li></li> <li>Morrison, Douglas, Peter Corke, and Ju\u0308rgen Leitner. \"Closing the loop for robotic grasping: A real-time, generative grasp synthesis approach.\" RSS (2018).</li> </ul>"},{"location":"KB/GLOW/","title":"GLOW","text":""},{"location":"KB/GLOW/#glow","title":"GLOW","text":"<pre><code>def unsqueeze2d_new(input, factor=2):\n    return rearrange(input, 'b (c h2 w2) h w -&gt; b c (h h2) (w w2)', h2=factor, w2=factor)\n\ndef squeeze2d_new(input, factor=2):\n    return rearrange(input, 'b c (h h2) (w w2) -&gt; b (c h2 w2) h w', h2=factor, w2=factor)\n</code></pre>"},{"location":"KB/GLUE/","title":"GLUE","text":""},{"location":"KB/GLUE/#glue","title":"GLUE","text":""},{"location":"KB/GOMS/","title":"GOMS","text":""},{"location":"KB/GOMS/#goms","title":"GOMS","text":""},{"location":"KB/GOMS/#parts","title":"Parts","text":"<ul> <li>Goals</li> <li>Operations</li> <li>Methods</li> <li>Selection Rules</li> </ul>"},{"location":"KB/GOMS/#rest","title":"Rest","text":"<ul> <li>Human Computer Interaction</li> <li>Atomic</li> <li>Reactive vs proactive</li> <li>Performance vs control</li> </ul>"},{"location":"KB/GPT/","title":"GPT","text":""},{"location":"KB/GPT/#gpt","title":"GPT","text":"<ul> <li>Pretrained using Unsupervised Learning and finetuned</li> <li>Log Likelihood Loss</li> <li></li> </ul>"},{"location":"KB/GPT3/","title":"GPT3","text":""},{"location":"KB/GPT3/#gpt3","title":"GPT3","text":"<ul> <li>Language Models are Few-Shot Learners</li> <li>shows that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches</li> <li>KB/Autoregressive.md language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting</li> <li>without any gradient updates or fine-tuning</li> <li>on-the-fly reasoning or domain adaptation</li> <li>methodological issues related to training on large web corpora</li> <li>can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans</li> </ul>"},{"location":"KB/GRConvNet/","title":"GRConvNet","text":""},{"location":"KB/GRConvNet/#grconvnet","title":"GRConvNet","text":"<ul> <li>Generative Residual Convolutional Neural Network</li> <li>Learning an object agnostic function to grasp objects</li> <li>Uses multi modal input data (RGB + depth images).</li> <li>Generates pixel-wise antipodal grasp configuration.</li> <li>State-of-the-art performance (97% on Cornell dataset).</li> <li>Use eye-to-hand camera configuration.</li> <li>Sulabh Kumra, et al. \"Antipodal robotic grasping using generative residual convolutional neural network.\" IROS 2020.</li> <li></li> </ul>"},{"location":"KB/GRU/","title":"Gated Recurrent Unit (GRU)","text":""},{"location":"KB/GRU/#gated-recurrent-unit-gru","title":"Gated Recurrent Unit (GRU)","text":"<ul> <li>Simplified [LSTM)](Long Short Term Memory (LSTM|Long Short Term Memory (LSTM|LSTM)](LSTM)](Long Short Term Memory (LSTM|Long Short Term Memory (LSTM|LSTM).md).md)</li> <li>It has an input and forget gate, no output gate</li> <li>Faster than LSTM in training, but does not perform well in many tasks</li> <li>Tries to forget what is not important</li> </ul>"},{"location":"KB/GRU/#the-math","title":"The Math","text":"<ul> <li>Two gates, Sigmoid<ul> <li>Reset : \\(\\(g_r = \\sigma(W_{hr}h_{t-1} + W_{xr}x_t + b_r)\\)\\)</li> <li>Update : \\(\\(g_u = \\sigma(W_{hu}h_{t-1} + W_{xu}x_t + b_u)\\)\\)</li> </ul> </li> <li>Hidden state proposal<ul> <li> \\[\\hat h_t = tanh(W_{xh}x_t + W_{hh}g_r\\cdot h_{t-1} + b_h)\\] </li> </ul> </li> <li>Final hidden state<ul> <li>Linear Interpolation between last hidden state and proposal</li> <li> \\[h_t = (1-g_u)\\cdot h_{t-1} + g_u \\cdot \\hat h_t\\] </li> </ul> </li> </ul>"},{"location":"KB/GTA5/","title":"GTA5","text":""},{"location":"KB/GTA5/#gta5","title":"GTA5","text":""},{"location":"KB/Galactica/","title":"Galactica","text":""},{"location":"KB/Galactica/#galactica","title":"Galactica","text":"<ul> <li>new large model for automatically organizing science developed by Meta AI and Papers with Code</li> <li>ability to train on it for multiple epochs without overfitting, where upstream and downstream performance improves with use of repeated token</li> <li>The dataset design is critical to the approach as all of it is processed in a common markdown format to blend knowledge between sources.</li> <li>Citations are processed via a certain token that allows researchers to predict a citation given any input context</li> <li>The capability of the model of predicting citations improves with scale and the model becomes better at the distribution of citations</li> <li>the model can perform multi-modal tasks involving SMILES chemical formulas and protein sequences</li> <li>transformer architecture in a decoder-only setup with GeLU activation for all model sizes.</li> </ul>"},{"location":"KB/Game%20Based%20Learning/","title":"Game Based Learning","text":""},{"location":"KB/Game%20Based%20Learning/#game-based-learning","title":"Game Based Learning","text":"<ul> <li>gamified learning tasks</li> <li>improve engagement</li> <li>Learning progress</li> <li>Interactivity<ul> <li>Seductive details<ul> <li>interesting, but irrelevant</li> </ul> </li> <li>Should support a specific function</li> </ul> </li> <li>Does it work?<ul> <li>Mixed</li> <li>Studies use many game elements in a study - makes it hard to understand if any work</li> <li>Not a lot of directed, guided</li> <li>Makes it fun though</li> </ul> </li> </ul>"},{"location":"KB/Gamification/","title":"Gamification","text":""},{"location":"KB/Gamification/#gamification","title":"Gamification","text":"<ul> <li>Use of game-like design elements</li> <li>Non game concepts</li> <li>Interface</li> <li>Mechanics</li> <li>Design</li> <li>Serious Games</li> <li>Game Based Learning</li> </ul>"},{"location":"KB/Gaming%20addiction/","title":"Gaming addiction","text":"<p>toc: true title: Gaming addiction</p> <p>categories: ['temp']</p>"},{"location":"KB/Gaming%20addiction/#gaming-addiction","title":"Gaming Addiction","text":"<ul> <li>fMRI was performed while showing game images to online game addicts.</li> <li>According to Brain Areas control group, right orbitofrontal cortex, right nucleus accumbens, bilateral anterior cingulate and medial frontal cortex, right dorsolateral prefrontal cortex and right caudate nucleus activation were observed.</li> <li>These areas are the rewarding areas</li> <li>The results show that the same addiction to substance can share the same neuro-biological mechanisms with the extreme gaming demands of online gaming addiction.</li> <li>Paper</li> </ul>"},{"location":"KB/Gamma%20Waves/","title":"Gamma Waves","text":""},{"location":"KB/Gamma%20Waves/#gamma-waves","title":"Gamma Waves","text":"<ul> <li>28-90 Hz</li> <li>Attention/Consciousness</li> <li></li> </ul>"},{"location":"KB/Gamma-aminobutyric%20Acid%20%28GABA%29/","title":"Gamma aminobutyric Acid (GABA)","text":""},{"location":"KB/Gamma-aminobutyric%20Acid%20%28GABA%29/#gamma-aminobutyric-acid-gaba","title":"Gamma-aminobutyric Acid (GABA)","text":"<ul> <li>A neurotransmitter implicated in brain development, muscle control, and reduced stress response</li> </ul>"},{"location":"KB/Gantry%20Robot/","title":"Gantry Robot","text":""},{"location":"KB/Gantry%20Robot/#gantry-robot","title":"Gantry Robot","text":"<ul> <li>A robot which has three degrees of freedom along the X, Y and Z coordinate system. Usually consists of a spooling system (used as a crane), which when reeled or unreeled provides the up and down motion along the Z axis. The spool can slide from left to right along a shaft which provides movement along the Z axis. The spool and shaft can move forward and back along tracks which provide movement along the Y axis. Usually used to position its end effector over a desired object and pick it up.</li> </ul>"},{"location":"KB/Gas%20Law/","title":"Gas Law","text":""},{"location":"KB/Gas%20Law/#gas-law","title":"Gas Law","text":"<ul> <li>combination of Boyle's Law and Charles' Law</li> <li> \\[\\frac{P_{1}V_{1}}{T_{1}}= \\frac{P_{2}V_{2}}{T_{2}}\\] </li> <li>Temp must be in Kelvin</li> <li>pressure x volume of a gas = number of moles x molar gas constant x absolute temperature</li> <li> \\[pV = nRT\\] </li> </ul>"},{"location":"KB/Gato/","title":"Gato","text":""},{"location":"KB/Gato/#gato","title":"Gato","text":"<ul> <li>A Generalist Agent</li> <li>Gato</li> <li>single generalist agent beyond the realm of text outputs, inspired by progress in large-scale language modeling</li> <li>multi-modal, multi-task, multi-embodiment generalist policy</li> <li>same network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens</li> <li>To enable processing this multi-modal data from different tasks and modalities, it is serialized into a flat sequence of tokens</li> <li>In this representation, Gato can be trained and sampled from akin to a standard large-scale language model</li> <li>Masking is used such that the loss function is applied only to target outputs, i.e text and various actions</li> <li>During deployment, sampled tokens are assembled into dialogue responses, captions, button presses, or other actions based on the context</li> <li>Transformer sequence models are effective as multi-task multi-embodiment policies, including for real-world text, vision and robotics tasks</li> </ul>"},{"location":"KB/Gaussian%20Baseline/","title":"Gaussian Baseline","text":""},{"location":"KB/Gaussian%20Baseline/#gaussian-baseline","title":"Gaussian Baseline","text":"<ul> <li>One of the first propositions was to add Gaussian noise to the original image</li> <li>Gaussian baseline was introduced by Smilkov et al. @smilkovSmoothGradRemovingNoise2017</li> <li>used a Gaussian distribution centered on the current image with a variance \u03c3\\sigma\u03c3.</li> <li>This variance is the only parameter when tuning the method</li> <li>This is in Smooth-Grad </li> </ul>"},{"location":"KB/Gaussian%20Distortion/","title":"Gaussian Distortion","text":""},{"location":"KB/Gaussian%20Distortion/#gaussian-distortion","title":"Gaussian Distortion","text":"<ul> <li>Grid width and height, and magnitude are kept the same as the random distortion values of 6, 6, and 5, respectively</li> <li>The Gaussian distortion has the added parameters of applying the distortion based on the 2D normal distribution</li> <li>normal distortion is applied to each grid point on a circular surface (corner=\"bell\") and with default values for the mean and standard deviation (\\(\\mu_{x}\\) = \\(\\mu_{y}\\) = 0.5,\\(\\sigma_{x}\\) = \\(\\sigma_{y}\\) = 0.05)</li> <li> \\[p(x,y) = exp\\{-(\\frac{(x-\\mu_{x})^{2}}{\\sigma_{x}} + \\frac{(x-\\mu_{y})^{2}}{\\sigma_{y}}))\\}\\] </li> </ul>"},{"location":"KB/Gaussian%20Filter/","title":"Gaussian Filter","text":""},{"location":"KB/Gaussian%20Filter/#gaussian-filter","title":"Gaussian Filter","text":"<ul> <li>Filtering with a discretized Gaussian function</li> <li>Weights follow \\(\\(G(x) = e^{-ax^{2}}\\)\\)</li> </ul>"},{"location":"KB/Gaze%20position/","title":"Gaze position","text":""},{"location":"KB/Gaze%20position/#gaze-position","title":"Gaze Position","text":"<ul> <li>Where the subject is looking</li> <li>Process how that moves with new stimuli</li> <li>Pupil size</li> <li>Gaze direction is a good metric of Attention</li> <li>Pupil Dilation</li> </ul>"},{"location":"KB/GenEth/","title":"GenEth","text":""},{"location":"KB/GenEth/#geneth","title":"GenEth","text":"<ul> <li>ethical dilemma analyzer</li> <li>ethical issues related to intelligent systems are likely to exceed the grasp of the original system designers, and designed GenEth to include ethicists into the discussion process in order to codify ethical principles in given application domains.</li> <li>Features: denoting the presence or absence of factors (e.g.,harm,benefit) with integer values;</li> <li>Duties: denoting the responsibility of an agent to minimize/maximize a given feature;</li> <li>Actions: denoting whether an action satisfies or violates certain duties as an integer tuple;</li> <li>Cases: used to compare pairs of actions on their collective ethical impact</li> <li>Principles: denoting the ethical preference among different actions as a tuple of integer tuples.</li> </ul>"},{"location":"KB/Gene%20Expression/","title":"Gene Expression","text":""},{"location":"KB/Gene%20Expression/#gene-expression","title":"Gene Expression","text":"<ul> <li>The process by which a gene</li> <li>\u2019s nucleotide sequence is transcribed into the form of RNA</li> <li>\u2014often as a prelude to being translated into a protein.</li> </ul>"},{"location":"KB/Generalization%20Curve/","title":"Generalization Curve","text":""},{"location":"KB/Generalization%20Curve/#generalization-curve","title":"Generalization Curve","text":"<ul> <li>A loss curve showing both the training set and the validation set. A generalization curve can help you detect possible overfitting. For example, the following generalization curve suggests overfitting because loss for the validation set ultimately becomes significantly higher than for the training set.</li> </ul>"},{"location":"KB/Generalizing%20Adversarial%20Explanations%20with%20Grad-CAM/","title":"Generalizing Adversarial Explanations with Grad-CAM","text":""},{"location":"KB/Generalizing%20Adversarial%20Explanations%20with%20Grad-CAM/#generalizing-adversarial-explanations-with-grad-cam","title":"Generalizing Adversarial Explanations with Grad-CAM","text":"<ul> <li>@Generalizing Adversarial Explanations with Grad-CAM</li> <li>Chakraborty, Tanmay, Utkarsh Trehan, Khawla Mallat, and Jean-Luc Dugelay. \u201cGeneralizing Adversarial Explanations with Grad-CAM.\u201d In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 186\u201392. New Orleans, LA, USA: IEEE, 2022. https://doi.org/10.1109/CVPRW56347.2022.00031.</li> <li>Adversarial Learning</li> </ul>"},{"location":"KB/Generalizing%20Adversarial%20Explanations%20with%20Grad-CAM/#intro","title":"Intro","text":"<ul> <li>The drawback of Grad-CAM is that it cannot be used to generalize CNN behaviour.</li> <li>extends Grad-CAM from example-based explanations to a method for explaining global model behaviour</li> <li>These metrics are computed by comparing a Normalized Inverted Structural Similarity Index (NISSIM) metric of the Grad-CAM generated heatmap for samples from the original test set and samples from the adversarial test set.</li> <li>We observe a consistent shift in the region highlighted in the Grad-CAM heatmap, reflecting its participation to the decision making, across all models under adversarial attacks.</li> <li>These adversarial attacks display specific properties, i) They are not perceptible to the human eye, ii) They are controllable, and iii) Transferability, i.e., an attack designed for one model is capable of attacking multiple models</li> <li>There are mainly two kinds of attacks: targeted and non-targeted attacks. Targeted attack makes a model predict a certain label for the adversarial example, while for non-targeted attacks the labels for adversarial examples are not important, as long as the model is wrong</li> <li>These attacks can also be subdivided into black-box attacks and white-box attacks. Black-box attacks have no information about the target model, training procedure, architecture, whereas white-box attacks know the target model, training procedure, architecture, parameters.</li> <li>The CKA-similarity algorithm was used to compare the hidden representations of broad and deep models . They found that when the model capacity is large compared to the training set, a block structure emerges, which shows that the models propagate the main component of their hidden representation.</li> <li>More recent methods leverage explainability of machine learning and use SHAP based signatures to detect adversarial attacks</li> <li>As a result, we observed a global pattern displayed by all models. The shifting in the region of participation can be defined as when a model sees adversarial examples. Some parts of the input image no longer participate in the decision-making, while new parts do participate.</li> <li>These changes are not deterministic, and given an adversarial example, there is no way to tell how it will affect the shift</li> <li>FGSM</li> </ul>"},{"location":"KB/Generalizing%20Adversarial%20Explanations%20with%20Grad-CAM/#new-metrics","title":"New Metrics","text":"<ul> <li>Normalized Inverted Structural Similarity Index</li> <li>Mean Observed Dissimilarity</li> <li>Variation in Dissimilarity Variation in Dissimilarity</li> </ul>"},{"location":"KB/Generalizing%20Adversarial%20Explanations%20with%20Grad-CAM/#face-dataset-case-study","title":"Face Dataset Case Study","text":"<ul> <li>VGGFace2</li> <li>First, we preprocess the dataset to align and crop the faces. Then, the dataset is split into 80% training, 10% testing, and 10% validation sets.</li> <li>Once the training step is completed, the stored models are loaded and used to generate perturbations from the test set using FGSM.</li> <li>Then the test set is attacked with different values of \\(\\epsilon\\) from the stored perturbations and these counterexamples are stored as perturbed test sets</li> <li>Finally, Grad- CAM was used to generate heatmaps for every layer in each model and each \u03f5 in the perturbed test set.</li> <li>VGG 16: we can observe clearly that all the attacks were successful and illustrates a clear shift of participating regions as the \u03f5 increases.</li> <li>ResNet50 : the number of layers are too many to pin point out some example, yet if observed very carefully the hidden layers as the \u03f5 increases, we can find a shifting in the region of participation.</li> <li>In ResNet101: it seems more resilient there are some observable region shifts, but overall much less.</li> <li>InceptionNet v3 : seems to have learnt something different, the focus was more on forehead than face, but the overall shifting is much higher for this model, we even see focus regions getting inverted as the \u03f5 increases.</li> <li>For XceptionNet : the phenomenon is more clear, some regions get expanded, and background areas are being highlighted.</li> <li>We use the heatmap obtained from the original image (without adversarial perturbation) as our ground truth heatmap, i.e., what the model expects to see in order to make a decision, then the second heatmap is generated from the adversarially attacked image, and we create a dataframe with all the NISSIM values for the entire test set comparing with the adversarial test set for all values of \u03f5.</li> <li>We can also use this metric to explain the performance of VGG16. Since the shift was smaller, the model was less likely to fail.</li> <li>We also find that deep networks perform better than wide networks for similar shifts</li> <li>The main idea, that is examined here, is that the lower the shift in distribution, more the model is robust to adversarial attacks</li> <li>This indicates that VGG16 is a stable model for this task, over the other models.</li> </ul>"},{"location":"KB/Generalizing%20Adversarial%20Explanations%20with%20Grad-CAM/#observations","title":"Observations","text":"<ul> <li>We see a shift in the focus of the model in different directions, sometimes backgrounds get highlighted, other times, participation region expands or shrinks.</li> <li>Deeper models are much robust to this changes, for similar amount of shift, deeper models provide better performance than wider models.</li> <li>neural networks fail because of a shifting behaviour in the region of participation to the decision-making, when the model sees adversarial examples, its focus changes and it now sees a different hidden representation</li> <li>The main observation to keep in mind is, as \u03f5 increases, the dissimilarity increases, indicating that the focus of the model is diverted when it is presented an adversarial example, this value indicates that the more the examples differ, the more likely the model will fail.</li> <li>We can observe a pattern that wide models fail more than deep models as the \u03f5 increases</li> </ul>"},{"location":"KB/Generalizing%20Adversarial%20Explanations%20with%20Grad-CAM/#images","title":"Images","text":""},{"location":"KB/Generative%20Models/","title":"Generative_Models","text":""},{"location":"KB/Generative%20Models/#generative_models","title":"Generative_Models","text":"<ul> <li>Basic GAN</li> <li>GAN</li> </ul>"},{"location":"KB/Generative%20RNN/","title":"Generative RNN","text":""},{"location":"KB/Generative%20RNN/#generative-rnn","title":"Generative RNN","text":"<ul> <li>initial sequence is used as seed and output is sampled\u00a0<ul> <li>random or argmax to sample</li> <li>normally not taking argmax but sample with respective\u00a0Softmax probabilities -&gt; allows to generate something different than input</li> </ul> </li> <li>new output is used as seed to generate next\u00a0</li> <li>repeat until termination criterion</li> </ul>"},{"location":"KB/Generative%20Spoken%20Language%20Modeling/","title":"Generative Spoken Language Modeling","text":""},{"location":"KB/Generative%20Spoken%20Language%20Modeling/#generative-spoken-language-modeling","title":"Generative Spoken Language Modeling","text":"<ul> <li>Generative Spoken Language Modeling from Raw Audio</li> <li>learns speech representations from CPC, Wav2Vec2.0, and HuBERT for synthesizing speech</li> <li>task of learning the acoustic and linguistic characteristics of a language from raw audio</li> <li>et of metrics to automatically evaluate the learned representations at acoustic and linguistic levels for both encoding and generation</li> <li>set up baseline systems consisting of a discrete speech encoder (returning pseudo-text units)</li> <li>generative language model (trained on pseudo-text)</li> <li>speech decoder (generating a waveform from pseudo-text)</li> <li>trained without supervision</li> <li>number of discrete units (50, 100, or 200) matters in a task-dependent and encoder-dependent way, and that some combinations approach text-based systems</li> </ul>"},{"location":"KB/Generative%20vs%20Discriminative%20Models/","title":"Generative vs Discriminative Models","text":""},{"location":"KB/Generative%20vs%20Discriminative%20Models/#generative-vs-discriminative-models","title":"Generative vs Discriminative Models","text":""},{"location":"KB/Generative%20vs%20Discriminative%20Models/#generative","title":"Generative","text":"<ul> <li>LDA</li> <li>Bayesian Model Estimation</li> <li>HMM</li> <li>Autoregressive</li> <li>Basic GAN</li> </ul>"},{"location":"KB/Generative%20vs%20Discriminative%20Models/#discriminative","title":"Discriminative","text":"<ul> <li>Logistic Regression</li> <li>SVM</li> <li>Decision Trees</li> <li>Random Forest</li> </ul>"},{"location":"KB/Geometric%20Transformations/","title":"Geometric Transformations","text":""},{"location":"KB/Geometric%20Transformations/#geometric-transformations","title":"Geometric Transformations","text":"<ul> <li>The safety of a Data Augmentation method refers to its likelihood of preserving the label post-transformation.</li> <li>A non-label preserving transformation could potentially strengthen the model\u2019s ability to output a response indicating that it is not confident about its prediction. However, achieving this would require refined labels post-augmentation.</li> <li>Due to the challenge of constructing refined labels for post-augmented data, it is important to consider the \u2018safety\u2019 of an augmentation. This is somewhat domain dependent</li> </ul>"},{"location":"KB/Gestalt%20Laws/","title":"Gestalt Laws","text":""},{"location":"KB/Gestalt%20Laws/#gestalt-laws","title":"Gestalt Laws","text":"<ul> <li>Good form can dominate other laws</li> <li>crossing swarms in our visual field are perceived as different swarms</li> </ul>"},{"location":"KB/Git%20Commands/","title":"Git Commands","text":"","tags":["mlops"]},{"location":"KB/Git%20Commands/#git-commands","title":"Git Commands","text":"<ul> <li>Sources<ul> <li>The Essential GitHub CLI Commands</li> </ul> </li> </ul>","tags":["mlops"]},{"location":"KB/Git%20Commands/#managing-gists","title":"Managing Gists","text":"<pre><code>gh gist create my\\_mergify\\_gist.py\n</code></pre> <pre><code>gh gist create --public my\\_mergify\\_gist.py\n</code></pre>","tags":["mlops"]},{"location":"KB/Git%20Commands/#list-all-your-gists","title":"List All Your Gists","text":"<pre><code>gh gist list\n</code></pre> <ul> <li>You can also apply filters on this list using the <code>--limit int</code> argument (default to 10) along with the <code>--public</code> and <code>--secret</code> flags.</li> </ul>","tags":["mlops"]},{"location":"KB/Git%20Commands/#view","title":"View","text":"<pre><code>gh gist view 4b5ba0b5daabf386ee01bc37ab667e58\n</code></pre>","tags":["mlops"]},{"location":"KB/Git%20Commands/#delete","title":"Delete","text":"<pre><code>gh gist delete 4b5ba0b5daabf386ee01bc37ab667e58\n</code></pre>","tags":["mlops"]},{"location":"KB/Git%20Commands/#managing-issues","title":"Managing Issues","text":"","tags":["mlops"]},{"location":"KB/Git%20Commands/#creating-an-issue","title":"Creating an Issue","text":"<pre><code>gh issue create --toc: true\ntitle \"Is it a bug?\" --body \"the behavior\u2019s description\"\n</code></pre>","tags":["mlops"]},{"location":"KB/Git%20Commands/#listing-all-the-repositorys-issues","title":"Listing All the repository\u2019s Issues","text":"<pre><code>gh issue list\n</code></pre> <ul> <li>You can even open your browser with <code>--web</code></li> </ul>","tags":["mlops"]},{"location":"KB/Git%20Commands/#status","title":"Status","text":"<pre><code>gh issue status\n</code></pre>","tags":["mlops"]},{"location":"KB/Git%20Commands/#closing-an-issue","title":"Closing an Issue","text":"<pre><code>gh issue close &lt;num&gt;\n</code></pre>","tags":["mlops"]},{"location":"KB/Git%20Commands/#reopening-an-issue","title":"Reopening an Issue","text":"<pre><code>gh issue reopen &lt;num&gt;\n</code></pre>","tags":["mlops"]},{"location":"KB/Git%20Commands/#managing-repositories","title":"Managing Repositories","text":"","tags":["mlops"]},{"location":"KB/Git%20Commands/#create-a-public-repository","title":"Create a Public Repository","text":"<pre><code>gh repo create\n</code></pre>","tags":["mlops"]},{"location":"KB/Git%20Commands/#forking-a-repository","title":"Forking a Repository","text":"<pre><code>gh repo fork Mergifyio/react-crisp\n</code></pre>","tags":["mlops"]},{"location":"KB/Git%20Commands/#listing-the-repository-of-an-account","title":"Listing the Repository of an account","text":"<pre><code>gh repo list CamClrt\n</code></pre> <ul> <li>You can filter this list down using the <code>--archived</code>, <code>--no-archived</code>, or <code>--source</code> flags.</li> </ul>","tags":["mlops"]},{"location":"KB/Git%20Commands/#managing-prs","title":"Managing PRs","text":"","tags":["mlops"]},{"location":"KB/Git%20Commands/#creating-a-pull-request-with-a-specific-title-and-body","title":"Creating a Pull Request with a Specific Title and Body","text":"<pre><code>gh pr create --toc: true\ntitle \"feat: my\\_super\\_feature\" --body \"all the details\" \n</code></pre>","tags":["mlops"]},{"location":"KB/Git%20Commands/#listing-all-the-pull-requests-in-the-repository","title":"Listing All the Pull Requests in the Repository","text":"<pre><code>gh pr list\n</code></pre> <ul> <li>this command allows you to apply a large number of filters like <code>--assignee</code>, <code>--base</code>, <code>--label</code>, and more</li> </ul>","tags":["mlops"]},{"location":"KB/Git%20Commands/#status-of-your-pull-requests","title":"Status of Your Pull Requests","text":"<pre><code>gh pr status\n</code></pre>","tags":["mlops"]},{"location":"KB/Git%20Commands/#getting-a-pull-request-to-inspect-it","title":"Getting a Pull Request to Inspect it","text":"<pre><code>gh pr checkout 2530\n</code></pre>","tags":["mlops"]},{"location":"KB/Git%20Commands/#displaying-continuous-integration-ci-status-for-a-specific-pull-request","title":"Displaying Continuous Integration (CI) Status for a Specific Pull Request","text":"<pre><code>gh pr checks 1234\n</code></pre>","tags":["mlops"]},{"location":"KB/Git%20Commands/#diff","title":"Diff","text":"<pre><code>gh pr checkout &lt;num&gt;\ngh pr diff\n</code></pre>","tags":["mlops"]},{"location":"KB/Git%20Commands/#merge","title":"Merge","text":"<pre><code>gh pr merge &lt;num&gt;\n</code></pre> <pre><code>gh pr merge -m -d &amp;lt;number&amp;gt; &amp;amp;&amp;amp; git pull\n</code></pre>","tags":["mlops"]},{"location":"KB/Git%20Commands/#display-the-title-body-and-other-information-about-a-pull-request","title":"Display the Title, Body, and other Information about a Pull Request.","text":"<pre><code>gh pr view\n</code></pre>","tags":["mlops"]},{"location":"KB/Git%20Commands/#make-a-pull-request-as-ready-for-review","title":"Make a Pull Request as Ready for Review","text":"<pre><code>gh pr ready\n</code></pre>","tags":["mlops"]},{"location":"KB/Git%20Commands/#add-a-review-to-a-pull-request","title":"Add a Review to a Pull Request","text":"<pre><code>gh pr review\n</code></pre>","tags":["mlops"]},{"location":"KB/Git%20Commands/#closereopen","title":"Close/reopen","text":"<pre><code>gh pr &lt;close, reopen&gt;\n</code></pre>","tags":["mlops"]},{"location":"KB/Glia/","title":"Glia","text":""},{"location":"KB/Glia/#glia","title":"Glia","text":"<ul> <li>The supporting cells of the central nervous system. They may contribute to the transmission of nerve impulses and play a critical role in protecting and nourishing neurons.</li> <li>Previously thought of as protective covering</li> <li>Central Nervous System<ul> <li>Astrocyte</li> <li>Microglia</li> <li>Ependymal Cell</li> <li>Ogliodendrocytes</li> </ul> </li> <li>Peripheral Nervous System<ul> <li>Satellite Cell</li> <li>Schwann Cell</li> </ul> </li> </ul>"},{"location":"KB/Glioblastoma/","title":"Glioblastoma","text":""},{"location":"KB/Glioblastoma/#glioblastoma","title":"Glioblastoma","text":"<ul> <li>An invasive brain tumor made up of glial tissue, blood vessels, and dead neurons.</li> </ul>"},{"location":"KB/Glioma/","title":"Glioma","text":""},{"location":"KB/Glioma/#glioma","title":"Glioma","text":"<ul> <li>A tumor that arises from the brain\u2019s glial tissue.</li> </ul>"},{"location":"KB/GloVE/","title":"GloVE","text":""},{"location":"KB/GloVE/#glove","title":"GloVE","text":""},{"location":"KB/GloVE/#explanation","title":"Explanation","text":"<ul> <li>GloVe: Global Vectors for Word Representation</li> <li>Word2Vec relies only on local information of language. That is, the semantics learnt for a given word, is only affected by the surrounding words.</li> <li>Unsupervised Learning algorithm which captures both global statistics and local statistics of a corpus</li> <li>aggregated global word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space</li> <li>whether distributional word representations are best learned from count-based methods or from prediction-based methods</li> <li>probe the underlying co-occurrence statistics of the corpus</li> <li>reformulated word2vec optimizations as a special kind of factorization for word co-occurence matrices</li> <li>Note that GloVe does not use neural networks</li> <li>utilizes this main benefit of count data while simultaneously capturing the meaningful linear substructures prevalent in recent log-bilinear prediction-based methods like word2vec</li> <li>global log-bilinear LinearRegression model for the unsupervised learning of word representations</li> <li></li> <li>There\u2019s a straight red column through all of these different words. They\u2019re similar along that dimension (and we don\u2019t know what each dimensions codes for)</li> <li>There are clear places where \u201cking\u201d and \u201cqueen\u201d are similar to each other and distinct from all the others. Could these be coding for a vague concept of royalty?</li> </ul>"},{"location":"KB/GloVE/#analogies","title":"Analogies","text":""},{"location":"KB/Global%20Average%20Pooling/","title":"Global Average Pooling","text":""},{"location":"KB/Global%20Average%20Pooling/#global-average-pooling","title":"Global Average Pooling","text":"<p>```mermaid graph TD;</p> <p>E1[Averages activations of each feature map] --&gt; E2[concatenates them] --&gt; E3[outputs as a vector] ```</p>"},{"location":"KB/Global%20Classification%20Accuracy/","title":"Global Classification Accuracy","text":""},{"location":"KB/Global%20Classification%20Accuracy/#global-classification-accuracy","title":"Global Classification Accuracy","text":"<ul> <li>an accuracy computed using all predictions in a complete experiment</li> </ul>"},{"location":"KB/Global%20Gradient%20Magnitude%20Based%20Pruning/","title":"Global Gradient Magnitude Based Pruning","text":""},{"location":"KB/Global%20Gradient%20Magnitude%20Based%20Pruning/#global-gradient-magnitude-based-pruning","title":"Global Gradient Magnitude Based Pruning","text":"<ul> <li>Identifies lowest absolute value \\((weight*gradient)\\) in the whole network and removes them</li> </ul>"},{"location":"KB/Global%20Magnitude%20Based%20Pruning/","title":"Global Magnitude Based Pruning","text":""},{"location":"KB/Global%20Magnitude%20Based%20Pruning/#global-magnitude-based-pruning","title":"Global Magnitude Based Pruning","text":"<ul> <li>Takes the lowest values in the entire network. Drops them.</li> </ul>"},{"location":"KB/Global%20and%20Sliding%20Window%20Attention/","title":"Global and Sliding Window Attention","text":""},{"location":"KB/Global%20and%20Sliding%20Window%20Attention/#global-and-sliding-window-attention","title":"Global and Sliding Window Attention","text":"<ul> <li>Sliding Window Attention and Dilated Sliding Window Attention are not always enough</li> <li>global attention\u201d on few pre-selected input locations.</li> <li>This attention is operation symmetric: that is, a token with a global attention attends to all tokens across the sequence, and all tokens in the sequence attend to it</li> <li></li> </ul>"},{"location":"KB/Glucose/","title":"Glucose","text":""},{"location":"KB/Glucose/#glucose","title":"Glucose","text":"<ul> <li>A natural sugar that is carried in the blood and is the principal source of energy for the cells of the brain and body.</li> </ul>"},{"location":"KB/Glymphatic%20System/","title":"Glymphatic System","text":""},{"location":"KB/Glymphatic%20System/#glymphatic-system","title":"Glymphatic System","text":"<ul> <li>The system that helps clear debris from the brain. During sleep, special glial cells called astrocytes form a network of conduits that allow cerebrospinal fluid to flush unwanted and unnecessary proteins out of the brain.</li> </ul>"},{"location":"KB/Glyphs/","title":"Glyphs","text":""},{"location":"KB/Glyphs/#glyphs","title":"Glyphs","text":"<ul> <li>Alpha Blending</li> </ul>"},{"location":"KB/Goodhart%27s%20Law/","title":"Goodhart's Law","text":""},{"location":"KB/Goodhart%27s%20Law/#goodharts-law","title":"Goodhart's Law","text":"<ul> <li>\u201cWhen a measure becomes a target, it ceases to be a good measure.\u201d</li> <li>Proxy Objective</li> <li>Rejection Sampling</li> </ul>"},{"location":"KB/Goodhart%27s%20Law/#refs","title":"Refs","text":"<ul> <li>openai</li> </ul>"},{"location":"KB/Google%20Conceptual%20Captions/","title":"Google Conceptual Captions","text":""},{"location":"KB/Google%20Conceptual%20Captions/#google-conceptual-captions","title":"Google Conceptual Captions","text":""},{"location":"KB/Google%20NMT/","title":"Google NMT","text":""},{"location":"KB/Google%20NMT/#google-nmt","title":"Google NMT","text":"<ul> <li>Google\u2019s Neural Machine Translation System: Bridging the Gap Between Human and Machine Translation<ul> <li>deep [LSTM)](Long Short Term Memory (LSTM|Long Short Term Memory (LSTM|LSTM)](LSTM)](Long Short Term Memory (LSTM|Long Short Term Memory (LSTM|LSTM).md).md) network with 8 encoder and 8 decoder layers using attention and residual connections</li> <li>improve parallelism and therefore decrease training time, their attention mechanism connects the bottom layer of the decoder to the top layer of the encoder</li> <li>low-precision arithmetic during inference computations (FP16 training ???)</li> <li>improve handling of rare words, we divide words into a limited set of common sub-word units</li> <li>good balance between the flexibility of \u201ccharacter\u201d-delimited models and the efficiency of \u201cword\u201d-delimited models</li> <li>Beam search technique employs a length-normalization procedure and uses a coverage penalty</li> </ul> </li> </ul>"},{"location":"KB/Google%20voice%20search%20task/","title":"Google voice search task","text":""},{"location":"KB/Google%20voice%20search%20task/#google-voice-search-task","title":"Google Voice Search Task","text":""},{"location":"KB/Grad-CAM/","title":"GradCAM","text":""},{"location":"KB/Grad-CAM/#gradcam","title":"GradCAM","text":"<ul> <li>@selvarajuGradCAMVisualExplanations</li> <li>Modified CAM</li> <li>Importance of feature map k for target class c<ul> <li>A is input</li> <li>\\(Y_{c}=\\text{score of class c}\\) : value of output before softmax</li> <li>grad of \\(Y_{c}\\) wrt A and take avg</li> <li>$$</li> </ul> </li> </ul> <p>\\alpha_{k}^{c}= average(\\partial \\frac{Y_{c}}{\\partial A^{k}{ij}}) $$     - If avg is high : important     - 0 : not     - neg : background/ others - Weighted combination -&gt; relu     - $$ L^{c}{GRADCAM}=Resize(ReLU(\\Sigma_{k}(\\alpha^{c}_{k}A^{k}))) $$     - This is a coarse heatmap because the image is resized     - ReLU used because we only care about positive values (actualy image pixel) - To identify Counterfactual Images, flip the signs     -  $$</p> <p>\\alpha_{k}^{c}=average(- \\partial \\frac{Y_{c}}{\\partial A^{k}_{ij}})</p> <p>$$     -  - Followed by Guided GradCAM</p>"},{"location":"KB/Grad-CAM/#other-stuff","title":"Other Stuff","text":"<ul> <li>producing \u2018visual explanations\u2019 for decisions from a large class of CNN-based models, making them more transparent and explainable</li> <li>Gradient-weighted Class Activation Mapping</li> <li>uses the gradients of any target concept</li> <li>flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept</li> <li>lend insights into failure modes of these models (showing that seemingly unreasonable predictions have reasonable explanations)</li> <li>are robust to adversarial perturbations</li> <li>are more faithful to the underlying model</li> <li>help achieve model generalization by identifying dataset bias</li> <li>identify important neurons through GradCAM and combine it with neuron names to provide textual explanations for model decisions</li> </ul>"},{"location":"KB/Grad-CAM/#gradcam-vs-cam","title":"GradCAM Vs CAM","text":"<ul> <li>Gradient-weighted Class Activation Mapping (Grad-CAM) is an improvement over Class Activation Mapping ([CAM]) that provides a more detailed and accurate visualization that provides a more detailed and accurate [visualization.md) of the regions of an image that are important for a given classification.</li> <li>CAM generates heatmaps by using global average pooling (GAP) in the final convolutional layer to generate a feature map, followed by a linear combination of the feature map and the class weight vector to generate a single class activation map. However, this approach does not take into account the gradients of the class scores with respect to the feature maps, which can provide additional information about the contribution of different regions of the image to the final classification decision.</li> <li>Grad-CAM, on the other hand, uses the gradients of the class scores with respect to the feature maps in order to generate heatmaps. Specifically, it uses the gradients of the class scores with respect to the final feature maps of the network, which are then upsampled to the same size as the input image. The resulting heatmap highlights the regions of the input image that are most important for the given classification.</li> <li>In summary, Grad-CAM is an improvement over CAM because it provides a more detailed and accurate visualization of the regions of an image that are important for a given classification by using gradients of the class scores with respect to the feature maps, providing additional information about the contribution of different regions of the image to the final classification decision.</li> </ul>"},{"location":"KB/GradCAM%2B%2B/","title":"GradCAM++","text":""},{"location":"KB/GradCAM%2B%2B/#gradcam","title":"GradCAM++","text":"<ul> <li>@chattopadhayGradCAMGeneralizedGradientBased2018</li> </ul>"},{"location":"KB/Gradient%20Accumulation/","title":"Gradient Accumulation","text":""},{"location":"KB/Gradient%20Accumulation/#gradient-accumulation","title":"Gradient Accumulation","text":"<ul> <li>Pytorch</li> <li>helps when the model is not able to be trained with a big enough batch size</li> <li>often caused by memory limitations of the GPU</li> <li>Accumulate the gradients (for each trainable model value) of several forward passes and after some steps use the accumulated gradients to update the weights</li> <li>Is then equal to using a large batch size</li> <li>example with \\(\\(SGD: \\theta_{i}=\\theta_{i}\u22121\u2212 \\alpha\\ast(\\Sigma_{i=0}^{N}grad_{\\theta_{i}})\\)\\)</li> </ul>"},{"location":"KB/Gradient%20Ascent/","title":"Gradient Ascent","text":""},{"location":"KB/Gradient%20Ascent/#gradient-ascent","title":"Gradient Ascent","text":"<ul> <li>To maximize loss function unlike Gradient Descent gradients</li> <li>Proportional to positive of gradient</li> <li> \\[\\theta_{t+1} = \\theta{t} + \\eta_t \\Sigma_{n=1}^N(\\nabla l_n(\\theta_t))^T\\] </li> </ul>"},{"location":"KB/Gradient%20Boosting/","title":"Gradient Boosting","text":""},{"location":"KB/Gradient%20Boosting/#gradient-boosting","title":"Gradient Boosting","text":"<ul> <li>A training algorithm where weak models are trained to iteratively improve the quality (reduce the loss) of a strong model. For example, a weak model could be a linear or small decision tree model. The strong model becomes the sum of all the previously trained weak models.</li> <li>In the simplest form of gradient boosting, at each iteration, a weak model is trained to predict the loss gradient of the strong model.</li> </ul>"},{"location":"KB/Gradient%20Checkpointing/","title":"Gradient Checkpointing","text":""},{"location":"KB/Gradient%20Checkpointing/#gradient-checkpointing","title":"Gradient Checkpointing","text":"<ul> <li>https://spell.ml/blog/gradient-checkpointing-pytorch-YGypLBAAACEAefHs</li> </ul>"},{"location":"KB/Gradient%20Clipping/","title":"Gradient Clipping","text":""},{"location":"KB/Gradient%20Clipping/#gradient-clipping","title":"Gradient Clipping","text":"<ul> <li>Limit the value or the norm of a gradient to a fixed Hyperparameter \u03bb.</li> <li>mitigate the Vanishing &amp; Exploding Gradients, exploding ones</li> <li>idea is to clip the gradients during Backpropagation to a certain threshold (limit the value)</li> <li>most often used in RNN or GAN, where Batch Normalisation is tricky to use</li> <li>methods<ul> <li>clip by norm<ul> <li>clip the whole gradient if its L2 norm is greater than the threshold</li> <li>remains the orientation</li> </ul> </li> <li>clip by value<ul> <li>clip the gradient by a fixed value</li> <li>problem: orientation of the gradient may change due to clipping<ul> <li>example: [0.9,100.0]\u2192[0.9,1.0]</li> <li>however, this works well in practice</li> </ul> </li> </ul> </li> </ul> </li> <li>pros:<ul> <li>larger batch sizes</li> </ul> </li> <li>cons:<ul> <li>sensible to tuning Hyperparameter \u03bb</li> </ul> </li> <li>Adaptive Gradient Clipping</li> </ul>"},{"location":"KB/Gradient%20Descent%20gradients/","title":"Gradient Descent","text":""},{"location":"KB/Gradient%20Descent%20gradients/#gradient-descent","title":"Gradient Descent","text":"<ul> <li>Backprop</li> <li> <p>Gradient Direction</p> </li> <li> <p>Gradient Magnitude</p> </li> <li>Edge Strength $\\(||\\triangledown f|| = \\sqrt{(\\frac{\\partial f}{\\partial x})^{2} + (\\frac{\\partial f}{\\partial y})^{2}}\\)</li> <li>Params \\(\\(\\theta\\)\\)</li> <li>Minimize loss function \\(\\(\\mathscr{L}(\\theta) = \\Sigma^N_{n=1}l_n(\\theta)\\)\\)</li> <li>Simple Gradient Descent</li> <li>SGD</li> <li>Mini Batch GD</li> <li>SGD Momentum</li> <li>Adagrad</li> <li>Nesterov Momentum</li> <li>AdaDelta</li> <li>Rmsprop</li> <li>Adam</li> </ul>"},{"location":"KB/Gradient%20Descent%20gradients/#_1","title":"\u2026","text":""},{"location":"KB/Gradient%20Direction/","title":"Gradient Direction","text":""},{"location":"KB/Gradient%20Direction/#gradient-direction","title":"Gradient Direction","text":"<ul> <li>Direction of Steepest Descent \\(\\(\\theta = tan^{-1}(\\frac{\\frac{\\partial f}{\\partial y}}{\\frac{\\partial f}{\\partial x}})\\)\\)</li> </ul>"},{"location":"KB/Gradient%20Sensitivity/","title":"Gradient Sensitivity","text":""},{"location":"KB/Gradient%20Sensitivity/#gradient-sensitivity","title":"Gradient Sensitivity","text":"<ul> <li>if for every input and baseline that differ in one feature but have different predictions, then the differing feature should be given a non-zero attribution</li> <li>If the function implemented by the deep network does not depend (mathematically) on some variable, then the attribution to that variable is always zero.</li> <li>The sensitivity axiom introduces the baseline</li> <li>A baseline is defined as an absence of a feature in an input</li> <li>This definition is confusing, especially when dealing with complex models, but the baseline could be interpreted as \u201cinput from the input space that produces a neutral prediction\u201d.</li> <li>A baseline can be treated as an input to produce a counterfactual explanation by checking how the model behaves when moving from baseline to the original image.</li> <li>The authors give the example of the baseline for an object recognition network, which is a black image.</li> <li>Authors argue that gradient-based methods are violating Sensitivity</li> <li>As an example, we are presented with the case of simple function, \\(\\(f(x)=1-ReLU(1-x)\\)\\) </li> <li>and the baseline being \\(x=0\\)</li> <li>When trying to generate attribution for \\(x=2\\), the functions\u2019 output changes from 0 to 1 but after \\(x=1\\), it becomes flat and causes the gradient to equal zero.</li> <li>Obviously, x attributes to the result, but because the function is flat at the input we are testing results in invalid attribution and breaks the Sensitivity</li> <li>Sundararajan et al. think that breaking Sensitivity causes gradients to focus on irrelevant features.</li> </ul>"},{"location":"KB/Gradio/","title":"Gradio","text":""},{"location":"KB/Gradio/#gradio","title":"Gradio","text":"<pre><code>from fastai.vision.all import *\npath = untar_data(URLs.PETS)\ndls = ImageDataLoaders.from_name_re(path, get_image_files(path/'images'), pat='(.+)_\\d+.jpg', item_tfms=Resize(460), batch_tfms=aug_transforms(size=224, min_scale=0.75))\nlearn = vision_learner(dls, models.resnet50, metrics=accuracy)\nlearn.fine_tune(1)\nlearn.path = Path('.')\nlearn.export()\n\nlearn = load_learner('export.pkl')\n\nlabels = learn.dls.vocab\ndef predict(img):\n    img = PILImage.create(img)\n    pred,pred_idx,probs = learn.predict(img)\n    return {labels[i]: float(probs[i]) for i in range(len(labels))}\n\ntoc: true\ntitle = \"Pet Breed Classifier\"\ndescription = \"A pet breed classifier trained on the Oxford Pets dataset with [fastai](./fastai.md). Created as a demo for Gradio and HuggingFace Spaces.\"\narticle=\"&lt;p style='text-align: center'&gt;&lt;a href='https://tmabraham.github.io/blog/gradio_hf_spaces_tutorial' target='_blank'&gt;Blog post&lt;/a&gt;&lt;/p&gt;\"\nexamples = ['siamese.jpg']\ninterpretation='default'\nenable_queue=True\n\ngr.Interface(fn=predict,inputs=gr.inputs.Image(shape=(512, 512)),outputs=gr.outputs.Label(num_top_classes=3),toc: true\ntitle=toc: true\ntitle,description=description,article=article,examples=examples,interpretation=interpretation,enable_queue=enable_queue).launch()\n</code></pre>"},{"location":"KB/Gram%20matrix/","title":"Gram matrix","text":""},{"location":"KB/Gram%20matrix/#gram-matrix","title":"Gram Matrix","text":"<pre><code>def gram_matrix_new(y):\n    b, ch, h, w = y.shape\n    return torch.einsum('bchw,bdhw-&gt;bcd', [y, y]) / (h * w)\n</code></pre>"},{"location":"KB/Granger%20Causallity/","title":"Granger Causallity","text":""},{"location":"KB/Granger%20Causallity/#granger-causallity","title":"Granger Causallity","text":"<ul> <li>Autoregressive</li> <li>If significant then electrode Granger-causes another</li> <li>Theres some causality but not sure if physical or causal</li> <li></li> <li>Partial Directed Coherence</li> <li>Directed Transfer Function</li> <li><ul> <li>Magnitude vs freq</li> <li>Undirected</li> <li>From O1 to PZ is different from PZ to O1</li> <li>How well can activity in one channel predict one in another</li> </ul> </li> </ul>"},{"location":"KB/Graph%20Based%20Distillation/","title":"Graph Based Distillation","text":""},{"location":"KB/Graph%20Based%20Distillation/#graph-based-distillation","title":"Graph Based Distillation","text":"<ul> <li>Lee and Song (2019) analysed intra-data rela- tions using a multi-head graph, in which the vertices are the features from different layers in CNNs. Park et al. (2019) directly transferred the mutual relations of data samples, i.e., to match edges between a teacher graph and a student graph. Tung and Mori (2019) used the similarity matrix to represent the mutual relations of the activations of the input pairs in teacher and student models. The similarity matrix of student matches that of teacher.</li> <li>Peng et al. (2019a) not only matched the response-based and feature-based knowl- edge, but also used the graph-based knowledge. In (Liu et al., 2019g), the instance features and instance relationships are modeled as vertexes and edges of the graph, respectively.</li> <li>Specifically, Luo et al. (2018) considered the modal- ity discrepancy to incorporate privileged information from the source domain. A directed graph, referred to as a distillation graph is introduced to explore the relationship between different modalities. Each vertex represent a [modality] and the edges indicate the connection strength between one modality and another.</li> <li>Minami et al. (2019) proposed a bidirectional graph-based diverse collaborative learning to explore diverse knowledge transfer patterns. Yao et al. (2020) introduced GNNs to deal with the knowledge trans- fer for graph-based knowledge.</li> <li>Besides, using knowl- edge distillation, the topological semantics of a graph convolutional teacher network as the topology-aware knowledge are transferred into the graph convolutional student network (Yang et al., 2020b)</li> </ul>"},{"location":"KB/Graph-based%20visual%20saliency/","title":"Graph-based visual saliency","text":"<ul> <li>TODO</li> <li>exploits channel-wise feature maps computed by linear filtering followed by a nonlinear transformation.</li> <li>To estimate saliency maps, the feature maps are transformed into activation maps and normalised by using the fully-connected directed graph of the feature maps.</li> </ul>"},{"location":"KB/Graph-based%20visual%20saliency/#graph-based-visual-saliency","title":"Graph-based Visual Saliency","text":""},{"location":"KB/Graphs/","title":"Graphs","text":""},{"location":"KB/Graphs/#graphs","title":"Graphs","text":"<ul> <li>Graph \\(G= (V,E)\\) where</li> <li>edges \\(E \\subseteq V \\times V\\)</li> <li>vertices \\(V\\)</li> <li>Small World graphs</li> </ul>"},{"location":"KB/Grasp%20Point%20Detection/","title":"Grasp Point Detection","text":""},{"location":"KB/Grasp%20Point%20Detection/#grasp-point-detection","title":"Grasp Point Detection","text":"<ul> <li>Choose a grasp point either from above or from side by considering:<ul> <li>Size of object\u2019s bounding box</li> <li>Principle axes</li> <li>Projections/Views</li> </ul> </li> <li>GGCNN, GRConvNet, MVGrasp, Unet Grasping, Learning to Detect Grasp Affordance, Volumetric Grasping Network , Affordance Detection Task Specific</li> <li>Kinesthetic Teaching</li> </ul>"},{"location":"KB/Gravity%20Loading/","title":"Gravity Loading","text":""},{"location":"KB/Gravity%20Loading/#gravity-loading","title":"Gravity Loading","text":"<ul> <li>The force exerted downward, due to the weight of the robot arm and/or the load at the end of the arm. The force creates an error with respect to position accuracy of the end effector. A compensating force can be computed and applied bringing the arm back to the desired position.</li> </ul>"},{"location":"KB/Gravity/","title":"Gravity","text":""},{"location":"KB/Gravity/#gravity","title":"Gravity","text":"<ul> <li>Mass\u00a0is a measure of an object's inertia.\u00a0Mass\u00a0also determines the strength of gravity. Because of gravity all objects are attracted to each other, but we mostly notice the attraction towards the Earth because it is so large and so close.</li> <li> \\[F_{g}= mg\\] </li> <li>\\(g = 9.8 m/s^2\\)</li> </ul>"},{"location":"KB/Greedy%20Policy/","title":"Greedy Policy","text":""},{"location":"KB/Greedy%20Policy/#greedy-policy","title":"Greedy Policy","text":"<ul> <li>In reinforcement learning, a policy that always chooses the action with the highest expected return.</li> </ul>"},{"location":"KB/Grey%20sheep%20problem/","title":"Grey sheep problem","text":""},{"location":"KB/Grey%20sheep%20problem/#grey-sheep-problem","title":"Grey Sheep Problem","text":"<ul> <li>Fall outside the bounds in something like KB/Clustering.md</li> <li>gets assigned to something close by but not really related</li> </ul>"},{"location":"KB/GridMask/","title":"GridMask","text":""},{"location":"KB/GridMask/#gridmask","title":"GridMask","text":"<ul> <li>@chenGridMaskDataAugmentation2020</li> <li>The algorithm tries to overcome drawbacks of [Cutout], [Random Erasing], and Hide and Seek that are prone to deleting important information entirely or leaving it untouched without making it harder for the algorithm to learn.</li> <li>To handle this, GridMask creates multiple blacked-out regions in evenly spaced grids to maintain a good balance between deletion and retention of critical information</li> <li>The number of masking grids and their sizes are tuneable</li> </ul>"},{"location":"KB/Grids/","title":"Grids","text":""},{"location":"KB/Grids/#grids","title":"Grids","text":""},{"location":"KB/Gripper/","title":"Gripper","text":""},{"location":"KB/Gripper/#gripper","title":"Gripper","text":"<ul> <li>An end effector that is designed for seizing and holding (ISO 8373) and \"grips\" or grabs an object. It is attached to the last link of the arm. It may hold an object using several different methods, such as: applying pressure between its \"fingers\", or may use magnetization or vacuum to hold the object, etc</li> <li></li> </ul>"},{"location":"KB/Group%20Modeling%20Approach/","title":"Group Modeling Approach","text":""},{"location":"KB/Group%20Modeling%20Approach/#group-modeling-approach","title":"Group Modeling Approach","text":"<ul> <li>Take all users -&gt; Split them into groups</li> <li>Not personalized</li> <li>Easy to classify a user</li> <li>Grey sheep problem</li> </ul>"},{"location":"KB/Group%20fairness/","title":"Group fairness","text":""},{"location":"KB/Group%20fairness/#group-fairness","title":"Group fairness","text":"<ul> <li>fairness is analyzed by modeling the differences between each subject and the rest of the population.</li> </ul>"},{"location":"KB/Guided%20BackProp/","title":"Guided BackProp","text":""},{"location":"KB/Guided%20BackProp/#guided-backprop","title":"Guided BackProp","text":"<ul> <li>@springenbergStrivingSimplicityAll2015</li> <li>Striving for simplicity, the All conv net</li> </ul>"},{"location":"KB/Guided%20BackProp/#summary","title":"Summary","text":"<ul> <li>Combination of [DeconvNet] and  Deep Inside Convolutional Networks </li> <li>DeconvNet has an issue with flow of negative gradients which decrease accuracy of higher layers</li> <li>Their idea is to combine two approaches and add a \u201cguide\u201d to the Saliency with the help of deconvolution.</li> <li>focus on the ReLU activation function</li> <li>When computing values at the Rectification component of the deconvnet, we are masking all non-positive values with the ReLU</li> <li>In that layer, the computed values are calculated only base on the top signal (reconstruction from the upper layer), and the input is ignored</li> <li>On the other hand, in the Saliency method, we are focusing on the gradient values computed base on the input image</li> <li>If we take deconvnet masking of the Rectification layer and apply it on the gradient values of the Saliency method, we could remove noise caused by the negative gradient values.</li> <li>Deconvolution guides backpropagation values of the Saliency method to produce sharper images</li> <li>The idea of GBP is often misunderstood and interpreted as \u201capplying deconvolution results on the saliency results\u201d.</li> <li>This is not true because ReLU masking extracted from the deconvnet is applied on every level and therefore affects the gradient values all the way down to the input of the CNN, not only at the first level of the CNN</li> </ul>"},{"location":"KB/Guided%20BackProp/#images","title":"Images","text":""},{"location":"KB/Guided%20GradCAM/","title":"Guided GradCAM","text":""},{"location":"KB/Guided%20GradCAM/#guided-grad-cam","title":"Guided Grad-CAM","text":"<ul> <li>@selvarajuGradCAMWhyDid2017</li> <li>Pointwise multiply betwen [Grad-CAM] and Guided BackProp</li> <li>Class Discriminative</li> <li>High resolution</li> <li>Similar to Occlusion Map but faster</li> <li>Guided [Grad-CAM] is a variation of Grad-CAM that combines the gradients of the class scores with respect to the feature maps with the gradients of a guided backpropagation algorithm. Guided backpropagation is a method for visualizing the internal representations of a neural network by backpropagating the output of the network to the input image, while only propagating the positive gradients.</li> <li>The main difference between [Grad-CAM] and Guided [Grad-CAM] is that while [Grad-CAM] focuses on finding the regions of an image that are most important for a given classification, Guided [Grad-CAM] also takes into account the positive gradients of the guided backpropagation algorithm, in order to provide a more fine-grained [visualization] of the internal representations of the network. This can make Guided Grad-CAM more effective for understanding how the model is making its decisions, and for identifying the specific features of an image that the model is using for a given classification.</li> <li></li> </ul>"},{"location":"KB/Gyrus/","title":"Gyrus","text":""},{"location":"KB/Gyrus/#gyrus","title":"Gyrus","text":"<ul> <li>The folding of the cortex increases the brain\u2019s surface area allowing more neurons to fit inside the skull and enabling higher functions</li> <li>Each groove between folds is called a sulcus.</li> <li>There are names for the folds and grooves that help define specific brain regions.</li> </ul>"},{"location":"KB/H3%20View/","title":"H3 View","text":""},{"location":"KB/H3%20View/#h3-view","title":"H3 View","text":""},{"location":"KB/HL7/","title":"HL7","text":"","tags":["medical"]},{"location":"KB/HL7/#hl7","title":"HL7","text":"<ul> <li>broader range of standards for clinical and administrative data exchange</li> <li>HL7apy<ul> <li>pip install hl7apy</li> </ul> </li> </ul>","tags":["medical"]},{"location":"KB/HMDB51/","title":"HMDB51","text":""},{"location":"KB/HMDB51/#hmdb51","title":"HMDB51","text":"<ul> <li>smaller video dataset for human action recognition </li> <li>7,000 video clips in this dataset belong to 51 human action categories </li> <li>videos in HMDB51 dataset have 320 x 240 pixels spatial resolution and 30 FPS frame rate </li> <li>the self-supervised models are fine-tuned on the dataset to evaluate the quality of the learned video features.</li> </ul>"},{"location":"KB/HNSW/","title":"HNSW","text":""},{"location":"KB/HNSW/#hnsw","title":"HNSW","text":"<ul> <li>approximate nearest neighbor search algorithm that enables efficient similarity search over dense vector representations</li> <li>It builds a multi-layer navigable small world graph structure to index the vectors, allowing for fast and scalable retrieval of semantically similar documents.</li> </ul>"},{"location":"KB/Hallucination%20Text%20Generation/","title":"Hallucination Text Generation","text":""},{"location":"KB/Hallucination%20Text%20Generation/#hallucination-text-generation","title":"Hallucination Text Generation","text":"<ul> <li>Survey of Hallucination in Natural Language Generation</li> <li>often produces false statements that are disconnected from reality because such models are not grounded in reality</li> <li>hallucinated texts</li> </ul>"},{"location":"KB/Hallucination/","title":"Hallucination","text":""},{"location":"KB/Hallucination/#hallucination","title":"Hallucination","text":"<ul> <li>The production of plausible-seeming but factually incorrect output by a generative model that purports to be making an assertion about the real world. For example, if a dialog agent claims that Barack Obama died in 1865, the agent is hallucinating.</li> </ul>"},{"location":"KB/Hamming%20Distance/","title":"Hamming Distance","text":""},{"location":"KB/Hamming%20Distance/#hamming-distance","title":"Hamming Distance","text":"<ul> <li> \\[d = \\Sigma_{i}|p_{i}- q_{i}|\\] </li> <li>Hamming distance is the number of values that are different between two vectors</li> <li>It is typically used to compare two binary strings of equal length.</li> <li>difficult to use when two vectors are not of equal length</li> </ul>"},{"location":"KB/Hand%20Guiding/","title":"Hand Guiding","text":""},{"location":"KB/Hand%20Guiding/#hand-guiding","title":"Hand Guiding","text":"<ul> <li>allows an operator to hand guide the robot to a desired position. This task can be achieved by utilizing additional external hardware mounted directly to the robot or by a robot specifically designed to support this feature. Both solutions will require elements of functional safety to be utilized. A risk assessment shall be used to determine if any additional safeguarding is necessary to mitigate risks within the robot system.</li> </ul>"},{"location":"KB/Hard%20Parameter%20Sharing/","title":"Hard Parameter Sharing","text":""},{"location":"KB/Hard%20Parameter%20Sharing/#hard-parameter-sharing","title":"Hard Parameter Sharing","text":""},{"location":"KB/Harmonic%20Drive/","title":"Harmonic Drive","text":""},{"location":"KB/Harmonic%20Drive/#harmonic-drive","title":"Harmonic Drive","text":"<ul> <li>Compact lightweight speed reducer that converts high speed low torque to low speed high torque. Usually found on the minor (smaller) axis.</li> </ul>"},{"location":"KB/Harness/","title":"Harness","text":""},{"location":"KB/Harness/#harness","title":"Harness","text":"<ul> <li>Usually several wires, bundled together to deliver power and/or signal communications to/from devices. For example, the robot motors are connected to the controller through a wire harness.</li> </ul>"},{"location":"KB/Hashing/","title":"Hashing","text":""},{"location":"KB/Hashing/#hashing","title":"Hashing","text":"<ul> <li>In machine learning, a mechanism for bucketing categorical data, particularly when the number of categories is large, but the number of categories actually appearing in the dataset is comparatively small.</li> <li>For example, Earth is home to about 60,000 tree species. You could represent each of the 60,000 tree species in 60,000 separate categorical buckets. Alternatively, if only 200 of those tree species actually appear in a dataset, you could use hashing to divide tree species into perhaps 500 buckets.</li> </ul>"},{"location":"KB/Hausdorff%20Distance/","title":"Hausdorff Distance","text":""},{"location":"KB/Hausdorff%20Distance/#hausdorff-distance","title":"Hausdorff Distance","text":"<ul> <li> \\[d= max_{i}(|p_{i}-q_{i}|)\\] </li> </ul>"},{"location":"KB/Haversine%20Distance/","title":"Haversine Distance","text":""},{"location":"KB/Haversine%20Distance/#haversine-distance","title":"Haversine Distance","text":"<ul> <li> \\[d = 2r\\times arcsin(\\sqrt{sin^{2}(\\frac{\\varphi_{2}-\\varphi_{1}}{2})+cos(\\varphi_{1})cos(\\varphi_{2})sin^{2}(\\frac{\\lambda_{2}-\\lambda_{1}}{2}))}\\] </li> <li>Haversine distance is the distance between two points on a sphere given their longitudes and latitudes</li> <li>The main difference is that no straight line is possible since the assumption here is that the two points are on a sphere.</li> <li>ssumed the points lie on a sphere</li> <li>As you might have expected, Haversine distance is often used in navigation</li> <li>calculate the distance between two countries when flying between them</li> <li>Note that it is much less suited if the distances by themselves are already not that large. The curvature will not have that large of an impact.</li> </ul>"},{"location":"KB/He%20Initialization/","title":"He Initialization","text":""},{"location":"KB/He%20Initialization/#he-initialization","title":"He Initialization","text":"<ul> <li>bring the variance of those outputs to approximately one</li> <li>However, Kumar indeed proves mathematically that for the ReLU activation function, the best weight initialization strategy is to initialize the weights randomly but with this variance:<ul> <li> \\[\\begin{equation} v^{2} = 2/N \\end{equation}\\] </li> </ul> </li> <li>For Sigmoid based activation functions</li> </ul>"},{"location":"KB/Heaviside/","title":"Heaviside","text":""},{"location":"KB/Heaviside/#heaviside","title":"Heaviside","text":"<ul> <li>$$\\begin{cases} 1, \\text{if } z \\geq 0\\ 0, \\text{if } z &lt; 0</li> </ul> <p>\\end{cases}$$</p> <ul> <li>used in Rosenblatt's\u00a0Perceptron</li> <li>not\u00a0Differentiable\u00a0-&gt;\u00a0SGD\u00a0not possible</li> <li>no practical use</li> </ul>"},{"location":"KB/Height%20Plots/","title":"Height Plots","text":""},{"location":"KB/Height%20Plots/#height-plots","title":"Height Plots","text":"<ul> <li>2D scalar field</li> <li> \\[\\{(x,y, f(x,y))|(x,y)\\in \\mathbb{R}^{2}\\}\\] </li> <li>Displacement along \\(z = f(x,y)\\)</li> <li></li> </ul>"},{"location":"KB/Heightmaps%20Kinesthetic/","title":"Heightmaps Kinesthetic","text":""},{"location":"KB/Heightmaps%20Kinesthetic/#heightmaps-kinesthetic","title":"Heightmaps Kinesthetic","text":"<ul> <li>Herzog, Alexander, et al. \"Learning of grasp selection based on shape-templates.\" Autonomous Robots 36: pp. 51-65, 2014</li> <li></li> <li></li> </ul>"},{"location":"KB/Helmholtz%20Theorem/","title":"Helmholtz Theorem","text":""},{"location":"KB/Helmholtz%20Theorem/#helmholtz-theorem","title":"Helmholtz Theorem","text":""},{"location":"KB/Help%20Abuse/","title":"Help Abuse","text":""},{"location":"KB/Help%20Abuse/#help-abuse","title":"Help Abuse","text":"<p>Some students ask for help even when they don't need it. In the extreme cases, some students ask for help on every step.</p>"},{"location":"KB/Help%20Refusal/","title":"Help Refusal","text":""},{"location":"KB/Help%20Refusal/#help-refusal","title":"Help Refusal","text":"<p>Some students refuse to ask for help even when they need it. They enter a long series of incorrect steps, which may be guesses, instead of clicking on the help button.</p>"},{"location":"KB/Heteroscedatic/","title":"Heteroscedatic","text":""},{"location":"KB/Heteroscedatic/#heteroscedatic","title":"Heteroscedatic","text":"<ul> <li>if \\(\\sigma^{2}\\) is a function of the input or variable in \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma^{2})\\)</li> <li></li> </ul>"},{"location":"KB/HiFI-GAN%20Denoising/","title":"HiFI-GAN_Denoising","text":""},{"location":"KB/HiFI-GAN%20Denoising/#hifi-gan_denoising","title":"HiFI-GAN_Denoising","text":"<ul> <li>HiFi-GAN: High-Fidelity Denoising and Dereverberation Based on Speech Deep [Features](./Features.md) in Adversarial Networks&gt;</li> <li>Real-world audio recordings are often degraded by factors such as noise, reverberation, and equalization distortion</li> <li>transform recorded speech to sound as though it had been recorded in a studio</li> <li>end-to-end feed-forward WaveNet architecture, trained with multi-scale adversarial discriminators in both the time domain and the time-frequency domain</li> <li>relies on the deep feature matching losses of the discriminators to improve the perceptual quality of enhanced speech</li> </ul>"},{"location":"KB/HiFI-GAN%20Synthesis/","title":"HiFI-GAN Synthesis","text":""},{"location":"KB/HiFI-GAN%20Synthesis/#hifi-gan-synthesis","title":"HiFI-GAN Synthesis","text":"<ul> <li>HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis</li> <li>synthesis</li> <li>As speech audio consists of sinusoidal signals with various periods, they demonstrate that modeling periodic patterns of an audio is crucial for enhancing sample quality</li> <li>shows a significant improvement in terms of synthesis speed.</li> <li>MOS</li> <li>characteristic of speech audio that consists of patterns with various periods and applied it to neural networks, and verified that the existence of the proposed discriminator greatly influences the quality of speech synthesis through the ablation study</li> <li>generalize to the mel-spectrogram inversion of unseen speakers and synthesize speech audio comparable to human quality from noisy inputs in an end-to-end setting</li> <li>progress towards on-device natural speech synthesis, which requires low latency and memory footprint</li> <li>generators of various configurations can be trained with the same discriminators and learning mechanism</li> <li>possibility of flexibly selecting a generator configuration according to the target specifications without the need for a time-consuming hyper-parameter search for the discriminators</li> </ul>"},{"location":"KB/Hide%20and%20Seek/","title":"Hide and Seek","text":""},{"location":"KB/Hide%20and%20Seek/#hide-and-seek","title":"Hide and Seek","text":"<ul> <li>@singhHideandSeekDataAugmentation2018</li> <li>divides an image into a specified number of grids and turns on and off each grid with an assigned probability</li> <li>various image regions are deleted, and they can be connected or disconnected from each other</li> <li>Values in turned-off regions are replaced with the average of all the pixel values in the entire dataset.</li> </ul>"},{"location":"KB/Hierarchial%20Refinement/","title":"Heirarchial Refinement","text":""},{"location":"KB/Hierarchial%20Refinement/#heirarchial-refinement","title":"Heirarchial Refinement","text":"<ul> <li>Split n dim volume -&gt; finite set of discrete regions</li> <li>n dim hypercubes</li> <li>Construct regions where there is a higher point density</li> <li>Fine grained info encoded -&gt; smaller hypercubes to increase resolution</li> <li>n = 2 : quadtree</li> <li>n = 3 : octree</li> <li></li> <li>Mesh refinement</li> </ul>"},{"location":"KB/Hierarchial%20Refinement/#_1","title":"\u2026","text":""},{"location":"KB/Hierarchical%20Edge%20Bundling/","title":"Hierarchical Edge Bundling","text":""},{"location":"KB/Hierarchical%20Edge%20Bundling/#hierarchical-edge-bundling","title":"Hierarchical Edge Bundling","text":"<ul> <li>Exploit the hierarchical structure to bundle non-hierarchical edges visually together</li> <li>conceptual similarity to bundling KB/Streamlines.md</li> <li></li> </ul>"},{"location":"KB/High%20pass%20filter/","title":"High pass filter","text":""},{"location":"KB/High%20pass%20filter/#high-pass-filter","title":"High Pass Filter","text":"<ul> <li>That passes signals with a frequency higher than a certain cutoff frequency and attenuates signals with frequencies lower than the cutoff frequency.</li> </ul>"},{"location":"KB/Higher%20Layer%20Capsule/","title":"Higher Layer Capsule","text":""},{"location":"KB/Higher%20Layer%20Capsule/#higher-layer-capsule","title":"Higher Layer Capsule","text":"<ul> <li>The outputs of the Primary Capsule are then passed to higher-layer capsules, which\u00a0combine the information from multiple primary capsules\u00a0to extract more abstract concepts, such as the identity of an object or the presence of a face.</li> <li>Routing by Agreement</li> </ul>"},{"location":"KB/Highway%20Convolutions/","title":"Highway Convolutions","text":""},{"location":"KB/Highway%20Convolutions/#highway-convolutions","title":"Highway Convolutions","text":"<ul> <li>Conv</li> </ul> <pre><code>class HighwayConv1dNew(nn.Conv1d):\n    def forward(self, inputs):\n        L = super().forward(inputs)\n        H1, H2 = rearrange(L, 'b (split c) t -&gt; split b c t', split=2)\n        torch.sigmoid_(H1)\n        return H1 * H2 + (1.0 - H1) * inputs\n</code></pre>"},{"location":"KB/Hinge%20Loss/","title":"Hinge","text":""},{"location":"KB/Hinge%20Loss/#hinge","title":"Hinge","text":"<ul> <li>Classification</li> <li>SVM </li> <li>the w are weights of the model</li> <li> <p>labels are 1 or -1</p> </li> <li> \\[\\mathrm{max}\\left( 0, 1 + \\mathrm{max}\\left( w_{y} \\cdot x - w_{t} \\cdot x \\right) \\right)\\] </li> <li> \\[L(y, \\hat y) = \\Sigma_{i}max(0, 1- y_{i}\\hat y_{i})\\] </li> <li> <p>maximum margin classification </p> </li> </ul>"},{"location":"KB/Hit%20list/","title":"Hit list","text":""},{"location":"KB/Hit%20list/#hit-list","title":"Hit List","text":"<ul> <li>A shape feature which is powerful for Retrieval may not be strong in Recognition!</li> <li>Feature B: hit list should provide nice, intuitive rank in a satisfying \u2018hit list\u2019</li> <li>Feature A: target word class should survive competit with the other word classes (emerging needle from the heterogeneous hay stack)</li> </ul>"},{"location":"KB/Holdout%20Data/","title":"Holdout Data","text":""},{"location":"KB/Holdout%20Data/#holdout-data","title":"Holdout Data","text":""},{"location":"KB/Homoscedatic/","title":"Homoscedatic","text":""},{"location":"KB/Homoscedatic/#homoscedatic","title":"Homoscedatic","text":"<ul> <li>if \\(\\sigma^{2}\\) is constant in \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma^{2})\\)</li> <li></li> </ul>"},{"location":"KB/Hopfield%20networks/","title":"Hopfield networks","text":""},{"location":"KB/Hopfield%20networks/#hopfield-networks","title":"Hopfield Networks","text":"<ul> <li>from</li> <li>Older architecture used to store and retrieve patterns</li> <li>Building blocks<ul> <li>Data (list of patterns)</li> <li>Network<ul> <li>Nodes</li> <li>All nodes are connected with each other</li> </ul> </li> <li>Retrieval<ul> <li>Input: partial pattern</li> <li>Output: full pattern (retrieved)<ul> <li>\"best match\" partial pattern to entire data</li> <li>Filling out the missing nodes with the best pattern is called\u00a0update rule</li> </ul> </li> </ul> </li> <li>Energy Function<ul> <li>Theoretical concept, similar to a loss function</li> <li>Is not being optimized directly but trough update function</li> </ul> </li> <li>Update function<ul> <li>Optimizes the pattern that will be retrieved to best match the partial pattern</li> </ul> </li> </ul> </li> </ul>"},{"location":"KB/How%20the%20Sciences%20Faded%20From%20India/","title":"How the Sciences Faded From India","text":"","tags":["indianhistory"]},{"location":"KB/How%20the%20Sciences%20Faded%20From%20India/#how-the-sciences-faded-from-india","title":"How the Sciences Faded From India","text":"<p>How the Sciences Faded From India   - Maintenance of secrecy, foreign invasions, easy availability of the means of subsistence, lack of royal patronage, and the apathy of the Indian people due to general introverted tendencies\u2013all these gradually stemmed the flow of scientific research in ancient India, and in time reduced it to an antique. Even then, we must certainly remember with pride that the scientific research of ancient India is indeed an essential chapter of Indian culture in the evolution of national pedigree   - during the colonization of India, a trend was set by the British in a systematic manner to discard all traditional systems of knowledge in India and to look at traditional practices with contempt. Unfortunately, this trend continued further after independence, and can still be detected even today. This resulted in the neglect of all the traditional knowledge systems, practices and indigenous science and technology systems of India.   - one of the bands of scholars whose primary interest was the converting of Hindus to the one \"true faith\" by any means necessary were those at the University of Oxford who started the Boden Professorship of Sanskrit.   - And the British system of education was a great means of making Indians forget their culture and all that they once were, and how India was once the wealthiest area in the world and a center for great learning   - Moreover, it was also a means to make the Indian people feel backwards and unworthy, and that only by accepting the Western values could they again become truly progressive and part of the civilized world.   - we can only guess at how much more advanced the world could have been, and how many more developments may have originated out of Vedic culture if it had been allowed to continue, uninterrupted by the invaders over the past 1000 years or so, whether they be the Muslims, Moghuls, the British, the Portuguese, or so on. They cared little for the culture and even preferred to destroy it, and had even less concern for the people. - T. B. Macaulay</p>","tags":["indianhistory"]},{"location":"KB/How%20to%20take%20your%20visual%20storytelling%20to%20the%20next%20level/","title":"How to take your visual storytelling to the next level","text":"","tags":["composition"]},{"location":"KB/How%20to%20take%20your%20visual%20storytelling%20to%20the%20next%20level/#how-to-take-your-visual-storytelling-to-the-next-level","title":"How to Take Your Visual Storytelling to the Next Level","text":"<ul> <li>Details matter. Little specks on clothes etc  </li> <li>Neutral light is best for taking shot. Easiest to control  </li> <li>thumbnails first always  </li> <li>planning is important</li> </ul>","tags":["composition"]},{"location":"KB/Huber/","title":"Huber/Smooth L1/Smooth MAE","text":""},{"location":"KB/Huber/#hubersmooth-l1smooth-mae","title":"Huber/Smooth L1/Smooth MAE","text":"<ul> <li>It is less sensitive to outliers than the MSE and in some cases prevents exploding #gradients</li> <li>Fast-RCNN</li> </ul> <p>if $$\\left( \\left|y - \u0177\\right| \\lt 1.0 \\right) &gt;1 $$</p> \\[\\frac{1}{\\mathrm{length}\\left( y \\right)} \\cdot \\mathrm{sum}\\left( 0.5 \\cdot \\left( y - \u0177 \\right)^{2} \\right)\\] <p>else</p> \\[\\frac{1}{\\mathrm{length}\\left( y \\right)} \\cdot \\mathrm{sum}\\left( \\left\\|y - \u0177\\right\\| - 0.5 \\right)\\]"},{"location":"KB/Huber/#_1","title":"\u2026","text":""},{"location":"KB/Human%20Action%20Recognition/","title":"Human Action Recognition","text":""},{"location":"KB/Human%20Action%20Recognition/#human-action-recognition","title":"Human Action Recognition","text":"<ul> <li>The action recognition task is often used to evaluate the quality of video features learned by self-supervised learning methods </li> <li>first trained on unlabeled video data with pretext tasks, then it is fine-tuned on action recognition datasets with human annotations to recognize the actions</li> </ul>"},{"location":"KB/Humanoid%20Vision%20Engine/","title":"Humanoid Vision Engine","text":""},{"location":"KB/Humanoid%20Vision%20Engine/#humanoid-vision-engine","title":"Humanoid Vision Engine","text":"<ul> <li>HVE</li> <li>summarize the contribution of shape, texture, and color in a given task (dataset) by separately computing the three features to support image classification</li> <li>end-to-end learning with backpropagation to simulate the learning process of humans and to summarize the contribution of shape, texture, and color</li> <li>advantage of end-to-end training is that we can avoid introducing human bias, which may influence the objective of contribution attribution</li> </ul>"},{"location":"KB/Huntington%E2%80%99s%20Disease/","title":"Huntington\u2019s Disease","text":""},{"location":"KB/Huntington%E2%80%99s%20Disease/#huntingtons-disease","title":"Huntington\u2019s Disease","text":"<ul> <li>A neurodegenerative disorder that causes progressive death of neurons in the brain, resulting in severe movement and cognitive problems. The disorder is caused by the mutation of a single gene</li> <li>\u2014and symptoms typically present when an individual is in his or her 30\u2019s or 40\u2019s.</li> </ul>"},{"location":"KB/HyTAS/","title":"HyTAS","text":"","tags":["automl"]},{"location":"KB/HyTAS/#hytas","title":"HyTAS","text":"","tags":["automl"]},{"location":"KB/HyTAS/#rebuttal","title":"Rebuttal","text":"","tags":["automl"]},{"location":"KB/HyTAS/#reviewer-2","title":"Reviewer 2","text":"<ul> <li>which seems to have the problem of local optimization<ul> <li>HyTAS seems to test specific variants of the same architecture : increasing depth/width. I think the reviewer meant that it does not test different kinds of transformer architectures -&gt; more branches/more concatenations etc. </li> <li>they thought it was a general metric to evaluate any kind of transformer. this is not the case. perhaps a clarification would be nice?</li> </ul> </li> <li>more detailed optimization<ul> <li>architecture diagram</li> </ul> </li> </ul>","tags":["automl"]},{"location":"KB/HyTAS/#general-comments","title":"General Comments","text":"<ul> <li>ZICO -&gt; ZICO++ ?</li> <li>difference between CNNs and transformers and why this architecture specifically?</li> </ul>","tags":["automl"]},{"location":"KB/Hybrid%20Word%20Segmentation/","title":"Hybrid Word Segmentation","text":""},{"location":"KB/Hybrid%20Word%20Segmentation/#hybrid-word-segmentation","title":"Hybrid Word Segmentation","text":"<ul> <li>combination</li> <li>weighted Finite State Transducer to identify dictionary entries</li> </ul>"},{"location":"KB/HyperStreamlines/","title":"HyperStreamlines","text":""},{"location":"KB/HyperStreamlines/#hyperstreamlines","title":"HyperStreamlines","text":""},{"location":"KB/Hypertension/","title":"Hypertension","text":""},{"location":"KB/Hypertension/#hypertension","title":"Hypertension","text":"<ul> <li>Unusually high blood pressure</li> </ul>"},{"location":"KB/Hypodermic%20Needle/","title":"Hypodermic Needle","text":""},{"location":"KB/Hypodermic%20Needle/#hypodermic-needle","title":"Hypodermic Needle","text":"<ul> <li>A very thin, hollow needle used with a syringe to inject substances into the body or to extract blood</li> </ul>"},{"location":"KB/Hypotension/","title":"Hypotension","text":""},{"location":"KB/Hypotension/#hypotension","title":"Hypotension","text":"<ul> <li>Unusually low blood pressure</li> </ul>"},{"location":"KB/Hypothalamus/","title":"Hypothalamus","text":""},{"location":"KB/Hypothalamus/#hypothalamus","title":"Hypothalamus","text":"<ul> <li>is located in the floor of the third ventricle and is the master control of the autonomic system.</li> <li>It plays a role in controlling behaviors such as hunger, thirst, sleep, and sexual response.</li> <li>It also regulates body temperature, blood pressure, emotions, and secretion of hormones.</li> </ul>"},{"location":"KB/Hysterectomy/","title":"Hysterectomy","text":""},{"location":"KB/Hysterectomy/#hysterectomy","title":"Hysterectomy","text":"<ul> <li>Surgical procedure to remove the uterus</li> </ul>"},{"location":"KB/ICA%20Noise%20Removal/","title":"ICA Noise Removal","text":""},{"location":"KB/ICA%20Noise%20Removal/#ica-noise-removal","title":"ICA Noise Removal","text":"<ul> <li>Notch filter</li> <li>High pass filter</li> </ul>"},{"location":"KB/ICA/","title":"ICA","text":""},{"location":"KB/ICA/#ica","title":"ICA","text":"<ul> <li>Independant Component Analysis</li> <li>Unmix combinations of signals</li> <li>Look for rotations of data into maximally independant components</li> <li>Not always orthogonal</li> <li>Better after noise removal</li> <li>Remove fewer than 20%</li> <li>Remove really bad parts first</li> <li></li> <li>ICA Noise Removal</li> </ul>"},{"location":"KB/IDRiD/","title":"IDRiD","text":""},{"location":"KB/IDRiD/#idrid","title":"IDRiD","text":"<ul> <li>IDRiD is con- cerned with the disease grade recognition of retina images, and the presence or absence of diseases is recognized from exudates and hemorrhages.</li> <li>IDRiD includes a segmentation label of disease regions annotated by a specialist</li> </ul>"},{"location":"KB/IID/","title":"IID","text":""},{"location":"KB/IID/#iid","title":"IID","text":"<ul> <li>Neural networks assumes that the data points are independent and identically distributed.</li> </ul>"},{"location":"KB/ILSVRC/","title":"ILSVRC","text":""},{"location":"KB/ILSVRC/#ilsvrc","title":"ILSVRC","text":""},{"location":"KB/IMDB/","title":"IMDB","text":""},{"location":"KB/IMDB/#imdb","title":"IMDB","text":"<ul> <li>Movie reviews</li> </ul>"},{"location":"KB/IRT/","title":"IRT","text":""},{"location":"KB/IRT/#irt","title":"IRT","text":"<ul> <li>assumes that every test item has a difficulty, and different items have different difficulties (Embretson &amp; Reise, 2000)</li> <li>Unfortunately, IRT generally assumes that test items are conditionally independent given the student's competence. This is seldom true of the raw measures collected at the step level by tutoring systems</li> <li>Although IRT has powerful features, such as calibration algorithms that empirically determine item difficulties and other parameters, considerable work is needed before it can be applied to tutoring systems.</li> </ul>"},{"location":"KB/ISIC%202018/","title":"ISIC 2018","text":""},{"location":"KB/ISIC%202018/#isic-2018","title":"ISIC 2018","text":"<ul> <li>2386 dermoscopy images, all of them annotated with patterns on skin lesions unanimously recognized as indicators of potential malignancy</li> <li>binary masks highlighting the presence of five \"features\" at pixel-level</li> <li>globules, streaks, pigment network, negative network, and milia-like cysts.</li> <li>The performances on the feature extraction task are measured using the Jaccard index.</li> </ul>"},{"location":"KB/ITM%20Loss/","title":"ITM Loss","text":""},{"location":"KB/ITM%20Loss/#itm-loss","title":"ITM Loss","text":"<ul> <li>ITM loss is an alignment loss that encompasses cross-KB/Modality.md interaction between image and text</li> <li>ITM requires positive and negative pairs</li> </ul>"},{"location":"KB/IVFADC/","title":"IVFADC","text":""},{"location":"KB/IVFADC/#ivfadc","title":"IVFADC","text":"<ul> <li>indexing structure that combines an inverted file system with a quantization-based approach</li> <li>It allows for efficient indexing and retrieval of large-scale vector databases by partitioning the vector space and using asymmetric distance computation.</li> </ul>"},{"location":"KB/Ideas%20for%20Fact%20Learning/","title":"Ideas for Fact Learning","text":""},{"location":"KB/Ideas%20for%20Fact%20Learning/#ideas-for-fact-learning","title":"Ideas for Fact Learning","text":"<ul> <li>Type of information<ul> <li>Methods of presentation</li> <li>Types of feedback</li> <li>Time pressure</li> <li>Study time</li> <li>Visualize progress</li> <li>Decide when an item is mastered</li> </ul> </li> <li>Increase system info about the user<ul> <li>Reaction times</li> <li>accuracy</li> <li>Eye Tracking</li> <li>biometric</li> <li>pupil dilation</li> <li>Detect learning styles</li> </ul> </li> <li>Framework<ul> <li>Gamification</li> </ul> </li> </ul>"},{"location":"KB/Identity%20Loss/","title":"Identity Loss","text":""},{"location":"KB/Identity%20Loss/#identity-loss","title":"Identity Loss","text":"<ul> <li> \\[L_{identity}(G,F) = \\mathbb{E}_{y \\sim p_{data}(y)}[||G(y)-y)||_{1}] + \\mathbb{E}_{x \\sim p_{data}(x)}[||F(x)-x)||_{1}]\\] </li> <li>The identity loss is used to preserve the color and prevent reverse color in the result.</li> <li>This loss can regularize the generator to be near an identity mapping when real samples of the target domain are provided. If something already looks like from the target domain, you should not map it into a different image.</li> <li>The model will be more conservative for unknown content.</li> <li>In general, it can help bette preserve the content if that is your priority.</li> </ul>"},{"location":"KB/Image%20Classification/","title":"Image Classification","text":""},{"location":"KB/Image%20Classification/#image-classification","title":"Image Classification","text":"<ul> <li>When choosing image classification as a downstream task to evaluate the quality of image features learned from self-supervised learning methods, the self-supervised learned model is applied on each image to extract features which then are used to train a classifier such as Support Vector Machine (SVM) [105].</li> </ul>"},{"location":"KB/Image%20Data%20Augmentation%20Survey/","title":"Image Data Augmentation Survey","text":""},{"location":"KB/Image%20Data%20Augmentation%20Survey/#image-data-augmentation-survey","title":"Image Data Augmentation Survey","text":""},{"location":"KB/Image%20Data%20Augmentation%20Survey/#introduction","title":"Introduction","text":"<ul> <li>sufficient open datasets like Imagenet [Russakovsky et al., 2015], MS-COCO [Lin et al., 2014] and PASCAL VOC [Everingham et al., 2015] are crucial to the development of deep learning models. </li> <li>imbalance among the developments of these three perspectives </li> <li>The core idea of data augmentation is to improve the sufficiency and diversity of training data by generating synthetic dataset </li> <li>The augmented data can be regarded as being extracted from a distribution that is close to the real one </li> <li>augmented dataset can represent more comprehensive characteristics </li> <li>data augmentation methods are tasks-independent </li> <li>Because the operations are performed on the image data and labels at the same time, and the label types are different under different tasks, the data augmentation methods for object detection task can not be directly applied to semantic segmentation task  </li> <li> </li> <li>it is meaningful to apply basic image manipulations only under the assumption that the existing data obeys the distribution close to the actual data distribution. </li> <li>some basic image manipulation methods, such as translation and rotation, suffer from the padding effect </li> <li>, some areas of the images will be moved out of the boundary and lost </li> <li> <p>Therefore, some interpolation methods will be applied to fill in the blank part. Generally, the region outside the image boundary is assumed to be constant 0, which will be black after manipulation. </p> </li> <li> <p>Image Erasing</p> </li> <li> <p>Image Mix</p> </li> <li> <p>Image Manipulation</p> </li> <li> <p>Auto Augment</p> </li> <li> <p>Feature Augmentation</p> </li> <li> <p>Deep Generative Models</p> </li> </ul>"},{"location":"KB/Image%20Data%20Augmentation%20Survey/#semantic-segmentation","title":"Semantic Segmentation","text":"<ul> <li>PASCAL VOC  </li> <li> </li> </ul>"},{"location":"KB/Image%20Data%20Augmentation%20Survey/#image-classification","title":"Image Classification","text":""},{"location":"KB/Image%20Data%20Augmentation%20Survey/#object-detection","title":"Object Detection","text":""},{"location":"KB/Image%20Data%20Augmentation%20Survey/#discussion-for-future-directions","title":"Discussion for Future Directions","text":"<ul> <li>lack of theoretical research on data augmentation   </li> <li>some methods can improve the accuracy, but we do not fully understand the reasons behind, such as pairing samples and mixup </li> <li>To human eyes, the augmented data with pairing samples and mixup are visually meaningless </li> <li>no theory on the size of sufficient training datasets </li> <li>The size of the dataset suitable for tasks and models is usually designed based on personal experience and through extensive experiments </li> <li>no unified metrics </li> <li>saturate the minority class and cause overfitting </li> <li>Ultimately, we expect generated data can simulate distribution similar with training data while diversity never losses. </li> <li>increase in the amount of training data is not exactly proportional to the increase in the performance </li> <li>When a certain amount of data is reached, continue to increase the data without improving the effect. </li> <li>despite the increase in the number of data, the diversity of data remains unchanged </li> <li>Since various data augmentation can be combined together to generate new image data, the selection and combination of data augmentation techniques are critical </li> <li>how to choose and combine methods is a key point when performing data augmentation </li> </ul>"},{"location":"KB/Image%20Data/","title":"Image Data","text":""},{"location":"KB/Image%20Data/#image-data","title":"Image Data","text":"<ul> <li>Cant use MLPs<ul> <li>Too many weights to learn</li> <li>No translation equi-invariance</li> </ul> </li> </ul>"},{"location":"KB/Image%20Erasing/","title":"Image Erasing","text":""},{"location":"KB/Image%20Erasing/#image-erasing","title":"Image Erasing","text":"<ul> <li>delete one or more sub-regions in the image </li> <li>replace the pixel values of these sub-regions with constant values or random values </li> <li>Hide-and-Seek </li> <li>random erasing </li> <li>GridMask </li> <li>Cutout </li> <li>FenceMask </li> <li>simulation of object occlusion strategy.</li> </ul>"},{"location":"KB/Image%20Generation%20with%20Colorization/","title":"Image Generation with Colorization","text":""},{"location":"KB/Image%20Generation%20with%20Colorization/#image-generation-with-colorization","title":"Image Generation with Colorization","text":"<ul> <li>Image colorization is a task of predicting a plausible color version of the photograph given a gray-scale photograph as input </li> <li>To correctly colorize each pixel, networks need to recognize objects and to group pixels of the same part together. Therefore, visual features can be learned in the process of accomplishing this task. </li> <li>Zhang et al. proposed to handle the uncertainty by posting the task as a clas- sification task and used class-rebalancing to increase the diversity of predicted colors [18] </li> <li>Some work specifically employs the image colorization task as the pretext for self- supervised image representation learning [18], [42], [82], [124] </li> <li>After the image colorization training is finished, the features learned through the colorization process are specifically evaluated on other downstream high-level tasks with transfer learning.</li> </ul>"},{"location":"KB/Image%20Generation%20with%20Inpainting/","title":"Image Generation with Inpainting","text":""},{"location":"KB/Image%20Generation%20with%20Inpainting/#image-generation-with-inpainting","title":"Image Generation with Inpainting","text":"<ul> <li>Image inpainting is a task of predicting arbitrary missing regions based on the rest of an image </li> <li>To correctly predict missing regions, networks are required to learn the common knowledge including the color and structure of the common objects </li> <li>Only by knowing this knowledge, networks are able to infer missing regions based on the rest part of the image.</li> </ul>"},{"location":"KB/Image%20Generation%20with%20Super%20Resolution/","title":"Image Generation with Super Resolution","text":""},{"location":"KB/Image%20Generation%20with%20Super%20Resolution/#image-generation-with-super-resolution","title":"Image Generation with Super Resolution","text":"<ul> <li>Image super-resolution (SR) is a task of enhancing the resolution of images </li> <li>With the help of fully convolutional networks, finer and realistic high-resolution images can be generated from low-resolution images </li> <li>perceptual loss which consists of an adversarial loss and a content loss </li> <li>With the perceptron loss, the SRGAN is able to recover photo-realistic textures from heavily downsampled images and show significant gains in perceptual quality. </li> <li>The networks for image super-resolution task are able to learn the semantic features of images</li> </ul>"},{"location":"KB/Image%20Generation/","title":"Image Generation","text":""},{"location":"KB/Image%20Generation/#image-generation","title":"Image Generation","text":"<ul> <li>Visual features are learned through the process of image generation tasks. </li> <li>image colorization </li> <li>super resolution </li> <li>inpainting </li> <li>image generation with Generative Adversarial Networks (GANs)</li> </ul>"},{"location":"KB/Image%20Manipulation/","title":"Image Manipulation","text":""},{"location":"KB/Image%20Manipulation/#image-manipulation","title":"Image Manipulation","text":"<ul> <li>image transformations, such as rotation, flipping, and cropping, etc </li> <li>manipulate the images directly and are easy to implement </li> <li>CutMix </li> <li>Fmix</li> <li>AugMix </li> <li>ManifoldMix</li> </ul>"},{"location":"KB/Image%20Mix/","title":"Image Mix","text":""},{"location":"KB/Image%20Mix/#image-mix","title":"Image Mix","text":"<ul> <li>mixing two or more images or sub-regions of images into one. </li> <li>synthesizing every new image with two images randomly selected in the training set, known as pairing samples. The synthesis method used is to average the intensity of two images on each pixel </li> <li>Mixup</li> </ul>"},{"location":"KB/Image%20Mixing%20and%20Deletion/","title":"Image Mixing and Deletion","text":""},{"location":"KB/Image%20Mixing%20and%20Deletion/#image-mixing-and-deletion","title":"Image Mixing and Deletion","text":"<ul> <li>@naveedSurveyImageMixing2023</li> <li>[Cutout] and CutMix argues that hindering image regions enforces the classifier to learn from the partially visible objects and understand the overall structure</li> <li>CutMix verifies this argument by showing enhanced focus towards the target class in</li> <li>Opposite to this, MixUp has shown to improve classifier's calibration and reduced prediction uncertainity</li> <li>mean of predictions vs accuracy where the confidence distribution for MixUp trained model is evenly distributed against the standard model whose disovertribution is towards higher conficence i.e. confidence</li> <li>Similarly, the loss contours obtained for a network trained with MixUp are smooth as compared to sharp contours in standarad training</li> <li>better generalization and robustness of MixUp against adversarial attacks.</li> <li>Mixup</li> <li>Cut and Delete</li> <li>Cutout</li> <li>Random Erasing</li> <li>Hide and Seek</li> <li>GridMask</li> <li>Adversarial Spatial Dropout for Occlusion</li> <li>Cut and Mix</li> <li>CutMix</li> <li>Attentive CutMix</li> <li>AttributeMix</li> <li>RICAP</li> <li>Mixed Example</li> <li>CowMask</li> <li>ResizeMix</li> <li>SaliencyMix</li> <li>Intra-Class Part Swapping</li> <li>SnapMix</li> <li>KeepAugment</li> <li>Visual Context Augmentation</li> <li>Cut, Paste and Learn</li> <li>Manifold MixUp</li> <li>AugMix</li> <li>SmoothMix</li> <li>Co-Mixup</li> <li>Sample Pairing</li> <li>Puzzle Mix</li> <li>ReMix</li> </ul>"},{"location":"KB/ImageNet/","title":"ImageNet","text":""},{"location":"KB/Imagen/","title":"Imagen","text":""},{"location":"KB/Imagen/#imagen","title":"Imagen","text":"<ul> <li>better top-1 accuracy on ImageNet than EfficientNet at similar latency</li> <li>Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding</li> <li>text-to-image diffusion model</li> <li>large Transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation</li> <li>Imagen produces \\(1024 \\times 1024\\) samples with unprecedented photorealism and alignment with text</li> <li>generic large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis</li> <li>increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model</li> <li>FID score</li> <li>COCO</li> </ul>"},{"location":"KB/Implementation%20Invariance/","title":"Implementation Invariance","text":""},{"location":"KB/Implementation%20Invariance/#implementation-invariance","title":"Implementation Invariance","text":"<ul> <li>Two networks are functionally equivalent if their outputs are equal for all inputs, despite having very different implementations.</li> <li>Attribution methods should satisfy Implementation Invariance, i.e., the attributions are always identical for two functionally equivalent networks.</li> </ul>"},{"location":"KB/Implicit%20Bias/","title":"Implicit Bias","text":""},{"location":"KB/Implicit%20Bias/#implicit-bias","title":"Implicit Bias","text":"<ul> <li>The unconscious attitudes, beliefs, or stereotypes we hold that have the power to affect our perceptions, actions, and decisions.</li> </ul>"},{"location":"KB/Implicit%20Bias/#implicit-bias_1","title":"Implicit Bias","text":"<ul> <li>Automatically making an association or assumption based on one\u2019s mental models and memories. Implicit bias can affect the following<ul> <li>How data is collected and classified.</li> <li>How machine learning systems are designed and developed.</li> </ul> </li> <li>For example, when building a classifier to identify wedding photos, an engineer may use the presence of a white dress in a photo as a feature. However, white dresses have been customary only during certain eras and in certain cultures.</li> </ul>"},{"location":"KB/Implicitly%20learning%20when%20to%20be%20ready%20-%20From%20instances%20to%20categories/","title":"Implicitly learning when to be ready - From instances to categories","text":""},{"location":"KB/Implicitly%20learning%20when%20to%20be%20ready%20-%20From%20instances%20to%20categories/#implicitly-learning-when-to-be-ready-from-instances-to-categories","title":"Implicitly learning when to be ready - From instances to categories","text":"<ul> <li> <p>Wouter Kruijne \u00b7 Riccardo M. Galli \u00b7 Sander A. Los</p> </li> <li> <p>role of long-term memory in guiding temporal preparation in speeded reaction time tasks.</p> </li> <li>In experiments with variable foreperiods between a warning stimulus (S1) and a target stimulus (S2), preparation is affected by foreperiod distributions experienced in the past, long after the distribution has changed</li> <li>associative nature of memory-guided preparation</li> <li>When distinct S1s predict different foreperiods, they can trigger differential preparation accordingly</li> <li>memory-guided preparation allows for another key feature of learning: the ability to generalize across acquired associations and apply them to novel situations</li> <li>Images of either category were paired with different distributions with predominantly shorter versus predominantly longer foreperiods.</li> <li>differential preparation to never-before seen images of either category, without being aware of the predictive nature of these categories</li> <li>continued doing so in a subsequent Transfer phase, after they had been informed that these contingencies no longer held</li> <li>rolling regression analysis revealed at a fine timescale how category-guided preparation gradually developed throughout the task</li> <li>explicit information about these contingencies only briefly disrupted memory-guided preparation</li> </ul>"},{"location":"KB/Implicitly%20learning%20when%20to%20be%20ready%20-%20From%20instances%20to%20categories/#preparation-across-phases","title":"Preparation across phases","text":""},{"location":"KB/Implicitly%20learning%20when%20to%20be%20ready%20-%20From%20instances%20to%20categories/#results","title":"Results","text":"<ul> <li>the RT-FP curve, separately for the different S1 types during the Acquisition and Transfer phase</li> <li>this curve is flatter for trials paired with S1E types than S1A types in both phases, indicating that the categories yielded differential preparation</li> <li>significant FP \u00d7 Phase interaction</li> <li>suggesting that there was a steeper RTFP slope during Acquisition than during Transfer</li> <li>evidence for a two-way S1 type \u00d7 FP interaction</li> <li>but not for a three way S1 type \u00d7 Phase \u00d7 FP interaction</li> <li>different S1 types led to differential preparation in both phases</li> <li>biased distributions in the Acquisition phase gave rise to long-lasting effects on preparation, persisting after the bias was removed</li> </ul>"},{"location":"KB/Implicitly%20learning%20when%20to%20be%20ready%20-%20From%20instances%20to%20categories/#time-course-of-differential-preparation","title":"Time course of differential preparation","text":"<ul> <li>participants might have gotten somewhat fatigued throughout each block, but that block breaks allowed them to largely recover to baseline</li> <li>the longer interruption between the Phases led to a more pronounced speeding up for the remaining two blocks</li> <li>overall temporal preparation remained largely consistent throughout the experiment</li> <li>the results with a shorter window of 40 trials highlight that the 'dip' in the S1 type \u00d7 FP time course at the start of the Transfer phase was very shortlived</li> <li>The brevity of this effect could therefore explain why our earlier work, using block-wise analyses, consistently led us to conclude that this transition between phases had no noticeable effect on differential preparation</li> </ul>"},{"location":"KB/Implicitly%20learning%20when%20to%20be%20ready%20-%20From%20instances%20to%20categories/#discussion","title":"Discussion","text":"<ul> <li>Our findings raise the possibility that participants in that study might have demonstrated similar memoryguided preparation even if they would have been unaware of the image-FP pairings</li> <li>complex interplay between implicit associative guidance and guidance by explicit awareness</li> <li>Nevertheless, differential preparation was robust once acquired, persisting well into the Transfer phase despite the change in underlying FP distributions</li> <li>longer Transfer phases, we similarly observed that the S1 type \u00d7 FP interaction barely attenuated across Transfer blocks</li> <li>The gradual acquisition of differential preparation and its longevity throughout the Transfer phase illustrate how temporal preparation is affected by long-term memory and sluggishly adapts to changing environmental statistics</li> <li>Many probability-driven models characterize preparation as guided by static representations of the current FP distribution (Janssen &amp; Shadlen, 2005; Grabenhorst et al., 2019; Trillenberg et al., 2000; Vangkilde et al., 2013), foregoing the role of memory and learning</li> <li>Transfer effects like those in the present study illustrate the need for a flexible basis for preparation, subject to learning and updating</li> </ul>"},{"location":"KB/Implicitly%20learning%20when%20to%20be%20ready%20-%20From%20instances%20to%20categories/#images","title":"Images","text":""},{"location":"KB/Impulse/","title":"Impulse","text":""},{"location":"KB/Impulse/#impulse","title":"Impulse","text":"<ul> <li>An impulse is defined as a force applied over a period of time. Applying a larger force or longer lasting force produces a larger impulse. A large impulse produces a large change in momentum.</li> <li> \\[J = \\Delta p \\] </li> <li> \\[F \\Delta t = \\Delta p\\] </li> <li> \\[F \\Delta t = mv - mu\\] </li> <li> \\[F \\Delta t = m \\Delta v\\] </li> <li> \\[\\Delta v = \\frac{F}{m} \\Delta t\\] </li> <li>J is impulse in kg m/s</li> <li>F is force</li> <li>p is momentum</li> <li>v,u is velocity</li> </ul>"},{"location":"KB/In%20Silico/","title":"In Silico","text":""},{"location":"KB/In%20Silico/#in-silico","title":"In Silico","text":"<ul> <li>An experimental method to study brain or neural function using computer modeling or computer simulation.</li> </ul>"},{"location":"KB/In%20Vitro/","title":"In Vitro","text":""},{"location":"KB/In%20Vitro/#in-vitro","title":"In Vitro","text":"<ul> <li>An experimental method to study brain or neural function by looking at cells outside a living organism, for example, in a test tube or petri dish.</li> </ul>"},{"location":"KB/In%20Vivo/","title":"In Vivo","text":""},{"location":"KB/In%20Vivo/#in-vivo","title":"In Vivo","text":"<ul> <li>An experimental method allowing scientists to study brain or neural function in a living organism.</li> </ul>"},{"location":"KB/In-group%20Bias/","title":"In group Bias","text":""},{"location":"KB/In-group%20Bias/#in-group-bias","title":"In-group Bias","text":"<ul> <li><code>#</code>fairness</li> <li>Showing partiality to one's own group or own characteristics. If testers or raters consist of the machine learning developer's friends, family, or colleagues, then in-group bias may invalidate product testing or the dataset.</li> <li>In-group bias is a form of group attribution bias.</li> </ul>"},{"location":"KB/Inattentional%20Blindness/","title":"Inattentional Blindness","text":""},{"location":"KB/Inattentional%20Blindness/#inattentional-blindness","title":"Inattentional Blindness","text":"<ul> <li>Viewers can fail to perceive visual elements or activities caused by an absence of attention to the unseen object</li> <li>Related to Change Blindness</li> </ul>"},{"location":"KB/Inception/","title":"Inception","text":""},{"location":"KB/Inception/#inception","title":"Inception","text":"<ul> <li>@szegedyGoingDeeperConvolutions2014</li> <li>Rethinking the Inception Architecture for Computer Vision</li> </ul>"},{"location":"KB/Inception/#v1","title":"V1","text":"<ul> <li>Conv at different filter scales to find different kinds of features -&gt; stack them up</li> <li>Increasing both the depth and width of the network while keeping computations at a manageable level</li> <li>Human visual system wherein information is processed at multiple scales and then aggregated locally</li> <li>channel dimensionality reduction, by reducing the output channels of the input</li> <li>To enable concatenation of features convolved with different kernels, they pad the output to make it the same size as the input.<ul> <li>without dilation</li> <li>padding \\(p = (k-1)/2p\\)</li> <li>since \\(out = in +2p -k +1\\)</li> </ul> </li> <li></li> </ul>"},{"location":"KB/Inception/#v2v3","title":"V2/V3","text":"<ul> <li>nxn Conv -&gt; 1xn followed by nx1 Conv</li> <li>5x5, 7x7 -&gt; 2 and three 3x3 seq Conv</li> <li>More filters (wider)</li> <li>Distributed the computational budget in a balanced way between the depth and width of the network</li> <li>Added Batch Normalization</li> <li></li> </ul>"},{"location":"KB/Inception/#v4","title":"V4","text":"<ul> <li>(from) Paper:\u00a0https://arxiv.org/pdf/1602.07261.pdf Year: 2016 Summary: New Residual\u00a0Inception\u00a0Architecture (deep\u00a0CNN)</li> <li>Why?<ul> <li>Introduction of residual connections on traditional architectures yielded SOTA performance (2015)</li> </ul> </li> <li>Research question<ul> <li>Are there benefits when combining residual connections with the\u00a0Inception\u00a0architecture?</li> </ul> </li> <li>Findings<ul> <li>Residual connections accelerates training of the\u00a0Inception\u00a0architecture</li> <li>Residual\u00a0Inception\u00a0outperforming similar architecture only close</li> <li>When the number of filters were higher than\u00a01\u2032000\u00a0the residual variants of the network died early in the training (e.g. outputted only zeros)<ul> <li>was not able to fix with lowering the\u00a0Learning Rate\u00a0or\u00a0Batch Normalization</li> </ul> </li> <li>Scaling down the residuals before adding them with the residual connection stabilized the training (factor:\u00a00.1\u22120.3)</li> </ul> </li> <li>General Ideas<ul> <li>Parallel convolutions: Similar to the GoogLeNet architecture within their\u00a0modules\u00a0the authors simultaneously use multiple convolutional branches with different receptive field sizes on the same input activation maps and again concatenate those activations for further processing.</li> <li>Reduction modules: Instead of simply applying a single max pooling or a 2-stride convolution to downsize the spatial dimensions, the authors dedicated whole modules to this task again employing parallel branches.\u00a0</li> <li>Strong usage of small convolutional kernels(e.g.\u00a03\u00d73): Throughout the network the authors pefer smaller convolutional kernel size over larger ones, as this enables the same receptive field with less parameters (e.g. a single\u00a05\u00d75convolution \u223c25\u00a0params  as 2 consecutive\u00a03\u00d73convolutions [\u223c18\u00a0params ], but the later has less parameters)</li> <li>Factorization of convolutions: They factorize convolutions of filter size\u00a0n\u00d7n\u00a0to a combination of\u00a01\u00d7n\u00a0and\u00a0n\u00d71\u00a0convolutions, in order to reduce the nr of parameters even further (e.g.\u00a07\u00d77\u00a0[\u223c49\u00a0params ] results in\u00a01\u00d77\u00a0and\u00a07\u00d71\u00a0[\u223c14\u00a0params ]!)</li> <li>Residual connections: In the\u00a0<code>Inception-ResNet-v1</code>\u00a0and\u00a0<code>Inception-ResNet-v2</code>\u00a0the authors employ the usage of residual connections. Although the residual version of the networks converge faster, the final accuracy seems to mainly depend on the model size.</li> <li>Usage of bottleneck layers: In order to reduce the cost of the individual convolutional branches within their modules, they apply\u00a01\u00d71convolutions at the beginning to reduce the depth of the input activation maps.</li> </ul> </li> <li>Remarks<ul> <li>Authors disagree with residual paper one some points\u00a0<ul> <li>Residual connections are nessecary for training deep convolutional models<ul> <li>They show that it is not hard to train very deep models which achieve high performance without residual connections</li> <li>They argue that residual connections do only speed up the training greatly</li> </ul> </li> <li>\"Warm up\" phases (pre-training with very low LR followed by a high LR) do not help to stabilize training very deep networks<ul> <li>high LR had the chance to destroy already learnt features</li> <li>Scaling should be used instead</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"KB/Inceptionism/","title":"Inceptionism","text":""},{"location":"KB/Inceptionism/#inceptionism","title":"Inceptionism","text":"<ul> <li>Google AI Blog: Inceptionism: Going Deeper into Neural Networks #Roam-Highlights</li> <li>Le Net</li> <li></li> <li></li> <li></li> <li></li> <li></li> <li></li> <li></li> <li>One of the challenges of neural networks is understanding what exactly goes on at each layer</li> <li>We know that after training, each layer progressively extracts higher and higher-level features of the image, until the final layer essentially makes a decision on what the image shows</li> <li>For example, the first layer maybe looks for edges or corners</li> <li>Intermediate layers interpret the basic features to look for overall shapes or components, like a door or a leaf</li> <li>The final few layers assemble those into complete interpretations\u2014these neurons activate in response to very complex things such as entire buildings or trees.</li> <li>One way to visualize what goes on is to turn the network upside down and ask it to enhance an input image in such a way as to elicit a particular interpretation.</li> <li>Why is this important? Well, we train networks by simply showing them many examples of what we want them to learn, hoping they extract the essence of the matter at hand (e.g., a fork needs a handle and 2-4 tines), and learn to ignore what doesn\u2019t matter (a fork can be any shape, size, color or orientation)</li> <li>But how do you check that the network has correctly learned the right features? It can help to visualize the network\u2019s representation of a fork.</li> <li>Indeed, in some cases, this reveals that the neural net isn\u2019t quite looking for the thing we thought it was.</li> <li>Instead of exactly prescribing which feature we want the network to amplify, we can also let the network make that decision</li> <li>In this case we simply feed the network an arbitrary image or photo and let the network analyze the picture</li> <li>We then pick a layer and ask the network to enhance whatever it detected.</li> <li>If we choose higher-level layers, which identify more sophisticated features in images, complex features or even whole objects tend to emerge</li> <li>Again, we just start with an existing image and give it to our neural net.</li> <li>We ask the network: \u201cWhatever you see there, I want more of it!\u201d This creates a KB/Feedback Loop.md: if a cloud looks a little bit like a bird, the network will make it look more like a bird</li> <li>This in turn will make the network recognize the bird even more strongly on the next pass and so forth, until a highly detailed bird appears, seemingly out of nowhere.</li> <li>The results are intriguing\u2014even a relatively simple neural network can be used to over-interpret an image, just like as children we enjoyed watching clouds and interpreting the random shapes</li> <li>This network was trained mostly on images of animals, so naturally it tends to interpret shapes as animals</li> <li>But because the data is stored at such a high abstraction, the results are an interesting remix of these learned features.</li> <li>Of course, we can do more than cloud watching with this technique</li> <li>We can apply it to any kind of image</li> <li>The results vary quite a bit with the kind of image, because the features that are entered bias the network towards certain interpretations</li> <li>For example, horizon lines tend to get filled with towers and pagodas</li> <li>Rocks and trees turn into buildings</li> <li>Birds and insects appear in images of leaves.</li> <li>We must go deeper: Iterations If we apply the algorithm iteratively on its own outputs and apply some zooming after each iteration, we get an endless stream of new impressions, exploring the set of things the network knows about</li> <li>We can even start this process from a random-noise image, so that the result becomes purely the result of the neural network, as seen in the following images:</li> <li>The techniques presented here help us understand and visualize how neural networks are able to carry out difficult classification tasks, improve network architecture, and check what the network has learned during training</li> <li>It also makes us wonder whether neural networks could become a tool for artists\u2014a new way to remix visual concepts\u2014or perhaps even shed a little light on the roots of the creative process in general.</li> </ul>"},{"location":"KB/Independence/","title":"Independence","text":""},{"location":"KB/Independence/#independence","title":"Independence","text":"<ul> <li>model predictions are independent of the sensitive feature.</li> <li>the proportion of positive samples (namely, those ones belonging to the class of interest) given by the model is the same for all the subgroups within the sensitive feature</li> </ul>"},{"location":"KB/Indirect%20Volume%20Visualization/","title":"Indirect Volume Visualization","text":""},{"location":"KB/Indirect%20Volume%20Visualization/#indirect-volume-visualization","title":"Indirect Volume Visualization","text":"<ul> <li>Isosurface</li> </ul>"},{"location":"KB/Individual%20Fairness/","title":"Individual Fairness","text":""},{"location":"KB/Individual%20Fairness/#individual-fairness","title":"Individual Fairness","text":"<ul> <li>A fairness metric that checks whether similar individuals are classified similarly. For example, Brobdingnagian Academy might want to satisfy individual fairness by ensuring that two students with identical grades and standardized test scores are equally likely to gain admission.</li> <li> <p>Note that individual fairness relies entirely on how you define \"similarity\" (in this case, grades and test scores), and you can run the risk of introducing new fairness problems if your similarity metric misses important information (such as the rigor of a student\u2019s curriculum).</p> </li> <li> <p>deals with fairness from the perspective of all individual</p> </li> </ul>"},{"location":"KB/Individual%20Modeling/","title":"Individual Modeling","text":""},{"location":"KB/Individual%20Modeling/#individual-modeling","title":"Individual Modeling","text":"<ul> <li>More personalized</li> <li>Less data per category</li> <li>Ramp up problem</li> </ul>"},{"location":"KB/Induced%20Pluripotent%20Stem%20Cell%20%28iPSC%29/","title":"Induced Pluripotent Stem Cell (iPSC)","text":""},{"location":"KB/Induced%20Pluripotent%20Stem%20Cell%20%28iPSC%29/#induced-pluripotent-stem-cell-ipsc","title":"Induced Pluripotent Stem Cell (iPSC)","text":"<ul> <li>A cell that has been taken from adult tissue and genetically modified to behave like an embryonic stem cell, with the ability to develop into any type of cell found in the body, including nerve cells.</li> </ul>"},{"location":"KB/Inductive%20Bias/","title":"Inductive Bias","text":""},{"location":"KB/Inductive%20Bias/#inductive-bias","title":"Inductive Bias","text":"<ul> <li>Set of assumptions that the learner uses to predict outputs of given inputs that it has not yet encountered</li> <li>In Bayesian<ul> <li>Bayesian Prior can shape the Bayesian Posterior in the way that it can be a similar distribution to the former</li> </ul> </li> <li>In KNN<ul> <li>we assume that similar data points are clustered near each other away from the dissimilar ones</li> </ul> </li> <li>in LinearRegression<ul> <li>we assume that the variable Y is linearly dependent on the explanatory variables X.</li> <li>Therefore, the resulting model linearly fits the training data. However, this assumption can limit the model\u2019s capacity to learn non-linear functions.</li> </ul> </li> <li>in Logistic Regression<ul> <li>assume that there\u2019s a hyperplane that separates the two classes from each other. This simplifies the problem, but one can imagine that if the assumption is not valid, we won\u2019t have a good model.</li> </ul> </li> <li>Non Relational Inductive Bias</li> <li>Relational Inductive Bias</li> </ul>"},{"location":"KB/Inductive%20Learning/","title":"Inductive Learning","text":""},{"location":"KB/Inductive%20Learning/#inductive-learning","title":"Inductive Learning","text":"<ul> <li>Bayesian is inductive learning</li> <li>Learning is identifying which hypothesis set is a concept</li> <li>Hypotheses don't disappear, they just become less likely</li> <li>Learning develops through more experience</li> <li>One challenge of Bayesian learning is that any small subset is consistent with many hypotheses</li> <li>Different hypotheses have different likelihoods based on the examples we are exposed to</li> <li>But in the end we also prefer smaller hypotheses over larger ones: The size principle</li> <li>Simple clustering methods can be used to get the data to automatically create the hypothesis space needed for Bayesian modelling</li> <li>Probabilities of different sets then match with human judgments surprisingly well</li> <li>Clustering based on biology worked worse!</li> <li>Clustering using linguistic co-occurrences with Latent Semantic Analysis also worked worse!</li> <li>Human subject judgements of similarity worked best</li> <li>Suggests some human reasoning relies on probability</li> <li>Bayesian learning can also learn categories</li> <li>Models are capable of making generalizations about the specific objects as well as the appropriate generalizations about categorization (superordinate categories!) in general.</li> <li>Advanced learning means learn constraints on what is a possible hypothesis</li> <li>Hierarchical Bayesian Modelling (HBM) can explain how we acquire overhypotheses</li> <li>using observations from the lowest level (data) and calculating statistical inferences</li> </ul>"},{"location":"KB/Inductive%20Sensor/","title":"Inductive Sensor","text":""},{"location":"KB/Inductive%20Sensor/#inductive-sensor","title":"Inductive Sensor","text":"<ul> <li>The class of proximity sensors, which has half of a ferrite core, whose coil is part of an oscillator circuit. When a metallic object enters this field, at some point, the object will absorb enough energy from the field to cause the oscillator to stop oscillating. This signifies that an object is present in a given proximity.</li> </ul>"},{"location":"KB/Inertia/","title":"Inertia","text":""},{"location":"KB/Inertia/#inertia","title":"Inertia","text":"<ul> <li>\"A body either remains at rest or continues to move at a constant velocity, unless acted upon by a net external force.\"</li> </ul>"},{"location":"KB/Inference%20Path/","title":"Inference Path","text":""},{"location":"KB/Inference%20Path/#inference-path","title":"Inference Path","text":"<ul> <li>In a decision tree, during inference, the route a particular example takes from the root to other conditions, terminating with a leaf.</li> </ul>"},{"location":"KB/Infix/","title":"Infix","text":""},{"location":"KB/Infix/#infix","title":"Infix","text":"<ul> <li>inserted inside the stem</li> </ul>"},{"location":"KB/Inflectional%20Morphology/","title":"Inflectional Morphology","text":""},{"location":"KB/Inflectional%20Morphology/#inflectional-morphology","title":"Inflectional Morphology","text":"<ul> <li>creates the new forms of the same word</li> <li>e.g.bring, brought, brings</li> </ul>"},{"location":"KB/Inflectional%20words/","title":"Inflectional words","text":""},{"location":"KB/Inflectional%20words/#inflectional-words","title":"Inflectional Words","text":"<ul> <li>boundaries unclear, can express more than one grammar meaning</li> </ul>"},{"location":"KB/Influence%20of%20image%20classification%20accuracy%20on%20saliency%20map%20estimation/","title":"Influence of image classification accuracy on saliency map estimation","text":""},{"location":"KB/Influence%20of%20image%20classification%20accuracy%20on%20saliency%20map%20estimation/#influence-of-image-classification-accuracy-on-saliency-map-estimation","title":"Influence of Image Classification Accuracy on Saliency Map Estimation","text":"<ul> <li>@oyamaInfluenceImageClassification2018.</li> </ul>"},{"location":"KB/Influence%20of%20image%20classification%20accuracy%20on%20saliency%20map%20estimation/#intro","title":"Intro","text":"<ul> <li>Saliency map estimation in computer vision aims to estimate the locations where people gaze in images.</li> <li>Since people tend to look at objects in images, the parameters of the model pre-trained on ImageNet for image classification are useful for the saliency map estimation</li> <li>no research on the relationship between the image classification accuracy and the performance of the saliency map estimation</li> <li>strong correlation between image classification accuracy and saliency map estimation accuracy</li> <li>It the models pre-trained on ImageNet are useful for saliency map estimation the parameters of is known that</li> <li>This would be because a human tends to look at the centres of objects , which are learned to be recognised in the pre-trained model for the ImageNet classification task.</li> <li>Although the model based on DenseNet has achieved the state-of-the-art performance in the ACPR 2017 paper, this additional study led to even better performance using the model based on dual path networks (DPNs)</li> </ul>"},{"location":"KB/Influence%20of%20image%20classification%20accuracy%20on%20saliency%20map%20estimation/#related-work","title":"Related Work","text":"<ul> <li>uses the coefficients of Attention based on Information the basis Maximization (AIM) calculated by the independent component analysis (ICA) in local image patches</li> <li>The distribution of the coefficients is estimated by the kernel density estimation, which is used for estimating saliency maps based on the the local patches self-information of</li> <li>Graph-based visual saliency</li> <li>Saliency using natural statistics</li> <li>Dynamic visual attention</li> <li>Adaptive Whitening Saliency</li> <li>SAM-ResNet</li> </ul>"},{"location":"KB/Influence%20of%20image%20classification%20accuracy%20on%20saliency%20map%20estimation/#components-of-readout-net","title":"Components of Readout Net","text":"<ul> <li>The operation attempts to directly minimise the reconstruction error of the input image under a KB/Sparsity.md constraint on an over-complete set of feature maps</li> <li>The mini-batch size and learning rate were set to 1 and 10\u22125 during training, respectively.</li> <li>We subtract the per-channels mean value of training images from each image as pre-processing</li> </ul>"},{"location":"KB/Influence%20of%20image%20classification%20accuracy%20on%20saliency%20map%20estimation/#dc","title":"DC","text":"<ul> <li>\"DC is also called as transposed convolution\"</li> <li>When DC is used for the up-sampling layers in Readout Net, the first, second, and third DC followed by a ReLU layer in Readout Net reduce the channels to 128, 64, and 32, respectively. Then, the 1 \u00d7 1 convolution reduces the channel to 1 to predict the saliency map. The filter size of DC was set to 4 \u00d7 4.</li> <li>method to recover a high-resolution image from its additional low resolution counterpart with little computational cost, by rearranging the data along the channel into feature maps with a convolution operation.</li> </ul>"},{"location":"KB/Influence%20of%20image%20classification%20accuracy%20on%20saliency%20map%20estimation/#spc","title":"SPC","text":"<ul> <li>When SPC is used for the up-sampling layers, each SPC layer reduces the channels to one forth, followed by a 3 \u00d7 3 convolution and a ReLU layer. Then, the 1 \u00d7 1 convolution predicts the saliency map from the output of the last SPC layer.</li> <li>each BI layer in the up-sampling network resizes a feature map twice while maintaining the feature-map channels, GPU was out of memory when three up-sampling layers were used for all channels of outputs of Main Net</li> </ul>"},{"location":"KB/Influence%20of%20image%20classification%20accuracy%20on%20saliency%20map%20estimation/#bi","title":"BI","text":"<ul> <li>the order of up-sampling and projection (1 \u00d7 1 convolution) can be inverted without any influence on the output</li> <li>the concatenated feature maps from Main Nets are first processed by the 1 \u00d7 1 convolution to output feature map, followed by the BI up-sampling layers. the 1-channel</li> </ul>"},{"location":"KB/Influence%20of%20image%20classification%20accuracy%20on%20saliency%20map%20estimation/#datasets","title":"Datasets","text":"<ul> <li>Salicon dataset</li> <li>OSIE</li> <li>PASCAL-S</li> <li>MIT1003</li> <li>MIT300</li> </ul>"},{"location":"KB/Influence%20of%20image%20classification%20accuracy%20on%20saliency%20map%20estimation/#metrics","title":"Metrics","text":"<ul> <li>AUC-Judd</li> <li>AUC-Borji</li> <li>Shuffled-AUC</li> </ul>"},{"location":"KB/Influence%20of%20image%20classification%20accuracy%20on%20saliency%20map%20estimation/#conclusions","title":"Conclusions","text":"<ul> <li>strong correlation between image classification accuracy and saliency map estimation accuracy.</li> <li>not only the architecture but also the initialisation strategy using the weights pre-trained with the ImageNet classification task were important for estimating the saliency maps</li> <li>model which is pre-trained with the ImageNet classification and has achieved high</li> <li>\"for performance on the classification task is also useful the\"</li> <li>\"saliency map estimation task\"</li> <li>human fixations often concentrate on objects in the image, while the model pre-trained on ImageNet can react on many objects in images because ImageNet has a wide variety of object categories.</li> <li>If the model is initialised with random weights and is trained on a fixation dataset with the limited categories of objects for saliency map estimation, to the objects in the training dataset the model would overfit</li> <li>if the model is trained for the image classification task which includes a wide variety of categories, overfitting for the objects in the training dataset would be suppressed owing to a large number of categories.</li> </ul>"},{"location":"KB/Influence%20of%20image%20classification%20accuracy%20on%20saliency%20map%20estimation/#images","title":"Images","text":"<ul> <li>{:height 598, :width 600}</li> <li></li> <li></li> </ul>"},{"location":"KB/Information%20Gain/","title":"Information Gain","text":""},{"location":"KB/Information%20Gain/#information-gain","title":"Information Gain","text":"<ul> <li>In decision forests, the difference between a node's entropy and the weighted (by number of examples) sum of the entropy of its children nodes. A node's entropy is the entropy of the examples in that node.</li> </ul>"},{"location":"KB/Information%20Visualization/","title":"Information Visualization","text":""},{"location":"KB/Information%20Visualization/#information-visualization","title":"Information Visualization","text":"<ul> <li>Visualization of abstract data</li> <li>Visual mappings often have to be learned</li> <li>spatial layout is chosen</li> <li>Perception</li> <li>Visual Encoding</li> </ul>"},{"location":"KB/Informativeness/","title":"Informativeness","text":""},{"location":"KB/Informativeness/#informativeness","title":"Informativeness","text":"<ul> <li>ML models are used with the ultimate intention of supporting decision making</li> <li>should not be forgotten that the problem being solved by the model is not equal to that being faced by its human counterpart</li> <li>great deal of information is needed in order to be able to relate the user's decision to the solution given by the model, and to avoid falling in misconception pitfalls.</li> <li>explainable ML models should give information about the problem being tackled.</li> </ul>"},{"location":"KB/Inhibitory%20Control%20Network/","title":"Inhibitory Control Network","text":"<p>toc: true title: Inhibitory Control Network</p> <p>categories: ['temp']</p>"},{"location":"KB/Inhibitory%20Control%20Network/#inhibitory-control-network","title":"Inhibitory Control Network","text":"<ul> <li>Brain areas related to response inhibition ability</li> <li>inferior Frontal Gyri and Medial Frontal Gyri, the Opercular Cingulate, Insular Cingulate, Orbital Posterior Cingulate and Posterior Parietal Cortex</li> </ul>"},{"location":"KB/Initialization/","title":"Initialization","text":""},{"location":"KB/Initialization/#initialization","title":"Initialization","text":"<ul> <li>Xavier Initialization , He Initialization , LeCun Init</li> </ul>"},{"location":"KB/Instance%20Normalization/","title":"Instance Normalization","text":""},{"location":"KB/Instance%20Normalization/#instance-normalization","title":"Instance Normalization","text":"<ul> <li>Contrast Normalization</li> <li> \\[     y_{tijk} = \\frac{x_{tijk} - \\mu_{ti}}{\\sqrt{\\sigma_{ti}^2 + \\epsilon}},     \\quad     \\mu_{ti} = \\frac{1}{HW}\\sum_{l=1}^W \\sum_{m=1}^H x_{tilm},     \\quad     \\sigma_{ti}^2 = \\frac{1}{HW}\\sum_{l=1}^W \\sum_{m=1}^H (x_{tilm} - mu_{ti})^2 \\] </li> <li>This prevents instance-specific mean and KB/Covariance.md shift simplifying the learning process.</li> <li>Intuitively, the normalization process allows to remove instance-specific contrast information from the content image in a task like image stylization, which simplifies generation.</li> <li></li> </ul>"},{"location":"KB/Instance-based%20Learning/","title":"Instance-based Learning","text":""},{"location":"KB/Instance-based%20Learning/#instance-based-learning","title":"Instance-based Learning","text":"<ul> <li>an object category is represented by a set of known instances a nearest neighbor classifier is used</li> <li>IBL considers category learning as a process of learning about the instances of the category:</li> <li>The training phase is very fast</li> <li>IBL can recognize objects using a very small number of experiences IBL is a baseline approach to evaluate object representations Simple and easy to implement</li> <li>Memory usage in instance-based systems is continuously growing. Computational complexity grows with the number of training instances</li> <li>The computational complexity of classifying a single new instance is O(n), where n is number of instances stored in perceptual memory.</li> <li>Salience and forgetting mechanisms can be used to bound the memory usage which are also useful for reducing the risk of overfitting to noise in the training set.</li> <li>Overfitting</li> <li>Sensitive to noise</li> <li></li> </ul>"},{"location":"KB/Instant%20NeRF/","title":"Instant NeRF","text":""},{"location":"KB/Instant%20NeRF/#instant-nerf","title":"Instant NeRF","text":"<ul> <li>Instant Neural Graphics Primitives with a Multiresolution Hash Encoding</li> <li>Neural Radiance Field</li> <li>Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate</li> <li>rely on task specific data structures</li> <li>new input encoding that permits the use of a smaller network without sacrificing quality</li> <li>educing the number of floating point and memory access operations</li> <li>near-instant training of neural graphics primitives on a single GPU for multiple tasks</li> <li>small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through Gradient Descent gradients</li> <li>automatically focuses on relevant detail, independent of task at hand</li> <li>low overhead</li> <li>In a gigapixel image, they represent an image by a neural network. SDF learns a signed distance function in 3D space whose zero level-set represents a 2D surface</li> <li>2D images and their camera poses to reconstruct a volumetric radiance-and-density field that is visualized using ray marching.</li> <li>neural volume learns a denoised radiance and density field directly from a volumetric path tracer.</li> <li>only vary the hash table size which trades off quality and performance</li> <li>disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern GPUs</li> <li>parallelism</li> <li>fully-fused Operator Fusion CUDA kernels with a focus on minimizing wasted bandwidth and compute operations</li> </ul>"},{"location":"KB/Instruction%20Bandwidth/","title":"Instruction Bandwidth","text":""},{"location":"KB/Instruction%20Bandwidth/#instruction-bandwidth","title":"Instruction Bandwidth","text":"<ul> <li>Bandwidth is the maximum amount of data that can travel through a 'channel'</li> </ul>"},{"location":"KB/Instruction%20Cycle/","title":"Instruction Cycle","text":""},{"location":"KB/Instruction%20Cycle/#instruction-cycle","title":"Instruction Cycle","text":"<ul> <li>The time it takes for a robot controller system's cycle to decode a command or instruction before it is executed. The Instruction Cycle must be analyzed very closely by robotic programmers to enable speedy and proper reaction to varying commands.</li> </ul>"},{"location":"KB/Instruction%20Latency/","title":"Instruction Latency","text":""},{"location":"KB/Instruction%20Latency/#instruction-latency","title":"Instruction Latency","text":"<ul> <li>Amount of time to complete a task(time , seconds)</li> <li>function of how long it takes the data to get sent all the way from the start point to the end</li> </ul>"},{"location":"KB/Instruction%20Pipelining/","title":"Instruction Pipelining","text":""},{"location":"KB/Instruction%20Pipelining/#instruction-pipelining","title":"Instruction Pipelining","text":"<ul> <li>used in the design of modern microprocessors, microcontrollers and CPUs to increase their Instruction Throughput for the entire workload</li> <li>divide the processing of a CPU instruction into a series of independent steps o microinstructions with storage at the end of each step.</li> <li>This allows the CPUs control logic to handle instructions at the processing rate of the slowest step, which is much faster than the time needed to process the instruction as a single step</li> <li>IF: Instruction Fetch</li> <li>ID: Instruction Decode, register fetch</li> <li>EX: Execution</li> <li>MEM: Memory Access</li> <li>WB: Register write Back</li> </ul>"},{"location":"KB/Instruction%20Throughput/","title":"Instruction Throughput","text":""},{"location":"KB/Instruction%20Throughput/#instruction-throughput","title":"Instruction Throughput","text":"<ul> <li>the number of instructions that can be executed in a unit of time</li> <li>(Jobs/Hour)</li> <li>how much data actually does travel through the 'channel' successfully</li> </ul>"},{"location":"KB/Instruction%20level%20programming/","title":"Instruction level programming","text":""},{"location":"KB/Instruction%20level%20programming/#instruction-level-programming","title":"Instruction Level Programming","text":"<ul> <li>ILP</li> <li>allows the compiler and the processor to overlap the execution of multiple instructions or even to change the order in which instructions are executed</li> <li>Instruction Pipelining</li> <li>SuperScalar</li> <li>Out of Order Execution</li> <li>Register Renaming</li> <li>Speculative Execution</li> <li>Branch Prediction</li> </ul>"},{"location":"KB/Insula/","title":"Insula","text":""},{"location":"KB/Insula/#insula","title":"Insula","text":"<ul> <li>Sometimes referred to as the insular cortex, this small region of the cerebrum is found deep within the lateral sulcus, and is believed to be involved in consciousness, emotion, and keeping the body in balance.</li> </ul>"},{"location":"KB/Integral%20Lines/","title":"Integral Lines","text":""},{"location":"KB/Integral%20Lines/#integral-lines","title":"Integral Lines","text":"<ul> <li>Seed particles in a flow field  </li> <li>Let the particles move in the flow field</li> <li>Compute and show the trajectories</li> <li></li> <li></li> <li>Streamlines</li> <li>Pathlines</li> </ul>"},{"location":"KB/Integrated%20Gradients/","title":"Integrated Gradients","text":""},{"location":"KB/Integrated%20Gradients/#integrated-gradients","title":"Integrated Gradients","text":"<ul> <li>@sundararajanAxiomaticAttributionDeep2017</li> </ul>"},{"location":"KB/Integrated%20Gradients/#terms","title":"Terms","text":"<ul> <li>Gradient Sensitivity</li> <li>Implementation Invariance</li> </ul>"},{"location":"KB/Integrated%20Gradients/#calculation","title":"Calculation","text":"<ul> <li>a function F representing our model</li> <li>input \\(x \\in \\mathbb{R}^{n}\\) because this is a general definition of IG and not CNN specific),</li> <li>baseline \\(x' \\in \\mathbb{R}^{n}\\)</li> <li>We assume a straight line path between x and x' and compute gradients along that path</li> <li>The integrated gradient along \\(i^{th}\\) dimension is defined as:</li> <li> \\[IntegratedGrads_i\u200b(x)::=(x_i\u200b-x_i'\u200b)\\times \\int_{\\alpha=0}^{1}\\frac{\\partial F(x' + \\alpha \\times (x-x'))}{\\partial x_{i}}d \\alpha\\] </li> <li>The original definition of Integrated Gradients is incalculable (because of the integral).</li> <li>Therefore, the implementation of the method uses approximated value by replacing the integral with the summation:</li> <li> \\[IntegratedGrads_i\u200b^{approx}(x)::=(x_i\u200b-x_i'\u200b)\\times \\Sigma_{k=1}^{m}\\frac{\\partial F(x' + \\frac{k}{m} \\times (x-x'))}{\\partial x_{i}} \\times \\frac{1}{m}\\] </li> <li>In the approximated calculation (Eq. 2), m defines a number of interpolation steps.</li> </ul>"},{"location":"KB/Integrated%20Gradients/#explanation-chatgpt","title":"Explanation (chatgpt)","text":"<ul> <li>In IntegratedGrads, the goal is to understand the contribution of each pixel in the input image towards the model's prediction. Specifically, for a given pixel \\(i\\), IntegratedGrads computes the partial derivative of the model output with respect to that pixel and integrates it along a path from a baseline image to the input image, weighting each step of the path by the partial derivative.</li> <li>The equation you provided is an approximation of IntegratedGrads, which involves dividing the path into \\(m\\) equally spaced steps and approximating the integral using a Riemann sum. Let's break down the equation using an example of an image of a bird:</li> <li>\\(IntegratedGrads_i^{approx}(x)\\) represents the attribution of pixel \\(i\\) towards the model's prediction, using the IntegratedGrads approximation.</li> <li>\\((x_i-x_i')\\) is the difference between the input image pixel \\(i\\) and the baseline pixel \\(i'\\).</li> <li>\\(\\Sigma_{k=1}^{m}\\) is a sum over the \\(m\\) steps of the path from the baseline image to the input image.</li> <li>\\(\\frac{\\partial F(x' + \\frac{k}{m} \\times (x-x'))}{\\partial x_i}\\) is the partial derivative of the model output \\(F\\) with respect to pixel \\(i\\) at the \\(k\\)-th step of the path, where \\(x' + \\frac{k}{m} \\times (x-x')\\) is the image at that step.</li> <li>\\(\\frac{1}{m}\\) is a weighting factor that ensures that each step of the path is given equal importance in the approximation.</li> <li>For example, let's say we have an image of a bird and we want to understand the contribution of the pixel at position \\((10, 20)\\) towards the model's prediction that the image is a bird. We set the baseline pixel value to be zero and divide the path into 10 steps. We compute the partial derivative of the model output with respect to pixel \\((10, 20)\\) at each step of the path and weight each step by \\(\\frac{1}{10}\\) to obtain the approximation of IntegratedGrads for pixel \\((10, 20)\\). The resulting attribution value will give us an indication of how important that pixel is in the model's prediction.</li> </ul>"},{"location":"KB/Integrated%20Gradients/#baselines","title":"Baselines","text":"<ul> <li>replacing the constant color baseline with an alternative</li> <li>Gaussian Baseline</li> <li>Blur Baseline</li> <li>Visualizing the Impact of Feature Attribution Baselines</li> </ul>"},{"location":"KB/Inter-rater%20Agreement/","title":"Inter rater Agreement","text":""},{"location":"KB/Inter-rater%20Agreement/#inter-rater-agreement","title":"Inter-rater Agreement","text":"<ul> <li>A measurement of how often human raters agree when doing a task. If raters disagree, the task instructions may need to be improved. Also sometimes called inter-annotator agreement or inter-rater reliability.</li> </ul>"},{"location":"KB/Interactivity/","title":"Interactivity","text":""},{"location":"KB/Interactivity/#interactivity","title":"Interactivity","text":"<ul> <li>the ability of a model to be interactive with the user as one of the goals targeted by an explainable ML model.</li> <li>ability to tweak and interact with the models is what ensures success</li> </ul>"},{"location":"KB/Interneuron/","title":"Interneuron","text":""},{"location":"KB/Interneuron/#interneuron","title":"Interneuron","text":"<ul> <li>Association neuron</li> <li>impulse moves between sensory and motor neurons</li> <li>mostly multipolar</li> </ul>"},{"location":"KB/Interpolation/","title":"Interpolation","text":""},{"location":"KB/Interpolation/#interpolation","title":"Interpolation","text":"<ul> <li>1D piecewise linear interpolation</li> <li>Bilinear Interpolation</li> <li>Barycentric Interpolation</li> </ul>"},{"location":"KB/Interpretability%20and%20Explainability%20A%20Machine%20Learning%20Zoo%20Mini-tour/","title":"Interpretability and Explainability A Machine Learning Zoo Mini-tour","text":""},{"location":"KB/Interpretability%20and%20Explainability%20A%20Machine%20Learning%20Zoo%20Mini-tour/#interpretability-and-explainability-a-machine-learning-zoo-mini-tour","title":"Interpretability and Explainability A Machine Learning Zoo Mini-tour","text":"<p>@marcinkevicsInterpretabilityExplainabilityMachine2023</p>"},{"location":"KB/Interpretability%20vs%20Neuroscience/","title":"Interpretability vs Neuroscience","text":""},{"location":"KB/Interpretability%20vs%20Neuroscience/#interpretability-vs-neuroscience","title":"Interpretability Vs Neuroscience","text":"<ul> <li>Interpretability vs Neuroscience (rough note) -- colah's blog%20--%20colah's%20blog)%20--%20colah's%20blog)</li> </ul>"},{"location":"KB/Interpretability%20vs%20Neuroscience/#you-can-get-the-responses-of-all-neurons-for-arbitrarily-many-stimuli","title":"You Can Get the Responses of All Neurons for Arbitrarily Many Stimuli","text":"<ul> <li>In neuroscience, one is limited in the number of neurons they can record from, their ability to select the neurons they record, and the number of stimuli they can record responses to.</li> <li>For artificial neural networks, we can record the responses of all neurons to arbitrarily many stimuli</li> <li>Turn arounds are much faster than biological experiments</li> <li>There's no recording noise, No synaptic fatigue.</li> </ul>"},{"location":"KB/Interpretability%20vs%20Neuroscience/#not-only-do-you-have-the-connectome-you-have-the-weights","title":"Not Only Do You Have the Connectome, You Have the Weights!","text":"<ul> <li>A major undertaking in neuroscience is the attempt to access the connectome</li> <li>Even if they succeed, they won\u2019t know the weights of those connections</li> <li>With artificial neural networks, all the connections and weights are simply there for us to look at.</li> <li>And since we also know how these artificial neurons are computed, in principle we have everything we need to just reason through and understand the neural network.</li> </ul>"},{"location":"KB/Interpretability%20vs%20Neuroscience/#weight-tying-massively-reduces-the-number-of-unique-neurons","title":"Weight-tying Massively Reduces the Number of Unique Neurons!","text":"<ul> <li>weight-tying,</li> <li>force many neurons to have the same weights</li> <li>he most common use of this is in convolutional neural networks, where each neuron has translated copies of itself with the same weights.</li> <li>in ImageNet conv nets, weight-tying often reduces the number of unique neurons in early vision by 10,000x or even more</li> <li>This results in artificial neural networks having many fewer neurons for early vision than their biological counterparts</li> <li>This means we can just literally study every single neuron.</li> </ul>"},{"location":"KB/Interpretability%20vs%20Neuroscience/#establishing-causality-by-optimizing-the-input","title":"Establishing Causality by Optimizing the Input","text":"<ul> <li>one of the thorniest issues in understanding neurons in artificial networks is separating correlation from causation.</li> <li>Does a neuron detect a dog head? Or does it just detect part of a dog head?</li> <li>There's a second very closely related problem: we don't know what the space of likely functions a neuron might perform is.</li> <li>this is also a challenge in neuroscience.</li> <li>We create stimuli \u201cfrom scratch\u201d to strongly activate neurons (or combinations of neurons) in artificial neural networks, by starting with random noise and optimizing the input.</li> <li>The key property of feature visualization is that anything in the resulting visualization there because it caused the neuron to fire more</li> <li>If feature visualization gives you a fully formed dog head with eyes and ears arranged appropriately, it must be detecting an entire dog head</li> <li>If it just gives an eye, it's probably only (or at least primarily) responding to that.</li> <li>Recent efforts in neuroscience have tried to develop similar methods [], by using an artificial neural network as a proxy for a biological one.</li> <li>unclear they give you the same ability to establish a causal link.</li> <li>It seems hard to exclude the possibility that the resulting stimulus might have content which causes the artificial neurons predicting the biological neuron to fire more, but aren't causally necessary for the biological neuron to fire.</li> </ul>"},{"location":"KB/Interpretability%20vs%20Neuroscience/#interventions-ablations-and-edits","title":"Interventions, Ablations, and Edits","text":"<ul> <li>Optogenetics has been a major methodological advance for neuroscience in allowing neuroscientists to temporarily ablate neurons, or to force them to activate.</li> <li>Artificial neural networks are trivial to manipulate at the level of neurons</li> <li>One can easily ablate neurons or set them to particular activations</li> <li>But one can also do more powerful \"circuit editing\" where one modifies parameters at a finer grained level.</li> <li>In image generation, Bau et al., 2018 show that you can ablate neurons to remove objects like tress and windows from generated images</li> <li>In RL, Hilton et al., 2020 show that you can ablate features to blind an agent to a particular enemy while leaving other competencies in tact</li> <li>More recently, Cammarata et al, 2021 reimplements a large chunk of neural network from scratch, and then splices it into a model.</li> </ul>"},{"location":"KB/Interpretability%20vs%20Neuroscience/#we-can-study-the-exact-same-model","title":"We Can Study the Exact Same Model.","text":"<ul> <li>Neuroscientists might study a model organism species, but each brain they study has different neurons</li> <li>If one neuroscientist reports on an interesting neuron they found, other neuroscientists can't directly study that same neuron</li> <li>In fact, the neuroscientists studying the original neuron will quickly lose access to it: probes can't be left in indefinitely, organisms die, human subjects leave, and even setting that aside neurons change over time.</li> <li>Studying artificial networks, we can collaboratively reverse engineer the same \u201cbrain\", building on each other.</li> <li>we have a shared web of thousands of \"footholds\" into InceptionV1, consisting of neurons we understand fairly well and know the connections between, which makes it massively easier to explore</li> </ul>"},{"location":"KB/Interpretability/","title":"Interpretability","text":""},{"location":"KB/Interpretability/#interpretability","title":"Interpretability","text":"<ul> <li>ability to explain or to provide the meaning in understandable terms to a human</li> </ul>"},{"location":"KB/Interpretation%20of%20Neural%20networks%20is%20fragile/","title":"Interpretation of Neural networks is fragile","text":""},{"location":"KB/Interpretation%20of%20Neural%20networks%20is%20fragile/#interpretation-of-neural-networks-is-fragile","title":"Interpretation of Neural Networks is Fragile","text":"<ul> <li>Ghorbani et al</li> <li>@ghorbaniInterpretationNeuralNetworks2018</li> </ul>"},{"location":"KB/Interpreting%20Attention/","title":"Interpreting Attention","text":""},{"location":"KB/Interpreting%20Attention/#interpreting-attention","title":"Interpreting Attention","text":"<ul> <li>Attention Interpretability Across NLP Tasks</li> <li>empirically prove the hypothesis that attention weights are interpretable and are correlated with feature importance measures</li> <li>n both single and pair sequence tasks, the attention weights in samples with original weights do make sense in general</li> <li>However, in the former case, the attention mechanism learns to give higher weights to tokens relevant to both kinds of sentiment.</li> <li>They show that attention weights in single sequence tasks do not provide a reason for the prediction, which in the case of pairwise tasks, attention do reflect the reasoning behind model output</li> <li>BertViz repo</li> </ul>"},{"location":"KB/Interpretive%20Labor/","title":"Interpretive Labor","text":""},{"location":"KB/Interpretive%20Labor/#interpretive-labor","title":"Interpretive Labor","text":"<ul> <li>There\u2019s a tradeoff between the energy put into explaining an idea, and the energy needed to understand it.</li> <li>On one extreme, the explainer can painstakingly craft a beautiful explanation, leading their audience to understanding without even realizing it could have been difficult</li> <li>On the other extreme, the explainer can do the absolute minimum and abandon their audience to struggle.</li> <li>That is, really outstanding tutorials, reviews, textbooks, and so on.</li> <li>we often have a group of researchers all trying to understand each other</li> <li>Just like before, the cost of explaining stays constant as the group grows, but the cost of understanding increases with each new member</li> <li>At some size, the effort to understand everyone else becomes too much.</li> <li>As a defense mechanism, people specialize, focusing on a narrower area of interest.</li> <li>The maintainable size of the field is controlled by how its members trade off the energy between communicating and understanding.</li> <li>Research debt is the accumulation of missing interpretive labor.</li> <li>It\u2019s extremely natural for young ideas to go through a stage of debt, like early prototypes in engineering.</li> <li>The problem is that we often stop at that point.</li> <li>Young ideas aren\u2019t ending points for us to put in a paper and abandon.</li> <li>When we let things stop there the debt piles up.</li> <li>It becomes harder to understand and build on each other\u2019s work and the field fragments.</li> </ul>"},{"location":"KB/Interview%20Tips/","title":"Interview Tips","text":""},{"location":"KB/Interview%20Tips/#interview-tips","title":"Interview Tips","text":""},{"location":"KB/Interview%20Tips/#intro","title":"Intro","text":"<ul> <li>Name</li> <li>Do some sleuthing about the interviewer</li> </ul>"},{"location":"KB/Interview%20Tips/#introduce","title":"Introduce","text":"<ul> <li>I have always been a creative technologist</li> <li>tinkering, building experiences and products either on my vision or someone elses. Sharing my knowledge, teaching is important too</li> <li>masters, specialization - AI, CV and analytics</li> <li>Artist!</li> </ul>"},{"location":"KB/Interview%20Tips/#what-makes-you-unique","title":"What Makes You Unique?","text":""},{"location":"KB/Interview%20Tips/#introduce-yourself","title":"Introduce Yourself","text":"<ul> <li>hiring managers are context switching to your interview. they probably dont know jack about you. so tell them</li> <li>Who you are professionally rn. One sentence</li> <li>Some sentences about your experience or education. start with recent and then go back in time</li> <li>Why is this the best next step for me. Aka why do I want this job</li> </ul>"},{"location":"KB/Interview%20Tips/#in-x-years","title":"In X Years?","text":"<ul> <li>long term goals - align with the company</li> <li>dont say a higher position</li> <li>how will this position help you grow in a direction you are proud of</li> <li>learning the industry, go to person with X skill, learn more skills, \"mentor others\", bigger projects</li> <li>know my strenghts, new opportunities to learn in the company and stuff</li> </ul>"},{"location":"KB/Interview%20Tips/#why-do-you-want-this-job","title":"Why Do You want This Job?","text":"<ul> <li>the motivation</li> <li>the job, the role, the team</li> </ul>"},{"location":"KB/Interview%20Tips/#what-questions-for-the-interviewer","title":"What Questions for the Interviewer?","text":"<ul> <li>I did read the job description, but from your perspective how would you describe the role?</li> <li>what makes this role available?</li> <li>what kind of additional responsibilites can be gained over time?</li> <li>what can be expected in say the first few months?</li> <li>how do you measure sucess for this role?</li> <li>what have been the biggest challenges for the role?</li> <li>what are some mistakes people have made in this role before?</li> </ul>"},{"location":"KB/Interview%20Tips/#send-a-thank-you-email-in-24-hours","title":"Send a Thank You Email in 24 Hours","text":""},{"location":"KB/Intra%20cluster%20variance/","title":"Intra Cluster Variance","text":""},{"location":"KB/Intra%20cluster%20variance/#intra-cluster-variance","title":"Intra Cluster Variance","text":"<ul> <li>$\\(J = \\Sigma_{j=1}^K \\Sigma_{x \\in S_j} ||x - \\mu_j||^2\\)</li> <li>Measure of how much the points in a given cluster spread</li> </ul>"},{"location":"KB/Intra-Class%20Part%20Swapping/","title":"Intra-Class Part Swapping","text":""},{"location":"KB/Intra-Class%20Part%20Swapping/#intra-class-part-swapping","title":"Intra-Class Part Swapping","text":"<ul> <li>@zhangIntraClassPartSwapping2021</li> <li>replaces most attentive regions of one image by the other</li> <li>Attentive regions are extracted using a classification activation map (CAM), thresholded for the most prominent region</li> <li>The attentive region in the source image is scaled and translated according to the attentive region of the target image for region replacement</li> <li>The label information of the output is similar to the target image as this approach relies on augmenting similar class images.</li> </ul>"},{"location":"KB/Intravenous/","title":"Intravenous","text":""},{"location":"KB/Intravenous/#intravenous","title":"Intravenous","text":"<ul> <li>Administration of medication or fluids by vein</li> </ul>"},{"location":"KB/Intubation/","title":"Intubation","text":""},{"location":"KB/Intubation/#intubation","title":"Intubation","text":"<ul> <li>Medical insertion of a tube into the body, for example, into the throat to assist with breathing</li> </ul>"},{"location":"KB/Intuitive%20Color%20spaces/","title":"Intuitive Color spaces","text":""},{"location":"KB/Intuitive%20Color%20spaces/#intuitive-color-spaces","title":"Intuitive Color Spaces","text":""},{"location":"KB/Invariant%20Distribution/","title":"Invariant Distribution","text":""},{"location":"KB/Invariant%20Distribution/#invariant-distribution","title":"Invariant Distribution","text":"<ul> <li>If g is the PDF. T(x|y) is the PDF of the transition kernel. Homogenous Markov Chain. Then g is PDF of an invariant distribution of T(x|y) if</li> <li> \\[g(x) = \\int_{\\mathbb{R}^{k}}T(x|y)g(y)dy\\] </li> <li>Atleast one invariant distribution</li> <li>Ergodic</li> </ul>"},{"location":"KB/Inverse%20Kinematics/","title":"Inverse Kinematics","text":""},{"location":"KB/Inverse%20Kinematics/#inverse-kinematics","title":"Inverse Kinematics","text":"<ul> <li>which joint movements (q) are needed achieve a particular robot end effector pose (\u03be)  </li> <li>q = \u03ba\u22121(\u03be) : q = {qi,i \u2208 [1,\u2026,n]}</li> </ul>"},{"location":"KB/Inverse%20Reinforcement%20Learning/","title":"Inverse Reinforcement Learning","text":""},{"location":"KB/Inverse%20Reinforcement%20Learning/#inverse-reinforcement-learning","title":"Inverse Reinforcement Learning","text":"<ul> <li>Basically, IRL is about studying from humans.</li> <li>Inverse reinforcement learning is the sphere of studying an agent\u2019s objectives, values, or rewards with the aid of using insights of its behavior.</li> <li>We can fit a reward function with the use of professional demonstrations. Once a reward feature is fitted, we are able to use Policy Gradient, Model-based RL or different RL to locate the ideal policy.</li> <li>For example, we are able to compute the policy gradient with the use of the reward feature as opposed to sampled rewards. With the policy gradient calculated, we optimize the policy closer to the finest rewards gain.</li> <li>As part of the IRL, the task is to collect a set of human-generated driving data and extract an approximation of that human's reward function for the task. Of course, this approximation necessarily relates to a simplified driving model.</li> <li>As Ng and Russell put it, \"the reward function, rather than the guideline, is the most concise, robust, and transferable definition of the task\" because it quantifies how good or bad certain actions are. Once we have the right reward function, the problem is finding the right guideline and can be solved using standard reinforcement learning methods.</li> <li>For our autonomous car example, we would use human driving data to automatically learn the correct functional weights for the reward. Since the task is fully described by the reward function, we don't even need to know the details of human politics as long as we have the right reward function to optimize.</li> </ul>"},{"location":"KB/Inverse%20Square%20Law/","title":"Inverse Square Law","text":""},{"location":"KB/Inverse%20Square%20Law/#inverse-square-law","title":"Inverse Square Law","text":"<ul> <li>for force on a charge in an electric field of another charge: Force is proportional to the product of the charges and inversely proportional to the square of the distance between them</li> <li> \\[F_{E}= \\frac{\\frac{1}{4pe_{0}}Q_{1}Q_{2}}{r^{2}}\\] </li> <li>force on a mass in a gravitational field of another mass: Force is proportional to the product of the masses and inversely proportional to the square of the distance between them</li> <li> \\[F_{G}= -G \\frac{m_{1}m_{2}}{r^{2}}\\] </li> </ul>"},{"location":"KB/Ion%20Channel/","title":"Ion Channel","text":""},{"location":"KB/Ion%20Channel/#ion-channel","title":"Ion Channel","text":"<ul> <li>A pore in the membrane of a neuron that allows ions to pass through, helping to shape action potentials.</li> </ul>"},{"location":"KB/Isolating%20words/","title":"Isolating words","text":""},{"location":"KB/Isolating%20words/#isolating-words","title":"Isolating Words","text":"<ul> <li>words do not divide into smaller units</li> </ul>"},{"location":"KB/Isoline/","title":"Isoline","text":""},{"location":"KB/Isoline/#isoline","title":"Isoline","text":"<ul> <li>2D</li> <li>Contour line</li> <li> \\[\\{(x,y|f(x,y)=c\\}\\] </li> <li>Curve along function has constant value c</li> <li></li> <li>Marching Squares</li> <li></li> <li>No isoline inside cells with same signs</li> <li>only consider cells with different signs</li> <li>access look-up table for respective case</li> </ul>"},{"location":"KB/Isosurface/","title":"Isosurface","text":""},{"location":"KB/Isosurface/#isosurface","title":"Isosurface","text":"<ul> <li>Marching Cubes</li> <li>Marching Tetrahedra</li> <li>Fractional Anisotropy</li> <li></li> </ul>"},{"location":"KB/Isotropic%20Architectures/","title":"Isotropic Architectures","text":""},{"location":"KB/Isotropic%20Architectures/#isotropic-architectures","title":"Isotropic Architectures","text":"<ul> <li>(of an object or substance) having a physical property which has the same value when measured in different directions.</li> <li>And precisely that is what an isotropic architecture is. Isotropic architectures do not produce pyramid shaped data transformations, but rather\u00a0fixed\u00a0ones where data does not change in shape and size</li> <li>In other (simpler) words, when you take a look at the value going through an\u00a0isotropic\u00a0network, it doesn't change in size.</li> <li>Like Transformer , MLP-Mixer</li> </ul>"},{"location":"KB/Issues/","title":"Issues","text":""},{"location":"KB/Issues/#issues","title":"Issues","text":"<ul> <li>Multiple Local Minima</li> <li>Saddle Points</li> <li>Vanishingexploding gradients</li> <li>Image Data</li> <li>Fitting</li> <li>Freedom</li> <li>Bias Variance Dilemma</li> <li>Complex Geometry</li> <li>Lack of information</li> </ul>"},{"location":"KB/Iterative%20Closest%20Point/","title":"Iterative Closest Point","text":""},{"location":"KB/Iterative%20Closest%20Point/#iterative-closest-point","title":"Iterative Closest Point","text":"<ul> <li>Start from initial guess</li> <li>Iterate</li> <li>For each point on M, find closest point on P</li> <li>Find best transform for this correspondence Transform M</li> <li>Good initial guess -&gt; Converges to global minimum</li> <li>The ICP is applicable when we have a relatively good starting point in advance.</li> <li>Otherwise, it will be trapped into the first local minimum and the solution will be useless.</li> <li>Without pose information, ICP-based approaches are unable to recover the proper transformations because of the ambiguity in surface matching.</li> <li></li> </ul>"},{"location":"KB/Jaccard%20Distance/","title":"Jaccard Distance","text":""},{"location":"KB/Jaccard%20Distance/#jaccard-distance","title":"Jaccard Distance","text":"<ul> <li> \\[D(x,y) = 1- \\frac{x\\cup y}{x\\cap y}\\] </li> <li>The Jaccard index (or Intersection over Union) is a metric used to calculate the similarity and diversity of sample sets. It is the size of the intersection divided by the size of the union of the sample sets.</li> <li>In practice, it is the total number of similar entities between sets divided by the total number of entities.</li> <li>To calculate the Jaccard distance we simply subtract the Jaccard index from 1</li> <li>highly influenced by the size of the dat</li> <li>Large datasets can have a big impact on the index as it could significantly increase the union whilst keeping the intersection similar</li> <li>The Jaccard index is often used in applications where binary or binarized data are used</li> <li>deep learning model predicting segments of an image</li> <li>text similarity analysis to measure how much word choice overlap there is between documents</li> </ul>"},{"location":"KB/Jensen%20Shannon%20Divergence%20Consistency%20Loss/","title":"Jensen Shannon Divergence Consistency Loss","text":""},{"location":"KB/Jensen%20Shannon%20Divergence%20Consistency%20Loss/#jensen-shannon-divergence-consistency-loss","title":"Jensen Shannon Divergence Consistency Loss","text":"<ul> <li> <p>@linDivergenceMeasuresBased</p> </li> <li> <p>enforces smother neural network responses</p> </li> <li>stable, consistent and insensitive across range of inputs</li> <li> \\[ \\mathcal{L}(p_{orig}, y)+ \\lambda JS(p_{orig};p_{augmix1}; p_{augmix2}) \\] </li> <li> \\[ M = \\frac{p_{orig}+p_{augmix1}+ p_{augmix2}}{3} \\] </li> <li> \\[ JS(p_{orig}; p_{augmix1};p_{augmix2}) = \\frac{1}{3}(KL[p_{orig}||M||]+KL[p_{augmix1}||M||]+KL[p_{augmix2}||M||]) \\] </li> </ul>"},{"location":"KB/Joint%20Factor%20Analysis/","title":"Joint Factor Analysis","text":""},{"location":"KB/Joint%20Factor%20Analysis/#joint-factor-analysis","title":"Joint Factor Analysis","text":"<ul> <li>Front-end Factor Analysis for Speaker Verification</li> <li>Joint Factor Analysis (JFA)</li> <li>eature extractor to learn a low-dimensional speaker representation for speaker verification, which is also used to model session and channel effects/variabilities</li> <li>In this new space, a given speech utterance is represented by a new vector named total factors (called the identity-vector or the \u201ci-vector\u201d)</li> <li>The i-vector is thus a feature that represents the characteristics of the frame-level features\u2019 distributive pattern</li> <li>KB/Dimensionality Reduction.md of the GMM supervector (although the GMM supervector is not extracted when computing the i-vector)</li> <li>extracted in a similar manner with the eigenvoice adaptation scheme or the JFA technique</li> <li>extracted per sentence</li> <li>Support-Vector-Machine-based system that uses the cosine kernel to estimate the similarity between the input data</li> <li>cosine similarity as the final decision score</li> <li>removed the SVM from the decision proces</li> <li>no speaker enrollment</li> <li>EER</li> <li>MinDCF</li> <li>NIST 2008 Speaker Recognition Evaluation dataset</li> <li>Up until d-vectors, the state-of-the-art speaker verification systems were based on the concept of i-vectors</li> </ul>"},{"location":"KB/Joint%20Interpolated%20Motion/","title":"Joint Interpolated Motion","text":""},{"location":"KB/Joint%20Interpolated%20Motion/#joint-interpolated-motion","title":"Joint Interpolated Motion","text":"<ul> <li>A method of coordinating the movement of the joints, such that all joints arrive at the desired location simultaneously. This method of servo control produces a predictable path regardless of speed and results in the fastest pick and place cycle time for a particular move.</li> </ul>"},{"location":"KB/Joint%20Motion%20Type/","title":"Joint Motion Type","text":""},{"location":"KB/Joint%20Motion%20Type/#joint-motion-type","title":"Joint Motion Type","text":"<ul> <li>Also known as Point-to-Point Motion, Joint Motion Type is a method of path interpolation that commands the movement of the robot by moving each joint directly to the commanded position so that all axis arrive to the position at the same time. Although the path is predictable, it will not be linear.</li> </ul>"},{"location":"KB/Joint%20Space/","title":"Joint Space","text":""},{"location":"KB/Joint%20Space/#joint-space","title":"Joint Space","text":"<ul> <li>a. Joint Space (or Joint Coordinates) is just a method of defining the position of the robot in terms of the value of each axis instead of as a TCP position. For example, the Home Position of a robot is often defined in Joint Space as each axis being at 0 degrees.</li> <li>b. The set of joint positions.</li> </ul>"},{"location":"KB/Joint%20Velocity/","title":"Joint Velocity","text":""},{"location":"KB/Joint%20Velocity/#joint-velocity","title":"Joint Velocity","text":"<ul> <li>KB/Joint Space.md trajectory is generally smoother than task space trajectory</li> <li></li> </ul>"},{"location":"KB/Jukebox/","title":"Jukebox","text":""},{"location":"KB/Jukebox/#jukebox","title":"Jukebox","text":"<ul> <li>generates music with singing in the raw audio domain</li> <li>earlier models in the text-to-music genre generated music symbolically in the form of a pianoroll which specifies timing, pitch and velocity.</li> <li>The challenging aspect is the non-symbolic approach where music is tried to be produced directly as a piece of audio</li> <li>the space of raw audio is extremely high dimensional which makes the problem very challenging</li> <li>the key issue is that modelling that raw audio produces long-range dependencies, making it computationally challenging to learn the high-level semantics of music.</li> <li>hierarchical VQ-VAE architecture to compress audio into a discrete space [14], with a loss function designed to retain the most amount of information.</li> <li>This model produces songs from very diferent genres such as rock, hip-hop and jazz.</li> </ul>"},{"location":"KB/KITTI/","title":"KITTI","text":""},{"location":"KB/KITTI/#kitti","title":"KITTI","text":"<ul> <li>collected from driving a car around a city which equipped with various sensors including high-resolution RGB camera, grayscale stereo camera, a 3D laser scanner, and highprecision GPS measurements and IMU accelerations from a combined GPS/IMU system </li> <li>Videos with various modalities captured by these sensors are available in this dataset.</li> </ul>"},{"location":"KB/KL%20Divergence/","title":"KL Divergence","text":""},{"location":"KB/KL%20Divergence/#kl-divergence","title":"KL Divergence","text":"<ul> <li>Classification</li> <li>Entropy + Cross Entropy</li> <li>Distribution Based metric</li> <li>Measures difference between two PDF</li> <li>We first define xlogx for a weird edge case \\(\\(x \\cdot \\log\\left( x \\right)\\)\\)</li> </ul> <p>Then entropy \\(\\(\\mathrm{sum}\\left( \\mathrm{xlogx}\\left( y \\right) \\right) \\cdot \\mathrm{//}\\left( 1, \\mathrm{size}\\left( y, 2 \\right) \\right)\\)\\)</p> <p>Then cce as defined before $$ - \\mathrm{sum}\\left( y \\cdot \\log\\left( \u0177 \\right) \\right)$$</p> <p>Finally KLD \\(\\(entropy + crossentropyloss\\)\\)</p> <ul> <li> \\[KL(p,q) = \\Sigma_x p(x) log\\frac{p(x)}{q(x)}\\] </li> </ul>"},{"location":"KB/KMeans/","title":"K Means","text":""},{"location":"KB/KMeans/#k-means","title":"K Means","text":"<ul> <li>Codebook vectors. No manifolds.</li> <li>Given: \\(\\((x_i)_{i= 1,..,N} \\in \\mathbb{R}^n\\)\\)</li> <li>Need : K clusters \\(\\(C_1 , \u2026 , C_K\\)\\)</li> <li>Randomly assign training points to K sets : \\(\\(S_j (j = 1, \u2026, K)\\)\\)</li> <li>Repeat:<ul> <li>For each set (\\(S_j\\)\\)<ul> <li>Mean \\(\\(\\mu_j = |S_j|^{-1} \\Sigma_{x \\in S_j} x\\)\\)</li> <li>Create new sets by putting points into set where \\(||x_i-\\mu_j||\\) is minimal</li> <li>If empty, dismiss and reduce K to K'</li> </ul> </li> </ul> </li> <li>Error quantity does not increase<ul> <li>Intra cluster variance</li> <li>Clusters are bounded by line Decision Boundaries and forms a Voronoi Cell</li> </ul> </li> <li>Does not work for curved boundaries</li> </ul>"},{"location":"KB/KMeans/#codebook-vector","title":"Codebook Vector","text":"<ul> <li>Each cluster represented by it</li> <li>Vector pointing to the mean of all vectors in the cluster</li> <li>Center of Gravity</li> </ul>"},{"location":"KB/Kalman%20Filter/","title":"Kalman Filter","text":""},{"location":"KB/Kalman%20Filter/#kalman-filter","title":"Kalman Filter","text":"<ul> <li>The standard Kalman filter is the optimum estimator when your system is linear and the system noise is Gaussian.</li> <li>linear systems with Gaussian noise</li> </ul>"},{"location":"KB/KeepAugment/","title":"KeepAugment","text":""},{"location":"KB/KeepAugment/#keepaugment","title":"KeepAugment","text":"<ul> <li>KeepAugment identifies the salient area in an image and assures the image generated by the augmentation strategies, for example, [Cutout], [RandAugment] [14], [CutMix] [82] or AutoAugment [13], contains salient region in it.</li> <li>standard augmentation may introduce distribution shifts</li> <li>increase fidelity of the augmented data</li> <li>use saliency maps to identify the regions of interest, then make sure those regins are not affected by the augmentation</li> <li>Given image \\(x\\) and label logit value \\(l_{y}\\) , \\(g_{i,j}(x,y)\\) to be the vanilla gradient \\(|\\nabla_{x}l_{y}(s)|\\) </li> <li>For RGB, channel wise maximum to get single saliency value for each pixel \\((i,j)\\)</li> </ul>"},{"location":"KB/KeepAugment/#selective-cut","title":"Selective Cut","text":"<ul> <li>Randomly sample regions \\(S\\) to be cut until importance score \\(\\mathcal{I}(S, x, y)\\) is smaller than threshold \\(\\tau\\)  $$ \\tilde x= (1-M(S)) \\odot x$$</li> <li>where \\(M(S) = |M_{ij}(S)|_{ij}\\) is the binary mask for \\(S\\), \\(M_{ij} = \\mathbb{I}((i,j) \\in S)\\)</li> </ul>"},{"location":"KB/KeepAugment/#selective-paste","title":"Selective Paste","text":"<ul> <li>image level augmented data \\(x' = \\mathcal{A}(x)\\) , uniformly sample region \\(S\\) that satisfies \\(\\mathcal{I}(S,x,y) &gt; \\tau\\) for a threshold \\(\\tau\\) </li> <li>paste the region \\(S\\) of the original image  \\(x\\) to \\(x'\\) <ul> <li> \\[\\tilde x = M(S) \\odot x + (1-M(S)) \\odot x'\\] </li> <li>\\(M_{ij}(S) = \\mathbb{I}((i,j) \\in S)\\) is the binary mask of the region \\(S\\)</li> </ul> </li> <li>we choose our threshold \\(\\tau\\) in an adaptive way.</li> <li>given an image and consider an region size h \u00d7 w of interest, we first calculate the importance scores of all possible candidate regions, following Eq. 1; then we set our threshold to be the \\(\\tau - quantile\\) value of all the importance scores \\(\\mathcal{I}(S,x,y)\\) of all candidate regions. For selective-cut, we uniformly keep sampling a mask region S until its corresponding score \\(\\mathcal{I}(S,x,y)\\) is smaller than the threshold. For selective-paste, we uniformly sample a region S with importance score is greater than the threshold</li> </ul>"},{"location":"KB/KeepAugment/#efficient-implementation-of-keepaugment","title":"Efficient Implementation of KeepAugment","text":""},{"location":"KB/KeepAugment/#low-resolution-based-approximation","title":"Low Resolution Based Approximation","text":"<ul> <li>we proceed as follows: a) for a given image x, we first generate a lowresolution copy and then calculate its saliency map; b) we map the low-resolution saliency maps to their corresponding original resolution</li> <li>This allows us to speed up the saliency maps calculation significantly, e.g., on ImageNet, we achieve roughly 3\u00d7 computation cost reduction by reducing the resolution from 224 to 112.</li> </ul>"},{"location":"KB/KeepAugment/#early-classification-head-based-approximation","title":"Early Classification Head Based Approximation","text":"<ul> <li>In practice, we add an additional average pooling layer and a linear head after the first block of our networks evaluated</li> <li>We achieve about 3\u00d7 computation cost reduction in computing saliency maps</li> </ul>"},{"location":"KB/KeepAugment/#region-level-augmentation","title":"Region-Level Augmentation","text":"<ul> <li>including Cutout [8] and random erasing [45], work by randomly masking out or modifying rectangular regions of the input images</li> <li>This procedure could be conveniently formulated as applying randomly generated binary masks to the original inputs</li> <li>Precisely, consider an input image x of size H \u00d7 W, and a rectangular region S of the image domain. Let M(S) = [Mij (S)]ij be the binary mask of S with Mij (S) = I((i, j) \u2208 S)</li> <li>Then the augmented data can be generated by modifying the image on region S, yielding images of form x\u2032 = (1 \u2212 M(S)) \u2299 x + M(S) \u2299 \u03b4, where \u2299 is element-wise multiplication, and \u03b4 can be either zeros (for Cutout) or random numbers (for random erasing)</li> </ul>"},{"location":"KB/KeepAugment/#image-level-augmentation","title":"Image-Level Augmentation","text":"<ul> <li>Exploiting the invariance properties of natural images, image-level augmentation methods apply label-invariant transformations on the whole image, such as solarization, sharpness, posterization, and color normalization</li> <li>often manually designed and heuristically chosen</li> <li>Recently, AutoAugment [4] applies reinforcement learning to automatically search optimal compositions of transformations</li> <li>Several subsequent works, including RandAugment [5], Fast AutoAugment [18], alleviate the heavy computational burden of searching on the space of transformation policies by designing more compact search spaces</li> </ul>"},{"location":"KB/KeepAugment/#data-augmentation-and-its-trade-offs","title":"Data Augmentation and Its Trade-offs","text":"<ul> <li>Although data augmentation increases the effective size of data, it may inevitably cause loss of information and introduce noise and ambiguity if the augmentation is not controlled properly</li> <li>To study this phenomenon empirically, we plot the train and testing accuracy on CIFAR-10 [16] when we apply Cutout with increasingly large cutout length in Figure 2(a), and RandAugment with increasing distortion magnitude </li> <li>As typically expected, the generalization (the gap between the training and testing accuracy on clean data) improves as the magnitude of the transform increases in both cases</li> <li>However, when the magnitudes of the transform are too large (\u2265 16 for Cutout and \u2265 12 for RandAugment ), the training accuracy (blue line), and hence the testing accuracy (red line), starts to degenerate, indicating that augmented data no longer faithfully represent the clean training data in this case, such that the training loss on augmented data no longer forms a good surrogate of the training loss on the clean data.</li> </ul>"},{"location":"KB/KeepAugment/#images","title":"Images","text":""},{"location":"KB/Kernel%20Filters/","title":"Kernel Filters","text":""},{"location":"KB/Kernel%20Filters/#kernel-filters","title":"Kernel Filters","text":"<ul> <li>sharpen and blur images</li> <li>These filters work by sliding an n \u00d7 n matrix across an image with either a Gaussian blur filter, which will result in a blurrier image, or a high contrast vertical or horizontal edge filter which will result in a sharper image along edges</li> <li>Intuitively, blurring images for Data Augmentation could lead to higher KB/Resistance.md to motion blur during testing</li> <li>Additionally, sharpening images for Data Augmentation could result in encapsulating more details about objects of interest.</li> <li>Kang et al. experiment with a unique kernel filter that randomly swaps the pixel values in an n\u00d7n sliding window. They call this augmentation technique PatchShuffle Regularization</li> </ul>"},{"location":"KB/Kernel%20Support%20Vector%20Machines%20%28KSVMs%29/","title":"Kernel Support Vector Machines (KSVMs)","text":""},{"location":"KB/Kernel%20Support%20Vector%20Machines%20%28KSVMs%29/#kernel-support-vector-machines-ksvms","title":"Kernel Support Vector Machines (KSVMs)","text":"<ul> <li>A classification algorithm that seeks to maximize the margin between positive and negative classes by mapping input data vectors to a higher dimensional space. For example, consider a classification problem in which the input dataset has a hundred features. To maximize the margin between positive and negative classes, a KSVM could internally map those features into a million-dimension space. KSVMs uses a loss function called hinge loss.</li> </ul>"},{"location":"KB/Kernel%20Visualization/","title":"Kernel Visualization","text":""},{"location":"KB/Kernel%20Visualization/#kernel-visualization","title":"# Kernel Visualization","text":"<ul> <li>Qualitatively visualize the kernels of the first convolution layer learned with the pretext tasks and compare the kernels from supervised model </li> <li>similarity of the kernels learned by supervised and selfsupervised models are compared to indicate the effectiveness of self-supervised methods</li> </ul>"},{"location":"KB/Ketamine/","title":"Ketamine","text":""},{"location":"KB/Ketamine/#ketamine","title":"Ketamine","text":"<ul> <li>A powerful anesthetic drug, originally manufactured for veterinary use, that has been shown to be an effective treatment for major depressive disorder, especially in patients who do not respond well to traditional antidepressant medications.</li> </ul>"},{"location":"KB/Kickstart%20AI/","title":"Kickstart AI","text":""},{"location":"KB/Kickstart%20AI/#kickstart-ai","title":"Kickstart AI","text":"<p>It is not every day that you find an AI company that not only wants to make a real-world impact but also cares about responsible AI. I found out about Kickstart.AI and the hackathons from some classmates (who are from my course at the RUG) who interned with you recently. Their project was on \"Using Data to Predict Food Insecurity.\" (Shray Juneja,Chaoyi Wang, and Lonneke Pulles study AI at the RUG as well.) Looking at the projects that Kickstart is involved in, it is obvious just how much talent and hard work is behind each of them. I really want to take on the challenge and contribute to making some awesome ideas a reality while being mindful of the societal impact and explainability of each of the models used.</p> <p>My expertise is a combination of data analytics and computer vision, and as of a month ago, I also have a Master in Artificial Intelligence from the University of Groningen. I am familiar with the tools required for basic and advanced AI and analytics from internships, research projects, papers, freelance work, and many personal projects. I love hackathons, and truly think that they bring out the best in every member of a team. That being said, what I love the most is building prototypes of ideas. Be it an AI-powered credit scoring model, a model that converts between materials (e.g., glass to wood), or applications powered by LLMs. Any idea is fair game. But I also care about how any of the solutions that I am part of affect the end users. My master thesis was on XAI, and I firmly believe that an AI model needs a detailed understanding of biases before production, another aspect that I am familiar with.</p> <p>I have a lot to learn, but as an AI developer, we are always students. It is not every day you find a position that you enjoy and also think you can contribute something to, and I sincerely hope you give me a chance!</p>"},{"location":"KB/Kinesthetic%20Teaching/","title":"Kinesthetic Teaching","text":""},{"location":"KB/Kinesthetic%20Teaching/#kinesthetic-teaching","title":"Kinesthetic Teaching","text":"<ul> <li>H. Kasaei et al., \u201cInteractive open-ended object, affordance and grasp learning for robotic manipulation.\u201d ICRA 2019.</li> <li>Formulate object grasping as a supervised learning problem,  </li> <li>An appropriate grasp configuration can be learned from human demonstrations</li> <li>Primary assumption -&gt; familiar objects can be grasped in a similar way.</li> <li>Heightmaps Kinesthetic</li> <li>Familar Object Grasping Object Viiew recog</li> </ul>"},{"location":"KB/Kinetic%20Energy/","title":"Kinetic Energy","text":""},{"location":"KB/Kinetic%20Energy/#kinetic-energy","title":"Kinetic Energy","text":"<ul> <li>half x mass x (velocity squared)</li> <li> \\[E_{K}= \\frac{1}{2}mv^{2}\\] </li> </ul>"},{"location":"KB/Kinetic%20Friction/","title":"Kinetic Friction","text":""},{"location":"KB/Kinetic%20Friction/#kinetic-friction","title":"Kinetic Friction","text":"<ul> <li> \\[F_{k} \\leq \\mu _{k}F_{N}\\] </li> <li>\\(F_{k}\\) is kinetic friction</li> <li>\\(F_{N}\\) is normal force</li> <li>\\(\\mu _s\\) is coefficient of friction</li> </ul>"},{"location":"KB/Kinetics/","title":"Kinetics","text":""},{"location":"KB/Kinetics/#kinetics","title":"Kinetics","text":"<ul> <li>large-scale, highquality dataset for human action recognition in videos </li> <li>500, 000 video clips covering 600 human action classes with at least 600 video clips for each action class </li> <li>Each video clip lasts around 10 seconds and is labeled with a single action class.</li> </ul>"},{"location":"KB/Klue%20ML%20Engineer/","title":"Klue ML Engineer","text":"","tags":["jobs"]},{"location":"KB/Klue%20ML%20Engineer/#klue-ml-engineer","title":"Klue ML Engineer","text":"<ul> <li>We are looking for Machine Learning Engineers to work with our team to deliver high quality products in the most efficient way.  </li> </ul> <p>We build machine learning services and data pipelines to automatically extract insights about competitors from both public and internal data sources. Every day, our services process millions of data points, including news articles, press releases, webpage changes, Slack posts, emails, reviews, CRM opportunities, and user actions. We utilize a broad array of ML techniques, including classification, clustering, recommendation, summarization, prompt engineering, vector search, and retrieval augmented generation.  </p> <p>\ud83d\udca1Klue + You?  </p> <p>Q: Klue who?  </p> <p>A: We\u2019re Klue and from a technical perspective, Klue\u2019s mission is to descale huge amounts of data to the human level, so people can process it and make use of it. Klue is that trusted intermediary, right now it\u2019s proven for sales enablement, but tomorrow it\u2019s all teams enablement.  </p> <p>Q: What level of experience are we looking for?  </p> <p>A: Right now, we are looking for experienced senior-level Machine Learning Engineers.  </p> <p>Q: What are we working on?  </p> <p>A: Services for collecting, processing and generating timely, relevant intel that is accurately linked to competitors, products, industries, and people. Our Machine Learning Engineers are primarily focused on the training, evaluation, and deployment of models to label, score, cluster, and generate insights.  </p> <p>Q: What technologies do we use?  </p> <p>A: Python, Transformers, Pytorch, Hugging Face, Spacy, Sklearn, Pinecone, Kubeflow, Vertex AI, BentoML, Aporia, JS, PostgreSQL, Elasticsearch, Redis, GCP, BigQuery, Docker/Kubernetes, Github, GPT.  </p> <p>We believe in using whatever tools make sense to get the job done and support our game-changing innovation.  </p> <p>Q: What skills do you bring?  </p> <ul> <li>You are an expert on the landscape of transformer models and are proficient with popular ML frameworks such as PyTorch, Hugging Face, or Scikit-Learn.</li> <li>You stay up to date on recent advances with LLMs and you demonstrate astute judgment in deciding whether to train a model with a custom architecture or leverage GPT.</li> <li>You ensure your experiments are reproducible, balancing swift discovery with scientific rigor.</li> <li>You are proficient in designing, implementing, and deploying RESTful APIs and you have experience with (non)relational and vector databases.</li> <li>You demonstrate good judgment when making architectural decisions and you understand how those decisions fit into the bigger picture  </li> </ul> <p>Q: What about total compensation &amp; benefits?  </p> <ul> <li>Benefits. We currently have a pension plan for our EU team</li> <li>Time off. Take what you need. We want the team to prioritize wellness and avoid burnout. We want to also give individuals autonomy to choose how and when they take vacation. We understand and respect that everyone\u2019s needs for time off are different, just like our team</li> </ul>","tags":["jobs"]},{"location":"KB/Knowledge%20Component/","title":"Knowledge Component","text":""},{"location":"KB/Knowledge%20Component/#knowledge-component","title":"Knowledge Component","text":"<ul> <li>A knowledge component can be a principle, a concept, a rule, a procedure, a fact, an association or any other fragment of task-specific information.</li> <li>Despite the connotations of knowledge, a knowledge component can be incorrect, in that instructors would rather that students not apply this knowledge component while achieving a task.</li> </ul>"},{"location":"KB/Knowledge%20Distillation%20Survey%202021/","title":"Knowledge Distillation Survey 2021","text":""},{"location":"KB/Knowledge%20Distillation%20Survey%202021/#knowledge-distillation-survey-2021","title":"Knowledge Distillation Survey 2021","text":"<ul> <li>model compression and acceleration techniques</li> <li>Low-rank factorization</li> <li>Transferred compact convolutional filters</li> <li>To address this issue, Bucilua et al. (2006) first proposed model compression to transfer the information from a large model or an ensem- ble of models into training a small model without a significant drop in accuracy. The knowledge transfer between a fully-supervised teacher model and a stu- dent model using the unlabeled data is also intro- duced for semi-supervised learning (Urner et al., 2011).</li> <li>The learning of a small model from a large model is later formally popularized as knowledge distilla- tion (Hinton et al., 2015). In knowledge distillation, a small student model is generally supervised by a large teacher model (Bucilua et al., 2006; Ba and Caruana, 2014; Hinton et al., 2015; Urban et al., 2017).</li> <li>The main idea is that the student model mimics the teacher model in order to obtain a competitive or even a superior performance. The key problem is how to transfer the knowledge from a large teacher model to a small student model. Basically, a knowledge distillation system is composed of three key components: knowledge, dis- tillation algorithm, and teacher-student architecture.</li> <li>Successful distillation relies on data geometry, optimization bias of distillation objective and strong monotonicity of the student classifier</li> <li>quantified the extraction of visual concepts from the intermediate layers of a deep neural network, to explain knowledge distillation (Cheng et al., 2020). Ji &amp; Zhu theoretically explained knowledge distillation on a wide neural network from the respective of risk bound, data efficiency and imperfect teacher (Ji and Zhu., 2020).</li> <li>Knowledge distillation has also been explored for label smoothing, for assessing the accuracy of the teacher and for obtaining a prior for the optimal output layer geometry (Tang et al., 2020).</li> <li>Furthermore, the knowledge transfer from one model to another in knowledge distillation can be extended to other tasks, such as adversar- ial attacks (Papernot et al., 2016), data augmenta- tion (Lee et al., 2019a; Gordon and Duh, 2019), data privacy and security (Wang et al., 2019a).</li> <li>A vanilla knowledge distillation uses the logits of a large deep model as the teacher knowledge (Hinton et al., 2015; Kim et al., 2018; Ba and Caruana, 2014; Mirzadeh et al., 2020)</li> <li>Further- more, the parameters of the teacher model (or the connections between layers) also contain another knowl- edge (Liu et al., 2019c)</li> <li>Response Based Knowledge</li> <li>Feature Based Knowledge</li> </ul>"},{"location":"KB/Knowledge%20Distillation/","title":"Knowledge Distillation","text":""},{"location":"KB/Knowledge%20Distillation/#knowledge-distillation","title":"Knowledge Distillation","text":"<ul> <li>Teacher model to help train the student model</li> <li>Teacher is often pre trained</li> <li>Student tries to imitate teacher</li> <li>Distillation Loss</li> <li>Knowledge Distillation Survey 2021</li> <li>Distilling the Knowledge in a Neural Network</li> </ul>"},{"location":"KB/Kvasir%20Dataset/","title":"Kvasir Dataset","text":""},{"location":"KB/Kvasir%20Dataset/#kvasir-dataset","title":"Kvasir Dataset","text":"<ul> <li>Simula Datasets - Kvasir #Roam-Highlights</li> <li>dataset containing images from inside the gastrointestinal (GI) tract</li> <li>The collection of images are classified into three important anatomical landmarks and three clinically significant findings.</li> <li>two categories of images related to endoscopic polyp removal</li> <li>The dataset consist of the images with different resolution from 720x576 up to 1920x1072 pixels</li> <li>Some of the included classes of images have a green picture in picture illustrating the position and configuration of the KB/Endoscope.md inside the bowel, by use of an electromagnetic imaging system</li> </ul>"},{"location":"KB/LASER/","title":"LASER","text":""},{"location":"KB/LASER/#laser","title":"LASER","text":"<ul> <li>Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond</li> <li>joint multilingual sentence representations</li> <li>LASER</li> <li>Language-Agnostic SEntence Representations</li> <li>93 languages, belonging to more than 30 different families and written in 28 different scripts</li> <li>universal language agnostic sentence embeddings</li> <li>train a single encoder to handle multiple languages, so that semantically similar sentences in different languages are close in the embedding space</li> <li>single BiLSTM encoder with a shared BPE vocabulary for all languages</li> <li>coupled with an auxiliary decoder and trained on publicly available parallel corpora</li> <li>learn a classifier on top of the resulting embeddings using English annotated data only, and transfer it to any of the 93 languages without any modification</li> <li>XNLI</li> <li>MLDoc</li> <li>BUCC</li> <li>test set of aligned sentences in 112 languages</li> </ul>"},{"location":"KB/LASER/#laser_1","title":"Laser","text":"<ul> <li>Acronym for Light Amplification by Stimulated Emission of Radiation. A device that produces a coherent monochromatic beam of light which is extremely narrow and focused but still within the visible light spectrum. This is commonly used as a non-contact sensor for robots. Robotic applications include: distance finding, identifying accurate locations, surface mapping, bar code scanning, cutting, welding etc.</li> </ul>"},{"location":"KB/LDA/","title":"LDA","text":""},{"location":"KB/LDA/#lda","title":"LDA","text":""},{"location":"KB/LDA/#steps","title":"Steps","text":"<ul> <li>Compute the\u00a0dd-dimensional mean vectors for the different classes from the dataset.</li> <li>Compute the scatter matrices (in-between-class and within-class scatter matrix).</li> <li>Compute the eigenvectors \\((e_1,e_2,...,e_de_1,e_2,...,e_d)\\) and corresponding eigenvalues \\((\u03bb_1,\u03bb_2,...,\u03bb_d\u03bb_1,\u03bb_2,...,\u03bb_d)\\) for the scatter matrices.</li> <li>Sort the eigenvectors by decreasing eigenvalues and choose\u00a0\\(k\\)\u00a0eigenvectors with the largest eigenvalues to form a\u00a0\\(d\u00d7k\\)\u00a0dimensional matrix\u00a0\\(W\\)\u00a0(where every column represents an KB/Eigenvector.md).</li> <li>Use this\u00a0\\(d\u00d7k\\)\u00a0KB/Eigenvector.md matrix to transform the samples onto the new subspace. This can be summarized by the matrix multiplication:\u00a0\\(Y=X\u00d7WY=X\u00d7W\\)\u00a0(where\u00a0\\(X\\)\u00a0is a\u00a0\\(n\u00d7d\\)-dimensional matrix representing the\u00a0\\(n\\)\u00a0samples, and\u00a0\\(y\\)\u00a0are the transformed\u00a0\\(n\u00d7k\\)-dimensional samples in the new subspace).</li> </ul>"},{"location":"KB/LIME/","title":"LIME","text":""},{"location":"KB/LIME/#lime","title":"LIME","text":"<ul> <li>@ribeiroWhyShouldTrust2016</li> <li>novel model-agnostic modular and extensible explanation technique that explains the predictions of any classifier in an interpretable and faithful manner</li> <li>learning an interpretable model locally around the prediction</li> <li>SP-LIME</li> <li>method to explain models by selecting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem and providing a global view of the model to users</li> <li>flexibility of these methods by explaining different models for text (e.g random forests) and image classification (e.g neural networks)</li> <li>usefulness of explanations is shown via novel experiments, both simulated and with human subjects</li> <li></li> </ul>"},{"location":"KB/LLM%20Guide/","title":"LLM Guide","text":"","tags":["llm"]},{"location":"KB/LLM%20Guide/#llm-guide","title":"LLM Guide","text":"<ul> <li>Refer to Complete AI Pipeline for extra steps</li> </ul>","tags":["llm"]},{"location":"KB/LLM%20Guide/#useful-types","title":"Useful Types","text":"","tags":["llm"]},{"location":"KB/LLM%20Guide/#training","title":"Training","text":"","tags":["llm"]},{"location":"KB/LLM%20Guide/#data-intake","title":"Data Intake","text":"","tags":["llm"]},{"location":"KB/LLM%20Guide/#deployment","title":"Deployment","text":"","tags":["llm"]},{"location":"KB/LLM%20Guide/#llm-metrics","title":"LLM Metrics","text":"","tags":["llm"]},{"location":"KB/LLM%20Guide/#bias","title":"Bias","text":"","tags":["llm"]},{"location":"KB/LLM%20Guide/#gui-tools","title":"GUI Tools","text":"<ul> <li>Azure AI Studio (PromptFlow)<ul> <li>PromptFlow</li> <li>PromptFlow RAG</li> </ul> </li> <li>Azure Open AI</li> <li>Azure Cognitive Search</li> <li>Azure Machine Learning</li> </ul>","tags":["llm"]},{"location":"KB/LRP/","title":"LRP","text":""},{"location":"KB/LRP/#lrp","title":"LRP","text":""},{"location":"KB/LaMDA/","title":"LaMDA","text":""},{"location":"KB/LaMDA/#lamda","title":"LaMDA","text":"<ul> <li>language model for dialog applications</li> <li>family of transformer-based neural language models specialized for dialog which have up to 137B parameters and are pre-trained on 1.56T words of public dialog data and web text.</li> <li>Fine-tuning can enable for safety and factual grounding of the model</li> <li>Only 0.001% of training data was used for fine-tuning, which is a great achievement of the model</li> <li>dialog modes take advantage of Transformers' ability to present long-term dependencies in text</li> <li>generally very well-suited for model scaling</li> <li>use of a single model to perform multiple tasks: it generates several responses, which are filtered for safety, grounded on an external knowledge source and reranked to find the highest-quality response.</li> </ul>"},{"location":"KB/Label%20Encoding/","title":"Label Encoding","text":""},{"location":"KB/Label%20Encoding/#label-encoding","title":"Label Encoding","text":"<ul> <li>also called Integer Encoding</li> <li>Each unique category is assigned an integer value</li> <li>e.g. \"red\" \u2192 0, \"blue\" \u2192 1, \u2026</li> <li>easy reversible</li> <li>can only be used when a ordinal relationship between the labels exist, e.g. winner ranking in string (\"first\", \"second\", \"third\")<ul> <li>if not and still used, can result in poor performance and unexpected results</li> </ul> </li> <li>numeric representations have a natural ordered relationship between each other and the models are able to understand that relationship</li> </ul>"},{"location":"KB/Label%20Smoothing/","title":"Label Smoothing","text":""},{"location":"KB/Label%20Smoothing/#label-smoothing","title":"Label Smoothing","text":"<ul> <li>Dense layer is generally the last one and combined with soft max leads to a Probability distribution</li> <li>Assume true label to be y, then a truth Probability distribution would be \\(p_i=1\\) If i=y and 0 otherwise</li> <li>During training, minimize negative Cross Entropy loss to make these distributions similar</li> <li>We know, \\(\\(\\mathscr{l}(p,q) = -log p_y = -z_y + log(\\Sigma^{K}_{i=1}exp(z_i))\\)\\)</li> <li>Where the optimal solution is \\(z^{\\ast}_{y}=\\inf\\)<ul> <li>The output scores are encouraged to be distinctive which leads to overfitting</li> <li>Leads to</li> </ul> </li> <li>Instead \\(\\(\\cases{1-\\epsilon&amp; if i=1\\\\\\frac{\\epsilon}{(K-1)} &amp; \\text{otherwise}}\\)\\)</li> <li>The optimal Solution is<ul> <li>\\(log((K-1)(1-\\epsilon)/ \\epsilon)+\\alpha\\) if \\(i=y\\)</li> <li>\\(\\alpha\\) otherwise<ul> <li>Any real number</li> <li>Finite output from the last layer that generalizes well</li> </ul> </li> </ul> </li> <li>If \\(\\epsilon =0\\) , \\(log((k-1)\\frac{1-\\epsilon}{\\epsilon})\\) is \\(\\infty\\)</li> <li>As \\(\\epsilon\\) increases, the gap decreases</li> <li>If \\(\\epsilon=\\frac{K-1}{K}\\), all optimizal \\(z^{\\ast}_{i}\\) are identical</li> </ul>"},{"location":"KB/Label%20bias/","title":"Label bias","text":""},{"location":"KB/Label%20bias/#label-bias","title":"Label Bias","text":"<ul> <li>This comes from the fact that semantic categories are often poorly defined, and different labellers may assign differing labels to the same type of object [11] (e.g. \"grass\" vs. \"lawn\", \"painting\" vs. \"picture\").</li> </ul>"},{"location":"KB/Labeled%20Faces%20in%20the%20Wild/","title":"Labeled Faces in the Wild","text":""},{"location":"KB/Labeled%20Faces%20in%20the%20Wild/#labeled-faces-in-the-wild","title":"Labeled Faces in the Wild","text":"<ul> <li>vis-www.cs.umass.edu/lfw/</li> </ul>"},{"location":"KB/Lack%20of%20information/","title":"Lack of Information","text":""},{"location":"KB/Lack%20of%20information/#lack-of-information","title":"Lack of Information","text":"<ul> <li>Data does not show how to extract optimal info</li> <li>Curse Of Dimensionality</li> </ul>"},{"location":"KB/Ladle%20Gripper/","title":"Ladle Gripper","text":""},{"location":"KB/Ladle%20Gripper/#ladle-gripper","title":"Ladle Gripper","text":"<ul> <li>An end-effector, which acts as a scoop. It is commonly used to scoop up liquids, transfer it to a mold and pour the liquid into the mold. Common for handling molten metal under hazardous conditions</li> </ul>"},{"location":"KB/Lagrangian%20Coherent%20Structure/","title":"Lagrangian Coherent Structure","text":""},{"location":"KB/Lagrangian%20Coherent%20Structure/#lagrangian-coherent-structure","title":"Lagrangian Coherent Structure","text":"<ul> <li>Lagrangian Grid</li> <li></li> </ul>"},{"location":"KB/Lagrangian%20Grid/","title":"Lagrangian Grid","text":""},{"location":"KB/Lagrangian%20Grid/#lagrangian-grid","title":"Lagrangian Grid","text":"<ul> <li>Focus on individual particles</li> <li>Attached are position, velocity, and other properties</li> <li>Explicit position</li> <li></li> </ul>"},{"location":"KB/Language%20Identification/","title":"Language Identification","text":""},{"location":"KB/Language%20Identification/#language-identification","title":"Language Identification","text":"<ul> <li>Identifying the language of the document</li> <li>Documents could be multilingual at the sentence level or paragraph level too</li> <li>Unique Character Set</li> <li>Shared Character Set</li> <li>Byte Range Distribution used for Character Set Identification</li> <li>sort the bytes in a \ufb01le by frequency count and use the sorted list as a signature vector for comparison via an n-gram model</li> </ul>"},{"location":"KB/Language%20dependence/","title":"Language dependence","text":""},{"location":"KB/Language%20dependence/#language-dependence","title":"Language Dependence","text":"<ul> <li>Range of orthographic conventions used in written languages to denote the boundaries between linguistic units such as syllables, words, or sentences</li> <li>Language Identification</li> </ul>"},{"location":"KB/Laplace%20Distribution/","title":"Laplace Distribution","text":""},{"location":"KB/Laplace%20Distribution/#laplace-distribution","title":"Laplace Distribution","text":"<ul> <li>PDF </li> <li> \\[f(x\\mid \\mu ,b)={\\frac {1}{2b}}\\exp \\left(-{\\frac {|x-\\mu |}{b}}\\right)\\] </li> <li>where\u00a0\\(\\mu\\)\u00a0is a\u00a0location parameter, and\u00a0\\(b&gt;0\\) which is sometimes referred to as the \"diversity\", is a\u00a0scale param If\u00a0\u03bc=0\u00a0and\u00a0b=1 the positive half-line is exactly an\u00a0Exponential Distribution\u00a0scaled by 1/2</li> <li>Kinda like Normal Distribution but normal distribution is expressed in terms of the squared difference from the mean\u00a0\u03bc, the Laplace density is expressed in terms of the\u00a0absolute difference from the mean</li> </ul>"},{"location":"KB/Laplacian%20Grid%20Smoothing/","title":"Laplacian Grid Smoothing","text":""},{"location":"KB/Laplacian%20Grid%20Smoothing/#laplacian-grid-smoothing","title":"Laplacian Grid Smoothing","text":"<ul> <li>new position is based on neighbor positions</li> <li> \\[p_{i}=\\frac{1}{N}\\Sigma_{i\u2026j}p_{j}\\] </li> </ul>"},{"location":"KB/Large%20Batch%20Training/","title":"Large Batch Training","text":""},{"location":"KB/Large%20Batch%20Training/#large-batch-training","title":"Large Batch Training","text":"<ul> <li>Generally slows down training</li> <li>If convex, convergence rate decreases with increase in batch size</li> <li>Learning Rate Scheduling</li> <li>Modified Batch Normalization with \\(\\gamma=0\\) for all BNs at the end of a residual block that micmics networks with less Layers and is easier to train at the start</li> <li>No bias decay</li> </ul>"},{"location":"KB/Large%20Kernel%20in%20Attention/","title":"Large Kernel in Attention","text":""},{"location":"KB/Large%20Kernel%20in%20Attention/#large-kernel-in-attention","title":"Large Kernel in Attention","text":"<ul> <li>self-attention can be viewed as a global depth-wise kernel that enables each layer to have a global receptive field.</li> <li>Swin Transformer (Liu et al., 2021e) is a ViTs variant that adopts local attention with a shifted window manner</li> <li>greatly improve the memory and computation efficiency with appealing performance</li> <li>Since the size of attention windows is at least 7, it can be seen as an alternative class of large kernel</li> <li>recent work (Guo et al., 2022b) proposes a novel large kernel attention module that</li> <li>uses stacked depthwise, small convolution, dilated convolution as well as pointwise convolution to capture both local and global structure</li> </ul>"},{"location":"KB/Large%20Kernel%20in%20Convolution/","title":"Large Kernel in Convolution","text":""},{"location":"KB/Large%20Kernel%20in%20Convolution/#large-kernel-in-convolution","title":"Large Kernel in Convolution","text":"<ul> <li>Global Convolutional Network (GCNs) (Peng et al., 2017) enlarges the kernel size to 15 by employing a combination of 1\u00d7M + M\u00d71 and M\u00d71 + 1\u00d7M convolutions.</li> <li>However, the proposed method leads to performance degradation on ImageNet</li> <li>or the utilization of varying convolutional kernel sizes to learn spatial patterns at different scales. With the popularity of VGG (Simonyan &amp; Zisserman, 2014), it has been common over the past decade to use a stack of small kernels (1\u00d71 or 3\u00d73) to obtain a large receptive</li> <li>field</li> <li>However, the performance improvement plateaus when further expanding the kernel size</li> <li>Han et al. (2021b) find that dynamic depth-wise convolution (7x7) performs on par with the local attention mechanism if we substitute the latter with the former in Swin Transformer</li> <li>Liu et al. (2022b) imitate the design elements of Swin Transformer (Liu et al., 2021e) and design ConvNeXt employed with 7x7 kernels, surpassing the performance of the former</li> <li>Lately, Chen et al. (2022) reveal large kernels to be feasible and beneficial for 3D networks too.</li> <li>Prior works have explored the idea of paralleling (Peng et al., 2017; Guo et al., 2022a) or stacking (Szegedy et al., 2017) two complementary Mx1 and 1xM kernels</li> <li>However, they limit the shorter edge to 1 and do not scale the kernel size beyond 51x51</li> </ul>"},{"location":"KB/Latent%20Dirchlet%20Allocation/","title":"Latent Dirchlet Allocation","text":""},{"location":"KB/Latent%20Dirchlet%20Allocation/#latent-dirchlet-allocation","title":"Latent Dirchlet Allocation","text":"<ul> <li>Discovers topics into a collection of documents</li> <li>Tags each document with topics</li> <li></li> </ul>"},{"location":"KB/Latent%20Semantic%20Analysis/","title":"Latent Semantic Analysis","text":""},{"location":"KB/Latent%20Semantic%20Analysis/#latent-semantic-analysis","title":"Latent Semantic Analysis","text":"<ul> <li>Roughly speaking, a Learning Event is considered to be included in the student's step if the degree of semantic similarity is above a certain threshold.</li> </ul>"},{"location":"KB/Latitude%20Junior%20ML%20Engineer/","title":"Latitude Junior ML Engineer","text":""},{"location":"KB/Latitude%20Junior%20ML%20Engineer/#latitude-junior-ml-engineer","title":"Latitude Junior ML Engineer","text":"<p>As the days go by, Machine Learning and AI are slowly becoming terms that every company wants to have in their portfolio. While this drive leads to many innovations, most companies are not sure how to do AI \"well\". They want to use AI but need clarification on whether it is required, how to handle bias, how to create proper data, or even what models (ML vs. DL) to choose. I believe that the Latitude data-first approach is the way to go, and in the long run, this approach will help many companies achieve their vision of AI. I love solving problems with creative analytics, and this application is to get a chance to take part in Latitude's mission to make a difference to customers with data.</p> <p>My expertise is a combination of data analytics and computer vision, and as of a month ago, I also have a Master in Artificial Intelligence from the University of Groningen. I am familiar with the tools required for basic and advanced AI and analytics from internships, research projects, papers, freelance work, and many personal projects. That being the case, I can step aside from my experience with AI and decide if a project needs another solution in reality. I have experience finding proper KPIs and can phrase them in the context of an ML (or not) problem and provide a means to solve them.</p> <p>The customer is king, they say, and the first step in having happy clients is understanding what they truly want and then being able to give them a solution they can use. That being the case, in any team, it is essential that each of the members can contribute something to the overall development of the projects. Although I have a lot to learn, I genuinely enjoy developing solutions that have a positive impact, and I can contribute quite a bit to any team I get the chance to work with.</p>"},{"location":"KB/Law%20of%20large%20numbers/","title":"Law of Large Numbers","text":""},{"location":"KB/Law%20of%20large%20numbers/#law-of-large-numbers","title":"Law of Large Numbers","text":"<ul> <li>If one carries out an infinite seq of independantly repeating same the same numerical mesurement and gets a sequence of measurement values \\(x_{1}, .. , x_{n}\\) where \\(x_{i} \\in \\mathbb{R}\\), then the mean value of the inital seq upto N will almost always converge to the same number \\(\\(\\mu_{N} = \\frac{1}{N}\\Sigma_{i=1}^{N}x_{i}\\)\\) which is the EXPECTATION of X \\(\\(\\mu_{N}=E[X]\\)\\)</li> <li>Kolmogorov axioms</li> </ul>"},{"location":"KB/Layer%20Normalization/","title":"Layer Normalization","text":""},{"location":"KB/Layer%20Normalization/#layer-normalization","title":"Layer Normalization","text":"<ul> <li>For RNNs etc</li> <li>Mean and variance calculated independantly for each element of the batch by aggregating over the features dimensions.</li> <li> (Compared to Batch Normalization) $$  \\begin{align*}\\</li> </ul> <p>&amp;\\mu_{\\mathcal{B}} \\leftarrow \\frac{1}{m}\\Sigma_{i=1}^{m}x_{i}\\</p> <p>&amp;\\sigma^{2}{\\mathcal{B}} \\leftarrow \\frac{1}{m}\\Sigma{i=1}^{m}(x_{i}-\\mu_{\\mathcal{B}})^{2}\\</p> <p>&amp;\\hat x_{i} \\leftarrow \\frac{x_{i}-\\mu_{\\mathcal{B}}}{\\sqrt{\\sigma^{2}_{\\mathcal{B}} + \\epsilon}}\\</p> <p>&amp;y_{i}= \\gamma \\hat x_{i}+ \\beta</p> <p>\\end{align*} $$</p>"},{"location":"KB/Layer%20Normalization/#problem","title":"Problem","text":"<ul> <li>From Visualizing the Loss Landscape of Neural Nets,</li> <li></li> </ul>"},{"location":"KB/Layers/","title":"Layers","text":""},{"location":"KB/Layers/#layers","title":"Layers","text":""},{"location":"KB/Layers/#notation","title":"Notation","text":"<ul> <li>Grad of a function f wrt A : \\(\\(\\nabla_Af\\)\\)</li> <li>Neuron Pre activation : Z</li> <li>Activations : Y</li> <li>Tensor shape : (w,h,c)</li> <li>Matrix multi : \\(\\(A\\cdot B\\)\\)</li> <li>Hadmard prod (coeff wise) : \\(\\(A \\circ B\\)\\)</li> </ul>"},{"location":"KB/Layerwise%20Conservation%20Principle/","title":"Layerwise Conservation Principle","text":""},{"location":"KB/Layerwise%20Conservation%20Principle/#layerwise-conservation-principle","title":"Layerwise Conservation Principle","text":"<ul> <li>which says that \"a network's output activity is fully redistributed through the layers of a DNN onto the input variables, i.e., neither positive nor negative evidence is lost.\"</li> </ul>"},{"location":"KB/Layerwise%20Gradient%20Magnitude%20Based%20Pruning/","title":"Layerwise Gradient Magnitude Based Pruning","text":""},{"location":"KB/Layerwise%20Gradient%20Magnitude%20Based%20Pruning/#layerwise-gradient-magnitude-based-pruning","title":"Layerwise Gradient Magnitude Based Pruning","text":"<ul> <li>Finds the lowest absolute value per layer and removes them</li> </ul>"},{"location":"KB/Layerwise%20Magnitude%20Based%20Pruning/","title":"Layerwise Magnitude Based Pruning","text":""},{"location":"KB/Layerwise%20Magnitude%20Based%20Pruning/#layerwise-magnitude-based-pruning","title":"Layerwise Magnitude Based Pruning","text":"<ul> <li>Takes the lowest values per layer in the network and prunes.</li> <li>Modifying the global layerwise and applying it per layer instead.</li> <li>To do this, we first make a copy of the weights. Then for every layer in the array, we find the least n values, take the nth value and set all the others to 0.</li> <li>As an edge case, if the number of elements entered is greater than the total length of the layer, then the entire layer is set to 0.</li> </ul>"},{"location":"KB/Layerwise%20Relevance%20Propagation/","title":"Layerwise Relevance Propagation","text":""},{"location":"KB/Layerwise%20Relevance%20Propagation/#layerwise-relevance-propagation","title":"Layerwise Relevance Propagation","text":"<ul> <li>It relies on a conservation principle to propagate the outcome decision back without using gradients. The idea behind it is a decomposition of prediction function as a sum of layerwise relevance values. When LRP is applied to deep ReLU networks, LRP can be understood as a deep Taylor decomposition of the prediction. This principle ensures that the prediction activity is fully redistributed through all the layers onto the input variables</li> <li>suffers from the shattered gradients problem</li> </ul>"},{"location":"KB/Le%20Net/","title":"Le Net","text":""},{"location":"KB/Le%20Net/#le-net","title":"Le Net","text":"<ul> <li>Spatial dims reduce with depth, no of neurons increase</li> <li></li> </ul>"},{"location":"KB/LeCake/","title":"LeCake","text":""},{"location":"KB/LeCake/#lecake","title":"LeCake","text":"<ul> <li>relative importance of each learning paradigm by Yann LeCun</li> <li>At the end of the day, everything tied to Supervised Learning</li> <li></li> </ul>"},{"location":"KB/LeCun%20Init/","title":"LeCun Init","text":""},{"location":"KB/LeCun%20Init/#lecun-init","title":"LeCun Init","text":"<ul> <li> \\[\\frac{1}{fan_{in}}\\] </li> </ul>"},{"location":"KB/Lead%20Test/","title":"Lead Test","text":""},{"location":"KB/Lead%20Test/#lead-test","title":"Lead Test","text":"<ul> <li>A test to reveal the quantity of lead in the bloodstream</li> </ul>"},{"location":"KB/Leaky%20Relu/","title":"Leaky Relu","text":""},{"location":"KB/Leaky%20Relu/#leaky-relu","title":"Leaky Relu","text":"<ul> <li>Andrew L. Maas, Awni Y. Hannun, Andrew Y. Ng (2014). Rectifier Nonlinearities Improve Neural Network Acoustic Models.</li> <li>has a small slope for negative values instead of a flat slope. The slope coefficient is determined before training, i.e. it is not learnt during training. This type of activation function is popular in tasks where we we may suffer from sparse gradients, for example training generative adversarial networks.</li> <li>The reasons can be numerous, but in order to fight the situation when suddenly lot\u2019s of neurons in the network simply do nothing</li> <li> \\[max(0.01x,x)\\] </li> <li></li> </ul>"},{"location":"KB/Leap%20Data%20Scientist/","title":"Leap Data Scientist","text":""},{"location":"KB/Leap%20Data%20Scientist/#leap-data-scientist","title":"Leap Data Scientist","text":"<p>As I write this motivation letter, the temperatures in NL have hit yet another record for the hottest September ever. One of the biggest reasons for this is global warming, and fossil fuels are a massive component. Leap's mission to enable the shift to cleaner energy greatly resonates with me, so I am applying for this position as a Data Scientist. At the core of making the world a greener and healthier place is data that every company has collected for decades. Using that vast data pool to inform better decisions is quite challenging but equally rewarding.</p> <p>My expertise is a combination of data analytics and computer vision, and as of a month ago, I also have a Master in Artificial Intelligence from the University of Groningen. I am familiar with the tools required for basic and advanced AI (such as PyTorch, Tensorflow, scikit-learn, and many others) and analytics from internships, research projects, papers, freelance work, and many personal projects. In my time at KPMG and Emirates NBD, I have worked on massive data analytics pipelines across every level, from data preprocessing to feature engineering to deploying analytics solutions. Python is my language of choice for analytics as well as Deep Learning and Machine learning and I am familiar with the common libraries in each of these domains.</p> <p>In any team, it is essential that each of the members can contribute something to the overall development of the projects. Although I have a lot to learn, I genuinely enjoy developing solutions that have a positive impact, and this position will be the perfect next step for me. I will be able to contribute to any team I get to work with.</p> <p>I hope you give me a chance to contribute to the efforts that we as a race must make for our future generations in the fight against climate change.</p>"},{"location":"KB/Learning%20Component/","title":"Learning Component","text":""},{"location":"KB/Learning%20Component/#learning-component","title":"Learning Component","text":"<ul> <li>analyses the trace of activities and extracts and conceptualizes possibly interesting experiences.</li> </ul>"},{"location":"KB/Learning%20Event/","title":"Learning Event","text":""},{"location":"KB/Learning%20Event/#learning-event","title":"Learning Event","text":"<ul> <li>A learning event is the construction or application of a Knowledge Component, often while trying to achieve the task. Learning events are mental events, whereas steps are physical events. A learning event occurs in the mind of the student where it cannot be observed, whereas a step occurs on the user interface and the computer can observe it. In the algebra example mentioned earlier, the step x=18.46 can be considered to result from three learning events: \\(\\(2.3*x=42.45 (cid:198) x=42.45/2.3 (cid:198) x=18.4565\u2026 (cid:198) x=18.46\\)\\)</li> </ul>"},{"location":"KB/Learning%20L2%20German%20Vocabulary%20Through%20Reading/","title":"Learning L2 German Vocabulary Through Reading","text":""},{"location":"KB/Learning%20L2%20German%20Vocabulary%20Through%20Reading/#learning-l2-german-vocabulary-through-reading","title":"Learning L2 German Vocabulary Through Reading","text":"<ul> <li> <p>Elke Peters , Jan H. Hulstijn , Lies Sercu , Madeline Lutjeharms</p> </li> <li> <p>This study investigated three techniques designed to increase the chances that second language (L2) readers look up and learn unfamiliar words during and after reading an L2 text</p> </li> <li>They could look up the meaning of unfamiliar words in an online dictionary</li> <li>Test announcement and word relevance substantially prompted participants to use the online dictionary more.</li> <li>Only test announcement and vocabulary task (not word relevance) affected performance in the word recognition test positively</li> <li>oth word relevance and postreading vocabulary task substantially affected word retention in the KB/Recall.md posttests</li> <li>low incidence of vocabulary acquisition through reading (\"input only\") can be substantially boosted by techniques that make students look up the meaning of unknown words, process their form-meaning relationship elaborately, and process them again after reading (\"input plus\").</li> </ul>"},{"location":"KB/Learning%20Rate%20Decay%20tricks/","title":"Learning Rate Decay","text":""},{"location":"KB/Learning%20Rate%20Decay%20tricks/#learning-rate-decay-tricks","title":"Learning Rate Decay #tricks","text":"<ul> <li>Scale of loss landscape changes</li> <li>Reduce step size near optima</li> <li>Factor \\(\\(\\alpha_{i+1} = d\\cdot \\alpha_i\\)\\)</li> <li>Cosine Learning Rate Decay</li> </ul>"},{"location":"KB/Learning%20Rate%20Decay%20tricks/#_1","title":"\u2026","text":""},{"location":"KB/Learning%20Rate%20Range%20Test/","title":"Learning Rate Range Test","text":""},{"location":"KB/Learning%20Rate%20Range%20Test/#learning-rate-range-test","title":"Learning Rate Range Test","text":"<ul> <li>Smith, LN (2018) A disciplined approach to neural network hyper-parameters: Part 1--learning rate, batch size, momentum, and weight decay arXiv preprint arXiv:1803.09820</li> <li>It is relatively straight-forward: in a test run, one starts with a very small learning rate, for which one runs the model and computes the loss on the validation data. One does this iteratively, while increasing the learning rate exponentially in parallel. One can then plot their findings into a diagram representing loss at the y axis and the learning rate at the x axis. The x value representing the lowest y value, i.e. the lowest loss, represents the optimal learning rate for the training data.</li> <li>The learning rate at this extrema is the largest value that can be used as the learning rate for the maximum bound with cyclical learning rates but a smaller value will be necessary when choosing a constant learning rate or the network will not begin to converge.</li> <li>Smoothed loss changes</li> </ul> <pre><code>for i in range(moving_average, len(learning_rates)):\n    loss_changes.append((losses[i] - losses[i - moving_average]) / moving_average)\n</code></pre>"},{"location":"KB/Learning%20Rate%20Scheduling/","title":"Learning Rate Scheduling","text":""},{"location":"KB/Learning%20Rate%20Scheduling/#learning-rate-scheduling","title":"Learning Rate Scheduling","text":"<ul> <li>Learning Rate Decay tricks</li> <li>Gradient Descent gradients</li> <li>Increasing the batch size, reduces noise in the #gradients so a larger learning rate is okay</li> <li>Linear Learning Rate Scaling</li> <li>Learning Rate Warmup</li> </ul>"},{"location":"KB/Learning%20Rate%20Warmup/","title":"Learning Rate Warmup","text":""},{"location":"KB/Learning%20Rate%20Warmup/#learning-rate-warmup","title":"Learning Rate Warmup","text":"<ul> <li>Small learning rate at the start and then a larger learning rate when the training is stabilized</li> <li>Linearly from 0 to initial rate</li> <li>First m batches to warm up and if the initial learning rate is \\(\\eta\\) then at batch i, \\(1 \\leq i \\leq m\\) , learning rate is \\(\\(\\frac{i\\eta}{m}\\)\\)</li> </ul>"},{"location":"KB/Learning%20from%20RGB-Flow%20Correspondence/","title":"Learning from RGB-Flow Correspondence","text":""},{"location":"KB/Learning%20from%20RGB-Flow%20Correspondence/#learning-from-rgb-flow-correspondence","title":"Learning from RGB-Flow Correspondence","text":"<ul> <li>Optical flow encodes object motions between adjacent frames </li> <li>RGB frames contain appearance information </li> <li>The correspondence of the two types of data can be used to learn general features </li> <li>This type of pretext tasks include optical flow estimation [151], [152] and RGB and optical flow correspondence verification [23]. </li> <li>Sayed et al. proposed to learn video features by verifying whether the input RGB frames and the optical flow corresponding to each other </li> <li>Two networks are employed while one is for extracting features from RGB input and another is for extracting features from optical flow input [24] </li> <li>network needs to capture mutual information between the two modalities </li> <li>mutual information across different modalities usually has higher semantic meaning compared to information which is modality specific </li> <li>Optical flow estimation is another type of pretext tasks </li> <li>FlowNet</li> </ul>"},{"location":"KB/Learning%20from%20Video%20Colorization/","title":"Learning from Video Colorization","text":""},{"location":"KB/Learning%20from%20Video%20Colorization/#learning-from-video-colorization","title":"Learning from Video Colorization","text":"<ul> <li>Temporal coherence </li> <li>consecutive frames within a short time have similar coherent appearance </li> <li>The coherence of color can be used to design pretext tasks for self-supervised learning </li> <li>One way to utilize color coherence is to use video colorization as a pretext task for self-supervised video feature learning. </li> <li>Video colorization is a task to colorize gray-scale frames into colorful frames </li> <li>Vondrick et al. proposed to constrain colorization models to solve video colorization by learning to copy colors from a reference frame </li> <li>Given the reference RGB frame and a gray-scale image, the network needs to learn the internal connection between the reference RGB frame and gray-scale image to colorize it. </li> <li>tackle video colorization by employing a fully convolution neural network </li> <li>Tran et al. proposed an U-shape convolution neural network for video colorization [160] </li> <li>The color coherence in videos is a strong supervision signal</li> </ul>"},{"location":"KB/Learning%20from%20Video%20Prediction/","title":"Learning from Video Prediction","text":""},{"location":"KB/Learning%20from%20Video%20Prediction/#learning-from-video-prediction","title":"Learning from Video Prediction","text":"<ul> <li>Video prediction is a task of predicting future frame sequences based on a limited number of frames of a video </li> <li>To predict future frames, network must learn the change in appearance within a given frame sequence </li> <li>Un-LSTM</li> <li>MCnet</li> <li>Temporal order verification</li> <li>Temporal order recognition</li> <li>Misra et al. proposed to use the temporal order verification as the pretext task to learn image features from videos with 2DConvNet [40] which has two main steps: (1) The frames with significant motions are sampled from videos according to the magnitude of optical flow, (2) The sampled frames are shuffled and fed to the network which is trained to verify whether the input data is in correct order. </li> <li>successfully verify the order of the input frames, the network is required to capture the subtle difference between the frames such as the movement of the person </li> <li>semantic features can be learned through the process of accomplishing this task </li> <li>the methods usually suffer from a massive dataset preparation step </li> <li>The frame sequences that used to train the network are selected based on the magnitude of the optical flow, and the computation process of optical flow is expensive and slow</li> </ul>"},{"location":"KB/Learning%20from%20Visual-Audio%20Correspondence/","title":"Learning from Visual-Audio Correspondence","text":""},{"location":"KB/Learning%20from%20Visual-Audio%20Correspondence/#learning-from-visual-audio-correspondence","title":"Learning from Visual-Audio Correspondence","text":"<ul> <li>correspondence between visual and audio streams to design VisualAudio Correspondence learning task [25], [26], [93], [154]. </li> <li>two subnetworks </li> <li>vision </li> <li>audio subnetwork </li> <li>input of vision subnetwork is a single frame or a stack of image frames and the vision subnetwork learns to capture visual features of the input data </li> <li>audio network is a 2DConvNet </li> <li>input is the Fast Fourier Transform (FFT) of the audio from the video </li> <li>Positive data are sampled by extracting video frames and audio from the same time of one video, while negative training data are generated by extracting video frames and audio from different videos or from different times of one video </li> <li>networks are trained to discover the correlation of video data and audio data to accomplish this task. </li> <li>inputs of the ConvNets are two kinds of data, the networks are able to learn the two kinds of information jointly by solving the pretext task.</li> </ul>"},{"location":"KB/Learning%20to%20Detect%20Grasp%20Affordance/","title":"Learning to Detect Grasp Affordance","text":""},{"location":"KB/Learning%20to%20Detect%20Grasp%20Affordance/#learning-to-detect-grasp-affordance","title":"Learning to Detect Grasp Affordance","text":"<ul> <li>Yikun Li, et al., Learning to Detect Grasp Affordances of 3D Objects using Deep Convolutional Neural Networks, Task-Informed Grasping workshop (TIG-II), RSS2019, Germany 2019.</li> <li></li> </ul>"},{"location":"KB/Learning%20with%20Context%20Similarity/","title":"Learning with Context Similarity","text":""},{"location":"KB/Learning%20with%20Context%20Similarity/#learning-with-context-similarity","title":"Learning with Context Similarity","text":"<ul> <li>Clustering is a method of grouping sets of similar data in the same clusters </li> <li>powerful ability of grouping data by using the attributes of the data </li> <li>In the self-supervised scenario, the clustering methods mainly employed as a tool to cluster image data </li> <li>A naive method would be to cluster the image data based on the hand-designed feature such as HOG [140], SIFT [141], or Fisher Vector [49] </li> <li>After the clustering, several clusters are obtained while the image within one cluster has a smaller distance in feature space and images from different clusters have a larger distance in feature space </li> <li>The smaller the distance in feature space, the more similar the image in the appearance in the RGB space </li> <li>Then a ConvNet can be trained to classify the data by using the cluster assignment as the pseudo class label </li> <li>the ConvNet needs to learn the invariance within one class and the variance among different classes </li> <li>Therefore, the ConvNet is able to learn semantic meaning of images </li> <li>Firstly, the image is clustered into different clusters which the images from the same cluster have smaller distance and images from different clusters have larger distance </li> <li>Then a ConvNet is trained to recognize the cluster assignment [34], [44] or to recognize whether two imaged are from same cluster [43] </li> <li>DeepCluster iteratively clusters images with Kmeans and use the subsequent assignments as supervision to update the weights of the network</li> </ul>"},{"location":"KB/Learning%20with%20Labels%20Generated%20by%20Game%20Engines/","title":"Learning with Labels Generated by Game Engines","text":""},{"location":"KB/Learning%20with%20Labels%20Generated%20by%20Game%20Engines/#learning-with-labels-generated-by-game-engines","title":"Learning with Labels Generated by Game Engines","text":"<ul> <li>Given models of various objects and layouts of environments, game engines are able to render realistic images and provide accurate pixel-level labels </li> <li>Since game engines can generate large-scale datasets with negligible cost, var- ious game engines such as Airsim [142] and Carla [143] have been used to generate large-scale synthetic datasets with high-level semantic labels including depth, # contours, surface normal, segmentation mask, and optical flow for training deep networks.  </li> <li> </li> <li>However, due to the domain gap between synthetic and real-world images, the ConvNet purely trained on synthetic images cannot be </li> <li>directly applied to real-world images </li> <li>the ConvNet trained with the semantic labels of the synthetic dataset can be effectively applied to real-world images. </li> <li>Ren and Lee proposed an unsupervised feature space domain adaptation method based on adversarial learning [30] </li> <li>the network predicts surface normal, depth, and instance contour for the synthetic images and a discriminator network D is employed to minimize the difference of feature space domains between real-world and synthetic data </li> <li>the network is able to capture visual features for real-world images  </li> <li>Jing et al. proposed to learn features by training a ConvNet to predict relative scene depths while the labels are generated from optical flow [92]. </li> <li>No matter what kind of labels used to train ConvNets, the general idea of this type of methods is to distill knowledge from hard-code detector </li> <li>The hard-code detector can be edge detector, salience detector, relative detector, etc </li> <li>no human-annotations are involved </li> <li>one drawback is that the semantic labels generated by hard-code detector usually are very noisy which need to specifically cope with.</li> </ul>"},{"location":"KB/Learning%20with%20Labels%20Generated%20by%20Hard-code%20Programs/","title":"Learning with Labels Generated by Hard-code Programs","text":""},{"location":"KB/Learning%20with%20Labels%20Generated%20by%20Hard-code%20Programs/#learning-with-labels-generated-by-hard-code-programs","title":"Learning with Labels Generated by Hard-code Programs","text":"<ul> <li>Applying hard-code programs is another way to automatically generate semantic labels such as salience, foreground masks, contours, depth for images and videos </li> <li>very large-scale datasets with generated semantic labels can be used for self- supervised feature learning </li> <li>Various hard-code programs have been applied to generate labels for self- supervised learning methods include methods for foreground object segmentation [81], edge detection [47], and relative depth prediction [92] </li> <li>Pathak et al. proposed to learn features by training a ConvNet to segment foreground objects in each frame of a video while the label is the mask of moving objects in videos [81] </li> <li>Li et al. proposed to learn features by training a ConvNet for edge prediction while labels are motion edges obtained from flow fields   </li> <li>After GAN-based methods obtained breakthrough results in image generation, researchers employed GAN to generate videos [85], [86], [144] </li> <li>VideoGAN <ul> <li>To model the motion of objects in videos, a two-stream network is proposed for video generation while one stream is to model the static regions in in videos as background and another stream is to model moving object in videos as foreground </li> <li>Videos are generated by the combination of the foreground and background streams </li> <li>each random variable in the latent space represents one video clip </li> <li>Tulyakov et al. argues that this assumption increases difficulties of the generation </li> </ul> </li> <li>MocoGAN <ul> <li>use the combination of two subspace to represent a video by disentangling the # context and motions in videos [86] </li> <li>context space which each variable from this space represents one identity </li> <li>motion space while the trajectory in this space represents the motion of the identity </li> <li>With the two sub-spaces, the network is able to generate videos with higher inception score. </li> <li>The generator learns to map latent vectors from latent space into videos, while discriminator learns to distinguish the real world videos with generated videos. </li> <li>After the video generation training on large-scale unlabeled dataset finished, the parameters of discriminator can be transferred to other downstream tasks [85].</li> </ul> </li> </ul>"},{"location":"KB/Learning%20with%20Spatial%20Context%20Structure/","title":"Learning with Spatial Context Structure","text":""},{"location":"KB/Learning%20with%20Spatial%20Context%20Structure/#learning-with-spatial-context-structure","title":"Learning with Spatial Context Structure","text":"<ul> <li>Images contain rich spatial context information such as the relative positions among different patches from an image which can be used to design the pretext task for selfsupervised learning </li> <li>The pretext task can be to predict the relative positions of two patches from same image [41], or to recognize the order of the shuffled a sequence of patches from same image [20], [88], [89] </li> <li>The context of full images can also be used as a supervision signal to design pretext tasks such as to recognize the rotating angles of the whole images [36] </li> <li>ConvNets need to learn spatial context information such as the shape of the objects and the relative positions of different parts of an object.  </li> <li> </li> <li>Doersch et al. is one of the pi- (b) oneer work of using spatial context cues for self- supervised visual feature learning [41] </li> <li>Random pairs of image patches are extracted from each image, then a ConvNet is trained to recognize the relative positions of the two image patches </li> <li>ConvNets need to recognize objects in images and learn the relationships among different parts of objects </li> <li>To avoid the network learns trivial solutions such as simply using edges in patches to accomplish the task, heavy data augmentation is applied during the training phase </li> <li>Following this idea, more methods are proposed to learn image features by solving more difficult spatial puzzles [20], [27], [87], [88], [89] </li> <li>Noroozi et al. attempted to solve an image Jigsaw puzzle with ConvNet [20] </li> <li>The shuffled image patches are fed to the network which trained to recognize the correct spatial locations of the input patches by learning spatial context </li> <li>Given 9 image patches from an image, there are 362, 880 (9!) possible permutations and a network is very unlikely to recognize all of them because of the ambiguity of the task </li> <li>To limit the number of permutations, usually, hamming distance is employed to choose only a subset of permutations among all the permutations that with relative large hamming distance. </li> <li>Only the selected permutations are used to train ConvNet to recognize the permutation of shuffled image patches [20], [35], [88], [89]</li> </ul>"},{"location":"KB/Least%20squares%20loss/","title":"Least squares loss","text":""},{"location":"KB/Least%20squares%20loss/#least-squares-loss","title":"Least squares loss","text":"<ul> <li> \\[L[\\phi] = \\Sigma^{t}_{t=1}(y_{i}- f[x_{i}, \\phi])^{2}\\] </li> <li>Assumptions : independent, drawn from a Normal Distribution with mean \\(\\mu = f[x_{i}, \\phi]\\)</li> </ul>"},{"location":"KB/Left%20psuedo%20inverse/","title":"Left psuedo inverse","text":""},{"location":"KB/Left%20psuedo%20inverse/#left-psuedo-inverse","title":"Left psuedo inverse","text":"<ul> <li> \\[(A'A)^{-1}A'\\] </li> </ul>"},{"location":"KB/Lemmatization/","title":"Lemmatization","text":""},{"location":"KB/Lemmatization/#lemmatization","title":"Lemmatization","text":"<ul> <li>Word-&gt; lemma</li> <li>saw : {see, saw}</li> <li>Morphological analysis : word-&gt; set of {lemma, tag}</li> </ul>"},{"location":"KB/Length%20Optimization/","title":"Length Optimization","text":""},{"location":"KB/Length%20Optimization/#length-optimization","title":"Length Optimization","text":""},{"location":"KB/Lesion/","title":"Lesion","text":""},{"location":"KB/Lesion/#lesion","title":"Lesion","text":"<ul> <li>An injury, area of disease, or surgical incision to body tissue. Much of what we know about the functions of brain structures or pathways comes from lesion mapping studies, where scientists observe the behavior of people with an injury to a distinct area of the brain or analyze the behavior of a laboratory animal resulting from a lesion made in the brain.</li> </ul>"},{"location":"KB/Lesion/#lesion_1","title":"Lesion","text":"<ul> <li>Damage or change to tissue, such as a cut, a wound or a sore</li> </ul>"},{"location":"KB/Lexical%20Ambiguity/","title":"Lexical Ambiguity","text":""},{"location":"KB/Lexical%20Ambiguity/#lexical-ambiguity","title":"Lexical Ambiguity","text":"<ul> <li>Lexical Disambiguation</li> <li>Will will Will\u2019s will</li> <li>Buffalo buffalo Buffalo buffalo</li> <li>Rose rose roes rows</li> </ul>"},{"location":"KB/Lexical%20Disambiguation/","title":"Lexical Disambiguation","text":""},{"location":"KB/Lexical%20Disambiguation/#lexical-disambiguation","title":"Lexical Disambiguation","text":"<ul> <li>process of determining the correct meaning of an individual word</li> <li>Word sense disambiguation</li> <li>Semantic Markers</li> </ul>"},{"location":"KB/Lexical%20Word%20Segmentation/","title":"Lexical Word Segmentation","text":""},{"location":"KB/Lexical%20Word%20Segmentation/#lexical-word-segmentation","title":"Lexical Word Segmentation","text":"<ul> <li>rule-based \u2013 syntax; semantics; morphological rules</li> </ul>"},{"location":"KB/Lexically%20Collective/","title":"Lexically Collective","text":""},{"location":"KB/Lexically%20Collective/#lexically-collective","title":"Lexically Collective","text":"<ul> <li>Each knight gathered at the castle.</li> </ul>"},{"location":"KB/Lexically%20Distributive/","title":"Lexically Distributive","text":""},{"location":"KB/Lexically%20Distributive/#lexically-distributive","title":"Lexically Distributive","text":"<ul> <li>Each girl smiled</li> </ul>"},{"location":"KB/Lexicon/","title":"Lexicon","text":""},{"location":"KB/Lexicon/#lexicon","title":"Lexicon","text":"<ul> <li>a module that tells what words there are and what properties they have</li> </ul>"},{"location":"KB/LibriSpeech/","title":"LibriSpeech","text":""},{"location":"KB/LibriSpeech/#librispeech","title":"LibriSpeech","text":""},{"location":"KB/Limbic%20system/","title":"Limbic system","text":""},{"location":"KB/Limbic%20system/#limbic-system","title":"Limbic System","text":"<ul> <li>is the center of our emotions, learning, and memory.</li> <li>Included in this system are the cingulate gyri, hypothalamus, amygdala (emotional reactions) and hippocampus (memory).</li> </ul>"},{"location":"KB/Limbic%20system/#limbic-system_1","title":"Limbic System","text":"<ul> <li>A group of evolutionarily older brain structures that encircle the top of the brain stem. The limbic structures play complex roles in emotions, instincts, and appetitive behaviors.</li> </ul>"},{"location":"KB/Limited%20features/","title":"Limited features","text":""},{"location":"KB/Limited%20features/#limited-features","title":"Limited features","text":"<ul> <li>Sample size disparities</li> <li>when using sensitive features, disparities between different subgroups can induce bias</li> </ul>"},{"location":"KB/Line%20Integral%20Convolution/","title":"Line Integral Convolution","text":""},{"location":"KB/Line%20Integral%20Convolution/#line-integral-convolution","title":"Line Integral Convolution","text":"<ul> <li>Mimic physical experiment: oil drops on surface, apply flow (wind)</li> <li>Intensity distribution along KB/Streamlines.md shows high correlation</li> <li>No correlation between neighboring KB/Streamlines.md</li> <li></li> <li></li> </ul>"},{"location":"KB/Linear%20Classifier%20Probes/","title":"Linear Classifier Probes","text":""},{"location":"KB/Linear%20Classifier%20Probes/#linear-classifier-probes","title":"Linear Classifier Probes","text":"<ul> <li>Understanding Intermediate Layers Using Linear Classifier Probes</li> <li>Black box</li> <li>monitor the features at every layer of a model and measure how suitable they are for classification</li> <li>\"Probes\"</li> <li>trained entirely independently of the model</li> <li>observe experimentally that the linear separability of features increase monotonically along the depth of the model</li> </ul>"},{"location":"KB/Linear%20Interpolated%20Motion/","title":"Linear Interpolated Motion","text":""},{"location":"KB/Linear%20Interpolated%20Motion/#linear-interpolated-motion","title":"Linear Interpolated Motion","text":"<ul> <li>Is a method of path interpolation that commands the movement of the robot by moving each joint in a coordinated motion so that all axis arrive to the position at the same time. The path of the Tool Control Point (TCP) is predictable and will be linear.</li> </ul>"},{"location":"KB/Linear%20Learning%20Rate%20Scaling/","title":"Linear Learning Rate Scaling","text":""},{"location":"KB/Linear%20Learning%20Rate%20Scaling/#linear-learning-rate-scaling","title":"Linear Learning Rate Scaling","text":"<ul> <li>If [He Initialization ] is used, 0.1 is a good learning rate for batch size 256 and for a larger b, \\(0.1\\times\\frac{\\mathrm{b}}{256}\\) is okay</li> </ul>"},{"location":"KB/Linear%20scale/","title":"Linear Scale Encoding","text":""},{"location":"KB/Linear%20scale/#linear-scale-encoding","title":"Linear Scale Encoding","text":"<ul> <li>Eg Likert scale</li> <li>A = {certainly not, rather not, dont know}</li> </ul>"},{"location":"KB/Linear%20scale/#_1","title":"\u2026","text":""},{"location":"KB/LinearRegression/","title":"Linear Regression","text":""},{"location":"KB/LinearRegression/#linear-regression","title":"Linear Regression","text":"<ul> <li>Minimization problem (\\((w,b) = argmin_{w^{\\ast} , b^{\\ast}} \\Sigma^N_{i-1}(w^{\\ast}x_{i} +b ^{\\ast} - y_{i})^2\\)\\)<ul> <li>Affine Function</li> <li> \\[w = argmin_{w^\\ast} || (\\Sigma^n_{j = 1} w^\\ast_j \\phi_j) - y || ^2\\] <ul> <li>w* is just \\(\\((w^\\ast_1 \u2026 w^\\ast_n)\\)\\)</li> <li> \\[u_j$$ of U forms orthonormal basis of $$\\mathscr{F}\\] </li> </ul> </li> </ul> </li> <li>X : nxN matrix , y : N dim vector</li> <li>Solution : \\(\\([w, b] \\in \\mathbb{R} ^{n+1}\\)\\)</li> <li> \\[w' = (XX')^{-1}X y\\] <ul> <li>If y is has vector data too (size k)<ul> <li> \\[W' = (XX')^{-1}XY\\] </li> <li>Y : N x k matrix</li> </ul> </li> </ul> </li> <li></li> <li>(\\(\\phi _1 , \\phi_2 \u2026\\)\\) form a subspace (\\(\\mathscr{F}\\)\\) with dim = n<ul> <li>linearly independant vectors. If not, drop as many as possible</li> </ul> </li> <li>The optimal solution y_opt is the projection of y on that subspace and has the smallest distance from y<ul> <li> \\[y_{opt} = w_1 \\phi_1 + w_2 \\phi_2\\] </li> </ul> </li> <li> \\[(\\Sigma^n_{j = 1} w^\\ast_j \\phi_j)$$ is a vector on $$\\mathscr{F}\\] </li> <li>Ridge Regression</li> <li>Window Based Regression</li> </ul>"},{"location":"KB/LinearRegression/#general-defination","title":"General Defination","text":"<ul> <li>Training data : \\(\\((x_i, y_i)_{i= 1,..,N}\\)\\) and \\(\\(\\in \\mathbb{R}^k\\)\\)</li> <li>Search space H<ul> <li>Candidate functions \\(\\(h: \\mathbb{R}^n \\rightarrow \\mathbb{R}^k\\)\\)</li> </ul> </li> <li>Loss function (\\(L : \\mathbb{R}^k \\times \\mathbb{R}^k \\rightarrow \\mathbb{R}^{n \\geq 0}\\)\\)<ul> <li>Quadratic Loss</li> </ul> </li> <li> <p>Solution : \\(\\(h_{opt} = argmin_{h \\in \\mathcal{H}}\\Sigma_{i=1}^N L(h(x_i), y_i)\\)\\)</p> <ul> <li> \\[\\mathcal{H}$$ is all linear functions from $$\\mathbb{R}^n$$ to $$\\mathbb{R}^k\\] </li> </ul> </li> <li> <p>Left psuedo inverse</p> </li> </ul>"},{"location":"KB/Linguistic%20details/","title":"Linguistic details","text":""},{"location":"KB/Linguistic%20details/#linguistic-details","title":"Linguistic Details","text":"<ul> <li>Phonetics</li> <li>Phonology</li> <li>Morphology</li> <li>Syntactic Analysis</li> <li>Semantic Analysis</li> <li>Pragmatics</li> </ul>"},{"location":"KB/Lisht/","title":"Lisht","text":""},{"location":"KB/Lisht/#lisht","title":"Lisht","text":"<ul> <li>Derivatives<ul> <li></li> </ul> </li> <li>blog #Roam-Highlights<ul> <li>Linearly Scaled Hyperbolic Tangent</li> <li>his activation function simply uses the Tanh function and scales it linearly, as follows</li> <li> \\[LiSHT(x) = x \\times tanh(x)\\] </li> <li>Essentially, LiSHT looks very much like Swish in terms of the first-order derivative. However, the range is expanded into the negative as well, which means that the vanishing gradient problem is reduced even further - at least in theory.</li> <li>In their work, Roy et al. (2019) report based on empirical testing that indeed, the vanishing gradient problems is reduced compared to Swish and traditional ReLU. Additional correlations between network learning and the shape of e.g. the LiSHT loss landscape were identified.</li> </ul> </li> </ul>"},{"location":"KB/Listen%20Attend%20Spell/","title":"Listen Attend Spell","text":""},{"location":"KB/Listen%20Attend%20Spell/#listen-attend-spell","title":"Listen Attend Spell","text":"<ul> <li>Listen, Attend and Spell</li> <li>LAS</li> <li>learns to transcribe speech utterances to characters</li> <li>nlike traditional DNN-HMM models, this model learns all the components of a speech recognizer jointly</li> <li>sequence-to-sequence framework</li> <li>trained end-to-end and has two main components: a listener (encoder) and a speller (decoder)</li> <li>listener is a pyramidal RNN encoder that accepts filter bank spectra as inputs, transforms the input sequence into a high level feature representation and reduces the number of timesteps that the decoder has to attend to.</li> <li>The speller is an attention-based RNN decoder that attends to the high level features and spells out the transcript one character at a time</li> <li>The proposed system does not use the concepts of phonemes, nor does it rely on pronunciation dictionaries or HMMs</li> <li>bypass the conditional independence assumptions of CTC, and show how they can learn an implicit language model that can generate multiple spelling variants given the same acoustics</li> <li>producing character sequences without making any independence assumptions between the characters is the key improvement of LAS over previous end-to-end CTC models</li> <li>used samples from the softmax classifier in the decoder as inputs to the next step prediction during training</li> <li>show how a language model trained on additional text can be used to rerank their top hypotheses</li> <li>Google voice search task</li> </ul>"},{"location":"KB/Load%20Cycle%20Time/","title":"Load Cycle Time","text":""},{"location":"KB/Load%20Cycle%20Time/#load-cycle-time","title":"Load Cycle Time","text":"<ul> <li>A manufacturing or assembly line process term, which describes the complete time to unload the last work piece and load the next one.</li> </ul>"},{"location":"KB/Load%20balancing/","title":"Load balancing","text":""},{"location":"KB/Load%20balancing/#load-balancing","title":"Load Balancing","text":"<ul> <li>divide the work equally among the available processors</li> </ul>"},{"location":"KB/Local%20Descriptor/","title":"Local Descriptor","text":""},{"location":"KB/Local%20Descriptor/#local-descriptor","title":"Local Descriptor","text":"<ul> <li>set of spin-images</li> <li>A spin-image feature is computed for every keypoint:</li> <li>The tangent plane is estimated</li> <li>A 2D histogram is computed along the a and b dimensions in the neighborhood of the keypoint</li> <li>Spin-image represents a small area of an object around a specific keypoints</li> <li></li> <li></li> </ul>"},{"location":"KB/Local%20Reference%20Frame/","title":"Local Reference Frame","text":""},{"location":"KB/Local%20Reference%20Frame/#local-reference-frame","title":"Local Reference Frame","text":"<ul> <li>Three principal axes of a given object's point cloud are firstly determined based on eigenvectors analysis (PCA)</li> <li></li> </ul>"},{"location":"KB/Local-LDA%20Object%20Representation/","title":"Local-LDA Object Representation","text":""},{"location":"KB/Local-LDA%20Object%20Representation/#local-lda-object-representation","title":"Local-LDA Object Representation","text":"<ul> <li>A variant of Latent Dirichlet Allocation (Local-LDA)</li> <li>learn structural semantic features (i.e. topics) from low-level feature cooccurrences for each category independently and incrementally.</li> <li></li> </ul>"},{"location":"KB/Localist%20units/","title":"Localist units","text":""},{"location":"KB/Localist%20units/#localist-units","title":"Localist Units","text":"<ul> <li>In a localist representation, a localist unit (neuron) is most active to one meaningful category</li> <li>In Input: when we build a network and let each input unit represent a a specific word</li> <li>In Output: when we allow outputs of single unit to be interpreted</li> <li>In the hidden units: is there evidence of localist encoding developing?</li> </ul>"},{"location":"KB/Locality/","title":"Locality","text":""},{"location":"KB/Locality/#locality","title":"Locality","text":"<ul> <li>Localist units</li> <li>In order to process an image, we start by capturing the local information. One way to do that is the use of a convolutional layer. It can capture the local relationship between the pixels of an image. Then, as we go deeper in the model, the local feature extractors help to extract the global features</li> <li></li> </ul>"},{"location":"KB/Location%20Aware%20Attention/","title":"Location Aware Attention","text":""},{"location":"KB/Location%20Aware%20Attention/#location-aware-attention","title":"Location Aware Attention","text":"<ul> <li>Chorowski et al., 2015</li> </ul>"},{"location":"KB/Location%20Base%20Attention/","title":"Location Base Attention","text":""},{"location":"KB/Location%20Base%20Attention/#location-base-attention","title":"Location Base Attention","text":"<ul> <li>Luong2015</li> <li>Attention Alignment score \\(\\alpha_{t,i} = softmax(W_{\\alpha}s_{t})\\)</li> </ul>"},{"location":"KB/Log%20Likelihood%20Loss/","title":"Log Likelihood Loss","text":""},{"location":"KB/Log%20Likelihood%20Loss/#log-likelihood-loss","title":"Log Likelihood Loss","text":"<ul> <li> \\[L(U) = \\Sigma_i log P(u_i| u_{i-k} ,\u2026, u_{i-1} )\\] </li> <li>k is size of context window of past tokens</li> </ul>"},{"location":"KB/Log%20likelihood%20criterion/","title":"Log likelihood criterion","text":""},{"location":"KB/Log%20likelihood%20criterion/#log-likelihood-criterion","title":"Log likelihood criterion","text":"<ul> <li>Since the product of these terms can be tiny, we can effectively maximize the log of the likelihood </li> <li>This is equivalent to the previous one</li> <li>overall maxima of the two criteria must be in the same place, so the best model parameters \u03c6\u02c6 are the same in both cases</li> <li></li> </ul>"},{"location":"KB/Log-odds/","title":"Log odds","text":""},{"location":"KB/Log-odds/#log-odds","title":"Log-odds","text":"<ul> <li>The logarithm of the odds of some event.</li> <li>If the event refers to a binary probability, then odds refers to the ratio of the probability of success (p) to the probability of failure (1-p).</li> </ul>"},{"location":"KB/LogCosh/","title":"Log Cosh","text":""},{"location":"KB/LogCosh/#log-cosh","title":"Log Cosh","text":"<ul> <li>works like the MSE, but is smoothed towards large errors (presumably caused by outliers) so that the final error score isn\u2019t impacted thoroughly.</li> </ul> <p>We first define the Softplus function \\(\\(\\log\\left( e^{x} + 1 \\right)\\)\\)</p> <p>Then , \\(\\(x = \u0177 - y\\)\\)</p> <p>logcosh = \\(\\(\\mathrm{mean}\\left( x + \\mathrm{softplus}\\left( -2 \\cdot x \\right) - \\log\\left( 2.0 \\right) \\right)\\)\\)</p>"},{"location":"KB/LogCosh/#_1","title":"\u2026","text":""},{"location":"KB/Logarithm/","title":"Logarithm","text":""},{"location":"KB/Logarithm/#logarithm","title":"Logarithm","text":"<ul> <li>Logarithms are sort of a measure of the \u201cbigness\u201d of a number; 1\u201310 is small (say, 0..1), 10\u2013100 are medium (1..2), 100\u20131000 are big (2..3). But, it makes a pretty huge difference if we\u2019re thinking about\u00a0\\(log(x)\\)\u00a0with\u00a0\\(x\\)\u00a0between 1 and 10, or with\u00a0\\(x\\)\u00a0above 10, or with\u00a0\\(x\\)\u00a0less than 1. An\u00a0\\(x\\)\u00a0between zero and 1 turns into a negative number</li> </ul>"},{"location":"KB/Logits/","title":"Logits","text":""},{"location":"KB/Logits/#logits","title":"Logits","text":"<ul> <li>The vector of raw (non-normalized) predictions that a classification model generates, which is ordinarily then passed to a normalization function.</li> </ul>"},{"location":"KB/Long%20Short%20Term%20Memory%20%28LSTM%29/","title":"Long Short Term Memory (LSTM)","text":""},{"location":"KB/Long%20Short%20Term%20Memory%20%28LSTM%29/#long-short-term-memory-lstm","title":"Long Short Term Memory (LSTM)","text":"<ul> <li>Smaller chance of exploding or vanishing #gradients</li> <li>Better ability to model long term dependencies</li> <li>Gated connections</li> <li>Gates that learn to forget some aspects, and remember others better</li> <li>Splitting state into parts -&gt; output pred and feature learning</li> <li>At the end of the day, these could not handle too long sequences. Therefore -&gt; Transformer</li> </ul>"},{"location":"KB/Long%20Short%20Term%20Memory%20%28LSTM%29/#the-math","title":"The Math","text":"<ul> <li>Gates<ul> <li>Forget (\\(g_f = \\sigma(W_{hf}h_{t-1} + W_{xf}x_t + b_f)\\)\\)<ul> <li>How much of the previous cell state is used</li> </ul> </li> <li>Input (\\(g_i = \\sigma(W_{hi}h_{t-1} + W_{xi}x_t + b_i)\\)\\)<ul> <li>How proposal is added to the state</li> </ul> </li> <li>Output (\\(g_o = \\sigma(W_{ho}h_{t-1} + W_{xo}x_t + b_o)\\)\\)<ul> <li>Component wise products</li> </ul> </li> </ul> </li> <li>Hidden state<ul> <li>(\\(C_t\\)\\) to model cross timestep dependencies<ul> <li>Cell state proposal : \\(\\(\\hat C = tanh(W_{hc}h_{t-1} + W_{xc}x_t + b_c)\\)\\)</li> <li>Final cell state : \\(\\(C_t = g_f \\cdot C_{t-1} + g_i\\cdot \\hat C\\)\\)</li> </ul> </li> <li>(\\(h_t\\)\\) to predict output<ul> <li> \\[h_t = g_o \\cdot \\sigma_y(C_t)\\] </li> </ul> </li> </ul> </li> </ul>"},{"location":"KB/Long%20Term%20Potentiation%20%28LTP%29/","title":"Long Term Potentiation (LTP)","text":""},{"location":"KB/Long%20Term%20Potentiation%20%28LTP%29/#long-term-potentiation-ltp","title":"Long Term Potentiation (LTP)","text":"<ul> <li>The persistent strengthening of a synapse with increased use, thought to underlie learning and memory.</li> </ul>"},{"location":"KB/Longformer/","title":"Longformer","text":""},{"location":"KB/Longformer/#longformer","title":"Longformer","text":"<ul> <li>Longformer: the Long-Document Transformer</li> <li>Transformer</li> <li>Sliding Window Attention</li> <li>Dilated Sliding Window Attention</li> <li>Global and Sliding Window Attention</li> <li>attention mechanism that scales linearly with sequence length</li> <li>drop-in replacement for the standard self-attention</li> <li>local windowed attention with a task motivated global attention</li> <li>text8</li> <li>enwik8</li> <li>consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA</li> <li>Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset</li> </ul>"},{"location":"KB/Loop%20Tiling/","title":"Loop Tiling","text":""},{"location":"KB/Loop%20Tiling/#loop-tiling","title":"Loop Tiling","text":"<ul> <li>Hardware memory layout in consideration</li> </ul>"},{"location":"KB/Loss%20for%20binary%20classification/","title":"Loss for binary classification","text":""},{"location":"KB/Loss%20for%20binary%20classification/#loss-for-binary-classification","title":"Loss for binary classification","text":"<ul> <li>Two classes</li> <li>Probability distribution - Bernoulli Distribution</li> <li>Training -&gt; Model to predict \\(\\lambda\\), but these values might not lie in [0,1] and we need it to. -&gt; Sigmoid</li> <li>Cross Entropy</li> <li></li> <li>This represents the probability that y = 1, and it follows that 1 \u2212 \u03bb represents the probability that y = 0. When we perform inference, we may want a point estimate of y, so we set y = 1 if \u03bb &gt; 0.5 and y = 0 otherwise.</li> <li></li> </ul>"},{"location":"KB/Loss%20for%20multiclass%20classification/","title":"Loss for multiclass classification","text":""},{"location":"KB/Loss%20for%20multiclass%20classification/#loss-for-multiclass-classification","title":"Loss for multiclass classification","text":"<ul> <li>Map \\(x\\) to one of \\(K &gt; 2\\) classes</li> <li>Distribution - Categorical Distribution</li> <li>The parameters are constrained to take values between zero and one, and they must collectively sum to one to ensure a valid probability distribution \\(\\(Pr(y=k) = \\lambda_{k}\\)\\)</li> <li>Network computes \\(K\\) params from input x</li> <li>Since the output of the network does not conform to the format, we use Softmax so results are positive, and the K numbers sum to one</li> <li></li> <li>Loss function : Negative Log Likelihood</li> <li>This is then - Cross Entropy</li> <li></li> </ul>"},{"location":"KB/Loss%20for%20univariate%20regression/","title":"Loss for univariate regression","text":""},{"location":"KB/Loss%20for%20univariate%20regression/#loss-for-univariate-regression","title":"Loss for univariate regression","text":"<ul> <li>Predict a single scalar output \\(y \\in \\mathbb{R}\\)</li> <li>Use univariate Normal Distribution <ul> <li></li> </ul> </li> <li></li> <li></li> <li>Find parameters \\(\\hat \\phi\\) that minimize \\(L[\\phi]\\)\\</li> <li>Least squares loss</li> </ul>"},{"location":"KB/Loss%20for%20univariate%20regression/#inference","title":"Inference","text":"<ul> <li>Predict the mean \\(\\mu = f[x, \\phi]\\) of the Normal Distribution over y</li> <li>We find the single best point estimate \\(\\hat y\\) and we take max of the predicted distribution \\(\\(\\hat y = \\underset{y}{argmax}[Pr(y|f|x, \\hat \\phi, \\sigma^{2})] = f[x, \\hat \\phi]\\)\\)</li> </ul>"},{"location":"KB/Loss%20for%20univariate%20regression/#estimating-if-variance-constant-everywhere","title":"Estimating if variance constant everywhere","text":"<ul> <li>Homoscedatic</li> <li>Since the equation does not depend on variance, we pretend \\(\\sigma^{2}\\) is a learned parameter and minimize it wrt \\(\\phi, \\sigma^{2}\\)</li> <li></li> <li></li> </ul>"},{"location":"KB/Loss%20for%20univariate%20regression/#estimating-if-variance-is-not-constant","title":"Estimating if variance is not constant","text":"<ul> <li>Heteroscedatic </li> <li>Train a network that computes both mean and variance</li> <li>Variance should be positive, but the result of composing networks might not be. To make it, pass it through the squaring function</li> <li></li> </ul>"},{"location":"KB/Loss%20for%20univariate%20regression/#homoscedatic-vs-heteroscedatic-regression","title":"Homoscedatic vs Heteroscedatic Regression","text":""},{"location":"KB/Lost%20in%20the%20Middle%20How%20Language%20Models%20Use%20Long%20Contexts/","title":"Lost in the Middle How Language Models Use Long Contexts","text":"","tags":["llm"]},{"location":"KB/Lost%20in%20the%20Middle%20How%20Language%20Models%20Use%20Long%20Contexts/#lost-in-the-middle-how-language-models-use-long-contexts","title":"Lost in the Middle How Language Models Use Long Contexts","text":"","tags":["llm"]},{"location":"KB/Lost%20in%20the%20Middle%20How%20Language%20Models%20Use%20Long%20Contexts/#summary","title":"Summary","text":"<p>When we feed LLMs with a long context, they tend to overlook the documents placed in the middle. So, contrary to what one might think, placing the least similar documents at the bottom isn't the best strategy. So, we should put the least similar ones in the middle, not at the bottom.</p>","tags":["llm"]},{"location":"KB/Lost%20in%20the%20Middle%20How%20Language%20Models%20Use%20Long%20Contexts/#findings","title":"Findings","text":"<ul> <li>performance can degrade significantly when changing the position of relevant information, indicating that current language models do not robustly make use of information in long input contexts.</li> <li>performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts, even for explicitly long-context models </li> <li>Encoder-decoder models are relatively robust to changes in the position of relevant information within their input context, but only when evaluated on sequences within its trainingtime sequence length. When evaluated on sequences longer than those seen during training, we observe a U-shaped performance curve</li> <li>Query-aware contextualization (placing the query before and after the documents or keyvalue pairs) enables near-perfect performance on the synthetic key-value task, but minimally changes trends in multi-document QA</li> <li>Even base language models (i.e., without instruction fine-tuning) show a U-shaped performance curve as we vary the position of relevant information in the input context.</li> <li>model performance saturates long before retriever recall saturates, indicating that current models fail to effectively use additional retrieved documents---using 50 documents instead of 20 retrieved documents only marginally improves performance (\u223c1.5% for GPT-3.5-Turbo and \u223c1% for claude-1.3).</li> </ul>","tags":["llm"]},{"location":"KB/Lost%20in%20the%20Middle%20How%20Language%20Models%20Use%20Long%20Contexts/#results","title":"Results","text":"<ul> <li>Our experimental setup is similar to the needlein-a-haystack experiments of Ivgi et al. (2023), who compare question answering performance when the relevant paragraph is placed (i) at the beginning of the input or (ii) a random position within the input. They find that encoder-decoder models have significantly higher performance when relevant information is placed at the start of the input context. In contrast, we study finer-grained changes in the position of relevant information.  </li> </ul>","tags":["llm"]},{"location":"KB/Low-rank%20factorization/","title":"Low-rank factorization","text":""},{"location":"KB/Low-rank%20factorization/#low-rank-factorization","title":"Low-rank Factorization","text":"<ul> <li>These methods identify re- dundant parameters of deep neural networks by em- ploying the matrix and tensor decomposition (Yu et al., 2017; Denton et al., 2014).</li> </ul>"},{"location":"KB/Lp%20Regularization/","title":"Lp Regularization","text":""},{"location":"KB/Lp%20Regularization/#lp-regularization","title":"Lp Regularization","text":"<ul> <li>Tikhonov</li> <li>Penalty considering weights</li> <li> \\[L^\\ast(\\theta) = L(\\theta) + \\lambda \\Sigma_i |\\theta_i|^p\\] <ul> <li>Lasso<ul> <li>p = 1</li> <li>Sparse</li> <li>With linear model : feature selection</li> </ul> </li> <li>Weight Decay<ul> <li>p = 2</li> <li>Bayesian</li> <li>Encourages optimization trajectory perpendicular to isocurves</li> <li></li> </ul> </li> </ul> </li> <li>Tune (\\(\\lambda\\)\\)<ul> <li>Grid search : log scale</li> <li>Too large : underfit, too small : overfit</li> <li>Cross Validation required</li> </ul> </li> </ul>"},{"location":"KB/Lumbar%20Puncture%20or%20Spinal%20Tap/","title":"Lumbar Puncture or Spinal Tap","text":""},{"location":"KB/Lumbar%20Puncture%20or%20Spinal%20Tap/#lumbar-puncture-or-spinal-tap","title":"Lumbar Puncture or Spinal Tap","text":"<ul> <li>Drawing of cerebrospinal fluid from the lumbar region of the back using a hollow needle</li> </ul>"},{"location":"KB/MAE/","title":"MAE","text":""},{"location":"KB/MAE/#mae","title":"MAE","text":"<ul> <li>loss converges to median of targets</li> <li>issues if gradient is close to 0</li> <li> \\[L(y, \\hat y) = n^{-1}\\Sigma_{i}|\\hat y _{i} -y_{i}|\\] </li> </ul>"},{"location":"KB/MAPE/","title":"MAPE","text":""},{"location":"KB/MAPE/#mape","title":"MAPE","text":"<ul> <li>mean absolute % error</li> </ul> \\[\\frac{1}{\\mathrm{length}\\left( y \\right)} \\cdot \\mathrm{sum}\\left( \\left\\|\\frac{y - \u0177}{y}\\right\\| \\right)\\]"},{"location":"KB/MCMC%20Sampling/","title":"MCMC Sampling","text":""},{"location":"KB/MCMC%20Sampling/#mcmc-sampling","title":"MCMC Sampling","text":"<ul> <li>Complex distribution with only a proto PDF \\(g_{0}\\) that is known</li> <li>Markov Chain</li> <li>Detailed Balance<ul> <li>Sufficient but not necessary for Markov Chain to be a Sampler for g</li> </ul> </li> <li>Factors for MC estimate</li> </ul>"},{"location":"KB/MCnet/","title":"MCnet","text":""},{"location":"KB/MCnet/#mcnet","title":"MCnet","text":"<ul> <li>Encoder-Decoder Convolutional Neural Network and Convolutional LSTM for video prediction </li> <li>two encoders, one is Content Encoder to capture the spatial layout of an image, and the other is Motion Encoder to model temporal dynamics within video clips. </li> <li>The spatial features and temporal features are concatenated to feed to the decoder to generate the next frame </li> <li>separately modeling temporal and spatial features, this model can effectively generate future frames recursively. </li> <li>Videos consist of various lengths of frames which have rich spatial and temporal information </li> <li>inherent temporal information within videos can be used as supervision signal for self-supervised feature learning </li> <li>pretext tasks have been proposed by utilizing temporal context relations including temporal order verification [29], [40], [90] and temporal order recognition [27], [39]</li> </ul>"},{"location":"KB/MILAN/","title":"MILAN","text":""},{"location":"KB/MILAN/#milan","title":"MILAN","text":"<ul> <li>Natural Language Descriptions of Deep Visual Features</li> <li>Some neurons in deep networks specialize in recognizing highly specific perceptual, structural, or semantic features of inputs</li> <li>identifying neurons that respond to individual concept categories</li> <li>richer characterization of neuron-level computation</li> <li>mutual-information-guided linguistic annotation of neurons</li> <li>generate open-ended, compositional, natural language descriptions of individual neurons in deep networks</li> <li>generates a description by searching for a natural language string that maximizes pointwise mutual information with the image regions in which the neuron is active</li> <li>MILANNOTATIONS</li> <li>fine-grained descriptions that capture categorical, relational, and logical structure in learned features</li> <li>characterizing the distribution and importance of neurons selective for attribute, category, and relational information in vision models.</li> <li>auditing, surfacing neurons sensitive to protected categories like race and gender in models trained on datasets intended to obscure these features</li> <li>editing, improving robustness in an image classifier by deleting neurons sensitive to text features spuriously correlated with class labels</li> </ul>"},{"location":"KB/MILANNOTATIONS/","title":"MILANNOTATIONS","text":""},{"location":"KB/MILANNOTATIONS/#milannotations","title":"MILANNOTATIONS","text":"<ul> <li>a dataset of fine-grained image annotations</li> </ul>"},{"location":"KB/MIMD/","title":"MIMD","text":""},{"location":"KB/MIMD/#mimd","title":"MIMD","text":"<ul> <li>Multiple instruction, multiple data</li> <li>Synchronous/Async , deterministic/non deterministic</li> <li>Most supercomputers</li> <li>Grids</li> <li>Multi processor SMP computers</li> <li>Also include SIMD sub components</li> <li></li> </ul>"},{"location":"KB/MISD/","title":"MISD","text":""},{"location":"KB/MISD/#misd","title":"MISD","text":"<ul> <li>Multiple instructions on single data</li> <li>Real time computers need to be fault tolerant where several processors execute the same data for producing the redundant data</li> <li>N-version programming</li> <li></li> </ul>"},{"location":"KB/MIT1003/","title":"MIT1003","text":""},{"location":"KB/MIT1003/#mit1003","title":"MIT1003","text":"<ul> <li>\"779 landscape images and 228 portrait images.\"</li> <li>The fixations were measured while 15 observers looked at an image for 3 s.</li> </ul>"},{"location":"KB/MIT300/","title":"MIT300","text":""},{"location":"KB/MIT300/#mit300","title":"MIT300","text":"<ul> <li>was the first data set with held-out human eye movements and is used as benchmark test data in MIT Saliency Benchmark</li> <li>\"300 natural\"</li> <li>The fixations were measured while 39 observers looked at an image for 3 s.</li> <li>\"indoor and outdoor scenes.\"</li> </ul>"},{"location":"KB/ML%20Production%20Flow/","title":"ML Production Flow","text":"","tags":["mlops"]},{"location":"KB/ML%20Production%20Flow/#ml-production-flow","title":"ML Production Flow","text":"<ul> <li>Project Setup</li> <li>Data Pipeline</li> <li>Modelling and training</li> <li>Serving</li> </ul>","tags":["mlops"]},{"location":"KB/MLCompany/","title":"MLCompany","text":""},{"location":"KB/MLCompany/#mlcompany","title":"MLCompany","text":"<ul> <li>Talent search : Rach\u00e8l Post : rpost@micompany.nl</li> </ul>"},{"location":"KB/MLCompany/#about-the-company","title":"About the Company","text":"<ul> <li>Accelerate and scale impact from AI.\u200b</li> <li>People and machines working together is already rocketing companies to new highs. This takes change and effort. But we\u2019re here to make it effortless for you.</li> <li>Build a globally defined and unified model production line. Enable model governance, in line with regulations.</li> <li>We\u2019ll help build up your team with AI skills in no time : train-the-trainer</li> <li>To achieve impact with Algorithms, we need to start with rethinking how you can change the way you do business. How more and better predictions can help changing the way you serve your customers, and run your operations. \u200b</li> <li>Predictions that can support or replace the decision making of your people. Through integrating these predictions into new digital applications, or into your existing system landscape.\u200b</li> <li>In both scenario\u2019s we go beyond the proof of concept and focus on AI that runs operational. Through connecting data, algorithms and applications we make your organization run smarter.\u200b</li> </ul>"},{"location":"KB/MLCompany/#inventory-management","title":"Inventory Management","text":"<ul> <li>AI in Supply Chain: A Novel Risk-based Approach to Inventory Management \u2013 MIcompany</li> <li>Inventory management, a critical component of the supply chain, involves ensuring that the right products are in the right place at the right time.</li> <li>In this article, we will present a novel approach to Inventory Management demand prediction which incorporates overstock costs and under-order costs into the decision-making process, as a complement to machine-learning time-series modeling</li> <li>While cost minimization lies at the core of this approach, supply chain managers may further customize this model to better address corporate goals and targets</li> <li>Inaccuracies in high-variance fast-moving items are understandable, and near-perfect accuracy is unattainable by even the best of models, so for a company whose inventory centers around fast-moving items, even generic trend predictions have the potential to make a sizable impact, and some AI-oriented startups claim to have made progress on that front.</li> <li>the forecast model with the highest possible accuracy is not necessarily that which is best for the business</li> <li>On the one hand, a business must have a comprehensive understanding of the possible costs it faces, which requires deep business and domain expertise.</li> <li>achieving near-optimal accuracy levels is nonetheless crucial, as a poor demand prediction model will impact all items alike</li> <li>The risk-based approach to inventory management utilizes over-ordering costs and under-ordering costs in the inventory level decision-making process.</li> <li>For instance, if an aircraft is sidelined due to engine failure and the airline does not have the exact engine model on hand, the airline will have no choice but to make an urgent shipment, often at a cost which may be 10-15% higher than the typical, non-urgent shipment; this is a prime example of an understock cost. Conversely, if the supplier orders too many engines which end up unused, they may be ultimately sold at a loss or written off entirely, in addition to the cost of capital incurred from ordering unnecessary items; both are considered over-ordering costs.</li> <li>By combining statistical methods and the industry\u2019s domain expertise regarding inventory costs, this system predicts the stock level which minimizes expected inventory costs.</li> </ul>"},{"location":"KB/MLCompany/#about","title":"About","text":"<ul> <li>About MIcompany \u2013 MIcompany</li> <li>ECG Analytics University</li> <li>Together with eBay, we are building the AI skills for a large and diverse group of employees to apply the power of AI and data</li> <li>KPN</li> <li>jointly built AI applications to optimize its network investments in DSL, fiber, and 5G.</li> <li>LeasePlan</li> <li>built a new AI platform and system that can value and price used cars more reliable than ever.</li> <li>change is not a cool AI-project or algorithm, but rather, it is about driving breakthroughs in directions that will last for many decades and leverage the potential of AI.</li> <li>We believe that to grasp the full potential of AI, a fundamental redesign of key processes is needed. This is a transformation that requires both a new way of working and new skill sets. We adopt a highly curious learning and highly practical doing mindset. Through our academy, we support our clients in building the skills required.</li> <li>MIcompany aims to push the good from AI by inspiring our people and clients. AI can make our life easier, more meaningful and healthier. We inspire our people and companies to capture these opportunities by identifying and capturing new application areas for AI.</li> </ul>"},{"location":"KB/MLCompany/#work","title":"Work","text":"<ul> <li>Diverse work with many different clients </li> <li>All-year coaching and trainings to really make you the best </li> <li>Nice people and an employee-centric organization</li> </ul>"},{"location":"KB/MLDoc/","title":"MLDoc","text":""},{"location":"KB/MLDoc/#mldoc","title":"MLDoc","text":""},{"location":"KB/MLIM/","title":"MLIM","text":""},{"location":"KB/MLIM/#mlim","title":"MLIM","text":"<ul> <li>MLIM: Vision-and-language Model Pre-training with Masked Language and Image Modeling</li> <li>Vision-and-Language Pre-training (VLP) improves model performance for downstream tasks that require image and text inputs</li> <li>Typically, in addition to the [Masked Language Modeling] (MLM) loss, alignment-based objectives are used for cross-modality interaction, and RoI feature regression and classification tasks for Masked ImageRegion Modeling (MIRM)</li> <li>Alignment-based objectives require pairings of image and text and heuristic objective functions</li> <li>Masking policies either do not take advantage of multi-KB/Modality.md or are strictly coupled with alignments generated by other models</li> <li>pre-trained using two pre-training tasks as a multi-loss objective given a mini-batch of image-text pairs: [Masked Language Modeling] (MLM) loss (as in BERT) for text, and image reconstruction (RECON) loss for image, coupled with Modality Aware Masking (MAM)</li> <li>determines the masking probability and applies masking to both word and image embedding</li> <li>based on BERT predict the masked words from available words and image regions</li> <li>follow BERT for this task: two-layer MLP MLM head outputting logits over the vocabulary</li> <li>MLM loss is negative log-likelihood for masked word</li> <li>RECON loss is an an average of pixel-wise sum of squared errors (SSE)</li> <li>Both image and word masking is realized by replacing an embedding with the embedding of <code>[MASK]</code></li> <li>transformer layers recognize <code>[MASK]</code></li> <li>\u2019s embedding as a special embedding that needs to be \u201cfilled in\u201d, independent of the KB/Modality.md, by attending to other vectors in the layer inputs</li> <li>unlike other architectures (LXMERT, UNiTER, ViLBERT, VLP, VL-BERT, VisualBERT, etc.), image masking is not based on image regions detected by the object detector, but a shallow CNN as an image embedder which is much more lightweight than deep models like ResNet and is designed to be masking friendly</li> <li>MLM + RECON losses apply only to the masked text/image areas and measure reconstructed text and image quality.</li> <li>no specific alignment loss</li> <li>[Modality] Aware Masking (MAM) to boost cross-modality interaction and take advantage of MLM and RECON losses that separately capture text and image reconstruction quality</li> <li>Since the the task of finding closely-matching (CM) item pairs requires a pair of image+text inputs, they exploit this multi-KB/Modality.md by employing Modality Dropout</li> <li>text-only, image-only, and image-text mode</li> <li>However, RECON instead of ITM loss offers better PR AUC</li> <li>Similarly, using the ITM loss together with MLM and RECON does not change the performance</li> </ul>"},{"location":"KB/MLM/","title":"MLM","text":""},{"location":"KB/MLM/#mlm-masked-language-modeling","title":"MLM (Masked Language Modeling)","text":"<ul> <li>from</li> <li>15% of the words in each sequence are replaced by\u00a0<code>[MASK]</code></li> <li>model tries to predict original values of the masked words</li> <li>uses the context provided by the other non-masked words in the sequences</li> <li>loss function only considers the predictions of the masked words, ignores non-masked ones<ul> <li>leads to slower convergence than with directional models</li> </ul> </li> <li>additions to standard architecture:<ul> <li>classification layer on top of the encoder output</li> <li>multiplying the encoders output vectors with the embedding matrix -&gt; transforms them into the vocabulary dimension</li> <li>calculating probability of each word in the vocabulary using\u00a0Softmax</li> </ul> </li> </ul>"},{"location":"KB/MLOps%20Learning/","title":"MLOps","text":"","tags":["mlops"]},{"location":"KB/MLOps%20Learning/#mlops-learning","title":"MLOps Learning","text":"","tags":["mlops"]},{"location":"KB/MLOps%20Learning/#list-of-things-i-want-to-learn","title":"List of Things I Want to Learn","text":"<ul> <li>This is as of 19th oct 2023 : 2023_10_19 </li> <li>More git commands that would be useful</li> <li>SQL + nosql </li> <li>REST API</li> <li>Kubernets/Airflow</li> <li>Unit testing in python</li> <li>https://fullstackdeeplearning.com/course/2022/</li> </ul>","tags":["mlops"]},{"location":"KB/MMLU/","title":"MMLU","text":""},{"location":"KB/MMLU/#mmlu","title":"MMLU","text":""},{"location":"KB/MNIST/","title":"MNIST","text":""},{"location":"KB/MNIST/#mnist","title":"MNIST","text":"<ul> <li>10 classes</li> <li>2 channels</li> <li>dataset consisting of handwritten digits</li> <li>28x28 pixels</li> <li>70'000 images</li> </ul>"},{"location":"KB/MRI/","title":"MRI","text":""},{"location":"KB/MRI/#mri","title":"MRI","text":"<ul> <li>Studies brain anatomy</li> <li>1 image</li> <li>1mm</li> <li>1) Put subject in big magnetic field (leave him there)  </li> <li>2) Transmit radio waves into subject [about 3 ms]  </li> <li>3) Turn off radio wave transmitter  </li> <li>4) Receive radio waves re-transmitted by subject  <ul> <li>Manipulate re-transmission with magnetic fields during this readout interval [10-100 ms: MRI is not a snapshot]  </li> </ul> </li> <li>5) Store measured radio wave data vs. time  <ul> <li>Now go back to 2) to get some more data  </li> </ul> </li> <li>Process raw data to reconstruct images  </li> <li>Allow subject to leave scanner</li> </ul>"},{"location":"KB/MSCOCO/","title":"MSCOCO","text":""},{"location":"KB/MSCOCO/#mscoco","title":"MSCOCO","text":""},{"location":"KB/MSE/","title":"MSE","text":""},{"location":"KB/MSE/#mse","title":"MSE","text":"<ul> <li> \\[L(x) = \\Sigma_i ||D(E(x_i))||^2\\] </li> <li> \\[MSE = \\frac{1}{N} \\Sigma^N_{i=1}(p(x_i) - y_i)^2\\] </li> <li>loss converges to mean of targets</li> </ul>"},{"location":"KB/MSLE/","title":"MSLE","text":""},{"location":"KB/MSLE/#msle","title":"MSLE","text":"<ul> <li>MSE log error</li> <li>Use MSLE when doing regression, believing that your target, conditioned on the input, is normally distributed, and you don\u2019t want large errors to be significantly more penalized than small ones, in those cases where the range of the target value is large.</li> </ul> \\[\\frac{1}{\\mathrm{length}\\left( y \\right)} \\cdot \\mathrm{sum}\\left( \\left( \\log\\left( y + 1 \\right) - \\log\\left( \u0177 + 1 \\right) \\right)^{2} \\right)\\]"},{"location":"KB/MUSAN/","title":"MUSAN","text":""},{"location":"KB/MUSAN/#musan","title":"MUSAN","text":"<ul> <li>which consists of over 900 noises, 42 hours of music from various genres and 60 hours of speech from twelve languages</li> </ul>"},{"location":"KB/MVCNN/","title":"MVCNN","text":""},{"location":"KB/MVCNN/#mvcnn","title":"MVCNN","text":"<ul> <li>Multi view CNN for 3D object recognition</li> <li>Limitations</li> <li>Generating multi-views is time consuming process</li> <li>Objects are partially visible due to (self) occlusion</li> <li>Number of categories should be defined in advance.</li> <li></li> </ul>"},{"location":"KB/MVGrasp/","title":"MVGrasp","text":""},{"location":"KB/MVGrasp/#mvgrasp","title":"MVGrasp","text":"<ul> <li>H. Kasaei, et al. \"MVGrasp: Real-Time Multi-View 3D Object Grasping in Highly Cluttered Environments.\" arXiv preprint arXiv:2103.10997 (2021).</li> <li>Render multiple views of objects and use a next best view view selection algorithm Generate pixel-wise grasp configuration for the given object view.  </li> <li>The gripper approaches the target object in an arbitrary direction.  </li> <li>Use a shallow network, and an eye-to-hand camera configuration.</li> <li></li> <li>Mixed autoencoder (CAE + DAE)  </li> <li>optimizer: RMSprop, learning_rate = 0.001  </li> <li>metrics: Intersection over Union (IoU) and reconstruction error loss: mean KB/Squared Error.md</li> <li></li> <li>Which view is suitable?<ul> <li>Depends on the pose of the target object and other objects  </li> <li>Most objects are graspable from either top or side -&gt; orthographic setup</li> <li>View entropy is used as the metric for selecting the best view</li> </ul> </li> </ul>"},{"location":"KB/Machine%20Learning%20Tool%20Landscape/","title":"Machine Learning Tool Landscape","text":"","tags":["mlops"]},{"location":"KB/Machine%20Learning%20Tool%20Landscape/#machine-learning-tool-landscape","title":"Machine Learning Tool Landscape","text":"<ul> <li>https://huyenchip.com/2020/06/22/mlops.html</li> </ul>","tags":["mlops"]},{"location":"KB/Machine%20Learning%20Tool%20Landscape/#stages","title":"Stages","text":"<ul> <li>ML Production Flow</li> <li></li> <li>Pre Alex Net  (pre-2012)<ul> <li>Mostly modeling and training, small frameworks</li> </ul> </li> <li>(2012-2015)<ul> <li>Let's throw data at it</li> <li>Data Pipeline</li> <li></li> </ul> </li> <li>(2016-now)<ul> <li>Production</li> <li>Most companies cant' afford pure research</li> <li>Serving tools</li> </ul> </li> </ul>","tags":["mlops"]},{"location":"KB/Machine%20Learning%20Tool%20Landscape/#problems","title":"Problems","text":"<ul> <li>Problems facing MLOps</li> <li></li> </ul>","tags":["mlops"]},{"location":"KB/Macroadaptation/","title":"Macroadaptation","text":""},{"location":"KB/Macroadaptation/#macroadaptation","title":"Macroadaptation","text":"<ul> <li>Among four common designs for outer loops, the most complex is based on a pedagogy called macroadaptation (Corbett &amp; Anderson, 1995; Shute, 1993)</li> <li>For each task that the tutoring system can assign, it knows which knowledge components are exercised by the task. For each Knowledge Component, the tutor maintains an estimate of the student's degree of mastery of that Knowledge Component</li> <li>When a student has completed a task and the tutor needs to select the next one, it chooses one based on the overlap between the tasks' knowledge components and the student's mastered knowledge components</li> <li>For example, it might assign a task that requires many knowledge components that are already mastered by the student and just two components that are not yet mastered.</li> <li>Some tutoring systems represent not only correct and incorrect knowledge components, but also other stable traits of students. They might represent learning styles and preferences, such as a preference for visual or verbal explanations, so they can choose tasks that are marked as compatible with the student's style or preference.</li> <li>For the outer loop to function correctly across multiple tasks and sessions, the information about the student must be stored on a server or on the student's computer's disk. This persistent information is often called a student model. Exactly what it contains depends on the type of outer loop</li> </ul>"},{"location":"KB/Magic3D/","title":"Magic3D","text":""},{"location":"KB/Magic3D/#magic3d","title":"Magic3D","text":"<ul> <li>text to 3D model</li> <li>While the Dreamfusion model achieves remarkable results, the method has two problems</li> <li>long processing time</li> <li>low-quality of the generated images</li> <li>these problems are addressed by Magic3D using a two-stage optimization framework</li> <li>Magic3D builds a low-resolution difusion prior and, then, it accelerates with a sparse 3D hash grid structure</li> <li>a textured 3D mesh model is furthered optimized with an ecient diferentiable render</li> <li>higher quality 3D shapes in both geometry and texture compared to DreamFusion</li> </ul>"},{"location":"KB/Magical%20maybe/","title":"Magical maybe","text":"<p>toc: true title: Magical maybe</p> <p>categories: ['temp']</p>"},{"location":"KB/Magical%20maybe/#magical-maybe","title":"Magical Maybe","text":"<ul> <li>Robert Sapolsky</li> <li>According to this idea; the individual may or may not find a notification when looking on the phone. There is a large increase in Dopamine levels when the indication is seen.</li> </ul>"},{"location":"KB/Magnetic%20Detectors/","title":"Magnetic Detectors","text":""},{"location":"KB/Magnetic%20Detectors/#magnetic-detectors","title":"Magnetic Detectors","text":"<ul> <li>Robot sensors that can sense the presence of ferromagnetic material. Solid-state detectors with appropriate amplification and processing can locate a metal object to a high degree of precision.</li> </ul>"},{"location":"KB/Malignant/","title":"Malignant","text":""},{"location":"KB/Malignant/#malignant","title":"Malignant","text":"<ul> <li>Refers to the presence of cancerous cells in a tumor or growth</li> </ul>"},{"location":"KB/Mallows%20Cp%20Statistic/","title":"Mallows Cp Statistic","text":""},{"location":"KB/Mallows%20Cp%20Statistic/#mallows-cp-statistic","title":"Mallows Cp Statistic","text":"<ul> <li> \\[C_{p}= \\frac{1}{n}(RSS + 2 p \\hat \\sigma^{2})\\] </li> </ul>"},{"location":"KB/Manhattan%20Distance/","title":"Manhattan Distance","text":""},{"location":"KB/Manhattan%20Distance/#manhattan-distance","title":"Manhattan Distance","text":"<ul> <li>Taxicab distance or City Block distance, calculates the distance between real-valued vectors</li> <li> \\[D(x,y) = \\Sigma_{i=1}^{k}|x_{i}-y_{i}|\\] </li> <li>There is no diagonal movement involved in calculating the distance.</li> <li>Manhattan distance seems to work okay for high dim data, it is a measure that is somewhat less intuitive than euclidean distance, especially when using in high-dimensional data</li> <li>more likely to give a higher distance value than euclidean distance since it does not the shortest path possible.</li> <li>When your dataset has discrete and/or binary attributes, Manhattan seems to work quite well since it takes into account the paths that realistically could be taken within values of those attributes.</li> </ul>"},{"location":"KB/Manifold%20MixUp/","title":"Manifold MixUp","text":""},{"location":"KB/Manifold%20MixUp/#manifold-mixup","title":"Manifold MixUp","text":"<ul> <li>mixes feature values generated from intermediate neural network layers.</li> <li>feedforward up to k layer of the network where the output feature maps are mixed</li> <li>The mixed feature maps are given input to the next layer and forward propagated up to the last layer.</li> <li>After the forward propagation, backward propagation is performed in the standard way with updated labels</li> </ul>"},{"location":"KB/Manifold/","title":"Manifold","text":""},{"location":"KB/Manifold/#manifold","title":"Manifold","text":"<ul> <li>Data manifolds are an abstraction</li> <li>Only geometric insights are important</li> <li>Locally around some point c where the PDF is large -&gt; It will stay large only for a small fraction of directions<ul> <li>Those directions span a low dimensional hyperplane around c</li> <li>\"low dimensional sheets\"</li> <li>curved path</li> </ul> </li> <li>In an n dimensional real vector space \\(\\mathbb{R}^{n}\\) . Embedding space<ul> <li>\\(m \\leq n\\) is a positive integer</li> <li>An m dim manifold \\(\\mathcal{M}\\) is a subset of the vector space where one can smoothly map a neighborhood of that point to a neighborhood of the origin in m dim Euclidean space<ul> <li>Locally represents Euclidean space</li> </ul> </li> </ul> </li> <li>Only surface and not interior</li> <li>No sharp edges or spikes</li> <li>Can be exploited by Adversarial Learning</li> <li>Examples<ul> <li>1 dim -&gt; Lines in some high dim figure : B</li> <li>2 dim -&gt; Surfaces : A</li> <li></li> <li></li> <li></li> <li></li> </ul> </li> </ul>"},{"location":"KB/Manifold/#refs","title":"Refs","text":"<ul> <li>tds</li> <li>way more stuff : bjlkeng #todo</li> </ul>"},{"location":"KB/ManifoldMix/","title":"ManifoldMix","text":""},{"location":"KB/ManifoldMix/#manifoldmix","title":"ManifoldMix","text":"<ul> <li>improve the hidden representations and decision boundaries of neural networks at multiple layers by mixing hidden representations rather than input samples.</li> </ul>"},{"location":"KB/Manipulator/","title":"Manipulator","text":""},{"location":"KB/Manipulator/#manipulator","title":"Manipulator","text":"<ul> <li>Gripper, hand, arm or other part of the body that can effect and move objects in the robot\u2019s environment</li> <li>Is an End-effector</li> </ul>"},{"location":"KB/Mapping%20to%20Geometry/","title":"Mapping to Geometry","text":""},{"location":"KB/Mapping%20to%20Geometry/#mapping-to-geometry","title":"Mapping to Geometry","text":"<ul> <li>Height Plots</li> <li>Contour</li> </ul>"},{"location":"KB/Marching%20Cubes/","title":"Marching Cubes","text":""},{"location":"KB/Marching%20Cubes/#marching-cubes","title":"Marching Cubes","text":"<ul> <li>3D version of Marching Squares</li> <li>Cell consists of 8 node values: (i+{0,1}, j+{0,1}, k+{0,1})</li> <li> <ol> <li>Consider a cell</li> </ol> </li> <li> <ol> <li>Classify each vertex as inside or outside</li> </ol> </li> <li> <ol> <li>Build an index</li> </ol> </li> <li> <ol> <li>Get edge list from table[index]</li> </ol> </li> <li> <ol> <li>Interpolate the edge location</li> </ol> </li> <li> \\[x = i + \\frac{(c-v[i])}{(v[i+1]-v[i])}\\] </li> <li> <ol> <li>Compute gradients</li> </ol> </li> <li>Finite Differences Central</li> <li> <ol> <li>Consider ambiguous cases</li> </ol> </li> <li>Midpoint Decider</li> <li>Asymptotic Decider</li> <li> <ol> <li>Go to next cell</li> </ol> </li> <li></li> </ul>"},{"location":"KB/Marching%20Cubes/#limitations","title":"Limitations","text":"<ul> <li>Produces many triangles</li> <li>Cannot represent sharp edges</li> <li>Produces \u201cugly\u201d (thin) triangles</li> <li>Produces ringing artifacts!</li> </ul>"},{"location":"KB/Marching%20Squares/","title":"Marching Squares","text":""},{"location":"KB/Marching%20Squares/#marching-squares","title":"Marching Squares","text":"<ul> <li>Also uses Interpolation</li> <li>Symmetries</li> <li></li> <li>Asymptotic Decider</li> <li>Midpoint Decider</li> </ul>"},{"location":"KB/Marching%20Tetrahedra/","title":"Marching Tetrahedra","text":""},{"location":"KB/Marching%20Tetrahedra/#marching-tetrahedra","title":"Marching Tetrahedra","text":"<ul> <li>Unstructured Grids</li> <li>May split other cell types into tetrahedra, however, at the cost of introduced error</li> <li>One - and three + or Two - and two +</li> </ul>"},{"location":"KB/Margin%20Ranking/","title":"Margin Ranking","text":""},{"location":"KB/Margin%20Ranking/#margin-ranking","title":"Margin Ranking","text":"<ul> <li>Creates a criterion that measures the loss given inputs x1x1x1 , x2x2x2 , two 1D mini-batch Tensors, and a label 1D mini-batch tensor yyy (containing 1 or -1).</li> <li>If y=1y = 1y=1 then it assumed the first input should be ranked higher (have a larger value) than the second input, and vice-versa for y=\u22121y = -1y=\u22121 .</li> <li>take avg</li> </ul> \\[\\frac{1}{\\mathrm{length}\\left( y \\right)} \\cdot \\mathrm{sum}\\left( \\mathrm{max}\\left( 0, \\left( - y \\right) \\cdot x1 - x2 + margin \\right) \\right)\\]"},{"location":"KB/Markov%20Chain/","title":"Markov Chain","text":""},{"location":"KB/Markov%20Chain/#markov-chain","title":"Markov Chain","text":"<ul> <li>Sequence of random variables such as \\(X_{n+1}\\) only depends on \\(X_{n}\\)</li> <li>Discrete time</li> <li>Stochastic process without memory</li> <li>Finite interval : [0, 1, \u2026 n]</li> <li>Right infinite : n = 1, 2, 3, \u2026</li> <li>Left right infinite : Integers<ul> <li>No start</li> </ul> </li> <li>Markov Initial Distribution</li> <li>Markov Transition Kernel</li> <li>Markov for Continuous Distributions</li> </ul>"},{"location":"KB/Markov%20Initial%20Distribution/","title":"Markov Initial Distribution","text":""},{"location":"KB/Markov%20Initial%20Distribution/#markov-initial-distribution","title":"Markov Initial Distribution","text":"<ul> <li>\\(P_{X}\\)</li> <li>Not needed for left right infinite ones</li> </ul>"},{"location":"KB/Markov%20Property/","title":"Markov Property","text":""},{"location":"KB/Markov%20Property/#markov-property","title":"Markov Property","text":"<ul> <li>A property of certain environments, where state transitions are entirely determined by information implicit in the current state and the agent\u2019s action.</li> </ul>"},{"location":"KB/Markov%20Random%20Field/","title":"Markov Random Field","text":""},{"location":"KB/Markov%20Random%20Field/#markov-random-field","title":"Markov Random Field","text":"<ul> <li>Generalized Markov Chain</li> </ul>"},{"location":"KB/Markov%20Transition%20Kernel/","title":"Markov Transition Kernel","text":""},{"location":"KB/Markov%20Transition%20Kernel/#markov-transition-kernel","title":"Markov Transition Kernel","text":"<ul> <li>\\(\\(T_{n}(x|y) = P_{n}(X_{n+1}= x | X_{n}= y)\\)\\) for all \\(x,y \\in S\\)</li> <li>Homogenous if \\(\\(T_{n}(x|y) = T_{n'}(x|y)\\)\\) for all n,n'</li> <li>First get a value from a random drow from \\(P_{X_{1}}\\)</li> <li>Then get the next from the distribution which is specified by the transition kernel</li> <li></li> </ul>"},{"location":"KB/Markov%20for%20Continuous%20Distributions/","title":"Markov for Continuous Distributions","text":""},{"location":"KB/Markov%20for%20Continuous%20Distributions/#markov-for-continuous-distributions","title":"Markov for Continuous Distributions","text":"<ul> <li>Family of PDF</li> <li>If a Markov Chain with state set S, matrix M is executed m times . The transition probabilities transmit between states and then, \\(\\(P(X_{n+m}=s_{j}|X_{n}= s_{i}) = M^{m}(i, j)\\)\\)</li> <li>where \\(M^{m}= M \\cdot M \\cdot M \u2026 \\cdot M\\) (m times)</li> <li>To get the PDF \\(\\(g^{n+1}(x) = \\int_{\\mathbb{R}^{k}}T(x|y)g^{n}(y)dy\\)\\)</li> <li>Invariant Distribution</li> </ul>"},{"location":"KB/Masked%20Autoencoders/","title":"Masked Autoencoders","text":""},{"location":"KB/Masked%20Autoencoders/#masked-autoencoders","title":"Masked Autoencoders","text":"<ul> <li>Masked Autoencoders are Scalable Vision Learners</li> <li>simple Self Supervised</li> <li>ImageNet and in Transfer Learning that an Auto Encoders \u2014- a simple self-supervised method similar to techniques in NLP \u2013 provides scalable benefits</li> <li>mask random patches of the input image and reconstruct the missing pixels</li> <li>asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens</li> <li>images and languages are signals of a different nature</li> <li>Images are merely recorded light without a semantic decomposition into the visual analogue of words</li> <li>The word (or subword) analog for images are pixels</li> <li>But decomposing the image into patches (like Vision Transformer reduces the quadratic computation cost of transformers compared to operating at the pixel level</li> <li>remove random patches that most likely do not form a semantic segment</li> <li>Likewise, MAE reconstructs pixels, which are not semantic entities</li> <li>hey find that masking a high proportion of the input image, e.g., 75%, yields a nontrivial and meaningful self-supervisory task</li> <li>train and throw away the decoder and fine-tune the encoder for downstream tasks</li> <li>Vanilla ViT-Huge model (ViTMAE) achieves the best accuracy</li> <li>ImageNet</li> <li>Transfer performance in downstream tasks outperforms supervised pre-training and shows promising scaling behavior</li> <li>semantics</li> <li>Occurs by way of a rich hidden representation inside the MAE</li> <li></li> </ul>"},{"location":"KB/Masked%20Language%20Modeling/","title":"Masked Language Modeling","text":""},{"location":"KB/Masked%20Language%20Modeling/#masked-language-modeling","title":"Masked Language Modeling","text":"<ul> <li>In Masked Language Modeling, an input sequence of tokens is provided, but with some of these tokens masked. The goal of the model is then to learn to predict the correct tokens that are hidden by the mask. If it can do so, it can learn token-level information given the context of the token.</li> <li>In BERT, this is done as follows. 15% of all word embedded tokens is masked at random. From this 15%, 80% of the tokens is represented with a token called , 10% is replaced with a random token and 10% is left alone. This ensures that masking is both relatively random and that the model does not zoom in to the token, which is available during pretraining but not during fine-tuning.</li> <li>This model is also capable of predicting words using the two masked sentences. It concatenates two masked words and tries to predict.</li> <li>these models where we are required to predict the context of words. Since the words can have different meanings in different places the model needs to learn deep and multiple representations of words.</li> <li>These models have shown improved performance levels in the downstream tasks such as syntactic tasks that require lower layer representation of certain models in place of a higher layer representation.</li> <li>We may also find their use in learning the deep bidirectional representations of words. The model should be able to learn the context of words from the start of the sentence as well as from the behind.</li> </ul>"},{"location":"KB/Masked%20Language%20Modeling/#tokenizing-data-bert","title":"Tokenizing Data BERT","text":"<ul> <li>In the tokenizer method, text_lst is the text corpus, max_length suggests the maximum number of allowable input tokens (the maximum is 512 for BERT base), and truncation set to True indicates that if the input size is more than the max_length, then the token from index number equal to max_length would be truncated i.e., for our example input tokens from index 100 would be dropped, padding set to True indicates the input length shorter than the max_length are padded, with padding token 0 and lastly, return_tensors indicates in what format do we want the output tensor and tf suggests that we expect tensorflow tensor. The tokenizer here returns three fields, as we have mentioned earlier.</li> <li>Now if we look at the \u201cinputs\u201d with the code print(inputs), we can see that the input_ids tensor is of shape 1567\u00d7100, and each row starts with the token 101, which is the id for the Special token [CLS] and ends with 0 which is the padding token indicating that the sentence length is less than 100. Also, there is a Special token 102, the [SEP] token, which is not visible, indicating the end of a sentence. Secondly, the token_type_ids are all 0 as there is only a single sentence as input. Finally, the attention_mask has ones at locations for the actual input tokens and zeros for the padding tokens.</li> </ul>"},{"location":"KB/Masked%20Language%20Modeling/#masking-input-tokens-bert","title":"Masking Input Tokens BERT","text":"<ul> <li>In the original research paper, 15% of the input tokens were masked, of which 80% were replaced with [MASK] tokens, 10% were replaced with random tokens, and another 10% were left as is. However, in our fine-tuning task, we are replacing 15% of the input tokens except for the special ones with only [MASK] i.e., we will not replace token numbers 101,102, and 0 with mask token 103. In the following lines of codes, the same logic is implemented</li> </ul>"},{"location":"KB/Mastectomy/","title":"Mastectomy","text":""},{"location":"KB/Mastectomy/#mastectomy","title":"Mastectomy","text":"<ul> <li>Surgical procedure to remove part or all of the breast</li> </ul>"},{"location":"KB/Mastery%20learning/","title":"Mastery learning","text":""},{"location":"KB/Mastery%20learning/#mastery-learning","title":"Mastery Learning","text":"<ul> <li>The outer loop implements a pedagogy called mastery learning (Bloom, 1984).</li> <li>The curriculum is structured as a sequence of units or a sequence of difficulty levels. When a student is working on a unit (or level of difficulty), the tutoring system keeps assigning tasks from that unit until the student has mastered the unit's knowledge. Only then does it allow the student to proceed to the next unit</li> <li>Thus, some students finish the curriculum having done fewer tasks than other students.</li> <li>This design for the outer loop is mostly used in self-paced courses.</li> <li>It is seldom used for a class-paced course, where it is important that all students stay together as they move through the curriculum.</li> <li>A common mistake is to develop a tutoring system with a fancy outer loop, then discover that instructors cannot use its features due to the class-paced nature of the course.</li> </ul>"},{"location":"KB/Material%20Processing%20Robot/","title":"Material Processing Robot","text":""},{"location":"KB/Material%20Processing%20Robot/#material-processing-robot","title":"Material Processing Robot","text":"<ul> <li>A robot designed and programmed so that it can machine, cut, form or change the shape, function or properties of materials it handles between the time the materials are first grasped and the time they are released in a manufacturing process.</li> </ul>"},{"location":"KB/Matrix%20notation%20for%20NNs/","title":"Matrix notation for NNs","text":""},{"location":"KB/Matrix%20notation%20for%20NNs/#matrix-notation-for-nns","title":"Matrix notation for NNs","text":""},{"location":"KB/Matrix%20notation%20for%20NNs/#example-for-simple-case","title":"Example for simple case","text":"<ul> <li>A NN consists of linear transformations alternating with loss functions.</li> <li></li> <li>or , (\\(\\begin{align*} \\mathbf{h = a[\\theta_{0}+ \\theta_{x}]} \\\\ \\mathbf{h' = a[\\psi_{0} + \\Psi h]}\\\\ \\mathbf{y' = \\phi'_{0}+ \\phi'h'} \\end{align*}\\)\\)<ul> <li>where  \\(\\mathbf{a[\\bullet]}\\) </li> </ul> </li> </ul>"},{"location":"KB/Matrix%20notation%20for%20NNs/#general-case","title":"General case","text":"<ul> <li>Vector of hidden units at layer \\(k\\) is \\(h_k\\) </li> <li>Vector of biases that contribute to hidden layer \\(k+1\\) is \\(\\beta_{k}\\) </li> <li>Weights that are applied to \\(k^{th}\\) layer and contribute to hidden layer \\((k+1)^{th}\\) is \\(\\Omega_{k}\\) </li> <li>General network \\(y = f[x, \\phi]\\)  with \\(K\\) layers is now</li> <li></li> </ul>"},{"location":"KB/Max%20Margin%20Loss/","title":"Max Margin Loss","text":""},{"location":"KB/Max%20Margin%20Loss/#max-margin-loss","title":"Max Margin Loss","text":"<ul> <li>Makes sure only dissimilar pairs with minimum distance m contribute to the loss</li> <li>Spring mass system</li> <li>Hinge Loss probably ??</li> </ul>"},{"location":"KB/Maximum%20Distance%20Baseline/","title":"Maximum Distance Baseline","text":""},{"location":"KB/Maximum%20Distance%20Baseline/#maximum-distance-baseline","title":"Maximum Distance Baseline","text":"<ul> <li>This baseline is called the Maximum Distance baseline and creates a baseline by constructing an image with the largest value of the L1 distance from the original image</li> <li>The problem with the maximum distance is that it doesn\u2019t represent the \u201cabsence of feature\u201d. It contains the information about the original image, just in a different form.</li> </ul>"},{"location":"KB/Maximum%20Likelihood/","title":"Maximum LIkelihood","text":""},{"location":"KB/Maximum%20Likelihood/#maximum-likelihood","title":"Maximum Likelihood","text":""},{"location":"KB/Maximum%20Likelihood/#effect","title":"Effect","text":"<ul> <li>Consider a model \\(f[x, \\phi]\\) that computes an output from input \\(x\\).</li> <li>Consider the model computes a conditional probability distribution \\(Pr(Y|x)\\) , Y is output</li> <li>This encourages each output \\(y_i\\) to have high probability under \\(Pr(y_{i}|x_{i})\\) computed from input \\(x_{i}\\)</li> <li></li> </ul>"},{"location":"KB/Maximum%20Likelihood/#computing-a-distribution-over-inputs","title":"Computing a distribution over inputs","text":"<ul> <li>Choose a parametric distribution \\(Pr(y|\\theta)\\) defined on output domain y. </li> <li>use the network to compute one or more of the parameters \\(\\theta\\) of this distribution</li> <li>For example, suppose the prediction domain is the set of real numbers, so y \u2208 R. Here, we might choose the univariate normal distribution, which is defined on R. This distribution is defined by the mean \u03bc and variance \u03c32, so \u03b8 = {\u03bc,\u03c3^2}. The machine learning model might predict the mean \u03bc, and the variance \u03c3^2 could be treated as an unknown constant.</li> </ul>"},{"location":"KB/Maximum%20Likelihood/#maximum-likelihood-criterion","title":"Maximum likelihood criterion","text":""},{"location":"KB/Maximum%20Likelihood/#log-likelihood-criterion","title":"Log likelihood criterion","text":""},{"location":"KB/Maximum%20Likelihood/#minimizing-log-likelihood-loss","title":"Minimizing Log Likelihood Loss","text":"<ul> <li>Negative Log Likelihood</li> </ul>"},{"location":"KB/Maximum%20Likelihood/#inference","title":"Inference","text":"<ul> <li>When we perform inference, we often want a point estimate rather than a distribution, so we return the maximum of the distribution</li> <li></li> </ul>"},{"location":"KB/Maximum%20Matching%20Algorithm/","title":"Maximum Matching Algorithm","text":""},{"location":"KB/Maximum%20Matching%20Algorithm/#maximum-matching-algorithm","title":"Maximum Matching Algorithm","text":"<ul> <li>Greedy</li> <li>Starts with first character</li> <li>Searches for the longest word in list starting with this character. If match is found, boundary is marked</li> </ul>"},{"location":"KB/Maximum%20likelihood%20criterion/","title":"Maximum likelihood criterion","text":""},{"location":"KB/Maximum%20likelihood%20criterion/#maximum-likelihood-criterion","title":"Maximum likelihood criterion","text":"<p>Since each observed output should have high probability under its corresponding distribution \\(Pr(y_{i}|\\theta_{i})\\), we can choose the model params \\(\\phi\\) so they MAXIMIZE the combined probability across all I training examples -  - Assumptions     - data are identically distributed     -      - independent and identically distributed</p>"},{"location":"KB/Maxout/","title":"Maxout","text":""},{"location":"KB/Maxout/#maxout","title":"Maxout","text":"<ul> <li> \\[f(x) = max(x, x\\cdot a)\\] </li> </ul>"},{"location":"KB/Mean%20Diffusivity/","title":"Mean Diffusivity","text":""},{"location":"KB/Mean%20Diffusivity/#mean-diffusivity","title":"Mean Diffusivity","text":"<ul> <li> \\[\\mu = \\frac{\\lambda _{1}+ \\lambda_{2}+ \\lambda_{3}}{3}\\] </li> <li></li> </ul>"},{"location":"KB/Mean%20Observed%20Dissimilarity/","title":"Mean Observed Dissimilarity","text":""},{"location":"KB/Mean%20Observed%20Dissimilarity/#mean-observed-dissimilarity","title":"Mean Observed Dissimilarity","text":"<ul> <li>is the mean of the NISSIM dissimilarity over the adversarial test set for similar levels of attack.</li> <li>So for every adversarial set X \u2217, calculate NISSIM value for all samples in that set, and divide by the total number of samples.</li> <li>(0,1], such that 0 indicates total similarity while 1 indicates total dissimilarity</li> <li> \\[MOD_{advset}= \\frac{1}{N}\\Sigma (NISSIM_{i})\\] </li> </ul>"},{"location":"KB/Media%20Distillery/","title":"Media Distillery","text":""},{"location":"KB/Media%20Distillery/#media-distillery","title":"Media Distillery","text":""},{"location":"KB/Media%20Distillery/#contact","title":"Contact","text":"<ul> <li>jobs@mediadistillery.com</li> <li>Jacqueline@mediadistillery.com</li> </ul>"},{"location":"KB/Media%20Distillery/#about","title":"About","text":"<ul> <li>Keep your viewers engaged to your platform along with every step of their journey</li> <li>Create new value with automated content chaptering, appealing images and topic labelling</li> <li>Enable better and more valuable ad placements in line with the content displayed</li> </ul>"},{"location":"KB/Media%20Distillery/#epg-correction","title":"EPG Correction","text":"<ul> <li>manually detecting\u00a0the start and end times of your TV programs, ad breaks and shorter video segments!</li> <li>redefine the\u00a0Electronic Program Guide (EPG) correction, content navigation and TV advertising using\u00a0state-of-the-art AI and Machine Learning technologies.</li> <li>By identifying the exact start and end times of ad breaks in video content, Ad Break Distillery enables ad skipping, insertion or replacement in the replay and catchup environment.</li> <li>With Chapter Markers viewers can easily navigate through video content to the parts they are interested in, which leads to a higher user engagement and satisfaction.</li> <li>EPG Correction\u2122 ensures seamless and VoD-like experiences for catch-up and replay by fully automated and real-time correction of the difference between the scheduled and the actual air times of TV programs.</li> <li>Media Distillery\u2019s award-winning Deep Content UnderstandingTM\u00a0technology can analyze video content to determine relevant topics, events or specific time markers, all in real-time and cloud-based. NOS implemented Media Distillery\u2019s product EPG Correction Distillery\u2122 on their 70 most popular TV channels, to provide automatic adjustments to update the actual start time of television programmes as they are broadcast, so consumers can enjoy their favorite video content instantly.</li> <li>NOS (NOS is the biggest communications and entertainment group in Portugal.) is also using the automatically generated\u00a0correct time markers\u00a0to insert pre-roll advertisements in their replay platform. This results in a natural transition from the content to the ad-block\u00a0for the consumer, while the inserted ads create\u00a0a solid revenue stream for NOS.</li> </ul>"},{"location":"KB/Media%20Distillery/#image-distillery","title":"Image Distillery","text":"<ul> <li>image chosen to be displayed in the UI is crisp and blur-free, that the actors\u2019/presenters\u2019 eyes are open, and the characters\u2019 positioning in-frame is aesthetically pleasing.</li> <li>The Images are created directly out of a broadcast signal or video asset, in a real-time and fully automated fashion. This means that even for live-programming appealing images can be provided and used to boost catch-up and replay viewing.</li> <li>Images can be generated from the entire program, or from a specified time range, to prevent inadvertently showing spoilers in the UI.</li> </ul>"},{"location":"KB/Media%20Distillery/#email","title":"Email","text":""},{"location":"KB/MediaMonks/","title":"MediaMonks","text":""},{"location":"KB/MediaMonks/#media-monks-cover-letter","title":"Media Monks Cover Letter","text":"<p>As Rogier Bikker's article on your website says, AI is truly taking the world by storm, but perhaps infinite content generation is not the key to customer engagement. At the end of the pipeline, what matters is the humans who use and are affected by technology. There are so many terms that keep showing up every day - ML, Computer Vision, Mixed reality. To most people, they do not mean much, but they have such tremendous potential if used wisely and with an understanding of the technologies. My interest is to help customers bring their visions to reality while also guiding them towards using AI better. </p> <p>My expertise is a combination of data analytics and computer vision, and as of a month ago, I also have a Master in Artificial Intelligence from the University of Groningen. I am familiar with the tools required for basic and advanced AI and analytics from internships, research projects, papers, freelance work, and many personal projects. But aside from that, I am an artist (Instagram : www.instagram.com/aiexistentialart). That being the case, I am quite familiar with Photoshop, Blender, and some other design and illustration tools. I've played with VR, can cook up demos on an Arduino, and am familiar with the technology behind StableDiffusion, RunwayML, MidJourney etc. Although I am not yet familiar with ZBrush, I do use Nomad Sculpt, and can learn pretty much any technology given time and some mentorship.</p> <p>As for talking about technical stuff in simpler terms, I have taught plenty of people about AI in workshops, webinars, and through many events. I write articles regularly and have been freelancing as a content creator for many years now.</p> <p>I have always loved creating concepts, both in my art (as a concept artist) and in my AI work. While it is impossible to be familiar with all the tools, they all share a common thread. That being the case, I will be able to contribute to any team I get to work with. It is not every day you find a position that you enjoy and also think you can contribute something to, and I sincerely hope you give me a chance to be a Monk with you :)</p>"},{"location":"KB/MediaMonks/#interview-prep","title":"Interview Prep","text":"<ul> <li>Hiring manager : Jakub Pawe\u0142czak </li> <li>Team members : Fredrick Charles Papworth,\u00a0Suzanne Elmazi, and\u00a0Ben Major</li> <li>They bridge between ideas and technology. They curate\u00a0creative\u00a0ideas and translate them through innovative solutions that are technology-based.</li> </ul>"},{"location":"KB/Median%20Filter/","title":"Median Filter","text":""},{"location":"KB/Median%20Filter/#median-filter","title":"Median Filter","text":"<ul> <li>Values are replaced by the median in a local surrounding</li> <li>non linear</li> <li>preserves edges</li> </ul>"},{"location":"KB/Mediatic%20Behavior/","title":"Mediatic Behavior","text":"<p>toc: true title: Mediatic Behavior</p> <p>categories: ['temp']</p>"},{"location":"KB/Mediatic%20Behavior/#mediatic-behavior","title":"Mediatic Behavior","text":"<ul> <li>Many individuals tend to resemble a role model.</li> <li>For example, in order to resemble the character in a television series, he or she unintentionally wears clothes similar to those she wore and uses his or her lines in daily life.</li> <li>This is basic type of media behavior</li> </ul>"},{"location":"KB/Memory%20Coupling/","title":"Memory Coupling","text":""},{"location":"KB/Memory%20Coupling/#memory-coupling","title":"Memory Coupling","text":"<ul> <li>TIghtly coupled</li> </ul>"},{"location":"KB/Memory%20to%20Memory%20Architecture/","title":"Memory to Memory Architecture","text":""},{"location":"KB/Memory%20to%20Memory%20Architecture/#memory-to-memory-architecture","title":"Memory to Memory Architecture","text":"<ul> <li>For all vector operation, operands are fetched directly from main memory, then routed to the functional unit</li> <li>Results are written back to main memory</li> <li>Large startup time</li> </ul>"},{"location":"KB/Memory-based%20learning/","title":"Memory-based learning","text":""},{"location":"KB/Memory-based%20learning/#memory-based-learning","title":"Memory-based Learning","text":"<ul> <li>Lazy learning</li> <li>All encountered examples are stored in memory in a multi-dimensional array, positioned according to relevant features</li> <li>New items are classified (comprehension) or generated (production) by searching for an example in memory that is closest to the target</li> <li>Because examplars are represented by their features even novel forms can be classified</li> <li>A generalization of the knn (k-nearest neighbors) algorithm</li> <li>Don't remove any infrequent or even solo forms. You might need the info</li> <li>Don't trim down the number of examples of a frequent form you have in the model. This effects it.</li> <li>Learning is storing, classification is analogy</li> <li>multiple long-distance dependencies</li> </ul>"},{"location":"KB/Mental%20Fatigue/","title":"Mental Fatigue","text":""},{"location":"KB/Mental%20Fatigue/#mental-fatigue","title":"Mental Fatigue","text":"<ul> <li>Resource depletion occurs</li> <li>Drop in motivation does not really happen</li> </ul>"},{"location":"KB/Mental%20Model%20Matching/","title":"Mental Model Matching","text":""},{"location":"KB/Mental%20Model%20Matching/#mental-model-matching","title":"Mental Model Matching","text":"<ul> <li>A user's mental model [42] of a technology is their internal understanding of how a technology works.</li> <li>People rely heavily on their mental models of technology to make decisions</li> <li>It has been found that XAI stakeholders use their mental models of XAI to decide when to use the technology [10], to evaluate how much to trust the outputted explanations [10, 20, 22], and to make sense of any results [22, 30]</li> <li>While ML practitioners may have had access to specialized training on how LLMs work, this is decidedly not the case for the vast majority of the general population</li> <li>How a general user believes an LLM to work may be very different from how it actually works, and this mismatch can be dangerous</li> <li>It is not difficult to imagine frightening scenarios where users anthropomorphize or deify an LLM chatbot, understanding it to be a \"magical\" source of ground truth. This could very quickly lead to conspiracy theories and the legitimization of disinformation campaigns [see, e.g., 23]</li> </ul>"},{"location":"KB/Mesh%20Smoothing/","title":"Mesh Smoothing","text":""},{"location":"KB/Mesh%20Smoothing/#mesh-smoothing","title":"Mesh Smoothing","text":"<ul> <li>Noisy volume data leads to a noisy surface grid:</li> <li>Smooth the volume data first, or</li> <li>Smooth the grid in post-processing</li> <li>Eventually simplifies the grid</li> </ul>"},{"location":"KB/Mesh%20refinement/","title":"Mesh refinement","text":""},{"location":"KB/Mesh%20refinement/#mesh-refinement","title":"Mesh Refinement","text":""},{"location":"KB/Mesolimbic%20Pathway/","title":"Mesolimbic Pathway","text":""},{"location":"KB/Mesolimbic%20Pathway/#mesolimbic-pathway","title":"Mesolimbic Pathway","text":"<ul> <li>A specialized brain circuit implicated in the processing of risk and reward information.</li> </ul>"},{"location":"KB/Meta%20AI%20Speech%20from%20Brain/","title":"Meta AI Speech from Brain","text":""},{"location":"KB/Meta%20AI%20Speech%20from%20Brain/#my-favourite-obsidian-plugins-for-research-notes-2-bonus-tips","title":"My Favourite Obsidian Plugins for Research Notes + 2 Bonus Tips","text":"<p>Obsidian is my favourite program for taking notes. Be it for research, general things I learn, summaries from papers, lecture notes and the like. Out of the box, it does so many things really well. But, its real power lies in the vast number of plugins it has. Most of these are user created, and you can even make your own (or hack one together)! In this sea of functionality, these are the top few that I use. Grouped by the type of task for easier lookup.</p> <p>(Disclaimer : I am not sponsored by either Obsidian or any of the authors of the plugins mentioned here. These are personal preferences.)</p>"},{"location":"KB/Meta%20AI%20Speech%20from%20Brain/#the-use-case","title":"The use case","text":"<p>I am a student, researcher and programmer. I take lecture notes, read a lot of research papers, articles and books. These come down to a lot of information. Of course, there\u2019s no way I can remember all of these bits of fragmented information.  Therefore, drumroll\u2026, I use Obsidian to help me put these bits of information in a place I can easily access. Since I use this almost everyday, I want taking notes to be as painless and efficient as possible.  These plugins are a huge help in doing exactly that. (Ordered by the type of task)</p>"},{"location":"KB/Meta%20AI%20Speech%20from%20Brain/#how-to-install-these-plugins","title":"How to install these plugins?","text":"<p>This is a simple step. Simple open Obsidian Settings, Scroll down a bit and select \u201cCommunity plugins\u201d. Disable Restrictive mode, and then browse to your hearts content!</p>"},{"location":"KB/Meta%20AI%20Speech%20from%20Brain/#writing","title":"Writing","text":"<p>Writing notes is the major objective here. So how do we make it extra painless? Plugins of course! - Dynamic Table of Contents : Many times, I take notes for a long form text. Sometimes these notes end up pretty huge, and it becomes slightly harder to find something. What about adding a Table Of Contents to the start? Sounds great, but what if we update the note? In comes this plugin, with an automatically updating TOC. - Tags : Super simple, also built in. Adding \u201c#topic1, #topic2 etc\u201d to a file to make it easier to search and organise. - Frontmatter Tag Suggest : Tags are great, but who remembers which ones they used before? Nobody. This plugin autocompletes tags based on ones you have used in previous notes. You can create new ones the normal way of course.  - Note Refactor : Made a huge note with a lot of headings? Why not split them into individual topics and maintain links to them? This makes it easier for you to have one major idea per note. Here I have a bunch of test headings, you can see how after applying them they become new notes that link to the current file. - Paste URL into selection : The name says it all doesn\u2019t it? - Templater : Another plugin I use daily. I like starting my notes with \u201cdate_created\u201d, \u201cdate_modified\u201d, \u201ctags\u201d, \u201ctoc: true title\u201d and insert the file name as the header. Since I do this for every single note, why not automate it? This plugin lets you create blocks of dynamic text to be inserted with a keyboard shortcut. I use \u201cCmd+Shift+I\u201d (\u201cControl+Shift+I\u201d for Windows) - Typewriter Scroll : Zen Mode is a way of life. This lets me focus on what I am writing by automatically scrolling the page, and dimming the rest of the text apart from the line I am currently writing. I do disable it while reading though. - Command Palette : This one is pretty obvious, but this built in plugin is just a text search. You can quickly open files with a (Cmd/Control + O) that brings up a searchable menu, or use (Cmd/Control + P) to bring up a searchable list of quick actions. - Vim Mode : This little option is not for everyone honestly. If you have never heard of Vim, just skip this point. I use vim as my default text editor for everything else. And I can\u2019t live without its keybindings. This just lets me use the vim keys for everything.</p>"},{"location":"KB/Meta%20AI%20Speech%20from%20Brain/#research","title":"Research","text":"<p>For research (AI research in my case), we have three main objectives :  - Merge important information from a large number of sources. - Find links between ideas that you did not see. - Maintain a daily log as something of a lab notebook. There are 5 plugins that fulfil these criteria pretty decently.  - Daily Notes : This is a Core plugin and comes with Obsidian. Essentially it\u2019s a journal. You can add whatever you want to it and it is created every day. I use it to keep a time stamped log of what I did that day. It is also useful if you just want to dump a bunch of information but don\u2019t want to format and organise it just yet. - TimeStamper : In my daily notes, I like having timestamps (eg - 9:30 : I did xyz). This plugin lets me set a custom format and a keyboard shortcut. I have set it to \u201cCmd+T\u201d (for Mac or Control+T for Windows) - Backlinks : A real game changer and another built in plugin. This shows you every file that either is linked in the current file, or refers to the current one. Identifying links between concepts, and finding more of them is absolutely invaluable in research. - Quick LaTEX for Obsidian : LaTEX is probably the easiest way of writing professional looking math-y stuff, be it equations or formulae or anything similar. This plugin has a lot of options for autocomplete, formatting, and makes my job almost ridiculously easy. Here\u2019s how it looks. (Just typing a random equation)</p>"},{"location":"KB/Meta%20AI%20Speech%20from%20Brain/#organising","title":"Organising","text":"<p>What do you do once you have a lot of files, you organise them of course! Now Obsidian by default makes it pretty easy to do this. But these plugins make organising less of a chore and much more of a fun thing to do. - Local Images : To make my notes more informative, I sometimes paste images. Now many times these are links from some website, which makes it a little risky, because what if the website stops working? This plugin automatically downloads image links in your notes and saves them locally. (It also links to the correct downloaded file.) - Graph view : Oh the gift and curse of a pretty graph. I sometimes use this to navigate between my links either to or from a file. It also gives me a very useful overview of what I have. I generally use the \u201cLocal graph\u201d that shows me a graph for the current note, rather than the \u201cGlobal\u201d full one which shows me everything. (It\u2019s pretty, but unhelpful) - Linter : Maybe I have a bunch of empty lines, empty list items, my headers are not in sentence case, my text is not formatted, my paragraphs are weird. Or anything like that. I am lazy, so I use the Linter plugin to automatically perform a bunch of processing and clean up my files.  - Tag Wrangler : Have a lot of tags? View/Edit/Change them across every file that uses them in one place. Also useful for finding files that match a few criteria. - File Cleaner : Remove empty files, unreferenced images etc. Keeping your \u201cDigital Garden\u201d pruned and bug free. - Obsidian Link Converter : Because I host my Obsidian Vault on a personal website, sometimes the links that Obsidian uses don\u2019t work, this plugin lets me mass convert them to a format that does.</p>"},{"location":"KB/Meta%20AI%20Speech%20from%20Brain/#bonus-tips","title":"Bonus tips!!","text":"<ul> <li>Make sure every file has a single major idea. If you have too many, use the \u201cNote Refactor\u201d to put them in their own files. This will make it extremely easy to refer to the \u201cIdeas\u201d in the text somewhere else instead of linking to the whole text. </li> <li>Want pages that consolidate all the notes that have a particular tag together and save them automatically to a single file? Say you want a file that has links to all the notes that have the tag \u201c#apple\u201d. Here is a little script that I wrote which does just that.</li> </ul>"},{"location":"KB/Meta%20AI%20Speech%20from%20Brain/#fin","title":"Fin","text":"<p>This article is in the hopes that it will help someone out. Maybe have the help that I did not. I do not know who it will reach. But to whoever it does, best of luck :)</p> <p>Like these/Want more? Buy me a coffee! Kofi</p> <p>Want articles on something specific? Just ask!</p> <p>You can always contact me on LinkedIn, or drop me an Email</p> <p>toc: true title: Meta AI Speech from Brain categories: ['architecture']</p>"},{"location":"KB/Meta%20AI%20Speech%20from%20Brain/#meta-ai-speech-from-brain","title":"Meta AI Speech from Brain","text":"<ul> <li>help people unable to communicate through speech, typing or gestures</li> <li>tries to decode language directly from noninvasive brain recordings</li> <li>challenge with this proposed method come from noise and diferences in each person's brain and where the sensors are placed.</li> <li>contrastive learning and used to maximally align noninvasive brain recordings and speech sounds</li> <li>A self-supervised learning model called wave2vec 2.0. is used to identify the complex representations of speech in the brains of volunteers listening to audiobooks</li> <li>The two noninvasive technologies used to measure neuronal activity are electroencephalography and magnetoencephalography.</li> <li>Training data comes from four opensource datasets which represent 150 hours of recordings of 169 volunteers listening to audiobooks</li> <li>EEG and MEG recordings are inserted into a brain model, which consists of a standard deep convolutional network with residual connections</li> <li>These recordings are what comes from individuals' brains</li> <li>both a speech model for sound and a brain model for MEG data.</li> <li>several components of the algorithm were beneficial to decoding performance</li> <li>algorithm improves as EEG and MEG recordings increase</li> <li>self-supervised trained AI can decode perveived speech despite noise and variability in that data.</li> </ul>"},{"location":"KB/Meta%20Learning%20Data%20Augmentations/","title":"Meta Learning Data Augmentations","text":""},{"location":"KB/Meta%20Learning%20Data%20Augmentations/#meta-learning-data-augmentations","title":"Meta Learning Data Augmentations","text":"<ul> <li>The concept of meta-learning in Deep Learning research generally refers to the concept of optimizing neural networks with neural networks.</li> <li>This approach has become very popular since the publication of NAS</li> </ul>"},{"location":"KB/Methods%20for%20Feature%20Learning/","title":"Methods for Feature Learning","text":""},{"location":"KB/Methods%20for%20Feature%20Learning/#methods-for-feature-learning","title":"Methods for Feature Learning","text":"<ul> <li>Start with a random dict and rep. Update D while keeping r fixed -&gt; find best r while keeping D fixed. Repeat until convergence.</li> <li>Move D in a direction to minimize loss and project it back<ul> <li>Gradient Descent gradients or LinearRegression</li> </ul> </li> </ul>"},{"location":"KB/Microbiota/","title":"Microbiota","text":""},{"location":"KB/Microbiota/#microbiota","title":"Microbiota","text":"<ul> <li>The community of various microorganisms found in the digestive tract. Scientists are now learning that microbes found in the microbiota can influence brain development, mood, and behavior.</li> </ul>"},{"location":"KB/Microglia/","title":"Microglia","text":""},{"location":"KB/Microglia/#microglia","title":"Microglia","text":"<ul> <li>A small, specialized glial cell that operates as the first line of immune defense in the central nervous system.</li> </ul>"},{"location":"KB/Micromarriage/","title":"Micromarriage","text":""},{"location":"KB/Micromarriage/#micromarriage","title":"Micromarriage","text":"<ul> <li>Micromarriages -- colah's blog</li> <li>A micromarriage is a one in a million chance that an action will lead to you getting married, relative to your default policy.</li> <li>Note that some actions, such as dying, have a negative number of micromarriages associated with them.</li> <li>Most people do not include being coercively forced into a marriage when calculating micromarriages.</li> </ul>"},{"location":"KB/Midpoint%20Decider/","title":"Midpoint Decider","text":""},{"location":"KB/Midpoint%20Decider/#midpoint-decider","title":"Midpoint Decider","text":"<ul> <li>check value in cell center and decide accordingly</li> <li></li> </ul>"},{"location":"KB/Midpoint%20Method/","title":"Midpoint Method","text":""},{"location":"KB/Midpoint%20Method/#midpoint-method","title":"Midpoint Method","text":""},{"location":"KB/Milin%20et%20al/","title":"Milin et al.","text":""},{"location":"KB/Milin%20et%20al/#milin-et-al","title":"Milin Et Al.","text":"<ul> <li>Towards cognitively plausible data science in language research (2016), Milin, Divjak, Dimitrijevic and Baayen</li> <li>Identify difficult and easy forms (from lemma to plural form)</li> <li>Check if human participants also react differently to independently identified difficult and easy forms Compare NDL learning model to TiMBL and human results</li> <li>MDVM computes the distance between two values of a feature to reflect their patterns of co-occurrence with categories</li> <li>Using MDVM adds an unsupervised learning component to MBL Hoste (2005) because essentially it clusters feature values and uses that information</li> <li>Using larger values of k with MDVM is helpful</li> <li>Easy words that are frequent tokens (forms) are reacted to faster</li> <li>Maybe this interaction doesn't occur with difficult words because there is less variation in the frequency of the difficult words?</li> <li>This seems similar to results with regular past tense forms in English:</li> <li>Strikingly, TiMBL's inflectional class probabilities turn out to be predictive in production and comprehension, i. e., for lexical decision latencies.</li> <li>Two Grapheme to Lexeme Measures</li> <li>Diversity Sum of the absolute values of the activations of all possible outcomes, given a set of input cues.</li> <li>Input cues that activate many different outcomes give rise to a highly diverse activation vector, which in turn indicates a high degree of uncertainty about the intended outcome.</li> <li>G2L-Prior Sum of the absolute values of the weights on the connections from all cues to a given outcome.</li> <li>independent of the actual cues encountered in the input</li> <li>reflects the prior availability of an outcome, its entrenchment in the learning network</li> <li>TiMBL assigns higher probabilities to forms belonging to lemmas with letter trigraphs that yield more diverse activations</li> <li>Those trigraphs belong to a rich exemplar space in the memory</li> <li>it would be expected that higher probabilities would result in shorter response latencies</li> <li>However, NDL's G2L-Diversity was in fact positively correlated with RTs, indicating inhibition, i. e. slower recognition.</li> <li>TiMBL probabilities are intended to capture the likelihood of a form's occurrence in production.</li> <li>in comprehension (lexicality judgments) high trigraphs diversity may hurt results</li> <li>Spontaneous recovery from extinction</li> <li>After a CS is learned to associated with a given Conditioned Response (CR), this association is unlearned</li> <li>Theoretically, it can not arise again without retraining</li> <li>But in real life, sometimes seemingly completely forgotten associations are reactivated</li> <li>shows extinction is not unlearning</li> <li>responses that disappear are not necessarily forgotten</li> <li>Suggests loss of activation is not simply the mirror of acquiring associations</li> <li>Given two conditions stimuli, (CS) where one is more salient, the more salient CS will develop a strong association with the CR (Conditioned Response)</li> <li>Some linguistic things can be learned with NDL and this might show use something about the problem</li> <li>What made NDL so nice for animal learning might not scale up to linguistic phenomena</li> <li>Inductive approaches to cognition</li> </ul>"},{"location":"KB/Minerva/","title":"Minerva","text":""},{"location":"KB/Minerva/#minerva","title":"Minerva","text":"<ul> <li>Language model capable of solving mathematical and scientific questions using step-by-step reasoning</li> <li>very clear focus on the collection of training data for this purpose</li> <li>solves quantitative reasoning problems,</li> <li>makes models at scale and employs best-in-class inference techniques</li> <li>Concretely, Minerva solves these problems by generating solutions step-by-step</li> <li>this means including calculations and symbolic manipulation without having the need for external tools such a calculator.</li> </ul>"},{"location":"KB/Mini%20Batch%20GD/","title":"Mini Batch GD","text":""},{"location":"KB/Mini%20Batch%20GD/#mini-batch-gd","title":"Mini Batch GD","text":"<ul> <li> \\[\\theta= \\theta-\\eta \\cdot \\nabla_{\\theta}J(\\theta; x^{i:i+n};y^{i;i+n})\\] </li> </ul>"},{"location":"KB/Minimal%20Semantic%20Commitment/","title":"Minimal Semantic Commitment","text":""},{"location":"KB/Minimal%20Semantic%20Commitment/#minimal-semantic-commitment","title":"Minimal Semantic Commitment","text":"<ul> <li>(Frazier et al., 1999)</li> <li>The MSC hypothesis distinguishes between two types of mental representations the processor might entertain upon encountering an underdetermined semantic constituent: if the representation is ambiguous, the processor will commit to just one interpretation and later revise it if necessary, but if the representation is vague, the processor refrains from committing to an interpretation, leaving some features underdetermined until further information is made available.</li> </ul>"},{"location":"KB/Minimization%20and%20reporting%20of%20negative%20impacts/","title":"Minimization and reporting of negative impacts","text":""},{"location":"KB/Minimization%20and%20reporting%20of%20negative%20impacts/#minimization-and-reporting-of-negative-impacts","title":"Minimization and reporting of negative impacts","text":"<ul> <li>reporting actions or decisions that yield a certain outcome by the system</li> <li>assessment of those outcomes</li> <li>consider the identification, assessment, documentation and minimization of their potential negative impacts</li> </ul>"},{"location":"KB/Minimizing%20Communication/","title":"Minimizing Communication","text":""},{"location":"KB/Minimizing%20Communication/#minimizing-communication","title":"Minimizing Communication","text":"<ul> <li>Reduce the number of messages passed</li> <li>Reduce amount of data passed in messages</li> </ul>"},{"location":"KB/Minkowski%20Distance/","title":"Minkowski Distance","text":""},{"location":"KB/Minkowski%20Distance/#minkowski-distance","title":"Minkowski Distance","text":"<ul> <li> \\[D(x,y) = (\\Sigma_{i=1}^{n}|x_{i}-y_{i}|^{p})^{\\frac{1}{p}}\\] </li> <li>It is a metric used in Normed vector space (n-dimensional real space), which means that it can be used in a space where distances can be represented as a vector that has a length.<ul> <li>Zero Vector \u2014 The zero vector has a length of zero whereas every other vector has a positive length. For example, if we travel from one place to another, then that distance is always positive. However, if we travel from one place to itself, then that distance is zero.</li> <li>Scalar Factor \u2014 When you multiple the vector with a positive number its length is changed whilst keeping its direction. For example, if we go a certain distance in one direction and add the same distance, the direction does not change.</li> <li>Triangle Inequality \u2014 The shortest distance between two points is a straight line.</li> </ul> </li> <li>Most interestingly about this distance measure is the use of parameter p. We can use this parameter to manipulate the distance metrics to closely resemble others.</li> <li>Common values of p are:<ul> <li>p=1 \u2014 Manhattan Distance</li> <li>p=2 \u2014 Euclidean Distance</li> <li>p=\\(\\infty\\) \u2014 Chebyshev Distance</li> </ul> </li> <li>The upside to p is the possibility to iterate over it and find the distance measure that works best for your use case.</li> </ul>"},{"location":"KB/Mirman%20et%20al/","title":"Mirman et al.","text":""},{"location":"KB/Mirman%20et%20al/#mirman-et-al","title":"Mirman Et Al.","text":"<ul> <li>train syllables in words, predicting the next syllable</li> <li>use network to train on different types of individual words, matching them with one of five objects, simulating word learning</li> <li>75 epocs 1000 syllable sequence, then it predicted almost perfectly the next syllable (teaching phonotactics of the language)</li> <li>Model trained to recognize one of five objects for each of five different two-syllable input patterns of three types 1. words (100% transitional probability) 2. partwords (25% probability transitions) 3. nonwords (0% transitions)</li> <li>Model is better at mapping two-syllable sequences to words when it has already been exposed to those sequences and they had high probabilities</li> <li>Novel-sequence non-word labels initially learned nearly as fast as word up to intermediate point.</li> <li>exposure to familiarization input allowed network to created distinct hidden representations for each syllable</li> <li>SRN can show how statistical learning supports word learning, showing a link</li> <li>Humans are good at learning sequences, even when the data is presented implicitly and even when the relationships are non-adjacent</li> <li>We aren't just sensitive to frequency: we are sensitive to actual transitional probabilities</li> <li>SRNs with very simple assumptions model non-adjacent learning and transitional probabilities</li> <li>Biological arguments for distributed representations</li> <li>Makes more sense that neurons get randomly assigned to be active for different inputs</li> <li>We can start with randomness and with learning it will become structured</li> <li>concepts are just bundles of features, that together become something</li> <li>Prevents catastrophic failures</li> </ul>"},{"location":"KB/Mirror%20Shift%20Function/","title":"Mirror Shift Function","text":""},{"location":"KB/Mirror%20Shift%20Function/#mirror-shift-function","title":"Mirror Shift Function","text":"<ul> <li>With the Mirror Shift Function, a job is converted to the job in which the path is symmetrical to that of the original job.</li> </ul>"},{"location":"KB/Misyak%20et%20al%202010/","title":"Misyak et al 2010","text":""},{"location":"KB/Misyak%20et%20al%202010/#misyak-et-al-2010","title":"Misyak Et Al 2010","text":"<ul> <li>Does the ability to learn statistical non-adjacent dependencies correlate with the ability to process non-adjacent dependencies in language?</li> <li>Can we model non-adjacent dependency learning with simple SRNs?</li> <li>allows us to see the continuous timecourse of statistical processing</li> <li>Uses both linguistic stimulus tokens and auditory cues</li> <li>on-line non-adjacency learning</li> <li>Investigation of Individual differences in language processing and statistical learning</li> <li>Participants trained in blocks of three word sequence trials.</li> <li>First and second word were random, but the third word was dependent on the first word.<ul> <li>Intervening second word creates non-adjacency</li> </ul> </li> <li>After final block: Prediction task where participants had to say what the third word was from two word sequences</li> <li>People can learn non-adjacent sequences with only implicit exposure</li> <li>SRN can capture performance on AGL tasks</li> <li>SRNs can deal with temporal structures and associations</li> <li>Localist representations: 30 input and output units, each unique unit corresponding to each nonword</li> <li>Standard backpropagation with a learning rate of 0.1 and momentum at 0.8</li> <li>The higher the prediction task accuracy (x-axis) the shorter reading times for object relatives.</li> <li>Even the people who are bad at sequential learning are still fluent speakers and listeners</li> <li>Is it possible that sequential learning and language learning are unrelated</li> <li>Maybe children are better at sequential learning, which helps them acquire languag</li> <li>Adults then lose this ability</li> </ul>"},{"location":"KB/Mixed%20Effect%20Models/","title":"Mixed Effect Models","text":""},{"location":"KB/Mixed%20Effect%20Models/#mixed-effect-models","title":"Mixed Effect Models","text":"<ul> <li>Are able to combine fixed factors and multiple KB/Random Factors.md in one analysis No longer necessary to do two ANOVAs</li> </ul>"},{"location":"KB/Mixed%20Example/","title":"Mixed Example","text":""},{"location":"KB/Mixed%20Example/#mixed-example","title":"Mixed Example","text":"<ul> <li>experimented with 14 different types of augmentation approaches. The output image is generated using the following techniques: vertical concatenation, horizontal concatenation, mixed concatenation, random 2x2, VH- mixup (vertical concatenation, horizontal concate- nation, and mixup), VH-BC+ (vertical concatena- tion, horizontal concatenation, and between-class), random square, random column interval, random row interval, random rows, random columns, ran- dom pixels, random elements, and noisy mixup.</li> <li>From all these approached, VHmixup has the best performance.</li> </ul>"},{"location":"KB/Mixed%20chunk%20attention/","title":"Mixed chunk attention","text":""},{"location":"KB/Mixed%20chunk%20attention/#mixed-chunk-attention","title":"Mixed Chunk Attention","text":"<ul> <li>an efficient linear approximation method that combines the benefits from partial and linear attention mechanisms, which is accelerator-friendly and highly competitive in quality.</li> <li>The method works on chunks of tokens and leverages local (within chunk) and global (between chunks) attention spans</li> </ul>"},{"location":"KB/Mixup/","title":"Mixup","text":""},{"location":"KB/Mixup/#mixup","title":"Mixup","text":"<ul> <li>@zhangMixupEmpiricalRisk2018</li> <li>Randomly sample two examples \\((x_{i}, y_{i})\\) and \\((x_{j}, y_{j})\\)</li> <li>New example by weighted 1D piecewise linear interpolation</li> <li> \\[ \\hat x = \\lambda x_{i}+(1-\\lambda)x_{j} \\] </li> <li> \\[ \\hat y = \\lambda y_{i}+(1-\\lambda)y_{j} \\] </li> <li>$\\lambda \\in 0,1</li> <li>New example \\((\\hat x, \\hat y)\\)</li> </ul>"},{"location":"KB/MoCO/","title":"MoCO","text":"<p>toc: true title: MoCO</p> <p>categories: ['temp']</p>"},{"location":"KB/MoCO/#moco","title":"MoCO","text":"<ul> <li>Momentum Contrast for Unsupervised Visual Representation Learning</li> <li>unsupervised visual representation learning</li> <li>contrastive learning as dictionary look-up, MoCo builds a dynamic dictionary with a queue and a moving-averaged encoder</li> <li>large and consistent dictionary on-the-fly</li> <li>ImageNet</li> <li>transfer well to downstream tasks.</li> <li>PASCAL VOC</li> <li>COCO</li> <li>visual representation encoder by matching an encoded query</li> <li>to a dictionary of encoded keys using a contrastive loss</li> <li>dictionary is built as a queue, with the current mini-batch enqueued</li> <li>oldest mini-batch dequeued</li> <li>slowly progressing encoder</li> <li>momentum update with the query encoder</li> <li></li> <li></li> </ul>"},{"location":"KB/Mobile%20Net/","title":"Mobile Net","text":""},{"location":"KB/Mobile%20Net/#mobile-net","title":"Mobile Net","text":"<ul> <li>@howardMobilenetsEfficientConvolutional2017</li> <li>@sandlerMobilenetv2InvertedResiduals2018</li> <li>Depthwise Separable</li> </ul>"},{"location":"KB/MobileOne/","title":"MobileOne","text":""},{"location":"KB/MobileOne/#mobileone","title":"MobileOne","text":"<ul> <li>An Improved One Millisecond Mobile Backbone</li> <li>extensive analysis of different metrics by deploying several mobile friendly networks on a mobile device</li> <li>identify and analyze architectural and optimization bottlenecks</li> <li>many times faster on mobile</li> <li>Inspired byRepVGG</li> <li>Either ReLU or SE-ReLU is used as activation. The trivial over-parameterization factor \\(k\\) is a hyperparameter which is tuned for every variant.</li> <li>better top-1 accuracy on ImageNet than EfficientNet at similar latency</li> <li></li> </ul>"},{"location":"KB/Modality%20Dropout/","title":"Modality Dropout","text":""},{"location":"KB/Modality%20Dropout/#modality-dropout","title":"Modality Dropout","text":"<ul> <li>MDO improves fine-tuning by randomly dropping one of the modalities</li> </ul>"},{"location":"KB/Modality/","title":"Modality","text":""},{"location":"KB/Modality/#modality","title":"Modality","text":"<ul> <li>A high-level data category. For example, numbers, text, images, video, and audio are five different modalities.</li> </ul>"},{"location":"KB/Mode%20Collapse/","title":"Mode Collapse","text":""},{"location":"KB/Mode%20Collapse/#mode-collapse","title":"Mode Collapse","text":"<ul> <li>Generator collapses and only predicts mean/median/mode of data instead of the prob distribution</li> </ul>"},{"location":"KB/Mode%20Switch/","title":"Mode Switch","text":""},{"location":"KB/Mode%20Switch/#mode-switch","title":"Mode Switch","text":"<ul> <li>As per safety standards, an industrial robot has three distinct modes of operation. These are Teach (also called Manual) and Play (also called Automatic) and Remote. Switching between these modes is performed using a key switch on the teach pendant and is called Mode Switch.</li> </ul>"},{"location":"KB/Modeling%20Driver%20Behavior%20with%20Cognitive%20Architecture/","title":"Modeling Driver Behavior with Cognitive Architecture","text":""},{"location":"KB/Modeling%20Driver%20Behavior%20with%20Cognitive%20Architecture/#modeling-driver-behavior-with-cognitive-architecture","title":"Modeling Driver Behavior with Cognitive Architecture","text":"<ul> <li>Salvucci, Dario D. \"Modeling driver behavior in a cognitive architecture.\" Human factors 48.2 (2006): 362-380.</li> </ul>"},{"location":"KB/Modeling%20Driver%20Behavior%20with%20Cognitive%20Architecture/#intro","title":"Intro","text":"<p>This paper explores the development of a rigorous computational model of driver behavior in a cognitive architecture \u2013 a computational framework with underlying psychological theories that incorporate basic properties and limitations of the human system</p> <p>An integrated driver model developed in the ACT-R (Adaptive Control of Thought-Rational) cognitive architecture is described that focuses on the component processes of control, monitoring, and decision making in a multilane highway environment</p> <p>This model accounts for the steering profiles, lateral position profiles, and gaze distributions of human drivers during lane keeping, curve negotiation, and lane changing.</p> <p>The model demonstrates how cognitive architectures facilitate understanding of driver behavior in the context of general human abilities and constraints and how the driving domain benefits cognitive architectures by pushing model development toward more complex, realistic tasks</p>"},{"location":"KB/Modeling%20Driver%20Behavior%20with%20Cognitive%20Architecture/#driving-and-integrated-driver-modeling","title":"Driving and Integrated Driver Modeling","text":"<p>useful to view driving and driver modeling in the context of the embodied cognition, task, and artifact (ETA) framework (Byrne, 2001; Gray, 2000; Gray &amp; Boehm-Davis, 2000).</p> <p>As the name suggests, this framework emphasizes three components of an integrated modeling effort: the task that a person attempts to perform, the artifact</p> <p>by which the person performs the task, and the embodied cognition by which the person perceives, thinks, and acts in the world through the artifact</p> <p>A sound understanding of each component is critical to developing rigorous integrated models of driver behavior.</p> <p>Michon (1985) identified three classes of task processes for driving: operational processes that involve manipulating control inputs for stable driving, tactical processes that govern safe interactions with the environment and other vehicles, and strategic processes for higher level reasoning and planning</p> <p>Some tasks are not continual but intermittent, arising in specific situations \u2013 for instance, parking a vehicle at a final destination.</p> <p>Between cognition and the vehicle lies the embodiment of the driver, namely the perceptual processes (visual, aural, vestibular, etc.) and motor processes (hands, feet) that provide the input from and output to the external world.</p> <p>Not surprisingly, there can be parallelism in this integrated system \u2013 for instance, moving the hand while visually encoding the lead car \u2013 but there are also capacity constraints and/or bottlenecks that sometimes result in degraded performance.</p>"},{"location":"KB/Modeling%20Driver%20Behavior%20with%20Cognitive%20Architecture/#pictures","title":"Pictures","text":""},{"location":"KB/Modeling%20Transfer/","title":"Modeling Transfer","text":""},{"location":"KB/Modeling%20Transfer/#modeling-transfer","title":"Modeling Transfer","text":"<ul> <li>given levels of mastery on a set of knowledge components, predict the performance on a new problem (Singley &amp; Anderson, 1989).</li> <li>For instance, suppose a student has mastered 15 of the 20 knowledge components required to do a task.</li> <li>This predicts the student's behavior on the task\u2014where the student will ask for help, how long it will take to do the task, which errors occur, etc</li> <li>However, these predictions can be inaccurate if the assumed knowledge components are not accurate reflections of how the student actually understands the task domain</li> </ul>"},{"location":"KB/Modeling%20motivation%20using%20goal%20competition%20in%20mental%20fatigue%20studies/","title":"Modeling motivation using goal competition in mental fatigue studies","text":""},{"location":"KB/Modeling%20motivation%20using%20goal%20competition%20in%20mental%20fatigue%20studies/#modeling-motivation-using-goal-competition-in-mental-fatigue-studies","title":"Modeling motivation using goal competition in mental fatigue studies","text":"<ul> <li> <p>Mega B. Herlambang a,b,\u2217, Niels A. Taatgen a, Fokie Cnossen</p> </li> <li> <p>Motivation can counteract the effects of mental fatigue</p> </li> <li>goal competition as a paradigm to understand the role of motivation and built three models of mental fatigue studies to demonstrate the mechanism in a cognitive architecture named PRIM</li> <li>model changes in performance levels by adjusting the value of the main task goals</li> <li>which controls the competition with distractions</li> <li>best model fits were obtained by a linear decrease in goal activation</li> <li>Modeling fatigue and motivation decline in PRIMs</li> <li>The assumption in this paper is that the decrease in task performance in mental fatigue is the result of a reduction in task motivation.</li> <li>reflected in a reduction in activation of the task goal over time</li> <li>As time progresses, individuals may experience an increase in the feeling of fatigue that reduces the subjective value of the main task, i.e., task motivation (Mu\u0308ller &amp; Apps, 2019) or goal activation (Agoal) in our models</li> <li>Consequently, goal activation is discounted by the feeling of fatigue</li> <li>perceived reward from doing the task (extrinsically or intrinsically) maintains the goal activation from declining.</li> <li>\\(\\(A_{goal(t)} = min(1 \\vee (P(t) - F(t)))\\)\\),</li> <li>Therefore, the relationship between the perceived reward (P), the feeling of fatigue (F), and goal activation (Agoal) at any given time (t) is</li> <li>goal activation value of one means that the model mainly focuses its attention on the main goal.</li> <li>The value P is influenced by previous perceived rewards. For example, when the previous incentive at t \u2212 1 is perceived as more valuable than the recent one at t, then the value Pt is smaller than Pt\u22121</li> <li>In contrast, the value of Pt is higher if the reward at t is perceived as more valuable than the previous one at t\u22121</li> <li>It is evident that motivation affects the ability to stay focused on a task and not be distracted by internal or external distractions (Herlambang et al., 2019)</li> <li>In the case of external distractions, task-unrelated stimuli may shift attention away from the main task, while internal distractions may manifest itself in the form of mind-wandering</li> </ul>"},{"location":"KB/Modeling%20motivation%20using%20goal%20competition%20in%20mental%20fatigue%20studies/#discussion","title":"Discussion","text":"<ul> <li>that goal competition is one of the key factors to understand the underlying mechanism of motivation in mental fatigue.</li> <li>decrease in performance is not due to a decrease in the capacity of the cognitive system (e.g., lower working memory capacity, slower motor system, less reliable long-term memory) but by a decrease in the ratio of cognitive ''cycles'' spent on the task as opposed to distractions.</li> <li>have modeled this by a decrease in the activation of the goal, which represents the level of motivation, which indirectly affects performance</li> <li>modeler can tune some parameters to obtain a good fit</li> <li>However, overdoing such parameter tuning may lead to overfitting, which makes the models difficult to generalize and does not represent the empirical data</li> </ul>"},{"location":"KB/Modeling%20motivation%20using%20goal%20competition%20in%20mental%20fatigue%20studies/#goal-activation-and-performance","title":"Goal activation and performance","text":"<ul> <li>To lower performance in the nonreward conditions in all tasks, we decreased task goal activation values over time</li> <li>The goal activation values in our models represent the subjective value of performing the tasks, with a high subjective value corresponds to a high level of motivation</li> <li>The reduction of goal activation was due to an increase in the feeling of fatigue (see Mu\u0308ller &amp; Apps, 2019) and a continuous decrease in the perceived reward from doing the tasks</li> <li>What our modeling efforts suggest is that over time, while the activation value of the main active goal is decreasing, which is due to an increase in the feeling of fatigue and a decrease in the perceived reward (see Eq. (3)), another future goal of an activity/stimulus, for example, a distraction, may start winning the competition with the main task, when the activation value of the distraction exceeds that of the main goal (i.e., it strongly attracts the individual), in which case the individual may start paying attention to the distraction. The distraction can become the new active goal, and the individual may forget the main goal, or choose to pay attention to both, but this will sacrifice performance.</li> <li>Goal competition is a continuous process that compares several future goals, and when the main task goal is perceived to be less valuable, another competing goal may start winning the competition, causing the individual to invest less mental effort in the main task and start investing in the competing goal</li> </ul>"},{"location":"KB/Modeling%20motivation%20using%20goal%20competition%20in%20mental%20fatigue%20studies/#limitation-challenge-and-future-research","title":"Limitation, challenge, and future research","text":"<ul> <li>solely adjusting goal activation levels may not be enough to model changes in performance.</li> <li>There are many parameters in PRIMs that can affect performance</li> <li>Although adjusting goal activation values as a way to model mental fatigue showed good results for the experiments we modeled, it is possible that this does not directly generalize to other studies</li> </ul>"},{"location":"KB/Modeling%20motivation%20using%20goal%20competition%20in%20mental%20fatigue%20studies/#images","title":"Images","text":""},{"location":"KB/Moment%20Exchange/","title":"Moment Exchange","text":""},{"location":"KB/Moment%20Exchange/#moment-exchange","title":"Moment Exchange","text":"<ul> <li>encouraging the models to utilize the moment information of latent features </li> <li>Specifically, the moments of the learned features of one training image are re- placed by those of another.</li> </ul>"},{"location":"KB/Moment%20in%20Time/","title":"Moment in Time","text":""},{"location":"KB/Moment%20in%20Time/#moment-in-time","title":"Moment in Time","text":"<ul> <li>large balanced and diverse dataset </li> <li>video under- </li> <li>standing </li> <li>1 million video clips that cover 339 classes, and each video lasts around 3 seconds </li> <li>average number of video clips for each class is 1, 757 with a median of 2, 775 </li> <li>videos that capturing visual and/or audible actions, produced by humans, animals, objects or nature</li> </ul>"},{"location":"KB/Momentum/","title":"Momentum","text":""},{"location":"KB/Momentum/#momentum","title":"Momentum","text":"<ul> <li>Momentum is the velocity of a body multiplied by its mass. A small force can quickly stop an object with low momentum, but a large or prolonged force is required to stop an object with high momentum.</li> <li> \\[p = mv\\] </li> <li>mass times velocity</li> </ul>"},{"location":"KB/Monk/","title":"Monk","text":""},{"location":"KB/Monk/#monk","title":"Monk","text":"<ul> <li>Hit list</li> <li></li> </ul>"},{"location":"KB/Moral%20Machine%20project/","title":"Moral Machine project","text":""},{"location":"KB/Moral%20Machine%20project/#moral-machine-project","title":"Moral Machine Project","text":"<ul> <li>MIT</li> <li>wisdom of the crowd to find resolutions for ethical dilemmas</li> <li>studying the perception of autonomous vehicles (AVs) which are controlled by AI and has the potential to harm pedestrians and/or passengers if they malfunction</li> <li>allows participants to judge various ethical dilemmas facing AVs which have malfunctioned, and select which outcomes they prefer.</li> <li>saving more lives</li> <li>protecting passengers</li> <li>upholding the law</li> <li>avoiding intervention</li> <li>gender preference</li> <li>species preference</li> <li>age</li> <li>social value preference.</li> <li>people generally prefer the AV to make sacrifices if more lives can be saved.</li> <li>self-reported preferences often do not align well with actual behaviours</li> </ul>"},{"location":"KB/Moral%20decision%20making%20frameworks%20for%20artificial%20intelligence/","title":"Moral decision making frameworks for artificial intelligence","text":""},{"location":"KB/Moral%20decision%20making%20frameworks%20for%20artificial%20intelligence/#moral-decision-making-frameworks-for-artificial-intelligence","title":"Moral Decision Making Frameworks for Artificial Intelligence","text":"<ul> <li>Vincent Conitzer, Walter Sinnott- Armstrong, Jana Schaich Borg, Yuan Deng, and Max Kramer.</li> <li>developing a general ethical decision making framework for AI based on game theory and machine learning</li> <li>For the game theory based framework, the authors suggest the extensive form (a generalization of game KB/Trees.md) as a foundation scheme to represent dilemmas</li> <li>current extensive form does not account for protected values in which an action can be treated as unethical regardless of its consequence</li> <li>extend the extensive form representation with passive actions for agents to select in order to be ethical</li> <li>machine learning based ethical decision-making</li> <li>classify whether a given action under a given scenario is morally right or wrong</li> <li>The main challenge in machine learning based moral decision-making is to design a generalizable representation of ethical dilemmas</li> <li>Game theory and machine learning can be combined into one framework in which game theoretic analysis of ethics is used as a feature to train machine learning approaches</li> </ul>"},{"location":"KB/MoralDM/","title":"MoralDM","text":""},{"location":"KB/MoralDM/#moraldm","title":"MoralDM","text":"<ul> <li>enables an agent to resolve ethical dilemmas by leveraging on two mechanisms</li> <li>1) first-principles reasoning, which makes decisions based on well-established ethical rules (e.g., protected values); and 2) analogical reasoning, which compares a given scenario to past resolved similar cases to aid decision-making.</li> <li>the exhaustive comparison approach by MoralDM is expected to become computationally intractable</li> <li>[Blass and Forbus, 2015], MoralDM is extended with structure mapping which trims the search space by computing the correspondences, candidate inferences and similarity scores between cases to improve the efficiency of analogical generalization</li> </ul>"},{"location":"KB/MoreMVCNN/","title":"MoreMVCNN","text":""},{"location":"KB/MoreMVCNN/#moremvcnn","title":"MoreMVCNN","text":""},{"location":"KB/Morpheme%20Generation/","title":"Morpheme Generation","text":""},{"location":"KB/Morpheme%20Generation/#morpheme-generation","title":"Morpheme Generation","text":"<ul> <li>See +past.verb = saw</li> </ul>"},{"location":"KB/Morpheme%20Segmentation/","title":"Morpheme Segmentation","text":""},{"location":"KB/Morpheme%20Segmentation/#morpheme-segmentation","title":"Morpheme Segmentation","text":"<ul> <li>De-nation-al-iz-ation</li> </ul>"},{"location":"KB/Morpheme/","title":"Morpheme","text":""},{"location":"KB/Morpheme/#morpheme","title":"Morpheme","text":"<ul> <li>words are built from smaller meaningful units called morphemes</li> <li>Allomorph</li> <li>Morphology Stem</li> <li>Morphology Affix</li> <li>Content Morpheme</li> <li>Functional Morpheme</li> <li>Morpheme Segmentation</li> <li>Morpheme Generation</li> <li>Morphotactic</li> </ul>"},{"location":"KB/Morphology%20Affix/","title":"Morphology Affix","text":""},{"location":"KB/Morphology%20Affix/#morphology-affix","title":"Morphology Affix","text":"<ul> <li>Bits and pieces that adhere to stems to change their meanings and grammatical functions</li> <li>Bound morpheme</li> <li>Prefix</li> <li>Suffix</li> <li>Infix</li> <li>Circumfix</li> </ul>"},{"location":"KB/Morphology%20Stem/","title":"Morphology Stem","text":""},{"location":"KB/Morphology%20Stem/#morphology-stem","title":"Morphology Stem","text":"<ul> <li>The core meaning bearing units \u2013 Main morpheme of the word</li> <li>Free morpheme</li> </ul>"},{"location":"KB/Morphology/","title":"Morphology","text":""},{"location":"KB/Morphology/#morphology","title":"Morphology","text":"<ul> <li>structure of words</li> <li>Morpheme</li> <li>It is concerned with inflection.</li> <li>It is also concerned with derivation of new words from existing ones, eg. lighthouse (formed from light &amp; house)</li> <li>Needs a Lexicon</li> <li>Inflectional Morphology</li> <li>Derivational Morphology</li> <li>Suppletion</li> <li>Word Compounding</li> <li>Word Blending</li> <li>Word Clipping</li> <li>Lemmatization</li> </ul>"},{"location":"KB/Morphotactic/","title":"Morphotactic","text":""},{"location":"KB/Morphotactic/#morphotactic","title":"Morphotactic","text":"<ul> <li>Which class of morphemes follow other class of morphemes</li> <li>Plural morphemes follow noun</li> <li>some endings go only on certain words not on everything.</li> <li>Do + er : doer</li> <li>Be + er :beer</li> </ul>"},{"location":"KB/Motor%20Memories/","title":"Motor Memories","text":""},{"location":"KB/Motor%20Memories/#motor-memories","title":"Motor Memories","text":"<ul> <li>motor memory is unique</li> <li>Some studies on Alzheimer\u2019s disease included participants who were previously musicians and couldn\u2019t remember their own families, but they could still play beautiful music. Clearly, there\u2019s a huge difference in the way that motor memories are formed</li> <li>Memories are thought to be encoded in the brain in the pattern of activity in networks of hundreds or thousands of neurons, sometimes distributed across distant brain regions</li> <li>memory trace</li> <li>When the researchers tested the animals\u2019 memory of this new skill weeks later, they found that those mice that still remembered the skill showed increased activity in the same neurons that were first identified during the learning period, showing that these neurons were responsible for encoding the skill</li> <li>two-photon microscopy</li> <li>\u201cengram neurons\u201d reprogram themselves as the mice learned</li> <li>Motor cortex engram cells took on new synaptic inputs \u2014 potentially reflecting information about the reaching movement \u2014 and themselves formed powerful new output connections in a distant brain region called the dorsolateral striatum \u2014 a key waystation through which the engram neurons can exert refined control over the animal\u2019s movements.</li> <li>These findings suggest that, in addition to being dispersed, motor memories are highly redundant.</li> <li>The researchers say that as we repeat learned skills, we are continually reinforcing the motor engrams by building new connections \u2014 refining the skill. It\u2019s what is meant by the term muscle memory \u2014 a refined, highly redundant network of motor engrams used so frequently that the associated skill seems automatic.</li> <li>Current thinking is that Parkinson\u2019s disease is the result of these motor engrams being blocked, but what if they\u2019re actually being lost and people are forgetting these skills?</li> </ul>"},{"location":"KB/Multi%20Head%20Attention/","title":"Multi Head Attention","text":""},{"location":"KB/Multi%20Head%20Attention/#multi-head-attention","title":"Multi Head Attention","text":"<ul> <li>ZihangDai et al., 2019</li> <li>which computes self-attention over the inputs, then adds back the residual and layer normalizes everything. The attention head can be split into multiple segments, hence the name\u00a0multi-head</li> <li>Multiple attention instances, each focusing on a different part of the input</li> <li>Words can mean different things in context<ul> <li>If using Self Attention, then this just gets summed up. Which is not very nice</li> <li>Several attention heads -&gt; different output vectors</li> <li>Concatenate them and pass through a linear transform -&gt; dimension back to k</li> </ul> </li> <li> \\[MultiHead(Q,K,V) = Concat(head_1, head_2, \u2026., head_h)W^O\\] <ul> <li> \\[head_i = Attention(QW_i^Q, KW_i^K , VW_i^V)\\] </li> </ul> </li> <li>W is learnable projections for attention params</li> <li></li> <li>To improve efficiency<ul> <li>Cut the incoming vector into chunks -&gt; no of attention heads</li> </ul> </li> </ul> <pre><code>class MultiHeadAttentionNew(nn.Module):\n    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):\n        super().__init__()\n        self.n_head = n_head\n\n        self.w_qs = nn.Linear(d_model, n_head * d_k)\n        self.w_ks = nn.Linear(d_model, n_head * d_k)\n        self.w_vs = nn.Linear(d_model, n_head * d_v)\n\n        nn.init.normal_(self.w_qs.weight, mean=0, std=np.sqrt(2.0 / (d_model + d_k)))\n        nn.init.normal_(self.w_ks.weight, mean=0, std=np.sqrt(2.0 / (d_model + d_k)))\n        nn.init.normal_(self.w_vs.weight, mean=0, std=np.sqrt(2.0 / (d_model + d_v)))\n\n        self.fc = nn.Linear(n_head * d_v, d_model)\n        nn.init.xavier_normal_(self.fc.weight)\n        self.dropout = nn.Dropout(p=dropout)\n        self.layer_norm = nn.LayerNorm(d_model)\n\n    def forward(self, q, k, v, mask=None):\n        residual = q\n        q = rearrange(self.w_qs(q), 'b l (head k) -&gt; head b l k', head=self.n_head)\n        k = rearrange(self.w_ks(k), 'b t (head k) -&gt; head b t k', head=self.n_head)\n        v = rearrange(self.w_vs(v), 'b t (head v) -&gt; head b t v', head=self.n_head)\n        attn = torch.einsum('hblk,hbtk-&gt;hblt', [q, k]) / np.sqrt(q.shape[-1])\n        if mask is not None:\n            attn = attn.masked_fill(mask[None], -np.inf)\n        attn = torch.softmax(attn, dim=3)\n        output = torch.einsum('hblt,hbtv-&gt;hblv', [attn, v])\n        output = rearrange(output, 'head b l v -&gt; b l (head v)')\n        output = self.dropout(self.fc(output))\n        output = self.layer_norm(output + residual)\n        return output, attn\n</code></pre>"},{"location":"KB/Multi%20Task%20Learning/","title":"Multi Task Learning","text":""},{"location":"KB/Multi%20Task%20Learning/#multi-task-learning","title":"Multi Task Learning","text":"<ul> <li>multiple outputs (as desired), and typically a shared trunk of weights can indirectly encode common or shared knowledge</li> <li>Can linearly combine loss for each task \\(L_{i}\\)</li> <li> \\[L(x,y, \\theta) = \\Sigma_{i}w_{i}L_{i}(f_{i}(x), y, \\theta_{i})\\] <ul> <li>\\(f_{i}(x)\\) is output head with weights \\(\\theta_{i}\\)</li> </ul> </li> <li>The exact scale of the weights does not matter as multiplying the loss by a positive scalar does not change the optimum.</li> <li>Hard Parameter Sharing</li> <li>Soft Parameter Sharing</li> <li>augment </li> <li>Attribute Selection</li> <li>Eavesdropping</li> <li>Representation Bias</li> </ul>"},{"location":"KB/Multi%20Teacher%20Distillation/","title":"Multi Teacher Distillation","text":""},{"location":"KB/Multi%20Teacher%20Distillation/#multi-teacher-distillation","title":"Multi Teacher Distillation","text":"<ul> <li>The multiple teacher networks can be individually and integrally used for distillation during the period of training a student network.</li> <li>To transfer knowledge from multiple teachers, the simplest way is to use the averaged response from all teachers as the supervision signal (Hinton et al., 2015)</li> <li>In addi- tion to the averaged logits from all teachers, You et al. (2017) further incorporated features from the inter- mediate layers in order to encourage the dissimilarity among different training samples.</li> <li>Fukuda et al. (2017) randomly selected one teacher from the pool of teacher networks at each it- eration. To transfer feature-based knowledge from mul- tiple teachers, additional teacher branches are added to the student networks to mimic the intermediate features of teachers (Park and Kwak, 2020; Asif et al., 2020). Born again networks address multiple teach- ers in a step-by-step manner, i.e., the student at the t step is used as the teacher of the student at the t+1 step (Furlanelloetal., 2018)</li> <li>To effi- ciently perform knowledge transfer and explore the power of multiple teachers, several alternative meth- ods have been proposed to simulate multiple teach- ers by adding different types of noise to a given teacher (Sau and Balasubramanian, 2016) or by us- ing stochastic blocks and skip connections (Lee et al., 2019c). Using multiple teacher models with feature ensembles, knowledge amalgamation is designed in (Shen et al., 2019a; Luo et al., 2019; Shen et al., 2019b; Luo et al., 2020). Through knowledge amalgamation, many public available trained deep models as teachers can be reused.</li> </ul>"},{"location":"KB/Multi%20Variate%20AR/","title":"Multi Variate AR","text":""},{"location":"KB/Multi%20Variate%20AR/#multi-variate-ar","title":"Multi Variate AR","text":"<ul> <li>predict future from past of another time series</li> </ul>"},{"location":"KB/MultiReader%20technique/","title":"MultiReader technique","text":""},{"location":"KB/MultiReader%20technique/#multireader-technique","title":"MultiReader Technique","text":"<ul> <li>which allows domain adaptation</li> <li>training a more accurate model that supports multiple keywords (i.e., \u201cOK Google\u201d and \u201cHey Google\u201d) as well as multiple languages/dialects</li> </ul>"},{"location":"KB/Multimodal%20Explanation/","title":"Multimodal Explanation","text":""},{"location":"KB/Multimodal%20Explanation/#multimodal-explanation","title":"Multimodal Explanation","text":"<ul> <li>The visual explanation was created by an attention mechanism that conveyed knowledge about what region of the image was important for the decision. This explanation guides the generation of the textual justification out of a LSTM feature, which is a prediction of a classification problem over all possible justifications.</li> </ul>"},{"location":"KB/Multinomial%20Distribution/","title":"Multinomial","text":""},{"location":"KB/Multinomial%20Distribution/#multinomial","title":"Multinomial","text":"<ul> <li> \\[P(D|\\theta) = \\frac{N!}{n_{1}!\u2026n_{l}!}\\Pi_{j=1}^{l}\\theta_{j}^{n_{j}}\\] </li> <li>\\(N = n_{1}+ \u2026.+ n_{l}\\)</li> <li>Generalized Binomial Distribution</li> <li>PMF</li> </ul>"},{"location":"KB/Multiple%20Local%20Minima/","title":"Multiple Local Minima","text":""},{"location":"KB/Multiple%20Local%20Minima/#multiple-local-minima","title":"Multiple Local Minima","text":""},{"location":"KB/Multiple%20Sclerosis/","title":"Multiple Sclerosis","text":""},{"location":"KB/Multiple%20Sclerosis/#multiple-sclerosis","title":"Multiple Sclerosis","text":"<ul> <li>A progressive neurodegenerative disease involving damage to the protective myelin sheaths of nerve cells in the brain and spinal cord. Symptoms include impaired movement, pain, and fatigue.</li> </ul>"},{"location":"KB/Multiple%20constraint-based%20theories/","title":"Multiple constraint-based theories","text":""},{"location":"KB/Multiple%20constraint-based%20theories/#multiple-constraint-based-theories","title":"Multiple Constraint-based Theories","text":"<ul> <li>describe language comprehension as an interactive process whereby all possible syntactic representations are simultaneously partially active and competing for more activation across time</li> <li>Unlike the syntax-first models, multiple sources of information, be they syntactic or non-syntactic, integrate immediately to determine the amount of activation provided to each of the competing alternatives</li> </ul>"},{"location":"KB/Multiplicative%20Attention/","title":"Multiplicative Attention","text":""},{"location":"KB/Multiplicative%20Attention/#multiplicative-attention","title":"Multiplicative Attention","text":"<ul> <li> \\[f_{att}(h_{i}, s_{j}) = h_{i}^{T}W_{a}s_{j}\\] </li> <li>Since Additive Attention performs better for scale, use a factor Scaled Dot Product Attention</li> </ul>"},{"location":"KB/Muse/","title":"Muse","text":""},{"location":"KB/Muse/#muse","title":"Muse","text":"<ul> <li>Text-to-image transformer model</li> <li>state-ofthe-art image generation while being more ecient than difusion or autoregressive models</li> <li>it is trained on a masked modelling task in discrete token space</li> <li>more ecient because of the use of discrete tokens and requiring fewer sampling iterations</li> <li>parallel decoding</li> <li>Muse is 10x faster at inference time than Imagen-3B or Parti-3B and 3x faster than Stable Difusion v 1.4</li> <li>Muse is also faster than than Stable Difusion in spite of both models working in the latent space of a VQGAN</li> </ul>"},{"location":"KB/Myelin/","title":"Myelin","text":""},{"location":"KB/Myelin/#myelin","title":"Myelin","text":"<ul> <li>The fatty substance that encases most nerve cell</li> <li>axons, helping to insulate and protect the nerve fiber and effectively speeding up the transmission of nerve impulses.</li> </ul>"},{"location":"KB/Myocardial%20Infarction/","title":"Myocardial Infarction","text":""},{"location":"KB/Myocardial%20Infarction/#myocardial-infarction","title":"Myocardial Infarction","text":"<ul> <li>Also known as a heart attack, where the heart is deprived of blood due to arterial blockage</li> </ul>"},{"location":"KB/N-dim%20Normal/","title":"N Dim Normal Distribution","text":""},{"location":"KB/N-dim%20Normal/#n-dim-normal-distribution","title":"N Dim Normal Distribution","text":"<ul> <li>Normal Distribution</li> <li>If data points are vectors \\(x = (x_{1}, \u2026, x_{n})'\\) and RVs X_i fulfill the Central Limit Theorem,</li> <li>PDF \\(\\(p(x) = \\frac{1}{(2\\pi)^{n/2}det(\\Sigma)^{\\frac{1}{2}}}exp\\left(-\\frac{1}{2}(x-\\mu)'\\Sigma^{-1}(x-\\mu)\\right)\\)\\)</li> <li>\\(\\mu\\) is expectation $EX_{1}, \u2026, X_{n})'x</li> <li> \\[\\Sigma(i,j) = E[(X_{i} - E[X_{i}])(X_{j}-E[X_{j}])]\\] </li> <li></li> <li> \\[\\hat \\mu = \\frac{1}{N}\\Sigma_{i}x_{i}$$ and $$\\hat \\Sigma = \\frac{1}{N-1}\\Sigma_{i}(x_{i}-\\hat\\mu)(x_{i}-\\hat\\mu)'\\] </li> </ul>"},{"location":"KB/NADAM/","title":"NADAM","text":""},{"location":"KB/NADAM/#nadam","title":"NADAM","text":"<ul> <li>extension of Adam that uses the Nesterov momentum technique to accelerate the optimization convergence further</li> <li>combines the momentum of Nesterov\u2019s method with the adaptive learning rates of Adam</li> <li> \\[ m_t = \\beta_1 * m_{(t-1)} + (1 - \\beta_1) * g_t $$ $$ v_t = \\beta_2 * v_{(t-1)} + (1 - \\beta_2) * g_t2$$ $$ \\hat{m_t} = \\frac{m_t}{1-\\beta_1t}\u00a0$$ $$ \\hat{v_t} = \\frac{v_t}{1-\\beta_2t}\u00a0$$ $$ \\text{parameter} = \\text{parameter} - learning\\_rate * \\frac{\\hat{m_t} + (1-\\beta_1) * g_t}{\\sqrt{\\hat{v_t}} + \\epsilon}\\] </li> <li>Where beta1 and beta2 are two hyperparameters, m_t and v_t are moving averages of the gradients, g_t is the gradient at time t, and learning_rate; epsilon is the same as before.</li> </ul>"},{"location":"KB/NCE/","title":"NCE","text":"<p>toc: true title: NCE</p> <p>categories: ['temp']</p>"},{"location":"KB/NCE/#nce","title":"NCE","text":"<ul> <li>Conditional Negative Sampling for Contrastive Learning of Visual Representations</li> <li>Contrastive Learning</li> <li>noise-contrastive estimation</li> <li>bound on mutual information between two views of an image</li> <li>randomly sampled negative examples to normalize the objective</li> <li>choosing difficult negatives, or those more similar to the current instance, can yield stronger representation</li> <li>Conditional Noise Contrastive Estimator</li> <li>sample negatives conditionally</li> <li>in a \u201cring\u201d around each positive, by approximating the partition function using samples from a class of conditional distributions</li> <li>hese estimators lower-bound mutual information</li> <li>higher bias but lower variance than NCE Bias Vs Variance</li> <li>Applying these estimators as objectives in contrastive representation learning</li> <li>transferring features to a variety of new image distributions from the meta-dataset collection</li> <li>Contrastive Loss</li> </ul>"},{"location":"KB/NIST%202008%20Speaker%20Recognition%20Evaluation%20dataset/","title":"NIST 2008 Speaker Recognition Evaluation dataset","text":""},{"location":"KB/NIST%202008%20Speaker%20Recognition%20Evaluation%20dataset/#nist-2008-speaker-recognition-evaluation-dataset","title":"NIST 2008 Speaker Recognition Evaluation Dataset","text":""},{"location":"KB/NIST%20SRE%202016%20Cantonese/","title":"NIST SRE 2016 Cantonese","text":""},{"location":"KB/NIST%20SRE%202016%20Cantonese/#nist-sre-2016-cantonese","title":"NIST SRE 2016 Cantonese","text":""},{"location":"KB/NLAIC%20Companies/","title":"NLAIC Companies","text":"","tags":["jobs"]},{"location":"KB/NLAIC%20Companies/#nlaic-companies","title":"NLAIC Companies","text":"","tags":["jobs"]},{"location":"KB/NLAIC%20Companies/#axveco","title":"Axveco","text":"<p>Open Sollicitatie - AI</p> <p>Hello!</p> <p>This is Subhaditya. I recently graduated with a masters in AI from the RUG (Groningen), and am looking for my next big challenge. While looking at your website, I found quite a lot of awesome projects (like the medical object recognition one) that made me quite interested in working at Axveco. I am currently looking for an AI role in the NL and I would love to discuss the possibility of any such roles with you.</p> <p>My experience is a combination of AI, Computer vision and Data Analytics. Over the past few years, I have worked with many clients (both freelance/part time and in internships) and helped them bring their AI projects to life. I am also quite familiar with the ins and outs of Deep Learning, NLP and image processing. Along with my technical knowledge, I enjoy helping clients define their vision and guiding them towards it. These are all tasks that you do, and so I think that I might be a good fit at Axveco.</p> <p>While I am not sure what kind of roles exist at the moment, I am open to any AI/Data Science related position. I am quite interested in working as part of a high performance team and the possibility of defining my own roles as well as growing my skills seems like a nice cherry on top as well. I\u2019ve attached my resume to this email as well.</p> <p>Looking forward to your reply,</p>","tags":["jobs"]},{"location":"KB/NLAIC%20Companies/#axelera","title":"Axelera","text":"<p>Open Sollicitatie - AI</p> <p>Hello!</p> <p>This is Subhaditya. I recently graduated with a masters in AI from the RUG (Groningen), and am looking for my next big challenge. While looking at your website, I found out about the Metis platform and some of the work your team is doing on Edge AI. Being a computer vision developer myself, I am obviously very interested in working at Axelera. I am currently looking for an AI role in the NL and I would love to discuss the possibility of any such roles with you.</p> <p>My experience is a combination of AI, Computer vision and Data Analytics. Over the past few years, I have worked with many clients (both freelance/part time and in internships) and helped them bring their AI projects to life. I am also quite familiar with the ins and outs of Deep Learning with a specialisation in Computer Vision. Along with my technical knowledge, what is more important is that I want to be part of your mission to democratise edge AI. I believe that this technology can lead to a lot of innovations in many fields, but without the right people, we will not be able to make AI for a green, fair and safe world. These are all tasks that you do, and so I think that I might be a good fit at Axelera.</p> <p>While I am not sure what kind of roles exist at the moment, I am open to any AI/Data Science related position. I\u2019ve attached my resume to this email as well.</p> <p>Looking forward to your reply,</p>","tags":["jobs"]},{"location":"KB/NLAIC%20Companies/#almende","title":"Almende","text":"<p>Hello Jan!</p> <p>This is Subhaditya. I recently graduated with a masters in AI from the RUG (Groningen), and am looking for my next big challenge. While looking at your website, I found quite a lot of awesome projects (FoodFriend and VARUS were my favourites) that made me very interested in working at the Health domain at Almende. I noticed that in many projects your team mentioned they wanted to test out neural networks but did not. I am currently looking for an AI role in the NL and I would love to discuss the possibility of any such roles with you.</p> <p>My experience is a combination of AI, Computer vision and Data Analytics. Over the past few years, I have worked with many clients (both freelance/part time and in internships) and helped them bring their AI projects to life. I am also quite familiar with the ins and outs of Deep Learning and how to apply them to a large variety of domains. Along with my technical skills, I love working on new projects, especially if they help people. These are all tasks that you do, and so I think that I might be a good fit at Almende.</p> <p>While I am not sure what kind of roles exist at the moment, I am open to any AI/Data Science related position. I\u2019ve attached my resume to this email as well.</p> <p>Looking forward to your reply,</p>","tags":["jobs"]},{"location":"KB/NLAIC%20Companies/#attendi","title":"Attendi","text":"<p>lennertjansen95@gmail.com</p> <p>Hello Lennert!</p> <p>This is Subhaditya. I recently graduated with a masters in AI from the RUG (Groningen), and am looking for my next big challenge. While looking at the Attendi website, I found out about some of the work your team is doing on Speech AI. Along with being a good cause to work on, it sounds like Attendi is a great place to work at. I am currently looking for an AI role in the NL and I would love to discuss the possibility of any such roles with you. (I got your email from your personal website haha)</p> <p>My experience is a combination of AI, and Data Analytics. Over the past few years, I have worked with many clients (both freelance/part time and in internships) and helped them bring their AI projects to life. I am also quite familiar with the ins and outs of Deep Learning and how to apply them to a large variety of domains. I have worked with LLMs and am also comfortable with PyTorch and Tensorflow. Along with my technical skills, I love working on new projects, especially if they help people. These are all tasks that you do, and so I think that I might be a good fit at Attendi.</p> <p>While I am not sure what kind of roles exist at the moment, I am open to any AI/Data Science related position. I\u2019ve attached my resume to this email as well.</p>","tags":["jobs"]},{"location":"KB/NLAIC%20Companies/#neolook-solutions","title":"Neolook Solutions","text":"<p>johan.thissen@gmail.com</p> <p>Hello Johan!</p> <p>Hope you are well.</p> <p>My name is Subhaditya, I recently graduated with a masters in AI from the RUG (Groningen), and am looking for my next big challenge. I heard about Neolook from one of my friends (she met you at a Hattrick event at the RUG) and found the premise very interesting. She mentioned that you also have ML/DL pipelines that you wanted to implement and research, which is why I thought I would drop you an email. I am currently looking for an AI position in the NL and I would love to discuss the possibility of any such roles with you.</p> <p>My experience is a combination of AI and Computer Vision. Over the past few years, I have worked with many clients (both freelance/part-time and in internships) and helped them bring their AI projects to life. I am quite familiar with DL libraries like PyTorch and Tensorflow along with implementing multi-modal models. Along with that, I am good at explaining concepts at a high level and have commercial experience in deploying ML applications.\u00a0</p> <p>I am quite interested in applying AI in healthcare, and when I saw that Neolook might give me a chance to do just that, I had to apply. While at the moment I have a lot to learn about neonatal healthcare, I am willing to do that. I am not queasy about walking through wards, and am comfortable with working alone and in teams as well.</p> <p>While I am not sure what kind of roles exist at Neolook at the moment, I am open to any AI-related positions. I\u2019ve attached my resume to this email as well. It would be awesome to have a chat with you Johan, I hope you give me a chance!</p>","tags":["jobs"]},{"location":"KB/NLAIC%20Companies/#atlasium-media","title":"Atlasium Media","text":"<p>Amin Gorti</p> <p>Hello Amin!</p> <p>My name is Subhaditya, I recently graduated with a masters in AI from the RUG (Groningen), and am looking for my next big challenge. I am quite interested in the AI consultancy role you just posted, and would love to talk to you about it :)</p>","tags":["jobs"]},{"location":"KB/NLAIC%20Companies/#valueblue","title":"ValueBlue","text":"<p>Dan Bersagui</p> <p>Hello Dan!</p> <p>My name is Subhaditya, I recently graduated with a masters in AI from the RUG (Groningen), and am looking for my next big challenge. I am quite interested in the AI role you just posted, and would love to talk to you and see if I could be a good fit for it.</p> <p>Best,</p> <p>SM</p> <p>Hello Dan!\u00a0</p> <p>My name is Subhaditya, I recently graduated with a masters in AI from the RUG (Groningen), and am looking for my next big challenge. I found this position on LinkedIn and the phrase \u201cArchitects of Change\u201d piqued my interest. I\u2019ve heard about BlueDolphin before, and imagine my surprise when I saw that you also wanted to implement AI algorithms as well! I am currently looking for an AI role in the NL and I would love to be the one to start the ML department at ValueBlue.</p> <p>My experience is a combination of AI and Data Analytics. I am proficient in setting up and tuning open source models across all levels. Over the past few years, I have worked with many clients (both freelance/part-time and in internships) and helped them bring their AI projects to life. I am quite familiar with AI libraries like PyTorch and Tensorflow along with MLOps tools. I have worked quite extensively with Neptune, Tensorboard and Docker for many years as well.\u00a0</p> <p>BlueDolphin has been around for a while and I\u2019m sure there is a lot of interesting data that was collected/obtained from public domains. This makes it all the more challenging and I am all for that.</p> <p>I am quite passionate about both the technical and human sides of AI. Being a new department working on a lot of cross functional teams requires this mentality, and I think that I do bring this and more to the table.\u00a0</p> <p>I hope you give me a chance!</p>","tags":["jobs"]},{"location":"KB/NLAIC%20Companies/#3d-universum","title":"3D Universum","text":"<p>info@3duniversum.com</p> <p>Hello!</p> <p>My name is Subhaditya, I recently graduated with a masters in AI from the RUG (Groningen), and am looking for my next big challenge. I found out about 3DUniversum from Dr.Sezer when I met him at an event last week. The work your team does is very much in line with my interests and experience and as I am currently looking for an AI position in the NL, I would love to discuss the possibility of any such roles at 3DUniversum.\u00a0</p> <p>I asked him if there were open roles, and he told me that the team was expanding and while most of your group is from UvA, I could give it a shot too.</p> <p>My experience is a combination of Computer Vision and AI. Over the past few years, I have worked with many clients (both freelance/part time and in internships) and helped them bring their AI projects to life. I am familiar with deep learning libraries like PyTorch and Tensorflow and am proficient in Python as well. I have worked with Synthetic media creation, Efficient Deep Learning, Mobile/Edge AI as well as 3D Segmentation and Detection.\u00a0</p> <p>Dr.Sezer told me quite a bit about projects like Deep Therapy, WeScan and FairFake and it somehow seemed like 3DUniversum would be the perfect place for me to go next. I do think that I bring something different to the table in the domains that your team works in, and I hope you give me a chance.</p> <p>My Github link is : http://github.com/SubhadityaMukherjee and I have attached my resume to this email as well.</p>","tags":["jobs"]},{"location":"KB/NLAIC%20Companies/#birdsai","title":"Birds.ai","text":"<p>info@birds.ai</p> <p>Hello!</p> <p>My name is Subhaditya. I recently graduated with a masters in AI from the RUG (Groningen), and am looking for my next big challenge. I found out about Birds.ai from the NLAIC website and found some of the projects your team has done (like the CV-based solar panel inspection project).\u00a0 The work your team does is very much in line with my interests and experience, and I would love to discuss the possibility of any such roles at Birds.ai.</p> <p>My experience is a combination of Computer Vision and Data Analytics. Over the past few years, I have worked with many clients (both freelance/part-time and in internships) and helped them bring their AI projects to life. I am familiar with deep learning libraries like PyTorch and TensorFlow. I have worked with drone image segmentation, vision-based analysis for parts of the manufacturing pipeline and similar projects.\u00a0</p> <p>Add in the consultancy work your team does, and somehow, it seems to be exactly what I want to do as the next step in my career. I do think that my skills fit the kind of projects Birds.ai does and that I bring something different to the table.\u00a0</p> <p>While I am not sure what kind of roles exist at the moment, I am open to any AI/Data Science related position. I\u2019ve also attached my resume to this email as a little more background if that helps.\u00a0</p>","tags":["jobs"]},{"location":"KB/NLAIC%20Companies/#bright-cape","title":"Bright Cape","text":"<p>Onno Hofstee, o.hofstee@brightcape.nl </p> <p>Hello Onno!</p> <p>My name is Subhaditya, I recently graduated with a masters in AI from the RUG (Groningen), and am looking for my next big challenge. I found out about BrightCape from the NLAIC website and the first thing that caught my eye was SARA, the health assistant. It seems like your team works on some very interesting projects and I would love to discuss the possibility of any AI/Data Analytics roles at BrightCape.\u00a0</p> <p>My experience is a combination of AI and Data Analytics. Over the past few years, I have worked with many clients (both freelance/part-time and in internships) and helped them bring their AI projects to life. I am familiar with deep learning libraries like PyTorch and TensorFlow along with the stack required to build Data Analysis pipelines and provide clients with the advice they need.</p> <p>Add in the consultancy work your team does, and somehow, it seems to be a nice fit with both what I can offer, and what I want to do as a next step. I do think that my skills fit the kind of projects BrightCape does and that I bring something different to the table.\u00a0</p> <p>While I am not sure what kind of roles exist at the moment, I am open to any AI/Data Science related position. I\u2019ve also attached my resume to this email as a little more background if that helps.\u00a0If you are hiring or will be soon, I would love to have a chat!</p> <p>PS: I am already excited about a potential ski trip. :)</p>","tags":["jobs"]},{"location":"KB/NLAIC%20Companies/#kepler-vision-internship","title":"Kepler Vision , Internship","text":"<p>info@keplervision.eu</p> <p>Hello!</p> <p>My name is Subhaditya, I recently graduated with a masters in AI from the RUG (Groningen), and am looking for my next big challenge. I found out about Kepler Vision from the NLAIC website, and saw that your team was looking for a software engineering intern. The Night Nurse software seems like a great project to start my career with and this email is my application for this position. I hope to get to learn a lot about how to be a good part of an engineering team, while also bringing my AI experience to the table.</p> <p>While I don\u2019t know what criteria you have in selecting an intern, this opportunity would be the absolute best next step for me. My main interests lie in Computer Vision, especially in using AI in healthcare. And somehow, this is exactly what this internship offers to teach? It would be such a miss if I did not try my best to get in.</p> <p>My experience is a combination of AI and Computer Vision. Over the past few years, I have worked with many clients (both freelance/part-time and in internships) and helped them bring their AI projects to life. But, what I now need to learn is how to work on larger scale projects such as Night Nurse. I am quite good with Python, FastAPI, and AI frameworks like PyTorch and Tensorflow, but am not very familiar with Terraform or Typescript. I definitely want to learn them though! And this internship would be absolutely perfect.</p> <p>I promise to put my best foot forward given a chance. I am quite a fast learning, and have a strong base in programming and AI. I hope you give me a shot.\u00a0</p> <p>PS. I have the Zuikjaar visa as well, valid for almost a year. So no issues there either.</p>","tags":["jobs"]},{"location":"KB/NLAIC%20Companies/#grwnxt","title":"Grwnxt","text":"<p>coen.hilbrands@grwnxt.com\u00a0, info@grwnxt.com </p> <p>Hello Coen!</p> <p>My name is Subhaditya, I recently graduated with a masters in AI from the RUG (Groningen), and am looking for my next big challenge. I found out about GrwNxt from the NLAIC website and was intrigued by the concept of GrwNxt Modules and how they could be used for reliably, combining Deep learning and plant science. I can imagine just how useful and scalable the technology would be, especially combined with AI and computer vision. The work your team does is very much in line with my interests and experience, and I would love to discuss the possibility of any such roles at GrwNxt.</p> <p>My experience is a combination of AI and Computer Vision, which is something I see that your team works on as well. Over the past few years, I have worked with many clients (both freelance/part-time and in internships) and helped them bring their AI projects to life. Now that I have a masters in AI, I\u2019m trying to find places I can contribute the most to, and I think GrwNxt fits quite well to that. From the presentation on your website, I see that the modules would use a lot of advanced data analytics combined with computer vision, which is what I am the most familiar with. I am proficient at using Deep learning libraries, such as PyTorch and Tensorflow and am comfortable with all stages of the AI pipeline. Since my main focus has always been Computer vision, I can help your team implement the advanced data analysis pipelines faster as well.\u00a0</p> <p>This project really interests me and while I am not sure what kind of roles you have at the moment, I am open to any AI/Data Science related position. Being able to contribute to the sustainable food revolution is such a good cause, and I would love to be a part of it.</p> <p>I\u2019ve also attached my resume to this email as a little more background if that helps.\u00a0If you are hiring or will be soon, I would love to have a chat!</p>","tags":["jobs"]},{"location":"KB/NLAIC%20Companies/#mindaffect","title":"MindAffect","text":"<p>careers@mindaffect.nl</p>","tags":["jobs"]},{"location":"KB/NLVR2%203/","title":"NLVR2","text":""},{"location":"KB/NLVR2%203/#nlvr2","title":"NLVR2","text":""},{"location":"KB/NUMA/","title":"NUMA","text":""},{"location":"KB/NUMA/#numa","title":"NUMA","text":"<ul> <li>Shared memory</li> <li>Often made by physically linking two or more SMP</li> <li>One SMP can directly access memory of another SMP</li> <li>Not all processors have equal access time to all memories</li> <li>Memory access across link is slower</li> <li>Cache Coherence</li> </ul>"},{"location":"KB/NaN%20Trap/","title":"NaN Trap","text":""},{"location":"KB/NaN%20Trap/#nan-trap","title":"NaN Trap","text":"<ul> <li>When one number in your model becomes a NaN during training, which causes many or all other numbers in your model to eventually become a NaN.</li> </ul>"},{"location":"KB/Names%20of%20individuals/","title":"Names of individuals","text":""},{"location":"KB/Names%20of%20individuals/#names-of-individuals","title":"Names of Individuals","text":"<ul> <li>Dan went to the movies.</li> <li>Dan should be understood to be some person named Dan. Although there are many, the speaker had one particular one in mind and the discourse context should tell us which.</li> </ul>"},{"location":"KB/Nasnet/","title":"Nasnet","text":""},{"location":"KB/Nasnet/#nasnet","title":"Nasnet","text":"<ul> <li>Neural Architecture Search</li> <li>Controller RNN (Basic RNN Architectures) produces architectures and evaluated until convergence</li> </ul>"},{"location":"KB/Nativists/","title":"Nativists","text":""},{"location":"KB/Nativists/#nativists","title":"Nativists","text":"<ul> <li>noun and verb and determiner phrase and various sentence constituents are not only real mental primitives, but are innately given inherently linguistic primitives of the mind.</li> </ul>"},{"location":"KB/Nearest%20Neighbor%20Retrieval/","title":"Nearest Neighbor Retrieval","text":""},{"location":"KB/Nearest%20Neighbor%20Retrieval/#nearest-neighbor-retrieval","title":"Nearest Neighbor Retrieval","text":"<ul> <li>images with similar appearance usually are closer in the feature space </li> <li>The nearest neighbor method is used to find the top K nearest neighbors from the feature space of the features learned by the self-supervised learned model [40], [41], [43]</li> </ul>"},{"location":"KB/Nebulizer/","title":"Nebulizer","text":""},{"location":"KB/Nebulizer/#nebulizer","title":"Nebulizer","text":"<ul> <li>A device used to deliver medication in an aerosol form through inhalation</li> </ul>"},{"location":"KB/Negative%20Log%20Likelihood/","title":"Negative Log Likelihood","text":""},{"location":"KB/Negative%20Log%20Likelihood/#negative-log-likelihood","title":"Negative Log Likelihood","text":"<ul> <li>Classification, Smaller quicker training, Simple tasks.</li> </ul> <p>$$ - \\mathrm{sum}\\left( \\log\\left( y \\right) \\right)$$ -  - Log Likelihood Loss</p>"},{"location":"KB/Negative%20Sampling/","title":"Negative Sampling","text":""},{"location":"KB/Negative%20Sampling/#negative-sampling","title":"Negative Sampling","text":"<ul> <li>introduce samples of words that are not neighbors</li> <li></li> <li></li> </ul>"},{"location":"KB/Negative%20Set%20Bias/","title":"Negative Set Bias","text":""},{"location":"KB/Negative%20Set%20Bias/#negative-set-bias","title":"Negative Set Bias","text":"<ul> <li>Datasets define a visual phenomenon (e.g. object, scene, event) not just by what it is (positive instances), but also by what it is not (negative instances)</li> <li>the space of all possible negatives in the visual world is astronomically large, so datasets are forced to rely on only a small sample</li> <li>ImageNet benefits from a large variability of negative examples and does not seem to be affected by a new external negative set, whereas Caltech and MSRC appear to be just too easy</li> <li>Unfortunately, it's not at all easy to stress-test the sufficiency of a negative set in the general case since it will require huge amounts of labelled (and unbiased) negative data.</li> <li>One remedy, proposed in this paper, is to add negatives from other datasets</li> <li>Another approach, suggested by Mark Everingham, is to use a few standard algorithms (e.g. bag of words) to actively mine hard negatives as part of dataset construction from a very large unlabelled set, and then manually going through them to weed out true positives. The down side is that the resulting dataset will be biased against existing algorithms.</li> </ul>"},{"location":"KB/Nesterov%20Momentum/","title":"Nesterov Momentum","text":""},{"location":"KB/Nesterov%20Momentum/#nesterov-momentum","title":"Nesterov Momentum","text":"<ul> <li>$$\\begin{align}</li> </ul> <p>&amp;v_{t}= \\gamma v_{t+1}+\\eta \\cdot \\nabla_{\\theta}J(\\theta - \\gamma v_{t-1}) \\</p> <p>&amp;\\theta = \\theta- v_{t}\\</p> <p>\\end{align}$$</p>"},{"location":"KB/Network%20Dissection%20Quantifying%20Interpretability%20of%20Deep%20Visual%20Representions/","title":"Network Dissection Quantifying Interpretability of Deep Visual Representions","text":""},{"location":"KB/Network%20Dissection%20Quantifying%20Interpretability%20of%20Deep%20Visual%20Representions/#network-dissection-quantifying-interpretability-of-deep-visual-representions","title":"Network Dissection Quantifying Interpretability of Deep Visual Representions","text":"<ul> <li> <p>David Bau\u2217, Bolei Zhou\u2217, Aditya Khosla, Aude Oliva, and Antonio Torralba</p> </li> <li> <p>Quantifying the interpretability of latent representations of CNNs by evaluating the alignment between individual hidden units and a set of semantic concepts</p> </li> <li>score the semantics of hidden units at each intermediate convolutional layer.</li> <li>The units with semantics are given labels across a range of objects, parts, scenes, textures, materials, and colors.</li> <li>interpretability of units is equivalent to random linear combinations of units</li> <li>analyze the effect of training iterations, compare networks trained with different initializations, examine the impact of network depth and width, and measure the effect of KB/Dropout.md and batch normalization on the interpretability of deep visual representations</li> </ul>"},{"location":"KB/Network%20Dissection%20Quantifying%20Interpretability%20of%20Deep%20Visual%20Representions/#introduction","title":"Introduction","text":"<ul> <li>The emergence of interpretable structure suggests that deep networks may be learning disentangled representations spontaneously.</li> <li>A disentangled representation aligns its variables with a meaningful factorization of the underlying problem structure</li> <li>Broden</li> </ul>"},{"location":"KB/Network%20Dissection%20Quantifying%20Interpretability%20of%20Deep%20Visual%20Representions/#network-dissection","title":"Network Dissection","text":"<ul> <li>Our measurement of interpretability for deep visual representations proceeds in three steps: 1. Identify a broad set of human-labeled visual concepts. 2. Gather hidden variables' response to known concepts. 3. Quantify alignment of hidden variableconcept pairs.</li> <li>In a fully interpretable local coding such as a one-hotencoding, each variable will match exactly with one humaninterpretable concept.</li> <li>Therefore we measure the alignment between single units and single interpretable concepts</li> <li>This does not gauge the discriminative power of the representation; rather it quantifies its disentangled interpretability.</li> <li>We then measure the alignment of each hidden unit of the CNN with each concept by evaluating the feature activation of each individual unit as a segmentation model for each concept</li> <li>To quantify the interpretability of a layer as a whole, we count the number of distinct visual concepts that are aligned with a unit in the layer</li> <li>Broden</li> </ul>"},{"location":"KB/Network%20Dissection%20Quantifying%20Interpretability%20of%20Deep%20Visual%20Representions/#scoring-unit-interpretability","title":"Scoring Unit Interpretability","text":"<ul> <li>evaluates every individual convolutional unit in a CNN as a solution to a binary segmentation task to every visual concept in Broden</li> <li>applied to any CNN using a forward pass without the need for training or backpropagation.</li> <li>For every input image x in the Broden dataset, the activation map \\(A_{k}(x)\\) of every internal convolutional unit k is collected.</li> <li>Then the distribution of individual unit activations ak is computed</li> <li>For each unit k, the top quantile level \\(T_{k}\\) is determined such that \\(P(a_{k} &gt; T_{k} = 0.005\\) over every spatial location of the activation map in the data set.</li> <li>input-resolution annotation mask \\(L_{c}\\) for some concept c</li> <li>the activation map is scaled up to the mask resolution \\(S_{k}(x)\\) from \\(A_{k}(x)\\) using bilinear interpolation, anchoring interpolants at the center of each unit's receptive field</li> <li>\\(S_{k}(x)\\) is then thresholded into a binary segmentation: \\(M_{k}(x) \\equiv S_{k}(x) \\leq T_{k}\\), selecting all regions for which the activation exceeds the threshold Tk. These segmentations are evaluated against every concept c in the data set by computing intersections \\(M_{k}(x) \\cap L_{c}(x)\\), for every (k, c) pair.</li> <li>The score of each unit k as segmentation for concept c is reported as a data-set-wide intersection over union score \\(\\(I_{o}U_{k,c}=\\frac{\\Sigma|M_{k}(x) \\cap L_{c}(x|}{\\Sigma|M_{k}(x) \\cup L_{c}(x)|}\\)\\)</li> <li>where | \u00b7 | is the cardinality of a set.</li> <li>The value of \\(IoU_{k,c}\\) is the accuracy of unit k in detecting concept c; we consider one unit k as a detector for concept c if IoUk,c exceeds a threshold</li> <li>Our qualitative results are insensitive to the IoU threshold: different thresholds denote different numbers of units as concept detectors</li> <li>For our comparisons we report a detector if IoUk,c &gt; 0.04.</li> <li>one unit might be the detector for multiple concepts; for the purpose of our analysis, we choose the top ranked label</li> <li>The IoU evaluating the quality of the segmentation of a unit is an objective confidence score for interpretability that is comparable across networks</li> <li>Note that network dissection works only as well as the underlying data set</li> <li>We conclude that interpretability is neither an inevitable result of discriminative power, nor is it a prerequisite to discriminative power.</li> <li>Instead, we find that interpretability is a different quality that must be measured separately to be understood.</li> </ul>"},{"location":"KB/Network%20Dissection%20Quantifying%20Interpretability%20of%20Deep%20Visual%20Representions/#measure-of-axis-aligned-interpretability","title":"Measure of Axis Aligned Interpretability","text":""},{"location":"KB/Network%20Dissection%20Quantifying%20Interpretability%20of%20Deep%20Visual%20Representions/#disentangled-concepts-by-layer","title":"Disentangled Concepts by Layer","text":"<ul> <li>Confirming intuition, color and texture concepts dominate at lower layers conv1 and conv2 while more object and part detectors emerge in conv5.</li> </ul>"},{"location":"KB/Network%20Dissection%20Quantifying%20Interpretability%20of%20Deep%20Visual%20Representions/#network-architectures-and-supervisions","title":"Network Architectures and Supervisions","text":"<ul> <li>In terms of network architecture, we find that interpretability of ResNet &gt; VGG &gt; GoogLeNet &gt; AlexNet</li> <li>Deeper architectures appear to allow greater interpretability. Places &gt; ImageNet.</li> <li>Self-supervised models create many texture detectors but relatively few object detectors; apparently, supervision from a self-taught primary task is much weaker at inferring interpretable concepts than supervised training on a large annotated data set</li> <li>The form of self-supervision makes a difference: for example, the colorization model is trained on colorless images, and almost no color detection units emerge</li> <li>We hypothesize that emergent units represent concepts required to solve the primary task.</li> </ul>"},{"location":"KB/Network%20Dissection%20Quantifying%20Interpretability%20of%20Deep%20Visual%20Representions/#training-conditions-vs-interpretability","title":"Training Conditions Vs. Interpretability","text":"<ul> <li>We can see that object detectors and part detectors begin emerging at about 10,000 iterations (each iteration processes a batch of 256 images)</li> <li>We do not find evidence of transitions across different concept categories during training</li> <li>For example, units in conv5 do not turn into texture or material detectors before becoming object or part detectors.</li> <li>Comparing different random initializations, the models converge to similar levels of interpretability, both in terms of the unique detector number and the total detector number; this matches observations of convergent learning</li> <li>For the network without KB/Dropout.md, more texture detectors emerge but fewer object detectors</li> <li>Batch normalization seems to decrease interpretability significantly.</li> <li>The batch normalization result serves as a caution that discriminative power is not the only property of a representation that should be measured.</li> <li>batch normalization 'whitens' the activation at each layer, which smooths out scaling issues and allows a network to easily rotate axes of intermediate representations during training</li> <li>While whitening apparently speeds training, it may also have an effect similar to random rotations analyzed in Sec. 3.2 which destroy interpretability</li> <li>interpretability is neither a prerequisite nor an obstacle to discriminative power</li> <li></li> <li></li> <li></li> </ul>"},{"location":"KB/Network%20Dissection%20Quantifying%20Interpretability%20of%20Deep%20Visual%20Representions/#discrimination-vs-interpretability","title":"Discrimination Vs. Interpretability","text":"<ul> <li>For each trained model, we extract the representation at the highest convolutional layer, and train a linear SVM with C = 0.001 on the training data for action40 action recognition task</li> <li>Thus the supervision tasks that encourage the emergence of more concept detectors may also improve the discrimination ability of deep features.</li> <li>accuracy on a representation when applied to a task is dependent not only on the number of concept detectors in the representation, but on the suitability of the set of represented concepts to the transfer task.</li> <li></li> </ul>"},{"location":"KB/Network%20Dissection%20Quantifying%20Interpretability%20of%20Deep%20Visual%20Representions/#layer-width-vs-interpretability","title":"Layer Width Vs. Interpretability","text":"<ul> <li>Depth has been shown to be important to high discrimination ability</li> <li>increasing the number of convolutional units at a layer significantly increases computational cost while yielding only marginal improvements in classification accuracy</li> <li>carefully designed wide residual network can achieve classification accuracy superior to the commonly used thin and deep counterparts.</li> <li>This may indicate a limit on the capacity of AlexNet to separate explanatory factors; or it may indicate that a limit on the number of disentangled concepts that are helpful to solve the primary task of scene classification.</li> <li></li> </ul>"},{"location":"KB/Neural%20Augmentation/","title":"Neural Augmentation","text":""},{"location":"KB/Neural%20Augmentation/#neural-augmentation","title":"Neural Augmentation","text":"<ul> <li>The Neural Style Transfer algorithm requires two parameters for the weights of the style and content loss. Perez and Wang presented an algorithm to meta-learn a Neural Style Transfer strategy called Neural Augmentation.</li> <li>e Neural Augmentation approach takes in two random images from the same class. The prepended augmentation net maps them into a new image through a CNN with 5 layers, each with 16 channels, 3\u00d73 filters, and ReLU activation functions. The image outputted from the augmentation is then transformed with another random image via Neural Style Transfer.</li> <li>This style transfer is carried out via the CycleGAN extension of the GAN framework</li> <li>These images are then fed into a classification model and the error from the classification model is backpropagated to update the Neural Augmentation net.</li> <li>The Neural Augmentation network uses this error to learn the optimal weighting for content and style images between different images as well as the mapping between images in the CNN</li> <li>The Neural Augmentation techniques tested consist of three levels based on the design of the loss function for the augmentation net (Content loss, Style loss via gram matrix, and no loss computer at this layer)</li> </ul>"},{"location":"KB/Neural%20Chimera/","title":"Neural Chimera","text":""},{"location":"KB/Neural%20Chimera/#neural-chimera","title":"Neural Chimera","text":"<ul> <li>A research model where human stem cells are transplanted into an animal embryo to follow the genetic, molecular, and functional processes of brain cells as they grow.</li> </ul>"},{"location":"KB/Neural%20Dynamics/","title":"Neural Dynamics","text":""},{"location":"KB/Neural%20Dynamics/#neural-dynamics","title":"Neural Dynamics","text":"<ul> <li>Continous -&gt; Discrete seq of words</li> <li>Use NN to generate hypothesis outputs vectors<ul> <li>As many components as possible target symbols</li> </ul> </li> </ul>"},{"location":"KB/Neural%20Induction/","title":"Neural Induction","text":""},{"location":"KB/Neural%20Induction/#neural-induction","title":"Neural Induction","text":"<ul> <li>A developmental process where ectodermal cells \u201cdecide\u201d to form the neural plate, the basis of what will eventually become the organism\u2019s nervous system.</li> </ul>"},{"location":"KB/Neural%20Network%20Architecture%20Cheat%20Sheet/","title":"Neural Network Architecture Cheat Sheet","text":""},{"location":"KB/Neural%20Network%20Architecture%20Cheat%20Sheet/#neural-network-architecture-cheat-sheet","title":"Neural Network Architecture Cheat Sheet","text":"<ul> <li>Spiking Networks</li> <li>Hidden Models</li> <li>Capsule Network</li> <li>Probability</li> <li>Recurrent</li> <li>Conv</li> </ul>"},{"location":"KB/Neural%20Probabilistic%20Model/","title":"Neural Probabilistic Model","text":""},{"location":"KB/Neural%20Probabilistic%20Model/#neural-probabilistic-model","title":"Neural Probabilistic Model","text":"<ul> <li>A Neural Probabilistic Language Model</li> <li>more compact and smoother representations based on distributed representations that can accommodate far more conditioning variables</li> <li>learning the joint probability function of sequences of words in a language was intrinsically difficult because of the curse of dimensionality</li> <li>learning a distributed representation for words which allows each training sentence to inform the model about an exponential/combinatorial number of semantically neighboring sentences</li> <li>The model learns simultaneously (i) a distributed representation for each word along with (ii) the probability function for word sequences, expressed in terms of these representations</li> <li>Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar</li> <li>significantly improves on state-of-the-art n gram models</li> </ul>"},{"location":"KB/Neural%20Radiance%20Field/","title":"Neural Radiance Field","text":""},{"location":"KB/Neural%20Radiance%20Field/#neural-radiance-field","title":"Neural Radiance Field","text":"<ul> <li>NeRF: Representing Scenes As Neural Radiance Fields for View Synthesis<ul> <li>synthesizing novel views of complex scenes</li> <li>optimizing an underlying continuous volumetric scene function using a sparse set of input views</li> <li>single continuous 5D coordinate (spatial location (x,y,z) and viewing direction (\u03b8,\u03d5))</li> <li>output is the volume density and view-dependent emitted radiance at that spatial location</li> <li>querying 5D coordinates along camera rays</li> <li>volume rendering techniques to project the output colors and densities into an image</li> <li>volume rendering is naturally differentiable</li> <li>set of images with known camera poses</li> <li>They describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes</li> </ul> </li> </ul>"},{"location":"KB/Neural%20Text%20Degeneration/","title":"Neural Text Degeneration","text":""},{"location":"KB/Neural%20Text%20Degeneration/#neural-text-degeneration","title":"Neural Text Degeneration","text":"<ul> <li>The Curious Case of Neural Text Degeneration</li> <li>deep analysis into the properties of the most common decoding methods for open-ended language generation</li> <li>surprising distributional differences between human text and machine text</li> <li>decoding strategies alone can dramatically effect the quality of machine text, even when generated from exactly the same neural language model</li> <li>likelihood maximizing decoding causes repetition and overly generic language usage</li> <li>sampling methods without truncation risk sampling from the low-confidence tail of a model\u2019s predicted distribution</li> <li>Nucleus Sampling</li> </ul>"},{"location":"KB/Neuroaesthetics/","title":"Neuroaesthetics","text":""},{"location":"KB/Neuroaesthetics/#neuroaesthetics","title":"Neuroaesthetics","text":"<ul> <li>A field within cognitive neuroscience that examines the neural underpinnings of what humans find visually appealing or beautiful.</li> </ul>"},{"location":"KB/Neurogenesis/","title":"Neurogenesis","text":""},{"location":"KB/Neurogenesis/#neurogenesis","title":"Neurogenesis","text":"<ul> <li>The production of new, maturing neurons by neural stem and progenitor cells. Rapid and widespread neurogenesis obviously occurs in the fetal brain in humans and other animals, but neuroscientists long believed that neurogenesis essentially does not occur in the adult human brain.</li> <li>However, over the past two decades, research has shown that it does in fact occur in the dentate gyrus of the hippocampus and possibly other brain regions. This \u201cadult neurogenesis\u201d appears to be vital for normal learning and memory, and may help protect the brain against stress and depression.</li> </ul>"},{"location":"KB/Neuroplasticity/","title":"Neuroplasticity","text":""},{"location":"KB/Neuroplasticity/#neuroplasticity","title":"Neuroplasticity","text":"<ul> <li>Also referred to as brain plasticity or neural plasticity, this is the ability of the brain to change throughout the lifespan, forming new synapses and neural connections in response to the environment.</li> </ul>"},{"location":"KB/Newtons%20Laws/","title":"Newtons Laws","text":""},{"location":"KB/Newtons%20Laws/#newtons-laws","title":"Newtons Laws","text":"<ul> <li>Inertia</li> <li>Force</li> <li>Equal And Opposite Force Pairs</li> </ul>"},{"location":"KB/No%20bias%20decay/","title":"No bias decay","text":""},{"location":"KB/No%20bias%20decay/#no-bias-decay","title":"No Bias Decay","text":"<ul> <li>No Learning Rate Decay tricks</li> <li>Equivalent to Lp Regularization L2 to all parameters to drive the values towards 0</li> <li>Only apply Regularization to the weights</li> <li>Leave Batch Normalization Layers alone</li> <li>LARS</li> </ul>"},{"location":"KB/Node%20Distribution/","title":"Node Distribution","text":""},{"location":"KB/Node%20Distribution/#node-distribution","title":"Node Distribution","text":"<p>-</p>"},{"location":"KB/Node%20Link%20Diagram/","title":"Node LInk Diagram","text":""},{"location":"KB/Node%20Link%20Diagram/#node-link-diagram","title":"Node LInk Diagram","text":"<ul> <li>Vertices (Nodes) are mapped to graphical shapes circles, squares, triangles, etc.</li> <li>Edges (Links) are mapped to straight or curved lines</li> <li>Nodes can freely be positioned</li> <li>Cross Minimization</li> <li>Bend Minimization</li> <li>Area Minimization</li> <li>Cross angle Maximization</li> <li>Length Optimization</li> <li>Symmetries Node Link</li> <li>Node Distribution</li> <li>Force Directed Graph Layout</li> <li>Hierarchical Edge Bundling</li> </ul>"},{"location":"KB/Noise%20Injection/","title":"Noise Injection","text":""},{"location":"KB/Noise%20Injection/#noise-injection","title":"Noise Injection","text":"<ul> <li>injecting a matrix of random values usually drawn from a Gaussian distribution</li> <li>Adding noise to images can help CNNs learn more robust features.</li> </ul>"},{"location":"KB/Noise%20Suppression/","title":"Noise Suppression","text":""},{"location":"KB/Noise%20Suppression/#noise-suppression","title":"Noise Suppression","text":"<ul> <li>reduce the intensity variation in big structures (such as organs in medical imaging data) -improve the detectability of edges between big structures,</li> <li>preserve small scale structures - Conv Based Noise Reduction</li> <li>Average Filter</li> <li>Gaussian Filter</li> <li>Mesh Smoothing</li> <li>Laplacian Grid Smoothing</li> </ul>"},{"location":"KB/Noise%20Tunnel/","title":"Noise Tunnel","text":""},{"location":"KB/Noise%20Tunnel/#noise-tunnel","title":"Noise Tunnel","text":"<ul> <li>@kokhlikyanCaptumUnifiedGeneric2020</li> </ul>"},{"location":"KB/Noise%20Tunnel/#summary","title":"Summary","text":"<ul> <li>Combines [SmoothGrad Square] + [Smooth-Grad] + VarGrad </li> <li>not an attribution method</li> </ul>"},{"location":"KB/Noise%20Tunnel/#using-smooth-grad","title":"Using Smooth-Grad","text":"<ul> <li>technique that improves the accuracy of attribution methods</li> <li>problem with ReLU activation function and gradients producing noisy, often irrelevant attributions</li> <li>the partial derivative \\(\\frac{\\partial F_c}{\\partial x_i}\\) of the models\u2019 score \\(F_c\\)\u200b for a class c with respect to the value of the pixel \\(x_{i}\\) fluctuates</li> <li>adding a Gaussian noise \\(\\mathcal {N}(0, 0.01^2)\\) and calculating an average of sampled attributions is going to solve the problem</li> <li>calculates the attribution \\(M_{c}\\) using any available method by providing that method an input with Gaussian noise</li> <li>calculates a mean value from all the samples to reduce the importance of less frequent attributions</li> <li>when adding noise to the input image, important attributions are going to be visible most of the time, and noise might change between attributions</li> <li> \\[\\hat M_{c}(x) = \\frac{1}{n}\\Sigma_{1}^{n}M_{c}(x + \\mathcal{N}(0, \\sigma^{2}))\\] </li> </ul>"},{"location":"KB/Noise%20Tunnel/#using-smoothgrad-square","title":"Using SmoothGrad Square","text":"<ul> <li>changes only the way that the mean value is calculated by using the mean of squared attributions instead of just attributions</li> <li>less noisy results</li> <li>but often removes less important features, which are still valid features</li> <li> \\[\\hat M_{c}(x) = \\frac{1}{n}\\Sigma_{1}^{n}\\sqrt{M_{c}(x + \\mathcal{N}(0, \\sigma^{2}))}\\] </li> </ul>"},{"location":"KB/Noise%20Tunnel/#using-vargrad","title":"Using VarGrad","text":"<ul> <li>variance version of the SmoothGrad</li> <li>Using SmoothGrad (Fig. 1c) seems to detect more edges of the input image (in comparison with pure IG attribution in [Fig. 1b]), and that can be interpreted as detecting decision boundary. SmoothGrad-Square (Fig. 1d) and VarGrad (Fig. 1e) are removing a large amount of noise but usually also some of the important features visible on the attribution from SmoothGrad</li> <li> \\[\\hat M_{c}(x) = \\frac{1}{n}\\Sigma_{k=1}^{n}\\{M_{c}(x + \\mathcal{N}(0, \\sigma^{2}))\\}^{2}- \\{\\hat M_{c}(x)\\}^{2}\\] </li> </ul>"},{"location":"KB/Noise%20Tunnel/#drawbacks","title":"Drawbacks","text":"<ul> <li>Even if the Noise Tunnel method improves the accuracy of the XAI methods it adds a large amount of computational overhead</li> <li>Every sample generated by the method requires the rerun of the whole XAI method</li> <li>That is a linear increase of computation and to make the method efficient you should use at least 5 generated noise samples</li> </ul>"},{"location":"KB/Noise%20Tunnel/#images","title":"Images","text":""},{"location":"KB/Noisy%20Relu/","title":"Noisy Relu","text":""},{"location":"KB/Noisy%20Relu/#noisy-relu","title":"Noisy Relu","text":"<ul> <li> \\[f(x) = max(0, x+Y) $$ where $$Y\\in Normal(0,1)\\] </li> </ul>"},{"location":"KB/Non%20Relational%20Inductive%20Bias/","title":"Non Relational Inductive Bias","text":""},{"location":"KB/Non%20Relational%20Inductive%20Bias/#non-relational-inductive-bias","title":"Non Relational Inductive Bias","text":"<ul> <li>Activation Functions<ul> <li>allow the model to capture the non-linearity hidden in the data</li> </ul> </li> <li>Dropout<ul> <li>helps the network avoid memorizing the data by forcing random subsets of the network to each learn the data pattern. As a result, the obtained model, in the end, is able to generalize better</li> </ul> </li> <li>Weight Decay<ul> <li>puts constraints on the model\u2019s weights</li> </ul> </li> <li>Batch Normalization , Layer Normalization , Instance Normalization<ul> <li>Reduces Covariate Shift</li> </ul> </li> <li>Augmentation</li> <li>Optimizers</li> </ul>"},{"location":"KB/Non-adjacent%20dependencies/","title":"Non-adjacent dependencies","text":""},{"location":"KB/Non-adjacent%20dependencies/#non-adjacent-dependencies","title":"Non-adjacent Dependencies","text":"<ul> <li>Wh-dependencies</li> <li>Extra-position</li> <li>Object-relative clauses</li> <li>Subject relative</li> <li>Subject-verb agreement</li> </ul>"},{"location":"KB/Non-response%20Bias/","title":"Non response Bias","text":""},{"location":"KB/Non-response%20Bias/#non-response-bias","title":"Non-response Bias","text":"<ul> <li>(also called participation bias)</li> <li>Users from certain groups opt-out of surveys at different rates than users from other groups.</li> </ul>"},{"location":"KB/Nonstationarity/","title":"Nonstationarity","text":""},{"location":"KB/Nonstationarity/#nonstationarity","title":"Nonstationarity","text":"<ul> <li>A feature whose values change across one or more dimensions, usually time. For example, the number of swimsuits sold at a particular store demonstrates nonstationarity because that number varies with the season. As a second example, the quantity of a particular fruit harvested in a particular region typically shows sharp nonstationarity over time.</li> </ul>"},{"location":"KB/Nootropics/","title":"Nootropics","text":""},{"location":"KB/Nootropics/#nootropics","title":"Nootropics","text":"<ul> <li>Drugs or supplements that are marketed as ways to improve cognitive functions like memory, attention, or creativity.</li> </ul>"},{"location":"KB/Normal%20Distribution/","title":"Normal Distribution","text":""},{"location":"KB/Normal%20Distribution/#normal-distribution","title":"Normal Distribution","text":"<ul> <li> \\[Pr(y|\\mu, \\sigma^{2}) = \\frac{1}{\\sqrt{2\\pi\\sigma}}e^{-\\frac{(y-\\mu)^{2}}{2\\sigma^{2}}}\\] </li> <li>Mean \\(\\mu\\) and std \\(\\sigma\\). \\(\\mu\\) is max and \\(\\mu \\pm \\sigma\\) is locations of zeros of second derivative</li> <li></li> <li>\\(\\mathcal{N}(0,1)\\)</li> <li>Central Limit Theorem</li> </ul>"},{"location":"KB/Normal%20Distribution/#properties","title":"Properties","text":"<ul> <li>Linear combinations of normal distributed independant RVs are normal distributed</li> <li>X,Y have means \\(\\mu\\) and v and variances \\(\\sigma^{2}\\) and \\(\\tau^{2}\\). Then \\(aX + bY\\) is normally distributed and has mean : \\(a\\mu + bv\\) and variance \\(\\alpha^{2}\\sigma^{2}+b^{2}\\tau^{2}\\)</li> </ul>"},{"location":"KB/Normal%20Distribution/#computing-the-value","title":"Computing the Value","text":"<ul> <li> \\[\\int_{a}^{b} \\frac{1}{\\sqrt{2\\pi\\sigma}}e^{-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}}dx\\] </li> <li>Transform \\(\\mathscr{N}(\\mu, \\sigma^{2})\\) to \\(\\mathscr{N}(0,1)\\)</li> <li> \\[Z = \\frac{X-\\mu}{\\sigma}\\] </li> <li> \\[\\int_{\\frac{a-\\mu}{\\sigma}}^{\\frac{b-\\mu}{\\sigma}}\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{(x)^{2}}{2}}dx\\] </li> <li>Compute by using Cumulative density function \\(\\phi\\)</li> <li>Iterative solvers</li> <li> \\[\\phi(\\frac{b-\\mu}{\\sigma})-\\phi(\\frac{a-\\mu}{\\sigma})\\] </li> <li> \\[\\hat \\mu = \\frac{1}{N}\\Sigma_{i}(x_{i})$$ $$\\hat \\sigma^{2}= \\frac{1}{N-1}\\Sigma_{i}(x_{i}-\\hat\\mu)^2\\] </li> </ul>"},{"location":"KB/Normalized%20Inverted%20Structural%20Similarity%20Index/","title":"Normalized Inverted Structural Similarity Index","text":""},{"location":"KB/Normalized%20Inverted%20Structural%20Similarity%20Index/#normalized-inverted-structural-similarity-index","title":"Normalized Inverted Structural Similarity Index","text":"<ul> <li>metric is calculated from Structural Similarity Index by inverting the range and then normalizing it.</li> <li>NISSIM bounds to (0, 1] where 0 means similar and 1 means dissimilar. Ideally we want this value as close to 0 as possible</li> <li> \\[NISSIM_{i}= \\frac{1-SSIM_{i}}{2}\\] </li> </ul>"},{"location":"KB/Norms%20as%20a%20basis%20for%20governing%20sociotechnical%20systems/","title":"Norms as a basis for governing sociotechnical systems","text":""},{"location":"KB/Norms%20as%20a%20basis%20for%20governing%20sociotechnical%20systems/#norms-as-a-basis-for-governing-sociotechnical-systems","title":"Norms as a Basis for Governing Sociotechnical Systems","text":"<ul> <li>Munindar P. Singh</li> <li>framework that uses social norms to govern autonomous entities' (e.g., AI agents' or human beings') behaviours</li> <li>inherently distributed rather than relying on a central authority</li> <li>Individuals maintain their autonomy through executing their own decision policies, but are subjected to social norms defined by the collective through roles</li> <li>Social norms are defined through a template containing codified commitment, authorization, prohibition, sanction and power</li> </ul>"},{"location":"KB/Notch%20filter/","title":"Notch filter","text":""},{"location":"KB/Notch%20filter/#notch-filter","title":"Notch Filter","text":"<ul> <li>A notch filter is a type of band-stop filter, which is a filter that attenuates frequencies within a specific range while passing all other frequencies unaltered</li> </ul>"},{"location":"KB/Nucleotide%20Sequence/","title":"Nucleotide Sequence","text":""},{"location":"KB/Nucleotide%20Sequence/#nucleotide-sequence","title":"Nucleotide Sequence","text":"<ul> <li>A specific and ordered array of nucleotides that make up a specific genetic variant or allele.</li> </ul>"},{"location":"KB/Nucleotide/","title":"Nucleotide","text":""},{"location":"KB/Nucleotide/#nucleotide","title":"Nucleotide","text":"<ul> <li>Sometimes referred to as a nucleic acid, these are the biological building blocks of DNA.</li> </ul>"},{"location":"KB/Nucleus%20Accumbens/","title":"Nucleus Accumbens","text":""},{"location":"KB/Nucleus%20Accumbens/#nucleus-accumbens","title":"Nucleus Accumbens","text":"<ul> <li>Part of the brain\u2019s reward circuitry, or mesolimbic pathway, this small region in the midbrain releases dopamine in response to rewarding experiences.</li> </ul>"},{"location":"KB/Nucleus%20Sampling/","title":"Nucleus Sampling","text":""},{"location":"KB/Nucleus%20Sampling/#nucleus-sampling","title":"Nucleus Sampling","text":"<ul> <li>Nucleus (or top-p) Sampling, a simple but effective method that captures the region of confidence of language models effectively to draw the best out of neural generation</li> <li>By sampling text from the dynamic nucleus of the probability distribution, which allows for diversity while effectively truncating the less reliable tail of the distribution, the resulting text better demonstrates the quality of human text, yielding enhanced diversity without sacrificing fluency and coherence.</li> </ul>"},{"location":"KB/Numerically%20Quantified%20Expressions/","title":"Numerically Quantified Expressions","text":""},{"location":"KB/Numerically%20Quantified%20Expressions/#numerically-quantified-expressions","title":"Numerically Quantified Expressions","text":"<ul> <li>NQEs express<ul> <li>Plurality</li> <li>Cardinality</li> </ul> </li> <li>Can be in scopal relations with other expressions</li> </ul>"},{"location":"KB/OPT/","title":"OPT","text":""},{"location":"KB/OPT/#opt","title":"OPT","text":"<ul> <li>OPT: Open Pre-trained Transformer Language Models</li> <li>Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning</li> <li>collection of auto-regressive/decoder-only pre-trained transformer-based language models ranging in size from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers</li> <li>replicate the performance and sizes of the GPT-3 class of models, while also applying the latest best practices in data curation and training efficiency</li> <li>OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop</li> </ul>"},{"location":"KB/OSIE/","title":"OSIE","text":""},{"location":"KB/OSIE/#osie","title":"OSIE","text":"<ul> <li>700 natural indoor and outdoor scenes, aesthetic photographs from Flickr and Google.</li> <li>The fixations were measured while 15 observers looked at the image for 3 s.</li> <li>the fixations for all observers were collected and blurred using the KB/Gaussian Filter.md with the standard deviation equivalent to 1\u00b0 in the visual angle</li> </ul>"},{"location":"KB/Object%20Detection/","title":"Object Detection","text":""},{"location":"KB/Object%20Detection/#object-detection","title":"Object Detection","text":"<ul> <li>localizing the position of objects in images and recognizing the category of the objects </li> <li>MSCOCO [99] and OpenImage [14] </li> <li>When using object detection as downstream task to evaluate the quality of the self-supervised image features, networks that trained with the pretext task on unlabeled large data are served as the pre-trained model for the Fast-RCNN [2] and then fine-tuned on object detection datasets, then the performance on the object detection task is evaluated to demonstrate the generalization ability of self- supervised learned features.</li> </ul>"},{"location":"KB/Object-relative%20clauses/","title":"Object-relative clauses","text":""},{"location":"KB/Object-relative%20clauses/#object-relative-clauses","title":"Object-relative Clauses","text":"<ul> <li>[The report] that the senator attacked [] admitted the error.</li> <li>[The senator] that the report attacked [] admitted the error.</li> </ul>"},{"location":"KB/Oblique%20Slicing/","title":"Oblique Slicing","text":""},{"location":"KB/Oblique%20Slicing/#oblique-slicing","title":"Oblique Slicing","text":"<ul> <li>Resample the data on arbitrarily oriented slices</li> <li>Exploit 3D texture mapping functionality</li> <li>Store volume in 3D texture</li> </ul>"},{"location":"KB/Obsidian%20tutorial/","title":"Obsidian tutorial","text":"","tags":["scientificresearch"]},{"location":"KB/Obsidian%20tutorial/#obsidian-tutorial","title":"Obsidian Tutorial","text":"","tags":["scientificresearch"]},{"location":"KB/Obsidian%20tutorial/#hello-there","title":"Hello There","text":"","tags":["scientificresearch"]},{"location":"KB/Obsidian%20tutorial/#creating-a-file","title":"Creating a File","text":"<ul> <li>Markdown basics<ul> <li>Paste into links</li> <li>Images<ul> <li>Drag and Drop vs Paste</li> </ul> </li> <li>Headings</li> <li>Bullets</li> <li>and numbers</li> <li>Code   <code>python   import numpy as np   s = np.array([1])   print(s)</code></li> <li>Tables<ul> <li>(Cmd/Control P -&gt; insert table)</li> </ul> </li> </ul> </li> <li>Linking files (double bracket)</li> <li>Zen Mode (Cmd/Control P -&gt; search)</li> </ul>","tags":["scientificresearch"]},{"location":"KB/Obsidian%20tutorial/#organization","title":"Organization","text":"<ul> <li>Tags - SmoothMix</li> <li>Templates - toc: true titler</li> <li>Folders</li> </ul>","tags":["scientificresearch"]},{"location":"KB/Obsidian%20tutorial/#navigation","title":"Navigation","text":"<ul> <li>Links and Backlinks (GUI)</li> <li>Forward and Backward (Cmd/Control + Option + Left/Right) </li> <li>Auto TOC</li> </ul>","tags":["scientificresearch"]},{"location":"KB/Obsidian%20tutorial/#general-tips","title":"General Tips","text":"<ul> <li>Search (Cmd/Control P)</li> <li>Using the GUI is perfectly fine</li> </ul>","tags":["scientificresearch"]},{"location":"KB/Obsidian%20tutorial/#taking-notes-from-elsewhere","title":"Taking Notes from Elsewhere","text":"<ul> <li>Websites : Roam highlighter</li> <li>PDF annotation : 3544548.3581388</li> </ul>","tags":["scientificresearch"]},{"location":"KB/Obsidian%20tutorial/#zotero","title":"Zotero","text":"<ul> <li>Better BibTex<ul> <li>Export library -&gt; Better CSL/JSON</li> <li>Check keep updated and background export</li> </ul> </li> <li>Plugin : Citations<ul> <li>Enter path of better CSL/JSON</li> </ul> </li> <li>(Cmd/Control + Shift + M)</li> </ul>","tags":["scientificresearch"]},{"location":"KB/Obsidian%20tutorial/#advanced-options-that-are-super-useful","title":"Advanced Options that Are Super Useful","text":"<ul> <li> <p>Quick Latex</p> </li> <li> \\[ma=\\frac{\\frac{3\\lambda}{4}+34\\epsilon}{10}\\] </li> <li> \\[\\begin{equation}a+b\\end{equation}\\] </li> <li> <p>Local Graph View (No shortcut : Cmd + Shift + G)</p> <ul> <li>Time stamper (Cmd + T)</li> </ul> </li> <li>Linting (Cmd + S)</li> <li>File Preview (hold Cmd/Control) Docker Cheatsheet</li> <li>Note Refactoring</li> <li>Export to PDF</li> </ul>","tags":["scientificresearch"]},{"location":"KB/Obsidian%20tutorial/#extra-resources-if-you-care","title":"Extra Resources if You Care","text":"<ul> <li>Obsidian plugins for research</li> <li>Obsidian daily notes</li> <li>Browser extensions that are useful</li> <li>pdfannots</li> </ul>","tags":["scientificresearch"]},{"location":"KB/Occipital%20lobe/","title":"Occipital lobe","text":""},{"location":"KB/Occipital%20lobe/#occipital-lobe","title":"Occipital Lobe","text":"<ul> <li>Interprets vision (color, light, movement)</li> </ul>"},{"location":"KB/Occlusion/","title":"Occlusion","text":""},{"location":"KB/Occlusion/#occlusion","title":"Occlusion","text":"<ul> <li>occurs if a target object is hidden (occluded) by other objects Self-occlusion</li> <li>from a certain viewpoint, one part of an object is occluded by another part.</li> </ul>"},{"location":"KB/Occult%20Blood%20Screen/","title":"Occult Blood Screen","text":""},{"location":"KB/Occult%20Blood%20Screen/#occult-blood-screen","title":"Occult Blood Screen","text":"<ul> <li>Use of a chemically treated card or pad to test for blood hidden in a stool sample</li> </ul>"},{"location":"KB/Odido/","title":"Odido","text":"","tags":["jobs"]},{"location":"KB/Odido/#subhaditya-mukherjee-odido-data-scientist-application","title":"Subhaditya Mukherjee : Odido - Data Scientist Application","text":"<p>Hello Pauline,</p> <p>It's nice to meet you!\u00a0</p> <p>My name is Subhaditya, I recently graduated with a masters in AI from the RUG (Groningen), and am looking for my next big challenge. I found the position a good fit for what I can offer and what I want to do next, and so this is my formal application for the Data Scientist position.</p> <p>Over the past few years, I have worked with many clients (both freelance/part-time and in internships) and helped them bring their AI projects to life. I have a mix of experience - from ML (scikit-learn, xgboost, pandas) to advanced Gen AI (LLMs, Diffusion models) to the stack of Python, Git, Docker etc. I have been working with AI for a long time now, and am quite familiar with the stack, something that I think would be a good fit in this position.</p> <p>My main background is in R&amp;D, and so the Intelligence and Insights division is where I think I see myself in the most. I love a good challenge, and getting to work on a product that will affect so many people is such a nice opportunity.</p>","tags":["jobs"]},{"location":"KB/Offline%20Distillation/","title":"Offline Distillation","text":""},{"location":"KB/Offline%20Distillation/#offline-distillation","title":"Offline Distillation","text":"<ul> <li>The first stage in offline distillation is usually not discussed as part of knowledge distillation, i.e., it is assumed that the teacher model is pre-defined. Little at- tention is paid to the teacher model structure and its re- lationship with the student model</li> <li>The main advantage of offline methods is that they are simple and easy to be implemented. For example, the teacher model may contain a set of mod- els trained using different software packages, possibly located on different machines. The knowledge can be extracted and stored in a cache.</li> <li>The offline distillation methods usually employ one- way knowledge transfer and two-phase training pro- cedure. However, the complex high-capacity teacher model with huge training time can not be avoided, while the training of the student model in offline distillation is usually efficient under the guidance of the teacher model.</li> <li>Moreover, the capacity gap between large teacher and small student always exists, and student often largely relies on teacher.</li> </ul>"},{"location":"KB/Ogliodendrocytes/","title":"Ogliodendrocytes","text":""},{"location":"KB/Ogliodendrocytes/#ogliodendrocytes","title":"Ogliodendrocytes","text":"<ul> <li>Wrap and insulate, forms Myelin sheath</li> </ul>"},{"location":"KB/Ohms%20Law/","title":"Ohms Law","text":""},{"location":"KB/Ohms%20Law/#ohms-law","title":"Ohms Law","text":"<p>- Potential difference = current x resistance \u00a0- \\(\\(V= IR\\)\\) \u00a0- Ohm's Law applied to the full circuit: Electromotive force = current x (sum of the circuit resistance and the internal resistance of the cell) \u00a0- \\(\\(EMF = I(R+r)\\)\\)</p>"},{"location":"KB/On%20the%20Distinction%20Between%20Perceived%20Duration%20and%20Event%20Timing%20-%20Towards%20a%20Unified%20Model%20of%20Time%20Perception/","title":"On the Distinction Between Perceived Duration and Event Timing - Towards a Unified Model of Time Perception","text":""},{"location":"KB/On%20the%20Distinction%20Between%20Perceived%20Duration%20and%20Event%20Timing%20-%20Towards%20a%20Unified%20Model%20of%20Time%20Perception/#on-the-distinction-between-perceived-duration-and-event-timing-towards-a-unified-model-of-time-perception","title":"On the Distinction Between Perceived Duration and Event Timing - Towards a Unified Model of Time Perception","text":"<ul> <li> <p>Darren Rhodes</p> </li> <li> <p>neural and computational bases for the processing of time remains unknown</p> </li> <li>he distinction between perceived event timing and perceived duration provides the current for navigating a river of contemporary approaches to time perception</li> <li>Recent work has advocated a Bayesian approach to time perception</li> <li>This framework has been applied to both duration and perceived timing, where prior expectations about when a stimulus might occur in the future (prior distribution) are combined with current sensory evidence (likelihood function) in order to generate the perception of temporal properties (posterior distribution)</li> <li>these models predict that the brain uses temporal expectations to bias perception in a way that stimuli are 'regularized' i.e. stimuli look more like what has been seen before</li> </ul>"},{"location":"KB/On%20the%20Distinction%20Between%20Perceived%20Duration%20and%20Event%20Timing%20-%20Towards%20a%20Unified%20Model%20of%20Time%20Perception/#from-perceived-duration-to-perceived-timing","title":"From Perceived Duration to Perceived Timing","text":"<ul> <li>The word 'perceived' here, is used in the loosest sense \u2014 the above methods cannot demonstrably show changes in low-level sensory processing of time (Rhodes)</li> </ul>"},{"location":"KB/On%20the%20Distinction%20Between%20Perceived%20Duration%20and%20Event%20Timing%20-%20Towards%20a%20Unified%20Model%20of%20Time%20Perception/#a-bayesian-model-of-perceived-event-timing","title":"A Bayesian Model of Perceived Event Timing","text":"<ul> <li>based on the dynamic updating of temporal expectations</li> <li>explain the asymmetries in the detection of irregularity and also in the perceived event timing of stimuli (Di Luca &amp; Rhodes)</li> <li>Within a single trial, perceived timing (the posterior distribution) is the result of combining the probability of sensing a stimulus (likelihood) with the time it was expected (prior)</li> <li>key tenet of the model is the relaxation of the assumption of normality in the probability distribution</li> <li>Probability distributions in the temporal domain are asserted to be necessarily asymmetric due to the way time flows</li> <li>The anisotropic nature of time means that evidence accumulated about stimulus timing for the likelihood function can only start after a short delay</li> <li>due to neural processing</li> <li>a stimulus cannot be sensed before a stimulus is presented</li> <li>always the chance it could be perceived a bit later than average due to noise in the sensory system</li> <li>As such, the perceived timing of stimuli in an environment where trials are isochronous should exhibit the temporal KB/Regularization.md effect</li> <li>early stimuli should be delayed towards expectation whilst late stimuli should be accelerated</li> <li>Stimuli presented on time, in contrast are perceptually accelerated</li> <li>stimuli that are presented in a random sequence of irregular timings, should not have any temporal expectations built up</li> <li>Therefore, they should not have any modulation of their perceived timing, suggesting that a prior is not built</li> <li>An implicit assumption of the model is that noisier measurements should lead to broader likelihood functions that are captured more by the prior probability distributions</li> <li>the Bayesian model of perceived timing can explain the delay of early stimuli as well as the acceleration of on time and later than expected stimuli</li> <li>Interval models do not make any explicit predictions about changes in the perceived timing of stimuli and as such cannot account for this data.</li> </ul>"},{"location":"KB/On%20the%20Importance%20of%20Visual%20Context%20for%20Data%20Augmentation%20in%20Scene%20Understanding/","title":"On the Importance of Visual Context for Data Augmentation in Scene Understanding","text":""},{"location":"KB/On%20the%20Importance%20of%20Visual%20Context%20for%20Data%20Augmentation%20in%20Scene%20Understanding/#on-the-importance-of-visual-context-for-data-augmentation-in-scene-understanding","title":"On the Importance of Visual Context for Data Augmentation in Scene Understanding","text":"<ul> <li>Nikita Dvornik, Julien Mairal, Senior Member, IEEE, and Cordelia Schmid, Fellow, IEEE</li> </ul>"},{"location":"KB/On%20the%20Importance%20of%20Visual%20Context%20for%20Data%20Augmentation%20in%20Scene%20Understanding/#abstract","title":"Abstract","text":"<ul> <li>simple image transformations can already improve predictive performance in most vision tasks, larger gains can be obtained by leveraging task-specific prior knowledge</li> <li>blending objects in existing scenes</li> <li>using instance segmentation annotations</li> <li>that randomly pasting objects on images hurts the performance, unless the object is placed in the right context.</li> <li>explicit context model by using a convolutional neural network</li> <li>predicts whether an image region is suitable for placing a given object or not. In our experiments</li> </ul>"},{"location":"KB/On%20the%20Importance%20of%20Visual%20Context%20for%20Data%20Augmentation%20in%20Scene%20Understanding/#introduction","title":"Introduction","text":"<ul> <li>scene understanding</li> <li>context model based on a convolutional neural network.</li> <li>The model estimates the likelihood of a particular object category to be present inside a box given its neighborhood, and then automatically finds suitable locations on images to place new objects and perform data augmentation.</li> </ul>"},{"location":"KB/On%20the%20Importance%20of%20Visual%20Context%20for%20Data%20Augmentation%20in%20Scene%20Understanding/#explicit-context-modeling-by-cnn","title":"Explicit Context Modeling by CNN","text":"<ul> <li>to guess the category of an object just by looking at its visual surroundings</li> <li>modeling by a convolutional neural network,</li> <li>Contextual data generation</li> <li>dataset that comes with bounding box</li> <li>object class annotations</li> <li>Each ground-truth bounding box in the dataset is able to generate positive \"contextual images\" that are used as input to the system</li> <li>One box is able to generate multiple different context images,</li> <li>To prevent distinguishing between positive and background images only by looking at the box shape and to force true visual context modeling, we estimate the shape distribution of positive boxes and sample the background ones from it</li> <li>we estimate the joint distribution of scale s and aspect ratio a with a two-dimensional histogram</li> <li>draw a pair (s, a) from this distribution in order to construct a background box</li> <li>Since in natural images there is more background boxes than the ones actually containing an object, we address the imbalance by sampling more background boxes,</li> </ul>"},{"location":"KB/On%20the%20Importance%20of%20Visual%20Context%20for%20Data%20Augmentation%20in%20Scene%20Understanding/#model-training","title":"Model training","text":"<ul> <li>The input to the network are the \"contextual images</li> <li>300 \u00d7 300</li> <li>output of the network is a label in {0, 1, ..., C}</li> <li>0-th class represents background and corresponds to a negative \"context image</li> <li>ResNet50</li> <li>change the last layer to be a softmax with C + 1 activations</li> </ul>"},{"location":"KB/On%20the%20Importance%20of%20Visual%20Context%20for%20Data%20Augmentation%20in%20Scene%20Understanding/#context-driven-data-augmentation","title":"Context-driven Data Augmentation","text":"<ul> <li>Selection of candidate locations for object placement</li> <li>Since the model takes into account not only the visual surroundings but a box's geometry too, we need to consider all possible boxes inside an image to maximize the recall</li> <li>However this is too costly and using 200 candidates was found to provide good enough bounding boxes among the top scoring ones.</li> <li>if an object of category c is present in an image it is a confident signal for the model to place another object of this class nearby.</li> <li>This often happens when only 200 candidate locations are sampled; however, evaluating more locations would introduce a computational overhead</li> <li>simple heuristic</li> <li>consists of drawing boxes in the neighborhood of this object</li> <li>and adding them to the final candidate set. The added boxes have the same geometry (up to slight</li> <li>distortions) as the neighboring object's box.</li> <li>Candidate scoring process</li> <li>softmax output.</li> <li>generating a contextual image is not deterministic, predictions on two contextual images corresponding to the same box may differ substantially,</li> <li>After the estimation stage we retain the boxes where an object category has score greater than 0.7</li> <li>Blending objects in their environment</li> <li>blend an object at the corresponding location</li> <li>different types of blending techniques (Gaussian or linear blur, simple copy-pasting with no postprocessing, or generating blur on the whole image to imitate motion), and randomly choose one of them in order to introduce a larger diversity of blending artefacts</li> <li>We also do not consider Poisson blending in our approach, which was considerably slowing down the data generation procedure</li> <li>for our task than in [5]. As a consequence, we do not need to exploit external data to perform data augmentation</li> </ul>"},{"location":"KB/On%20the%20Importance%20of%20Visual%20Context%20for%20Data%20Augmentation%20in%20Scene%20Understanding/#updating-image-annotation","title":"Updating image annotation.","text":"<ul> <li>Once a new object is placed in the scene, we generate a bounding box for object detection by drawing the tightest box around that object</li> <li>In case where an initial object is too occluded by the blended one, i.e. the IoU between their boxes is higher than 0.8, we delete the bounding box of the original object from the annotations</li> <li>If a new instance occludes more than 80% of an object already present in the scene, we discard annotations for all pixels belonging to the latter instance.</li> <li>To obtain semantic segmentation masks from instance segmentations, each instance pixel is labeled with the corresponding objects class.</li> </ul>"},{"location":"KB/On%20the%20Importance%20of%20Visual%20Context%20for%20Data%20Augmentation%20in%20Scene%20Understanding/#why-is-random-placement-not-working","title":"Why is Random Placement not Working?","text":"<ul> <li>as violation of context constraints imposed by the dataset</li> <li>objects looking \"out of the scene\" due to different illumination conditions</li> <li>simply artifacts introduced due to blending techniques</li> </ul>"},{"location":"KB/On%20the%20Importance%20of%20Visual%20Context%20for%20Data%20Augmentation%20in%20Scene%20Understanding/#impact-of-blending-when-the-context-is-right","title":"Impact of blending when the context is right","text":"<ul> <li>lack of visual context and the presence of blending artefacts may explain the performance drop</li> <li>presence of difference in illumination and blending artefacts is not critical for the object detection task</li> <li>Reducing the need for pixel-wise object annotation</li> <li>Our data augmentation technique requires instance-level segmentations, which are not always available in realistic scenarios</li> <li>relax the annotation requirements for our approach and show that it is possible to use the method when only bounding boxes are available</li> <li>Semantic segmentation + bounding box annotation</li> <li>Instance segmentation masks provide annotations to each pixel in an image and specify (i) an instance a pixel belongs to and (ii) class of that instance</li> <li>If these annotations are not available</li> <li>one may approximate them with semantic segmentation and bounding boxes annotation</li> <li>Semantic segmentation annotations are also pixel-wise, however they annotate each pixel only with the object category.</li> <li>Instance-specific information could be obtained from object bounding boxes, however this type of annotation is not pixel-wise and in some cases is not sufficient to assign each pixel to the correct instanc</li> <li>as long as a pixel in semantic map is covered by only one bounding box, it uniquely defines the object it belong</li> <li>otherwise, if more than one box covers the pixel, it is not clear which object it comes from</li> <li>When deriving approximate instance masks from semantic segmentation and bounding boxes (see Figure 9, column 2), we randomly order the boxes and assign pixels from a semantic map to the corresponding instances</li> <li>Whenever a pixel could be assigned to multiple boxes we choose a box that comes first in the orderin</li> </ul>"},{"location":"KB/On%20the%20Importance%20of%20Visual%20Context%20for%20Data%20Augmentation%20in%20Scene%20Understanding/#importance-of-context-modeling-quality-for-scene-understanding","title":"Importance of Context Modeling Quality for Scene Understanding","text":"<ul> <li>quality of a context model is mainly influenced by the amount of data it has received for training</li> <li>we increase the data size used for context modeling, we can see how both detection and segmentation improve; however, this gain diminishes as the data size keeps growin</li> <li>to improve scene understanding, the context model has to get visual context \"approximately right\" and further improvement is most likely limited by other factors such as unrealistic generated scenes and limited number of instances that are being copy-pasted</li> <li>On the other hand, if the context model is trained with little data, as in the case of using only 5% of the full set, our augmentation strategy tends to the random one and shows little improvement</li> </ul>"},{"location":"KB/On%20the%20Importance%20of%20Visual%20Context%20for%20Data%20Augmentation%20in%20Scene%20Understanding/#images","title":"Images","text":""},{"location":"KB/On%20the%20overlap%20between%20Grad-CAM%20saliency%20maps%20and%20explainable%20visual%20features%20in%20skin%20cancer%20images/","title":"On the overlap between Grad-CAM saliency maps and explainable visual features in skin cancer images","text":""},{"location":"KB/On%20the%20overlap%20between%20Grad-CAM%20saliency%20maps%20and%20explainable%20visual%20features%20in%20skin%20cancer%20images/#on-the-overlap-between-grad-cam-saliency-maps-and-explainable-visual-features-in-skin-cancer-images","title":"On the Overlap Between Grad-CAM Saliency Maps and Explainable Visual Features in Skin Cancer Images","text":"<ul> <li>@nunnariOverlapGradCAMSaliency2021</li> <li>Nunnari, Fabrizio, Md Abdul Kadir, and Daniel Sonntag. 2021. \u201cOn the Overlap Between Grad-CAM Saliency Maps and Explainable Visual Features in Skin Cancer Images.\u201d Pp. 241\u201353 in Machine Learning and Knowledge Extraction. Vol. 12844, Lecture Notes in Computer Science, edited by A. Holzinger, P. Kieseberg, A. M. Tjoa, and E. Weippl. Cham: Springer International Publishing.</li> </ul>"},{"location":"KB/On%20the%20overlap%20between%20Grad-CAM%20saliency%20maps%20and%20explainable%20visual%20features%20in%20skin%20cancer%20images/#intro","title":"Intro","text":"<ul> <li>Dermatologists recognize melanomas by inspecting images in which they identify human-comprehensible visual features.</li> <li>investigate to what extent such features correspond to the saliency areas identified on CNNs trained for classification</li> <li>Saliency maps are images that indicate the pixels areas contributing to a certain classification decision. Saliency maps are normally encoded as greyscale images or converted to heatmaps for visual inspection.</li> <li>to what extent saliency maps can be used to identify visual features of skin lesions</li> </ul>"},{"location":"KB/On%20the%20overlap%20between%20Grad-CAM%20saliency%20maps%20and%20explainable%20visual%20features%20in%20skin%20cancer%20images/#related-work","title":"Related Work","text":"<ul> <li>ISIC 2018</li> <li>RISE</li> <li>Jahanifar et al. also propose a modified DRFI (Discriminative Regional Feature Integration) technique for a similar task for multi-level segmentation task</li> <li>By combining multiple segmentation masks, they produce a more accurate mask.</li> <li>During the generation of the mask, they use a threshold value of 0.5, but they did not provide a reason for which they choose this value.</li> </ul>"},{"location":"KB/On%20the%20overlap%20between%20Grad-CAM%20saliency%20maps%20and%20explainable%20visual%20features%20in%20skin%20cancer%20images/#classification-architectures-and-models","title":"Classification Architectures and Models","text":"<ul> <li>RESNET50</li> <li>VGG16</li> </ul>"},{"location":"KB/On%20the%20overlap%20between%20Grad-CAM%20saliency%20maps%20and%20explainable%20visual%20features%20in%20skin%20cancer%20images/#data-preparation","title":"Data Preparation","text":"<ul> <li>As an additional feature, we compute the pixels-wise union of all the features</li> <li>In our experiments, we ignore the skin lesion samples with no features.</li> <li>The generation of the saliency maps consists of running the Grad-CAM algorithm on each skin lesion picture with non-black union mask</li> <li>We repeat the procedure for both the VGG16 and the RESNET50 models, generating the SV and SR greyscale picture sets</li> <li>To compare the saliency maps with ground truth maps, we scaled up SV and SR to the resolution of the original images using a nearest neighbour filter.</li> <li>We can observe that all distributions are strongly right skewed, and all \\(J_{s}\\) are mostly below 0.2, with the exception of a peak in performance for the pigment network clas</li> </ul>"},{"location":"KB/On%20the%20overlap%20between%20Grad-CAM%20saliency%20maps%20and%20explainable%20visual%20features%20in%20skin%20cancer%20images/#first-experiment","title":"First Experiment","text":"<ul> <li>With the first experiment we aim at identifying the threshold value that leads to a maximization of the overlap between saliency maps and ground truth</li> <li>To do so, we converted each saliency map into 11 binary maps using thresholds from 0.0 to 1.0 with steps of 0.1</li> <li>Then, we proceed by computing the Jaccard indices J between the ground truth and all of the processed saliencies S x V and S x R.</li> <li>For VGG16, among the features classes, the best threshold ranges between 0.4 and 0.7. The minimum J index is 0.0 on all categories, meaning that among all samples there is always at least one map with zero-overlap with the ground truth. The highest average (J=0.141) and maximum (J=0.797) belong to the pigmented network class.</li> <li>When switching to RESNET50, the best thresholds range between 0.3 and 0.7. With respect to VGG16, pigmented network and streaks present the worse performance, while the average J increases for the other three classes</li> <li>Surprisingly, the Jaccard indices measured with the RESNET50 maps, which have a resolution limited to 8x8 pixels, are comparable to the ones extracted from the VGG16 models (24x24 pixels)</li> <li>The second hypothesis is that the lower resolution of the RESNET50 maps is compensated by the higher accuracy of the classification model, i.e., a better overall overlap.</li> </ul>"},{"location":"KB/On%20the%20overlap%20between%20Grad-CAM%20saliency%20maps%20and%20explainable%20visual%20features%20in%20skin%20cancer%20images/#second-experiment","title":"Second Experiment","text":"<ul> <li>diving the samples into Melanoma and Nevus, and into correctly vs. wrongly classified samples.</li> <li>Here, the Jaccard indices are calculated using the union feature and using the best threshold identified in the first experiment, hence on S 0.5 R V and S 0.3</li> <li>For VGG16, we can observe that the mean J for correctly classified melanomas (0.135) is similar to the union class average (0.132).</li> <li>However, when melanomas are wrongly classified, the Jaccard index drops to 0.086, meaning that the saliency maps diverges from the ground truth</li> </ul>"},{"location":"KB/On%20the%20overlap%20between%20Grad-CAM%20saliency%20maps%20and%20explainable%20visual%20features%20in%20skin%20cancer%20images/#observation","title":"Observation","text":"<ul> <li>This could effectively help doctors is spotting a wrong classification</li> <li>The idea is that: if the classifier tells the doctor that the sample is a melanoma, but then the reported saliency areas diverge a lot from what would be manually marked, then doctors can be more easily induced to think that the system is misclassifying the image</li> </ul>"},{"location":"KB/On%20the%20overlap%20between%20Grad-CAM%20saliency%20maps%20and%20explainable%20visual%20features%20in%20skin%20cancer%20images/#discussion","title":"Discussion","text":"<ul> <li>Among the five features, only Pigment Network reaches the same level of accuracy of the union class.</li> <li>maximum J=0.136</li> <li>This is a huge annotation overhead when compared to labeling images with their diagnose class.</li> <li>The value of the threshold to reach the best J index varies among datasets and features. Since it is not possible to analytically foresee the best threshold of a given dataset, we suggest the development of interactive exploratory visual interfaces, where dermatologists can autonomously control the saliency threshold value in an interactive fashion for exploration.</li> <li>However, from a decomposition between classes and correctness of classification, it appears that, for higher resolution maps (24x24 pixels on VGG16), saliency maps overlap much better with ground truth features when the classifier is correctly classifying a melanoma (J=0.135) and performance drops when the prediction is incorrect (J=0.086).</li> <li>Further, we would like to investigate on better options for thresholding. In this paper, a global threshold, in the range of 0.0 to 1.0, was simultaneously searched and applied to all the saliency map.</li> <li>This allows for an \"emersion\" of the most relevant region of interests of a global scale</li> <li>However, there might be regions of saliency below the global threshold which are relevant with respect to the local surrounding area</li> <li>To spot local maxima, we could split the maps into tiles, or super-pixels, and iteratively identify multiple local threshold values based on the range of saliency values of each region.</li> <li>Finally, the current implementation of Grad-CAM returns saliency maps whose range is filled by stretching the range of activation values of the target convolution layer.</li> <li>Each saliency map is forced to use the full activation range, independent of other samples.</li> <li>In so doing, regions of interests are \"forced\" to emerge, even when the activation values of the inner layer are lower when compared to other images.</li> </ul>"},{"location":"KB/On%20the%20overlap%20between%20Grad-CAM%20saliency%20maps%20and%20explainable%20visual%20features%20in%20skin%20cancer%20images/#future-work","title":"Future Work","text":"<ul> <li>As future work, we could consider performing saliency normalization according to global statistics (mean and variance) on the tested set.</li> </ul>"},{"location":"KB/On%20the%20overlap%20between%20Grad-CAM%20saliency%20maps%20and%20explainable%20visual%20features%20in%20skin%20cancer%20images/#images","title":"Images","text":""},{"location":"KB/One%20cycle%20policy/","title":"One cycle policy","text":""},{"location":"KB/One%20cycle%20policy/#one-cycle-policy","title":"One Cycle Policy","text":"<ul> <li>@smithSuperConvergenceVeryFast2018</li> </ul>"},{"location":"KB/One%20cycle%20policy/#step","title":"Step","text":"<ul> <li>recommends to do a cycle with two steps of equal lengths, one going from a lower learning rate to a higher one than go back to the minimumThe maximum should be the value picked with the Learning Rate Finder, and the lower one can be ten times lower</li> <li>Then, the length of this cycle should be slightly less than the total number of epochs, and, in the last part of training, we should allow the learning rate to decrease more than the minimum, by several orders of magnitude.</li> <li>The idea of starting slower isn't new: using a lower value to warm-up the training is ofen done, and this is exactly what the first part is achieving</li> <li>Leslie doesn't recommend to switch to a higher value directly, however, but to rather slowly go there linearly, and to take as much time going up as going down.</li> <li>the during the middle of the cycle, the high learning rates will act as regularization method, and keep the network from overfitting</li> <li>They will prevent the model to land in a steep area of the loss function, preferring to find a minimum that is flatteapproximates of the hessian were lower, indicating that the SGD was finding a wider flat area</li> <li>Then the last part of the training, with descending learning rates up until annihilation will allow us to go inside a steeper local minimum inside that smoother part</li> <li>Surprisingly, applying this policy even allows us to pick larger maximum learning rates, closer to the minimum of the plot we draw when using the learning rate finder</li> <li>Those trainings are a bit more dangerous in the sense that the loss can go too far away and make the whole thing diverge In those cases, it can be worth to try with a longer cycle before going to a slower learning rate, since a long warm-up seems to help</li> <li></li> </ul>"},{"location":"KB/One%20cycle%20policy/#cyclical-momentum","title":"Cyclical Momentum","text":"<ul> <li>To accompany the movement toward larger learning rates, Leslie found in his experiments that decreasing the momentum led to better results</li> <li>This supports the intuition that in that part of the training, we want the SGD to quickly go in new directions to find a flatter area, so the new gradients need to be given more weight</li> <li>According to Leslie, the exact best value of momentum chosen during the whole training can give us the same final results, but using cyclical momentums removes the hassle of trying multiple values and running several full cycles, losing precious time.</li> <li>In his opinion, the batch size should be set to the highest possible value to fit in the available memory. Then the other hyper-parameters we may have (dropout for instance) can be tuned the same way as weight decay, or just by trying on a cycle and see the results they give</li> <li>Training with the 1cycle policy at high learning rates is a method of regularization in itself, so we shouldn't be surprised if we have to reduce the other forms of regularization we were previously using when we put it in place</li> <li></li> </ul>"},{"location":"KB/One%20hot/","title":"One Hot Encoding","text":""},{"location":"KB/One%20hot/#one-hot-encoding","title":"One Hot Encoding","text":"<ul> <li>Given \\(A = {a_{1}, \u2026 , a_{k}}\\)</li> <li>Turn each \\(a_{v}\\) into k dim binary vector \\(v_{v} \\in {0,1}^{k}\\) which is 0 everywhere execpt at position v</li> <li>Symbolic input</li> <li>k dim one hot vector</li> <li></li> </ul>"},{"location":"KB/Opacity%20Correction/","title":"Opacity Correction","text":""},{"location":"KB/Opacity%20Correction/#opacity-correction","title":"Opacity Correction","text":"<ul> <li>Opacity component \\(o\\) in transfer function stored with respect to a standard step size \\(\\Delta t\\)</li> <li>Different step sizes \\(\\Delta t^{\\ast}\\)</li> <li>Dynamic step sizes</li> <li>generate pre-integrated function</li> <li> \\[o^{\\ast} = 1- (1-o)^{\\frac{\\Delta t^{\\ast}}{\\Delta t}}\\] </li> <li>evaluate after obtaining \\(o\\) from transfer function</li> <li>apply before compositing`</li> </ul>"},{"location":"KB/OpenML%20%3C%3E%20scikit-learn%20hackathon/","title":"OpenML <> scikit-learn hackathon","text":"","tags":["deeplearning"]},{"location":"KB/OpenML%20%3C%3E%20scikit-learn%20hackathon/#openml-scikit-learn-hackathon","title":"OpenML &lt;&gt; Scikit-learn Hackathon","text":"","tags":["deeplearning"]},{"location":"KB/OpenML%20%3C%3E%20scikit-learn%20hackathon/#logistics","title":"Logistics","text":"","tags":["deeplearning"]},{"location":"KB/OpenML%20%3C%3E%20scikit-learn%20hackathon/#location","title":"Location","text":"<p>27th Floor, ring at \u201cProbabl\u201d</p> <p>Montparnasse Tower\u00a033 Avenue du Maine, West Entrance (on your left when leaving the train station), 27th Floor, ring at \u201cProbabl\u201d</p> <p></p>","tags":["deeplearning"]},{"location":"KB/OpenML%20%3C%3E%20scikit-learn%20hackathon/#contact-persons","title":"Contact Persons","text":"<p>+33783822597 (Fran\u00e7ois Goupil)</p> <p>+33760407677 (Charl\u00e8ne Bizollon)</p>","tags":["deeplearning"]},{"location":"KB/OpenML%20%3C%3E%20scikit-learn%20hackathon/#communication-channel","title":"Communication Channel","text":"<p>Please join the OpenML slack server and the dedicated hackathon channel for easy communication and updates.</p> <p>Slack:\u00a0https://join.slack.com/t/openml/shared_invite/zt-2ktk2cj1c-r637o20pfCc0H7PS8OUGtA</p> <p>Channel: #hackathon-probabl</p>","tags":["deeplearning"]},{"location":"KB/OpenML%20%3C%3E%20scikit-learn%20hackathon/#wifi","title":"Wifi","text":"<p>SSD: :probabl.guest</p> <p>PWD: :probabl.</p> <p>Note: At some point be ready to use your own mobile data (we are experiencing some difficulties)</p> <p>SSD: :probabl.eiffel-2.4</p> <p>PWD: :probabl.</p> <p>Note: low bandwidth</p>","tags":["deeplearning"]},{"location":"KB/OpenML%20%3C%3E%20scikit-learn%20hackathon/#schedule","title":"Schedule","text":"","tags":["deeplearning"]},{"location":"KB/OpenML%20%3C%3E%20scikit-learn%20hackathon/#june-24-0900-1800","title":"June 24 - 09:00-18:00","text":"09:00-09:30 Welcome Coffee and Croissants 09:30-11:30 Introduction - Short presentation of the scikit-learn and OpenML projects + Probabl (Joaquin and Pieter for OpenML, Guillaume Lemaitre for scikit-learn, Yann Lechelle for Probabl)- Quick round where everyone introduces themselves.- Plan break-out sessions or suggest new ones. 10:30-11:30 Breakout (1) Organizing community events and Onboarding Contributors (Maren) 11:30-13:00 Lunch Bouillon Chartier 13:00-14:00 Breakout (2) Governance, Funding and Sponsorship (Adrin + Fran\u00e7ois) 14:00-15:00 Breakout (3) Future Collaboration between scikit-learn and OpenML (Guillaume) 15:00-18:00 Code Code: explore each other\u2019s projects. <p>After the official programme each day, there\u2019s a suggested bar and restaurant to go to</p> <p>Bar + Restaurant: Le Falstaff </p>","tags":["deeplearning"]},{"location":"KB/OpenML%20%3C%3E%20scikit-learn%20hackathon/#june-25-0900-1800","title":"June 25 - 09:00-18:00","text":"09:00-10:00 Coffee/croissants Croissant Talk by Joaquin 10:00-11:00 Breakout (6) Collaboration Ecosystem for Open-Source Machine Learning 11:00-12:00 Breakout (7) Academic and Industrial Scope of OpenML and Probabl\u00a0in AI - Collaboration 12:00-13:00 Lunch TranTranZai 13:00-14:00 Breakout (5) Probabl Product Technical Discussion (Camille) 14:00-17:00 Coding Coding 17:00-18:00 Breakout (4) Development Tooling and Workflows <p>Bar + Restaurant: Food Society Paris</p>","tags":["deeplearning"]},{"location":"KB/OpenML%20%3C%3E%20scikit-learn%20hackathon/#june-26-0900-1300","title":"June 26 - 09:00-13:00","text":"9:00-10:00 Breakout + coffee/croissants Joaquin et al.: we have a bit of a delay checking out of our Airbnb but will be there shortly. 10:00-12:00 TBD Open / Ad-hoc 12:00-13:00 Wrap-up 13:00-14:00 Lunch + end of the hackathon Subway in Montparnasse","tags":["deeplearning"]},{"location":"KB/OpenML%20%3C%3E%20scikit-learn%20hackathon/#breakout-sessions-ideas","title":"Breakout Sessions Ideas","text":"<p>This document contains the preliminary agenda and suggestion for breakout sessions. Breakout sessions are discussions where we can brainstorm or exchange our experiences on specific topics. Feel free to propose additional sessions.</p> <p>\ud83d\udca1 Feel free to add new session topics below, there is a template at the end.</p>","tags":["deeplearning"]},{"location":"KB/OpenML%20%3C%3E%20scikit-learn%20hackathon/#1-organizing-community-events-and-onboarding-contributors-day-1","title":"1. Organizing Community Events and Onboarding Contributors [Day 1]","text":"<p>leader:\u00a0Maren Westermann</p> <p>description:</p> <ul> <li>Share our experiences organizing hackathons. How do you attract attendees? How do you make sure that the work at a hackathon is fruitful? Where should you organize your hackathons, and how are they funded? Are online open-source sprints/hackathons an option for you?</li> <li>What process and documentation should be in place to help onboard new contributors? How to get them started effectively, and how do you make sure they stay with the project?</li> <li>How do we get our projects known to users?</li> </ul> <p></p> <p>notes:</p> <ul> <li>community sprints: everyone is invited to take part, in particular newcomers, to \u00a0contribute to an open source project</li> <li> <p>Important (FYI: we are a bit out of sync)</p> </li> <li> <p>Have a list of curated issues</p> </li> <li>Start with documentation issues for new contributors</li> <li> <p>Come up with issues before the sprint need a curated list for first time contributors (especially beginner friendly ones)</p> </li> <li> <p>meta-issues for a group of related issues: once one is fixed it can serve as a contribution template</p> </li> <li> <p>documentation issues: people start by reading the documentation around the issues</p> </li> <li>documentation about how to contribute was missing and is too long</li> <li>rewrote the contributors\u2019 guide to be more concise but more beginner friendly + some video tutorials to get started with github-based contributions</li> <li>keeping beginner issues away for the sprints (so other people don\u2019t jump on it before)</li> <li> <p>difference between OpenML Hackathon and Scikit-learn sprints</p> </li> <li> <p>OpenML Hackathons are 1 week and bigger</p> </li> <li>Core developer sprints vs. New contributors sprints</li> <li>Openml has 7 repositories with different programming languages (backend, APIs, frontend,...)</li> <li>Some documentation for first time contributors but not exhaustive</li> <li>One-to-one mentoring to get started with a working dev setup</li> <li> <p>Typically requires several days\u2019 investment</p> </li> <li> <p>onboarding new contributors online and in person are two different processes</p> </li> <li> <p>Hard to make people feel connected and stay long-term</p> </li> <li> <p>Social aspect is important: organizing recurrent events (every few months) to development a more long term engagement</p> </li> <li> <p>Joint Pyladies Paris / scikit-learn core contributors events</p> </li> <li> <p>near one-to-one mentoring</p> </li> <li>recurrent every few months</li> <li>a few hours in the evening</li> <li> <p>retention is low but allowed to developed social bounds</p> </li> <li> <p>Important to have maintainers be present to slowly build a connection.</p> </li> <li>Retention is low, be realistic on expectations - but the outliers are what matters</li> <li> <p>Personal connection: if they know you, people are more likely to contribute</p> </li> <li> <p>OpenML hackathons are useful for core maintainers to secure some time to contribute to the project for several solid hours in a row.</p> </li> <li> <p>0 full-time contributor.</p> </li> <li>part time engineers for academic projects</li> <li> <p>nice locations because thanks to EU funding</p> </li> <li> <p>Can OpenML use students to help contribute feature</p> </li> <li> <p>Hard to get high-quality submissions</p> </li> <li> <p>How to incentivize people?</p> </li> <li> <p>Build career: show of with sklearn contribution on a resume (less long-term contribution)</p> </li> <li>Sense of community</li> <li>Useful for your own research</li> <li> <p>Hard to have \u2018flashy results\u2019 (e.g. genAI apps), how can we solve that?</p> </li> <li> <p>How to scale time investment?</p> </li> <li> <p>PyLadies / sprints: only few hours in the evening (6-9pm)</p> </li> <li>Every 2 months, 15-30 (capped) people show up</li> <li> <p>How are sprints structured?</p> </li> <li> <p>Pre-sprint: online, so people have the right setup</p> </li> <li>At least one organizer (e.g. Maren for pyladies, supported by core devs)</li> <li> <p>Shortlist of issues for each sprint</p> </li> <li> <p>Paid internships: great way to find good people and build</p> </li> <li> <p>Requires funding</p> </li> <li> <p>Mentoring takes time, but helps people take over some tasks</p> </li> <li>Slack discussions</li> <li> <p>Generative AI? Bad quality, wastes time. SKlearn only allows human contributions.</p> </li> <li> <p>How to focus attention? E.g. key project this quarter?</p> </li> </ul> <p></p>","tags":["deeplearning"]},{"location":"KB/OpenML%20%3C%3E%20scikit-learn%20hackathon/#2-governance-funding-and-sponsorship","title":"2. Governance, Funding and Sponsorship","text":"<p>leader:\u00a0Adrin + Fran\u00e7ois</p> <p>description: What are our experiences with our governance structures?\u00a0What are opportunities for open source projects to make money to pay for e.g., server costs, organizing events, and so on? How do we argue the importance of our projects to motivate a funder/sponsor? Can we quantify our contribution?</p> <p>notes:</p> <ul> <li>Governance is a living document. It matters for the community.</li> <li> <p>How to combine an open-source library with a for-profit company</p> </li> <li> <p>Be clear about what parts are community-\u201downed\u201d and which parts are company-owned</p> </li> <li> <p>Keep discussions of the community aspect on community Slack. Decision processes must remain open.</p> </li> <li> <p>write public version of important decisions</p> </li> <li> <p>Still creates confusion (for users and contributors)</p> </li> <li>Keep people informed either through mailing lists, monthly meetings etc. (This takes a long time, but is worth it in the long run)</li> <li>communicate upcoming discussions in mailing list</li> <li>contributing to sklearn open doors to work at companies</li> <li> <p>INRIA foundation:</p> </li> <li> <p>50k EUR for 2 meetings yearly where company can state priorities</p> </li> <li>No requirement, only if useful for community</li> <li>Is it sustainable? For academic salaries. As long as sklearn stays useful companies will keep doing this. Requires that someone at the sponsoring company cares that sklearn doesn\u2019t decline.</li> <li>Also advertising (logo on website)</li> <li> <p>Modelled on Linux foundation.</p> </li> <li> <p>Probabl:</p> </li> <li> <p>Will put a RE to work on a certain issue, but for a lot more money</p> </li> <li> <p>Companies prefer this (they don\u2019t know how to hire a good sklearn engineer)</p> </li> <li> <p>Leadership by effort:</p> </li> <li> <p>Put your own time into the aspects that are important to you</p> </li> <li> <p>NVIDIA: Have people work at other companies to work on sklearn</p> </li> <li> <p>How much effort?</p> </li> <li> <p>For projects that need faster cycles, create a separate package (e.g. skops, hazardous,...) - but this creates maintenance work.</p> </li> <li>Hard to get new reviews in since it takes so long. Multiple rounds of reviews even for a simple spelling error.</li> <li>Library for putting sklearn models into prod -&gt; skops</li> <li> <p>Having documentation lead, community interaction lead,... does it help and how much?</p> </li> <li> <p>Lowers the bar (core dev is a really high bar)</p> </li> <li>Speeds up decisions</li> <li>Need more people in the different teams</li> </ul> <p></p>","tags":["deeplearning"]},{"location":"KB/OpenML%20%3C%3E%20scikit-learn%20hackathon/#3-future-collaboration-between-scikit-learn-and-openml-day-1-2-sessions","title":"3. Future Collaboration between Scikit-learn and OpenML [Day 1, 2 sessions]","text":"<p>leader:\u00a0Guillaume Lemaitre</p> <p>description: scikit-learn can fetch datasets from OpenML, users can automatically evaluate scikit-learn models on OpenML tasks. What other future collaborations are interesting to explore?</p> <ul> <li> <p>fetch_openml / download_openml improvements (parquet?)</p> </li> <li> <p>dataset upload via parquet -&gt; coming (not fully supported by python API yet)</p> </li> <li>croissant integration in scikit-learn openml data fetcher? -&gt; Tuesday morning</li> <li>provenance tracking and reproducibility for openml dataset (make it standard to provide a script, possibly hosted externally on GitHub or similar, to show how to reconstruct the openml hosted parquet file from the original dataset format/location.</li> <li>collaborative feedback (per dataset issue tracker) to report and discuss dataset related problems with dataset owner/uploader</li> <li>If fetch-openml fails, what to do?</li> <li>add support for benchmarking? : benchopt</li> </ul> <p>notes:</p> <ul> <li> <p>Fetch_openml</p> </li> <li> <p>ARFF parser is a headache</p> </li> <li>Logically, it makes sense to first download the data file localy and then load it with pandas or polars (pandas in rust)</li> <li>sklearn does not load parquet right now</li> <li> <p>Sparse datasets are still an issue (not supported by parquet)</p> </li> <li> <p>Make all sparse dataset dense and store them in parquet (will still compress nicely). Most sparse datasets aren\u2019t that large.</p> </li> <li> <p>Some of these datasets may be one-hot-encoded datasets</p> </li> <li> <p>Pyarrow is most supported (fastparquet is not). Polars can read parquet natively. Pyarrow does not support pyodide.</p> </li> <li> <p>Have an explanation for differences between versions</p> </li> <li> <p>Versions discussion</p> </li> <li> <p>Versions of dataset on openml very confusing</p> </li> <li>Version not related lineage of dataset, very confusing to user</li> <li>Versions of datasets not searchable</li> <li> <p>Use case: go on openml, search for a dataset, \u201cwhich version of the dataset to select?\u201d</p> </li> <li> <p>Benchmarking</p> </li> <li> <p>interesting for probabl.ai to share models and benchmarks on OpenML?</p> </li> <li> <p>Sklearn Pipeline Representation</p> </li> <li> <p>Use HTML widget for sklearn pipeline diagrams</p> </li> </ul> <p>todo:</p> <ul> <li>openml: check whether all parquet file can be read with polars and pandas</li> <li>openml: convert all sparse datasets to dense to store then in parquet</li> <li>openml: have an explanation for differences between versions. When people upload a new version of a dataset, ask for an explanation.</li> <li>openml: sort dataset by the quality of the datasheet. Show user/datasetname/id as the name on the webui, remove/rename \u201cversion\u201d</li> <li>openml website: implement a way to open an issue to contact the dataset owner</li> <li> <p>openml: datasheet has section on preprocessing, where people can point to a github link with preprocessing code, encourage users to do this (e.g. dataset quality score) and allow people to report problems</p> </li> <li> <p>sklearn: try to load parquet files from OpenML in fetch_openml</p> </li> <li>openml: visualization of the sklearn pipelines (flow)</li> </ul>","tags":["deeplearning"]},{"location":"KB/OpenML%20%3C%3E%20scikit-learn%20hackathon/#35-croissant-talk","title":"3.5 Croissant Talk","text":"<p>notes:</p> <ul> <li> <p>Github</p> </li> <li> <p>Croissant is a metadata description format</p> </li> <li>Ml datasets are a combination of structured and unstructured data, which make them complicated to manage</li> <li>Croissant was built on top of schema.org, and has more details relative to it</li> <li> <p>The format has 4 layers</p> </li> <li> <p>dataset level metadata</p> </li> <li>resource description</li> <li>content structure</li> <li> <p>ml semantics</p> </li> <li> <p>Croissant does not require any changes to underlying data</p> </li> <li>Analysis and visualization tools work out of the box for all datasets</li> <li>Using croissant, datasets can be exposed consistently throughout platforms</li> <li>Collaborations with google, hugging face, google dataset search also exist</li> <li>Openml has deeper dataset description by default, slightly lesser in HF and kaggle</li> <li>Once loaded, datasets can be imported elsewhere (torch, tf etc) easily</li> <li>Croissant editor - web app where you can use a GUI to enter the dataset descriptions</li> <li>NeurIPS also now recommends using the Croissant format</li> <li>Supports the Core RAI vocab for explainable AI</li> <li>If images/other files - points to the path</li> </ul> <p>todo:</p> <ul> <li>integer precision and more detailed dtypes</li> <li>How are uploaded files linked to each other?</li> <li>Lineage of datasets</li> </ul> <p></p>","tags":["deeplearning"]},{"location":"KB/OpenML%20%3C%3E%20scikit-learn%20hackathon/#4-development-tooling-and-workflows","title":"4. Development Tooling and Workflows","text":"<p>leader:\u00a0Pieter</p> <p>description: Automation is important to create more sustainable workloads and generally improves overall project quality. What tooling and workflows are employed in your projects to run tests, ensure code quality, help contributors, and so on? Which do you find most useful? Are there decisions have you come to regret? What are your major pain points? What are our responsibilities as open source projects, should we be embracing platforms such as e.g., CodeBerg/Forgejo more?</p> <p>Notes:</p> <ul> <li>Switch to open-source tools like CodeBerg once if offers more conveniences</li> <li>There is a GitHub maintainer org that you can apply to (if you are maintainer of an important enough package) that can give you more direct access to GH dev/projects.</li> <li>Use of Azure workflows in scikit-learn is largely historical, but also provides a spread over different (free) usage limits</li> <li>GPU actions with a limited budget</li> <li> <p>GitHub actions workflow problems</p> </li> <li> <p>testing is a pain and not really supported (easier on Azure)</p> </li> <li> <p>badish documentation but better than Azure</p> </li> <li> <p>run documentation examples that are linked to changes in PR diff - use Circle CI because it can easily render the generated HTML in the browser (as opposed to GH where you download the artifact)</p> </li> <li>bot for linter errors to post it as comment helped a lot</li> <li>aiming for 100% code coverage, including all validation though that is centralized. Disable coverage for certainly not tested parts of the code. Also test errors and types and warnings.</li> </ul>","tags":["deeplearning"]},{"location":"KB/OpenML%20%3C%3E%20scikit-learn%20hackathon/#5-probabl-product-technical-discussion","title":"5. Probabl Product Technical Discussion","text":"<p>leader:\u00a0 Camille Troillard</p> <p>description: Presentation/discussion of the Probabl technical product and potential collaboration.</p> <ul> <li>What to do to put sklearn in production, to make it commercially viable</li> <li> <p>Help data scientists do better ML</p> </li> <li> <p>Better understand their model\u2019s behavior</p> </li> <li> <p>Build something that we\u2019re proud of (and we\u2019re picky)</p> </li> <li> <p>Let people do what they do (don\u2019t interfere), but show interesting things along the way</p> </li> <li>You should not require a platform, but it should be very easy to switch to a platform</li> <li>Educate people. E.g. you\u2019re changing the metric but this metric doesn\u2019t make sense.</li> <li> <p>Interactive dashboard that shows results of your experiment</p> </li> <li> <p>code and outputs side by side</p> </li> <li> <p>Like Weights and Biases, but runs locally</p> </li> <li> <p>Outputs data in a portable DB, results are registered and predictions are shown when they are available</p> </li> <li> <p>unified API to the whole infrastructure stack (like MetaFlow)</p> </li> <li> <p>Button to \u2018push to production\u2019 or \u2018push to OpenML\u2019 depending on the user</p> </li> <li>\u201cWe\u2019ve been spoonfed microservices in order to become addicted to CSPs\u201d</li> </ul> <p>Feedback for OpenML</p> <ul> <li>Have a clear tagline, e.g. \u2018Frictionless ML resources\u2019</li> <li>Better search interface</li> <li>Nice visualizations for the run page</li> <li>Fast website</li> </ul> <p></p>","tags":["deeplearning"]},{"location":"KB/OpenML%20%3C%3E%20scikit-learn%20hackathon/#6-collaboration-ecosystem-for-open-source-machine-learning","title":"6. Collaboration Ecosystem for Open-Source Machine Learning","text":"<p>leader:\u00a0Lennart Purucker</p> <p>description: What other open-source frameworks are struggling with the same questions we are struggling with? Should we reach out to them? Is there a need for a collaboration ecosystem in open-source machine learning/AI? What are lessons learned from which others might benefit? What are lessons learned from others from which we might benefit?</p> <p>notes:</p> <ul> <li> <p>Struggles for Open Source</p> </li> <li> <p>Copying and learning from scikit-learn projects</p> </li> <li> <p>Bots, CI/CD, CI logic</p> </li> <li> <p>see https://scientific-python.org/, https://scipy.org/ </p> </li> <li> <p>https://learn.scientific-python.org/development/ </p> </li> <li> <p>helps with CI/CD setup and provide more documentation on how to setup a open source project</p> </li> <li> <p>Main issue is human traffic for open-source</p> </li> <li> <p>opening issues, PRs, \u2026</p> </li> <li> <p>What other open-source frameworks are struggling with the same questions we are struggling with?</p> </li> <li> <p>https://learn.scientific-python.org/contributors/setup/ecosystem/ </p> </li> <li> <p>ML Backbone</p> </li> <li> <p>Scikit-learn, PyTorch, TensorFlow, MLR, MLJ</p> </li> <li> <p>XGBoost, LightGBM, CatBoost</p> </li> <li> <p>OpenML, Pandas, NumPy, SciPy, Polars</p> </li> <li> <p>Python Backbone</p> </li> <li> <p>Pip / PyPi, Conda, vu</p> </li> <li> <p>Ray, Joblib</p> </li> <li> <p>ML Applications / AutoML / \u2026</p> </li> <li> <p>AMTLK, Auto-Sklearn, FLAML, AutoGluon, H2O \u2026</p> </li> <li> <p>Should we reach out to them?</p> </li> <li> <p>Company-driven open-source vs. community-driven open-source</p> </li> <li> <p>company-driven example</p> </li> <li> <p>tensorflow, (PyTorch)</p> </li> <li> <p>Internal CI vs. open-source CI</p> </li> <li> <p>community-driven</p> </li> <li> <p>scikit-learn</p> </li> <li> <p>Via GitHub</p> </li> <li> <p>Is there a need for a collaboration ecosystem in open-source machine learning/AI?</p> </li> <li> <p>Only if we have problems, otherwise unnecessary overhead.</p> </li> <li> <p>Are they my dependencies or am I there dependencies?</p> </li> <li> <p>What are lessons learned from which others might benefit? / What are lessons learned from others from which we might benefit?</p> </li> <li> <p>mostly the governance documents</p> </li> <li>document CI</li> <li>see https://scientific-python.org/ </li> <li>only start testing / maintaining other environments one request / when issues arise</li> <li>https://scikit-learn.org/stable/developers/minimal_reproducer.html </li> </ul> <p></p>","tags":["deeplearning"]},{"location":"KB/OpenML%20%3C%3E%20scikit-learn%20hackathon/#7-academic-and-industrial-scope-of-openml-and-probabl-in-ai","title":"7. Academic and Industrial Scope of OpenML and Probabl in AI","text":"<p>leader:\u00a0Lennart Purucker</p> <p>who else is joining?\u00a0Yann Lechelle</p> <p>description: Where do we see ourselves in the general field of AI/ML? Are we only tabular data? Are we connected to GenAI, Computer Vision, and NLP? How is our connection to industry applications? How do we effectively explain our position to stakeholders (who read too much about GenAI)?</p> <ul> <li> <p>Input Modalities</p> </li> <li> <p>Tabular (OpenML, Scikit-learn)</p> </li> <li>Time Series</li> <li>Vision (OpenML Soon)</li> <li>NLP</li> <li>Graphs</li> <li> <p>(Other)</p> </li> <li> <p>Output Modalities / Task</p> </li> <li> <p>Scalar Regression\u00a0(OpenML, Scikit-learn)</p> </li> <li>Quantile Regression (OpenML, Scikit-learn)</li> <li>Multiclass Classification (OpenML, Scikit-learn)</li> <li>No-target / Unsupervised / Data Insights \u00a0(OpenML, Scikit-learn)</li> <li>Survival Analysis (Scikit-learn)</li> <li>Forecasting</li> <li>Anomaly Classification</li> <li>Anomaly Detection (OpenML, Scikit-learn)</li> <li> <p>Generative AI: Structured Predictions</p> </li> <li> <p>ML Techniques in AI/ML</p> </li> <li> <p>Traditional ML Algorithms (SVM, RF, Boosting) (OpenML, Scikit-learn)</p> </li> <li>Traditional Deep Learning (OpenML)</li> <li> <p>Large Foundation Models</p> </li> <li> <p>What do Stakeholders Understand?</p> </li> <li> <p>Time Series</p> </li> <li> <p>GenAI</p> </li> <li> <p>Notes:</p> </li> <li> <p>scikit-learn limitation is the API definition</p> </li> <li> <p>probabl: \u201cown your data science\u201d</p> </li> <li> <p>border scope, may include other things besides scikit-learn</p> </li> <li>also deep learning and large foundation models</li> <li> <p>scope is wide around open-source technology</p> </li> <li> <p>can we connect OpenML to probabl scope</p> </li> <li>\u201cexporting\u201d the API?</li> <li>LLMs \u201cdo\u201d UX for ML</li> </ul>","tags":["deeplearning"]},{"location":"KB/OpenML%20%3C%3E%20scikit-learn%20hackathon/#_1","title":"OpenML <> scikit-learn hackathon","text":"<p>Aggregated To-Dos</p> <p>Openml</p> <ul> <li>check whether all parquet file can be read with polars and pandas</li> <li>convert all sparse datasets to dense to store then in parquet</li> <li>have an explanation for differences between versions. When people upload a new version of a dataset, ask for an explanation.</li> <li>sort dataset by the quality of the datasheet. Show user/datasetname/id as the name on the webui, remove/rename \u201cversion\u201d</li> <li>website: implement a way to open an issue to contact the dataset owner</li> <li>datasheet has section on preprocessing, where people can point to a github link with preprocessing code, encourage users to do this (e.g. dataset quality score) and allow people to report problems</li> <li>visualization of the sklearn pipelines (flow)</li> <li>data quality plots</li> <li>Does the UX of OpenML need work</li> </ul> <p>probabl</p> <ul> <li>try to load parquet files from OpenML in fetch_openml</li> </ul> <p>croissant</p> <ul> <li>integer precision and more detailed dtypes</li> <li>How are uploaded files linked to each other</li> </ul>","tags":["deeplearning"]},{"location":"KB/OpenML%20Software%20Engineer/","title":"OpenML Software Engineer","text":"","tags":["jobs"]},{"location":"KB/OpenML%20Software%20Engineer/#software-engineer-deep-learning-application","title":"Software Engineer - Deep Learning Application","text":"<p>Hello Joaquin,  </p> <p>My name is Subhaditya, it is nice to meet you :)</p> <p>I recently graduated with a masters in AI from the RUG (Groningen), and am now looking for a job in the Netherlands. I remember seeing the OpenML Github repo a while back and thinking it would be really cool to work with AI researchers with a love for open source. But I had just started my masters then, and somehow did not end up reaching out. So when I found this position while looking through Academic Transfer, I absolutely had to apply. I have been working with software engineering and AI for a few years now and I am very aware of the need for accurate benchmarks and sharing models efficiently. With so many people suddenly giving in to the AI hype, crafting tools that cater to the real problems of researchers trying to build AI, something like OpenML, is even more relevant now. Given that, I would love to bring my experience to this position, while also hopefully learning a lot from your team there too.</p> <p>Over the past few years, I have worked with many clients (both freelance/part-time and in internships) and helped them bring their AI and data projects to life. My experience includes working with large datasets, building ML/DL models and pipelines and also sharing my knowledge with my peers in technical and non technical fields. I am quite good at using PyTorch / TensorFlow / Keras and have some experience with Flux.jl and Max as well. I am also familiar with the Hugging Face API / PyTorch lightning and the ONNX format. More than anything though, I am a programmer at heart. I have been an avid believer in open source ever since I started using linux almost seven years ago, and although I have a lot to learn, I do think that I am a good fit for this position. (Here is my Github - http://github.com/subhadityamukherjee)</p> <p>This letter is getting a little long, but I did want to add that I would love to contribute to OpenML and bring about positive changes in the AI community going forward. So many people with not much AI experience are starting to enter this field, and perhaps we could help make their experience a lot smoother, while also promoting more open source research. I have no means of knowing how experienced the other candidates are, but I do hope you give me a shot!</p> <p>PS. I still have my Zoekjaar visa till September, so I can start ASAP as well.</p>","tags":["jobs"]},{"location":"KB/Operator%20Fusion/","title":"Operator Fusion","text":""},{"location":"KB/Operator%20Fusion/#operator-fusion","title":"Operator Fusion","text":"<ul> <li>Some DirectML operators support a concept known as fusion. Operator fusion is a way to improve performance by merging one operator (typically, an activation function) into a different operator so that they are executed together without requiring a roundtrip to memory.</li> <li>https://arxiv.org/abs/2108.13342</li> </ul>"},{"location":"KB/Ophthalmoscope/","title":"Ophthalmoscope","text":""},{"location":"KB/Ophthalmoscope/#ophthalmoscope","title":"Ophthalmoscope","text":"<ul> <li>An instrument used to examine the eye's fundus, retina and other structures</li> </ul>"},{"location":"KB/Opportunistic%20Learning/","title":"Opportunistic Learning","text":""},{"location":"KB/Opportunistic%20Learning/#opportunistic-learning","title":"Opportunistic Learning","text":"<ul> <li>apart from learning from a batch of labelled training data at predefined times or according to a predefined training schedule, the robot must be prepared to accept a new example when it is observed or becomes available.</li> </ul>"},{"location":"KB/Optical%20Encoder/","title":"Optical Encoder","text":""},{"location":"KB/Optical%20Encoder/#optical-encoder","title":"Optical Encoder","text":"<ul> <li>A detection sensor, which measures linear or rotary motion by detecting the movement of markings past a fixed beam of light. This can be used to count revolutions, identify parts, etc.</li> </ul>"},{"location":"KB/Optical%20Proximity%20Sensors/","title":"Optical Proximity Sensors","text":""},{"location":"KB/Optical%20Proximity%20Sensors/#optical-proximity-sensors","title":"Optical Proximity Sensors","text":"<ul> <li>Robot sensors which measure visible or invisible light reflected from an object to determine distance. Lasers are used for greater accuracy.</li> </ul>"},{"location":"KB/Optimizers/","title":"Optimization","text":""},{"location":"KB/Optimizers/#optimization","title":"Optimization","text":"<ul> <li>Gradient Descent gradients</li> <li>Adagrad</li> <li>Rmsprop</li> <li>Adam</li> <li>Learning Rate Decay tricks</li> <li>Early Stopping tricks</li> </ul>"},{"location":"KB/Optimizers/#_1","title":"\u2026","text":""},{"location":"KB/Optimizing%20Code/","title":"Optimizing Work","text":""},{"location":"KB/Optimizing%20Code/#optimizing-work","title":"Optimizing Work","text":"<ul> <li>Vectorization</li> <li>Parallelization</li> <li>Loop Tiling</li> <li>Operator Fusion</li> <li>Block Sparse Kernel</li> </ul>"},{"location":"KB/Optogenetics/","title":"Optogenetics","text":""},{"location":"KB/Optogenetics/#optogenetics","title":"Optogenetics","text":"<ul> <li>An innovative neuroscientific technique that uses light to turn genetically modified neurons on and off at will, in live animals.</li> </ul>"},{"location":"KB/Orbisk/","title":"Orbisk","text":""},{"location":"KB/Orbisk/#orbisk","title":"Orbisk","text":"<ul> <li>hannah@orbisk.com : Hannah Nesmith-Beck</li> </ul> <p>Growing up in Dubai and India, I've been seeing the effects of food waste almost every day. While many of us are taking steps to reduce personal food waste at the individual level, more is needed in industries. After I moved to the Netherlands, I started using the app TooGood2Go, and every time I went to any of the restaurants, they had so many bags of food to give away it was very disheartening. Orbi is a great product, from what information I could find on the website. Growing up, I could do nothing about food waste, but if my programming skills can contribute even a little to the fight, I'm definitely in.</p> <p>My expertise is a combination of data analytics and computer vision, and as of a month ago, I also have a Masters in Artificial Intelligence from the University of Groningen. I am familiar with the tools required for basic and advanced AI from internships, research projects, papers, freelance work, and many personal projects. From what I can gather, Orbi has an image classification algorithm at its heart. But perhaps the bigger problem is the data itself. This is a challenging problem, but tackling it even more efficiently with the proper steps should be possible. While I do have some ideas on improving the efficiency and performance of the system, I would be happy to work on any other task at hand as well. </p> <p>Above everything, having even a tiny positive impact on society is something that I am passionate about. Orbi is an excellent step forward in the fight against food waste, and I hope to get a chance to make it even more worthwhile for industries to adopt Orbi as part of their daily process.</p>"},{"location":"KB/Orbisk/#email","title":"Email","text":"<p>Hello Hannah,</p> <p>This is Subhaditya. I recently graduated with a masters in AI from the University of Groningen and am looking for my next big challenge. I found out about Orbisk while looking for positions that would let me use my skills for a good cause. While I am not sure what positions are open, I thought it would be great to reach out and see if I can, perhaps in some way, be part of the fight against food waste with Orbisk.</p> <p>As a summary - my expertise and interest is a combination of computer vision and data analytics. That being the case, I am looking for a junior role in an AI/ML position. I have attached my resume to this email as well, just in case we can discuss a potential match.</p> <p>A little bit about me: </p> <p>Growing up in Dubai and India, I've been seeing the effects of food waste almost every day. While many of us are taking steps to reduce personal food waste at the individual level, more is needed in industries. After I moved to the Netherlands, I started using the app TooGood2Go, and every time I went to any of the restaurants, they had so many bags of food to give away it was very disheartening. Orbi is a great product, from what information I could find on the website. Growing up, I could do nothing about food waste, but if my programming skills can contribute even a little to the fight, I'm definitely in.</p> <p>My expertise is a combination of data analytics and computer vision. I am familiar with the tools required for basic and advanced AI from internships, research projects, papers, freelance work, and many personal projects. From what I can gather, Orbi has an image classification algorithm at its heart. But perhaps the bigger problem is the data itself. This is a challenging problem, but tackling it even more efficiently with the proper steps should be possible. While I do have some ideas on improving the efficiency and performance of the system, I would be happy to work on any other task at hand as well. </p> <p>Thank you for your time and I hope to hear from you soon :) Best, Subhaditya Mukherjee</p>"},{"location":"KB/Organoid/","title":"Organoid","text":""},{"location":"KB/Organoid/#organoid","title":"Organoid","text":"<ul> <li>A research model that uses pluripotent stem cells (iPSCs) to grow structures made of organ-specific cell types.</li> </ul>"},{"location":"KB/Orthogonal%20Initialization/","title":"Orthogonal Initialization","text":""},{"location":"KB/Orthogonal%20Initialization/#orthogonal-initialization","title":"Orthogonal Initialization","text":"<ul> <li>simple approach to solving the problem Vanishingexploding gradients</li> <li>when applying repeated matrix multiplications, the eigenvalues are what dictate the growth or death of the result</li> <li>eigenvalues of orthogonal matrices have an absolute value of 1</li> <li>no matter how many matrix multiplications the result doesen't explode nor vanishes</li> </ul>"},{"location":"KB/Orthogonal%20Slicing/","title":"Orthogonal Slicing","text":""},{"location":"KB/Orthogonal%20Slicing/#orthogonal-slicing","title":"Orthogonal Slicing","text":"<ul> <li>Interactively resample the data on slices perpendicular to x-,y-,z-axis</li> <li>Use visualization techniques for Isoline, Height Plots</li> </ul>"},{"location":"KB/OrthographicNet/","title":"OrthographicNet","text":""},{"location":"KB/OrthographicNet/#orthographicnet","title":"OrthographicNet","text":"<ul> <li>Orthographic projection is used as a universal language among people in engineering professions:</li> <li>Projection lines are parallel to each other and are perpendicular to the plane, An accurate outline of the visible face of the object is obtained.</li> <li></li> <li></li> </ul>"},{"location":"KB/Ostension/","title":"Ostension","text":""},{"location":"KB/Ostension/#ostension","title":"Ostension","text":"<ul> <li>The notion of ostension in Relevance Theory and in Natural Pedagogy Ostension is a notion of the Relevance Theory of Sperber and Wilson</li> <li>Ostension is the behaviour when the communicator makes manifest his/her intention to make something manifest, i.e., perceptible or inferable, to the listener</li> <li>Ostensive Information</li> <li>Humans try to obtain from every item of information as great a contextual effect as possible for as small a processing effort as possible.</li> <li>Children tend to give more credit to information derived from ostensive communication than to information obtained via direct experience</li> </ul>"},{"location":"KB/Ostensive%20Information/","title":"Ostensive Information","text":""},{"location":"KB/Ostensive%20Information/#ostensive-information","title":"Ostensive Information","text":"<ul> <li>Ostensive communication provides two kinds of information<ul> <li>information changing the listener's cognitive state </li> <li>information communicating that the first layer of information is presented intentionally</li> </ul> </li> </ul>"},{"location":"KB/Otoscope%20or%20Auriscope/","title":"Otoscope or Auriscope","text":""},{"location":"KB/Otoscope%20or%20Auriscope/#otoscope-or-auriscope","title":"Otoscope or Auriscope","text":"<ul> <li>A device for examining the external ear cavity</li> </ul>"},{"location":"KB/Out%20of%20Order%20Execution/","title":"Out of Order Execution","text":""},{"location":"KB/Out%20of%20Order%20Execution/#out-of-order-execution","title":"Out of Order Execution","text":"<ul> <li>allow the processor to avoid a class of delays that occur when the data needed to perform an operation are unavailable</li> <li>Instruction fetch.</li> <li>Instruction dispatch to an instruction queue (also called instruction buffer)</li> <li>The instruction waits in the queue until its input operands are available.</li> <li>The instruction is issued to the appropriate functional unit and executed by that unit.</li> <li>The results are queued (Re-order Buffer).</li> <li>Only after all older instructions have their results written back to the register file, then this result is written back to the register.</li> </ul>"},{"location":"KB/Out-group%20Homogeneity%20Bias/","title":"Out group Homogeneity Bias","text":""},{"location":"KB/Out-group%20Homogeneity%20Bias/#out-group-homogeneity-bias","title":"Out-group Homogeneity Bias","text":"<ul> <li>The tendency to see out-group members as more alike than in-group members when comparing attitudes, values, personality traits, and other characteristics. In-group refers to people you interact with regularly; out-group refers to people you do not interact with regularly. If you create a dataset by asking people to provide attributes about out-groups, those attributes may be less nuanced and more stereotyped than attributes that participants list for people in their in-group.</li> </ul>"},{"location":"KB/Out-of-bag%20Evaluation%20%28OOB%20evaluation%29/","title":"Out of bag Evaluation (OOB evaluation)","text":""},{"location":"KB/Out-of-bag%20Evaluation%20%28OOB%20evaluation%29/#out-of-bag-evaluation-oob-evaluation","title":"Out-of-bag Evaluation (OOB evaluation)","text":"<ul> <li>A mechanism for evaluating the quality of a decision forest by testing each decision tree against the examples</li> <li>not used during training of that decision tree.</li> <li>Out-of-bag evaluation is a computationally efficient and conservative approximation of the cross-validation mechanism</li> </ul>"},{"location":"KB/Overhypotheses/","title":"Overhypotheses","text":""},{"location":"KB/Overhypotheses/#overhypotheses","title":"Overhypotheses","text":"<ul> <li>Parse trees for sentences,</li> <li>These can be explained related to grammar</li> <li>Grammars are structured according to general princples in Universal grammar</li> <li>Do children generalize learned object names as representing shape rather than other features? Yes!</li> <li>First order generalization</li> <li>Second order generalization</li> <li>Suggests that learning of overhypotheses can also be modeled with Bayesian learning</li> </ul>"},{"location":"KB/Oxytocin/","title":"Oxytocin","text":""},{"location":"KB/Oxytocin/#oxytocin","title":"Oxytocin","text":"<ul> <li>Sometimes referred to as the \u201ccuddle chemical,\u201d this hormone can work as a neurotransmitter in the brain and has been linked to social attachment and parental care. While there are \u201clove\u201d sprays on the market that are said to contain oxytocin, there is no evidence that these concoctions have any effect on social relationships.</li> </ul>"},{"location":"KB/PASCAL%20VOC/","title":"PASCAL VOC","text":""},{"location":"KB/PASCAL%20VOC/#pascal-voc","title":"PASCAL VOC","text":""},{"location":"KB/PASCAL-S/","title":"PASCAL-S","text":""},{"location":"KB/PASCAL-S/#pascal-s","title":"PASCAL-S","text":"<ul> <li>\"subset of the validation data in the PASCAL VOC 2010\"</li> <li>\"850 natural images\"</li> <li>The fixations were measured while eight observers looked at an image for 2 s.</li> </ul>"},{"location":"KB/PCA/","title":"PCA","text":""},{"location":"KB/PCA/#pca","title":"PCA","text":"<ul> <li>m dim affine hyperplace spanned by first m eigenvectors. Only manifolds and no codebook vectors</li> <li>Be able to reconstruct x from f(x) : decoding function \\(\\(x \\approx d \\circ f(x)\\)\\)</li> <li></li> </ul>"},{"location":"KB/PCA/#steps","title":"Steps","text":"<ol> <li>Center data (A)<ul> <li>Subtract their mean from each pattern.</li> <li> \\[\\mu = \\frac{1}{N}\\Sigma_{i}x_{i}$$ and getting patterns $$\\hat x_{i}=x_{i}-\\mu\\] </li> <li>Point cloud with center of Gravity : origin<ul> <li>Extend more in some \"directions\" characterized by unit norm direction vectors \\(u \\in \\mathbb{R}^n\\) .</li> <li>Distance of a point from the origin in the direction of u : projection of \\(\\bar x_i\\) on u aka inner product \\(u'\\bar x_i\\)</li> <li>Extension of cloud in direction u : Mean square dist to origin.</li> <li>Largest extension : \\(\\(u_{1}= argmax_{u_{1}, ||u|| = 1} \\frac{1}{N}\\Sigma_{i}(u '\\bar x_i)^2\\)\\)</li> <li>Since centered: mean is 0 and \\(\\frac{1}{N}\\Sigma_{i}(u'\\bar x_i)^2\\) is the variance</li> <li>\\(u_1\\) is the longest direction : First PC : PC1</li> </ul> </li> </ul> </li> <li>Project points (B)<ul> <li>Find orthogonal (90deg) subspace . (n-1) dim linear</li> <li>Map all points \\(\\bar x\\) to \\(\\(\\bar x ^{\\ast}=\\bar x- (u' \\bar x_i^\\ast)^2\\)\\)- Second PC : PC2</li> </ul> </li> <li>Rinse and repeat (C)</li> <li>New PCs plotted in original cloud (D)</li> <li>For featurres \\(f_{k}: \\mathbb{R}^{n}\\rightarrow \\mathbb{R}\\) , \\(x \\rightarrow u'_{k}\\bar x\\)</li> <li>Reconstruction : (\\(x= \\mu + \\Sigma_{k= 1, \u2026,n}f_{k}(x)u_{k}\\)\\)<ul> <li>First few PCs till index m<ul> <li>\\((f_{1}(x), \u2026, f_{m}(x))'\\)</li> <li>Decoding function \\(\\(d: (f_{1}(x), \u2026, f_{m}(x))' \\rightarrow \\mu + \\Sigma_{k= 1}f_{k}(x)u_{k}\\)\\)</li> </ul> </li> </ul> </li> <li>How good is the reconstruction<ul> <li> \\[\\Sigma_{k=m+1}^{n} (\\frac{1}{N}\\Sigma_if_{k}(x_i)^2)\\] </li> <li>Relative amount of dissimilarity to mean empirical variance of patterns - 1<ul> <li> \\[\\frac{\\Sigma_{k=m+1}^{n} (\\frac{1}{N}\\Sigma_if_{k}(x_i)^2)}{\\Sigma_{k=1}^{n} (\\frac{1}{N}\\Sigma_if_{k}(x_i)^2)}\\] </li> <li>Ratio very small as index k grows. Very little info lost by reducing dims. Aka good for very high dim stuff.</li> </ul> </li> </ul> </li> <li>Compute SVD<ul> <li>\\(u_{1,}\u2026u_{n}\\) form orthonormal, real eigenvectors</li> <li>variances \\(\\sigma_{1}^{2,}\u2026, \\sigma_{n}^2\\) are eigenvalues</li> <li>\\(C = U\\Sigma U'\\) to get PC vectors \\(u_k\\) lined up in U and variances \\(\\sigma_k^2\\) as eigenvalues in \\(\\Sigma\\)</li> <li>If we want to preserve 98% variance : Rhs of (1) st. ratio is (1-0.98)</li> </ul> </li> </ol>"},{"location":"KB/PDF/","title":"Probability Density Function","text":""},{"location":"KB/PDF/#probability-density-function","title":"Probability Density Function","text":"<ul> <li>If X is a random variable(RV) that takes values in \\(S \\subseteq \\mathbb{R}^{n}\\). PDF is a fn \\(f:S \\rightarrow \\mathbb{R}^{\\geq0}\\) that satisfies:<ul> <li>For every subvolume \\(A \\subseteq S\\) of S, the prob \\(P(X \\in A)\\) that X gives a value in A is \\(\\(P(X\\in A) = \\int_Af(x)dx\\)\\)</li> </ul> </li> <li>Function which maps from a random process to a quantified version<ul> <li>eg. 1 when heads and 0 when tails</li> </ul> </li> </ul>"},{"location":"KB/PEER/","title":"PEER","text":""},{"location":"KB/PEER/#peer","title":"PEER","text":"<ul> <li>trained on edit histories to cover the entire writing process</li> <li>Plan, Edit, Explain and Repeat</li> <li>These steps are repeated until the text is in a satisfactory state that requires no further updates</li> <li>The model allow to decompose the task of writing a paper into multiple easier subtasks</li> <li>the model allows humans to intervene at any time and steer the model in any direction</li> <li>Wikipedia edit histories</li> <li>The approach is a selftraining, using models to infill missing data and then train other models on this synthetic data</li> <li>The downside of this comes from comments being very noisy and a lack of citations, which tries to be compensated by a retrieval system which does not always work</li> <li>The entire process of formulating a plan, collecting documents, performing an edit and explaining it can be repeated multiple times until arriving at a sequence of text</li> <li>a DeepSpeed transformer is used</li> </ul>"},{"location":"KB/PIUQ/","title":"PIUQ","text":""},{"location":"KB/PIUQ/#piuq","title":"PIUQ","text":"<ul> <li>Problematic Internet Use Questionnaire (PIUQ)</li> <li>a validated self-report scale with good reliability and validity characteristics</li> <li>The questionnaire contains 18 items, each scored on a 5-point Likert-type scale ranging from 1 (never) to 5 (always)</li> </ul>"},{"location":"KB/PMF/","title":"Probability Mass Function","text":""},{"location":"KB/PMF/#probability-mass-function","title":"Probability Mass Function","text":"<ul> <li>pmf</li> <li>Given a discrete sample space S<ul> <li>S is a function \\(p : S \\rightarrow [0,1]\\) whos total mass is 1</li> <li>satisfies \\(\\Sigma_{s \\in S}p(s) = 1\\)</li> </ul> </li> </ul>"},{"location":"KB/PQ/","title":"PQ","text":""},{"location":"KB/PQ/#pq","title":"PQ","text":"<ul> <li>PQ is a vector compression and indexing method that quantizes high-dimensional vectors into compact binary codes</li> <li>enables efficient storage and retrieval of dense vectors while preserving their semantic properties.</li> </ul>"},{"location":"KB/PRelu/","title":"Parametric Relu","text":""},{"location":"KB/PRelu/#parametric-relu","title":"Parametric Relu","text":"<ul> <li> \\[max(\\alpha x,x)\\] </li> <li></li> </ul>"},{"location":"KB/PWC%20Data%20Analyst%20Junior/","title":"PWC Data Analyst Junior","text":"","tags":["jobs"]},{"location":"KB/PWC%20Data%20Analyst%20Junior/#associate-ai-specialist-application-subhaditya-mukherjee","title":"Associate AI Specialist Application - Subhaditya Mukherjee","text":"<p>As the days go by, Machine Learning and AI are slowly becoming terms that every company wants to have in their portfolio. While this drive leads to many innovations, most companies are not sure how to do AI \"well\". They want to use AI but need clarification on whether it is required, how to handle bias, how to create proper data, or even what models (ML vs. DL) to choose. The job of an AI specialist, then, is to provide the key information required to find and fulfil KPIs given any project. This job would be the perfect next step for me, and so I hope you give me a chance to work with you and your team.</p> <p>My interest is at the intersection of applied AI and explainability, and as of a month ago, I also have a Master in Artificial Intelligence from the University of Groningen. My masters thesis was on improving the explainability of vision models, which gave me quite a bit of experience in this domain. I am quite familiar with Python, deep learning frameworks such as PyTorch and Tensorflow and other tricks of the trade. But this would be my first job, and I have a lot to learn as well. I am willing to put time and effort into learning new technologies and enabling more efficient AI integration with future clients.</p> <p>The customer is king, they say, and the first step in having happy clients is understanding what they truly want and then being able to give them a solution they can use. I hope to be part of a team at PWC working on enabling stakeholders to implement responsible AI in their products. </p>","tags":["jobs"]},{"location":"KB/PaLM/","title":"PaLM","text":""},{"location":"KB/PaLM/#palm","title":"PaLM","text":"<ul> <li>PaLM: Scaling Language Modeling with Pathways</li> <li>single 540 billion parameter dense Transformer language model</li> <li>few-shot language understanding and generation</li> <li>drastically reduces the number of task-specific training examples needed to adapt the model to a particular application</li> <li>Pathways Language Model</li> <li>6144 TPU v4 chips</li> <li>breakthrough performance on reasoning tasks, which require multi-step logical inference</li> <li>combination of scale and chain-of-thought prompting, where the model is explicitly prompted to generate a natural language logical inference chain before making its predictio</li> <li>write explicit logical inference chains to both explain jokes and answer complex questions about scenarios</li> <li>Big-Bench</li> <li>suggest that the improvements from scale for few-shot language understanding have not yet plateaued</li> <li>When they compare results from PaLM 540B to our own identically trained 62B and 8B model variants, improvements are typically log-linear.</li> <li>certain capabilities of language models only emerge when trained at sufficient scale, and there are additional capabilities that could emerge from future generations of models</li> <li>demonstrating that prompting the model to generate explicit inference chains can drastically increase the quality of the predictions themselves</li> <li>model\u2019s generation (rather than just understanding) capabilities can be immensely beneficial even for tasks that are modeled as categorical prediction or regression, which typically do not require significant language generation</li> <li>comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale</li> <li>ethical considerations related to large language models and discuss potential mitigation strategies</li> </ul>"},{"location":"KB/Padded%20Conv/","title":"Padded Conv","text":""},{"location":"KB/Padded%20Conv/#padded-conv","title":"Padded Conv","text":"<ul> <li> \\[(N_i, N_o, C, F)\\] </li> <li>Filters transform from C -&gt; F channels</li> <li>Mirror, Reflect</li> </ul>"},{"location":"KB/Palletizing/","title":"Palletizing","text":""},{"location":"KB/Palletizing/#palletizing","title":"Palletizing","text":"<ul> <li>The process of stacking packages (i.e., boxes, bags, containers, etc.) in an organized fashion on a pallet.</li> </ul>"},{"location":"KB/Parallel%20Coordinate%20Plots/","title":"Parallel Coordinate Plots","text":""},{"location":"KB/Parallel%20Coordinate%20Plots/#parallel-coordinate-plots","title":"Parallel Coordinate Plots","text":"<ul> <li>Enhancement<ul> <li>Permute axes (horizontally) to and swap their direction (vertically) minimize crossings</li> <li>Add histograms on axes to show lines per unit data value</li> <li>Visually group/cluster polylines histograms</li> </ul> </li> </ul>"},{"location":"KB/Parallel%20Granularity/","title":"Parallel Granularity","text":""},{"location":"KB/Parallel%20Granularity/#parallel-granularity","title":"Parallel Granularity","text":"<ul> <li>Ration of computation to communication</li> <li>Coarse : Large computation between communication</li> <li>Fine : Small computation between communication</li> </ul>"},{"location":"KB/Parallel%20Processing/","title":"Parallel Processing","text":""},{"location":"KB/Parallel%20Processing/#parallel-processing","title":"Parallel Processing","text":"<ul> <li>Load balancing</li> <li>Minimizing Communication</li> <li>Overlap Communication</li> </ul>"},{"location":"KB/Parallel%20Runner/","title":"Parallel Runner","text":""},{"location":"KB/Parallel%20Runner/#parallel-runner","title":"Parallel Runner","text":"<pre><code>import random\nimport numpy as np\nimport concurrent\nfrom typing import *\nfrom concurrent.futures import ProcessPoolExecutor\nfrom types import SimpleNamespace\nimport os\nfrom pathlib import Path\n\ndef ifnone(a, b):\n    \"\"\"\n    Return if None\n    \"\"\"\n    return b if a is None else a\n\n\ndef listify(o):\n    \"\"\"\n    Convert to list\n    \"\"\"\n    if o is None:\n        return []\n    if isinstance(o, list):\n        return o\n    if isinstance(o, str):\n        return [o]\n    if isinstance(o, Iterable):\n        return list(o)\n    return [o]\n\n\ndef num_cpus() -&gt; int:\n    \"Get number of cpus\"\n    try:\n        return len(os.sched_getaffinity(0))\n    except AttributeError:\n        return os.cpu_count()\n\n\n_default_cpus = min(16, num_cpus())\ndefaults = SimpleNamespace(\n    cpus=_default_cpus, cmap=\"viridis\", return_fig=False, silent=False\n)\n\n\ndef parallel(func, arr: Collection, max_workers: int = None, leave=False):  # %t\n    \"Call `func` on every element of `arr` in parallel using `max_workers`.\"\n    max_workers = ifnone(max_workers, defaults.cpus)\n    if max_workers &lt; 2:\n        results = [\n            func(o, i)\n            for i, o in tqdm.tqdm(enumerate(arr), total=len(arr), leave=leave)\n        ]\n    else:\n        with ProcessPoolExecutor(max_workers=max_workers) as ex:\n            futures = [ex.submit(func, o, i) for i, o in enumerate(arr)]\n            results = []\n            for f in tqdm.tqdm(\n                concurrent.futures.as_completed(futures), total=len(arr), leave=leave\n            ):\n                results.append(f.result())\n    if any([o is not None for o in results]):\n        return results\n\n</code></pre>"},{"location":"KB/Parallel%20Shift%20Function/","title":"Parallel Shift Function","text":""},{"location":"KB/Parallel%20Shift%20Function/#parallel-shift-function","title":"Parallel Shift Function","text":"<ul> <li>Parallel Shift refers to the shifting of an object from a fixed position in such a way that all points within the object move an equal distance.</li> </ul>"},{"location":"KB/Parallelization/","title":"Parallel","text":""},{"location":"KB/Parallelization/#parallel","title":"Parallel","text":"<ul> <li>Independant work chunks -&gt; operate simultaneously</li> </ul>"},{"location":"KB/Parasympathetic/","title":"Parasympathetic","text":""},{"location":"KB/Parasympathetic/#parasympathetic","title":"Parasympathetic","text":"<ul> <li>Relaxes the body</li> </ul>"},{"location":"KB/Parent%20Approximations/","title":"Parent Approximations","text":""},{"location":"KB/Parent%20Approximations/#parent-approximations","title":"Parent Approximations","text":"<ul> <li>It learns a compact two-level decision set in which each rule explains parts of the model behavior unambiguously and is a combined objective function to optimize these aspects: high agreement between the explanation and the model; little overlap between the decision rules in the explanation; the explanation decision set is lightweight and small.</li> </ul>"},{"location":"KB/Parietal%20lobe/","title":"Parietal lobe","text":""},{"location":"KB/Parietal%20lobe/#parietal-lobe","title":"Parietal Lobe","text":"<ul> <li>Interprets language, words</li> <li>Sense of touch, pain, temperature (sensory strip)</li> <li>Interprets signals from vision, hearing, motor, sensory and memory</li> <li>Spatial and visual perception</li> </ul>"},{"location":"KB/Parietal%20lobe/#parietal-lobe_1","title":"Parietal Lobe","text":"<ul> <li>The area of the brain\u2019s cerebrum located just behind the central sulcus. It is concerned primarily with the reception and processing of sensory information from the body and is also involved in map interpretation and spatial orientation (recognizing one\u2019s position in space in relation to other objects or places).</li> </ul>"},{"location":"KB/Parkinson%E2%80%99s%20Disease/","title":"Parkinson\u2019s Disease","text":""},{"location":"KB/Parkinson%E2%80%99s%20Disease/#parkinsons-disease","title":"Parkinson\u2019s Disease","text":"<ul> <li>A neurodegenerative disorder characterized by tremor, slowed movement, and speech changes due to the death of dopamine</li> <li>neurons located in the substantia nigra.</li> </ul>"},{"location":"KB/Partial%20Dependence%20Plot/","title":"Partial Dependence Plot","text":""},{"location":"KB/Partial%20Dependence%20Plot/#partial-dependence-plot","title":"Partial Dependence Plot","text":"<ul> <li>Another approach [29] shows the marginal effect of one or two features on the prediction of learning techniques using Partial Dependence Plots</li> <li>The method gives a statement about the global relationship of a feature and whether its relation to the outcome is linear, monotonic, or more complex.</li> <li>The PDP is the average of Individual Conditional Expectation (ICE) over all features</li> <li>ICE [30] points to how the prediction changes if a feature changes.</li> <li>The PDP is limited to two features.</li> </ul>"},{"location":"KB/Participation%20Bias/","title":"Participation Bias","text":""},{"location":"KB/Participation%20Bias/#participation-bias","title":"Participation Bias","text":"<ul> <li>Synonym for non-response bias</li> </ul>"},{"location":"KB/Particle%20Filter/","title":"Particle Filter","text":""},{"location":"KB/Particle%20Filter/#particle-filter","title":"Particle Filter","text":"<ul> <li>Particle filter algorithm works for any arbitrary distribution and not just Gaussian. Particle filter is computationally more expensive than Kalman filter</li> <li>non linear systems.</li> </ul>"},{"location":"KB/Particle%20Visualization/","title":"Particle Visualization","text":""},{"location":"KB/Particle%20Visualization/#particle-visualization","title":"Particle Visualization","text":"<ul> <li>points with only a (3D) coordinate</li> <li>potentially enriched with attributes like radius, velocity, etc.</li> <li>scattered data Data Structures</li> <li>lack of topological information (neighborhood)</li> <li>typically many particles (e.g. atoms, galaxies)</li> <li>Atomistic visualization</li> <li>via glyphs (i.e. spheres)</li> <li>explicit (geometry) or implicit Raycasting</li> <li>Surface-based visualization</li> <li>extract surface geometry</li> <li>particles: point samples describing a surface (e.g. in fluids or molecules)</li> <li>Volume-based visualization</li> <li>particles: point samples of a volume</li> <li>Raycasting and hierarchical Data Structures</li> </ul>"},{"location":"KB/Parts%20of%20action/","title":"Parts of action","text":""},{"location":"KB/Parts%20of%20action/#parts-of-action","title":"Parts of Action","text":"<ul> <li>Lynn went on a business trip to New York.</li> <li>She left on an early morning flight.</li> <li>Taking a flight should be recognized as part of going on a trip.</li> </ul>"},{"location":"KB/Parts%20of%20entities/","title":"Parts of entities","text":""},{"location":"KB/Parts%20of%20entities/#parts-of-entities","title":"Parts of Entities","text":"<ul> <li>Tracy opened the book she just bought.</li> <li>The toc: true title page was torn.</li> <li>The phrase \u2018the toc: true title page\u2019 should be recognized as being part of the book tat was just bought.</li> </ul>"},{"location":"KB/PatchGAN/","title":"PatchGAN","text":""},{"location":"KB/PatchGAN/#patchgan","title":"PatchGAN","text":"<ul> <li>Type of discriminator</li> <li>only penalizes structure at the scale of local image patches</li> <li>tries to classify if each \\(N \\times N\\) patch in an image is real or fake</li> <li>discriminator is run convolutionally across the image, averaging all responses to provide the ultimate output of \\(D\\)</li> <li>effectively models the image as a Markov random field</li> <li>assuming independence between pixels separated by more than a patch diameter</li> <li>type of texture/style loss</li> <li>rather the regular GAN maps from a 256\u00d7256 image to a single scalar output, which signifies \u201creal\u201d or \u201cfake\u201d, whereas the PatchGAN maps from 256\u00d7256 to an NxN (here 70\u00d770) array of outputs X, where each \\(X_{ij}\\) signifies whether the patch ij in the image is real or fake.</li> <li></li> </ul>"},{"location":"KB/Pathlines/","title":"Pathlines","text":""},{"location":"KB/Pathlines/#pathlines","title":"Pathlines","text":""},{"location":"KB/Pearson%20Correlation/","title":"Pearson Correlation","text":"<p>toc: true title: Pearson Correlation</p> <p>categories: ['temp']</p>"},{"location":"KB/Pearson%20Correlation/#pearson-correlation","title":"Pearson Correlation","text":"<ul> <li>One type of Correlation</li> <li> \\[r = \\frac{n(\\Sigma xy) - (\\Sigma x)(\\Sigma y)}{\\sqrt{[n\\Sigma x^{2} - (\\Sigma x)^{2}][n\\Sigma y^{2} - (\\Sigma y)^{2}]}}\\] </li> </ul>"},{"location":"KB/Pendant%20Teaching/","title":"Pendant Teaching","text":""},{"location":"KB/Pendant%20Teaching/#pendant-teaching","title":"Pendant Teaching","text":"<ul> <li>The mapping and recording of the position and orientation of a robot and/or manipulator system as the robot is manually moved in increments from an initial state along a path to a final goal state. The position and orientation of each critical point (joints, robot base, etc.) is recorded and stored in a database for each taught position the robot passes through on its path toward its final goal. The robot may now repeat the path on its own by following the path stored in the database.</li> </ul>"},{"location":"KB/People%20Art%20Dataset/","title":"People Art Dataset","text":""},{"location":"KB/People%20Art%20Dataset/#people-art-dataset","title":"People Art Dataset","text":""},{"location":"KB/Perception%20Component/","title":"Perception Component","text":""},{"location":"KB/Perception%20Component/#perception-component","title":"Perception Component","text":"<ul> <li>processes all momentary information coming from sensors.</li> </ul>"},{"location":"KB/Perception/","title":"Perception","text":""},{"location":"KB/Perception/#perception","title":"Perception","text":"<ul> <li>Perception \u2014 process by which we interpret the things around us through sensory stimuli</li> <li>Cognition \u2014 mental processes assisting us to remember, think, know, judge, solve problems, etc.</li> <li>Preattentive Processing</li> <li>Gestalt Laws</li> <li>Postattentive Amnesia</li> <li>Change Blindness</li> <li>Inattentional Blindness</li> </ul>"},{"location":"KB/Perceptron/","title":"Perceptron","text":""},{"location":"KB/Perceptron/#perceptron","title":"Perceptron","text":"<ul> <li> \\[f(x)=sign(\\Sigma _i w_ix_i +b) = sign(\\mathbf{w^Tx}+b)\\] <ul> <li>\\(\\(sign(x) = \\begin{cases} 1 &amp; x\\geq0 \\\\ 0 &amp; otherwise\\end{cases}\\)\\) </li> </ul> </li> <li>computational graph</li> <li>Multi layer<ul> <li>Stack multiple perceptrons</li> <li> \\[\\begin{align} \\\\&amp; h_0 = x h1= sign(\\mathbf{w_1^T}+b_1) \\\\ &amp;\u2026\\\\&amp; h1= sign(\\mathbf{w_{L-1}^T}+b_L) \\end{align}\\] </li> </ul> </li> </ul>"},{"location":"KB/Perceptual%20Messages/","title":"Perceptual Messages","text":""},{"location":"KB/Perceptual%20Messages/#perceptual-messages","title":"Perceptual Messages","text":"<ul> <li>large (e.g., point cloud)</li> <li>data flows continuously at the sensor output frequency (30Hz).</li> </ul>"},{"location":"KB/Perceptually%20Uniform/","title":"Perceptually Uniform","text":""},{"location":"KB/Perceptually%20Uniform/#perceptually-uniform","title":"Perceptually Uniform","text":"<ul> <li>Euclidean distance corresponds to perceptual difference</li> <li>e.g., CIELUV,CIELAB, (L,a,b*).</li> </ul>"},{"location":"KB/Peripheral%20Nervous%20System/","title":"Peripheral Nervous System","text":""},{"location":"KB/Peripheral%20Nervous%20System/#peripheral-nervous-system","title":"Peripheral Nervous System","text":"<ul> <li>All the nerves that branch off from the brain</li> <li>Both directions</li> <li>Allow Central Nervous System to communicate with the body</li> <li>Afferent + Efferent</li> </ul>"},{"location":"KB/Perplexity/","title":"Perplexity","text":""},{"location":"KB/Perplexity/#perplexity","title":"Perplexity","text":"<ul> <li>Perplexity is defined as the exponentiated average negative log-likelihood of a sequence.</li> <li>If we have a tokenized sequence \\(X = (x_0, x_1, \\dots, x_t)\\), then the perplexity of \\(X\\) is, \\(\\(\\text{PPL}(X) = \\exp \\left\\{ {-\\frac{1}{t}\\sum_i^t \\log p_\\theta (x_i|x_{&lt;i}) } \\right\\}\\)\\)where \\(\\log p_\\theta (x_i|x_{&lt;i})\\) is the log-likelihood of the ith token conditioned on the preceding tokens \\(x_{&lt;i}\\) according to our model.</li> <li>Intuitively, it can be thought of as an evaluation of the model's ability to predict uniformly among the set of specified tokens in a corpus.</li> <li>Importantly, this means that the tokenization procedure has a direct impact on a model's perplexity which should always be taken into consideration when comparing different models.</li> <li>This is also equivalent to the exponentiation of the Cross Entropy between the data and model predictions</li> </ul>"},{"location":"KB/PhD%20vs%20Startup%20vs%20Big%20Company/","title":"PhD vs Startup vs Big Company","text":"","tags":["mlops"]},{"location":"KB/PhD%20vs%20Startup%20vs%20Big%20Company/#phd-vs-startup-vs-big-company","title":"PhD Vs Startup Vs Big Company","text":"<ul> <li>https://huyenchip.com/2018/10/08/career-advice-recent-cs-graduates.html</li> </ul>","tags":["mlops"]},{"location":"KB/PhD%20vs%20Startup%20vs%20Big%20Company/#phd-vs-no-phd","title":"PhD Vs no PhD","text":"<ul> <li> <p>Arguments supporting PhD include:</p> <ul> <li>You\u2019ll have time to immerse yourself in research.</li> <li>If you want to become a professor, you have to do a PhD.</li> <li>Many top research labs such as DeepMind only interview PhD candidates.</li> <li>You won\u2019t be too poor as AI internships pay well.</li> </ul> </li> <li> <p>Arguments supporting no-PhD include:</p> <ul> <li>There should be more people joining industry to bring research into production.</li> <li>By the time you finish your PhD, what you learn might no longer be relevant.</li> <li>Many professors have side gigs in the industry anyway so you can still work with them.</li> <li>You won\u2019t be poor for the next five years.</li> </ul> </li> <li> <p>Pursuing your passion, it turns out, is not legal in the US when you\u2019re an international student.</p> </li> </ul>","tags":["mlops"]},{"location":"KB/PhD%20vs%20Startup%20vs%20Big%20Company/#big-companies-vs-startups","title":"Big Companies Vs Startups","text":"","tags":["mlops"]},{"location":"KB/Pharmacotherapy/","title":"Pharmacotherapy","text":""},{"location":"KB/Pharmacotherapy/#pharmacotherapy","title":"Pharmacotherapy","text":"<ul> <li>The use of pharmaceutical drugs for therapeutic purposes.</li> </ul>"},{"location":"KB/Phases%20of%20Simulated%20User%20Experiments/","title":"Phases of Simulated User Experiments","text":""},{"location":"KB/Phases%20of%20Simulated%20User%20Experiments/#phases-of-simulated-user-experiments","title":"Phases of Simulated User Experiments","text":"<ul> <li>Evolution<ul> <li>The classification performance should be improved as the number of examples per category increases while NO new categories are introduced.</li> </ul> </li> <li>Recovery<ul> <li>By increasing the number of categories, it is expected that the prediction accuracy decreases. The time spent in system evolution until correcting and adjusting all current categories defines recovery.</li> </ul> </li> <li>Breakpoint<ul> <li>Eventually the learning agent reaches to a breakpoint where the agent is no longer able to learn more categories.</li> </ul> </li> </ul>"},{"location":"KB/Phenaki/","title":"Phenaki","text":""},{"location":"KB/Phenaki/#phenaki","title":"Phenaki","text":"<ul> <li>capable of performing realistic video synthesis, given a sequence of textual prompts</li> <li>Phenaki is the first model that can generate videos from open domain time variable prompts</li> <li>To address data issues, it performs joint training on a large image-text pairs dataset as well as a smaller number of video-text examples can result in generalization beyond what is available in the video datasets.</li> <li>image-text datasets having billions of inputs</li> <li>limitations come from computational capabilities for videos of variable length</li> <li>the C-ViViT encoder, the training transformer and the video generator</li> <li>The encoder gets a compressed representation of videos.</li> <li>First tokens are transformed into embeddings.</li> <li>This is followed by the temporal transformer, then the spatial transformer</li> <li>After the output of the spatial transformer, they apply a single linear projection without activation to map the tokens back to pixel space</li> <li>Consequently, the model generates temporally coherent and diverse videos conditioned on open domain prompts even when the prompt is a new composition of concepts</li> </ul>"},{"location":"KB/Phenotype/","title":"Phenotype","text":""},{"location":"KB/Phenotype/#phenotype","title":"Phenotype","text":"<ul> <li>A set of traits or characteristics resulting from the interaction of one\u2019s genes with the environment.</li> </ul>"},{"location":"KB/Phonetics/","title":"Phonetics","text":""},{"location":"KB/Phonetics/#phonetics","title":"Phonetics","text":"<ul> <li>deals with the physical building blocks of a language sound system.</li> <li>eg. sounds of \u2018k\u2019, \u2018t\u2019 and \u2018e\u2019 in \u2018kite</li> </ul>"},{"location":"KB/Phong%20Lighting/","title":"Phong Lighting","text":""},{"location":"KB/Phong%20Lighting/#phong-lighting","title":"Phong Lighting","text":""},{"location":"KB/Phonology/","title":"Phonology","text":""},{"location":"KB/Phonology/#phonology","title":"Phonology","text":"<ul> <li>organisation of speech sounds within a language.</li> <li>eg. (1) different \u2018k\u2019 sounds in \u2018kite\u2019 vs \u2018coat\u2019</li> <li>(2) different \u2018t\u2019 and \u2018p\u2019 sounds in \u2018top\u2019 vs \u2018pot\u2019</li> </ul>"},{"location":"KB/Phrase%20Representation%20Learning/","title":"Phrase Representation Learning","text":""},{"location":"KB/Phrase%20Representation%20Learning/#phrase-representation-learning","title":"Phrase Representation Learning","text":"<ul> <li>Learning Phrase Representations Using RNN Encoder\u2013Decoder for Statistical Machine Translation</li> <li>two recurrent neural networks Basic RNN Architectures that is together able to learn the mapping from a sequence of an arbitrary length to another sequence, possibly from a different set, of an arbitrary length.</li> <li>either score a pair of sequences (in terms of a conditional probability) or generate a target sequence given a source sequence</li> <li>jointly trained to maximize the conditional probability of a target sequence given a source sequence</li> <li>reset gate and an update gate that adaptively control how much each hidden unit remembers or forgets while reading/generating a sequenc</li> <li>RNN Encoder\u2013Decoder to score each phrase pair in the phrase table</li> <li>capture linguistic regularities in the phrase pairs well</li> <li>BLEU</li> </ul>"},{"location":"KB/Picasso%20Dataset/","title":"Picasso Dataset","text":""},{"location":"KB/Picasso%20Dataset/#picasso-dataset","title":"Picasso Dataset","text":""},{"location":"KB/Pick%20and%20Place%20Cycle/","title":"Pick and Place Cycle","text":""},{"location":"KB/Pick%20and%20Place%20Cycle/#pick-and-place-cycle","title":"Pick and Place Cycle","text":"<ul> <li>The amount of time it takes for a manipulator to pick up an object and place it in a desired location, then return to its rest position. This includes time during the acceleration and deceleration phases of a particular task. The robot movement is controlled from one point location in space to another in a Point-to-Point (PTP) motion system. Each point is programmed into the robot's control memory and then played back during the work cycle.</li> </ul>"},{"location":"KB/Picky%20Puppet%20Method/","title":"Picky Puppet Method","text":""},{"location":"KB/Picky%20Puppet%20Method/#picky-puppet-method","title":"Picky Puppet Method","text":"<ul> <li>For children</li> <li>Ask if a \"puppet\" would like it</li> </ul>"},{"location":"KB/Pinch%20Points/","title":"Pinch Points","text":""},{"location":"KB/Pinch%20Points/#pinch-points","title":"Pinch Points","text":"<ul> <li>A pinch point is any point at which it is possible for a person or part of a person\u2019s body to be caught between moving parts of a machine, or between the moving and stationary parts of a machine, or between material and any part of the machine. A pinch point does not have to cause injury to a limb or body part, although it might cause injury \u2013 it only has to trap or pinch the person to prevent them from escaping or removing the trapped part from the pinch point.</li> </ul>"},{"location":"KB/Pineal%20gland/","title":"Pineal gland","text":""},{"location":"KB/Pineal%20gland/#pineal-gland","title":"Pineal Gland","text":"<ul> <li>It is located behind the third ventricle.</li> <li>It helps regulate the body\u2019s internal clock and circadian rhythms by secreting melatonin.</li> <li>It has some role in sexual development.</li> </ul>"},{"location":"KB/Pipes/","title":"Pipes","text":""},{"location":"KB/Pipes/#pipes","title":"Pipes","text":"<ul> <li>Allows vector operation to be performed in parallel on multiple elements of the vector</li> </ul>"},{"location":"KB/Pituitary%20gland/","title":"Pituitary gland","text":""},{"location":"KB/Pituitary%20gland/#pituitary-gland","title":"Pituitary Gland","text":"<ul> <li>lies in a small pocket of bone at the skull base called the sella turcica.</li> <li>The pituitary gland is connected to the hypothalamus of the brain by the pituitary stalk.</li> <li>Known as the \u201cmaster gland,\u201d it controls other endocrine glands in the body.</li> <li>It secretes hormones that control sexual development, promote bone and muscle growth, and respond to stress.</li> </ul>"},{"location":"KB/Pituitary%20gland/#pituitary-gland_1","title":"Pituitary Gland","text":"<ul> <li>An endocrine organ at the base of the brain that is closely linked with the KB/Hypothalamus.md. The pituitary gland is composed of two lobes, the anterior and posterior lobes, and secretes hormones that regulate the activity of the other endocrine organs in the body.</li> </ul>"},{"location":"KB/Pix2Seq/","title":"Pix2Seq","text":""},{"location":"KB/Pix2Seq/#pix2seq","title":"Pix2Seq","text":"<ul> <li>Pix2seq: a Language Modeling Framework for Object Detection</li> <li>generic framework for object detection</li> <li>object detection as a language modeling task conditioned on the observed pixel inputs</li> <li>Object descriptions (e.g., bounding boxes and class labels) are expressed as sequences of discrete tokens, and we train a neural network to perceive the image and generate the desired sequence</li> <li>COCO</li> <li>output can be represented by a relatively concise sequence of discrete tokens (e.g., keypoint detection, image captioning, visual question answering)</li> <li>autoregressive</li> <li>stop inference when the ending token is produced</li> <li>applying it to offline inference, or online scenarios where the objects of interest are relatively sparse</li> <li>entirely based on human annotation</li> </ul>"},{"location":"KB/Places/","title":"Places","text":""},{"location":"KB/Places/#places","title":"Places","text":"<ul> <li>The Places dataset [107] is proposed for scene recognition and contains more than 2.5 million images covering more than 205 scene categories with more than 5, 000 images per category.</li> </ul>"},{"location":"KB/Places365/","title":"Places365","text":""},{"location":"KB/Places365/#places365","title":"Places365","text":"<ul> <li>2nd generation of the Places databas </li> <li>high-level visual understanding tasks, such as scene context, object recognition, action and event prediction, and theoryof-mind inference </li> <li>more than 10 million images covering more than 400 classes and 5, 000 to 30, 000 training images per class.</li> </ul>"},{"location":"KB/PlantCLEF/","title":"PlantCLEF","text":""},{"location":"KB/PlantCLEF/#plantclef","title":"PlantCLEF","text":"<ul> <li>PlantCLEF dataset is a collection of images of plants, with a total of around 57,000 images and over 200 different plant species.</li> </ul>"},{"location":"KB/Pluripotency/","title":"Pluripotency","text":""},{"location":"KB/Pluripotency/#pluripotency","title":"Pluripotency","text":"<ul> <li>The quality of certain undifferentiated cells that allows them to develop into one of many different cell types.</li> </ul>"},{"location":"KB/Point%20Cloud/","title":"Point Cloud Data","text":""},{"location":"KB/Point%20Cloud/#point-cloud-data","title":"Point Cloud Data","text":"<ul> <li>PointNet++</li> </ul>"},{"location":"KB/Point%20Distribution/","title":"Point Distribution","text":""},{"location":"KB/Point%20Distribution/#point-distribution","title":"Point Distribution","text":"<ul> <li>PDF is impossible to use</li> <li>Probability mass is concentrated in a few points</li> <li>Dirac Delta</li> <li>Hyperdistributions</li> </ul>"},{"location":"KB/Point-to-Point/","title":"Point to Point","text":""},{"location":"KB/Point-to-Point/#point-to-point","title":"Point-to-Point","text":"<ul> <li>Manipulator motion in which a limited number of points along a projected path of motion is specified. The manipulator moves from point to point rather than a continuous smooth path.</li> </ul>"},{"location":"KB/PointNet%2B%2B/","title":"PointNet++","text":""},{"location":"KB/PointNet%2B%2B/#pointnet","title":"PointNet++","text":""},{"location":"KB/PointNet%2B%2B/#todo","title":"todo","text":""},{"location":"KB/Poisson%20Distribution/","title":"Possion Distribution","text":""},{"location":"KB/Poisson%20Distribution/#possion-distribution","title":"Possion Distribution","text":"<ul> <li>Probability that an event occurs k times within a given time interval</li> <li>Eg:</li> <li>k meteors within 100 years</li> <li>k calls in an hour</li> <li>Also can count spatially circumscribed events</li> <li>no of dust particles in a mm of air</li> <li>no of diamons in ton of ore</li> <li>Expected no of events \\(E[X]\\) : rate \\(\\lambda\\)</li> <li>PMF : \\(\\(p(k) = \\frac{\\lambda^{k}e^{-k}}{k!}\\)\\)</li> <li></li> <li>Eg:</li> <li>N 1-hour protocols for calls : \\(n_{i} (i = 1, \u2026, N)\\)</li> <li> \\[\\hat\\lambda =\\frac{1}{N}\\Sigma_{i}n_{i}\\] </li> </ul>"},{"location":"KB/Poisson%20Loss/","title":"Poisson Loss","text":""},{"location":"KB/Poisson%20Loss/#poisson-loss","title":"Poisson Loss","text":"<ul> <li>When data is from Poisson Distribution</li> </ul> \\[\\frac{1}{\\mathrm{length}\\left( y \\right)} \\cdot \\mathrm{sum}\\left( \u0177 - \\log\\left( \u0177 \\right) \\right)\\]"},{"location":"KB/Poisson%20Process/","title":"Poisson Process","text":""},{"location":"KB/Poisson%20Process/#poisson-process","title":"Poisson Process","text":"<ul> <li>Waiting times between two consecutive spikes are Exponential Distribution</li> </ul>"},{"location":"KB/Poki%20Data%20Scientist/","title":"Poki Data Scientist","text":""},{"location":"KB/Poki%20Data%20Scientist/#poki-data-scientist","title":"Poki Data Scientist","text":""},{"location":"KB/Poki%20Data%20Scientist/#why-do-you-want-to-work-at-poki","title":"Why Do You want to Work at Poki?","text":"<ul> <li>grown up playing games on websites such as miniclip</li> <li>data science experience : interesting challenge</li> <li>sounds like a fun place to work</li> <li>perfect next step in my career, learn and grow with Poki and be a part of people enjoying games on the web </li> </ul> <p>Growing up, I used to spend countless hours playing games online on websites such as Miniclip (does anyone remember Club Penguin? :) ). Being a data scientist with a master in AI, finding ways to ensure players have a nice experience using Poki while also being able to make sure they find the games they want sounds like a great challenge. Poki sounds like a fun place to work, and this would be the perfect next step in my career. </p> <p>It is not every day that you find a position that you think you might enjoy and also know that you can contribute to upon being given a chance, and I did not want to let this opportunity slide.</p>"},{"location":"KB/Poki%20Data%20Scientist/#can-you-describe-a-time-where-you-identified-an-area-of-improvement-and-the-way-you-followed-up-on-that","title":"Can You Describe a time where You Identified an Area of Improvement and the way You Followed up on That?","text":"<ul> <li>In my work at Emirates NBD (one of the national banks of Dubai), we were building an analytics pipeline to identify fraud transactions. While the pipeline worked, they were using a Microsoft service to do the analytics and the performance was not great.</li> <li>In my time there I helped them build an analytics pipeline from scratch using Python + SQL. I contributed quite a bit to the feature engineering steps that would later be part of the final analytics program used across Emirates NBD.</li> </ul> <p>In my work at Emirates NBD (one of the largest banks in Dubai), the team was attempting to improve an analytics pipeline to identify fraudulent transactions. While the pipeline worked reasonably well, they were using a Microsoft service to do the analytics, and the performance was not great. In my time there, I helped them build an analytics pipeline from scratch using Python + SQL. I contributed quite a bit to the feature engineering steps that would later be part of the final analytics program used across Emirates NBD. Many of those changes led to improving the final accuracy by a decent amount and made it much easier for my coworkers to push the product to the upper management.</p>"},{"location":"KB/Poki%20Data%20Scientist/#when-you-look-at-the-presentation-of-content-on-our-platform-what-question-comes-to-mind","title":"When You Look at the Presentation of Content on Our Platform, what Question Comes to Mind?","text":"<ul> <li>Clustered, icons are similar</li> <li>Computer vision + analytics: Check for something like that. </li> <li>Given an example</li> </ul> <p>While Poki looks great, one thing that immediately comes to mind is that many of the icons are rather visually similar (eg : Apple Knight, Apple Knight mini dungeons ; Mekabolt, Day of Meat). There are also a lot of icons immediately, which is slightly overwhelming.  This is unintentional, of course, but might make it harder for a user to pick what they want. Using a computer vision similarity technique could be useful in identifying them and pushing them somewhere else on the grid. This would be something that I would love to discuss as well.</p>"},{"location":"KB/Polynomial%20Trajectories/","title":"Polynomial Trajectories","text":""},{"location":"KB/Polynomial%20Trajectories/#polynomial-trajectories","title":"Polynomial Trajectories","text":""},{"location":"KB/Polysynthetic%20words/","title":"Polysynthetic words","text":""},{"location":"KB/Polysynthetic%20words/#polysynthetic-words","title":"Polysynthetic Words","text":"<ul> <li>complex words that function as a sentence (Chukchi and Inuktitut)</li> </ul>"},{"location":"KB/Pooling/","title":"Pooling","text":""},{"location":"KB/Pooling/#pooling","title":"Pooling","text":"<ul> <li>Summarize low level features</li> <li>Reduce input dims</li> <li>Max/Avg</li> <li>Too much pooling reduces performance<ul> <li>Multiple convs first</li> </ul> </li> <li>Max pool + dilated/Strided convs control effective receptive field size</li> </ul>"},{"location":"KB/Population%20Based%20Augmentation/","title":"Population Based Augmentation","text":""},{"location":"KB/Population%20Based%20Augmentation/#population-based-augmentation","title":"Population Based Augmentation","text":"<ul> <li>PBA can match the performance of AutoAugment on multiple datasets with less computation time</li> </ul>"},{"location":"KB/Population%20Correlation/","title":"Population Correlation","text":"<p>toc: true title: Population Correlation</p> <p>categories: ['temp']</p>"},{"location":"KB/Population%20Correlation/#population-correlation","title":"Population Correlation","text":"<ul> <li> \\[\\rho_{xy}= \\frac{\\sigma_{xy}}{\\sigma_{x}\\sigma_{y}}\\] </li> <li>\\(\\sigma\\) is the Standard Deviation</li> <li>\\(\\sigma_{xy}\\) is the Covariance</li> </ul>"},{"location":"KB/Position%20Encoding/","title":"Position Encoding","text":""},{"location":"KB/Position%20Encoding/#position-encoding","title":"Position Encoding","text":"<ul> <li>Transformers are feed forward. So need a way to inject position into seq</li> <li> \\[PE(pos, 2i) = sin(\\frac{pos}{10000^\\frac{2i}{d_{model}}})\\] </li> <li> \\[PE(pos, 2i+1) = cos(\\frac{pos}{10000^\\frac{2i}{d_{model}}})\\] </li> <li>Conceptually, adding word order to a sentence<ul> <li>Something like (\"Hello\", 1) , (\"from\",2) , (\"me\", 3)</li> </ul> </li> </ul>"},{"location":"KB/Position%20Wise%20Feed%20Forward/","title":"Position Wise Feed Forward","text":""},{"location":"KB/Position%20Wise%20Feed%20Forward/#position-wise-feed-forward","title":"Position Wise Feed Forward","text":"<ul> <li> \\[FFN(x) = max(0, xW_{1}+b_{1})W_{2}+b_{2}\\] </li> <li>Dense Layers are applied along the last (512) dims</li> </ul>"},{"location":"KB/Positron%20Emission%20Tomography%20%28PET%29/","title":"Positron Emission Tomography (PET)","text":""},{"location":"KB/Positron%20Emission%20Tomography%20%28PET%29/#positron-emission-tomography-pet","title":"Positron Emission Tomography (PET)","text":"<ul> <li>An imaging technique, often used in brain imaging. For a PET scan of the brain, a radioactive \u201cmarker\u201d that emits, or releases, positrons (parts of an atom that release gamma radiation) is injected into the bloodstream. Detectors outside of the head can sense these \u201cpositron emissions,\u201d which are then reconstructed using sophisticated computer programs to create computer images. Since blood flow and metabolism increase in brain regions at work, those areas have higher concentrations of the marker, and researchers can see which brain regions activate during certain tasks or exposure to sensory stimuli. Ligands can be added to a PET scan to detect pathological entities such as amyloid or tau deposits.</li> </ul>"},{"location":"KB/Post%20Classification/","title":"Post Classification","text":""},{"location":"KB/Post%20Classification/#post-classification","title":"Post Classification","text":"<ul> <li>Interpolation of scalars at several vertices</li> <li>Classification via Transfer Function</li> <li></li> </ul>"},{"location":"KB/Post-processing%20Your%20Model%27s%20Output/","title":"Post processing Your Model's Output","text":""},{"location":"KB/Post-processing%20Your%20Model%27s%20Output/#post-processing-your-models-output","title":"Post-processing Your Model's Output.","text":"<ul> <li>Altering the loss function to incorporate a penalty for violating a fairness metric.</li> <li>Directly adding a mathematical constraint to an optimization problem.</li> <li>A synthetic feature formed by crossing (taking a Cartesian product of) individual binary features obtained from categorical data or from continuous features via bucketing. Feature crosses help represent nonlinear relationships.</li> </ul>"},{"location":"KB/Postattentive%20Amnesia/","title":"Postattentive Amnesia","text":""},{"location":"KB/Postattentive%20Amnesia/#postattentive-amnesia","title":"Postattentive Amnesia","text":"<ul> <li>No additional information is saved in the visual system between different scenes</li> </ul>"},{"location":"KB/Posterior%20Mean%20estimate/","title":"Posterior Mean Estimate","text":""},{"location":"KB/Posterior%20Mean%20estimate/#posterior-mean-estimate","title":"Posterior Mean Estimate","text":"<ul> <li>If need a single, definite model estimate -&gt; Get mean value of posterior \\(\\(\\hat \\theta = \\theta^{PME} = \\int_{\\mathbb{R}^{K}}\\theta h(\\theta|D)d\\theta\\)\\)</li> </ul>"},{"location":"KB/Postsynaptic%20Cell/","title":"Postsynaptic Cell","text":""},{"location":"KB/Postsynaptic%20Cell/#postsynaptic-cell","title":"Postsynaptic Cell","text":"<ul> <li>The neuron on the receiving end of a nerve impulse transmitted from another neuron.</li> </ul>"},{"location":"KB/Power%20and%20Force%20Limiting%20%28PFL%29/","title":"Power and Force Limiting (PFL)","text":""},{"location":"KB/Power%20and%20Force%20Limiting%20%28PFL%29/#power-and-force-limiting-pfl","title":"Power and Force Limiting (PFL)","text":"<ul> <li>Collaborative feature that allows both the operator and robot to work in proximity to one another by ensuring the robot will slow down and stop before a contact situation occurs. In order for this feature to be safely implemented, functional safety and additional detection hardware must be used. A risk assessment shall be used determine if any additional safeguarding is necessary to mitigate risks within the robot system.</li> </ul>"},{"location":"KB/Power/","title":"Power","text":""},{"location":"KB/Power/#power","title":"Power","text":"<ul> <li>current x potential difference</li> <li> \\[P = IV\\] </li> </ul>"},{"location":"KB/Pragmatics/","title":"Pragmatics","text":""},{"location":"KB/Pragmatics/#pragmatics","title":"Pragmatics","text":"<ul> <li>Important relationships that may hold between phrases and parts of their discourse context</li> <li>Parts of entities</li> <li>Parts of action</li> <li>Entities involving in actions</li> <li>Elements of sets</li> <li>Names of individuals</li> </ul>"},{"location":"KB/Pre%20Classification/","title":"Pre Classification","text":""},{"location":"KB/Pre%20Classification/#pre-classification","title":"Pre Classification","text":"<ul> <li>Classification of scalars at each sample via Transfer Function</li> <li>Interpolation of RGBA values</li> <li></li> </ul>"},{"location":"KB/Pre%20Integrated%20Volume%20Rendering/","title":"Pre Integrated Volume Rendering","text":""},{"location":"KB/Pre%20Integrated%20Volume%20Rendering/#pre-integrated-volume-rendering","title":"Pre Integrated Volume Rendering","text":"<ul> <li>Assume<ul> <li>Linear interpolations of scalar values in a ray segment</li> <li>Constant length of ray segment L</li> </ul> </li> <li>Pre computation from a slab</li> <li> \\[s_{L}(t) = s_{b}+ \\frac{t}{L}(s_{f}-s_{b})\\] </li> <li>becomes</li> <li> \\[c_{i}= \\int_{0}^{L}g(t)e^{-\\int_{t}^{L}{\\kappa(t')d_{t}}}dt' \\] </li> <li> \\[o_{i}= e^{\\int_{0}^{L}\\kappa(t)}d_{t} \\] </li> <li></li> </ul>"},{"location":"KB/Preattentive%20Processing/","title":"Preattentive Processing","text":""},{"location":"KB/Preattentive%20Processing/#preattentive-processing","title":"Preattentive Processing","text":"<ul> <li>some visual properties are detected very rapidly and in parallel by low level visual processes</li> <li></li> </ul>"},{"location":"KB/Precision%20Recall%20Curve/","title":"Precision Recall Curve","text":""},{"location":"KB/Precision%20Recall%20Curve/#precision-recall-curve","title":"Precision Recall Curve","text":"<ul> <li>Precision + Recall</li> <li>appropriate when dataset imbalanced</li> <li>no-skill line changes based on the distribution of the positive to negative classes<ul> <li>horizontal line with the value of the ratio of positive cases in the dataset</li> <li>balanced this is 0.5</li> </ul> </li> <li>skilful model is represented by a curve that bows towards (1,1) above the flat line of no skill</li> </ul>"},{"location":"KB/Precision/","title":"Precision","text":""},{"location":"KB/Precision/#precision","title":"Precision","text":"<ul> <li> \\[\\frac{TP}{TP+FP}\\] </li> <li>How many samples are actually positive out of the total number of predicted positive samples? -&gt; How precise is the model in predicting positive samples?</li> </ul>"},{"location":"KB/Predicate/","title":"Predicate","text":""},{"location":"KB/Predicate/#predicate","title":"Predicate","text":"<ul> <li>the part of a sentence or clause containing a verb and stating something about the subject (e.g.\u00a0went home\u00a0in\u00a0John went home\u00a0)</li> </ul>"},{"location":"KB/Predicting%20Student%20learning%20Curve/","title":"Predicting Student learning Curve","text":""},{"location":"KB/Predicting%20Student%20learning%20Curve/#predicting-student-learning-curve","title":"Predicting Student Learning Curve","text":"<ul> <li>(Croteau, Heffernan, &amp; Koedinger, 2004).</li> <li>The learning curve of a Knowledge Component graphs the durations of its learning events (or their probability of error).</li> <li>a learning curve should be a smoothly descending power-law or exponential curve</li> <li>For instance, the first Learning Event for a Knowledge Component may take 50 seconds, because the student is constructing the Knowledge Component by referring to the textbook and asking the tutoring system for help. The next Learning Event might take only 25 seconds because the student is reconstructing the Knowledge Component. On the third Learning Event, the student recalls the Knowledge Component after a brief struggle, so the event takes 12 seconds. The fourth event takes 6 seconds, the fifth takes 3 seconds, and so on. However, if the representation of knowledge is inaccurate, a Knowledge Component's learning curve may have a huge jump in the middle or be quite jagged</li> </ul>"},{"location":"KB/Prediction%20Difference%20Analysis/","title":"Prediction Difference Analysis","text":""},{"location":"KB/Prediction%20Difference%20Analysis/#prediction-difference-analysis","title":"Prediction Difference Analysis","text":"<ul> <li>Their goal was to improve and interpret DNNs. Their technique was based on the univariate approach of [94] and the idea that the relevance of an input feature with respect to a class can be estimated by measuring how the prediction changes if the feature is removed. Zintgraf et al. removed several features at one time using their knowledge about the images by strategically choosing patches of connected pixels as the feature sets. Instead of going through all individual pixels, they considered all patches of a special size implemented in a sliding window fashion. They visualized the effects of different window sizes and marginal versus conditional sampling and displayed feature maps of different hidden layers and top-scoring classes.</li> </ul>"},{"location":"KB/Prediction%20assumption/","title":"Prediction assumption","text":""},{"location":"KB/Prediction%20assumption/#prediction-assumption","title":"Prediction Assumption","text":"<ul> <li>every model that aims to predict an output Y from an input X makes the assumption that it's possible to predict Y based on X.</li> </ul>"},{"location":"KB/Predictive%20Parity/","title":"Predictive Parity","text":""},{"location":"KB/Predictive%20Parity/#predictive-parity","title":"Predictive Parity","text":"<ul> <li>A fairness metric that checks whether, for a given classifier, the precision rates are equivalent for subgroups under consideration.</li> </ul>"},{"location":"KB/Predictive%20Uncertainty/","title":"Predictive Uncertainty","text":""},{"location":"KB/Predictive%20Uncertainty/#predictive-uncertainty","title":"Predictive Uncertainty","text":""},{"location":"KB/Preferences%20and%20ethical%20principles%20in%20decision%20making/","title":"Preferences and ethical principles in decision making","text":""},{"location":"KB/Preferences%20and%20ethical%20principles%20in%20decision%20making/#preferences-and-ethical-principles-in-decision-making","title":"Preferences and Ethical Principles in Decision Making","text":"<ul> <li>Andrea Loreggia, Nicholas Mattei, Francesca Rossi, and Kristen Brent Venable.</li> <li>leverage the CP-net formalism to represent the exogenous ethics priorities and endogenous subjective preferences</li> <li>distance between CPnets so as to enable AI agents to make decisions using their subjective preferences if they are close enough to the ethical principles</li> </ul>"},{"location":"KB/Prefix/","title":"Prefix","text":""},{"location":"KB/Prefix/#prefix","title":"Prefix","text":"<ul> <li>precede the stem: do / undo</li> </ul>"},{"location":"KB/Prepositions/","title":"Prepositions","text":""},{"location":"KB/Prepositions/#prepositions","title":"Prepositions","text":"<ul> <li>relates phrases (at, on, of, about)</li> </ul>"},{"location":"KB/Presence-sensing%20Safeguarding%20Device/","title":"Presence sensing Safeguarding Device","text":""},{"location":"KB/Presence-sensing%20Safeguarding%20Device/#presence-sensing-safeguarding-device","title":"Presence-sensing Safeguarding Device","text":"<ul> <li>A device designed, constructed and installed to create a sensing field to detect an intrusion into such field by people, robots or objects</li> </ul>"},{"location":"KB/Pressure%20%3D%20ForceArea/","title":"Pressure = ForceArea","text":""},{"location":"KB/Pressure%20%3D%20ForceArea/#pressure-forcearea","title":"Pressure = Force/Area","text":"<ul> <li> \\[P = \\frac{F}{A}\\] </li> </ul>"},{"location":"KB/Presynaptic%20Cell/","title":"Presynaptic Cell","text":""},{"location":"KB/Presynaptic%20Cell/#presynaptic-cell","title":"Presynaptic Cell","text":"<ul> <li>In synaptic transmission, the neuron that sends a nerve impulse across the synaptic cleft to another neuron.</li> </ul>"},{"location":"KB/Pretext%20Task/","title":"Pretext Task","text":""},{"location":"KB/Pretext%20Task/#pretext-task","title":"Pretext Task","text":"<ul> <li>pre-designed tasks </li> <li>visual features are learned by learning objective functions of pretext tasks</li> </ul>"},{"location":"KB/Pretext%20Tasks/","title":"Pretext Tasks","text":""},{"location":"KB/Pretext%20Tasks/#pretext-tasks","title":"Pretext Tasks","text":"<ul> <li> <p>Image Generation</p> </li> <li> <p>Video Generation</p> </li> <li> <p>Context Similarity</p> </li> <li> <p>Spatial Context Structure</p> </li> <li> <p>Temporal Context Structure</p> </li> <li> <p>Free Semantic Label-based Method</p> </li> <li> <p>Cross Modal-based Methods</p> </li> <li> <p>Semantic Segmentation</p> </li> <li> <p>Object Detection</p> </li> <li> <p>Image Classification</p> </li> <li> <p>Human Action Recognition </p> </li> <li> <p>Kernel Visualization</p> </li> <li> <p>Feature Map Visualization</p> </li> <li> <p>Nearest Neighbor Retrieval</p> </li> </ul>"},{"location":"KB/Primary%20Capsule/","title":"Primary Capsule","text":""},{"location":"KB/Primary%20Capsule/#primary-capsule","title":"Primary Capsule","text":"<ul> <li>lowest layer of a capsule network </li> <li>processing the\u00a0raw input image</li> <li>Each capsule in the primary layer is sensitive to a specific feature of the input image, such as an edge or a particular shape.</li> </ul>"},{"location":"KB/Primary%20Capsule/#convolution","title":"Convolution","text":"<ul> <li>The primary capsules in a capsule network are created by applying a series of convolutional filters to the input image. Each filter is responsible for detecting a specific feature in the input image, such as an edge or a particular shape.</li> </ul>"},{"location":"KB/Primary%20Capsule/#reshape","title":"Reshape","text":"<ul> <li>The outputs of the convolutional filters are then reshaped into a grid of \u201ccapsules,\u201d each of which corresponds to a specific location in the input image.</li> </ul>"},{"location":"KB/Primary%20Capsule/#squash","title":"Squash","text":"<ul> <li>The outputs of the capsules are then \u201csquashed\u201d to ensure that they have a non-negative scalar value, which allows the network to learn more easily to differentiate between objects and background.</li> </ul>"},{"location":"KB/Prion/","title":"Prion","text":""},{"location":"KB/Prion/#prion","title":"Prion","text":"<ul> <li>A protein aggregate that can multiply itself, inducing the formation of new aggregates from individual copies of the protein it encounters. Prions have the potential to spread within the body and brain, and even from one organism to another\u2014\u201cinfectiously,\u201d like a virus. The first prions described were hardy aggregates of PrP, the prion protein. They are responsible for a set of rapid, fatal, and potentially transmissible neurodegenerative diseases including Creutzfeldt-Jakob disease and bovine spongiform encephalopathy (\u201cmad cow disease\u201d). Many researchers now argue that protein aggregates in other neurodegenerative diseases, such as the A\u03b2 and tau plaques of Alzheimer\u2019s, have such similar properties that they also deserve to be called prions.</li> </ul>"},{"location":"KB/Prismatic%20Joint/","title":"Prismatic Joint","text":""},{"location":"KB/Prismatic%20Joint/#prismatic-joint","title":"Prismatic Joint","text":"<ul> <li>Linear movement like a piston</li> </ul>"},{"location":"KB/Privacy%20awareness/","title":"Privacy awareness","text":""},{"location":"KB/Privacy%20awareness/#privacy-awareness","title":"Privacy awareness","text":"<ul> <li>ML models may have complex representations of their learned patterns</li> <li>the ability to explain the inner relations of a trained model by non-authorized third parties may also compromise the differential privacy of the data origin</li> </ul>"},{"location":"KB/Probability/","title":"Probability","text":""},{"location":"KB/Probability/#probability","title":"Probability","text":"<ul> <li>Frequentist</li> <li>Bayesian</li> </ul>"},{"location":"KB/Problems%20facing%20MLOps/","title":"Problems facing MLOps","text":"","tags":["mlops"]},{"location":"KB/Problems%20facing%20MLOps/#problems-facing-mlops","title":"Problems Facing MLOps","text":"","tags":["mlops"]},{"location":"KB/Problems%20facing%20MLOps/#general","title":"General","text":"<ul> <li>Coding is not the whole story</li> <li>It\u2019s easier for great engineers to pick up ML knowledge, but it\u2019s a lot harder for ML experts to become great engineers.</li> <li>Use \"off the shelf models\"</li> <li>Companies focus on improving data, but not model</li> <li>Model sizes are hard</li> <li>Dataset/model versioning is hard</li> <li>Experiment Tracking<ul> <li>Hyperparameter tuning is important and it\u2019s not surprising to find several that focus on it, </li> <li>none seems to catch on because the bottleneck for hyperparameter tuning is not the setup, but the computing power needed to run it.</li> </ul> </li> <li>Data monitoring<ul> <li>Distribution shift</li> </ul> </li> <li>Labelling<ul> <li>How often do we need to re-label</li> </ul> </li> <li>CI/CD<ul> <li>Testing</li> <li>Re-train model</li> </ul> </li> <li>Deployment<ul> <li>How often to package and deploy</li> <li>One reason for the lack of serving solutions is the lack of communication between researchers and production engineers.</li> <li>Small companies, whose employees can see the entire stack, are constrained by their immediate product needs</li> </ul> </li> <li>Model Compression<ul> <li>Compress ML Models</li> </ul> </li> <li>Optimizing Inference</li> <li>Edge devices</li> <li>Privacy<ul> <li>GDPR</li> </ul> </li> <li>OSS vs Open Core<ul> <li>Since OSS has become a standard, it\u2019s challenging for startups to figure out a business model that works.</li> <li>Any tooling company started has to compete with existing open-source tools.</li> </ul> </li> </ul>","tags":["mlops"]},{"location":"KB/Programmable%20Logical%20Controller%20%28PLC%29/","title":"Programmable Logical Controller (PLC)","text":""},{"location":"KB/Programmable%20Logical%20Controller%20%28PLC%29/#programmable-logical-controller-plc","title":"Programmable Logical Controller (PLC)","text":"<ul> <li>A solid-state control system, which has a user programmable memory for storage of instructions to implement specific functions such as: I/O control logic, timing, counting arithmetic and data manipulation</li> <li>A PLC consists of a central processor, input/output interface, memory and programming device, which typically uses relay equivalent symbols.</li> </ul>"},{"location":"KB/PromptIR/","title":"PromptIR","text":""},{"location":"KB/PromptIR/#promptir","title":"PromptIR","text":"<ul> <li>Summary : Summary : Uses a transformer network to prediction a \"prompt\" that says how degraded an image is. And based on that, decides what module to use. </li> <li>Deep learning-based methods have significantly improved image restoration performance, however, they have limited generalization ability to different degradation types and levels </li> <li>requires training individual models for each specific degradation and knowing the input degradation type to apply the relevant model. </li> <li>prompt-based learning approach, PromptIR, for All-In-One image restoration that can effectively restore images from various types and levels of degradation </li> <li>uses prompts to encode degradation-specific information </li> <li>generic and efficient plugin module with few lightweight prompts that can be used to restore images of various types and levels of degradation with no prior information on the corruptions present in the image  </li> <li> </li> <li>AirNet </li> <li>addresses the all-in-one restoration task by employing the contrastive learning paradigm. </li> <li>involves training an extra encoder to differentiate various types of image degradations </li> <li>Although AirNet [29] yields state-of-the-art results, it struggles to model fully disentangled representations of different corruption types </li> <li>Furthermore, the usage of an additional encoder for contrastive learning leads to a higher training burden due to the two-stage training approach. </li> </ul>"},{"location":"KB/PromptIR/#method","title":"Method","text":"<ul> <li>While the model is initially \"blind\" to the nature of degradation, its performance in recovering a clean image can be enhanced by providing implicit contextual information about the type of degradation  </li> <li> </li> <li>From a given degraded input image I \u2208 RH \u00d7W \u00d73 </li> <li>first extracts low-level features F0 \u2208 RH\u00d7W\u00d7C by applying a convolution operation; where H \u00d7 W is the spatial resolution and C denotes the channels. </li> <li>eature embeddings F0 undergo a 4-level hierarchical encoder-decoder, transforming into deep features Fr \u2208 RH \u00d7W \u00d72C </li> <li> </li> <li>Starting from the high-resolution input, the goal of the encoder is to progressively reduce the spatial resolution while . </li> <li>From the low-resolution latent features Fl, the aim of the decoder is to gradually recover the highresolution clean output </li> <li>ncorporate prompt block </li> <li>Prompt blocks are adapter modules that sequentially connect every two levels of the decoder. </li> </ul>"},{"location":"KB/PromptIR/#aim-to-learn-a-single-model-m-to-restore-an-image-i-from-a-degraded-image-i-that-has-been-degraded-using-a-degradation-d-while-having-no-prior-information-about-d","title":"aim to learn a single model M to restore an image I from a degraded image I, that has been degraded using a degradation D, while having no prior information about D.","text":""},{"location":"KB/PromptIR/#each-level-of-the-encoder-decoder-employs-several-transformer-blocks-with-the-number-of-blocks-gradually-increasing-from-the-top-level-to-the-bottom-level-to-maintain-computational-efficiency","title":"Each level of the encoder-decoder employs several Transformer blocks, with the number of blocks gradually increasing from the top level to the bottom level to maintain computational efficiency.","text":""},{"location":"KB/PromptIR/#prompt-block","title":"Prompt Block","text":""},{"location":"KB/PromptIR/#prompt-generation-module","title":"Prompt Generation Module","text":"<ul> <li>Prompt components Pc form a set of learnable parameters that interact with the incoming features to embed degradation information </li> <li>features-prompt interaction is to directly use the learned prompts to calibrate the features. </li> <li> </li> <li>shared space to facilitate correlated knowledge sharing among prompt components. </li> <li>To generate prompt-weights from the input features Fl </li> <li>first applies global average pooling (GAP) across spatial dimension to generate feature vector v 2 RC\u02c6 </li> <li>pass v through a channeldownscaling convolution layer to obtain a compact feature vector, followed by the softmax operation, thus yielding prompt-weights w 2 RN </li> <li>use these weights to make adjustments in prompt components, followed by a 3 x 3 convolution layer  </li> <li> </li> <li>Since at inference time, it is necessary for the restoration network to be able to handle images of different resolutions, we cannot use the prompt components Pc with a fixed size. </li> <li>bilinear upsampling operation to upscale the prompt components </li> </ul>"},{"location":"KB/PromptIR/#dynamically-predicts-attention-based-weights-from-the-input-features-and-apply-them-to-prompt-components-to-yield-input-conditioned-prompts-p","title":"dynamically predicts attention-based weights from the input features and apply them to prompt components to yield input-conditioned prompts P","text":""},{"location":"KB/PromptIR/#prompt-interaction-module","title":"Prompt Interaction Module","text":"<ul> <li>enable interaction between the input features Fl and prompts P for a guided restoration. </li> <li> </li> <li>pass the concatenated representations through a Transformer block that exploits degradation information encoded in the prompts and transforms the input features. </li> <li>The Transformer block is composed of two sequentially connected sub-modules: Multi-Dconv head transposed attention (MDTA), and Gated-Dconv feedforward network (GDFN). MDTA applies self-attention operation across channels rather than the spatial dimension and has linear complexity. </li> <li>The goal of GDFN is to transform features in a controlled manner, i.e., suppressing the less informative features and allowing only useful ones to propagate through the network  </li> <li> </li> <li> </li> </ul>"},{"location":"KB/PromptIR/#in-pim-we-concatenate-the-generated-prompts-with-the-input-features-along-the-channel-dimension","title":"In PIM, we concatenate the generated prompts with the input features along the channel dimension.","text":""},{"location":"KB/PromptIR/#_1","title":"PromptIR","text":"<ul> <li>Implementation Details </li> <li>end-to-end trainable and requires no pretraining of any individual component </li> <li>4-level encoder-decoder, with varying numbers of Transformer blocks at each level, specifically [4, 6, 6, 8] from level-1 to level-4. </li> <li>one prompt block between every two consecutive decoder levels, totaling 3 prompt blocks in the overall PromptIR network </li> <li>The total number of prompt components are 5 </li> <li>The model is trained with a batch size of 32 in the all-in-one setting, and with a batch of 8 in the single-task setting </li> <li>The network is optimized with an L1 loss, and we use Adam optimizer (1 = 0.9, 2 = 0.999) with learning rate 2e 4 for 200 epochs. </li> <li> </li> <li>BSD400 </li> <li>WED </li> <li>o </li> <li>BSD68 </li> <li>Urban100 </li> <li>Rain100L </li> <li>SOTS  </li> <li> </li> <li> </li> <li> </li> </ul>"},{"location":"KB/PromptIR/#cropped-patches-of-size-128-x-128","title":"cropped patches of size 128 x 128","text":""},{"location":"KB/Propose-but-verify/","title":"Propose-but-verify","text":""},{"location":"KB/Propose-but-verify/#propose-but-verify","title":"Propose-but-verify","text":"<ul> <li>Speakers guess a word-concept association</li> <li>Keep guess until they encounter contradicting information</li> <li>Seems to correctly model competitions between features/cues</li> </ul>"},{"location":"KB/Protein%20Folding/","title":"Protein Folding","text":""},{"location":"KB/Protein%20Folding/#protein-folding","title":"Protein Folding","text":"<ul> <li>The process by which the chain of amino acids that make up a protein assumes its functional shape. The protein clumps and tangles that occur in some neurodegenerative disorders are thought to be triggered when proteins \u201cmisfold.\u201d</li> </ul>"},{"location":"KB/Protein%20Modeling/","title":"Protein Modeling","text":""},{"location":"KB/Protein%20Modeling/#protein-modeling","title":"Protein Modeling","text":"<ul> <li>Using Bayesian models</li> <li>Task : Estimate Probability mass function(because discrete) for a finite, discrete distribution -&gt; given a histogram from a sample</li> <li>Large number of categories and small number of observations</li> <li><ul> <li>Estimate Probability distrib of amino acids in each column in a protein class. 20 dim PMF (one for each site)</li> <li>Can be aligned</li> <li>High chances of class not being present in data<ul> <li>MLE will assign 0 Probability to X</li> <li>Wrong decision made for a lot of them that were not in the training set</li> <li>Cannot use</li> </ul> </li> </ul> </li> <li>20 dim PMF for amnio acid distrib : \\(\\theta = (\\theta_{1}, \u2026 ,\\theta_{20})' = (P(X=A), \u2026, P(X=Y))'\\)<ul> <li>count vectors of amino acids found in a given site in training data D</li> <li>Distributed according to Multinomial Distribution with l = 20</li> </ul> </li> </ul>"},{"location":"KB/Protein%20Modeling/#using-prior","title":"Using Prior","text":"<ul> <li>0 probabilities should not occur. (\\(\\mathcal{H} = (\\theta_{1}, \u2026, \\theta_{20})' \\in \\mathbb{R}^{20}|\\theta_{j} \\in (0,1)\\)\\) and $$ \\Sigma_{j} \\theta_{j}=1$$<ul> <li>19 dim hypervolume</li> <li>Continuous space and so can use PDF</li> <li>Dirichlet Distribution is used to represent it because parameterized with l = 20</li> </ul> </li> <li></li> <li>\\(\\alpha\\)s fixed beforehand</li> </ul>"},{"location":"KB/Proto%20Distributions/","title":"Proto Distributions","text":""},{"location":"KB/Proto%20Distributions/#proto-distributions","title":"Proto Distributions","text":"<ul> <li>Distributions</li> <li>Occur in Bayesian</li> </ul>"},{"location":"KB/Proto%20Distributions/#continuous-spaces","title":"Continuous Spaces","text":"<ul> <li>Proto PDF</li> <li> \\[p(x|\\theta) = \\frac{1}{\\int_{\\mathbb{R}^{k}}p_{0}(x| \\theta)dx}p_{0}(x|\\theta)\\] </li> <li>\\(p_{0}\\) gives the shape of the PDF</li> <li>\\(\\frac{1}{\\int_{\\mathbb{R}^{k}}p_{0}(x| \\theta)dx}\\) is a normalization so it integrates to 1</li> <li></li> <li>Most of the time we dont know a distribution but only its proto distribution. This is actually enough sometimes</li> </ul>"},{"location":"KB/Proto%20Distributions/#discrete-spaces","title":"Discrete Spaces","text":"<ul> <li>Proto PMF</li> </ul>"},{"location":"KB/Proto%20PDF/","title":"Proto PDF","text":""},{"location":"KB/Proto%20PDF/#proto-pdf","title":"Proto PDF","text":"<ul> <li>Generalized PDF</li> <li>Proto PDF \\(g_{0}\\)</li> <li>Any non negative function \\(g_{0}: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}\\) that has a finite integral \\(\\(\\int_{\\mathbb{R}^{n}}g_{0}(x)dx\\)\\)</li> <li>If we divide \\(g_{0}\\) by its integral -&gt; we get a normal PDF g</li> </ul>"},{"location":"KB/Proto%20PMF/","title":"Proto PMF","text":""},{"location":"KB/Proto%20PMF/#proto-pmf","title":"Proto PMF","text":"<ul> <li>Generalized PMF</li> <li> \\[p(x|\\theta) = \\frac{1}{\\Sigma_{x \\in S}p_{0}(s)}p_{0}(x)\\] </li> <li>S is huge if there are many random variables</li> </ul>"},{"location":"KB/Proximity%20Sensor/","title":"Proximity Sensor","text":""},{"location":"KB/Proximity%20Sensor/#proximity-sensor","title":"Proximity Sensor","text":"<ul> <li>A non-contact sensing device used to sense when objects are a short distance away, and it can determine the distance of the object.</li> </ul>"},{"location":"KB/Proxy%20Attention/","title":"Proxy Attention","text":""},{"location":"KB/Proxy%20Attention/#proxy-attention","title":"Proxy Attention","text":"<ul> <li>Let \\(I_{s} \\in \\mathbb{R}^{W \\times H \\times C}\\) be a random source image</li> <li>applied saliency map \\(I_{vs}\\) <ul> <li>\\(I_{vs} = f(I_{s}) \\in \\mathbb{R}^{W \\times H}\\)</li> <li>Augmented image \\(I_{a}\\)</li> <li>\\(\\odot\\) elementwise multi </li> <li>Mask \\(M \\in \\{0,1\\}^{W \\times H}\\) </li> </ul> </li> </ul>"},{"location":"KB/Proxy%20Objective/","title":"Proxy Objective","text":""},{"location":"KB/Proxy%20Objective/#proxy-objective","title":"Proxy Objective","text":"<ul> <li>Easier to change or measure than the actual objective</li> <li>Suppose we have some sample space \\(S\\) (such as the set of possible question-answer pairs), some Probability distribution \\(P\\) over \\(S\\), a true objective (or \u201creward\u201d) \\(R_{true}: S \\to \\mathbb{R}\\) , proxy objective \\(R_{proxy}:S \\to \\mathbb{R}\\) and we optimize \\(R_{proxy}\\) to get a new distribution \\(P'\\)</li> <li>\\(E_{x'\\sim P'}[Rtrue(x\u2032)]\\) is how well the true objective is optimized<ul> <li>Monte Carlo estimator used</li> <li>If \\(N \\geq n\\) samples from P, simultaneously consider every possible subset of these samples of size nnn, weight each sample by the number of subsets for which it is the best according to the proxy objective, and then take the weighted average true objective \\(\\(\\binom{k-1}{n-1}\\)\\) where k is the rank of the sample under the proxy objective, from 1 (worst) up to N (best)</li> <li>Can reuse samples of n</li> </ul> </li> <li>KL Divergence \\(P' || P\\) measures how much optimization is done<ul> <li>As long as Continous , \\(\\(n - \\frac{n-1}{n}\\)\\)</li> </ul> </li> </ul>"},{"location":"KB/Proxy%20Objective/#refs","title":"Refs","text":"<ul> <li>openai</li> </ul>"},{"location":"KB/Proxy%20features/","title":"Proxy features","text":""},{"location":"KB/Proxy%20features/#proxy-features","title":"Proxy Features","text":"<ul> <li>there may be correlated features with sensitive ones that can induce bias even when the sensitive features are not present in the dataset.</li> </ul>"},{"location":"KB/Pruning/","title":"Pruning","text":""},{"location":"KB/Pruning/#pruning","title":"Pruning","text":"<ul> <li>Mainly that of being able to reduce the size, cost and computational requirements of my models, all while maintaning the accuracy (sort of atleast).</li> <li>Generally this comes about by removing parameters in some form or fashion.</li> <li>Rather than taking a mask, we can prune certain parts of the network by setting them to 0 or by dropping them if required. (aka weights and biases)</li> <li>In most cases, the network is first trained for a while. Then pruned. Which reduces its accuracy and is thus trained again (fine tuning). This cycle is repeated until we get the results we require.</li> <li>Major Types of Pruning Methods</li> <li>Structure Based Pruning</li> <li>Scoring Pruning Approaches</li> <li>Scheduling</li> <li>Fine Tuning Based Pruning</li> <li>Global Magnitude Based Pruning</li> <li>Global Gradient Magnitude Based Pruning</li> <li>Layerwise Gradient Magnitude Based Pruning</li> <li>Random Pruning</li> <li>Layerwise Magnitude Based Pruning</li> </ul>"},{"location":"KB/Pseudo%20Label/","title":"Pseudo Label","text":""},{"location":"KB/Pseudo%20Label/#pseudo-label","title":"Pseudo Label","text":"<ul> <li>Pseudo labels are automatically generated labels based on data attributes for pretext tasks</li> </ul>"},{"location":"KB/Psycholinguistics/","title":"Psycholinguistics","text":""},{"location":"KB/Psycholinguistics/#psycholinguistics","title":"Psycholinguistics","text":"<ul> <li>The study of how the brain processes or produces language</li> <li>Linguistics proper is more concerned with what the structure of language is</li> <li>Also examines how linguistic processing interacts with executive functions, e.g. \u2022 Working memory \u2022 Inhibition</li> </ul>"},{"location":"KB/Psychosis/","title":"Psychosis","text":""},{"location":"KB/Psychosis/#psychosis","title":"Psychosis","text":"<ul> <li>A severe symptom of mental illness in which a person\u2019s thoughts and perceptions are so disordered that the individual loses touch with reality.</li> </ul>"},{"location":"KB/Pulse%20Coordinates/","title":"Pulse Coordinates","text":""},{"location":"KB/Pulse%20Coordinates/#pulse-coordinates","title":"Pulse Coordinates","text":"<ul> <li>Yaskawa robots define robot joint axes position in degrees for revolute joints. Pulse is also another way to specify robot joint position, and it does so in robot motor encoder pulse counts.</li> </ul>"},{"location":"KB/Pulse%20Oximeter/","title":"Pulse Oximeter","text":""},{"location":"KB/Pulse%20Oximeter/#pulse-oximeter","title":"Pulse Oximeter","text":"<ul> <li>A small device that clips to the finger, toe or earlobe used to measure blood oxygen saturation</li> </ul>"},{"location":"KB/Punctuation/","title":"Punctuation","text":""},{"location":"KB/Punctuation/#punctuation","title":"Punctuation","text":"<ul> <li>Punctuation characters are treated as separate tokens \u2013 usually</li> </ul>"},{"location":"KB/Pupil%20Dilation/","title":"Pupil Dilation","text":""},{"location":"KB/Pupil%20Dilation/#pupil-dilation","title":"Pupil Dilation","text":"<ul> <li>Pupil diameter responds to more than just light<ul> <li>When something was visually pleasing</li> <li>Harder problems - thinking time</li> <li>Reaches an asymptote when the task is too difficult , processing load</li> </ul> </li> <li>Measure of resource allocation</li> <li></li> <li>Light based response - parasympathetic</li> <li>Increases with emotional stimulation</li> <li></li> <li>Speech replacement</li> </ul>"},{"location":"KB/Puzzle%20Mix/","title":"Puzzle Mix","text":""},{"location":"KB/Puzzle%20Mix/#puzzle-mix","title":"Puzzle Mix","text":"<ul> <li>@kimPuzzleMixExploiting2020</li> <li>learns to augment two images optimally based on saliency.</li> <li>Images are divided into regions for the mixup</li> <li>The algorithm learns to transport the salient region of one image such that the output image has the maximized saliency from both images.</li> <li> \\[h(x_{0}, x_{1}) = (1-z) \\odot \\Pi_{0}^{T}x_{0} + z \\odot \\Pi_{1}^{T}x_{1}\\] </li> <li>where \\(z_{i}\\) is a binary mask, \\(\\lambda = \\frac{1}{n}\\Sigma_{i}z_{i}\\) is the mixing ratio and \\(\\Pi_{0}, \\Pi_{1}\\) are represent \\(n \\times n\\) grids that denote the amount of mass that is transported during transport of the image patch to another location. </li> </ul>"},{"location":"KB/Pyramidal%20cell/","title":"Pyramidal cell","text":""},{"location":"KB/Pyramidal%20cell/#pyramidal-cell","title":"Pyramidal Cell","text":"<ul> <li><ul> <li>Folds on sides have same charge</li> <li>Adds up so can be measured</li> <li>Equivalent Current Dipole</li> </ul> </li> </ul>"},{"location":"KB/Pytorch%20Tricks/","title":"Pytorch Tricks","text":""},{"location":"KB/Pytorch%20Tricks/#pytorch-tricks","title":"Pytorch Tricks","text":"<ul> <li>Also look at fastai</li> </ul>"},{"location":"KB/Pytorch%20Tricks/#get-params-of-a-layer","title":"Get Params of a Layer","text":"<pre><code>m = learn.model\nl = m.get_submodule('0.model.stem.1')\nlist(l.parameters())\n</code></pre>"},{"location":"KB/Pytorch%20Tricks/#interact","title":"Interact","text":"<pre><code>from ipywidgets import interact\n@interact(m=1.5, b=1.5)\ndef plot_relu(m,b):\n    plot_function(partial(relu, m, b), ylim = (-1, 4))\n</code></pre>"},{"location":"KB/Pytorch%20Tricks/#set-dataset-directory","title":"Set Dataset Directory","text":"<pre><code>import os\nos.environ[\"TORCH_HOME\"] = \"/media/hdd/Datasets/\"\nos.environ[\"FASTAI_HOME\"] = \"/media/hdd/Datasets/\"\n</code></pre>"},{"location":"KB/Quadratic%20Loss/","title":"Quadratic Loss","text":""},{"location":"KB/Quadratic%20Loss/#quadratic-loss","title":"Quadratic Loss","text":"<ul> <li>$\\(W = argmin_{W^{\\ast}}\\Sigma^N_{i=1} ||W^{\\ast} x_i - y_i||^2\\)</li> <li>\\(\\Delta : \\mathbb{R}^{n} \\rightarrow \\mathbb{R}, x \\rightarrow E[Y|X = x]\\) is the gold standard for minimizing this. But \\(\\Delta\\) is unknown</li> </ul>"},{"location":"KB/Quadratic%20Potential%20Field/","title":"Quadratic Potential Field","text":""},{"location":"KB/Quadratic%20Potential%20Field/#quadratic-potential-field","title":"Quadratic Potential Field","text":"<ul> <li>make attractive potential to the goal \\(\\(U_{att}(q)=\\frac{1}{2}D(q,q_{goal})\\)\\)</li> <li></li> </ul>"},{"location":"KB/Quantifier%20spreading%20children%20misled%20by%20ostensive%20cues/","title":"Quantifier spreading children misled by ostensive cues","text":""},{"location":"KB/Quantifier%20spreading%20children%20misled%20by%20ostensive%20cues/#quantifier-spreading-children-misled-by-ostensive-cues","title":"Quantifier Spreading Children Misled by Ostensive Cues","text":"<ul> <li> <p>Katalin \u00c9. Kiss and Tam\u00e1s Z\u00e9t\u00e9nyi</p> </li> <li> <p>TL;DR : Use real images instead of Drawings</p> </li> <li>economy of the stimulus employed in child language experiments may lend an increased ostensive effect to the message communicated to the child</li> <li>Thus, when the visual stimulus in a sentence-picture matching task is a minimal model abstracting away from the details of the situation, children often regard all the elements of the stimulus as ostensive clues to be represented in the corresponding sentence</li> <li>The use of such minimal stimuli is mistaken when the experiment aims to test whether or not a certain element of the stimulus is relevant for the linguistic representation or interpretation</li> <li>It is claimed that children find a universally quantified sentence like Every girl is riding a bicycle to be a false description of a picture showing three girls riding bicycles and a solo bicycle because they are misled to believe that all the elements in the visual stimulus are relevant, hence all of them are to be represented by the corresponding linguistic description.</li> <li>When the iconic drawings were replaced by photos taken in a natural environment rich in accidental details, the occurrence of quantifier spreading was radically reduced.</li> <li>It is shown that an extra object in the visual stimulus can lead to the rejection of the sentence also in the case of sentences involving no quantification, which gives further support to the claim that the source of the problem is not (or not only) the grammatical or cognitive difficulty of quantification but the unintended ostensive effect of the extra object.</li> <li>The reason for the unexpected reactions is that the experimental stimulus presented to the child is devoid of any episodic details; it merely contains a few iconic symbols, which suggests to the child that the irrelevant details have been omitted; hence every element of the stimulus, including the one whose relevance the experiment aims to test, is to be interpreted as an ostensive signal, i.e., every element of the stimulus is significant.</li> <li>Ostension</li> </ul>"},{"location":"KB/Quantifier%20spreading%20children%20misled%20by%20ostensive%20cues/#quantifier-spreading-as-an-ostensive-effect","title":"Quantifier Spreading as an Ostensive Effect","text":"<ul> <li>The phenomenon</li> <li>Every girl is riding a bicycle.</li> <li>Although every one of the three girls in the picture is riding a bicycle, many children find the sentence false</li> <li>When asked \"Why?\", they point at the solo bicycle, and say something like \"Not that bicycle\", i.e., they show 'Exhaustive Pairing' under an extra object condition.</li> <li>Quantifier spreading also has a somewhat less common variant, called \"Perfectionist Response\".1 It occurs when a universally quantified sentence like (2a) is to be matched with a picture like Figure 2, which contains an element that is neither identical with the referent of the subject, nor identical with the referent of the VP-internal complement.2 (2) a. Every dog is eating a bone. b. No, not that one.</li> </ul>"},{"location":"KB/Quantifier%20spreading%20children%20misled%20by%20ostensive%20cues/#theories-of-quantifier-spreading","title":"Theories of Quantifier Spreading","text":"<ul> <li>In fact, children are not fully consistent in assigning to universally quantified sentences interpretations of type (3b); the adult interpretation illustrated in (3a),</li> <li>too, appears to be accessible also to those favoring the spreading reading.</li> <li>The event quantification analysis of Philip (1995) has been criticized on several grounds. For example, it predicts that quantifier spreading is only attested in the case of eventive sentences. In fact, as shown by Philip (2011), it also occurs with sentences of type (4), which contain no event variable:</li> <li>Furthermore, as Crain et al. (1996) point out, the analysis of every as an event quantifier does not account for the \"perfectionist\" mistake, i.e., for the case when the sentence questioned in (1a) is found false in the presence of an extra participant that is neither a girl, nor a bicycle</li> <li>The fact that children have initially access to two interpretations of universally quantified sentences (those of type (3b) and (3a)), one of which is later eliminated, raises a learnability problem, as well \u2013 under the assumption that children acquiring their mother tongue only have access to positive evidence.</li> <li>Several experiments on quantifier spreading have shown that the rate of spreading is affected by pragmatic factors, e.g., a rich linguistic or visual context reduces spreading (cf. Crain et al. 1996</li> <li>However, some of the evidence concerning the role of extra elements appears to be contradictory; e.g., in the case of quantifier spreading, both the increasing of the number of extra objects (Freeman, Sinha &amp; Stedmon 1982), and the decreasing of the size of the extra object (Philip 2011: 377) have been found to reduce the proportion of spreading, which has not been given a principled explanation.</li> <li>Relevance Account</li> <li>Salient Object Strategy</li> <li>Quantifier spreading is due to the increased ostensive effect of iconic stimuli</li> <li>We hypothesized that quantifier spreading is elicited in experimental situations where the stimulus is not embedded in a context, and is devoid of episodic details, as a consequence of which it gains a \u2013 potentially misleading \u2013 concentrated ostensive effect</li> <li>Crucially, however, when the stimulus only contains a few iconic symbols, every one of its elements gains an ostensive effect.</li> </ul>"},{"location":"KB/Quantifier%20spreading%20children%20misled%20by%20ostensive%20cues/#experiment","title":"Experiment","text":""},{"location":"KB/Quantifier%20spreading%20children%20misled%20by%20ostensive%20cues/#participants","title":"Participants","text":"<ul> <li>We tested 82 children from 5 Budapest kindergartens, whose mean age was 5;3 years (SD=0.73).</li> <li>We also carried out the experiment with an adult control group consisting of 24 university students, whose mean age was 21 years (SD=1.61).</li> </ul>"},{"location":"KB/Quantifier%20spreading%20children%20misled%20by%20ostensive%20cues/#procedure","title":"Procedure","text":"<ul> <li>The child, the experimenter, and a helper were seated at a table in front of a laptop in a quiet room of the kindergarten.</li> <li>The helper held a teddy bear</li> <li>The experimenter told the child that they would look at pictures on the computer screen together.</li> <li>They would listen to what the bear said about each picture, and the experimenter would ask the subject whether or not it was true.</li> </ul>"},{"location":"KB/Quantifier%20spreading%20children%20misled%20by%20ostensive%20cues/#materials","title":"Materials","text":"<ul> <li>16 sentence\u2013picture pairs (8 fillers and 8 test pairs) were presented to the subjects Each test sentence involved the universal quantifier minden \u201bevery'</li> <li>Four sentence\u2013picture pairs were of the type which can elicit the Exhaustive Pairing mistake, i.e., they involved an extra object (see example (7) and Figures 3, 4), and four sentence\u2013picture pairs were of the type which can elicit the Perfectionist Response, i.e., they contained an extra element neither identical with the referent of the subject, nor identical with the referent of the VPinternal complement (see example (8) and Figures 5, 6)</li> </ul>"},{"location":"KB/Quantifier%20spreading%20children%20misled%20by%20ostensive%20cues/#results","title":"Results","text":"<ul> <li>The stimuli consisting of a quantified sentence and a drawing elicited quantifier spreading in 27% of the children's answers. In the case of the stimuli consisting of a quantified sentence and a photo, the rate of quantifier spreading dropped to 15%. Among the adults, the rate of quantifier spreading was 6% and 5%, respectively</li> </ul>"},{"location":"KB/Quantifier%20spreading%20children%20misled%20by%20ostensive%20cues/#discussion","title":"Discussion","text":"<ul> <li>In language acquisition experiments, experimenters tend to use iconic visual stimuli</li> <li>in order to eliminate irrelevant distractors, and to ensure that children only react to the controlled factor(s)</li> <li>Our results suggest that this method is mistaken when the experiment aims to test whether or not an element in the stimulus is relevant for the linguistic representation</li> <li>If the visual stimulus is a minimal model devoid of episodic details, children tend to interpret all of its elements as ostensive clues to be represented linguistically</li> <li>If the ostensive effect is diminished by the use of photos taken in natural environments, the proportion of QS is reduced by nearly 50%.</li> <li>The only photo which elicited a relatively high proportion (36%) of quantifier spreading answers (Figure 8) is a picture of a fairly artificial-looking setup with remarkably few details:</li> <li>Decreasing the size of the extra object makes the object less salient; but increasing the number of the extra objects does not necessarily decrease their salience, and what is more, it is not clear why an increase in the salience of the extra object should result in the increased frequency of quantifier spreading responses.</li> <li>Misleading ostensive effect in other types of acquisition experiments An example: A test of exhaustivity</li> <li>t has been tested in several experiments (e.g., Beaver &amp; Onea 2011; Kas &amp; Luk\u00e1cs 2013; Pint\u00e9r 2016) whether the exhaustivity of the preverbal focus of the Hungarian sentence (corresponding roughly to an English cleft constituent) is an inherent semantic property or a cancellable pragmatic implicature</li> <li>The tasks involved truth value judgements; experimenters aimed to find out whether children and adults accept a focus construction like (11) as a true description of a non-exhaustive situation like that in Figure 11 (both cited from Pint\u00e9r 2016):</li> </ul>"},{"location":"KB/Quantifier%20spreading%20children%20misled%20by%20ostensive%20cues/#results_1","title":"Results","text":"<ul> <li>The rate of rejection of the sentences as true descriptions of the visually represented situations significantly correlated with the type of the visual representation.</li> <li>The sentence\u2013 drawing pairs were rejected in 10.53% of the cases.</li> <li>n the case of the sentence\u2013photo pairs, the rate of rejection was a mere 3.51%.</li> <li>Just as in Pint\u00e9r's (2016) experiment, the rate of rejection (i.e., the rate of the exhaustive interpretation of the sentences) was slightly even higher in the adult control group: 13.33% in the case of sentence\u2013drawing pairs, and 8.88% in the case of sentence\u2013photo pairs (see Figure 18). When we asked the subjects giving</li> <li>negative answers why e.g. (14) was not true of Figure 14, they consistently gave answers of the following type: \"Because the woman is also feeding the ducks\".</li> <li>15 children (39%) gave at least one negative answer</li> </ul>"},{"location":"KB/Quantifier%20spreading%20children%20misled%20by%20ostensive%20cues/#discussion_1","title":"Discussion","text":"<ul> <li>The sentences tested in this experiment involved no special linguistic or cognitive difficulty; they were simple declarative sentences with no quantification, let alone universal quantification; nevertheless, 10.53% of the preschoolers evaluated them as false descriptions of the drawings intended to represent them visually.</li> <li>Since this rate is not high (though it is comparable to the 12% of partial rejection obtained by Pint\u00e9r 2016 in this age group), we might be tempted to attribute it to noise (children's failure to pay attention, etc.)</li> <li>However, if the 10.53% rate of rejection had been due to noise, it would not have dropped to 3.51% when the visual stimuli were represented by photos.</li> <li>The comments of the children giving negative answers made it clear that they rejected the given sentence\u2013picture pair because the picture contained extra objects that were not repre-</li> <li>sented linguistically</li> <li>Crucially, the photos contained many more extra agents and extra objects than the drawings, still they elicited significantly less rejections.</li> <li>What made the presence of extra objects in the drawings ostensive was the minimality of the drawings, suggesting that everything irrelevant had been eliminated from them.</li> </ul>"},{"location":"KB/Quantifier%20spreading%20children%20misled%20by%20ostensive%20cues/#pictures","title":"Pictures","text":""},{"location":"KB/Quantifiers/","title":"Quantifiers","text":""},{"location":"KB/Quantifiers/#quantifiers","title":"Quantifiers","text":"<ul> <li>how many are identified (all, some, none) A type of word that indicates quantity</li> <li>Nominal</li> <li>Each, Every, All, Many, Some, No, Few, Most</li> <li>Adverbial</li> <li>Quantify over time: Always, Usually, Never, Rarely</li> <li>Quantify over space: Everywhere, Somewhere, Nowhere</li> <li>Others</li> <li>Modals are used to show that we believe something is certain, probable or possible</li> <li>Modal verbs: Can, Could, May, Might, Must</li> <li>Modal quantifiers: Perhaps, Necessarily, Maybe</li> </ul>"},{"location":"KB/Quantifying%20Uncertainty/","title":"Quantifying Uncertainty","text":""},{"location":"KB/Quantifying%20Uncertainty/#quantifying-uncertainty","title":"Quantifying Uncertainty","text":""},{"location":"KB/Quantile%20Bucketing/","title":"Quantile Bucketing","text":""},{"location":"KB/Quantile%20Bucketing/#quantile-bucketing","title":"Quantile Bucketing","text":"<ul> <li>Distributing a feature's values into buckets so that each bucket contains the same (or almost the same) number of examples.</li> </ul>"},{"location":"KB/Quantile%20Regression/","title":"Quantile Regression","text":""},{"location":"KB/Quantile%20Regression/#quantile-regression","title":"Quantile Regression","text":"<ul> <li>predict a quantile</li> <li>risk models</li> <li>fit a Heteroscedatic model and then estimating the quantile based on the predicted Normal Distribution</li> <li>or use Quantile loss</li> </ul>"},{"location":"KB/Quantile%20loss/","title":"Quantile loss","text":""},{"location":"KB/Quantile%20loss/#quantile-loss","title":"Quantile loss","text":"<p> - \\(\\alpha\\) is the quantile that needs to be predicted</p>"},{"location":"KB/Quantized%20Distillation/","title":"Quantized Distillation","text":""},{"location":"KB/Quantized%20Distillation/#quantized-distillation","title":"Quantized Distillation","text":"<ul> <li>Network quantization reduces the computation com- plexity of neural networks by converting high-precision networks (e.g., 32-bit floating point) into low-precision networks (e.g., 2-bit and 8-bit). Meanwhile, knowledge distillation aims to train a small model to yield a performance comparable to that of a complex model.</li> <li>Specifically, Polino et al. (2018) proposed a quan- tized distillation method to transfer the knowledge to a weight-quantized student network. In (Mishra and Marr, 2018), the proposed quantized KD is called the \u201cap- prentice\u201d. A high precision teacher network transfers knowledge to a small low-precision student network. To ensure that a small student network accurately mim- ics a large teacher network, the full-precision teacher network is first quantized on the feature maps, and then the knowledge is transferred from the quantized teacher to a quantized student network (Wei et al., 2018).</li> </ul>"},{"location":"KB/Quasi-static%20Clamping/","title":"Quasi static Clamping","text":""},{"location":"KB/Quasi-static%20Clamping/#quasi-static-clamping","title":"Quasi-static Clamping","text":"<ul> <li>A type of contact between a person and part of a robot system where the body part can be clamped between the moving part of the robot system &amp; another fixed or moving part of the robot cell</li> </ul>"},{"location":"KB/RACE/","title":"RACE","text":""},{"location":"KB/RACE/#race","title":"RACE","text":""},{"location":"KB/RAGAS%20-%20automated%20evaluation%20of%20RAG/","title":"RAGAS - automated evaluation of RAG","text":"","tags":["llm"]},{"location":"KB/RAGAS%20-%20automated%20evaluation%20of%20RAG/#ragas-automated-evaluation-of-rag","title":"RAGAS - Automated Evaluation of RAG","text":"","tags":["llm"]},{"location":"KB/RAGAS%20-%20automated%20evaluation%20of%20RAG/#ragas-automated-evaluation-of-retrieval-augmented-generation","title":"RAGAS: Automated Evaluation of Retrieval Augmented Generation","text":"<ul> <li>(Note : Well. I am not convinced. They did come up with some ways of testing it. But all of those ways used the LLM to test itself, which is uh. Not very nice or representative. I am unsure of how accurate any of these are.</li> <li>Logically it seems slightly flawed, but I respect the hustle)</li> </ul>","tags":["llm"]},{"location":"KB/RAGAS%20-%20automated%20evaluation%20of%20RAG/#ragas-retrieval-augmented-generation-assessment","title":"RAGAS (Retrieval Augmented Generation Assessment)","text":"<ul> <li>framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines</li> <li>evaluate these different dimensions without having to rely on ground truth human annotations</li> <li>While the usefulness of retrieval-augmented strategies is clear, their implementation requires a significant amount of tuning, as the overall performance will be affected by the retrieval model, the considered corpus, the LM, or the prompt formulation, among others</li> </ul>","tags":["llm"]},{"location":"KB/RAGAS%20-%20automated%20evaluation%20of%20RAG/#evaluation-strategies","title":"Evaluation Strategies","text":"<ul> <li>standard RAG setting</li> <li>given a question q, the system first retrieves some context c(q) and then uses the retrieved context to generate an answer as(q)</li> <li>(Note : This seems like an interesting start so far)</li> <li>we usually do not have access to human-annotated datasets or reference answers</li> </ul>","tags":["llm"]},{"location":"KB/RAGAS%20-%20automated%20evaluation%20of%20RAG/#faithfulness","title":"Faithfulness","text":"<ul> <li>answer should be grounded in the given context</li> <li>important to avoid hallucinations, and to ensure that the retrieved context can act as a justification for the generated answer</li> <li>RAG systems are often used in applications where the factual consistency of the generated text w.r.t. the grounded sources is highly important</li> <li></li> <li>(Note : Basically they use an LLM to generate an answer and then check with the ground truth to see how closely it matched by comparing the number of matches/total number of statements)</li> </ul>","tags":["llm"]},{"location":"KB/RAGAS%20-%20automated%20evaluation%20of%20RAG/#answer-relevance","title":"Answer Relevance","text":"<ul> <li>generated answer should address the actual question that was provided</li> <li>(Note : Basically uses cosine similarity. I am unsure of how different it is to that usual llm but oh well) </li> </ul>","tags":["llm"]},{"location":"KB/RAGAS%20-%20automated%20evaluation%20of%20RAG/#context-relevance","title":"Context Relevance","text":"<ul> <li>retrieved context should be focused, containing as little irrelevant information as possible</li> <li>gpt-3.5-turbo-16k model</li> <li>penalise the inclusion of redundant information.</li> <li>(Note : -.-, they just asked the LLM to find the most important sentences and then divided it by the total number of sentences. I am now progressively getting more and more annoyed with this paper)</li> </ul>","tags":["llm"]},{"location":"KB/RAHP/","title":"RAHP","text":""},{"location":"KB/RAHP/#rahp","title":"RAHP","text":"<ul> <li>Recall at high precision</li> </ul>"},{"location":"KB/RANSAC/","title":"RANSAC","text":""},{"location":"KB/RANSAC/#ransac","title":"RANSAC","text":"<ul> <li>It is an iterative method to estimate parameters of a mathematical model from a set of observed data</li> <li>A simple example is fitting a line to a set of observations.</li> <li>Outliers are points that don't \"fit\" the model and points that do fit are called \"inliers\"</li> <li>Table detection<ul> <li>The algorithm starts by generating plane hypotheses based on three unique points.</li> <li>For each plane hypothesis, distances from all points in the point cloud to the plane are computed.</li> <li>The plane hypotheses are then scored based on counting the number of inlier points, e.g., distance to the plane \uf8ff 20mm.</li> <li></li> </ul> </li> <li>The RANSAC algorithm is repeated for a certain number of iterations, e.g., n = 200.</li> <li>Object detection<ul> <li>It is now possible to extract the points which lie directly above it.</li> <li>By removing the table, we have a point cloud where all the objects that are on top of the table are included.</li> <li>The obtained point cloud is then segmented into individual clusters Each small group of points will be treated as an object candidate.</li> <li></li> </ul> </li> <li></li> </ul>"},{"location":"KB/RETAIn/","title":"RETAIn","text":""},{"location":"KB/RETAIn/#retain","title":"RETAIn","text":"<ul> <li>REverse Time AttentIoN mechanism</li> <li>The approach mimics physician practice by attending to the EHR data. Two RNNs are trained in a reverse time order with the goal of efficiently generating the appropriate attention variables. It is based on a two-level neural attention generation process that detects influential past visits and significant clinical variables to improve accuracy and interpretability.</li> <li>for application to Electronic Health Record (EHR) data.</li> </ul>"},{"location":"KB/RETRO/","title":"RETRO","text":""},{"location":"KB/RETRO/#retro","title":"RETRO","text":"<ul> <li>Improving Language Models by Retrieving from Trillions of Tokens</li> <li>Retrieval-Enhanced Transformer</li> <li>RETRO</li> <li>enhances auto-regressive language models by conditioning on document chunks retrieved from a large corpus</li> <li>based on local similarity with preceding tokens</li> <li>comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25x fewer parameters</li> <li>frozen BERT retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training</li> <li>Wikitext103</li> <li>Pile</li> <li>improving semi-parametric language models through explicit memory can provide an orthogonal, more efficient approach than raw parameter scaling as they seek to build more powerful language models</li> </ul>"},{"location":"KB/RICAP/","title":"RICAP","text":""},{"location":"KB/RICAP/#ricap","title":"RICAP","text":"<ul> <li>@takahashiDataAugmentationUsing2020</li> <li>Random image KB/Cropping.md and patching</li> <li>KB/Cropping.md four regions from randomly sampled images and augmenting them to create a new image.</li> <li>The generated image has mixed labels proportional to the pasted area</li> <li>The area of cropped regions in the output image is determined by sampling through uniform distribution</li> <li>anywhere-RICAP (origin can be anywhere), center-RICAP (origin can only be in the middle of the image), and corner-RICAP (origin can only be in corners</li> <li>Corner-RICAP has shown the best performance because a larger region of one image is visible to the network to learn</li> </ul>"},{"location":"KB/RISE/","title":"RISE","text":""},{"location":"KB/RISE/#rise","title":"RISE","text":"<ul> <li>@petsiukRISERandomizedInput2018</li> <li>based on a stochastic approach</li> <li>Input images are iteratively altered via random noise, and the final saliency map is composed by accumulating the partial estimations</li> <li>However, its application requires much more computational power, as it needs to run hundreds of thousands of prediction cycles</li> <li>it seems that RISE is not able to highlight regions of interest of skin lesion images with the same reliability as on pictures of real-world objects</li> <li>In the first step, it creates a segmentation mask and applies it to the dermoscopic image. Secondly, it creates a structure segmentation mask to identify the structure of the dermoscopic image. After masking, the original segmented image and some nonvisual metadata are fed into a convolutional neural network for classification</li> </ul>"},{"location":"KB/ROC%20Curve/","title":"ROC Curve","text":""},{"location":"KB/ROC%20Curve/#roc-curve","title":"ROC Curve","text":"<ul> <li>appropriate when the data is balanced</li> <li>x-axis : False Positive</li> <li>y-axis : True Positive</li> <li>area under the curve (AUC) can be used as a summary of the model performance<ul> <li>assign a higher probability to a randomly chosen real positive occurrence than a negative occurrence on average</li> </ul> </li> <li>Interpretation<ul> <li>Smaller values on the x-axis of the plot indicate lower false positives and higher true negatives.</li> <li>Larger values on the y-axis of the plot indicate higher true positives and lower false negatives.</li> </ul> </li> </ul>"},{"location":"KB/Rabobank/","title":"Rabobank","text":"","tags":["jobs"]},{"location":"KB/Rabobank/#subhaditya-mukherjee-rabobank-ai-strategist-application","title":"Subhaditya Mukherjee : Rabobank - AI Strategist Application","text":"<p>Hello Ante,</p> <p>It's nice to meet you!\u00a0 My name is Subhaditya, I recently graduated with a masters in AI from the RUG (Groningen), and am looking for my next big challenge. I found the position a good fit for what I can offer and what I want to do next, and so this is my formal application for the AI Strategist position.</p> <p>Over the past few years, I have worked with many clients (both freelance/part-time and in internships) and helped them bring their AI projects to life. I have a mix of experience, ranging from consulting to building technical AI projects across domains. I believe that this experience makes me a good fit for this position, as not only can I map out the steps needed to arrive at an AI based product, I can also break it down across multiple levels of understanding.\u00a0 I have been working with AI for a long time now, and am quite familiar with the stack - from LLMs to GenAI to other advanced AI models. I also have an understanding of reinforcement learning and am pretty proficient in Python.\u00a0</p> <p>I understand just how deeply important ethically future proofing the baking system is. The world is moving ahead with AI, and if we do not keep up with the trend, it will move ahead without us. I hope to be part of that change at Rabobank, and help co-create strategies that help the workforce in the long run.</p> <p>Hope to hear from you soon :)</p>","tags":["jobs"]},{"location":"KB/RadboudUMC/","title":"RadboudUMC","text":"","tags":["jobs"]},{"location":"KB/RadboudUMC/#fair-software-engineer-application","title":"FAIR Software Engineer Application","text":"<p>Hello! (I want to say James, but I am not sure)</p> <p>My name is Subhaditya, I recently graduated with a masters in AI from the RUG (Groningen), and am now looking for a job in the Netherlands. As a software engineer with an interest in healthcare and experience in AI, the FAIR RSE position seemed like the perfect next step for me, and I did not want to miss out on applying for it. Having been working on computer vision for a few years now, I am very aware of the need for better data management, especially for complex and privacy oriented workflows. I would love to bring this experience to DIAG, while also hopefully learning a lot from the team there too.</p> <p>Over the past few years, I have worked with many clients (both freelance/part-time and in internships) and helped them bring their AI and data projects to life. My experience includes working with large datasets, building ML/DL models and pipelines and also sharing my knowledge with my peers in technical and non technical fields. I have worked with AWS, Google Cloud and DICOM before as well. I am quite proficient in Python, CI/CD and integrating 3rd party APIs with existing codebases.\u00a0</p> <p>I have a masters in AI (with my thesis being on Explainable AI) and that also brought me\u00a0 experience with setting up training and also the FAIR principles. More importantly though, using AI in healthcare has been my motivation for many years now, and one of the reasons I got into the field in the first place. That being the case, I feel like I am in a better position compared to general software engineers in understanding what is required and why.</p> <p>This letter is getting a little long, but I did want to add that I would love to contribute to your team\u2019s mission of making Radboud UMC the best campus for AI research, and in turn helping a lot of researchers get their work done faster and more responsibly. I do hope you give me a shot :)</p> <p>Best,</p> <p>Subhaditya Mukherjee</p>","tags":["jobs"]},{"location":"KB/Radial%20Plot/","title":"Radial Plot","text":""},{"location":"KB/Radial%20Plot/#radial-plot","title":"Radial Plot","text":"<ul> <li>number of associated attributes limited  </li> <li>distinct distinguishable values per attribute limited  </li> <li>visual mappings must be learned for correct interpretation</li> </ul>"},{"location":"KB/Ramp%20up%20problem/","title":"Ramp up problem","text":""},{"location":"KB/Ramp%20up%20problem/#ramp-up-problem","title":"Ramp up Problem","text":"<ul> <li>No data for a user -&gt; picked one</li> <li>Broad generalization</li> </ul>"},{"location":"KB/RandAugment/","title":"RandAugment","text":""},{"location":"KB/RandAugment/#randaugment","title":"RandAugment","text":"<ul> <li>Randaugment: Practical automated data augmentation with a reduced search space</li> <li>Ekin D. Cubuk \u2217, Barret Zoph\u2217, Jonathon Shlens, Quoc V. Le</li> </ul>"},{"location":"KB/RandAugment/#chatgpt-summary","title":"ChatGPT Summary","text":"<ul> <li>Large-scale adoption of data augmentation methods is hindered by the need for a separate and expensive search phase.</li> <li>Commonly, a smaller proxy task is used to overcome the expense of the search phase, but it is not clear if the optimized hyperparameters found on the proxy task are also optimal for the actual task.</li> <li>The process of designing automated augmentation strategies is being rethought.</li> <li>It is proposed to only search for a single distortion magnitude that jointly controls all operations, which reduces computational expense and eliminates the need for a separate proxy task.</li> <li>The proposed method was tested on various datasets including [CIFAR]-10, CIFAR 100, SVHN, ImageNet, and COCO, and showed improvement in performance without the use of a proxy task.</li> <li>The proposed method, RandAugment, uses a parameter-free procedure of always selecting a transformation with uniform probability from a set of K=14 available transformations, and a single distortion magnitude that jointly controls all operations.</li> <li>RandAugment is able to achieve comparable or better performance compared to other automated augmentation methods, such as AutoAugment, without the need for a separate proxy task.</li> <li>The results suggest that the optimal data augmentation policies may depend on the specific model and dataset size, and a small proxy task may not provide the best indicator of performance on a larger task.</li> </ul>"},{"location":"KB/RandAugment/#abstract","title":"Abstract","text":"<ul> <li>An obstacle to a large-scale adoption of these methods is that they require a separate and expensive search phase</li> <li>A common way to overcome the expense of the search phase was to use a smaller proxy task.</li> <li>However, it was not clear if the optimized hyperparameters found on the proxy task are also optimal for the actual task.</li> <li>rethink the process of designing automated augmentation strategies</li> <li>it is sufficient to only search for a single distortion magnitude that jointly controls all operations</li> <li>propose a simplified search space that vastly reduces the computational expense of automated augmentation, and permits the removal of a separate proxy task.</li> <li>CIFAR-10</li> <li>CIFAR 100</li> <li>SVHN</li> <li>ImageNet</li> <li>COCO datasets</li> </ul>"},{"location":"KB/RandAugment/#systematic-failures-of-a-separate-proxy-task","title":"Systematic Failures of a Separate Proxy Task","text":"<ul> <li>A central premise of learned data augmentation is to construct a small, proxy task that may be reflective of a larger task</li> <li>Although this assumption is sufficient for identifying learned augmentation policies to improve performance, it is unclear if this assumption is overly stringent and may lead to sub-optimal data augmentation policies.</li> <li>two separate dimensions that are commonly restricted to achieve a small proxy task: model size and dataset size</li> <li>First, we train a family of Wide-ResNet architectures, where the model size may be systematically altered through the widening parameter governing the number of convolutional filters</li> <li>For each of these networks, we train the model on CIFAR-10 and measure the final accuracy compared to a baseline model trained with default data augmentations (i.e. horizontal flips and pad-and-crop)</li> <li>The Wide-ResNet models are trained with the additional K=14 data augmentations (see Section 3) over a range of global distortion magnitudes M parameterized on a uniform linear scale ranging from [0, 30]</li> <li>Namely, larger networks demand larger data distortions for KB/Regularization.md</li> <li>Conversely, a policy learned on a proxy task (such as AutoAugment) provides a fixed distortion magnitude (Figure 1b, dashed line) for all architectures that is clearly sub-optimal.</li> <li>A second dimension for constructing a small proxy task is to train the proxy on a small subset of the training data</li> <li>We first observe that models trained on smaller training sets may gain more improvement from data augmentation</li> <li>see that the optimal distortion magnitude is larger for models that are trained on larger datasets.</li> <li>optimal distortion magnitude increases monotonically with training set size</li> <li>One hypothesis for this counter-intuitive behavior is that aggressive data augmentation leads to a low signal-to-noise ratio in small datasets</li> <li>learned augmentation may learn an augmentation strength more tailored to the proxy task instead of the larger task of interest.</li> <li>The dependence of augmentation strength on the dataset and model size indicate that a small proxy task may provide a sub-optimal indicator of performance on a larger task.</li> </ul>"},{"location":"KB/RandAugment/#automated-data-augmentation-without-a-proxy-task","title":"Automated Data Augmentation without a Proxy Task","text":"<ul> <li>The reason we wish to remove the search phase is because a separate search phase significantly complicates training and is computationally expensive.</li> <li>In order to remove a separate search phase, we aspire to fold the parameters for the data augmentation strategy into the hyper-parameters for training a model.</li> <li>Indeed, previous work enumerated a policy in terms of choosing which transformations to apply out of K=14 available transformations, and probabilities for applying each transformation:</li> <li>age diversity, we replace the learned policies and probabilities for applying each</li> <li>transformation with a parameter-free procedure of always selecting a transformation with uniform probability \\(\\frac{1}{K}\\) </li> <li>Given N transformations for a training image, RandAugment may thus express \\(K^{N}\\) potential policies.</li> <li>magnitude of the each augmentation distortion.</li> <li>Briefly, each transformation resides on an integer scale from 0 to 10 where a value of 10 indicates the maximum scale for a given transformation</li> <li>A data augmentation policy consists of identifying an integer for each augmentation.</li> <li>and postulate that a single global distortion M may suffice for parameterizing all transformations</li> <li>We experimented with four methods for the schedule of M during training: constant magnitude, random magnitude, a linearly increasing magnitude, and a random magnitude with increasing upper bound</li> <li>The resulting algorithm contains two parameters N and M</li> <li>Both parameters are human-interpretable such that larger values of N and M increase KB/Regularization.md strength</li> <li>In order to reduce the parameter space but still maintain imInvestigating the dependence on the included transformations</li> <li>RandAugment is largely insensitive to the selection of transformations for different datasets.</li> <li>We see that while KB/Geometric Transformations.md individually make the most difference, some of the color transformations lead to a degradation of validation accuracy on average</li> <li>Surprisingly, rotate can significantly improve performance and lower variation even when included in small subsets of RandAugment transformations, while posterize seems to hurt all subsets of all sizes.</li> </ul>"},{"location":"KB/RandAugment/#learning-the-probabilities-for-selecting-image-transformations","title":"Learning the Probabilities for Selecting Image Transformations","text":"<ul> <li>For K=14 image transformations and N =2 operations, \u03b1ij constitutes 28 parameters. We initialize all weights such that each transformation is equal probability (i.e. RandAugment), and update these parameters based on how well a model classifies a held out set of validation images distorted by \u03b1ij.</li> <li>This approach was inspired by density matching [19], but instead uses a differentiable approach in lieu of Bayesian optimization.</li> <li>We label this method as a 1st-order density matching approximation.</li> <li>The 1st -order method improves accuracy by more than 3.0% for both models on reduced CIFAR-10 compared to the baseline of flips and pad-and-crop</li> <li>Although the density matching approach is promising, this method can be expensive as one must apply all K transformations N times to each image independently.</li> <li>Hence, because the computational demand of KN transformations is prohibitive for large images, we reserve this for future exploration.</li> <li>learning the probabilities through density matching may improve the performance on small-scale tasks and reserve explorations to larger-scale tasks for the future.</li> <li>RandAugment selects all image transformations with equal probability</li> <li>This opens up the question of whether learning K probabilities may improve performance further.</li> <li>Most of the image transformations (except posterize, equalize, and autoContrast) are differentiable, which permits backpropagation to learn the K probabilities</li> </ul>"},{"location":"KB/RandAugment/#discussion","title":"Discussion","text":"<ul> <li>not tailoring the number of distortions and the distortion magnitude to the dataset size nor the model size leads to sub-optimal performance</li> <li>In previous work, scaling learned data augmentation to larger dataset and models have been a notable obstacle. For example, [AutoAugment] and Fast AutoAugment could only be optimized for small models on reduced subsets of data</li> <li>The proposed method scales quite well to datasets such as ImageNet and COCO while incurring minimal computational cost (e.g. 2 hyper-parameters), but notable predictive performance gains.</li> </ul>"},{"location":"KB/RandAugment/#images","title":"Images","text":""},{"location":"KB/Random%20Directions/","title":"Random Directions","text":""},{"location":"KB/Random%20Directions/#random-directions","title":"Random Directions","text":"<ul> <li>1D<ul> <li> \\[ f(\\alpha, \\beta) = L(\\theta^{*}+ \\alpha \\delta) \\] </li> </ul> </li> <li>2D<ul> <li> \\[ f(\\alpha, \\beta) = L(\\theta^{*}+ \\alpha \\delta + \\beta \\eta) \\] </li> </ul> </li> <li>\\(\\theta, \\eta\\) are random direction vector and \\(\\theta^{*}\\) is optimal weights vector</li> </ul>"},{"location":"KB/Random%20Distortion/","title":"Random Distortion","text":""},{"location":"KB/Random%20Distortion/#random-distortion","title":"Random Distortion","text":"<ul> <li>The parameters of this distortion are given in terms of grid width, grid height, and magnitude. The granularity of the distortion is controlled by the grid width and height, representing the number of horizontal and vertical divisions to apply distortion to. Both values are chosen as 6, and the magnitude of the distortion is chosen as 5. As images are only 32\u00d732 pixels, distortion is expected to produce unrealistic examples.</li> </ul>"},{"location":"KB/Random%20Erasing/","title":"Random Erasing","text":""},{"location":"KB/Random%20Erasing/#random-erasing","title":"Random Erasing","text":"<ul> <li>@zhongRandomErasingData2020</li> <li>deletes contiguous rectangular image regions similar to KB/Cutout.md with minor differences in region selection procedure.</li> <li>Opposite to KB/Cutout.md, where deletion is applied on all the images, here it is performed with a probability of either applying it or not</li> <li>In every iteration, region size is defined randomly with upper and lower limits on region area and aspect ratio.</li> <li>Additional to this, random erasing provides region-aware deletion for object detection and person identification tasks</li> <li>Regions inside the object bounding boxes are randomly erased to generate occlusions</li> </ul>"},{"location":"KB/Random%20Factors/","title":"Random Factors","text":""},{"location":"KB/Random%20Factors/#random-factors","title":"Random Factors","text":"<ul> <li>These are factors that can affect the outcome that we do not design, and cannot control</li> <li>Two common types</li> <li>Participant<ul> <li>People differ. Some people are very strict. Some people accept everything.</li> </ul> </li> <li>Item<ul> <li>Some sentences are weird with monkeys. Some situations are hard to draw so the picture is a bit odd.</li> </ul> </li> <li>When we analyze the data we have to take into account that there will be variation present because of these random factors.</li> </ul>"},{"location":"KB/Random%20Pruning/","title":"Random Pruning","text":""},{"location":"KB/Random%20Pruning/#random-pruning","title":"Random Pruning","text":"<ul> <li>Each weight independantly considered and dropped with a fraction of network required</li> <li>For this we first take the number of values to prune by identifying the total size of the weights and then multiplying it by the fraction of values to remove.</li> </ul>"},{"location":"KB/Rank%20%28Tensor%29/","title":"Rank (Tensor)","text":""},{"location":"KB/Rank%20%28Tensor%29/#rank-tensor","title":"Rank (Tensor)","text":"<ul> <li>The number of dimensions in a Tensor. For instance, a scalar has rank 0, a vector has rank 1, and a matrix has rank 2.</li> </ul>"},{"location":"KB/Rapid%20Eye%20Movement%20%28REM%29%20Sleep/","title":"Rapid Eye Movement (REM) Sleep","text":""},{"location":"KB/Rapid%20Eye%20Movement%20%28REM%29%20Sleep/#rapid-eye-movement-rem-sleep","title":"Rapid Eye Movement (REM) Sleep","text":"<ul> <li>A stage of sleep occurring approximately 90 minutes after sleep onset characterized by increased brain activity, rapid eye movements, and muscle relaxation.</li> </ul>"},{"location":"KB/Rational%20inference/","title":"Rational inference","text":""},{"location":"KB/Rational%20inference/#rational-inference","title":"Rational Inference","text":"<ul> <li>Uses all Available cues</li> </ul>"},{"location":"KB/Raycasting/","title":"Raycasting","text":""},{"location":"KB/Raycasting/#raycasting","title":"Raycasting","text":"<ul> <li>Volume Rendering Equation</li> <li>Back To Front Raycasting</li> <li>Color Compositing</li> <li>Front to Back Raycasting</li> <li>Sampling Ray Casting</li> <li>Classification Ray Casting</li> <li>Slice Based Volume Rendering</li> <li>Voxel Projection</li> </ul>"},{"location":"KB/ReMix/","title":"ReMix","text":""},{"location":"KB/ReMix/#remix","title":"ReMix","text":"<ul> <li>@caoReMixImagetoImageTranslation2021</li> <li>addresses the issue of class imbalance by generating mixed images for minority classes.</li> <li>in the case of label assignment, it sets the label of the output image to the minority class.</li> </ul>"},{"location":"KB/Real%20Time%20Image%20Saliency%20for%20Black%20Box%20Classifiers/","title":"Real Time Image Saliency for Black Box Classifiers","text":""},{"location":"KB/Real%20Time%20Image%20Saliency%20for%20Black%20Box%20Classifiers/#real-time-image-saliency-for-black-box-classifiers","title":"Real Time Image Saliency for Black Box Classifiers","text":"<ul> <li>Piotr Dabkowski</li> <li>Yarin Gal</li> <li>@dabkowskiRealTimeImage2017</li> </ul>"},{"location":"KB/Real%20Time%20Image%20Saliency%20for%20Black%20Box%20Classifiers/#tldr","title":"TL;DR","text":"<ul> <li>New metric to judge how good a saliency map is using the largest rectangle that can define it. Training to reduce adversarial artefacts introduced due to masking with non smooth masks</li> </ul>"},{"location":"KB/Real%20Time%20Image%20Saliency%20for%20Black%20Box%20Classifiers/#abstract","title":"Abstract","text":"<ul> <li>fast saliency detection method that can be applied to any differentiable image classifier</li> <li>masking model</li> <li>manipulate the scores of the classifier by masking salient parts of the input image</li> <li>requires a single forward pass to perform saliency detection</li> <li>CIFAR</li> <li>ImageNet</li> <li>new metric for saliency</li> </ul>"},{"location":"KB/Real%20Time%20Image%20Saliency%20for%20Black%20Box%20Classifiers/#image-saliency-and-introduced-evidence","title":"Image Saliency and Introduced Evidence","text":"<ul> <li>no single obvious metric that could measure the quality of the produced map</li> <li>In simple terms, the saliency map is defined as a summarised explanation of where the classifier \"looks\" to make its prediction.</li> <li></li> <li>SSR</li> <li>SDR</li> <li>In order to be as informative as possible we would like to find a region that performs well as both SSR and SDR.</li> <li>Both SDR and SSR remove some evidence from the image</li> <li>there are few ways of removing evidence, for example by blurring the evidence, setting it to a constant colour, adding noise, or by completely KB/Cropping.md out the unwanted parts</li> <li>Unfortunately, each one of these methods introduces new evidence that can be used by the classifier as a side effec</li> </ul>"},{"location":"KB/Real%20Time%20Image%20Saliency%20for%20Black%20Box%20Classifiers/#fighting-the-introduced-evidence","title":"Fighting the Introduced Evidence","text":"<ul> <li>by manipulating the image we always introduce some extra evidence applying a mask M to the image X to obtain the edited image E</li> <li>the simplest case we can simply multiply X and M element-wise:</li> <li> \\[E = X \\odot M\\] </li> <li>This operation sets certain regions of the image to a constant \"0\" colour</li> <li>While setting a larger patch of the image to \"0\" may sound rather harmless (perhaps following the assumption that the mean of all colors carries very little evidence), we may encounter problems when the mask M is not smooth</li> <li>in the worst case, can be used to introduce a large amount of additional evidence by generating adversarial artifacts</li> <li>Adversarial artifacts generated by the mask are very small in magnitude and almost imperceivable for humans, but they are able to completely destroy the original prediction of the classifier</li> <li>we may change the way we apply a mask to reduce the amount of unwanted evidence due to specifically-crafted masks</li> <li> \\[E = X \\odot M + A \\odot (1-M)\\] </li> <li>where A is an alternative image</li> <li>A can be chosen to be for example a highly blurred version of X</li> <li>In such case mask M simply selectively adds blur to the image X and therefore it is much harder to generate high-frequency-high-evidence artifacts</li> <li>Unfortunately, applying blur does not eliminate existing evidence very well, especially in the case of images with low spatial frequencies like a seashore or mountains.</li> <li>Another reasonable choice of A is a random constant colour combined with highfrequency noise. This makes the resulting image E more unpredictable at regions where M is low and therefore it is slightly harder to produce a reliable artifact.</li> <li>encourage smoothness of the mask M for example via a total variation (TV) penalty</li> <li>We can also directly resize smaller masks to the required size as resizing can be seen as a smoothness mechanism.</li> </ul>"},{"location":"KB/Real%20Time%20Image%20Saliency%20for%20Black%20Box%20Classifiers/#a-new-saliency-metric","title":"A New Saliency Metric","text":"<ul> <li>In order to make sure that the preserved region is free from adversarial artifacts instead of masking we can crop the image.</li> <li>We propose to find the tightest rectangular crop that contains the entire salient region and to feed that rectangular region to the classifier to directly verify whether it is able to recognise the requested class</li> <li> \\[s(a,p) = log(\\overset{\\sim}a)- log(p)\\] </li> <li>\\(\\overset{\\sim}a = max(a, 0.05)\\)</li> <li>Here a is the area of the rectangular crop as a fraction of the total image size and p is the probability of the requested class returned by the classifier based on the cropped region.</li> <li>The metric is almost a direct translation of the SSR</li> <li>We threshold the area at 0.05 in order to prevent instabilities at low area fractions.</li> <li>Good saliency detectors will be able to significantly reduce the crop size without reducing the classification probability, and therefore a low value for the saliency metric is a characteristic of good saliency detectors.</li> <li>this measure can be seen as the relative amount of information between an indicator variable with probability p and an indicator variable with probability a\u2014or the concentration of information in the cropped region.</li> <li>Because most image classifiers accept only images of a fixed size and the crop can have an arbitrary size, we resize the crop to the required size disregarding aspect ratio</li> </ul>"},{"location":"KB/Real%20Time%20Image%20Saliency%20for%20Black%20Box%20Classifiers/#the-saliency-objective","title":"The Saliency Objective","text":"<ul> <li>want to find a mask M that is smooth and performs well at both SSR([SSR].md) and SDR</li> <li>given class c of interest, and an input image X, to find a saliency map M for class c, our objective function L is given by</li> <li> \\[L(M) = \\lambda_{1}TV(M) + \\lambda_{2}AV(M) - log(f_{c}(\\Phi(X,M)))+\\lambda_{3}f_{c}(\\Phi(X, 1-M))^{\\lambda_{4}}\\] </li> <li>fc is a softmax probability of the class c of the black box image classifier and TV(M) is the total variation of the mask defined simply as</li> <li> \\[TV(M) = \\Sigma_{i,j}(M_{ij}-M_{ij+1})^{2}+ \\Sigma_{ij}(M_{ij}-M_{i+1j})^{2}\\] </li> <li>AV(M) is the average of the mask elements, taking value between 0 and 1, \\(\\lambda_{i}\\) are regularisers</li> <li>function \\(\\Phi\\) removes the evidence from the image as introduced in the previous section</li> <li> \\[\\Phi(X, M) = X \\odot M + A \\odot (1-M)\\] </li> <li>In total, the objective function is composed of 4 terms. The first term enforces mask smoothness, the second term encourages that the region is small. The third term makes sure that the classifier is able to recognise the selected class from the preserved region. Finally, the last term ensures that the probability of the selected class, after the salient region is removed, is low</li> <li>Setting \\(\\lambda_{4}\\) to a value smaller than 1 (e.g. 0.2) helps reduce this probability to very small values.</li> </ul>"},{"location":"KB/Real%20Time%20Image%20Saliency%20for%20Black%20Box%20Classifiers/#masking-model","title":"Masking Model","text":"<ul> <li>The mask can be found iteratively for a given image-class pair by directly optimising the objective function</li> <li>Unfortunately, iteratively finding the mask is not only very slow, as normally more than 100 iterations are required, but it also causes the mask to greatly overfit to the image and a large TV penalty is needed to prevent adversarial artifacts from forming</li> <li>Therefore, the produced masks are blurry, imprecise, and overfit to the specific</li> <li>image rather than capturing the general behaviour of the classifie</li> <li>develop a trainable masking model that can produce the desired masks in a single forward pass without direct access to the image classifier after training</li> <li>The masking model receives an image and a class selector as inputs and learns to produce masks that minimise our objective function</li> <li>In order to succeed at this task, the model must learn which parts of the input image are considered salient by the black box classifier</li> <li>n theory, the model can still learn to develop adversarial masks that perform well on the objective function, but in practice it is not an easy task, because the model itself acts as some sort of a \"regulariser\" determining which patterns are more likely and which are less.</li> <li>Unet Architecture</li> <li>so that the masking model can use feature maps from multiple resolutions</li> <li>The ResNet-50 model contains feature maps of five different scales, where each subsequent scale block downsamples the input by a factor of two</li> <li>The purpose of the feature filter is to attenuate spatial locations which contents do not correspond to the selected class.</li> <li>Therefore, the feature filter performs the initial localisation, while the following upsampling blocks fine-tune the produced masks</li> <li>The output of the feature filter Y at spatial location i, j is given by:</li> <li> \\[Y_{ij}= X_{ij}\\sigma(X_{ij}^T C_{s})\\] </li> <li>Xij is the output of the Scale 5 block at spatial location i, j; Cs is the embedding of the selected class s and \\(\\sigma(\\cdot)\\) is the sigmoid nonlinearity. Class embedding C can be learned as part of the overall objective.</li> <li>The upsampler blocks take the lower resolution feature map as input and upsample it by a factor of two using transposed convolution</li> <li>afterwards they concatenate the upsampled map with the corresponding feature map from ResNet and follow that with three bottleneck blocks</li> <li>Finally, to the output of the last upsampler block (Upsampler Scale 2) we apply 1x1 convolution to produce a feature map with with just two channels</li> <li>The mask Ms is obtained from</li> <li> \\[M_{s}= \\frac{abs(C_{o})}{abs(C_{o})+ abs(C_{1})}\\] </li> <li>We use this nonstandard nonlinearity because sigmoid and tanh nonlinearities did not optimise properly and the extra degree of freedom from two channels greatly improved training</li> </ul>"},{"location":"KB/Real%20Time%20Image%20Saliency%20for%20Black%20Box%20Classifiers/#training-process","title":"Training Process","text":"<ul> <li>train the masking model to directly minimise the objective function</li> <li>he weights of the pre-trained ResNet encoder (red blocks in figure 4) are kept fixed</li> <li>during the training.</li> <li>sometimes supply a class selector for a fake class and to apply only the area penalty term of the objective function.</li> <li>Under this setting the model must pay attention to the class selector, as the only way it can reduce loss in case of a fake label is by setting the mask to zero</li> <li>During training, we set the probability of the fake label occurrence to 30%</li> <li>One can also greatly speed up the embedding training by ensuring that the maximal value of \\(\\sigma(X_{ij}^{T}C_{s})\\) from equation 7 is high in case of a correct label and low in case of a fake label.</li> <li>evidence removal function \\(\\Phi(X,M)\\)</li> <li>In order to prevent the model from adapting to any single evidence removal scheme the alternative image A is randomly generated every time the function is called</li> <li>In 50% of cases the image A is the blurred version of X (we use a Gaussian blur with = \\(\\sigma=10\\) to achieve a strong blur) and in the remainder of cases, A is set to a random colour image with the addition of a Gaussian noise.</li> <li>Such a random scheme greatly improves the quality of the produced masks as the model can no longer make strong assumptions about the final look of the image.</li> <li>ImageNet</li> <li>three different black-box classifiers: AlexNet [6], GoogLeNet [15] and ResNet-50 [4]</li> <li>These models are treated as black boxes</li> <li>The selected parameters of the objective function are \\(\\lambda_{1} = 10, \\lambda_{2} = 103, \\lambda_{3} = 5, \\lambda_{4} = 0.3\\)</li> <li>The first upsampling block has 768 output channels and with each subsequent upsampling block we reduce the number of channels by a factor of two. We train each masking model as described in section 4.1 on 250,000 images from the ImageNet training set.</li> <li>The masks produced by models trained on GoogLeNet and ResNet are sharp and precise and would produce accurate object segmentations. The saliency model trained on AlexNet produces much stronger and slightly larger saliency regions, possibly because AlexNet is a less powerful model which needs more evidence for successful classification.</li> </ul>"},{"location":"KB/Real%20Time%20Image%20Saliency%20for%20Black%20Box%20Classifiers/#results","title":"Results","text":""},{"location":"KB/Real%20Time%20Image%20Saliency%20for%20Black%20Box%20Classifiers/#future-research","title":"Future Research","text":"<ul> <li>modifying the approach to produce high quality, weakly supervised, image segmentations</li> <li>Moreover, because our model can be run in real-time, it can be used for video saliency detection to instantly explain decisions made by black-box classifiers such as the ones used in autonomous vehicles</li> <li>Lastly, our model might have biases of its own \u2014 a fact which does not seem to influence the model performance in finding biases in other black boxes according to the various metrics we used</li> </ul>"},{"location":"KB/Real%20Time%20Image%20Saliency%20for%20Black%20Box%20Classifiers/#images","title":"Images","text":""},{"location":"KB/Reasoning%20Component/","title":"Reasoning Component","text":""},{"location":"KB/Reasoning%20Component/#reasoning-component","title":"Reasoning Component","text":"<ul> <li>updates the world model and determines plans to achieve goals.</li> </ul>"},{"location":"KB/Recall/","title":"Recall","text":""},{"location":"KB/Recall/#recall","title":"Recall","text":"<ul> <li> \\[\\frac{TP}{TP+FN}\\] </li> <li>Model needs to remember the features such that it does not miss-classify a positive case as a negative one. -&gt; How many positive samples does the model remember?</li> <li># positive classes correctly predicted / total # positive classes</li> </ul>"},{"location":"KB/Receptive%20field/","title":"Receptive field","text":""},{"location":"KB/Receptive%20field/#receptive-field","title":"Receptive Field","text":"<ul> <li>Not for Dense, only local connected layers like Conv, Pooling</li> <li>A neuron's receptive field is the patch of the total field of view that a single neuron has access to</li> <li>Almost a logarithmic relationship between classification accuracy and receptive field size<ul> <li>Large fields are almost necessary for high level recognition tasks, but with diminishing rewards</li> </ul> </li> <li> \\[r(i-1) = s_{i}\\times r_{i} + (k_{i}-s_{i})\\] </li> <li></li> <li>Recursive<ul> <li> \\[r_{0} = \\Sigma^{L}_{i=1}((k_{i}-1)\\Pi_{j=1}^{l-1}s_{j})+1\\] </li> </ul> </li> <li>How to increase receptive field<ul> <li>Add more Conv</li> <li>Add pooling or higher stride</li> <li>Causal Dilated Conv</li> <li>Depthwise Separable</li> </ul> </li> </ul>"},{"location":"KB/Recessive/","title":"Recessive","text":""},{"location":"KB/Recessive/#recessive","title":"Recessive","text":"<ul> <li>A genetic trait or disease that appears only in patients who have received two copies of a mutant gene, one from each parent.</li> </ul>"},{"location":"KB/Recipe%20for%20constructing%20loss%20functions/","title":"Recipe for constructing loss functions","text":""},{"location":"KB/Recipe%20for%20constructing%20loss%20functions/#recipe-for-constructing-loss-functions","title":"Recipe for constructing loss functions","text":"<ul> <li>Using Maximum Likelihood</li> <li>For training data \\({x_{i}, y_{i}}\\)<ul> <li>Choose a probability distribution \\(Pr{y|\\theta}\\) defined over the domain of the predictions y with distribution parameters \\(\\theta\\)</li> <li>Choose an ML model \\(f|x, \\phi|\\) where \\(\\theta = f|x, \\phi|\\) and \\(Pr(y|\\theta) = Pr(y|f|x, \\phi|)\\) </li> <li>Training -&gt; Find the parameters \\(\\phi\\) that minimize the Negative Log Likelihood over the training data \\({x_{i}, y_{i}}\\) </li> <li>Inference -&gt; Either return \\(Pr(y|f[x, \\hat \\phi])\\) or the value where this distribution is minimized</li> </ul> </li> </ul>"},{"location":"KB/Recommender%20System/","title":"Recommender System","text":""},{"location":"KB/Recommender%20System/#recommender-system","title":"Recommender System","text":"<ul> <li>Group Modeling Approach</li> <li>Individual Modeling</li> <li>Collaborative Recommender</li> <li>Content Based Recommender</li> </ul>"},{"location":"KB/Reconstruction%20loss/","title":"Reconstruction loss","text":""},{"location":"KB/Reconstruction%20loss/#reconstruction-loss","title":"Reconstruction loss","text":"<ul> <li>The second loss function in capsule networks is called \u201creconstruction loss.\u201d This loss function ensures the network can reconstruct an object from its lower-level features. </li> <li>This is accomplished by training the network to reconstruct an image from the output of the capsule layers.</li> </ul>"},{"location":"KB/Recurrent/","title":"Recurrent","text":""},{"location":"KB/Recurrent/#recurrent","title":"Recurrent","text":"<ul> <li>Sequences as inputs/outputs</li> <li>Sequential processing</li> <li>Turing complete</li> <li>memory through state persisted between timesteps<ul> <li>operation invariant to the sequence</li> <li>reduces no of params needed</li> </ul> </li> <li>Output comes back as input<ul> <li></li> </ul> </li> <li>variable sized inputs and outputs : encoder decoder</li> <li>Three weight matrices and two bias vectors.</li> <li> \\[h_t = \\sigma_h(W_{hh}h_{t-1} + W_{xh}x_t + b_h)\\] </li> <li> \\[y_t = \\sigma_y(W_{hy}h_t + b_y)\\] </li> <li>Stateful : hidden state kept across batches of inputs</li> <li>Activation usually Sigmoid or Tanh</li> <li>BPTT<ul> <li></li> <li> </li> </ul> </li> <li>Training stuff<ul> <li>Softmax but on every output vector simultaneously<ul> <li>If  is lower (eg between 0 and 0.5). It becomes more confident and hence more conservative</li> <li>Near 0 is very diverse and less confident</li> </ul> </li> <li>Feed a char into the RNN -&gt; distribution over characters that comes next -&gt; Sample from it -&gt; Feed it back</li> </ul> </li> <li>Some basic patterns from here<ul> <li>The model first discovers the general word-space structure and then rapidly starts to learn the words.</li> <li>First starting with the short words and then eventually the longer ones.</li> <li>Topics and themes that span multiple words (and in general longer-term dependencies) start to emerge only much later.</li> </ul> </li> </ul>"},{"location":"KB/Recurrent/#gradients","title":"gradients","text":"<ul> <li>If eigen decomposition \\(\\(W = Q\\wedge^tQ\\)\\), then \\(\\(h_t = Q^T\\wedge^tQ\\)\\)</li> <li>If less than 0 then will converge to 0 or if bigger then will explore to infinity -&gt; long sequences</li> <li>Element wise clipping #tricks<ul> <li>Clip if bigger than value</li> </ul> </li> <li>Norm clipping<ul> <li>Clip if $\\(||g|| &gt;v\\) set \\(g = \\frac{gv}{||g||}\\)</li> <li>v can be decided by trial and error</li> </ul> </li> </ul>"},{"location":"KB/Redress/","title":"Redress","text":""},{"location":"KB/Redress/#redress","title":"Redress","text":"<ul> <li>includes mechanisms that ensure an adequate redress for situations when unforeseen unjust adverse impacts take place.</li> <li>Guaranteeing a redress for those non-predicted scenarios is a key to ensure trust. Special attention should be paid to vulnerable persons or groups.</li> </ul>"},{"location":"KB/Reflex%20Hammer/","title":"Reflex Hammer","text":""},{"location":"KB/Reflex%20Hammer/#reflex-hammer","title":"Reflex Hammer","text":"<ul> <li>A specially designed hammer used to test deep tendon or motor reflexes</li> </ul>"},{"location":"KB/RegNet/","title":"RegNet","text":""},{"location":"KB/RegNet/#regnet","title":"RegNet","text":"<ul> <li>network design paradigm</li> <li>discover design principles that generalize across settings</li> <li>arrives at a low-dimensional design space consisting of simple, regular networks</li> <li>widths and depths of good networks can be explained by a quantized linear function</li> <li>outperforms EfficientNet</li> </ul>"},{"location":"KB/Regex%20cheatsheet/","title":"Regex cheatsheet","text":"","tags":["cheatsheets"]},{"location":"KB/Regex%20cheatsheet/#regex-cheatsheet","title":"Regex Cheatsheet","text":"","tags":["cheatsheets"]},{"location":"KB/Regex%20cheatsheet/#dictionary-replace","title":"Dictionary Replace","text":"","tags":["cheatsheets"]},{"location":"KB/Regex%20cheatsheet/#match-before-or-after-a-character","title":"Match before or after a Character","text":"","tags":["cheatsheets"]},{"location":"KB/Regex%20cheatsheet/#specific-types","title":"Specific Types","text":"","tags":["cheatsheets"]},{"location":"KB/Regex%20cheatsheet/#ranges-of-characters","title":"Ranges of Characters","text":"","tags":["cheatsheets"]},{"location":"KB/Regex%20cheatsheet/#repetition","title":"Repetition","text":"","tags":["cheatsheets"]},{"location":"KB/Regex%20cheatsheet/#capture-parts-of-strings","title":"Capture Parts of Strings","text":"","tags":["cheatsheets"]},{"location":"KB/Regex%20cheatsheet/#before-or-after-the-match","title":"Before or after the Match","text":"<ul> <li>No space before, No space after, space before and after<ul> <li></li> </ul> </li> </ul>","tags":["cheatsheets"]},{"location":"KB/Regex%20cheatsheet/#change-matching-start-to-finish-case-whitespace-single_line","title":"Change Matching - Start to Finish, Case, Whitespace, single_line","text":"","tags":["cheatsheets"]},{"location":"KB/Regex%20cheatsheet/#unicode","title":"Unicode","text":"","tags":["cheatsheets"]},{"location":"KB/Regex%20cheatsheet/#complicated-examples","title":"Complicated Examples","text":"","tags":["cheatsheets"]},{"location":"KB/Regex%20cheatsheet/#find-all-sentences-that-ends-with-and-replace-with-or-newline-and-not","title":"Find All Sentences that Ends with [! , . ?] and Replace with . or Newline and not .","text":"","tags":["cheatsheets"]},{"location":"KB/Regex%20cheatsheet/#find-all-emails-and-replace-with-at-and-with-dot","title":"Find All Emails and Replace @ with [at] and . with [dot]","text":"","tags":["cheatsheets"]},{"location":"KB/Regex%20cheatsheet/#find-all-phone-numbers","title":"Find All Phone Numbers","text":"","tags":["cheatsheets"]},{"location":"KB/Region%20Growing/","title":"Region Growing","text":""},{"location":"KB/Region%20Growing/#region-growing","title":"Region Growing","text":"<ul> <li>Automatic Segmentation</li> <li>Requires seed point</li> <li>Leakage through holes in Contour</li> <li></li> </ul>"},{"location":"KB/Region%20Proposal/","title":"Region Proposal","text":""},{"location":"KB/Region%20Proposal/#region-proposal","title":"Region Proposal","text":"<ul> <li>region proposal generation that shares full-image convolutional features with the detection network,</li> <li>nearly cost-free region proposals</li> <li>haring convolutional features with the down-stream detection network</li> <li>simultaneously predicts object bounds and objectness scores at each position</li> </ul>"},{"location":"KB/Register%20Renaming/","title":"Register Renaming","text":""},{"location":"KB/Register%20Renaming/#register-renaming","title":"Register Renaming","text":"<ul> <li>used to avoid unnecessary serialization of program operations caused by the reuse of registers by those operations, in order to enable out-of-order execution</li> <li></li> </ul>"},{"location":"KB/Register%20to%20Register%20Architecture/","title":"Register to Register Architecture","text":""},{"location":"KB/Register%20to%20Register%20Architecture/#register-to-register-architecture","title":"Register to Register Architecture","text":"<ul> <li>All vector operations occur between vector registers</li> <li>If necessary, operands are fetched from main memory into a set of vector registers (load-store unit)</li> <li>SIMD based on this</li> <li></li> </ul>"},{"location":"KB/Regularization%20Rate/","title":"Regularization Rate","text":""},{"location":"KB/Regularization%20Rate/#regularization-rate","title":"Regularization Rate","text":"<ul> <li>A scalar value, represented as lambda, specifying the relative importance of the KB/Regularization.md function.</li> <li>Raising the KB/Regularization.md rate reduces overfitting but may make the model less accurate.</li> <li> </li> <li>The final stage of a recommendation system, during which scored items may be re-graded according to some other (typically, non-ML) algorithm. Re-ranking evaluates the list of items generated by the scoring phase, taking actions such as<ul> <li>Eliminating items that the user has already purchased.</li> <li>Boosting the score of fresher items.</li> </ul> </li> </ul>"},{"location":"KB/Regularization%20Rate/#re-ranking","title":"re-ranking","text":""},{"location":"KB/Regularization%20Term/","title":"Term","text":""},{"location":"KB/Regularization%20Term/#term","title":"Term","text":"<ul> <li>Penalty term</li> <li>Cost function that penalizes model params \\(\\theta\\) with a high degree of flexibility</li> <li> \\[h_{opt} = argmin_{h \\in \\mathcal{H}} \\frac{1}{N}\\Sigma_{i=1}^N L(h(x_i), y_i)+ \\alpha^{2}R(\\theta_h)\\] </li> <li>\\(\\alpha^2\\) determines how much regularizer affects the model<ul> <li>Larger : soft models</li> <li>Increasing -&gt; Down regulating flexibility</li> <li>0 = overfitting and unregularized risk</li> <li>\\(\\infty\\) does not care about training data at all. Only cares about minimal penalty<ul> <li>Dead model</li> </ul> </li> </ul> </li> </ul>"},{"location":"KB/Regularization%20Term/#types","title":"Types","text":"<ul> <li>Lp Regularization for p =2<ul> <li>Soft models</li> <li>Squared sum of model params</li> </ul> </li> </ul>"},{"location":"KB/Regularization/","title":"Regularization","text":""},{"location":"KB/Regularization/#regularization","title":"Regularization","text":"<ul> <li>Regularization Term</li> <li>Dropout</li> <li>VariationalRecurrent Dropout</li> <li>Batch Normalization</li> <li>Layer Normalization</li> <li>Augmentation</li> <li>Lp Regularization</li> <li>Pruning</li> <li>Effects of Regularization</li> </ul>"},{"location":"KB/Reinforcement%20Learning/","title":"Reinforcement Learning","text":""},{"location":"KB/Reinforcement%20Learning/#reinforcement-learning","title":"Reinforcement Learning","text":""},{"location":"KB/Reinforcement%20Learning/#anchor","title":"anchor","text":""},{"location":"KB/Rejection%20Sampling/","title":"Rejection Sampling","text":""},{"location":"KB/Rejection%20Sampling/#rejection-sampling","title":"Rejection Sampling","text":"<ul> <li>Also called Best-of-n sampling</li> <li>No CDF with a simple inverse</li> <li>Importance sampling</li> <li>Use a simpler distribution which is somewhat related to the target PDF<ul> <li>Sample by transformation</li> </ul> </li> <li>Now if we can take a Proto PDF \\(g_{0} \\geq f\\) where we can sample from the PDF g</li> <li>Take a point from g with Probability \\(f(\\tilde x)/g_{0}(\\tilde x)\\)<ul> <li>Either accept or reject if satisfies Probability</li> <li>If accepted then return the sample</li> </ul> </li> <li>Drop a point from \\(g_{0}(x)\\) with that Probability</li> <li>Depends on how close g is to f of course</li> <li><ul> <li>If the ratio \\(\\frac{f}{g_{0}}\\) is small. (aka f is bigger), then there are many rejections and the algo will be slow. Impossible to not do in high dim spaces</li> </ul> </li> </ul>"},{"location":"KB/Relational%20Inductive%20Bias/","title":"Relational Inductive Bias","text":""},{"location":"KB/Relational%20Inductive%20Bias/#relational-inductive-bias","title":"Relational Inductive Bias","text":"<ul> <li>Weak Relation Bias</li> <li>Locality</li> <li>Sequential Relation Bias</li> <li>Arbitrary Relation Bias</li> </ul>"},{"location":"KB/Relative%20Multi%20Head%20Self%20Attention/","title":"Relative Multi Head Self Attention","text":""},{"location":"KB/Relative%20Multi%20Head%20Self%20Attention/#relative-multi-head-self-attention","title":"Relative Multi Head Self Attention","text":"<ul> <li>ZihangDai et al., 2019</li> </ul>"},{"location":"KB/Relevance%20Account/","title":"Relevance Account","text":""},{"location":"KB/Relevance%20Account/#relevance-account","title":"Relevance Account","text":"<ul> <li>(Philip 2011)</li> <li>pragmatic extension of the theory of Drozd and Loosbroek (2006)</li> <li>Philip claims that the problem that children have to solve when assigning a domain to a universal quantifier is which objects in the context should be taken as relevant.</li> <li>Adults rely on their world knowledge in identifying the presupposed set</li> <li>As formulated in the Normal World Constraint, \"if an object is contextually relevant, then there is a normal situation that it is part of.\"</li> </ul>"},{"location":"KB/Relevance%20Theory/","title":"Relevance Theory","text":""},{"location":"KB/Relevance%20Theory/#relevance-theory","title":"Relevance Theory","text":"<ul> <li>Relevance Theory is built on the assumption that attention and thought processes automatically turn toward information that seems relevant, capable of yielding cognitive effects</li> <li>As the Principle of Relevance states, communicated information comes with a guarantee of relevance.</li> <li>The higher the cognitive effect of the information, and/or the more economically it is communicated, the greater its relevance.</li> </ul>"},{"location":"KB/Relu/","title":"Relu","text":""},{"location":"KB/Relu/#relu","title":"Relu","text":"<ul> <li> \\[ReLU(x) = max(0,x)\\] </li> <li> \\[\\frac{d}{d_x}ReLU(X) = \\begin{cases}0 &amp; x \\geq 0 \\\\ 1 &amp; otherwise \\end{cases}\\] </li> <li>He init</li> <li>MLP, CNN : Hidden</li> <li></li> <li>Leaky Relu</li> <li>PRelu</li> <li>Noisy Relu</li> </ul>"},{"location":"KB/Remission/","title":"Remission","text":""},{"location":"KB/Remission/#remission","title":"Remission","text":"<ul> <li>Describes a disease that is not getting worse</li> </ul>"},{"location":"KB/RepLKNet/","title":"RepLKNet","text":""},{"location":"KB/RepLKNet/#replknet","title":"RepLKNet","text":"<ul> <li>impressively manages to scale the kernel size to 31\u00d731 with improved performance</li> <li>the performance starts to saturate as the kernel size continues growing, compared to the scaling trend of ad- vanced ViTs such as Swin Transformer</li> <li>explore the possibility of training extreme convolutions larger than 31\u00d731 and test whether the performance gap can be eliminated by strategically enlarging convolutions.</li> </ul>"},{"location":"KB/RepVGG/","title":"RepVGG","text":"<p>toc: true title: RepVGG</p> <p>categories: ['temp']</p>"},{"location":"KB/RepVGG/#repvgg","title":"RepVGG","text":"<ul> <li>RepVGG: Making Vgg-style ConvNets Great Again<ul> <li>stack of \\(3\\times3\\) Conv and Relu during inference time</li> <li>training-time model has a multi-branch topology</li> <li>decoupling of the training-time and inference-time architecture is realized by a structural re-parameterization technique</li> <li>5 stages and conducts down-Sampling via stride-2 convolution at the beginning of a stage</li> <li>identity and 1 \\times 1 branches, but only for training</li> <li>ImageNet</li> <li>higher accuracy and show favorable accuracy-speed trade-off compared to the state-of-the-art models like EfficientNet and RegNet</li> <li></li> </ul> </li> </ul>"},{"location":"KB/Representation%20Bias/","title":"Representation Bias","text":""},{"location":"KB/Representation%20Bias/#representation-bias","title":"Representation Bias","text":"<ul> <li>As multitask is performed, introduces bias to learn Features and generalizing for other tasks</li> <li>increases chances that more general features are learned</li> </ul>"},{"location":"KB/Res%20Net%20D/","title":"Res Net D","text":""},{"location":"KB/Res%20Net%20D/#res-net-d","title":"Res Net D","text":""},{"location":"KB/Res%20Net/","title":"Res Net","text":""},{"location":"KB/Res%20Net/#res-net","title":"Res Net","text":"<ul> <li>@heDeepResidualLearning2016</li> <li>Deep Residual Learning for Image Recognition</li> <li>Deeper Networks have Issues because of vanishing #gradients</li> <li>Propagate gradients forward for deeper networks</li> <li>Skip connections</li> <li>output of F(x) has the same dims as x -&gt; add</li> <li>If only spatial dims match (aka not channels) -&gt; concat</li> <li>less params than VGG</li> <li>Skip Connection</li> <li>Sadly, one of the creators Jian Sun passed away yesterday. (16-6-22)</li> </ul> <pre><code>def make_layer(inplanes, planes, block, n_blocks, stride=1):\n    downsample = None\n    if stride != 1 or inplanes != planes * block.expansion:\n        # output size won't match input, so adjust residual\n        downsample = nn.Sequential(\n            nn.Conv2d(inplanes, planes * block.expansion,\n                      kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n    return nn.Sequential(\n        block(inplanes, planes, stride, downsample),\n        *[block(planes * block.expansion, planes) for _ in range(1, n_blocks)]\n    )\n\ndef ResNetNew(block, layers, num_classes=1000):    \n    e = block.expansion\n\n    resnet = nn.Sequential(\n        Rearrange('b c h w -&gt; b c h w', c=3, h=224, w=224),\n        nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n        nn.BatchNorm2d(64),\n        nn.ReLU(inplace=True),\n        nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n        make_layer(64,      64,  block, layers[0], stride=1),\n        make_layer(64 * e,  128, block, layers[1], stride=2),\n        make_layer(128 * e, 256, block, layers[2], stride=2),\n        make_layer(256 * e, 512, block, layers[3], stride=2),\n        # combined AvgPool and view in one averaging operation\n        Reduce('b c h w -&gt; b c', 'mean'),\n        nn.Linear(512 * e, num_classes),\n    )\n\n    # [initialization](./Initialization.md)\n    for m in resnet.modules():\n        if isinstance(m, nn.Conv2d):\n            n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n            m.weight.data.normal_(0, math.sqrt(2. / n))\n        elif isinstance(m, nn.BatchNorm2d):\n            m.weight.data.fill_(1)\n            m.bias.data.zero_()\n    return resnet\n</code></pre>"},{"location":"KB/Rescorla-Wagner%20Algorithm/","title":"Rescorla-Wagner Algorithm","text":""},{"location":"KB/Rescorla-Wagner%20Algorithm/#rescorla-wagner-algorithm","title":"Rescorla-Wagner Algorithm","text":"<ul> <li>Rescorla &amp; Wagner (1972): animals and humans also learn associations by paying attention to what is not associated.</li> <li> \\[\\Delta V = \\alpha \\beta_{1}\\beta_{2}(\\lambda = \\Sigma V)\\] </li> <li>\u25b6 V = association strength \u25b6 \u2206 V : Change in association strength \u25b6 \u03bb = maximum values of the unconditional stimulus \u25b6 Set to 1: when US is present (food) \u25b6 Set to 0: when not present \u25b6 \u03b1 = learning rate \u25b6 \u03b2 = varies the effects of negative or positive evidence \u25b6 \u03a3V = sum of associated strengths for all cues/features/conditions stimuli</li> <li>negative instances are also useful to learning</li> <li>Logical Problem of Lang Acquisition</li> <li>Children don't get negative evidence = must be innate</li> <li>Cross-situational learning</li> <li>Propose-but-verify</li> <li>Rescorla-Wagner Blocking</li> <li>Rescorla-Wagner = error-driven</li> <li>After a strong association is made, as long as it is confirmed by data, no new learning will occur</li> <li>The model only learns when the predicted outcome differs from actual outcome</li> </ul>"},{"location":"KB/Rescorla-Wagner%20Blocking/","title":"Rescorla-Wagner Blocking","text":""},{"location":"KB/Rescorla-Wagner%20Blocking/#rescorla-wagner-blocking","title":"Rescorla-Wagner Blocking","text":"<ul> <li>If an outcome is already strongly associated with a stimuli (cue or feature), the presence of other features, regardless of how consistent they are, will not lead to a new association</li> </ul>"},{"location":"KB/Research%20Debt/","title":"Research Debt","text":""},{"location":"KB/Research%20Debt/#research-debt","title":"Research Debt","text":"<ul> <li>Research Debt</li> <li>Achieving a research-level understanding of most topics is like climbing a mountain.</li> <li>Aspiring researchers must struggle to understand vast bodies of work that came before them, to learn techniques, and to gain intuition</li> <li>Upon reaching the top, the new researcher begins doing novel work, throwing new stones onto the top of the mountain and making it a little taller for whoever comes next.</li> <li>People expect the climb to be hard</li> <li>It reflects the tremendous progress and cumulative effort that\u2019s gone into mathematics</li> <li>The climb is seen as an intellectual pilgrimage, the labor a rite of passage</li> <li>But the climb could be massively easier</li> <li>It\u2019s entirely possible to build paths and staircases into these mountains.</li> <li>That is, really outstanding tutorials, reviews, textbooks, and so on.</li> <li>technical debt</li> <li>institutional debt</li> <li>Poor Exposition</li> <li>Interpretive Labor</li> <li>Clear Thinking</li> <li>Research Distillation</li> </ul>"},{"location":"KB/Research%20Distillation/","title":"Research Distillation","text":""},{"location":"KB/Research%20Distillation/#research-distillation","title":"Research Distillation","text":"<ul> <li>It can be incredibly satisfying, combining deep scientific understanding, empathy, and design to do justice to our research and lay bare beautiful insights.</li> <li>Distillation is also hard</li> <li>It\u2019s tempting to think of explaining an idea as just putting a layer of polish on it, but good explanations often involve transforming the idea.</li> <li>This kind of refinement of an idea can take just as much effort and deep understanding as the initial discovery.</li> <li>We can\u2019t solve research debt by having one person write a textbook</li> </ul>"},{"location":"KB/Research%20Engineer%20in%20Human%20Modeling%20for%20Automated%20Driving%20delft/","title":"Research Engineer in Human Modeling for Automated Driving delft","text":"","tags":["mlops"]},{"location":"KB/Research%20Engineer%20in%20Human%20Modeling%20for%20Automated%20Driving%20delft/#research-engineer-in-human-modeling-for-automated-driving-subhaditya-mukherjee","title":"Research Engineer in Human Modeling for Automated Driving - Subhaditya Mukherjee","text":"<p>As AI gets more advanced, applications such as automated driving become more feasible. But the real challenge is ensuring responsible and reliable implementations of algorithms with a safety-first mindset. That being the case, I am quite interested in contributing to research in this domain. I remember working on a similar project in my bachelors with a startup for a little bit, but sadly, COVID-19 put a halt to their operations, and I could not continue with them. The work was quite interesting though, and I was sad about losing the opportunity back then. So, when this position showed up in my search, I did not want to miss out again.</p> <p>As of a month ago, I have a masters in AI from the University of Groningen, which now also puts me in a better position to tackle the challenges faced in developing these models. My interest is at the intersection of applied AI and explainability, and I am quite familiar with Python, deep learning frameworks such as PyTorch and Tensorflow and other tricks of the trade. Interestingly enough, I also enjoy teaching and was a TA for three courses in my masters as well. While I bring programming, AI and computer vision skills to the table, I am willing to learn whatever else is required for the project. I firmly believe in human-in-the-loop solutions to life-critical problems such as this one, and I can also contribute to the explainability and fairness of the system. </p> <p>I believe that this position would be the perfect next step for me as it not only is a project with real-world impact, but also involves quite a bit of interdisciplinary research. I hope you give me a chance, and I look forward to hearing back from you.</p> <p>Thank you, Subhaditya Mukherjee</p>","tags":["mlops"]},{"location":"KB/Research%20Intimacy/","title":"Research Intimacy","text":""},{"location":"KB/Research%20Intimacy/#research-intimacy","title":"Research Intimacy","text":"<ul> <li>Internalizing obscure knowledge, equations, relationships, and ways of thinking related to a research topic.</li> <li>link</li> </ul>"},{"location":"KB/Resistance/","title":"Resistance","text":""},{"location":"KB/Resistance/#resistance","title":"Resistance","text":"<ul> <li>resistance =\u00a0resistivity x length\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 cross sectional area</li> <li> \\[R = r/A\\] </li> </ul>"},{"location":"KB/ResizeMix/","title":"ResizeMix","text":""},{"location":"KB/ResizeMix/#resizemix","title":"ResizeMix","text":"<ul> <li>@qinResizeMixMixingData2020</li> <li>performs random image KB/Cropping.md and pasting</li> <li>ResizeMix solves the random region KB/Cropping.md problem that misallocates the output image label in certain cases where the pasted region does not contain any object informa- tion</li> <li>scaling down (scale rate is sampled from a uniform dis- tribution) the selected image completely and past- ing it randomly on the target image</li> <li>modified labels of the output image are always accurate and proportionate to the mixing.</li> </ul>"},{"location":"KB/Response%20Based%20Knowledge/","title":"Response Based Knowledge","text":""},{"location":"KB/Response%20Based%20Knowledge/#response-based-knowledge","title":"Response Based Knowledge","text":"<ul> <li>neural response of the last output layer of the teacher model. The main idea is to directly mimic the final prediction of the teacher model.</li> <li>Given a vector of logits z as the outputs of the last fully connected layer of a deep model, the Distillation Loss for response-based knowledge can be formulated as</li> <li>response in object detection task may contain the logits together with the offset of a bounding box (Chen et al., 2017). In semantic landmark localization tasks, e.g., human pose estimation, the response of the teacher model may include a heatmap for each landmark (Zhang et al., 2019a)</li> </ul>"},{"location":"KB/Restricted%20Boltzmann%20Machine/","title":"Restricted Boltzmann Machine","text":""},{"location":"KB/Restricted%20Boltzmann%20Machine/#restricted-boltzmann-machine","title":"Restricted Boltzmann Machine","text":"<ul> <li>Start of Deep learning</li> </ul>"},{"location":"KB/RetinaNet/","title":"RetinaNet","text":""},{"location":"KB/RetinaNet/#retinanet","title":"RetinaNet","text":"<ul> <li>Simple dense detector for Focal Loss</li> </ul>"},{"location":"KB/Reuptake/","title":"Reuptake","text":""},{"location":"KB/Reuptake/#reuptake","title":"Reuptake","text":"<ul> <li>A process by which released neurotransmitters are absorbed for subsequent re-use.</li> </ul>"},{"location":"KB/Revisiting%20variable%20foreperiod%20effects%20evaluating%20the%20repetition%20priming%20account/","title":"Revisiting variable foreperiod effects evaluating the repetition priming account","text":""},{"location":"KB/Revisiting%20variable%20foreperiod%20effects%20evaluating%20the%20repetition%20priming%20account/#revisiting-variable-foreperiod-effects-evaluating-the-repetition-priming-account","title":"Revisiting variable foreperiod effects evaluating the repetition priming account","text":"<ul> <li>Tianfang Han &amp; Robert W. Proctor</li> </ul>"},{"location":"KB/Revisiting%20variable%20foreperiod%20effects%20evaluating%20the%20repetition%20priming%20account/#abstract","title":"Abstract","text":"<ul> <li>A warning signal preceding an imperative stimulus by a certain foreperiod can accelerate responses (foreperiod effect)</li> <li>When foreperiod is varied within a block, the foreperiod effect on reaction time (RT) is modulated by both the current and the prior foreperiod</li> <li>The multiple-trace theory of Los et al. (Frontiers in Psychology, 5, Article 1058, 2014) attributes the slope of the foreperiod-RT function to the foreperiod distribution</li> <li>with a non-aging foreperiod distribution, the variableforeperiod paradigm yields unequal sequential-effect sizes at the different foreperiods, consistent with the multiple-trace theory but contrary to Capizzi et al.'s repetition-priming account</li> <li>The foreperiod-RT functions are similar to those of the fixedforeperiod paradigm, which is not predicted by the multiple trace theory</li> </ul>"},{"location":"KB/Revisiting%20variable%20foreperiod%20effects%20evaluating%20the%20repetition%20priming%20account/#experiment-1","title":"Experiment 1","text":"<ul> <li>Both the foreperiods and foreperiod distribution were the same, but a choice-reaction task</li> <li>Responses were faster when the current foreperiod was 400 ms compared to 1,400 ms</li> <li>was also a main effect of Foreperiod Sequence,</li> <li>Responses were faster when the current foreperiod was the same as the previous one compared to when they were different.</li> <li>the interaction of Current Foreperiod \u00d7 Foreperiod Sequence indicated that the SFP effect was larger at the short foreperiod than at the long foreperiod</li> <li>This result is inconsistent with the repetition priming account of Capizzi et al., according to which the SFP effects should be of similar sizes at short and long foreperiods</li> <li>The larger SFP effect at short than long foreperiods is consistent with Los et al.'s (2014) multiple trace theory</li> <li>This is because it assumes that a long previous foreperiod produces inhibition to the critical moment of the short foreperiod but a short previous foreperiod does not affect the preparation at the critical moment of the long foreperiod</li> <li>symmetric sequential effects are more likely to be found in choicereaction tasks compared to a simple reaction scenario.</li> <li>the larger proportion of shorter foreperiod trials could be the basis of that foreperiod's advantage in terms of response speed by having more previous memory traces contributing to the activation at the shorter foreperiod's critical moment</li> <li>Alternatively, the increasing foreperiod-RT function in Experiment 1 shared the same direction as in a fixedforeperiod paradigm</li> <li>without the effect from the additional processes in a variable-foreperiod paradigm, the foreperiod-RT relation in the two foreperiod paradigms will be in the same direction</li> </ul>"},{"location":"KB/Revisiting%20variable%20foreperiod%20effects%20evaluating%20the%20repetition%20priming%20account/#experiment-2","title":"Experiment 2","text":"<ul> <li>two very short foreperiods (50 ms and 200 ms) were used</li> <li>This prediction means that the shorter preceding foreperiod should produce faster responses regardless of the length of the current foreperiod</li> <li>Based on the current assumptions of MTP, the preparation at any foreperiod is determined by the activationinhibition states (strengths of activation and inhibition) stored in each [memory trace], the strength of each [memory trace] (memory trace, and the total number of previous memory traces with each foreperiod</li> <li>Participants were more likely to make errors when encountering foreperiod repetition compared to alternation</li> <li>main effect of Current Foreperiod was not significant</li> <li>The interaction between Foreperiod Sequence and Current Foreperiod was not significant</li> <li>First, the main effect of Current Foreperiod was found, indicating a decreasing foreperiod-RT function in the short-foreperiod scenario</li> <li>This direction is consistent with the prediction based on the fixed-foreperiod effect but this foreperiod-RT function, especially its opposite direction from that observed in Experiment 1, cannot be predicted from the current assumptions of the MTP</li> <li>A small interaction was found between Current Foreperiod and Foreperiod Sequence</li> <li>It is worth noting that participants were more likely to make errors when the current foreperiod matched the previous one compared to when it did not</li> </ul>"},{"location":"KB/Revisiting%20variable%20foreperiod%20effects%20evaluating%20the%20repetition%20priming%20account/#experiment-3","title":"Experiment 3","text":"<ul> <li>The main effect of Current Foreperiod was not significant</li> <li>neither was the interaction between Current Foreperiod and Foreperiod Sequence</li> <li>With regard to the variable-foreperiod effect, although a significant main effect was not detected (p = .085), the numerical difference in RT at the two foreperiods pointed in the same direction as the significant fixedforeperiod effect</li> </ul>"},{"location":"KB/Revisiting%20variable%20foreperiod%20effects%20evaluating%20the%20repetition%20priming%20account/#discussion","title":"Discussion","text":"<ul> <li>Los et al. (2017) used a visual warning signal and a visual imperative stimulus and found that blocks with the same foreperiod distribution (exponential or antiexponential) induced a short-term carryover effect on the foreperiod-RT function in subsequent blocks with a uniform distribution</li> <li>Crowe and Kent (2019) used an auditory pair of stimuli and found a similar but more limited carryover effect (lasting for only one block)</li> <li>imply that having the fixed-foreperiod blocks performed immediately after the variable-foreperiod blocks could have made it more difficult to measure the fixed-foreperiod effect precisely, which could be a potential limitation of the current design</li> <li>The key step of reconnecting the two foreperiod paradigms was taken by Los et al. (2014), in which a simplified version of the MTP without the activation-inhibition ratio was used to account for the fixedforeperiod effect</li> <li>A lower maximum and greater temporal dispersion as the imperative moment is moved further from the warning signal were added to predict a shorter RT at the short foreperiod</li> <li>by using a non-aging foreperiod distribution, the variableforeperiod effect would get back to its baseline, which is the foreperiod-RT function in a fixed-foreperiod paradigm</li> </ul>"},{"location":"KB/Revisiting%20variable%20foreperiod%20effects%20evaluating%20the%20repetition%20priming%20account/#conclusion","title":"Conclusion","text":"<ul> <li>The results of this study suggest that the SFP effect reflects a benefit of repetition, which can be attributed to the memory of prior trials</li> <li>SFP effect was larger at the shorter foreperiod, which is consistent with MTP</li> <li>On the other hand, we showed that in a variable foreperiod paradigm, when the conditional probability of the imperative stimulus appearing at the next foreperiod stays constant over time, the foreperiodRT function follows the foreperiod-RT relation in a fixed foreperiod paradigm.</li> <li>This consistency between different foreperiod paradigms is not predicted by the MTP, which attributes the foreperiod-RT function to the proportions of foreperiods</li> </ul>"},{"location":"KB/Revisiting%20variable%20foreperiod%20effects%20evaluating%20the%20repetition%20priming%20account/#images","title":"Images","text":""},{"location":"KB/Revolute%20Joint/","title":"Revolute Joint","text":""},{"location":"KB/Revolute%20Joint/#revolute-joint","title":"Revolute Joint","text":"<ul> <li>The joints of a robot, which are capable of rotary motion.</li> </ul>"},{"location":"KB/Ridge%20Regression/","title":"Ridge Regression","text":""},{"location":"KB/Ridge%20Regression/#ridge-regression","title":"Ridge Regression","text":"<ul> <li>LinearRegression</li> <li>\\(\\((XX')\\)\\) suffers from numerical instability when almost singular (real world data. sparse I think)</li> <li>Add a tiny term<ul> <li> \\[w'_{opt} = (XX' + \\alpha ^2 I_{nxn})^{-1}XY\\] </li> <li>Solution to overfitting</li> </ul> </li> </ul>"},{"location":"KB/Risk%20Mitigation/","title":"Risk Mitigation","text":""},{"location":"KB/Risk%20Mitigation/#risk-mitigation","title":"Risk Mitigation","text":"<ul> <li>A secondary step in the risk assessment process that involves reducing the level of risk for the identified tasks, by applying risk reduction measures in order to eliminate or mitigate the hazards.</li> </ul>"},{"location":"KB/Rmsprop/","title":"Rmsprop","text":""},{"location":"KB/Rmsprop/#rmsprop","title":"Rmsprop","text":"<ul> <li>RL</li> <li>More stable than Adagrad</li> <li>Moving exponential avg : older grads given less weight</li> <li>$$\\begin{align}\\</li> </ul> <p>&amp; E[g^{2}]{t}= 0.9E[g^{2}]{t-1}+ 0.1g^{2}_{t}\\</p> <p>&amp; \\theta_{t+1}= \\theta_{t}- \\frac{\\eta}{\\sqrt{E[g^{2}]){t}+\\epsilon}}g{t}</p> <p>\\end{align}</p> <p>$$ - Suggested \\(\\gamma=0.9\\) and \\(\\eta= 0.001\\)</p>"},{"location":"KB/RoBERTa/","title":"RoBERTa","text":""},{"location":"KB/RoBERTa/#roberta","title":"RoBERTa","text":"<ul> <li>RoBERTa: a Robustly Optimized BERT Pretraining Approach</li> <li>evaluates a number of design decisions when pretraining BERT models</li> <li>They find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it.</li> <li>performance can be substantially improved by training the model longer, with bigger batches over more data; removing the next sentence prediction objective; training on longer sequences; and dynamically changing the masking pattern applied to the training data</li> <li>GLUE</li> <li>RACE</li> <li>SQuAD</li> <li>only the masked language model objective</li> </ul>"},{"location":"KB/Robot%20Range%20Limit%20Monitoring/","title":"Robot Range Limit Monitoring","text":""},{"location":"KB/Robot%20Range%20Limit%20Monitoring/#robot-range-limit-monitoring","title":"Robot Range Limit Monitoring","text":"<ul> <li>Monitors the manipulator arm or its tool to be in the designated safety area</li> </ul>"},{"location":"KB/Robotic%20Joints/","title":"Robotic Joints","text":""},{"location":"KB/Robotic%20Joints/#robotic-joints","title":"Robotic Joints","text":"<ul> <li>Joints connect the manipulator links</li> <li>A joint normally provides one Controllable Degree of Freedom (CDOF)</li> <li>For each CDOF one separate actuator is needed</li> <li>An endeffector with many (C)DOFs needs a lot of actuators!</li> <li>Rotary Joint , Prismatic Joint</li> </ul>"},{"location":"KB/Robust%20RegNet/","title":"Robust RegNet","text":"<p>toc: true title: Robust RegNet</p> <p>categories: ['temp']</p>"},{"location":"KB/Robust%20RegNet/#robust-regnet","title":"Robust RegNet","text":"<ul> <li>Vision Models are More Robust and Fair When Pretrained on Uncurated Images Without Supervision</li> <li>Unsupervised Learning</li> <li>Discriminative Self Supervised learning allows training models on any random group of internet images, and possibly recover salient information that helps differentiate between the images</li> <li>ImageNet</li> <li>object-centric features that perform on par with supervised features on most object-centric downstream tasks</li> <li>learn any salient and more representative information present in diverse unbounded set of images from across the globe</li> <li>without any data pre-processing or prior assumptions about what we want the model to learn</li> <li>RegNet</li> <li>scaled to a dense 10 billion parameters</li> <li>pre-trained using the SwaV self-supervised method on a large collection of 1 billion randomly selected public images from Instagram with a diversity of gender, ethnicity, cultures, and locations</li> <li>captures well semantic information</li> <li>captures information about artistic style and learns salient information such as geo-locations and multilingual word embeddings based on visual content only.</li> <li>large-scale self-supervised pre-training yields more robust, fair, less harmful, and less biased results than supervised models or models trained on object centric datasets such as ImageNet</li> </ul>"},{"location":"KB/Robust%20regression/","title":"Robust regression","text":""},{"location":"KB/Robust%20regression/#robust-regression","title":"Robust regression","text":"<ul> <li>regression models that minimize MAE rather than MSE</li> <li>Laplace Distribution estimates the median output for a given input rather than the mean</li> </ul>"},{"location":"KB/Rod/","title":"Rod","text":""},{"location":"KB/Rod/#rod","title":"Rod","text":"<ul> <li>A type of photoreceptor, usually found on the outer edges of the retina, that helps facilitate peripheral vision.</li> </ul>"},{"location":"KB/Roll/","title":"Roll","text":""},{"location":"KB/Roll/#roll","title":"Roll","text":"<ul> <li>Rotation of the robot end effector in a plane perpendicular to the end of the manipulator arm.</li> </ul>"},{"location":"KB/Rotary%20Joint/","title":"Rotary Joint","text":""},{"location":"KB/Rotary%20Joint/#rotary-joint","title":"Rotary Joint","text":""},{"location":"KB/Rotary%20Vector%20Drive%20%28RV%29/","title":"Rotary Vector Drive (RV)","text":""},{"location":"KB/Rotary%20Vector%20Drive%20%28RV%29/#rotary-vector-drive-rv","title":"Rotary Vector Drive (RV)","text":"<ul> <li>A brand name for a speed reduction device that converts high speed low torque to low speed high torque, usually used on the major (larger) axis . See Cyclo Drive and Harmonic Drive.</li> </ul>"},{"location":"KB/Rotational%20Invariance/","title":"Rotational Invariance","text":""},{"location":"KB/Rotational%20Invariance/#rotational-invariance","title":"Rotational Invariance","text":"<ul> <li>In an image classification problem, an algorithm's ability to successfully classify images even when the orientation of the image changes. For example, the algorithm can still identify a tennis racket whether it is pointing up, sideways, or down. Note that rotational invariance is not always desirable; for example, an upside-down 9 should not be classified as a 9.</li> </ul>"},{"location":"KB/Routing%20by%20Agreement/","title":"Routing by Agreement","text":""},{"location":"KB/Routing%20by%20Agreement/#routing-by-agreement","title":"Routing by Agreement","text":"<ul> <li>The outputs of the higher-layer capsules are determined by a process called \u201crouting by agreement,\u201d</li> <li>determines the strength of the connection between capsules in different layers.</li> </ul>"},{"location":"KB/Runge%20Kutta/","title":"Runge Kutta","text":""},{"location":"KB/Runge%20Kutta/#runge-kutta","title":"Runge Kutta","text":"<ul> <li>Fourth Order</li> <li></li> <li></li> </ul>"},{"location":"KB/S2ST/","title":"S2ST","text":""},{"location":"KB/S2ST/#s2st","title":"S2ST","text":"<ul> <li>Direct Speech-to-speech Translation with Discrete Units</li> <li>direct speech-to-speech translation (S2ST) model that translates speech from one language to speech in another language without relying on intermediate text generation</li> <li>self-supervised discrete speech encoder on the target speech</li> <li>training a sequence-to-sequence speech-to-unit translation</li> <li>model to predict the discrete representations of the target speech</li> <li>When target text transcripts are available, they design a joint speech and text training framework that enables the model to generate dual KB/Modality.md output (speech and text) simultaneously in the same inference pass</li> <li>Fisher Spanish-English</li> </ul>"},{"location":"KB/SAM-ResNet/","title":"SAM-ResNet","text":""},{"location":"KB/SAM-ResNet/#sam-resnet","title":"SAM-ResNet","text":"<ul> <li>@corniaPredictingHumanEye2018</li> <li>uses LSTM to compute an attention map. The feature map extracted by ResNet is input to attentive convolutional LSTM that focuses on the most salient regions of the input image to iteratively refine the predicted saliency map.</li> <li>We recently proposed a Saliency Attentive Model (SAM) which, in contrast, incorporates attentive mechanisms to iteratively refine saliency predictions. Overall, it is composed by three main components: a Dilated Convolutional Network that extracts feature maps from the input image, an Attentive Convolutional LSTM which recurrently enhances saliency features and a learned prior module that incorporates the human-gaze center bias in the final predictions</li> </ul>"},{"location":"KB/SAM-ResNet/#attentive-convolutional-lstm","title":"Attentive Convolutional LSTM","text":"<ul> <li>The feature maps coming from the dilated network are then input to an Attentive Convolutional model, which recurrently process saliency features at different locations.</li> <li>Moreover, we exploit the sequential nature of LSTM to process features in an iterative way. The input of the LSTM is computed, at each step, through an attentive mechanism which focuses on different regions of the image.</li> <li>An attention map is generated by convolving the previous hidden state and the input (i.e. a stack of feature maps); once normalized through the softmax operator, this is applied to the input with an element-wise product</li> <li>After a fixed number of iterations, the last hidden state is taken as the output of this module.</li> </ul>"},{"location":"KB/SAM-ResNet/#learned-priors","title":"Learned Priors.","text":"<ul> <li>Finally, the output of the Attentive LSTM is combined with multiple learned priors which are used to model the center bias present in the human-eye fixations. Differently from existing works, which included predefined priors, we let the network learn its own priors. To reduce the number of parameters and facilitate the learning, we constraint that each prior should be a 2d Gaussian function, whose mean and covariance matrix are freely learnable. In this manner, priors are inferred purely from data,</li> <li>Thanks to this strategy, the predicted saliency maps are rescaled, for both versions, by a factor of 8 instead of 32 as in the original CNNs.</li> <li>In our work, we go beyond classical feed-forward networks to predict saliency maps and</li> <li>propose a Saliency Attentive Model which incorporates neural attention mechanisms to iteratively refine predictions</li> <li>Here, we provide experimental results on other popular saliency datasets to confirm the effectiveness and the generalization capabilities of our model, which enable us to reach the state of the art on all considered datasets.</li> </ul>"},{"location":"KB/SAM-ResNet/#loss-function","title":"Loss Function","text":"<ul> <li>linear combination of three saliency evaluation metrics: the Normalized Scanpath Saliency (NSS), the Linear Correlation Coefficient (CC) and the KullbackLeibler Divergence (KL-Div)</li> <li>SALICON</li> <li>omposed by 20, 000 images with corresponding saliency maps computed from mouse movements</li> </ul>"},{"location":"KB/SAM-ResNet/#images","title":"Images","text":""},{"location":"KB/SBU%20Captions/","title":"SBU Captions","text":""},{"location":"KB/SBU%20Captions/#sbu-captions","title":"SBU Captions","text":""},{"location":"KB/SDR/","title":"SDR","text":""},{"location":"KB/SDR/#sdr","title":"SDR","text":"<ul> <li>Smallest destroying region</li> <li>smallest region of the image that when removed, prevents a confident classification.</li> </ul>"},{"location":"KB/SELU/","title":"SELU","text":""},{"location":"KB/SELU/#selu","title":"SELU","text":"<ul> <li> \\[selu(x) = \\lambda x , \\text{if } x &gt;0 , \\text{else }, \\alpha(e^{x}-1)\\] </li> </ul>"},{"location":"KB/SELU/#information","title":"Information","text":"<ul> <li>Paper:\u00a0https://arxiv.org/pdf/1706.02515.pdf</li> <li>scaled variant of the\u00a0Elu\u00a0function</li> <li>does internal normalization (\"self-normalizing\")<ul> <li>each layer preserves the mean and the variance from the previous one</li> <li>normalization happens within the activation function</li> <li>to work:<ul> <li>input features must be standardized</li> <li>architecture must be sequential<ul> <li>self-normalizing not guaranteed otherwise</li> </ul> </li> <li>SELU as activation\u00a0</li> <li>custom\u00a0Initialization<ul> <li>zero mean</li> <li>standard deviation:\u00a0\\(\\sqrt{\\frac{1}{ \\#inputs}}\\)</li> </ul> </li> <li>if all layers are dense (in paper), but other research showed that it also works for CNNs</li> </ul> </li> </ul> </li> <li>has two fixed parameters\u00a0\u03b1\u00a0and\u00a0\u03bb<ul> <li>not hyperparameters nor learnt parameters</li> <li>derived from the inputs (\u03bc=0,\u00a0std=1)</li> <li>\u03b1\u22481.6732,\u00a0\u03bb\u22481.0507</li> </ul> </li> <li>Pros:<ul> <li>no\u00a0Vanishingexploding gradients</li> <li>cannot die as\u00a0Relu</li> <li>converges faster and to a better result than other activation functions</li> <li>significantly outperformed other activation functions for deep networks</li> </ul> </li> <li>Cons:<ul> <li>Computational heavier</li> </ul> </li> </ul>"},{"location":"KB/SGD%20Momentum/","title":"SGD Momentum","text":""},{"location":"KB/SGD%20Momentum/#sgd-momentum","title":"SGD Momentum","text":"<ul> <li>$$\\begin{align}</li> </ul> <p>&amp;v_{t}= \\gamma v_{t+1}+\\eta \\cdot \\nabla_{\\theta}J(\\theta) \\</p> <p>&amp;\\theta = \\theta- v_{t}\\</p> <p>\\end{align}$$</p>"},{"location":"KB/SGD/","title":"SGD","text":""},{"location":"KB/SGD/#sgd","title":"SGD","text":"<ul> <li>instead of taking the whole dataset for each iteration, we randomly select the batches of data</li> <li>The procedure is first to select the initial parameters w and learning rate n. Then randomly shuffle the data at each iteration to reach an approximate minimum.</li> <li>full of noise</li> <li>Due to an increase in the number of iterations, the overall computation time increases.</li> <li> \\[\\theta = \\theta - \\eta \\cdot \\nabla_{\\theta} J(\\theta ; x^{i}; y^{i} )\\] <ul> <li>For each example \\(x^{i}\\) and label \\(y^{i}\\)</li> </ul> </li> </ul>"},{"location":"KB/SHAP/","title":"SHAP","text":""},{"location":"KB/SHAP/#shap","title":"SHAP","text":"<ul> <li>@lundbergUnifiedApproachInterpreting2017</li> <li>help users interpret the predictions of complex models</li> <li>unclear how these methods are related and when one method is preferable over another</li> <li>unified framework for interpreting predictions</li> <li>SHAP</li> <li>SHapley Additive exPlanations</li> <li>game theoretic approach to explain the output of any machine learning model</li> <li>connects optimal credit allocation with local explanations using the classic Shapley values from game theory and their related extensions</li> <li>assigns each feature an importance value for a particular prediction</li> <li>identification of a new class of additive feature importance measures</li> <li>theoretical results showing there is a unique solution in this class with a set of desirable properties</li> <li>notable because several recent methods in the class lack the proposed desirable properties</li> <li>present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches</li> </ul>"},{"location":"KB/SIMD/","title":"SIMD","text":""},{"location":"KB/SIMD/#simd","title":"SIMD","text":"<ul> <li>Single instruction on multiple data</li> <li>Graphics, Image processing</li> <li>Synchronous, Deterministic</li> <li>GPU</li> <li></li> <li>Vector Processor</li> <li>Limited by Amdahl's Law</li> <li>More energy efficient than MIMD</li> <li>Time space duality</li> </ul>"},{"location":"KB/SISD/","title":"SISD","text":""},{"location":"KB/SISD/#sisd","title":"SISD","text":"<ul> <li>Single Instruction, Single Data</li> <li>Deterministic</li> <li>-</li> </ul>"},{"location":"KB/SLAK/","title":"SLAK","text":""},{"location":"KB/SLAK/#slak","title":"SLAK","text":"<ul> <li>MORE CONVNETS IN THE 2020S: SCALING UP KERNELS BEYOND 51 \u00d7 51 USING KB/Sparsity.md</li> <li> <p>Shiwei Liu, Tianlong Chen, Xiaohan Chen, Xuxi Chen, Qiao Xiao, Boqian Wu, Tommi Ka \u0308 rkka \u0308 inen, Mykola Pechenizkiy, Decebal Constantin Mocanu, Zhangyang Wang</p> </li> <li> <p>The dominant role of convolutional neural networks (CNNs) seems to be challenged by increasingly effective transformer-based models.</p> </li> <li>advanced convolutional models strike back with large ker- nels motivated by the local-window attention mechanism, showing appealing perfor- mance and efficiency</li> <li>RepLKNet</li> <li>This study ends up with a recipe for applying extremely large kernels from the perspective of KB/Sparsity.md, which can smoothly scale up kernels to 61\u00d761 with better performance</li> <li>Built on this recipe, we propose Sparse Large Kernel Network (SLaK), a pure CNN architec- ture equipped with sparse factorized 51\u00d751 kernels that can perform on par with or better than state-of-the-art hierarchical Transformers and modern ConvNet architec- tures like ConvNeXt and RepLKNet</li> </ul>"},{"location":"KB/SLAK/#related-work","title":"RELATED WORK","text":"<ul> <li>Large Kernel in Attention</li> <li>Large Kernel in Convolution</li> <li>Dynamic Sparsity</li> <li>Sparse Evolutionary Training</li> </ul>"},{"location":"KB/SLAK/#failures-of-existing-approaches-to-go-beyond-31x31-kernels","title":"FAILURES OF EXISTING APPROACHES TO GO BEYOND 31x31 KERNELS","text":"<ul> <li>It is important to note that all models are trained for a reduced length of 120 epochs in this section, just to sketch the scaling trends of large kernel sizes.</li> <li>Following the design in RepLKNet, we set the kernel size of each stage as [51, 49, 47, 13] and [61, 59, 57, 13],</li> <li>naively enlarging kernel size from 7x7 to 31x31 decreases the performance although the receptive field may be enlarged by using extremely large kernels, it</li> <li>might fail to maintain the desirable property of locality.</li> <li>Since the stem cell in standard ResNet (He et al., 2016) and ConvNeXt results in a</li> <li>4x KB/Downsampling.md of the input images, extreme kernels with</li> </ul>"},{"location":"KB/SLAK/#a-recipe-for-extremely-large-kernels-beyond-31x31","title":"A RECIPE FOR EXTREMELY LARGE KERNELS BEYOND 31x31","text":"<ul> <li>Decomposing a large kernel into two rectangular, parallel kernels smoothly scales the kernel size up to 61x61</li> <li>Although using convolutions with medium sizes (e.g., 31x31) seemingly can directly avoid this problem, we want to investigate if we can further push the performance of CNNs by using (global) extreme convolutions</li> <li>approximate the large MxM kernel with a combination of two parallel and rectangular convolutions whose kernel size is MxN and NxM (where N &lt; M), respectively, as shown in Figure 1. Following Ding et al. (2022), we keep a 5x5 layer parallel to the large kernels and summed up their outputs after a batch norm layer.</li> <li>This decomposition balances between capturing long-range dependencies and extracting local detail features</li> <li>In stark contrast, the overhead of our method increases just linearly with the kernel size</li> <li>As the decomposition reduces learnable parameters and FLOPs, it is no surprise to observe our network to initially sacrifice accuracy slightly compared to the original RepLKNet at medium kernel sizes i.e. 31x31</li> <li>However, as the convolution size continues to increase, our method can scale kernel size up to 61x61 with improved performance.</li> </ul>"},{"location":"KB/SLAK/#use-sparse-groups-expand-more-width","title":"Use Sparse Groups, Expand More Width","text":"<ul> <li>significantly boosts the model capacity.</li> <li>Instead of using the standard group convolution, ConvNeXt simply employs depthwise convolutions with an increased width to achieve the goal of \"use more groups, expand width\". In this paper, we attempt to extend this principle from a KB/Sparsity.md-inspired perspective \u2013 \"use sparse groups, expand more width\".</li> <li>replace the dense convolutions with sparse convolutions, where the sparse kernels are randomly constructed based on the layer-wise KB/Sparsity.md ratio of SNIP (Lee et al., 2019)</li> <li>After construction, we train the sparse model with dynamic KB/Sparsity.md (Mocanu et al., 2018; Liu et al., 2021b), where the sparse weights are dynamically adapted during training by pruning the weights with the lowest magnitude and growing the same number of weights randomly.</li> <li>Doing so enables dynamic adaptation of sparse weights, leading to better local features.</li> <li>As kernels are sparse throughout training, the corresponding parameter count and training/inference FLOPs are only proportional to the dense models.</li> <li>dynamic KB/Sparsity.md notably reduces more than 2.0 GFLOPs, despite causing temporary performance degradation.</li> <li>Dynamic KB/Sparsity.md allows us to computation-friendly scale the model size up</li> <li>For example, using the same KB/Sparsity.md (40%), we can expand the model width by 1.3x while keeping the parameter count and FLOPs roughly the same as the dense model</li> </ul>"},{"location":"KB/SLAK/#large-kernels-generalize-better-than-small-kernels-with-our-recipe","title":"Large Kernels Generalize Better Than Small Kernels with Our Recipe","text":"<ul> <li>performance consistently increases with kernel size, up to 51x51</li> <li>Applying each part of our proposed recipe to 7x7 kernels leads to either no gain or marginal gains compared to our 51x51 kernels. This break-down experiment justifies our claim: large kernel is the root of power, and our proposed recipe helps unleash such power from large kernels.</li> </ul>"},{"location":"KB/SLAK/#slak_1","title":"SLAK","text":"<ul> <li>SLaK is built based on the architecture of ConvNeXt</li> <li>The design of the stage compute ratio and the stem cell are inherited from ConvNeXt</li> <li>The number of blocks in each stage is [3, 3, 9, 3] for SLaK-T and [3, 3, 27, 3] for SLaK-S/B</li> <li>The stem cell is simply a convolution layer with 4x4 kernels and 4 strides. Page 6</li> <li>We first directly increase the kernel size of ConvNeXt to [51, 49, 47, 13] for each stage, and replace each MxM kernel with a combination of Mx5 and 5xM kernels</li> <li>We find that adding a BatchNorm layer directly after each decomposed kernel is crucial before summing the output up</li> <li>urther sparsify the whole network and expand the width of stages by 1.3x, ending up with SLaK</li> </ul>"},{"location":"KB/SLAK/#evaluation-of-slak-imagenet-1k","title":"EVALUATION OF SLAK ImageNet-1K","text":"<ul> <li>ADE20K</li> <li>PASCAL VOC 2007</li> <li>COCO</li> <li>SLaK is not only able to capture long-range dependence but also the local context features.</li> <li>In comparison, high-contribution pixels of SLaK spread in a much larger ERF, and some high-contribution pixels emerge in non-center areas.</li> <li>SLaK balances between capturing long-range dependencies and focusing on the local details.</li> <li>SLaK seems to automatically recover the inductive bias of peripheral vision (Lettvin et al., 1976; Min et al., 2022) in the human vision system: the entire visual field is partitioned into multiple regions from near the gaze center to distant areas; humans have high- resolution processing near the gaze center (central and para-central regions), and decrease the resolution of processing for mid and far peripheral regions.</li> </ul>"},{"location":"KB/SLAK/#kernel-scaling-efficiency","title":"KERNEL SCALING EFFICIENCY","text":"<ul> <li>We simply replace all the kernels in stages of ConvNeXt-T with a set of kernel sizes from 7 to 151 and report the required GFLOPs and the number of parameters</li> <li>One can clearly see the big gap between full-kernel scaling (yellow lines) and kernel decomposition (green lines) as the kernel size increases beyond 31x31.</li> <li>Even using the ultra-large 151x151 kernels, using our methods would require fewer FLOPs and parameters, compared to full-kernel scaling with 51x51 kernel</li> <li>EFFECTIVE RECEPTIVE FIELD (ERF)</li> </ul>"},{"location":"KB/SLAK/#experiment","title":"Experiment","text":""},{"location":"KB/SLAK/#settings-imagenet-1k","title":"SETTINGS IMAGENET-1K","text":"<ul> <li>We share the (pre-)training settings of SLaK on ImageNet-1K in this section. We train SLaK for 300 epochs (Section 5.1) and 120 epochs (Section 4) using AdamW (Loshchilov &amp; Hutter, 2019) with a batch size of 4096, and a weight decay of 0.05. The only differnce between models training for 300 epochs and 120 epochs is the training time. The learning rate is 4e-3 with a 20-epoch linear warmup followed by a cosine decaying schedule. For data augmentation, we use the default setting of [RandAugment] (Cubuk et al., 2020) in Timm (Wightman, 2019) \u2013 \"rand-m9-mstd0.5- inc1\", Label Smoothing (Szegedy et al., 2016) coefficient of 0.1, Mixup (Zhang et al., 2017) with \u21b5 = 0.8, [Cutmix] (Yun et al., 2019) with \u21b5 = 1.0, Random Erasing.</li> </ul>"},{"location":"KB/SLAK/#semantic-segmentation-on-ade20k","title":"SEMANTIC SEGMENTATION ON ADE20K","text":"<ul> <li>We follow the training setting used in Ding et al. (2022); Liu et al. (2022b) using UperNet (Xiao et al., 2018) implemented by MMSegmentation (Contributors, 2020) with the 80K/160K-iteration training schedule. We conduct experiments with both short and long training procedures. The backbones are pre-trained on ImageNet-1K with 224x224 input for 120/300 epochs and then are finetuned with UperNet (Xiao et al., 2018) for 80K/160K iterations, respectively. We report the mean Intersection over Union (mIoU) with single-scale. All the hyperparameters are the exactly the same as the ones used in the official GitHub repository of ConvNeXt (con, 2021).</li> </ul>"},{"location":"KB/SLAK/#object-detection-and-segmentation-on-coco","title":"OBJECT DETECTION AND SEGMENTATION ON COCO","text":"<ul> <li>For COCO experiments, we follow the training settings used in BEiT, Swin, and ConvNeXt using MMDetection (Chen et al., 2019) and MMSegmentation (Contributors, 2020) toolboxes. The final model weights are adopted (instead of EMA weights) from ImageNet-1K pre-training with 224x224 input. We also conduct experiments with both short and long training procedures. The backbones are pre- trained on ImageNet-1K with 224x224 input for 120/300 epochs and then are finetuned with Cascade Mask R-CNN (Cai &amp; Vasconcelos, 2018) for 12/36 epochs, respectively. All the hyperparameters are the exactly the same as the ones used in the official GitHub repository of ConvNeXt (con, 2021).</li> </ul>"},{"location":"KB/SLAK/#object-detection-on-pascal-voc-2007","title":"OBJECT DETECTION ON PASCAL VOC 2007","text":"<ul> <li>We follow (Liu et al., 2021e) and finetune Faster-RCNN on PASCAL VOC dataset with SLaK-T as the backbone. We use multi-scale setting (Carion et al., 2020; Sun et al., 2021) which leads to the length of the shorter side between 480 and 800 and the ones of the longer side at most 1333. The model is trained with AdamW for 36 epochs with a learning rate of 0.0001, a weight decay of 0.05, and a batch size of 16.</li> </ul>"},{"location":"KB/SLAK/#some-more-effects","title":"Some More Effects","text":""},{"location":"KB/SLAK/#trade-off-between-kbsparsitymd-and-width","title":"TRADE-OFF BETWEEN KB/Sparsity.md AND WIDTH","text":"<ul> <li>As we expected, the model's performance keeps increasing as model width</li> <li>increases until the width factor reaches 1.5x, after which increasing width further starts to hurt the performance apparently due to the training difficulties associated with highly sparse neural networks.</li> </ul>"},{"location":"KB/SLAK/#effect-of-the-shorter-edge-n-on-slak","title":"EFFECT OF THE SHORTER EDGE N ON SLAK","text":"<ul> <li>We vary the shorter edge N 2 [3, 5, 7] and report the accuracy. All models were trained with AdamW on ImageNet-1K for 120 epochs. We empirically find that N=5 give us the best results, whereas N = 3 and N = 7 has slightly lower accuracy. We hence think it reasonable to choose N = 5 as the default option.</li> </ul>"},{"location":"KB/SLAK/#erf-quantitation-of-models-with-different-kernel-sizes","title":"ERF QUANTITATION OF MODELS WITH DIFFERENT KERNEL SIZES","text":"<ul> <li>Larger r suggests a smoother distribution of high-contribution pixels. We can see that with global kernels, SLaK naturally considers a larger range of pixels to make decisions than ConvNeXt and RepLKNet.</li> </ul>"},{"location":"KB/SLAK/#configurations-of-dynamic-kbsparsitymd","title":"CONFIGURATIONS OF DYNAMIC KB/Sparsity.md","text":"<ul> <li>Following Liu et al. (2021c), we specifically tune two factors for SLaK-T that control the strength of weight adaptation, adaptation frequency f and adaptation rate p. Adaptation frequency determines after how many training iterations we adjust the sparse weights, and the latter controls the ratio of the weight that we adjust at each adaptation</li> <li>f = 2000</li> <li>and p = 0.5 works best for SLak-T. For SLak-S/B, we directly choose f = 100 and p = 0.3 without careful tuning.</li> </ul>"},{"location":"KB/SLAK/#limitations","title":"LIMITATIONS","text":"<ul> <li>sparse architecture is implemented with binary masks due to the limited support of sparse neural networks by the commonly used hardware such as GPU and TPU</li> <li>Therefore, the inference FLOPs reported in the main paper are the theoretical values.</li> <li>Once this great potential is supported in the future, it can have a significant positive impact on our planet by saving a huge amount of energy and reducing overall total carbon emissions.</li> <li>Although not the focus of this current work, it would be interesting for future work to examine the speedup of sparse large kernels, using such specialized hardware accelerators, as we see much improvement room of promise here.</li> </ul>"},{"location":"KB/SMOTE/","title":"SMOTE","text":""},{"location":"KB/SMOTE/#smote","title":"SMOTE","text":"<ul> <li>@SMOTESyntheticMinority</li> <li>is a popular augmentation used to alleviate problems with class imbalance. This technique is applied to the feature space by joining the k nearest neighbors to form new instances.</li> </ul>"},{"location":"KB/SMP/","title":"SMP","text":""},{"location":"KB/SMP/#smp","title":"SMP","text":"<ul> <li>Architecture where multiple processors share a single address space and access to all resources</li> <li>Shared memory computing</li> <li>Connected by a bus</li> </ul>"},{"location":"KB/SOMs/","title":"Self Organizing Maps","text":""},{"location":"KB/SOMs/#self-organizing-maps","title":"Self Organizing Maps","text":"<ul> <li>Kohonen maps</li> <li>Crumbled up grid of SOM neurons</li> <li> </li> <li>Essentially : need to map a high dim space to a grid of neurons while trying to preserve the neighborhood relations from the high dim space. This is technically impossible so compromise.</li> <li>First initialized with small random values</li> <li>For each new pattern, identify Best Maching Unit based on current vectors. Reduce the value of r. And pull the point to the part of the grid with similar weight vectors.<ul> <li>Update weights \\(\\(w(v_{kl}) \\leftarrow w(v_{kl}) + \\lambda f_r(d(v_{kl}, v_{BMU}))(x-w(v_{kl}))\\)\\)</li> <li>\\(\\lambda\\) is learning rate</li> <li>d is Euclidean Distance between two neurons in grid.</li> <li>\\(f_{r}(0)=1\\). Tends to 0 as argument grows. r is radius. Greater values, will spread it out.</li> <li>Regulated by \\(f_r(d(v_{kl}, v_{BMU}))\\)</li> </ul> </li> <li>Eventually this will lead to an organization. Covered evenly after a while. Eventually neighbors of \\(v_0\\) would have weights towards \\(w(v_0)\\) . And \\(w(v_{0)} \\approx mean(all patterns x)\\)</li> <li>Repeat until response stops. Each members BMU rate is too low to expand.</li> <li>Start with large r and then slow down.</li> </ul>"},{"location":"KB/SOMs/#neuromorphic","title":"neuromorphic","text":""},{"location":"KB/SP-LIME/","title":"SP-LIME","text":""},{"location":"KB/SP-LIME/#sp-lime","title":"SP-LIME","text":"<ul> <li> <p>@ribeiroWhyShouldTrust2016a</p> </li> <li> <p>model judges whether you can trust the whole model or not. It selects a picked diverse set of representative instances with LIMEs via submodular optimization. The user should evaluate the black box by regarding the feature words of the selected instances. It is conceivable that it also recognizes bias or systematic susceptibility to adversarial examples. With this knowledge, it is also possible to improve a bad model. SP-LIME was researched with text data, but the authors claimed that it can be transferred to models for any data type.</p> </li> <li>This was inspired by CAM and Grad-CAM and tested the explanator on randomly chosen images from the COCO dataset [91], applied to the pre-trained neural network VGG-16 using the Kullback\u2013Leibler (KL) divergence</li> </ul>"},{"location":"KB/SQL-Tutor/","title":"SQL-Tutor","text":""},{"location":"KB/SQL-Tutor/#sql-tutor","title":"SQL-Tutor","text":"<ul> <li>Outer loop: SQL-Tutor (http://www.aw-bc.com/databaseplacedemo/sqltutor.html) teaches students how to write a query to a relational database (B. Martin &amp; Mitrovic, 2002; Mitrovic, 2003; Mitrovic &amp; Ohlsson, 1999). Each task consists of a database and information to be retrieved from it. In Figure 5, for instance, the student has been given a database of movie information and has been asked to write a query that will List the toc: true titles and numbers of all movies that have won at least one Academy Award and have been made in or after 1988.</li> <li>Inner loop: Students write a query in the SQL language by clicking on buttons and filling in blanks. This may take several minutes. At any point, they can press the Submit Answer button. The tutor, which has been completely silent up until now, analyzes the student's query to find its flaws. It gives a variety of levels of feedback and hints.</li> <li>One way to think of SQL-Tutor's inner loop is that the student takes multiple steps, each comprised of filling in a blank in the query</li> <li>Unlike tutors that give feedback as soon as a student had taken a step, SQL-Tutor delays its feedback until the student requests it.</li> <li>Step analysis: In order to analyze steps, SQL-Tutor has a set of constraints, where a constraint consists of a relevance condition and a satisfaction condition. If the relevance condition is false of the students' step, then the constraint is irrelevant, so the tutor says nothing about it. If the constraint has a true relevance condition and a true satisfaction condition, then the constraint is satisfied and the tutor says nothing about it.</li> <li>If the relevance condition is true, and the satisfaction condition is false, then the student's step violates the constraint and the tutor has identified a topic worth talking about. In particular, every constraint has two messages.</li> <li>Depending on the feedback level selected by the tutor or the student, one of them may be presented to the student when the constraint is violated. One message describes the constraint and its violation briefly. The other presents more details.</li> <li>Although the constraints are task independent, many of them refer to a correct solution of the problem, which is stored in the tutoring system.</li> <li>The relationship between steps, learning events and constraints is quite simple in the SQL-Tutor. Each constraint corresponds to a Knowledge Component.</li> </ul>"},{"location":"KB/SQuAD/","title":"SQuAD","text":""},{"location":"KB/SQuAD/#squad","title":"SQuAD","text":""},{"location":"KB/SRN/","title":"SRN","text":""},{"location":"KB/SRN/#srn","title":"SRN","text":"<ul> <li>Just a simple RNN Cell</li> <li></li> <li></li> <li>Vanishingexploding gradients , in Backprop, they break down when sequences are long.</li> <li>Distance between the relevant words are too long</li> <li>Followed up [LSTM)](Long Short Term Memory (LSTM|Long Short Term Memory (LSTM|LSTM)](LSTM)](Long Short Term Memory (LSTM|Long Short Term Memory (LSTM|LSTM).md).md)</li> </ul>"},{"location":"KB/SSR/","title":"SSR","text":""},{"location":"KB/SSR/#ssr","title":"SSR","text":"<ul> <li>Smallest sufficient region</li> <li>smallest region of the image that alone allows a confident classification</li> </ul>"},{"location":"KB/STL-10/","title":"STL-10","text":""},{"location":"KB/STL-10/#stl-10","title":"STL-10","text":"<ul> <li>specifically designed for developing unsupervised feature learning </li> <li>500 labeled training images, 800 testing images, and 100, 000 unlabeled images # covering 10</li> <li>classes which include airplane, bird, car, cat, deer, dog, horse, monkey, ship, and truck. </li> </ul>"},{"location":"KB/SUNCG/","title":"SUNCG","text":""},{"location":"KB/SUNCG/#suncg","title":"SUNCG","text":"<ul> <li>UNCG dataset is a large synthetic 3D scene repository for indoor scenes which con- sists of over 45, 000 different scenes with manually created realistic room and furniture layouts </li> <li>The synthetic depth, object level semantic labels, and volumetric ground truth are available</li> </ul>"},{"location":"KB/SVHN/","title":"SVHN","text":"<p>toc: true title: SVHN</p> <p>categories: ['temp']</p>"},{"location":"KB/SVHN/#svhn","title":"SVHN","text":"<p>toc: true title: SVHN categories: ['temp']</p>"},{"location":"KB/SVHN/#svhn_1","title":"SVHN","text":"<ul> <li>recognizing digits and numbers in natural scene images which obtained from house numbers from Google Street View images </li> <li>600,000 images and all digits have been resized to a fixed resolution of 32 x 32 pixel</li> </ul>"},{"location":"KB/SYNTHIA/","title":"SYNTHIA","text":""},{"location":"KB/SYNTHIA/#synthia","title":"SYNTHIA","text":""},{"location":"KB/Saddle%20Points/","title":"Saddle Points","text":""},{"location":"KB/Saddle%20Points/#saddle-points","title":"Saddle Points","text":"<ul> <li>Points where the gradient is zero but it is at the same a local minima and maxima (both relative)</li> <li></li> </ul>"},{"location":"KB/Safeguard/","title":"Safeguard","text":""},{"location":"KB/Safeguard/#safeguard","title":"Safeguard","text":"<ul> <li>A barrier guard, device or safety procedure designed for the protection of personnel</li> </ul>"},{"location":"KB/Safety%20Integrity%20Level/","title":"Safety Integrity Level","text":""},{"location":"KB/Safety%20Integrity%20Level/#safety-integrity-level","title":"Safety Integrity Level","text":"<ul> <li>Safety Integrity Level (SIL) is IEC\u2019s method for determining the performance level of a safety system. SIL 2 corresponds to ISO Performance Level \u201cd\u201d, and SIL 3 corresponds to ISO Performance Level \u201ce\u201d. ISO 10218 allows for the use of either.</li> </ul>"},{"location":"KB/Safety%20Logic%20Circuit/","title":"Safety Logic Circuit","text":""},{"location":"KB/Safety%20Logic%20Circuit/#safety-logic-circuit","title":"Safety Logic Circuit","text":"<ul> <li>The safety logic circuit monitors safety critical external devices such as the light curtains and FSU generated signals. The safety logic circuit is programmed via an intuitive user interface that is supported on the Yaskawa programming pendant. It enables to set up the logical operations, such as stopping the manipulator or outputting a signal if the servos are on.</li> </ul>"},{"location":"KB/Saffran%2C%20Aslin%20and%20Newport/","title":"Saffran, Aslin and Newport","text":""},{"location":"KB/Saffran%2C%20Aslin%20and%20Newport/#saffran-aslin-and-newport","title":"Saffran, Aslin and Newport","text":"<ul> <li>8-month-olds can segment a continuous stream of speech syllables, containing no acoustic or prosodic cues to word boundaries, into wordlike units after only 2 min of listening experience</li> </ul>"},{"location":"KB/Salemi%20and%20Zamani%20-%202024%20-%20Evaluating%20Retrieval%20Quality%20in%20Retrieval-Augmente/","title":"Salemi and Zamani - 2024 - Evaluating Retrieval Quality in Retrieval-Augmente","text":"","tags":["llm"]},{"location":"KB/Salemi%20and%20Zamani%20-%202024%20-%20Evaluating%20Retrieval%20Quality%20in%20Retrieval-Augmente/#salemi-and-zamani-2024-evaluating-retrieval-quality-in-retrieval-augmente","title":"Salemi and Zamani - 2024 - Evaluating Retrieval Quality in Retrieval-Augmente","text":"<ul> <li>@salemiEvaluatingRetrievalQuality2024</li> </ul>","tags":["llm"]},{"location":"KB/Salemi%20and%20Zamani%20-%202024%20-%20Evaluating%20Retrieval%20Quality%20in%20Retrieval-Augmente/#evaluating-retrieval-quality-in-retrieval-augmented-generation-evaluatingretrievalqualityinretrieval-augmentedgeneration","title":"Evaluating Retrieval Quality in Retrieval-Augmented Generation {#evaluatingretrievalqualityinretrieval-augmentedgeneration}","text":"","tags":["llm"]},{"location":"KB/Salemi%20and%20Zamani%20-%202024%20-%20Evaluating%20Retrieval%20Quality%20in%20Retrieval-Augmente/#note-something-is-fishy-about-this-paper","title":"Note: Something is Fishy about This Paper.","text":"<ul> <li>Instead of passing end to end, they want to pass documents one by one for evaluation. That seems slightly suspicious how they got so much improvement by doing that :/</li> <li>All the part about correlation also seems a little weird.</li> <li>Something does not add up about this paper.</li> <li>eRAG</li> <li>each document in the retrieval list is individually utilized by the large language model within the RAG system</li> <li>improvements in Kendall's g correlation ranging from 0.168 to 0.494</li> </ul>","tags":["llm"]},{"location":"KB/Salemi%20and%20Zamani%20-%202024%20-%20Evaluating%20Retrieval%20Quality%20in%20Retrieval-Augmente/#introduction","title":"INTRODUCTION","text":"<ul> <li>end-to-end evaluation lacks transparency regarding which retrieved document contributed to the generated output, hindering interpretability of the system's behavior</li> <li>resource- intensive, consuming signifcant time and computational power, particularly when dealing with a large set of retrieval results con- sumed by the LLM</li> <li>many ranking systems rely on interleaving (i.e., replacing one or more documents in the result list) for evaluation and optimization, which further complicates the evaluation, as slight variations in retrieval results necessitate re-computation of the RAG pipeline</li> <li>optimizing ranking models often requires document-level feedback, such as user clicks</li> </ul>","tags":["llm"]},{"location":"KB/Salemi%20and%20Zamani%20-%202024%20-%20Evaluating%20Retrieval%20Quality%20in%20Retrieval-Augmente/#evaluating-retrievers-in-rag","title":"EVALUATING RETRIEVERS IN RAG","text":"<ul> <li>human judgment to assess the relevance of a query to documents within a corpus</li> <li>human annotation can be costly and is often impractical for evaluating all documents in a corpus</li> <li>downstream ground truth out- put associated with the query to provide weak relevance labels</li> <li>eRAG, a novel approach that involves utilizing the LLM in RAG system itself as the arbiter for generating labels to evaluate the retrieval model. </li> </ul>","tags":["llm"]},{"location":"KB/Salemi%20and%20Zamani%20-%202024%20-%20Evaluating%20Retrieval%20Quality%20in%20Retrieval-Augmente/#using-downstream-large-language-model-in-rag-as-doc-ument-annotator","title":"Using Downstream Large Language Model in RAG as Doc- Ument Annotator","text":"","tags":["llm"]},{"location":"KB/Salemi%20and%20Zamani%20-%202024%20-%20Evaluating%20Retrieval%20Quality%20in%20Retrieval-Augmente/#retrieval-evaluation-metrics","title":"Retrieval Evaluation Metrics","text":"","tags":["llm"]},{"location":"KB/Salemi%20and%20Zamani%20-%202024%20-%20Evaluating%20Retrieval%20Quality%20in%20Retrieval-Augmente/#main-findings","title":"Main Findings","text":"<ul> <li>How do different retrieval evaluation methods correlate with the end-to-end downstream performance in RAG?.</li> <li>Interestingly, the most common approaches, KILT Provenance and Annotation with LLMs, that are, document-level relevance labels and using LLMs to assign a relevance label to each retrieved document, have the lowest correlation with the down- stream performance of the LLM</li> <li>How do different retrieval evaluation methods in RAG per- form as the size of retrieval results increases?</li> <li>varied the number of retrieved documents and computed the correlation between the metric with highest correlation for each method in Table 1 at each specifed number of retrieved documents and the downstream performance of the LLM given that number of retrieved documents </li> </ul>","tags":["llm"]},{"location":"KB/Salicon%20dataset/","title":"Salicon dataset","text":""},{"location":"KB/Salicon%20dataset/#salicon-dataset","title":"Salicon Dataset","text":"<ul> <li>\"10,000 images for training and 5000 images for validation\"</li> <li>\"Each image was viewed by 60 observers\"</li> <li>this dataset Different from other fixation datasets, is large-scale mouse-tracking data through Amazon Mechanical Turk</li> <li>Although this dataset is not the fixation dataset, it is well known that the distribution of mouse tracking points is similar to the distribution of fixations and that the parameters of the model trained on Salicon dataset are useful for saliency map estimation</li> </ul>"},{"location":"KB/Salience%20Map/","title":"Salience Map","text":""},{"location":"KB/Salience%20Map/#salience-map","title":"Salience Map","text":""},{"location":"KB/Salience%20Map/#explained","title":"Explained","text":"<ul> <li>Specifies parts of the image that contribute the most to the activity of a specific layer or the entire decision</li> <li>\\(Y_{c}=\\text{score of class c}\\) : value of output before softmax</li> <li> \\[saliency = max_{r,g,b}(|\\frac{\\partial Y_{c}}{\\partial I}|)\\] </li> <li>grad of \\(Y_{c}\\) wrt input - matrix with shape similiar to input</li> <li>if value close to 0 : small changes in input have no effect on output</li> <li>if high magnitude : small changes can have a major impact</li> <li>positive : roughly the location of the target object</li> <li>negative : competing class objects : background or instance</li> <li>Abs value for heatmap</li> <li>grads : backprop on \\(Y_{c}\\) instead of loss</li> </ul>"},{"location":"KB/Salience%20Map/#three-approaches","title":"Three Approaches","text":""},{"location":"KB/Salience%20Map/#dencov","title":"Dencov","text":"<ul> <li>Use DeconvNet</li> <li>Use backprop to compute the gradients of logits wrt input : Deep Inside Convolutional Networks</li> <li>Guided BackProp</li> </ul>"},{"location":"KB/Salience%20Map/#features","title":"Features","text":"<ul> <li>One of the oldest interpretation methods</li> <li>Salience maps of important features are calculated, and they show superpixels that have influenced the prediction most, for example</li> <li>To create a map of important pixels, one can repeatedly feed an architecture with several portions of inputs and compare the respective output, or one can visualize them directly by going rearwards through the inverted network from an output of interest;</li> <li>Grouped in this category as well is exploiting neural networks with activation atlases through feature inversion. This method can reveal how the network typically represents some concepts</li> <li>Considering image or text portions that maximize the activation of interesting neurons or whole layers can lead to the interpretation of the responsible area of individual parts of the architecture.</li> </ul>"},{"location":"KB/Saliency%20using%20natural%20statistics/","title":"Saliency using natural statistics","text":""},{"location":"KB/Saliency%20using%20natural%20statistics/#saliency-using-natural-statistics","title":"Saliency Using Natural Statistics","text":"<ul> <li>applies a Bayesian framework using local feature maps to estimate saliency maps.</li> <li>The probability distribution of features is learned not from each individual test image but from statistics calculated over the training set of natural images.</li> </ul>"},{"location":"KB/Saliency%20vs%20Attention/","title":"Saliency vs Attention","text":""},{"location":"KB/Saliency%20vs%20Attention/#saliency-vs-attention","title":"Saliency Vs Attention","text":"<ul> <li>Input saliency methods are addressing the goal head-on: they reveal why one particular model prediction was made in terms of how relevant each input word was to that prediction</li> <li>input saliency methods typically take the entire computation path into account, all the way from the input word embeddings to the target output prediction value</li> <li>Attention weights do not: they reflect, at one point in the computation, how much the model attends to each input representation, but those representations might already have mixed in information from other inputs</li> <li>Ironically, attention-as-explanation is sometimes evaluated by comparing it against gradient-based measures, which again begs the question why we wouldn't use those measures in the first place</li> <li>In terms of efficiency, it is true that for attention only a forward pass is required, but many other methods discussed at most require a forward and then a backward pass, which is still extremely efficient.</li> </ul>"},{"location":"KB/SaliencyMix/","title":"SaliencyMix","text":""},{"location":"KB/SaliencyMix/#saliencymix","title":"SaliencyMix","text":"<ul> <li>@uddinSaliencyMixSaliencyGuided2021</li> <li>extracts salient regions and pastes them on the corresponding location in the target image</li> <li>The salient region is extracted around the maximum intensity pixel location in the saliency map</li> <li>Regional dropout is one of the popular solutions that guides the model to focus on less discriminative parts by randomly removing image regions, resulting in improved regularization</li> <li>however, such information removal is undesirable.</li> <li>On the other hand, recent strategies suggest to randomly cut and mix patches and their labels among training images, to enjoy the advantages of regional dropout without having any pointless pixel in the augmented images.</li> <li>selects a representative image patch with the help of a saliency map and mixes this indicative patch with the target image</li> <li>Furthermore, models that are trained with SaliencyMix help to improve the object detection performance</li> <li>we cut a source patch and mix it to the target image and also mix their labels proportionally to the size of the mixed patches</li> <li>But in order to prevent the model from learning any irrelevant feature representation, the proposed method enforces to select a source patch in a way so that it must contains information about the source object</li> <li>It first extracts a saliency map of the source image to highlight the objects of interest and then selects a patch surrounding the peak salient region to mix with the target image.</li> </ul>"},{"location":"KB/SaliencyMix/#algorithm","title":"Algorithm","text":"<ul> <li>If \\(I_{s} \\in \\mathbb{R}^{W \\times H \\times C}\\) is randomly selected training source with label \\(y_{s}\\). Saliency map is \\(I_{vs}= f(I_{s})\\) where \\(I_{vs}\\) is the visual saliency. \\(f(\\cdot)\\) is a saliency model. </li> <li>Search for a pixel \\(I_{vs}^{i,j}\\) with maximum intensity</li> <li>$$</li> </ul> <p>i,j = argmax(I_{vs}) $$ - Patch selected by centering on the \\(I_{vs}^{i,j}\\) pixel or with this pixel in the patch - Patch determined on a ration \\(\\lambda\\) chosen from Uniform Distribution </p>"},{"location":"KB/SaliencyMix/#mixing-patches-and-labels","title":"Mixing Patches and Labels","text":"<ul> <li>Another image \\(I_{s} \\in \\mathbb{R}^{W \\times H \\times C}\\)</li> <li>\\(I_{a} = M \\odot I_{s} + M' \\odot I_{t}\\)<ul> <li>\\(M \\in \\{0,1\\}^{W, H}\\)</li> </ul> </li> <li>labels \\(y_{a} = \\lambda y_{t}+ (1-\\lambda)y_{s}\\)</li> </ul>"},{"location":"KB/SaliencyMix/#different-ways-of-selecting-and-mixing-the-source-patch","title":"DIFFERENT WAYS OF SELECTING AND MIXING THE SOURCE PATCH","text":"<ul> <li>schemes: (i) Salient to Corresponding, that selects the source patch from the most salient region and mix it to the corresponding location of the target image; (ii) Salient to Salient, that selects the source patch from the most salient region and mix it to the salient region of the target image; (iii) Salient to Non-Salient, that selects the source patch from the most salient region but mix it to the non-salient region of the target image; (iv) Non-Salient to Salient, that selects the source patch from the non-salient region of the source image but mix it to the salient region of the target image; and (v) Non-Salient to NonSalient, that selects the source patch from the non-salient region of the source image and also mix it to the non-salient region of the target image.</li> <li>To find out the non-salient region, we use the least important pixel of an image.</li> </ul>"},{"location":"KB/SaliencyMix/#images","title":"Images","text":""},{"location":"KB/Salient%20Object%20Strategy/","title":"Salient Object Strategy","text":""},{"location":"KB/Salient%20Object%20Strategy/#salient-object-strategy","title":"Salient Object Strategy","text":"<ul> <li>\"if an object is contextually relevant, then it is salient\" (Philip 2011: 370\u2013371)</li> </ul>"},{"location":"KB/Sample%20Correlation/","title":"Sample Correlation","text":"<p>toc: true title: Sample Correlation</p> <p>categories: ['temp']</p>"},{"location":"KB/Sample%20Correlation/#sample-correlation","title":"Sample Correlation","text":"<ul> <li>A type of Correlation</li> <li> \\[r_{xy}= \\frac{s_{xy}}{s_{x}s_{y}}\\] </li> <li>\\(s_{x}\\), \\(s_y\\) are the sample Standard Deviation</li> <li>\\(s_y\\) is the sample Covariance</li> </ul>"},{"location":"KB/Sample%20Pairing/","title":"Sample Pairing","text":""},{"location":"KB/Sample%20Pairing/#sample-pairing","title":"Sample Pairing","text":"<ul> <li>@inoueDataAugmentationPairing2018</li> <li>merges two images by averaging their pixel intensities</li> <li>The new image has the same training image label opposite to MixUp and other approaches where labels are updated according to the proportion of image mixing.</li> <li>one epoch on ImageNet and 100 epochs on other datasets are completed without SamplePairing before mixed image data is added to the training</li> <li>Once the SamplePairing images are added to the training set, they run in cycles between 8:2 epochs, 8 with SamplePairing images, 2 without.</li> </ul>"},{"location":"KB/Sampler/","title":"Sampler","text":""},{"location":"KB/Sampler/#sampler","title":"Sampler","text":"<ul> <li>Given :<ul> <li>\\(P_{X}\\) is a distribution on a measure space (E,B)</li> <li>A seq of \\(X_{1}, X_{2}, \u2026\\) of random variables is a sampler if for all \\(A \\in B\\)</li> <li> \\[P_{X}(A) = lim_{N \\rightarrow \\infty} \\frac{1}{N}\\Sigma_{i=1}^{N}1_{A}\\circ X_{i}\\] </li> <li>\\(1_A\\) is an indicator function for A</li> </ul> </li> <li>\\(X_{1}, X_{2}, \u2026\\) need not have the same distribution or need to be independant</li> <li>Dream : All \\(X_{i}\\) are uniformly distributed on [0,1]. (Impossible)</li> </ul>"},{"location":"KB/Sampling%20Bias/","title":"Sampling Bias","text":""},{"location":"KB/Sampling%20Bias/#sampling-bias","title":"Sampling Bias","text":"<ul> <li>Data is not collected randomly from the target group.</li> </ul>"},{"location":"KB/Sampling%20Ray%20Casting/","title":"Sampling Ray Casting","text":""},{"location":"KB/Sampling%20Ray%20Casting/#sampling-ray-casting","title":"Sampling Ray Casting","text":"<ul> <li>selection of positions along the ray</li> <li>Early Ray Termination</li> <li></li> </ul>"},{"location":"KB/Sanity%20Checks%20for%20Saliency%20Maps/","title":"Sanity Checks for Saliency Maps","text":""},{"location":"KB/Sanity%20Checks%20for%20Saliency%20Maps/#sanity-checks-for-saliency-maps","title":"Sanity Checks for Saliency Maps","text":"<ul> <li>@adebayoSanityChecksSaliency2020</li> <li>whether saliency methods are insensitive to model and data</li> <li>Insensitivity is highly undesirable, because it would mean that the \"explanation\" isunrelated to model and data</li> <li>Methods that are insensitive to model and training data are similar to edge detectors</li> <li>Edge detectors simply highlight strong pixel color changes in images and areunrelated to a prediction model or abstract features of the image, and require no training</li> <li>The methods tested were Vanilla Gradient, Gradient x Input, Integrated Gradients,Guided Backpropagation, Guided Grad-CAM and SmoothGrad (with VanillaGradient).</li> <li>Vanilla Gradient and Grad-CAM passed the insensitivity check, while GuidedBackpropagation and Guided Grad-CAM failed</li> <li>However, the sanity checks paper itself has found some criticism from Tomsett et al.(2020) 89 with a paper called \"Sanity checks for caliency metrics\" (of course</li> <li>They found that there is a lack of consistency for evaluation metrics</li> <li>So we are back to where we started ... It remains difficult to evaluate the visual explanations. This makes it very difficult for a practitioner.</li> </ul>"},{"location":"KB/Satellite%20Cell/","title":"Satellite Cell","text":""},{"location":"KB/Satellite%20Cell/#satellite-cell","title":"Satellite Cell","text":"<ul> <li>Surround neuron cell bodies</li> <li>Similar to Astrocyte</li> </ul>"},{"location":"KB/Satisficing%20Heuristic/","title":"Satisficing Heuristic","text":""},{"location":"KB/Satisficing%20Heuristic/#satisficing-heuristic","title":"Satisficing Heuristic","text":"<ul> <li>Good enough</li> <li>Less time</li> <li>Less knowledge</li> </ul>"},{"location":"KB/Scalar%20Articles/","title":"Scalar Articles","text":""},{"location":"KB/Scalar%20Articles/#scalar-articles","title":"Scalar Articles","text":""},{"location":"KB/Scalar%20Articles/#done","title":"Done","text":"<ul> <li>[Word2Vec] with gensim</li> <li>CycleGAN</li> <li>[Masked Language Modeling] with BERT</li> <li>DCGAN</li> <li>Conditional GAN</li> <li>Stack GAN</li> <li>Basic GAN</li> </ul>"},{"location":"KB/Scalar%20Articles/#in-progress","title":"In Progress","text":"<ul> <li>Generative vs Discriminative Models</li> </ul>"},{"location":"KB/Scalar%20Color%20Coding/","title":"Scalar Color Coding","text":""},{"location":"KB/Scalar%20Color%20Coding/#scalar-color-coding","title":"Scalar Color Coding","text":"<ul> <li>Mean Diffusivity</li> <li>Fractional Anisotropy</li> <li>Eigenvector</li> </ul>"},{"location":"KB/Scalar%20Register/","title":"Scalar Register","text":""},{"location":"KB/Scalar%20Register/#scalar-register","title":"Scalar Register","text":"<ul> <li>Single elements for interconnecting Vector Functional Units, Vector Load Store Units, and registers</li> </ul>"},{"location":"KB/Scaled%20Dot%20Product%20Attention/","title":"Scaled Dot Product Attention","text":""},{"location":"KB/Scaled%20Dot%20Product%20Attention/#scaled-dot-product-attention","title":"Scaled Dot Product Attention","text":"<ul> <li>Vaswani et al., 2017</li> <li>Q is query, K is key V is value. Same dims</li> <li>\\(q_{i}= W_{q}x_i\\) , \\(k_{i}= W_{k}x_{i}\\) , \\(v_{i}= W_{v}x_{i}\\)<ul> <li>\\(w_{ij}' = q_{i}^{T}k_{j}\\)</li> <li>\\(y_{i}= \\Sigma_{j}w_{ij}v_{j}\\)</li> </ul> </li> <li>Softmax is sensitive to large values. Which sucks for the #gradients</li> <li>The avg value of the dot product grows with embedding dimension k. So scale back.<ul> <li>\\(\\sqrt{k}\\) . Vector in \\(\\mathbb{R}^{k}\\) with all values as c</li> <li>Euclidean length is \\(\\sqrt{kc}\\)</li> </ul> </li> <li> \\[Attention(Q, K,V) = softmax(\\frac{QK^T}{\\sqrt{d_{k}}})V\\] </li> <li>Generalization of Soft Attention</li> <li></li> <li>Attention Alignment score \\(\\(\\alpha_{t,i} = \\frac{s_{t}^{T}h_{i}}{\\sqrt{n}}\\)\\)</li> </ul>"},{"location":"KB/Scaled%20benefits/","title":"Scaled benefits","text":""},{"location":"KB/Scaled%20benefits/#scaled-benefits","title":"Scaled Benefits","text":"<ul> <li>The more prepared - the more benefits</li> </ul>"},{"location":"KB/Scatter%20and%20Gather/","title":"Scatter and Gather","text":""},{"location":"KB/Scatter%20and%20Gather/#scatter-and-gather","title":"Scatter and Gather","text":"<ul> <li>Retrieves data elements scattered thorughout memory and packs them into sequential vectors in vector registers</li> <li>Promotes data locality and reduces data pollution</li> </ul>"},{"location":"KB/Scene%20based%20text%20to%20image%20generation/","title":"Scene based text to image generation","text":""},{"location":"KB/Scene%20based%20text%20to%20image%20generation/#scene-based-text-to-image-generation","title":"Scene Based Text to Image Generation","text":"<ul> <li>Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors</li> <li>text-to-image generation</li> <li>enabling a simple control mechanism complementary to text in the form of a scene</li> <li>introducing elements that substantially improve the tokenization process by employing domain-specific knowledge over key image regions</li> <li>adapting classifier-free guidance for the transformer use case</li> <li>They attempt to progress text-to-image generation towards a more interactive experience, where people can perceive more control over the generated outputs, thus enabling real-world applications such as storytelling</li> <li>focus on improving key image aspects that are significant in human perception, such as faces and salient objects, resulting in higher favorability of their method in human evaluations and objective metrics</li> <li>Through scene controllability, they introduce several new capabilities: (i) scene editing, (ii) text editing with anchor scenes, (iii) overcoming out-of-distribution text prompts, and (iv) story illustration generation</li> </ul>"},{"location":"KB/SceneNet%20RGB-D/","title":"SceneNet RGB-D","text":""},{"location":"KB/SceneNet%20RGB-D/#scenenet-rgb-d","title":"SceneNet RGB-D","text":"<ul> <li>large indoor synthetic video dataset which consists of 5 million rendered RGB-D images from over 15K trajectories in synthetic layouts with random but physically simulated object poses </li> <li>pixel level annotations for scene understanding problems such as semantic segmentation, instance segmentation, and object detection, and also for geometric computer vision problems such as optical flow, depth estimation, camera pose estimation, and 3D reconstruction</li> </ul>"},{"location":"KB/Scheduling/","title":"Scheduling","text":""},{"location":"KB/Scheduling/#scheduling","title":"Scheduling","text":"<ul> <li>Some prune all the weights at once</li> <li>Others prune iteratively using loops or some other condition</li> </ul>"},{"location":"KB/Schipol%20Data%20Scientist/","title":"Schipol Data Scientist","text":""},{"location":"KB/Schipol%20Data%20Scientist/#schipol-data-scientist","title":"Schipol Data Scientist","text":"<ul> <li>Bonus:<ul> <li>Have experience with computer vision.</li> <li>Have knowledge about cloud computing</li> </ul> </li> <li>Work on scaling up our DeepTurnaround product to a multi-airport product.</li> <li>Continuous monitoring, development and optimization of our state-of-the-art Computer Vision model.</li> <li>Develop and implement new technical features.</li> <li>Work within a skilled development product team consisting of multiple data scientists, data engineers and full stack developers.</li> <li>And\u2026 work at a cool location where you will see aircrafts passing by your window.\u00a0</li> <li>By leveraging our AI systems, we generate detections from camera images in real-time to capture events occurring around the aircraft. Ultimately, our aim is to enhance the decision-making process in operations, leading to a reduction in delays and CO2 emissions.</li> </ul>"},{"location":"KB/Schipol%20Data%20Scientist/#motivation","title":"Motivation","text":"<ul> <li>DeepTurnaround <ul> <li>The turnaround process is a series of tasks that need to be completed from the time an aircraft arrives at the assigned gate until it is ready for departure - A large scale optimization process</li> <li>Useful tech, saves time, money and makes the Schipol experience even better for travellers</li> <li>Impact of work</li> <li>Guess how it works <ul> <li>Object detection for individual cases</li> <li>combined data from two cameras</li> </ul> </li> <li>Potential ideas for improvement<ul> <li>In the future, we expect to include features which can benefit safety and sustainability officers/managers as well. - Thesis on XAI</li> <li>Perhaps a third camera/ for more personell focused view, even more fine grained control. (Eg. Uniforms, crowd density )</li> </ul> </li> <li>Computer Vision + Datascience<ul> <li>Perfect mix of things where I can contribute to</li> <li>experience</li> </ul> </li> </ul> </li> </ul>"},{"location":"KB/Schipol%20Data%20Scientist/#recruiter-email","title":"Recruiter Email","text":"<ul> <li>Resume sent through the process</li> <li>Is this position still open?</li> <li>Just finished a masters in AI from RUG.</li> <li>DeepTurnaround - have many ideas to contribute and work with implementing for a better airport experience at either Schipol or other customers</li> </ul>"},{"location":"KB/Schipol%20Data%20Scientist/#motivation-letter","title":"Motivation Letter","text":"<p>Hello!</p> <p>This is Subhaditya Mukherjee. I graduated from the University of Groningen with a masters in Artificial Intelligence last month and just started looking for a job here in the Netherlands. My area of expertise and interest is a combination of Computer Vision and Data Analytics, which is what this position is about. </p> <p>I enjoy traveling and am familiar with how frustrating airports can be on a long journey. But there are hundreds of moving parts, and even finding analytics for the same can be a challenge. Deep Turnaround is a very interesting project to me for this reason. The website does not state many technical details, but it seems that the system is a combination of custom data collection, fine-grained object detection networks, and data analytics, each of which must be quite challenging, given the scope of the task. </p> <p>That being said, I have some ideas that could make Deep Turnaround even more useful to our stakeholders, and I would love to discuss and work on them to help make the airport experience a little more streamlined. Of course, I would be willing to work on any other feature that is proposed. These are just some ideas. (These might be already implemented, but I did not see these labels in the video on the website). One of the first ideas that comes to mind is using the two existing cameras to also analyze crowd information to make the passenger experience more streamlined and avoid chaos while boarding. Or perhaps, collecting pictures of worker uniforms would enable Deep Turnaround to log more specific events. Neither of these would require any extra infrastructure costs, which is probably also one of your constraints. </p> <p>My previous internships and projects (both personal and freelance) have included Deep Object Detection, Computer Vision, Data warehousing and analytics, and Dashboarding. These experiences will let me contribute to any AI team, and I am willing to learn whatever else is necessary for any future projects.</p> <p>I have a lot to learn, and I want to work with a team that is making a real-world impact in a sector close to my heart like this one. It is not every day you find a position that you enjoy and also think you can contribute something to, and I sincerely hope you give me a chance!</p> <p>Thank you, Subhaditya Mukherjee</p>"},{"location":"KB/Schipol%20Data%20Scientist/#email-to-recruiter","title":"Email to Recruiter","text":""},{"location":"KB/Schipol%20Data%20Scientist/#cold-email","title":"Cold email","text":"<p>fhoogenboom@schiphol.com - Floris Hoogenboom - Head of Data and AI products</p>"},{"location":"KB/Schwann%20Cell/","title":"Schwann Cell","text":""},{"location":"KB/Schwann%20Cell/#schwann-cell","title":"Schwann Cell","text":"<ul> <li>Insulate , helps form Myelin</li> <li>Similar to Ogliodendrocytes</li> </ul>"},{"location":"KB/ScoreCAM/","title":"ScoreCAM","text":""},{"location":"KB/ScoreCAM/#scorecam","title":"ScoreCAM","text":"<ul> <li>@wangScoreCAMScoreWeightedVisual2020</li> <li>In Score-Cam, we use the weights of the score obtained for a specific target class.</li> <li>The first step involved is passing images to a CNN model and performing a forward_pass. After the forward pass, the activations are extracted from last convolutional layer in the network. </li> <li>Each Activation Map obtained from the last layer having shape \\(1\\times m \\times n\\) is then upsampled using bilinear-interpolation to the same size as the Input Image. </li> <li>After Upsampling the activation maps, the resultant activation maps are normalized with each pixel within the range of [0,1] to maintain the relative intensities between the pixels <ul> <li> \\[A_{i,j}^{k}= \\frac{A_{i,j}^{k}}{max A^{k}- min A^{k}}\\] </li> </ul> </li> <li>After the Normalization of the Activation Maps is complete, the highlighted areas of the activation maps are projected on the input space by multiplying each normalized activation map(1 x W x H) with the Original Input Image(3 x W x H) to obtain a masked image M with shape 3 x W x H<ul> <li> \\[M^{k}= A^{k} \\cdot I\\] </li> </ul> </li> <li>The Masked Images M thus obtained are then passed to Convolutional Neural Network with SoftMax output<ul> <li> \\[S_{k} = Softmax(F(M^{k}))\\] </li> </ul> </li> <li>After getting the scores for each class we extract the score of the target class to represent the importance of the kth activation map.<ul> <li> \\[w_{k}^{c}= S_{k}^{c}\\] </li> </ul> </li> <li>compute the sum across all the activation maps for the linear combination between the target class score and each activation map</li> <li>apply pixel-wise ReLU to the final activation map<ul> <li> \\[L^{c}_{ScoreCAM} = ReLU(\\underset{k}\\Sigma w_{k}^{c}A^{k})\\] </li> </ul> </li> <li>ReLU because we are interested only in the features that have a positive influence on the class of interest</li> </ul>"},{"location":"KB/ScoreCAM/#advantages","title":"Advantages","text":"<ul> <li>can be used in any Convolutional Neural Network architecture and don't require retraining of the model to produce saliency maps like CAM</li> <li>class discriminative</li> <li>removes irrelevant noise to produce a meaningful saliency map</li> <li>Softmax scores as weights and removes the dependence on unstable gradients</li> </ul>"},{"location":"KB/Scoring%20Pruning%20Approaches/","title":"Scoring Pruning Approaches","text":""},{"location":"KB/Scoring%20Pruning%20Approaches/#scoring-pruning-approaches","title":"Scoring Pruning Approaches","text":"<ul> <li>Like all networks, scoring becomes essential when we try to choose which parameter to get rid of.</li> <li>Some authors suggest removing based on absolute values, others decide to prune based on the contributions of that parameter to the entire network.</li> <li>Others remove based on a score given.</li> <li>Some perform Pruning locally, while others perform it globally across the network.</li> </ul>"},{"location":"KB/Second%20Language%20Vocabulary%20Learning%20%2C%20The%20role%20of%20context%20%20versus%20translation/","title":"Second Language Vocabulary Learning , The role of context  versus translation","text":""},{"location":"KB/Second%20Language%20Vocabulary%20Learning%20%2C%20The%20role%20of%20context%20%20versus%20translation/#second-language-vocabulary-learning-the-role-of-context-versus-translation","title":"Second Language Vocabulary Learning , The Role of Context Versus Translation","text":"<ul> <li> <p>PETER PRINCE</p> </li> <li> <p>A widespread view of vocabulary learning is that it is advisable to make the shift away from learning words with their translations and to rely on second language (L2) context as soon as possible</p> </li> <li>Such faith in context learning has not always received experimental support, however, nor is it commonly shared by L2 learners</li> <li>An experiment in which subjects were tested on their KB/Recall.md of newly learned words was conducted to determine the relative advantages and disadvantages of both context learning and translation learning as a function of learner proficiency</li> </ul>"},{"location":"KB/Second%20Language%20Vocabulary%20Learning%20%2C%20The%20role%20of%20context%20%20versus%20translation/#results","title":"Results","text":"<ul> <li>reveal a superiority of translation learning in terms of quantity, but an inability on the part of weaker learners to transfer their knowledge into L2 contexts</li> <li>suggested that alternative learning strategies that combine the advantages of the two techniques should be explored.</li> </ul>"},{"location":"KB/Second%20order%20generalization/","title":"Second order generalization","text":""},{"location":"KB/Second%20order%20generalization/#second-order-generalization","title":"Second Order Generalization","text":"<ul> <li>Present model with novel example (not seen in training)</li> </ul>"},{"location":"KB/SegNet/","title":"Seg Net","text":""},{"location":"KB/SegNet/#seg-net","title":"Seg Net","text":"<ul> <li>Precursor to Unet</li> <li>No Skip connections</li> </ul>"},{"location":"KB/Selection%20Bias/","title":"Selection Bias","text":""},{"location":"KB/Selection%20Bias/#selection-bias","title":"Selection Bias","text":"<ul> <li>Errors in conclusions drawn from sampled data due to a selection process that generates systematic differences between samples observed in the data and those not observed. The following forms of selection bias exist</li> <li>datasets often prefer particular kinds of images</li> <li>However, getting images from the Internet does not in itself guarantee a fair sampling, since keyword-based searches will return only particular types of images</li> <li>Obtaining data from multiple sources</li> <li>even better to start with a large collection of unannotated images and label them by crowd-sourcing</li> </ul>"},{"location":"KB/Self%20Attention%20GAN/","title":"Self Attention GAN","text":""},{"location":"KB/Self%20Attention%20GAN/#self-attention-gan","title":"Self Attention GAN","text":"<ul> <li>Self Attention + Generative Models</li> </ul> <pre><code>class Self_Attn_New(nn.Module):\n    \"\"\" Self attention Layer\"\"\"\n    def __init__(self, in_dim):\n        super().__init__()\n        self.query_conv = nn.Conv2d(in_dim, out_channels=in_dim//8, kernel_size=1)\n        self.key_conv = nn.Conv2d(in_dim, out_channels=in_dim//8, kernel_size=1)\n        self.value_conv = nn.Conv2d(in_dim, out_channels=in_dim, kernel_size=1)\n        self.gamma = nn.Parameter(torch.zeros([1]))\n\n    def forward(self, x):\n        proj_query = rearrange(self.query_conv(x), 'b c h w -&gt; b (h w) c')\n        proj_key = rearrange(self.key_conv(x), 'b c h w -&gt; b c (h w)')\n        proj_value = rearrange(self.value_conv(x), 'b c h w -&gt; b (h w) c')\n        energy = torch.bmm(proj_query, proj_key)\n        attention = F.softmax(energy, dim=2)\n        out = torch.bmm(attention, proj_value)\n        out = x + self.gamma * rearrange(out, 'b (h w) c -&gt; b c h w',\n                                         **parse_shape(x, 'b c h w'))\n        return out, attention\n</code></pre>"},{"location":"KB/Self%20Attention/","title":"Self Attention","text":""},{"location":"KB/Self%20Attention/#self-attention","title":"Self Attention","text":"<ul> <li>paper</li> <li>Basically Scaled Dot Product Attention</li> <li>Q,K,V all from same module but prev layer</li> <li>Weighted average over all input vectors (\\(y_{i}= \\Sigma_{j}w_{ij}x_{j}\\)\\)<ul> <li>j is over the sequence</li> <li>weights sum to 1 over j</li> <li>\\(w_{ij}\\) is derived \\(w^{'}_{ij}=x_{i}^{T}x_{j}\\)<ul> <li>Any value between -inf to +inf so Softmax is applied</li> </ul> </li> <li>\\(x_i\\) is the input vector at the same pos as the current output vector \\(y_i\\)</li> </ul> </li> <li>Propagates info between vectors</li> <li></li> <li>The process<ul> <li>Assign every word t in the vocabular an Embedding</li> <li>Feeding this into a self attention layer we get another seq of vectors \\(y_{the}\\) , \\(y_{cat}\\) etc</li> <li>each of the \\(y_{something}\\) is a weighted sum over all the embedding vectors in the first seq weighted by their normalized dot product with \\(v_{something}\\)</li> <li>the dot product shows how related the vectors are in the sequence<ul> <li>weights determined by them</li> <li>Self-Attention layer may give more weights to those input vectors that are more similar to each other when generating the output vectors</li> </ul> </li> </ul> </li> <li>Properties<ul> <li>Inputs are a set (not sequence)</li> <li>If input seq is permuted, the output is too</li> <li>Ignores the sequential nature of input by itself</li> </ul> </li> <li>Code</li> </ul> <pre><code>def attention(K, V, Q):\n    _, n_channels, _ = K.shape\n    A = torch.einsum('bct,bcl-&gt;btl', [K, Q])\n    A = F.softmax(A * n_channels ** (-0.5), 1)\n    R = torch.einsum('bct,btl-&gt;bcl', [V, A])\n    return torch.cat((R, Q), dim=1)\n</code></pre>"},{"location":"KB/Self%20Attention/#ref","title":"Ref","text":"<ul> <li>perterbloem</li> </ul>"},{"location":"KB/Self%20Distillation/","title":"Self Distillation","text":""},{"location":"KB/Self%20Distillation/#self-distillation","title":"Self Distillation","text":"<ul> <li>To be specific, Yuan et al. proposed teacher-free knowledge distillation meth- ods based on the analysis of label smoothing reg- ularization (Yuan et al., 2020). Hahn and Choi pro- posed a novel self-knowledge distillation method, in which the self-knowledge consists of the predicted probabilities instead of traditional soft probabilities (Hahn and Choi, 2019).</li> <li>These predicted probabilities are defined by the feature representations of the train- ing model. They reflect the similarities of data in feature embedding space. Yun et al. proposed class- wise self-knowledge distillation to match the output distributions of the training model between intra- class samples and augmented samples within the same source with the same model (Yun et al., 2020).</li> <li>In addition, the self-distillation proposed by Lee et al. (2019a) is adopted for data augmentation and the self- knowledge of augmentation is distilled into the model itself. Self distillation is also adopted to optimize deep models (the teacher or student networks) with the same architecture one by one (Furlanello et al., 2018; Bagherinezhad et al., 2018)</li> <li>both self-distillation and online distillation are properly in- tegrated via the multiple knowledge transfer frame- work (Sun et al., 2021).</li> </ul>"},{"location":"KB/Self%20Supervised%20Survey/","title":"Self Supervised Survey","text":""},{"location":"KB/Self%20Supervised%20Survey/#self-supervised-survey","title":"Self Supervised Survey","text":""},{"location":"KB/Self%20Supervised%20Survey/#abstract","title":"Abstract","text":"<ul> <li>Large-scale labeled data are generally required to train deep neural networks in order to obtain better performance in visual feature learning from images or videos for computer vision applications </li> <li>as a subset of unsupervised learning methods, self-supervised learning methods are proposed to learn general image and video features from large-scale unlabeled data without using any human-annotated labels </li> </ul>"},{"location":"KB/Self%20Supervised%20Survey/#motivation","title":"Motivation","text":"<ul> <li>The performance of deep convolutional neural networks (ConvNets) greatly depends on their capability and the amount of training data. </li> <li>collection and annotation of large-scale datasets are time-consuming and expensive </li> <li>Compared to image datasets, collection and annotation of video datasets are more expensive due to the temporal dimension </li> <li>To avoid time-consuming and expensive data annotations, many self-supervised methods were proposed to learn visual features from large-scale unlabeled images or videos without using any human annotations </li> <li>During the self-supervised training phase, a predefined pretext task is designed for ConvNets to solve, and the pseudo labels for the pretext task are automatically generated based on some attributes of data </li> <li>Then the ConvNet is trained to learn object functions of the pretext task </li> <li>After the self-supervised training finished, the learned visual features can be further transferred to downstream tasks (especially when only relatively small data available) as pretrained models to improve performance and overcome over- fitting. </li> <li> <p>shallow layers capture general low-level features like edges, corners, and textures while deeper layers capture task related high-level features </p> </li> <li> <p>Pseudo Label</p> </li> <li> <p>Pretext Task</p> </li> <li> <p>Downstream Task</p> </li> <li> <p>Weakly-supervised Learning</p> </li> </ul>"},{"location":"KB/Self%20Supervised%20Survey/#formulation-of-different-learning-schemas","title":"FORMULATION OF DIFFERENT LEARNING SCHEMAS","text":"<ul> <li> <p>Supervised Learning Formulation</p> </li> <li> <p>Semi-Supervised Learning Formulation</p> </li> <li> <p>Weakly Supervised Learning Formulation</p> </li> <li>Self-supervised Learning</li> </ul>"},{"location":"KB/Self%20Supervised%20Survey/#nn","title":"NN","text":"<ul> <li>Spatiotemporal Convolutional Neural Network</li> </ul>"},{"location":"KB/Self%20Supervised%20Survey/#pretext-tasks","title":"Pretext Tasks","text":"<ul> <li>Pretext Tasks</li> </ul>"},{"location":"KB/Self%20Supervised%20Survey/#datasets","title":"Datasets","text":"<ul> <li> <p>Places</p> </li> <li> <p>Places365</p> </li> <li> <p>SUNCG</p> </li> <li> <p>SVHN</p> </li> <li> <p>STL-10</p> </li> <li> <p> </p> </li> <li> <p>YFCC100M</p> </li> <li> <p>SceneNet RGB-D</p> </li> <li> <p>Moment in Time</p> </li> <li> <p>Kinetics</p> </li> <li> <p>AudioSet</p> </li> <li> <p>KITTI</p> </li> <li> <p>UCF101</p> </li> <li> <p>HMDB51 </p> </li> </ul>"},{"location":"KB/Self%20Supervised%20Survey/#other-tasks","title":"Other Tasks","text":"<ul> <li> <p>Image Generation with Inpainting</p> </li> <li> <p>Image Generation with Super Resolution</p> </li> <li> <p>Image Generation with Colorization</p> </li> <li> <p>Learning with Context Similarity</p> </li> <li> <p>Learning with Spatial Context Structure</p> </li> <li> <p>Learning with Labels Generated by Game Engines</p> </li> <li> <p>Learning with Labels Generated by Hard-code Programs</p> </li> <li> <p>Learning from Video Colorization</p> </li> <li> <p>Learning from Video Prediction</p> </li> <li> <p>Learning from RGB-Flow Correspondence</p> </li> <li> <p>Learning from Visual-Audio Correspondence</p> </li> <li> <p>Ego-motion</p> </li> </ul>"},{"location":"KB/Self%20Supervised%20Vision%20Transformers/","title":"Self Supervised Vision Transformers","text":"<p>toc: true title: Self Supervised Vision Transformers</p> <p>categories: ['temp']</p>"},{"location":"KB/Self%20Supervised%20Vision%20Transformers/#self-supervised-vision-transformers","title":"Self Supervised Vision Transformers","text":"<ul> <li>An Empirical Study of Training Self-Supervised Vision Transformers<ul> <li>recipes for Vision Transformer are yet to be built</li> <li>Self Supervised</li> <li>instability is a major issue that degrades accuracy, and it can be hidden by apparently good results</li> <li>improved when training is made more stable</li> <li>MoCO v3, a framework which offers an incremental improvement of MoCO</li> <li></li> </ul> </li> </ul>"},{"location":"KB/Self%20Supervised/","title":"Self Supervised","text":""},{"location":"KB/Self%20Supervised/#self-supervised","title":"Self Supervised","text":"<ul> <li>Subset of Unsupervised Learning</li> <li>ConvNet trained with supervisory signals that are generated from data itself</li> <li>Uses source task only based on input data</li> <li>No human biases, less discriminative</li> </ul>"},{"location":"KB/Self%20Supervised/#anchor","title":"anchor","text":""},{"location":"KB/Self-supervised%20Learning/","title":"Self-supervised Learning","text":""},{"location":"KB/Self-supervised%20Learning/#self-supervised-learning","title":"Self-supervised Learning","text":"<ul> <li>Compared to supervised learning methods which require a data pair Xi and Yi while Yi is annotated by human labors, self-supervised learning also trained with data Xi along with its pseudo label Pi while Pi is automatically generated for a pre-defined pretext task without involving any human annotation </li> <li>The pseudo label Pi can be generated by using attributes of images or videos such as the context of images [18], [19], [20], [36], or by traditional hand-designed methods [49], [50], [51].  </li> <li> </li> <li>As long as the pseudo labels P are automatically generated without involving human annotations, then the methods belong to self-supervised learning.</li> </ul>"},{"location":"KB/Semantic%20Analysis/","title":"Semantic Analysis","text":""},{"location":"KB/Semantic%20Analysis/#semantic-analysis","title":"Semantic Analysis","text":"<ul> <li>Meaning of the language</li> <li>Meanings of the word to extend and perhaps disambiguate the result returned by the syntactic parse</li> <li>Look up the individual words in a dictionary (or Lexicon) and extract their meanings</li> <li>But many words have several meanings<ul> <li>Lexical Disambiguation</li> </ul> </li> <li>Sentence level processing</li> </ul>"},{"location":"KB/Semantic%20Data/","title":"Semantic Data","text":""},{"location":"KB/Semantic%20Data/#semantic-data","title":"Semantic Data","text":"<ul> <li>represents the world by a set of subject-predicate-object triple Therefore, the size of the messages is small.</li> </ul>"},{"location":"KB/Semantic%20Grammar/","title":"Semantic Grammar","text":""},{"location":"KB/Semantic%20Grammar/#semantic-grammar","title":"Semantic Grammar","text":"<ul> <li>combine syntactic, semantic and pragmatic knowledge into a single set of rules in the form of a grammar</li> </ul>"},{"location":"KB/Semantic%20Markers/","title":"Semantic Markers","text":""},{"location":"KB/Semantic%20Markers/#semantic-markers","title":"Semantic Markers","text":"<ul> <li>PHYSICAL-OBJECT</li> <li>ANIMATE-OBJECT</li> <li>ABSTRACT-OBJECT</li> <li>Unfortunately, to solve Lexical Disambiguation problem complete, it becomes necessary to introduce more and more finely grained semantic markers</li> </ul>"},{"location":"KB/Semantic%20Segmentation/","title":"Semantic Segmentation","text":""},{"location":"KB/Semantic%20Segmentation/#semantic-segmentation","title":"Semantic Segmentation","text":"<ul> <li>task of assigning semantic labels to each pixel in images </li> <li>autonomous driving, human-machine interaction, and robotic </li> <li>Fully Convolutional Network (FCN) [4], DeepLab [5], PSPNet [6] and datasets such as PASCAL VOC [96], CityScape [97], ADE20K [98]. </li> <li>FCN [4] is a milestone work for semantic segmentation since it started the era of applying fully convolution network (FCN) to solve this task </li> <li>When using semantic segmentation as downstream task to evaluate the quality of image features learned by selfsupervised learning methods, the FCN is initialized with the parameters trained with the pretext task and fine-tuned on the semantic segmentation dataset, then the performance on the semantic segmentation task is evaluated and compared with that of other self-supervised methods.</li> </ul>"},{"location":"KB/Semantics%20influences%20form/","title":"Semantics influences form","text":""},{"location":"KB/Semantics%20influences%20form/#semantics-influences-form","title":"Semantics Influences Form","text":"<ul> <li>Past tense choices mediated by perceived semantic similarity to neighbors, e.g. drank</li> <li>Adults under time pressure also make overgeneralization errors at rates from 6% to 31%</li> </ul>"},{"location":"KB/Semi%20Supervised/","title":"Semi Supervised","text":""},{"location":"KB/Semi%20Supervised/#semi-supervised","title":"Semi Supervised","text":"<ul> <li>Obtain weak labels instead of class labels</li> <li>Eg: \"similar\"</li> <li>Contrastive Loss</li> <li>Triplet Loss</li> <li>Max Margin Loss</li> </ul>"},{"location":"KB/Semi%20Supervised/#anchor","title":"anchor","text":""},{"location":"KB/Semi-Supervised%20Learning%20Formulation/","title":"Semi-Supervised Learning Formulation","text":""},{"location":"KB/Semi-Supervised%20Learning%20Formulation/#semi-supervised-learning-formulation","title":"Semi-Supervised Learning Formulation","text":"<ul> <li>given a small labeled dataset X and a large unlabeled dataset Z, for each data Xi in X, there is a corresponding human-annotated label Yi  </li> <li> </li> <li>For a set of N labeled training data</li> </ul>"},{"location":"KB/Sense-Plan-Act%20Model/","title":"Sense-Plan-Act Model","text":""},{"location":"KB/Sense-Plan-Act%20Model/#sense-plan-act-model","title":"Sense-Plan-Act Model","text":"<ul> <li>Deliberative planning has three main steps that are performed in sequence: Sensing,</li> <li>Planning</li> <li>Acting (executing the plan)</li> </ul>"},{"location":"KB/Sensitivity/","title":"Sensitivity","text":""},{"location":"KB/Sensitivity/#sensitivity","title":"Sensitivity","text":"<ul> <li> \\[TPR = \\frac{TP}{TP + FN}\\] </li> </ul>"},{"location":"KB/Sensory%20Feedback/","title":"Sensory Feedback","text":""},{"location":"KB/Sensory%20Feedback/#sensory-feedback","title":"Sensory Feedback","text":"<ul> <li>Variable data measured by sensors and relayed to the controller in a Closed-loop System. If the controller receives feedback that lies outside an acceptable range, then an error has occurred. The controller sends an error signal to the robot. The robot makes the necessary adjustments in accordance with the error signal.</li> </ul>"},{"location":"KB/Sentence%20Segmentation/","title":"Sentence Segmentation","text":""},{"location":"KB/Sentence%20Segmentation/#sentence-segmentation","title":"Sentence Segmentation","text":"<ul> <li>identify the processing unit, consists of one or more words</li> <li>Sentence boundary detection</li> <li>Sentence boundary disambiguation</li> <li>Sentence boundary recognition</li> </ul>"},{"location":"KB/Sentence%20level%20processing/","title":"Sentence level processing","text":""},{"location":"KB/Sentence%20level%20processing/#sentence-level-processing","title":"Sentence Level Processing","text":"<ul> <li>Semantic Grammar</li> <li>Case Grammar</li> <li>Conceptual Parsing</li> <li>Approximately Compositional Semantic Parsing</li> </ul>"},{"location":"KB/Sentiment%20Neuron/","title":"Sentiment Neuron","text":""},{"location":"KB/Sentiment%20Neuron/#sentiment-neuron","title":"Sentiment Neuron","text":"<ul> <li>Linear model + L1 Lp Regularization</li> <li>Uses very few learned units</li> <li>Single sentiment neuron that predicts the sentiment value</li> <li></li> <li>Can be useful for Unsupervised Learning</li> </ul>"},{"location":"KB/Sentiment%20Neuron/#refs","title":"Refs","text":""},{"location":"KB/SentimentAnalysis/","title":"SentimentAnalysis","text":""},{"location":"KB/SentimentAnalysis/#sentimentanalysis","title":"SentimentAnalysis","text":"<ul> <li>Sentiment Neuron</li> <li>Block Sparse Kernel</li> </ul>"},{"location":"KB/Separation/","title":"Separation","text":""},{"location":"KB/Separation/#separation","title":"Separation","text":"<ul> <li>model predictions are independent of the sensitive feature given the target variable</li> <li>Equalized Odds</li> <li>in classification models, the True Positive (TP) rate and the False Positive (FP) rate are the same in all the subgroups within the sensitive feature</li> </ul>"},{"location":"KB/Sepsis/","title":"Sepsis","text":""},{"location":"KB/Sepsis/#sepsis","title":"Sepsis","text":"<ul> <li>An imbalance in the body's response to infection that injures the body's tissues and organs</li> </ul>"},{"location":"KB/Seq2Seq/","title":"Seq2Seq","text":""},{"location":"KB/Seq2Seq/#seq2seq","title":"Seq2Seq","text":"<ul> <li>Basic RNN Architectures</li> <li>Long term dependency Issues</li> <li>Even if hidden state vector has a high dimensionality, cannot hold all info</li> <li>Sequence to Sequence Learning with Neural Networks</li> <li>encoder-decoder learning to map sequences to sequences</li> <li>multilayered Long Short-Term Memory [LSTM)](Long Short Term Memory (LSTM|Long Short Term Memory (LSTM|LSTM)](LSTM)](Long Short Term Memory (LSTM|Long Short Term Memory (LSTM|LSTM).md).md)</li> <li>large deep LSTM with a limited vocabulary can outperform a standard statistical machine translation (SMT)-based system whose vocabulary is unlimited on a large-scale MT task</li> <li>WMT14</li> <li>BLEU score</li> <li>reversing the order of the words in all source sentences (but not target sentences) improved the LSTM\u2019s performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier</li> </ul>"},{"location":"KB/Sequential%20Relation%20Bias/","title":"Sequential Relation Bias","text":""},{"location":"KB/Sequential%20Relation%20Bias/#sequential-relation-bias","title":"Sequential Relation Bias","text":"<ul> <li>Sometimes our data has a sequential characteristic. For instance, time series and sentences consist of sequential elements that appear one after another. To model this pattern, we can introduce a Recurrent layer to our network:</li> <li></li> </ul>"},{"location":"KB/Sequential%20effects%20within%20a%20short%20foreperiod%20context%20Evidence%20for%20the%20conditioning%20account%20of%20temporal%20preparation/","title":"Sequential effects within a short foreperiod context Evidence for the conditioning account of temporal preparation","text":""},{"location":"KB/Sequential%20effects%20within%20a%20short%20foreperiod%20context%20Evidence%20for%20the%20conditioning%20account%20of%20temporal%20preparation/#sequential-effects-within-a-short-foreperiod-context-evidence-for-the-conditioning-account-of-temporal-preparation","title":"Sequential effects within a short foreperiod context Evidence for the conditioning account of temporal preparation","text":"<ul> <li>Michael B. Steinborn , Bettina Rolke, Daniel Bratzke, Rolf Ulrich</li> </ul>"},{"location":"KB/Sequential%20effects%20within%20a%20short%20foreperiod%20context%20Evidence%20for%20the%20conditioning%20account%20of%20temporal%20preparation/#abstract","title":"Abstract","text":"<ul> <li>Responses to an imperative stimulus (IS) are especially fast when they are preceded by a warning signal (WS).</li> <li>When the interval between WS and IS (the foreperiod, FP) is variable, reaction time (RT) is not only influenced by the current FP but also by the FP of the preceding trial</li> <li>sequential effects originate from a trace conditioning process, in which the individuals learn the temporal WS\u2013IS relationship in a trial-by-trial manner</li> <li>that trace conditioning is maximal when the temporal interval between the conditioned and unconditioned stimulus is between 0.25 and 0.60 s</li> <li>, one would predict that sequential effects occur especially within short FP contexts.</li> <li>However, this prediction is contradicted by Karlin [Karlin, L. (1959)]</li> <li>investigate temporal preparation for short FPs</li> <li>The results provide strong evidence for sequential effects within a short FP context and thus support the trace conditioning account of temporal preparation.</li> </ul>"},{"location":"KB/Sequential%20effects%20within%20a%20short%20foreperiod%20context%20Evidence%20for%20the%20conditioning%20account%20of%20temporal%20preparation/#experiment-1","title":"Experiment 1","text":"<ul> <li>Anticipatory responding was controlled by using a choice RT task</li> <li>There was a main effect of the factor FP-set on RT, F(1,21) = 219.3, partial g2 = .91, p &lt; .001, indicating that RT was shorter in the short FP-set (366 ms) than in the long FP-set</li> <li>RT benefit for the short FP-set might be attributable to a better general ability to process short time intervals than long ones (e.g., Klemmer, 1957; Na\u0308a\u0308ta\u0308nen et al., 1974).</li> <li>main effect of FPn on RT,</li> <li>RT decreased as \\(FP_n\\) increased</li> <li>FP in the preceding trial also influenced RT in the current trial as revealed by a main effect of \\(FP_{n-1}\\) on RT</li> <li>increased as \\(FP_{n-1}\\) decreased</li> <li>\\(FP_{n-1}\\) \\(FP_n\\) interaction effect on RT</li> <li>when the preceding FP was long, RT in a current trial decreased with increasing FP and this effect was</li> <li>weaker when a short FP preceded a current trial</li> <li>asymmetry of the sequential FP effect was smaller for the short FP-set than for the long FP-set.</li> <li>Most important, however, the sequential FP effect was not restricted to the long FP-set but was also present for the short FP-set of FP-durations below 0.6 s</li> </ul>"},{"location":"KB/Sequential%20effects%20within%20a%20short%20foreperiod%20context%20Evidence%20for%20the%20conditioning%20account%20of%20temporal%20preparation/#experiment-2","title":"Experiment 2","text":"<ul> <li>assessed sequential FP effects in a simple RT task employing only the short FP-set</li> <li>catch trial technique</li> <li>compared the asymmetrical sequential FP effect in a condition with 0% catch trials (referred to as no-CT condition) to a condition with 25% catch trials (referred to as CT condition)</li> <li>RT was prolonged in the CT condition</li> <li>main effect of FPn on RT</li> <li>decrease of RT with increasing FPn</li> <li>\\(FP_{n-1}\\) influenced RT</li> <li>influence of the preceding FP was unaffected by CT</li> <li>asymmetrical sequential FP effect again showed up in the \\(FP_{n-1}\\) \\(FP_n\\) interaction on RT</li> <li>anticipatory responses also increased with decreasing preceding FP</li> <li>There was a significant \\(FP_{n-1}\\) \\(FP_n\\) interaction on RT for the no-CT condition</li> <li>weak CT \\(FP_n\\) interaction effect on RT</li> </ul>"},{"location":"KB/Sequential%20effects%20within%20a%20short%20foreperiod%20context%20Evidence%20for%20the%20conditioning%20account%20of%20temporal%20preparation/#experiment-3","title":"Experiment 3","text":"<ul> <li>FP-range employed in Karlin's study was too dense and therefore did not produce sufficient temporal uncertainty</li> <li>simple RTs were much faster (220 ms) than choice RTs</li> <li>RT pattern differed between the task conditions,</li> <li>In contrast to Experiment 2, an upwardsloping FP-RT effect was observed</li> <li>RT increased from the shortest towards the longest \\(FP_n\\)</li> <li>RT decreased with increasing \\(FP_{n-1}\\)</li> <li>responses in short \\(FP_n\\) trials were always fast irrespective of \\(FP_{n-1}\\).</li> <li>In contrast, responses in long FPn trials were on average slower and showed a sequential modulation.</li> <li>Precisely, in long FPn trials, responses were relatively fast when \\(FP_{n-1}\\) was also long compared to when \\(FP_{n-1}\\) was short.</li> <li>In sum, the simple RT condition revealed especially fast responses and an extraordinary high percentage of anticipatory responses in short FPn trials, even though FPn1 was long</li> <li>is consistent with the results of Karlin (1959) and suggests that participants mainly prepared for an early imperative moment without re-preparing in long FPn trials</li> <li>the RT pattern in the simple RT condition clearly indicates that Karlin's (1959) finding was not an anomalous result but a reliable empirical phenomenon that occurs when average FPs are small and the FPrange is very dense</li> <li>The overall pattern of results (simple and choice RT condition) is consistent with the view that participants already attained maximal preparation at the short \\(FP_n\\) and were not able to re-prepare when the IS did not occur at the short \\(FP_n\\).</li> <li>Instead, they may have relied on residual preparatory activity from the early imperative moment (Alegria, 1974; Alegria, 1975b).</li> <li>instance, they may have shifted a single moment of peak preparation in a rather analog way, that is, after a short \\(FP_n\\) they expected the IS somewhat earlier, after a long FPn1 \\(FP_{n-1}\\) somewhat later</li> </ul>"},{"location":"KB/Sequential%20effects%20within%20a%20short%20foreperiod%20context%20Evidence%20for%20the%20conditioning%20account%20of%20temporal%20preparation/#general-discussion","title":"General discussion","text":"<ul> <li>when the FP-range is too dense, the typical asymmetrical sequential FP effect does not occur.</li> <li>Hence, the RT pattern observed in the simple RT condition demonstrates that Karlin's (1959) finding of a reversed sequential FP effect is a robust phenomenon that occurs in simple RT tasks when the FP-range is very dense.</li> <li>We suggest that when the FP-range is very dense, the individuals may not represent three distinct imperative moments but a single relatively noisy one to which they attain preparation</li> <li>The result pattern of the simple RT condition indicates that participants attained preparation at an early imperative moment because responses were especially fast and a high amount of anticipatory responses were observed in short FPn trials</li> <li>The observation of a reversed sequential effect, however, shows that the moment of peak preparation was still influenced by the preceding trial.</li> <li>In particular, participants may have expected the IS after a short FPn1 somewhat earlier, but after a long</li> <li>\\(FP_{n-1}\\) somewhat later in time</li> <li>Critically, when the small FP-range does not enable a sharpedged representation of three distinct critical moments but only a noisy representation of a single critical moment, then the process that produces the asymmetrical sequential FP effects at short FP</li> <li>no sequential FP effect in short \\(FP_n\\) trials should be expected in this situation</li> <li>Importantly, a rather analog sequential adjustment of a single but early preparatory peak should result in a sequential modulation at later imperative moments, as is exactly observed in the simple RT condition of Experiment 3</li> <li>If temporal preparation had increased with \\(FP_n\\)-length, this should have resulted in more efficient performance</li> </ul>"},{"location":"KB/Sequential%20effects%20within%20a%20short%20foreperiod%20context%20Evidence%20for%20the%20conditioning%20account%20of%20temporal%20preparation/#images","title":"Images","text":""},{"location":"KB/Serious%20Games/","title":"Serious Games","text":""},{"location":"KB/Serious%20Games/#serious-games","title":"Serious Games","text":"<ul> <li>Support learning</li> <li>Simulators of real world tasks</li> <li>Eg: response simulator, kerbal space program, sustainable energy management</li> </ul>"},{"location":"KB/Serotonin/","title":"Serotonin","text":""},{"location":"KB/Serotonin/#serotonin","title":"Serotonin","text":"<ul> <li>A neurotransmitter believed to play many roles, including, but not limited to, temperature regulation, sensory perception, and the onset of sleep. Neurons using serotonin as a transmitter are found in the brain and in the gut. A number of antidepressant medications are targeted to brain serotonin systems.</li> </ul>"},{"location":"KB/Servo%20Control/","title":"Servo Control","text":""},{"location":"KB/Servo%20Control/#servo-control","title":"Servo Control","text":"<ul> <li>The process by which the control system of the robot checks if the attained pose of the robot corresponds to the pose specified by the motion planning with required performance and safety criteria.</li> </ul>"},{"location":"KB/Servo%20Motor/","title":"Servo Motor","text":""},{"location":"KB/Servo%20Motor/#servo-motor","title":"Servo Motor","text":"<ul> <li>An electrical power mechanism used to effect motion or maintains position of the robot</li> </ul>"},{"location":"KB/Servo%20Pack/","title":"Servo Pack","text":""},{"location":"KB/Servo%20Pack/#servo-pack","title":"Servo Pack","text":"<ul> <li>An alternating, current electrical power mechanism that is controlled through logic to convert electrical supply power that is in a sine wave form to a Pulse Width Modulated (PWM) square form, delivered to the motors for motor control: speed, direction, acceleration, deceleration and braking control.</li> </ul>"},{"location":"KB/Servo-controlled%20Robot/","title":"Servo controlled Robot","text":""},{"location":"KB/Servo-controlled%20Robot/#servo-controlled-robot","title":"Servo-controlled Robot","text":"<ul> <li>The control of a robot through the use of a Closed-loop Servo-system, in which the position of the robot axis is measured by feedback devices and is stored in the controller's memory</li> </ul>"},{"location":"KB/Servo-system/","title":"Servo system","text":""},{"location":"KB/Servo-system/#servo-system","title":"Servo-system","text":"<ul> <li>A system in which the controller issues commands to the motors, the motors drive the arm, and an encoder sensor measures the motor rotary motions and signals the amount of the motion back to the controller.</li> </ul>"},{"location":"KB/Shading/","title":"Shading","text":""},{"location":"KB/Shading/#shading","title":"Shading","text":""},{"location":"KB/Shallow%20vs%20deep%20networks/","title":"Shallow vs deep networks","text":""},{"location":"KB/Shallow%20vs%20deep%20networks/#shallow-vs-deep-networks","title":"Shallow vs deep networks","text":"Shallow Networks Deep Networks single hidden layer multiple hidden layers can model continuous functions, but needs more hidden layers to do so composition of shallow networks, so few can represent more complex functions With one input, one output and \\(D &gt; 2\\) hidden units -&gt; \\(D+1\\) linear regions , \\(3D+1\\) params One input, one output, K layers of \\(D&gt;2\\) hidden units -&gt; \\((D+1)^{K}\\) linear regions and \\(3D + 1 + (K-1)D(D+1)\\) params Flexibility of functions is limited by number of parameters Complex dependencies and symmetries More hidden units to achieve approximation Less hidden units to achieve approximation -&gt; Depth efficiency Processing data like images is almost infeasible Pretty easy to do so. Process local image regions in parallel and then gradually integrate information from increasingly large regions Hard to fit data Overparameterized deep models have a large family of roughly equivalent solutions that are easy to find Does not generalize well to new data Generalizes better than shallow networks NA If the number of hidden units D in each of the K layers is the same, and D is an integer multiple of the input dimensionality \\(D_i\\), then the maximum number of linear regions \\(N_r\\) - \\(N_{r}= (\\frac{D}{D_{i}}+1)^{D_{i}(K-1)} \\cdot \\Sigma_{j=0}^{D_{i}} \\begin{pmatrix}C \\\\ J\\end{pmatrix}\\)"},{"location":"KB/Shapes%20Dataset/","title":"Shapes Dataset","text":""},{"location":"KB/Shapes%20Dataset/#shapes-dataset","title":"Shapes Dataset","text":"<ul> <li>Convention is that the first dimension of the dataset belongs to the features and the second to the number of samples. The operations can then be systematically applied from the left.  </li> <li>Exception!, in TensorFlow and PyTorch it is the other way around!</li> <li>\\((\\#features, \\#samples)\\)</li> </ul>"},{"location":"KB/Shared%20Character%20Set/","title":"Shared Character Set","text":""},{"location":"KB/Shared%20Character%20Set/#shared-character-set","title":"Shared Character Set","text":"<ul> <li>helps in narrowing down to a small set of languages</li> <li>Arabic &amp; Persian<ul> <li>Share same characters but one has supplemental characters</li> </ul> </li> <li>Russian &amp; Ukrainian<ul> <li>Same Character Set - Different Frequencies</li> </ul> </li> <li>Norwegian &amp; Swedish</li> </ul>"},{"location":"KB/Sharpness%20and%20Flatness/","title":"Sharpness and Flatness","text":""},{"location":"KB/Sharpness%20and%20Flatness/#sharpness-and-flatness","title":"Sharpness and Flatness","text":"<ul> <li>Three 1D loss surfaces of a VGG-9 network with layer normalization instead of filter normalization, sorted from flattest to sharpest minimum, according to the authors</li> <li></li> </ul>"},{"location":"KB/Shear/","title":"Shear","text":""},{"location":"KB/Shear/#shear","title":"Shear","text":"<ul> <li>Stretches the image in one of the axial planes, i.e. shear occurs along the x-axis or y-axis. A maximum shear of \u00b120\u25e6 is used to ensure class preservation.</li> </ul>"},{"location":"KB/Shepard%20Interpolation/","title":"Shepard Interpolation","text":""},{"location":"KB/Shepard%20Interpolation/#shepard-interpolation","title":"Shepard Interpolation","text":"<ul> <li>$$</li> </ul> <p>\\begin{cases}\\Sigma^{N}{i=1}w{i}(x)f_{i}&amp; \\text{if } d(x,x_{i}) \\neq 0 \\forall i\\f_{i}&amp; \\text{if } d(x, x_{i})=0\\</p> <p>\\end{cases}</p> <p>$$ - \\(w_{i}(x) = \\frac{1}{d(x,x_{i})^{p}}\\) - Neighborhood N determines points aka radius</p>"},{"location":"KB/Sherlock/","title":"Sherlock","text":""},{"location":"KB/Sherlock/#sherlock","title":"Sherlock","text":"<ul> <li>Outer loop: Sherlock (Katz et al., 1998) tutors the troubleshooting of a large piece of simulated electrical equipment, an avionics test station.</li> <li>It provides many views on the equipment and its schematics, so no screenshots are included here.</li> <li>Inner loop: Troubleshooting the equipment requires taking many steps, such as measuring voltages and replacing suspect parts. Sherlock gives unsolicited feedback only if a step is unsafe, that is, if the step would cause serious damage to the equipment or the student if it were done in the real world</li> <li>Sherlock also makes extensive feedback available after the student has solved the problem. Sherlock's default post-solution feedback is to display a side-by-side summary of the student's solution and Sherlock's ideal solution.</li> </ul>"},{"location":"KB/Shock%20Detection%20Function/","title":"Shock Detection Function","text":""},{"location":"KB/Shock%20Detection%20Function/#shock-detection-function","title":"Shock Detection Function","text":"<ul> <li>Shock detection is a function supported by the Yaskawa robot controller that reduces the impact of a robot collision by stopping the manipulator without any external sensor when the tool or the manipulator collide with a peripheral device.</li> </ul>"},{"location":"KB/Shortcuts%20to%20Quantifier%20Interpretation%20in%20Children%20and%20Adults/","title":"Shortcuts to Quantifier Interpretation in Children and Adults","text":""},{"location":"KB/Shortcuts%20to%20Quantifier%20Interpretation%20in%20Children%20and%20Adults/#shortcuts-to-quantifier-interpretation-in-children-and-adults","title":"Shortcuts to Quantifier Interpretation in Children and Adults","text":"<ul> <li>Patricia J. Brooks &amp; Irina Sekerina</li> </ul>"},{"location":"KB/Shortcuts%20to%20Quantifier%20Interpretation%20in%20Children%20and%20Adults/#intro","title":"Intro","text":"<ul> <li>Summarized in 2022-10-10</li> <li>Errors involving universal quantification are common in contexts depicting sets of individuals in partial, one-to-one correspondence</li> <li>quantifier-spreading errors are more common with distributive quantifiers each and every than with all.</li> <li>pairs of pictures</li> <li>selected one corresponding to a sentence containing a universal quantifier</li> <li>not in correspondence, with correct sentence interpretation requiring their attention</li> <li>Children younger than 9 years made numerous errors</li> <li>with poorer performance in distributive contexts than collective ones</li> <li>21 native, English-speaking adults, given a similar task with the distributive quantifier every, also made childlike errors undermines accounts positing immature syntactic structures as the error source</li> <li>errors seemingly reflect inaccurate syntax to semantics mapping, with adults and children alike resorting to processing shortcuts.</li> <li>Quantificational terms such as all, usually, and most are a crucial type of linguistic device used to indicate which sets of individuals or events have which properties and relationships</li> <li>acquisition may be delayed relative to other sorts of lexical items (e.g., nouns and verbs) because their complex patterns of usage often result in interpretive ambiguities</li> <li>96 children (5- to 9-year-olds)</li> <li>Both pictures showed extra objects</li> <li>In spoken language, however, the intensifier interpretation is predominant in which it is conventional to say all even when it does not imply exhaustivity</li> <li>I left all my money at home would not preclude their having money in the bank</li> <li>all is used primarily as an adverbial intensifier in both child language and child-directed speech</li> <li>every and each involve additional lexical complexity</li> <li>Every appears inside compounds\u2014for example, everybody\u2014and it vacillates between collective and distributive interpretations when used as a quantifier</li> <li>Each is more uniformly distributive but has the further conceptual requirement that the individuals modified by each be successively scanned</li> <li>Every and each both occur only rarely in either childdirected or child speech, which limits opportunities for children to acquire their patterns of usage.</li> <li>overexhaustive search</li> <li>involves failure to properly restrict the domain of the universal quantifier to the</li> <li>exhaustive pairing</li> <li>classic spreading</li> <li>noun phrase (NP) it modifies</li> <li>complementary error referred to as underexhaustive search</li> <li>Very young children occasionally make other, more surprising errors in interpreting universal quantifiers, such as answering no to the question Is every bunny eating a carrot? when shown, for example, a picture of three rabbits, each eating a carrot, along with a dog eating a bone. This error is referred to as bunny spreading</li> <li>Children's errors with universal quantification have led to controversies with respect to how to explain them. One view is that the errors stem from children's deficient syntactic representations (Kang (2001), Philip (1995; 1996), Roeper and de Villiers (1993), Roeper and Matthei (1975), Roeper et al. (2005)). Philip (1995; 1996), following Roeper and Matthei (1975) suggested that the classic spreading error is due to the fact that children syntactically misinterpret distributive universal quantifiers (e.g., each or every in English, cada in Spanish or Portuguese) as sentential adverbials that range over events as opposed to individuals. Roeper et al. (2005) described a sequence of steps of how children start with a general syntactic representation of every as an adverbial intensifier that gets progressively more specific as every changes its position in the syntactic representation.</li> <li>Rather, the errors are presumed to involve shallow processing, resulting in inaccurate mapping between syntactic and semantic representations</li> <li>First, Crain et al. (1996) did not systematically vary the position of the universal quantifier in the test questions or statements while holding constant the introductory story and scenario</li> <li>prototypical scenario</li> <li>This feature of their design provided children with unambiguous cues as to which set of entities was the focus of attention.</li> <li>Crain et al.'s (1996) claim that preschoolers have full competence with universal quantifiers would seem to be undermined by the fact that even older school-age children make errors identifying the domain of a universal quantifier.</li> <li>First, there have been great discrepancies in error rates across the many studies that have almost exclusively utilized the Truth Value Judgment Task, ranging from near perfect performance in Crain et al. (1996) to extremely high error rates in Kang (2001), that is, over 80% errors in 6- to 7-year-olds</li> <li>Our sentence\u2013picture matching task does not have the same demand characteristics as the Truth Value Judgment Task, and in our opinion, it provides a more accurate way of evaluating whether children's interpretations of sentences with universal quantifiers vary systematically as a function of the position of the</li> <li>quantifier in the sentence and the type of scene</li> <li>examine whether there is an asymmetry in the distribution of errors as a function of the syntactic position of the universal quantifier</li> <li>address the controversy as to whether children perform better in tasks with collective universal quantifiers (cf. Brooks and Braine (1996)) or with distributive ones (cf. Drozd (1996))</li> <li>Finally, because accounts positing syntactic deficits as the source of quantifierspreading errors (e.g., Kang (2001), Philip (1996), Roeper et al. (2005)) generally assume that adults are essentially error free in their comprehension of basic sentences containing universal quantifiers (i.e., their syntax is perfect), we tested a group of adults on a version of our task (Experiment 3) to evaluate this claim and to allow a more complete investigation of the developmental trajectory of quantifier acquisition from 5-year-olds to adults.</li> </ul>"},{"location":"KB/Shortcuts%20to%20Quantifier%20Interpretation%20in%20Children%20and%20Adults/#experiment-1","title":"EXPERIMENT 1","text":""},{"location":"KB/Shortcuts%20to%20Quantifier%20Interpretation%20in%20Children%20and%20Adults/#participants","title":"Participants","text":"<ul> <li>We recruited and tested twelve 5-year-olds (M = 5;5, range = 5;2\u20135;11), twelve 6-year-olds (M = 6;6, range = 6;2\u20136;10), twelve 7-yearolds (M = 7;6, range = 7;1\u20137;11), twelve 8-year-olds (M = 8;6, range = 8;0\u20138;11), and twelve 9-year-olds (M = 9;6, range = 9;1\u20139;11) at private elementary schools and after-school programs in Atlanta, Georgia.</li> <li>24 pairs of pictures depicting people involved in various activities such as carrying boxes, washing pets, or watering plants.</li> <li>Four types of picture pairs were constructed.</li> <li>three people individually engaged in an activity with three distinct objects or animals</li> <li>three people engaged in an activity with three objects</li> <li>Collective picture pair types 3 and 4 were variations of collective picture pair type 2, with new foils created to match the distributive pairs in terms of target\u2013foil similarity</li> <li>Across picture pairs, a variety of contexts with transitive actional verbs were used so that each sentence type could be presented multiple times without repeating any pictures</li> <li>All of the contexts involved humans acting on animate or inanimate objects</li> <li>Six sentence types were used</li> <li> <ul> <li>(1) Each of the (people) is (verb)ing an (object), for example, Each of the men is washing a bear. (2) There is a (person) (verb)ing each of the (objects), for example, There is a man washing each of the bears. (3) Every (person) is (verb)ing an (object), for example, Every man is washing a bear. (4) There is a (person) (verb)ing every (object), for example, There is a man washing every bear. (5) All of the (people) are (verb)ing an (object), for example, All of the men are washing a bear.</li> </ul> </li> <li> <ul> <li>(6) There is a (person) (verb)ing all of the (objects), for example, There is a man washing all of the bears.</li> </ul> </li> <li>Twelve additional pairs of pictures served as filler items.</li> </ul>"},{"location":"KB/Shortcuts%20to%20Quantifier%20Interpretation%20in%20Children%20and%20Adults/#procedure","title":"Procedure","text":"<ul> <li>single, 20-min session conducted in a quiet room of their school</li> <li>We showed children two pictures at a time and asked them to point to the picture that went best with a sentence read aloud</li> <li>After the child looked at both pictures, the experimenter read the corresponding sentence and asked the child to point to the picture that went best with the sentence.</li> <li>without providing any corrective feedback</li> <li>Across trials, we randomized the position of the correct picture</li> <li>Children made no errors on filler sentences, and these trials were not examined further.</li> <li>mixed-design analysis of variance</li> <li>The dependent variable was the proportion of correct picture choices for Sentence Types 1 through 4</li> <li>arcsine transformed these proportions and all others</li> </ul>"},{"location":"KB/Shortcuts%20to%20Quantifier%20Interpretation%20in%20Children%20and%20Adults/#analysis","title":"Analysis","text":"<ul> <li>The analysis showed significant main effects of syntactic position, F(1, 55) = 19.03, p &lt; .001, and age, F(4, 55) = 7.22, p &lt; .001. No other main effects or interactions were significant</li> <li>As shown in Table 1, comparisons against chance performance (50%) revealed that only the 9-year-olds as a group were above chance in selecting the correct pictures (Figure 1b) for sentences with the universal quantifier modifying the direct object.</li> <li>At this criterion, one 5-year-old (8%), two 6-year-olds (17%), five 7- year-olds (42%), five 8-year-olds (42%), and nine 9-year-olds (75%) performed above chance.</li> <li>Overall, children were correct in 95.8% of their picture selections when the original collective pictures of Brooks and Braine were used (see Figure 2) but only 83.1% of trials with the modified collective pictures (see Figures 3 and 4).</li> <li>children's responses became more consistently correct with age, with only children 7 years and older performing above chance as a group in the modified task when the quantifier modified the direct object.</li> <li>To examine whether individual children were above chance in selecting the appropriate collective picture on the modified task with all, we again used the binomial distribution ( p &lt; .05), with above-chance performance requiring 6 out of 6 correct responses</li> <li>At this criterion, zero 5-year-olds (0%), five 6-year-olds (42%), seven 7-year-olds (58%), seven 8-year-olds (58%), and seven 9-year-olds (58%) performed above chance.</li> <li>f we adopt a more lenient criterion (5 of 6), which is proportionally comparable to the 10 of 12 required for above-chance performance with sentences containing each or every, then six 5-year-olds (50%), eight 6-yearolds (67%), ten 7-year-olds (83%), eight 8-year-olds (67%), and ten 9-year-olds (83%) showed consistently strong individual performance.</li> <li>we conducted one additional mixed-design ANOVA with Quantifier (all, each, every) and Syntactic Position as within-subjects factors and Age as a between-subjects factor.</li> <li>The main effect of syntactic position was significant, F(1, 55) = 16.38, p &lt; .001, but was qualified by an interaction of quantifier and age, F(1, 55) = 7.13, p &lt; .01</li> <li>comprehension performance was more accurate when the universal quantifier modified the subject of the sentence.</li> <li>This effect of syntactic position, however, was highly significant for sentences with each or every but only marginally significant for sentences with all (see above for F tests for main effects of syntactic position in separate analyses by quantifier). No other interactions were significant.</li> <li>In general, their picture selections were more accurate for sentences with a universal quantifier modifying the subject in comparison to the direct object of a transitive actional verb.</li> <li>Although the Philip (1996) and Kang studies have shown a similar pattern of subject/object asymmetry to this study and Brooks and Braine (1996), we note that children performed at much higher levels of accuracy in our sentence\u2013picture matching task compared to the Truth Value Judgment Task. In none of our conditions, at any age, were children significantly below chance in their picture selections. This contrasts especially with Kang, who reported error rates over 80% in both English-speaking and Korean-speaking 6- and 7-year-olds</li> <li>However, by age 7, children were consistently correct in their picture choices regardless of the syntactic position of all in the sentence.</li> <li>This suggests that the collective groupings may have helped focus their at</li> <li>tention on the relevant set of entities modified by the quantifier.</li> <li>More generally, the Truth Value Judgment Task allows children to reject a picture for a variety of reasons (and it is often hard to discern the basis for children's pattern of responding)</li> <li>In Experiment 1, we used a sentence\u2013picture matching procedure in which children needed only to find the picture that matched the sentence. This task eliminated opportunities for participants to consider whether a collective versus distributive interpretation of the sentence was preferred and furthermore allowed us to carefully match our collective and distributive pictures with respect to the composition of the foils.</li> </ul>"},{"location":"KB/Shortcuts%20to%20Quantifier%20Interpretation%20in%20Children%20and%20Adults/#experiment-2","title":"Experiment 2","text":"<ul> <li>We designed Experiment 2 to eliminate this interpretive confound through the use of locative scenes.</li> <li>locative pictures with animals and other entities shown in containers of various sorts (e.g., bananas in baskets, bears in beds).</li> <li>universal quantifiers in three syntactic constructions that support distributive interpretations in a locative context</li> <li>Across constructions, we systematically varied both the syntactic position of the universal quantifier and whether the subject of the sentence referred to the containers or the entities in them</li> </ul>"},{"location":"KB/Shortcuts%20to%20Quantifier%20Interpretation%20in%20Children%20and%20Adults/#method","title":"Method","text":"<ul> <li>Participants. Twelve 7-year-olds (M = 7;6, range = 7;1\u20137;10), twelve 8-year-olds (M = 8;6, range = 8;0\u20138;11), and twelve 9-year-olds (M = 9;5, range = 9;0\u20139;10) took part in the experiment. We recruited and tested these children at the same schools as in Experiment 1. None of the children in Experiment 2 participated in the previous experiment.</li> </ul>"},{"location":"KB/Shortcuts%20to%20Quantifier%20Interpretation%20in%20Children%20and%20Adults/#materials","title":"Materials","text":"<ul> <li>27 pairs of pictures depicting various entities arranged in containers (e.g., alligators in bathtubs, turtles in tanks, apples in bowls)</li> <li>The pictures showed distributive arrangements with the entities and containers in partial, one-to-one correspondence with each other.</li> <li>Both pictures depicted three entities each located in a unique container. One picture showed two extra empty containers (see Figure 5a), and the other picture showed two objects that were not in containers (see Figure 5b).</li> <li>nine sentence types</li> <li> <ul> <li>(7) All of the (objects) are in a (container), for example, All of the alligators are in a bathtub. (8) All of the (containers) have an (object) in them, for example, All of the bathtubs have an alligator in them.</li> </ul> </li> <li>There is an (object) in all of the (containers), for example, There is an alligator in all of the bathtubs. Each of the (objects) is in a (container), for example, Each of the alligators is in a bathtub. Each of the (containers) has an (object) in it, for example, Each of the bathtubs has an alligator in it. There is an (object) in each of the (containers), for example, There is an alligator in each of the bathtubs. Every (object) is in a (container), for example, Every alligator is in a bathtub. Every (container) has an (object) in it, for example, Every bathtub has an alligator in it. There is an (object) in every (container), for example, There is an alligator in every bathtub.</li> </ul>"},{"location":"KB/Shortcuts%20to%20Quantifier%20Interpretation%20in%20Children%20and%20Adults/#analysis_1","title":"Analysis","text":"<ul> <li>The main effect of quantifier and all of the interactions involving quantifiers were not significant.</li> <li>Across syntactic constructions, 7-year-olds preferred the picture with the extra animals or objects as opposed to the picture with the extra containers.</li> <li>Both the 7- and 8-year-olds made correct picture selections at an above-chance level only for sentences with the universal quantifier modifying the noun correspond</li> <li>ing to the containers irrespective of the syntactic construction.</li> <li>n contrast, the majority of 9-year-olds correctly varied their picture selections in accordance with the varying syntactic constructions and performed above chance as a group for all sentence types.</li> <li>Experiment 2 replicated one of the main findings of Experiment 1: Only 9-yearolds as a group consistently identified the domain of the universal quantifier and selected the appropriate picture at above-chance levels for distributive events in which sets of objects were in partial, one-to-one correspondence.</li> <li>Rather, irrespective of the syntactic construction, they showed better performance on sentences with the quantifier modifying the containers</li> <li>The observed bias to prefer locative scenes in which all of the containers were filled (the so-called garage-centered bias) has been observed many times; see Drozd (2001) for a review</li> <li>These results are difficult to reconcile with Kang's</li> <li>Moreover, all of the age groups failed to show any effect of quantifier in Experiment 2 in contrast to Experiment 1</li> <li>That is, the children failed to show a familiarity effect with better performance for sentences with all.</li> <li>he differing results for the two experiments indicate that the collective scenes used with all in Experiment 1 were easier than the distributive ones used in Experiment 2 (see also Brinkmann et al. (1996)).</li> <li>It appears that these scenarios involving partial, one-toone correspondence pose considerable challenges for children.</li> <li>Although Experiment 2 provided no evidence that children distinguished the quantifier all from each or every, we emphasize that previous work (Brooks et al. (2001), Brooks et al. (1998)) has shown that children do readily distinguish these quantifiers on semantic grounds</li> </ul>"},{"location":"KB/Shortcuts%20to%20Quantifier%20Interpretation%20in%20Children%20and%20Adults/#experiment-3","title":"Experiment 3","text":"<ul> <li>In Experiment 3, we examined whether adults would make errors restricting the domain of a universal quantifier in a similar picture-selection task with distributive, locative scenes.</li> <li>Testing adults is important because syntactic accounts do not readily predict errors in syntactically competent adults</li> <li>Brooks and Braine (1996, Experiment 1) tested adults with actional scenes and found no errors. Their data, however, came from 10 undergraduates at a highly selective private university (Carnegie Mellon) and thus may not be representative of adults in general.</li> <li>Here we tested monolingual English-speaking undergraduates at a highly diverse public university. This experiment constituted pilot work to establish a paradigm suitable for an eye-tracking study.</li> <li>First, we included two filler pictures along with each target and foil picture, and second, we presented sentences with the quantifier every but did not test each or all.</li> <li>Participants. We recruited 22 monolingual, adult native speakers of English (16 women, 6 men; M age = 26 years, range = 18\u201349) from introductory psychology classes at the College of Staten Island, City University of New York, who received extra credit for their participation.</li> <li>We created PowerPoint\u00ae slides comprising four pictures that were presented simultaneously (see Figure 6 for an example). These slides depicted two sets of objects in partial, one-to-one correspondence</li> <li>Each array contained two pictures similar to those used in Experiment 2 (compare Figures 5 and 6), along with two filler pictures</li> <li>We presented each sentence type six times, in randomized order, for a total of 12 trials</li> <li>To permit naming, we numbered the pictures from 1 to 4, with the position of the target randomized across trials. We used a tape recorder to record participants' responses.</li> </ul>"},{"location":"KB/Shortcuts%20to%20Quantifier%20Interpretation%20in%20Children%20and%20Adults/#observations","title":"Observations","text":"<ul> <li>Across all participants, no errors were made on filler sentences indicating that the participants were generally compliant with the task instructions</li> <li>performance on the task was not at ceiling, with adults making errors on an average of 21% of the trials</li> <li>This error rate, which is numerically higher than the rate observed with the 9-year-olds of Experiments 1 and 2, is likely due to the added complexity of the task involving four pictures as opposed to two.</li> <li>Across the trials involving sentences with every, adult participants never selected either of the filler pictures. This indicates that their response set was effectively the same as that of the children in Experiment 2.</li> <li>Thus, unlike the children in Experiment 2, the adults did not show a preference for locative scenes in which all of the containers were filled.</li> <li>A further examination of data indicated that two adults selected the same picture on 12 of 12 trials indicating no sensitivity to the position of the quantifier in the sentence</li> <li>The fact that half of our adult participants made considerable numbers of errors in restricting every to its domain is not readily explicated by syntactic accounts positing immature syntactic representations as the source of children's quantifier-spreading errors</li> <li>Our findings that both children and adults make errors in quantifier interpretation are more readily explained by the underspecification account of Sanford and Sturt (2002).</li> <li>The crucial step involves deficient mapping from syntactic structure to a semantic representation,</li> <li>Although grammatically competent adults are capable of construing correct and fully specified semantic representations of utterances with quantifiers, it does not always happen</li> <li>The results demonstrate that many school-age children and adults had considerable difficulty in restricting the domain of a universal quantifier, especially when two sets of entities were in partial, one-to-one correspondence. This result contrasts most dramatically with the near perfect performance of preschool children in Crain et al. (1996)</li> <li>This suggests that the problem does not reside in the child's syntax, given the similarities in sentence structures used across studies, but in</li> <li>stead has to do with the difficulty of selecting the appropriate set of entities and avoiding distraction by salient objects.</li> <li>Taken together, the experiments suggest that it was the collective scenes as opposed to the use of all that improved children's performance in Experiment 1. Collective scenes were easier presumably because the group depiction aided the child in isolating one set of entities relative to the other</li> <li>Conversely, the distributive scenes were more difficult because the pictures were more visually symmetric. The observed difference in performance for collective versus distributive scenes seems to undermine syntactic accounts of children's errors given that the structures of the corresponding sentences were essentially the same.</li> <li>The fact that children's errors in Experiment 2 were not randomly distributed indicates that they noticed the extra objects and/or containers in the distributive pictures</li> <li>Again, only 9-yearolds consistently varied their picture selections in accordance with the varying syntactic constructions and did not show a strong preference for one picture configuration at the expense of the other.</li> <li>Their performance on a modified version of the sentence\u2013picture matching task was not only below ceiling, but their error rate was numerically higher than that of the 9-year-olds of Experiments 1 and 2. Note, however, that in contrast to the 7- and 8-year-olds' patterns, the adults' errors were equally distributed between the locative pictures with extra animals or objects versus extra containers.</li> <li>We suspect that both children and adults make errors in comprehension because they engage in shallow processing that causes inaccurate mapping between syntactic and semantic representations.</li> <li>In interpreting universal quantifiers, children construct underspecified representations using simpler processing strategies and then rely on pragmatics to solve the task.</li> <li>Adults also may construct underspecified representations as a first step that may or may not be followed by the application of algorithm. We speculate that adults stop at an underspecified representation when there are other demands on attention under conditions of working memory load, fatigue, or lack of cognitive effort.</li> <li>Another possibility with respect to the results of Experiment 2 is that the children may have gradually picked up on the fact that the universal quantifier modified the noun corresponding to the containers in two of three of the sentences.</li> <li>In either case, once children were fixated on a particular picture configuration, they perseverated and were reluctant to consider a competing picture as a possible alternative, even in the face of a conflicting sentence structure</li> <li>The suggestion that the children processed the sentences deterministically is not a new one.</li> <li>have indicated that children tend not to revise their initially incorrect interpretations of temporary syntactic or referential ambiguities even when disambiguating information becomes available.</li> <li>Shallow processing also provides a straightforward explanation of the errors made by adults in Experiment 3. Their high error rates suggest that adult listeners often do not tax their limited information-processing capacities by conducting exhaustive syntactic analyses of sentences but rather make use of simpler strategies in generating reasonable guesses</li> <li>These findings led Ferreira et al. (2002) to conclude that the meaning people obtain for a sentence is often not a reflection of its true content (p. 11) and that language processing often yields a merely good enough representation of a sentence's meaning</li> <li>This statement is an apt characterization of the performance of many school-age children and adults in our experiments. More generally, the comprehension of universal quantifiers seems an ideal domain for exploring the dynamics of attention allocation, and cognitive effort, in language processing.</li> </ul>"},{"location":"KB/Shrinkage/","title":"Shrinkage","text":""},{"location":"KB/Shrinkage/#shrinkage","title":"Shrinkage","text":"<ul> <li>A hyperparameter in gradient boosting that controls overfitting. Shrinkage in gradient boosting is analogous to learning rate in gradient descent. Shrinkage is a decimal value between 0.0 and 1.0. A lower shrinkage value reduces overfitting more than a larger shrinkage value.</li> </ul>"},{"location":"KB/ShuffleNet/","title":"ShuffleNet","text":""},{"location":"KB/ShuffleNet/#shufflenet","title":"ShuffleNet","text":"<ul> <li>Channel Shuffle</li> </ul> <pre><code>def channel_shuffle_new(x, groups):\n    return rearrange(x, 'b (c1 c2) h w -&gt; b (c2 c1) h w', c1=groups)\n</code></pre> <pre><code>class ShuffleUnitNew(nn.Module):\n    def __init__(self, in_channels, out_channels, groups=3, \n                 grouped_conv=True, combine='add'):\n        super().__init__()\n        first_1x1_groups = groups if grouped_conv else 1\n        bottleneck_channels = out_channels // 4\n        self.combine = combine\n        if combine == 'add':\n            # ShuffleUnit Figure 2b\n            self.left = Rearrange('...-&gt;...') # identity\n            depthwise_stride = 1\n        else:\n            # ShuffleUnit Figure 2c\n            self.left = nn.AvgPool2d(kernel_size=3, stride=2, padding=1)\n            depthwise_stride = 2\n            # ensure output of concat has the same channels as original output channels.\n            out_channels -= in_channels\n            assert out_channels &gt; 0\n\n        self.right = nn.Sequential(\n            # Use a 1x1 grouped or non-grouped convolution to reduce input channels\n            # to bottleneck channels, as in a ResNet bottleneck module.\n            conv1x1(in_channels, bottleneck_channels, groups=first_1x1_groups),\n            nn.BatchNorm2d(bottleneck_channels),\n            nn.ReLU(inplace=True),\n            # channel shuffle\n            Rearrange('b (c1 c2) h w -&gt; b (c2 c1) h w', c1=groups),\n            # 3x3 depthwise convolution followed by batch \n            conv3x3(bottleneck_channels, bottleneck_channels,\n                    stride=depthwise_stride, groups=bottleneck_channels),\n            nn.BatchNorm2d(bottleneck_channels),\n            # Use 1x1 grouped convolution to expand from \n            # bottleneck_channels to out_channels\n            conv1x1(bottleneck_channels, out_channels, groups=groups),\n            nn.BatchNorm2d(out_channels),\n        )        \n\n    def forward(self, x):\n        if self.combine == 'add':\n            combined = self.left(x) + self.right(x)\n        else:\n            combined = torch.cat([self.left(x), self.right(x)], dim=1)\n        return F.relu(combined, inplace=True)\n</code></pre>"},{"location":"KB/Shuffled-AUC/","title":"Shuffled-AUC","text":""},{"location":"KB/Shuffled-AUC/#shuffled-auc","title":"Shuffled-AUC","text":"<ul> <li>FPR is calculated based on the negatives which are determined by fixation points of all the other images in the dataset.</li> <li>\"AUC for the curve is calculated as sAUC.\"</li> </ul>"},{"location":"KB/Sigmoid/","title":"Sigmoid","text":""},{"location":"KB/Sigmoid/#sigmoid","title":"Sigmoid","text":"<ul> <li> \\[\\sigma(x) = \\frac{1}{1+exp(-x)}\\] </li> <li> \\[\\frac{d\\sigma}{dx}(x) = \\sigma(x)(1-\\sigma(x))\\] <ul> <li>max : 0.25</li> </ul> </li> <li>Logistic</li> <li>Xavier/Glorot init</li> <li>RNN : Hidden</li> <li>Bernoulli Distribution over a binary variable</li> <li></li> <li></li> </ul>"},{"location":"KB/SimCLR/","title":"SimCLR","text":"<p>toc: true title: SimCLR</p> <p>categories: ['temp']</p>"},{"location":"KB/SimCLR/#simclr","title":"SimCLR","text":"<ul> <li>A Simple Framework for Contrastive Learning of Visual Representations<ul> <li>contrastive learning of visual representations</li> <li>without requiring specialized architectures or a memory bank</li> <li>composition of data augmentations plays a critical role in defining effective predictive tasks</li> <li>introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations</li> <li>contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning</li> <li>use of a nonlinear head at the end of the network, and the loss function</li> <li>Res Net</li> <li>Two separate data augmentation operators are sampled from the same family of augmentations</li> <li>applied to each data example to obtain two correlated views</li> <li>After training is completed, they throw away the projection head and use the encoder for downstream tasks</li> <li>head \\(g(\\cdot)\\)</li> <li>encoder \\(f(\\cdot)\\)</li> <li>representation \\(h\\)</li> <li></li> </ul> </li> </ul>"},{"location":"KB/Simple%20Gradient%20Descent/","title":"Simple Gradient Descent","text":""},{"location":"KB/Simple%20Gradient%20Descent/#simple-gradient-descent","title":"Simple Gradient Descent","text":"<ul> <li> \\[\\theta = \\theta - \\eta \\cdot \\nabla_{\\theta} J(\\theta)\\] </li> <li>It starts with some coefficients, sees their cost, and searches for cost value lesser than what it is now.</li> <li>It moves towards the lower weight and updates the value of the coefficients.</li> <li>The process repeats until the local minimum is reached. A local minimum is a point beyond which it can not proceed.</li> </ul>"},{"location":"KB/Simulations%20Of%20language/","title":"Mirman Et Al","text":"<p>Basic RNN Architectures - Simple rnn performed like human learners     - sensitive to transitional prob and freq</p>"},{"location":"KB/Simulations%20Of%20language/#mirman-et-al","title":"Mirman Et Al","text":""},{"location":"KB/Single%20unit%20recording/","title":"Single unit recording","text":"<ul> <li>Authors predictions: units will either be on or off (no activation or nearly fully activated)</li> </ul>"},{"location":"KB/Single%20unit%20recording/#single-unit-recording","title":"Single Unit Recording","text":""},{"location":"KB/Singularity/","title":"Singularity","text":""},{"location":"KB/Singularity/#singularity","title":"Singularity","text":"<ul> <li>A configuration where two joints of the robot arm become co-axial (aligned along a common axis). In a singular configuration, smooth path following is normally impossible and the robot may lose control. The term originates from the behavior of the Jacobian matrix, which becomes singular (i.e., has no inverse) in these configurations.</li> </ul>"},{"location":"KB/Sketched%20Update/","title":"Sketched Update","text":""},{"location":"KB/Sketched%20Update/#sketched-update","title":"Sketched Update","text":"<ul> <li>Learn a full model update, then compress it before sending to the server.</li> <li>First computes the full Hit during local training without any constraints, and then approximates, or encodes, the update in a (lossy) compressed form before sending to the server. The server decodes the updates before doing the aggregation.</li> <li>Subsampling - Instead of sending Hit , each client only communicates matrix H\u0302it which is formed from a random subset of the (scaled) values of Hit.</li> <li>Quantize the weights -Improving the quantization by structured random rotations. The above 1-bit and multi-bit quantization approach work best when the scales are approximately equal across different dimensions.</li> <li>In the decoding phase, the server needs to perform the inverse rotation before aggregating all the updates.</li> </ul>"},{"location":"KB/Sketching/","title":"Sketching","text":""},{"location":"KB/Sketching/#sketching","title":"Sketching","text":"<ul> <li>In unsupervised machine learning, a category of algorithms that perform a preliminary similarity analysis on examples. Sketching algorithms use a locality-sensitive hash function</li> <li>to identify points that are likely to be similar, and then group them into buckets.</li> <li>Sketching decreases the computation required for similarity calculations on large datasets. Instead of calculating similarity for every single pair of examples in the dataset, we calculate similarity only for each pair of points within each bucket.</li> </ul>"},{"location":"KB/Skew%20Tilt/","title":"Skew Tilt","text":""},{"location":"KB/Skew%20Tilt/#skew-tilt","title":"Skew Tilt","text":"<ul> <li>The image is tilted forwards, backwards, left, or right a maximum of 22.5\u00b0. This gives the illusion that the image is being viewed from a different perspective than originally seen and creates realistic examples.</li> </ul>"},{"location":"KB/Skewed%20data/","title":"Skewed data","text":""},{"location":"KB/Skewed%20data/#skewed-data","title":"Skewed data","text":"<ul> <li>bias within the data acquisition process.</li> </ul>"},{"location":"KB/Skip%20Connection/","title":"Skip Connection","text":""},{"location":"KB/Skip%20Connection/#skip-connection","title":"Skip Connection","text":"<ul> <li> \\[x_i = F(x_{i-1}) + x_{i-1}\\] </li> <li>Effect Of Depth</li> <li>Previous layer gradient carried to next module untouched -&gt; loss surface is smoother</li> <li>Transfer #gradients to prevent Vanishingexploding gradients</li> <li>Learns the difference (residual) \\(\\(F(x) = H(x)-x\\)\\)</li> </ul>"},{"location":"KB/Skip%20Gram/","title":"Skip Gram","text":""},{"location":"KB/Skip%20Gram/#skip-gram","title":"Skip Gram","text":"<ul> <li>the distributed representation of the input word is used to predict the context.</li> <li>tries to predict the neighbors of a word</li> <li>works well with a small amount of the training data, represents well even rare words or phrases.</li> <li>Skip-gram rely on single words input, it is less sensitive to overfit frequent words, because even if frequent words are presented more times that rare words during training, they still appear individually</li> <li>tends to study different contexts separately</li> <li>needs more data to be trained contains more knowledge about the context.</li> <li>takes in pairs (word1, word2) generated by moving a window across text data, and trains a 1-hidden-layer neural network based on the synthetic task of given an input word, giving us a predicted probability distribution of nearby words to the input. </li> <li>A virtual KB/One hot.md encoding of words goes through a \u2018projection layer\u2019 to the hidden layer; these projection weights are later interpreted as the word embeddings. </li> <li>So if the hidden layer has 300 neurons, this network will give us 300-dimensional word embeddings.</li> <li>also uses Negative Sampling</li> </ul>"},{"location":"KB/Slice%20Based%20Volume%20Rendering/","title":"Slice Based Volume Rendering","text":""},{"location":"KB/Slice%20Based%20Volume%20Rendering/#slice-based-volume-rendering","title":"Slice Based Volume Rendering","text":"<ul> <li>assign transparency inversely proportional to the number of slices</li> <li></li> </ul>"},{"location":"KB/Sliding%20Window%20Attention/","title":"Sliding Window Attention","text":""},{"location":"KB/Sliding%20Window%20Attention/#sliding-window-attention","title":"Sliding Window Attention","text":"<ul> <li>Given the importance of local context, the sliding window attention pattern employs a fixed-size window attention surrounding each token<ul> <li>multiple stacked layers of such windowed attention results in a large receptive field, where top layers have access to all input locations and have the capacity to build representations that incorporate information across the entire input</li> <li>But a model with typical multiple stacked transformers will have a large receptive field. This is analogous to CNNs where stacking layers of small kernels leads to high level features that are built from a large portion of the input (receptive field)</li> <li>Depending on the application, it might be helpful to use different values of \\(w\\) for each layer to balance between efficiency and model representation capacity.</li> </ul> </li> <li>Given a fixed window size \\(w\\), each token attends to \\(\\frac{1}{2}w\\) tokens on each size</li> <li>Complexity is \\(O(n \\times w)\\)<ul> <li>\\(w\\) should be small compared to \\(n\\)</li> </ul> </li> <li>With \\(l\\) layers, receptive field size is \\(l \\times w\\)</li> <li></li> </ul>"},{"location":"KB/SlimStampen/","title":"SlimStampen","text":""},{"location":"KB/SlimStampen/#slimstampen","title":"SlimStampen","text":"<ul> <li>Predicting University Students' Exam Performance Using a Model-Based Adaptive Fact-Learning System</li> </ul>"},{"location":"KB/SlimStampen/#literature","title":"Literature","text":"<ul> <li>Digital learning systems allow learners to track their progress and make study decisions informed by data1.</li> <li>For example, Duolingo, a language- learning tool, shows learners an overview of their mastery of each lesson in a dashboard (Figure 1(a) in Settles &amp; Meeder, 2016). Rosetta Stone, another language-learning tool, has a similar dashboard and includes a suggested next study activity (Ridgeway, Mozer, &amp; Bowles, 2017).</li> <li>Adaptive learning systems take this a step further by assuming control over some study choices that might otherwise be made by learners. Using an internal model of the learner that is informed by the learner\u2019s performance, such systems can adapt the learning experience in real time (VanLehn, 2006).</li> <li>The adaptation can include changing the difficulty of the problems presented to the learner, changing the amount of feedback that the learner receives, and changing the scheduling of repetitions within and between learning sessions</li> <li>What type and degree of adaptivity are most beneficial is an empirical question and depends on whether the adaptive system accurately traces the acquisition and forgetting of knowledge over time. If implemented well, adaptive learning systems can help students achieve more effective study behaviour by facilitating spaced repetition, active study, and other effective techniques.</li> </ul>"},{"location":"KB/SlimStampen/#slimstampen-a-model-based-adaptive-fact-learning-system-app","title":"SlimStampen: A Model-Based Adaptive Fact-Learning System App","text":"<ul> <li>Following correct answers, the next trial commenced after one second. For incorrect answers, feedback remained on the screen until the learner pressed the \u201cNext\u201d button at the bottom of the screen (see Figures 1b and 1d), making the feedback similar to the study trials.</li> </ul>"},{"location":"KB/SlimStampen/#scheduling-algorithm","title":"Scheduling Algorithm","text":"<ul> <li>extension of the adaptive item-learning model by Pavlik and Anderson (2005; 2008) and has been tested in laboratory settings (van Rijn, van Maanen, &amp; van Woudenberg, 2009; Sense, Behrens, Meijer, &amp; van Rijn, 2016; Sense, Meijer, &amp; van Rijn, 2018) but has not been deployed in a university course before.</li> <li>his model capitalizes on the spacing effect (see Dempster, 1988, for a review) within a single session by scheduling repetitions as far apart as possible, while also</li> <li>optimizing for the testing effect (see van den Broek et al., 2016, for a review) by repeating items soon enough that most responses are correct.</li> <li>The model represents every encountered item by a unique memory chunk, based on the ACT-R theory of declarative memory (Anderson, 2007).</li> <li>Each chunk has an activation\u2014a representation of the ease with which that item could be retrieved\u2014that receives a boost whenever an item is re-encoded and that decays over time</li> <li>The activation A of a chunk i at time t, given n previous encounters at t1,\u2026,tn seconds ago, is<ul> <li> \\[A_{i}(t) = ln(\\Sigma_{j=1}^{n}t_{j}^{-d_{i}(t)}\\] </li> <li> \\[d_{i}(t)=c \\ast e^{A_{i}(t_{n-1})}+\\alpha_{i} \\] </li> </ul> </li> <li>When a new trial commences, the model determines the activation of all items 15 seconds in the future, and if the item with the lowest activation has an activation value below a retrieval threshold, that item will be scheduled for presentation</li> <li>If all predicted activations are above the retrieval threshold, the model will introduce a new item</li> <li>y selecting items on the basis of their activation, items will be repeated with as much spacing as possible, while ensuring that, theoretically, a correct response can still be given.</li> <li>The decay of the activation (parameter d in Equation (1)) varies between items to account for differences in difficulty. The higher this decay, the faster a chunk\u2019s activation will decrease, causing it to be repeated sooner than an item with a lower decay.</li> <li>The decay d of a chunk i at time t depends on the activation of the chunk at the time of its previous encounter, as well as an offset that we label the rate of forgetting, \u03b1</li> <li>The model assumes that each item has a standard initial rate of forgetting when it is first presented. However, this value is updated during learning</li> <li>At each presentation, the model calculates an expected response time, E(RT ), based on the activation at the time of the presentation (e\u2212Ai , based on Equation (5) in Anderson, Bothell, Lebiere, &amp; Matessa, 1998) and an estimated reading time of the prompt (based on the number of characters in the prompt; see Section 2.2.1 in Nijboer, 2011, for details).</li> <li>The accuracy of the response and the mismatch between expected and observed response time are used to update the value of the rate-of-forgetting parameter.</li> <li>Using both accuracy and response time to update the model allows for adjustment of the parameter estimate after any response, not just after an incorrect response.</li> <li>A correct but slower-than-expected response signals that the memory trace has decayed further than assumed, meaning that the item\u2019s true rate of forgetting is higher than the current estimate.</li> <li>That is, when a learner arrives at the right answer but takes longer than anticipated, they likely struggled to KB/Recall.md the information</li> <li>Conversely, an incorrect or missing response suggests that the activation of the</li> <li>item\u2019s [memory trace] actually dropped below the retrieval threshold, which means that the true rate of forgetting should be higher because this item\u2019s activation was expected to be above the threshold (which was fixed at ACT-R.</li> <li>An unexpectedly fast correct response, on the other hand, indicates a stronger- than-expected memory trace and implies that the estimated rate of forgetting should be adjusted downward.</li> <li>Since interruption or distraction can cause disproportionately large response times, observed response times are capped before their mismatch with the expected response time is calculated.</li> <li>To update the rate of forgetting after each trial, the model uses a binary search in a small window around the previous value to identify the rate of forgetting that minimizes the mismatch between E(RT) and RT\u2032</li> </ul>"},{"location":"KB/SlimStampen/#usage-of-the-system","title":"Usage of the System","text":"<ul> <li>Most students exhibited strong \u201ccramming\u201d behaviour, with much higher SlimStampen usage in the days leading up to the exam: in both cohorts, we observed a sharp increase in activity starting around 10 days before the exam and peaking on the last day. As the exam neared, usage intensified throughout the day and extended into the night.</li> </ul>"},{"location":"KB/SlimStampen/#exam-performance","title":"Exam Performance","text":"<ul> <li>In both cohorts, students that used SlimStampen (92.6% of students) obtained higher grades than those that did not\u2014averaging 6.91 compared to 5.86, respectively</li> <li>a direct comparison of these groups is problematic due to selection effects and the imbalanced distributio</li> </ul>"},{"location":"KB/SlimStampen/#amount-of-practice","title":"Amount of Practice","text":"<ul> <li>number of study trials completed was positively correlated with the final grade completing more trials was associated with higher grades on the exam</li> <li>The number of unique days on which a learner engaged with the tool\u2014an index of spaced practice\u2014was also positively correlated with exam grades (r = 0.27, t(283) = 4.81, p &lt; 0.001)</li> <li>the two measures of engagement were strongly and positively correlated (r = 0.75, t(283) = 18.78, p &lt; 0.001).</li> </ul>"},{"location":"KB/SlimStampen/#studied-versus-non-studied-items","title":"Studied Versus Non-studied Items","text":"<ul> <li>We observed a large difference between exam questions that learners had used the system to study and questions that they had not3: students\u2019 accuracy was 83.7% on studied items but only 53.6% on unstudied items</li> <li>A mixed-effects logistic regression (with random intercepts for learners and items) confirmed that encountering an item during SlimStampen rehearsal considerably increased the chances of a correct answer on the exam (bstudied/not studied = 1.70, SE = 0.18, z = 9.06, p &lt; 0.001).</li> </ul>"},{"location":"KB/SlimStampen/#rates-of-forgetting-and-grades","title":"Rates of Forgetting and Grades","text":"<ul> <li>The rate of forgetting, which was initially estimated for each learner\u2013item combination, was converted into a learner-specific rate of forgetting by averaging over all studied items</li> <li>The negative correlation shows that a learner who was estimated to forget material more slowly also tended to obtain higher grades.</li> <li>In practice, a possible relationship between someone\u2019s rate of forgetting and eventual exam performance would be most useful if it could be detected ahead of time rather than on the day of the exam\u2014when it is too late to potentially help struggling students, for example</li> <li>This pattern could be driven by additional learners that start at the last minute and demonstrate poor learning performance and poor grades</li> </ul>"},{"location":"KB/SlimStampen/#predicting-performance-on-individual-exam-questions","title":"Predicting Performance on Individual Exam Questions","text":"<ul> <li>The results reported so far confirm that the expected patterns emerged in the aggregate: a learner\u2019s average rate of forgetting was strongly related to their average performance on the exam</li> <li>A step-wise backward elimination procedure was used to find the best model: starting with the full model, the term with the lowest absolute z-value was removed until the simpler model was no longer preferred on the basis of BIC and AIC (Gelman &amp; Hill, 2006).</li> <li>Additionally, the estimated rate of forgetting modulated the effect such that learner\u2013 item combinations with very low rates of forgetting have a higher chance of yielding a correct answer.</li> <li>The differences in rates of forgetting are especially pronounced at a low number of repetitions due to the non-linear mapping between the predictors and the predicted probability introduced by the logit function</li> </ul>"},{"location":"KB/SlimStampen/#predicting-performance-on-the-exam","title":"Predicting Performance on the Exam","text":"<ul> <li>We used lasso regression (Tibshirani, 1996) to predict grades using nine predictors: a student\u2019s accuracy during study, their cohort, their cumulative usage time, the number of days on which they used the system, the number of items they studied, the number of sessions they recorded, the number of trials they completed, their estimated rate of forgetting, and their median response time</li> <li>The advantage of lasso regression is that the shrinkage term handles multicollinearity between the predictors by shrinking their coefficients</li> <li>The shrinkage is achieved by imposing a cost function on the magnitude of the</li> <li>coefficients themselves: the best fit is achieved by the model that minimizes the OLS with the smallest coefficients. In fact, coefficients are shrunk entirely if they do not explain sufficient variance to justify inclusion in the model. In lasso regression, predictors must be normalized to ensure that the shrinkage term affects all predictors equally. A convenient consequence of normalized predictors is that their post-shrinkage</li> <li>coefficients directly indicate their importance: since all predictors are on the same scale, the most important predictor retains the largest (absolute) coefficient.</li> <li>250-fold cross-validation procedure</li> </ul>"},{"location":"KB/SlimStampen/#comparing-self-reported-and-recorded-study-times","title":"Comparing Self-Reported and Recorded Study Times","text":"<ul> <li>This means that students who used SlimStampen more did not necessarily self- report studying more overall. Thus, the positive association between more SlimStampen usage and higher grades was unlikely to be a consequence of higher motivation alone.</li> <li>This suggests, unsurprisingly, that general studiousness led to higher exam performance</li> <li>More interestingly, time spent studying with SlimStampen was time well spent, as the expected gain in grades associated with additional hours of study was 0.11 points, compared to only 0.03 points gained by an hour of unspecified study time.</li> </ul>"},{"location":"KB/SlimStampen/#discussion","title":"Discussion","text":"<ul> <li>Students\u2019 rates of forgetting, estimated by the system during use, were correlated with exam performance up to two weeks before the exam (Figure 2), even though &lt; 5% of the data were available at that point</li> <li>Furthermore, rate-of-forgetting estimates for individual facts were predictive of learners\u2019 performance on the associated exam questions, along with the number of times these facts were repeated during study</li> <li>One limitation of the sample was that we did not know what other study methods students may have used alongside the system. It is possible that the spike in activity in the days preceding the exam was caused by students verifying that they had retained the knowledge obtained through other study activities</li> </ul>"},{"location":"KB/SlimStampen/#implications","title":"Implications","text":"<ul> <li>controlling within-session study decisions through the adaptive fact-learning system, leaving other study decisions\u2014when to study, which chapter to study, how long to study, and whether to study with open response or multiple-choice questions\u2014to the learner</li> <li>students still made sub-optimal decisions about when to repeat a lesson that they had studied previously.</li> <li>Alternatively, the system could suggest the lesson that would yield the largest learning gain at the moment a student decides to start a session</li> </ul>"},{"location":"KB/SlimStampen/#pictures","title":"Pictures","text":""},{"location":"KB/Small%20World%20graphs/","title":"Small World graphs","text":""},{"location":"KB/Small%20World%20graphs/#small-world-graphs","title":"Small World Graphs","text":"<ul> <li>Any two nodes in the graph are connected via a smalll number of steps</li> </ul>"},{"location":"KB/Smart%20Augmentation/","title":"Smart Augmentation","text":""},{"location":"KB/Smart%20Augmentation/#smart-augmentation","title":"Smart Augmentation","text":"<ul> <li>utilizes a similar concept as the Neural Augmentation technique</li> <li>However, the combination of images is derived exclusively from the learned parameters of a prepended CNN, rather than using the Neural Style Transfer algorithm.</li> <li>another approach to meta-learning augmentations</li> <li>This is done by having two networks, Network-A and Network-B. Network-A is an augmentation network that takes in two or more input images and maps them into a new image or images to train Network-B. The change in the error rate in Network-B is then</li> <li>backpropagated to update Network-A.</li> <li>Additionally another loss function is incorporated into Network-A to ensure that its outputs are similar to others within the class. Network-A uses a series of convolutional layers to produce the augmented image</li> <li>The conceptual framework of Network-A can be expanded to use several Networks trained in parallel. Multiple Network-As could be very useful for learning class-specific augmentations via meta-learning</li> </ul>"},{"location":"KB/Smooth-Grad/","title":"Smooth-Grad","text":""},{"location":"KB/Smooth-Grad/#smooth-grad","title":"Smooth-Grad","text":"<ul> <li>@smilkovSmoothGradRemovingNoise2017</li> <li>reduces visual noise and, hence, improves visual explanations about how a DNN is making a classification decision. Comparing their work to several gradient-based sensitivity map methods such as LRP, [DeepLift], and Integrated Gradients (IG) [96], which estimate the global importance of each pixel and create saliency maps, showed that Smooth-Grad focuses on local sensitivity and calculates averaging maps with a smoothing effect made from several small perturbations of an input image. The effect is enhanced by further training with these noisy images and finally having an impact on the quality of sensitivity maps by sharpening them.</li> <li>a local, post hoc approach gave visual and textual justifications of the predictions with the help of two novel explanation datasets through crowd sourcing.</li> <li>involves adding random noise to the input and computing the attribution maps multiple times with the noisy inputs. </li> <li>The final attribution map is obtained by averaging the maps obtained from the noisy inputs. The idea behind this technique is that the noise added to the input image will cause the model to activate different features in the input, resulting in a more stable and interpretable attribution map.</li> </ul>"},{"location":"KB/Smooth-Grad/#technical-details","title":"Technical Details","text":"<ul> <li>Consider an image classification task where an input image \\(x\\) is to be classified as a single class from a set \\(C\\). For every class \\(c \\in C\\), the output class is represented as \\(class(x) = argmax_{c \\in C}S_{c}(x)\\). Using this \\(class\\), a sensitivity map \\(M_{c}(x)\\) can be generated by differentiating with respect to \\(x\\), \\(M_{c}(x) = \\frac{\\partial S_{c}}{\\partial x}\\) . \\(M_{c}\\), being a sensitivity map, thus represents the influential regions of the image used to make the prediction. Since these maps are noisy in nature, Smilkov et al. propose SmoothGrad, a modification of the previous method where instead of using \\(\\partial S_{c}\\), a smoothing is applied using a Gaussian kernel to \\(\\partial S_{c}\\). The authors also find that it is not possible to directly compute the smoothing due to high dimensionality, and thus approximate the calculation by averaging multiple maps computed in the neighborhood of \\(x\\) using random sampling. The final SmoothGrad equation then becomes \\(\\hat M_{c}(x) = \\frac{1}{n}\\Sigma_{1}^{n}M_{c}(x + \\mathcal{N}(0, \\sigma^{2}))\\), where \\(\\mathcal{N}(0, \\sigma^{2})\\) is the Gaussian noise and \\(\\sigma\\) is the standard deviation.</li> </ul>"},{"location":"KB/SmoothGrad%20Square/","title":"SmoothGrad Square","text":""},{"location":"KB/SmoothGrad%20Square/#smoothgrad-square","title":"SmoothGrad Square","text":"<ul> <li> smoothgrad squre</li> </ul>"},{"location":"KB/SmoothMix/","title":"SmoothMix","text":""},{"location":"KB/SmoothMix/#smoothmix","title":"SmoothMix","text":"<ul> <li>@leeSmoothMixSimpleEffective2020</li> <li>mask-based approach</li> <li>matching closely with the [Cutout] and CutMix techniques</li> <li>the mask has soft edges with gradually decreasing intensity</li> <li>the mixing strategy is the same</li> <li>The augmented image has mixed pixel values depending on the strength of the mask</li> <li> \\[\\lambda= \\frac{\\Sigma_{i=1}^{W}\\Sigma_{j=1}^{H}G_{ij}}{WH}\\] </li> <li>Gij is the pixel value of mask G, H is height, and W is width</li> <li>xnew = G.xa + (1 \u2212 G).xb</li> <li>ynew = \u03bb.ya + (1 \u2212 \u03bb).yb</li> </ul>"},{"location":"KB/Smoothness/","title":"Smoothness","text":""},{"location":"KB/Smoothness/#smoothness","title":"Smoothness","text":"<ul> <li>Every supervised machine learning method assumes that there's a set of functions that can transform inputs into outputs such that similar inputs are transformed into similar outputs. If an input X produces an output Y, then an input close to X would produce an output proportionally close to Y.</li> </ul>"},{"location":"KB/SnapMix/","title":"SnapMix","text":""},{"location":"KB/SnapMix/#snapmix","title":"SnapMix","text":"<ul> <li>@huangSnapMixSemanticallyProportional2021</li> <li>augments training images by ex- tracting and merging random image regions of dif- ferent sizes, where the region size is drawn through the beta distribution for both the images.</li> <li>Gen- erated image label is assigned based on semantic composition from normalized (sum to one) CAMs.</li> <li>However, the summation of label coefficients can exceed beyond one depending on the semantic composition of the output image.</li> </ul>"},{"location":"KB/Social%20Construction%20of%20XAI%2C%20do%20we%20need%20one%20definition%20to%20rule%20them%20all/","title":"Social Construction of XAI, do we need one definition to rule them all","text":""},{"location":"KB/Social%20Construction%20of%20XAI%2C%20do%20we%20need%20one%20definition%20to%20rule%20them%20all/#social-construction-of-xai-do-we-need-one-definition-to-rule-them-all","title":"Social Construction of XAI, Do We Need One Definition to Rule Them All","text":"<ul> <li>@ehsanSocialConstructionXAI2022</li> </ul>"},{"location":"KB/Social%20Construction%20of%20XAI%2C%20do%20we%20need%20one%20definition%20to%20rule%20them%20all/#abstract","title":"Abstract","text":"<ul> <li>In this paper, we argue why a singular definition of XAI is neither feasible nor desirable at this stage of XAI's development</li> <li>We view XAI through the lenses of Social Construction of Technology (SCOT) to explicate how diverse stakeholders (relevant social groups) have different interpretations (interpretative flexibility) that shape the meaning of XAI. Forcing a standardization (closure) on the pluralistic interpretations too early can stifle innovation and lead to premature conclusions.</li> </ul>"},{"location":"KB/Social%20Construction%20of%20XAI%2C%20do%20we%20need%20one%20definition%20to%20rule%20them%20all/#of-bicycles-explainable-ai","title":"Of Bicycles &amp; Explainable AI","text":"<ul> <li>As we reflect on the evolution of the bicycle, why and how did things evolve the way they did?</li> <li>We will address this question using three concepts from SCOT. First, we have relevant social groups\u2014stakeholders with skin in the game such as bikers, families of bikers, mechanics fixing bikes, etc. These are the ones who are involved in or affected by a technological development</li> <li>Different relevant social groups have their own interpretive flexibility\u2014 interpretations of what it means to be a bicycle.</li> <li>ifferent interpretive flexibilities can give rise to different types of bicycles such as mountain bikes, electric bikes, BMX bikes, etc</li> <li>Finally, we have the notion of closure\u2013 over time, some interpretations of the bicycle achieved stability while others withered out (e.g., equal sized wheels won out over differently-sized wheels</li> <li>Just like bicycles, XAI has its relevant social groups</li> <li>Let's consider two relevant social groups: the Natural Language Processing (NLP) and Computer Visions (CV) communitie</li> <li>Given each group has its own ways of knowing (epistemology), there is interpretive flexibility on how they operationalize the notion of explainability</li> <li>in NLP question-answering, explanations are often of the form of additional text that justifies the ground truth answer</li> <li>In CV, object recognition can consider saliency maps that show how visual features correlate to a predicted label</li> <li>This is to be expected because, unlike bicycles, we don't have 200+ years of development to reach clusters of closures yet.</li> <li>olving XAI challenges may require more than just \"opening the black-box\" [6]</li> <li>Human-centered XAI (HCXAI) advocates to tackle XAI problems through a sociotechnical view (vs. a purely technical one) [7]</li> <li>We need to consider who is opening the box just as much as the algorithmic mechanisms of opening it</li> <li>Whereas a lot of initial focus was on developers and data scientists as end-users of XAI systems, there is a growing recognition that we need to accommodate a diverse set of end-users, especially non-AI experts [10, 11]</li> </ul>"},{"location":"KB/Social%20Construction%20of%20XAI%2C%20do%20we%20need%20one%20definition%20to%20rule%20them%20all/#making-progress-in-xai","title":"Making Progress in XAI","text":"<ul> <li>XAI is pluralistic</li> <li>Given the different epistemic cultures co-existing in the space,we cannot expect monolithic conformity at this stage.</li> <li>Pluralism, however, does not mean that anything goes; in fact, it's the opposite\u2014we need to be precise in our articulation of what we mean by explainability when we communicate.</li> <li>Thus, instead of using the term at face value, whenever we write a paper, we should strive to justify how our conception of explainability satisfies some of the shared goals we have in the space.</li> <li>who is saying what, when, and why. To grasp the flavor of explainability in a given context, we need to pay attention to a relevant social group's interpretation of it and how that informs their operationalization.</li> <li>While the notion of XAI is in flux, we are fortunate to join the conversation at this stage. We have substantial agency in steering the discourse, a privilege we need to exercise responsibly.</li> </ul>"},{"location":"KB/Soft%20Attention/","title":"Soft Attention","text":""},{"location":"KB/Soft%20Attention/#soft-attention","title":"Soft Attention","text":"<ul> <li>For a simple Seq2Seq, all hidden state vectors \\(h_t\\) across timesteps are linearly combined</li> <li> \\[c_i = \\Sigma_{j=1}^T \\alpha_{ij} h_j\\] </li> <li> \\[a_{ij} = \\frac{exp(e_{ij})}{\\Sigma_{k=1}^T exp(e_{ij})}\\] </li> <li></li> </ul>"},{"location":"KB/Soft%20Parameter%20Sharing/","title":"Soft Parameter Sharing","text":""},{"location":"KB/Soft%20Parameter%20Sharing/#soft-parameter-sharing","title":"Soft Parameter Sharing","text":"<ul> <li>Constrain weights by adding terms to loss function<ul> <li> \\[||W_{A}-W_{B}||^{2} + ||W_{A}-W_{C}||^{2}\\] </li> </ul> </li> </ul>"},{"location":"KB/Softlimit%20Setting%20Function/","title":"Softlimit Setting Function","text":""},{"location":"KB/Softlimit%20Setting%20Function/#softlimit-setting-function","title":"Softlimit Setting Function","text":"<ul> <li>The Softlimit Setting Function is a function to set the axis travel limit range of the manipulator motion in software.</li> </ul>"},{"location":"KB/Softmax/","title":"Softmax","text":""},{"location":"KB/Softmax/#softmax","title":"Softmax","text":"<ul> <li>Output : probabilities</li> <li> \\[p = \\frac{1}{\\Sigma_{i = 1, .., n}e^{\\frac{\\alpha y_{i}}{T}}}(e^{\\frac{\\alpha y_{1}}{T}} , e^{\\frac{\\alpha y_{2}}{T}} , \u2026, e^{\\frac{\\alpha y_{n}}{T}})'\\] </li> <li>Softer argmax (0,1)</li> <li>Multinoulli</li> <li></li> </ul>"},{"location":"KB/Softmax/#entropy","title":"Entropy","text":"<ul> <li>\\(\\alpha\\) determines entropy</li> <li>If it is 0, and Uniform Distribution and limit to infinity -&gt; binary vector which is 0 everywhere except at position i when y is maximal</li> </ul>"},{"location":"KB/Softmax/#temperature","title":"Temperature","text":"<ul> <li>Higher the T -&gt; Softer it the distribution. Aka less confident about distribution</li> <li>Lower -&gt; Harder. More confident</li> <li></li> </ul>"},{"location":"KB/Softplus/","title":"Softplus","text":""},{"location":"KB/Softplus/#softplus","title":"Softplus","text":"<ul> <li> \\[\\ln(1+e^x)\\] </li> </ul>"},{"location":"KB/Somatic/","title":"Somatic","text":""},{"location":"KB/Somatic/#somatic","title":"Somatic","text":"<ul> <li>Voluntary</li> <li>Skeletal movement</li> </ul>"},{"location":"KB/Somatosensory%20Cortex/","title":"Somatosensory Cortex","text":""},{"location":"KB/Somatosensory%20Cortex/#somatosensory-cortex","title":"Somatosensory Cortex","text":"<ul> <li>Located in the parietal lobe, this region of the brain processes touch, pressure, and pain information.</li> </ul>"},{"location":"KB/Sono-stimulation/","title":"Sono stimulation","text":""},{"location":"KB/Sono-stimulation/#sono-stimulation","title":"Sono-stimulation","text":"<ul> <li>The activation of neural networks using ultrasound.</li> </ul>"},{"location":"KB/Sonogenetics/","title":"Sonogenetics","text":""},{"location":"KB/Sonogenetics/#sonogenetics","title":"Sonogenetics","text":"<ul> <li>A novel investigative approach that turns genetically modified neurons on and off using ultrasonic waves.</li> </ul>"},{"location":"KB/Soundify/","title":"Soundify","text":""},{"location":"KB/Soundify/#soundify","title":"Soundify","text":"<ul> <li>In video editing, sound in half of the story</li> <li>for professional video editing, the problems come from finding suitable sounds, aligning sounds, video and tuning parameters</li> <li>matches sound efects to video - uses quality sound efects libraries and CLIP - Concretely, the system has three parts: classification, synchronization, and mix</li> <li>The classification matches efects to a video by classifying sound emitters within - To reduce the distinct sound emitters, the video is split based on absolute color histogram distances</li> <li>In the synchronization part, intervals are identified comparing efects label with each frame and pinpointing consecutive matches above a threshold</li> <li>In the mix part, efects are split into around one-second chunks - chunks are stitched via crossfades.</li> </ul>"},{"location":"KB/Sparse%20Dictionary%20Learning%20Loss/","title":"Sparse Dict Learning Loss","text":""},{"location":"KB/Sparse%20Dictionary%20Learning%20Loss/#sparse-dict-learning-loss","title":"Sparse Dict Learning Loss","text":"<ul> <li>$\\(L(X) = n^{-1}\\Sigma_i^n ||x_i - Dr_i ||^2 + \\lambda \\Sigma_i |r_i|\\)</li> <li>\\(\\lambda \\Sigma_i |r_i|\\) is Lasso/L1 Lp Regularization</li> <li>Predictions : \\(r = argmin_r ||x- Dr_i ||^2 + \\lambda \\Sigma_i |r_i|\\)</li> </ul>"},{"location":"KB/Sparse%20Encoder%20Indexes/","title":"Sparse Encoder Indexes","text":""},{"location":"KB/Sparse%20Encoder%20Indexes/#sparse-encoder-indexes","title":"Sparse Encoder Indexes","text":"<ul> <li>efficiently handle large-scale knowledge bases with diverse data formats and structures.</li> <li>For semantic search, one may compare different approaches of combining dense vector indexes and sparse encoder indexes to traditional keyword-based retrieval.</li> </ul>"},{"location":"KB/Sparse%20Evolutionary%20Training/","title":"Sparse Evolutionary Training","text":""},{"location":"KB/Sparse%20Evolutionary%20Training/#sparse-evolutionary-training","title":"Sparse Evolutionary Training","text":"<ul> <li>(Mocanu et al., 2018; Liu et al., 2021b)</li> <li>which randomly initializes the sparse connectivity between layers randomly and dynamically adjusts the sparse connectivity via a parameter prune-and-grow scheme during the course of training</li> <li>The parameter prune-and-grow scheme allows the model's sparse structure to gradually evolve, achieving better performance than naively training a static sparse network</li> </ul>"},{"location":"KB/Sparse%20Transformer/","title":"Sparse Transformer","text":""},{"location":"KB/Sparse%20Transformer/#sparse-transformer","title":"Sparse Transformer","text":"<ul> <li>paper</li> <li>Uses Strided Attention</li> </ul>"},{"location":"KB/Sparsity/","title":"Sparsity","text":""},{"location":"KB/Sparsity/#sparsity","title":"Sparsity","text":"<ul> <li>The number of elements set to zero (or null) in a vector or matrix divided by the total number of entries in that vector or matrix. For example, consider a 10x10 matrix in which 98 cells contain zero.</li> </ul>"},{"location":"KB/Spatial%20Context%20Structure/","title":"Spatial Context Structure","text":""},{"location":"KB/Spatial%20Context%20Structure/#spatial-context-structure","title":"Spatial Context Structure","text":"<ul> <li>based on the spatial relations among image patches </li> <li>image jigsaw puzzle </li> <li>context prediction </li> <li>geometric transformation recognition</li> </ul>"},{"location":"KB/Spatial%20Transformer/","title":"Spatial Transformer","text":""},{"location":"KB/Spatial%20Transformer/#spatial-transformer","title":"Spatial Transformer","text":"<ul> <li>Transformer</li> </ul> <pre><code>class SpacialTransformNew(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        # Spatial [Transformer|[transformer](./Transformer.md) localization-network\n        linear = nn.Linear(32, 3 * 2)\n        # Initialize the weights/bias with identity transformation\n        linear.weight.data.zero_()\n        linear.bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n\n        self.compute_theta = nn.Sequential(\n            nn.Conv2d(1, 8, kernel_size=7),\n            nn.MaxPool2d(2, stride=2),\n            nn.ReLU(True),\n            nn.Conv2d(8, 10, kernel_size=5),\n            nn.MaxPool2d(2, stride=2),\n            nn.ReLU(True),\n            Rearrange('b c h w -&gt; b (c h w)', h=3, w=3),\n            nn.Linear(10 * 3 * 3, 32),\n            nn.ReLU(True),\n            linear,\n            Rearrange('b (row col) -&gt; b row col', row=2, col=3),\n        )\n\n    # Spatial transformer network forward function\n    def stn(self, x):\n        grid = F.affine_grid(self.compute_theta(x), x.size())\n        return F.grid_sample(x, grid)\n</code></pre>"},{"location":"KB/Spatiotemporal%20Convolutional%20Neural%20Network/","title":"Spatiotemporal Convolutional Neural Network","text":""},{"location":"KB/Spatiotemporal%20Convolutional%20Neural%20Network/#spatiotemporal-convolutional-neural-network","title":"Spatiotemporal Convolutional Neural Network","text":"<ul> <li>3D convolution operation was first proposed in 3DNet [62] for human action recognition </li> <li>Compared to 2DConvNets which individually extract the spatial information of each frame and then fuse them together as video features, 3DConvNets are able to simultaneously extract both spatial and temporal features from multiple frames. </li> <li>VGG-like 11-layer 3DConvNet designed for human action recognition </li> <li>The network contains 8 convolutional layers, and 3 fully connected layers. All the kernels have the size of 3 x 3 x 3, the convolution stride is fixed to 1 pixel </li> <li>The input of C3D is 16 consecutive RGB frames where the appearance and temporal cues from 16-frame clips are extracted </li> <li>However, the paper of long-term temporal convolutions (LTC) [67] argues that, for # the long-lasting actions, 16 frames are insufficient to represent whole actions which last longer.</li> </ul>"},{"location":"KB/Speaker%20Verification/","title":"Speaker Verification","text":""},{"location":"KB/Speaker%20Verification/#speaker-verification","title":"Speaker Verification","text":"<ul> <li>Deep Neural Networks for Small Footprint Text-dependent Speaker Verification</li> <li>nvestigates the use of deep neural networks (DNNs) to train speaker embeddings for a small footprint text-dependent speaker verification task</li> <li>stacked filterbank features as input</li> <li>During speaker enrollment, the trained DNN is used to extract speaker-specific features/embeddings by averaging the activations from the last hidden layer (called deep-vectors or \u201cd-vectors\u201d for short), which is taken as the speaker model</li> <li>d-vector is extracted for each utterance and compared to the enrolled speaker model to make a verification decision by calculating the KB/Cosine Distance.md between the test d-vector and the claimed speaker\u2019s d-vector, similar to the i-vector framework</li> <li>A verification decision is made by comparing the distance to a threshold</li> <li>DNN based speaker verification system achieves good performance compared to a popular i-vector system on a small footprint text-dependent speaker verification task</li> </ul>"},{"location":"KB/Speakers%20in%20the%20Wild/","title":"Speakers in the Wild","text":""},{"location":"KB/Speakers%20in%20the%20Wild/#speakers-in-the-wild","title":"Speakers in the Wild","text":""},{"location":"KB/SpecAugment/","title":"SpecAugment","text":""},{"location":"KB/SpecAugment/#specaugment","title":"SpecAugment","text":"<ul> <li>SpecAugment: a Simple Data Augmentation Method for Automatic Speech Recognition</li> <li>simple data augmentation method for KB/Speech Recognition.md</li> <li>applied directly to the feature inputs of a neural network</li> <li>warping the features, masking blocks of frequency channels, and masking blocks of time steps</li> <li>apply SpecAugment on Listen, Attend and Spell (LAS) networks for end-to-end KB/Speech Recognition.md tasks</li> <li>LibriSpeech</li> <li>Swichboard</li> <li>end-to-end LAS networks by augmenting the training set using simple handcrafted policies</li> <li>converts ASR from an over-fitting to an under-fitting problem, and they are able to gain performance by using bigger networks and training longer</li> </ul>"},{"location":"KB/Specificity/","title":"Specificity","text":""},{"location":"KB/Specificity/#specificity","title":"Specificity","text":"<ul> <li> \\[Specificity = \\frac{TN}{TN+FP}\\] </li> </ul>"},{"location":"KB/Spectrogram/","title":"Spectrogram","text":""},{"location":"KB/Spectrogram/#spectrogram","title":"Spectrogram","text":""},{"location":"KB/Speculative%20Execution/","title":"Speculative Execution","text":""},{"location":"KB/Speculative%20Execution/#speculative-execution","title":"Speculative Execution","text":"<ul> <li>Allow the execution of complete instructions or parts of instructions before being sure whether this execution is required</li> </ul>"},{"location":"KB/Speculum/","title":"Speculum","text":""},{"location":"KB/Speculum/#speculum","title":"Speculum","text":"<ul> <li>An instrument used when examining body orifices to help widen the opening</li> </ul>"},{"location":"KB/Speech%20Emotion%20Recognition/","title":"Speech Emotion Recognition","text":""},{"location":"KB/Speech%20Emotion%20Recognition/#speech-emotion-recognition","title":"Speech Emotion Recognition","text":"<ul> <li>GAN-based Data Generation for Speech Emotion Recognition</li> <li>form of speech emotion spectrograms</li> <li>used for training speech emotion recognition networks</li> <li>nvestigate the usage of GANs for capturing the data manifold when the data is eyes-off, i.e., where they can train networks using the data but cannot copy it from the clients</li> <li>CNN-based GAN with spectral normalization on both the generator and discriminator, both of which are pre-trained on large unlabeled speech corpora</li> <li>even after the data on the client is lost, their model can generate similar data that can be used for model bootstrapping in the future</li> </ul>"},{"location":"KB/Speech%20Recognition/","title":"Speech Recognition","text":""},{"location":"KB/Speech%20Recognition/#speech-recognition","title":"Speech Recognition","text":"<ul> <li>Recurrent Neural Network Based Language Model<ul> <li>50% reduction of Perplexity</li> <li>mixture of several Basic RNN Architectures</li> <li>Wall Street Journal task</li> <li>connectionist language models are superior to standard n gram techniques, except their high computational (training) complexity</li> <li>break the myth that language modeling is just about counting n-grams, and that the only reasonable way how to improve results is by acquiring new training dat</li> </ul> </li> <li>Towards End-To-End Speech Recognition with Recurrent Neural Networks<ul> <li>character-level speech recognition system that directly transcribes audio data with text using a recurrent neural network</li> <li>combination of the deep bidirectional LSTM recurrent neural network architecture and a modified Connectionist Temporal Classification (CTC) objective function</li> <li>word error rate</li> <li>Wall Street Journal task</li> </ul> </li> </ul>"},{"location":"KB/Speech%20Resynthesis/","title":"Speech Resynthesis","text":""},{"location":"KB/Speech%20Resynthesis/#speech-resynthesis","title":"Speech Resynthesis","text":"<ul> <li>Speech Resynthesis from Discrete Disentangled Self-Supervised Representations</li> <li>self-supervised discrete representations for the task of speech resynthesis</li> <li>separately extract low-bitrate representations for speech content, prosodic information, and speaker identity</li> <li>This allows to synthesize speech in a controllable manner</li> <li>evaluate the F0 reconstruction, speaker identification performance (for both resynthesis and voice conversion), recordings\u2019 intelligibility, and overall quality using subjective human evaluation</li> <li>ultra-lightweight speech codec</li> </ul>"},{"location":"KB/Spiking%20Networks/","title":"Spiking Networks","text":""},{"location":"KB/Spiking%20Networks/#spiking-networks","title":"Spiking Networks","text":"<ul> <li>Poisson Process</li> </ul>"},{"location":"KB/Spirometer/","title":"Spirometer","text":""},{"location":"KB/Spirometer/#spirometer","title":"Spirometer","text":"<ul> <li>A device that measures the amount of air breathed in and out by the lungs</li> </ul>"},{"location":"KB/Spline%20Motion%20Type/","title":"Spline Motion Type","text":""},{"location":"KB/Spline%20Motion%20Type/#spline-motion-type","title":"Spline Motion Type","text":"<ul> <li>A calculated path that the robot executesthat may be parabolic in shape. A spline motion may also accomplish a free form curve with mixtures of circular and parabolic shapes.</li> </ul>"},{"location":"KB/Spline/","title":"Spline","text":""},{"location":"KB/Spline/#spline","title":"Spline","text":"<ul> <li>A smooth, continuous function used to approximate a set of functions that are uniquely defined on a set of sub-intervals. The approximating function and the set of functions being approximated intersect at a sufficient number of points to insure a high degree of accuracy in the approximation. The purpose for the smooth function is to allow a robot manipulator to complete a task without jerky motion.</li> </ul>"},{"location":"KB/Square%20Integrable/","title":"Square Integrable","text":""},{"location":"KB/Square%20Integrable/#square-integrable","title":"Square Integrable","text":"<ul> <li>Real valued RV with PDF p is square integrable if the Uncentered Second moment is finite</li> <li>\\(\\(E[X^{2}] = \\int_\\mathbb{R} x^{2}p(x)dx\\)\\) is finite</li> </ul>"},{"location":"KB/Squared%20Error/","title":"Squared Error","text":""},{"location":"KB/Squared%20Error/#squared-error","title":"Squared Error","text":"<ul> <li> \\[(y- f(x))^2\\] </li> <li>Regression</li> </ul>"},{"location":"KB/Squared%20Hinge/","title":"Squared Hinge","text":""},{"location":"KB/Squared%20Hinge/#squared-hinge","title":"Squared Hinge","text":"<ul> <li>Hinge Loss</li> <li>problems involving yes/no (binary) decisions and when you\u2019re not interested in knowing how certain the classifier is about the classification</li> <li>Tanh for last layer</li> <li>maximum margin</li> </ul> \\[\\mathrm{sum}\\left( \\left( \\mathrm{max}\\left( 0, 1 - y \\cdot \u0177 \\right) \\right)^{2} \\right)\\]"},{"location":"KB/Stable%20Difusion/","title":"Stable Difusion","text":""},{"location":"KB/Stable%20Difusion/#stable-difusion","title":"Stable Difusion","text":"<ul> <li>latent-difusion model - The main diference of this model with respect to the other ones is the use of a latent difusion model and that it performs image modification as it can perform operations in its latent space</li> <li>Stable Difusion consists of two parts: the text encoder and the image generator - The image information creator works completely in the latent space</li> <li>This property makes it faster than previous difusion models that worked in a pixel space</li> </ul>"},{"location":"KB/Stack%20GAN/","title":"Stack GAN","text":""},{"location":"KB/Stack%20GAN/#stack-gan","title":"Stack GAN","text":"<ul> <li> <p>Text to Image synthesis</p> </li> <li>StackGAN decomposes the hard problem into more manageable sub-problems through a sketch-refinement process.</li> <li>The Stage-I GAN sketches the primitive shape and colors of the object based on the given text description, yielding Stage-I low-resolution images.</li> <li>The Stage-II GAN takes Stage-I results and text descriptions as inputs and generates high-resolution images with photo-realistic details. It is able to rectify defects in Stage-I results and add compelling details with the refinement process</li> <li>Multi Modal. Large no of ims that fit the given text</li> </ul>"},{"location":"KB/Stack%20GAN/#todo","title":"todo","text":""},{"location":"KB/Stack%20GAN/#architecture","title":"Architecture","text":"<ul> <li> <p>256\u00d7256 photo-realistic images conditioned on text descriptions. sketch-refinement process.</p> </li> <li>Conditioning Augmentation technique that encourages smoothness in the latent conditioning manifold.</li> </ul>"},{"location":"KB/Stack%20GAN/#introduction","title":"Introduction","text":"<ul> <li>Generating photo-realistic images from text is an important problem and has tremendous applications, including photo-editing, computer-aided design, etc</li> <li>very difficult to train GAN to generate high-resolution photo-realistic images from text descriptions</li> <li>Simply adding more upsampling layers in state-ofthe-art GAN models for generating high-resolution (e.g., 256\u00d7256) images generally results in training instability</li> <li>supports of natural image distribution and implied model distribution may not overlap in high dimensional pixel space</li> <li>more severe as the image resolution increases In analogy to how human painters draw</li> <li>By conditioning on the Stage-I result and the text again, Stage-II GAN learns to capture the text information that is omitted by Stage-I GAN and draws more details for the object.</li> </ul>"},{"location":"KB/Stack%20GAN/#conditioning-augmentation","title":"Conditioning Augmentation","text":"<ul> <li>Conditioning Augmentation technique to produce additional conditioning variables c\u02c6</li> <li>we randomly sample the latent variables c\u02c6 from an independent Gaussian distribution \\(\\mathcal{N}(\\mu(\\varphi_{t}), \\Sigma(\\varphi_{t}))\\), where the mean \\(\\mu(\\varphi_{t})\\) and diagonal covariance matrix \\(\\Sigma(\\varphi_{t})\\) are functions of the text embedding \\(\\varphi_{t}\\)</li> <li>The proposed Conditioning Augmentation yields more training pairs given a small number of imagetext pairs, and thus encourages robustness to small perturbations along the conditioning manifold</li> <li>KB/Regularization.md term to the objective of the generator during training \\(\\(D_{KL}(\\mathcal{N}(\\mu(\\varphi_{t}), \\Sigma(\\varphi_{t})) || \\mathcal{N}(0,I))\\)\\)</li> <li>KL Divergence between the standard Gaussian distribution and the conditioning Gaussian distribution</li> <li>The randomness introduced in the Conditioning Augmentation is beneficial for modeling text to image translation as the same sentence usually corresponds to objects with various poses and appearances.</li> </ul>"},{"location":"KB/Stack%20GAN/#stage-i-gan","title":"Stage-I GAN","text":"<ul> <li>\\(\\varphi_{t}\\) be the text embedding of the given description</li> <li>The Gaussian conditioning variables \\(\\hat c_{0}\\) for text embedding are sampled from N(\u03bc0(\u03c6t),\u01a90(\u03c6t)) to capture the meaning of \\(\\varphi_{t}\\) with variations</li> <li>Conditioned on c\u02c60 and random variable z, Stage-I GAN trains the discriminator D0 and the generator G0 by alternatively maximizing \\(\\mathcal{L}_{D_{0}}\\) in Eq. (3) and minimizing \\(\\mathcal{L}_{G_{0}}\\)</li> <li> \\[\\mathcal{L}_{D_{0}} = \\mathbb{E}_{(I_{0},t)\\sim p_{data}}[log D_{0}(I_{0}, \\varphi_{t})]+\\mathbb{E}_{z \\sim p_{z}, t \\sim p_{data}}[log(1- D_{0}(G_{0}(z , \\hat{c_{0}}, \\varphi_{t})))]\\] </li> <li> \\[\\mathcal{L}_{G_{0}}= \\mathbb{E}_{z \\sim p_{z}, t \\sim p_{data}}[log(1- D_{0}(G_{0}(z, \\hat{c_{0}}),\\varphi_{t}))] + \\lambda D_{KL}(\\mathcal{N}(\\mu_{0}(\\varphi_{t}), \\Sigma_{0}(\\varphi_{t}))|| \\mathcal{N}(0, I))\\] </li> <li>where the real image I0 and the text description t are from the true data distribution pdata</li> <li>z is a noise vector randomly sampled from a given distribution pz (Gaussian distribution in this paper</li> <li>\u03bb is a KB/Regularization.md parameter that balances the two terms</li> <li>\u03bb = 1 for all the exps.</li> <li>both \\(\\mu_{0}(\\varphi_{t})\\) and \\(\\Sigma_{0}(\\varphi_{t})\\) are learned jointly with the rest of the network.</li> <li>For the generator G0, to obtain text conditioning variable c\u02c60, the text embedding \u03c6t is first fed into a fully connected layer to generate \u03bc0 and \u03c30 (\u03c30 are the values in the diagonal of \u01a90) for the Gaussian distribution N(\u03bc0(\u03c6t),\u01a90(\u03c6t)</li> <li>\u02c60 are then sampled from the Gaussian distribution c\u02c6 = \u03bc +\u03c3 \u2299\u03b5</li> <li>trained by alternatively maximizing LD in Eq. (5) and minimizing LG in Eq. (6),</li> <li>concatenated with a Nz dimensional noise vector to generate a W0 \u00d7 H0 image by a series of up-sampling blocks</li> <li>the text embedding \u03c6t is first compressed to Nd dimensions using a fully-connected layer</li> <li>and then spatially replicated to form a Md \u00d7 Md \u00d7 Nd tensor.</li> <li>the image is fed through a series of down-sampling blocks until it has Md \u00d7 Md spatial dimension</li> <li>Then, the image filter map is concatenated along the channel dimension with the text tensor</li> <li>The resulting tensor is further fed to a 1\u00d71 convolutional layer to jointly learn features across the image and the text.</li> <li>Finally, a fullyconnected layer with one node is used to produce the decision score.</li> </ul>"},{"location":"KB/Stack%20GAN/#stage-ii-gan","title":"Stage-II GAN","text":"<ul> <li>Low-resolution images generated by Stage-I GAN usually lack vivid object parts and might contain shape distortions.</li> <li>is conditioned on low-resolution images and also the text embedding again to correct defects in Stage-I results</li> <li>The Stage-II GAN completes previously ignored text information to generate more photo-realistic details.</li> <li>Conditioning on the low-resolution result s0 = G0(z, c\u02c60) and Gaussian latent variables c\u02c6</li> <li>Different from the original GAN formulation, the random noise z is not used in this stage with the assumption that the randomness has already been preserved by s0</li> <li>Gaussian conditioning variables c\u02c6 used in this stage and c\u02c60 used in Stage-I GAN share the same pre-trained text encoder, generating the same</li> <li></li> <li>text embedding \u03c6t.</li> <li>StageI and Stage-II Conditioning Augmentation have different fully connected layers for generating different means and standard deviations</li> <li>In this way, Stage-II GAN learns to capture useful information in the text embedding that is omitted by Stage-I GAN.</li> </ul>"},{"location":"KB/Stack%20GAN/#model-architecture","title":"Model Architecture.","text":"<ul> <li>Stage-II generator as an encoder-decoder network with residual blocks</li> <li>text embedding \u03c6t is used to generate the Ng dimensional text conditioning vector c\u02c6</li> <li>spatially replicated to form a Mg \u00d7Mg \u00d7Ng tensor</li> <li>Stage-I result s0 generated by Stage-I GAN is fed into several KB/Downsampling.md blocks (i.e., encoder) until it has a spatial size of Mg \u00d7 Mg</li> <li>The image features and the text features are concatenated along the channel dimension</li> <li>The encoded image features coupled with text features are fed into several residual blocks, which are designed to learn multi-modal representations across image and text feature</li> <li> <p>series of up-sampling layers</p> </li> <li> <p>are used to generate a W \u21e5H high-resolution</p> </li> <li>Such a generator is able to help rectify defects in the input image while add</li> <li>more details to generate the realistic high-resolution image.</li> <li>For the discriminator, its structure is similar to that of Stage-I discriminator with only extra down-sampling blocks since the image size is larger in this stage</li> <li>To explicitly enforce GAN to learn better alignment between the image and the conditioning text, rather than using the vanilla discriminator, we adopt the matching-aware discriminator</li> <li>During training, the discriminator takes real images and their corresponding text descriptions as positive sample pairs, whereas negative sample pairs consist of two groups</li> <li>Implementation details</li> <li>up-sampling blocks consist of the nearest-neighbor upsampling followed by a 3\u21e53 stride 1 convolution</li> <li></li> <li>Batch normalization [11] and ReLU activation are applied after every convolution except the last one</li> <li>The residual blocks consist of 3\u21e53 stride 1 convolutions, Batch normalization and ReLU. Two residual blocks are used in 128\u21e5128 StackGAN models while four are used in 256\u21e5256 models. The down-sampling blocks consist of 4\u21e54 stride 2 convolutions, Batch normalization and LeakyReLU, except that the first one does not have Batch normalization.</li> <li>Bydefault,Ng =128,Nz =100,Mg =16,Md =4, Nd = 128, W0 = H0 = 64 and W = H = 256</li> <li>For training, we first iteratively train D0 and G0 of Stage-I GAN for 600 epochs by fixing Stage-II GAN</li> <li>Then we iteratively train D and G of Stage-II GAN for another 600 epochs by fixing Stage-I GAN.</li> <li>All networks are trained using ADAM solver with batch size 64 and an initial learning rate of 0.0002. The learning rate is decayed to 1/2 of its previous value every 100 epochs.</li> </ul>"},{"location":"KB/Stack%20GAN/#datasets-and-evaluation-metrics-cub","title":"Datasets and evaluation metrics CUB","text":"<ul> <li>Oxford-102</li> <li>MS COCO</li> <li>Evaluation metrics</li> <li>inception score</li> <li>I = exp(ExDKL(p(y|x) || p(y))),</li> <li>where x denotes one generated sample, and y is the label predicted by the Inception model</li> <li>he intuition behind this metric is that good models should generate diverse but meaningful images.</li> <li>Therefore, the KL divergence between the marginal distribution p(y) and the conditional distribution p(y|x) should be larg</li> </ul>"},{"location":"KB/Stack%20GAN/#conclusions","title":"Conclusions","text":"<ul> <li>The proposed method decomposes the text-to-image synthesis to a novel sketch-refinement process.</li> <li></li> <li>Stage-I GAN sketches the object following basic color and shape constraints from given text descriptions. Stage-II GAN corrects the defects in Stage-I results and adds more details, yielding higher resolution images with better image quality</li> <li>Compared to existing text-to-image generative models, our method generates higher resolution images (e.g., 256\u21e5256) with more photo-realistic details and diversity. *</li> </ul>"},{"location":"KB/Stacking%20RNN/","title":"Stacking RNN","text":""},{"location":"KB/Stacking%20RNN/#stacking-rnn","title":"Stacking RNN","text":"<ul> <li>Deeper</li> <li>Each level -&gt; output is seq of features that is input at next set of Layers in the hierarchy</li> <li></li> </ul>"},{"location":"KB/Staged%20Training/","title":"Staged Training","text":""},{"location":"KB/Staged%20Training/#staged-training","title":"Staged Training","text":"<ul> <li>A tactic of training a model in a sequence of discrete stages. The goal can be either to speed up the training process, or to achieve better model quality.</li> </ul>"},{"location":"KB/Standard%20Deviation/","title":"Standard Deviation","text":"<p>toc: true title: Standard Deviation</p> <p>categories: ['temp']</p>"},{"location":"KB/Standard%20Deviation/#standard-deviation","title":"Standard Deviation","text":"<ul> <li>measure of dispersement</li> <li>how much your data is spread out around the mean</li> <li>Normal Distribution</li> <li> \\[std = \\sqrt{\\frac{\\Sigma(x-\\bar x)^{2}}{n-1}}\\] </li> </ul>"},{"location":"KB/Stanford%20Dogs/","title":"Stanford Dogs","text":""},{"location":"KB/Stanford%20Dogs/#stanford-dogs","title":"Stanford Dogs","text":"<ul> <li>@khoslaNovelDatasetFineGrained</li> <li>This dataset contains images of 120 breeds of dogs, with a total of 20,580 images.</li> </ul>"},{"location":"KB/StarGAN%20v2/","title":"StarGAN v2","text":""},{"location":"KB/StarGAN%20v2/#stargan-v2","title":"StarGAN V2","text":"<ul> <li>generate diverse images across multiple domains </li> <li>define the domain and style of images as visually distinct category groups and the specific appearance of each image, respectively</li> </ul>"},{"location":"KB/StarGAN/","title":"StarGAN","text":""},{"location":"KB/StarGAN/#stargan","title":"StarGAN","text":"<ul> <li>improve the scalability and robustness in handling more than two domains </li> <li>builds only one model to perform image-to-image translation among multiply domains </li> <li>In the generation phase, we just need to provide the generator with the source image and an attribute label which indicates the target domain </li> <li>takes the domain label as an additional input and learns a deterministic mapping per each domain, which may result in the same output per each domain given an input image</li> </ul>"},{"location":"KB/Static%20Friction/","title":"Static Friction","text":""},{"location":"KB/Static%20Friction/#static-friction","title":"Static Friction","text":"<ul> <li> \\[F_{s} \\leq \\mu _{s}F_{N}\\] </li> <li>\\(F_{s}\\) is static friction</li> <li>\\(F_{N}\\) is normal force</li> <li>\\(\\mu _s\\) is coefficient of friction</li> </ul>"},{"location":"KB/Static%20Graph%20Execution/","title":"Static Graph Execution","text":""},{"location":"KB/Static%20Graph%20Execution/#static-graph-execution","title":"Static Graph Execution","text":"<ul> <li>Computational Graph is first built then is executed</li> <li>less readable code and more difficult to debug</li> <li>better performance</li> <li>once compiled the graph architecture is static</li> </ul>"},{"location":"KB/Stationarity/","title":"Stationarity","text":""},{"location":"KB/Stationarity/#stationarity","title":"Stationarity","text":"<ul> <li>A property of data in a dataset, in which the data distribution stays constant across one or more dimensions. Most commonly, that dimension is time, meaning that data exhibiting stationarity doesn't change over time. For example, data that exhibits stationarity doesn't change from September to December.</li> </ul>"},{"location":"KB/Statistical%20Word%20Segmentation/","title":"Statistical Word Segmentation","text":""},{"location":"KB/Statistical%20Word%20Segmentation/#statistical-word-segmentation","title":"Statistical Word Segmentation","text":"<ul> <li>use mutual information between characters from a corpus</li> </ul>"},{"location":"KB/Stem%20Cells/","title":"Stem Cells","text":""},{"location":"KB/Stem%20Cells/#stem-cells","title":"Stem Cells","text":"<ul> <li>Cells that have the potential to differentiate, to develop into many different specific cell types with specialized functions.</li> </ul>"},{"location":"KB/Steve/","title":"Steve","text":""},{"location":"KB/Steve/#steve","title":"Steve","text":"<ul> <li>Outer loop: Steve is a tutoring system that teaches hierarchical, multi-step procedures, such as how to start a large air compressor</li> <li>take Inner loop: Students steps by manipulating graphical widgets, such as clicking on a valve icon to open it or on a dipstick to check the oil level</li> <li>Steve can give immediate feedback</li> <li>Steve can also execute a step for the student. In fact, it can demonstrate the whole procedure for a student, explaining each step as it goes.</li> <li>Step analysis: Steve interprets the student's step by matching it to a set of anticipated steps. In particular, after each student step, Steve computes all possible correct next steps (usually there is just one).</li> <li>Notice that there is a one-to-one relationship between the step, the Learning Event and the Knowledge Component. This is a major contributor to the simplicity of Steve's analysis.</li> <li>Step generation: The student's most recent correct step is, by definition, part of the procedure being taught</li> <li>Steve uses immediate feedback to block incorrect steps, so Steve always knows where in the procedure the student is.</li> <li>When it needs to give hints on what to do next, it merely picks the next step. If there are multiple steps that could follow the student's most recent correct step, then Steve lists them and lets the student choose.</li> </ul>"},{"location":"KB/Stochastic%20ensemble%20learning/","title":"Stoch Ensemble Learning","text":""},{"location":"KB/Stochastic%20ensemble%20learning/#stoch-ensemble-learning","title":"Stoch Ensemble Learning","text":"<ul> <li>Stochastic algo repeatedly executed with random seeds</li> <li>Stronger the randomness -&gt; more members included -&gt; stronger reg</li> </ul>"},{"location":"KB/Stochastic%20ensemble%20learning/#maybe-related-to","title":"Maybe related to","text":"<ul> <li>Ensemble Distillation</li> <li>Ensemble of Shape Functions</li> </ul>"},{"location":"KB/Stratified%20Random%20Sampling/","title":"Stratified Random Sampling","text":""},{"location":"KB/Stratified%20Random%20Sampling/#stratified-random-sampling","title":"Stratified Random Sampling","text":"<ul> <li>Divides the population into relatively homogeneous groups (strata) and samples each stratum at random</li> </ul>"},{"location":"KB/Stream%20Ribbons/","title":"Stream Ribbons","text":""},{"location":"KB/Stream%20Ribbons/#stream-ribbons","title":"Stream Ribbons","text":""},{"location":"KB/Stream%20Surfaces/","title":"Stream Surfaces","text":""},{"location":"KB/Stream%20Surfaces/#stream-surfaces","title":"Stream Surfaces","text":""},{"location":"KB/Streamline%20Stopping%20Criterion/","title":"Streamline Stopping Criterion","text":""},{"location":"KB/Streamline%20Stopping%20Criterion/#streamline-stopping-criterion","title":"Streamline Stopping Criterion","text":"<ul> <li>distance to neighboring streamline too small</li> <li>streamline leaves domain  </li> <li>after maximum number of integration steps</li> </ul>"},{"location":"KB/Streamlines/","title":"Streamlines","text":""},{"location":"KB/Streamlines/#streamlines","title":"Streamlines","text":"<ul> <li>Streamline Stopping Criterion</li> </ul>"},{"location":"KB/Striatum/","title":"Striatum","text":""},{"location":"KB/Striatum/#striatum","title":"Striatum","text":"<ul> <li>A small group of subcortical structures, including the caudate nucleus, putamen, and nucleus accumbens, located in the midbrain. These regions are implicated in both movement and reward-related behaviors</li> </ul>"},{"location":"KB/Strided%20Attention/","title":"Strided Attention","text":""},{"location":"KB/Strided%20Attention/#strided-attention","title":"Strided Attention","text":"<ul> <li>paper</li> <li>Sparse factorizations of the attention matrix</li> <li>Reduce to \\(O(n\\sqrt{n})\\)</li> <li>Recompute attention matrices to save memory</li> <li>Fast attention kernels</li> <li>Works nicely for images, music etc with a periodic structure</li> <li>Otherwise with the strided pattern , the spatial coordinates do not correlate with the positions the elements might be more relevant in the future</li> <li></li> </ul>"},{"location":"KB/Strided/","title":"Strided","text":""},{"location":"KB/Strided/#strided","title":"Strided","text":"<ul> <li>Normally S = 1</li> <li>S&gt;1 -&gt; Downsampling</li> <li>Dilated</li> <li>Spaces in the filter kernel</li> <li>D = 1 : normal Conv aka D-1 spaces</li> <li>Effective Filter size : \\(\\(\\hat F = F + (F-1)(D-1)\\)\\)</li> <li></li> </ul>"},{"location":"KB/Strip%20Mining/","title":"Strip Mining","text":""},{"location":"KB/Strip%20Mining/#strip-mining","title":"Strip Mining","text":"<ul> <li>Generates code to allow vector operands whose size is less than or greater than size of vector registers</li> </ul>"},{"location":"KB/Stroke/","title":"Stroke","text":""},{"location":"KB/Stroke/#stroke","title":"Stroke","text":"<ul> <li>A neurological event that occurs when the blood supply to the brain is blocked, depriving the tissue of oxygen, or when there is a bleed into the brain due to the rupturing of an artery.</li> </ul>"},{"location":"KB/Stroop%20Task/","title":"Stroop Task","text":"<p>toc: true title: Stroop Task</p> <p>categories: ['temp']</p>"},{"location":"KB/Stroop%20Task/#stroop-task","title":"Stroop Task","text":"<ul> <li>Verbal<ul> <li>The Stroop phenomenon demonstrates that it is difficult to name the ink color of a color word if there is a mismatch between ink color and word. For example, the word GREEN printed in red ink</li> </ul> </li> <li>Non Verbal<ul> <li>A series of white arrows pointing either left or right was displayed against a black background either on the left or right side of a centered fixation cross. Half of the stimuli were pointing in the same direction as their position on the screen</li> </ul> </li> </ul>"},{"location":"KB/Structural%20Risk%20Minimization/","title":"Structural Risk Minimization","text":""},{"location":"KB/Structural%20Risk%20Minimization/#structural-risk-minimization","title":"Structural Risk Minimization","text":"<ul> <li>An algorithm that balances two goals<ul> <li>The desire to build the most predictive model (for example, lowest loss).</li> <li>The desire to keep the model as simple as possible (for example, strong KB/Regularization.md).</li> <li>or example, a function that minimizes loss+KB/Regularization.md on the training set is a structural risk minimization algorithm.</li> </ul> </li> </ul>"},{"location":"KB/Structural%20Similarity%20Index/","title":"Structural Similarity Index","text":""},{"location":"KB/Structural%20Similarity%20Index/#structural-similarity-index","title":"Structural Similarity Index","text":"<ul> <li>method for predicting the perceived quality of digital television and cinematic pictures, as well as other kinds of digital images and videos. SSIM is used for measuring the similarity between two images</li> <li> \\[\\hbox{SSIM}(x,y) = \\frac{(2\\mu_x\\mu_y + c_1)(2\\sigma_{xy} + c_2)}{(\\mu_x^2 + \\mu_y^2 + c_1)(\\sigma_x^2 + \\sigma_y^2 + c_2)}\\] </li> <li>\\(\\mu_x\\) the pixel sample mean of \\(x\\);</li> <li>\\(\\mu_y\\) the pixel sample mean of \\(y\\);</li> <li>\\(\\sigma_x^2\\) the variance of \\(x\\);</li> <li>\\(\\sigma_y^2\\) the variance of \\(y\\);</li> <li>\\(\\sigma_{xy}\\) the KB/Covariance.md of \\(x\\) and \\(y\\);</li> <li>\\(c_1 = (k_1L)^2\\), \\(c_2 = (k_2L)^2\\) two variables to stabilize the division with weak denominator;</li> <li>\\(L\\) the dynamic range of the pixel-values (typically this is \\(2^{\\#bits\\ per\\ pixel}-1\\));</li> <li>\\(k_1 = 0.01\\) and \\(k_2 = 0.03\\) by default.</li> </ul>"},{"location":"KB/Structure%20Based%20Pruning/","title":"Structure Based Pruning","text":""},{"location":"KB/Structure%20Based%20Pruning/#structure-based-pruning","title":"Structure Based Pruning","text":"<ul> <li>Regarding structural choices, some authors choose to prune individual parameters which produces a sparse network (lots of 0s). This might not be very ideal for storing efficiently.</li> <li>Some others consider methods where they group certain parameters and remove them as groups. This is more optimized.</li> </ul>"},{"location":"KB/Structured%20Update/","title":"Structured Update","text":""},{"location":"KB/Structured%20Update/#structured-update","title":"Structured Update","text":"<ul> <li>Directly learn an update from a restricted space that can be parametrized using a smaller number of variables.</li> <li>We train directly the updates of this structure</li> <li>Random mask. We restrict the update Hit to be a sparse matrix, following a pre-defined random KB/Sparsity.md pattern</li> </ul>"},{"location":"KB/Style%20GAN/","title":"Style GAN","text":""},{"location":"KB/Style%20GAN/#style-gan","title":"Style GAN","text":"<ul> <li>builds the picture layer after layer, where the layers get bigger and more accurate</li> <li>For example, the first layer is 4 by 4 pixels, the second 8 by 8, and so on</li> <li>every new layer can benefit from the less granular results of the previous ones</li> <li>better separate the generator and the discriminator, which ensures less dependence of the generator on the training set</li> <li>his allows one to, for example, reduce discrimination in the generated pictures</li> </ul>"},{"location":"KB/Subgenual%20Cortex/","title":"Subgenual Cortex","text":""},{"location":"KB/Subgenual%20Cortex/#subgenual-cortex","title":"Subgenual Cortex","text":"<ul> <li>The region in the back of the frontal lobes, found below the corpus callosum, which has been implicated in mood states.</li> </ul>"},{"location":"KB/Subject%20relative/","title":"Subject relative","text":""},{"location":"KB/Subject%20relative/#subject-relative","title":"Subject Relative","text":"<ul> <li>The judge that ignored the doctor watched the movie about Colombian drug dealers</li> <li>Object relative The judge that the doctor ignored watched the movie about Colombian drug dealers.</li> <li>Subjects with higher working memory were significantly better at interpreting object relatives than subjects with lower working memory</li> </ul>"},{"location":"KB/Subject-verb%20agreement/","title":"Subject-verb agreement","text":""},{"location":"KB/Subject-verb%20agreement/#subject-verb-agreement","title":"Subject-verb Agreement","text":"<ul> <li>John almost always reads the course papers before the lecture. I almost always forget to buy cat food.</li> <li>Relations with inflectional morphemes</li> <li>La femme est belle. L'homme est beau.</li> </ul>"},{"location":"KB/Substantia%20Nigra/","title":"Substantia Nigra","text":""},{"location":"KB/Substantia%20Nigra/#substantia-nigra","title":"Substantia Nigra","text":"<ul> <li>This small region in the midbrain is part of the brain\u2019s reward system. In Parkinson\u2019s disease, the dopamine neurons in this region die off, leading to the disorder\u2019s movement-related and cognitive symptoms.</li> </ul>"},{"location":"KB/Subthalamic%20Nucleus/","title":"Subthalamic Nucleus","text":""},{"location":"KB/Subthalamic%20Nucleus/#subthalamic-nucleus","title":"Subthalamic Nucleus","text":"<ul> <li>A small brain structure, located in the basal ganglia, that plays an important role in coordinating movement. It is the most common target for neuromodulation techniques, like KB/Deep Brain Stimulation.md, to help diminish the symptoms of Parkinson\u2019s disease.</li> </ul>"},{"location":"KB/Sufficiency/","title":"Sufficiency","text":""},{"location":"KB/Sufficiency/#sufficiency","title":"Sufficiency","text":"<ul> <li>the Positive Predictive Value is the same for all subgroups within the sensitive feature. This criteria is also known as Predictive Rate Parity.</li> </ul>"},{"location":"KB/Suffix/","title":"Suffix","text":""},{"location":"KB/Suffix/#suffix","title":"Suffix","text":"<ul> <li>follow the stem: eat / eats</li> </ul>"},{"location":"KB/Sugar%20Factory%20Task/","title":"Sugar Factory Task","text":""},{"location":"KB/Sugar%20Factory%20Task/#sugar-factory-task","title":"Sugar Factory Task","text":"<ul> <li>Berry and Broadbent ; Wallach</li> <li>Set no of workers per day</li> <li> \\[P_{t}=2W_{t}-P_{t-1}+ RandomFactor(-1/0/1)\\] <ul> <li>P: Production value</li> <li>W : No of workers</li> </ul> </li> </ul>"},{"location":"KB/Sulcus/","title":"Sulcus","text":""},{"location":"KB/Sulcus/#sulcus","title":"Sulcus","text":"<ul> <li>A shallower groove on the brain\u2019s cerebrum (deeper grooves are called fissures).</li> </ul>"},{"location":"KB/Summit/","title":"Summit","text":""},{"location":"KB/Summit/#summit","title":"Summit","text":"<ul> <li>@hohmanSummitScalingDeep2019</li> <li>combines two scalable tools: (1) activation aggregation discovers important neurons; (2) neuron-influenced aggregation identifies relationships among such neurons. An attribution graph that reveals and summarizes crucial neuron associations and substructures that contribute to a model's outcomes is created. Summit combines famous methods such as computing synthetic prototypes of features and showing examples from the dataset that maximize special neurons of different layers. Deeper in the graph, it is examined how the low-level features combine to create high-level features. Novel as well is that it exploits neural networks with activation atlases [63]</li> <li>This method uses feature inversion to visualize millions of activations from an image classification network to create an explorable activation atlas of features the network has learned. Their approach is able to reveal visual abstractions within a model and even high-level misunderstandings in a model that can be exploited. Activation atlases are a novel way to peer into convolutional vision networks and represents a global, hierarchical, and human-interpretable overview of concepts within the hidden layers.</li> <li>To quantify how much a layer influences the next, the authors aggregate the influences by creating a tensor \\(I^{l}\\) for all the layers of the network (\\(l\\)). How important channel \\(i\\) of the layer \\(l-1\\) is determined by the aggregate tensor \\(I^{l}_{cij}\\) where \\(j\\) represents the output channel and \\(c\\) is the class of the image. Considering the \\(j^{th}\\) kernel of the layer \\(K^{(j)} \\in \\mathbb{R}^{H \\times W \\times C_{l-1}}\\), a single channel \\(Y\\) can be represented using the 3D convolution operation by \\(Y_{:,:,j}= X \\ast K^{(j)}\\). This is equivalent to it's representation by the 2D convolution \\(Y_{:,:,j}= \\Sigma_{i=1}^{C_{l-1}} X_{:,:,i} \\ast K^{(j)}_{:,:,i}\\). </li> <li>\\(X_{:,:,i} \\ast K^{(j)}_{:,:,i}\\) is the contribution of the current channel from the previous layer and the maximum of this value is used to generate the influence map.</li> </ul>"},{"location":"KB/Super%20Resolution/","title":"Super Resolution","text":""},{"location":"KB/Super%20Resolution/#super-resolution","title":"Super Resolution","text":"<pre><code>def SuperResolutionNetNew(upscale_factor):\n    return nn.Sequential(\n        nn.Conv2d(1, 64, kernel_size=5, padding=2),\n        nn.ReLU(inplace=True),\n        nn.Conv2d(64, 64, kernel_size=3, padding=1),\n        nn.ReLU(inplace=True),\n        nn.Conv2d(64, 32, kernel_size=3, padding=1),\n        nn.ReLU(inplace=True),\n        nn.Conv2d(32, upscale_factor ** 2, kernel_size=3, padding=1),\n        Rearrange('b (h2 w2) h w -&gt; b (h h2) (w w2)', h2=upscale_factor, w2=upscale_factor),\n    )\n</code></pre>"},{"location":"KB/SuperQuadrics/","title":"SuperQuadrics","text":""},{"location":"KB/SuperQuadrics/#superquadrics","title":"SuperQuadrics","text":""},{"location":"KB/SuperScalar/","title":"SuperScalar","text":""},{"location":"KB/SuperScalar/#superscalar","title":"SuperScalar","text":"<ul> <li>A superscalar CPU architecture implements Instruction level programming inside a single processor which allows faster CPU throughput at the same clock rate.</li> <li>A superscalar processor executes more than one instruction during a clock cycle</li> <li>Simultaneously dispatches multiple instructions to multiple redundant functional units built inside the processor.</li> </ul>"},{"location":"KB/Superposition%20Catastrophe/","title":"Superposition Catastrophe","text":""},{"location":"KB/Superposition%20Catastrophe/#superposition-catastrophe","title":"Superposition Catastrophe","text":"<ul> <li>Bowers et al. (2014)</li> <li>Common claim of connnectionist models: they learn the best representations for a given task</li> <li>Learned representations are emergent, not stipulated</li> <li>If a PDP model learns localist codes when coding for multiple things at the same time, strongly suggests that the superposition problem pressures models to learn selective (e.g. localist) coding.</li> <li>Recurrent network</li> <li>Simple task, given vocabulary of 30 words</li> <li>Banding/selective responses do not appear with distributed letter coding when chance of ambiguity is null</li> <li>This means: when ambiguity/superposition catastrophe is very possible, hidden units learn selective responses</li> <li>Selective responses 'emerge' as a response to the potential for superposition catastrophe</li> <li>Recurrent networks trained to store multiple things at the same time over the same set of units learn highly selective (localist) representations</li> </ul>"},{"location":"KB/Supervised%20Learning%20Formulation/","title":"Supervised Learning Formulation","text":""},{"location":"KB/Supervised%20Learning%20Formulation/#supervised-learning-formulation","title":"Supervised Learning Formulation","text":"<ul> <li>given a dataset X, for each data Xi in X, there is a corresponding human-annotated label Yi </li> <li>For a set of N labeled training data D = {Xi}Ni=0, the training loss function is defined as </li> <li>data collection and annotation usually are expensive and may require special skills</li> </ul>"},{"location":"KB/Suppletion/","title":"Suppletion","text":""},{"location":"KB/Suppletion/#suppletion","title":"Suppletion","text":"<ul> <li>word is completely replaced by something that has no connection at the surface label</li> <li>Go to went.</li> <li>Good to better</li> </ul>"},{"location":"KB/Swichboard/","title":"Swichboard","text":""},{"location":"KB/Swichboard/#swichboard","title":"Swichboard","text":""},{"location":"KB/Swin%20Transformer/","title":"Swin Transformer","text":"<p>toc: true title: Swin Transformer</p> <p>categories: ['temp']</p>"},{"location":"KB/Swin%20Transformer/#swin-transformer","title":"Swin Transformer","text":"<ul> <li>Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows<ul> <li>Vision Transformer</li> <li>general-purpose backbone for computer vision</li> <li>hierarchical feature representation</li> <li>linear computational complexity with respect to input image size</li> <li>shifted window based Self Attention</li> <li>address the challenges in adapting Transformer from language to vision</li> <li>limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection</li> <li>flexibility to model at various scales</li> <li>linear computational complexity with respect to image size</li> <li>ImageNet</li> <li>COCO</li> <li>ADE20K</li> <li>The hierarchical design and the shifted window approach also prove beneficial for all Perception Architectures.</li> <li>Ratio of 1:1:3:1</li> </ul> </li> </ul>"},{"location":"KB/Swish/","title":"Swish","text":""},{"location":"KB/Swish/#swish","title":"Swish","text":"<ul> <li> \\[x\\cdot sigmoid(x)\\] </li> <li>While Swish reportedly improves model performance (Ramachandran et al., 2017), it still does not allow you to avoid Vanishingexploding gradients</li> <li>Even though the vanishing gradients problem is much less severe in case of Swish, only inputs of \\(x &gt;= 2\\) result in gradients of 1 and (sometimes) higher. In any other case, the gradient will still cause the chain to get smaller with increasing layers.</li> <li>Move to Lisht</li> <li>non monotonic</li> <li>First, it is bounded below. Swish therefore benefits from KB/Sparsity.md similar to ReLU. Very negative weights are simply zeroed out.</li> <li>Second, it is unbounded above. This means that for very large values, the outputs do not saturate to the maximum value (i.e., to 1 for all the neurons). According to the authors of the Swish paper, this is what set ReLU apart from the more traditional activation functions.</li> <li>Third, separating Swish from ReLU, the fact that it is a smooth curve means that its output landscape will be smooth. This provides benefits when optimizing the model in terms of convergence towards the minimum loss.</li> <li>Fourth, small negative values are zeroed out in ReLU (since f(x) = 0 for x &lt; 0). However, those negative values may still be relevant for capturing patterns underlying the data, whereas large negative values may be zeroed out (for reasons of KB/Sparsity.md, as we saw above). The smoothness property and the values of f(x) &lt; 0 for x \u2248 0 yield this benefit. This is a clear win over ReLU.</li> <li></li> </ul>"},{"location":"KB/Symbolic%20learning%20model/","title":"Symbolic learning model","text":""},{"location":"KB/Symbolic%20learning%20model/#symbolic-learning-model","title":"Symbolic Learning Model","text":"<ul> <li>Verb tokens are used instead of verb types.</li> <li>No sharp discontinuities in the supply of regular and irregular verb tokens in 'parental speech'</li> <li>Verb tokens sampled randomly with replacement according to the Francis-Kucera frequency estimates for English verbs</li> </ul>"},{"location":"KB/Symbolic%20models/","title":"Symbolic models","text":""},{"location":"KB/Symbolic%20models/#symbolic-models","title":"Symbolic Models","text":"<ul> <li>Code letters separately from order</li> <li>This type of model does make transposition errors \u25ee Can because B is a 'thing' on its own, separate from order</li> <li>Botvinick, M. M., &amp; Plaut, D. C. (2006). Short-term memory for serial order: a recurrent neural network model. Psychological review, 113(2), 201.</li> <li>Model could later correctly reproduce novel sequences</li> <li>But all letters had occurred in each position</li> </ul>"},{"location":"KB/Symmetries%20Node%20Link/","title":"Symmetries Node Link","text":""},{"location":"KB/Symmetries%20Node%20Link/#symmetries-node-link","title":"Symmetries Node Link","text":""},{"location":"KB/Sympathetic/","title":"Sympathetic","text":""},{"location":"KB/Sympathetic/#sympathetic","title":"Sympathetic","text":"<ul> <li>Mobilizes body into action</li> </ul>"},{"location":"KB/Synaptic%20Pruning/","title":"Synaptic Pruning","text":""},{"location":"KB/Synaptic%20Pruning/#synaptic-pruning","title":"Synaptic Pruning","text":"<ul> <li>A process by which specialized cells called microglia eliminate unnecessary synapses as part of normal and healthy brain development.</li> </ul>"},{"location":"KB/Synaptic%20Transmission/","title":"Synaptic Transmission","text":""},{"location":"KB/Synaptic%20Transmission/#synaptic-transmission","title":"Synaptic Transmission","text":"<ul> <li>The process of nerve-to-nerve communication in the central nervous system, whereby one neuron sends a chemical signal across the synaptic cleft to another neuron.</li> </ul>"},{"location":"KB/Syntactic%20Ambiguity/","title":"Syntactic Ambiguity","text":""},{"location":"KB/Syntactic%20Ambiguity/#syntactic-ambiguity","title":"Syntactic Ambiguity","text":"<ul> <li>Flying plane is very dangerous</li> <li>The man saw a boy with binoculars</li> <li>The teacher wears sunglasses</li> <li>Becase students are bright</li> </ul>"},{"location":"KB/Syntactic%20Analysis/","title":"Syntactic Analysis","text":""},{"location":"KB/Syntactic%20Analysis/#syntactic-analysis","title":"Syntactic Analysis","text":"<ul> <li>concerned with the construction of sentences.</li> <li>Syntactic structure indicates how the words are related to each other</li> <li>Syntax tree is assigned by a grammer and a Lexicon</li> <li>Context Free Grammar</li> </ul>"},{"location":"KB/Syntactic%20Bootstrapping/","title":"Syntactic Bootstrapping","text":""},{"location":"KB/Syntactic%20Bootstrapping/#syntactic-bootstrapping","title":"Syntactic Bootstrapping","text":"<ul> <li>\"John wugged Mary yesterday\" vs \"John wugged Marry\"</li> </ul>"},{"location":"KB/Syntax%20First%20models/","title":"Syntax First models","text":""},{"location":"KB/Syntax%20First%20models/#syntax-first-models","title":"Syntax First Models","text":"<ul> <li>Syntax-first models (e.g., Ferreira &amp; Clifton, 1986; Frazier &amp; Clifton, 1996) have traditionally proposed that, at a point of syntactic ambiguity, syntactic heuristics alone select a single structure to pursue</li> <li>recovery from a misanalysis is achieved via a separate reanalysis mechanism that uses semantic and contextual information</li> <li>propose that only one representation is active at any given time and that nonsyntactic information only influences interpretation at a later reanalysis stage.</li> </ul>"},{"location":"KB/S%C3%B8rensen-Dice%20Index/","title":"S\u00f8rensen-Dice Index","text":""},{"location":"KB/S%C3%B8rensen-Dice%20Index/#srensen-dice-index","title":"S\u00f8rensen-Dice Index","text":"<ul> <li>very similar to Jaccard index</li> <li>Although they are calculated similarly the S\u00f8rensen-Dice index is a bit more intuitive because it can be seen as the percentage of overlap between two sets, which is a value between 0 and 1</li> <li>overstate the importance of sets with little to no ground truth positive sets</li> <li>As a result, it could dominate the average score taken over multiple sets</li> <li>It weights each item inversely proportionally to the size of the relevant set rather than treating them equally.</li> <li>$\\(D(x,y) = \\frac{2|x\\cap y|}{|x|+|y|}\\)</li> <li></li> </ul>"},{"location":"KB/T.%20B.%20Macaulay/","title":"T. B. Macaulay","text":"","tags":["indianhistory"]},{"location":"KB/T.%20B.%20Macaulay/#t-b-macaulay","title":"T. B. Macaulay","text":"<ul> <li>Macaulay came from a deeply religious Protestant family, so his motivation was to convert numerous Hindus to Christianity, which he thought would also help the administrative problems that the English were facing</li> <li>His plan was to create an educational system that would make an educated elite that would naturally be English by their own choice, and thus give up their own Hindu traditions</li> <li>Then they would also work and cooperate more efficiently with the English administration.</li> <li>facilitate the British colonialism by making Indians, especially the Brahmanas, collaborators loyal to their new masters</li> <li>\"Our English schools are flourishing wonderfully. The effect of this education on the Hindus is prodigious... It is my belief that if our plans of education are followed up, there will not be a single idolater among the respectable classes in Bengal thirty years hence. And this will be effected without any efforts to proselytize, without the smallest interference with religious liberty, by natural operation of knowledge and reflection. I heartily rejoice in the project.\"</li> <li>\"It is, I believe, no exaggeration to say, that all the historical information which has been collected from all the books written in Sanskrit language is less valuable than what may be found in the most paltry abridgements used at preparatory schools in England. In every branch of physical or moral philosophy, the relative position of the two nations is nearly the same.\"</li> <li>\"In one point I fully agree with the gentlemen to whose general views I am opposed. I feel with them, that it is impossible for us, with our limited means, to attempt to educate the body of the people. We must at present do our best to form a class who may be interpreters between us and the millions whom we govern; a class of persons, Indian in blood and colour, but English in taste, in opinions, in morals, and in intellect. To that class we may leave it to refine the vernacular dialects of the country, to enrich those dialects with terms of science borrowed from the Western nomenclature, and to render them by degrees fit vehicles for conveying knowledge to the great mass of the population.\"</li> <li>This is in reference that Macaulay figured that the British would not be able to educate all Indians in the ways of the British, but could indeed create a class of them who would be English in taste, opinions, morals and intellect, and then engage them in influencing the rest of India, thus helping the English in their job of overseeing the rest of India, and, of course, converting them.</li> <li>\"Through the whole Hindoo Pantheon you will look in vain for anything resembling those beautiful and majestic forms which stood in the shrines of ancient Greece. All is hideous, and grotesque, and ignoble. As this superstition is of all superstitions the most inelegant, so is it of all superstitions the most immoral. Emblems of vice are objects of public worship. Acts of vice are acts of public worship. The courtesans are as much a part of the establishment of the temple, as much ministers of the god, as the priests. Crimes against life, crimes against property, are not only permitted but enjoined by this odious theology.\"</li> </ul>","tags":["indianhistory"]},{"location":"KB/TIghtly%20coupled/","title":"TIghtly coupled","text":""},{"location":"KB/TIghtly%20coupled/#tightly-coupled","title":"TIghtly Coupled","text":"<ul> <li>UMA</li> <li>NUMA</li> </ul>"},{"location":"KB/TIme%20Series/","title":"Time Series Prediction","text":""},{"location":"KB/TIme%20Series/#time-series-prediction","title":"Time Series Prediction","text":"<ul> <li>Task:<ul> <li>discrete time, equidistant timesteps</li> <li>Approximate teacher signal</li> <li>Input signal : \\(\\(u(t)_{t \\in \\mathcal{T}}\\)\\)</li> <li>Desired output : \\(\\((y(t))_{t \\in \\mathcal{T}} = u(t+h))_{t \\in \\mathcal{T}}\\)\\)</li> <li>Dynamical system state : \\(\\(x(t) \\in \\mathbb{R}^n\\)\\)</li> <li>Temporal evolution governed by<ul> <li>State update map<ul> <li>Governs how \\(\\(x(t)\\)\\) develops over time</li> <li> \\[x(t+1) = f(x(t) , u(t+1))\\] </li> </ul> </li> <li>Observation function<ul> <li>What output can be observed when in state \\(\\(x(t)\\)\\)</li> <li> \\[y(t) = g(x(t))\\] </li> </ul> </li> </ul> </li> </ul> </li> <li>Short range<ul> <li>PDE , ODE</li> </ul> </li> <li>Long range<ul> <li>HMM</li> <li>Recurrent</li> </ul> </li> </ul>"},{"location":"KB/TO%20LOOK%20AT/","title":"To Look at","text":""},{"location":"KB/TO%20LOOK%20AT/#to-look-at","title":"To Look at","text":"<ul> <li>http://karpathy.github.io/2019/04/25/recipe/</li> <li>https://arxiv.org/abs/1311.2901</li> <li>http://torch.ch/blog/2015/09/07/spatial_transformers.html</li> <li>http://dpmd.ai/Ithaca-blog</li> <li>https://notesonai.com/Layer+Normalization</li> </ul>"},{"location":"KB/TPU%20Node/","title":"TPU Node","text":""},{"location":"KB/TPU%20Node/#tpu-node","title":"TPU Node","text":"<ul> <li>A TPU resource on Google Cloud Platform with a specific TPU type.</li> </ul>"},{"location":"KB/TPU%20Pod/","title":"TPU Pod","text":""},{"location":"KB/TPU%20Pod/#tpu-pod","title":"TPU Pod","text":"<ul> <li>A specific configuration of TPU devices in a Google data center.</li> </ul>"},{"location":"KB/TPU%20Slice/","title":"TPU Slice","text":""},{"location":"KB/TPU%20Slice/#tpu-slice","title":"TPU Slice","text":"<ul> <li>A TPU slice is a fractional portion of the TPU devices in a TPU Pod</li> </ul>"},{"location":"KB/TREEQN/","title":"TREEQN","text":""},{"location":"KB/TREEQN/#treeqn","title":"TREEQN","text":"<pre><code>def transition(zl):\n  # -- [batch_size x num_actions x hidden_dimension]\n  return zl.unsqueeze(1) + F.tanh(torch.einsum(\"bk,aki-&gt;bai\", [zl, W]) + b)\n</code></pre>"},{"location":"KB/TREPAN/","title":"TREPAN","text":""},{"location":"KB/TREPAN/#trepan","title":"TREPAN","text":"<ul> <li>or DeepRED</li> <li>TREPAN is an algorithm for extracting comprehensible, symbolic representations from trained neural networks</li> <li>The authors demonstrated that TREPAN is able to produce Decision Trees that are accurate and comprehensible and maintain a high level of fidelity to the networks from which they were extracted.</li> <li>According to the authors of DeepRED, their method is the first attempt to extract rules and make a DNN's decision more transparent.</li> </ul>"},{"location":"KB/TSDF/","title":"TSDF","text":""},{"location":"KB/TSDF/#tsdf","title":"TSDF","text":"<ul> <li>Truncated Signed Distance Function</li> </ul>"},{"location":"KB/Tacotron/","title":"Tacotron","text":""},{"location":"KB/Tacotron/#tacotron","title":"Tacotron","text":"<ul> <li>CBHG<ul> <li>[Conv](Gated Recurrent Unit (GRU|Gated Recurrent Unit (GRU|Conv](Conv](Gated Recurrent Unit (GRU|Gated Recurrent Unit (GRU|Conv.md).md)</li> </ul> </li> </ul> <pre><code>class CBHG_Old(nn.Module):\n    \"\"\"CBHG module: a [recurrent](./Recurrent.md) neural network composed of:\n        - 1-d convolution banks\n        - Highway networks + residual connections\n        - Bidirectional gated [recurrent](./Recurrent.md) units\n    \"\"\"\n\n    def __init__(self, in_dim, K=16, projections=[128, 128]):\n        super(CBHG, self).__init__()\n        self.in_dim = in_dim\n        self.relu = nn.ReLU()\n        self.conv1d_banks = nn.ModuleList(\n            [BatchNormConv1d(in_dim, in_dim, kernel_size=k, stride=1,\n                             padding=k // 2, activation=self.relu)\n             for k in range(1, K + 1)])\n        self.max_pool1d = nn.MaxPool1d(kernel_size=2, stride=1, padding=1)\n\n        in_sizes = [K * in_dim] + projections[:-1]\n        activations = [self.relu] * (len(projections) - 1) + [None]\n        self.conv1d_projections = nn.ModuleList(\n            [BatchNormConv1d(in_size, out_size, kernel_size=3, stride=1,\n                             padding=1, activation=ac)\n             for (in_size, out_size, ac) in zip(\n                 in_sizes, projections, activations)])\n\n        self.pre_highway = nn.Linear(projections[-1], in_dim, bias=False)\n        self.highways = nn.ModuleList(\n            [Highway(in_dim, in_dim) for _ in range(4)])\n\n        self.gru = nn.GRU(\n            in_dim, in_dim, 1, batch_first=True, bidirectional=True)\n\ndef forward_new(self, inputs, input_lengths=None):\n    x = rearrange(inputs, 'b t c -&gt; b c t')\n    _, _, T = x.shape\n    # Concat conv1d bank outputs\n    x = rearrange([conv1d(x)[:, :, :T] for conv1d in self.conv1d_banks], \n                 'bank b c t -&gt; b (bank c) t', c=self.in_dim)\n    x = self.max_pool1d(x)[:, :, :T]\n\n    for conv1d in self.conv1d_projections:\n        x = conv1d(x)\n    x = rearrange(x, 'b c t -&gt; b t c')\n    if x.size(-1) != self.in_dim:\n        x = self.pre_highway(x)\n\n    # Residual connection\n    x += inputs\n    for highway in self.highways:\n        x = highway(x)\n\n    # (B, T_in, in_dim*2)\n    outputs, _ = self.gru(self.highways(x))\n\n    return outputs\n</code></pre>"},{"location":"KB/Tainted%20data/","title":"Tainted data","text":""},{"location":"KB/Tainted%20data/#tainted-data","title":"Tainted data","text":"<ul> <li>errors in the data modelling definition, wrong feature labelling, and other possible causes.</li> </ul>"},{"location":"KB/Taking%20on%20semantic%20commitments%2C%20II%20collective%20versus%20distributive%20readings/","title":"Taking on semantic commitments, II collective versus distributive readings","text":""},{"location":"KB/Taking%20on%20semantic%20commitments%2C%20II%20collective%20versus%20distributive%20readings/#taking-on-semantic-commitments-ii-collective-versus-distributive-readings","title":"Taking on Semantic Commitments, II Collective Versus Distributive Readings","text":"<ul> <li>Lyn Frazier, Jeremy M. Pacht, Keith Raynerb</li> </ul> <ul> <li>Minimal Semantic Commitment</li> <li>Given ambiguous representations, the MSC hypothesis predicts that the processor will commit to one interpretation</li> <li>In an experiment designed to evaluate these hypotheses with respect to the representation of distributivity, participants' eye movements were recorded as they read sentences containing distributive or collective predicates that were either disambiguated by a preceding adverb or left locally ambiguous by delaying the disambiguating adverb until the end of the predicate</li> <li>The results suggested that a semantic commitment is made in locally indeterminate cases as evidenced by a significant interaction of ambiguity and distributivity in first pass times, total times, and regressions</li> <li>Hence we argue that the distributive/collective distinction is treated as a matter of ambiguity rather than as one of vagueness</li> <li>In the absence of evidence for a distributive reading, the processor commits itself to a collective reading sometime during the processing of the predicate (before the disambiguation in our late disambiguation examples)</li> <li>By the MSC hypothesis, this will predict that a premature decision, one made early on the basis of little information, should not occur during immediate processing</li> <li>The point is that no commitment will be made in the absence of supporting evidence</li> <li>By contrast, given the MSC hypothesis, the notion that a string is grammatically ambiguous predicts that the processor encounters a choice point on initial analysis, adopting one representation rather than another</li> <li>In our experiment we only examined collective/distributive subjects in single clause sentences</li> <li>The ambiguity hypothesis predicts an interaction</li> <li>the vagueness hypothesis doesn't</li> <li>T</li> <li>In order to test for possible pragmatic biases in the predicate, participants were asked to rate the 'naturalness' of the locally ambiguous collective or distributive predicates contained in the experimental sentences</li> <li>when the actual conjoined NP subject in the experimental materials was replaced by a pronominal subject</li> <li>Thirty-two students native English speakers</li> <li>Each version of the questionnaire contained eight sentences in which the locally ambiguous predicate was subsequently disambiguated towards a distributive interpretation using each and eight sentences in which the predicate was subsequently disambiguated towards a collective interpretation using together</li> <li>7-point scale</li> <li>They were told that there were no right or wrong answers and that we were only interested in their opinions of the naturalness of the sentences</li> <li>another rating study was conducted using truncated versions of the locally ambiguous items such that each item ended after the word each or together</li> <li>examined first pass reading times, total reading times, and the pattern of regressions for different regions of the target sentences</li> <li>Sixty undergraduate students bite bar</li> </ul>"},{"location":"KB/Taking%20on%20semantic%20commitments%2C%20II%20collective%20versus%20distributive%20readings/#experiment","title":"Experiment","text":"<ul> <li>dealt with where readers look during reading and that they should read each sentence for comprehension</li> <li>Sixteen experimental sentences were embedded in 107 filler sentences</li> <li>Each sentence appeared in one of four versions, as in (7) above: in two versions (a and c), the predicate received a distributive interpretation, and in the other two (b and d) the predicate received a collective interpretation</li> <li>region</li> <li>he first region consisted of the words preceding the predicate (a conjoined NP)</li> <li>the second region consisted of the predicate itself</li> <li>the third region comprised the next three words, or next two words if the third word was the last word in the sentence</li> <li>and the fourth region was the remainder of the sentence</li> <li>The fourth region was created solely so that results from the third region would be free of sentence wrapup effects</li> <li>In the first region, none of the effects were significant</li> <li>In the second region, there was an effect of ambiguity, wherein the locally ambiguous versions were read faster than unambiguous versions, possibly due to a preference for adverbs to appear in verb-phrase final position</li> <li>he raw reading times analyses and analyses of residuals were consistent in</li> <li>indicating that the interaction did not approach significance</li> <li>In the third region, the different versions of any given sentence were once again identical</li> <li>Here, locally ambiguous versions were read somewhat slower overall than unambiguous versions</li> <li>However, this difference failed to approach significance in the subjects analysis (F1(1,59) = 1</li> <li>91, P \u0000 0</li> <li>16) and was only marginally significant by items</li> <li>Crucially, each of these marginally significant main effects was qualified by a highly robust interaction between ambiguity and predicate type, suggesting that distributive predicates were read more slowly than collective predicates in the locally ambiguous versions</li> <li>Pairwise comparisons confirmed that while distributives were read more slowly in the ambiguous versions (F1(1,59) = 10</li> <li>84, P \u0000 0</li> <li>01</li> <li>F2(1,14) =20</li> <li>48, P \u0000 0</li> <li>001), there was no reliable predicate effect in the unambiguous versions</li> <li>The results of this analysis for first pass times revealed striking differences between ambiguous and unambiguous forms</li> <li>In the residuals analyses, distributives (each) were read slower than collectives (together) in both ambiguous and unambiguous forms</li> <li>However, the effect was much larger in ambiguous forms than unambiguous forms (223 ms vs</li> <li>43 ms), and while in ambiguous forms the effect was significan</li> <li>Distributives were significantly faster than collectives in unambiguous forms (means: 844 ms vs</li> <li>925 ms</li> <li>F1(1,59) = 4</li> <li>72, P \u0000 0</li> <li>05, F2 (1,14) = 10</li> <li>39, P \u0000 0</li> <li>01), but significantly slower than collectives in ambiguous forms</li> <li>n the third region, where the different versions were again identical, there was a main effect of ambiguity, with ambiguous versions being read slower overall than unambiguous versions</li> <li>There was also a robust predicate effect (F1(1,59) = 9</li> <li>53, P \u0000 0</li> <li>01</li> <li>F2(1,14) = 10</li> <li>88, P \u0000 0</li> <li>01), which indicated that distributives were read more slowly overall</li> <li>In the second region, there were no significant differences across conditions in the percentages of trials on which regressions occurred out of the region (Fs \u0000 1)</li> <li>In the third region, the mean percentage of trials on which regressions occurred out of the region was 19%, 8%, 4% and 4% for the ambiguous-distributive, ambiguouscollective, unambiguous-distributive, and unambiguous-collective conditions, respectively</li> <li>There were significantly more trials involving regressions out of the third region in the ambiguous than the unambiguous versions</li> <li>The predictions of the vagueness hypothesis were clearly disconfirmed</li> <li>Given the MSC hypothesis, the vagueness hypothesis predicts no interaction between ambiguity and sentence form: in both the ambiguous distributive (7a) and the unambiguous distributive (7c) the processor should postulate a distributive operator when each is encountered</li> <li>ounter to this prediction, the ambiguous distributive form was substantially more difficult to understand than the other sentence forms</li> <li>This may be seen in the significant interaction of ambiguity and sentence form in first pass and total times in region three, as well as in the regressions out of region three and regressions into region two</li> <li>Readers clearly exhibited a preference for the collective reading of the ambiguous portion of the sentences in our experiment</li> <li>These results make it difficult to maintain the assumptions needed to salvage the vagueness hypothesis</li> <li>Instead, given the MSC hypothesis, they support the assumption that the correct grammatical account of collective/distributive differences treats the distinction as one of ambiguity rather than as one of vagueness, at least in cases like those tested, where subject-predicate relations are involved</li> <li>We turn now to alternative interpretations of our results</li> <li>The question is whether the results can be attributed directly to the complexity of the distributive reading</li> <li>We think not</li> <li>It may be true that distributive readings, even unambiguous ones, are slightly more complex than collective readings</li> <li>This suggests that readers may not simply add information to the current representation of these locally ambiguous forms when each is encountered</li> <li>Similarly the results are difficult to reconcile with a parallel processing view unless the processor has computed both a collective and a distributive representation and then abandoned the distributive representation before each is encountered</li> </ul>"},{"location":"KB/Tanh/","title":"Tanh","text":""},{"location":"KB/Tanh/#tanh","title":"Tanh","text":"<ul> <li> \\[\\frac{e^x-e^{-x}}{e^x+e^{-x}}\\] </li> <li>RNN : Hidden</li> <li>Xavier/Glorot init</li> <li></li> </ul>"},{"location":"KB/Task%20%28endeffector%29%20Space%20Vs%20Joint%20Space/","title":"Task (endeffector) Space Vs. Joint Space","text":""},{"location":"KB/Task%20%28endeffector%29%20Space%20Vs%20Joint%20Space/#task-endeffector-space-vs-joint-space","title":"Task (endeffector) Space Vs. Joint Space","text":""},{"location":"KB/Tau%20Protein/","title":"Tau Protein","text":""},{"location":"KB/Tau%20Protein/#tau-protein","title":"Tau Protein","text":"<ul> <li>A type of protein abundantly found in neurons. When this protein is not adequately cleared from the brain, it can form tangles that are a key pathology of several neurodegenerative disorders including frontotemporal degeneration, CTE, and Alzheimer\u2019s disease.</li> </ul>"},{"location":"KB/Teach%20Lock/","title":"Teach Lock","text":""},{"location":"KB/Teach%20Lock/#teach-lock","title":"Teach Lock","text":"<ul> <li>While the Teach Lock is set, the mode of operation is tied to the Teach Mode and the machines cannot be played back using either START to \u201cTEACH\u201d before beginning to teach.</li> </ul>"},{"location":"KB/Teacher%20Forcing/","title":"Teacher Forcing","text":""},{"location":"KB/Teacher%20Forcing/#teacher-forcing","title":"Teacher Forcing","text":"<ul> <li>from</li> <li>Technique where the target word (ground truth word) is passed as the next input to the decoder instead of its last prediction.</li> <li>common technique to train\u00a0Basic RNN Architectures\u00a0or\u00a0Transformer<ul> <li>used in imageCaptioning\u00a0, Machine Translation</li> <li>but also in Time Series forecasting</li> </ul> </li> <li>intuition<ul> <li>math exam with dependent questions, e.g. a) depends on b), b) on c) and so on</li> <li>if a) is wrong, all subsequent questions are also wrong</li> <li>teacher forcing: after answering question a), the teacher compares it to the correct solution and grades it and then gives us the correct answer for a) to continue with</li> </ul> </li> <li>for example in sequence generation with\u00a0RNN\u00a0the situation is similar<ul> <li>each prediction depends on the last one, thus when one is wrong all subsequent will be wrong as well</li> </ul> </li> <li>no memorization can happen<ul> <li>the network can not look into the future</li> <li>ground truth is only fed as last \\(y_{t-1}\\) prediction not as the current \\(y_{t}\\)</li> </ul> </li> <li>loss does not need to be updated at each timestep, only needs to have a list with the true predictions of the model from which then the loss is calculated</li> <li>pros<ul> <li>training converges faster, because early predictions are very bad</li> </ul> </li> <li>cons<ul> <li>no ground truth label during inference, thus no teacher forcing</li> <li>discrepancy between training and inference scores<ul> <li>can lead to poor model performance and instability</li> <li>known as\u00a0Exposure Bias</li> </ul> </li> </ul> </li> </ul>"},{"location":"KB/Teacher%20Student%20Architecture/","title":"Teacher Student Architecture","text":""},{"location":"KB/Teacher%20Student%20Architecture/#teacher-student-architecture","title":"Teacher Student Architecture","text":"<ul> <li>The gap is further reduced by residual learning, i.e., the assistant structure is used to learn the residual error (Gao et al., 2021).</li> </ul>"},{"location":"KB/Telomere/","title":"Telomere","text":""},{"location":"KB/Telomere/#telomere","title":"Telomere","text":"<ul> <li>The protective cap found at the end of a chromosome. Research studies suggest these caps may be shortened in neurodegenerative diseases.</li> </ul>"},{"location":"KB/Temporal%20Context%20Structure/","title":"Temporal Context Structure","text":""},{"location":"KB/Temporal%20Context%20Structure/#temporal-context-structure","title":"Temporal Context Structure","text":"<ul> <li>The temporal order from videos is used as supervision signal </li> <li>verify whether the input frame sequence in correct order </li> <li>recognize the order of the frame sequence</li> </ul>"},{"location":"KB/Temporal%20Conv/","title":"Temporal Conv","text":""},{"location":"KB/Temporal%20Conv/#temporal-conv","title":"Temporal Conv","text":"<ul> <li>FCN + Causal 1D Conv + Residual</li> <li>Outperforms Basic RNN Architectures such as Long Short Term Memory (LSTM)).md) and Gated Recurrent Unit (GRU)).md)</li> </ul>"},{"location":"KB/Temporal%20lobe/","title":"Temporal lobe","text":""},{"location":"KB/Temporal%20lobe/#temporal-lobe","title":"Temporal Lobe","text":"<ul> <li>Understanding language (Wernicke Area)</li> <li>Memory</li> <li>Hearing</li> <li>Sequencing and organization</li> <li>Long-term memory is processed in the hippocampus of the temporal lobe and is activated when you want to memorize something for a longer time.</li> </ul>"},{"location":"KB/Temporal%20order%20recognition/","title":"Temporal order recognition","text":""},{"location":"KB/Temporal%20order%20recognition/#temporal-order-recognition","title":"Temporal order recognition","text":"<ul> <li>recognize the order of a sequence of input frames.</li> </ul>"},{"location":"KB/Temporal%20order%20verification/","title":"Temporal order verification","text":""},{"location":"KB/Temporal%20order%20verification/#temporal-order-verification","title":"Temporal order verification","text":"<ul> <li>verify whether a sequence of input frames is in correct temporal order</li> </ul>"},{"location":"KB/TemporalLearning/","title":"Temporal Learning","text":""},{"location":"KB/TemporalLearning/#temporal-learning","title":"Temporal Learning","text":"<ul> <li>Recurrent</li> <li>Online Learning</li> <li>Causal Systems</li> </ul>"},{"location":"KB/Tensor%20Processing%20Unit/","title":"Tensor Processing Unit","text":""},{"location":"KB/Tensor%20Processing%20Unit/#tensor-processing-unit","title":"Tensor Processing Unit","text":"<ul> <li>An application-specific integrated circuit (ASIC) that optimizes the performance of machine learning workloads.</li> </ul>"},{"location":"KB/Test-time%20Augmentation/","title":"Test-time Augmentation","text":""},{"location":"KB/Test-time%20Augmentation/#test-time-augmentation","title":"Test-time Augmentation","text":"<ul> <li>augmenting data at test-time as well</li> <li>This can be seen as analogous to ensemble learning techniques in the data space.</li> <li>By taking a test image and augmenting it in the same way as the training images, a more robust prediction can be derived.</li> <li>restrict the speed of the model</li> <li>promising practice for applications such as medical image diagnosis</li> <li>Radosavovic et al. denote test-time augmentation as data distillation to describe the use of ensembled predictions to get a better representation of the image.</li> <li>They also found better uncertainty estimation when using test-time augmentation, reducing highly confident but incorrect predictions.</li> <li>Matsunaga et al. also demonstrate the effectiveness of test-time augmentation on skin lesion classification, using KB/Geometric Transformations.md such as rotation, translation, scaling, and Flipping.</li> <li>A robust classifier is thus defined as having a low variance in predictions across augmentations</li> <li>In their experiments searching for augmentations with Reinforcement Learning, Minh et al. measure robustness by distorting test images with a 50% probability and contrasting the accuracy on un-augmented data with the augmented data.</li> <li>Some classification models lie on the fence in terms of their necessity for speed. This suggests promise in developing methods that incrementally upgrade the confidence of prediction. This could be done by first outputting a prediction with little or no testtime augmentation and then incrementally adding test-time augmentations to increase the confidence of the prediction.</li> <li>However, it is difficult to aggregate predictions on geometrically transformed images in object detection and semantic segmentation. Curriculum learning</li> <li>strategy for selecting training data that beats random selection</li> <li>best to initially train with the original data only and then finish training with the original and augmented data, although there is no clear consensus</li> </ul>"},{"location":"KB/Text%20Normalization/","title":"Text Normalization","text":""},{"location":"KB/Text%20Normalization/#text-normalization","title":"Text Normalization","text":"<ul> <li>Merging di\ufb00erent written forms of a token into a canonical normalized form</li> </ul>"},{"location":"KB/Text%20Preprocessing/","title":"Text Preprocessing","text":""},{"location":"KB/Text%20Preprocessing/#text-preprocessing","title":"Text Preprocessing","text":"<ul> <li>Document Triage</li> <li>Text Segmentation</li> </ul>"},{"location":"KB/Text%20Segmentation/","title":"Text Segmentation","text":""},{"location":"KB/Text%20Segmentation/#text-segmentation","title":"Text Segmentation","text":"<ul> <li>Word Segmentation</li> <li>Text Normalization</li> <li>Sentence Segmentation</li> <li>Character-set dependence</li> <li>Language dependence</li> <li>Corpus dependence</li> <li>Application dependence</li> </ul>"},{"location":"KB/Textbooks%20are%20all%20you%20need/","title":"Textbooks are all you need","text":""},{"location":"KB/Textbooks%20are%20all%20you%20need/#textbooks-are-all-you-need","title":"Textbooks Are All You Need","text":"<ul> <li>Summary : This paper proposes a new code assisting model that uses Python text books to train as textbooks have more curated knowledge. There are quite a few limitations such as not learning, maths and being specific to the type of textbooks used. </li> </ul>"},{"location":"KB/Textbooks%20are%20all%20you%20need/#abstract","title":"Abstract","text":"<ul> <li>phi-1 </li> <li>new large language model for code, </li> <li>significantly smaller size than competing models: </li> <li>Transformer-based model with 1.3B parameters, trained for 4 days on 8 A100s, using a selection of \"textbook quality\" data from the web (6B tokens) </li> <li>synthetically generated textbooks and exercises with GPT-3.5 </li> <li>surprising emergent properties compared to phi-1-base, our model before our finetuning stage on a dataset of coding exercises, and phi-1-small, a smaller model with 350M parameters trained with the same pipeline as phi-1 that still achieves 45% on HumanEval.  </li> <li> </li> </ul>"},{"location":"KB/Textbooks%20are%20all%20you%20need/#training-details-and-the-importance-of-high-quality-data","title":"Training Details and the Importance of High-quality Data","text":"<ul> <li>training data. Unli+ke previous work that used standard sources of text data for code generation, such as The Stack [KLA 22] (which contains sourcecode from repositor+ies with permissive licenses) and other not optimal for teaching the model how to reason and plan algorithmically. </li> <li>The standard code datasets [KLA+22, LCC+22] form a large and diverse corpus covering broad range of topics and use cases. </li> <li>sufer from several drawbacks: </li> <li>samples are not self-contained, meaning that they depend on other modules or files that are external to the snippet, making them hard to understand without additional context. </li> <li>Typical examples do not involve any meaningful computation, but rather consist of trivial or boilerplate code, such as defining constants, setting parameters, or configuring GUI element </li> <li>Samples that do contain algorithmic logic are often buried inside complex or poorly documented functions, making them dicult to follow or learn from. </li> <li>examples are skewed towards certain topics or use cases, resulting in an unbalanced distribution of coding concepts and skills across the dataset. </li> <li>language models would benefit from a training set that has the same qualities as what a human would perceive as a good \"textbook\": it should be clear, self-contained, instructive, and balanced. </li> <li>three main datasets: </li> <li>filtered code-language dataset </li> <li>subset of The Stack and StackOverflow, obtained by using a language model-based classifier (consisting of about 6B tokens). </li> <li>synthetic textbook dataset </li> <li>&lt;1B tokens of GPT-3.5 generated Python textbooks. </li> <li>small synthetic exercises dataset </li> <li>\u223c180M tokens of Python exercises and solutions. </li> <li>s of Python exercises and solutions. contain less than 7B tokens </li> <li>CodeTextbook </li> </ul>"},{"location":"KB/Textbooks%20are%20all%20you%20need/#the-central-ingredient-our-model-relies-on-textbook-quality-training-data","title":"the central ingredient our model relies on textbook-quality training data.","text":""},{"location":"KB/Textbooks%20are%20all%20you%20need/#filtering-of-existing-code-datasets-using-a-transformer-based-classifier","title":"Filtering of Existing Code Datasets Using a Transformer-based Classifier","text":"<ul> <li>Python subset of the deduplicated version of The Stack and the StackOverflow, which together contain over 35 million files/samples, totalling over 35B tokens. </li> <li>annotate the quality of a small subset of these files (about 100k samples) using GPT- 4: given a code snippet, the model is prompted to \"determine its educational value for a student whose goal is to learn basic coding concepts\". </li> <li>use this annotated dataset to train a random forest classifier that predicts the quality of a file/sample using its output embedding from a pretrained codegen model as features. </li> <li>unlike GPT-3.5, which we use extensively to generate synthetic content (discussed below), we use GPT-4 minimally only for annotations on the quality of a small subset of The Stack and StackOverflow  </li> <li> </li> </ul>"},{"location":"KB/Textbooks%20are%20all%20you%20need/#creation-of-synthetic-textbook-quality-datasets","title":"Creation of Synthetic Textbook-quality Datasets","text":"<ul> <li>main challenges in creating a high-quality dataset for code generation is ensuring that the examples are diverse and non-repetitive. </li> <li>the examples should cover a wide range of coding concepts, skills, and scenarios, and that they should vary in their level of diculty, complexity, and style. </li> <li>exposes the language model to diferent ways of expressing and solving problems in code, it reduces the risk of overfitting or memorizing specific patterns or solutions, and it increases the generalization and robustness of the model to unseen or novel tasks </li> <li>not trivial, especially when using synthetic data generated by another language model. </li> <li>Simply prompting the model to produce a coding textbook or a set of exercises, even with some variation in the instructions or the parameters, will likely result in a very homogeneous and redundant dataset, where the same concepts and solutions are repeated over and over with minor changes. </li> <li>language models tend to follow the most probable or common paths given their training data and their priors, and they lack the creativity or the incentive to explore alternative or novel ways of generating code. </li> <li>one needs to find the right \"trick\" </li> <li>The synthetic textbook dataset </li> <li>diversity is obtained by providing constraints on topics and target audience of the generated textbook  </li> <li> </li> <li>The CodeExercises dataset </li> <li>conduct explicit decontamination and alternative evaluations in the following sections to ensure that problems similar to those from HumanEval benchmark are not seen during finetuning.  </li> <li> </li> </ul>"},{"location":"KB/Textbooks%20are%20all%20you%20need/#model-architecture-and-training","title":"Model Architecture and Training","text":"<ul> <li>some recent models like CodeGen [NPH+22], PaLM [CND+22], and GPT-NeoX [BBH+22]. The architecture for our 1.3B parameter phi-1 model consists of 24 layers, hidden dimension of 2048, MLP-inner dimension of 8192, and 32 attention heads of dimension 64 each. The smaller 350M parameter phi- 1-small model consists of 20 layers, hidden dimension of 1024, MLP-inner dimension+of 4096, and 16 attention heads of dimension 64 each. We also use a rotary position+embedding [SLP 21] with rotary </li> <li>could further boost performance and eciency [LAZ 23]. </li> <li>For both pretraining and finetuning, we concatenate our respective datasets into a single dimensional </li> <li>array with </li> <li>array with \"\u0000\u0000endoftext\u0000\u0000\" token used for separating the files. We train our models on sequence length of optimizer, linear-warmup-linear-decay learning rate schedule, and attention and residual dropout of 0.1. </li> <li>Pretraining </li> <li>phi-1-base was trained on the CodeTextbook dataset </li> <li>phi-1 is obtained by finetuning phi-1-base on the CodeExercises dataset. </li> </ul>"},{"location":"KB/Textbooks%20are%20all%20you%20need/#spikes-of-model-capability-after-finetuning-on-codeexercises","title":"Spikes of Model Capability after Finetuning on CodeExercises","text":"<ul> <li>the model after finetuning also exhibits a substantial improvement in executing tasks that are not featured in the finetuning dataset. </li> <li>This includes managing intricate algorithmic tasks and using external libraries. </li> <li>suggests that our finetuning process might have helped the model in reorganizing and consolidating the knowledge acquired during pretraining, even if such knowledge is not explicitly present in our CodeExercises dataset </li> <li>Finetuning improves the model's understanding </li> <li>Finetuning improves the model's ability to use external libraries </li> </ul>"},{"location":"KB/Textbooks%20are%20all%20you%20need/#evaluation-on-unconventional-problems-with-llm-grading","title":"Evaluation on Unconventional Problems with LLM Grading","text":"<ul> <li>A potential concern with the surprisingly good performance of phi-1 on HumanEval (see Table 1 and Figure 2.1) is that there might be memorization stemming from contamination of the CodeExercises dataset1 </li> <li>To minimize bias and leakage, the new evaluation problems were created by a dedicated team in our group that did not access the CodeExercises dataset or the final model. </li> <li>They created 50 new problems in the same format as HumanEval with instructions to design problems that are unlikely to appear in real-world code bases or as coding exercises. </li> <li>To evaluate candidate solutions, we therefore adopt the approach of using GPT-4 to grade the solution </li> <li>This approach has two distinct advantages: (1) by using GPT-4 as a grader, we can leverage its knowledge and generative abilities to obtain a more fine-grained and meaningful signal of the student model's coding capabilities, and (2) it obviates the need for tests </li> <li>Our prompt instructs the LLM to evaluate a student's solution first in a short verbal evaluation followed by grades from 0 to 10 </li> </ul>"},{"location":"KB/Textbooks%20are%20all%20you%20need/#data-pruning-for-unbiased-performance-evaluation","title":"Data Pruning for Unbiased Performance Evaluation","text":"<ul> <li>training on CodeExercises leads to a substantial boost in the performance of the model on the HumanEval benchmark </li> <li>To investigate this boost, we propose to prune the CodeExercises dataset by removing files that are \"similar\" to those in HumanEval. </li> <li>\"strong form\" of data decontamination. </li> <li>retrain our model on such pruned data, and still observe strong performance on HumanEval. </li> <li>such data pruning experiment is a fair way to evaluate performance </li> </ul>"},{"location":"KB/Textbooks%20are%20all%20you%20need/#n-gram-overlap","title":"N-gram Overlap","text":"<ul> <li>N-gram measures the similarity of text segments based on the shared n-word sequences. </li> <li>n-gram overlap between the docstrings of each humaneval question and each exercise in the CodeExercises dataset that was generated </li> <li>found 4 humaneval questions with 13-gram overlap with at least one of the entries in our dataset </li> <li>all the 4 overlap cases in the 13-gram are all false positives such as the example # below. </li> <li>Our n-gram overlap analysis shows that our dataset has minimal letter-by-letter overlap with HumanEval. </li> </ul>"},{"location":"KB/Textbooks%20are%20all%20you%20need/#embedding-and-syntax-based-similarity-analysis","title":"Embedding and Syntax-based Similarity Analysis","text":"<ul> <li>now turn to the pruning experiments </li> <li>n-gram analysis is not refined enough to find similar code snippets between HumanEval and CodeExercises </li> <li>combination of embedding and syntax-based distances. </li> <li>For the embedding distance we compute the L2 distance </li> <li>between the embedding of+the code snippets where the embedding is derived from a pre-trained CodeGenMono 350M model [NPH 23]. </li> <li>For the syntax-based distance we calculate the (string) edit distance between the abstract syntax trees (ASTs) of two given code snippets.  </li> <li> </li> </ul>"},{"location":"KB/Textbooks%20are%20all%20you%20need/#conclusion","title":"Conclusion","text":"<ul> <li>Just as a comprehensive, well-crafted textbook can provide a student with the necessary knowledge to master a new subject, our work demonstrates the remarkable impact of high-quality data in honing a language model's proficiency in code- generation tasks. </li> <li>By crafting \"textbook quality\" data we were able to train a model that surpasses almost all open-source models on coding benchmarks such as HumanEval and MBPP despite being 10x smaller in model size and 100x smaller in dataset size. </li> <li>phi-1 is specialized in Python coding </li> <li>phi-1 lacks the domain-specific knowledge of larger models such as programming with specific APIs or using less common packages. </li> <li>due to the structured nature of the datasets and the lack of diversity in terms of language and style, phi-1 is less robust to stylistic variations or errors in the </li> </ul>"},{"location":"KB/Textbooks%20are%20all%20you%20need/#limitation-of-phi-1","title":"Limitation of Phi-1","text":"<ul> <li>Our model is sensitive to various perturbations of prompts. </li> <li>First, its performance drops significantly as the length of the prompt increases, as it tends to ignore, forget or misinterpret parts of the prompt when it is too long </li> <li>phi-1 demonstrates less robustness in handling natural language compared to ChatGPT or StarCoder, particularly with ambiguous prompts. </li> <li>This may be because we filter out certain types of data from the training process to guarantee textbook-level quality. </li> <li>A primary constraint of our model, particularly when contrasted with alternatives like StarCoder, lies in its performance on tasks involving counting and spatial reasoning </li> </ul>"},{"location":"KB/Textless%20Speech%20Emotion%20Conversion/","title":"Textless Speech Emotion Conversion","text":""},{"location":"KB/Textless%20Speech%20Emotion%20Conversion/#textless-speech-emotion-conversion","title":"Textless Speech Emotion Conversion","text":"<ul> <li>Textless Speech Emotion Conversion Using Discrete and Decomposed Representations</li> <li>Speech emotion conversion</li> <li>modifying the perceived emotion of a speech utterance while preserving the lexical content and speaker identity</li> <li>spoken language translation task</li> <li>decomposition of the speech signal into discrete learned representations, consisting of phonetic-content units, prosodic features, speaker, and emotion</li> <li>modify the speech content by translating the phonetic-content units to a target emotion, and then predict the prosodic features based on these units</li> <li>speech waveform is generated by feeding the predicted representations into a neural vocoder</li> <li>beyond spectral and parametric changes of the signal</li> <li>model non-verbal vocalizations, such as laughter insertion, yawning removal, etc</li> </ul>"},{"location":"KB/Thalamus/","title":"Thalamus","text":""},{"location":"KB/Thalamus/#thalamus","title":"Thalamus","text":"<ul> <li>Serves as a relay station for almost all information that comes and goes to the cortex.</li> <li>It plays a role in pain sensation, attention, alertness and memory.</li> </ul>"},{"location":"KB/The%20Behavior%20of%20Tutoring%20Systems/","title":"The Behavior of Tutoring Systems","text":""},{"location":"KB/The%20Behavior%20of%20Tutoring%20Systems/#the-behavior-of-tutoring-systems","title":"The Behavior of Tutoring Systems","text":"<ul> <li>Kurt VanLehn</li> </ul>"},{"location":"KB/The%20Behavior%20of%20Tutoring%20Systems/#intro","title":"Intro","text":"<ul> <li>Tutoring systems are described as having two loops.</li> <li>The outer loop executes once for each task, where a task usually consists of solving a complex, multi-step problem</li> <li>The inner loop executes once for each step taken by the student in the solution of a task. The inner loop can give feedback and hints on each step. The inner loop can also assess the student's evolving competence and update a student model, which is used by the outer loop to select a next task that is appropriate for the student.</li> <li>A task usually takes several minutes to an hour or so. Most tutors assume that the student is working alone on a task, but some (e.g., Zachary et al., 1999) have the student work as one member of a multi-student team.</li> <li>The tasks and the tutor's user interface are usually designed so that completing a task requires multiple steps, where a step is a user interface action that the student takes in order to achieve a task.</li> <li>Let us use knowledge for the information possessed by students that determines their behavior on task. Let us also assume that knowledge can be decomposed, and let us use the term Knowledge Component for the units into which it is decomposed.</li> <li>Knowledge Component</li> <li>Learning Event</li> <li>Modeling Transfer</li> <li>Predicting Student learning Curve</li> <li>Tutor</li> </ul>"},{"location":"KB/The%20Behavior%20of%20Tutoring%20Systems/#the-outer-loop","title":"THE OUTER LOOP","text":"<ul> <li>The main responsibility of the outer loop is to decide which task the student should do next. Its other responsibilities, such as presenting the task to the student, are more mundane and will not be mentioned again. The main design issues are (1) selecting a task intelligently and (2) obtaining a rich set of tasks to select from.</li> </ul>"},{"location":"KB/The%20Behavior%20of%20Tutoring%20Systems/#task-selection","title":"Task Selection","text":"<ul> <li>The outer loop displays a menu and lets the student select the next task.</li> <li>The outer loop assigns tasks in a fixed sequence</li> <li>Mastery learning</li> <li>Macroadaptation</li> <li>In addition to selecting a task, some tutoring systems also select a mode for the task. For instance, Steve can either (1) demonstrate how to do the task by taking each step itself and explaining it, or (2) hint each step before the student attempts it, or (3) let the student try to solve the task without such unsolicited hints.</li> <li>In addition to providing some kind of mechanism for selecting tasks, the designer must provide a set of tasks for the outer loop to select from. It is often surprising how many problems are necessary in order to field a tutoring system. For a 13 week semester at 10 problems per week, the tutoring system needs 130 problems at minimum</li> <li>Ideally, the tutor can generate its own tasks when given a specification of the desired characteristics.</li> <li>For instance, given a specification of a Knowledge Component to be taught, the SQL- Tutor (Martin &amp; Mitrovic, 2002) generates a database query, written in SQL, that involves the Knowledge Component. A human author then writes text for a problem that has this database query as its solution. Using this technique, a human author was able to create 200 problems in about 3 hours\u2014 enough for a 6 week instructional module in an SQL course. Such programs are called problem generators.</li> </ul>"},{"location":"KB/The%20Behavior%20of%20Tutoring%20Systems/#the-inner-loop","title":"THE INNER LOOP","text":"<ul> <li>Whereas the outer loop is about tasks, the inner loop is about steps within a task</li> </ul>"},{"location":"KB/The%20Behavior%20of%20Tutoring%20Systems/#minimal-feedback","title":"MINIMAL FEEDBACK","text":"<ul> <li>Minimal feedback usually indicates whether a step is correct or incorrect, although other categories are sometimes used as well.</li> </ul>"},{"location":"KB/The%20Behavior%20of%20Tutoring%20Systems/#categories-of-minimal-feedback","title":"Categories of Minimal Feedback","text":"<ul> <li>Although most tutoring systems use just two categories, correct and incorrect, it is possible to use others.</li> <li>Suppose a step is not part of an ideal solution to the problem, but it might be part of a non-ideal solution. The instructors might wish to consider this as a third category for minimal feedback\u2014correct but non-optimal</li> <li>For instance, when students solve physics problems on Andes, they can enter an equation that is true of the given physical situation but is not necessary for solving the problem.</li> <li>If instructors sometimes disagree on what makes for the best solution to the problem, it might be wise to subcategorize non-optimal. For instance, because some Sherlock instructors emphasize reducing time and others emphasize reducing costs, Sherlock could subcategorize non-optimal steps as wastes time or wastes parts. In short, the categories used for minimal feedback should reflect the pedagogical policies of the instructors.</li> <li>If a step cannot be classified as into one of the minimal feedback categories (correct, incorrect, non-optimal, etc.), then it lands in an unrecognizable classification.</li> <li>There are basically just three ways to treat such unrecognizable steps. (1) Some tutoring systems, such as the Algebra Cognitive Tutor, assume that they can recognize all correct steps, so they treat unrecognizable steps as incorrect. (2) Other tutoring systems, such as the SQL-Tutor, assume that students will sometimes produce novel, correct solutions, so they treat unrecognized steps as correct. (3) Lastly, the tutoring system could simply tell the student that the step is unrecognizable. In this case, unrecognizable is yet another category for minimal feedback. When to give minimal feedback</li> <li>Immediate feedback: Andes, Algebra Cognitive Tutor, Steve and AutoTutor give feedback immediately after each student step. They vary considerably in how the feedback is presented.</li> <li>Delayed feedback: Only if the step violates a safety regulation will Sherlock give immediate feedback as the student solves a problem. However, after the problem is solved, the student can request a reviewing mode where minimal feedback is given. In particular, as Sherlock replays the student's solution step-by-step, it indicates which steps did not contribute any diagnostic information.</li> <li>Demand feedback: The SQL-Tutor gives feedback only when the student clicks on the Submit button. Because the student can continue working on their solution after receiving such feedback, this does not count as delayed feedback.</li> <li>The feedback policy can be a function of the student's competence. For instance, if the student is nearing mastery, then the feedback is delayed whereas a less competent student would be given the same task with immediate feedback</li> <li>This policy has been observed in human tutoring, and is one instance of fading the scaffolding, (Collins et al., 1989), because feedback is a kind of scaffolding (pedagogically useful help).</li> </ul>"},{"location":"KB/The%20Behavior%20of%20Tutoring%20Systems/#next-step-hints","title":"NEXT-STEP HINTS","text":"<ul> <li> <ol> <li>When should the tutor give a hint about the next step? Should it wait for the student to ask? Should it give unsolicited hints when it detects guessing or floundering? 2. What step should the tutor suggest? For instance, if there are multiple paths to a solution, and the student appears to be following a long complex one, should the tutor suggest starting over? 3. How can the tutor give hints that maximize learning, keep frustration under control, and allow the student to finish the problem?</li> </ol> </li> <li>The common wisdom is that a tutor should give a hint on the next step only if the student really needs it. If the student can enter a correct step by thinking hard enough, then the system should refuse to give a hint even if the student asks for one</li> <li>On the other hand, if the student is likely to waste time and get frustrated by trying in vain to enter a correct step, or if the student is making repeated guesses instead of trying to figure out the next step, then the tutoring system should probably give a hint even if the student doesn't ask for one.</li> <li>DT Tutor</li> <li>Help Abuse</li> <li>Help Refusal</li> </ul>"},{"location":"KB/The%20Behavior%20of%20Tutoring%20Systems/#which-step-to-hint","title":"Which Step to Hint","text":"<ul> <li>The step must be correct.</li> <li>The hinted step should not have been done already by the student.</li> <li>Instructors' preferences should be honored.</li> <li>If the student has a plan for solving the problem or even for just the next step, then the tutor should hint the step that the student is trying to complete.</li> <li>A somewhat more complex case occurs when the student has just made an incorrect step and received minimal negative feedback.</li> <li>In AI, this is called the plan recognition problem</li> </ul>"},{"location":"KB/The%20Behavior%20of%20Tutoring%20Systems/#how-to-hint-the-next-step","title":"How to Hint the Next Step","text":"<ul> <li>Perhaps the most common policy is to construct a short sequence of hints for the next step.</li> <li>The first hints are weak\u2014they divulge little information so that students are encouraged to do most of the thinking themselves. Later hints are stronger.</li> <li>If the step requires only one Knowledge Component, then a standard hint sequence is Point, Teach and Bottom-out (Hume, Michael, Rovick, &amp; Evens, 1996)</li> <li>Pointing hints mention problem conditions that should remind the student of the Knowledge Component's relevance.</li> <li>Teaching hints describe the Knowledge Component briefly and show how to apply it.</li> <li>Bottom-out hints tell the student the step</li> </ul>"},{"location":"KB/The%20Behavior%20of%20Tutoring%20Systems/#error-specific-feedback","title":"ERROR-SPECIFIC FEEDBACK","text":"<ul> <li>The service consists of analyzing an incorrect step in order to determine the incorrect Learning Event(s) that led to it, then giving instruction that is aimed at preventing that incorrect Learning Event(s) from occurring again</li> <li>There are many techniques for diagnosing errors. Most require that authors observe student errors, figure out what is causing a common error, write an appropriate error type for it, and implement some way to recognize such errors.</li> <li>FOIL: First, Outer, Inner and Last</li> </ul>"},{"location":"KB/The%20Behavior%20of%20Tutoring%20Systems/#how-to-give-error-specific-feedback","title":"How to Give Error-specific Feedback","text":"<ul> <li>The main purpose of error-specific feedback is to get the student to change their knowledge so that they will not make this mistake again</li> <li>Correcting one's knowledge is sometimes compared to debugging a piece of software (Ohlsson, 1996). A programmer must first find evidence that the bug exists, then find out what the bug is, and then fix the bug. When tutees are not working with a tutor, they must do the whole self-debugging process themselves. They must first notice that a step is incorrect, then find the flaw in their reasoning and knowledge, then figure out what the correct learning events and knowledge components should be</li> <li>Many tutors present feedback as a sequence of hints that are loosely associated with the stages of self-debugging. When the student enters an incorrect step, the tutor begins the sequence by simply giving minimal feedback. This is like providing the student with evidence of a knowledge bug but giving no further help toward identifying the bug or correcting it.</li> </ul>"},{"location":"KB/The%20Behavior%20of%20Tutoring%20Systems/#when-to-give-error-specific-feedback","title":"When to Give Error-specific Feedback","text":"<ul> <li>Students sometimes make careless errors, called slips in psychology (Norman, 1981), but fail to notice that they have made them, which can cause them to waste time looking for deep, potential misconceptions.</li> <li>To ameliorate into slips and potential this, Andes divides error misunderstandings. types</li> <li>Slips are often made by the experts, including the instructors.</li> <li>A potential misunderstanding is an error type that could be due to many factors, including incorrect beliefs, but it is almost never seen in expert's work</li> <li>When a student enters an incorrect step that is classified as a slip, then Andes gives the error specific remediation immediately, e.g., You forgot to include a unit on a number. If the error is classified as a potential misunderstanding, then Andes merely turns the incorrect step red, and lets the student ask for an error-specific hint if they want one.</li> <li>Some student steps contain two or more errors. In order to keep the communication simple, the tutoring system should probably respond to only one of them. Students typically fix that error only, so the tutoring system can then mention the second error</li> </ul>"},{"location":"KB/The%20Behavior%20of%20Tutoring%20Systems/#what-kind-of-assessment-is-required","title":"What Kind of Assessment is Required?","text":"<ul> <li>A fundamental issue is the grain-size or granularity of the assessment.</li> <li>The granularity of an assessment refers to the degree of aggregation over the task domain's knowledge. An assessment is fine-grained if it provides, for instance, a probability of mastery for each Knowledge Component in the task domain. An assessment is coarse-grained if it provides a single number that indicates the student's overall competence.</li> <li>Assessments are decision aids, and as a general heuristic, the bigger the decision, the coarser the required assessment. If a decision affects a whole semester, e.g., whether the student needs to retake a</li> <li>required course, then the assessment should cover the whole semester and lead ultimately to a yes-no decision, so a single number per student is appropriate.</li> <li>If a decision affects only, say, whether the tutor starts at the beginning of a hint sequence or in the middle, then only a small part of a fine-grained assessment is relevant</li> <li>The general idea is that if the decision is small, in that it affects only a small amount of instructional time, then only a small amount of the domain's knowledge can be accessed during that time and thus the relevant decision aid is the student's competence on just that knowledge.</li> <li>Tutors make many small decisions, so they are the main customers for fine-grained assessments.</li> <li>Coarse-grained assessment</li> <li>Fine Grained assesment</li> </ul>"},{"location":"KB/The%20Behavior%20of%20Tutoring%20Systems/#issues-with-assesment","title":"Issues with Assesment","text":"<ul> <li>There are many other issues involved with assessment. Here are a few:</li> <li>There is a big difference between little evidence and mixed evidence. If one merely takes the frequency of success relative to opportunities, then 0.5 can mean both 1 success and 1 failure (little evidence) or 20 successes and 20 failures (mixed evidence).</li> <li>Students should be learning, so their knowledge should be changing. When should old evidence be ignored?</li> <li>Should help given by the tutoring system be counted as evidence of learning or of lack of knowledge?</li> <li>Are all learning events equally difficult? If not, then item response theory (IRT) can be used so that success on easy learning events provides less evidence of competence than success on difficult learning events.</li> <li>If the tutoring system includes error-specific feedback based on error types, should it somehow include the frequency of occurrence of the error types in its assessments?</li> <li>Prior probabilities of mastery are not independent. If a student has not mastered a Knowledge Component that is taught early in the course, then it is less likely that the student has mastered.</li> <li>Knowledge components that appear later.</li> </ul>"},{"location":"KB/The%20Behavior%20of%20Tutoring%20Systems/#examples-of-tutoring-systems","title":"Examples of Tutoring Systems","text":"<ul> <li>Steve</li> <li>Algebra Cognitive Tutor</li> <li>Andes</li> <li>Sherlock</li> <li>AutoTutor</li> <li>SQL-Tutor</li> </ul>"},{"location":"KB/The%20Differentiation%20Condition/","title":"The Differentiation Condition","text":""},{"location":"KB/The%20Differentiation%20Condition/#the-differentiation-condition","title":"The Differentiation Condition","text":"<ul> <li>A sentence containing a quantified phrase headed by each can only be true of event structures which are totally distributive. Each individual object in the restrictor set of the quantified phrase must be associated with its own subevent, in which the predicate applies to that object, and which can be differentiated in some way from the other subevents.</li> </ul>"},{"location":"KB/The%20Effect%20of%20Three%20Consecutive%20Context%20Sentences%20on%20EFL%20Vocabulary-Learning/","title":"The Effect of Three Consecutive Context Sentences on EFL Vocabulary-Learning","text":""},{"location":"KB/The%20Effect%20of%20Three%20Consecutive%20Context%20Sentences%20on%20EFL%20Vocabulary-Learning/#the-effect-of-three-consecutive-context-sentences-on-efl-vocabulary-learning","title":"The Effect of Three Consecutive Context Sentences on EFL Vocabulary-Learning","text":"<ul> <li> <p>Sasan Baleghizadeh and Mohammad Naseh Nasrollahy Shahry</p> </li> <li> <p>effect of three consecutive context sentences instead of one</p> </li> <li>Thirty-three Iranian EFL learners were asked to learn 20 challenging English words in two conditions</li> <li>The results of both immediate and delayed post-tests revealed a positive role for context sentences in vocabulary learning</li> <li>It is proposed that successful vocabulary learning through context sentences could be attributed to the mixed effects of both context and frequency of occurrence.</li> </ul>"},{"location":"KB/The%20Effect%20of%20Three%20Consecutive%20Context%20Sentences%20on%20EFL%20Vocabulary-Learning/#results","title":"Results","text":"<ul> <li>seven students knew at least one of the target words, so their scores on the posttests were excluded</li> <li>data were analyzed by using a paired sample t-test because it was a within-subjects comparison and the scores on both learning conditions were related</li> <li>the participants' performance on words that had appeared in context sentences plus their L1 equivalents was significantly better than their performance on words paired only with their L1 equivalent</li> <li>Discussion</li> <li>Learners exposed to context sentences did better in terms retaining words, and they were also able to compose more correct sentences with them</li> <li>The learners who were exposed to context sentences had three sentences on which to draw as models, and it is plausible that part of their better sentence-making scores could be accounted for by their exposure to these sentences.</li> <li>frequency and context have an important place in vocabulary-learnin</li> <li>lthough learning new words through context-free activities such as working on word pairs might be a powerful tool to enhance one's breadth of vocabulary knowledge, this study provides strong evidence that adding a minimum of three contextually appropriate sentences to L1 glosses results in a significant improvement in vocabulary-learning</li> <li>need to furnish learners with more sample sentences when it comes to presenting vocabulary</li> <li>authors of textbooks seem to have a propensity for presenting isolated words either in designated boxes or in the context of a passage, which essentially provides only one context for the given word</li> <li>It appears that students would be in a better position to learn and retain new words if they were provided with repeated contexts through exposure to more sample sentences</li> <li>In other words, is it the elaborative nature of the context, or is it the frequency of occurrence that promotes better vocabulary learning? Future research is warranted to unravel this issue.</li> </ul>"},{"location":"KB/The%20Programmers%20Brain/","title":"The Programmers Brain","text":"","tags":["deeplearning"]},{"location":"KB/The%20Programmers%20Brain/#the-programmers-brain","title":"The Programmers Brain","text":"<ul> <li>Short + long term memory - problem + software knowledge</li> <li>reading complicated/new code<ul> <li>chunking : words vs letters</li> </ul> </li> <li>Programmers to reproduce code<ul> <li></li> <li>experts rely on a bunch of things to \"do well\"</li> </ul> </li> </ul>","tags":["deeplearning"]},{"location":"KB/The%20Repulsive%20Potential/","title":"The Repulsive Potential","text":""},{"location":"KB/The%20Repulsive%20Potential/#the-repulsive-potential","title":"The Repulsive Potential","text":"<ul> <li>The robot is pushed away from obstacles and attracted to the goal</li> <li> \\[U_{rep}(q)=\\frac{\\eta}{D(q,Q)}\\] </li> <li></li> </ul>"},{"location":"KB/The%20Reward%20Experiment/","title":"The Reward Experiment","text":""},{"location":"KB/The%20Reward%20Experiment/#the-reward-experiment","title":"The Reward Experiment","text":"<ul> <li>\u201cWhy aren\u2019t we always prepared?\u201d  <ul> <li>Is it a matter of motivation?  </li> <li>Reward bocks: flat(ter) preparation curves?</li> </ul> </li> <li>Reward blocks: more memory effects<ul> <li>Cognition Hazard Rates</li> </ul> </li> <li>Reward blocks: less memory effects<ul> <li>Cognitive fMTP</li> </ul> </li> <li></li> <li>Fanning<ul> <li>smaller if reward</li> <li>less effects of previous trial</li> </ul> </li> </ul>"},{"location":"KB/The%20Reward%20Experiment/#findings","title":"Findings","text":"<ul> <li> <p>The effects of reward and brightness on preparation</p> <ul> <li>Additive effects : neither affects the slope</li> <li>even when motivated, preparation depends on FP</li> <li> <p>Stimulus visibility does not interact with preparation</p> <ul> <li>Maybe not a visual attention process</li> </ul> </li> <li> <p></p> </li> <li>Reward has more effects when targets are highly visible</li> <li>Interaction : Reward * Brightness</li> <li></li> <li>Effects on accuracy</li> <li>Reward blocks\u2192 higher accuracy  </li> <li>High visibility \u2192 higher accuracy  </li> <li>More errors with longer foreperiod\u2026.  <ul> <li>\u2026But only when rewarded/motivated  </li> <li>\u2026And mainly when visibility low</li> </ul> </li> <li></li> <li>Effects on memory from past trials</li> <li>Past trial effects \\(\\(fp \\ast fp(n-1)\\)\\)</li> <li>This effect (the fanning) is essentially constant across target visibility and reward  </li> <li>If anything: a little bit larger when there is no reward.</li> <li></li> <li>Effects on memory</li> <li>Up to n-5</li> <li>When there is reward, there is slightly \u2018faster forgetting\u2019</li> <li>Faster forgetting when rewarded slightly</li> <li></li> </ul> </li> </ul>"},{"location":"KB/The%20Self%20Organization%20of%20Explicit%20Attitudes/","title":"The Self Organization of Explicit Attitudes","text":""},{"location":"KB/The%20Self%20Organization%20of%20Explicit%20Attitudes/#the-self-organization-of-explicit-attitudes","title":"The Self Organization of Explicit Attitudes","text":"<ul> <li>Michael T. Wojnowicz, Melissa J. Ferguson, Rick Dale, and Michael J. Spivey</li> </ul>"},{"location":"KB/The%20Self%20Organization%20of%20Explicit%20Attitudes/#abstract","title":"ABSTRACT","text":"<ul> <li>How do minds produce explicit attitudes over several hundred milliseconds?</li> <li>implicit biases beyond cognitive control and subjective awareness, yet mental processing may culminate in an explicit attitude that feels personally endorsed and corroborates voluntary intentions</li> <li>self-reported explicit attitudes derive from a continuous, temporally dynamic process, whereby multiple simultaneously conflicting sources of information selforganize into a meaningful mental representation</li> <li>While our participants reported their explicit (like vs. dislike) attitudes toward White versus Black people by moving a cursor to a ''like'' or ''dislike'' response box, we recorded streaming xand y-coordinates from their hand-movement trajectories</li> <li>participants' hand-movement paths exhibited greater curvature toward the ''dislike'' response when they reported positive explicit attitudes toward Black people than when they reported positive explicit attitudes toward White people</li> <li>these trajectories were characterized by movement disorder and competitive velocity profiles that were predicted under the assumption that the deliberate attitudes emerged from continuous interactions between multiple simultaneously conflicting constraints.</li> <li>For example, an implicit attitude toward a stimulus can be unintentionally activated by the mere presence of that stimulus.</li> <li>Given that many people demonstrate spontaneous initial biases toward traditionally stigmatized groups, how do they overcome these biases to explicitly report positive attitudes toward the same groups?</li> <li>coexistence of multiple attitudes and an emphasis on the temporal dynamics of how they influence evaluative responses</li> <li>Rather than selecting among the specific theories, we invoked the encompassing theoretical framework of self-organization to guide an exploration of those temporal dynamics, and made specific predictions for what should result from multiple attitudes interacting over time.</li> <li>In early moments of processing, distributed representations are partially consistent with multiple interpretations because of their proximity to multiple neural population codes.</li> <li>However, a continuous accrual of information causes the distributed pattern to dynamically ''sharpen'' into a confident (selected) interpretation, forcing other, partially activated, competing alternative representations, decisions, or actions to gradually die out.</li> <li>The latter attitude will eventually activate other subsystems, such as language and memory, thus making the attitude seem explicit</li> <li>What makes the first attitude implicit is not necessarily that it was generated in a different subsystem, but simply that it did not hold sway long enough to activate those language and memory subsystems.</li> <li>Mental processing generically involves KB/Recurrent.md processing loops (or cyclic feedback) between higher-order integrative regions and lower-level informational sources (Lamme &amp; Roelfsema, 2000; O'Reilly, 1998; Spivey, 2007)</li> <li>These higher-order integrative regions enforce representational competition, in which increasing the activation of one particular interpretation inhibits alternatives.</li> <li>The unfolding cognitive dynamics may be revealed in continuous motor output</li> <li>Because mental processing is KB/Recurrent.md, motor representations begin specifying movement parameters probabilistically, rather than waiting for a perfectly completed cognitive command</li> <li>If the phrase ''Black people'' evokes elevated dynamic competition between simultaneously active ''like'' and ''dislike'' representations, movement trajectories for ''Black people'' should exhibit evidence of nonlinear dynamics in their velocity profiles, as well as increased spatial disorder in the curviness of the trajectories.</li> </ul>"},{"location":"KB/The%20Self%20Organization%20of%20Explicit%20Attitudes/#experiment-1","title":"EXPERIMENT 1","text":""},{"location":"KB/The%20Self%20Organization%20of%20Explicit%20Attitudes/#method","title":"Method","text":"<ul> <li>Streaming xand y-coordinates of mouse-cursor movements were recorded from 68 Cornell University undergraduates (43 female and 25 male) as they performed a simple explicit-attitude task.</li> <li>2 s for participants to view the evaluative response options</li> <li>Participants then clicked on a small box at the bottom of the screen to reveal a stimulus word or phrase and dragged the mouse toward their selected evaluative response to that stimulus</li> <li>Responses to the two stimulus repetitions were averaged together to yield a single measurement for each participant for all statistical analyses.</li> </ul>"},{"location":"KB/The%20Self%20Organization%20of%20Explicit%20Attitudes/#results-and-discussion","title":"Results and Discussion","text":"<ul> <li>Compared with the trajectories for ''White people,'' the trajectories for ''Black people'' curved significantly more toward the ''dislike'' response option observed differential motor curvatures could have been generated by a stage-based sequence of decisional commands, rather than by continuous motor attraction to the ''dislike'' response.</li> <li>If motor execution required the complete prespecification of a unique target destination, rather than tracking of motor trajectory parameters that continuously evolved midflight, then a mean trajectory could look differentially curved because of the effect of averaging in replanned trajectories</li> <li>To accommodate the empirical mean trajectory, which initially moved upward rather than actually toward ''dislike,'' such an account would need to predict a bimodal distribution of curvatures that included some trajectories that were very curved and others that were not curved.</li> <li>However, the distribution of trajectory curvatures shows no evidence of bimodality The standard cutoff for inferring bimodality in a distribution is b &gt; 0.55</li> <li>Neither the ''Black people'' nor the ''White people'' trajectories had distributions that met this cutoff, and in fact, the ''Black people'' trajectories formed a distribution of movement curvature that was closer to normal (b 5 0.24, skewness 5 0.613, kurtosis 5 2.57) than the ''White people'' trajectories (b 5 0.301, skewness 5 0.98, kurtosis 5 3.44).</li> <li>Velocity profiles were constructed by analyzing the temporal derivatives of motion toward the ''like'' response box along the x-coordinate.</li> <li>Our velocity predictions came from Usher and McClelland's (2003) differential equations for modeling the dynamics of competition between mental representations</li> <li>where, in this case, x1 and x2 represent the activations of the mental representations for ''like'' and ''dislike,'' dx1 and dx2 represent the change in the activation of the two mental representations in a time step of size dt, I1 and I2 represent excitatory input to the representations from informational sources, bf1 and bf2 represent the inhibitory input from each mental representation to the other (lateral inhibition), and fi (where i 5 1 or 2) is equal to xi if xi is greater than zero.</li> <li>differential equations for competition dynamics, a strong evaluative competitor (dislike, x2) sends intensified and prolonged lateral inhibition (bf2) to the ''like'' evaluation (x1)</li> <li>Thus, strong competition alters the velocity profile of the movement toward the evaluative attractor (dx1/dt), reducing velocity toward the attractor early on in processing</li> <li>as the more active alternative begins to win the competition, this lateral inhibition is gradually lifted, thus increasing velocity later in processing to produce greater acceleration</li> <li>Moreover, this particular dynamic pattern (reduced early velocity and greater later acceleration) should lead to greater peak velocity, if jerk is minimized as the system achieves equivalent integral under the curve (where the integral represents net change in activation or location)</li> <li>The spatial-disorder analysis investigated the regularity of change in x-coordinate location over time</li> <li>To investigate whether the ''Black people'' trajectories had more wiggles, blips, and other irregularities than the ''White people'' trajectories, we analyzed x-coordinate location over time, but only after the trajectory began moving in the positive x direction</li> <li>The ''Black people'' trajectories had significantly greater deviation from the sigmoidal fit</li> <li>indicated disorderly variation around the x dimension in those trajectories.</li> </ul>"},{"location":"KB/The%20Self%20Organization%20of%20Explicit%20Attitudes/#experiment-2","title":"EXPERIMENT 2","text":"<ul> <li>our claim is that multiple, partially active mental representations compete for the privilege of driving evaluative responses, imposing a set of response options that are not particularly competitive should change the motor dynamics</li> </ul>"},{"location":"KB/The%20Self%20Organization%20of%20Explicit%20Attitudes/#method_1","title":"Method","text":"<ul> <li>Sixty-six Cornell University undergraduates (40 female and 26 male) were asked to classify words (e.g., ''ice cream,'' ''sunshine,'' ''boron'') as something they liked (''like'') or as the name of a chemical element (''chemical'')</li> <li>We analyzed data only from the 63 participants who consistently chose the ''like'' response for both ''Black people'' and ''White people'' on both repetitions of these trials, and who reported in a poststudy questionnaire that they were not forced into selecting ''like'' by the paradigm.</li> </ul>"},{"location":"KB/The%20Self%20Organization%20of%20Explicit%20Attitudes/#results","title":"Results","text":"<ul> <li>According to statistical analyses on maximum deviation and distance traveled, the ''Black people'' and ''White people'' trajectories no longer differed in their curvature toward the competing respons</li> <li>Thus, the results of Experiment 1 are not attributable merely to responses to ''Black people'' involving a longer latency to settle on a positive evaluation</li> <li>thereby drifting for longer in empty regions of movement space before curving</li> <li>toward the ''like'' response box</li> <li>Rather, the ''dislike'' response option in Experiment 1 was actively pulling movement trajectories toward it, in a way that the ''chemical'' response option in Experiment 2 did not.</li> </ul>"},{"location":"KB/The%20Self%20Organization%20of%20Explicit%20Attitudes/#experiment-3","title":"EXPERIMENT 3","text":"<ul> <li>Experiment 1 may have diverged because of subtle confounds that do not refer to people at all.</li> </ul>"},{"location":"KB/The%20Self%20Organization%20of%20Explicit%20Attitudes/#method_2","title":"Method","text":"<ul> <li>Seventy-one Cornell University undergraduates (37 female and 34 male) were asked to classify stimuli as something they liked (''like'') or disliked (''dislike'')</li> <li>The crucial stimuli in this experiment were ''African Americans'' and ''Caucasians. Results</li> <li>The trajectories for ''African Americans'' curved significantly more toward the ''dislike'' response than the trajectories for ''Caucasians,</li> <li>The motor trajectories evolved over time in accordance with the competitive velocity predictions, as reported in Experiment</li> <li>The ''African Americans'' trajectories, compared with the ''Caucasians'' trajectories, had significantly greater maximum xcoordinate acceleration</li> <li>Moreover, as we found for ''Black people'' trajectories in Experiment 1, the ''African Americans'' trajectories exhibited greater spatial disorder than the ''Caucasians'' trajectories, even after moving toward the ''like'' response, as indicated by significantly greater mean deviation from the sigmoidal fi</li> </ul>"},{"location":"KB/The%20Self%20Organization%20of%20Explicit%20Attitudes/#general-discussion","title":"GENERAL DISCUSSION","text":"<ul> <li>People's hand-movement trajectories for explicitly evaluating ''Black people'' and ''White people'' were distinct as measured by three properties of movement dynamics: shape, time, and order.</li> <li>explicit attitudes evolve through continuous temporal dynamics during real-time mental processing, with graded motor curvature revealing the influence of tendencies toward dislike</li> <li>evidence for cleanly separated (i.e., discrete, rather than continuous) explicit decisions, in which an initial response was executed solely toward the ''dislike'' response box and then a corrective response was executed midflight toward the ''like'' response box.</li> <li>Rather, the results suggest that a dynamic competition process may be what allows a single explicit attitude choice to emerge from multiple, potentially conflicting evaluative influences (e.g., Busemeyer &amp; Townsend, 1993; Usher &amp; McClelland, 2003)</li> <li>the mind may host a continuously evolving blend of (implicit) evaluative decisions from which the eventual (explicit) behavioral choice emerges.</li> </ul>"},{"location":"KB/The%20Self%20Organization%20of%20Explicit%20Attitudes/#pictures","title":"Pictures","text":""},{"location":"KB/The%20Unreliability%20of%20Saliency%20Methods/","title":"The Unreliability of Saliency Methods","text":""},{"location":"KB/The%20Unreliability%20of%20Saliency%20Methods/#the-unreliability-of-saliency","title":"The Unreliability of Saliency","text":"<ul> <li>@kindermansReliabilitySaliencyMethods2017</li> <li> unreliable</li> <li>Pieter-Jan Kindermans, Sara Hooker, Julius Adebayo, Maximilian Alber, Kristof T. Sch\u00fctt, Sven D\u00e4hne, Dumitru Erhan, Been Kim <code>toc</code></li> </ul>"},{"location":"KB/The%20elephant%20in%20the%20interpretability%20room/","title":"The elephant in the interpretability room","text":""},{"location":"KB/The%20elephant%20in%20the%20interpretability%20room/#the-elephant-in-the-interpretability-room","title":"The Elephant in the Interpretability Room","text":"<ul> <li>@bastingsElephantInterpretabilityRoom2020</li> </ul>"},{"location":"KB/The%20elephant%20in%20the%20interpretability%20room/#intro","title":"Intro","text":"<ul> <li>While attention conveniently gives us one weight per input token and is easily extracted, it is often unclear toward what goal it is used as explanation.</li> <li>that often that goal, whether explicitly stated or not, is to find out what input tokens are the most relevant to a prediction, and that the implied user for the explanation is a model developer</li> <li>For this goal and user, we argue that input saliency methods are better suited, and that there are no compelling reasons to use attention, despite the coincidence that it provides a weight for each input.</li> <li></li> </ul>"},{"location":"KB/The%20elephant%20in%20the%20interpretability%20room/#the-attention-debate","title":"The Attention Debate","text":"<ul> <li>summarize the debate on whether attention is explanation</li> <li>mostly features simple BiLSTM text classifiers</li> <li>Unlike Transformers (Vaswani et al., 2017), they only contain a single attention mechanism, which is typically MLP-based (Bahdanau et al., 2015)</li> <li></li> </ul>"},{"location":"KB/The%20elephant%20in%20the%20interpretability%20room/#is-attention-not-explanation","title":"Is Attention (not) Explanation?","text":"<ul> <li>Jain and Wallace (2019) show that attention is often uncorrelated with gradientbased feature importance measures, and that one can often find a completely different set of attention weights that results in the same prediction</li> <li>Serrano and Smith (2019) find, by modifying attention weights, that they often do not identify those representations that are most most important to the prediction of the model</li> <li>Finally, Pruthi et al. (2020) propose a method to produce deceptive attention weights. Their method reduces how much weight is assigned to a set of 'impermissible' tokens, even when the models demonstratively rely on those tokens for their predictions.</li> </ul>"},{"location":"KB/The%20elephant%20in%20the%20interpretability%20room/#was-the-right-task-analyzed","title":"Was the Right Task Analyzed","text":"<ul> <li>the performance of an NMT model degrades substantially if uniform weights are used, while random attention weights affect the text classification performance minimally.</li> <li>even for the task of MT, the first case where attention was visualized to inspect a model (\u00a71), Ding et al. (2019) find that saliency methods (\u00a73) yield better word alignments.</li> </ul>"},{"location":"KB/The%20elephant%20in%20the%20interpretability%20room/#can-attention-be-improved","title":"Can Attention Be Improved","text":"<ul> <li>Mohankumar et al. (2020) observe high similarity between the hidden representations of LSTM states and propose a diversity-driven training objective that makes the hidden representations more diverse across time steps</li> <li>using representation erasure that the resulting attention weights result in decision flips more easily as compared to vanilla attention</li> <li>Deng et al. (2018) propose variational attention as an alternative to the soft attention of Bahdanau et al. (2015), arguing that the latter is not alignment, only an approximation thereof</li> <li> <p>additional benefit of allowing posterior alignments, conditioned on the input and the output sentences.</p> </li> <li> <p>Saliency vs Attention</p> </li> </ul>"},{"location":"KB/The%20elephant%20in%20the%20interpretability%20room/#attention-is-not-not-interesting","title":"Attention is not not Interesting","text":"<ul> <li>Voita et al. (2019) and Michel et al. (2019) analyze the role of attention heads in the Transformer architecture and identify a few distinct functions they have, and Strubell et al. (2018) train attention heads to perform dependency parsing, adding a linguistic bias</li> <li>Strout et al. (2019) demonstrate that supervised attention helps humans accomplish a task faster than random or unsupervised attention</li> </ul>"},{"location":"KB/The%20elephant%20in%20the%20interpretability%20room/#beyond-saliency","title":"Beyond Saliency","text":"<ul> <li>counterfactual analysis might lead to insights, aided by visualization tools (Vig, 2019; Hoover et al., 2020; Abnar and Zuidema, 2020)</li> <li>The DiffMask method of DeCao et al. (2020) adds another dimension: it not only reveals in what layer a model knows what inputs are important, but also where important information is stored as it flows through the layers of the mode</li> </ul>"},{"location":"KB/The%20elephant%20in%20the%20interpretability%20room/#limitations-of-saliency","title":"Limitations of Saliency","text":"<ul> <li>changes in the predicted probabilities may be due to the fact that the corrupted input falls off the manifold of the training data (Hooker et al., 2019)</li> <li>a drop in probability can be explained by the input being OOD and not by an important feature missing</li> <li>at least some of the saliency methods are not reliable and produce unintuitive results (Kindermans et al.</li> <li>2017) or violate certain axioms (Sundararajan et al., 2017).</li> <li>A more fundamental limitation is the expressiveness of input saliency methods</li> <li>Obviously, a bag of per-token saliency weights can be called an explanation only in a very narrow sense</li> <li>One can overcome some limitations of the flat representation of importance by indicating dependencies between important features (for example, Janizek et al. (2020) present an extension of IG which explains pairwise feature interactions) but it is hardly possible to fully understand why a deep non-linear model produced a certain prediction by only looking at the input tokens</li> </ul>"},{"location":"KB/The%20warning%20stimulus%20as%20retrieval%20cue%20The%20role%20of%20associative%20memory%20in%20temporal%20preparation/","title":"The warning stimulus as retrieval cue The role of associative memory in temporal preparation","text":""},{"location":"KB/The%20warning%20stimulus%20as%20retrieval%20cue%20The%20role%20of%20associative%20memory%20in%20temporal%20preparation/#the-warning-stimulus-as-retrieval-cue-the-role-of-associative-memory-in-temporal-preparation","title":"The warning stimulus as retrieval cue The role of associative memory in temporal preparation","text":"<ul> <li>Sander A. Los a,* , Jurre Nieuwenstein a , Anass Bouharab a , David J. Stephens a , Martijn Meeter a , Wouter Kruijne b</li> </ul>"},{"location":"KB/The%20warning%20stimulus%20as%20retrieval%20cue%20The%20role%20of%20associative%20memory%20in%20temporal%20preparation/#abstract","title":"Abstract","text":"<ul> <li>warned reaction time task</li> <li>the warning stimulus (S1) initiates a process of temporal preparation, which promotes a speeded response to the impending target stimulus (S2)</li> <li>participants learn the timing of S2 by storing a memory trace on each trial, which contains a temporal profile of the events on that trial.</li> <li>On each new trial, S1 serves as a retrieval cue that implicitly and associatively activates memory traces created on earlier trials, which jointly drive temporal preparation for S2</li> <li>two different S1s were associated with two different distributions of S1-S2 intervals: one with predominantly short and one with predominantly long intervals</li> <li>Experiments differed regarding the S1 features that made up a pair, ranging from highly distinct (e.g., tone and flash) to more similar (e.g., red and green flash) and verbal (i.e., \"short\" vs \"long\").</li> <li>This cueing effect persisted in a subsequent transfer phase, in which the contingency between S1 and the timing of S2 was broken \u2013 a fact participants were informed of in advance</li> <li>these findings support the role of S1 as an implicit retrieval cue, consistent with MTP.</li> <li>A multiple trace theory of temporal preparation</li> <li>In this paradigm, the researcher varies, within a block of trials, the duration of the foreperiod between a warning stimulus (S1) and a target stimulus (S2), and measures the participant's response time (RT) with respect to S2.</li> <li>as the foreperiod increases, mean RT decreases toward an asymptote (e.g., Niemi &amp; N \u0308 aat  \u0308 anen,  \u0308 1981; Woodrow, 1914), indicating a gradual growth of temporal preparation toward a maximum</li> <li>exponential (\"nonageing\") distribution</li> <li>frequency of consecutive foreperiods decreases according to a fixed rate</li> <li>and the RT \u2013 foreperiod function has been shown to be approximately flat</li> <li>MTP, which makes three main assumptions.</li> <li>within-trial processing dynamics</li> <li>detection of S1 prompts a preactivation of task relevant effectors, which is counteracted throughout the foreperiod by a process of continuous inhibition</li> <li>Inhibition is lifted when S2 is presented, allowing activation to drive response execution</li> <li>when transcranial magnetic stimulation is applied to human motor cortex, the motor evoked potential measured at the corresponding effector has been shown to be smaller during the foreperiod than at baseline, prior to S1 onset</li> <li>Since this reduced activation has been found for all potential effectors in a choice reaction task, it has been argued to reflect a general mechanism of impulse control that prevents premature response</li> </ul>"},{"location":"KB/The%20warning%20stimulus%20as%20retrieval%20cue%20The%20role%20of%20associative%20memory%20in%20temporal%20preparation/#trace-formation","title":"trace formation","text":"<ul> <li>unique memory trace is created on each trial, which contains the temporal profile of inhibition (during the foreperiod) and activation (after S2 occurrence) experienced on that trial, along with representations of S1, S2, and the response to S2</li> <li>each new memory trace is added to an accumulating pool of memory traces created on earlier trials</li> <li>these traces vary in strength</li> <li>strength of each trace is maximal upon its formation and gradually reduces toward an asymptotic value as it grows older</li> </ul>"},{"location":"KB/The%20warning%20stimulus%20as%20retrieval%20cue%20The%20role%20of%20associative%20memory%20in%20temporal%20preparation/#trace-expression","title":"trace expression","text":"<ul> <li>previously formed memory traces jointly determine the state of temporal preparation during the ongoing foreperiod</li> <li>process is initiated on each trial by the presentation of S1</li> <li>as the foreperiod elapses, each retrieved trace contributes to preparation in accordance with its strength and its momentary value of activation or inhibition</li> <li>at each moment during the foreperiod the state of preparation is determined by the ratio of the weighted activation over inhibition values aggregated across memory traces</li> <li>we assume that the state of temporal preparation reached at the moment of S2 presentation determines RT according to an inversely proportional function</li> <li>This relationship can be appreciated by conceiving the state of temporal preparation as the distance of potential neural excitability relative to a fixed motor-action limit</li> <li>the consecutive foreperiods occur with a ratio of 1:2:4:8, temporal preparation is very low just after the presentation of S1 in view of the low ratio of activation over inhibition across memory traces.</li> <li>These dynamics thus give rise to the typically observed steep RT \u2013 foreperiod functio</li> <li>In the case of an exponential distribution (Fig. 1B), where consecutive foreperiods occur with a ratio of 8:4:2:1, activation starts to dominate inhibition quickly after the presentation of S1.</li> <li>Thus, preparation is already close to ceiling by the time the shortest foreperiod has elapsed and it remains at that level if the foreperiod lengthens (Fig. 1D), yielding the characteristically flat RT \u2013 foreperiod function</li> </ul>"},{"location":"KB/The%20warning%20stimulus%20as%20retrieval%20cue%20The%20role%20of%20associative%20memory%20in%20temporal%20preparation/#images","title":"Images","text":""},{"location":"KB/There%20and%20back%20again/","title":"There and back again","text":""},{"location":"KB/There%20and%20back%20again/#there-and-back-again","title":"There and back again","text":"<ul> <li>@rebuffiThereBackAgain2020</li> </ul>"},{"location":"KB/Thesis%20Flow/","title":"Thesis Flow","text":""},{"location":"KB/Thesis%20Flow/#augmentation","title":"Augmentation","text":""},{"location":"KB/Thesis%20Flow/#methods","title":"Methods","text":"<ul> <li>Attentive CutMix </li> <li>AttributeMix</li> <li>AugMix</li> <li>GridMask</li> <li>Hide and Seek </li> <li>Image Mixing and Deletion </li> <li>Intra-Class Part Swapping </li> <li>Puzzle Mix </li> <li>KeepAugment</li> <li>RICAP</li> <li>RandAugment</li> <li>Random Distortion </li> <li>Random Erasing </li> <li>ReMix</li> <li>ResizeMix</li> <li>SMOTE</li> <li>SaliencyMix</li> <li>Sample Pairing </li> <li>Visual Context Augmentation </li> <li>SmoothMix</li> <li>SnapMix</li> <li>SpecAugment</li> <li>Co-Mixup</li> <li>Cut and Mix </li> <li>Data Augmentation via Latent Space Interpolation for Image Classification </li> <li>Data Augmentation with Curriculum Learning </li> </ul>"},{"location":"KB/Thesis%20Flow/#disadvantages","title":"Disadvantages","text":""},{"location":"KB/Thesis%20Flow/#xai","title":"XAI","text":""},{"location":"KB/Thesis%20Flow/#methods_1","title":"Methods","text":"<ul> <li>Adaptive Whitening Saliency </li> <li>Bayesian Rule List </li> <li>CAM</li> <li>Conductance</li> <li>DeconvNet</li> <li>Deep Inside Convolutional Networks </li> <li>Deep Visual Explanation </li> <li>DeepFool</li> <li>DeepLIFT</li> <li>Dynamic visual attention </li> <li>Embedding Human Knowledge into Deep Neural Network via Attention Map </li> <li>Generalizing Adversarial Explanations with Grad-CAM </li> <li>Graph-based visual saliency </li> <li>Guided BackProp </li> <li>Guided GradCAM </li> <li>Influence of image classification accuracy on saliency map estimation </li> <li>Integrated Gradients </li> <li>Interpretation of Neural networks is fragile </li> <li>Noise Tunnel </li> <li>On the overlap between Grad-CAM saliency maps and explainable visual features in skin cancer images </li> <li>RISE</li> <li>Real Time Image Saliency for Black Box Classifiers </li> <li>SAM-ResNet</li> <li>SDR</li> <li>SP-LIME</li> <li>Salience Map </li> <li>Smooth-Grad</li> <li>SmoothGrad Square </li> <li>Summit</li> <li>VarGrad</li> <li>ScoreCAM</li> </ul>"},{"location":"KB/Thesis%20Flow/#disadvantages_1","title":"Disadvantages","text":"<ul> <li>Beware of Inmates Running the Asylum </li> <li>The Unreliability of Saliency Methods </li> </ul>"},{"location":"KB/Thesis%20Flow/#architectures","title":"Architectures","text":"<ul> <li>Res Net </li> <li>Vision Transformer </li> <li>ConvNeXt</li> <li>Mobile Net </li> <li>Vgg</li> <li>Xception</li> </ul>"},{"location":"KB/Theta%20Waves/","title":"Theta Waves","text":""},{"location":"KB/Theta%20Waves/#theta-waves","title":"Theta Waves","text":"<ul> <li>4-9 Hz theta</li> <li>Memory/Decision</li> <li></li> </ul>"},{"location":"KB/Threaded%20Cognition/","title":"Threaded Cognition","text":""},{"location":"KB/Threaded%20Cognition/#threaded-cognition","title":"Threaded Cognition","text":"<ul> <li>Aka no real \"switch\" between one task to next but bottlenecks</li> <li>Cognition is Threaded but overlaps in the sense that different areas can not be accessed if it uses same areas as the other task at the same time</li> <li>If no expertise, consult Declarative memory all the time. Until it's not required anymore if in long term store.</li> </ul>"},{"location":"KB/Thrombosis/","title":"Thrombosis","text":""},{"location":"KB/Thrombosis/#thrombosis","title":"Thrombosis","text":"<ul> <li>A blood clot that forms inside a blood vessel restricting blood flow</li> </ul>"},{"location":"KB/Through-beam/","title":"Through beam","text":""},{"location":"KB/Through-beam/#through-beam","title":"Through-beam","text":"<ul> <li>An object detection system used within a robot's imaging sensor system. A finely focused beam of light is mounted at one end and a detector at the other. When the beam of light is broken, an object is sensed.</li> </ul>"},{"location":"KB/Time%20Dependant%20Vector%20Field/","title":"Time Dependant Vector Field","text":""},{"location":"KB/Time%20Dependant%20Vector%20Field/#time-dependant-vector-field","title":"Time Dependant Vector Field","text":"<ul> <li>Lagrangian Coherent Structure</li> </ul>"},{"location":"KB/Time%20Measuring%20Function/","title":"Time Measuring Function","text":""},{"location":"KB/Time%20Measuring%20Function/#time-measuring-function","title":"Time Measuring Function","text":"<ul> <li>Time measuring function measures the execution time for the specified section in the job or the signal output time of the specified signal.</li> </ul>"},{"location":"KB/Time%20space%20duality/","title":"Time space duality","text":""},{"location":"KB/Time%20space%20duality/#time-space-duality","title":"Time Space Duality","text":"<ul> <li>Array : Instruction operates on multiple data elements at the same time</li> <li>Vector : Instruction operates on multiple data elements in consecutive time steps</li> </ul>"},{"location":"KB/Time/","title":"Time","text":""},{"location":"KB/Time/#time","title":"Time","text":"<ul> <li> \\[\\Delta t = t_{f}-t_{i}\\] </li> </ul>"},{"location":"KB/TinyBERT/","title":"TinyBERT","text":""},{"location":"KB/TinyBERT/#tinybert","title":"TinyBERT","text":"<ul> <li>TinyBERT: Distilling BERT for Natural Language Understanding</li> <li>novel Transformer distillation method to accelerate inference and reduce model size while maintaining accuracy</li> <li>specially designed for knowledge distillation (KD) of the Transformer-based models</li> <li>plenty of knowledge encoded in a large teacher BERT can be effectively transferred to a small student Tiny-BERT</li> <li>GLUE</li> </ul>"},{"location":"KB/Token%20Embedding/","title":"Token Embedding","text":""},{"location":"KB/Token%20Embedding/#token-embedding","title":"Token Embedding","text":"<ul> <li>WordPiece Tokenizer</li> </ul>"},{"location":"KB/Tokenizer/","title":"Tokenizer","text":""},{"location":"KB/Tokenizer/#tokenizer","title":"Tokenizer","text":"<ul> <li>Tokenizer expands the contraction to recover the essential grammatical features of the pronoun and the Verb.</li> <li>Space-delimited languages</li> <li>White space delimited tokens may not be the valid token</li> <li>Chinese and Thai<ul> <li>Words are written in succession with no indication of word boundaries</li> </ul> </li> <li>Word Structure</li> <li>Punctuation</li> </ul>"},{"location":"KB/Top%20Down%20Parsing/","title":"Top Down Parsing","text":""},{"location":"KB/Top%20Down%20Parsing/#top-down-parsing","title":"Top Down Parsing","text":""},{"location":"KB/Tourette%E2%80%99s%20Syndrome/","title":"Tourette\u2019s Syndrome","text":""},{"location":"KB/Tourette%E2%80%99s%20Syndrome/#tourettes-syndrome","title":"Tourette\u2019s Syndrome","text":"<ul> <li>A neurological disorder, beginning in childhood, characterized by repetitive, involuntary movements or vocalizations, called tics.</li> </ul>"},{"location":"KB/Towards%20A%20Rigorous%20Science%20of%20Interpretable%20Machine%20Learning/","title":"Towards A Rigorous Science of Interpretable Machine Learning","text":""},{"location":"KB/Towards%20A%20Rigorous%20Science%20of%20Interpretable%20Machine%20Learning/#towards-a-rigorous-science-of-interpretable-machine-learning","title":"Towards A Rigorous Science of Interpretable Machine Learning","text":"<ul> <li>@doshi-velezRigorousScienceInterpretable2017</li> </ul>"},{"location":"KB/Tower/","title":"Tower","text":""},{"location":"KB/Tower/#tower","title":"Tower","text":"<ul> <li>A component of a deep neural network that is itself a deep neural network without an output layer. Typically, each tower reads from an independent data source. Towers are independent until their output is combined in a final layer.</li> </ul>"},{"location":"KB/Traces%20of%20times%20past%20Representations%20of%20temporal%20intervals%20in%20memory/","title":"Traces of times past Representations of temporal intervals in memory","text":""},{"location":"KB/Traces%20of%20times%20past%20Representations%20of%20temporal%20intervals%20in%20memory/#traces-of-times-past-representations-of-temporal-intervals-in-memory","title":"Traces of times past Representations of temporal intervals in memory","text":"<ul> <li> <p>Niels Taatgen &amp; Hedderik van Rijn</p> </li> <li> <p>The results show that the adjustment of one interval carried over to the other interval, indicating that subjects were not able to completely separate the two representations</p> </li> <li>assumes that the representation of an interval is based on a pool of recent experiences</li> </ul>"},{"location":"KB/Traces%20of%20times%20past%20Representations%20of%20temporal%20intervals%20in%20memory/#representations-of-time","title":"Representations of time","text":"<ul> <li>The distinction between a solid memory representation for both durations, on the one hand, and a \"pool of experiences,\" on the other, does not need to be problematic for the comparison process, if one assumes that both approaches eventually result in the retrieval of a single representation that can be compared to the current clock value.</li> </ul>"},{"location":"KB/Traces%20of%20times%20past%20Representations%20of%20temporal%20intervals%20in%20memory/#bayesian-modeling-approach-by-jazayeri-and-shadlen-2010","title":"Bayesian modeling approach by Jazayeri and Shadlen (2010)","text":"<ul> <li>The assumption of their model is that humans take two factors into account when estimating the duration of an interval</li> <li>One of these factors is the temporal context: What is the range of possible durations for an interval? The second factor is the (self-)knowledge about the imprecision involved in estimating this interval</li> <li>Visual inspection of the results suggests, indeed, that in the FF condition the short interval was estimated as longer and the long interval as shorter, consistent with earlier findings (e.g., Grondin, 2005), and suggesting that both estimates influence each other.</li> <li>estimates of the short interval were influenced by changes in the duration of the long interval, because the short interval's estimations resemble a dampened pattern of the long interval</li> <li>clear interaction between condition and range</li> <li>Linear mixed-effect models provided information about the contributions of individual factors to a dependent variable and about the reliability of the estimates.</li> <li>Previous feedback also modifies the interval: If the feedback on the previous short interval was \"too short,\" the estimate is increased by 92 ms, but when it was \"too long,\" it is decreased the current estimate by 106 ms</li> <li>representation of an interval is the result of a pool of recent experiences and not of a single representation</li> <li>This is indicated by the relatively small intercepts of the regression formula and the susceptibility of the estimates to changes in the other interval</li> </ul>"},{"location":"KB/Traces%20of%20times%20past%20Representations%20of%20temporal%20intervals%20in%20memory/#general-discussion","title":"General discussion","text":"<ul> <li>The results not only show that the representations of two intervals tend to shift toward each other, but also that a change in the duration of one interval not only affects the representation of that interval, but also the representation of the unchanged interval.</li> <li>These findings support a model in which the representation of a time interval is not a single memory trace, but a pool of experiences in which recency and match to the current request determine the impact of single experiences.</li> <li>The main specific choice we made in this model was to treat every experience with each of the intervals as a separate memory trace</li> <li>However, we could not come up with a modification of the perturbation system that could (1) produce relatively stable performance for both durations, (2) adjust itself to changes in the standard, and (3) show influences of the changed long interval on the short interval.</li> <li>As already mentioned in the introduction, the fit to the data does not hinge on the linear or nonlinear representation of time in the clock component\u2014as long as a clock component produces temporal information, the pool model would be able to produce new temporal estimates</li> <li>a combination of a memory system and a time estimation system can explain the data discussed in this study, despite the fact that neither system was specifically designed for these experiments.</li> </ul>"},{"location":"KB/Traces%20of%20times%20past%20Representations%20of%20temporal%20intervals%20in%20memory/#images","title":"Images","text":""},{"location":"KB/Tracking%20the%20Continuity%20of%20Language%20Comprehension%20Computer%20Mouse%20Trajectories%20Suggest%20Parallel%20Syntactic%20Processing/","title":"Tracking the Continuity of Language Comprehension Computer Mouse Trajectories Suggest Parallel Syntactic Processing","text":""},{"location":"KB/Tracking%20the%20Continuity%20of%20Language%20Comprehension%20Computer%20Mouse%20Trajectories%20Suggest%20Parallel%20Syntactic%20Processing/#tracking-the-continuity-of-language-comprehension-computer-mouse-trajectories-suggest-parallel-syntactic-processing","title":"Tracking the Continuity of Language Comprehension Computer Mouse Trajectories Suggest Parallel Syntactic Processing","text":"<ul> <li>Thomas A. Farmera, Sarah A. Cargilla, Nicholas C. Hindya, Rick Daleb, Michael J. Spiveya</li> </ul>"},{"location":"KB/Tracking%20the%20Continuity%20of%20Language%20Comprehension%20Computer%20Mouse%20Trajectories%20Suggest%20Parallel%20Syntactic%20Processing/#abstract","title":"Abstract","text":"<ul> <li>Although several theories of online syntactic processing assume the parallel activation of multiple syntactic representations, evidence supporting simultaneous activation has been inconclusive</li> <li>continuous and non-ballistic properties of computer mouse movements are exploited</li> <li>procure evidence regarding parallel versus serial processing</li> <li>Participants heard structurally ambiguous sentences while viewing scenes with properties either supporting or not supporting the difficult modifier interpretation</li> <li>The curvatures of the elicited trajectories revealed both an effect of visual context and graded competition between simultaneously active syntactic representations</li> </ul>"},{"location":"KB/Tracking%20the%20Continuity%20of%20Language%20Comprehension%20Computer%20Mouse%20Trajectories%20Suggest%20Parallel%20Syntactic%20Processing/#introduction","title":"Introduction","text":"<ul> <li>Sentences such as, \"The adolescent hurried through the door tripped,\" are difficult to process because, at least temporarily, multiple possible structural representations exist</li> <li>garden-path effect</li> <li>Syntax First models</li> <li>Multiple constraint-based theories</li> <li>what feel like garden-path effects are due to the incorrect syntactic alternative winning much of the competition during the early portion of the sentence, and then nonconforming information from the latter portion of the sentence inducing a laborious reversal of that activation pattern</li> <li>As a result, one can expect that some garden-path events may be very mild, some moderate, and some extreme such that a wide variety of sentence readings should all belong to one population of events with a relatively continuous distribution.</li> <li>Unrestricted Race Model</li> <li>When ambiguous sentences like 1a are heard in the presence of visual scenes where only one possible referent is present (an apple already on a towel), along with an incorrect destination (an empty towel), and a correct destination (a box), as in the top portion of Fig. 1, about 50% of the time participants fixate the incorrect destination after hearing the first PP.</li> <li>After the second disambiguating PP is heard, eye movements tend to be redirected to the correct referent and then to the correct destination</li> <li>This garden-path effect can, however, be modulated by contextual information contained within the visual scene</li> <li>it seems that when two possible referents are present, an expectation is created that they will be discriminated amongst, thus forcing a modifier interpretation of the</li> <li>ambiguous PP</li> <li>The attenuation of looks to the incorrect destination by the presence of two possible referents, then, is evidence for an early influence of non-syntactic (even nonlinguistic) information on the parsing process and is problematic for traditional syntax-first accounts discussed earlier.</li> <li>However, because saccadic eye movements are generally ballistic, they either send the eyes to fixate an object associated with a garden-path interpretation or they do not.</li> <li>The evidence from this paradigm, therefore, is also consistent with the Unrestricted Race model, where the various constraints are combined immediately, but on any given trial only one syntactic representation is initially pursued</li> <li>across experimental trials, distributions of eye-movement patterns are almost always bimodal because the fixations are coded as binomial</li> <li>There are saccades to locations on the display corresponding to either one of the possible representations, but almost never to a blank region in between those two potential targets</li> <li>In the following experiment, we examined the dynamics of hand movement in the same sentence comprehension scenario with the goal of determining whether the non-ballistic, continuous nature of computer mouse trajectories can serve to tease apart these two remaining theoretical accounts.</li> </ul>"},{"location":"KB/Tracking%20the%20Continuity%20of%20Language%20Comprehension%20Computer%20Mouse%20Trajectories%20Suggest%20Parallel%20Syntactic%20Processing/#experiment","title":"Experiment","text":"<ul> <li>computer mouse movements can serve as an informative indicator of the cognitive processes underlying spoken-word recognition (Spivey, Grosjean, &amp; Knoblich, 2005)</li> <li>In addition, whereas self-paced reading affords 2 to 3 data points (button presses) per second, and eye-movement data allow for approximately 3 to 4 data points (saccades) per second, \"mouse tracking\" yields somewhere between 30 and 60 data points per second, depending on the sampling rate of the software used.</li> <li>The context and garden-path effects reported in the visual world paradigm are highly replicable when tracking eye movements</li> <li>As such, recording mouse movements in the visual world paradigm can serve as a strong test case by which to evaluate the efficacy of the mouse-tracking procedure for the study of language processing in real time.</li> </ul>"},{"location":"KB/Tracking%20the%20Continuity%20of%20Language%20Comprehension%20Computer%20Mouse%20Trajectories%20Suggest%20Parallel%20Syntactic%20Processing/#expected-prediction","title":"Expected Prediction","text":"<ul> <li>1) Averaged trajectories recorded in response to ambiguous sentences in the onereferent context should show significantly more curvature toward the incorrect destination than the averaged trajectories elicited by unambiguous sentences\u2014a pattern corresponding to the garden-path effect. 2)</li> <li>The curvature of averaged trajectories in the two referent condition should not differ statistically between ambiguous and unambiguous sentences, thus demonstrating an influence of referential context on the garden-path effect.</li> <li>The second purpose of this study, then, was to exploit the continuity of the mousemovement trajectories to discriminate between these two remaining theoretical accounts</li> <li>a measure of curvature magnitude was used to determine the amount of spatial attraction toward the incorrect destination that was exhibited by the ambiguousand unambiguous-sentence trajectories in the one-referent context.</li> <li>If only one representation were active at any one time, as the unrestricted race account predicts, then the trial-by-trial distribution of trajectory curvatures in the ambiguous-sentence condition should be either (a) bimodal\u2014comprised of highly curved garden-path movements and noncurved, correct-interpretation movements, or (b) uniformly in the more extreme curved range, indicating that almost every trial exhibited a garden-path effect</li> </ul>"},{"location":"KB/Tracking%20the%20Continuity%20of%20Language%20Comprehension%20Computer%20Mouse%20Trajectories%20Suggest%20Parallel%20Syntactic%20Processing/#method","title":"Method","text":""},{"location":"KB/Tracking%20the%20Continuity%20of%20Language%20Comprehension%20Computer%20Mouse%20Trajectories%20Suggest%20Parallel%20Syntactic%20Processing/#participants","title":"Participants","text":"<ul> <li>Forty right-handed, native English-speaking undergraduates from Cornell University participated in the study for extra credit in psychology courses</li> <li>only right-handed individuals to avoid variability associated with subtle kinematic differences in leftward and rightward movement of the left versus the right arms.</li> </ul>"},{"location":"KB/Tracking%20the%20Continuity%20of%20Language%20Comprehension%20Computer%20Mouse%20Trajectories%20Suggest%20Parallel%20Syntactic%20Processing/#materials-and-procedures","title":"Materials and Procedures","text":"<ul> <li>Sixteen experimental items 102 filler sentences</li> <li>The unambiguous version (1b) of each of the 16 experimental items was recorded first, and then the \"that\" was removed to produce the ambiguous (1a) sentence condition</li> <li>Each visual context corresponding to the 16 experimental items was varied to produce a oneand two-referent condition</li> <li>The one-referent visual context (illustrated in Fig. 1, top) contained the target referent (an apple on a towel), an incorrect destination (a second towel), the correct destination (a box), and a distracter object (a flower). In the two-referent context, all items were the same except that the distracter object was replaced with a second possible referent (such as an apple on a napkin). Twenty-four filler scenes, designed to accompany filler sentences, were also constructed.</li> <li>In critical trials for both the oneand two-referent conditions, the target referent always appeared in the top left corner of the screen, the incorrect destination always appeared in the top right corner of the screen, and the correct destination</li> <li>was always located at the bottom right portion of the screen.</li> <li>Given that the scene layout was held constant across all items in each experimental condition, a left-to-right movement was always necessary</li> <li>Although there could exist a systematic bias toward specific locations in the display when moving rightward, this was viewed as unproblematic given that the bias would be held constant across both the ambiguous and unambiguous sentences, which were directly compared in all statistical analyses, for each context.</li> <li>In each scene, participants saw four to six color images, depending on how many objects were needed for the scene</li> </ul>"},{"location":"KB/Tracking%20the%20Continuity%20of%20Language%20Comprehension%20Computer%20Mouse%20Trajectories%20Suggest%20Parallel%20Syntactic%20Processing/#results","title":"Results","text":""},{"location":"KB/Tracking%20the%20Continuity%20of%20Language%20Comprehension%20Computer%20Mouse%20Trajectories%20Suggest%20Parallel%20Syntactic%20Processing/#data-screening-and-coding","title":"Data Screening and Coding","text":"<ul> <li>A trajectory was considered valid and submitted to further analysis if it was initiated at the top left quadrant of the display and terminated in the bottom right quadrant, indicating that the correct referent had been picked up and then placed at the correct destination. This screening procedure resulted in 27 deleted trials, accounting for less than 5% of all experimental trials.</li> <li>To make sure that trajectories in one condition were not initiated (or that objects were not grabbed) at a systematically different region of the display than in the other conditions, we conducted two 2 (Context) \u00d7 2 (Ambiguity) ANOVAs on the x and y coordinates, separately.</li> <li>There was no significant main effect or interaction for either the x or the y coordinates (all ps were nonsignificant) indicating that, across conditions, the trajectories were initiated at approximately the same location of the display</li> <li>Subsequently, all analyzable trajectories were \"time normalized\" to 101 timesteps by a procedure described in Spivey et al. (2005) and Dale et al. (2007).</li> </ul>"},{"location":"KB/Tracking%20the%20Continuity%20of%20Language%20Comprehension%20Computer%20Mouse%20Trajectories%20Suggest%20Parallel%20Syntactic%20Processing/#context-and-garden-path-effects","title":"Context and Garden-path Effects","text":"<ul> <li>The mean trajectories from ambiguous and unambiguous sentences in the onereferent context, illustrated in Fig. 1 (top), demonstrate that the average ambiguoussentence trajectory was more curved toward the incorrect destination than the average trajectory elicited by the unambiguous sentences</li> <li>Thus, in the presence of the garden-path effect, it seems clear that there exists more spatial attraction toward the incorrect destination for the ambiguous sentences.</li> <li>In addition, in line with the time-normalized analyses presented above, none of the</li> <li>eight time bins in the two-referent context showed the ambiguousand unambiguous-sentence trajectories significantly diverging for either the x or the y coordinates.</li> </ul>"},{"location":"KB/Tracking%20the%20Continuity%20of%20Language%20Comprehension%20Computer%20Mouse%20Trajectories%20Suggest%20Parallel%20Syntactic%20Processing/#serial-versus-parallel-activation","title":"Serial Versus Parallel Activation","text":"<ul> <li>garden-path trials and some non-garden-path trials, the majority of the trajectories elicited in this condition fell somewhere in between those two extremes, forming a single population of non-, somewhat-, and highly curved responses.</li> <li>To determine whether any bimodality is present in the distribution of responses, we computed the area under the curve on a trial-by-trial basis</li> <li>The b value for each distribution is less than .555, indicating no presence of bimodality within the distributions.</li> <li>Notably, with regard to the distribution of responses in the one-referent, ambiguous-sentence condition, b &lt; .555 indicates that the graded spatial attraction effects elicited in this condition came not from two different types of trials but from a single population of trials.</li> <li>Finally, one might argue that bimodality was not detected (thus, b &lt; .555) in the crucial one-referent, ambiguous-sentence condition due to a lack of statistical power resulting from the relatively small number of trials in the garden-path distribution.</li> <li>To address this concern, we created an artificial distribution with a sample size almost identical to our crucial gardenpath distribution by randomly sampling 50% of the trials from the one-referent, ambiguoussentence condition (where garden pathing was observed) and 50% of the trials from the onereferent, unambiguoussentence condition.</li> <li>By examining the distributional properties of the area-under-the-curve values produced by the garden-path and non-garden-path trials together, we can thus determine whether the bimodality statistic (b) we used to assess the bimodality of the garden-path distribution (above) is capable of detecting bimodality in a case where the response distribution should clearly be bimodal</li> </ul>"},{"location":"KB/Tracking%20the%20Continuity%20of%20Language%20Comprehension%20Computer%20Mouse%20Trajectories%20Suggest%20Parallel%20Syntactic%20Processing/#general-discussion","title":"General Discussion","text":"<ul> <li>In the one-referent context, participants' mouse movements in response to the ambiguous sentences curved significantly closer to the top right of the screen (toward the incorrect destination) than in response to unambiguous sentences.</li> <li>Thus, it would seem that when only one referent was present, the incorrect destination (e.g., the towel) was partially considered relevant, until disambiguating information was processed\u2014a trend corresponding to the garden-path effect associated with this condition.</li> <li>The fact that most mouse trajectories began while the speech file was still being heard suggests that the effect of visual context modulating the garden path took place during early moments of processing the linguistic input, not during a second stage of syntactic reanalysis.</li> <li>In addition, by capitalizing on the continuous, non-linear, and non-ballistic properties of trajectories produced by computer mouse movements, mouse tracking has the potential to answer questions that have been difficult to answer with more traditional methodologies.</li> <li>What does distinguish between these two accounts is the gradiency observed in the curvature of the trajectories in the garden-path condition</li> <li>If the Unrestricted Race model posits that only one syntactic representation is pursued at any one time, then it must predict that mouse movements in a gardenpath condition should initially move either in the direction of the correct destination or in the direction of the incorrect destination (producing either a bimodal distribution or an all-curved distribution)</li> <li>In contrast, because the constraintbased account posits simultaneous graded activation of multiple syntactic alternatives, it predicts that mouse movements can move in directions that are dynamically weighted combinations of the two competing destinations (producing a unimodal distribution of moderate curvatures).</li> <li>Fig. 4 shows that although approximately 5% of the trajectories moved all the way to the incorrect destination before changing direction, the vast majority of the trajectories responsible for the mean curvature were unmistakably graded in their degree of spatial attraction toward the incorrect destination.</li> <li>The lack of bimodality in the distribution of trial-by-trial trajectory curvatures suggests that the garden-path effect so frequently associated with this manipulation is not an all-or-none phenomenon\u2014that is, the activation of one structural representation does not forbid simultaneous activation of other possible representations</li> <li>Through a large-scale survey of children's computer use, for example, Calvert, Rideout, Woolard, Barr, and Strouse (2005) found that the mean age at which a child was able to point and click a computer mouse was 3.5 years, and that the mean age of the onset of autonomous computer use was 3.7 years</li> <li>we believe mouse tracking can serve as \"the poor man's eye tracker,\" providing detailed indices of cognitive processing to laboratories that cannot afford expensive eye-tracking equipment.</li> </ul>"},{"location":"KB/Tracking%20the%20Continuity%20of%20Language%20Comprehension%20Computer%20Mouse%20Trajectories%20Suggest%20Parallel%20Syntactic%20Processing/#pictures","title":"Pictures","text":""},{"location":"KB/Tractability/","title":"Tractability","text":""},{"location":"KB/Tractability/#tractability","title":"Tractability","text":"<ul> <li>Let X be the input and Z be the latent representation of X. Every generative model makes the assumption that it's tractable to compute the probability P(Z | X).</li> </ul>"},{"location":"KB/Training%20Trajectories/","title":"Training Trajectories","text":""},{"location":"KB/Training%20Trajectories/#training-trajectories","title":"Training Trajectories","text":""},{"location":"KB/Training-serving%20Skew/","title":"Training serving Skew","text":""},{"location":"KB/Training-serving%20Skew/#training-serving-skew","title":"Training-serving Skew","text":"<ul> <li>The difference between a model's performance during training and that same model's performance during serving.</li> </ul>"},{"location":"KB/Trajectory%20Planning/","title":"Trajectory Planning","text":""},{"location":"KB/Trajectory%20Planning/#trajectory-planning","title":"Trajectory Planning","text":"<ul> <li>Scheduled motion to follow, including time information</li> <li>After finding a path, the trajectory definition is completed by the choice of a timing law</li> </ul>"},{"location":"KB/Trajectory%20Plotting%20with%20PCA/","title":"Trajectory Plotting with PCA","text":""},{"location":"KB/Trajectory%20Plotting%20with%20PCA/#trajectory-plotting-with-pca","title":"Trajectory Plotting with PCA","text":"<ul> <li>The problem: the path lies in a low-dimensional space.</li> <li>The solution: PCA</li> <li>Construct a matrix $$ M = [\\theta_{0}- \\theta_{n};...;\\theta_{n-1}-\\theta_{n}] $$</li> <li>where,</li> <li>\\(\\theta_{i}\\) is the model params during epoch i</li> <li>n is number of epochs</li> <li>apply PCA to M and use 2 most explanatory directions</li> <li></li> </ul>"},{"location":"KB/Trajectory/","title":"Trajectory","text":""},{"location":"KB/Trajectory/#trajectory","title":"Trajectory","text":"<ul> <li>In reinforcement learning, a sequence of tuples that represent a sequence of state transitions of the agent, where each tuple corresponds to the state, action, reward, and next state for a given state transition.</li> </ul>"},{"location":"KB/Transcranial%20Electrical%20Stimulation%20%28tDCS%20and%20tACS%29/","title":"Transcranial Electrical Stimulation (tDCS and tACS)","text":""},{"location":"KB/Transcranial%20Electrical%20Stimulation%20%28tDCS%20and%20tACS%29/#transcranial-electrical-stimulation-tdcs-and-tacs","title":"Transcranial Electrical Stimulation (tDCS and tACS)","text":"<ul> <li>A non-invasive procedure that applies electrical stimulation to the scalp to increase or decrease neural signaling. The two main types are direct current stimulation (tDCS) and alternating current stimulation (tACS). They are used for therapeutic purposes as well as to study cognitive processing.</li> </ul>"},{"location":"KB/Transcranial%20Magnetic%20Stimulation%20%28TMS%29/","title":"Transcranial Magnetic Stimulation (TMS)","text":""},{"location":"KB/Transcranial%20Magnetic%20Stimulation%20%28TMS%29/#transcranial-magnetic-stimulation-tms","title":"Transcranial Magnetic Stimulation (TMS)","text":"<ul> <li>A non-invasive procedure that uses the energy from a strong magnet to stimulate changes in neural processing from above the scalp. It is used as a treatment for depression as well as a research method to investigate cognitive processes.</li> </ul>"},{"location":"KB/Transducer/","title":"Transducer","text":""},{"location":"KB/Transducer/#transducer","title":"Transducer","text":"<ul> <li>A device that converts energy from one form to another. Generally, a device that converts an input signal into an output signal of a different form. It can also be thought of as a device which converts static signals detected in the environment (such as pressure) into an electrical signal that is sent to a robot's control system.</li> </ul>"},{"location":"KB/Transfer%20Function/","title":"Transfer Function","text":""},{"location":"KB/Transfer%20Function/#transfer-function","title":"Transfer Function","text":"<ul> <li>Map a scalar to color and opacity</li> <li>Determines which features of the data are visible / highlighted</li> <li>Can be stored inside a color lookup table (LUT)</li> <li></li> <li></li> <li>Opacity Correction</li> </ul>"},{"location":"KB/Transfer%20Learning%20or%20%20Self-supervised%20Learning%3F%20A%20Tale%20of%20%20Two%20Pretraining%20Paradigms/","title":"Transfer Learning or  Self-supervised Learning? A Tale of  Two Pretraining Paradigms","text":""},{"location":"KB/Transfer%20Learning%20or%20%20Self-supervised%20Learning%3F%20A%20Tale%20of%20%20Two%20Pretraining%20Paradigms/#transfer-learning-or-self-supervised-learning-a-tale-of-two-pretraining-paradigms","title":"Transfer Learning or Self-supervised Learning? A Tale of Two Pretraining Paradigms","text":"<ul> <li>@yangTransferLearningSelfsupervised2020</li> <li>[Transfer Learning]  vs Self Supervised </li> </ul>"},{"location":"KB/Transfer%20Learning%20or%20%20Self-supervised%20Learning%3F%20A%20Tale%20of%20%20Two%20Pretraining%20Paradigms/#comparison-using","title":"Comparison Using","text":"<ul> <li>5 different image-based source domains</li> <li>4 target tasks<ol> <li>from daily-life objects</li> <li>general scenes</li> <li>nature</li> <li>medical pictures areas.</li> </ol> </li> <li> <p>Four different experimental setups </p> </li> <li> <p>Effect of domain difference between source and target task</p> </li> <li>Effect of amount of pretraining data</li> <li>Effect of class imbalance in source data</li> <li>Effect of using target data for additional pretraining</li> <li>ResNet-50</li> </ul>"},{"location":"KB/Transfer%20Learning%20or%20%20Self-supervised%20Learning%3F%20A%20Tale%20of%20%20Two%20Pretraining%20Paradigms/#results","title":"Results","text":""},{"location":"KB/Transfer%20Learning%20or%20%20Self-supervised%20Learning%3F%20A%20Tale%20of%20%20Two%20Pretraining%20Paradigms/#domain-difference","title":"Domain Difference","text":"<ul> <li>Large<ul> <li>SSL outperforms TL</li> </ul> </li> <li>Small<ul> <li>TL outperforms SSL</li> <li>SSL is less sensitive to domain difference than TL.</li> </ul> </li> </ul>"},{"location":"KB/Transfer%20Learning%20or%20%20Self-supervised%20Learning%3F%20A%20Tale%20of%20%20Two%20Pretraining%20Paradigms/#amount-of-pretraining-data","title":"Amount of Pretraining Data","text":"<ul> <li>Small(for same source task)<ul> <li>SSL outperforms TL</li> </ul> </li> <li>Large(for same source task)<ul> <li>TL outperforms SSL</li> <li>SSL is less sensitive to amount of pretraining data than TL, when domain difference is small</li> </ul> </li> </ul>"},{"location":"KB/Transfer%20Learning%20or%20%20Self-supervised%20Learning%3F%20A%20Tale%20of%20%20Two%20Pretraining%20Paradigms/#class-imbalance-source-data","title":"Class Imbalance (Source Data)","text":"<ul> <li>SSL is more robust to class imbalance than TL</li> </ul>"},{"location":"KB/Transfer%20Learning%20or%20%20Self-supervised%20Learning%3F%20A%20Tale%20of%20%20Two%20Pretraining%20Paradigms/#additional-pretraining","title":"Additional Pretraining","text":"<ul> <li>For SSL, using target task for additional pretraining works better vs using only source data, but not for TL.</li> </ul>"},{"location":"KB/Transfer%20Learning%20or%20%20Self-supervised%20Learning%3F%20A%20Tale%20of%20%20Two%20Pretraining%20Paradigms/#what-is-left","title":"What is Left","text":"<ul> <li>can be extended to other forms of data including speech, signals and text</li> <li>Only used ResNet architecture, need to investigate other architectures</li> <li>Image Transformers etc. are not considered</li> <li>Correlation between performance and factors is studied and potential reasons behind it are discussed, a deeper investigation of these potential reasons might be beneficial</li> </ul>"},{"location":"KB/Transfer%20Learning/","title":"Transfer Learning","text":""},{"location":"KB/Transfer%20Learning/#transfer-learning","title":"Transfer Learning","text":"<ul> <li>Transfer learning involves extrapolating a reward function for a new environment based on reward functions from many similar environments that might then transfer in a wrong way</li> <li>Uses source task with human assigned labels </li> <li>Biased to human assigned labels, more discriminative</li> </ul>"},{"location":"KB/Transfer%20Learning/#refs","title":"Refs","text":"<ul> <li>openai</li> </ul>"},{"location":"KB/Transferability/","title":"Transferability","text":""},{"location":"KB/Transferability/#transferability","title":"Transferability","text":"<ul> <li>Explainability is also an advocate for transferability, since it may ease the task of elucidating the boundaries that might affect a model, allowing for a better understanding and implementation</li> <li>the mere understanding of the inner relations taking place within a model facilitates the ability of a user to reuse this knowledge in another problem</li> <li>cases in which the lack of a proper understanding of the model might drive the user toward incorrect assumptions and fatal consequences</li> </ul>"},{"location":"KB/Transferred%20compact%20convolutional%20filters/","title":"Transferred compact convolutional filters","text":""},{"location":"KB/Transferred%20compact%20convolutional%20filters/#transferred-compact-convolutional-filters","title":"Transferred Compact Convolutional Filters","text":"<ul> <li>These methods remove inessential parameters by transferring or compressing the convolutional filters (Zhai et al., 2016).</li> </ul>"},{"location":"KB/Transformer%20Physics/","title":"Transformer Physics","text":""},{"location":"KB/Transformer%20Physics/#transformer-physics","title":"Transformer Physics","text":"<ul> <li>ratio of the voltages across the coils of a transformer = the ratio of the turns on the coils</li> <li> \\[\\frac{V_{1}}{V_{2}}= \\frac{N_{1}}{N_{2}}\\] </li> </ul>"},{"location":"KB/Transformer-XL/","title":"Transformer-XL","text":""},{"location":"KB/Transformer-XL/#transformer-xl","title":"Transformer-XL","text":"<ul> <li>Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context</li> <li>Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling</li> <li>learning dependency beyond a fixed length without disrupting temporal coherence</li> <li>segment-level recurrence mechanism and a novel positional encoding scheme</li> <li>resolves the context fragmentation problem</li> <li>enwiki8</li> <li>WikiText</li> <li>One Billion Word</li> <li>Penn Treebank</li> </ul>"},{"location":"KB/Transformer/","title":"Transformer","text":""},{"location":"KB/Transformer/#transformer","title":"Transformer","text":"<ul> <li>Encoder Decoder</li> <li>Auto regressive : decoder outputs fed back as inputs to decoder</li> <li>Decoder can access not only the hidden step of the last time step from the encoder, but all the hidden states from the encoder</li> <li>During decoding, consider pairwise relationshop between decoder state and all the returned states from the encoder<ul> <li>Some words relevant, others are not</li> </ul> </li> <li>Transform all hidden states from the encoder into context vectors, that shows how the decoding step is relevant to the input sequences</li> <li>Attention</li> <li>Basic Transformer</li> </ul>"},{"location":"KB/Transformer/#nice-little-blogs","title":"Nice Little Blogs","text":"<ul> <li>lillog</li> </ul>"},{"location":"KB/Transitional%20probabilities/","title":"Transitional probabilities","text":""},{"location":"KB/Transitional%20probabilities/#transitional-probabilities","title":"Transitional Probabilities","text":"<ul> <li> \\[\\text{Prob of }Y|X = \\frac{\\text{freq of} XY}{\\text{freq of }X}\\] </li> <li>Longer listening times for non-words indicates recognition of words</li> <li>Transitional probabilities can be high within words, and low at word boundaries as in Aslin et al</li> <li>But then you can also manipulate the frequency at which each word occurs, and in doing so, also the frequency of the syllables</li> <li>Graf Estes et al</li> <li>Transition frequencies can be made high because two words occur very often next to each other</li> </ul>"},{"location":"KB/Transitive%20verb/","title":"Transitive verb","text":""},{"location":"KB/Transitive%20verb/#transitive-verb","title":"Transitive Verb","text":"<ul> <li>a verb with a direct noun object</li> <li>I cooked a duck belonging to her</li> </ul>"},{"location":"KB/Translational%20Invariance/","title":"Translational Invariance","text":""},{"location":"KB/Translational%20Invariance/#translational-invariance","title":"Translational Invariance","text":"<ul> <li>In an image classification problem, an algorithm's ability to successfully classify images even when the position of objects within the image changes. For example, the algorithm can still identify a dog, whether it is in the center of the frame or at the left end of the frame.</li> </ul>"},{"location":"KB/Transparency/","title":"Transparency","text":""},{"location":"KB/Transparency/#transparency","title":"Transparency","text":"<ul> <li>a model is considered to be transparent if by itself it is understandable. Since a model can feature different degrees of understandability, transparent models in Section 3 are divided into three categories: simulatable models, decomposable models and algorithmically transparent models</li> <li>understandability is a two-sided matter: model understandability and human understandability in @gunningExplainableArtificialIntelligence</li> </ul>"},{"location":"KB/Transposed%20Conv/","title":"Transposed Conv","text":""},{"location":"KB/Transposed%20Conv/#transposed-conv","title":"Transposed Conv","text":"<ul> <li>use a transformation going in the opposite direction of a normal convolution, i.e., from something that has the shape of the output of some convolution to something that has the shape of its input while maintaining a connectivity pattern that is compatible with said convolution.</li> <li>Upsampling</li> <li>Input i, kernel k, padding p, stride s , \\(\\(o = (i-1) \\times s +k -2p\\)\\)</li> <li>Steps<ul> <li>Calculate new Param's z and p'</li> <li>Between each row and columns of the input, insert z number of zeros.\u00a0This increases the size of the input to\u00a0\\((2*i -1) \\times (2*i -1)\\)</li> <li>Pad the modified image with p' no of zeros</li> <li>Standard conv with stride of 1</li> <li></li> </ul> </li> <li></li> </ul>"},{"location":"KB/Trapezoidal%20Trajectory/","title":"Trapezoidal Trajectory","text":""},{"location":"KB/Trapezoidal%20Trajectory/#trapezoidal-trajectory","title":"Trapezoidal Trajectory","text":""},{"location":"KB/Treemap/","title":"Treemap, Node Link, Stacking","text":""},{"location":"KB/Treemap/#treemap","title":"Treemap","text":""},{"location":"KB/Trees/","title":"Trees","text":""},{"location":"KB/Trees/#trees","title":"Trees","text":"<ul> <li>Decision Trees</li> <li></li> <li>Node Link Diagram</li> </ul>"},{"location":"KB/Triplet%20Loss/","title":"Triplet Loss","text":""},{"location":"KB/Triplet%20Loss/#triplet-loss","title":"Triplet Loss","text":"<ul> <li>Given an achor, pull similar closer and push dissimilar away</li> <li>Face recog FaceNet</li> <li>Anchor, positive sample are neigbors while neg isnt</li> <li></li> <li>For each triplet, this condition must hold $\\(||f(x^a) - f(x^p)||^2 + \\alpha \\gt f(x^a) - f(x^n)||^2\\)</li> <li>\\(\\alpha\\) is a margin b/w positive and neg</li> <li>Loss to minimize $\\(L(\\theta) = \\Sigma_i^n||f(x^a) - f(x^p)||^2 + f(x^a) - f(x^n)||^2 + \\alpha\\)</li> <li>Harmonic Triplet Loss</li> </ul>"},{"location":"KB/Trustworthiness/","title":"Trustworthiness","text":""},{"location":"KB/Trustworthiness/#trustworthiness","title":"Trustworthiness","text":"<ul> <li>However, declaring a model as explainable as per its capabilities of inducing trust might not be fully compliant with the requirement of model explainability</li> <li>Trustworthiness might be considered as the confidence of whether a model will act as intended when facing a given problem</li> <li>it does not imply that every trustworthy model can be considered explainable on its own, nor is trustworthiness a property easy to quantify</li> <li>Trust might be far from being the only purpose of an explainable model since the relation among the two, if agreed upon, is not reciprocal</li> </ul>"},{"location":"KB/Tug%20of%20war%20between%20RAG%20and%20LLM%20prior/","title":"Tug of war between RAG and LLM prior","text":"","tags":["llm"]},{"location":"KB/Tug%20of%20war%20between%20RAG%20and%20LLM%20prior/#tug-of-war-between-rag-and-llm-prior","title":"Tug of War between RAG and LLM prior","text":"<ul> <li>@wuHowFaithfulAre2024</li> </ul>","tags":["llm"]},{"location":"KB/Tug%20of%20war%20between%20RAG%20and%20LLM%20prior/#how-faithful-are-rag-models-quantifying-the-tug-of-war-between-rag-and-llms-internal-prior","title":"How Faithful Are RAG Models? Quantifying the Tug-of-war between RAG and LLMs\\' Internal prior","text":"<ul> <li>(Note : Basically shows that it's not possible for an LLM to fix it's own hallucinations. Kinda like, if you twist the data around, then after a point the model stops relying on what it knows and starts believing in all the nonsense)</li> <li>in cases when the LLM alone incorrectly answers a question, does providing the correct retrieved content always fix the error?</li> <li>in cases where the retrieved content is incorrect, does the LLM know to ignore the wrong information, or does it recapitulate the error?</li> <li>systematically analyze the tug-of-war between a LLM's internal knowledge (i.e. its prior) and the retrieved information in settings when they disagree</li> <li>the more the modified information deviates from the model's prior, the less likely the model is to prefer it</li> <li>The likelihood of the LLM to adhere to the retrieved information presented in context (RAG preference rate) is inversely correlated with the model's confidence in its response without context (its prior probability).</li> <li>LLMs will increasingly revert to their priors when the original context is progressively modified with unrealistic values.</li> </ul>","tags":["llm"]},{"location":"KB/Tug%20of%20war%20between%20RAG%20and%20LLM%20prior/#methods","title":"Methods","text":"<ul> <li>Our main analysis consists of evaluating the RAG question-answering capabilities of GPT-4 when introducing varying levels of perturbations on the RAG documents</li> </ul>","tags":["llm"]},{"location":"KB/Tug%20of%20war%20between%20RAG%20and%20LLM%20prior/#concordance","title":"Concordance","text":"<ul> <li>the agreement between the reference answer generated based on the article content, and the model's answer to the corresponding generated question</li> <li>This is computed for both the model's answer with and without context.</li> </ul>","tags":["llm"]},{"location":"KB/Tug%20of%20war%20between%20RAG%20and%20LLM%20prior/#modifying-the-retrieved-documents","title":"Modifying the Retrieved Documents","text":"<ul> <li>In three datasets with numerical answers (Drug Dosages, Sports Records, Latest News), we produce ten modifications that act as multipliers on the original value: 0.1, 0.2, 0.4, 0.8, 1.2, 1.5, 2.0, 3.0, 5.0, 10.0.</li> <li>for a name like Bob Green, a slight modification implies a small tweak to another real name (Rob Greene), whereas a significant modification produces a similar but fictitious name (Bilgorn Grevalle), and a comical modification is an absurd variant (Blob Lawnface).</li> <li>Because of differences in how each modified fact might appear in the retrieved text, we utilize GPT-4 to generate the perturbed excerpts for drug dosages and news. Each modified fact is replaced in the original retrieved text.</li> </ul>","tags":["llm"]},{"location":"KB/Tug%20of%20war%20between%20RAG%20and%20LLM%20prior/#rag-vs-model-prior-analyses","title":"RAG Vs Model Prior Analyses","text":"<ul> <li>main analysis we perform in this study is comparing the RAG preference of a model against its internal prior</li> <li>The LLM is first queried with a question without context</li> <li>This response and the average probability of the tokens (accessed via the log probs) are referred to as the model's prior response and the prior probability, respectively</li> <li>The LLM is then queried again, this time with the retrieved content present in the prompt.</li> <li>if the response is still the same as the prior response, then the model prefers its prior</li> <li>On the other hand, if the model response aligns with the information present in the retrieved content, then the model prefers RAG</li> <li>For each dataset, the RAG preference rate is computed as the average across all RAG queries.</li> <li>RAG preference rate is compared against two measurements: the prior probability and the deviation from the prior value.</li> </ul>","tags":["llm"]},{"location":"KB/Tug%20of%20war%20between%20RAG%20and%20LLM%20prior/#results","title":"Results","text":"","tags":["llm"]},{"location":"KB/Tug%20of%20war%20between%20RAG%20and%20LLM%20prior/#concordance_1","title":"Concordance","text":"<ul> <li>we observe that the model's prior response only agreed with the reference answer 34.7% on average</li> <li>RAG answers elevated the concordance to 94%</li> <li>in the minority of cases where providing the retrieved content fails to correct the LLM, we find that the model simply responds with its original prior answer about 20% of the time. </li> </ul>","tags":["llm"]},{"location":"KB/Tug%20of%20war%20between%20RAG%20and%20LLM%20prior/#rag-preference-rate-vs-prior-probability","title":"RAG Preference Rate vs. Prior Probability","text":"<ul> <li>The slope indicates the effect of stronger model confidence on the model's preference for the information presented in the retrieved context; we observe different slopes (ranging from -0.1 to -0.45), suggesting that the effectiveness of RAG in different QA domains can be characterized as being relatively susceptible (e.g., with Dates questions) or robust (e.g., with News questions) to the model's internal prior knowledge confidence.</li> </ul>","tags":["llm"]},{"location":"KB/Tug%20of%20war%20between%20RAG%20and%20LLM%20prior/#rag-preference-rate-vs-deviation-from-prior","title":"RAG Preference Rate Vs Deviation from Prior","text":"<ul> <li>as the RAG value diverges from the model's prior, the model is less likely to adopt the RAG value over its own initial response</li> </ul>","tags":["llm"]},{"location":"KB/Tug%20of%20war%20between%20RAG%20and%20LLM%20prior/#effect-of-prompting-technique-on-rag-adherence","title":"Effect of Prompting Technique on RAG Adherence","text":"<ul> <li>We observe lower and steeper drops in RAG adherence with the loose vs strict prompts, suggesting that prompt wording plays a significant factor in controlling RAG adherence.</li> <li>we quantify a tug-of-war between the strength of the model's prior and the rate at which the model</li> <li>adheres to the RAG document's facts</li> </ul>","tags":["llm"]},{"location":"KB/Tuning%20Model%20Flexibility/","title":"Tuning Model Flexibility","text":""},{"location":"KB/Tuning%20Model%20Flexibility/#tuning-model-flexibility","title":"Tuning Model Flexibility","text":"<ul> <li>Class Size</li> <li>Regularization</li> <li>Ridge Regression</li> <li>Adding noise</li> <li>Cross Validation</li> </ul>"},{"location":"KB/Tutor/","title":"Tutor","text":""},{"location":"KB/Tutor/#tutor","title":"Tutor","text":"<ul> <li>However, a tutoring system does not have to replace a teacher or run an after-school remediation session</li> <li>Its role in the student's learning ecology can be anything\u2014a smart piece of paper; an encouraging coach; a stimulating peer, etc. The technology is quite neutral</li> <li>For instance, tutoring systems have been built that do not teach their task domain at all, but instead encourage students to discover its principles (e.g., Shute &amp; Glaser, 1990).</li> </ul>"},{"location":"KB/Two-photon%20Microscopy/","title":"Two photon Microscopy","text":""},{"location":"KB/Two-photon%20Microscopy/#two-photon-microscopy","title":"Two-photon Microscopy","text":"<ul> <li>An advanced microscopy technique that uses fluorescent markers to look at living tissue approximately one millimeter below the skin\u2019s surface.</li> </ul>"},{"location":"KB/Types%20of%20Words/","title":"Types of Words","text":""},{"location":"KB/Types%20of%20Words/#types-of-words","title":"Types of Words","text":"<ul> <li>Content words</li> <li>Function words</li> </ul>"},{"location":"KB/Types%20of%20uncertainty/","title":"Types of Uncertainty","text":""},{"location":"KB/Types%20of%20uncertainty/#types-of-uncertainty","title":"Types of Uncertainty","text":"<ul> <li>Aleatoric</li> <li>Epistemic</li> <li>Predictive Uncertainty</li> </ul>"},{"location":"KB/UCF101/","title":"UCF101","text":""},{"location":"KB/UCF101/#ucf101","title":"UCF101","text":"<ul> <li>UCF101 </li> <li>widely used video dataset for human action recognition </li> <li>13, 370 video clips with more than 27 hours belonging to 101 categories in this dataset </li> <li>spatial resolution of 320 x 240 pixels and 25 FPS frame rate </li> <li>dataset has been widely used for evaluating the performance of human action recognition</li> </ul>"},{"location":"KB/ULMFit/","title":"ULMFit","text":""},{"location":"KB/ULMFit/#ulmfit","title":"ULMFit","text":"<ul> <li>English Wikipedia -&gt; IMDB%20-%3E%20%5B%5BIMDB)] Classifier</li> </ul>"},{"location":"KB/UMA/","title":"UMA","text":""},{"location":"KB/UMA/#uma","title":"UMA","text":"<ul> <li>Uniform memory access</li> <li>SMP</li> <li>Identical processors</li> <li>Identical processors</li> <li>Equal access and access times to memory</li> <li>Cache Coherence</li> </ul>"},{"location":"KB/Ultrasound/","title":"Ultrasound","text":""},{"location":"KB/Ultrasound/#ultrasound","title":"Ultrasound","text":"<ul> <li>Imaging produced by high-frequency sound waves, usually used to view internal organs</li> </ul>"},{"location":"KB/Un-LSTM/","title":"Un-LSTM","text":""},{"location":"KB/Un-LSTM/#un-lstm","title":"Un-LSTM","text":"<ul> <li>Due to the powerful ability of modeling long-term dynamic in videos, LSTM is used in both the encoder and decoder [37]. </li> <li>Since its superior ability to model temporal dynamics, most of them use LSTM or LSTM variant to encode temporal dynamics in videos or to infer the future frames [37], [146], [147], [164], [165] </li> <li>can be employed for self-supervised feature learning without using human- annotations </li> <li>encoder-decoder pipeline in which the encoder to model spatial and temporal features from the given video clips and the decoder to generate future frames based on feature extracted by encoder.</li> </ul>"},{"location":"KB/Unawareness/","title":"Unawareness","text":""},{"location":"KB/Unawareness/#unawareness","title":"Unawareness","text":"<ul> <li>A situation in which sensitive attributes are present, but not included in the training data. Because sensitive attributes are often correlated with other attributes of one\u2019s data, a model trained with unawareness about a sensitive attribute could still have disparate impact with respect to that attribute, or violate other fairness constraints.</li> </ul>"},{"location":"KB/Unbiased%20Look%20at%20Dataset%20Bias/","title":"Unbiased Look at Dataset Bias","text":"<p><code>toc</code> - Unbiased Look at Dataset Bias - Alexei A. Efros Antonio Torralba - @Unbiased look at dataset bias - ## Abstract - some datasets, that started out as data capture efforts aimed at representing the visual world, have become closed worlds unto themselves - despite the best efforts of their creators, the datasets appear to have a strong buildin bias - Of course, much of the bias can be accounted for by the divergent goals of the different datasets: some captured more urban scenes, others more rural landscapes; some collected professional photographs, others the amateur snapshots from the Internet; some focused on entire scenes, others on single objects, etc - Caltech has a strong preference for side views, while ImageNet is into racing cars; PASCAL have cars at noncanonical view-points; SUNS and LabelMe cars appear to be similar, except LabelMe cars are often occluded by small objects, etc - Name That Dataset - ## Measuring Dataset Bias - settle for a few standard checks, a diagnostic of dataset health if you will. - Cross-dataset generalization - KB/Selection Bias.md - Capture bias - Label bias - Negative Set Bias - ## Measuring Dataset's Value - two basic ways of improving the performance - The first solution is to improve the features, the object representation and the learning algorithm for the detector. - The second solution is to simply enlarge the amount of data available for training. - So, what is the value of current datasets when used to train algorithms that will be deployed in the real world? The answer that emerges can be summarized as: \"better than nothing, but not by much\". - ## Discussion - that the reason is not that datasets are bad, but that our object representations and recognition algorithms are terrible and end up over-learning aspects of the visual data that relates to the dataset and not to the ultimate visual task. - In fact, a human learns about vision by living in a reduced environment with many potential local biases and yet the visual system is robust enough to overcome this. - Should we care about the quality of our datasets? If the goal is to reduce computer vision to a set of feature vectors that can be used in some machine learning algorithm, then maybe not. But if the goal is to build algorithms that can understand the visual world, then, having the right datasets will be crucial. - ## Images -  - </p>"},{"location":"KB/Unbiased%20Look%20at%20Dataset%20Bias/#unbiased-look-at-dataset-bias","title":"Unbiased Look at Dataset Bias","text":""},{"location":"KB/Uncertainity%20in%20classification/","title":"Uncertainty Classification","text":""},{"location":"KB/Uncertainity%20in%20classification/#uncertainty-classification","title":"Uncertainty Classification","text":"<ul> <li>Distributions</li> <li>Use Softmax or Sigmoid</li> </ul>"},{"location":"KB/Uncertainity%20in%20regression/","title":"Reg Uncertainty","text":""},{"location":"KB/Uncertainity%20in%20regression/#reg-uncertainty","title":"Reg Uncertainty","text":"<ul> <li>LinearRegression</li> <li>Confidence intervals</li> <li>Prob that output belongs to this interval</li> <li>\\(f(x) \\in [a,b]\\)</li> <li>Mean and Variance</li> <li>\\(f(x) \\pm \\sigma\\)</li> <li>\\(f(x) \\in [f(x) - \\sigma,f(x) + \\sigma]\\)</li> </ul>"},{"location":"KB/Uncertainty/","title":"Uncertainty","text":""},{"location":"KB/Uncertainty/#categories-anchor","title":"categories: ['anchor']","text":"<ul> <li>./Aleatoric.md</li> <li>./Entropy.md</li> <li>./Epistemic.md</li> <li>./Heteroscedatic.md</li> <li>./Homoscedatic.md</li> <li>./Inceptionism.md</li> <li>Interpretability vs Neuroscience.md</li> <li>./LIME.md</li> <li>Predictive Uncertainty.md</li> <li>./SHAP.md</li> <li>Types of uncertainty.md</li> <li>Uncertainity in regression.md</li> </ul>"},{"location":"KB/Understandability/","title":"Understandability","text":""},{"location":"KB/Understandability/#understandability","title":"Understandability","text":"<ul> <li>denotes the characteristic of a model to make a human understand its function \u2013 how the model works \u2013 without any need for explaining its internal structure or the algorithmic means by which the model processes data internally</li> </ul>"},{"location":"KB/Undue%20Inducement/","title":"Undue Inducement","text":""},{"location":"KB/Undue%20Inducement/#undue-inducement","title":"Undue Inducement","text":"<ul> <li>When the value of something received in a clinical trial is so large that the study participant may agree to take risks that are not in their best interests.</li> </ul>"},{"location":"KB/Unet%20Grasping/","title":"Unet Grasping","text":""},{"location":"KB/Unet%20Grasping/#unet-grasping","title":"Unet Grasping","text":"<ul> <li>Y. Li, L. Schomaker, and H. Kasaei. \"Learning to Grasp 3D Objects using Deep Residual U-Nets.\" RO-MAN 2020.</li> <li>Formulate object grasping as a part segmentation problem</li> <li>Detect graspable shape primitives  </li> <li>The gripper is free to approach objects from arbitrary directions.</li> <li></li> <li></li> </ul>"},{"location":"KB/Unet/","title":"Unet","text":""},{"location":"KB/Unet/#unet","title":"Unet","text":"<ul> <li>Skip Connection</li> </ul>"},{"location":"KB/Unicode%2050/","title":"Unicode 5.0","text":""},{"location":"KB/Unicode%2050/#unicode-50","title":"Unicode 5.0","text":"<ul> <li>UNICODE Consortium 2006</li> <li>100,000 distinct characters</li> <li>75 supported scripts</li> <li>UTF-8 Variable Length Character Encoding<ul> <li>1-4 bytes for each character (max )6</li> <li>ASCII requires 1 byte</li> <li>Alphabetic Systems require 2 bytes</li> <li>Chinese \u2013Japanese-Korean \u2013 3 bytes (sometimes 4 bytes)</li> </ul> </li> </ul>"},{"location":"KB/Uniform%20Distribution/","title":"Uniform Distribution","text":""},{"location":"KB/Uniform%20Distribution/#uniform-distribution","title":"Uniform Distribution","text":"<ul> <li>If \\(I = [a_{1}, b_{1}]\\times ..\\times[a_{n}, b_{n}]\\) is n dim interval in \\(\\mathbb{R}^{n}\\)</li> <li>PDF \\(\\(p(x) = \\begin{cases}\\frac{1}{(b_{1}-a_{1})\\cdot\u2026\\cdot(b_{n}, a_{n})} &amp; \\text{if } x \\in I\\\\[2ex]0&amp;\\text{if x }\\notin I \\end{cases}\\)\\)</li> <li>No need to learn, no shape that can be specified</li> </ul>"},{"location":"KB/Uniform%20Sampling/","title":"Uniform Sampling","text":""},{"location":"KB/Uniform%20Sampling/#uniform-sampling","title":"Uniform Sampling","text":"<ul> <li>Random Sampler</li> <li>Uniform Distribution</li> <li>If need to sample from another distribution with a PDF f(x). Can use a uniform Sampler on the distribution [0,1] to indirectly sample from it<ul> <li>Coordinate transform</li> <li>CDF</li> <li>Get a Sampler for by \\(\\(X_{i} = \\varphi^{-1}\\circ U_{i}\\)\\)</li> <li></li> </ul> </li> </ul>"},{"location":"KB/Uniform%20baseline/","title":"Uniform baseline","text":""},{"location":"KB/Uniform%20baseline/#uniform-baseline","title":"Uniform baseline","text":"<ul> <li>This time, the baseline doesn\u2019t require an input image and uses only uniform distribution to generate a baseline</li> <li>The problem with selecting a baseline is not solved, and for any further experiments, the \u201cblack image\u201d baseline is going to be used.</li> </ul>"},{"location":"KB/Unique%20Character%20Set/","title":"Unique Character Set","text":""},{"location":"KB/Unique%20Character%20Set/#unique-character-set","title":"Unique Character Set","text":"<ul> <li>helps in identifying the language</li> <li>Greek or Hebrew</li> </ul>"},{"location":"KB/Universal%20Approximation%20Theorem/","title":"Universal Approximation Theorem","text":""},{"location":"KB/Universal%20Approximation%20Theorem/#universal-approximation-theorem","title":"Universal Approximation Theorem","text":"<ul> <li>What this means that given an x and a y, the NN can identify a mapping between them. \"Approximately\".</li> <li>This is required when we have non linearly separable data.</li> <li>So we take a non linear function, for example the Sigmoid. \\(\\(\\frac{1}{1 + e^{ - \\left( w^{T}x + b \\right)}}\\)\\).</li> <li>Then we have to combine multiple such neurons in a way such that we can accurately model our problem. The end result is a complex function and the existing weights are distributed across many Layers.</li> <li> <p>The Universal approximation theorem states that     &gt; a feed forward network with a single hidden layer containing a finite number of neurons can approximate continuous functions on compact subsets of \\(\\mathbb{R}\\) , under mild assumptions on the activation function.</p> </li> <li> <p>a feed forward network : take an input, apply a function, get an output, repeat</p> </li> <li>a single hidden layer : yes you can use more, but theoretically\u2026</li> <li>finite number of neurons: you can do it without needing an infinite computer</li> <li>approximate continuous functions: continuous functions are anything which dont have breaks/holes in between. This just says that it is possible to approximate the mapping which we talked about \\(\\mathbb{R}\\) is just the set of all real numbers</li> <li>All this boils down to the fact that a neural network can approximate any complex relation given an input and an output.</li> <li></li> <li></li> </ul>"},{"location":"KB/Universal%20Approximation%20Theorem/#refs","title":"Refs","text":"<ul> <li>mm</li> </ul>"},{"location":"KB/Universal%20Quantifiers/","title":"Universal Quantifiers","text":""},{"location":"KB/Universal%20Quantifiers/#universal-quantifiers","title":"Universal Quantifiers","text":"<ul> <li>All the boys are building a snowman</li> <li>Each boy is building a snowman</li> <li>Two universally quantified sentences that involve/exhaust the totality of boys</li> <li>Collective Interpretation</li> <li>Distributive Interpretation</li> <li>Distributive Interpretation (2)</li> <li>Cumulative Interpretation</li> </ul>"},{"location":"KB/Unrestricted%20Race%20Model/","title":"Unrestricted Race Model","text":""},{"location":"KB/Unrestricted%20Race%20Model/#unrestricted-race-model","title":"Unrestricted Race Model","text":"<ul> <li>The Unrestricted Race model (Traxler, Pickering, &amp; Clifton, 1998; van Gompel, Pickering, Pearson, &amp; Liversedge, 2005; van Gompel, Pickering, &amp; Traxler, 2001) follows in the footsteps of constraint-based models in proposing simultaneous integration of multiple constraints from statistical, semantic, and contextual sources</li> <li>However, rather than ambiguity resolution being based on a temporally dynamic competition process, the Unrestricted Race model posits an instantaneous probabilistic selection among the weighted alternatives of an ambiguity.</li> <li>much like the syntax-first models, it must hypothesize a separate reanalysis mechanism that is responsible for garden-path effects when the initial selected alternative turns out to be syntactically or semantically inappropriate.</li> <li>the Unrestricted Race model predicts that sentences with garden-paths and sentences without garden-paths are two separate populations of events</li> <li>In other words, in conditions where mean performance is expected to exhibit a garden-path effect, there should exist one of two possible patterns: (a) a bimodal distribution of some substantial gardenpath responses and some non-gardenpath responses, or (b) practically all trials exhibiting substantial garden-path effect</li> </ul>"},{"location":"KB/Unsupervised%20Data%20Generation/","title":"Unsupervised Data Generation","text":""},{"location":"KB/Unsupervised%20Data%20Generation/#unsupervised-data-generation","title":"Unsupervised Data Generation","text":"<ul> <li>training data creation procedure named Unsupervised Data Generation (UDG), which leverages few-shot prompts to synthesize high-quality training data without real human annotations</li> </ul>"},{"location":"KB/Unsupervised%20Learning/","title":"Unsupervised Learning","text":""},{"location":"KB/Unsupervised%20Learning/#unsupervised-learning","title":"Unsupervised Learning","text":"<ul> <li>Discover useful things from raw data</li> <li>Representation/Embedding Learning</li> <li>If labels , train network and take intermediate Layers</li> <li>Clustering</li> <li>PCA</li> <li>Feature Learning</li> <li>Hidden Models</li> <li>Generative Models</li> <li>Anomaly Detection</li> <li>Auto Encoders</li> </ul>"},{"location":"KB/Unsupervised%20Learning/#_1","title":"\u2026","text":""},{"location":"KB/Untitledp/","title":"Untitledp","text":"<pre><code>def proxy_one_batch(config, input_wrong, cam):\n    grads = cam(input_tensor=input_wrong.to(config[\"device\"]), targets=None)\n    grads = torch.Tensor(grads).to(config[\"device\"]).unsqueeze(1).expand(-1, 3, -1, -1)\n    normalized_inps = inv_normalize(input_wrong)\n\n    if config[\"pixel_replacement_method\"] != \"blended\":\n        output =  torch.where(\n            grads &gt; config[\"proxy_threshold\"],\n            dict_decide_change[config[\"pixel_replacement_method\"](grads),\n            normalized_inps,\n        )\n    else:\n        output= torch.where(\n            grads &gt; config[\"proxy_threshold\"],\n            (1 - config[\"proxy_image_weight\"] * grads) * normalized_inps,\n            normalized_inps,\n        )\n    del grads\n    return output\n\n</code></pre> <pre><code>def proxy_callback(config, input_wrong_full, label_wrong_full, cam):\n    # TODO Save Classwise fraction\n    chosen_inds = int(np.ceil(config[\"change_subset_attention\"] * len(label_wrong_full)))\n    # TODO some sort of decay?\n    # TODO Remove min and batchify\n\n    input_wrong_full = input_wrong_full[:chosen_inds]\n    label_wrong_full = label_wrong_full[:chosen_inds]\n\n    processed_labels = []\n    processed_thresholds = []\n\n    for i in tqdm(range(0, len(input_wrong_full), config[\"batch_size\"]), desc=\"Running proxy\"):\n        try:\n            input_wrong = input_wrong_full[i:i+config[\"batch_size\"]\n            label_wrong = label_wrong_full[i:i+config[\"batch_size\"]\n\n            try:\n                input_wrong = torch.squeeze(torch.stack(input_wrong, dim=1))\n                label_wrong = torch.squeeze(torch.stack(label_wrong, dim=1))\n            except:\n                input_wrong = torch.squeeze(input_wrong)\n                label_wrong = torch.squeeze(label_wrong)\n\n            thresholded_ims = proxy_one_batch(config, input_wrong.to(config[\"device\"]), cam)\n            processed_thresholds.extend(thresholded_ims.detach().cpu())\n            processed_labels.extend(label_wrong)\n\n\n    processed_thresholds = torch.stack(processed_thresholds, dim = 0).detach()\n    batch_size = processed_thresholds.size(0)\n\n    for ind in tqdm(range(batch_size), total=batch_size, desc=\"Saving images\"):\n        label = config[\"label_map\"][processed_labels[ind].item()]\n        save_name = (\n            config[\"ds_path\"] / label / f\"proxy-{ind}-{config['global_run_count']}.jpeg\"\n        )\n        tfm(processed_thresholds[ind, :, :, :]).save(save_name)\n\n</code></pre>"},{"location":"KB/Uplift%20Modeling/","title":"Uplift Modeling","text":""},{"location":"KB/Uplift%20Modeling/#uplift-modeling","title":"Uplift Modeling","text":"<ul> <li>A modeling technique, commonly used in marketing, that models the \"causal effect\" (also known as the \"incremental impact\") of a \"treatment\" on an \"individual.\" Here are two examples</li> <li>Doctors might use uplift modeling to predict the mortality decrease (causal effect) of a medical procedure (treatment) depending on the age and medical history of a patient (individual).</li> <li>Marketers might use uplift modeling to predict the increase in probability of a purchase (causal effect) due to an advertisement (treatment) on a person (individual).</li> </ul>"},{"location":"KB/Upweighting/","title":"Upweighting","text":""},{"location":"KB/Upweighting/#upweighting","title":"Upweighting","text":"<ul> <li>Applying a weight to the downsampled class equal to the factor by which you downsampled.</li> </ul>"},{"location":"KB/Use%20Case%20Utility/","title":"Use Case Utility","text":""},{"location":"KB/Use%20Case%20Utility/#use-case-utility","title":"Use Case Utility","text":"<ul> <li>XAI and LLMs are often tools for accomplishing some other goal.</li> <li>Very limited work has explored the utility of LLMs in use-case\u2013 specified user studies, but a user study on Microsoft/Github's Copilot [1], an LLM-based code generation tool, found that it \"did not necessarily improve the task completion time or success rate\" [52]</li> <li>LLM outputs often sound very confident, even if what they are saying is hallucinated [50]</li> <li>When the user inquires about the incorrectness, they also have a documented tendency to argue that the user is wrong and that their response is correct. In fact, some have called LLMs \"mansplaining as a service\" [34]</li> <li>This can make it more difficult for humans to implement cognitive checks on LLM outputs.</li> <li>While some recent LLM work has outlined categories of failure modes for LLMs based on the types of cognitive biases use [29], we push for greater work in this field</li> </ul>"},{"location":"KB/Useful%20Codes/","title":"Useful Codes","text":""},{"location":"KB/Useful%20Codes/#useful-codes","title":"Useful Codes","text":"<ul> <li>Parallel Runner</li> </ul>"},{"location":"KB/Utilitarian%20ethics/","title":"Utilitarian ethics","text":""},{"location":"KB/Utilitarian%20ethics/#utilitarian-ethics","title":"Utilitarian Ethics","text":"<ul> <li>resulting decisions often aim to produce the best aggregate consequences</li> </ul>"},{"location":"KB/VAE/","title":"Variational Autoencoder","text":""},{"location":"KB/VAE/#variational-autoencoder","title":"Variational Autoencoder","text":"<ul> <li>Some control over distribution of learned features</li> <li>Eg: Decoder as a generative model</li> <li>Constraint loss function and a given Probability \\(\\mathcal{D}\\)<ul> <li>Eg: By Loss func KL Divergence and prob distribution $\\(L(X) = n^{-1}\\Sigma_i||x_i - D(E(\\tilde x))||^2 + \\lambda \\cdot KL(f_i, d)\\)</li> <li>Use 2D unit distribution. 0 mean, unit variance</li> <li>Latent vector : \\(\\(f=\\mu + \\epsilon e^{2log\\sigma}\\)\\)</li> </ul> </li> <li>$\\(L(X) = n^{-1}\\Sigma_i||x_i - D(E(\\tilde x))||^2 + \\frac{1}{2n}\\Sigma_i(e^{log\\sigma(x_i)} + \\mu(x_i)^2 -1 -log(\\sigma (x_i))\\)</li> <li>Encoder predicts mean and std \\(\\(E(x_i) = (\\mu(x_i) , log \\sigma(x_i))\\)\\)</li> </ul>"},{"location":"KB/VGGFace2%203/","title":"VGGFace2","text":""},{"location":"KB/VGGFace2%203/#vggface2","title":"VGGFace2","text":"<ul> <li>A dataset for recognising faces across pose and age.</li> <li>The VGGFace2 dataset is made of around 3.31 million images divided into 9131 classes, each representing a different person identity.</li> </ul>"},{"location":"KB/VGGish/","title":"VGGish","text":""},{"location":"KB/VGGish/#vggish","title":"VGGish","text":"<ul> <li>CNN Architectures for Large-Scale Audio Classification</li> <li>applying various state-of-the-art image networks with CNN architectures to audio and show that they are capable of excellent results on audio classification</li> <li>examine fully connected deep neural networks such as AlexNet, VGG, InceptionNet, and ResNet</li> <li>The input audio is divided into non-overlapping 960 ms frames which are decomposed by applying the Fourier transform, resulting in a spectrogram</li> <li>spectrogram is integrated into 64 mel-spaced frequency bins, and the magnitude of each bin is log-transformed</li> <li>gives log-mel spectrogram patches that are passed on as input to all classifiers</li> <li>Acoustic Event Detection</li> <li>train a classifier on embeddings learned from the video-level task on AudioSet</li> <li>model for AED with embeddings learned from these classifiers does much better than raw features on the Audio Set AED classification task</li> <li>derivatives of image classification networks do well on the audio classification task</li> <li>increasing the number of labels they train on provides some improved performance over subsets of labels</li> <li>performance of models improves as they increase training set size,</li> <li>model using embeddings learned from the video-level task do much better than a baseline on the AudioSet classification task</li> </ul>"},{"location":"KB/VICReg/","title":"VICReg","text":"<ul> <li>VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning</li> <li>Self Supervised</li> <li>based on maximizing the agreement between Embedding vectors from different views of the same image</li> <li>rivial solution is obtained when the encoder outputs constant vectors</li> <li>Mode Collapse is often avoided through implicit biases</li> <li>explicitly avoids the collapse problem with a simple regularization term on the variance of the embeddings along each dimension individually</li> <li>triple objective: learning invariance to different views with a invariance term, avoiding collapse of the representations with a variance preservation term, and maximizing the information content of the representation with a covariance regularization term</li> <li>Bias Vs Variance</li> <li>combines the variance term with a decorrelation mechanism based on redundancy reduction and covariance regularization</li> <li>does not require the embedding branches to be identical or even similar</li> </ul>"},{"location":"KB/VL-BEIT/","title":"VL-BEIT","text":""},{"location":"KB/VL-BEIT/#vl-beit","title":"VL-BEIT","text":"<ul> <li>VL-BEIT: Generative Vision-Language Pretraining</li> <li>vision-language foundation model</li> <li>simple and effective approach to pretraining a bidirectional multimodal Transformer encoder for both vision-language and vision tasks learned by generative pretraining</li> <li>conducts masked prediction on both monomodal and multimodal data with a shared Transformer</li> <li>solely employs generative pretraining tasks, including KB/Masked Language Modeling.md on texts, masked image modeling on images, and masked vision-language modeling on image-text pairs</li> <li>learned from scratch with one unified pretraining task, one shared backbone, and one-stage training which renders it conceptually simple and empirically effective</li> <li>transferable visual features</li> </ul>"},{"location":"KB/VQAv2%203/","title":"VQAv2","text":""},{"location":"KB/VQAv2%203/#vqav2","title":"VQAv2","text":""},{"location":"KB/VTAB/","title":"VTAB","text":"<p>toc: true title: VTAB</p> <p>categories: ['temp']</p>"},{"location":"KB/VTAB/#vtab","title":"VTAB","text":""},{"location":"KB/Vacuum%20Cup%20Hand/","title":"Vacuum Cup Hand","text":""},{"location":"KB/Vacuum%20Cup%20Hand/#vacuum-cup-hand","title":"Vacuum Cup Hand","text":"<ul> <li>An end-effector for a robot arm which is used to grasp light to moderate weight objects, using suction, for manipulation. Such objects may include glass, plastic; etc. Commonly used because of its virtues of reduced object slide slipping while within the grasp of the vacuum cup.</li> </ul>"},{"location":"KB/Vagus%20Nerve/","title":"Vagus Nerve","text":""},{"location":"KB/Vagus%20Nerve/#vagus-nerve","title":"Vagus Nerve","text":"<ul> <li>One of the twelve pairs of cranial nerves in the human body, the vagus nerve connects the brain stem to the body, transmitting information from the brain to the major organs and other tissues.</li> </ul>"},{"location":"KB/Van%20Mises%20distribution/","title":"Van Mises distribution","text":""},{"location":"KB/Van%20Mises%20distribution/#van-mises-distribution","title":"Van Mises distribution","text":""},{"location":"KB/Vanishing%20Gradient/","title":"Vanishing Gradient","text":""},{"location":"KB/Vanishing%20Gradient/#vanishing-gradient","title":"Vanishing Gradient","text":"<ul> <li>Deltas become smaller initially. using [Sigmoid] -&gt; ill conditioning </li> <li> \\[g(x) = (1+e^{-x})^{-1}\\] </li> <li> \\[\\nabla_{x}[g] = g(1-g) \\in [0,1]\\] </li> <li>Saturation and prevent Backprop </li> <li> \\[g(x) \\approx 1 \\rightarrow \\nabla_{x}[g] \\approx 0 \\] </li> <li>Weight matrices are usually initialized with random values \\(|w_{ji}| &lt;&lt; 1\\) <ul> <li>gradient magnitueds decay exponentially -&gt; max eigenvalue</li> </ul> </li> </ul>"},{"location":"KB/VarGrad/","title":"VarGrad","text":""},{"location":"KB/VarGrad/#vargrad","title":"VarGrad","text":""},{"location":"KB/VarGrad/#-richtervargradlowvariancegradient2020","title":"- @richterVarGradLowVarianceGradient2020","text":""},{"location":"KB/Variable%20Importances/","title":"Variable Importances","text":""},{"location":"KB/Variable%20Importances/#variable-importances","title":"Variable Importances","text":"<ul> <li>A set of scores that indicates the relative importance of each feature to the model.</li> </ul>"},{"location":"KB/Variation%20in%20Dissimilarity%20Variation%20in%20Dissimilarity/","title":"Variation in Dissimilarity Variation in Dissimilarity","text":""},{"location":"KB/Variation%20in%20Dissimilarity%20Variation%20in%20Dissimilarity/#variation-in-dissimilarity-variation-in-dissimilarity","title":"Variation in Dissimilarity Variation in Dissimilarity","text":"<ul> <li>is the variance of the NISSIM metric over the adversarial set over different levels of attack eps for a model, this shows the distribution of the attack on the model when different levels of attack are performed</li> <li>Ideally, we would want the distribution to be stable for different levels of attack</li> <li> \\[m_{h}= \\frac{1}{eps}\\Sigma(NISSIM_{eps})\\] </li> <li> \\[VID = \\sqrt{\\frac{\\Sigma(NISSIM_{eps}-m_{h})^{2}}{eps}}\\] </li> </ul>"},{"location":"KB/VariationalRecurrent%20Dropout/","title":"Variational/Recurrent Dropout","text":""},{"location":"KB/VariationalRecurrent%20Dropout/#variationalrecurrent-dropout","title":"Variational/Recurrent Dropout","text":"<ul> <li>Basic RNN Architectures</li> <li>Only on the non Recurrent parts such as inputs and outputs</li> <li>In recorrent parts, use the same dropout mask for all time steps</li> <li>Same dropout mask for each time step</li> <li></li> </ul>"},{"location":"KB/VariationalRecurrent%20Dropout/#_1","title":"\u2026","text":""},{"location":"KB/VattenFall%20Data%20Scientist/","title":"VattenFall Data Scientist","text":""},{"location":"KB/VattenFall%20Data%20Scientist/#vattenfall-motivation-letter-subhaditya","title":"VattenFall Motivation Letter - Subhaditya","text":"<p>As I write this motivation letter, the temperatures in NL have hit yet another record for the hottest September ever. One of the biggest reasons for this is global warming, and fossil fuels are a massive component. VattenFall's mission to be fossil-free in one generation and help customers shift towards sustainable energy sources using data greatly resonates with me, so I am applying for this position as a Data Scientist. At the core of making the world a greener and healthier place is data that every company has collected for decades. Using that vast data pool to inform better decisions is quite challenging but equally rewarding.</p> <p>My expertise is a combination of data analytics/BI and computer vision, and as of a month ago, I also have a Master in Artificial Intelligence from the University of Groningen. I am familiar with the tools required for basic and advanced AI and analytics from internships, research projects, papers, freelance work, and many personal projects. In my previous internship at KPMG, I built 10+ dashboards using PowerBI and Azure for a project with the Abu Dhabi government. I have made several analytics pipelines and machine learning implementations in other internships. I am familiar with Python and can build advanced AI pipelines for many tasks.</p> <p>In any team, it is essential that each of the members can contribute something to the overall development of the projects. Although I have a lot to learn, I genuinely enjoy developing solutions that have a positive impact, and I can contribute quite a bit to any team I get the chance to work with. If there is anything I do not know, I am also ready to develop those skills quickly.</p> <p>I hope you give me a chance to be a part of the generation that finally starts relying on greener energy sources for our own and future generations.</p>"},{"location":"KB/Vector%20Assembly%20level/","title":"Vector Assembly level","text":""},{"location":"KB/Vector%20Assembly%20level/#vector-assembly-level","title":"Vector Assembly Level","text":""},{"location":"KB/Vector%20Chaining/","title":"Vector Chaining","text":""},{"location":"KB/Vector%20Chaining/#vector-chaining","title":"Vector Chaining","text":"<ul> <li>Equivalent to data forwarding in vector processors</li> <li>Results of one pipeline are fed into operand registers of another pipeline</li> </ul>"},{"location":"KB/Vector%20Functional%20Units/","title":"Vector Functional Units","text":""},{"location":"KB/Vector%20Functional%20Units/#vector-functional-units","title":"Vector Functional Units","text":"<ul> <li>Fully pipelined, new operation every cycle</li> <li>Performs arithmetic and logic operations</li> <li>Typically 4-8 different units</li> </ul>"},{"location":"KB/Vector%20Load%20Store%20Units/","title":"Vector Load Store Units","text":""},{"location":"KB/Vector%20Load%20Store%20Units/#vector-load-store-units","title":"Vector Load Store Units","text":"<ul> <li>Moves vectors between memory and registers</li> </ul>"},{"location":"KB/Vector%20Processor/","title":"Vector Processor","text":""},{"location":"KB/Vector%20Processor/#vector-processor","title":"Vector Processor","text":"<ul> <li>Memory to Memory Architecture</li> <li>Register to Register Architecture</li> <li>Vector Register</li> <li>Scalar Register</li> <li>Vector Functional Units</li> <li>Vector Load Store Units</li> <li>Strip Mining</li> <li>Vector Chaining</li> <li>Scatter and Gather</li> <li>Pipes</li> <li>Vector Assembly level</li> </ul>"},{"location":"KB/Vector%20Quantization/","title":"Vector Quantization","text":""},{"location":"KB/Vector%20Quantization/#vector-quantization","title":"Vector Quantization","text":"<ul> <li>Partitioned into k cells whose center of Gravity vectors are indexed</li> <li>Indices used as symbolic encodings</li> <li>Discretization</li> </ul>"},{"location":"KB/Vector%20Register/","title":"Vector Register","text":""},{"location":"KB/Vector%20Register/#vector-register","title":"Vector Register","text":"<ul> <li>Typically 8-32 vector registers with 64 -128 64-bit elements</li> <li>Each contains a vector of double-precision numbers</li> <li>Register size determines the maximum vector length</li> <li>Each includes at least 2 read and 1 write ports</li> </ul>"},{"location":"KB/Vectorization/","title":"Vectorization","text":""},{"location":"KB/Vectorization/#vectorization","title":"Vectorization","text":"<ul> <li>Hardware primitives</li> <li>Prioritize those are contiguous in memory</li> </ul>"},{"location":"KB/Velocity/","title":"Velocity","text":""},{"location":"KB/Velocity/#velocity","title":"Velocity","text":"<ul> <li>Displacement wrt time</li> <li> \\[v_{avg}= \\frac{\\Delta{x}}{\\Delta t}\\] </li> </ul>"},{"location":"KB/Verb/","title":"Verb","text":""},{"location":"KB/Verb/#verb","title":"Verb","text":"<ul> <li>Actions, relationships</li> <li>Transitive verb</li> <li>DiTransitive verb</li> <li>Action Transitive verb</li> </ul>"},{"location":"KB/Vertebral%20Arteries/","title":"Vertebral Arteries","text":""},{"location":"KB/Vertebral%20Arteries/#vertebral-arteries","title":"Vertebral Arteries","text":"<ul> <li>The major arteries of the neck, which merge to form the basilar artery.</li> </ul>"},{"location":"KB/Vestibular%20System/","title":"Vestibular System","text":""},{"location":"KB/Vestibular%20System/#vestibular-system","title":"Vestibular System","text":"<ul> <li>Regions in the body and brain that help support balance in movement. Many people with hearing loss experience some degree of balance difficulties, since the vestibular (or balance) system and the auditory (or hearing) systems are so closely related.</li> </ul>"},{"location":"KB/Vgg/","title":"Vgg","text":""},{"location":"KB/Vgg/#vgg","title":"Vgg","text":"<ul> <li>@simonyanVeryDeepConvolutional2014</li> <li>Very Deep Convolutional Networks for Large-Scale Image Recognition</li> <li>Deeper Alex Net</li> <li>Object detection and Image captioning</li> <li>5x5 -&gt; two 3x3</li> <li>No of filters increase according to depth</li> <li>No of filters : increase by power of two</li> <li>Filter size : Odd numbers</li> <li>SGD + LR Schedule</li> <li>three non-linear activations (instead of one), which makes the function more discriminative</li> </ul>"},{"location":"KB/ViLT/","title":"ViLT","text":""},{"location":"KB/ViLT/#vilt","title":"ViLT","text":"<ul> <li>ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision</li> <li>Vision-and-Language Transformer</li> <li>seeks to improve performance on various joint vision-and-language downstream tasks</li> <li>Current approaches to VLP heavily rely on image feature extraction processes using convolutional visual embedding networks (e.g., Faster R-CNN and ResNets), which involve region supervision (e.g., object detection) and the convolutional architecture (e.g., ResNet)</li> <li>This is problematic in terms of both efficiency/speed, in that extracting input features requires much more computation than the multimodal interaction steps; and expressive power, as it is upper bounded to the expressive power of the visual embedder and its predefined visual vocabulary.</li> <li>minimal VLP model, which is monolithic in that the processing of visual inputs is drastically simplified to just the same convolution-free manner that they process textual inputs</li> <li>removing the need for object detectors</li> <li>avoiding heavyweight image encoders by directly embedding low-level pixel data with a single-layer projection and achieves similar results with reduced complexity,</li> <li>Self-supervision is accomplished using (i) Image Text Matching (ITM) loss and (ii) Masked Language Model (MLM) loss</li> <li>ITM Loss</li> <li>For text, ViLT simply reuses Masked Language Model - (MLM), used in BERT.</li> <li>MSCOCO</li> <li>Visual Genome</li> <li>SBU Captions</li> <li>Google Conceptual Captions</li> <li>VQAv2</li> <li>NLVR2</li> <li>Flickr30K</li> <li>ViLT is over 10x faster than previous VLP models, yet with competitive or better downstream task performance</li> <li>VLP needs to focus more on the multi-KB/Modality.md interactions aspect inside the transformer module rather than engaging in an arms race that merely powers up unimodal embedders</li> </ul>"},{"location":"KB/Video%20Generation/","title":"Video Generation","text":""},{"location":"KB/Video%20Generation/#video-generation","title":"Video Generation","text":"<ul> <li>Visual features are learned through the process of video generation tasks </li> <li>video prediction</li> </ul>"},{"location":"KB/Viewpoint%20Feature%20Histogram/","title":"Viewpoint Feature Histogram","text":""},{"location":"KB/Viewpoint%20Feature%20Histogram/#viewpoint-feature-histogram","title":"Viewpoint Feature Histogram","text":"<ul> <li>VFH produces a histogram that encodes the geometry of the object and its viewpoint.</li> <li>For every pair of a point and the center of mass, a reference frame is constructed and the three angular variations are computed (a, q, f) and also d which represents distance between (point, center of mass)</li> <li>Another statistical feature is computed between the central viewpoint direction and the normal estimated at each point.</li> <li>The quality of VFH description depends on the quality of the surface normal estimation at each point.</li> <li></li> </ul>"},{"location":"KB/Virtue%20ethics/","title":"Virtue ethics","text":""},{"location":"KB/Virtue%20ethics/#virtue-ethics","title":"Virtue Ethics","text":"<ul> <li>an agent is ethical if and only if it acts and thinks according to some moral values</li> <li>Agents with virtue ethics should exhibit an inner drive to be perceived favourably by others</li> </ul>"},{"location":"KB/Vision%20Explainibility/","title":"Vision Explainibility","text":""},{"location":"KB/Vision%20Explainibility/#vision-explainibility","title":"Vision Explainibility","text":""},{"location":"KB/Vision%20Explainibility/#links-useful","title":"Links Useful","text":"<ul> <li>Captum Algos Comparison</li> </ul>"},{"location":"KB/Vision%20Explainibility/#flow","title":"Flow","text":"<ul> <li>DeconvNet (2013)</li> <li>Deep Inside Convolutional Networks (2014)</li> <li>Guided BackProp (2015) Aka All Conv net<ul> <li>Building up on Deep Inside Convolutional Networks and DeconvNet</li> </ul> </li> <li>Salience Map<ul> <li>Not class discriminative</li> <li>Noise</li> <li>Not appealing</li> </ul> </li> <li>CAM<ul> <li>less noisy</li> <li>not class discriminative</li> <li>Worked only a restricted set of CNN templates</li> </ul> </li> <li>Grad-CAM<ul> <li>class discriminative</li> <li>not high res</li> <li>Works for any arbitrary CNN</li> </ul> </li> <li>Occlusion Map<ul> <li>Same as the next but not very fast</li> </ul> </li> <li>Guided GradCAM</li> <li>DeepLIFT</li> <li>Noise Tunnel</li> <li>Smooth-Grad</li> <li>SmoothGrad Square</li> <li>VarGrad</li> <li>Integrated Gradients</li> <li>Proxy Attention</li> <li>Conductance</li> </ul>"},{"location":"KB/Vision%20Explainibility/#disadvantages","title":"Disadvantages","text":"<ul> <li>The Unreliability of Saliency Methods</li> <li>Interpretation of Neural networks is fragile</li> <li>Fine grained data</li> </ul>"},{"location":"KB/Vision%20Explainibility/#links","title":"Links","text":""},{"location":"KB/Vision%20Transformer/","title":"Vision Transformer","text":""},{"location":"KB/Vision%20Transformer/#vision-transformer","title":"Vision Transformer","text":"<ul> <li>@dosovitskiyImageWorth16x162021</li> <li>paper</li> <li></li> <li>Transformer applied directly to sequences/patches of images</li> <li>Lower computational resources</li> <li>ImageNet , CIFAR, VTAB</li> <li>Do Vision Transformers See Like Convolutional Neural Networks?</li> <li>analyzes the internal representation structure of ViTs and Conv on image classification benchmarks</li> <li>striking differences in the features and internal structures between the two architectures</li> <li>ViT having more uniform representations across all layers</li> <li>early aggregation of global information</li> <li>spatial localization</li> <li>discovering ViTs successfully preserve input spatial information with CLS tokens</li> <li>finding larger ViT models develop significantly stronger intermediate representations through larger pretraining datasets</li> <li>MLP-Mixer</li> </ul>"},{"location":"KB/Visual%20Associative/","title":"Visual Associative","text":""},{"location":"KB/Visual%20Associative/#visual-associative","title":"Visual Associative","text":"<ul> <li>Can this variable allow us to spontaneously group items in a group? [i.e., 1 group from all]</li> </ul>"},{"location":"KB/Visual%20Commonsense%20Reasoning/","title":"Visual Commonsense Reasoning","text":""},{"location":"KB/Visual%20Commonsense%20Reasoning/#visual-commonsense-reasoning","title":"Visual Commonsense Reasoning","text":"<ul> <li>Given a challenging question about an image, a machine must answer correctly and then provide a rationale justifying its answer</li> <li>From Recognition to Cognition: Visual Commonsense Reasoning</li> <li>290k multiple choice QA problems derived from 110k movie scenes</li> <li>Adversarial Learning</li> </ul>"},{"location":"KB/Visual%20Context%20Augmentation/","title":"Visual Context Augmentation","text":""},{"location":"KB/Visual%20Context%20Augmentation/#visual-context-augmentation","title":"Visual Context Augmentation","text":"<ul> <li>@dvornikModelingVisualContext2018</li> <li>learns to place object instances at an image location depending on the surrounding context.</li> <li>A neural network is trained for this purpose.</li> <li>The training data is pre- pared to generate a context image with the masked- out object inside it.</li> <li>From an image, 200 context sub-images are generated surrounding the blacked- out bounding box. The neural network learns to predict the category (object or background) in masked pixels.</li> <li>The object instances are placed inside the selected boxes to generate a new train- ing image</li> </ul>"},{"location":"KB/Visual%20Cortex/","title":"Visual Cortex","text":""},{"location":"KB/Visual%20Cortex/#visual-cortex","title":"Visual Cortex","text":"<ul> <li>The area of the cerebrum that is specialized for vision. It lies primarily in the occipital lobe at the rear of the brain and is connected to the eyes by the optic nerves.</li> </ul>"},{"location":"KB/Visual%20Encoding/","title":"Visual Encoding","text":""},{"location":"KB/Visual%20Encoding/#visual-encoding","title":"Visual Encoding","text":"<ul> <li>Characteristics of Visual Variables</li> </ul>"},{"location":"KB/Visual%20Genome/","title":"Visual Genome","text":""},{"location":"KB/Visual%20Genome/#visual-genome","title":"Visual Genome","text":""},{"location":"KB/Visual%20Implicit%20Learning/","title":"Visual Implicit Learning","text":""},{"location":"KB/Visual%20Implicit%20Learning/#visual-implicit-learning","title":"Visual Implicit Learning","text":"<ul> <li>Does ability to learn implicit dependencies correlate with ability to correctly predict the next word in speech?</li> <li>Participants saw a sequence of colors light up on screen</li> <li>They then had to reproduce it by clicking on the same sequence</li> <li>Auditory Only Sentence Perception</li> <li>25 highly predictable and 25 zero-predictability sentences</li> <li>acoustically degraded by processing them with a sinewave vocoder to 6 channels</li> <li>Ability to pick up/learn statistical dependencies seems almost to be a new cognitive function</li> <li>Shows a potential connection between sequence learning and language modelling ability</li> <li>Learning simple dependencies is not so difficult it seems. But natural language has many dependencies at a distance</li> </ul>"},{"location":"KB/Visual%20Length/","title":"Visual Length","text":""},{"location":"KB/Visual%20Length/#visual-length","title":"Visual Length","text":"<ul> <li>Across how many changes in this variable are distinctions possible? [i.e., how many can I see?]</li> </ul>"},{"location":"KB/Visual%20Ordered/","title":"Visual Ordered","text":""},{"location":"KB/Visual%20Ordered/#visual-ordered","title":"Visual Ordered","text":"<ul> <li>Can this variable allow us to spontaneously perceive an order [i.e., what is smaller and what is bigger?]</li> </ul>"},{"location":"KB/Visual%20Quantitative/","title":"Visual Quantitative","text":""},{"location":"KB/Visual%20Quantitative/#visual-quantitative","title":"Visual Quantitative","text":"<ul> <li>Can the difference between two marks in this variable be interpreted numerically? [i.e., corresponds to 5?]</li> </ul>"},{"location":"KB/Visual%20Selective/","title":"Visual Selective","text":""},{"location":"KB/Visual%20Selective/#visual-selective","title":"Visual Selective","text":"<ul> <li>Can this variable allow us to spontaneously differentiate/isolate items from groups? [i.e., 1 item from all]</li> </ul>"},{"location":"KB/Visual%20Servo%20System/","title":"Visual Servo System","text":""},{"location":"KB/Visual%20Servo%20System/#visual-servo-system","title":"Visual Servo System","text":"<ul> <li>The task in visual servoing is to use visual information to control the robot\u2019s end-effector relative to a target object</li> <li></li> <li>Chaumette, Franc\u0327ois, and Seth Hutchinson. \"Visual servo control. II. Advanced approaches [Tutorial].\" IEEE Robotics &amp; Automation Magazine 14.1 (2007): 109-118.</li> <li>Morrison, Douglas, Peter Corke, and Ju\u0308rgen Leitner. \"Closing the loop for robotic grasping: A real-time, generative grasp synthesis approach.\" RSS (2018).</li> <li></li> <li></li> <li></li> </ul>"},{"location":"KB/VisualGPT/","title":"VisualGPT","text":""},{"location":"KB/VisualGPT/#visualgpt","title":"VisualGPT","text":"<ul> <li>image captioning model</li> <li>leverages knowledge from the pretrained language model GPT-2</li> <li>bridge the semantic gap between diferent modalities - novel encoder-decoder attention mechanism [33] is designed with an unsaturated rectified gating function</li> <li>the biggest advantage of this model is that it does not need for as much data as other image-to-text models</li> <li>improving data eciency in image captioning networks would enable quick data curation, description of rare objects, and applications in specialized domains</li> </ul>"},{"location":"KB/Visualization%20Of%20Layers/","title":"Visualization Of Layers","text":""},{"location":"KB/Visualization%20Of%20Layers/#visualization-of-layers","title":"Visualization Of Layers","text":"<ul> <li>Tanh</li> <li>\\(tanh(Wx+b)\\)</li> <li> <ol> <li>A linear transformation by the \u201cweight\u201d matrix\u00a0\\(W\\)</li> </ol> </li> <li> <ol> <li>A translation by the vector\u00a0\\(b\\)</li> </ol> </li> <li> <ol> <li>Point-wise application of tanh.</li> </ol> </li> </ul>"},{"location":"KB/Visualizing%20the%20Impact%20of%20Feature%20Attribution%20Baselines/","title":"Visualizing the Impact of Feature Attribution Baselines","text":""},{"location":"KB/Visualizing%20the%20Impact%20of%20Feature%20Attribution%20Baselines/#visualizing-the-impact-of-feature-attribution-baselines","title":"Visualizing the Impact of Feature Attribution Baselines","text":"<ul> <li>@sturmfelsVisualizingImpactFeature2020</li> <li>Maximum Distance Baseline</li> <li>Uniform baseline</li> </ul>"},{"location":"KB/Visualizing%20the%20Loss%20Landscape%20of%20Neural%20Nets/","title":"Visualizing the Loss Landscape of Neural Nets","text":""},{"location":"KB/Visualizing%20the%20Loss%20Landscape%20of%20Neural%20Nets/#visualizing-the-loss-landscape-of-neural-nets","title":"Visualizing the Loss Landscape of Neural Nets","text":"<ul> <li>@liVisualizingLossLandscape2018</li> <li>Generalization error relates to convexity in the loss landscape<ul> <li></li> </ul> </li> <li>Random Directions</li> <li>Sharpness and Flatness</li> <li>Training Trajectories</li> </ul>"},{"location":"KB/Visualizing%20the%20Loss%20Landscape%20of%20Neural%20Nets/#solutions","title":"Solutions","text":"<ul> <li>Layer Normalization</li> <li>Filter Wise Normalization</li> <li>Trajectory Plotting with PCA</li> </ul>"},{"location":"KB/Visualizing%20the%20Loss%20Landscape%20of%20Neural%20Nets/#images","title":"Images","text":""},{"location":"KB/Volterra%20expansion/","title":"Volterra","text":""},{"location":"KB/Volterra%20expansion/#volterra","title":"Volterra","text":"<ul> <li>Add higher order polynomials</li> <li>Adding too much leads to combinatorial explosion -&gt; Pruning scheme</li> <li>Adding all polynomials of degree 2<ul> <li>(\\(d + d(d+1) /2\\)\\) input components<ul> <li> \\[{u_1, u_2, \u2026,u_d} \\cup {u_iu_j | 1 \\leq i \\leq j \\leq d}\\] </li> </ul> </li> </ul> </li> </ul>"},{"location":"KB/Volume%20Rendering%20Equation/","title":"Volume Rendering Equation","text":""},{"location":"KB/Volume%20Rendering%20Equation/#volume-rendering-equation","title":"Volume Rendering Equation","text":"<ul> <li>Light-emitting particles fill volume</li> <li>Emission-absorption mode</li> <li>Based on a physical model for radiation</li> <li>Interaction of light with matter at the macroscopic scale, neglecting Diffraction, Interference, Wave-character, Polarization, etc.</li> <li>\\(\\kappa\\) is fraction of absorbed light</li> <li>\\(g\\) is fraction of emitted light</li> <li> \\[\\frac{dL}{ds} = g(s) - \\kappa(s)L(s)\\] </li> <li>aka Emission -Absorption</li> <li> \\[T(s_{1}, s_{2}) = e^{-\\int_{s_{1}}^{s_{2}}\\kappa(s')ds'}\\] </li> <li> \\[L_{1}^{n}= g(n)+T(n)L_{1}^{n-1}\\] </li> </ul>"},{"location":"KB/Volume%20Visualization/","title":"Volume Visualization","text":""},{"location":"KB/Volume%20Visualization/#volume-visualization","title":"Volume Visualization","text":"<ul> <li>Orthogonal Slicing</li> <li>Oblique Slicing</li> <li>Isosurface</li> <li>Volumetric Illumination</li> </ul>"},{"location":"KB/Volumetric%20Grasping%20Network/","title":"Volumetric Grasping Network","text":""},{"location":"KB/Volumetric%20Grasping%20Network/#volumetric-grasping-network","title":"Volumetric Grasping Network","text":"<ul> <li>M. Breyer, et al., \"Volumetric Grasping Network: Real-time 6 DOF Grasp Detection in Clutter\", ICLR 2020</li> <li>Learning to grasp 3D objects by constructing a full model of the scene</li> <li>TSDF</li> <li></li> </ul>"},{"location":"KB/Volumetric%20Illumination/","title":"Volumetric Illumination","text":""},{"location":"KB/Volumetric%20Illumination/#volumetric-illumination","title":"Volumetric Illumination","text":"<ul> <li>Phong Lighting</li> <li>Finite Differences</li> <li>Shading</li> <li>Raycasting</li> </ul>"},{"location":"KB/Von%20Neumann%20Architecture/","title":"Von Neumann Architecture","text":""},{"location":"KB/Von%20Neumann%20Architecture/#von-neumann-architecture","title":"Von Neumann Architecture","text":"<p>-</p>"},{"location":"KB/Voronoi%20Cell/","title":"Voronoi Cell","text":""},{"location":"KB/Voronoi%20Cell/#voronoi-cell","title":"Voronoi Cell","text":"<ul> <li>Partition a plane into n convex polygons -&gt; each containing one generating point and every point is closer to its generating point than others</li> <li></li> </ul>"},{"location":"KB/Voronoi%20Cell/#refs","title":"Refs","text":"<ul> <li>wolf</li> </ul>"},{"location":"KB/Voxel%20Projection/","title":"Voxel Projection","text":""},{"location":"KB/Voxel%20Projection/#voxel-projection","title":"Voxel Projection","text":"<ul> <li>Volume = field of 3D interpolation kernels  </li> <li>One kernel at each grid voxel  </li> <li>Each kernel leaves a 2D footprint on screen</li> <li>Weighted footprints accumulate into image</li> <li></li> </ul>"},{"location":"KB/WMT14/","title":"WMT14","text":""},{"location":"KB/WMT14/#wmt14","title":"WMT14","text":""},{"location":"KB/WOMBO%20Dream/","title":"WOMBO Dream","text":""},{"location":"KB/WOMBO%20Dream/#wombo-dream","title":"WOMBO Dream","text":"<ul> <li>The app creates generative art from a descriptive text in various pre-determined styles.</li> <li>The app uses two machine learning technologies that combine a neural network to generate images and an algorithm that interprets text descriptions.</li> <li>Both algorithms learn from each iteration, meaning that every request generates a unique outcome.</li> </ul>"},{"location":"KB/Wageningen%20Uni%20AI/","title":"Wageningen Uni AI","text":""},{"location":"KB/Wageningen%20Uni%20AI/#wageningen-researcher-computer-vision-technology-for-animal-science","title":"Wageningen : Researcher Computer Vision Technology for Animal Science","text":"<p>With the demands on the food industry growing every day, it is easy for companies to start ignoring the needs of the animals that are the most significant part of the pipeline. We hear so many horror stories about the conditions some of these animals face, but even more humane places have to meet many needs. On a larger scale, using technologies like AI can make it much less labor-intensive to provide suitable conditions for these animals and make sure they live as close to their original lives in nature as possible. That being said, the intersection between computer vision and animal welfare is quite interesting to me, hence this application.</p> <p>As of a month ago, I have a masters in AI from the University of Groningen. My expertise is a combination of data analytics and computer vision for practical applications. I am familiar with the tools required for basic and advanced AI and analytics from internships, research projects, papers, freelance work, and many personal projects. I am comfortable with building deep learning pipelines, image and data analysis, OpenCV applications, image processing, etc. </p> <p>In any team, it is essential that each of the members can contribute something to the overall development of the projects. Although I have a lot to learn, I genuinely enjoy developing solutions that have a positive impact, and I can contribute quite a bit to any team I get the chance to work with. If there is anything I do not know, I am also ready to develop those skills quickly. I hope you give me a chance to work on providing better conditions for animals in this industry. </p>"},{"location":"KB/Wall%20Street%20Journal%20task/","title":"Wall Street Journal task","text":""},{"location":"KB/Wall%20Street%20Journal%20task/#wall-street-journal-task","title":"Wall Street Journal Task","text":""},{"location":"KB/WaveGlow/","title":"WaveGlow","text":""},{"location":"KB/WaveGlow/#waveglow","title":"WaveGlow","text":"<ul> <li>WaveGlow: a Flow-based Generative Network for Speech Synthesis</li> <li>flow-based network capable of generating high quality speech from mel-spectrograms</li> <li>combines insights from Glow and WaveNet in order to provide fast, efficient and high-quality audio synthesis, without the need for auto-regression</li> <li>mplemented using only a single network, trained using only a single cost function: maximizing the likelihood of the training data, which makes the training procedure simple and stable</li> <li>more than 500 kHz on an NVIDIA V100 GPU</li> <li>Mean Opinion Scores</li> </ul>"},{"location":"KB/Weak%20Relation%20Bias/","title":"Weak Relation Bias","text":""},{"location":"KB/Weak%20Relation%20Bias/#weak-relation-bias","title":"Weak Relation Bias","text":"<ul> <li>relationship between the neural units is weak, meaning that they\u2019re somewhat independent of each other. The choice of including a fully connected layer in the net can represent this kind of relationship</li> <li></li> </ul>"},{"location":"KB/Weakly%20Supervised%20Learning%20Formulation/","title":"Weakly Supervised Learning Formulation","text":""},{"location":"KB/Weakly%20Supervised%20Learning%20Formulation/#weakly-supervised-learning-formulation","title":"Weakly Supervised Learning Formulation","text":"<ul> <li>For weakly supervised visual feature learning, given a dataset X, for each data Xi in X, there is a corresponding coarse-grained label Ci.  </li> <li></li> </ul>"},{"location":"KB/Weakly-supervised%20Learning/","title":"Weakly-supervised Learning","text":""},{"location":"KB/Weakly-supervised%20Learning/#weakly-supervised-learning","title":"Weakly-supervised Learning","text":"<ul> <li>learning methods to learn with coarse-grained labels or inaccurate labels </li> <li>The cost of obtaining weak supervision labels is generally much cheaper than fine- grained labels for supervised methods.</li> </ul>"},{"location":"KB/WebGPT/","title":"WebGPT","text":""},{"location":"KB/WebGPT/#webgpt","title":"WebGPT","text":"<ul> <li>WebGPT: Browser-assisted Question-answering with Human Feedback</li> <li>fine-tuned version of GPT-3 to more accurately answer open-ended questions using a text-based web browser.</li> <li>submits search queries, follows links, and scrolls up and down web pages</li> <li>trained to cite its sources</li> <li>By setting up the task so that it can be performed by humans, they are able to train models on the task using imitation learning</li> <li>models must collect references while browsing in support of their answers</li> <li>ELI5</li> <li>dataset of questions asked by Reddit users</li> <li>fine-tuning GPT-3 using behavior cloning, and then performing rejection sampling against a reward model trained to predict human preferences</li> </ul>"},{"location":"KB/Weight%20Decay%20Vs%20L2%20Regularization/","title":"Weight Decay Vs L2 Regularization","text":""},{"location":"KB/Weight%20Decay%20Vs%20L2%20Regularization/#weight-decay-vs-l2-regularization","title":"Weight Decay Vs L2 Regularization","text":"<ul> <li>L2 regularization is a classic method to reduce over-fitting, and consists in adding to the loss function the sum of the squares of all the weights of the model, multiplied by a given hyper-parameter</li> </ul> <pre><code>final_loss = loss + wd * all_weights.pow(2).sum() / 2\n</code></pre> <ul> <li>where wd is the hyper-parameter to set</li> <li>This is also called weight decay, because when applying vanilla SGD it's equivalent to updating the weight like this</li> </ul> <pre><code>w = w - lr * w.grad - lr * wd * w\n</code></pre> <ul> <li>In this equation we see how we subtract a little portion of the weight at each step, hence the name decay</li> <li>So why make a distinction between those two concepts if they are the same thing</li> <li>The answer is that they are only the same thing for vanilla SGD, but as soon as we add momentum, or use a more sophisticated optimizer like Adam, L2 regularization (first equation) and weight decay (second equation) become different</li> <li>When using the Adam optimizer, it gets even more different: in the case of L2 regularization we add this \\(wd\\times w\\) to the gradients then compute a moving average of the gradients and their squares before using both of them for the update. Whereas the weight decay method simply consists in doing the update, then subtract to each weight.</li> <li>And after experimenting with this, Ilya Loshchilov and Frank Hutter suggest in their article we should use weight decay with Adam, and not the L2 regularization that classic deep learning libraries implement.</li> <li>Inside the step function of the optimizer, only the gradients are used to modify the parameters, the value of the parameters themselves isn't used at all</li> <li>It still has to be done after the gradients are computed</li> <li>the optimizer should have been set with wd=0 otherwise it will do some L2 regularization, which is exactly what we don't want</li> <li>loop over all the parameters and do our little weight decay update</li> </ul>"},{"location":"KB/Weight/","title":"Weight","text":""},{"location":"KB/Weight/#weight","title":"Weight","text":"<ul> <li>mass x gravitational field\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 strength</li> <li> \\[w = mg\\] </li> </ul>"},{"location":"KB/Weighted%20Alternating%20Least%20Squares/","title":"Weighted Alternating Least Squares","text":""},{"location":"KB/Weighted%20Alternating%20Least%20Squares/#weighted-alternating-least-squares","title":"Weighted Alternating Least Squares","text":"<ul> <li>An algorithm for minimizing the objective function during matrix factorization in recommendation systems, which allows a downweighting of the missing examples. WALS minimizes the weighted KB/Squared Error.md between the original matrix and the reconstruction by alternating between fixing the row factorization and column factorization.</li> </ul>"},{"location":"KB/Wernicke%20Area/","title":"Wernicke Area","text":""},{"location":"KB/Wernicke%20Area/#wernicke-area","title":"Wernicke Area","text":"<ul> <li>Damage to this area causes Wernicke's Aphasia.</li> <li>The individual may speak in long sentences that have no meaning, add unnecessary words, and even create new words.</li> <li>They can make speech sounds, however they have difficulty understanding speech and are therefore unaware of their mistakes.</li> </ul>"},{"location":"KB/Wh-dependencies/","title":"Wh-dependencies","text":""},{"location":"KB/Wh-dependencies/#wh-dependencies","title":"Wh-dependencies","text":"<ul> <li>[What type of network] do you plan to build [t]?</li> <li>Nobody knows [what] the brain is doing [t].</li> </ul>"},{"location":"KB/What%20is%20being%20Transferred%20in%20transfer%20learning/","title":"What is being Transferred in transfer learning","text":""},{"location":"KB/What%20is%20being%20Transferred%20in%20transfer%20learning/#what-is-being-transferred-in-transfer-learning","title":"What is being Transferred in transfer learning","text":"<ul> <li>@neyshaburWhatBeingTransferred2020</li> </ul>"},{"location":"KB/What%20is%20being%20Transferred%20in%20transfer%20learning/#abstract","title":"Abstract","text":"<ul> <li>desired capability for machines is the ability to transfer their knowledge of one domain to another where data is (usually) scarce</li> <li>what enables a successful transfer and which part of the network is responsible for tha</li> <li>series of analyses on transferring to block-shuffled images, we separate the effect of feature reuse from learning low-level statistics of data and show that some benefit of transfer learning comes from the latter when training from pre-trained weights, the model stays in the same basin in the loss landscape and different instances of such model are similar in feature space and close in parameter space</li> </ul>"},{"location":"KB/What%20is%20being%20Transferred%20in%20transfer%20learning/#main-contributions-and-takeaways","title":"Main contributions and takeaways","text":"<ul> <li>For a successful transfer both feature-reuse and low-level statistics of the data are important.</li> <li>Models trained from pre-trained weights make similar mistakes on target domain, have similar features and are surprisingly close in distance in the parameter space</li> <li>hey are in the same basins of the loss landscape models trained from random initialization do not live in the same basin, make different mistakes, have different features and are farther away in 2 distance in the parameter space</li> <li>Modules in the lower layers are in charge of general features and modules in higher layers are more sensitive to perturbation of their parameters.</li> <li>T: trained, P: Pre-trained, RI: random initialization. Therefore we use the following abbreviations for the four models throughout the paper: RI (random initialization), P (pre-trained model), RI-T (model trained on target domain from random initialization), P-T (model trained/fine-tuned on target domain starting from pre-trained weights) </li> <li>IMAGENET </li> <li>CHEXPERT</li> <li>DOMAINNET</li> </ul>"},{"location":"KB/What%20is%20being%20Transferred%20in%20transfer%20learning/#what-is-being-transferred","title":"What is being transferred","text":""},{"location":"KB/What%20is%20being%20Transferred%20in%20transfer%20learning/#role-of-feature-reuse","title":"Role of feature reuse","text":"<ul> <li>Human visual system is compositional and hierarchical: neurons in the primary visual cortex (V1) respond to low level features like edges, while upper level neurons (e.g. the grandmother cell [Gross, 2002]) respond to complex semantic inputs Modern convolutional neural networks trained on large scale visual data are shown to form similar feature hierarchies [Bau et al., 2017, Girshick et al., 2014] The benefits of transfer learning are generally believed to come from reusing the pre-trained feature hierarchy useful when the downstream tasks are too small or not diverse enough to learn good feature representations</li> <li>create a series of modified downstream tasks which are increasingly distant from normal visual domains In particular, we partition the image of the downstream tasks into equal sized blocks and shuffle the blocks randomly The shuffling disrupts high level visual features in those images but keeps the low level statistics about the pixel values intact The extreme case of block size 224 x 224 means no shuffling; in the other extreme case, all the pixels in the image are shuffled, making any of the learned visual features in</li> <li>pre-training completely useless</li> <li>We conclude that feature reuse plays a very important role in transfer learning, especially when the downstream task shares similar visual features with the pre-training domain. But there are other factors at play: in these experiments we change the size of the shuffled blocks all the way to 1 and even try shuffling the channels of the input, therefore, the only object that is preserved here is the set of all pixel values which can be treated as a histogram/distribution We refer to those information as low-level statistics, to suggest that they are void of visual/semantic structural information. The low-level statistics lead to significant benefits of transfer learning, especially on optimization speed.</li> <li>We observe that two instances of P-T are highly similar across different layers. owever, between P-T and RIT instance or two RI-T instances, the similarity is very low. feature similarity is much stronger in the penultimate layer than any earlier layers both between P-T and RI-T instance and two RI-T instances, however, still an order of magnitude smaller than similarity between two P-T layers.</li> <li>These experiments show that the initialization point, whether pre-trained or random, drastically impacts feature similarity, and although both networks are showing high accuracy, they are not that similar in the feature space. two P-T are reusing the same features</li> <li>Distance in parameter space istance between two models in the parameter space More specifically, we measure the <code>L2</code> distance between 2 P-Ts and 2 RI-Ts, both per module and for the entire network Interestingly, RI-Ts are farther from each other compared to two P-Ts</li> <li>distance between modules increases as we move towards higher layers in the network.</li> </ul>"},{"location":"KB/What%20is%20being%20Transferred%20in%20transfer%20learning/#performance-barriers-and-basins-in-the-loss-landscape","title":"Performance barriers and basins in the loss landscape","text":"<ul> <li>A commonly used criterion for better generalization performance is the flatness of the basin of the loss landscape near the final solution. In a flat basin, the weights could be locally perturbed without hurting the performance, while in a narrow basin, moving away from the minimizer would quickly hit a barrier, indicated by a sudden increase in the loss.</li> <li>We evaluate a series of models along the linear interpolation of the two weights { 2 } any two minimizers of a deep network can be connected via a non-linear low-loss path Garipov et al. [2018], Draxler et al. [2018], Fort and Jastrzebski [2019] n contrast, due to the non-linear and compositional structure of neural networks, the linear combination of the weights of two good performing models does not necessarily define a well behaved model, thus performance barriers are generally expected along the linear interpolation path owever, in the case when the two solutions belong to the same flat basin of the loss landscape, the linear interpolation remains in the basin As a result, a performance barrier is absent interpolating two random solutions from the same basin could generally produce solutions closer to the center of the basin, which potentially have better generalization performance than the end points</li> <li>we require that for most points on the basin, their convex combination is on the basin as well. This extra constraint would allow us to have multiple basins that may or may not be connected though a low-loss (nonlinear) path.</li> <li>Given a loss function \\(l: \\mathcal{R}^{n} \\rightarrow \\mathcal{R}^{+}\\), closed convex set \\(S \\subset \\mathcal{R}^{n}\\), S is a \\((\\epsilon , \\delta)\\) basin for l iff S has the following properties<ul> <li>Let \\(U_S\\) be the uniform distribution over set S and \\(\\mu_{S,l}\\) be the expected value of the loss on samples generated from \\(U_S\\). Then,<ul> <li> \\[\\mathcal{E}_{w \\sim U_{S}}[|\\mathcal{l}(w)-\\mu_{S, \\mathcal{l}}] \\leq \\epsilon\\] </li> </ul> </li> <li>For any two points \\(w_{1}, w_{2} \\in S\\), let \\(f(w_{1}, w_{2}) = w_{1}+ \\overset{\\sim}\\alpha(w_{2}-w_{1})\\) where \\(\\overset{\\sim}\\alpha = max\\{\\alpha|w_{1}+ \\alpha(w_{2}-w_{1})\\}\\)<ul> <li> \\[\\mathcal{E}_{w_{1}, w_{2} \\sim U_{S}, v \\sim \\mathcal{N}(0, \\frac{\\delta^{2}}{n}I_{n})}[\\mathcal{l}(f(w_{1}, w_{2})+v )- \\mu_{S,l}] \\geq 2 \\epsilon\\] </li> </ul> </li> <li>Let \\(\\kappa(w_{1}, w_{2}, v) = f(w_{1}, w_{2})+ \\frac{v}{||f(w_{1}, w_{2})-w_{1}||}(f(w_{1}, w_{2})-w_{1})\\). Then,<ul> <li> \\[\\mathcal{E}_{w_{1}, w_{2} \\sim U_{S}, v \\sim \\mathcal{N}(0,\\delta^{2})}[\\mathcal{l}(\\kappa(w_{1}, w_{2}, |v|))-\\mu_{S,l}] \\geq 2 \\epsilon\\] </li> </ul> </li> </ul> </li> <li>there are three requirements for a convex set to be a basin The first requirement is that for most points on the basin, their loss should be close to the expected value of the loss in the basin. This notion is very similar to requiring the loss to have low variance for points on the basin4. The last two requirements ensure that the loss of points in the vicinity of the basin is higher than the expected loss on the basin. In particular, the</li> <li>second requirement does that by adding Gaussian noise to the points in the basin and requiring the loss to be higher than the expected loss in the basin. The third requirement does something similar along the subspaces spanned by extrapolating the points in the basin. That is, if one exits the basin by extrapolating two points on the basin, the loss should increase.</li> <li>Starting with the two P-T solutions, we extrapolated beyond their connecting intervals to find the basin boundary, and calculated the parameters according to Definition 3.1. We found that each pair of P-T solutions live in a (0.0038, 49.14)-basin, (0.0054, 98.28)-basin and (0.0034, 49.14)-basin for rea, cipart and quickdraw, respectively</li> </ul>"},{"location":"KB/What%20is%20being%20Transferred%20in%20transfer%20learning/#module-criticality","title":"Module Criticality","text":"<ul> <li>Given \\(e &gt; 0\\) and network \\(f_\\Theta\\) ,<ul> <li> \\[\\mu_{i, \\epsilon}(f_{\\Theta})= \\underset{0 \\leq \\alpha_{i}, \\sigma_{i} \\leq 1}{min}{\\frac{\\alpha_{i}^{2}||\\theta_{i}^{F}-\\theta_{i}^{0}||^{2}_{Fr}}{\\sigma_{i}^{2}}}:\\{ \\mathbb{E}_{u \\sim \\mathcal{N}(0, \\sigma_{i}^{2})}[\\mathcal{L}_{S}(f_{\\theta_{i}^{\\alpha}}+u,\\Theta_{-i}^{F})] \\leq \\epsilon \\}\\] </li> </ul> </li> <li>different layers of the network show different robustness to perturbation of their weight values [Zhang et al., 2019a]</li> <li>They noted that for some modules, which they called critical, the performance of the model drops significantly after rewinding, while for others the performance is not impacted</li> <li>Module criticality is a measure that can be calculated for each module and captures how critical each module is.</li> <li>module criticality captures the role of the module in the generalization performance of the whole architecture and can be used as a measure of capacity of the module and predict the generalization performance we extend the definition of module criticality by looking at both direct path that linearly connect the initial and final value of the module and the optimization path generated by an optimizer from initialization to the final solution</li> <li>We also look into the final value of a weight matrix in addition to its optimal value during training as the start point of the path for investigating criticality</li> <li>We note a similar pattern as observed in the supervised case. The only difference is that the 'FC' layer becomes critical for P-T model, which is expected.</li> </ul>"},{"location":"KB/What%20is%20being%20Transferred%20in%20transfer%20learning/#which-pre-trained-checkpoint-is-most-useful-for-transfer-learning","title":"Which pre-trained checkpoint is most useful for transfer learning?","text":"<ul> <li>independence between the improvements on optimization speed and final performance. Moreover, this is in line with the loss landscape observations in Section 3.3. Earlier checkpoints in pre-training are out of basin of the converged model and at some point during training we enter the basin (which is the same for pre-train and fine-tune models</li> <li>This also explains the plateau of performance after some checkpoints. herefore, we can start from earlier</li> <li>checkpoints in pre-training.</li> </ul>"},{"location":"KB/What%20is%20being%20Transferred%20in%20transfer%20learning/#conclusion","title":"Conclusion","text":"<ul> <li>We investigated the role of feature reuse through shuffling the blocks of input and showed that when trained from pre-trained weights initialization, the network stays in the same basin of the solution, features are similar and models are close in the L2 distance in parameter space confirmed that lower layers are in charge of more general features</li> <li>one can use top singular values and directions for each module for initialization and investigate if this suffices for good transfer, or ensure initialization at the same basin but adding randomization to enhance diversity and improve generalization</li> <li>taking model average of models in the same basin does not disturb the performance</li> </ul>"},{"location":"KB/What%20is%20being%20Transferred%20in%20transfer%20learning/#images","title":"Images","text":""},{"location":"KB/Whisper/","title":"Whisper","text":""},{"location":"KB/Whisper/#whisper","title":"Whisper","text":"<ul> <li>Audio-to-Text converter</li> <li>multi-lingual speech recognition, translation and language identification</li> <li>goal of a speech recognition system should be to work reliably out of the box in a broad range of environments without requiring supervised fine-tuning of a decoder for every deployment distribution</li> <li>lack of a high-quality pre-trained decoder.</li> <li>680,000 hours of labeled audio data</li> <li>broken in 30 second segments paired with the subset of the transcript that occurs within that time segment.</li> <li>encoder-deccoder transformer</li> </ul>"},{"location":"KB/Whos%20Thinking%2C%20A%20push%20for%20human%20centered%20evaluation%20of%20LLMs/","title":"Whos Thinking, A push for human centered evaluation of LLMs","text":""},{"location":"KB/Whos%20Thinking%2C%20A%20push%20for%20human%20centered%20evaluation%20of%20LLMs/#whos-thinking-a-push-for-human-centered-evaluation-of-llms","title":"Whos Thinking, A Push for Human Centered Evaluation of LLMs","text":"<ul> <li>@dattaWhoThinkingPush2023</li> </ul>"},{"location":"KB/Whos%20Thinking%2C%20A%20push%20for%20human%20centered%20evaluation%20of%20LLMs/#abstract","title":"ABSTRACT","text":"<ul> <li>In this paper, we draw parallels between the relatively mature field of XAI and the rapidly evolving research boom around large language models (LLMs)</li> <li>Specifically, we argue that humans' tendencies\u2014 again, complete with their cognitive biases and quirks\u2014should rest front and center when evaluating deployed LLMs</li> </ul>"},{"location":"KB/Whos%20Thinking%2C%20A%20push%20for%20human%20centered%20evaluation%20of%20LLMs/#large-language-models","title":"Large Language Models","text":"<ul> <li>Current LLM evaluation mechanisms include quantitative metrics measuring notions of accuracy (how similar are the generated outputs to the expected outputs), robustness (how resilient is the model to transformations of the input), calibration (how meaningful are the generated probabilities in respect to uncertainty), efficiency (what are the energy, carbon, and time costs for training and inference) and more [35].</li> <li>There are substantial environmental costs associated with the volume of computational power required for training and inference [6, 49]</li> <li>Counterfactual fairness [24] examines how perturbing the demographic signals of existing test examples can change the performance of the model (e.g. \"He worked at the local hospital\" versus \"She worked at the local hospital\")</li> <li>other concerning forms of biases such as stereotypical associations, erasure, and over-representation in the semantics of its output [35, 38]</li> <li>LLMs have been shown to produce toxic outputs.</li> <li>LLMs often suffer from factual errors\u2014they can \"hallucinate\" information [50] by providing very confident-sounding but entirely false responses</li> <li>Chatbot LLMs have also been found to engage in disturbingly emotional personal conversations when session lengths are not limited [47].</li> <li>There are also privacy concernswork has shown that LLMs are susceptible to training data leakage under adversarial attack [11].</li> </ul>"},{"location":"KB/Whos%20Thinking%2C%20A%20push%20for%20human%20centered%20evaluation%20of%20LLMs/#parallels-between-xai-and-llms","title":"PARALLELS BETWEEN XAI AND LLMS","text":"<ul> <li>LLM outputs are often meant for some downstream decision or task\u2014what email to send to your client, what quick summary of an important document you will read, what answer is provided for a pertinent question</li> <li>Advocates push for first identifying a specific use case, then understanding the types of transparency useful and relevant for each stakeholder in that context, then designing the explanation method with these learnings held front and center, and finally evaluating how helpful the explanation was for specific tasks through practitioner user studies</li> <li> <p>A system is valid if it does what it purports to do. If it doesn't, it is likely because there are issues of alignment between the intent of the system builder and the property the algorithm optimizes [40].</p> </li> <li> <p>Cognitive Engagement</p> </li> <li> <p>Mental Model Matching</p> </li> <li> <p>Use Case Utility</p> </li> </ul>"},{"location":"KB/Whos%20Thinking%2C%20A%20push%20for%20human%20centered%20evaluation%20of%20LLMs/#why-is-this-important","title":"WHY IS THIS IMPORTANT","text":"<ul> <li>the potential scale of LLM usage is massive as a fundamentally lower-level ML- based building block than XAI</li> <li>ChatGPT [43] set historic records for its customer growth, with over 100 million users in its first 2 months [2].</li> <li>Unlike XAI, whose users are largely technical practitioners [7] focusing on one pipeline, LLMs are often designed for public use and are meant to help with a wide variety of tasks.</li> <li>The ability to influence or make decisions is a form of inherent power, and offloading cognition onto AI agents must first be met with caution.</li> <li>The consequences of not having a qualitative understanding of how humans interact with LLM outputs is grave</li> </ul>"},{"location":"KB/Wickelphones/","title":"Wickelphones","text":""},{"location":"KB/Wickelphones/#wickelphones","title":"Wickelphones","text":"<ul> <li>Pro: capture just enough info about the context that determines irregular past-tense verb forms ense verbs, e.g.sing -sang, ring -rang</li> <li>1 wickelphone = 1 input unit and 1 output unit, connection matrix of 42875 * 42875!!</li> <li>Instead, reduce wickelphones to wickelfeatures (1210), where each wickelphone becomes 16 features</li> <li>Verbs with same stem and past tense</li> <li>English has many verbs where the past and stem are the same:<ul> <li>put, fit, spread</li> </ul> </li> <li>Usually verbs ending with -t or -d are likely have no-change</li> <li>Model also started to let the past be the same as the stem</li> <li>Verbs with vowel change</li> <li>Model made two errors older children have been shown to make: 1. stem + ed, e.g. comed, singed 2. past-form + ed, e.g. camed, sanged</li> <li>Does not explain differences in response times between irregular and regular verbs</li> <li>Models production only, not comprehension</li> <li>You can't reverse the model like you can reverse a rule</li> <li>Model can't generalize</li> <li>Computational models (Rule-based, Connectionist and MBL) all use adult vocabulary as input to simulate children's learning</li> <li>Children don't show vocabulary burst between State 2 and 3 (needed to produce U-shaped learning with Connectionist Model)</li> <li>Thousands of exposures</li> <li>No distinction between tokens and types, each verb simply included once</li> </ul>"},{"location":"KB/Wide%20Deep%20Recommender/","title":"Wide Deep Recommender","text":""},{"location":"KB/Wide%20Deep%20Recommender/#wide-deep-recommender","title":"Wide Deep Recommender","text":"<ul> <li>Wide &amp; Deep Learning for Recommender Systems</li> <li>Generalized linear models with nonlinear feature transformations are widely used for large-scale regression and classification problems with sparse inputs.</li> <li>Memorization of feature interactions through a wide set of cross-product feature transformations are effective and interpretable, while generalization requires more feature engineering effort</li> <li>However, memorization and generalization are both important for recommender systems.</li> <li>With less feature engineering, deep neural networks can generalize better to unseen feature combinations through low-dimensional dense embeddings learned for the sparse features</li> <li>However, deep neural networks with embeddings can over-generalize and recommend less relevant items when the user-item interactions are sparse and high-rank.</li> <li>jointly trained wide linear models and deep neural networks \u2013 to combine the benefits of memorization and generalization for recommender systems</li> <li>Wide linear models can effectively memorize sparse feature interactions using cross-product feature transformations, while deep neural networks can generalize to previously unseen feature interactions through low dimensional embeddings</li> <li>In other words, the fusion of wide and deep models combines the strengths of memorization and generalization, and provides us with better recommendation systems</li> <li>The two models are trained jointly with the same loss function.</li> <li>Google Play Store</li> </ul>"},{"location":"KB/Width%20Efficiency%20of%20Neural%20Networks/","title":"Width Efficiency of Neural Networks","text":""},{"location":"KB/Width%20Efficiency%20of%20Neural%20Networks/#width-efficiency-of-neural-networks","title":"Width Efficiency of Neural Networks","text":"<ul> <li>there exist classes of wide, shallow networks that can only be expressed by narrow networks with polynomial depth</li> <li>polynomial lower bound on width is less restrictive than the exponential lower bound on depth, suggesting that depth is more important</li> <li>the price for making the width small is only a linear increase in the network depth for networks with ReLU activation</li> </ul>"},{"location":"KB/Window%20Based%20Regression/","title":"Window Based Regression","text":""},{"location":"KB/Window%20Based%20Regression/#window-based-regression","title":"Window Based Regression","text":"<ul> <li>TIme Series</li> <li>input window : \\(\\(u(t-d+1), u(t-d+2), \u2026. , u(t-1) , u(t)\\)\\)</li> <li>Require regression function (\\(f:(\\mathbb{R}^k)^d \\rightarrow \\mathbb{R}^m\\)\\)<ul> <li>\\(\\(k \\times d\\)\\) dim matrix</li> <li>Flatten into \\(\\(d \\cdot k\\)\\) vector and apply Quadratic Loss</li> </ul> </li> </ul>"},{"location":"KB/Window%20Based%20Regression/#non-linearity","title":"Non Linearity","text":"<ul> <li>Add fixed nonlinear transforms to input arguments : eg polynomials</li> <li>Volterra expansion</li> </ul>"},{"location":"KB/Wisdom%20of%20the%20Crowd/","title":"Wisdom of the Crowd","text":""},{"location":"KB/Wisdom%20of%20the%20Crowd/#wisdom-of-the-crowd","title":"Wisdom of the Crowd","text":"<ul> <li>The idea that averaging the opinions or estimates of a large group of people (\"the crowd\") often produces surprisingly good results.</li> </ul>"},{"location":"KB/Word%20Blending/","title":"Word Blending","text":""},{"location":"KB/Word%20Blending/#word-blending","title":"Word Blending","text":"<ul> <li>Parts of two different words are combined</li> <li>Breakfast+lunch : brunch</li> <li>Smoke+fog:smog</li> </ul>"},{"location":"KB/Word%20Clipping/","title":"Word Clipping","text":""},{"location":"KB/Word%20Clipping/#word-clipping","title":"Word Clipping","text":"<ul> <li>Longer words are shortened</li> <li>Doctor, laboratory, refrigerator</li> </ul>"},{"location":"KB/Word%20Compounding/","title":"Word Compounding","text":""},{"location":"KB/Word%20Compounding/#word-compounding","title":"Word Compounding","text":"<ul> <li>Words formed by combining two or more words</li> <li>Adj +Adj= Adj bitter + sweet : bitter-sweet</li> <li>N + N = N rain+bow rain-bow</li> </ul>"},{"location":"KB/Word%20Segmentation/","title":"Word Segmentation","text":""},{"location":"KB/Word%20Segmentation/#word-segmentation","title":"Word Segmentation","text":"<ul> <li>breaks up the sequence of characters in a text by locating the word boundaries</li> <li>Maximum Matching Algorithm</li> <li>Forward Backward Matching</li> <li>Statistical Word Segmentation</li> <li>Lexical Word Segmentation</li> <li>Hybrid Word Segmentation</li> </ul>"},{"location":"KB/Word%20Structure/","title":"Word Structure","text":""},{"location":"KB/Word%20Structure/#word-structure","title":"Word Structure","text":"<ul> <li>Isolating words</li> <li>Agglutinating words</li> <li>Inflectional words</li> <li>Polysynthetic words</li> </ul>"},{"location":"KB/Word%20Vectors/","title":"Word Vectors","text":""},{"location":"KB/Word%20Vectors/#word-vectors","title":"Word Vectors","text":"<ul> <li>Essentially word Embedding</li> <li>Text processing</li> <li>We can represent ideas/sentences/documents as vectors to feed into any kind of model</li> <li>Useful because vectors can be easily compared to find similarity<ul> <li>eg : Cosine Similarity</li> </ul> </li> <li>Higher dimensions make it challenging</li> <li></li> <li>Vectors that are metrically close to each other</li> <li>GloVE</li> <li></li> </ul>"},{"location":"KB/Word2Vec/","title":"Word2Vec","text":""},{"location":"KB/Word2Vec/#word2vec","title":"Word2Vec","text":"<ul> <li>Efficient Estimation of Word Representations in Vector Space</li> <li>Duality of vector representations of words derived by various models on a collection of syntactic and semantic language tasks involving word similarity</li> <li>possible to train high quality word vectors using very simple model architectures</li> <li>[Skip Gram] or CBOW</li> </ul>"},{"location":"KB/Word2Vec/#training","title":"Training","text":"<ul> <li>Ask it to predict a vector with probabilities</li> <li>Find error vector</li> <li>Update params</li> <li>to generate high-quality embeddings using a high-performance model, we can switch the model\u2019s task from predicting a neighboring word And switch it to a model that takes the input and output word, and outputs a score indicating if they\u2019re neighbors or not (0 for \u201cnot neighbors\u201d, 1 for \u201cneighbors\u201d).</li> <li>This simple switch changes the model we need from a neural network, to a logistic regression model \u2013 thus it becomes much simpler and much faster to calculate. + Negative Sampling</li> <li>Embedding and Context matrices randomly initialized</li> <li></li> <li></li> </ul>"},{"location":"KB/Word2Vec/#hyperparams","title":"Hyperparams","text":""},{"location":"KB/Word2Vec/#window-size","title":"Window Size","text":"<ul> <li>smaller window sizes (2-15) lead to embeddings where high similarity scores between two embeddings indicates that the words are interchangeable (notice that antonyms are often interchangable if we\u2019re only looking at their surrounding words \u2013 e.g. good and bad often appear in similar contexts).</li> <li>Larger window sizes (15-50, or even more) lead to embeddings where similarity is more indicative of relatedness of the words</li> <li>default is 5 (two words - word - two words)</li> </ul>"},{"location":"KB/Word2Vec/#no-of-negative-samples","title":"No of Negative Samples","text":"<ul> <li>The original paper prescribes 5-20 as being a good number of negative samples. It also states that 2-5 seems to be enough when you have a large enough dataset. The Gensim default is 5 negative samples.</li> </ul>"},{"location":"KB/Words-and-Rules/","title":"Words-and-Rules","text":""},{"location":"KB/Words-and-Rules/#words-and-rules","title":"Words-and-Rules","text":"<ul> <li>Look up past-tense         - No past-tense stored? Generate form<ul> <li>Why symbolic? It uses the abstract 'verb'</li> <li>Rules that refer to these categories are used to guide processing</li> <li>past tense</li> <li>Irregular forms stored in associative memory (declarative memory)</li> <li>Symbolic rules produce past tense forms (procedural memory)</li> <li>look-up is quicker than rule application</li> <li>rule application takes more time but is always done 'on-the-fly'</li> <li>Time 1: Every form is memorized (irregular and regular)</li> <li>Time 2: child notices pattern: verb root+ed = past</li> <li>Child creates a rule</li> <li>Child applies rule to all forms: overgeneralization</li> <li>Time 3: Child realizes that there are irregular and regular forms<ul> <li>creates a dual system: irregular forms are retrieved from memory, regular forms are created by a rule</li> <li>new and novel verbs will get regular endings in past tense unless exposed to irregular past</li> <li>the fact that children seem to learn a rule = language must be symbolic</li> </ul> </li> <li>Traditional U-shaped learning predicts children won't be able to create past-tense forms for novel verbs when they are in the initial stage. This isn't consistent with data</li> <li>Overregularization is not common</li> <li>Cannot account for the presence of two past forms<ul> <li>e.g. dream/dreamed-dreamt, light/lit-lighted</li> </ul> </li> <li>In production experiments<ul> <li>Irregulars produced faster</li> <li>Frequent irregulars are produced faster than infrequent irregulars (Prasada et al., 1990; Albright &amp; Hayes 2003)</li> <li>No difference between frequent and infrequent regulars</li> </ul> </li> <li>Maybe because experiments always present the root form? (1) This is a girl who knows how to dance. She did the same<ul> <li>Root presentation might mask differences due to frequency in regulars</li> </ul> </li> </ul> </li> <li>Challenges of Words-and-rules</li> </ul>"},{"location":"KB/Work%20Envelope/","title":"Work Envelope","text":""},{"location":"KB/Work%20Envelope/#work-envelope","title":"Work Envelope","text":"<ul> <li>The set of all points which a manipulator can reach without intrusion. Sometimes the shape of the work space, and the position of the manipulator itself can restrict the work envelope.</li> </ul>"},{"location":"KB/Wrist/","title":"Wrist","text":""},{"location":"KB/Wrist/#wrist","title":"Wrist","text":"<ul> <li>A set of rotary joints between the arm and the robot end-effector that allow the end-effector to be oriented to the work-piece. In most cases the wrist can have degrees of freedom which enable it to grasp an object with roll, pitch, and yaw orientation.</li> </ul>"},{"location":"KB/X%20Vectors/","title":"X Vectors","text":""},{"location":"KB/X%20Vectors/#x-vectors","title":"X Vectors","text":"<ul> <li>X-Vectors: Robust DNN Embeddings for Speaker Recognition</li> <li>data augmentation to improve performance of deep neural network (DNN) embeddings for speaker recognition</li> <li>trained to discriminate between speakers, maps variable-length utterances to fixed-dimensional embeddings called x-vectors</li> <li>prior studies have found that embeddings leverage large-scale training datasets better than i-vectors, it can be challenging to collect substantial quantities of labeled data for training</li> <li>use data augmentation, consisting of added noise and reverberation, as an inexpensive method to multiply the amount of training data and improve robustness</li> <li>Their data augmentation strategy employs additive noises and reverberation</li> <li>Reverberation involves convolving room impulse responses (RIR) with audio</li> <li>simulated RIRs described by Ko et al.</li> <li>reverberation itself is performed with the multicondition training tools in the Kaldi ASpIRE recipe</li> <li>For additive noise, they use the MUSAN dataset,</li> <li>PLDA classifier is used in the x-vector framework to make the final decision, similar to i-vector systems</li> <li>x-vectors are compared with i-vector baselines on Speakers in the Wild and NIST SRE 2016 Cantonese where they achieve superior performance on the evaluation datasets</li> </ul>"},{"location":"KB/XAI/","title":"XAI","text":""},{"location":"KB/XAI/#xai","title":"XAI","text":"<pre><code>graph LR;\n\nNode_0[deep inside convolutional networks visualising image classification models and saliency maps]\nNode_31[striving for simplicity the all convolutional net]\nNode_35[the unreliability of saliency methods]\nNode_3[real time image saliency for black box classifiers]\nNode_33[network dissection quantifying interpretability of deep visual representations]\nNode_5[understanding deep networks via extremal perturbations and smooth masks]\nNode_36[methods for interpreting and understanding deep neural networks]\nNode_38[explaining explanations an overview of interpretability of machine learning]\nNode_8[explainable artificial intelligence xai concepts taxonomies opportunities and challenges toward responsible ai]\nNode_9[did the model understand the question]\nNode_29[smoothgrad removing noise by adding noise]\nNode_34[how important is a neuron]\nNode_12[computationally efficient measures of internal neuron importance]\nNode_30[learning important features through propagating activation differences]\nNode_46[a unified approach to interpreting model predictions]\nNode_15[influencedirected explanations for deep convolutional networks]\nNode_40[towards better understanding of gradientbased attribution methods for deep neural networks]\nNode_17[a survey on neural network interpretability]\nNode_18[opportunities and challenges in explainable artificial intelligence xai a survey]\nNode_19[explainable artificial intelligence for tabular data a survey]\nNode_47[peeking inside the blackbox a survey on explainable artificial intelligence xai]\nNode_21[explainable artificial intelligence xai in deep learningbased medical image analysis]\nNode_22[a systematic review of human computer interaction and explainable artificial intelligence in healthcare with artificial intelligence techniques]\nNode_42[explainer a visual analytics framework for interactive and explainable machine learning]\nNode_24[visual analytics for humancentered machine learning]\nNode_25[exploiting explanations for model inversion attacks]\nNode_26[if only we had better counterfactual explanations five key deficits to rectify in the evaluation of counterfactual xai techniques]\nNode_27[visualizing and understanding convolutional networks]\nNode_28[network in network]\nNode_32[sanity checks for saliency maps]\nNode_37[interpretation of neural networks is fragile]\nNode_39[explanations can be manipulated and geometry is to blame]\nNode_41[distilling the knowledge in a neural network]\nNode_43[why should i trust you explaining the predictions of any classifier]\nNode_44[explainable ai beware of inmates running the asylum or how i learnt to stop worrying and love the social and behavioural sciences]\nNode_45[explanation in humanai systems a literature metareview synopsis of key ideas and publications and bibliography for explainable ai]\nNode_0 --&gt; Node_27\nNode_31 --&gt; Node_28\nNode_31 --&gt; Node_27\nNode_35 --&gt; Node_29\nNode_35 --&gt; Node_30\nNode_35 --&gt; Node_27\nNode_3 --&gt; Node_31\nNode_3 --&gt; Node_27\nNode_33 --&gt; Node_27\nNode_5 --&gt; Node_31\nNode_5 --&gt; Node_29\nNode_5 --&gt; Node_32\nNode_5 --&gt; Node_27\nNode_36 --&gt; Node_31\nNode_36 --&gt; Node_27\nNode_38 --&gt; Node_33\nNode_38 --&gt; Node_30\nNode_38 --&gt; Node_27\nNode_8 --&gt; Node_28\nNode_9 --&gt; Node_31\nNode_9 --&gt; Node_30\nNode_29 --&gt; Node_30\nNode_34 --&gt; Node_30\nNode_12 --&gt; Node_34\nNode_12 --&gt; Node_30\nNode_30 --&gt; Node_31\nNode_46 --&gt; Node_30\nNode_15 --&gt; Node_31\nNode_15 --&gt; Node_35\nNode_15 --&gt; Node_34\nNode_40 --&gt; Node_36\nNode_40 --&gt; Node_29\nNode_40 --&gt; Node_30\nNode_40 --&gt; Node_27\nNode_17 --&gt; Node_31\nNode_17 --&gt; Node_37\nNode_17 --&gt; Node_35\nNode_17 --&gt; Node_33\nNode_17 --&gt; Node_36\nNode_17 --&gt; Node_38\nNode_17 --&gt; Node_39\nNode_17 --&gt; Node_32\nNode_17 --&gt; Node_30\nNode_17 --&gt; Node_27\nNode_17 --&gt; Node_40\nNode_18 --&gt; Node_31\nNode_18 --&gt; Node_37\nNode_18 --&gt; Node_35\nNode_18 --&gt; Node_32\nNode_18 --&gt; Node_30\nNode_18 --&gt; Node_27\nNode_18 --&gt; Node_40\nNode_19 --&gt; Node_30\nNode_47 --&gt; Node_41\nNode_47 --&gt; Node_33\nNode_47 --&gt; Node_29\nNode_47 --&gt; Node_27\nNode_21 --&gt; Node_31\nNode_21 --&gt; Node_32\nNode_21 --&gt; Node_27\nNode_22 --&gt; Node_42\nNode_42 --&gt; Node_30\nNode_42 --&gt; Node_40\nNode_24 --&gt; Node_42\nNode_25 --&gt; Node_33\nNode_25 --&gt; Node_38\nNode_25 --&gt; Node_29\nNode_25 --&gt; Node_32\nNode_25 --&gt; Node_30\nNode_25 --&gt; Node_27\nNode_26 --&gt; Node_43\nNode_26 --&gt; Node_44\nNode_26 --&gt; Node_45\nNode_26 --&gt; Node_46\nNode_26 --&gt; Node_27\nNode_26 --&gt; Node_47\n</code></pre>"},{"location":"KB/XLM-R/","title":"XLM-R","text":""},{"location":"KB/XLM-R/#xlm-r","title":"XLM-R","text":"<ul> <li>Unsupervised Cross-lingual Representation Learning at Scale</li> <li>pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks</li> <li>Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data</li> <li>significantly outperforms multilingual BERT</li> <li>low-resource languages</li> <li>positive transfer and capacity dilution</li> <li>performance of high and low resource languages at scale</li> <li>possibility of multilingual modeling without sacrificing per-language performance</li> </ul>"},{"location":"KB/XLNet/","title":"XLNet","text":""},{"location":"KB/XLNet/#xlnet","title":"XLNet","text":"<ul> <li>XLNet: Generalized Autoregressive Pretraining for Language Understanding</li> <li>modeling bidirectional contexts</li> <li>denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on KB/Autoregressive.md language modeling</li> <li>However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy</li> <li>generalized [autoregressive] pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order (thereby proposing a new objective called Permutation Language Modeling), and (2) overcomes the limitations of BERT thanks to its autoregressive formulation</li> <li>uses a permutation language modeling objective to combine the advantages of KB/Autoregressive.md and autoencoder methods</li> </ul>"},{"location":"KB/XLSR/","title":"XLSR","text":""},{"location":"KB/XLSR/#xlsr","title":"XLSR","text":""},{"location":"KB/Xavier%20Initialization/","title":"Xavier Initialization","text":""},{"location":"KB/Xavier%20Initialization/#xavier-initialization","title":"Xavier Initialization","text":"<ul> <li> \\[\\mathrm{a=\\sqrt{\\frac{6}{\\left(\\mathrm{d}\\mathrm{_{\\mathrm{in}}^{\\mathrm{ }}}+\\mathrm{d}_{\\mathrm{out}} \\right)}}}\\] </li> <li>Random values drawn uniformly from \\([-a,a]\\)</li> <li>For Batch Normalization Layers, \\(\\gamma =1\\) and \\(\\beta=0\\)</li> <li>For Tanh based activating neural nets</li> </ul>"},{"location":"KB/Xception/","title":"Xception","text":""},{"location":"KB/Xception/#xception","title":"Xception","text":"<ul> <li>@cholletXceptionDeepLearning2017</li> <li>Only use Depthwise Separable convs + Inception modules</li> <li>Cross channel and spatial correlations can be decoupled completely</li> </ul>"},{"location":"KB/YFCC100M/","title":"YFCC100M","text":""},{"location":"KB/YFCC100M/#yfcc100m","title":"YFCC100M","text":"<ul> <li>Yahoo Flickr Creative Commons 100 Million Dataset </li> <li>large public multimedia collection from Flickr, consisting of 100 million media data, of which around 99.2 million are images and 0.8 million are videos </li> <li>statistics on hashtags used in the YFCC100M dataset show that the data distribution is severely unbalanced</li> </ul>"},{"location":"KB/YOLO/","title":"YOLO","text":""},{"location":"KB/YOLO/#yolo","title":"YOLO","text":"<ul> <li>You Only Look Once: Unified, Real-Time Object Detection</li> <li>LinearRegression problem</li> <li>predicts bounding boxes and class probabilities directly from full images in one evaluation</li> <li>loss function that directly corresponds to detection performance and the entire model is trained jointly</li> <li>Picasso Dataset, People Art Dataset</li> </ul>"},{"location":"KB/Yaw/","title":"Yaw","text":""},{"location":"KB/Yaw/#yaw","title":"Yaw","text":"<ul> <li>Rotation of the end-effector in a horizontal plane around the end of the manipulator arm. Side to side motion at an axis.</li> </ul>"},{"location":"KB/You%20can%20play%2020%20questions%20with%20nature%20and%20win/","title":"You Can Play 20 Questions with Nature and Win","text":""},{"location":"KB/You%20can%20play%2020%20questions%20with%20nature%20and%20win/#you-can-play-20-questions-with-nature-and-win","title":"You Can Play 20 Questions with Nature and Win","text":"<ul> <li>You can play 20 questions with nature and win: Categorical versus coordinate spatial relations as a case study</li> <li>Stephen M. Kosslyn</li> </ul>"},{"location":"KB/You%20can%20play%2020%20questions%20with%20nature%20and%20win/#intro","title":"Intro","text":"<ul> <li>Alan Newell famously asserted that \"You can't play 20 questions with nature and win\" (Newell, A. (1973)</li> <li>focused on the futility of studying binary distinctions</li> <li>However, the distinction between categorical and coordinate spatial relations representations has turned out to be fruitful</li> <li>First, from the outset this distinction was cast within the context of a theory of a more general processing system; second, it was formulated from the perspective of multiple levels of analysis within a processing system, and thereby bridges characteristics of information processing with characteristics of the brain.</li> <li>In the game of 20 questions, one player thinks of an object or situation, and the others attempt to guess it by asking a series of binary questions: is it living? is it an animal? is it domesticated?</li> <li>Each question reduces the search space, and eventually a questioner can pounce on just the right answer</li> <li>Newell argued that this game is a bad model for how science should be conducted.</li> <li>decried the tendency of psychologists to formulate and test binary distinctions\u2014 such as those between episodic versus semantic memory, serial versus parallel search, and gradual versus all-or-none learning</li> <li>more often than not such distinctions are illusory, and after an enormous amount of research ultimately all we know is that nature resists clear-cut binary divisions.</li> <li>We should consider how to fit the available data together into a single coherent story.</li> <li>And in his view, the best way to do this is to attempt to build computer simulation models that mimic human performance.</li> </ul>"},{"location":"KB/You%20can%20play%2020%20questions%20with%20nature%20and%20win/#drawing-distinctions-within-processing-systems","title":"Drawing Distinctions Within Processing Systems","text":"<ul> <li>fundamental problem with most (if not all) of the binary distinctions that Newell railed against</li> <li>distinctions were formulated independently of concerns about how the putative representations or processes would operate within the context of a more general processing system</li> </ul>"},{"location":"KB/You%20can%20play%2020%20questions%20with%20nature%20and%20win/#divide-and-conquer","title":"Divide-and-conquer","text":"<ul> <li>complex tasks never are accomplished by a single process, all in one swoop</li> <li>most tasks are treated as if they are combinations of simpler sub-tasks, each of which is grappled with by a separate aspect of the overall processing system.</li> <li>brain has clearly divided processing of object properties, such as shape and color, from processing of spatial properties, such as location</li> <li>Location is registered by a system that processes spatial properties\u2014the socalled \"dorsal system\", which runs from the occipital lobe to posterior parietal cortex.</li> <li>Thus, the two problems (recognizing objects in different locations and being able to specify location) have contradictory requirements\u2014and it is rather elegant that the brain deals with each in a separate system.</li> </ul>"},{"location":"KB/You%20can%20play%2020%20questions%20with%20nature%20and%20win/#weak-modularity","title":"Weak Modularity","text":"<ul> <li>The brain has numerous specialized systems</li> <li>But these systems are not \"modules\" of the sort proposed by Fodor (1983).</li> <li>Fodor's modules are independent, in the sense that the workings of one cannot affect the inner workings of another</li> <li>However, given the nature of the neuroanatomy of the brain, we are better off conceptualizing processing in terms of neural networks\u2014 which may share some cortex and some types of processing.</li> <li>Moreover, we should expect \"leakage\" between these systems. Aspects of a theory of high-level vision</li> <li>Kosslyn &amp; Koenig, 1992)</li> </ul>"},{"location":"KB/You%20can%20play%2020%20questions%20with%20nature%20and%20win/#aspects-of-high-level-vision","title":"Aspects of High Level Vision","text":""},{"location":"KB/You%20can%20play%2020%20questions%20with%20nature%20and%20win/#visual-buffer","title":"Visual Buffer","text":"<ul> <li>visual input during perception is organized in a series of KB/Brain Areas.md in the occipital lobe, which I have grouped into a single function structure called the visual buffer</li> <li>These areas are topographically organized, such that the pattern of activation over the surface of the cortex (roughly) preserves the pattern of activation on the retina.</li> <li>Most of the connections among neurons in these areas are short and inhibitory</li> <li>The output from the visual buffer is a representation of edges and regions of an object</li> </ul>"},{"location":"KB/You%20can%20play%2020%20questions%20with%20nature%20and%20win/#object-properties-processing-system","title":"Object Properties Processing System","text":"<ul> <li>Output from the visual buffer flows into the ventral system, where it is compared to stored visual memories</li> <li>If a match is found, the object (or part of an object) is recognized. Spatial properties processing system</li> <li>Output from the visual buffer also flows into the dorsal system, where location and other spatial properties are computed.</li> </ul>"},{"location":"KB/You%20can%20play%2020%20questions%20with%20nature%20and%20win/#long-term-associative-memory","title":"Long-term Associative Memory","text":"<ul> <li>The outputs from the object properties processing and spatial properties processing systems converge on long-term associative memories</li> <li>Such memories specify the spatial relations among objects or parts of objects. A problem in vision and a possible solution</li> <li>The distinction between the ventral and dorsal systems makes sense from the perspective of the two principles briefly outlined earlier, divide-and-conquer and weak modularity</li> <li>How can the visual system identify objects when they can project an almost infinite number of images?</li> <li>no new parts are added to the image when the object is contorted in its many and varied ways, although some parts may be occluded</li> <li>Thus, if a sufficient number of individual parts can be recognized, this is a strong indication that a specific object is present</li> <li>the spatial relations between parts remain constant if they are described in a relatively abstract way</li> </ul>"},{"location":"KB/You%20can%20play%2020%20questions%20with%20nature%20and%20win/#categorical-spatial-relation","title":"Categorical Spatial Relation","text":"<ul> <li>A category is an equivalence class; for instance, if you hold one hand next to the other, the first will remain left or right of the second no matter how high, low, or far away it is from the other hand. Once assigned to the category, the spatial relations are treated as equivalent, with any differences (e.g. between a bent versus outstretched arm) ignored.</li> <li>However, the dorsal system cannot compute only categorical spatial relations representations.</li> <li>Such representations are useless for another key role of the dorsal system, namely reaching and navigation</li> <li>Knowing that a table is \"in front of\" you (a categorical spatial relation) will not help you walk around it, or pull your chair up to it</li> <li>In these cases you need precise metric information, and you need such</li> <li>information relative to your body, a part of your body, or relative to another object that serves as an \"origin\"</li> </ul>"},{"location":"KB/You%20can%20play%2020%20questions%20with%20nature%20and%20win/#coordinate-spatial-relation","title":"Coordinate Spatial Relation","text":"<ul> <li>categorical spatial relations representations typically can be captured by a word or two, and the left cerebral hemisphere is better than the right at such processing</li> <li>coordinate spatial relations representations are essential for navigation, and the right cerebral hemisphere is better than the left at such processing</li> <li>In short, here is an example of a situation where 20 questions seems to be working</li> <li>At the first cut, we divided the entire system into two coarsely defined subsystems, distinguishing between the object-properties-processing ventral system and the spatial-properties-processing dorsal system</li> <li>At the second cut, we focused on the dorsal system, and now divided it into two more finely characterized subsystems, which compute categorical versus coordinate spatial relations representations.</li> </ul>"},{"location":"KB/You%20can%20play%2020%20questions%20with%20nature%20and%20win/#levels-of-analysis","title":"Levels of Analysis","text":"<ul> <li>based largely on that of Marr (1982), but adapted in various ways to be more appropriate for cognitive processing rather than vision per se</li> <li>a fundamental characteristic of a theory of a processing system is that it begins with an analysis of the task to be accomplished</li> <li>The theory of the computation can be conceptualized as specifying a black box, which takes a specific input and produces a specific output; this output in turn is used as input to yet other processes.</li> <li>According to Marr, whereas a theory of the computation describes what is computed, a theory of the algorithm specifies how it is computed.</li> <li>An algorithm consists of a step-by-step procedure that guarantees that a certain output will be produced on the basis of a certain input.</li> <li>Finally, algorithms are implemented in hardware (on a computer) or \"wetware\" (in a brain)</li> <li>The level of the implementation specifies how an algorithm is physically realized</li> <li>This observation seems particularly relevant to the encoding and use of spatial relations representations (e.g. Baciu et al., 1999; Kosslyn et al., 1998).</li> <li>Interdependence among levels</li> <li>Marr sometimes wrote as if a theory at one level of analysis could be formulated with only weak links to theories at the other levels.</li> <li>However, computations rely on algorithms, and those algorithms have to operate in</li> <li>a brain that does some things well and other things not so well</li> <li>In addition, as evolution progressed, older parts of the brain often were relatively preserved\u2014new areas were added, but the old ones rarely were redesigned from scratch.</li> <li>Thus, the newer portions had to work with the older ones, which may not have been optimal for the final product (cf. Allman, 1999).</li> <li>characteristics at each of the levels of analysis affect theorizing at the other levels\u2014 and hence a powerful approach to theorizing about cognition requires that all three levels of analysis be considered at the same time.</li> <li>At the level of the algorithm, conceptualizing processing within the context of the larger system played a central role; the fact that object properties and spatial properties are processed separately provided a key constraint on the theory of what is computed and how such computation proceeds</li> <li>the idea that the two cerebral hemispheres would differ for the two kinds of processing not only helps to specify the nature of the representations and processes, but also offers one way to test the hypothesis.</li> <li>Leveraging multi-level theories</li> </ul>"},{"location":"KB/You%20can%20play%2020%20questions%20with%20nature%20and%20win/#why-is-it-important-that-scientists-be-able-to-play-20-questions-with-nature-and-win","title":"Why is it Important That Scientists Be Able to Play 20 Questions with Nature and Win?","text":"<ul> <li>One reason is simple: cognitive processing is extraordinarily complex, and we must find ways to gain traction in studying it.</li> <li>I argue that multi-level theories, which bridge from information processing to the brain, should play a special role in playing the science game of 20 questions.</li> <li>First, they lead researchers to collect different sorts of data.</li> <li>when theorizing on the basis of such varied types of data, there are more constraints on the theory.</li> <li>Moreover, multi-level theories must respect qualitatively different sorts of constraints simultaneously</li> <li>Perhaps paradoxically, the more constraints that are available the easier it is to theorize, even though it is more difficult to fit all the constraints together within a common framework</li> <li>Newell was troubled not simply by the failure of most binary distinctions to lead to fruitful research, but also by the lack of accumulation of such results.</li> <li>He had the sense that research was not accumulating to paint a coherent overall picture, but instead isolated fragments of knowledge were being collected.</li> <li>The brain is, after all, a single organ.</li> </ul>"},{"location":"KB/Z%20Normalization/","title":"Z Normalization","text":""},{"location":"KB/Z%20Normalization/#z-normalization","title":"Z Normalization","text":"<ul> <li>Centering and rescaling the data so that a zero mean and unit variance is obtained.</li> <li>Only compute the parameters \u03bck,\u03c3k on the training set!</li> <li>For image data, normalization is not done per pixel but computed over all the pixels</li> </ul>"},{"location":"KB/Z-Space%20Entanglement/","title":"Z-Space Entanglement","text":""},{"location":"KB/Z-Space%20Entanglement/#z-space-entanglement","title":"Z-Space Entanglement","text":"<ul> <li>Another challenge with controllable generation is referred to as entanglement in Z-space.</li> <li>When the z-space is entangled, this means movement in different directions has an effect on multiple features in the output simultaneously.</li> <li>Even if these features aren't correlated, an entangled z-space results in a single feature change modifying more than one feature in the output.</li> <li>Entanglement happens commonly if the number of dimensions in the z-space isn't large enough</li> </ul>"},{"location":"KB/Zeiler%20Fergus/","title":"Zeiler Fergus","text":""},{"location":"KB/Zeiler%20Fergus/#zeiler-fergus","title":"Zeiler Fergus","text":"<ul> <li>multiple interleaved layers of Conv, non-linear Activation Functions, local response normalizations, and max Pooling</li> </ul>"},{"location":"KB/Zero%20Label%20Language%20Learning/","title":"Zero Label Language Learning","text":""},{"location":"KB/Zero%20Label%20Language%20Learning/#zero-label-language-learning","title":"Zero Label Language Learning","text":"<ul> <li>Towards Zero-Label Language Learning</li> <li>Unsupervised Data Generation</li> <li>SuperGLUE</li> <li>Treat LMs as few-shot generators (rather than few-shot learners)</li> <li>Create prompts with  pair(s) <li>Ask the model to generate more for the same label</li> <li>The emphasis is on the labelled data generation (rather than inference)</li> <li>The new idea is about generating more data and going with conventional route</li> <li>This paper confirms all the above by introducing UDG using LMs, even for complex higher-order tasks and empirically shows classical fine-tuning with more data works better.</li>"},{"location":"KB/architecture/","title":"Architecture","text":""},{"location":"KB/architecture/#categories-anchor","title":"categories: ['anchor']","text":"<ul> <li>./ADVENT.md</li> <li>./ALBERT.md</li> <li>./Adagrad.md</li> <li>Adaptive Input Representation.md</li> <li>Additive Attention.md</li> <li>Affordance Detection Task Specific.md</li> <li>Alex Net.md</li> <li>./Alphacode.md</li> <li>Attention NMT.md</li> <li>./Attention.md</li> <li>./AudioLM.md</li> <li>Auto Encoders.md</li> <li>./AutoDistill.md</li> <li>./BART.md</li> <li>./BERT.md</li> <li>Bahdanau Attention.md</li> <li>Basic GAN.md</li> <li>Basic RNN Architectures.md</li> <li>Basic Transformer.md</li> <li>Beam search.md</li> <li>Bi Directional RNN.md</li> <li>Bias nodes.md</li> <li>Big Bird.md</li> <li>./BinaryBERT.md</li> <li>./BlockNeRF.md</li> <li>./CLIP.md</li> <li>Capsule Layer.md</li> <li>Capsule Network.md</li> <li>Chat GPT is Not All You Need.md</li> <li>./ChatGPT.md</li> <li>./Chinchilla.md</li> <li>Classifier Gradients.md</li> <li>./Codex.md</li> <li>Collaborative Topic Regression.md</li> <li>Conditional GAN.md</li> <li>./Conformer.md</li> <li>Content Based Attention.md</li> <li>Contrastive Predictive Coding.md</li> <li>./ConvBERT.md</li> <li>./ConvNeXt.md</li> <li>Convolutional RNN.md</li> <li>Curriculum Learning.md</li> <li>./CycleGAN.md</li> <li>DALL-E 3.md</li> <li>./DALL-E.md</li> <li>DALL\u00b7E 2.md</li> <li>./DCGAN.md</li> <li>./DLRM.md</li> <li>./DeepFM.md</li> <li>./DeepNet.md</li> <li>./DeepPERF.md</li> <li>Denoising Autoencoder.md</li> <li>Dense Net.md</li> <li>Diffusion LM.md</li> <li>Dilated Sliding Window Attention.md</li> <li>./DistillBERT.md</li> <li>Dot Product Attention.md</li> <li>./Dreamfusion.md</li> <li>Dynamic Sparsity.md</li> <li>./ELECTRA.md</li> <li>./ELMO.md</li> <li>./EfficientNet.md</li> <li>./Elu.md</li> <li>Encoder Decoder Attention.md</li> <li>Ensemble Distillation.md</li> <li>./FLASH.md</li> <li>./FLAVA.md</li> <li>./FaceNet.md</li> <li>Factorized Embedding Parameters.md</li> <li>Familar Object Grasping Object Viiew recog.md</li> <li>./FastText.md</li> <li>Faster RCNN.md</li> <li>Feature Correlationa.md</li> <li>Fixed Factorization Attention.md</li> <li>./Flamingo.md</li> <li>./FlowNet.md</li> <li>GAN Z Space.md</li> <li>./GAU.md</li> <li>./GELU.md</li> <li>./GGCNN.md</li> <li>./GLOW.md</li> <li>./GPT.md</li> <li>./GPT3.md</li> <li>./GRConvNet.md</li> <li>./GRU.md</li> <li>./Galactica.md</li> <li>./Gato.md</li> <li>Generative Models.md</li> <li>Generative RNN.md</li> <li>Generative Spoken Language Modeling.md</li> <li>Generative vs Discriminative Models.md</li> <li>./GloVE.md</li> <li>Global Average Pooling.md</li> <li>Global and Sliding Window Attention.md</li> <li>Google NMT.md</li> <li>./Grad-CAM.md</li> <li>Hallucination Text Generation.md</li> <li>HiFI-GAN Denoising.md</li> <li>HiFI-GAN Synthesis.md</li> <li>Higher Layer Capsule.md</li> <li>Highway Convolutions.md</li> <li>Hopfield networks.md</li> <li>./Imagen.md</li> <li>./Inception.md</li> <li>Instance Normalization.md</li> <li>Instant NeRF.md</li> <li>Interpreting Attention.md</li> <li>Isotropic Architectures.md</li> <li>Joint Factor Analysis.md</li> <li>./Jukebox.md</li> <li>./LASER.md</li> <li>./LaMDA.md</li> <li>Large Kernel in Attention.md</li> <li>Large Kernel in Convolution.md</li> <li>Le Net.md</li> <li>Learning to Detect Grasp Affordance.md</li> <li>Linear Classifier Probes.md</li> <li>Listen Attend Spell.md</li> <li>Location Aware Attention.md</li> <li>Location Base Attention.md</li> <li>Long Short Term Memory (LSTM).md</li> <li>./Longformer.md</li> <li>./MCnet.md</li> <li>./MLIM.md</li> <li>./MLM.md</li> <li>./MVGrasp.md</li> <li>./Magic3D.md</li> <li>Masked Autoencoders.md</li> <li>./Minerva.md</li> <li>Mixed chunk attention.md</li> <li>Mobile Net.md</li> <li>./MobileOne.md</li> <li>Multi Head Attention.md</li> <li>Multiplicative Attention.md</li> <li>./Muse.md</li> <li>./Nasnet.md</li> <li>Neural Network Architecture Cheat Sheet.md</li> <li>Neural Probabilistic Model.md</li> <li>Neural Text Degeneration.md</li> <li>Noisy Relu.md</li> <li>./OPT.md</li> <li>./PEER.md</li> <li>./PRelu.md</li> <li>./PaLM.md</li> <li>Padded Conv.md</li> <li>./PatchGAN.md</li> <li>./Phenaki.md</li> <li>Phrase Representation Learning.md</li> <li>./Pix2Seq.md</li> <li>Point Cloud.md</li> <li>./PointNet++.md</li> <li>Position Encoding.md</li> <li>Position Wise Feed Forward.md</li> <li>Primary Capsule.md</li> <li>./RETRO.md</li> <li>Receptive field.md</li> <li>./RegNet.md</li> <li>Region Proposal.md</li> <li>Relative Multi Head Self Attention.md</li> <li>./Relu.md</li> <li>./RepLKNet.md</li> <li>Res Net D.md</li> <li>Res Net.md</li> <li>Restricted Boltzmann Machine.md</li> <li>./RetinaNet.md</li> <li>./Rmsprop.md</li> <li>./RoBERTa.md</li> <li>Routing by Agreement.md</li> <li>./S2ST.md</li> <li>./SLAK.md</li> <li>./SRN.md</li> <li>Scaled Dot Product Attention.md</li> <li>Scene based text to image generation.md</li> <li>./SegNet.md</li> <li>Self Attention GAN.md</li> <li>Self Attention.md</li> <li>./Seq2Seq.md</li> <li>./ShuffleNet.md</li> <li>./Sigmoid.md</li> <li>Sliding Window Attention.md</li> <li>Soft Attention.md</li> <li>./Softmax.md</li> <li>./Softplus.md</li> <li>./Soundify.md</li> <li>Sparse Evolutionary Training.md</li> <li>Sparse Transformer.md</li> <li>Spatial Transformer.md</li> <li>Speaker Verification.md</li> <li>Speech Emotion Recognition.md</li> <li>Speech Recognition.md</li> <li>Speech Resynthesis.md</li> <li>Spiking Networks.md</li> <li>Stable Difusion.md</li> <li>Stack GAN.md</li> <li>Stacking RNN.md</li> <li>StarGAN v2.md</li> <li>./StarGAN.md</li> <li>Strided Attention.md</li> <li>./Strided.md</li> <li>Style GAN.md</li> <li>./Swish.md</li> <li>./TSDF.md</li> <li>./Tacotron.md</li> <li>./Tanh.md</li> <li>Teacher Forcing.md</li> <li>Temporal Conv.md</li> <li>./TemporalLearning.md</li> <li>Textless Speech Emotion Conversion.md</li> <li>./TinyBERT.md</li> <li>Token Embedding.md</li> <li>./Transformer-XL.md</li> <li>./Transformer.md</li> <li>Transposed Conv.md</li> <li>./ULMFit.md</li> <li>./Un-LSTM.md</li> <li>Unet Grasping.md</li> <li>./Unet.md</li> <li>./VAE.md</li> <li>./VGGish.md</li> <li>./VICReg.md</li> <li>./VL-BEIT.md</li> <li>./Vgg.md</li> <li>./ViLT.md</li> <li>./VisualGPT.md</li> <li>Volumetric Grasping Network.md</li> <li>./WaveGlow.md</li> <li>./WebGPT.md</li> <li>./Whisper.md</li> <li>Wide Deep Recommender.md</li> <li>Window Based Regression.md</li> <li>./Word2Vec.md</li> <li>X Vectors.md</li> <li>./XLM-R.md</li> <li>./XLNet.md</li> <li>./Xception.md</li> <li>./YOLO.md</li> <li>Z-Space Entanglement.md</li> <li>Zeiler Fergus.md</li> <li>cross-layer parameter sharing.md</li> <li>./dGSLM.md</li> <li>./data2vec.md</li> <li>./i-Code.md</li> <li>./pGLSM.md</li> <li>./wave2vec.md</li> </ul>"},{"location":"KB/augment/","title":"Augment","text":""},{"location":"KB/augment/#categories-anchor","title":"categories: ['anchor']","text":"<ul> <li>A survey on Image Data Augmentation for Deep Learning.md</li> <li>Adversarial Spatial Dropout for Occlusion.md</li> <li>Alleviating Class Imbalance with Data Augmentation.md</li> <li>Attentive CutMix.md</li> <li>./AttributeMix.md</li> <li>./AugMix.md</li> <li>Augmentation-wise Weight Sharing strategy.md</li> <li>Augmented Random Search.md</li> <li>Auto Augment.md</li> <li>./AutoAugment.md</li> <li>./Co-Mixup.md</li> <li>Color Space Transformations.md</li> <li>./CowMask.md</li> <li>./Cropping.md</li> <li>Cut and Delete.md</li> <li>Cut and Mix.md</li> <li>Cut, Paste and Learn.md</li> <li>./CutMix.md</li> <li>./Cutout.md</li> <li>Data Augmentation via Latent Space Interpolation for Image Classification.md</li> <li>Data Augmentation with Curriculum Learning.md</li> <li>Deep Generative Models.md</li> <li>Fast AutoAugment.md</li> <li>./FeatMatch.md</li> <li>Feature Augmentation.md</li> <li>Feature Space Augmentation.md</li> <li>./Flipping.md</li> <li>./Fmix.md</li> <li>GAN\u2010based Data Augmentation.md</li> <li>Gaussian Distortion.md</li> <li>Geometric Transformations.md</li> <li>./GridMask.md</li> <li>Hide and Seek.md</li> <li>Image Erasing.md</li> <li>Image Manipulation.md</li> <li>Image Mix.md</li> <li>Image Mixing and Deletion.md</li> <li>Intra-Class Part Swapping.md</li> <li>./KeepAugment.md</li> <li>Kernel Filters.md</li> <li>Manifold MixUp.md</li> <li>./ManifoldMix.md</li> <li>Meta Learning Data Augmentations.md</li> <li>Mixed Example.md</li> <li>Moment Exchange.md</li> <li>Neural Augmentation.md</li> <li>Noise Injection.md</li> <li>On the Importance of Visual Context for Data Augmentation in Scene Understanding.md</li> <li>Population Based Augmentation.md</li> <li>Puzzle Mix.md</li> <li>./RICAP.md</li> <li>./RandAugment.md</li> <li>Random Distortion.md</li> <li>Random Erasing.md</li> <li>./ReMix.md</li> <li>./ResizeMix.md</li> <li>./SMOTE.md</li> <li>./SaliencyMix.md</li> <li>Sample Pairing.md</li> <li>./Shear.md</li> <li>Skew Tilt.md</li> <li>Smart Augmentation.md</li> <li>./SmoothMix.md</li> <li>./SnapMix.md</li> <li>./SpecAugment.md</li> <li>Test-time Augmentation.md</li> <li>Visual Context Augmentation.md</li> </ul>"},{"location":"KB/bitter_lesson/","title":"bitter_lesson","text":"","tags":["deeplearning"]},{"location":"KB/bitter_lesson/#the-bitter-lesson","title":"The Bitter Lesson","text":"<ul> <li>Richard sutton</li> <li>The biggest lesson that can be read from 70 years of AI research is that general methods that leverage computation are ultimately the most effective, and by a large margin</li> <li>Seeking an improvement that makes a difference in the shorter term, researchers seek to leverage their human knowledge of the domain, but the only thing that matters in the long run is the leveraging of computation</li> </ul> <ul> <li>In computer chess, the methods that defeated the world champion, Kasparov, in 1997, were based on massive, deep search. At the time, this was looked upon with dismay by the majority of computer chess researchers who had pursued methods that leveraged human understanding of the special structure of chess. When a simpler, search-based approach with special hardware and software proved vastly more effective, these human-knowledge-based chess researchers were not good losers. They said that ``brute force\\\" search may have won this time, but it was not a general strategy, and anyway it was not how people played chess. These researchers wanted methods based on human input to win and were disappointed when they did not.</li> <li>Learning by self play, and learning in general, is like search in that it enables massive computation to be brought to bear.</li> <li>statistical methods won out over the human-knowledge-based methods</li> <li>Deep learning methods rely even less on human knowledge, and use even more computation, together with learning on huge training sets, to produce dramatically better speech recognition systems.</li> <li>The bitter lesson is based on the historical observations that 1) AI researchers have often tried to build knowledge into their agents, 2) this always helps in the short term, and is personally satisfying to the researcher, but 3) in the long run it plateaus and even inhibits further progress, and 4) breakthrough progress eventually arrives by an opposing approach based on scaling computation by search and learning</li> </ul> <ul> <li>great power of general purpose methods, of methods that continue to scale with increased computation even as the available computation becomes very great</li> <li>scale arbitrarily in this way are search and learning.</li> <li>the actual contents of minds are tremendously, irredeemably complex; we should stop trying to find simple ways to think about the contents of minds, such as simple ways to think about space, objects, multiple agents, or symmetries.</li> <li>They are not what should be built in, as their complexity is endless; instead we should build in only the meta-methods that can find and capture this arbitrary complexity</li> <li>they can find good approximations, but the search for them should be by our methods, not by us</li> </ul>","tags":["deeplearning"]},{"location":"KB/blind%20ethical%20judgement/","title":"blind ethical judgement","text":""},{"location":"KB/blind%20ethical%20judgement/#blind-ethical-judgement","title":"Blind Ethical Judgement","text":"<ul> <li>the given agent's state and knowledge are unknown</li> </ul>"},{"location":"KB/brain/","title":"Brain","text":""},{"location":"KB/brain/#categories-anchor","title":"categories: ['anchor']","text":"<ul> <li>Action Potential.md</li> <li>Adrenal Glands.md</li> <li>./Adrenaline.md</li> <li>./Allele.md</li> <li>Alpha Waves.md</li> <li>Alzheimer\u2019s Disease.md</li> <li>Amino Acid.md</li> <li>./Amygdala.md</li> <li>Amyloid Plaque.md</li> <li>Amyloid-beta (A\u03b2) Protein.md</li> <li>Amyotrophic Lateral Sclerosis (ALS).md</li> <li>./Angiography.md</li> <li>./Apoptosis.md</li> <li>./Astrocyte.md</li> <li>Axon Terminal.md</li> <li>./Axon.md</li> <li>./BOLD.md</li> <li>Basal Ganglia.md</li> <li>Basilar Artery.md</li> <li>Belmont Principles.md</li> <li>Belmont Report.md</li> <li>./Beneficence.md</li> <li>Beta Waves.md</li> <li>Biological Neuron.md</li> <li>./Biomarkers.md</li> <li>Blood-brain Barrier.md</li> <li>Brain Areas.md</li> <li>Brain Cortex.md</li> <li>Brain Organoid.md</li> <li>Brain Oscillations.md</li> <li>Brain-derived Neurotrophic Factor (BDNF).md</li> <li>BrainWave Coherence.md</li> <li>BrainWave CrossFrequency Coupling.md</li> <li>BrainWave Synchronization.md</li> <li>./Brainstem.md</li> <li>Brocas Area.md</li> <li>CRISPR (clustered Regularly-interspaced Short Palindromic repeats).md</li> <li>Central Sulcus.md</li> <li>Cerebellar Artery.md</li> <li>./Cerebellum.md</li> <li>Cerebral Palsy.md</li> <li>Cerebrospinal Fluid (CSF).md</li> <li>./Cerebrum.md</li> <li>./Chimera.md</li> <li>Chronic Encephalopathy Syndrome (CES).md</li> <li>Chronic Traumatic Encephalopathy (CTE).md</li> <li>./Cochlea.md</li> <li>./Concussion.md</li> <li>./Cone.md</li> <li>Corpus callosum.md</li> <li>Cortical Homunculus.md</li> <li>./Cortisol.md</li> <li>Deep Brain Stimulation.md</li> <li>Delta Waves.md</li> <li>./Dendrites.md</li> <li>Digital Phenotyping.md</li> <li>Down Syndrome.md</li> <li>EEG Cap.md</li> <li>./EEG.md</li> <li>Electroconvulsive Therapy (ECT).md</li> <li>./Epigenetics.md</li> <li>./Epilepsy.md</li> <li>./Eugenics.md</li> <li>Frontal Operculum.md</li> <li>Frontal lobe.md</li> <li>Functional Connectivity.md</li> <li>Gamma Waves.md</li> <li>Gamma-aminobutyric Acid (GABA).md</li> <li>Gene Expression.md</li> <li>./Glia.md</li> <li>./Glioblastoma.md</li> <li>./Glioma.md</li> <li>./Glucose.md</li> <li>Glymphatic System.md</li> <li>Granger Causallity.md</li> <li>./Gyrus.md</li> <li>Huntington\u2019s Disease.md</li> <li>./Hypothalamus.md</li> <li>Implicit Bias.md</li> <li>In Silico.md</li> <li>In Vitro.md</li> <li>In Vivo.md</li> <li>Induced Pluripotent Stem Cell (iPSC).md</li> <li>./Insula.md</li> <li>Ion Channel.md</li> <li>./Ketamine.md</li> <li>./Lesion.md</li> <li>Limbic system.md</li> <li>Long Term Potentiation (LTP).md</li> <li>./MRI.md</li> <li>Mesolimbic Pathway.md</li> <li>./Microbiota.md</li> <li>./Microglia.md</li> <li>Multiple Sclerosis.md</li> <li>./Myelin.md</li> <li>Neural Chimera.md</li> <li>Neural Induction.md</li> <li>./Neuroaesthetics.md</li> <li>./Neurogenesis.md</li> <li>./Neuroplasticity.md</li> <li>./Nootropics.md</li> <li>Nucleotide Sequence.md</li> <li>./Nucleotide.md</li> <li>Nucleus Accumbens.md</li> <li>Occipital lobe.md</li> <li>./Optogenetics.md</li> <li>./Organoid.md</li> <li>./Oxytocin.md</li> <li>Parietal lobe.md</li> <li>Parkinson\u2019s Disease.md</li> <li>./Pharmacotherapy.md</li> <li>./Phenotype.md</li> <li>Pineal gland.md</li> <li>Pituitary gland.md</li> <li>./Pluripotency.md</li> <li>Positron Emission Tomography (PET).md</li> <li>Postsynaptic Cell.md</li> <li>Presynaptic Cell.md</li> <li>./Prion.md</li> <li>Protein Folding.md</li> <li>./Psychosis.md</li> <li>Rapid Eye Movement (REM) Sleep.md</li> <li>./Recessive.md</li> <li>./Reuptake.md</li> <li>./Rod.md</li> <li>./Serotonin.md</li> <li>Somatosensory Cortex.md</li> <li>./Sono-stimulation.md</li> <li>./Sonogenetics.md</li> <li>./Spectrogram.md</li> <li>Stem Cells.md</li> <li>./Striatum.md</li> <li>./Stroke.md</li> <li>Subgenual Cortex.md</li> <li>Substantia Nigra.md</li> <li>Subthalamic Nucleus.md</li> <li>./Sulcus.md</li> <li>Synaptic Pruning.md</li> <li>Synaptic Transmission.md</li> <li>Tau Protein.md</li> <li>./Telomere.md</li> <li>Temporal lobe.md</li> <li>./Thalamus.md</li> <li>Theta Waves.md</li> <li>Tourette\u2019s Syndrome.md</li> <li>Transcranial Electrical Stimulation (tDCS and tACS).md</li> <li>Transcranial Magnetic Stimulation (TMS).md</li> <li>Two-photon Microscopy.md</li> <li>Undue Inducement.md</li> <li>Vagus Nerve.md</li> <li>Vertebral Arteries.md</li> <li>Vestibular System.md</li> <li>Visual Cortex.md</li> <li>Wernicke Area.md</li> <li>aphasia.md</li> <li>./fMRI.md</li> </ul>"},{"location":"KB/cheatsheets/","title":"Cheatsheets","text":""},{"location":"KB/cheatsheets/#categories-anchor","title":"categories: ['anchor']","text":""},{"location":"KB/cogitivemodel/","title":"Cogitivemodel","text":""},{"location":"KB/cogitivemodel/#categories-anchor","title":"categories: ['anchor']","text":""},{"location":"KB/cogneuro/","title":"Cogneuro","text":""},{"location":"KB/cogneuro/#categories-anchor","title":"categories: ['anchor']","text":"<ul> <li>Attentions and salience.md</li> <li>./BOLD.md</li> <li>Berkeley et al.md</li> <li>./Connectome.md</li> <li>Cross-situational learning.md</li> <li>Deductive Approaches.md</li> <li>Distributive units.md</li> <li>First order generalization.md</li> <li>Inductive Learning.md</li> <li>Interpretability vs Neuroscience.md</li> <li>Localist units.md</li> <li>Milin et al.md</li> <li>Mirman et al.md</li> <li>Motor Memories.md</li> <li>./Overhypotheses.md</li> <li>./Propose-but-verify.md</li> <li>Rescorla-Wagner Algorithm.md</li> <li>Rescorla-Wagner Blocking.md</li> <li>Second order generalization.md</li> <li>Single unit recording.md</li> <li>Superposition Catastrophe.md</li> <li>Symbolic models.md</li> <li>Transitional probabilities.md</li> <li>./conditioning.md</li> <li>./fMRI.md</li> <li>memory trace.md</li> </ul>"},{"location":"KB/cognitivemodel/","title":"Cognitivemodel","text":""},{"location":"KB/cognitivemodel/#categories-anchor","title":"categories: ['anchor']","text":"<ul> <li>ACT-R Chunk.md</li> <li>./ACT-R.md</li> <li>Active tracking.md</li> <li>CogMod Final Paper.md</li> <li>Cognition Hazard Rates.md</li> <li>Cognitive Foreperiod.md</li> <li>Cognitive Multitasking.md</li> <li>Cognitive Preparation.md</li> <li>Cognitive fMTP.md</li> <li>Declarative Memory Blending.md</li> <li>Declarative memory.md</li> <li>Dikes and Rivers.md</li> <li>Implicitly learning when to be ready - From instances to categories.md</li> <li>Mental Fatigue.md</li> <li>Modeling Driver Behavior with Cognitive Architecture.md</li> <li>Modeling motivation using goal competition in mental fatigue studies.md</li> <li>On the Distinction Between Perceived Duration and Event Timing - Towards a Unified Model of Time Perception.md</li> <li>Revisiting variable foreperiod effects evaluating the repetition priming account.md</li> <li>Scaled benefits.md</li> <li>Sequential effects within a short foreperiod context Evidence for the conditioning account of temporal preparation.md</li> <li>Sugar Factory Task.md</li> <li>The Reward Experiment.md</li> <li>The warning stimulus as retrieval cue The role of associative memory in temporal preparation.md</li> <li>Threaded Cognition.md</li> <li>Traces of times past Representations of temporal intervals in memory.md</li> <li>You can play 20 questions with nature and win.md</li> </ul>"},{"location":"KB/composition/","title":"Composition","text":""},{"location":"KB/composition/#categories-anchor","title":"categories: ['anchor']","text":""},{"location":"KB/conditioning/","title":"conditioning","text":""},{"location":"KB/conditioning/#conditioning","title":"Conditioning","text":"<ul> <li>Unconditioned Stimulus (US) (=food)</li> <li>Another feature or cue (bell), becomes Conditioned Stimulus (CS)</li> <li>A response that indicates association (salvation)</li> <li>The unconditioned stimulus is what we want to predict, our outcome</li> <li>Conditioned Stimuli or cues are the features we are learning to use to predict the outcome (US)</li> <li>Current Vector V t ij changes with each learning instance</li> <li> \\[V_{ij}^{t+1}= V_{ij}^{t}+ \\Delta V_{ij}^{t}\\] </li> <li> \\[\\Delta V_{ij}^{t}=(1-\\text{connection weight}_{j}^{t})\\] </li> <li>\u25b6 V = association strength \u25b6 i = current input (cue) \u25b6 j = current output (outcome) \u25b6 \u2206 V : Change in association strength</li> <li>Pavlov discovered evidence of positive associations</li> <li>Associations increase in strength whenever a feature and a cue occur together</li> </ul>"},{"location":"KB/croissant/","title":"croissant","text":"","tags":["metadata"]},{"location":"KB/croissant/#croissant","title":"Croissant","text":"<ul> <li>Croissant is a metadata description format</li> <li>Ml datasets are a combination of structured and unstructured data, which make them complicated to manage</li> <li>Croissant was built on top of schema.org, and has more details relative to it</li> <li>The format has 4 layers<ul> <li>dataset level metadata</li> <li>resource description</li> <li>content structure</li> <li>ml semantics</li> </ul> </li> <li>Croissant does not require any changes to underlying data</li> <li>Analysis and visualization tools work out of the box for all datasets</li> <li>Using croissant, datasets can be exposed consistently throughout platforms</li> <li>Collaborations with google, hugginface, google dataset search also exist</li> <li>openml has deeper dataset description by default, slightly lesser in HF and kaggle</li> <li>once loaded, datasets can be imported elsewhere (torch, tf etc) easily</li> <li>Croissant editor - web app where you can use a GUI to enter the dataset descriptions</li> <li>NeurIPS also now recommends using the Croissant format</li> </ul>","tags":["metadata"]},{"location":"KB/cross-layer%20parameter%20sharing/","title":"cross-layer parameter sharing","text":""},{"location":"KB/cross-layer%20parameter%20sharing/#cross-layer-parameter-sharing","title":"Cross-layer Parameter Sharing","text":"<ul> <li>between encoder segments, layer parameters are shared for every similar subsegment.</li> <li>This means that e.g. with 12 encoder segments:<ul> <li>The multi-head self-attention subsegments share parameters (i.e. weights) across all twelve layers.</li> <li>The same is true for the feedforward segments.</li> </ul> </li> <li>The consequence of this change is that the number of parameters is reduced significantly, simply because they are shared.</li> <li>the stabilization of the neural network due to parameter sharing. In other words, beyond simply reducing the computational cost involved with training, the paper suggests that sharing parameters can also improve the training process.</li> </ul>"},{"location":"KB/dGSLM/","title":"dGSLM","text":""},{"location":"KB/dGSLM/#dgslm","title":"dGSLM","text":"<ul> <li>Generative Spoken Dialogue Language Modeling</li> <li>dGSLM</li> <li>first \u201ctextless\u201d model able to generate audio samples of naturalistic spoken dialogues</li> <li>unsupervised spoken unit discovery coupled with a dual-tower transformer architecture with cross-attention trained on 2000 hours of two-channel raw conversational audio Fisher Spanish-English without any text or labels</li> <li>generate speech, laughter and other paralinguistic signals in the two channels simultaneously and reproduces naturalistic turn taking</li> </ul>"},{"location":"KB/data2vec/","title":"data2vec","text":""},{"location":"KB/data2vec/#data2vec","title":"data2vec","text":"<ul> <li>data2vec: a General Framework for Self-supervised Learning in Speech, Vision and Language</li> <li>closer to general self-supervised learning</li> <li>framework that uses the same learning method for either speech, NLP or computer vision</li> <li>predict latent representations of the full input data based on a masked view of the input in a KB/Self Distillation.md setup using a standard Transformer architecture</li> <li>Instead of predicting KB/Modality.md-specific targets such as words, visual tokens or units of human speech which are local in nature, data2vec predicts contextualized latent representations that contain information from the entire inpu</li> <li>Today\u2019s self-supervised learning research almost typically focuses on a single KB/Modality.md</li> <li>As a result, researchers specializing in one KB/Modality.md often adopt a totally different strategy than those specializing in another.</li> <li>For each KB/Modality.md, algorithms anticipate distinct units: pixels or visual tokens for images, words for the text, and learned sound inventories for voice</li> <li>teaching models to anticipate their own representations of the incoming data, regardless of mode</li> <li>Instead of predicting visual tokens, phrases, or sounds, a single algorithm may work with completely different sorts of input by focusing on these representations \u2014 the layers of a neural network</li> <li>robust normalization of the features for the job that would be trustworthy in different modalities to directly predict representations.</li> <li>The method starts by computing target representations from an image, a piece of text, or a voice utterance using a teacher network</li> <li>After that, a portion of the input was masked and repeated with a student network, which predicts the teacher\u2019s latent representations</li> <li>Even though it only has a partial view of the data, the student model must predict accurate input data</li> <li>The instructor network is identical to the student network, except with somewhat out-of-date weights.</li> <li>ImageNet</li> <li>surpassed wav2vec 2.0 and HuBERT</li> <li>GLUE</li> <li>Method:</li> <li>data2vec is trained by predicting the model representations of the full input data given a partial view of the input</li> <li>They first encode a masked version of the training sample (model in student mode) and then construct training targets by encoding the unmasked version of the input sample with the same model but when parameterized as an exponentially moving average of the model weights (model in teacher mode)</li> <li>The target representations encode all of the information in the training sample and the learning task is for the student to predict these representations given a partial view of the input.</li> <li>Modality encoding:</li> <li>The model architecture used is the standard Transformer architecture with a KB/Modality.md-specific encoding of the input data borrowed from prior work:</li> <li>For computer vision, they have used the ViT-strategy of encoding an image as a sequence of patches, each spanning 16x16 pixels, input to a linear transformation.</li> <li>Speech data is encoded using a multi-layer 1-D convolutional neural network that maps 16 kHz waveform to 50 Hz representations.</li> <li>Text is pre-processed to obtain sub-word units, which are then embedded in distributional space via learned embedding vectors.</li> </ul>"},{"location":"KB/dataset/","title":"Dataset","text":""},{"location":"KB/dataset/#categories-anchor","title":"categories: ['anchor']","text":"<ul> <li>./1D-ALVINN.md</li> <li>AudioSet classification.md</li> <li>./AudioSet.md</li> <li>./BUCC.md</li> <li>Benchmark LLM.md</li> <li>Billion Word.md</li> <li>./BooksCorpus.md</li> <li>./Broden.md</li> <li>./CIFAR.md</li> <li>./COCO.md</li> <li>CUB-200-2011 4.md</li> <li>./CUB-200-2011.md</li> <li>./Cityscapes.md</li> <li>./CommonCrawl.md</li> <li>./DrawBench.md</li> <li>English Wikipedia.md</li> <li>./Europarl-ST.md</li> <li>FGVC Aircraft.md</li> <li>./FGVCx.md</li> <li>Fashion MNIST.md</li> <li>Fine grained datasets.md</li> <li>Fisher Spanish-English.md</li> <li>./Flickr30K.md</li> <li>./GLUE.md</li> <li>./GTA5.md</li> <li>Google Conceptual Captions.md</li> <li>Google voice search task.md</li> <li>./HMDB51.md</li> <li>./IDRiD.md</li> <li>./ILSVRC.md</li> <li>./IMDB.md</li> <li>ISIC 2018.md</li> <li>./KITTI.md</li> <li>./Kinetics.md</li> <li>Kvasir Dataset.md</li> <li>Labeled Faces in the Wild.md</li> <li>./LibriSpeech.md</li> <li>./MILANNOTATIONS.md</li> <li>./MIT1003.md</li> <li>./MIT300.md</li> <li>./MLDoc.md</li> <li>./MMLU.md</li> <li>./MNIST.md</li> <li>./MSCOCO.md</li> <li>./MUSAN.md</li> <li>Moment in Time.md</li> <li>NIST 2008 Speaker Recognition Evaluation dataset.md</li> <li>NIST SRE 2016 Cantonese.md</li> <li>NLVR2 3.md</li> <li>./OSIE.md</li> <li>PASCAL VOC.md</li> <li>./PASCAL-S.md</li> <li>People Art Dataset.md</li> <li>Picasso Dataset.md</li> <li>./Places.md</li> <li>./Places365.md</li> <li>./PlantCLEF.md</li> <li>./RACE.md</li> <li>SBU Captions.md</li> <li>./SQuAD.md</li> <li>./STL-10.md</li> <li>./SUNCG.md</li> <li>./SYNTHIA.md</li> <li>Salicon dataset.md</li> <li>SceneNet RGB-D.md</li> <li>Shapes Dataset.md</li> <li>Speakers in the Wild.md</li> <li>Stanford Dogs.md</li> <li>./Swichboard.md</li> <li>./UCF101.md</li> <li>VGGFace2 3.md</li> <li>VQAv2 3.md</li> <li>Visual Commonsense Reasoning.md</li> <li>Visual Genome.md</li> <li>./WMT14.md</li> <li>Wall Street Journal task.md</li> <li>./XLSR.md</li> <li>./YFCC100M.md</li> <li>./iNaturalist.md</li> </ul>"},{"location":"KB/ethics/","title":"Ethics","text":""},{"location":"KB/ethics/#categories-anchor","title":"categories: ['anchor']","text":"<ul> <li>A declarative modular framework for representing and applying ethical principles.md</li> <li>A low-cost ethics shaping approach for designing reinforcement learning agents.md</li> <li>A voting-based system for ethical decision making.md</li> <li>./Belief-Desire-Intention.md</li> <li>Building Ethics into Artificial Intelligence.md</li> <li>Capture bias.md</li> <li>Collective Ethical Decision Frameworks.md</li> <li>Consequentialist ethics.md</li> <li>Coping Theory.md</li> <li>Coverage of ethics within the artificial intelligence and machine learning academic literature.md</li> <li>Cross-dataset generalization.md</li> <li>Deontological ethics.md</li> <li>Embedding ethical principles in collective decision support systems.md</li> <li>Ethical dilemmas.md</li> <li>Even angels need the rules AI, roboethics, and the law.md</li> <li>./GenEth.md</li> <li>Label bias.md</li> <li>Moral Machine project.md</li> <li>Moral decision making frameworks for artificial intelligence.md</li> <li>./MoralDM.md</li> <li>Negative Set Bias.md</li> <li>Norms as a basis for governing sociotechnical systems.md</li> <li>Preferences and ethical principles in decision making.md</li> <li>Selection Bias.md</li> <li>Unbiased Look at Dataset Bias.md</li> <li>Utilitarian ethics.md</li> <li>Virtue ethics.md</li> <li>blind ethical judgement.md</li> <li>fully informed ethical judgement.md</li> <li>partially informed ethical judgement.md</li> <li>sacred values.md</li> <li>./swap-dominance.md</li> <li>trolley scenario.md</li> </ul>"},{"location":"KB/explainability/","title":"Explainability","text":""},{"location":"KB/explainability/#categories-anchor","title":"categories: ['anchor']","text":"<ul> <li>./Accessibility.md</li> <li>Adaptive Whitening Saliency.md</li> <li>Analysis of Explainers of Black Box Deep Neural Networks for Computer Vision A Survey.md</li> <li>./Auditability.md</li> <li>Back Propamine.md</li> <li>Bayesian Rule List.md</li> <li>Beware of Inmates Running the Asylum.md</li> <li>Beware of Inmates Running the Asylum.md</li> <li>Blur Baseline.md</li> <li>./Broden.md</li> <li>./CAM.md</li> <li>./CAM.md</li> <li>./Causability.md</li> <li>./Causality.md</li> <li>Classifying a specific image region using convolutional nets with an ROI mask as input.md</li> <li>Classifying a specific image region using convolutional nets with an ROI mask as input.md</li> <li>Cognitive Engagement.md</li> <li>Comparing Data Augmentation Strategies for Deep Image Classification.md</li> <li>Comparing Data Augmentation Strategies for Deep Image Classification.md</li> <li>./Comprehensibility.md</li> <li>./Conductance.md</li> <li>./Conductance.md</li> <li>./Confidence.md</li> <li>Contributions of Shape, Texture, and Color in Visual Recognition Abstract.md</li> <li>Contributions of Shape, Texture, and Color in Visual Recognition Abstract.md</li> <li>Counterfactual Images.md</li> <li>Counterfactual Impact Evaluation.md</li> <li>./DeconvNet.md</li> <li>Deep Inside Convolutional Networks.md</li> <li>Deep Neural Networks are Easily Fooled High Confidence Predictions for Unrecognizable Images.md</li> <li>Deep Visual Explanation.md</li> <li>./DeepFool.md</li> <li>./DeepLIFT.md</li> <li>./DeepLIFT.md</li> <li>Dynamic visual attention.md</li> <li>./EigenCAM.md</li> <li>./Elaborateness.md</li> <li>Embedding Human Knowledge into Deep Neural Network via Attention Map.md</li> <li>Embedding Human Knowledge into Deep Neural Network via Attention Map.md</li> <li>Explainability Defn.md</li> <li>Explainability Taxonomy.md</li> <li>Explainable Artificial Intelligence (XAI) Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI.md</li> <li>Explanation is not a Technical Term.md</li> <li>./Explanator.md</li> <li>./FGSM.md</li> <li>./Fairness.md</li> <li>./Faithfulness.md</li> <li>Filter Wise Normalization.md</li> <li>./GAM.md</li> <li>Gaussian Baseline.md</li> <li>Generalizing Adversarial Explanations with Grad-CAM.md</li> <li>Generalizing Adversarial Explanations with Grad-CAM.md</li> <li>./GradCAM++.md</li> <li>Gradient Sensitivity.md</li> <li>Graph-based visual saliency.md</li> <li>Group fairness.md</li> <li>Guided BackProp.md</li> <li>Guided GradCAM.md</li> <li>Image Data Augmentation Survey.md</li> <li>Implementation Invariance.md</li> <li>./Independence.md</li> <li>Influence of image classification accuracy on saliency map estimation.md</li> <li>Influence of image classification accuracy on saliency map estimation.md</li> <li>./Informativeness.md</li> <li>Integrated Gradients.md</li> <li>Integrated Gradients.md</li> <li>./Interactivity.md</li> <li>Interpretability and Explainability A Machine Learning Zoo Mini-tour.md</li> <li>./Interpretability.md</li> <li>Interpretation of Neural networks is fragile.md</li> <li>Interpretation of Neural networks is fragile.md</li> <li>./LRP.md</li> <li>Layerwise Conservation Principle.md</li> <li>Layerwise Conservation Principle.md</li> <li>Layerwise Relevance Propagation.md</li> <li>Limited features.md</li> <li>./Manifold.md</li> <li>Maximum Distance Baseline.md</li> <li>Mean Observed Dissimilarity.md</li> <li>Mental Model Matching.md</li> <li>Minimization and reporting of negative impacts.md</li> <li>Multimodal Explanation.md</li> <li>Network Dissection Quantifying Interpretability of Deep Visual Representions.md</li> <li>Noise Tunnel.md</li> <li>Noise Tunnel.md</li> <li>Normalized Inverted Structural Similarity Index.md</li> <li>On the overlap between Grad-CAM saliency maps and explainable visual features in skin cancer images.md</li> <li>On the overlap between Grad-CAM saliency maps and explainable visual features in skin cancer images.md</li> <li>Parent Approximations.md</li> <li>Partial Dependence Plot.md</li> <li>Prediction Difference Analysis.md</li> <li>Privacy awareness.md</li> <li>./PromptIR.md</li> <li>Proxy Attention.md</li> <li>Proxy Attention.md</li> <li>Proxy features.md</li> <li>./RETAIn.md</li> <li>./RISE.md</li> <li>./RandAugment.md</li> <li>Random Directions.md</li> <li>Real Time Image Saliency for Black Box Classifiers.md</li> <li>Real Time Image Saliency for Black Box Classifiers.md</li> <li>./Redress.md</li> <li>./SAM-ResNet.md</li> <li>./SDR.md</li> <li>./SP-LIME.md</li> <li>./SSR.md</li> <li>Salience Map.md</li> <li>Salience Map.md</li> <li>Saliency using natural statistics.md</li> <li>Saliency vs Attention.md</li> <li>Sanity Checks for Saliency Maps.md</li> <li>./ScoreCAM.md</li> <li>./Separation.md</li> <li>Sharpness and Flatness.md</li> <li>Skewed data.md</li> <li>./Smooth-Grad.md</li> <li>SmoothGrad Square.md</li> <li>SmoothGrad Square.md</li> <li>Social Construction of XAI, do we need one definition to rule them all.md</li> <li>Structural Similarity Index.md</li> <li>./Sufficiency.md</li> <li>./Summit.md</li> <li>./TREPAN.md</li> <li>Tainted data.md</li> <li>Textbooks are all you need.md</li> <li>The Unreliability of Saliency Methods.md</li> <li>The Unreliability of Saliency Methods.md</li> <li>The elephant in the interpretability room.md</li> <li>There and back again.md</li> <li>Towards A Rigorous Science of Interpretable Machine Learning.md</li> <li>Training Trajectories.md</li> <li>Trajectory Plotting with PCA.md</li> <li>./Transferability.md</li> <li>./Transparency.md</li> <li>./Trustworthiness.md</li> <li>./Understandability.md</li> <li>Uniform baseline.md</li> <li>Use Case Utility.md</li> <li>./VarGrad.md</li> <li>./VarGrad.md</li> <li>Variation in Dissimilarity Variation in Dissimilarity.md</li> <li>Vision Explainibility.md</li> <li>Vision Explainibility.md</li> <li>Visualizing the Impact of Feature Attribution Baselines.md</li> <li>Visualizing the Loss Landscape of Neural Nets.md</li> <li>What is being Transferred in transfer learning.md</li> <li>Whos Thinking, A push for human centered evaluation of LLMs.md</li> <li>./XAI.md</li> <li>journals/2022-10-05.md</li> <li>journals/2022-10-17.md</li> <li>./pixelattribution.md</li> </ul>"},{"location":"KB/fMRI/","title":"fMRI","text":""},{"location":"KB/fMRI/#fmri","title":"fMRI","text":"<ul> <li>Unlike MRI. Studies measures brain activity by detecting changes associated with blood flow</li> <li>3mm</li> <li>BOLD</li> </ul>"},{"location":"KB/fastai/","title":"fastai","text":""},{"location":"KB/fastai/#fastai","title":"Fastai","text":"<ul> <li>Fastai Blocks</li> <li>Fastai Interpretation</li> <li>Fastai Deployment</li> <li>Fastai Tricks</li> </ul>"},{"location":"KB/flow/","title":"Flow","text":""},{"location":"KB/flow/#categories-anchor","title":"categories: ['anchor']","text":"<ul> <li>content/notes/Vision Explainibility.md</li> </ul>"},{"location":"KB/fully%20informed%20ethical%20judgement/","title":"fully informed ethical judgement","text":""},{"location":"KB/fully%20informed%20ethical%20judgement/#fully-informed-ethical-judgement","title":"Fully Informed Ethical Judgement","text":"<ul> <li>with complete information about a given agent's state and knowledge</li> <li>no quantitative measure of how far a behaviour is from rightfulness or goodness</li> </ul>"},{"location":"KB/gen_quotes/","title":"Gen quotes","text":"Quotes Random Quotes   This page displays 10 random Quotes from my personal repository of Quotes I have collected (and continue to collect) over the years. Sadly, most of them do not have any citations. I will eventually try to write a script that finds the authors of the same.   These have been collected from books, websites, movies and music and each of them has taught me something over the years. Here I am sharing them with you, dear visitor :) Show Random Points"},{"location":"KB/gensim/","title":"gensim","text":""},{"location":"KB/gensim/#gensim","title":"Gensim","text":"<ul> <li>library</li> </ul>"},{"location":"KB/gensim/#w2vec-code","title":"w2vec Code","text":"<pre><code>nltk==3.6.1  \nnode2vec==0.4.3  \npandas==1.2.4  \nmatplotlib==3.3.4  \ngensim==4.0.1  \nscikit-learn=0.24.1\n</code></pre> <pre><code>import nltk  \nnltk.download('stopwords')  \nnltk.download('punkt')\n\nimport pandas as pd\n\nimport nltk\n\nimport string\n\nimport matplotlib.pyplot as plt\n\nfrom nltk.corpus import stopwords\n\nfrom nltk import word_tokenize\n\nfrom gensim.models import Word2Vec as w2v\n\nfrom sklearn.decomposition import PCA\n\n# constants\n\nPATH = 'data/shakespeare.txt'\n\nsw = stopwords.words('english')\n\nplt.style.use('ggplot')\n\n# nltk.download('punkt')\n\n# nltk.download('stopwords')\n\n# import data\n\nlines = []\n\nwith open(PATH, 'r') as f:\n\nfor l in f:\n    lines.append(l)\n\n# remove new lines\nlines = [line.rstrip('\\n') for line in lines]\n\n# make all characters lower\nlines = [line.lower() for line in lines]\n\n# remove punctuations from each line\nlines = [line.translate(str.maketrans('', '', string.punctuation)) for line in lines]\n\n# tokenize\nlines = [word_tokenize(line) for line in lines]\n\ndef remove_stopwords(lines, sw = sw):\n    '''\n    The purpose of this function is to remove stopwords from a given array of \n    lines.\n\n    params:\n        lines (Array / List) : The list of lines you want to remove the stopwords from\n        sw (Set) : The set of stopwords you want to remove\n\n    example:\n        lines = remove_stopwords(lines = lines, sw = sw)\n    '''\n\n    res = []\n    for line in lines:\n        original = line\n        line = [w for w in line if w not in sw]\n        if len(line) &lt; 1:\n            line = original\n        res.append(line)\n    return res\n\nfiltered_lines = remove_stopwords(lines = lines, sw = sw)\n\nw = w2v(\n    filtered_lines,\n    min_count=3,  \n    sg = 1,       # 1 for skip gram, 0 for cbow\n    window=7      \n)       \n\nprint(w.wv.most_similar('thou'))\n\nemb_df = (\n    pd.DataFrame(\n        [w.wv.get_vector(str(n)) for n in w.wv.key_to_index],\n        index = w.wv.key_to_index\n    )\n)\nprint(emb_df.shape)\nemb_df.head()\n\npca = PCA(n_components=2, random_state=7)\npca_mdl = pca.fit_transform(emb_df)\n\nemb_df_PCA = (\n    pd.DataFrame(\n        pca_mdl,\n        columns=['x','y'],\n        index = emb_df.index\n    )\n)\n\nplt.clf()\nfig = plt.figure(figsize=(6,4))\n\nplt.scatter(\n    x = emb_df_PCA['x'],\n    y = emb_df_PCA['y'],\n    s = 0.4,\n    color = 'maroon',\n    alpha = 0.5\n)\n\nplt.xlabel('PCA-1')\nplt.ylabel('PCA-2')\nplt.toc: true\ntitle('PCA Visualization')\nplt.plot()\n</code></pre>"},{"location":"KB/handwritingRecognition/","title":"handwritingRecognition","text":""},{"location":"KB/handwritingRecognition/#handwritingrecognition","title":"handwritingRecognition","text":"<ul> <li>https://arxiv.org/pdf/1912.10205.pdf<ul> <li>https://github.com/Canjie-Luo/Text-Image-Augmentation</li> <li></li> <li></li> </ul> </li> <li>https://github.com/FactoDeepLearning/VerticalAttentionOCR<ul> <li>https://arxiv.org/pdf/2012.03868v2.pdf</li> <li>segmentation free</li> <li></li> <li></li> </ul> </li> </ul>"},{"location":"KB/heteroscedastic%20nonlinear%20regression/","title":"heteroscedastic nonlinear regression","text":""},{"location":"KB/heteroscedastic%20nonlinear%20regression/#heteroscedastic-nonlinear-regression","title":"heteroscedastic nonlinear regression","text":"<ul> <li>Heteroscedatic</li> <li>mean and the variance of the output are functions of the input</li> </ul>"},{"location":"KB/i-Code/","title":"i-Code","text":""},{"location":"KB/i-Code/#i-code","title":"i-Code","text":"<ul> <li>i-Code: an Integrative and Composable Multimodal Learning Framework</li> <li>Human intelligence is multimodal; they integrate visual, linguistic, and acoustic signals to maintain a holistic worldview</li> <li>Most current pretraining methods, however, are limited to one or two modalities.</li> <li>jointly learns representations for vision, language and speech into a unified, shared and general-purpose vector representation</li> <li>data from each [modality] are first given to pretrained single-modality encoder</li> <li>The encoder outputs are then integrated with a multimodal fusion network, which uses novel attention mechanisms and other architectural innovations to effectively combine information from the different modalities</li> <li>new objectives including (i) masked [modality] modeling and (ii) cross-modality contrastive learning</li> <li>pretraining on dual-[modality] datasets can also yield competitive or even better performance than pretraining on videos, the data resource that previous three-modality models were restricted to</li> <li>dynamically process single, dual, and triple-KB/Modality.md data during training and inference, flexibly projecting different combinations of modalities into a single representation space</li> <li>GLUE</li> <li>merge-attention layers and (b) co-attention layers</li> <li>fusion architecture</li> <li>mechanisms that merge and cross the attention scores of different modalities, namely merge-attention (based on self-attention) and co-attention (based on self- and cross-attention) respectively</li> </ul>"},{"location":"KB/iNaturalist/","title":"iNaturalist","text":""},{"location":"KB/iNaturalist/#inaturalist","title":"iNaturalist","text":"<ul> <li>This dataset contains images of thousands of different species of plants and animals, with a total of over 8 million images.</li> </ul>"},{"location":"KB/ill%20conditioning/","title":"ill conditioning","text":""},{"location":"KB/ill%20conditioning/#ill-conditioning","title":"ill conditioning","text":"<ul> <li>Gradient can behave in unexpected ways, e.g in the figure above, very large gradients near flat regions.</li> <li></li> </ul>"},{"location":"KB/imageCaptioning/","title":"Image Captioning","text":""},{"location":"KB/imageCaptioning/#image-captioning","title":"Image Captioning","text":""},{"location":"KB/inter-sentence%20coherence%20loss/","title":"inter-sentence coherence loss","text":""},{"location":"KB/inter-sentence%20coherence%20loss/#inter-sentence-coherence-loss","title":"Inter-sentence Coherence Loss","text":"<ul> <li>inter-sentence coherence loss called sentence-order prediction (SOP) is used.</li> <li>The key problem with this loss is that it merges topic prediction and coherence prediction into one task.</li> <li>Intuitively, we can argue that topic prediction is much easier than coherence prediction. The consequence is that when the model discovers this, it can focus entirely on this subtask, and forget about the coherence prediction task; actually taking the path of least resistance. The authors actually demonstrate that this is happening with the NSP task, replacing it within their work with a sentence-order prediction or SOP task.</li> </ul>"},{"location":"KB/jobs/","title":"Jobs","text":""},{"location":"KB/jobs/#categories-anchor","title":"categories: ['anchor']","text":"<ul> <li>ABN Amro AI Dev.md</li> <li>ABN amro ml.md</li> <li>./Bunq.md</li> <li>Cold email templates.md</li> <li>DevOn AI dev.md</li> <li>Eneco Data Scientist.md</li> <li>Interview Tips.md</li> <li>Kickstart AI.md</li> <li>Latitude Junior ML Engineer.md</li> <li>Leap Data Scientist.md</li> <li>./MLCompany.md</li> <li>Media Distillery.md</li> <li>./MediaMonks.md</li> <li>./Orbisk.md</li> <li>Poki Data Scientist.md</li> <li>Schipol Data Scientist.md</li> <li>VattenFall Data Scientist.md</li> <li>Wageningen Uni AI.md</li> </ul>"},{"location":"KB/jobtemp/","title":"jobtemp","text":"","tags":["jobs"]},{"location":"KB/jobtemp/#jobtemp","title":"Jobtemp","text":"","tags":["jobs"]},{"location":"KB/jobtemp/#dexter-energy","title":"Dexter Energy","text":"<ul> <li>jordan@dexterenergy.ai</li> <li>Jordan Cantlow</li> <li> </li> <li> <p>Klue ML Engineer</p> </li> </ul>","tags":["jobs"]},{"location":"KB/language/","title":"Language","text":""},{"location":"KB/language/#categories-anchor","title":"categories: ['anchor']","text":"<ul> <li>2 X 2 Study.md</li> <li>2 byte character set.md</li> <li>8 bit character set.md</li> <li>A matter of ambiguity? Using eye movements to examine collective vs distributive interpretations of plural sets.md</li> <li>./ANOVA.md</li> <li>./ASCII.md</li> <li>Action Transitive verb.md</li> <li>./Adjective.md</li> <li>./Adverb.md</li> <li>Agglutinating words.md</li> <li>./Allomorph.md</li> <li>Application dependence.md</li> <li>Approximately Compositional Semantic Parsing.md</li> <li>Attentions and salience.md</li> <li>Berkeley et al.md</li> <li>Bottom Up Parsing.md</li> <li>Bound morpheme.md</li> <li>Cardinality Principle.md</li> <li>Case Grammar.md</li> <li>Challenges of Words-and-rules.md</li> <li>Character-set dependence.md</li> <li>./Circumfix.md</li> <li>Collective Interpretation.md</li> <li>Collectivity, Distributivity, and the Interpretation of Plural Numerical Expressions in Child and Adult Language.md</li> <li>Conceptual Parsing.md</li> <li>./Connectionism.md</li> <li>Connectionist Networks.md</li> <li>./Connectives.md</li> <li>Content Morpheme.md</li> <li>Content words.md</li> <li>Context Free Grammar.md</li> <li>Corpus dependence.md</li> <li>Cross-situational learning.md</li> <li>Cumulative Interpretation.md</li> <li>Deductive Approaches.md</li> <li>Derivational Morphology.md</li> <li>./Determiners.md</li> <li>DiTransitive verb.md</li> <li>Distributive Interpretation (2).md</li> <li>Distributive Interpretation.md</li> <li>Distributive units.md</li> <li>Document Triage.md</li> <li>Elements of sets.md</li> <li>Elman 1990.md</li> <li>Elman 1991.md</li> <li>Elman 1992.md</li> <li>Elman 1993.md</li> <li>./Emergentism.md</li> <li>Entities involving in actions.md</li> <li>Evidence For Distributivity Effects in Comprehension.md</li> <li>./Extra-position.md</li> <li>Final Paper Language Modeling.md</li> <li>First order generalization.md</li> <li>Fixed Factors.md</li> <li>Forward Backward Matching.md</li> <li>Free morpheme.md</li> <li>Function words.md</li> <li>Functional Morpheme.md</li> <li>Hybrid Word Segmentation.md</li> <li>Inductive Learning.md</li> <li>./Infix.md</li> <li>Inflectional Morphology.md</li> <li>Inflectional words.md</li> <li>Isolating words.md</li> <li>Language Identification.md</li> <li>Language dependence.md</li> <li>Latent Dirchlet Allocation.md</li> <li>./Lemmatization.md</li> <li>Lexical Ambiguity.md</li> <li>Lexical Disambiguation.md</li> <li>Lexical Word Segmentation.md</li> <li>Lexically Collective.md</li> <li>Lexically Distributive.md</li> <li>./Lexicon.md</li> <li>Linguistic details.md</li> <li>Localist units.md</li> <li>Maximum Matching Algorithm.md</li> <li>Memory-based learning.md</li> <li>Milin et al.md</li> <li>Minimal Semantic Commitment.md</li> <li>Mirman et al.md</li> <li>Misyak et al 2010.md</li> <li>Mixed Effect Models.md</li> <li>Morpheme Generation.md</li> <li>Morpheme Segmentation.md</li> <li>./Morpheme.md</li> <li>Morphology Affix.md</li> <li>Morphology Stem.md</li> <li>./Morphology.md</li> <li>./Morphotactic.md</li> <li>Multiple constraint-based theories.md</li> <li>Names of individuals.md</li> <li>./Nativists.md</li> <li>Non-adjacent dependencies.md</li> <li>Numerically Quantified Expressions.md</li> <li>Object-relative clauses.md</li> <li>./Ostension.md</li> <li>Ostensive Information.md</li> <li>./Overhypotheses.md</li> <li>Parts of action.md</li> <li>Parts of entities.md</li> <li>./Phonetics.md</li> <li>./Phonology.md</li> <li>Picky Puppet Method.md</li> <li>Polysynthetic words.md</li> <li>./Pragmatics.md</li> <li>./Predicate.md</li> <li>./Prefix.md</li> <li>./Prepositions.md</li> <li>./Propose-but-verify.md</li> <li>./Psycholinguistics.md</li> <li>./Punctuation.md</li> <li>Quantifier spreading children misled by ostensive cues.md</li> <li>./Quantifiers.md</li> <li>Random Factors.md</li> <li>Relevance Account.md</li> <li>Relevance Theory.md</li> <li>Rescorla-Wagner Algorithm.md</li> <li>Rescorla-Wagner Blocking.md</li> <li>Saffran, Aslin and Newport.md</li> <li>Salient Object Strategy.md</li> <li>Second order generalization.md</li> <li>Semantic Analysis.md</li> <li>Semantic Grammar.md</li> <li>Semantic Markers.md</li> <li>Semantics influences form.md</li> <li>Sentence Segmentation.md</li> <li>Sentence level processing.md</li> <li>Shared Character Set.md</li> <li>Shortcuts to Quantifier Interpretation in Children and Adults.md</li> <li>Single unit recording.md</li> <li>Statistical Word Segmentation.md</li> <li>Subject relative.md</li> <li>Subject-verb agreement.md</li> <li>./Suffix.md</li> <li>Superposition Catastrophe.md</li> <li>./Suppletion.md</li> <li>Symbolic learning model.md</li> <li>Symbolic models.md</li> <li>Syntactic Ambiguity.md</li> <li>Syntactic Analysis.md</li> <li>Syntactic Bootstrapping.md</li> <li>Syntax First models.md</li> <li>Taking on semantic commitments, II collective versus distributive readings.md</li> <li>Text Normalization.md</li> <li>Text Preprocessing.md</li> <li>Text Segmentation.md</li> <li>The Differentiation Condition.md</li> <li>The Self Organization of Explicit Attitudes.md</li> <li>./Tokenizer.md</li> <li>Top Down Parsing.md</li> <li>Tracking the Continuity of Language Comprehension Computer Mouse Trajectories Suggest Parallel Syntactic Processing.md</li> <li>Transitional probabilities.md</li> <li>Transitive verb.md</li> <li>Types of Words.md</li> <li>Unicode 50.md</li> <li>Unique Character Set.md</li> <li>Universal Quantifiers.md</li> <li>Unrestricted Race Model.md</li> <li>./Verb.md</li> <li>Visual Implicit Learning.md</li> <li>./Wh-dependencies.md</li> <li>./Wickelphones.md</li> <li>Word Blending.md</li> <li>Word Clipping.md</li> <li>Word Compounding.md</li> <li>Word Segmentation.md</li> <li>Word Structure.md</li> <li>./Words-and-Rules.md</li> <li>./conditioning.md</li> <li>past tense.md</li> </ul>"},{"location":"KB/loss/","title":"Loss","text":""},{"location":"KB/loss/#categories-anchor","title":"categories: ['anchor']","text":"<ul> <li>0-1 Loss.md</li> <li>./AUC-Borji.md</li> <li>./AUC-Judd.md</li> <li>Absolute Error.md</li> <li>Adversarial Loss.md</li> <li>Akaike Information Criterion.md</li> <li>Attention Alignment.md</li> <li>BCE with Logits.md</li> <li>./BLEU.md</li> <li>Bayesian Information Criterion.md</li> <li>Binary Cross Entropy.md</li> <li>./CTC.md</li> <li>Confusion Matrix.md</li> <li>Contrastive Loss.md</li> <li>Cosine Similarity.md</li> <li>Cross Entropy.md</li> <li>Cycle Consistency Loss.md</li> <li>Dice Score.md</li> <li>Focal Loss.md</li> <li>./GE2E.md</li> <li>Hinge Loss.md</li> <li>./Huber.md</li> <li>ITM Loss.md</li> <li>Identity Loss.md</li> <li>Intra cluster variance.md</li> <li>Jensen Shannon Divergence Consistency Loss.md</li> <li>KL Divergence.md</li> <li>Log Likelihood Loss.md</li> <li>./LogCosh.md</li> <li>./MAE.md</li> <li>./MAPE.md</li> <li>./MSE.md</li> <li>./MSLE.md</li> <li>Mallows Cp Statistic.md</li> <li>Margin Ranking.md</li> <li>Max Margin Loss.md</li> <li>Negative Log Likelihood.md</li> <li>./PatchGAN.md</li> <li>./Perplexity.md</li> <li>Poisson Loss.md</li> <li>Precision Recall Curve.md</li> <li>./Precision.md</li> <li>Quadratic Loss.md</li> <li>./RAHP.md</li> <li>ROC Curve.md</li> <li>./Recall.md</li> <li>Reconstruction loss.md</li> <li>./SDR.md</li> <li>./SSR.md</li> <li>./Sensitivity.md</li> <li>./Shuffled-AUC.md</li> <li>Sparse Dictionary Learning Loss.md</li> <li>./Specificity.md</li> <li>Squared Error.md</li> <li>Squared Hinge.md</li> <li>Triplet Loss.md</li> <li>inter-sentence coherence loss.md</li> </ul>"},{"location":"KB/mastersthesis/","title":"Mastersthesis","text":""},{"location":"KB/mastersthesis/#categories-anchor","title":"categories: ['anchor']","text":""},{"location":"KB/medical/","title":"Medical","text":""},{"location":"KB/medical/#categories-anchor","title":"categories: ['anchor']","text":"<ul> <li>./Acute.md</li> <li>./Afib.md</li> <li>./Angina.md</li> <li>./Appendectomy.md</li> <li>./Benign.md</li> <li>./Biopsy.md</li> <li>Blood Culture.md</li> <li>Blood Lancet.md</li> <li>Blood Swab.md</li> <li>./C-section.md</li> <li>./Chronic.md</li> <li>Coronary Bypass.md</li> <li>./Defibrillator.md</li> <li>./Dialyser.md</li> <li>./Dialysis.md</li> <li>./Edema.md</li> <li>./Embolism.md</li> <li>./Endoscope.md</li> <li>./Foley.md</li> <li>./Forceps.md</li> <li>./Fracture.md</li> <li>./Hypertension.md</li> <li>Hypodermic Needle.md</li> <li>./Hypotension.md</li> <li>./Hysterectomy.md</li> <li>./Intravenous.md</li> <li>./Intubation.md</li> <li>Lead Test.md</li> <li>Lumbar Puncture or Spinal Tap.md</li> <li>./Malignant.md</li> <li>./Mastectomy.md</li> <li>Myocardial Infarction.md</li> <li>./Nebulizer.md</li> <li>Occult Blood Screen.md</li> <li>./Ophthalmoscope.md</li> <li>Otoscope or Auriscope.md</li> <li>Pulse Oximeter.md</li> <li>Reflex Hammer.md</li> <li>./Remission.md</li> <li>./Sepsis.md</li> <li>./Speculum.md</li> <li>./Spirometer.md</li> <li>./Thrombosis.md</li> <li>./Ultrasound.md</li> </ul>"},{"location":"KB/memory%20trace/","title":"memory trace","text":""},{"location":"KB/memory%20trace/#memory-trace","title":"Memory Trace","text":"<ul> <li>the researchers were able to identify specific neurons in the brain\u2019s motor cortex \u2014 an area responsible for controlling movements \u2014 that were activated during the learning process.</li> <li>The researchers tagged these potential engram cells with a fluorescent marker so they could see if they also played a role in recalling the memory later on</li> </ul>"},{"location":"KB/pGLSM/","title":"pGLSM","text":""},{"location":"KB/pGLSM/#pglsm","title":"pGLSM","text":"<ul> <li>Text-Free Prosody-Aware Generative Spoken Language Modeling</li> <li>similar to how GPT-2 can generate coherent paragraphs</li> <li>builds upon </li> <li>addresses the generative aspects of speech pre-training</li> <li>replacing text with discovered phone-like units for language modeling and shows the ability to generate meaningful novel sentences</li> <li>the units used in GSLM discard most of the prosodic information</li> <li>fails to leverage prosody for better comprehension, and does not generate expressive speech</li> <li>prosody-aware generative spoken language model (pGSLM)</li> <li>multi-stream transformer language model (MS-TLM) of speech, represented as discovered unit and prosodic feature streams, and an adapted HiFi-GAN model converting MS-TLM outputs to waveform</li> </ul>"},{"location":"KB/partially%20informed%20ethical%20judgement/","title":"partially informed ethical judgement","text":""},{"location":"KB/partially%20informed%20ethical%20judgement/#partially-informed-ethical-judgement","title":"Partially Informed Ethical Judgement","text":"<ul> <li>with some information about a given agent's state and knowledge</li> </ul>"},{"location":"KB/past%20tense/","title":"past tense","text":""},{"location":"KB/past%20tense/#past-tense","title":"Past Tense","text":"<ul> <li>When a speaker needs to say the past tense form of a verb if the verb has an irregular form, choose that if not, add -ed</li> </ul>"},{"location":"KB/pixelattribution/","title":"pixelattribution","text":""},{"location":"KB/pixelattribution/#pixelattribution","title":"Pixelattribution","text":""},{"location":"KB/pixelattribution/#pixel-attribution","title":"Pixel Attribution","text":"<ul> <li>Pixel attribution is a special case of feature attribution, but for images</li> <li>Feature attribution explains individual predictions by attributing each input feature according to how much it changed the prediction (negatively or positively)</li> <li>The features can be input pixels, tabular data or words</li> </ul>"},{"location":"KB/pixelattribution/#occlusion-or-perturbation-based","title":"Occlusion or Perturbation-based","text":"<ul> <li>Methods like SHAP and LIME manipulate parts of the image to generateexplanations (modelagnostic).</li> </ul>"},{"location":"KB/pixelattribution/#gradient-based","title":"Gradient-based","text":"<ul> <li>compute the gradient of the prediction (or classification score) with respect to theinput feature</li> <li>The gradient-based methods (of which there are many) mostly differ in how thegradient is computed.</li> <li>explanation has the same size as the input image (or at least can be meaningfullyprojected onto it) and they assign each pixel a value that can be interpreted as therelevance of the pixel to the prediction or classification of that image.</li> </ul>"},{"location":"KB/pixelattribution/#gradient-only-methods","title":"Gradient-only Methods","text":"<ul> <li>whether a change in a pixel would change the prediction</li> <li>Examples are Vanilla Gradient and Grad-CAM</li> <li>If I were to increase the color values of the pixel, the predicted class probabilitywould go up (for positive gradient) or down (for negative gradient</li> <li>The larger the absolute value of the gradient, the stronger the effect of a change ofthis pixel.</li> </ul>"},{"location":"KB/pixelattribution/#path-attribution-methods","title":"Path-attribution Methods","text":"<ul> <li>compare the current image to a reference image, which can be an artificial \"zero\"image such as a completely grey image</li> <li>The difference in actual and baseline prediction is divided among the pixels</li> <li>The baseline image can also be multiple images: a distribution of images</li> <li>model-specific gradient-based methods such as Deep Taylor and IntegratedGradients</li> <li>model-agnostic methods such as LIME and SHAP.</li> <li>Some path-attribution methods are \"complete\", meaning that the sum of therelevance scores for all input features is the difference between the prediction of theimage and the prediction of a reference image</li> <li>Examples are SHAP and Integrated Gradients.</li> <li>The difference between classification scores of the actual image and the baselineimage are attributed to the pixels</li> <li>The choice of the reference image (distribution) has a big effect on the explanation</li> </ul>"},{"location":"KB/pixelattribution/#vanilla-gradient","title":"Vanilla Gradient","text":"<ul> <li>We calculate the gradient of the loss function for the class we are interested in withrespect to the input pixels</li> <li>This gives us a map of the size of the input features with negative to positive values.</li> <li>The recipe for this approach is:</li> <li> <ol> <li>Perform a forward pass of the image of interest.</li> </ol> </li> <li> <ol> <li>Compute the gradient of the class score of interest with respect to the input pixels:</li> </ol> </li> <li></li> <li>Here we set all other classes to zero.</li> <li> <ol> <li>Visualize the gradients. You can either show the absolute values or highlight negative and positive contributions separately.</li> </ol> </li> <li>More formally, we have an image I and the convolutional neural \ud835\udc46\ud835\udc50(\ud835\udc3c) network gives ita score</li> <li>for class c</li> <li>The score is a highly non-linear function of our image.</li> <li>The idea behind using the gradient is that we can approximate that score byapplying a first-order Taylor expansion</li> <li></li> <li>some ambiguity how to perform a backward pass of the gradients</li> <li>since non-linear units such as ReLU (Rectifying Linear Unit) \"remove\" the sign</li> <li>So when we do a backpass, we do not know whether to assign a positive or negativeactivation</li> <li> </li> <li> <p>This means that when the activation of a neuron is zero, we do not know which valueto backpropagate</p> </li> <li>In the case of Vanilla Gradient, the ambiguity is resolved as follows: \ud835\udeff\ud835\udc53= \ud835\udeff\ud835\udc53</li> <li></li> <li>\ud835\udc08 e r e , activation at the lower layer was negative, and one where it is positive orzero</li> <li>Vanilla Gradient takes the gradient we have backpropagated so far up to layer n+1,and then simply sets the gradients to zero where the activation at the layer below isnegative</li> </ul>"},{"location":"KB/pixelattribution/#problems-with-vanilla-gradient","title":"Problems with Vanilla Gradient","text":"<ul> <li>saturation problem</li> <li>When ReLU is used, and when the activation goes below zero, then the activation iscapped at zero and does not change any more</li> </ul>"},{"location":"KB/pixelattribution/#deconvnet","title":"DeconvNet","text":"<ul> <li>almost identical to Vanilla Gradient</li> <li>The goal of DeconvNet is to reverse a neural network and the paper proposesoperations that are reversals of the filtering, pooling and activation layers</li> <li>but apart from the reversal of the ReLU layer, DeconvNet is equivalent to the Vanilla Gradient approach</li> <li>Vanilla Gradient can be seen as a generalization of DeconvNet</li> <li>DeconvNet makes a different choice for backpropagating the gradient- 12:24  through ReLU:</li> <li></li> <li>When backpassing from layer n to layer n-1, DeconvNet \"remembers\" which of theactivations in layer n were set to zero in the forward pass and sets them to zero in layer n-1</li> <li>\ud835\udc4bctivations with a negative value in layer x are set to zero in layer n-1.</li> </ul>"},{"location":"KB/pixelattribution/#grad-cam","title":"Grad-CAM","text":"<ul> <li>Grad-CAM provides visual explanations for CNN decisions</li> <li>Unlike other methods, the gradient is not backpropagated all the way back to theimage, but (usually) to the last convolutional layer to produce a coarse localizationmap that highlights important regions of the image.</li> <li>Gradient-weighted Class Activation Map</li> <li>based on the gradient of the neural networks</li> <li>assigns each neuron a relevance score for the decision of interest</li> <li>This decision of interest can be the class prediction (which we find in the outputlayer), but can theoretically be any other layer in the neural network</li> <li>GradCAM can be used with different CNNs: with fully-connected layers, forstructured output such as captioning and in multi-task outputs, and forreinforcement learning.</li> <li>s a reminder, the first convolutional layer of a CNN takes as input the images andoutputs feature maps that encode learned features</li> <li>higher-level convolutional layers do the same, but take as input the feature maps ofthe previous convolutional layers</li> <li>There are k feature maps in the last \ud835\udc34 ,\ud835\udc34 ,...,\ud835\udc34</li> <li>Grad-CAM has to decide how important each of the k feature map was to our class cthat we are interested in</li> <li>We have to weight each pixel of each feature map with the gradient before weaverage over the feature maps</li> <li>This gives us a heatmap which highlights regions that positively or negatively affectthe class of interest</li> <li>This heatmap is send through the ReLU function, which is a fancy way of saying thatwe set all negative values to zero</li> <li>Grad-CAM removes all negative values by using a ReLU function, with the argumentthat we are only interested in the parts that contribute to the selected class c and not to other classes</li> <li>The word pixel might be misleading here as the feature map is smaller than theimage (because of the pooling units) but is mapped back to the original image</li> <li>We then scale the GradCAM map to the interval [0,1] for visualization purposes and overlay it over the original image.</li> <li></li> <li>Forward-propagate the input image through the convolutional neural network.</li> <li>Obtain the raw score for the class of interest, meaning the activation of the neuron before the softmax layer.</li> <li>Set all other class activations to zero.</li> <li></li> <li>Back-propagate the gradient of the class of interest to the last \ud835\udeff\ud835\udc66\ud835\udc50 \ud835\udeff\ud835\udc34\ud835\udc58 convolutional layer before the fully connected layers.</li> <li>Weight each feature map \"pixel\" by the gradient for the class. Indices i and j refer to the width and height dimensions.</li> <li></li> <li>This means that the gradients are globally pooled</li> <li>Calculate an average of the feature maps, weighted per pixel by the gradient.</li> <li>Apply ReLU to the averaged feature map.</li> <li>Scale values to the interval between 0 and 1. Upscale the image and overlay it over the original image.</li> <li>Additional step for Guided Grad-CAM: Multiply the heatmap with guided backpropagation.</li> </ul>"},{"location":"KB/pixelattribution/#guided-grad-cam","title":"Guided Grad-CAM","text":"<ul> <li>From the description of Grad-CAM you can guess that the localization is verycoarse, since the last convolutional feature maps have a much coarser resolution compared to the input image</li> <li>In contrast, other attribution techniques backpropagate all the way to the inputpixels</li> <li>They are therefore much more detailed and can show you individual edges or spotsthat contributed most to a prediction</li> <li>You compute for an image both the GradCAM explanation and the explanationfrom another attribution method, such as Vanilla Gradient</li> <li>The Grad-CAM output is then upsampled with bilinear interpolation, then both mapsare multiplied element-wise</li> <li>Grad-CAM works like a lense that focuses on specific parts of the pixel-wiseattribution map.</li> </ul>"},{"location":"KB/pixelattribution/#smoothgrad","title":"# SmoothGrad","text":"<ul> <li>make gradient-based explanations less noisy by adding noise and averaging overthese artificially noisy gradient</li> <li>SmoothGrad is not a standalone explanation method, but an extension to anygradientbased explanation method.</li> <li> <ol> <li>Generate multiple versions of the image of interest byadding noise to it. 2.Create pixel attribution maps for all images. </li> </ol> </li> <li> <ol> <li>Average the pixel attribution maps.</li> </ol> </li> <li>The theory is that the derivative fluctuates greatly at small scales</li> <li>Neural networks have no incentive during training to keep the gradients smooth,their goal is to classify images correctly</li> <li>Averaging over multiple maps \"smooths out\" these fluctuations</li> <li></li> <li></li> <li></li> <li>And that is the big issue with all of these methods</li> <li>We do not have a ground truth for the explanations</li> <li>We can only, in a first step, reject explanations that obviously make no sense (andeven in this step we do not have strong confidence</li> </ul>"},{"location":"KB/pixelattribution/#disadvantages","title":"Disadvantages","text":"<ul> <li>it is difficult to know whether an explanation is correct, and a huge part of theevaluation is only qualitative</li> <li>Pixel attribution methods can be very fragile</li> <li>Ghorbani et al. (2019)86 showed that introducing small (adversarial) perturbationsto an image, which still lead to the same prediction, can lead to very different pixelsbeing highlighted as explanations.</li> <li>Kindermans et al. (2019) 87 also showed that these pixel attribution methods can behighly unreliable</li> <li>They added a constant shift to the input data, meaning they added the same pixelchanges to all images</li> <li>They compared two networks, the original network and the \"shifted\" network wherethe bias of the first layer is changed to adapt for the constant pixel shift</li> <li>Both networks produce the same predictions.</li> <li>Further, the gradient is the same for both</li> <li>But the explanations changed, which is an undesirable property. They looked at DeepLift, Vanilla Gradient and Integrated Gradients.</li> </ul>"},{"location":"KB/plan%20recognition%20problem/","title":"plan recognition problem","text":""},{"location":"KB/plan%20recognition%20problem/#plan-recognition-problem","title":"Plan Recognition Problem","text":"<ul> <li>It is likely that the student wants a hint on how to do that step correctly. If the tutor can determine which correct step corresponds to the incorrect step entered by the student, then it can safely hint that correct step. On the other hand, if the tutor cannot determine which correct step corresponds to the student's step, the tutor may ask the student what step they were trying to enter. Andes does this for unrecognizable equations that the student wants help on.</li> <li>If none of these simple cases apply, then the tutoring system has to somehow figure out which of the possible next steps is most likely to be part of the solution that the student is trying to pursue.</li> <li>(Russell &amp; Norvig, 2003)</li> <li>The general idea is to match the sequence of steps leading up to the most recent student step against step sequences that are either generated by a planner or pulled from a plan library. Once the tutor has determined the plan that the student seems to be following, then it can easily determine what the next step in that plan is.</li> </ul>"},{"location":"KB/probabl%20hackathon/","title":"probabl hackathon","text":"","tags":["deeplearning"]},{"location":"KB/probabl%20hackathon/#probabl-hackathon","title":"Probabl Hackathon","text":"","tags":["deeplearning"]},{"location":"KB/probabl%20hackathon/#intro","title":"Intro","text":"<ul> <li>doc</li> <li>around scikitlearn -&gt; profits<ul> <li>inria</li> <li>fit predict and a few tools</li> <li>\"product company\"</li> <li>pre-industrialization</li> </ul> </li> <li>automatic analysis : sklearn too?</li> <li>croissant<ul> <li>also for benchmarks at some point</li> </ul> </li> </ul>","tags":["deeplearning"]},{"location":"KB/probabl%20hackathon/#breakouts","title":"Breakouts","text":"","tags":["deeplearning"]},{"location":"KB/probabl%20hackathon/#organizing-community-events-and-onboarding-contributors","title":"Organizing Community Events and Onboarding Contributors","text":"","tags":["deeplearning"]},{"location":"KB/psychology/","title":"Psychology","text":""},{"location":"KB/psychology/#categories-anchor","title":"categories: ['anchor']","text":"<ul> <li>Attentions and salience.md</li> <li>Berkeley et al.md</li> <li>Challenges of Words-and-rules.md</li> <li>./Connectionism.md</li> <li>Connectionist Networks.md</li> <li>Cross-situational learning.md</li> <li>Deductive Approaches.md</li> <li>Distributive units.md</li> <li>Elman 1990.md</li> <li>Elman 1991.md</li> <li>Elman 1992.md</li> <li>Elman 1993.md</li> <li>./Emergentism.md</li> <li>./Extra-position.md</li> <li>First order generalization.md</li> <li>Inductive Learning.md</li> <li>Localist units.md</li> <li>Memory-based learning.md</li> <li>Milin et al.md</li> <li>Mirman et al.md</li> <li>Misyak et al 2010.md</li> <li>./Nativists.md</li> <li>Non-adjacent dependencies.md</li> <li>Object-relative clauses.md</li> <li>./Overhypotheses.md</li> <li>./Propose-but-verify.md</li> <li>Rescorla-Wagner Algorithm.md</li> <li>Rescorla-Wagner Blocking.md</li> <li>Saffran, Aslin and Newport.md</li> <li>Second order generalization.md</li> <li>Semantics influences form.md</li> <li>Single unit recording.md</li> <li>Subject relative.md</li> <li>Subject-verb agreement.md</li> <li>Superposition Catastrophe.md</li> <li>Symbolic learning model.md</li> <li>Symbolic models.md</li> <li>Transitional probabilities.md</li> <li>Visual Implicit Learning.md</li> <li>./Wh-dependencies.md</li> <li>./Wickelphones.md</li> <li>./Words-and-Rules.md</li> <li>./conditioning.md</li> </ul>"},{"location":"KB/random%20dump/","title":"Random Dump","text":""},{"location":"KB/random%20dump/#random-dump","title":"Random Dump","text":""},{"location":"KB/regularize/","title":"Regularize","text":""},{"location":"KB/regularize/#categories-anchor","title":"categories: ['anchor']","text":"<ul> <li>./AdaIn.md</li> <li>./Adam.md</li> <li>Batch Normalization.md</li> <li>./DeepNorm.md</li> <li>./Dropout.md</li> <li>Effects of Regularization.md</li> <li>Fine Tuning Based Pruning.md</li> <li>Global Gradient Magnitude Based Pruning.md</li> <li>Global Magnitude Based Pruning.md</li> <li>He Initialization.md</li> <li>Label Smoothing.md</li> <li>Layer Normalization.md</li> <li>Layerwise Gradient Magnitude Based Pruning.md</li> <li>Layerwise Magnitude Based Pruning.md</li> <li>LeCun Init.md</li> <li>Leaky Relu.md</li> <li>Learning Rate Range Test.md</li> <li>Lp Regularization.md</li> <li>./Mixup.md</li> <li>Modality Dropout.md</li> <li>No bias decay.md</li> <li>./Optimizers.md</li> <li>Orthogonal Initialization.md</li> <li>./Pruning.md</li> <li>Random Pruning.md</li> <li>Regularization Term.md</li> <li>./Regularization.md</li> <li>./Scheduling.md</li> <li>Scoring Pruning Approaches.md</li> <li>Structure Based Pruning.md</li> <li>Tuning Model Flexibility.md</li> <li>VariationalRecurrent Dropout.md</li> <li>Weight Decay Vs L2 Regularization.md</li> <li>Xavier Initialization.md</li> </ul>"},{"location":"KB/robotics/","title":"Robotics","text":""},{"location":"KB/robotics/#categories-anchor","title":"categories: ['anchor']","text":"<ul> <li>Action Component.md</li> <li>Active Compliant Robot.md</li> <li>./Actuator.md</li> <li>Affordance Detection Task Specific.md</li> <li>./Anthropomorphic.md</li> <li>Articulated Manipulator.md</li> <li>./Articulation.md</li> <li>Assembly Robot.md</li> <li>Average Number of Stored Instances per Category.md</li> <li>Bag of Words robotics.md</li> <li>Base Link.md</li> <li>./Burn-in.md</li> <li>./Carousel.md</li> <li>Centrifugal Force.md</li> <li>Circular Motion Type.md</li> <li>./Clamp.md</li> <li>./Clamping.md</li> <li>Compliant Robot.md</li> <li>Contact Sensor.md</li> <li>Continuous Path.md</li> <li>Cyclo Drive.md</li> <li>Cylindrical Topology.md</li> <li>Degrees of Freedom.md</li> <li>./Direct-drive.md</li> <li>Drop Delivery.md</li> <li>Dual-memory Approach.md</li> <li>Enabling Device.md</li> <li>./End-effector.md</li> <li>./Endpoint.md</li> <li>Ensemble of Shape Functions.md</li> <li>Eye-to-hand System.md</li> <li>Familar Object Grasping Object Viiew recog.md</li> <li>Fine-grained Object Recognition.md</li> <li>./Forgetting.md</li> <li>Forward Kinematic Solution.md</li> <li>Forward Kinematics.md</li> <li>./GGCNN.md</li> <li>./GRConvNet.md</li> <li>Gantry Robot.md</li> <li>Global Classification Accuracy.md</li> <li>Grasp Point Detection.md</li> <li>Gravity Loading.md</li> <li>./Gripper.md</li> <li>Hand Guiding.md</li> <li>Harmonic Drive.md</li> <li>./Harness.md</li> <li>Heightmaps Kinesthetic.md</li> <li>Humanoid Vision Engine.md</li> <li>Inductive Sensor.md</li> <li>Instance-based Learning.md</li> <li>Instruction Cycle.md</li> <li>Inverse Kinematics.md</li> <li>Inverse Reinforcement Learning.md</li> <li>Iterative Closest Point.md</li> <li>Joint Interpolated Motion.md</li> <li>Joint Motion Type.md</li> <li>Joint Space.md</li> <li>Joint Velocity.md</li> <li>Kalman Filter.md</li> <li>Kinesthetic Teaching.md</li> <li>Ladle Gripper.md</li> <li>Learning Component.md</li> <li>Learning to Detect Grasp Affordance.md</li> <li>Linear Interpolated Motion.md</li> <li>Load Cycle Time.md</li> <li>Local Descriptor.md</li> <li>Local Reference Frame.md</li> <li>Local-LDA Object Representation.md</li> <li>./MVCNN.md</li> <li>./MVGrasp.md</li> <li>Magnetic Detectors.md</li> <li>./Manipulator.md</li> <li>Material Processing Robot.md</li> <li>Mirror Shift Function.md</li> <li>Mode Switch.md</li> <li>./MoreMVCNN.md</li> <li>Neural Radiance Field.md</li> <li>./Occlusion.md</li> <li>Opportunistic Learning.md</li> <li>Optical Encoder.md</li> <li>Optical Proximity Sensors.md</li> <li>./OrthographicNet.md</li> <li>./Palletizing.md</li> <li>Parallel Shift Function.md</li> <li>Particle Filter.md</li> <li>Pendant Teaching.md</li> <li>Perception Component.md</li> <li>Perceptual Messages.md</li> <li>Phases of Simulated User Experiments.md</li> <li>Pick and Place Cycle.md</li> <li>Pinch Points.md</li> <li>./Point-to-Point.md</li> <li>Polynomial Trajectories.md</li> <li>Power and Force Limiting (PFL).md</li> <li>Presence-sensing Safeguarding Device.md</li> <li>Prismatic Joint.md</li> <li>Programmable Logical Controller (PLC).md</li> <li>Proximity Sensor.md</li> <li>Pulse Coordinates.md</li> <li>Quadratic Potential Field.md</li> <li>Quasi-static Clamping.md</li> <li>./RANSAC.md</li> <li>Reasoning Component.md</li> <li>Revolute Joint.md</li> <li>Risk Mitigation.md</li> <li>Robot Range Limit Monitoring.md</li> <li>Robotic Joints.md</li> <li>./Roll.md</li> <li>Rotary Joint.md</li> <li>Rotary Vector Drive (RV).md</li> <li>./Safeguard.md</li> <li>Safety Integrity Level.md</li> <li>Safety Logic Circuit.md</li> <li>Semantic Data.md</li> <li>Sense-Plan-Act Model.md</li> <li>Sensory Feedback.md</li> <li>Servo Control.md</li> <li>Servo Motor.md</li> <li>Servo Pack.md</li> <li>Servo-controlled Robot.md</li> <li>./Servo-system.md</li> <li>Shock Detection Function.md</li> <li>./Singularity.md</li> <li>Softlimit Setting Function.md</li> <li>Spline Motion Type.md</li> <li>./Spline.md</li> <li>./TSDF.md</li> <li>Task (endeffector) Space Vs Joint Space.md</li> <li>Teach Lock.md</li> <li>The Repulsive Potential.md</li> <li>./Through-beam.md</li> <li>Time Measuring Function.md</li> <li>Trajectory Planning.md</li> <li>./Transducer.md</li> <li>Trapezoidal Trajectory.md</li> <li>Unet Grasping.md</li> <li>Vacuum Cup Hand.md</li> <li>Viewpoint Feature Histogram.md</li> <li>Visual Servo System.md</li> <li>Volumetric Grasping Network.md</li> <li>Work Envelope.md</li> <li>./Wrist.md</li> <li>./Yaw.md</li> </ul>"},{"location":"KB/sacred%20values/","title":"sacred values","text":""},{"location":"KB/sacred%20values/#sacred-values","title":"Sacred Values","text":"<ul> <li>morally forbids the commitment of certain actions regardless of consequences</li> </ul>"},{"location":"KB/swap-dominance/","title":"swap-dominance","text":""},{"location":"KB/swap-dominance/#swap-dominance","title":"Swap-dominance","text":"<ul> <li>when ranking alternatives to form a model of ethical preferences</li> <li>When new decisions need to be made, the summarized model is used to compute a collective decision that results in the best possible outcome</li> </ul>"},{"location":"KB/textless-lib/","title":"textless-lib","text":""},{"location":"KB/textless-lib/#textless-lib","title":"Textless-lib","text":"<ul> <li>textless-lib: a Library for Textless Spoken Language Processing</li> <li>Textless spoken language processing research aims to extend the applicability of standard NLP toolset onto spoken language and languages with few or no textual resources</li> <li>PyTorch</li> <li>speaker probing, (ii) speech resynthesis and compression, and (iii) speech continuation.</li> </ul>"},{"location":"KB/todo/","title":"Todo","text":""},{"location":"KB/todo/#todo","title":"ToDO","text":"<ul> <li>graphs for every loss function and it\u2019s derivative</li> <li>reflow gradient descent document</li> <li>https://github.com/kennethleungty/MLOps-Specialization-Notes</li> </ul>"},{"location":"KB/treecoverSegmentation/","title":"Tree Cover Segmentation","text":""},{"location":"KB/treecoverSegmentation/#tree-cover-segmentation","title":"Tree Cover Segmentation","text":"<ul> <li>treecover segmentation PointNet++<ul> <li>Data collected from above</li> <li>Normalization : height, xy</li> <li>Rotation</li> <li>Jiggling ??</li> <li>Labeling<ul> <li> Segmentation algorithm. Canopy hide model</li> <li>Weighted loss + Focal Loss</li> </ul> </li> </ul> </li> </ul>"},{"location":"KB/treecoverSegmentation/#2d-methods","title":"2d Methods","text":"<ul> <li>Watershed + Unet</li> <li></li> <li><ul> <li>\\(\\Theta\\) is just clippingpng]</li> <li>The sqrt makes it a little smoother</li> </ul> </li> </ul>"},{"location":"KB/treecoverSegmentation/#ref","title":"Ref","text":"<ul> <li>Max Freudenberg - Gottingen Uni Germany</li> <li>Adrian Stroker - Gottingen Uni Germany</li> </ul>"},{"location":"KB/trolley%20scenario/","title":"trolley scenario","text":""},{"location":"KB/trolley%20scenario/#trolley-scenario","title":"Trolley Scenario","text":"<ul> <li>making a participant actively cause harm to an innocent bystander by pushing him on to the train track in order to save the lives of five people</li> </ul>"},{"location":"KB/usermodel/","title":"Usermodel","text":""},{"location":"KB/usermodel/#categories-anchor","title":"categories: ['anchor']","text":"<ul> <li>Algebra Cognitive Tutor.md</li> <li>./Andes.md</li> <li>./AutoTutor.md</li> <li>Coarse-grained assessment.md</li> <li>Collaborative Recommender.md</li> <li>Content Based Recommender.md</li> <li>DT Tutor.md</li> <li>Declarative memory.md</li> <li>Effects of Contextual Cues on Inferring and Remembering Meanings of New Word.md</li> <li>Extensions to SlimStampen.md</li> <li>Eye Tracking.md</li> <li>Filter Bubble Problem.md</li> <li>Final Paper User Models.md</li> <li>Fine Grained assesment.md</li> <li>./GOMS.md</li> <li>Game Based Learning.md</li> <li>./Gamification.md</li> <li>Gaze position.md</li> <li>Grey sheep problem.md</li> <li>Group Modeling Approach.md</li> <li>Help Abuse.md</li> <li>Help Refusal.md</li> <li>./IRT.md</li> <li>Ideas for Fact Learning.md</li> <li>Individual Modeling.md</li> <li>Knowledge Component.md</li> <li>Latent Semantic Analysis.md</li> <li>Learning Event.md</li> <li>Learning L2 German Vocabulary Through Reading.md</li> <li>./Macroadaptation.md</li> <li>Mastery learning.md</li> <li>Modeling Driver Behavior with Cognitive Architecture.md</li> <li>Modeling Transfer.md</li> <li>Predicting Student learning Curve.md</li> <li>Pupil Dilation.md</li> <li>Ramp up problem.md</li> <li>Rational inference.md</li> <li>Recommender System.md</li> <li>./SQL-Tutor.md</li> <li>Satisficing Heuristic.md</li> <li>Second Language Vocabulary Learning , The role of context  versus translation.md</li> <li>Serious Games.md</li> <li>./Sherlock.md</li> <li>./SlimStampen.md</li> <li>./Steve.md</li> <li>The Behavior of Tutoring Systems.md</li> <li>The Effect of Three Consecutive Context Sentences on EFL Vocabulary-Learning.md</li> <li>./Tutor.md</li> <li>plan recognition problem.md</li> </ul>"},{"location":"KB/visualization/","title":"Visualization","text":""},{"location":"KB/visualization/#categories-anchor","title":"categories: ['anchor']","text":"<ul> <li>1D piecewise linear interpolation.md</li> <li>Area Minimization.md</li> <li>Asymptotic Decider.md</li> <li>Average Filter.md</li> <li>Back To Front Raycasting.md</li> <li>Barycentric Interpolation.md</li> <li>Bend Minimization.md</li> <li>Bilinear Interpolation.md</li> <li>Change Blindness.md</li> <li>Characteristics of Visual Variables.md</li> <li>Classification Ray Casting.md</li> <li>Clutter In Visualisation.md</li> <li>Color Compositing.md</li> <li>Color Spaces.md</li> <li>./ColorMap.md</li> <li>Complex Geometry.md</li> <li>./Contour.md</li> <li>Conv Based Noise Reduction.md</li> <li>Countouring with Transparency.md</li> <li>Critical Points.md</li> <li>Cross Minimization.md</li> <li>Cross angle Maximization.md</li> <li>./Cuboids.md</li> <li>Curl And Vorticity.md</li> <li>./Cylinders.md</li> <li>Data Structures.md</li> <li>Diffusion Tensor.md</li> <li>./Divergence.md</li> <li>Divide Oriented.md</li> <li>Early Ray Termination.md</li> <li>./Eigenvector.md</li> <li>./Ellipsoids.md</li> <li>Entourage Plot.md</li> <li>Euler Integration.md</li> <li>Eulerian Grid.md</li> <li>./Filtering.md</li> <li>Finite Differences.md</li> <li>First order integration.md</li> <li>Force Directed Graph Layout.md</li> <li>Fractional Anisotropy.md</li> <li>Front to Back Raycasting.md</li> <li>Gaussian Filter.md</li> <li>Gestalt Laws.md</li> <li>./Glyphs.md</li> <li>./Graphs.md</li> <li>./Grids.md</li> <li>H3 View.md</li> <li>Height Plots.md</li> <li>Helmholtz Theorem.md</li> <li>Hierarchial Refinement.md</li> <li>Hierarchical Edge Bundling.md</li> <li>High pass filter.md</li> <li>./HyperStreamlines.md</li> <li>ICA Noise Removal.md</li> <li>Inattentional Blindness.md</li> <li>./Inceptionism.md</li> <li>Indirect Volume Visualization.md</li> <li>Information Visualization.md</li> <li>Integral Lines.md</li> <li>./Interpolation.md</li> <li>Intuitive Color spaces.md</li> <li>./Isoline.md</li> <li>./Isosurface.md</li> <li>Lagrangian Coherent Structure.md</li> <li>Lagrangian Grid.md</li> <li>Laplacian Grid Smoothing.md</li> <li>Length Optimization.md</li> <li>Line Integral Convolution.md</li> <li>Mapping to Geometry.md</li> <li>Marching Cubes.md</li> <li>Marching Squares.md</li> <li>Marching Tetrahedra.md</li> <li>Mean Diffusivity.md</li> <li>Median Filter.md</li> <li>Mesh Smoothing.md</li> <li>Mesh refinement.md</li> <li>Midpoint Decider.md</li> <li>Midpoint Method.md</li> <li>Node Link Diagram.md</li> <li>Noise Suppression.md</li> <li>Notch filter.md</li> <li>Oblique Slicing.md</li> <li>Opacity Correction.md</li> <li>Orthogonal Slicing.md</li> <li>Parallel Coordinate Plots.md</li> <li>Particle Visualization.md</li> <li>./Pathlines.md</li> <li>./Perception.md</li> <li>Perceptually Uniform.md</li> <li>Phong Lighting.md</li> <li>Post Classification.md</li> <li>Postattentive Amnesia.md</li> <li>Pre Classification.md</li> <li>Pre Integrated Volume Rendering.md</li> <li>Radial Plot.md</li> <li>./Raycasting.md</li> <li>Region Growing.md</li> <li>Runge Kutta.md</li> <li>Sampling Ray Casting.md</li> <li>Scalar Color Coding.md</li> <li>./Shading.md</li> <li>Shepard Interpolation.md</li> <li>Slice Based Volume Rendering.md</li> <li>./Spectrogram.md</li> <li>Stream Ribbons.md</li> <li>Stream Surfaces.md</li> <li>Streamline Stopping Criterion.md</li> <li>./Streamlines.md</li> <li>./SuperQuadrics.md</li> <li>Symmetries Node Link.md</li> <li>Time Dependant Vector Field.md</li> <li>Transfer Function.md</li> <li>./Treemap.md</li> <li>Visual Associative.md</li> <li>Visual Encoding.md</li> <li>Visual Length.md</li> <li>Visual Ordered.md</li> <li>Visual Quantitative.md</li> <li>Visual Selective.md</li> <li>Visualization Of Layers.md</li> <li>Volume Rendering Equation.md</li> <li>Volume Visualization.md</li> <li>Volumetric Illumination.md</li> <li>Voxel Projection.md</li> </ul>"},{"location":"KB/wave2vec/","title":"wave2vec","text":""},{"location":"KB/wave2vec/#wave2vec","title":"wave2vec","text":"<ul> <li>wav2vec: Unsupervised Pre-training for Speech Recognition</li> <li>Reducing the need for manually annotated data is important for developing systems that understand non-English languages, particularly those with limited existing training sets of transcribed speech</li> <li>first application of unsupervised pre-training to KB/Speech Recognition.md using a fully convolutional model that learns representations of raw, unlabeled audio</li> <li>trained on large amounts of unlabeled audio data and the resulting representations are then used to improve acoustic model training</li> <li>pre-train a simple multi-layer convolutional neural network optimized via a noise contrastive binary classification task</li> <li>learn the difference between original speech examples and modified versions, often repeating this task hundreds of times for each second of audio, and predicting the correct audio milliseconds into the future</li> <li>beats traditional ASR systems that rely solely on transcribed audio</li> <li>experiments on WSJ reduce WER of a strong character-based log-mel filterbank baseline</li> <li>more data for pre-training improves performance and that this approach not only improves resource-poor setups, but also settings where all WSJ training data is used</li> </ul>"}]}