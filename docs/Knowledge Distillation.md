---
title: Knowledge Distillation
---

# Knowledge Distillation
- Teacher model to help train the student model
- Teacher is often pre trained
- Student tries to imitate teacher
- [[Distillation Loss]]






























































































## Backlinks

> - [DeiT](DeiT.md)
>   - [[Knowledge Distillation]]

_Backlinks last generated 2022-07-26 20:33:15_
