---
title: FLASH

tags: architecture 
---

# FLASH
- [Transformer Quality in Linear Time](https://arxiv.org/abs/2202.10447)
- weaknesses in handling long sequences
- FLASH
- performant layer (gated linear unit) and by combining it with an accelerator-efficient approximation strategy ([mixed chunk attention](Mixed%20chunk%20attention.md))
- [[GAU]]
- [[Mixed chunk attention]] 
-
- outperforms three baselines: vanilla [Transformer](Transformer.md), Performer and Combiner in terms of quality and efficiency
- Wiki
- PG-19


















