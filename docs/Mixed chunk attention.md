---
title: Mixed chunk attention

tags: architecture 
---

# Mixed Chunk Attention
- an efficient linear approximation method that combines the benefits from partial and linear attention mechanisms, which is accelerator-friendly and highly competitive in quality.
- The method works on chunks of tokens and leverages local (within chunk) and global (between chunks) attention spans


## Backlinks

> - [FLASH](FLASH.md)
>   - [[Mixed chunk attention]]

_Backlinks last generated 2022-06-27 22:06:24_
