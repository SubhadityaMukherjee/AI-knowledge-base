<?xml version="1.0" encoding="UTF-8" ?>
<rss version="2.0">
    <channel>
      <title>Subhaditya&#039;s KB</title>
      <link>https://https://subhadityamukherjee.github.io/AI-knowledge-base/</link>
      <description>Last 10 notes on Subhaditya&#039;s KB</description>
      <generator>Quartz -- quartz.jzhao.xyz</generator>
      <item>
    <title>OpenML x SURF</title>
    <link>https://https:/subhadityamukherjee.github.io/AI-knowledge-base/KB/OpenML-x-SURF</link>
    <guid>https://https:/subhadityamukherjee.github.io/AI-knowledge-base/KB/OpenML-x-SURF</guid>
    <description>OpenML X SURF supernets - architecture + weight better than transfer learning For Openml Stuff hide test data SURF What They Do consulting help key researchers doesnt scale too well ...</description>
    <pubDate>Fri, 08 Nov 2024 12:08:10 GMT</pubDate>
  </item><item>
    <title>The Unified Causal AI Pipeline</title>
    <link>https://https:/subhadityamukherjee.github.io/AI-knowledge-base/KB/The-Unified-Causal-AI-Pipeline</link>
    <guid>https://https:/subhadityamukherjee.github.io/AI-knowledge-base/KB/The-Unified-Causal-AI-Pipeline</guid>
    <description>The Unified Causal AI Pipeline Sara Magliacana - prof at UvA Intro predict the effect of actions and decide effective policies - and how dl - can learn representations from unstructured ...</description>
    <pubDate>Fri, 08 Nov 2024 09:59:28 GMT</pubDate>
  </item><item>
    <title>Kipf Normalization</title>
    <link>https://https:/subhadityamukherjee.github.io/AI-knowledge-base/KB/Kipf-Normalization</link>
    <guid>https://https:/subhadityamukherjee.github.io/AI-knowledge-base/KB/Kipf-Normalization</guid>
    <description>Kipf Normalization .</description>
    <pubDate>Wed, 30 Oct 2024 10:47:41 GMT</pubDate>
  </item><item>
    <title>Edge Graphs</title>
    <link>https://https:/subhadityamukherjee.github.io/AI-knowledge-base/KB/Edge-Graphs</link>
    <guid>https://https:/subhadityamukherjee.github.io/AI-knowledge-base/KB/Edge-Graphs</guid>
    <description>Edge Graphs Edge Graphs each edge in the original graph becomes a node, and every two edges with a common node in the original graph create an edge in the new graph ...</description>
    <pubDate>Wed, 30 Oct 2024 10:29:57 GMT</pubDate>
  </item><item>
    <title>Layers for GNNs</title>
    <link>https://https:/subhadityamukherjee.github.io/AI-knowledge-base/KB/Layers-for-GNNs</link>
    <guid>https://https:/subhadityamukherjee.github.io/AI-knowledge-base/KB/Layers-for-GNNs</guid>
    <description>Layers for GNNs Layers for GNNs combined messages from adjacent nodes by summing them together with the transformed current node â†’ post-multiplying the node embedding matrix H by ...</description>
    <pubDate>Wed, 30 Oct 2024 10:29:57 GMT</pubDate>
  </item><item>
    <title>Transductive Models</title>
    <link>https://https:/subhadityamukherjee.github.io/AI-knowledge-base/KB/Transductive-Models</link>
    <guid>https://https:/subhadityamukherjee.github.io/AI-knowledge-base/KB/Transductive-Models</guid>
    <description>Transductive Models Transductive Models considers both the labeled and unlabeled data at the same time Semi Supervised It does not produce a rule but merely a labeling for the unknown ...</description>
    <pubDate>Wed, 30 Oct 2024 10:29:57 GMT</pubDate>
  </item><item>
    <title>Inductive Models</title>
    <link>https://https:/subhadityamukherjee.github.io/AI-knowledge-base/KB/Inductive-Models</link>
    <guid>https://https:/subhadityamukherjee.github.io/AI-knowledge-base/KB/Inductive-Models</guid>
    <description>Inductive Models Inductive Models we exploit a training set of labeled data to learn the relation between the inputs and outputs. Then we apply this to new test data.</description>
    <pubDate>Wed, 30 Oct 2024 10:29:57 GMT</pubDate>
  </item><item>
    <title>Batching for GNN</title>
    <link>https://https:/subhadityamukherjee.github.io/AI-knowledge-base/KB/Batching-for-GNN</link>
    <guid>https://https:/subhadityamukherjee.github.io/AI-knowledge-base/KB/Batching-for-GNN</guid>
    <description>Batching for GNN Batching for GNN GIven I training graphs {X_{i}, A_{i}} and labels y_{i}, params {\Phi = \{\beta_{k}, \Omega_{k}}\}^{K}_{k=0} , SGDand Binary Cross Entropy We ...</description>
    <pubDate>Wed, 30 Oct 2024 10:29:57 GMT</pubDate>
  </item><item>
    <title>Example GCN Layer</title>
    <link>https://https:/subhadityamukherjee.github.io/AI-knowledge-base/KB/Example-GCN-Layer</link>
    <guid>https://https:/subhadityamukherjee.github.io/AI-knowledge-base/KB/Example-GCN-Layer</guid>
    <description>Example GCN Layer Example GCN Layer this is equivariant, can cope with any number of neighbors, uses the graph srtucture to provide a Relational Inductive Bias and parameter sharing ...</description>
    <pubDate>Wed, 30 Oct 2024 10:29:57 GMT</pubDate>
  </item><item>
    <title>Parameter Sharing for Graphs</title>
    <link>https://https:/subhadityamukherjee.github.io/AI-knowledge-base/KB/Parameter-Sharing-for-Graphs</link>
    <guid>https://https:/subhadityamukherjee.github.io/AI-knowledge-base/KB/Parameter-Sharing-for-Graphs</guid>
    <description>Parameter Sharing for Graphs Parameter Sharing for Graphs We could learn a model with separate parameters associated with each node. However, now the network must independently learn the meaning of the connections in the graph at each position, and training would require many graphs with the same topology.</description>
    <pubDate>Wed, 30 Oct 2024 10:29:57 GMT</pubDate>
  </item>
    </channel>
  </rss>