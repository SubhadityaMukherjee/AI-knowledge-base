# Page 5
Underline:
DALL·E 2
Highlight:
generate original, genuine and realistic images and art from a prompt consisting on
a text description
Highlight:
DALL·E 2 manages to combine concepts, attributes and di↵erent styles
Highlight:
it uses the CLIP neural network
Underline:
CLIP
Highlight:
is a neural network trained on a variety of (image, text) pairs
Highlight:
Using CLIP, that can be instructed in natural language to predict the most relevant
text snippet, given an image, the model has recently merged as a successful
representation learner for images
Highlight:
Concretely, CLIP embeddings have several desirable properties
Highlight:
they are robust to image distribution shift, have impressive zero-shot capabilities
and have been fine-tuned to achieve state-of-the- art results
Highlight:
the CLIP image embedding decoder module is combined with a prior model, which
generates possible CLIP image embeddings from a given text caption
Page 6
Underline:
IMAGEN
Highlight:
text-to-image di↵usion model
## Highlight:
main discovery observed with this model made is that large language models, pre-
trained on text-only corpora, are very e↵ective at encoding text for image synthesis
Highlight:
increasing the size of the language model boosts both sample fidelity and image-
text alignment much more than increasing the size of the image di↵usion model
Underline:
Drawbench
Highlight:
set of 200 prompts that support the evaluation and comparison of text-to-image
models
Highlight:
the model is based on a pretrained text encoder (like BERT [12]) that performs a
mapping from text to a sequence of word embeddings and a cascade of conditional
di↵usion models that map these embeddings to images of increasing resolutions
Underline:
Stable Di↵usion
Highlight:
latent-di↵usion model
Highlight:
The main di↵erence of this model with respect to the other ones is the use of a
latent di↵usion model and that it performs image modification as it can perform
operations in its latent space
Highlight:
Stable Di↵usion consists of two parts: the text encoder and the image generator
Highlight:
The image information creator works completely in the latent space
Highlight:
This property makes it faster than previous di↵usion models that worked in a pixel
space
Page 7
Underline:
# Muse
Highlight:
Text-to-image transformer model
Highlight:
state-of- the-art image generation while being more ecient than di↵usion or
autoregres- sive models
Highlight:
it is trained on a masked modelling task in discrete token space
Highlight:
more ecient because of the use of discrete tokens and requiring fewer sampling
iterations
Highlight:
parallel decoding
Highlight:
Muse is 10x faster at inference time than Imagen-3B or Parti-3B and 3x faster than
Stable Di↵usion v 1.4
Highlight:
Muse is also faster than than Stable Di↵usion in spite of both models working in the
latent space of a VQGAN
Underline:
Dreamfusion
Highlight:
text-to-3D model
Highlight:
uses a pretrained 2D text-to-image di↵usion model to perform text- to-3D
synthesis
Highlight:
Dreamfusion replaces previous CLIP tech- niques with a loss derived from
distillation of a 2D di↵usion model
Highlight:
the di↵usion model can be used as a loss within a generic continuous optimization
problem to generate samples
## Highlight:
sampling in parameter space is much harder than in pixels as we want to create 3D
models that look like good images when rendered from random angles
Highlight:
this model uses a di↵er- entiable generator
Highlight:
Other approaches are focused on sampling pixels, however, this model instead
focuses on creating 3D models that look like good images when rendered from
random angles
Page 9
Underline:
Magic3D
Highlight:
text to 3D model
Highlight:
While the Dreamfusion model achieves remarkable results, the method has two
problems
Highlight:
long processing time
Highlight:
low-quality of the generated images
Highlight:
these problems are addressed by Magic3D using a two-stage optimization
framework
Highlight:
Magic3D builds a low-resolution di↵usion prior and, then, it accelerates with a
sparse 3D hash grid structure
Highlight:
a textured 3D mesh model is furthered optimized with an ecient di↵erentiable
render
Highlight:
higher quality 3D shapes in both geometry and texture compared to DreamFusion
## Underline:
Flamingo
Highlight:
A Visual Language Model created by Deepmind using few shot learning on a wide
range of open-ended vision and language tasks, simply by being prompted with a
few input/output examples
Highlight:
the input of Flamingo contains visually conditioned autoregressive text generation
models able to ingest a sequence of text tokens interleaved with images and/or
videos
Page 10
Highlight:
and produce text as output
Highlight:
A query is made to the model along with a photo or a video and the model answers
with a text answer
Highlight:
Flamingo models take advantage of two complementary models: a vision model that
analyzes visual scenes and a large language model which performs a basic form of
reasoning
Highlight:
The language model is trained on a large amount of text data.
Underline:
VisualGPT
Highlight:
image captioning model
Highlight:
leverages knowledge from the pretrained language model GPT-2
Highlight:
bridge the semantic gap between di↵erent modalities
Highlight:
novel encoder-decoder attention mechanism [33] is designed with an unsaturated
# rectified gating function
Highlight:
the biggest advantage of this model is that it does not need for as much data as
other image-to-text models
Highlight:
improving data eciency in image captioning networks would enable quick data
curation, description of rare objects, and applications in specialized domains
Page 11
Underline:
Phenaki
Highlight:
capable of performing realistic video synthesis, given a sequence of textual prompts
Highlight:
Phenaki is the first model that can generate videos from open domain time variable
prompts
Highlight:
To address data issues, it performs joint training on a large image-text pairs dataset
as well as a smaller number of video-text exam- ples can result in generalization
beyond what is available in the video datasets.
Highlight:
image-text datasets having billions of inputs
Page 12
Highlight:
limitations come from computational capabilities for videos of variable length
Highlight:
the C-ViViT encoder, the training transformer and the video generator
Highlight:
The encoder gets a compressed representation of videos.
Highlight:
First tokens are transformed into embeddings.
Highlight:
# This is followed by the temporal transformer, then the spatial transformer
Highlight:
After the output of the spatial trans- former, they apply a single linear projection
without activation to map the tokens back to pixel space
Highlight:
Consequently, the model generates temporally coherent and diverse videos
conditioned on open domain prompts even when the prompt is a new composition
of concepts
Page 13
Underline:
Soundify
Highlight:
In video editing, sound in half of the story
Highlight:
for professional video editing, the problems come from finding suitable sounds,
aligning sounds, video and tuning parameters
Highlight:
matches sound e↵ects to video
Highlight:
uses quality sound e↵ects libraries and CLIP
Highlight:
Concretely, the system has three parts: classification, synchronization, and mix
Highlight:
The classification matches e↵ects to a video by classifying sound emitters within
Highlight:
To reduce the distinct sound emitters, the video is split based on absolute color
histogram distances
Highlight:
In the synchronization part, intervals are identified comparing e↵ects label with
each frame and pinpointing consecutive matches above a threshold
Highlight:
In the mix part, e↵ects are split into around one-second chunks
Highlight:
chunks are stitched via crossfades.
Underline:
AudioLM
Highlight:
maps the input audio into a sequence of discrete tokens and casts audio generation
as language mod- eling task in this representation space
Highlight:
training on large corpora of raw
Page 14
Highlight:
audio waveforms
Highlight:
learns to generate natural and coherent continua- tions given short prompts
Highlight:
extended beyond speech by generating coherent piano music continuations,
despite being trained without any symbolic representation of music
Highlight:
When it comes to audio synthesis, multiple scales make achieving high audio quality
while displaying consistency very challenging
Highlight:
This gets achieved by this model by combining recent advances in neural audio
compression, self-supervised representation learning and language modelling.
Underline:
Jukebox
Highlight:
generates music with singing in the raw audio domain
Highlight:
earlier models in the text-to-music genre generated music symbolically in the form
of a pianoroll which specifies timing, pitch and velocity.
## Highlight:
The challenging aspect is the non-symbolic approach where music is tried to be
produced directly as a piece of audio
Highlight:
the space of raw audio is extremely high dimensional which makes the problem very
challenging
Highlight:
the key issue is that modelling that raw audio produces long-range dependencies,
making it computationally challenging to learn the high-level semantics of music.
Highlight:
hi- erarchical VQ-VAE architecture to compress audio into a discrete space [14],
with a loss function designed to retain the most amount of information.
Highlight:
This model produces songs from very di↵erent genres such as rock, hip-hop and
jazz.
Underline:
Whisper
Highlight:
Audio-to-Text converter
Highlight:
multi-lingual speech recognition, translation and language identification
Highlight:
goal of a speech recognition system should be to work reliably out of the box in a
broad range of environments without requiring supervised fine-tuning of a decoder
for every deployment distribution
Highlight:
lack of a high-quality pre-trained decoder.
Highlight:
680,000 hours of labeled audio data
Page 15
Highlight:
broken in 30 second segments paired with the subset of the transcript that occurs
# within that time segment.
Highlight:
encoder-deccoder transformer
Underline:
ChatGPT
Highlight:
interacts in a conversational way
Highlight:
the model answers follow-up questions, challenges incorrect premises and reject
inappropriate requests
Highlight:
Reinforcement Learning for Human Feedback
Highlight:
an initial model is trained using supervised fine-tuning: human AI trainers would
provide conversations in which they played both sides, the user and an AI assistant
Highlight:
those people would be given the model-written re- sponses to help them compose
their response
Highlight:
This dataset was mixed to that of InstructGPT [3], which was transformed into a
dialogue format
Underline:
LaMDA
Highlight:
language model for dialog applications
Highlight:
family of transformer-based neural language models specialized for dialog which
have up to 137B parameters and are pre-trained on 1.56T words of public dialog
data and web text.
Highlight:
Fine-tuning can enable for safety and factual grounding of the model
## Highlight:
Only 0.001% of training data was used for fine-tuning, which is a great achievement
of the model
Highlight:
dialog modes take advantage of Transformers’ ability to present long-term
dependencies in text
Highlight:
generally very well-suited for model scaling
Highlight:
use of a single model to perform multiple tasks: it generates several responses,
which are filtered for safety, grounded on an external knowledge source and re-
ranked to find the highest-quality response.
Underline:
PEER
Highlight:
trained on edit histories to cover the entire writing process
Page 16
Highlight:
Plan, Edit, Explain and Repeat
Highlight:
These steps are repeated until the text is in a satisfactory state that requires no
further updates
Highlight:
The model allow to decompose the task of writing a paper into multiple easier
subtasks
Highlight:
the model allows humans to intervene at any time and steer the model in any
direction
Highlight:
Wikipedia edit histories
Highlight:
The approach is a self- training, using models to infill missing data and then train
# other models on this synthetic data
Highlight:
The downside of this comes from comments being very noisy and a lack of citations,
which tries to be compensated by a retrieval system which does not always work
Page 17
Highlight:
The entire process of formulating a plan, collecting documents, performing an edit
and explaining it can be repeated multiple times until arriving at a sequence of text
Highlight:
a DeepSpeed transformer is used
Underline:
Meta AI Speech from Brain
Highlight:
help people unable to communicate through speech, typing or gestures
Highlight:
tries to decode language directly from noninvasive brain recordings
Highlight:
challenge with this proposed method come from noise and di↵erences in each
person’s brain and where the sensors are placed.
Highlight:
contrastive learning and used to max- imally align noninvasive brain recordings and
speech sounds
Highlight:
A self-supervised learning model called wave2vec 2.0. is used to identify the
complex representa- tions of speech in the brains of volunteers listening to
audiobooks
Highlight:
The two nonin- vasive technologies used to measure neuronal activity are
electroencephalography and magnetoencephalography.
Highlight:
Training data comes from four opensource datasets which represent 150 hours of
recordings of 169 volunteers listening to audiobooks
## Highlight:
EEG and MEG record- ings are inserted into a brain model, which consists of a
standard deep convolu- tional network with residual connections
Highlight:
These recordings are what comes from individuals’ brains
Highlight:
both a speech model for sound and a brain model for MEG data.
Highlight:
several components of the algorithm were beneficial to decoding performance
Highlight:
algorithm improves as EEG and MEG recordings increase
Highlight:
self-supervised trained AI can decode perveived speech despite noise and
variability in that data.
Underline:
Codex
Highlight:
translates text to code
Highlight:
general-purpose programming model, as it can be applied to basically any
programming task
Highlight:
Programming can be broken down into two parts: breaking a problem down into
simpler problems and mapping those problems into existing code (libraries, APIs, or
functions) that already exist
Highlight:
The second part is the most time-barring part for programmers, and it is where
Codex excels the most
Page 18
Highlight:
model is fine-tuned from GPT-3, which already contains strong natural language
representations
## Underline:
Alphacode
Highlight:
Other language models have demonstrated an impressive ability to generate code,
but these systems still perform poorly when evaluated on more complex, unseen
problems
Highlight:
Alphacode is a system for code generation for problems that require for deeper
reasoning
Highlight:
having an extensive dataset for training and evaluation, large and ecient transformer
based architectures and a large-scale model sampling.
Highlight:
model is firstly pre-trained through GitHub repos- itories amounting to 715.1 GB of
code.
Highlight:
more extensive dataset than Codex’s pre training dataset.
Highlight:
For the training to be better, a fine-tuning dataset is introduced from the Codeforces
plataform
Highlight:
Codecontests are conducted, for the validation phase, in which we better the per-
formance of the model.
Highlight:
transformer-based architecture, they use an encoder-decoder transformer
architecture
Highlight:
Compared to decoder-only archi- tectures commonly used, this architecture allows
for a bidirectional description and extra flexibility.
Highlight:
shallow encoder and a deep encoder to further the model’s ecienc
Highlight:
o reduce the cost of sampling, multi-query attention is used.
## Underline:
Galactica
Highlight:
new large model for automatically organizing science developed by Meta AI and
Papers with Code
Highlight:
ability to train on it for multiple epochs without overfitting, where up- stream and
downstream performance improves with use of repeated token
Highlight:
The dataset design is critical to the approach as all of it is processed in a common
markdown format to blend knowledge between sources.
Highlight:
Citations are processed via a certain token that allows researchers to predict a
citation given any in- put context
Highlight:
The capability of the model of predicting citations improves with scale and the
model becomes better at the distribution of citations
Highlight:
the model can perform multi-modal tasks involving SMILES chemical formulas and
protein sequences
Highlight:
transformer architecture in a decoder-only setup with GeLU activation for all model
sizes.
Underline:
Minerva
Highlight:
Language model capable of solving mathematical and scientific ques- tions using
step-by-step reasoning
Highlight:
very clear focus on the collec- tion of training data for this purpose
Highlight:
solves quantitative reasoning problems,
# Page 19
Highlight:
makes models at scale and employs best-in-class inference techniques
Highlight:
Concretely, Minerva solves these problems by generating solutions step-by-step
Highlight:
this means including calculations and symbolic manipulation without having the
need for external tools such a calculator.

## ABSTRACT
- The impressive lifelong learning in animal brains is primarily enabled by plasticchanges in synaptic connectivity
- Importantly, these changes are not passive, but are actively controlled byneuromodulation, which is itself under the control of the brain
- The resulting self-modifying abilities of the brain play an important role in learningand adaptation, and are a major basis for biological reinforcement learning
- artificial neural networks with such neuromodulated plasticity can be trained withgradient descen
- differentiable Hebbian plasticity
- neuromodulated plasticity improves the performance of neural networks on bothreinforcement learning and supervised learning tasks
- neuromodulated plastic LSTMs with millions of parameters outperform standardLSTMs on a benchmark language modeling task
## BACKGROUND: DIFFERENTIABLE HEBBIAN PLASTICITY
- (Miconi, 2016; Miconi et al., 2018)
- allows gradient descent to optimize not just the weights, but also the plasticity ofeach connection
## 
- each connection in the network is augmented with a Hebbian plastic componentthat grows and decays automatically as a result of ongoing activity. In effect, eachconnection contains a fixed and a plastic component:
- ![img_p1_1](img_p1_1.png)
- Hebbi,j is initialized to zero at the beginning of each episode/lifetime, and is updatedautomatically
- purely episodic/intra-life quantity
- wi,j, fi,j and ⌘ are the structural components of the network, which are optimizedby gradient descent between episodes/lifetimes to minimize the expected loss overan episode.
- Clip(x) in Eq. 2 is any function or procedure that constrains Hebbi,j to the [1, 1]range, to negate the inherent instability of Hebbian learning.
- distinction between the ⌘ and fi,j parameters: ⌘ is the intra-life "learning rate" ofplastic connections
- determines how fast new information is incorporated into the plastic component
- fi,j is a scale parameter, which determines the maximum magnitude of the plasticcomponent (since Hebbi,j is constrained to the [-1,1] range)
- # in contrast to other approaches using uniform plasticity (Schmidhuber, 1993a),including "fast weights" (Ba et al., 2016), the amount of plasticity in each connection(represented by fi,j ) is trainable, allowing the meta-optimizer to design complexlearning strategies
- implementing a plastic recurrent network only requires less than four additional linesof code over a standard recurrent network implementation
## BACKPROPAMINE: DIFFERENTIABLE NEUROMODULATION OF PLASTICITY
- plasticity is modulated on a moment-to-moment basis by a networkcontrolledneuromodulatory signal M (t)
- The computation of M (t) could be done in various ways; at present, it is simply asingle scalar output of the network, which is used either directly (for the simple RLtasks) or passed through a meta-learned vector of weights (one for eachconnection, for the language modeling task)
## SIMPLE NEUROMODULATION
- make the (global) ⌘ parameter depend on the output of one or more neurons in thenetwork
- Because ⌘ essentially determines the rate of plastic change, placing it undernetwork control allows the network to determine how plastic connections should beat any given time.
- only modification to the equations above in this simple neuromodulation variant is toreplace ⌘ in Eq. 2 with the network-computed, time-varying neuromodulatory signalM (t)
- ![img_p2_1](img_p2_1.png)
## 
## RETROACTIVE NEUROMODULATION AND ELIGIBILITY TRACES
- alternative neuromodulation scheme that takes inspiration from the short-termretroactive effects of neuromodulatory dopamine on Hebbian plasticity in animalbrains
- dopamine was shown to retroactively gate the plasticity induced by past activity,within a short time window of about 1s (Yagishita et al., 2014; He et al., 2015; Fisheret al., 2017; Cassenaer & Laurent, 2012)
- Thus, Hebbian plasticity does not directly modify the synaptic weights, but creates afast-decaying "potential" weight change, which is only incorporated into the actualweights if the synapse receives dopamine within a short time window
- eligibility trace
- keeping memory of which synapses contributed to recent activity, while thedopamine signal modulates the transformation of these eligibility traces into actualplastic changes.
- ![img_p3_1](img_p3_1.png)
## ABSTRACT
- The impressive lifelong learning in animal brains is primarily enabled by plasticchanges in synaptic connectivity
- Importantly, these changes are not passive, but are actively controlled byneuromodulation, which is itself under the control of the brain
- The resulting self-modifying abilities of the brain play an important role in learningand adaptation, and are a major basis for biological reinforcement learning
- artificial neural networks with such neuromodulated plasticity can be trained withgradient descen
- differentiable Hebbian plasticity
- neuromodulated plasticity improves the performance of neural networks on bothreinforcement learning and supervised learning tasks
- neuromodulated plastic LSTMs with millions of parameters outperform standardLSTMs on a benchmark language modeling task
## BACKGROUND: DIFFERENTIABLE HEBBIAN PLASTICITY
- (Miconi, 2016; Miconi et al., 2018)
- allows gradient descent to optimize not just the weights, but also the plasticity ofeach connection
## 
- each connection in the network is augmented with a Hebbian plastic componentthat grows and decays automatically as a result of ongoing activity. In effect, eachconnection contains a fixed and a plastic component:
- ![img_p1_1](img_p1_1.png)
- Hebbi,j is initialized to zero at the beginning of each episode/lifetime, and is updatedautomatically
- purely episodic/intra-life quantity
- wi,j, fi,j and ⌘ are the structural components of the network, which are optimizedby gradient descent between episodes/lifetimes to minimize the expected loss overan episode.
- Clip(x) in Eq. 2 is any function or procedure that constrains Hebbi,j to the [1, 1]range, to negate the inherent instability of Hebbian learning.
- distinction between the ⌘ and fi,j parameters: ⌘ is the intra-life "learning rate" ofplastic connections
- determines how fast new information is incorporated into the plastic component
- fi,j is a scale parameter, which determines the maximum magnitude of the plasticcomponent (since Hebbi,j is constrained to the [-1,1] range)
- # in contrast to other approaches using uniform plasticity (Schmidhuber, 1993a),including "fast weights" (Ba et al., 2016), the amount of plasticity in each connection(represented by fi,j ) is trainable, allowing the meta-optimizer to design complexlearning strategies
- implementing a plastic recurrent network only requires less than four additional linesof code over a standard recurrent network implementation
## BACKPROPAMINE: DIFFERENTIABLE NEUROMODULATION OF PLASTICITY
- plasticity is modulated on a moment-to-moment basis by a networkcontrolledneuromodulatory signal M (t)
- The computation of M (t) could be done in various ways; at present, it is simply asingle scalar output of the network, which is used either directly (for the simple RLtasks) or passed through a meta-learned vector of weights (one for eachconnection, for the language modeling task)
## SIMPLE NEUROMODULATION
- make the (global) ⌘ parameter depend on the output of one or more neurons in thenetwork
- Because ⌘ essentially determines the rate of plastic change, placing it undernetwork control allows the network to determine how plastic connections should beat any given time.
- only modification to the equations above in this simple neuromodulation variant is toreplace ⌘ in Eq. 2 with the network-computed, time-varying neuromodulatory signalM (t)
- ![img_p2_1](img_p2_1.png)
## 
## RETROACTIVE NEUROMODULATION AND ELIGIBILITY TRACES
- alternative neuromodulation scheme that takes inspiration from the short-termretroactive effects of neuromodulatory dopamine on Hebbian plasticity in animalbrains
- dopamine was shown to retroactively gate the plasticity induced by past activity,within a short time window of about 1s (Yagishita et al., 2014; He et al., 2015; Fisheret al., 2017; Cassenaer & Laurent, 2012)
- Thus, Hebbian plasticity does not directly modify the synaptic weights, but creates afast-decaying "potential" weight change, which is only incorporated into the actualweights if the synapse receives dopamine within a short time window
- eligibility trace
- keeping memory of which synapses contributed to recent activity, while thedopamine signal modulates the transformation of these eligibility traces into actualplastic changes.
- ![img_p3_1](img_p3_1.png)---
title: <% tp.file.title %>

tags: explainability 
date modified: <% tp.date.now("dddd Do MMMM YYYY, ddd") %>
date created: <% tp.date.now("dddd Do MMMM YYYY, ddd") %>
---

# <% tp.file.title %>
```toc
```
- *Summary* : Summary : Uses a transformer network to prediction a "prompt" that says how degraded an image is. And based on that, decides what module to use. 
- Deep learning-based methods have significantly improved image restoration performance, however, they have limited generalization ability to different degradation types and levels 
- requires training individual models for each specific degradation and knowing the input degradation type to apply the relevant model. 
- prompt-based learning approach, PromptIR, for All-In-One image restoration that can effectively restore images from various types and levels of degradation 
- uses prompts to encode degradation-specific information 
- generic and efficient plugin module with few lightweight prompts that can be used to restore images of various types and levels of degradation with no prior information on the corruptions present in the image  
- ![img_p1_1](img_p1_1.png) 
- AirNet 
- addresses the all-in-one restoration task by employing the contrastive learning paradigm. 
- involves training an extra encoder to differentiate various types of image degradations 
- Although AirNet [29] yields state-of-the-art results, it struggles to model fully disentangled representations of different corruption types 
- Furthermore, the usage of an additional encoder for contrastive learning leads to a higher training burden due to the two-stage training approach. 
## Method 
- # aim to learn a single model M to restore an image I from a degraded image I, that has been degraded using a degradation D, while having no prior information about D. 
- While the model is initially "blind" to the nature of degradation, its performance in recovering a clean image can be enhanced by providing implicit contextual information about the type of degradation  
- ![img_p2_1](img_p2_1.png) 
- From a given degraded input image I ∈ RH ×W ×3 
- first extracts low-level features F0 ∈ RH×W×C by applying a convolution operation; where H × W is the spatial resolution and C denotes the channels. 
- eature embeddings F0 undergo a 4-level hierarchical encoder-decoder, transforming into deep features Fr ∈ RH ×W ×2C 
- # Each level of the encoder-decoder employs several Transformer blocks, with the number of blocks gradually increasing from the top level to the bottom level to maintain computational efficiency. 
- Starting from the high-resolution input, the goal of the encoder is to progressively reduce the spatial resolution while . 
- From the low-resolution latent features Fl, the aim of the decoder is to gradually recover the highresolution clean output 
- ncorporate prompt block 
- Prompt blocks are adapter modules that sequentially connect every two levels of the decoder. 
## Prompt Block  
- ![img_p3_1](img_p3_1.png) 
## Prompt Generation Module 
- Prompt components Pc form a set of learnable parameters that interact with the incoming features to embed degradation information 
- features-prompt interaction is to directly use the learned prompts to calibrate the features. 
- # dynamically predicts attention-based weights from the input features and apply them to prompt components to yield input-conditioned prompts P 
- shared space to facilitate correlated knowledge sharing among prompt components. 
- To generate prompt-weights from the input features Fl 
- first applies global average pooling (GAP) across spatial dimension to generate feature vector v 2 RCˆ 
- pass v through a channeldownscaling convolution layer to obtain a compact feature vector, followed by the softmax operation, thus yielding prompt-weights w 2 RN 
- use these weights to make adjustments in prompt components, followed by a 3 x 3 convolution layer  
- ![img_p4_1](img_p4_1.png) 
- Since at inference time, it is necessary for the restoration network to be able to handle images of different resolutions, we cannot use the prompt components Pc with a fixed size. 
- bilinear upsampling operation to upscale the prompt components 
## Prompt Interaction Module 
- enable interaction between the input features Fl and prompts P for a guided restoration. 
- # In PIM, we concatenate the generated prompts with the input features along the channel dimension. 
- pass the concatenated representations through a Transformer block that exploits degradation information encoded in the prompts and transforms the input features. 
- The Transformer block is composed of two sequentially connected sub-modules: Multi-Dconv head transposed attention (MDTA), and Gated-Dconv feedforward network (GDFN). MDTA applies self-attention operation across channels rather than the spatial dimension and has linear complexity. 
- The goal of GDFN is to transform features in a controlled manner, i.e., suppressing the less informative features and allowing only useful ones to propagate through the network  
- ![img_p5_1](img_p5_1.png)  
- ![img_p5_2](img_p5_2.png) 
##  
- ![img_p6_1](img_p6_1.png) 
- Implementation Details 
- end-to-end trainable and requires no pretraining of any individual component 
- 4-level encoder-decoder, with varying numbers of Transformer blocks at each level, specifically [4, 6, 6, 8] from level-1 to level-4. 
- one prompt block between every two consecutive decoder levels, totaling 3 prompt blocks in the overall PromptIR network 
- The total number of prompt components are 5 
- The model is trained with a batch size of 32 in the all-in-one setting, and with a batch of 8 in the single-task setting 
- The network is optimized with an L1 loss, and we use Adam optimizer (1 = 0.9, 2 = 0.999) with learning rate 2e 4 for 200 epochs. 
- # cropped patches of size 128 x 128 
- BSD400 
- WED 
- o 
- BSD68 
- Urban100 
- Rain100L 
- SOTS  
- ![img_p8_1](img_p8_1.png)  
- ![img_p8_2](img_p8_2.png)  
- ![img_p9_1](img_p9_1.png) ---
title: <% tp.file.title %>

tags: explainability 
date modified: <% tp.date.now("dddd Do MMMM YYYY, ddd") %>
date created: <% tp.date.now("dddd Do MMMM YYYY, ddd") %>
---

# <% tp.file.title %>
```toc
```
- *Summary* : Summary : Uses a transformer network to prediction a "prompt" that says how degraded an image is. And based on that, decides what module to use. 
- Deep learning-based methods have significantly improved image restoration performance, however, they have limited generalization ability to different degradation types and levels 
- requires training individual models for each specific degradation and knowing the input degradation type to apply the relevant model. 
- prompt-based learning approach, PromptIR, for All-In-One image restoration that can effectively restore images from various types and levels of degradation 
- uses prompts to encode degradation-specific information 
- generic and efficient plugin module with few lightweight prompts that can be used to restore images of various types and levels of degradation with no prior information on the corruptions present in the image  
- ![img_p1_1](img_p1_1.png) 
- AirNet 
- addresses the all-in-one restoration task by employing the contrastive learning paradigm. 
- involves training an extra encoder to differentiate various types of image degradations 
- Although AirNet [29] yields state-of-the-art results, it struggles to model fully disentangled representations of different corruption types 
- Furthermore, the usage of an additional encoder for contrastive learning leads to a higher training burden due to the two-stage training approach. 
## Method 
- # aim to learn a single model M to restore an image I from a degraded image I, that has been degraded using a degradation D, while having no prior information about D. 
- While the model is initially "blind" to the nature of degradation, its performance in recovering a clean image can be enhanced by providing implicit contextual information about the type of degradation  
- ![img_p2_1](img_p2_1.png) 
- From a given degraded input image I ∈ RH ×W ×3 
- first extracts low-level features F0 ∈ RH×W×C by applying a convolution operation; where H × W is the spatial resolution and C denotes the channels. 
- eature embeddings F0 undergo a 4-level hierarchical encoder-decoder, transforming into deep features Fr ∈ RH ×W ×2C 
- # Each level of the encoder-decoder employs several Transformer blocks, with the number of blocks gradually increasing from the top level to the bottom level to maintain computational efficiency. 
- Starting from the high-resolution input, the goal of the encoder is to progressively reduce the spatial resolution while . 
- From the low-resolution latent features Fl, the aim of the decoder is to gradually recover the highresolution clean output 
- ncorporate prompt block 
- Prompt blocks are adapter modules that sequentially connect every two levels of the decoder. 
## Prompt Block  
- ![img_p3_1](img_p3_1.png) 
## Prompt Generation Module 
- Prompt components Pc form a set of learnable parameters that interact with the incoming features to embed degradation information 
- features-prompt interaction is to directly use the learned prompts to calibrate the features. 
- # dynamically predicts attention-based weights from the input features and apply them to prompt components to yield input-conditioned prompts P 
- shared space to facilitate correlated knowledge sharing among prompt components. 
- To generate prompt-weights from the input features Fl 
- first applies global average pooling (GAP) across spatial dimension to generate feature vector v 2 RCˆ 
- pass v through a channeldownscaling convolution layer to obtain a compact feature vector, followed by the softmax operation, thus yielding prompt-weights w 2 RN 
- use these weights to make adjustments in prompt components, followed by a 3 x 3 convolution layer  
- ![img_p4_1](img_p4_1.png) 
- Since at inference time, it is necessary for the restoration network to be able to handle images of different resolutions, we cannot use the prompt components Pc with a fixed size. 
- bilinear upsampling operation to upscale the prompt components 
## Prompt Interaction Module 
- enable interaction between the input features Fl and prompts P for a guided restoration. 
- # In PIM, we concatenate the generated prompts with the input features along the channel dimension. 
- pass the concatenated representations through a Transformer block that exploits degradation information encoded in the prompts and transforms the input features. 
- The Transformer block is composed of two sequentially connected sub-modules: Multi-Dconv head transposed attention (MDTA), and Gated-Dconv feedforward network (GDFN). MDTA applies self-attention operation across channels rather than the spatial dimension and has linear complexity. 
- The goal of GDFN is to transform features in a controlled manner, i.e., suppressing the less informative features and allowing only useful ones to propagate through the network  
- ![img_p5_1](img_p5_1.png)  
- ![img_p5_2](img_p5_2.png) 
##  
- ![img_p6_1](img_p6_1.png) 
- Implementation Details 
- end-to-end trainable and requires no pretraining of any individual component 
- 4-level encoder-decoder, with varying numbers of Transformer blocks at each level, specifically [4, 6, 6, 8] from level-1 to level-4. 
- one prompt block between every two consecutive decoder levels, totaling 3 prompt blocks in the overall PromptIR network 
- The total number of prompt components are 5 
- The model is trained with a batch size of 32 in the all-in-one setting, and with a batch of 8 in the single-task setting 
- The network is optimized with an L1 loss, and we use Adam optimizer (1 = 0.9, 2 = 0.999) with learning rate 2e 4 for 200 epochs. 
- # cropped patches of size 128 x 128 
- BSD400 
- WED 
- o 
- BSD68 
- Urban100 
- Rain100L 
- SOTS  
- ![img_p8_1](img_p8_1.png)  
- ![img_p8_2](img_p8_2.png)  
- ![img_p9_1](img_p9_1.png) ---
title: <% tp.file.title %>

tags: explainability 
date modified: <% tp.date.now("dddd Do MMMM YYYY, ddd") %>
date created: <% tp.date.now("dddd Do MMMM YYYY, ddd") %>
---

# <% tp.file.title %>
```toc
```
- *Summary* : Summary : Uses a transformer network to prediction a "prompt" that says how degraded an image is. And based on that, decides what module to use. 
- Deep learning-based methods have significantly improved image restoration performance, however, they have limited generalization ability to different degradation types and levels 
- requires training individual models for each specific degradation and knowing the input degradation type to apply the relevant model. 
- prompt-based learning approach, PromptIR, for All-In-One image restoration that can effectively restore images from various types and levels of degradation 
- uses prompts to encode degradation-specific information 
- generic and efficient plugin module with few lightweight prompts that can be used to restore images of various types and levels of degradation with no prior information on the corruptions present in the image  
- ![img_p1_1](img_p1_1.png) 
- AirNet 
- addresses the all-in-one restoration task by employing the contrastive learning paradigm. 
- involves training an extra encoder to differentiate various types of image degradations 
- Although AirNet [29] yields state-of-the-art results, it struggles to model fully disentangled representations of different corruption types 
- Furthermore, the usage of an additional encoder for contrastive learning leads to a higher training burden due to the two-stage training approach. 
## Method 
- # aim to learn a single model M to restore an image I from a degraded image I, that has been degraded using a degradation D, while having no prior information about D. 
- While the model is initially "blind" to the nature of degradation, its performance in recovering a clean image can be enhanced by providing implicit contextual information about the type of degradation  
- ![img_p2_1](img_p2_1.png) 
- From a given degraded input image I ∈ RH ×W ×3 
- first extracts low-level features F0 ∈ RH×W×C by applying a convolution operation; where H × W is the spatial resolution and C denotes the channels. 
- eature embeddings F0 undergo a 4-level hierarchical encoder-decoder, transforming into deep features Fr ∈ RH ×W ×2C 
- # Each level of the encoder-decoder employs several Transformer blocks, with the number of blocks gradually increasing from the top level to the bottom level to maintain computational efficiency. 
- Starting from the high-resolution input, the goal of the encoder is to progressively reduce the spatial resolution while . 
- From the low-resolution latent features Fl, the aim of the decoder is to gradually recover the highresolution clean output 
- ncorporate prompt block 
- Prompt blocks are adapter modules that sequentially connect every two levels of the decoder. 
## Prompt Block  
- ![img_p3_1](img_p3_1.png) 
## Prompt Generation Module 
- Prompt components Pc form a set of learnable parameters that interact with the incoming features to embed degradation information 
- features-prompt interaction is to directly use the learned prompts to calibrate the features. 
- # dynamically predicts attention-based weights from the input features and apply them to prompt components to yield input-conditioned prompts P 
- shared space to facilitate correlated knowledge sharing among prompt components. 
- To generate prompt-weights from the input features Fl 
- first applies global average pooling (GAP) across spatial dimension to generate feature vector v 2 RCˆ 
- pass v through a channeldownscaling convolution layer to obtain a compact feature vector, followed by the softmax operation, thus yielding prompt-weights w 2 RN 
- use these weights to make adjustments in prompt components, followed by a 3 x 3 convolution layer  
- ![img_p4_1](img_p4_1.png) 
- Since at inference time, it is necessary for the restoration network to be able to handle images of different resolutions, we cannot use the prompt components Pc with a fixed size. 
- bilinear upsampling operation to upscale the prompt components 
## Prompt Interaction Module 
- enable interaction between the input features Fl and prompts P for a guided restoration. 
- # In PIM, we concatenate the generated prompts with the input features along the channel dimension. 
- pass the concatenated representations through a Transformer block that exploits degradation information encoded in the prompts and transforms the input features. 
- The Transformer block is composed of two sequentially connected sub-modules: Multi-Dconv head transposed attention (MDTA), and Gated-Dconv feedforward network (GDFN). MDTA applies self-attention operation across channels rather than the spatial dimension and has linear complexity. 
- The goal of GDFN is to transform features in a controlled manner, i.e., suppressing the less informative features and allowing only useful ones to propagate through the network  
- ![img_p5_1](img_p5_1.png)  
- ![img_p5_2](img_p5_2.png) 
##  
- ![img_p6_1](img_p6_1.png) 
- Implementation Details 
- end-to-end trainable and requires no pretraining of any individual component 
- 4-level encoder-decoder, with varying numbers of Transformer blocks at each level, specifically [4, 6, 6, 8] from level-1 to level-4. 
- one prompt block between every two consecutive decoder levels, totaling 3 prompt blocks in the overall PromptIR network 
- The total number of prompt components are 5 
- The model is trained with a batch size of 32 in the all-in-one setting, and with a batch of 8 in the single-task setting 
- The network is optimized with an L1 loss, and we use Adam optimizer (1 = 0.9, 2 = 0.999) with learning rate 2e 4 for 200 epochs. 
- # cropped patches of size 128 x 128 
- BSD400 
- WED 
- o 
- BSD68 
- Urban100 
- Rain100L 
- SOTS  
- ![img_p8_1](img_p8_1.png)  
- ![img_p8_2](img_p8_2.png)  
- ![img_p9_1](img_p9_1.png) 