---
title: Mixed chunk attention

tags: architecture 
date modified: Thursday, August 11th 2022, 12:32:50 am
date created: Tuesday, July 26th 2022, 8:33:15 pm
---

# Mixed Chunk [[Attention]]
- an efficient linear approximation method that combines the benefits from partial and linear [[Attention|attention]] mechanisms, which is accelerator-friendly and highly competitive in quality.
- The method works on chunks of tokens and leverages local (within chunk) and global (between chunks) [[Attention|attention]] spans

## Backlinks

> - [FLASH](FLASH.md)
>   - [[Mixed chunk attention]]
>    
> - [Attention](Attention.md)
>   - [[Mixed chunk attention]]

_Backlinks last generated 2022-10-04 13:01:19_
