---
title: MSE
tags: loss
---

# MSE
- $$L(x) = \Sigma_i ||D(E(x_i))||^2$$
- $$MSE = \frac{1}{N} \Sigma^N_{i=1}(p(x_i) - y_i)^2$$


























































## Backlinks

> - [Cross [Entropy](Entropy.md)](Cross Entropy.md)
>   - [[MSE]]
>    
> - [AutoEncoder](Auto Encoders.md)
>   - [[MSE]] : Unsupervised
>    
> - [MSLE](MSLE.md)
>   - [[MSE]] log error
>    
> - [Log Cosh](LogCosh.md)
>   - works like the [[MSE]], but is smoothed towards large errors (presumably caused by outliers) so that the final error score isnâ€™t impacted thoroughly.
>    
> - [Huber/Smooth L1/Smooth MAE](Huber.md)
>   - It is less sensitive to outliers than the [[MSE]] and in some cases prevents exploding #gradients

_Backlinks last generated 2022-06-24 12:00:32_
