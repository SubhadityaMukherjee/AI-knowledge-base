---
title: Interpreting Attention

categories: ['architecture']
date modified: Monday, October 10th 2022, 2:02:24 pm
date created: Tuesday, July 26th 2022, 8:33:15 pm
---

# Interpreting [Attention](Attention.qmd)
- [Attention Interpretability Across NLP Tasks](https://arxiv.org/abs/1909.11218)
- empirically prove the hypothesis that [attention](Attention.qmd) weights are interpretable and are correlated with feature importance measures
- n both single and pair sequence tasks, the [attention](Attention.qmd) weights in samples with original weights do make sense in general
- However, in the former case, the [attention](Attention.qmd) mechanism learns to give higher weights to tokens relevant to both kinds of sentiment.
- They show that [attention](Attention.qmd) weights in single sequence tasks do not provide a reason for the prediction, which in the case of pairwise tasks, [attention](Attention.qmd) do reflect the reasoning behind model output
- [BertViz](https://github.com/jessevig/bertviz) repo



