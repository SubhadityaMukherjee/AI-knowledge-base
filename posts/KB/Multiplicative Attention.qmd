---
toc: true
title: Multiplicative Attention

categories: ['architecture']
date modified: Monday, October 10th 2022, 2:02:21 pm
date created: Tuesday, July 26th 2022, 8:33:15 pm
---

# Multiplicative [Attention](Attention.qmd)
- ![Pasted%20image%2020220621174943](images/Pasted%20image%2020220621174943.png)
- $$f_{att}(h_{i}, s_{j}) = h_{i}^{T}W_{a}s_{j}$$
- Since [Additive Attention](Additive%20Attention.qmd) performs better for scale, use a factor [Scaled Dot Product Attention](Scaled%20Dot%20Product%20Attention.qmd)



