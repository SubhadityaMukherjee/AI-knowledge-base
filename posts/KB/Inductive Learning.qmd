---
toc: true
title: Inductive Learning

categories: ['languagecogneuropsychology']
date modified: Monday, October 10th 2022, 2:02:25 pm
date created: Tuesday, July 26th 2022, 8:33:15 pm
---

# Inductive Learning
- [Bayesian](Bayesian.qmd) is inductive learning
- Learning is identifying which hypothesis set is a concept
- Hypotheses don't disappear, they just become less likely
- Learning develops through more experience
- One challenge of [Bayesian](Bayesian.qmd) learning is that any small subset is consistent with many hypotheses
- Different hypotheses have different likelihoods based on the examples we are exposed to
- But in the end we also prefer smaller hypotheses over larger ones: The size principle
- Simple [clustering](Clustering.qmd) methods can be used to get the data to automatically create the hypothesis space needed for [Bayesian](Bayesian.qmd) modelling
- Probabilities of different sets then match with human judgments surprisingly well
- [Clustering](Clustering.qmd) based on biology worked worse!
- [Clustering](Clustering.qmd) using linguistic co-occurrences with [Latent Semantic Analysis](Latent%20Semantic%20Analysis.qmd) also worked worse!
- Human subject judgements of similarity worked best
- Suggests some human reasoning relies on [probability](Probability.qmd)
- [Bayesian](Bayesian.qmd) learning can also learn categories
- Models are capable of making generalizations about the specific objects as well as the appropriate generalizations about categorization (superordinate categories!) in general.
- Advanced learning means learn constraints on what is a possible hypothesis
- Hierarchical [Bayesian](Bayesian.qmd) Modelling (HBM) can explain how we acquire [overhypotheses](Overhypotheses.qmd)
- using observations from the lowest level (data) and calculating statistical inferences



