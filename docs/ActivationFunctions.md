# Activation Functions
- [[Initialization]]

- [[Sigmoid]]

- [[Relu]]
- [[Elu]]
- [[Maxout]] 

- [[Tanh]] 

- [[Softmax]]

- [[Softplus]]

- [[Swish]]

## Refs
- [mlmastery](https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/)## Backlinks
* [[Backprop]]
	* Solved by [[ActivationFunctions]]
* [[DeepLearning]]
	* [[ActivationFunctions]]

