---
title: Cross Modal Distillation

tags: knowledgedistillation 
date modified: Monday, October 10th 2022, 2:02:09 pm
date created: Tuesday, October 4th 2022, 11:50:02 am
---

# Cross Modal Distillation
- Moreover, Do et al. (2019) proposed a knowledge distillation-based visual question answering method, in which knowledge from trilinear interaction teacher model with image-question-answer as inputs is distilled into the learning of a bilinear interaction student model with image-question as inputs



