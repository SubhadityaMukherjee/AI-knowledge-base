---
tags: temp
title: Knowledge Distillation
date modified: Monday, October 10th 2022, 2:02:24 pm
date created: Tuesday, July 26th 2022, 8:33:15 pm
---

# Knowledge Distillation
- Teacher model to help train the student model
- Teacher is often pre trained
- Student tries to imitate teacher
- [Distillation_Loss](Distillation_Loss.md)
- [Knowledge_Distillation_Survey_2021](Knowledge_Distillation_Survey_2021.md)
- [Distilling_the_Knowledge_in_a_Neural_Network](Distilling_the_Knowledge_in_a_Neural_Network.md)

