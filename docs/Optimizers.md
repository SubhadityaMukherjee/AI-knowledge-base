# Optimization
- [[Gradient Descent gradients]]

- [[Adagrad]]

- [[Rmsprop]]

- [[Adam]]

- [[Learning Rate Decay tricks]]

- [[Early Stopping tricks]]
## Backlinks
<<<<<<< HEAD
* [[Adam]]
	* [[Optimization]]

## ...
=======
* [[Vanishingexploding gradients]]
	* [[Regularization]] , [[Optimizers]] , [[Architectures]]
* [[DeepLearning]]
	* [[Optimizers]]
* [[Bias Variance Dilemma]]
	* Tune on [[Emperical Risk]] instead using [[Optimizers]] 

>>>>>>> 1dd38fd29e2ea89a9d6c64b1ecd9e965740dd3c9
