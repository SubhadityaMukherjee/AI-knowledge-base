---
title: Meta AI Speech from Brain
tags: architecture 
---
```toc
```
## Meta AI Speech from Brain
- help people unable to communicate through speech, typing or gestures
- tries to decode language directly from noninvasive brain recordings
- challenge with this proposed method come from noise and diferences in each person's brain and where the sensors are placed.
- contrastive learning and used to maximally align noninvasive brain recordings and speech sounds
- A self-supervised learning model called wave2vec 2.0. is used to identify the complex representations of speech in the brains of volunteers listening to audiobooks
- The two noninvasive technologies used to measure neuronal activity are electroencephalography and magnetoencephalography.
- Training data comes from four opensource datasets which represent 150 hours of recordings of 169 volunteers listening to audiobooks
- EEG and MEG recordings are inserted into a brain model, which consists of a standard deep convolutional network with residual connections
- These recordings are what comes from individuals' brains
- both a speech model for sound and a brain model for MEG data.
- several components of the algorithm were beneficial to decoding performance
- algorithm improves as EEG and MEG recordings increase
- self-supervised trained AI can decode perveived speech despite noise and variability in that data.

## Backlinks

> - [Chatgptisnotallyouneed](Chat GPT is Not All You Need.md)
>   - [[Meta AI Speech from Brain]]

_Backlinks last generated 2023-05-31 15:18:49_
