---
title: DistillBERT

tags: architecture 
---

# DistillBERT
- [DistilBERT, a Distilled Version of BERT: Smaller, Faster, Cheaper and Lighter](https://arxiv.org/abs/1910.01108)
- Huggingface
- general-purpose pre-trained version of BERT
- 40% smaller, 60% faster, cheaper to pre-train, and retains 97% of the language understanding capabilities
- knowledge distillation during the pre-training phase
- triple loss combining language modeling, distillation and cosine-distance losses


## Backlinks

> - [](journals/2022-06-27.md)
>   - [[DistillBERT]]

_Backlinks last generated 2022-06-27 22:06:24_
