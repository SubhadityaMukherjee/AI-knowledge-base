---
categories: ['temp']
toc: true
title: Knowledge Distillation
date modified: Monday, October 10th 2022, 2:02:24 pm
date created: Tuesday, July 26th 2022, 8:33:15 pm
---

# Knowledge Distillation
- Teacher model to help train the student model
- Teacher is often pre trained
- Student tries to imitate teacher
- [Distillation Loss](Distillation%20Loss.qmd)
- [Knowledge Distillation Survey 2021](Knowledge%20Distillation%20Survey%202021.qmd)
- [Distilling the Knowledge in a Neural Network](Distilling%20the%20Knowledge%20in%20a%20Neural%20Network.qmd)



