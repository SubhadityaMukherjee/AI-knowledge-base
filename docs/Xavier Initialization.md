---
title: Xavier Initialization
tags: regularize 
---

# Xavier [Initialization](Initialization.md)
- $$\mathrm{a=\sqrt{\frac{6}{\left(\mathrm{d}\mathrm{_{\mathrm{in}}^{\mathrm{ }}}+\mathrm{d}_{\mathrm{out}} \right)}}}$$
- Random values drawn uniformly from $[-a,a]$
- For [Batch Normalization](Batch%20Normalization.md) [Layers](Layers.md), $\gamma =1$ and $\beta=0$
- For Tanh based activating neural nets


