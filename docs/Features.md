# Features

## Dimensions

### Wide
- Had to train
- More number of neurons
- Easy parallel
- Infinitely wide -> Gaussian process

### Deep
- Easier to train
- Less data
- Linear amount
- Difficult to parallelize

## Why
- Domain Adaptation
- Structure exploitation
- Relevant features

## Random Things
- 1 hidden layer perceptron -> Universal fn estimator
- Best generalization -> First order optimization
## Backlinks
<<<<<<< HEAD
* [[anchor]]
	* [[Features]]

## ...



## Backlinks
* [[Features]]
	* [[Features]]
* [[anchor]]
	* [[Features]]

## ...
=======
* [[DeepLearning]]
	* [[Features]]

>>>>>>> 1dd38fd29e2ea89a9d6c64b1ecd9e965740dd3c9
