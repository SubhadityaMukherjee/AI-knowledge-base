---
toc: true
title: FLASH

categories: ['architecture']
date modified: Monday, October 10th 2022, 2:02:28 pm
date created: Tuesday, July 26th 2022, 8:33:15 pm
---

# FLASH
- [Transformer Quality in Linear Time](https://arxiv.org/abs/2202.10447)
- weaknesses in handling long sequences
- FLASH
- performant layer (gated linear unit) and by combining it with an accelerator-efficient approximation strategy ([mixed chunk attention](Mixed%20chunk%20attention.qmd))
- [GAU](GAU.qmd)
- [Mixed chunk attention](Mixed%20chunk%20attention.qmd)
- outperforms three baselines: vanilla [Transformer](Transformer.qmd), Performer and Combiner in terms of quality and efficiency
- Wiki
- PG-19



