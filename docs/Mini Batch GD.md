---
title: Mini Batch GD

tags: optimizer gradients 
---

# Mini Batch GD
- $$\theta= \theta-\eta \cdot \nabla_{\theta}J(\theta; x^{i:i+n};y^{i;i+n})$$


## Backlinks

> - [Gradient Descent #gradients](Gradient Descent gradients.md)
>   - [[Mini Batch GD]]

_Backlinks last generated 2022-06-26 12:48:16_
