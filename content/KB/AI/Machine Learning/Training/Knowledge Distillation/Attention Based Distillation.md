---
toc: true
title: Attention Based Distillation

tags: ['knowledgedistillation']
date modified: Monday, October 10th 2022, 2:02:09 pm
date created: Tuesday, October 4th 2022, 11:51:17 am
---

# [[Attention]] Based Distillation
- That is to say, knowledge about feature embedding is transferred using [[Attention]] map functions. Unlike the [[Attention]] maps, a different attentive knowledge distillation method was proposed by Song et al. (2018). An [[Attention]] mechanism is used to assign different confidence rules (Song et al., 2018).



