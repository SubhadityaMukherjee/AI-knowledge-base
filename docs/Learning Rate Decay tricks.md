## Learning Rate Decay #tricks
- Scale of loss landscape changes 
- Reduce step size near optima
- Factor $$\alpha_{i+1} = d\cdot \alpha_i$$
- LR Scheduling



## Backlinks
* [[Optimization]]
	* [[Learning Rate Decay tricks]]

## ...