---
title: Activation Functions
---

# Activation Functions
- [[Initialization]]

- [[Sigmoid]]

- [[Relu]]
- [[Elu]]
- [[Maxout]]

- [[Tanh]]

- [[Softmax]]

- [[Softplus]]

- [[Swish]]

## Refs
- [mlmastery](https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/)
#anchor

## â€¦


































































































## Backlinks

> - [Non [[Relational Inductive Bias]]](Non Relational Inductive Bias.md)
>   - [[ActivationFunctions]]
>    
> - [](DeepLearning.md)
>   - [[ActivationFunctions]]
>    
> - [Zeiler Fergus](Zeiler Fergus.md)
>   - multiple interleaved [[Layers|layers]] of [[Conv]], non-linear [[ActivationFunctions]], local response normalizations, and max [[Pooling]]
>    
> - [Backprop](Backprop.md)
>   - Solved by [[ActivationFunctions]]

_Backlinks last generated 2022-07-26 20:33:15_
