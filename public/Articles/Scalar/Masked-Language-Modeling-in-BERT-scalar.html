<!DOCTYPE html>
<html lang="en"><head><title>Masked Language Modeling in BERT</title><meta charset="utf-8"/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com"/><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=IBM Plex Mono&amp;family=Schibsted Grotesk:wght@400;700&amp;family=Source Sans Pro:ital,wght@0,400;0,600;1,400;1,600&amp;display=swap"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:title" content="Masked Language Modeling in BERT"/><meta property="og:description" content="Masked Language Modeling in BERTscalar masked language model explained :::section{.abstract} Overview Language modelling is a massive domain and has many sub-research areas ..."/><meta property="og:image" content="https://https://subhadityamukherjee.github.io/AI-knowledge-base//static/og-image.png"/><meta property="og:width" content="1200"/><meta property="og:height" content="675"/><link rel="icon" href="../../static/icon.png"/><meta name="description" content="Masked Language Modeling in BERTscalar masked language model explained :::section{.abstract} Overview Language modelling is a massive domain and has many sub-research areas ..."/><meta name="generator" content="Quartz"/><link href="../../index.css" rel="stylesheet" type="text/css" spa-preserve/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css" rel="stylesheet" type="text/css" spa-preserve/><script src="../../prescript.js" type="application/javascript" spa-preserve></script><script type="application/javascript" spa-preserve>const fetchData = fetch("../../static/contentIndex.json").then(data => data.json())</script></head><body data-slug="Articles/Scalar/Masked-Language-Modeling-in-BERT-scalar"><div id="quartz-root" class="page"><div id="quartz-body"><div class="left sidebar"><h2 class="page-title"><a href="../..">Subhaditya's KB</a></h2><div class="spacer mobile-only"></div><div class="search"><button class="search-button" id="search-button"><p>Search</p><svg role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title>Search</title><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"></path><circle cx="8" cy="8" r="7"></circle></g></svg></button><div id="search-container"><div id="search-space"><input autocomplete="off" id="search-bar" name="search" type="text" aria-label="Search for something" placeholder="Search for something"/><div id="search-layout" data-preview="true"></div></div></div></div><button class="darkmode" id="darkmode"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" id="dayIcon" x="0px" y="0px" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35" xml:space="preserve" aria-label="Dark mode"><title>Dark mode</title><path d="M6,17.5C6,16.672,5.328,16,4.5,16h-3C0.672,16,0,16.672,0,17.5    S0.672,19,1.5,19h3C5.328,19,6,18.328,6,17.5z M7.5,26c-0.414,0-0.789,0.168-1.061,0.439l-2,2C4.168,28.711,4,29.086,4,29.5    C4,30.328,4.671,31,5.5,31c0.414,0,0.789-0.168,1.06-0.44l2-2C8.832,28.289,9,27.914,9,27.5C9,26.672,8.329,26,7.5,26z M17.5,6    C18.329,6,19,5.328,19,4.5v-3C19,0.672,18.329,0,17.5,0S16,0.672,16,1.5v3C16,5.328,16.671,6,17.5,6z M27.5,9    c0.414,0,0.789-0.168,1.06-0.439l2-2C30.832,6.289,31,5.914,31,5.5C31,4.672,30.329,4,29.5,4c-0.414,0-0.789,0.168-1.061,0.44    l-2,2C26.168,6.711,26,7.086,26,7.5C26,8.328,26.671,9,27.5,9z M6.439,8.561C6.711,8.832,7.086,9,7.5,9C8.328,9,9,8.328,9,7.5    c0-0.414-0.168-0.789-0.439-1.061l-2-2C6.289,4.168,5.914,4,5.5,4C4.672,4,4,4.672,4,5.5c0,0.414,0.168,0.789,0.439,1.06    L6.439,8.561z M33.5,16h-3c-0.828,0-1.5,0.672-1.5,1.5s0.672,1.5,1.5,1.5h3c0.828,0,1.5-0.672,1.5-1.5S34.328,16,33.5,16z     M28.561,26.439C28.289,26.168,27.914,26,27.5,26c-0.828,0-1.5,0.672-1.5,1.5c0,0.414,0.168,0.789,0.439,1.06l2,2    C28.711,30.832,29.086,31,29.5,31c0.828,0,1.5-0.672,1.5-1.5c0-0.414-0.168-0.789-0.439-1.061L28.561,26.439z M17.5,29    c-0.829,0-1.5,0.672-1.5,1.5v3c0,0.828,0.671,1.5,1.5,1.5s1.5-0.672,1.5-1.5v-3C19,29.672,18.329,29,17.5,29z M17.5,7    C11.71,7,7,11.71,7,17.5S11.71,28,17.5,28S28,23.29,28,17.5S23.29,7,17.5,7z M17.5,25c-4.136,0-7.5-3.364-7.5-7.5    c0-4.136,3.364-7.5,7.5-7.5c4.136,0,7.5,3.364,7.5,7.5C25,21.636,21.636,25,17.5,25z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" id="nightIcon" x="0px" y="0px" viewBox="0 0 100 100" style="enable-background:new 0 0 100 100" xml:space="preserve" aria-label="Light mode"><title>Light mode</title><path d="M96.76,66.458c-0.853-0.852-2.15-1.064-3.23-0.534c-6.063,2.991-12.858,4.571-19.655,4.571  C62.022,70.495,50.88,65.88,42.5,57.5C29.043,44.043,25.658,23.536,34.076,6.47c0.532-1.08,0.318-2.379-0.534-3.23  c-0.851-0.852-2.15-1.064-3.23-0.534c-4.918,2.427-9.375,5.619-13.246,9.491c-9.447,9.447-14.65,22.008-14.65,35.369  c0,13.36,5.203,25.921,14.65,35.368s22.008,14.65,35.368,14.65c13.361,0,25.921-5.203,35.369-14.65  c3.872-3.871,7.064-8.328,9.491-13.246C97.826,68.608,97.611,67.309,96.76,66.458z"></path></svg></button></div><div class="center"><div class="page-header"><div class="popover-hint"><nav class="breadcrumb-container" aria-label="breadcrumbs"><div class="breadcrumb-element"><a href="../../">Home</a><p> ❯ </p></div><div class="breadcrumb-element"><a href="../../Articles/">Articles</a><p> ❯ </p></div><div class="breadcrumb-element"><a href="../../Articles/Scalar/">Scalar Academy Articles</a><p> ❯ </p></div><div class="breadcrumb-element"><a href>Masked Language Modeling in BERT</a></div></nav><h1 class="article-title">Masked Language Modeling in BERT</h1><p show-comma="true" class="content-meta"><span>Sep 18, 2024</span><span>11 min read</span></p><ul class="tags"><li><a href="../../tags/article" class="internal tag-link">article</a></li><li><a href="../../tags/scalar" class="internal tag-link">scalar</a></li></ul></div></div><article class="popover-hint"><h1 id="masked-language-modeling-in-bertscalar"><a href="../../KB/Masked-Language-Modeling" class="internal alias" data-slug="KB/Masked-Language-Modeling">Masked Language Modeling</a> in BERT<a href="../.././../tags/scalar" class="tag-link internal alias" data-slug="tags/scalar">scalar</a><a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#masked-language-modeling-in-bertscalar" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h1>
<p>masked language model explained</p>
<p>:::section{.abstract}</p>
<h2 id="overview">Overview<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#overview" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>Language modelling is a massive domain and has many sub-research areas. One such domain involves understanding contextual information about words in a sentence. This task can be done in many ways, and masked language modelling is one such method. In the past few years, Transformer based models have reached SOTA(state of the art) in many NLP domains. BERT is one such model. In this article, we will understand how to train BERT to fill in missing words in a sentence using MLM.</p>
<p>:::
:::section{.scope}</p>
<h2 id="scope-of-the-article">Scope of the Article<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#scope-of-the-article" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>This article covers the following topics:</p>
<ul>
<li><a href="../../KB/Masked-Language-Modeling" class="internal alias" data-slug="KB/Masked-Language-Modeling">Masked Language Modeling</a> explained in an easy-to-understand manner</li>
<li>Quickly recap all the pre-requisite terms to build an MLM model</li>
<li>Go over some libraries that are essential to MLM</li>
<li>How to build a BERT-based MLM model using our data in Tensorflow/Keras</li>
</ul>
<p>:::
:::section{.main}</p>
<h2 id="masked-language-modeling-explained"><a href="../../KB/Masked-Language-Modeling" class="internal alias" data-slug="KB/Masked-Language-Modeling">Masked Language Modeling</a> Explained<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#masked-language-modeling-explained" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>To a NN model, the word context has no meaning. So, we need to find ways to make the model consider surrounding words to learn which context words appear. For example, consider the sentence, “I am eating an ice cream”. In this, the “ice cream” is being “eaten”. What would an appropriate word be if we now remove the word “eating” and have the sentence as “I am ___ an ice cream”? We can consider something along the lines of “licking”, “eating”, “sharing”, etc. However, we cannot say “drowning”, “cycle”, “chair”, or other random words.</p>
<p>In the same way, to ensure the model learns which word is appropriate, it needs to understand the structure of language. As modellers, we need to help it do so.
Quite simply, all we do is give the model inputs with blanks as a “token” <code>&lt;MASK></code> along with the word that should be there. We can create data in this format by taking any text and running over it. How to do so will be explained later on.</p>
<h2 id="quick-recap">Quick Recap<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#quick-recap" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>This section gives a small recap of all the important concepts we need to understand the code.</p>
<h3 id="tokens">Tokens<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#tokens" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p>Tokens can be considered parts of a sentence that contribute to understanding. A sentence such as “I am Jane Austen” can have the tokens <code>[&quot;I&quot;, &quot;am&quot;, &quot;Jane&quot;, &quot;Austen&quot;]</code>. As a computer does not understand words, we need to convert these to numerical values. To do so, we give each word a unique ID. Something like <code>[100, 101, 102, 103]</code> etc.</p>
<h3 id="attention">Attention<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#attention" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p>One of the most important concepts in Machine learning today, attention mechanisms give models the power to decide where to look in an input. Compared to CNNs, this makes models such as Transformers perform much better as it simultaneously learns which parts of the input would be better along with what the input could be classified.</p>
<h3 id="transformers">Transformers<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#transformers" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p>Transformers are a large family of models that employ different variants of attention mechanisms. They first started in NLP but have also branched out to computer vision due to their immense potential. They generally have an Encoder-Decoder architecture with multiple “heads”. Each head is responsible for its attention.</p>
<h3 id="bert">BERT<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#bert" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p>BERT is part of a sub-family of transformer architectures made for NLP and led to a sea of other new models that reached SOTA. It can be used in many different contexts, especially MLM, next-word prediction, question answering and many more.</p>
<h3 id="transfer-learning">Transfer Learning<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#transfer-learning" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p>A training paradigm that changed the DL world by allowing any researcher with limited resources to use SOTA models for inference and fine-tune them to their needs. A model like BERT requires Terabytes of data before it can be trained to extremely high levels. This level of computing is almost impossible for most people to have apart from companies with massive funding and resources. However, now we can use their trained models and make them work for our tasks.</p>
<h3 id="stopwords">Stopwords<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#stopwords" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p>Words that do not add much value to the model but are repeated enough times for it to become a problem. These are generally removed before modelling.</p>
<h2 id="mlm-vs-clm-vs-word2vec">MLM vs CLM vs Word2Vec<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#mlm-vs-clm-vs-word2vec" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>The major difference between MLM and CLM is that CLM can only take into account words that occur before it in a sentence, unlike MLM, which is bi-directional. This difference means that CLM does better for generating large amounts of text. However, MLM is better for contextually understanding text (refer to the <a href="../../KB/Masked-Language-Modeling" class="internal alias" data-slug="KB/Masked-Language-Modeling">Masked Language Modeling</a> Explained section).
Word2Vec, on the other hand, has similar ideas but the embeddings it generates have weaker contextual information than Transformer based models. The outputs it produces can also be used as a part of BERT training, although it is not usually required.</p>
<h2 id="libraries-we-need">Libraries We Need<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#libraries-we-need" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>We need a few libraries to make this work.</p>
<h3 id="tensorflowkeras">Tensorflow/Keras<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#tensorflowkeras" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p>TF is one of the major DL libraries in Python. We will be using it for training our model.</p>
<h3 id="hugging-face-transformers">Hugging Face Transformers<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#hugging-face-transformers" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p>This library is one of the recent developments in the open-source community. It is a database of trained models and datasets that can be directly used in any codebase. They have thousands of tasks, making it extremely easy to get results fast. We will use their pre-trained BERT model.</p>
<h3 id="nltk">NLTK<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#nltk" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p>A text processing library that we will use to clean up our text before passing it to the model.</p>
<h3 id="seabornmatplotlib">Seaborn/Matplotlib<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#seabornmatplotlib" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p>These libraries are used for plotting our training performance.</p>
<h2 id="implementation">Implementation<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#implementation" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>Now for the code.</p>
<h3 id="imports">Imports<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#imports" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p>We first import all the required packages. We also download the stopwords and punctuation data from nltk.</p>
<p>“`python
import nltk<br/>
nltk.download(‘stopwords’)<br/>
nltk.download(‘punkt’)
nltk.download(‘gutenberg’)
import string
from nltk.corpus import stopwords
import seaborn as sns</p>
<p>from transformers import BertTokenizer, TFBertForMaskedLM
import tensorflow as tf
import numpy as np
import re
import matplotlib.pyplot as plt
sns.set()</p>
<pre><code>
### Data
For this demo, we use text from the book &quot;Emma&quot; by &quot;Jane Austen&quot;. This dataset is a public domain dataset from Project Gutenberg that comes with nltk. We can download it using the following code. 
&quot;`python
import nltk
nltk.download('gutenberg')
!cp /root/nltk_data/corpora/gutenberg/austen-emma.txt sample.txt
</code></pre>
<p>We can also use custom text by creating a text file called “sample.txt” in the same directory as the code and pasting whatever we want. (Make sure it is English text).</p>
<p>We then pre-process the data by removing stopwords and punctuation and converting the words into tokens BERT needs. Since every Transformer model has their configuration of tokens in the pre-trained model, we will use the tokenizer that Hugging face provides us.</p>
<p>(Note: Here, we only take the first 1000 lines from the text. Language models take a long time to train; this is just a demo. If we have GPUs, the entire data can be used.)
“`Python
tokenizer = BertTokenizer.from_pretrained(‘bert-base-uncased’)</p>
<p>sw = stopwords.words(‘english’) # this is a list of stopwords</p>
<h1 id="take-first-1000-sentences-for-demo-purposes">Take first 1000 sentences for demo purposes<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#take-first-1000-sentences-for-demo-purposes" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h1>
<p>with open(‘sample.txt’, ‘r’) as f:
lines = f.readlines()[:1000]</p>
<h1 id="remove-new-lines-convert-all-to-lowercase-remove-punctuation-and-stop-words-and-tokenize">Remove new lines, convert all to lowercase, remove punctuation and stop words and tokenize<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#remove-new-lines-convert-all-to-lowercase-remove-punctuation-and-stop-words-and-tokenize" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h1>
<p>lines = [line.rstrip(‘\n’).lower() for line in lines]
lines = [line.translate(str.maketrans(”, ”, string.punctuation)) for line in lines]</p>
<p>def rem_stops_line(line, words):
# Function to remove stopwords per line if they are present in the line
if len(line) >1:
return [w for w in line if w not in words]
else:
return line</p>
<p>def remove_stops(text, words = sw):
# Remove stop words for an entire text. Separate functions make it easier to parallelize if required.
return [rem_stops_line(line, words) for line in text]</p>
<p>filtered_lines = remove_stops(text = lines, words = sw)
inputs = tokenizer(lines,max_length=100,truncation=True,padding=‘max_length’,return_tensors=‘tf’)</p>
<pre><code>
### Masked Language Model Explained Further
We use the model from the Transformers library directly. The uncased model converts all text into lowercase. Other models do not, and any of them can be used. This one was chosen for the sake of this demo.
&quot;`Python
model = TFBertForMaskedLM.from_pretrained('bert-base-uncased')
</code></pre>
<p>In <a href="../../KB/Masked-Language-Modeling" class="internal alias" data-slug="KB/Masked-Language-Modeling">Masked Language Modeling</a>, we explained that every sentence needs to be converted to a format with words masked using a special token, <code>&lt;MASK></code>. We can do that by using the tokenized words and making the model aware of which token number corresponds to this special token. (In this case, it is 103). In the original paper, token numbers 101 and 102 were replaced, but we ignore that here. (It is not relevant for now.)</p>
<p>“`Python</p>
<h1 id="mask">MASK<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#mask" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h1>
<p>inp_ids = []
lbs = []
for idx, inp in enumerate(inputs.input_ids.numpy()):
current_tokens = list(set(range(100)) -
set(np.where((inp <span class="text-highlight"> 101) | (inp </span> 102)
| (inp == 0))[0].tolist()))
# Find number of tokens to mask
num_token_masked = 0.15 * len(current_tokens)
token_to_mask = np.random.choice(np.array(actual_tokens),
size=int(num_token_masked),
replace=False)
# Store special token and inform model
inp[token_to_mask.tolist()] = 103
inp_ids.append(inp)</p>
<h1 id="convert-the-tokens-to-tensor-format">Convert the tokens to tensor format<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#convert-the-tokens-to-tensor-format" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h1>
<p>inp_ids = tf.convert_to_tensor(inp_ids)
inputs[‘input_ids’] = inp_ids</p>
<pre><code>
### Training the Model
Now that we have all the required data and the model, we need to train it on our data.
If the system does not have a GPU or access to a cloud GPU is unavailable, this model will take a very long time to train. Consider using lesser data.

Considering we have a GPU, we first check if TF can find it.
&quot;`Python
print('GPU name: ', tf.config.experimental.list_physical_devices('GPU'))
</code></pre>
<p>We use a Sparse Categorical Crossentropy loss with logits enabled. (logits are enabled if the model does not end with a Softmax. BERT does not.). We use a learning rate of 1e-3 for the Adam Optimizer.
Finally, we run training for around ten epochs.
“`Python</p>
<h1 id="compile-and-train">Compile and Train<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#compile-and-train" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h1>
<p>lr = 1e-3
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),loss=loss)
history = model.fit([inputs.input_ids,inputs.attention_mask],inputs.labels,verbose=1,batch_size=16,epochs=10)</p>
<pre><code>
### Loss
We plot the loss per epoch to see if our model is learning anything.
We can see that the loss has decreased, which is a good sign! Our model is learning. More data and longer training will help the model be better than before.

### Prediction
Just training a model is useless. We need to be able to use it for prediction. To do that, we need to define a few functions. We need to be able to find the `&lt;MASK>` tokens in the sentence, we need to tokenize the sentence and pass it into the model for prediction. And finally, we need to do this for multiple sentences. 
The following functions do exactly these. 

```python
def find_masks(inp):
    # Find position of the masks in a sentence
    return np.where(inp.input_ids.numpy()[0] == 103)[0].tolist()

def single_prediction(model, inp, mask_loc):
    # Prediction for all the positions of the masks
    return model(inp).logits[0].numpy()[mask_loc]

def return_prediction(model, query):
    # Return a prediction for a single sentence
    inp = tokenizer(query,return_tensors='tf')
    mask_loc = find_masks(inp)
    # Find prediction with the highest confidence
    predicted_tokens = np.argmax(single_prediction(model, inp, mask_loc),axis=1).tolist()
    # Decode the numerical value of the returned ID back to the word 
    return tokenizer.decode(predicted_tokens)

def multiple_preds(model, query_list):
    # Return predictions for a list of queries
    preds = [f&quot;{x} -> {return_prediction(model, x).split(' ')}&quot; for x in query_list]
    for i in preds: print(i)
</code></pre>
<h2 id="predictions">Predictions<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#predictions" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>After training, we can finally give the model a practice exam! Since we fine-tuned it on the book “Emma”, we give it the following sentences. <code>[&quot;too well [MASK] for her&quot;, &quot;nice to [MASK] her&quot;, &quot;Emma [MASK] a girl who wanted a [MASK]&quot;]</code></p>
<p>“`Python
query_list = [“too well [MASK] for her”, “nice to [MASK] her”, “Emma [MASK] a girl who wanted a [MASK]”]
multiple_preds(model, query_list)</p>
<pre><code>
As an output we get the following. We see that it performs quite well even with such a short training! 

&quot;`plaintext
too well [MASK] for her -> ['suited']
nice to [MASK] her -> ['see']
Emma [MASK] a girl who wanted a [MASK] -> ['was', 'chance']
</code></pre>
<h2 id="further-tips">Further Tips<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#further-tips" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<ul>
<li>Language models are generally very heavy to use. If possible, using Mixed Precision training helps</li>
<li>Having more GPU memory is more useful than having a faster GPU for language models</li>
<li>Multiple GPUs are useful for expanding available memory</li>
<li>There are smaller variants of BERT that use less memory.</li>
<li>Hugging Face has a huge list of models that we can use. Trying them might lead to better results.</li>
<li>To get over the overwhelming number of pre-trained models, pick the task and find benchmarks in that task. <a href="https://paperswithcode.com/methods" class="external">PaperswithCode<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> is a great place to start.</li>
</ul>
<p>:::
:::section{.summary}</p>
<h2 id="conclusion">Conclusion<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#conclusion" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>This article showed us masked language modelling explained. We learnt the following:</p>
<ul>
<li>What MLM is and when to use it.</li>
<li>How to pre-process our data for MLM</li>
<li>How to fine-tune pre-trained BERT models for MLM</li>
<li>How to perform predictions over multiple sentences with our fine-tuned models.</li>
</ul>
<p>:::</p></article><hr/><div class="page-footer"></div></div><div class="right sidebar"><div class="graph"><h3>Graph View</h3><div class="graph-outer"><div id="graph-container" data-cfg="{&quot;drag&quot;:true,&quot;zoom&quot;:true,&quot;depth&quot;:1,&quot;scale&quot;:1.1,&quot;repelForce&quot;:0.5,&quot;centerForce&quot;:0.3,&quot;linkDistance&quot;:30,&quot;fontSize&quot;:0.6,&quot;opacityScale&quot;:1,&quot;showTags&quot;:true,&quot;removeTags&quot;:[],&quot;focusOnHover&quot;:false}"></div><button id="global-graph-icon" aria-label="Global Graph"><svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 55 55" fill="currentColor" xml:space="preserve"><path d="M49,0c-3.309,0-6,2.691-6,6c0,1.035,0.263,2.009,0.726,2.86l-9.829,9.829C32.542,17.634,30.846,17,29,17
                s-3.542,0.634-4.898,1.688l-7.669-7.669C16.785,10.424,17,9.74,17,9c0-2.206-1.794-4-4-4S9,6.794,9,9s1.794,4,4,4
                c0.74,0,1.424-0.215,2.019-0.567l7.669,7.669C21.634,21.458,21,23.154,21,25s0.634,3.542,1.688,4.897L10.024,42.562
                C8.958,41.595,7.549,41,6,41c-3.309,0-6,2.691-6,6s2.691,6,6,6s6-2.691,6-6c0-1.035-0.263-2.009-0.726-2.86l12.829-12.829
                c1.106,0.86,2.44,1.436,3.898,1.619v10.16c-2.833,0.478-5,2.942-5,5.91c0,3.309,2.691,6,6,6s6-2.691,6-6c0-2.967-2.167-5.431-5-5.91
                v-10.16c1.458-0.183,2.792-0.759,3.898-1.619l7.669,7.669C41.215,39.576,41,40.26,41,41c0,2.206,1.794,4,4,4s4-1.794,4-4
                s-1.794-4-4-4c-0.74,0-1.424,0.215-2.019,0.567l-7.669-7.669C36.366,28.542,37,26.846,37,25s-0.634-3.542-1.688-4.897l9.665-9.665
                C46.042,11.405,47.451,12,49,12c3.309,0,6-2.691,6-6S52.309,0,49,0z M11,9c0-1.103,0.897-2,2-2s2,0.897,2,2s-0.897,2-2,2
                S11,10.103,11,9z M6,51c-2.206,0-4-1.794-4-4s1.794-4,4-4s4,1.794,4,4S8.206,51,6,51z M33,49c0,2.206-1.794,4-4,4s-4-1.794-4-4
                s1.794-4,4-4S33,46.794,33,49z M29,31c-3.309,0-6-2.691-6-6s2.691-6,6-6s6,2.691,6,6S32.309,31,29,31z M47,41c0,1.103-0.897,2-2,2
                s-2-0.897-2-2s0.897-2,2-2S47,39.897,47,41z M49,10c-2.206,0-4-1.794-4-4s1.794-4,4-4s4,1.794,4,4S51.206,10,49,10z"></path></svg></button></div><div id="global-graph-outer"><div id="global-graph-container" data-cfg="{&quot;drag&quot;:true,&quot;zoom&quot;:true,&quot;depth&quot;:-1,&quot;scale&quot;:0.9,&quot;repelForce&quot;:0.5,&quot;centerForce&quot;:0.3,&quot;linkDistance&quot;:30,&quot;fontSize&quot;:0.6,&quot;opacityScale&quot;:1,&quot;showTags&quot;:true,&quot;removeTags&quot;:[],&quot;focusOnHover&quot;:true}"></div></div></div><div class="toc desktop-only"><button type="button" id="toc" class aria-controls="toc-content" aria-expanded="true"><h3>Table of Contents</h3><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="fold"><polyline points="6 9 12 15 18 9"></polyline></svg></button><div id="toc-content" class><ul class="overflow"><li class="depth-0"><a href="#masked-language-modeling-in-bertscalar" data-for="masked-language-modeling-in-bertscalar">Masked Language Modeling in BERTscalar</a></li><li class="depth-1"><a href="#overview" data-for="overview">Overview</a></li><li class="depth-1"><a href="#scope-of-the-article" data-for="scope-of-the-article">Scope of the Article</a></li><li class="depth-1"><a href="#masked-language-modeling-explained" data-for="masked-language-modeling-explained">Masked Language Modeling Explained</a></li><li class="depth-1"><a href="#quick-recap" data-for="quick-recap">Quick Recap</a></li><li class="depth-2"><a href="#tokens" data-for="tokens">Tokens</a></li><li class="depth-2"><a href="#attention" data-for="attention">Attention</a></li><li class="depth-2"><a href="#transformers" data-for="transformers">Transformers</a></li><li class="depth-2"><a href="#bert" data-for="bert">BERT</a></li><li class="depth-2"><a href="#transfer-learning" data-for="transfer-learning">Transfer Learning</a></li><li class="depth-2"><a href="#stopwords" data-for="stopwords">Stopwords</a></li><li class="depth-1"><a href="#mlm-vs-clm-vs-word2vec" data-for="mlm-vs-clm-vs-word2vec">MLM vs CLM vs Word2Vec</a></li><li class="depth-1"><a href="#libraries-we-need" data-for="libraries-we-need">Libraries We Need</a></li><li class="depth-2"><a href="#tensorflowkeras" data-for="tensorflowkeras">Tensorflow/Keras</a></li><li class="depth-2"><a href="#hugging-face-transformers" data-for="hugging-face-transformers">Hugging Face Transformers</a></li><li class="depth-2"><a href="#nltk" data-for="nltk">NLTK</a></li><li class="depth-2"><a href="#seabornmatplotlib" data-for="seabornmatplotlib">Seaborn/Matplotlib</a></li><li class="depth-1"><a href="#implementation" data-for="implementation">Implementation</a></li><li class="depth-2"><a href="#imports" data-for="imports">Imports</a></li><li class="depth-0"><a href="#take-first-1000-sentences-for-demo-purposes" data-for="take-first-1000-sentences-for-demo-purposes">Take first 1000 sentences for demo purposes</a></li><li class="depth-0"><a href="#remove-new-lines-convert-all-to-lowercase-remove-punctuation-and-stop-words-and-tokenize" data-for="remove-new-lines-convert-all-to-lowercase-remove-punctuation-and-stop-words-and-tokenize">Remove new lines, convert all to lowercase, remove punctuation and stop words and tokenize</a></li><li class="depth-0"><a href="#mask" data-for="mask">MASK</a></li><li class="depth-0"><a href="#convert-the-tokens-to-tensor-format" data-for="convert-the-tokens-to-tensor-format">Convert the tokens to tensor format</a></li><li class="depth-0"><a href="#compile-and-train" data-for="compile-and-train">Compile and Train</a></li><li class="depth-1"><a href="#predictions" data-for="predictions">Predictions</a></li><li class="depth-1"><a href="#further-tips" data-for="further-tips">Further Tips</a></li><li class="depth-1"><a href="#conclusion" data-for="conclusion">Conclusion</a></li></ul></div></div><div class="backlinks"><h3>Backlinks</h3><ul class="overflow"><li>No backlinks found</li></ul></div></div></div><footer class><p>Created with <a href="https://quartz.jzhao.xyz/">Quartz v4.3.1</a> © 2025</p><ul><li><a href="https://github.com/SubhadityaMukherjee/AI-knowledge-base/">GitHub</a></li></ul></footer></div></body><script type="application/javascript">function c(){let t=this.parentElement;t.classList.toggle("is-collapsed");let l=t.classList.contains("is-collapsed")?this.scrollHeight:t.scrollHeight;t.style.maxHeight=l+"px";let o=t,e=t.parentElement;for(;e;){if(!e.classList.contains("callout"))return;let n=e.classList.contains("is-collapsed")?e.scrollHeight:e.scrollHeight+o.scrollHeight;e.style.maxHeight=n+"px",o=e,e=e.parentElement}}function i(){let t=document.getElementsByClassName("callout is-collapsible");for(let s of t){let l=s.firstElementChild;if(l){l.addEventListener("click",c),window.addCleanup(()=>l.removeEventListener("click",c));let e=s.classList.contains("is-collapsed")?l.scrollHeight:s.scrollHeight;s.style.maxHeight=e+"px"}}}document.addEventListener("nav",i);window.addEventListener("resize",i);
</script><script type="module">
          let mermaidImport = undefined
          document.addEventListener('nav', async () => {
            if (document.querySelector("code.mermaid")) {
              mermaidImport ||= await import('https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.7.0/mermaid.esm.min.mjs')
              const mermaid = mermaidImport.default
              const darkMode = document.documentElement.getAttribute('saved-theme') === 'dark'
              mermaid.initialize({
                startOnLoad: false,
                securityLevel: 'loose',
                theme: darkMode ? 'dark' : 'default'
              })

              await mermaid.run({
                querySelector: '.mermaid'
              })
            }
          });
          </script><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/contrib/copy-tex.min.js" type="application/javascript"></script><script src="../../postscript.js" type="module"></script></html>