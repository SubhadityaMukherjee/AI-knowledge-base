---
title: Additive Attention

tags: architecture 
---

# Additive [[Attention]]
- [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf)
- Uses a one layer feedforward network to calculate [[Attention Alignment]]
- Oh, basically it is the same as [[Bahdanau Attention]]





















































## Backlinks

> - [Bahdanau [[Attention]]](Bahdanau Attention.md)
>   - Same as [[Additive Attention]]
>    
> - [Multiplicative [[Attention]]](Multiplicative Attention.md)
>   - Since [[Additive Attention]] performs better for scale, use a factor [[Scaled Dot Product Attention]]
>    
> - [Attention](Attention.md)
>   - [[Additive Attention]]

_Backlinks last generated 2022-07-26 20:33:15_
