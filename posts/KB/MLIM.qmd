---
toc: true
title: MLIM

categories: ['architecture']
date modified: Monday, October 10th 2022, 2:02:23 pm
date created: Tuesday, July 26th 2022, 8:33:15 pm
---

# MLIM
- [MLIM: Vision-and-language Model Pre-training with Masked Language and Image Modeling](https://arxiv.org/abs/2109.12178)
- Vision-and-Language Pre-training (VLP) improves model performance for downstream tasks that require image and text inputs
- Typically, in addition to the [Masked Language Modeling] (MLM) loss, alignment-based objectives are used for cross-[modality](Masked Language Modeling] (MLM) loss, alignment-based objectives are used for cross-[modality.qmd) interaction, and RoI feature regression and classification tasks for Masked ImageRegion Modeling (MIRM)
- Alignment-based objectives require pairings of image and text and heuristic objective functions
- Masking policies either do not take advantage of multi-[modality](modality.qmd) or are strictly coupled with alignments generated by other models
- pre-trained using two pre-training tasks as a multi-loss objective given a mini-batch of image-text pairs: [Masked Language Modeling] (MLM) loss (as in BERT) for text, and image reconstruction (RECON) loss for image, coupled with [Modality](Masked Language Modeling] (MLM) loss (as in BERT) for text, and image reconstruction (RECON) loss for image, coupled with [Modality.qmd) Aware Masking (MAM)
- determines the masking probability and applies masking to both word and image [embedding](Embedding.qmd)
- based on BERT predict the masked words from available words and image regions
- follow BERT for this task: two-layer MLP MLM head outputting [logits](Logits.qmd) over the vocabulary
- MLM loss is negative log-likelihood for masked word
- RECON loss is an an average of pixel-wise sum of squared errors (SSE)
- Both image and word masking is realized by replacing an [embedding](Embedding.qmd) with the [embedding](Embedding.qmd) of `[MASK]`
- transformer [layers](Layers.qmd) recognize `[MASK]`
- ’s [embedding](Embedding.qmd) as a special [embedding](Embedding.qmd) that needs to be “filled in”, independent of the [modality](modality.qmd), by attending to other vectors in the layer inputs
- unlike other architectures (LXMERT, UNiTER, ViLBERT, VLP, VL-BERT, VisualBERT, etc.), image masking is not based on image regions detected by the object detector, but a shallow CNN as an image embedder which is much more lightweight than deep models like ResNet and is designed to be masking friendly
- [MLM](MLM.qmd) + RECON losses apply only to the masked text/image areas and measure reconstructed text and image quality.
- no specific alignment loss
- [Modality] Aware Masking (MAM) to boost cross-[modality](Modality] Aware Masking (MAM) to boost cross-[modality.qmd) interaction and take advantage of MLM and RECON losses that separately capture text and image reconstruction quality
- Since the the task of finding closely-matching (CM) item pairs requires a pair of image+text inputs, they exploit this multi-[modality](modality.qmd) by employing [Modality Dropout](Modality%20Dropout.qmd)
- text-only, image-only, and image-text mode
- However, RECON instead of [ITM loss](ITM%20Loss.qmd) offers better PR AUC
- Similarly, using the [ITM loss](ITM%20Loss.qmd) together with MLM and RECON does not change the performance



