---
title: BERT
tags: architecture
---

# BERT
- Bidirectional Encoder rep from transformers
- Uses [Token Embedding](Token%20Embedding.md)
- [[Self supervised]]
- Masked language modeling, next sentence prediction
- ![[../assets/Pasted image 20220307183916.png]]
- [CLS] : start of classification task, [SEP] between sentences, [MASK] : masked token


