---
title: Dilated Sliding Window Attention

tags: architecture 
---

# Dilated Sliding Window Attention
- Analgous to dilated CNN
- Assuming a fixed $d$ and $w$ for all layers, receptive field is $l \times d \times w$ which can reach tens of thousands of tokens even with small values of $d$
- ![](assets/Pasted%20image%2020220621181124.png)


## Backlinks

> - [Longformer](Longformer.md)
>   - [[Dilated Sliding Window Attention]]
>    
> - [Attention](Attention.md)
>   - [[Dilated Sliding Window Attention]]

_Backlinks last generated 2022-06-21 18:32:55_
