---
title: Entropy
---

# Entropy
- Measure of information content
- $$H = -\Sigma_{x}P(x)logP(x) = \Sigma_{x}P(x)log \frac{1}{P(x)}$$
- Units : bits of $log_{2}$
- [[Uniform Distribution]] maximizes entropy. Results harder to predict
- [[Normal Distribution]] maximizes entropy for a fixed mean and variance.## Backlinks
* [[Uncertainty]]
	* [[Entropy]]

## Backlinks
* [[Entropy]]
	* [[Entropy]]
* [[Cross Entropy]]
	* [[Entropy]]
* [[Uncertainty]]
	* [[Entropy]]

