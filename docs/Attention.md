---
title: Attention
tags: architecture
---

# Attention
- Model can decide where to look in the input

- [[Soft Attention]]

- [[Scaled Dot Product Attention]]

- [[Encoder Decoder Attention]]

- [[Self Attention]]

- [[Multi Head Attention]]
































