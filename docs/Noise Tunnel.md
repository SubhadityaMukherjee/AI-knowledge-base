---
title: Noise Tunnel

tags: explainability explainability 
date modified: Tuesday, December 6th 2022, 12:43:54 pm
date created: Tuesday, December 6th 2022, 12:43:53 pm
---

# Noise Tunnel
```toc
```
- @kokhlikyanCaptumUnifiedGeneric2020

## Summary
- Combines [[SmoothGrad Square]] + [[Smooth-Grad]] + [[VarGrad]] 
- not an attribution method
## Using [[Smooth-Grad]] 
- technique that improves the accuracy of attribution methods
- problem with ReLU activation function and gradients producing noisy, often irrelevant attributions
- the partial derivative $\frac{\partial F_c}{\partial x_i}$ of the modelsâ€™ score $F_c$â€‹ for a class c with respect to the value of the pixel $x_{i}$ fluctuates
- adding a Gaussian noise $\mathcal {N}(0, 0.01^2)$ and calculating an average of sampled attributions is going to solve the problem
- calculates the attribution $M_{c}$ using any available method by providing that method an input with Gaussian noise
- calculates a mean value from all the samples to reduce the importance of less frequent attributions
- when adding noise to the input image, important attributions are going to be visible most of the time, and noise might change between attributions
- $$\hat M_{c}(x) = \frac{1}{n}\Sigma_{1}^{n}M_{c}(x + \mathcal{N}(0, \sigma^{2}))$$
## Using [[SmoothGrad Square]] 
- changes only the way that the mean value is calculated by using the mean of squared attributions instead of just attributions
- less noisy results
- but often removes less important features, which are still valid features
- $$\hat M_{c}(x) = \frac{1}{n}\Sigma_{1}^{n}\sqrt{M_{c}(x + \mathcal{N}(0, \sigma^{2}))}$$

## Using [[VarGrad]] 
- variance version of the SmoothGrad
- Using SmoothGrad ([Fig. 1c](#figure-1)) seems to detect more edges of the input image (in comparison with pure IG attribution in \[Fig. 1b\]), and that can be interpreted as detecting decision boundary. SmoothGrad-Square ([Fig. 1d](#figure-1)) and VarGrad ([Fig. 1e](#figure-1)) are removing a large amount of noise but usually also some of the important features visible on the attribution from SmoothGrad
- $$\hat M_{c}(x) = \frac{1}{n}\Sigma_{k=1}^{n}\{M_{c}(x + \mathcal{N}(0, \sigma^{2}))\}^{2}- \{\hat M_{c}(x)\}^{2}$$
## Drawbacks 
- Even if the Noise Tunnel method improves the accuracy of the XAI methods it adds a large amount of computational overhead
- Every sample generated by the method requires the rerun of the whole XAI method
- That is a linear increase of computation and to make the method efficient you should use at least 5 generated noise samples

## Images
- ![[images/Pasted image 20230310122837.png]]

## Backlinks

> - [Vision Explainibility](Vision Explainibility.md)
>   - [[Noise Tunnel]]

_Backlinks last generated 2023-01-28 14:37:47_
