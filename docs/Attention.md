---
title: Attention
tags: architecture
---

# Attention
- Model can decide where to look in the input

- [Self Attention](Self%20Attention.md)
- [Additive Attention](Additive%20Attention.md)
- [Dot Product Attention](Dot%20Product%20Attention.md)
- [Location Aware Attention](Location%20Aware%20Attention.md)
- [Relative Multi Head Self Attention](Relative%20Multi%20Head%20Self%20Attention.md)
- [Soft Attention](Soft%20Attention.md)
- [Scaled Dot Product Attention](Scaled%20Dot%20Product%20Attention.md)
- [Encoder Decoder Attention](Encoder%20Decoder%20Attention.md)
- [Multi Head Attention](Multi%20Head%20Attention.md)
- [Strided Attention](Strided%20Attention.md)
- [Fixed Factorization Attention](Fixed%20Factorization%20Attention.md)
- [Sliding Window Attention](Sliding%20Window%20Attention.md)
- [Dilated Sliding Window Attention](Dilated%20Sliding%20Window%20Attention.md)
- [Global and Sliding Window Attention](Global%20and%20Sliding%20Window%20Attention.md)
- [Content Based Attention](Content%20Based%20Attention.md)
- [Location Base Attention](Location%20Base%20Attention.md)
- [Mixed chunk attention](Mixed%20chunk%20attention.md)










