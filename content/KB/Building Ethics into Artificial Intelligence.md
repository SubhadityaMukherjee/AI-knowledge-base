---
toc: true
title: Building Ethics into Artificial Intelligence

tags: ['ethics']
date modified: Wednesday, January 18th 2023, 4:11:17 pm
date created: Wednesday, January 18th 2023, 3:24:42 pm
---

# Building Ethics into Artificial Intelligence


- Han Yu, Zhiqi Shen, Chunyan Miao, Cyril Leung, Victor R. Lesser, Qiang Yang

## Abstract
- taxonomy which divides the field into four areas: 1) exploring ethical dilemmas; 2) individual ethical decision frameworks; 3) collective ethical decision frameworks; and 4) ethics in human-AI interactions

## Types
- [[Consequentialist ethics]]
- [[Utilitarian ethics]]
- [[Deontological ethics]]
- [[Virtue ethics]]
- [[Ethical dilemmas]]

## Exploring Ethical Dilemmas
- explore the ethical dilemmas in the target application scenarios [Anderson and Anderson, 2014]
- [[GenEth]]
- [[Moral Machine project]]

## Individual Ethical Decision Frameworks
- AI research community largely agrees that generalized frameworks are preferred over ad-hoc rules
- if updates are provided by people, some review mechanisms should be put in place to prevent abuse
- moral decision-making by humans not only involves utilitarian considerations, but also moral rules.
- Such rules often involve protected values (a.k.a. [[sacred values]])
- [[MoralDM]]
- [[Belief-Desire-Intention]]
- [[blind ethical judgement]]
- [[partially informed ethical judgement]]
- [[fully informed ethical judgement]]
- [[Moral decision making frameworks for artificial intelligence]]
- [[Preferences and ethical principles in decision making]]
- [[A declarative modular framework for representing and applying ethical principles..md|A declarative modular framework for representing and applying ethical principles.]]
- [[A low-cost ethics shaping approach for designing reinforcement learning agents]]
- [[Even angels need the rules AI, roboethics, and the law]]
- [[Norms as a basis for governing sociotechnical systems]]
- [[Embedding ethical principles in collective decision support systems]]
- [[A voting-based system for ethical decision making]]
- [[swap-dominance]]
- satisfying consequentialist ethics Ethics in Human-AI Interactions Belmont Report
- [Luckin, 2017; Yu et al., 2017b]
- 1) people's personal autonomy should not be violated (they should be able to maintain their free will when interacting with the technology); 2) benefits brought
- about by the technology should outweigh risks; and 3) the benefits
- and risks should be distributed fairly among the users (people should not be discriminated based on their personal backgrounds such as race, gender and religion)
- persuasion agents
- [Kang et al., 2015; Rosenfeld and Kraus, 2016]
- [Stock et al., 2016]
- large-scale study to investigate human perceptions on the ethics of persuasion by an AI agent
- [[trolley scenario]]
- authors tested three persuasive strategies: 1) appealing to the participants emotionally; 2) presenting the participants with utilitarian arguments; and 3) lying
- participants hold a strong preconceived negative attitude towards the persuasion agent, and argumentation-based and lying-based persuasion strategies work better than emotional persuasion strategies
- did not show significant variation across genders or cultures
- adoption of persuasion strategies should take into account differences in individual personality, ethical attitude and expertise in the given domain.
- [[Coping Theory]]
- Argumentation-based explainable AI
- [Fan and Toni, 2015; Langley et al., 2017] well suited to the consequentialist ethics
- depending on how the explanations are used, researchers need to strike a balance on the level of details to be included
- Full transparency may be too overwhelming if the objective is to persuade a user to follow a time-critical recommendation
- useful as a mechanism to trace the AI decision process afterwards not enough transparency may hamper users' trust in the AI



