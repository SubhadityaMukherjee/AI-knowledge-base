---
toc: true
title: Attention Based Distillation

tags: ['knowledgedistillation']
date modified: Monday, October 10th 2022, 2:02:09 pm
date created: Tuesday, October 4th 2022, 11:51:17 am
---

# [Attention](Attention.md) Based Distillation
- That is to say, knowledge about feature embedding is transferred using [Attention](Attention.md) map functions. Unlike the [Attention](Attention.md) maps, a different attentive knowledge distillation method was proposed by Song et al. (2018). An [Attention](Attention.md) mechanism is used to assign different confidence rules (Song et al., 2018).



