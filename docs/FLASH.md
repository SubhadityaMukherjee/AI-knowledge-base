---
title: FLASH

tags: architecture 
date modified: Monday, October 10th 2022, 2:02:28 pm
date created: Tuesday, July 26th 2022, 8:33:15 pm
---

# FLASH
- [Transformer Quality in Linear Time](https://arxiv.org/abs/2202.10447)
- weaknesses in handling long sequences
- FLASH
- performant layer (gated linear unit) and by combining it with an accelerator-efficient approximation strategy ([[Mixed chunk attention|mixed chunk attention]])
- [[GAU]]
- [[Mixed chunk attention]]
- outperforms three baselines: vanilla [[Transformer]], Performer and Combiner in terms of quality and efficiency
- Wiki
- PG-19

