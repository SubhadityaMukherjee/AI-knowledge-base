---
title: DistillBERT

tags: architecture 
date modified: Thursday, August 11th 2022, 12:32:54 am
date created: Tuesday, July 26th 2022, 8:33:15 pm
---

# DistillBERT
- [DistilBERT, a Distilled Version of BERT: Smaller, Faster, Cheaper and Lighter](https://arxiv.org/abs/1910.01108)
- Huggingface
- general-purpose pre-trained version of BERT
- 40% smaller, 60% faster, cheaper to pre-train, and retains 97% of the language understanding capabilities
- [[Knowledge Distillation|knowledge distillation]] during the pre-training phase
- triple loss combining language modeling, distillation and cosine-distance losses

