<!DOCTYPE html>
<html lang="en"><head><title>SLAK</title><meta charset="utf-8"/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com"/><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=IBM Plex Mono&amp;family=Schibsted Grotesk:wght@400;700&amp;family=Source Sans Pro:ital,wght@0,400;0,600;1,400;1,600&amp;display=swap"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:title" content="SLAK"/><meta property="og:description" content="SLAK MORE CONVNETS IN THE 2020S: SCALING UP KERNELS BEYOND 51 × 51 USING Sparsity Shiwei Liu, Tianlong Chen, Xiaohan Chen, Xuxi Chen, Qiao Xiao, Boqian Wu, Tommi Ka ̈ rkka ̈ inen, Mykola ..."/><meta property="og:image" content="https://https://subhadityamukherjee.github.io/AI-knowledge-base//static/og-image.png"/><meta property="og:width" content="1200"/><meta property="og:height" content="675"/><link rel="icon" href="../static/icon.png"/><meta name="description" content="SLAK MORE CONVNETS IN THE 2020S: SCALING UP KERNELS BEYOND 51 × 51 USING Sparsity Shiwei Liu, Tianlong Chen, Xiaohan Chen, Xuxi Chen, Qiao Xiao, Boqian Wu, Tommi Ka ̈ rkka ̈ inen, Mykola ..."/><meta name="generator" content="Quartz"/><link href="../index.css" rel="stylesheet" type="text/css" spa-preserve/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css" rel="stylesheet" type="text/css" spa-preserve/><script src="../prescript.js" type="application/javascript" spa-preserve></script><script type="application/javascript" spa-preserve>const fetchData = fetch("../static/contentIndex.json").then(data => data.json())</script></head><body data-slug="KB/SLAK"><div id="quartz-root" class="page"><div id="quartz-body"><div class="left sidebar"><h2 class="page-title"><a href="..">Subhaditya's KB</a></h2><div class="spacer mobile-only"></div><div class="search"><button class="search-button" id="search-button"><p>Search</p><svg role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title>Search</title><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"></path><circle cx="8" cy="8" r="7"></circle></g></svg></button><div id="search-container"><div id="search-space"><input autocomplete="off" id="search-bar" name="search" type="text" aria-label="Search for something" placeholder="Search for something"/><div id="search-layout" data-preview="true"></div></div></div></div><button class="darkmode" id="darkmode"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" id="dayIcon" x="0px" y="0px" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35" xml:space="preserve" aria-label="Dark mode"><title>Dark mode</title><path d="M6,17.5C6,16.672,5.328,16,4.5,16h-3C0.672,16,0,16.672,0,17.5    S0.672,19,1.5,19h3C5.328,19,6,18.328,6,17.5z M7.5,26c-0.414,0-0.789,0.168-1.061,0.439l-2,2C4.168,28.711,4,29.086,4,29.5    C4,30.328,4.671,31,5.5,31c0.414,0,0.789-0.168,1.06-0.44l2-2C8.832,28.289,9,27.914,9,27.5C9,26.672,8.329,26,7.5,26z M17.5,6    C18.329,6,19,5.328,19,4.5v-3C19,0.672,18.329,0,17.5,0S16,0.672,16,1.5v3C16,5.328,16.671,6,17.5,6z M27.5,9    c0.414,0,0.789-0.168,1.06-0.439l2-2C30.832,6.289,31,5.914,31,5.5C31,4.672,30.329,4,29.5,4c-0.414,0-0.789,0.168-1.061,0.44    l-2,2C26.168,6.711,26,7.086,26,7.5C26,8.328,26.671,9,27.5,9z M6.439,8.561C6.711,8.832,7.086,9,7.5,9C8.328,9,9,8.328,9,7.5    c0-0.414-0.168-0.789-0.439-1.061l-2-2C6.289,4.168,5.914,4,5.5,4C4.672,4,4,4.672,4,5.5c0,0.414,0.168,0.789,0.439,1.06    L6.439,8.561z M33.5,16h-3c-0.828,0-1.5,0.672-1.5,1.5s0.672,1.5,1.5,1.5h3c0.828,0,1.5-0.672,1.5-1.5S34.328,16,33.5,16z     M28.561,26.439C28.289,26.168,27.914,26,27.5,26c-0.828,0-1.5,0.672-1.5,1.5c0,0.414,0.168,0.789,0.439,1.06l2,2    C28.711,30.832,29.086,31,29.5,31c0.828,0,1.5-0.672,1.5-1.5c0-0.414-0.168-0.789-0.439-1.061L28.561,26.439z M17.5,29    c-0.829,0-1.5,0.672-1.5,1.5v3c0,0.828,0.671,1.5,1.5,1.5s1.5-0.672,1.5-1.5v-3C19,29.672,18.329,29,17.5,29z M17.5,7    C11.71,7,7,11.71,7,17.5S11.71,28,17.5,28S28,23.29,28,17.5S23.29,7,17.5,7z M17.5,25c-4.136,0-7.5-3.364-7.5-7.5    c0-4.136,3.364-7.5,7.5-7.5c4.136,0,7.5,3.364,7.5,7.5C25,21.636,21.636,25,17.5,25z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" id="nightIcon" x="0px" y="0px" viewBox="0 0 100 100" style="enable-background:new 0 0 100 100" xml:space="preserve" aria-label="Light mode"><title>Light mode</title><path d="M96.76,66.458c-0.853-0.852-2.15-1.064-3.23-0.534c-6.063,2.991-12.858,4.571-19.655,4.571  C62.022,70.495,50.88,65.88,42.5,57.5C29.043,44.043,25.658,23.536,34.076,6.47c0.532-1.08,0.318-2.379-0.534-3.23  c-0.851-0.852-2.15-1.064-3.23-0.534c-4.918,2.427-9.375,5.619-13.246,9.491c-9.447,9.447-14.65,22.008-14.65,35.369  c0,13.36,5.203,25.921,14.65,35.368s22.008,14.65,35.368,14.65c13.361,0,25.921-5.203,35.369-14.65  c3.872-3.871,7.064-8.328,9.491-13.246C97.826,68.608,97.611,67.309,96.76,66.458z"></path></svg></button></div><div class="center"><div class="page-header"><div class="popover-hint"><nav class="breadcrumb-container" aria-label="breadcrumbs"><div class="breadcrumb-element"><a href="../">Home</a><p> ❯ </p></div><div class="breadcrumb-element"><a href="../KB/">Knowledge Base</a><p> ❯ </p></div><div class="breadcrumb-element"><a href>SLAK</a></div></nav><h1 class="article-title">SLAK</h1><p show-comma="true" class="content-meta"><span>Sep 18, 2024</span><span>10 min read</span></p><ul class="tags"><li><a href="../tags/architecture" class="internal tag-link">architecture</a></li></ul></div></div><article class="popover-hint"><h1 id="slak">SLAK<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#slak" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h1>
<ul>
<li>
<p>MORE CONVNETS IN THE 2020S: SCALING UP KERNELS BEYOND 51 × 51 USING <a href="../KB/Sparsity" class="internal alias" data-slug="KB/Sparsity">Sparsity</a></p>
</li>
<li>
<p>Shiwei Liu, Tianlong Chen, Xiaohan Chen, Xuxi Chen, Qiao Xiao, Boqian Wu, Tommi Ka ̈ rkka ̈ inen, Mykola Pechenizkiy, Decebal Constantin Mocanu, Zhangyang Wang</p>
</li>
<li>
<p>The dominant role of convolutional neural networks (CNNs) seems to be challenged by increasingly effective transformer-based models.</p>
</li>
<li>
<p>advanced convolutional models strike back with large ker- nels motivated by the local-window attention mechanism, showing appealing perfor- mance and efficiency</p>
</li>
<li>
<p><a href="../KB/RepLKNet" class="internal alias" data-slug="KB/RepLKNet">RepLKNet</a></p>
</li>
<li>
<p>This study ends up with a recipe for applying extremely large kernels from the perspective of <a href="../KB/Sparsity" class="internal alias" data-slug="KB/Sparsity">Sparsity</a>, which can smoothly scale up kernels to 61×61 with better performance</p>
</li>
<li>
<p>Built on this recipe, we propose Sparse Large Kernel Network (SLaK), a pure CNN architec- ture equipped with sparse factorized 51×51 kernels that can perform on par with or better than state-of-the-art hierarchical Transformers and modern ConvNet architec- tures like ConvNeXt and RepLKNet</p>
</li>
</ul>
<h2 id="related-work">RELATED WORK<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#related-work" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<ul>
<li><a href="../KB/Large-Kernel-in-Attention" class="internal alias" data-slug="KB/Large-Kernel-in-Attention">Large Kernel in Attention</a></li>
<li><a href="../KB/Large-Kernel-in-Convolution" class="internal alias" data-slug="KB/Large-Kernel-in-Convolution">Large Kernel in Convolution</a></li>
<li><a href="../KB/Dynamic-Sparsity" class="internal alias" data-slug="KB/Dynamic-Sparsity">Dynamic Sparsity</a></li>
<li><a href="../KB/Sparse-Evolutionary-Training" class="internal alias" data-slug="KB/Sparse-Evolutionary-Training">Sparse Evolutionary Training</a></li>
</ul>
<h2 id="failures-of-existing-approaches-to-go-beyond-31x31-kernels">FAILURES OF EXISTING APPROACHES TO GO BEYOND 31x31 KERNELS<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#failures-of-existing-approaches-to-go-beyond-31x31-kernels" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<ul>
<li>It is important to note that all models are trained for a reduced length of 120 epochs in this section, just to sketch the scaling trends of large kernel sizes.</li>
<li>Following the design in RepLKNet, we set the kernel size of each stage as [51, 49, 47, 13] and [61, 59, 57, 13],</li>
<li>naively enlarging kernel size from 7x7 to 31x31 decreases the performance although the receptive field may be enlarged by using extremely large kernels, it</li>
<li>might fail to maintain the desirable property of locality.</li>
<li>Since the stem cell in standard ResNet (He et al., 2016) and ConvNeXt results in a</li>
<li>4x <a href="../KB/Downsampling" class="internal alias" data-slug="KB/Downsampling">Downsampling</a> of the input images, extreme kernels with</li>
</ul>
<h2 id="a-recipe-for-extremely-large-kernels-beyond-31x31">A RECIPE FOR EXTREMELY LARGE KERNELS BEYOND 31x31<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#a-recipe-for-extremely-large-kernels-beyond-31x31" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<ul>
<li>Decomposing a large kernel into two rectangular, parallel kernels smoothly scales the kernel size up to 61x61</li>
<li>Although using convolutions with medium sizes (e.g., 31x31) seemingly can directly avoid this problem, we want to investigate if we can further push the performance of CNNs by using (global) extreme convolutions</li>
<li>approximate the large MxM kernel with a combination of two parallel and rectangular convolutions whose kernel size is MxN and NxM (where N &lt; M), respectively, as shown in Figure 1. Following Ding et al. (2022), we keep a 5x5 layer parallel to the large kernels and summed up their outputs after a batch norm layer.</li>
<li>This decomposition balances between capturing long-range dependencies and extracting local detail features</li>
<li>In stark contrast, the overhead of our method increases just linearly with the kernel size</li>
<li>As the decomposition reduces learnable parameters and FLOPs, it is no surprise to observe our network to initially sacrifice accuracy slightly compared to the original RepLKNet at medium kernel sizes i.e. 31x31</li>
<li>However, as the convolution size continues to increase, our method can scale kernel size up to 61x61 with improved performance.</li>
</ul>
<h3 id="use-sparse-groups-expand-more-width">Use Sparse Groups, Expand More Width<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#use-sparse-groups-expand-more-width" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<ul>
<li>significantly boosts the model capacity.</li>
<li>Instead of using the standard group convolution, ConvNeXt simply employs depthwise convolutions with an increased width to achieve the goal of “use more groups, expand width”. In this paper, we attempt to extend this principle from a <a href="../KB/Sparsity" class="internal alias" data-slug="KB/Sparsity">Sparsity</a>-inspired perspective – “use sparse groups, expand more width”.</li>
<li>replace the dense convolutions with sparse convolutions, where the sparse kernels are randomly constructed based on the layer-wise <a href="../KB/Sparsity" class="internal alias" data-slug="KB/Sparsity">Sparsity</a> ratio of SNIP (Lee et al., 2019)</li>
<li>After construction, we train the sparse model with dynamic <a href="../KB/Sparsity" class="internal alias" data-slug="KB/Sparsity">Sparsity</a> (Mocanu et al., 2018; Liu et al., 2021b), where the sparse weights are dynamically adapted during training by pruning the weights with the lowest magnitude and growing the same number of weights randomly.</li>
<li>Doing so enables dynamic adaptation of sparse weights, leading to better local features.</li>
<li>As kernels are sparse throughout training, the corresponding parameter count and training/inference FLOPs are only proportional to the dense models.</li>
<li>dynamic <a href="../KB/Sparsity" class="internal alias" data-slug="KB/Sparsity">Sparsity</a> notably reduces more than 2.0 GFLOPs, despite causing temporary performance degradation.</li>
<li>Dynamic <a href="../KB/Sparsity" class="internal alias" data-slug="KB/Sparsity">Sparsity</a> allows us to computation-friendly scale the model size up</li>
<li>For example, using the same <a href="../KB/Sparsity" class="internal alias" data-slug="KB/Sparsity">Sparsity</a> (40%), we can expand the model width by 1.3x while keeping the parameter count and FLOPs roughly the same as the dense model</li>
</ul>
<h3 id="large-kernels-generalize-better-than-small-kernels-with-our-recipe">Large Kernels Generalize Better Than Small Kernels with Our Recipe<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#large-kernels-generalize-better-than-small-kernels-with-our-recipe" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<ul>
<li>performance consistently increases with kernel size, up to 51x51</li>
<li>Applying each part of our proposed recipe to 7x7 kernels leads to either no gain or marginal gains compared to our 51x51 kernels. This break-down experiment justifies our claim: large kernel is the root of power, and our proposed recipe helps unleash such power from large kernels.</li>
</ul>
<h2 id="slak-1">SLAK<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#slak-1" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<ul>
<li>SLaK is built based on the architecture of ConvNeXt</li>
<li>The design of the stage compute ratio and the stem cell are inherited from ConvNeXt</li>
<li>The number of blocks in each stage is [3, 3, 9, 3] for SLaK-T and [3, 3, 27, 3] for SLaK-S/B</li>
<li>The stem cell is simply a convolution layer with 4x4 kernels and 4 strides. Page 6</li>
<li>We first directly increase the kernel size of ConvNeXt to [51, 49, 47, 13] for each stage, and replace each MxM kernel with a combination of Mx5 and 5xM kernels</li>
<li>We find that adding a BatchNorm layer directly after each decomposed kernel is crucial before summing the output up</li>
<li>urther sparsify the whole network and expand the width of stages by 1.3x, ending up with SLaK</li>
</ul>
<h2 id="evaluation-of-slak-imagenet-1k">EVALUATION OF SLAK ImageNet-1K<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#evaluation-of-slak-imagenet-1k" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<ul>
<li>ADE20K</li>
<li>PASCAL VOC 2007</li>
<li>COCO</li>
<li>SLaK is not only able to capture long-range dependence but also the local context features.</li>
<li>In comparison, high-contribution pixels of SLaK spread in a much larger ERF, and some high-contribution pixels emerge in non-center areas.</li>
<li>SLaK balances between capturing long-range dependencies and focusing on the local details.</li>
<li>SLaK seems to automatically recover the inductive bias of peripheral vision (Lettvin et al., 1976; Min et al., 2022) in the human vision system: the entire visual field is partitioned into multiple regions from near the gaze center to distant areas; humans have high- resolution processing near the gaze center (central and para-central regions), and decrease the resolution of processing for mid and far peripheral regions.</li>
</ul>
<h3 id="kernel-scaling-efficiency">KERNEL SCALING EFFICIENCY<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#kernel-scaling-efficiency" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<ul>
<li>We simply replace all the kernels in stages of ConvNeXt-T with a set of kernel sizes from 7 to 151 and report the required GFLOPs and the number of parameters</li>
<li>One can clearly see the big gap between full-kernel scaling (yellow lines) and kernel decomposition (green lines) as the kernel size increases beyond 31x31.</li>
<li>Even using the ultra-large 151x151 kernels, using our methods would require fewer FLOPs and parameters, compared to full-kernel scaling with 51x51 kernel</li>
<li>EFFECTIVE RECEPTIVE FIELD (ERF)</li>
</ul>
<h2 id="experiment">Experiment<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#experiment" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<h3 id="settings-imagenet-1k">SETTINGS IMAGENET-1K<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#settings-imagenet-1k" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<ul>
<li>We share the (pre-)training settings of SLaK on ImageNet-1K in this section. We train SLaK for 300 epochs (Section 5.1) and 120 epochs (Section 4) using AdamW (Loshchilov &amp; Hutter, 2019) with a batch size of 4096, and a weight decay of 0.05. The only differnce between models training for 300 epochs and 120 epochs is the training time. The learning rate is 4e-3 with a 20-epoch linear warmup followed by a cosine decaying schedule. For data augmentation, we use the default setting of [RandAugment] (Cubuk et al., 2020) in Timm (Wightman, 2019) – “rand-m9-mstd0.5- inc1”, Label Smoothing (Szegedy et al., 2016) coefficient of 0.1, Mixup (Zhang et al., 2017) with ↵ = 0.8, [Cutmix] (Yun et al., 2019) with ↵ = 1.0, [Random Erasing](RandAugment] (Cubuk et al., 2020) in Timm (Wightman, 2019) – “rand-m9-mstd0.5- inc1”, Label Smoothing (Szegedy et al., 2016) coefficient of 0.1, Mixup (Zhang et al., 2017) with ↵ = 0.8, [Cutmix] (Yun et al., 2019) with ↵ = 1.0, [Random Erasing.md) (Zhong et al., 2020) with p = 0.25, Stochastic Depth with drop rate of 0.1 for SLaK-T, 0.4 for SLaK-S, and 0.5 for SLaK-B, Layer Scale (Touvron et al., 2021c) of initial value of 1e- 6, and EMA with a decay factor of 0.9999. We train SLaK-T with NVIDIA A100 GPUs and the rest of models are trained with NVIDIA V100.</li>
</ul>
<h3 id="semantic-segmentation-on-ade20k">SEMANTIC SEGMENTATION ON ADE20K<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#semantic-segmentation-on-ade20k" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<ul>
<li>We follow the training setting used in Ding et al. (2022); Liu et al. (2022b) using UperNet (Xiao et al., 2018) implemented by MMSegmentation (Contributors, 2020) with the 80K/160K-iteration training schedule. We conduct experiments with both short and long training procedures. The backbones are pre-trained on ImageNet-1K with 224x224 input for 120/300 epochs and then are finetuned with UperNet (Xiao et al., 2018) for 80K/160K iterations, respectively. We report the mean Intersection over Union (mIoU) with single-scale. All the hyperparameters are the exactly the same as the ones used in the official GitHub repository of ConvNeXt (con, 2021).</li>
</ul>
<h3 id="object-detection-and-segmentation-on-coco">OBJECT DETECTION AND SEGMENTATION ON COCO<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#object-detection-and-segmentation-on-coco" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<ul>
<li>For COCO experiments, we follow the training settings used in BEiT, Swin, and ConvNeXt using MMDetection (Chen et al., 2019) and MMSegmentation (Contributors, 2020) toolboxes. The final model weights are adopted (instead of EMA weights) from ImageNet-1K pre-training with 224x224 input. We also conduct experiments with both short and long training procedures. The backbones are pre- trained on ImageNet-1K with 224x224 input for 120/300 epochs and then are finetuned with Cascade Mask R-CNN (Cai &amp; Vasconcelos, 2018) for 12/36 epochs, respectively. All the hyperparameters are the exactly the same as the ones used in the official GitHub repository of ConvNeXt (con, 2021).</li>
</ul>
<h3 id="object-detection-on-pascal-voc-2007">OBJECT DETECTION ON PASCAL VOC 2007<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#object-detection-on-pascal-voc-2007" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<ul>
<li>We follow (Liu et al., 2021e) and finetune Faster-RCNN on PASCAL VOC dataset with SLaK-T as the backbone. We use multi-scale setting (Carion et al., 2020; Sun et al., 2021) which leads to the length of the shorter side between 480 and 800 and the ones of the longer side at most 1333. The model is trained with AdamW for 36 epochs with a learning rate of 0.0001, a weight decay of 0.05, and a batch size of 16.</li>
</ul>
<h2 id="some-more-effects">Some More Effects<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#some-more-effects" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<h3 id="trade-off-between-sparsity-and-width">TRADE-OFF BETWEEN <a href="../KB/Sparsity" class="internal alias" data-slug="KB/Sparsity">Sparsity</a> AND WIDTH<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#trade-off-between-sparsity-and-width" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<ul>
<li>As we expected, the model’s performance keeps increasing as model width</li>
<li>increases until the width factor reaches 1.5x, after which increasing width further starts to hurt the performance apparently due to the training difficulties associated with highly sparse neural networks.</li>
</ul>
<h3 id="effect-of-the-shorter-edge-n-on-slak">EFFECT OF THE SHORTER EDGE N ON SLAK<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#effect-of-the-shorter-edge-n-on-slak" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<ul>
<li>We vary the shorter edge N 2 [3, 5, 7] and report the accuracy. All models were trained with AdamW on ImageNet-1K for 120 epochs. We empirically find that N=5 give us the best results, whereas N = 3 and N = 7 has slightly lower accuracy. We hence think it reasonable to choose N = 5 as the default option.</li>
</ul>
<h3 id="erf-quantitation-of-models-with-different-kernel-sizes">ERF QUANTITATION OF MODELS WITH DIFFERENT KERNEL SIZES<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#erf-quantitation-of-models-with-different-kernel-sizes" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<ul>
<li>Larger r suggests a smoother distribution of high-contribution pixels. We can see that with global kernels, SLaK naturally considers a larger range of pixels to make decisions than ConvNeXt and RepLKNet.</li>
</ul>
<h3 id="configurations-of-dynamic-sparsity">CONFIGURATIONS OF DYNAMIC <a href="../KB/Sparsity" class="internal alias" data-slug="KB/Sparsity">Sparsity</a><a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#configurations-of-dynamic-sparsity" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<ul>
<li>Following Liu et al. (2021c), we specifically tune two factors for SLaK-T that control the strength of weight adaptation, adaptation frequency f and adaptation rate p. Adaptation frequency determines after how many training iterations we adjust the sparse weights, and the latter controls the ratio of the weight that we adjust at each adaptation</li>
<li>f = 2000</li>
<li>and p = 0.5 works best for SLak-T. For SLak-S/B, we directly choose f = 100 and p = 0.3 without careful tuning.</li>
</ul>
<h2 id="limitations">LIMITATIONS<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#limitations" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<ul>
<li>sparse architecture is implemented with binary masks due to the limited support of sparse neural networks by the commonly used hardware such as GPU and TPU</li>
<li>Therefore, the inference FLOPs reported in the main paper are the theoretical values.</li>
<li>Once this great potential is supported in the future, it can have a significant positive impact on our planet by saving a huge amount of energy and reducing overall total carbon emissions.</li>
<li>Although not the focus of this current work, it would be interesting for future work to examine the speedup of sparse large kernels, using such specialized hardware accelerators, as we see much improvement room of promise here.</li>
</ul></article><hr/><div class="page-footer"></div></div><div class="right sidebar"><div class="graph"><h3>Graph View</h3><div class="graph-outer"><div id="graph-container" data-cfg="{&quot;drag&quot;:true,&quot;zoom&quot;:true,&quot;depth&quot;:1,&quot;scale&quot;:1.1,&quot;repelForce&quot;:0.5,&quot;centerForce&quot;:0.3,&quot;linkDistance&quot;:30,&quot;fontSize&quot;:0.6,&quot;opacityScale&quot;:1,&quot;showTags&quot;:true,&quot;removeTags&quot;:[],&quot;focusOnHover&quot;:false}"></div><button id="global-graph-icon" aria-label="Global Graph"><svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 55 55" fill="currentColor" xml:space="preserve"><path d="M49,0c-3.309,0-6,2.691-6,6c0,1.035,0.263,2.009,0.726,2.86l-9.829,9.829C32.542,17.634,30.846,17,29,17
                s-3.542,0.634-4.898,1.688l-7.669-7.669C16.785,10.424,17,9.74,17,9c0-2.206-1.794-4-4-4S9,6.794,9,9s1.794,4,4,4
                c0.74,0,1.424-0.215,2.019-0.567l7.669,7.669C21.634,21.458,21,23.154,21,25s0.634,3.542,1.688,4.897L10.024,42.562
                C8.958,41.595,7.549,41,6,41c-3.309,0-6,2.691-6,6s2.691,6,6,6s6-2.691,6-6c0-1.035-0.263-2.009-0.726-2.86l12.829-12.829
                c1.106,0.86,2.44,1.436,3.898,1.619v10.16c-2.833,0.478-5,2.942-5,5.91c0,3.309,2.691,6,6,6s6-2.691,6-6c0-2.967-2.167-5.431-5-5.91
                v-10.16c1.458-0.183,2.792-0.759,3.898-1.619l7.669,7.669C41.215,39.576,41,40.26,41,41c0,2.206,1.794,4,4,4s4-1.794,4-4
                s-1.794-4-4-4c-0.74,0-1.424,0.215-2.019,0.567l-7.669-7.669C36.366,28.542,37,26.846,37,25s-0.634-3.542-1.688-4.897l9.665-9.665
                C46.042,11.405,47.451,12,49,12c3.309,0,6-2.691,6-6S52.309,0,49,0z M11,9c0-1.103,0.897-2,2-2s2,0.897,2,2s-0.897,2-2,2
                S11,10.103,11,9z M6,51c-2.206,0-4-1.794-4-4s1.794-4,4-4s4,1.794,4,4S8.206,51,6,51z M33,49c0,2.206-1.794,4-4,4s-4-1.794-4-4
                s1.794-4,4-4S33,46.794,33,49z M29,31c-3.309,0-6-2.691-6-6s2.691-6,6-6s6,2.691,6,6S32.309,31,29,31z M47,41c0,1.103-0.897,2-2,2
                s-2-0.897-2-2s0.897-2,2-2S47,39.897,47,41z M49,10c-2.206,0-4-1.794-4-4s1.794-4,4-4s4,1.794,4,4S51.206,10,49,10z"></path></svg></button></div><div id="global-graph-outer"><div id="global-graph-container" data-cfg="{&quot;drag&quot;:true,&quot;zoom&quot;:true,&quot;depth&quot;:-1,&quot;scale&quot;:0.9,&quot;repelForce&quot;:0.5,&quot;centerForce&quot;:0.3,&quot;linkDistance&quot;:30,&quot;fontSize&quot;:0.6,&quot;opacityScale&quot;:1,&quot;showTags&quot;:true,&quot;removeTags&quot;:[],&quot;focusOnHover&quot;:true}"></div></div></div><div class="toc desktop-only"><button type="button" id="toc" class aria-controls="toc-content" aria-expanded="true"><h3>Table of Contents</h3><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="fold"><polyline points="6 9 12 15 18 9"></polyline></svg></button><div id="toc-content" class><ul class="overflow"><li class="depth-0"><a href="#slak" data-for="slak">SLAK</a></li><li class="depth-1"><a href="#related-work" data-for="related-work">RELATED WORK</a></li><li class="depth-1"><a href="#failures-of-existing-approaches-to-go-beyond-31x31-kernels" data-for="failures-of-existing-approaches-to-go-beyond-31x31-kernels">FAILURES OF EXISTING APPROACHES TO GO BEYOND 31x31 KERNELS</a></li><li class="depth-1"><a href="#a-recipe-for-extremely-large-kernels-beyond-31x31" data-for="a-recipe-for-extremely-large-kernels-beyond-31x31">A RECIPE FOR EXTREMELY LARGE KERNELS BEYOND 31x31</a></li><li class="depth-2"><a href="#use-sparse-groups-expand-more-width" data-for="use-sparse-groups-expand-more-width">Use Sparse Groups, Expand More Width</a></li><li class="depth-2"><a href="#large-kernels-generalize-better-than-small-kernels-with-our-recipe" data-for="large-kernels-generalize-better-than-small-kernels-with-our-recipe">Large Kernels Generalize Better Than Small Kernels with Our Recipe</a></li><li class="depth-1"><a href="#slak-1" data-for="slak-1">SLAK</a></li><li class="depth-1"><a href="#evaluation-of-slak-imagenet-1k" data-for="evaluation-of-slak-imagenet-1k">EVALUATION OF SLAK ImageNet-1K</a></li><li class="depth-2"><a href="#kernel-scaling-efficiency" data-for="kernel-scaling-efficiency">KERNEL SCALING EFFICIENCY</a></li><li class="depth-1"><a href="#experiment" data-for="experiment">Experiment</a></li><li class="depth-2"><a href="#settings-imagenet-1k" data-for="settings-imagenet-1k">SETTINGS IMAGENET-1K</a></li><li class="depth-2"><a href="#semantic-segmentation-on-ade20k" data-for="semantic-segmentation-on-ade20k">SEMANTIC SEGMENTATION ON ADE20K</a></li><li class="depth-2"><a href="#object-detection-and-segmentation-on-coco" data-for="object-detection-and-segmentation-on-coco">OBJECT DETECTION AND SEGMENTATION ON COCO</a></li><li class="depth-2"><a href="#object-detection-on-pascal-voc-2007" data-for="object-detection-on-pascal-voc-2007">OBJECT DETECTION ON PASCAL VOC 2007</a></li><li class="depth-1"><a href="#some-more-effects" data-for="some-more-effects">Some More Effects</a></li><li class="depth-2"><a href="#trade-off-between-sparsity-and-width" data-for="trade-off-between-sparsity-and-width">TRADE-OFF BETWEEN Sparsity AND WIDTH</a></li><li class="depth-2"><a href="#effect-of-the-shorter-edge-n-on-slak" data-for="effect-of-the-shorter-edge-n-on-slak">EFFECT OF THE SHORTER EDGE N ON SLAK</a></li><li class="depth-2"><a href="#erf-quantitation-of-models-with-different-kernel-sizes" data-for="erf-quantitation-of-models-with-different-kernel-sizes">ERF QUANTITATION OF MODELS WITH DIFFERENT KERNEL SIZES</a></li><li class="depth-2"><a href="#configurations-of-dynamic-sparsity" data-for="configurations-of-dynamic-sparsity">CONFIGURATIONS OF DYNAMIC Sparsity</a></li><li class="depth-1"><a href="#limitations" data-for="limitations">LIMITATIONS</a></li></ul></div></div><div class="backlinks"><h3>Backlinks</h3><ul class="overflow"><li><a href="../KB/architecture" class="internal">architecture</a></li></ul></div></div></div><footer class><p>Created with <a href="https://quartz.jzhao.xyz/">Quartz v4.3.1</a> © 2025</p><ul><li><a href="https://github.com/SubhadityaMukherjee/AI-knowledge-base/">GitHub</a></li></ul></footer></div></body><script type="application/javascript">function c(){let t=this.parentElement;t.classList.toggle("is-collapsed");let l=t.classList.contains("is-collapsed")?this.scrollHeight:t.scrollHeight;t.style.maxHeight=l+"px";let o=t,e=t.parentElement;for(;e;){if(!e.classList.contains("callout"))return;let n=e.classList.contains("is-collapsed")?e.scrollHeight:e.scrollHeight+o.scrollHeight;e.style.maxHeight=n+"px",o=e,e=e.parentElement}}function i(){let t=document.getElementsByClassName("callout is-collapsible");for(let s of t){let l=s.firstElementChild;if(l){l.addEventListener("click",c),window.addCleanup(()=>l.removeEventListener("click",c));let e=s.classList.contains("is-collapsed")?l.scrollHeight:s.scrollHeight;s.style.maxHeight=e+"px"}}}document.addEventListener("nav",i);window.addEventListener("resize",i);
</script><script type="module">
          let mermaidImport = undefined
          document.addEventListener('nav', async () => {
            if (document.querySelector("code.mermaid")) {
              mermaidImport ||= await import('https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.7.0/mermaid.esm.min.mjs')
              const mermaid = mermaidImport.default
              const darkMode = document.documentElement.getAttribute('saved-theme') === 'dark'
              mermaid.initialize({
                startOnLoad: false,
                securityLevel: 'loose',
                theme: darkMode ? 'dark' : 'default'
              })

              await mermaid.run({
                querySelector: '.mermaid'
              })
            }
          });
          </script><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/contrib/copy-tex.min.js" type="application/javascript"></script><script src="../postscript.js" type="module"></script></html>