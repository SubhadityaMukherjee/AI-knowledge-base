---
title: Linear Learning Rate Scaling
---

# Linear Learning Rate Scaling
- If [[Initialization|He [Initialization]] ]] is used, 0.1 is a good learning rate for batch size 256 and for a larger b, $0.1\times\frac{\mathrm{b}}{256}$ is okay


































































































## Backlinks

> - [Learning Rate [[Scheduling]]](Learning Rate Scheduling.md)
>   - [[Linear Learning Rate Scaling]]

_Backlinks last generated 2022-07-26 20:33:15_
