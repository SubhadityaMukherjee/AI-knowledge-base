---
tags: temp
title: Gradient Descent #gradients
date modified: Monday, October 10th 2022, 2:02:26 pm
date created: Tuesday, July 26th 2022, 8:33:15 pm
---

# Gradient Descent #gradients
- [[Backprop]]
- Gradient Direction ~ Steepest Descent $$\theta = tan^{-1}(\frac{\frac{\partial f}{\partial y}}{\frac{\partial f}{\partial x}})$$
- Gradient Magnitude ~ Edge Strength $$||\triangledown f|| = \sqrt{(\frac{\partial f}{\partial x})^{2} + (\frac{\partial f}{\partial y})^{2}}$
- Params $$\theta$$
- Minimize loss function $$\mathscr{L}(\theta) = \Sigma^N_{n=1}l_n(\theta)$$
- [[Simple Gradient Descent]]
- [[SGD]]
- [[Mini Batch GD]]
- [[SGD Momentum]]
- [[Adagrad]]
- [[Nesterov Momentum]]
- [[AdaDelta]]
- [[Rmsprop]]
- [[Adam]]

## â€¦

## Backlinks
> - [Optimization](Optimizers.md)
>   - [[Gradient Descent gradients]]
>
> - [Instant NeRF](Instant NeRF.md)
>   - small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through [[Gradient Descent gradients]]
>
> - [AutoEncoder](Auto Encoders.md)
>   - Learn using [[Gradient Descent gradients]]
>
> - [GAN](Generative Models.md)
>   - G : [[Gradient Descent gradients]]
>
> - [Learning Rate [[Scheduling]]](Learning Rate Scheduling.md)
>   - [[Gradient Descent gradients]]
>
> - [Gradient Ascent](Gradient Ascent.md)
>   - To maximize loss function unlike [[Gradient Descent gradients]]
>
> - [Contrastive [[loss|Loss]]](Contrastive Loss.md)
>   - Minimize distance between similar inputs [[Gradient Descent gradients]], maximize between dissimilar [[Gradient Ascent]]
>
> - [Methods for [[Feature Learning]]](Methods for Feature Learning.md)
>   - [[Gradient Descent gradients]] or [[LinearRegression]]

_Backlinks last generated 2022-10-04 13:01:19_
