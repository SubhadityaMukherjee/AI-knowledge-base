---
title: Attention Based Distillation

tags: knowledgedistillation 
date modified: Monday, October 10th 2022, 2:02:09 pm
date created: Tuesday, October 4th 2022, 11:51:17 am
---

# [[Attention]] Based Distillation
- That is to say, knowledge about feature embedding is transferred using [[Attention|attention]] map functions. Unlike the [[Attention|attention]] maps, a different attentive knowledge distillation method was proposed by Song et al. (2018). An [[Attention|attention]] mechanism is used to assign different confidence rules (Song et al., 2018).

## Backlinks
> - [Distillation Algorithms](Distillation Algorithms.md)
>   - [[Attention Based Distillation]]

_Backlinks last generated 2022-10-04 13:01:19_
