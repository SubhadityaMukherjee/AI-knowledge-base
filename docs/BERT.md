---
title: BERT
tags: architecture
---

# BERT
- Bidirectional Encoder rep from transformers
- Uses [Token Embedding](Token%20Embedding.md)
- [[Self supervised]]
- Masked language modeling, next sentence prediction
- ![[../assets/Pasted image 20220307183916.png]]
- [CLS] : start of classification task, [SEP] between sentences, [MASK] : masked token




## Backlinks

> - [BEiT](BEiT.md)
>   - proposed method is critical to make [[BERT]] like pre-training (i.e., auto-encoding with masked input) work well for image Transformers

_Backlinks last generated 2022-06-21 18:32:55_
