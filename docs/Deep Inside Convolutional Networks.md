---
title: Deep Inside Convolutional Networks

tags: explainability 
date modified: Saturday, January 14th 2023, 4:51:44 pm
date created: Friday, November 18th 2022, 1:28:08 pm
---

# Deep Inside Convolutional Networks
- Karen Simonyan, Andrea Vedaldi, Andrew Zisserman
```toc
```
- The paper "Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps" by Simonyan, Vedaldi, and Zisserman (2014) proposes a method for visualizing and interpreting the internal representations of deep convolutional neural networks.
- A method is proposed for visualizing the internal representations of a deep CNN by backpropagating the output of the network to the input image.
- The method is used to visualize the internal representations of a CNN trained on the ImageNet dataset.
- The internal representations of the CNN are shown to have a hierarchical structure, with early layers learning simple features such as edges and textures, and later layers learning more complex features such as object parts and entire objects.
- A method is proposed for visualizing the saliency of an image region, which is defined as the gradient of the output of the network with respect to the input image.
- Saliency maps are generated by this method, which highlight the regions of an image that are most important for a given classification.
- The saliency maps generated by the method are shown to be qualitatively similar to the attention maps generated by humans.
- The saliency maps can be used to generate class-specific saliency maps, which highlight the regions of an image that are most important for a specific class.
- The method is used to visualize the internal representations of a CNN trained on the COCO dataset, which is a dataset of images with multiple objects and classes.
- The method can be used to visualize the internal representations of a CNN trained on a wide range of datasets and architectures.
- The method is used to visualize the internal representations of a CNN trained on a dataset of natural images and the internal representations of the network are shown to be similar to the representations of the primary visual cortex of the brain.
- The method is used to visualize the internal representations of a CNN trained on a dataset of images with natural scenes and the internal representations of the network are shown to be similar to the representations of the higher visual areas of the brain.
- The method is used to visualize the internal representations of a CNN trained on a dataset of images with faces and the internal representations of the network are shown to be similar to the representations of the fusiform face area of the brain.
- The saliency maps can be used to generate class-specific saliency maps, which highlight the regions of an image that are most important for a specific class.
- The method is used to analyze the internal representations of a CNN trained on a dataset of images with faces and the internal representations of the network are shown to be similar to the representations of the fusiform face area of the brain.
- The saliency maps can be used to generate class-specific saliency maps, which highlight the regions of an image that are most important for a specific class.
- The method is used to visualize the internal representations of a CNN trained on a dataset of images with faces and the internal representations of the network are shown to be similar to the representations of the fusiform face area of the brain.
- The saliency maps can be used to generate class-specific saliency maps, which highlight the regions of an image that are most important for a specific class.
- The method is concluded to be a powerful tool for visualizing and interpreting the internal representations of deep convolutional neural networks.

## Backlinks

> - [Salience Map](Salience Map.md)
>   - Use backprop to compute the gradients of logits wrt input : [[Deep Inside Convolutional Networks]]

_Backlinks last generated 2023-01-28 14:37:47_
