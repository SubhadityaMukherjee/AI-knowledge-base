## Softmax
- Output : probabilities
- $$\frac{e^{\frac{x}{T}}}{\Sigma_k(e^{\frac{x_{k}}{T}})}$$
- Softer argmax (0,1)
- Multinoulli

## T is the Temperature
- Higher the T -> Softer it the distribution. Aka less confident about distribution
<<<<<<< HEAD
- Lower -> Harder. More confident



## Backlinks
* [[Activation Functions]]
	* [[Softmax]]
* [[gradients]]
	* [[Softmax]] but on every output vector simultaneously

## ...
=======
- Lower -> Harder. More confident## Backlinks
* [[Recurrent]]
	* [[Softmax]] but on every output vector simultaneously
* [[ActivationFunctions]]
	* [[Softmax]]

>>>>>>> 1dd38fd29e2ea89a9d6c64b1ecd9e965740dd3c9
