## Learning Rate Decay #tricks
- Scale of loss landscape changes 
- Reduce step size near optima
- Factor $$\alpha_{i+1} = d\cdot \alpha_i$$
<<<<<<< HEAD
- LR Scheduling



## Backlinks
* [[Optimization]]
	* [[Learning Rate Decay tricks]]

## ...
=======
- LR Scheduling## Backlinks
* [[Optimizers]]
	* [[Learning Rate Decay tricks]]

>>>>>>> 1dd38fd29e2ea89a9d6c64b1ecd9e965740dd3c9
