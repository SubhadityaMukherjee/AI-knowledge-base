---
tags: temp
title: Learning Rate Decay #tricks
date modified: Thursday, August 11th 2022, 12:32:51 am
date created: Tuesday, July 26th 2022, 8:33:15 pm
---

# Learning Rate Decay #tricks
- Scale of loss landscape changes
- Reduce step size near optima
- Factor $$\alpha_{i+1} = d\cdot \alpha_i$$
- [[Cosine Learning Rate Decay]]

## â€¦

## Backlinks

> - [Optimization](Optimizers.md)
>   - [[Learning Rate Decay tricks]]
>    
> - [No Bias Decay](No bias decay.md)
>   - No [[Learning Rate Decay tricks]]
>    
> - [Learning Rate [[Scheduling]]](Learning Rate Scheduling.md)
>   - [[Learning Rate Decay tricks]]

_Backlinks last generated 2022-10-04 13:01:19_
