---
title: Attention
tags: architecture
date modified: Monday, October 10th 2022, 2:02:34 pm
date created: Tuesday, July 26th 2022, 8:33:15 pm
---

# Attention
- Model can decide where to look in the input
- [Self_Attention](Self_Attention.md)
- [Additive_Attention](Additive_Attention.md)
- [Dot_Product_Attention](Dot_Product_Attention.md)
- [Location_Aware_Attention](Location_Aware_Attention.md)
- [Relative_Multi_Head_Self_Attention](Relative_Multi_Head_Self_Attention.md)
- [Soft_Attention](Soft_Attention.md)
- [Scaled_Dot_Product_Attention](Scaled_Dot_Product_Attention.md)
- [Encoder_Decoder_Attention](Encoder_Decoder_Attention.md)
- [Multi_Head_Attention](Multi_Head_Attention.md)
- [Strided_Attention](Strided_Attention.md)
- [Fixed_Factorization_Attention](Fixed_Factorization_Attention.md)
- [Sliding_Window_Attention](Sliding_Window_Attention.md)
- [Dilated_Sliding_Window_Attention](Dilated_Sliding_Window_Attention.md)
- [Global_and_Sliding_Window_Attention](Global_and_Sliding_Window_Attention.md)
- [Content_Based_Attention](Content_Based_Attention.md)
- [Location_Base_Attention](Location_Base_Attention.md)
- [Mixed_chunk_attention](Mixed_chunk_attention.md)

