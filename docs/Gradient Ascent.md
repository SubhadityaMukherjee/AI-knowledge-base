# Gradient Ascent
- To maximize loss function unlike [[Gradient Descent gradients]]
- Proportional to positive of gradient
- $$\theta_{t+1} = \theta{t} + \eta_t \Sigma_{n=1}^N(\nabla l_n(\theta_t))^T$$
## Backlinks
* [[Contrastive Loss]]
	* Minimize distance between similar inputs [[Gradient Descent gradients]], maximize between dissimilar [[Gradient Ascent]]

<<<<<<< HEAD
## ...



## Backlinks
* [[Gradient Ascent]]
	* Minimize distance between similar inputs [[Gradient Descent gradients]], maximize between dissimilar [[Gradient Ascent]]
* [[Contrastive Loss]]
	* Minimize distance between similar inputs [[Gradient Descent gradients]], maximize between dissimilar [[Gradient Ascent]]

## ...
=======
>>>>>>> 1dd38fd29e2ea89a9d6c64b1ecd9e965740dd3c9
