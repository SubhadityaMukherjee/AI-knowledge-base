---
tags: temp
title: Learning Rate Decay #tricks
date modified: Monday, October 10th 2022, 2:02:24 pm
date created: Tuesday, July 26th 2022, 8:33:15 pm
---

# Learning Rate Decay #tricks
- Scale of loss landscape changes
- Reduce step size near optima
- Factor $$\alpha_{i+1} = d\cdot \alpha_i$$
- [[Cosine Learning Rate Decay]]

## â€¦

## Backlinks
> - [Optimization](Optimizers.md)
>   - [[Learning Rate Decay tricks]]
>
> - [No Bias Decay](No bias decay.md)
>   - No [[Learning Rate Decay tricks]]
>
> - [Learning Rate [[Scheduling]]](Learning Rate Scheduling.md)
>   - [[Learning Rate Decay tricks]]

_Backlinks last generated 2022-10-04 13:01:19_
