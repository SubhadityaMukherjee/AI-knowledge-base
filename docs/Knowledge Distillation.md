---
tags: temp
title: Knowledge Distillation
date modified: Tuesday, October 4th 2022, 12:50:33 pm
date created: Tuesday, July 26th 2022, 8:33:15 pm
---

# Knowledge Distillation
- Teacher model to help train the student model
- Teacher is often pre trained
- Student tries to imitate teacher
- [[Distillation Loss]]
- [[Knowledge Distillation Survey 2021]]
- [[Distilling the Knowledge in a Neural Network]]

