---
tags: temp
title: Linear Learning Rate Scaling
date modified: Wednesday, August 10th 2022, 11:41:27 am
date created: Tuesday, July 26th 2022, 8:33:15 pm
---

# Linear Learning Rate Scaling
- If [He [Initialization](Initialization.md) ]] is used, 0.1 is a good learning rate for batch size 256 and for a larger b, $0.1\times\frac{\mathrm{b}}{256}$ is okay

