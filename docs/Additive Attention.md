---
title: Additive Attention

tags: architecture 
date modified: Monday, October 10th 2022, 2:02:34 pm
date created: Tuesday, July 26th 2022, 8:33:15 pm
---

# Additive [[Attention]]
- [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf)
- Uses a one layer feedforward network to calculate [[Attention Alignment]]
- Oh, basically it is the same as [[Bahdanau Attention]]

## Backlinks
> - [Bahdanau [[Attention]]](Bahdanau Attention.md)
>   - Same as [[Additive Attention]]
>
> - [Multiplicative [[Attention]]](Multiplicative Attention.md)
>   - Since [[Additive Attention]] performs better for scale, use a factor [[Scaled Dot Product Attention]]
>
> - [Attention](Attention.md)
>   - [[Additive Attention]]

_Backlinks last generated 2022-10-04 13:01:19_
