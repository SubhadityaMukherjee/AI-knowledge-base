# Optimization
- [[Gradient Descent gradients]]

- [[Adagrad]]

- [[Rmsprop]]

- [[Adam]]

- [[Learning Rate Decay tricks]]

- [[Early Stopping tricks]]
