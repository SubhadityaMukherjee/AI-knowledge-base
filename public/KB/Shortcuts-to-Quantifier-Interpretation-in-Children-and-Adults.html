<!DOCTYPE html>
<html lang="en"><head><title>Shortcuts to Quantifier Interpretation in Children and Adults</title><meta charset="utf-8"/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com"/><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=IBM Plex Mono&amp;family=Schibsted Grotesk:wght@400;700&amp;family=Source Sans Pro:ital,wght@0,400;0,600;1,400;1,600&amp;display=swap"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:title" content="Shortcuts to Quantifier Interpretation in Children and Adults"/><meta property="og:description" content="Shortcuts to Quantifier Interpretation in Children and Adults Patricia J. Brooks &amp;amp; Irina Sekerina Intro Summarized in 2022-10-10 Errors involving universal quantification are common in contexts depicting sets of individuals in partial, one-to-one correspondence quantifier-spreading errors are more common with distributive quantifiers each and every than with all."/><meta property="og:image" content="https://https://subhadityamukherjee.github.io/AI-knowledge-base//static/og-image.png"/><meta property="og:width" content="1200"/><meta property="og:height" content="675"/><link rel="icon" href="../static/icon.png"/><meta name="description" content="Shortcuts to Quantifier Interpretation in Children and Adults Patricia J. Brooks &amp;amp; Irina Sekerina Intro Summarized in 2022-10-10 Errors involving universal quantification are common in contexts depicting sets of individuals in partial, one-to-one correspondence quantifier-spreading errors are more common with distributive quantifiers each and every than with all."/><meta name="generator" content="Quartz"/><link href="../index.css" rel="stylesheet" type="text/css" spa-preserve/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css" rel="stylesheet" type="text/css" spa-preserve/><script src="../prescript.js" type="application/javascript" spa-preserve></script><script type="application/javascript" spa-preserve>const fetchData = fetch("../static/contentIndex.json").then(data => data.json())</script></head><body data-slug="KB/Shortcuts-to-Quantifier-Interpretation-in-Children-and-Adults"><div id="quartz-root" class="page"><div id="quartz-body"><div class="left sidebar"><h2 class="page-title"><a href="..">Subhaditya's KB</a></h2><div class="spacer mobile-only"></div><div class="search"><button class="search-button" id="search-button"><p>Search</p><svg role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title>Search</title><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"></path><circle cx="8" cy="8" r="7"></circle></g></svg></button><div id="search-container"><div id="search-space"><input autocomplete="off" id="search-bar" name="search" type="text" aria-label="Search for something" placeholder="Search for something"/><div id="search-layout" data-preview="true"></div></div></div></div><button class="darkmode" id="darkmode"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" id="dayIcon" x="0px" y="0px" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35" xml:space="preserve" aria-label="Dark mode"><title>Dark mode</title><path d="M6,17.5C6,16.672,5.328,16,4.5,16h-3C0.672,16,0,16.672,0,17.5    S0.672,19,1.5,19h3C5.328,19,6,18.328,6,17.5z M7.5,26c-0.414,0-0.789,0.168-1.061,0.439l-2,2C4.168,28.711,4,29.086,4,29.5    C4,30.328,4.671,31,5.5,31c0.414,0,0.789-0.168,1.06-0.44l2-2C8.832,28.289,9,27.914,9,27.5C9,26.672,8.329,26,7.5,26z M17.5,6    C18.329,6,19,5.328,19,4.5v-3C19,0.672,18.329,0,17.5,0S16,0.672,16,1.5v3C16,5.328,16.671,6,17.5,6z M27.5,9    c0.414,0,0.789-0.168,1.06-0.439l2-2C30.832,6.289,31,5.914,31,5.5C31,4.672,30.329,4,29.5,4c-0.414,0-0.789,0.168-1.061,0.44    l-2,2C26.168,6.711,26,7.086,26,7.5C26,8.328,26.671,9,27.5,9z M6.439,8.561C6.711,8.832,7.086,9,7.5,9C8.328,9,9,8.328,9,7.5    c0-0.414-0.168-0.789-0.439-1.061l-2-2C6.289,4.168,5.914,4,5.5,4C4.672,4,4,4.672,4,5.5c0,0.414,0.168,0.789,0.439,1.06    L6.439,8.561z M33.5,16h-3c-0.828,0-1.5,0.672-1.5,1.5s0.672,1.5,1.5,1.5h3c0.828,0,1.5-0.672,1.5-1.5S34.328,16,33.5,16z     M28.561,26.439C28.289,26.168,27.914,26,27.5,26c-0.828,0-1.5,0.672-1.5,1.5c0,0.414,0.168,0.789,0.439,1.06l2,2    C28.711,30.832,29.086,31,29.5,31c0.828,0,1.5-0.672,1.5-1.5c0-0.414-0.168-0.789-0.439-1.061L28.561,26.439z M17.5,29    c-0.829,0-1.5,0.672-1.5,1.5v3c0,0.828,0.671,1.5,1.5,1.5s1.5-0.672,1.5-1.5v-3C19,29.672,18.329,29,17.5,29z M17.5,7    C11.71,7,7,11.71,7,17.5S11.71,28,17.5,28S28,23.29,28,17.5S23.29,7,17.5,7z M17.5,25c-4.136,0-7.5-3.364-7.5-7.5    c0-4.136,3.364-7.5,7.5-7.5c4.136,0,7.5,3.364,7.5,7.5C25,21.636,21.636,25,17.5,25z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" id="nightIcon" x="0px" y="0px" viewBox="0 0 100 100" style="enable-background:new 0 0 100 100" xml:space="preserve" aria-label="Light mode"><title>Light mode</title><path d="M96.76,66.458c-0.853-0.852-2.15-1.064-3.23-0.534c-6.063,2.991-12.858,4.571-19.655,4.571  C62.022,70.495,50.88,65.88,42.5,57.5C29.043,44.043,25.658,23.536,34.076,6.47c0.532-1.08,0.318-2.379-0.534-3.23  c-0.851-0.852-2.15-1.064-3.23-0.534c-4.918,2.427-9.375,5.619-13.246,9.491c-9.447,9.447-14.65,22.008-14.65,35.369  c0,13.36,5.203,25.921,14.65,35.368s22.008,14.65,35.368,14.65c13.361,0,25.921-5.203,35.369-14.65  c3.872-3.871,7.064-8.328,9.491-13.246C97.826,68.608,97.611,67.309,96.76,66.458z"></path></svg></button></div><div class="center"><div class="page-header"><div class="popover-hint"><nav class="breadcrumb-container" aria-label="breadcrumbs"><div class="breadcrumb-element"><a href="../">Home</a><p> ❯ </p></div><div class="breadcrumb-element"><a href="../KB/">Knowledge Base</a><p> ❯ </p></div><div class="breadcrumb-element"><a href>Shortcuts to Quantifier Interpretation in Children and Adults</a></div></nav><h1 class="article-title">Shortcuts to Quantifier Interpretation in Children and Adults</h1><p show-comma="true" class="content-meta"><span>Sep 18, 2024</span><span>20 min read</span></p><ul class="tags"><li><a href="../tags/language" class="internal tag-link">language</a></li></ul></div></div><article class="popover-hint"><h1 id="shortcuts-to-quantifier-interpretation-in-children-and-adults">Shortcuts to Quantifier Interpretation in Children and Adults<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#shortcuts-to-quantifier-interpretation-in-children-and-adults" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h1>
<ul>
<li>Patricia J. Brooks &amp; Irina Sekerina</li>
</ul>
<h2 id="intro">Intro<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#intro" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<ul>
<li>Summarized in <a href="../journals/2022-10-10" class="internal alias" data-slug="journals/2022-10-10">2022-10-10</a></li>
<li>Errors involving universal quantification are common in contexts depicting sets of individuals in partial, one-to-one correspondence</li>
<li>quantifier-spreading errors are more common with distributive quantifiers each and every than with all.</li>
<li>pairs of pictures</li>
<li>selected one corresponding to a sentence containing a universal quantifier</li>
<li>not in correspondence, with correct sentence interpretation requiring their attention</li>
<li>Children younger than 9 years made numerous errors</li>
<li>with poorer performance in distributive contexts than collective ones</li>
<li>21 native, English-speaking adults, given a similar task with the distributive quantifier every, also made childlike errors undermines accounts positing immature syntactic structures as the error source</li>
<li>errors seemingly reflect inaccurate syntax to semantics mapping, with adults and children alike resorting to processing shortcuts.</li>
<li>Quantificational terms such as all, usually, and most are a crucial type of linguistic device used to indicate which sets of individuals or events have which properties and relationships</li>
<li>acquisition may be delayed relative to other sorts of lexical items (e.g., nouns and verbs) because their complex patterns of usage often result in interpretive ambiguities</li>
<li>96 children (5- to 9-year-olds)</li>
<li>Both pictures showed extra objects</li>
<li>In spoken language, however, the intensifier interpretation is predominant in which it is conventional to say all even when it does not imply exhaustivity</li>
<li>I left all my money at home would not preclude their having money in the bank</li>
<li>all is used primarily as an adverbial intensifier in both child language and child-directed speech</li>
<li>every and each involve additional lexical complexity</li>
<li>Every appears inside compounds—for example, everybody—and it vacillates between collective and distributive interpretations when used as a quantifier</li>
<li>Each is more uniformly distributive but has the further conceptual requirement that the individuals modified by each be successively scanned</li>
<li>Every and each both occur only rarely in either childdirected or child speech, which limits opportunities for children to acquire their patterns of usage.</li>
<li>overexhaustive search</li>
<li>involves failure to properly restrict the domain of the universal quantifier to the</li>
<li>exhaustive pairing</li>
<li>classic spreading</li>
<li>noun phrase (NP) it modifies</li>
<li>complementary error referred to as underexhaustive search</li>
<li>Very young children occasionally make other, more surprising errors in interpreting universal quantifiers, such as answering no to the question Is every bunny eating a carrot? when shown, for example, a picture of three rabbits, each eating a carrot, along with a dog eating a bone. This error is referred to as bunny spreading</li>
<li>Children’s errors with universal quantification have led to controversies with respect to how to explain them. One view is that the errors stem from children’s deficient syntactic representations (Kang (2001), Philip (1995; 1996), Roeper and de Villiers (1993), Roeper and Matthei (1975), Roeper et al. (2005)). Philip (1995; 1996), following Roeper and Matthei (1975) suggested that the classic spreading error is due to the fact that children syntactically misinterpret distributive universal quantifiers (e.g., each or every in English, cada in Spanish or Portuguese) as sentential adverbials that range over events as opposed to individuals. Roeper et al. (2005) described a sequence of steps of how children start with a general syntactic representation of every as an adverbial intensifier that gets progressively more specific as every changes its position in the syntactic representation.</li>
<li>Rather, the errors are presumed to involve shallow processing, resulting in inaccurate mapping between syntactic and semantic representations</li>
<li>First, Crain et al. (1996) did not systematically vary the position of the universal quantifier in the test questions or statements while holding constant the introductory story and scenario</li>
<li>prototypical scenario</li>
<li>This feature of their design provided children with unambiguous cues as to which set of entities was the focus of attention.</li>
<li>Crain et al.’s (1996) claim that preschoolers have full competence with universal quantifiers would seem to be undermined by the fact that even older school-age children make errors identifying the domain of a universal quantifier.</li>
<li>First, there have been great discrepancies in error rates across the many studies that have almost exclusively utilized the Truth Value Judgment Task, ranging from near perfect performance in Crain et al. (1996) to extremely high error rates in Kang (2001), that is, over 80% errors in 6- to 7-year-olds</li>
<li>Our sentence–picture matching task does not have the same demand characteristics as the Truth Value Judgment Task, and in our opinion, it provides a more accurate way of evaluating whether children’s interpretations of sentences with universal quantifiers vary systematically as a function of the position of the</li>
<li>quantifier in the sentence and the type of scene</li>
<li>examine whether there is an asymmetry in the distribution of errors as a function of the syntactic position of the universal quantifier</li>
<li>address the controversy as to whether children perform better in tasks with collective universal quantifiers (cf. Brooks and Braine (1996)) or with distributive ones (cf. Drozd (1996))</li>
<li>Finally, because accounts positing syntactic deficits as the source of quantifierspreading errors (e.g., Kang (2001), Philip (1996), Roeper et al. (2005)) generally assume that adults are essentially error free in their comprehension of basic sentences containing universal quantifiers (i.e., their syntax is perfect), we tested a group of adults on a version of our task (Experiment 3) to evaluate this claim and to allow a more complete investigation of the developmental trajectory of quantifier acquisition from 5-year-olds to adults.</li>
</ul>
<h2 id="experiment-1">EXPERIMENT 1<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#experiment-1" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<h3 id="participants">Participants<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#participants" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<ul>
<li>We recruited and tested twelve 5-year-olds (M = 5;5, range = 5;2–5;11), twelve 6-year-olds (M = 6;6, range = 6;2–6;10), twelve 7-yearolds (M = 7;6, range = 7;1–7;11), twelve 8-year-olds (M = 8;6, range = 8;0–8;11), and twelve 9-year-olds (M = 9;6, range = 9;1–9;11) at private elementary schools and after-school programs in Atlanta, Georgia.</li>
<li>24 pairs of pictures depicting people involved in various activities such as carrying boxes, washing pets, or watering plants.</li>
<li>Four types of picture pairs were constructed.</li>
<li>three people individually engaged in an activity with three distinct objects or animals</li>
<li>three people engaged in an activity with three objects</li>
<li>Collective picture pair types 3 and 4 were variations of collective picture pair type 2, with new foils created to match the distributive pairs in terms of target–foil similarity</li>
<li>Across picture pairs, a variety of contexts with transitive actional verbs were used so that each sentence type could be presented multiple times without repeating any pictures</li>
<li>All of the contexts involved humans acting on animate or inanimate objects</li>
<li>Six sentence types were used</li>
<li>
<ul>
<li>(1) Each of the (people) is (verb)ing an (object), for example, Each of the men is washing a bear. (2) There is a (person) (verb)ing each of the (objects), for example, There is a man washing each of the bears. (3) Every (person) is (verb)ing an (object), for example, Every man is washing a bear. (4) There is a (person) (verb)ing every (object), for example, There is a man washing every bear. (5) All of the (people) are (verb)ing an (object), for example, All of the men are washing a bear.</li>
</ul>
</li>
<li>
<ul>
<li>(6) There is a (person) (verb)ing all of the (objects), for example, There is a man washing all of the bears.</li>
</ul>
</li>
<li>Twelve additional pairs of pictures served as filler items.</li>
</ul>
<h3 id="procedure">Procedure<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#procedure" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<ul>
<li>single, 20-min session conducted in a quiet room of their school</li>
<li>We showed children two pictures at a time and asked them to point to the picture that went best with a sentence read aloud</li>
<li>After the child looked at both pictures, the experimenter read the corresponding sentence and asked the child to point to the picture that went best with the sentence.</li>
<li>without providing any corrective feedback</li>
<li>Across trials, we randomized the position of the correct picture</li>
<li>Children made no errors on filler sentences, and these trials were not examined further.</li>
<li>mixed-design analysis of variance</li>
<li>The dependent variable was the proportion of correct picture choices for Sentence Types 1 through 4</li>
<li>arcsine transformed these proportions and all others</li>
</ul>
<h3 id="analysis">Analysis<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#analysis" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<ul>
<li>The analysis showed significant main effects of syntactic position, F(1, 55) = 19.03, p &lt; .001, and age, F(4, 55) = 7.22, p &lt; .001. No other main effects or interactions were significant</li>
<li>As shown in Table 1, comparisons against chance performance (50%) revealed that only the 9-year-olds as a group were above chance in selecting the correct pictures (Figure 1b) for sentences with the universal quantifier modifying the direct object.</li>
<li>At this criterion, one 5-year-old (8%), two 6-year-olds (17%), five 7- year-olds (42%), five 8-year-olds (42%), and nine 9-year-olds (75%) performed above chance.</li>
<li>Overall, children were correct in 95.8% of their picture selections when the original collective pictures of Brooks and Braine were used (see Figure 2) but only 83.1% of trials with the modified collective pictures (see Figures 3 and 4).</li>
<li>children’s responses became more consistently correct with age, with only children 7 years and older performing above chance as a group in the modified task when the quantifier modified the direct object.</li>
<li>To examine whether individual children were above chance in selecting the appropriate collective picture on the modified task with all, we again used the binomial distribution ( p &lt; .05), with above-chance performance requiring 6 out of 6 correct responses</li>
<li>At this criterion, zero 5-year-olds (0%), five 6-year-olds (42%), seven 7-year-olds (58%), seven 8-year-olds (58%), and seven 9-year-olds (58%) performed above chance.</li>
<li>f we adopt a more lenient criterion (5 of 6), which is proportionally comparable to the 10 of 12 required for above-chance performance with sentences containing each or every, then six 5-year-olds (50%), eight 6-yearolds (67%), ten 7-year-olds (83%), eight 8-year-olds (67%), and ten 9-year-olds (83%) showed consistently strong individual performance.</li>
<li>we conducted one additional mixed-design ANOVA with Quantifier (all, each, every) and Syntactic Position as within-subjects factors and Age as a between-subjects factor.</li>
<li>The main effect of syntactic position was significant, F(1, 55) = 16.38, p &lt; .001, but was qualified by an interaction of quantifier and age, F(1, 55) = 7.13, p &lt; .01</li>
<li>comprehension performance was more accurate when the universal quantifier modified the subject of the sentence.</li>
<li>This effect of syntactic position, however, was highly significant for sentences with each or every but only marginally significant for sentences with all (see above for F tests for main effects of syntactic position in separate analyses by quantifier). No other interactions were significant.</li>
<li>In general, their picture selections were more accurate for sentences with a universal quantifier modifying the subject in comparison to the direct object of a transitive actional verb.</li>
<li>Although the Philip (1996) and Kang studies have shown a similar pattern of subject/object asymmetry to this study and Brooks and Braine (1996), we note that children performed at much higher levels of accuracy in our sentence–picture matching task compared to the Truth Value Judgment Task. In none of our conditions, at any age, were children significantly below chance in their picture selections. This contrasts especially with Kang, who reported error rates over 80% in both English-speaking and Korean-speaking 6- and 7-year-olds</li>
<li>However, by age 7, children were consistently correct in their picture choices regardless of the syntactic position of all in the sentence.</li>
<li>This suggests that the collective groupings may have helped focus their at</li>
<li>tention on the relevant set of entities modified by the quantifier.</li>
<li>More generally, the Truth Value Judgment Task allows children to reject a picture for a variety of reasons (and it is often hard to discern the basis for children’s pattern of responding)</li>
<li>In Experiment 1, we used a sentence–picture matching procedure in which children needed only to find the picture that matched the sentence. This task eliminated opportunities for participants to consider whether a collective versus distributive interpretation of the sentence was preferred and furthermore allowed us to carefully match our collective and distributive pictures with respect to the composition of the foils.</li>
</ul>
<h2 id="experiment-2">Experiment 2<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#experiment-2" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<ul>
<li>We designed Experiment 2 to eliminate this interpretive confound through the use of locative scenes.</li>
<li>locative pictures with animals and other entities shown in containers of various sorts (e.g., bananas in baskets, bears in beds).</li>
<li>universal quantifiers in three syntactic constructions that support distributive interpretations in a locative context</li>
<li>Across constructions, we systematically varied both the syntactic position of the universal quantifier and whether the subject of the sentence referred to the containers or the entities in them</li>
</ul>
<h3 id="method">Method<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#method" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<ul>
<li>Participants. Twelve 7-year-olds (M = 7;6, range = 7;1–7;10), twelve 8-year-olds (M = 8;6, range = 8;0–8;11), and twelve 9-year-olds (M = 9;5, range = 9;0–9;10) took part in the experiment. We recruited and tested these children at the same schools as in Experiment 1. None of the children in Experiment 2 participated in the previous experiment.</li>
</ul>
<h3 id="materials">Materials<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#materials" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<ul>
<li>27 pairs of pictures depicting various entities arranged in containers (e.g., alligators in bathtubs, turtles in tanks, apples in bowls)</li>
<li>The pictures showed distributive arrangements with the entities and containers in partial, one-to-one correspondence with each other.</li>
<li>Both pictures depicted three entities each located in a unique container. One picture showed two extra empty containers (see Figure 5a), and the other picture showed two objects that were not in containers (see Figure 5b).</li>
<li>nine sentence types</li>
<li>
<ul>
<li>(7) All of the (objects) are in a (container), for example, All of the alligators are in a bathtub. (8) All of the (containers) have an (object) in them, for example, All of the bathtubs have an alligator in them.</li>
</ul>
</li>
<li>There is an (object) in all of the (containers), for example, There is an alligator in all of the bathtubs. Each of the (objects) is in a (container), for example, Each of the alligators is in a bathtub. Each of the (containers) has an (object) in it, for example, Each of the bathtubs has an alligator in it. There is an (object) in each of the (containers), for example, There is an alligator in each of the bathtubs. Every (object) is in a (container), for example, Every alligator is in a bathtub. Every (container) has an (object) in it, for example, Every bathtub has an alligator in it. There is an (object) in every (container), for example, There is an alligator in every bathtub.</li>
</ul>
<h3 id="analysis-1">Analysis<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#analysis-1" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<ul>
<li>The main effect of quantifier and all of the interactions involving quantifiers were not significant.</li>
<li>Across syntactic constructions, 7-year-olds preferred the picture with the extra animals or objects as opposed to the picture with the extra containers.</li>
<li>Both the 7- and 8-year-olds made correct picture selections at an above-chance level only for sentences with the universal quantifier modifying the noun correspond</li>
<li>ing to the containers irrespective of the syntactic construction.</li>
<li>n contrast, the majority of 9-year-olds correctly varied their picture selections in accordance with the varying syntactic constructions and performed above chance as a group for all sentence types.</li>
<li>Experiment 2 replicated one of the main findings of Experiment 1: Only 9-yearolds as a group consistently identified the domain of the universal quantifier and selected the appropriate picture at above-chance levels for distributive events in which sets of objects were in partial, one-to-one correspondence.</li>
<li>Rather, irrespective of the syntactic construction, they showed better performance on sentences with the quantifier modifying the containers</li>
<li>The observed bias to prefer locative scenes in which all of the containers were filled (the so-called garage-centered bias) has been observed many times; see Drozd (2001) for a review</li>
<li>These results are difficult to reconcile with Kang’s</li>
<li>Moreover, all of the age groups failed to show any effect of quantifier in Experiment 2 in contrast to Experiment 1</li>
<li>That is, the children failed to show a familiarity effect with better performance for sentences with all.</li>
<li>he differing results for the two experiments indicate that the collective scenes used with all in Experiment 1 were easier than the distributive ones used in Experiment 2 (see also Brinkmann et al. (1996)).</li>
<li>It appears that these scenarios involving partial, one-toone correspondence pose considerable challenges for children.</li>
<li>Although Experiment 2 provided no evidence that children distinguished the quantifier all from each or every, we emphasize that previous work (Brooks et al. (2001), Brooks et al. (1998)) has shown that children do readily distinguish these quantifiers on semantic grounds</li>
</ul>
<h2 id="experiment-3">Experiment 3<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#experiment-3" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<ul>
<li>In Experiment 3, we examined whether adults would make errors restricting the domain of a universal quantifier in a similar picture-selection task with distributive, locative scenes.</li>
<li>Testing adults is important because syntactic accounts do not readily predict errors in syntactically competent adults</li>
<li>Brooks and Braine (1996, Experiment 1) tested adults with actional scenes and found no errors. Their data, however, came from 10 undergraduates at a highly selective private university (Carnegie Mellon) and thus may not be representative of adults in general.</li>
<li>Here we tested monolingual English-speaking undergraduates at a highly diverse public university. This experiment constituted pilot work to establish a paradigm suitable for an eye-tracking study.</li>
<li>First, we included two filler pictures along with each target and foil picture, and second, we presented sentences with the quantifier every but did not test each or all.</li>
<li>Participants. We recruited 22 monolingual, adult native speakers of English (16 women, 6 men; M age = 26 years, range = 18–49) from introductory psychology classes at the College of Staten Island, City University of New York, who received extra credit for their participation.</li>
<li>We created PowerPoint® slides comprising four pictures that were presented simultaneously (see Figure 6 for an example). These slides depicted two sets of objects in partial, one-to-one correspondence</li>
<li>Each array contained two pictures similar to those used in Experiment 2 (compare Figures 5 and 6), along with two filler pictures</li>
<li>We presented each sentence type six times, in randomized order, for a total of 12 trials</li>
<li>To permit naming, we numbered the pictures from 1 to 4, with the position of the target randomized across trials. We used a tape recorder to record participants’ responses.</li>
</ul>
<h2 id="observations">Observations<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#observations" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<ul>
<li>Across all participants, no errors were made on filler sentences indicating that the participants were generally compliant with the task instructions</li>
<li>performance on the task was not at ceiling, with adults making errors on an average of 21% of the trials</li>
<li>This error rate, which is numerically higher than the rate observed with the 9-year-olds of Experiments 1 and 2, is likely due to the added complexity of the task involving four pictures as opposed to two.</li>
<li>Across the trials involving sentences with every, adult participants never selected either of the filler pictures. This indicates that their response set was effectively the same as that of the children in Experiment 2.</li>
<li>Thus, unlike the children in Experiment 2, the adults did not show a preference for locative scenes in which all of the containers were filled.</li>
<li>A further examination of data indicated that two adults selected the same picture on 12 of 12 trials indicating no sensitivity to the position of the quantifier in the sentence</li>
<li>The fact that half of our adult participants made considerable numbers of errors in restricting every to its domain is not readily explicated by syntactic accounts positing immature syntactic representations as the source of children’s quantifier-spreading errors</li>
<li>Our findings that both children and adults make errors in quantifier interpretation are more readily explained by the underspecification account of Sanford and Sturt (2002).</li>
<li>The crucial step involves deficient mapping from syntactic structure to a semantic representation,</li>
<li>Although grammatically competent adults are capable of construing correct and fully specified semantic representations of utterances with quantifiers, it does not always happen</li>
<li>The results demonstrate that many school-age children and adults had considerable difficulty in restricting the domain of a universal quantifier, especially when two sets of entities were in partial, one-to-one correspondence. This result contrasts most dramatically with the near perfect performance of preschool children in Crain et al. (1996)</li>
<li>This suggests that the problem does not reside in the child’s syntax, given the similarities in sentence structures used across studies, but in</li>
<li>stead has to do with the difficulty of selecting the appropriate set of entities and avoiding distraction by salient objects.</li>
<li>Taken together, the experiments suggest that it was the collective scenes as opposed to the use of all that improved children’s performance in Experiment 1. Collective scenes were easier presumably because the group depiction aided the child in isolating one set of entities relative to the other</li>
<li>Conversely, the distributive scenes were more difficult because the pictures were more visually symmetric. The observed difference in performance for collective versus distributive scenes seems to undermine syntactic accounts of children’s errors given that the structures of the corresponding sentences were essentially the same.</li>
<li>The fact that children’s errors in Experiment 2 were not randomly distributed indicates that they noticed the extra objects and/or containers in the distributive pictures</li>
<li>Again, only 9-yearolds consistently varied their picture selections in accordance with the varying syntactic constructions and did not show a strong preference for one picture configuration at the expense of the other.</li>
<li>Their performance on a modified version of the sentence–picture matching task was not only below ceiling, but their error rate was numerically higher than that of the 9-year-olds of Experiments 1 and 2. Note, however, that in contrast to the 7- and 8-year-olds’ patterns, the adults’ errors were equally distributed between the locative pictures with extra animals or objects versus extra containers.</li>
<li>We suspect that both children and adults make errors in comprehension because they engage in shallow processing that causes inaccurate mapping between syntactic and semantic representations.</li>
<li>In interpreting universal quantifiers, children construct underspecified representations using simpler processing strategies and then rely on pragmatics to solve the task.</li>
<li>Adults also may construct underspecified representations as a first step that may or may not be followed by the application of algorithm. We speculate that adults stop at an underspecified representation when there are other demands on attention under conditions of working memory load, fatigue, or lack of cognitive effort.</li>
<li>Another possibility with respect to the results of Experiment 2 is that the children may have gradually picked up on the fact that the universal quantifier modified the noun corresponding to the containers in two of three of the sentences.</li>
<li>In either case, once children were fixated on a particular picture configuration, they perseverated and were reluctant to consider a competing picture as a possible alternative, even in the face of a conflicting sentence structure</li>
<li>The suggestion that the children processed the sentences deterministically is not a new one.</li>
<li>have indicated that children tend not to revise their initially incorrect interpretations of temporary syntactic or referential ambiguities even when disambiguating information becomes available.</li>
<li>Shallow processing also provides a straightforward explanation of the errors made by adults in Experiment 3. Their high error rates suggest that adult listeners often do not tax their limited information-processing capacities by conducting exhaustive syntactic analyses of sentences but rather make use of simpler strategies in generating reasonable guesses</li>
<li>These findings led Ferreira et al. (2002) to conclude that the meaning people obtain for a sentence is often not a reflection of its true content (p. 11) and that language processing often yields a merely good enough representation of a sentence’s meaning</li>
<li>This statement is an apt characterization of the performance of many school-age children and adults in our experiments. More generally, the comprehension of universal quantifiers seems an ideal domain for exploring the dynamics of attention allocation, and cognitive effort, in language processing.</li>
</ul></article><hr/><div class="page-footer"></div></div><div class="right sidebar"><div class="graph"><h3>Graph View</h3><div class="graph-outer"><div id="graph-container" data-cfg="{&quot;drag&quot;:true,&quot;zoom&quot;:true,&quot;depth&quot;:1,&quot;scale&quot;:1.1,&quot;repelForce&quot;:0.5,&quot;centerForce&quot;:0.3,&quot;linkDistance&quot;:30,&quot;fontSize&quot;:0.6,&quot;opacityScale&quot;:1,&quot;showTags&quot;:true,&quot;removeTags&quot;:[],&quot;focusOnHover&quot;:false}"></div><button id="global-graph-icon" aria-label="Global Graph"><svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 55 55" fill="currentColor" xml:space="preserve"><path d="M49,0c-3.309,0-6,2.691-6,6c0,1.035,0.263,2.009,0.726,2.86l-9.829,9.829C32.542,17.634,30.846,17,29,17
                s-3.542,0.634-4.898,1.688l-7.669-7.669C16.785,10.424,17,9.74,17,9c0-2.206-1.794-4-4-4S9,6.794,9,9s1.794,4,4,4
                c0.74,0,1.424-0.215,2.019-0.567l7.669,7.669C21.634,21.458,21,23.154,21,25s0.634,3.542,1.688,4.897L10.024,42.562
                C8.958,41.595,7.549,41,6,41c-3.309,0-6,2.691-6,6s2.691,6,6,6s6-2.691,6-6c0-1.035-0.263-2.009-0.726-2.86l12.829-12.829
                c1.106,0.86,2.44,1.436,3.898,1.619v10.16c-2.833,0.478-5,2.942-5,5.91c0,3.309,2.691,6,6,6s6-2.691,6-6c0-2.967-2.167-5.431-5-5.91
                v-10.16c1.458-0.183,2.792-0.759,3.898-1.619l7.669,7.669C41.215,39.576,41,40.26,41,41c0,2.206,1.794,4,4,4s4-1.794,4-4
                s-1.794-4-4-4c-0.74,0-1.424,0.215-2.019,0.567l-7.669-7.669C36.366,28.542,37,26.846,37,25s-0.634-3.542-1.688-4.897l9.665-9.665
                C46.042,11.405,47.451,12,49,12c3.309,0,6-2.691,6-6S52.309,0,49,0z M11,9c0-1.103,0.897-2,2-2s2,0.897,2,2s-0.897,2-2,2
                S11,10.103,11,9z M6,51c-2.206,0-4-1.794-4-4s1.794-4,4-4s4,1.794,4,4S8.206,51,6,51z M33,49c0,2.206-1.794,4-4,4s-4-1.794-4-4
                s1.794-4,4-4S33,46.794,33,49z M29,31c-3.309,0-6-2.691-6-6s2.691-6,6-6s6,2.691,6,6S32.309,31,29,31z M47,41c0,1.103-0.897,2-2,2
                s-2-0.897-2-2s0.897-2,2-2S47,39.897,47,41z M49,10c-2.206,0-4-1.794-4-4s1.794-4,4-4s4,1.794,4,4S51.206,10,49,10z"></path></svg></button></div><div id="global-graph-outer"><div id="global-graph-container" data-cfg="{&quot;drag&quot;:true,&quot;zoom&quot;:true,&quot;depth&quot;:-1,&quot;scale&quot;:0.9,&quot;repelForce&quot;:0.5,&quot;centerForce&quot;:0.3,&quot;linkDistance&quot;:30,&quot;fontSize&quot;:0.6,&quot;opacityScale&quot;:1,&quot;showTags&quot;:true,&quot;removeTags&quot;:[],&quot;focusOnHover&quot;:true}"></div></div></div><div class="toc desktop-only"><button type="button" id="toc" class aria-controls="toc-content" aria-expanded="true"><h3>Table of Contents</h3><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="fold"><polyline points="6 9 12 15 18 9"></polyline></svg></button><div id="toc-content" class><ul class="overflow"><li class="depth-0"><a href="#shortcuts-to-quantifier-interpretation-in-children-and-adults" data-for="shortcuts-to-quantifier-interpretation-in-children-and-adults">Shortcuts to Quantifier Interpretation in Children and Adults</a></li><li class="depth-1"><a href="#intro" data-for="intro">Intro</a></li><li class="depth-1"><a href="#experiment-1" data-for="experiment-1">EXPERIMENT 1</a></li><li class="depth-2"><a href="#participants" data-for="participants">Participants</a></li><li class="depth-2"><a href="#procedure" data-for="procedure">Procedure</a></li><li class="depth-2"><a href="#analysis" data-for="analysis">Analysis</a></li><li class="depth-1"><a href="#experiment-2" data-for="experiment-2">Experiment 2</a></li><li class="depth-2"><a href="#method" data-for="method">Method</a></li><li class="depth-2"><a href="#materials" data-for="materials">Materials</a></li><li class="depth-2"><a href="#analysis-1" data-for="analysis-1">Analysis</a></li><li class="depth-1"><a href="#experiment-3" data-for="experiment-3">Experiment 3</a></li><li class="depth-1"><a href="#observations" data-for="observations">Observations</a></li></ul></div></div><div class="backlinks"><h3>Backlinks</h3><ul class="overflow"><li><a href="../KB/Final-Paper-Language-Modeling" class="internal">Final Paper LM</a></li><li><a href="../KB/language" class="internal">language</a></li><li><a href="../consistency-report" class="internal">consistency-report</a></li></ul></div></div></div><footer class><p>Created with <a href="https://quartz.jzhao.xyz/">Quartz v4.3.1</a> © 2024</p><ul><li><a href="https://github.com/SubhadityaMukherjee/AI-knowledge-base/">GitHub</a></li></ul></footer></div></body><script type="application/javascript">function c(){let t=this.parentElement;t.classList.toggle("is-collapsed");let l=t.classList.contains("is-collapsed")?this.scrollHeight:t.scrollHeight;t.style.maxHeight=l+"px";let o=t,e=t.parentElement;for(;e;){if(!e.classList.contains("callout"))return;let n=e.classList.contains("is-collapsed")?e.scrollHeight:e.scrollHeight+o.scrollHeight;e.style.maxHeight=n+"px",o=e,e=e.parentElement}}function i(){let t=document.getElementsByClassName("callout is-collapsible");for(let s of t){let l=s.firstElementChild;if(l){l.addEventListener("click",c),window.addCleanup(()=>l.removeEventListener("click",c));let e=s.classList.contains("is-collapsed")?l.scrollHeight:s.scrollHeight;s.style.maxHeight=e+"px"}}}document.addEventListener("nav",i);window.addEventListener("resize",i);
</script><script type="module">
          let mermaidImport = undefined
          document.addEventListener('nav', async () => {
            if (document.querySelector("code.mermaid")) {
              mermaidImport ||= await import('https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.7.0/mermaid.esm.min.mjs')
              const mermaid = mermaidImport.default
              const darkMode = document.documentElement.getAttribute('saved-theme') === 'dark'
              mermaid.initialize({
                startOnLoad: false,
                securityLevel: 'loose',
                theme: darkMode ? 'dark' : 'default'
              })

              await mermaid.run({
                querySelector: '.mermaid'
              })
            }
          });
          </script><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/contrib/copy-tex.min.js" type="application/javascript"></script><script src="../postscript.js" type="module"></script></html>