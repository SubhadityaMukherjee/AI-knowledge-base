---
title: Simple Gradient Descent

tags: optimizer gradients 
---

# Simple Gradient Descent
- $$\theta = \theta - \eta \cdot \nabla_{\theta} J(\theta)$$
- It starts with some coefficients, sees their cost, and searches for cost value lesser than what it is now.
- It moves towards the lower weight and updates the value of the coefficients.
- The process repeats until the local minimum is reached. A local minimum is a point beyond which it can not proceed.


## Backlinks

> - [Gradient Descent #gradients](Gradient Descent gradients.md)
>   - [[Simple Gradient Descent]]

_Backlinks last generated 2022-06-26 12:48:16_
