---
toc: true
title: Basic GAN

categories: ['architecturegenerative']
date modified: Tuesday, January 10th 2023, 1:38:03 pm
date created: Wednesday, December 21st 2022, 3:10:38 pm
---

# Basic GAN

- [Generative Adversarial Networks](https://arxiv.org/abs/1406.2661)
- Learn a prob distribution directly from data generated by that distribution
- no need for any [[Markov Chain.md|Markov Chain]] or unrolled approximate inference networks during either training or generation of samples
- [[Adversarial Learning.md|Adversarial Learning]]
- ![[Pasted image 20220310210209.png|im]]
- Min Max game $$max_D min_G V(G,D)$$ where $$V(G,D) = \mathbb{E}_{p_{data}(x)}logD(x) + \mathbb{E}_{p_{data}(x)}log(1-D(x))$$
- G : [[Gradient Descent gradients.md|Gradient Descent gradients]]
- D : [[Gradient Ascent.md|Gradient Ascent]]
- Discriminator Loss (Given Generator)
	- $$L_{disc}(D_\theta) =-\frac{1}{2}(\mathbb{E}_{x\sim p_{real}(x)}[log(D_{theta}(x))] + \mathbb{E}_{x\sim p_{latent}(x)}[log(1- D_\theta(G_\phi(z)))])$$
- Generator Loss (Given Discriminator)
	- $$L_{gen}(G_{\phi})= - \mathbb{E}_{z\sim p_{latent}(z)}[log(D_\theta(G_\phi(z)))]$$
	- This is low if Discriminator is fooled by Gen, $D_{\theta}(x_{gen}) \approx 1$

## Training
- pick mini batch of samples 
- update discriminator with Gradient Descent based** on discriminator loss with generator obtained from previous update
- update the generator with Gradient Descent based on generator loss with the discriminator from the previous step

## [[Issues.md|Issues]]
- [[Mode Collapse.md|Mode Collapse]]



