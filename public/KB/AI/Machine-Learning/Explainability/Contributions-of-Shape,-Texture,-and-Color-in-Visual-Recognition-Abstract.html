<!DOCTYPE html>
<html lang="en"><head><title>Contributions of Shape, Texture, and Color in Visual Recognition Abstract</title><meta charset="utf-8"/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com"/><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=IBM Plex Mono&amp;family=Schibsted Grotesk:wght@400;700&amp;family=Source Sans Pro:ital,wght@0,400;0,600;1,400;1,600&amp;display=swap"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:title" content="Contributions of Shape, Texture, and Color in Visual Recognition Abstract"/><meta property="og:description" content="Contributions of Shape, Texture, and Color in Visual Recognition Abstract /zotero Yunhao Ge, Yao Xiao, Zhi Xu, Xingrui Wang, and Laurent Itti Abstract [humanoid vision engine](humanoid ..."/><meta property="og:image" content="https://https://subhadityamukherjee.github.io/AI-knowledge-base//static/og-image.png"/><meta property="og:width" content="1200"/><meta property="og:height" content="675"/><link rel="icon" href="../../../../static/icon.png"/><meta name="description" content="Contributions of Shape, Texture, and Color in Visual Recognition Abstract /zotero Yunhao Ge, Yao Xiao, Zhi Xu, Xingrui Wang, and Laurent Itti Abstract [humanoid vision engine](humanoid ..."/><meta name="generator" content="Quartz"/><link href="../../../../index.css" rel="stylesheet" type="text/css" spa-preserve/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css" rel="stylesheet" type="text/css" spa-preserve/><script src="../../../../prescript.js" type="application/javascript" spa-preserve></script><script type="application/javascript" spa-preserve>const fetchData = fetch("../../../../static/contentIndex.json").then(data => data.json())</script></head><body data-slug="KB/AI/Machine-Learning/Explainability/Contributions-of-Shape,-Texture,-and-Color-in-Visual-Recognition-Abstract"><div id="quartz-root" class="page"><div id="quartz-body"><div class="left sidebar"><h2 class="page-title"><a href="../../../..">Subhaditya's KB</a></h2><div class="spacer mobile-only"></div><div class="search"><button class="search-button" id="search-button"><p>Search</p><svg role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title>Search</title><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"></path><circle cx="8" cy="8" r="7"></circle></g></svg></button><div id="search-container"><div id="search-space"><input autocomplete="off" id="search-bar" name="search" type="text" aria-label="Search for something" placeholder="Search for something"/><div id="search-layout" data-preview="true"></div></div></div></div><button class="darkmode" id="darkmode"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" id="dayIcon" x="0px" y="0px" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35" xml:space="preserve" aria-label="Dark mode"><title>Dark mode</title><path d="M6,17.5C6,16.672,5.328,16,4.5,16h-3C0.672,16,0,16.672,0,17.5    S0.672,19,1.5,19h3C5.328,19,6,18.328,6,17.5z M7.5,26c-0.414,0-0.789,0.168-1.061,0.439l-2,2C4.168,28.711,4,29.086,4,29.5    C4,30.328,4.671,31,5.5,31c0.414,0,0.789-0.168,1.06-0.44l2-2C8.832,28.289,9,27.914,9,27.5C9,26.672,8.329,26,7.5,26z M17.5,6    C18.329,6,19,5.328,19,4.5v-3C19,0.672,18.329,0,17.5,0S16,0.672,16,1.5v3C16,5.328,16.671,6,17.5,6z M27.5,9    c0.414,0,0.789-0.168,1.06-0.439l2-2C30.832,6.289,31,5.914,31,5.5C31,4.672,30.329,4,29.5,4c-0.414,0-0.789,0.168-1.061,0.44    l-2,2C26.168,6.711,26,7.086,26,7.5C26,8.328,26.671,9,27.5,9z M6.439,8.561C6.711,8.832,7.086,9,7.5,9C8.328,9,9,8.328,9,7.5    c0-0.414-0.168-0.789-0.439-1.061l-2-2C6.289,4.168,5.914,4,5.5,4C4.672,4,4,4.672,4,5.5c0,0.414,0.168,0.789,0.439,1.06    L6.439,8.561z M33.5,16h-3c-0.828,0-1.5,0.672-1.5,1.5s0.672,1.5,1.5,1.5h3c0.828,0,1.5-0.672,1.5-1.5S34.328,16,33.5,16z     M28.561,26.439C28.289,26.168,27.914,26,27.5,26c-0.828,0-1.5,0.672-1.5,1.5c0,0.414,0.168,0.789,0.439,1.06l2,2    C28.711,30.832,29.086,31,29.5,31c0.828,0,1.5-0.672,1.5-1.5c0-0.414-0.168-0.789-0.439-1.061L28.561,26.439z M17.5,29    c-0.829,0-1.5,0.672-1.5,1.5v3c0,0.828,0.671,1.5,1.5,1.5s1.5-0.672,1.5-1.5v-3C19,29.672,18.329,29,17.5,29z M17.5,7    C11.71,7,7,11.71,7,17.5S11.71,28,17.5,28S28,23.29,28,17.5S23.29,7,17.5,7z M17.5,25c-4.136,0-7.5-3.364-7.5-7.5    c0-4.136,3.364-7.5,7.5-7.5c4.136,0,7.5,3.364,7.5,7.5C25,21.636,21.636,25,17.5,25z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" id="nightIcon" x="0px" y="0px" viewBox="0 0 100 100" style="enable-background:new 0 0 100 100" xml:space="preserve" aria-label="Light mode"><title>Light mode</title><path d="M96.76,66.458c-0.853-0.852-2.15-1.064-3.23-0.534c-6.063,2.991-12.858,4.571-19.655,4.571  C62.022,70.495,50.88,65.88,42.5,57.5C29.043,44.043,25.658,23.536,34.076,6.47c0.532-1.08,0.318-2.379-0.534-3.23  c-0.851-0.852-2.15-1.064-3.23-0.534c-4.918,2.427-9.375,5.619-13.246,9.491c-9.447,9.447-14.65,22.008-14.65,35.369  c0,13.36,5.203,25.921,14.65,35.368s22.008,14.65,35.368,14.65c13.361,0,25.921-5.203,35.369-14.65  c3.872-3.871,7.064-8.328,9.491-13.246C97.826,68.608,97.611,67.309,96.76,66.458z"></path></svg></button></div><div class="center"><div class="page-header"><div class="popover-hint"><nav class="breadcrumb-container" aria-label="breadcrumbs"><div class="breadcrumb-element"><a href="../../../../">Home</a><p> ❯ </p></div><div class="breadcrumb-element"><a href="../../../../KB/">KB</a><p> ❯ </p></div><div class="breadcrumb-element"><a href="../../../../KB/AI/">AI</a><p> ❯ </p></div><div class="breadcrumb-element"><a href="../../../../KB/AI/Machine-Learning/">Machine Learning</a><p> ❯ </p></div><div class="breadcrumb-element"><a href="../../../../KB/AI/Machine-Learning/Explainability/">Explainability</a><p> ❯ </p></div><div class="breadcrumb-element"><a href>Contributions of Shape, Texture, and Color in Visual Recognition Abstract</a></div></nav><h1 class="article-title">Contributions of Shape, Texture, and Color in Visual Recognition Abstract</h1><p show-comma="true" class="content-meta"><span>Sep 18, 2024</span><span>8 min read</span></p><ul class="tags"><li><a href="../../../../tags/explainability" class="internal tag-link">explainability</a></li></ul></div></div><article class="popover-hint"><h1 id="contributions-of-shape-texture-and-color-in-visual-recognition-abstract">Contributions of Shape, Texture, and Color in Visual Recognition Abstract<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#contributions-of-shape-texture-and-color-in-visual-recognition-abstract" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h1>
<ul>
<li>/zotero</li>
<li>Yunhao Ge, Yao Xiao, Zhi Xu, Xingrui Wang, and Laurent Itti
<figure data-rehype-pretty-code-figure><pre tabindex="0" data-language="toc" data-theme="github-light github-dark"><code data-language="toc" data-theme="github-light github-dark" style="display:grid;"><span data-line> </span></code></pre></figure>
</li>
</ul>
<h2 id="abstract">Abstract<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#abstract" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<ul>
<li>[humanoid vision engine](humanoid vision engine.md) (HVE) that explicitly and separately computes shape, texture, and color features from images</li>
<li>resulting feature vectors are then concatenated to support the final classification</li>
<li>HVE can summarize and rankorder the contributions of the three features to object recognition.</li>
<li>We use human experiments to confirm that both HVE and humans predominantly use some specific features to support the classification of specific classes</li>
<li>To demonstrate more usefulness of HVE, we use it to simulate the open-world zeroshot learning ability of humans with no attribute labeling</li>
<li>Finally, we show that HVE can also simulate human imagination ability with the combination of different features.</li>
</ul>
<h2 id="introduction">Introduction<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#introduction" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<ul>
<li>A widely accepted intuition about the success of CNNs on perceptual tasks is that CNNs are the most predictive models for the human ventral stream object recognition</li>
<li>To understand which feature is more important for CNN-based recognition, recent paper shows promising results: ImageNet-trained CNNs are biased towards texture while increasing shape bias improves accuracy and robustness [33]</li>
<li>Here, inspired by HVS, we wish to find a general way to understand how shape, texture, and color contribute to a recognition task by pure data-driven learning.</li>
<li>It has been shown by neuroscientists that there are separate neural pathways to process these different visual features in primate</li>
<li>Among the many kinds of features crucial to visual recognition in humans, the shape property is the one that we primarily rely on in static object recognition [16]. Meanwhile, some previous studies show that surface-based cues also play a key role in our vision system</li>
<li>For example, [21] shows that scene recognition is faster for color images compared with grayscale ones</li>
<li>[Humanoid Vision Engine](Humanoid Vision Engine.md)</li>
</ul>
<h3 id="image-parsing-and-foreground-identification">Image Parsing and Foreground Identification.<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#image-parsing-and-foreground-identification" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<ul>
<li>we use the entity segmentation method [41] to simulate the process of parsing objects from a scene in our brain.</li>
<li>Entity segmentation is an open-world model and can segment the object from the image without labels.</li>
<li>This method aligns with human behavior, which can (at least in some cases; e.g., autostereograms [29]) segment an object without deciding what it is</li>
<li>After we get the segmentation of the image, we use a pre-trained CNN and <a href="../../../../KB/AI/Machine-Learning/Models/Grad-CAM" class="internal alias" data-slug="KB/AI/Machine-Learning/Models/Grad-CAM">Grad-CAM</a> [47] to find the foreground object among all masks.</li>
<li>We design three different feature extractors after identifying the foreground object segment: shape extractor, texture extractor, and color extractor, similar to the separate neural pathways in the human brain which focus on specific property</li>
</ul>
<h3 id="shape-feature-extractor">Shape Feature Extractor<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#shape-feature-extractor" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<ul>
<li>want to keep both 2D and 3D shape information while eliminating the information of texture and color</li>
<li>first use a 3D depth prediction model [44,43] to obtain the 3D depth information of the whole image</li>
<li>After element-wise multiplying the 3D depth estimation and 2D mask of the object, we obtain our shape feature</li>
<li>We can notice that this feature only contains 2D shape and 3D structural information (the 3D depth) and without color or texture information</li>
</ul>
<h3 id="texture-feature-extractor">Texture Feature Extractor<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#texture-feature-extractor" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<ul>
<li>want to keep both local and global texture information while eliminating shape and color information.</li>
<li>to remove the color information, we convert the RGB object segmentation to a grayscale image</li>
<li>cut this image into several square patches with an adaptive strategy (the patch size and location are adaptive with object sizes to cover more texture information)</li>
<li>If the overlap ratio between the patch and the original 2D object segment is larger than a threshold τ, we add that patch to a patch pool (we set τ to be 0.99 in our experiments, which means the over 99% of the area of the patch belongs to the object</li>
<li>Since we want to extract both local (one patch) and global (whole image) texture information, we randomly select 4 patches from the patch pool and concatenate them into a new texture image</li>
</ul>
<h3 id="color-feature-extractor">Color Feature Extractor<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#color-feature-extractor" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<ul>
<li>The first method is phase scrambling</li>
</ul>
<h3 id="phase-scrambling">Phase Scrambling<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#phase-scrambling" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<ul>
<li>transforms the image into the frequency domain using the fast Fourier transform (FFT)</li>
<li>In the frequency domain, the phase of the signal is then randomly scrambled, which destroys shape information while preserving color statistics</li>
<li>Then we use IFFT to transfer back to image space</li>
<li>We also used simple color histograms (see suppl.) as an alternative, but the results were not as good, hence we focus here on the phase scrambling approach for color representation.</li>
</ul>
<h3 id="humanoid-neural-network">Humanoid Neural Network<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#humanoid-neural-network" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<ul>
<li>After preprocessing, we have three features</li>
<li>To simulate the separate neural pathways in humans’ brains for different feature information [1,11], we design three feature representation encoders for shape, texture, and color, respectively</li>
<li>ResNet-18 [24] as the backbone for all feature encoders to project the three types of features to the corresponding well-separated embedding spaces.</li>
<li>hard to define the ground-truth label of the distance between features.</li>
<li>Given that the objects from the same class are relatively consistent in shape, texture, and color, the encoders can be trained in the classification problem independently instead, with the supervision of class labels.</li>
<li>fter training our encoders as classifiers, the feature map of the last convolutional layer will serve as the final feature representation</li>
<li>We also propose a gradient-based contribution attribution method to interpret the contributions of shape, texture, and color to the classification decision,</li>
<li>Take the shape feature as an example, given a prediction p and the probability of</li>
<li>class k, namely pk, we compute the gradient of pk with respect to the shape feature Vs</li>
<li>gradient as shape importance weights ↵sk</li>
<li>In other words, Ssk represents the “contribution” of shape feature to classifying this</li>
<li>image as class k</li>
</ul>
<h3 id="effectiveness-of-feature-encoders">Effectiveness of Feature Encoders<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#effectiveness-of-feature-encoders" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<ul>
<li>handcrafted three subsets of ImageNet</li>
<li>Shape-biased dataset containing 12 classes, where the classes were chosen which intuitively are strongly determined by shape</li>
<li>Texture-biased dataset uses 14 classes which we believed are more strongly determined by texture</li>
<li>Color-biased dataset includes 17 classes</li>
<li>After pre-processing the original images and getting their feature images, we input the feature images into feature encoders and get the T-SNE</li>
<li>Each row represents one feature-biased dataset and each column is bounded with one feature encoder, each image shows the results of one combination</li>
</ul>
<h3 id="effectiveness-of-humanoid-neural-network">Effectiveness of Humanoid Neural Network<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#effectiveness-of-humanoid-neural-network" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<ul>
<li>As these classifiers classify images based on corresponding feature representation, we call them feature nets.</li>
<li>If we combine these three feature nets with the interpretable aggregation module, the classification accuracy is very close to the upper bound, which means our vision system can classify images based on these three features almost as well as based on the full original color images.</li>
</ul>
<h2 id="more-humanoid-applications-with-hve-open-world-zero-shot-learning-with-hve">More Humanoid Applications with HVE Open-world Zero-shot Learning with HVE<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#more-humanoid-applications-with-hve-open-world-zero-shot-learning-with-hve" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<ul>
<li>Most current methods [37,32,13] need humans to provide detailed attribute labels for each image, which is costly in time and energy. However, given an image from an unseen class, humans can still describe it with their learned knowledge</li>
<li>First, to represent learnt knowledge, we use feature extractors</li>
<li>To retrieve learnt classes as description, we calculate the average distance dkm</li>
<li>between Iun and images of other class k in the latent space on feature m Open-world classification</li>
<li>To further predict the actual class of Iun based on the feature-wise description, we use ConceptNet as common knowledge to conduct reasoning</li>
<li>We form a reasoning root pool R⇤ consisting of feature roots Rs, Rt, Rc obtained during image description, and shared attribute roots Ras , Rat , Rac . The reasoning roots will be our evidence for reasoning</li>
<li>We humans can intuitively imagine an object when seeing one aspect of a feature, especially when this feature is prototypical (contribute most to classification)</li>
<li>For instance, we can imagine a zebra when seeing its stripe (texture). This process is similar but harder than the classical image generation task since the input features <a href="../../../../KB/Modality" class="internal alias" data-slug="KB/Modality">Modality</a> here dynamic which can be any feature among shape, texture, or color</li>
</ul>
<h2 id="cross-feature-retrieval">Cross Feature Retrieval<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#cross-feature-retrieval" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<ul>
<li>In order to reasonably retrieve the most possible other two corresponding features given only one feature (among shape, texture, or color), we learn a feature agnostic encoder that projects the three features into one same feature space and makes sure that the features belonging to the same class are in the nearby regions.</li>
<li>In the retrieval process, given any feature of any object, we can map it into the cross feature embedding space by the corresponding encoder net and the feature agnostic net</li>
<li>Then we apply the 2 norm to find the other two features closest to the input one as output. The output is correct if they belong to the same class as the input.</li>
</ul>
<h2 id="cross-feature-imagination">Cross Feature Imagination<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#cross-feature-imagination" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<ul>
<li>To stimulate imagination, we propose a crossfeature imagination model to generate a plausible final image with the input and retrieved features</li>
<li>Inspired by the pixel2pixel GAN[26] and AdaIN[25] in the style transfer, we design a crossfeature pixel2pixel GAN model to generate the final image.</li>
</ul>
<h2 id="pictures">Pictures<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#pictures" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<ul>
<li><img src="../../../.././images/Pasted-image-20221105123820.png" alt/></li>
<li><img src="../../../.././images/Pasted-image-20221105123830.png" alt/></li>
<li><img src="../../../.././images/Pasted-image-20221105123841.png" alt/></li>
<li><img src="../../../.././images/Pasted-image-20221105123855.png" alt/></li>
<li><img src="../../../.././images/Pasted-image-20221105123905.png" alt/></li>
<li><img src="../../../.././images/Pasted-image-20221105123919.png" alt/></li>
<li><img src="../../../.././images/Pasted-image-20221105123931.png" alt/></li>
<li><img src="../../../.././images/Pasted-image-20221105123951.png" alt/></li>
</ul></article><hr/><div class="page-footer"></div></div><div class="right sidebar"><div class="graph"><h3>Graph View</h3><div class="graph-outer"><div id="graph-container" data-cfg="{&quot;drag&quot;:true,&quot;zoom&quot;:true,&quot;depth&quot;:1,&quot;scale&quot;:1.1,&quot;repelForce&quot;:0.5,&quot;centerForce&quot;:0.3,&quot;linkDistance&quot;:30,&quot;fontSize&quot;:0.6,&quot;opacityScale&quot;:1,&quot;showTags&quot;:true,&quot;removeTags&quot;:[],&quot;focusOnHover&quot;:false}"></div><button id="global-graph-icon" aria-label="Global Graph"><svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 55 55" fill="currentColor" xml:space="preserve"><path d="M49,0c-3.309,0-6,2.691-6,6c0,1.035,0.263,2.009,0.726,2.86l-9.829,9.829C32.542,17.634,30.846,17,29,17
                s-3.542,0.634-4.898,1.688l-7.669-7.669C16.785,10.424,17,9.74,17,9c0-2.206-1.794-4-4-4S9,6.794,9,9s1.794,4,4,4
                c0.74,0,1.424-0.215,2.019-0.567l7.669,7.669C21.634,21.458,21,23.154,21,25s0.634,3.542,1.688,4.897L10.024,42.562
                C8.958,41.595,7.549,41,6,41c-3.309,0-6,2.691-6,6s2.691,6,6,6s6-2.691,6-6c0-1.035-0.263-2.009-0.726-2.86l12.829-12.829
                c1.106,0.86,2.44,1.436,3.898,1.619v10.16c-2.833,0.478-5,2.942-5,5.91c0,3.309,2.691,6,6,6s6-2.691,6-6c0-2.967-2.167-5.431-5-5.91
                v-10.16c1.458-0.183,2.792-0.759,3.898-1.619l7.669,7.669C41.215,39.576,41,40.26,41,41c0,2.206,1.794,4,4,4s4-1.794,4-4
                s-1.794-4-4-4c-0.74,0-1.424,0.215-2.019,0.567l-7.669-7.669C36.366,28.542,37,26.846,37,25s-0.634-3.542-1.688-4.897l9.665-9.665
                C46.042,11.405,47.451,12,49,12c3.309,0,6-2.691,6-6S52.309,0,49,0z M11,9c0-1.103,0.897-2,2-2s2,0.897,2,2s-0.897,2-2,2
                S11,10.103,11,9z M6,51c-2.206,0-4-1.794-4-4s1.794-4,4-4s4,1.794,4,4S8.206,51,6,51z M33,49c0,2.206-1.794,4-4,4s-4-1.794-4-4
                s1.794-4,4-4S33,46.794,33,49z M29,31c-3.309,0-6-2.691-6-6s2.691-6,6-6s6,2.691,6,6S32.309,31,29,31z M47,41c0,1.103-0.897,2-2,2
                s-2-0.897-2-2s0.897-2,2-2S47,39.897,47,41z M49,10c-2.206,0-4-1.794-4-4s1.794-4,4-4s4,1.794,4,4S51.206,10,49,10z"></path></svg></button></div><div id="global-graph-outer"><div id="global-graph-container" data-cfg="{&quot;drag&quot;:true,&quot;zoom&quot;:true,&quot;depth&quot;:-1,&quot;scale&quot;:0.9,&quot;repelForce&quot;:0.5,&quot;centerForce&quot;:0.3,&quot;linkDistance&quot;:30,&quot;fontSize&quot;:0.6,&quot;opacityScale&quot;:1,&quot;showTags&quot;:true,&quot;removeTags&quot;:[],&quot;focusOnHover&quot;:true}"></div></div></div><div class="toc desktop-only"><button type="button" id="toc" class aria-controls="toc-content" aria-expanded="true"><h3>Table of Contents</h3><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="fold"><polyline points="6 9 12 15 18 9"></polyline></svg></button><div id="toc-content" class><ul class="overflow"><li class="depth-0"><a href="#contributions-of-shape-texture-and-color-in-visual-recognition-abstract" data-for="contributions-of-shape-texture-and-color-in-visual-recognition-abstract">Contributions of Shape, Texture, and Color in Visual Recognition Abstract</a></li><li class="depth-1"><a href="#abstract" data-for="abstract">Abstract</a></li><li class="depth-1"><a href="#introduction" data-for="introduction">Introduction</a></li><li class="depth-2"><a href="#image-parsing-and-foreground-identification" data-for="image-parsing-and-foreground-identification">Image Parsing and Foreground Identification.</a></li><li class="depth-2"><a href="#shape-feature-extractor" data-for="shape-feature-extractor">Shape Feature Extractor</a></li><li class="depth-2"><a href="#texture-feature-extractor" data-for="texture-feature-extractor">Texture Feature Extractor</a></li><li class="depth-2"><a href="#color-feature-extractor" data-for="color-feature-extractor">Color Feature Extractor</a></li><li class="depth-2"><a href="#phase-scrambling" data-for="phase-scrambling">Phase Scrambling</a></li><li class="depth-2"><a href="#humanoid-neural-network" data-for="humanoid-neural-network">Humanoid Neural Network</a></li><li class="depth-2"><a href="#effectiveness-of-feature-encoders" data-for="effectiveness-of-feature-encoders">Effectiveness of Feature Encoders</a></li><li class="depth-2"><a href="#effectiveness-of-humanoid-neural-network" data-for="effectiveness-of-humanoid-neural-network">Effectiveness of Humanoid Neural Network</a></li><li class="depth-1"><a href="#more-humanoid-applications-with-hve-open-world-zero-shot-learning-with-hve" data-for="more-humanoid-applications-with-hve-open-world-zero-shot-learning-with-hve">More Humanoid Applications with HVE Open-world Zero-shot Learning with HVE</a></li><li class="depth-1"><a href="#cross-feature-retrieval" data-for="cross-feature-retrieval">Cross Feature Retrieval</a></li><li class="depth-1"><a href="#cross-feature-imagination" data-for="cross-feature-imagination">Cross Feature Imagination</a></li><li class="depth-1"><a href="#pictures" data-for="pictures">Pictures</a></li></ul></div></div><div class="backlinks"><h3>Backlinks</h3><ul class="overflow"><li><a href="../../../../KB/AI/Machine-Learning/Explainability/_Index_of_Explainability" class="internal">_Index_of_Explainability</a></li></ul></div></div></div><footer class><p>Created with <a href="https://quartz.jzhao.xyz/">Quartz v4.3.1</a> © 2025</p><ul><li><a href="https://github.com/SubhadityaMukherjee/AI-knowledge-base/">GitHub</a></li></ul></footer></div></body><script type="application/javascript">function c(){let t=this.parentElement;t.classList.toggle("is-collapsed");let l=t.classList.contains("is-collapsed")?this.scrollHeight:t.scrollHeight;t.style.maxHeight=l+"px";let o=t,e=t.parentElement;for(;e;){if(!e.classList.contains("callout"))return;let n=e.classList.contains("is-collapsed")?e.scrollHeight:e.scrollHeight+o.scrollHeight;e.style.maxHeight=n+"px",o=e,e=e.parentElement}}function i(){let t=document.getElementsByClassName("callout is-collapsible");for(let s of t){let l=s.firstElementChild;if(l){l.addEventListener("click",c),window.addCleanup(()=>l.removeEventListener("click",c));let e=s.classList.contains("is-collapsed")?l.scrollHeight:s.scrollHeight;s.style.maxHeight=e+"px"}}}document.addEventListener("nav",i);window.addEventListener("resize",i);
</script><script type="module">
          let mermaidImport = undefined
          document.addEventListener('nav', async () => {
            if (document.querySelector("code.mermaid")) {
              mermaidImport ||= await import('https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.7.0/mermaid.esm.min.mjs')
              const mermaid = mermaidImport.default
              const darkMode = document.documentElement.getAttribute('saved-theme') === 'dark'
              mermaid.initialize({
                startOnLoad: false,
                securityLevel: 'loose',
                theme: darkMode ? 'dark' : 'default'
              })

              await mermaid.run({
                querySelector: '.mermaid'
              })
            }
          });
          </script><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/contrib/copy-tex.min.js" type="application/javascript"></script><script type="application/javascript">
        const socket = new WebSocket('ws://localhost:3001')
        // reload(true) ensures resources like images and scripts are fetched again in firefox
        socket.addEventListener('message', () => document.location.reload(true))
      </script><script src="../../../../postscript.js" type="module"></script></html>