---
title: Additive Attention

tags: architecture 
date modified: Thursday, August 11th 2022, 12:32:57 am
date created: Tuesday, July 26th 2022, 8:33:15 pm
---

# Additive [[Attention]]
- [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf)
- Uses a one layer feedforward network to calculate [[Attention Alignment]]
- Oh, basically it is the same as [[Bahdanau Attention]]

## Backlinks

> - [Bahdanau [[Attention]]](Bahdanau Attention.md)
>   - Same as [[Additive Attention]]
>    
> - [Multiplicative [[Attention]]](Multiplicative Attention.md)
>   - Since [[Additive Attention]] performs better for scale, use a factor [[Scaled Dot Product Attention]]
>    
> - [Attention](Attention.md)
>   - [[Additive Attention]]

_Backlinks last generated 2022-10-04 13:01:19_
