---
title: Interpreting Attention

tags: architecture 
---

# Interpreting [Attention](Attention.md)
- [Attention Interpretability Across NLP Tasks](https://arxiv.org/abs/1909.11218)
- empirically prove the hypothesis that [attention](Attention.md) weights are interpretable and are correlated with feature importance measures
- n both single and pair sequence tasks, the [attention](Attention.md) weights in samples with original weights do make sense in general
- However, in the former case, the [attention](Attention.md) mechanism learns to give higher weights to tokens relevant to both kinds of sentiment.
- They show that [attention](Attention.md) weights in single sequence tasks do not provide a reason for the prediction, which in the case of pairwise tasks, [attention](Attention.md) do reflect the reasoning behind model output
- [BertViz](https://github.com/jessevig/bertviz) repo
















