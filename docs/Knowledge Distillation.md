---
title: Knowledge Distillation
---

# Knowledge Distillation
- Teacher model to help train the student model
- Teacher is often pre trained
- Student tries to imitate teacher
- [[Distillation Loss]]






































































