---
title: TinyBERT

tags: architecture 
---

# TinyBERT
- [TinyBERT: Distilling BERT for Natural Language Understanding](https://arxiv.org/abs/1909.10351)
- novel Transformer distillation method to accelerate inference and reduce model size while maintaining accuracy
- specially designed for knowledge distillation (KD) of the Transformer-based models
- plenty of knowledge encoded in a large teacher BERT can be effectively transferred to a small student Tiny-BERT
- GLUE


## Backlinks

> - [](journals/2022-06-27.md)
>   - [[TinyBERT]]

_Backlinks last generated 2022-06-27 22:06:24_
