---
title: FLASH

tags: architecture 
---

# FLASH
- [Transformer Quality in Linear Time](https://arxiv.org/abs/2202.10447)
- weaknesses in handling long sequences
- FLASH
- performant layer (gated linear unit) and by combining it with an accelerator-efficient approximation strategy (mixed chunk attention)
- [[GAU]]
- [[Mixed chunk attention]] 
-
- outperforms three baselines: vanilla Transformer, Performer and Combiner in terms of quality and efficiency
- Wiki
- PG-19


## Backlinks

> - [](journals/2022-06-27.md)
>   - [[FLASH]]

_Backlinks last generated 2022-06-27 22:06:24_
