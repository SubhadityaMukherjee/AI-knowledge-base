---
title: Chapter 9 - Regularization
toc: true
tags:
  - regularization
  - normalization
date modified: Wednesday 18th September 2024, Wed
date created: Wednesday 18th September 2024, Wed
---

# Chapter 9 - Regularization
```toc
```
> Skeptic : [[Mixup]]
> Skeptic: How does this tie into [[Optimizers]]
> Skeptic : [[Label Smoothing]] : Mentioned later
> [[Batch Normalization]] - The idea is to normalize the inputs of each layer in such a way that, they have a mean activation output zero and a unit standard deviation.
> Skeptic : [[AdamW]]

- [[Regularization]]
## Explicit Regularization
- [[Regularization Term]]
- From [[Maximum Likelihood]], 
	- ![[Pasted image 20240918104924.webp]]
- ![[Pasted image 20240918104935.webp]]
- from [[Negative Log Likelihood]]

## [[Lp Regularization]]

## [[Gradient Descent]]

## [[SGD]]

## Improving Performance
- [[Early Stopping tricks]]
- Ensembling models
- [[Dropout]]
- [[Adversarial Learning]]
- [[Bayesian]]
- [[Transfer Learning]]
- [[Multi Task Learning]]
- [[../../Tag Pages/augmentation.md|augmentation]]
- [[Label Smoothing]]
- [[Self Supervised]]