<!DOCTYPE html>
<html lang="en"><head><title>PromptIR</title><meta charset="utf-8"/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com"/><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=IBM Plex Mono&amp;family=Schibsted Grotesk:wght@400;700&amp;family=Source Sans Pro:ital,wght@0,400;0,600;1,400;1,600&amp;display=swap"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:title" content="PromptIR"/><meta property="og:description" content="PromptIR Summary : Summary : Uses a transformer network to prediction a “prompt” that says how degraded an image is. And based on that, decides what module to use."/><meta property="og:image" content="https://https://subhadityamukherjee.github.io/AI-knowledge-base//static/og-image.png"/><meta property="og:width" content="1200"/><meta property="og:height" content="675"/><link rel="icon" href="../../../../static/icon.png"/><meta name="description" content="PromptIR Summary : Summary : Uses a transformer network to prediction a “prompt” that says how degraded an image is. And based on that, decides what module to use."/><meta name="generator" content="Quartz"/><link href="../../../../index.css" rel="stylesheet" type="text/css" spa-preserve/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css" rel="stylesheet" type="text/css" spa-preserve/><script src="../../../../prescript.js" type="application/javascript" spa-preserve></script><script type="application/javascript" spa-preserve>const fetchData = fetch("../../../../static/contentIndex.json").then(data => data.json())</script></head><body data-slug="KB/AI/Machine-Learning/Explainability/PromptIR"><div id="quartz-root" class="page"><div id="quartz-body"><div class="left sidebar"><h2 class="page-title"><a href="../../../..">Subhaditya's KB</a></h2><div class="spacer mobile-only"></div><div class="search"><button class="search-button" id="search-button"><p>Search</p><svg role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title>Search</title><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"></path><circle cx="8" cy="8" r="7"></circle></g></svg></button><div id="search-container"><div id="search-space"><input autocomplete="off" id="search-bar" name="search" type="text" aria-label="Search for something" placeholder="Search for something"/><div id="search-layout" data-preview="true"></div></div></div></div><button class="darkmode" id="darkmode"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" id="dayIcon" x="0px" y="0px" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35" xml:space="preserve" aria-label="Dark mode"><title>Dark mode</title><path d="M6,17.5C6,16.672,5.328,16,4.5,16h-3C0.672,16,0,16.672,0,17.5    S0.672,19,1.5,19h3C5.328,19,6,18.328,6,17.5z M7.5,26c-0.414,0-0.789,0.168-1.061,0.439l-2,2C4.168,28.711,4,29.086,4,29.5    C4,30.328,4.671,31,5.5,31c0.414,0,0.789-0.168,1.06-0.44l2-2C8.832,28.289,9,27.914,9,27.5C9,26.672,8.329,26,7.5,26z M17.5,6    C18.329,6,19,5.328,19,4.5v-3C19,0.672,18.329,0,17.5,0S16,0.672,16,1.5v3C16,5.328,16.671,6,17.5,6z M27.5,9    c0.414,0,0.789-0.168,1.06-0.439l2-2C30.832,6.289,31,5.914,31,5.5C31,4.672,30.329,4,29.5,4c-0.414,0-0.789,0.168-1.061,0.44    l-2,2C26.168,6.711,26,7.086,26,7.5C26,8.328,26.671,9,27.5,9z M6.439,8.561C6.711,8.832,7.086,9,7.5,9C8.328,9,9,8.328,9,7.5    c0-0.414-0.168-0.789-0.439-1.061l-2-2C6.289,4.168,5.914,4,5.5,4C4.672,4,4,4.672,4,5.5c0,0.414,0.168,0.789,0.439,1.06    L6.439,8.561z M33.5,16h-3c-0.828,0-1.5,0.672-1.5,1.5s0.672,1.5,1.5,1.5h3c0.828,0,1.5-0.672,1.5-1.5S34.328,16,33.5,16z     M28.561,26.439C28.289,26.168,27.914,26,27.5,26c-0.828,0-1.5,0.672-1.5,1.5c0,0.414,0.168,0.789,0.439,1.06l2,2    C28.711,30.832,29.086,31,29.5,31c0.828,0,1.5-0.672,1.5-1.5c0-0.414-0.168-0.789-0.439-1.061L28.561,26.439z M17.5,29    c-0.829,0-1.5,0.672-1.5,1.5v3c0,0.828,0.671,1.5,1.5,1.5s1.5-0.672,1.5-1.5v-3C19,29.672,18.329,29,17.5,29z M17.5,7    C11.71,7,7,11.71,7,17.5S11.71,28,17.5,28S28,23.29,28,17.5S23.29,7,17.5,7z M17.5,25c-4.136,0-7.5-3.364-7.5-7.5    c0-4.136,3.364-7.5,7.5-7.5c4.136,0,7.5,3.364,7.5,7.5C25,21.636,21.636,25,17.5,25z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" id="nightIcon" x="0px" y="0px" viewBox="0 0 100 100" style="enable-background:new 0 0 100 100" xml:space="preserve" aria-label="Light mode"><title>Light mode</title><path d="M96.76,66.458c-0.853-0.852-2.15-1.064-3.23-0.534c-6.063,2.991-12.858,4.571-19.655,4.571  C62.022,70.495,50.88,65.88,42.5,57.5C29.043,44.043,25.658,23.536,34.076,6.47c0.532-1.08,0.318-2.379-0.534-3.23  c-0.851-0.852-2.15-1.064-3.23-0.534c-4.918,2.427-9.375,5.619-13.246,9.491c-9.447,9.447-14.65,22.008-14.65,35.369  c0,13.36,5.203,25.921,14.65,35.368s22.008,14.65,35.368,14.65c13.361,0,25.921-5.203,35.369-14.65  c3.872-3.871,7.064-8.328,9.491-13.246C97.826,68.608,97.611,67.309,96.76,66.458z"></path></svg></button></div><div class="center"><div class="page-header"><div class="popover-hint"><nav class="breadcrumb-container" aria-label="breadcrumbs"><div class="breadcrumb-element"><a href="../../../../">Home</a><p> ❯ </p></div><div class="breadcrumb-element"><a href="../../../../KB/">KB</a><p> ❯ </p></div><div class="breadcrumb-element"><a href="../../../../KB/AI/">AI</a><p> ❯ </p></div><div class="breadcrumb-element"><a href="../../../../KB/AI/Machine-Learning/">Machine Learning</a><p> ❯ </p></div><div class="breadcrumb-element"><a href="../../../../KB/AI/Machine-Learning/Explainability/">Explainability</a><p> ❯ </p></div><div class="breadcrumb-element"><a href>PromptIR</a></div></nav><h1 class="article-title">PromptIR</h1><p show-comma="true" class="content-meta"><span>Sep 18, 2024</span><span>5 min read</span></p><ul class="tags"><li><a href="../../../../tags/explainability" class="internal tag-link">explainability</a></li></ul></div></div><article class="popover-hint"><h1 id="promptir">PromptIR<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#promptir" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h1>
<ul>
<li><em>Summary</em> : Summary : Uses a transformer network to prediction a “prompt” that says how degraded an image is. And based on that, decides what module to use.</li>
<li>Deep learning-based methods have significantly improved image restoration performance, however, they have limited generalization ability to different degradation types and levels</li>
<li>requires training individual models for each specific degradation and knowing the input degradation type to apply the relevant model.</li>
<li>prompt-based learning approach, PromptIR, for All-In-One image restoration that can effectively restore images from various types and levels of degradation</li>
<li>uses prompts to encode degradation-specific information</li>
<li>generic and efficient plugin module with few lightweight prompts that can be used to restore images of various types and levels of degradation with no prior information on the corruptions present in the image</li>
<li><img src="../../../.././images/img_p1_1.png" alt/></li>
<li>AirNet</li>
<li>addresses the all-in-one restoration task by employing the contrastive learning paradigm.</li>
<li>involves training an extra encoder to differentiate various types of image degradations</li>
<li>Although AirNet [29] yields state-of-the-art results, it struggles to model fully disentangled representations of different corruption types</li>
<li>Furthermore, the usage of an additional encoder for contrastive learning leads to a higher training burden due to the two-stage training approach.</li>
</ul>
<h2 id="method">Method<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#method" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<ul>
<li>
<h1 id="aim-to-learn-a-single-model-m-to-restore-an-image-i-from-a-degraded-image-i-that-has-been-degraded-using-a-degradation-d-while-having-no-prior-information-about-d">aim to learn a single model M to restore an image I from a degraded image I, that has been degraded using a degradation D, while having no prior information about D.<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#aim-to-learn-a-single-model-m-to-restore-an-image-i-from-a-degraded-image-i-that-has-been-degraded-using-a-degradation-d-while-having-no-prior-information-about-d" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h1>
</li>
<li>While the model is initially “blind” to the nature of degradation, its performance in recovering a clean image can be enhanced by providing implicit contextual information about the type of degradation</li>
<li><img src="../../../.././images/img_p2_1.png" alt/></li>
<li>From a given degraded input image I ∈ RH ×W ×3</li>
<li>first extracts low-level features F0 ∈ RH×W×C by applying a convolution operation; where H × W is the spatial resolution and C denotes the channels.</li>
<li>eature embeddings F0 undergo a 4-level hierarchical encoder-decoder, transforming into deep features Fr ∈ RH ×W ×2C</li>
<li>
<h1 id="each-level-of-the-encoder-decoder-employs-several-transformer-blocks-with-the-number-of-blocks-gradually-increasing-from-the-top-level-to-the-bottom-level-to-maintain-computational-efficiency">Each level of the encoder-decoder employs several Transformer blocks, with the number of blocks gradually increasing from the top level to the bottom level to maintain computational efficiency.<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#each-level-of-the-encoder-decoder-employs-several-transformer-blocks-with-the-number-of-blocks-gradually-increasing-from-the-top-level-to-the-bottom-level-to-maintain-computational-efficiency" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h1>
</li>
<li>Starting from the high-resolution input, the goal of the encoder is to progressively reduce the spatial resolution while .</li>
<li>From the low-resolution latent features Fl, the aim of the decoder is to gradually recover the highresolution clean output</li>
<li>ncorporate prompt block</li>
<li>Prompt blocks are adapter modules that sequentially connect every two levels of the decoder.</li>
</ul>
<h2 id="prompt-block">Prompt Block<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#prompt-block" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<ul>
<li><img src="../../../.././images/img_p3_1.png" alt/></li>
</ul>
<h2 id="prompt-generation-module">Prompt Generation Module<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#prompt-generation-module" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<ul>
<li>Prompt components Pc form a set of learnable parameters that interact with the incoming features to embed degradation information</li>
<li>features-prompt interaction is to directly use the learned prompts to calibrate the features.</li>
<li>
<h1 id="dynamically-predicts-attention-based-weights-from-the-input-features-and-apply-them-to-prompt-components-to-yield-input-conditioned-prompts-p">dynamically predicts attention-based weights from the input features and apply them to prompt components to yield input-conditioned prompts P<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#dynamically-predicts-attention-based-weights-from-the-input-features-and-apply-them-to-prompt-components-to-yield-input-conditioned-prompts-p" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h1>
</li>
<li>shared space to facilitate correlated knowledge sharing among prompt components.</li>
<li>To generate prompt-weights from the input features Fl</li>
<li>first applies global average pooling (GAP) across spatial dimension to generate feature vector v 2 RCˆ</li>
<li>pass v through a channeldownscaling convolution layer to obtain a compact feature vector, followed by the softmax operation, thus yielding prompt-weights w 2 RN</li>
<li>use these weights to make adjustments in prompt components, followed by a 3 x 3 convolution layer</li>
<li><img src="../../../.././images/img_p4_1.png" alt/></li>
<li>Since at inference time, it is necessary for the restoration network to be able to handle images of different resolutions, we cannot use the prompt components Pc with a fixed size.</li>
<li>bilinear upsampling operation to upscale the prompt components</li>
</ul>
<h2 id="prompt-interaction-module">Prompt Interaction Module<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#prompt-interaction-module" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<ul>
<li>enable interaction between the input features Fl and prompts P for a guided restoration.</li>
<li>
<h1 id="in-pim-we-concatenate-the-generated-prompts-with-the-input-features-along-the-channel-dimension">In PIM, we concatenate the generated prompts with the input features along the channel dimension.<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#in-pim-we-concatenate-the-generated-prompts-with-the-input-features-along-the-channel-dimension" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h1>
</li>
<li>pass the concatenated representations through a Transformer block that exploits degradation information encoded in the prompts and transforms the input features.</li>
<li>The Transformer block is composed of two sequentially connected sub-modules: Multi-Dconv head transposed attention (MDTA), and Gated-Dconv feedforward network (GDFN). MDTA applies self-attention operation across channels rather than the spatial dimension and has linear complexity.</li>
<li>The goal of GDFN is to transform features in a controlled manner, i.e., suppressing the less informative features and allowing only useful ones to propagate through the network</li>
<li><img src="../../../.././images/img_p5_1.png" alt/></li>
<li><img src="../../../../img_p5_2.png" alt="img_p5_2"/></li>
</ul>
<h2 id></h2>
<ul>
<li><img src="../../../.././images/img_p6_1.png" alt/></li>
<li>Implementation Details</li>
<li>end-to-end trainable and requires no pretraining of any individual component</li>
<li>4-level encoder-decoder, with varying numbers of Transformer blocks at each level, specifically [4, 6, 6, 8] from level-1 to level-4.</li>
<li>one prompt block between every two consecutive decoder levels, totaling 3 prompt blocks in the overall PromptIR network</li>
<li>The total number of prompt components are 5</li>
<li>The model is trained with a batch size of 32 in the all-in-one setting, and with a batch of 8 in the single-task setting</li>
<li>The network is optimized with an L1 loss, and we use Adam optimizer (1 = 0.9, 2 = 0.999) with learning rate 2e 4 for 200 epochs.</li>
<li>
<h1 id="cropped-patches-of-size-128-x-128">cropped patches of size 128 x 128<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#cropped-patches-of-size-128-x-128" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h1>
</li>
<li>BSD400</li>
<li>WED</li>
<li>o</li>
<li>BSD68</li>
<li>Urban100</li>
<li>Rain100L</li>
<li>SOTS</li>
<li><img src="../../../.././images/img_p8_1.png" alt/></li>
<li><img src="../../../.././images/img_p8_2.png" alt/></li>
<li><img src="../../../.././images/img_p9_1.png" alt/></li>
</ul></article><hr/><div class="page-footer"></div></div><div class="right sidebar"><div class="graph"><h3>Graph View</h3><div class="graph-outer"><div id="graph-container" data-cfg="{&quot;drag&quot;:true,&quot;zoom&quot;:true,&quot;depth&quot;:1,&quot;scale&quot;:1.1,&quot;repelForce&quot;:0.5,&quot;centerForce&quot;:0.3,&quot;linkDistance&quot;:30,&quot;fontSize&quot;:0.6,&quot;opacityScale&quot;:1,&quot;showTags&quot;:true,&quot;removeTags&quot;:[],&quot;focusOnHover&quot;:false}"></div><button id="global-graph-icon" aria-label="Global Graph"><svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 55 55" fill="currentColor" xml:space="preserve"><path d="M49,0c-3.309,0-6,2.691-6,6c0,1.035,0.263,2.009,0.726,2.86l-9.829,9.829C32.542,17.634,30.846,17,29,17
                s-3.542,0.634-4.898,1.688l-7.669-7.669C16.785,10.424,17,9.74,17,9c0-2.206-1.794-4-4-4S9,6.794,9,9s1.794,4,4,4
                c0.74,0,1.424-0.215,2.019-0.567l7.669,7.669C21.634,21.458,21,23.154,21,25s0.634,3.542,1.688,4.897L10.024,42.562
                C8.958,41.595,7.549,41,6,41c-3.309,0-6,2.691-6,6s2.691,6,6,6s6-2.691,6-6c0-1.035-0.263-2.009-0.726-2.86l12.829-12.829
                c1.106,0.86,2.44,1.436,3.898,1.619v10.16c-2.833,0.478-5,2.942-5,5.91c0,3.309,2.691,6,6,6s6-2.691,6-6c0-2.967-2.167-5.431-5-5.91
                v-10.16c1.458-0.183,2.792-0.759,3.898-1.619l7.669,7.669C41.215,39.576,41,40.26,41,41c0,2.206,1.794,4,4,4s4-1.794,4-4
                s-1.794-4-4-4c-0.74,0-1.424,0.215-2.019,0.567l-7.669-7.669C36.366,28.542,37,26.846,37,25s-0.634-3.542-1.688-4.897l9.665-9.665
                C46.042,11.405,47.451,12,49,12c3.309,0,6-2.691,6-6S52.309,0,49,0z M11,9c0-1.103,0.897-2,2-2s2,0.897,2,2s-0.897,2-2,2
                S11,10.103,11,9z M6,51c-2.206,0-4-1.794-4-4s1.794-4,4-4s4,1.794,4,4S8.206,51,6,51z M33,49c0,2.206-1.794,4-4,4s-4-1.794-4-4
                s1.794-4,4-4S33,46.794,33,49z M29,31c-3.309,0-6-2.691-6-6s2.691-6,6-6s6,2.691,6,6S32.309,31,29,31z M47,41c0,1.103-0.897,2-2,2
                s-2-0.897-2-2s0.897-2,2-2S47,39.897,47,41z M49,10c-2.206,0-4-1.794-4-4s1.794-4,4-4s4,1.794,4,4S51.206,10,49,10z"></path></svg></button></div><div id="global-graph-outer"><div id="global-graph-container" data-cfg="{&quot;drag&quot;:true,&quot;zoom&quot;:true,&quot;depth&quot;:-1,&quot;scale&quot;:0.9,&quot;repelForce&quot;:0.5,&quot;centerForce&quot;:0.3,&quot;linkDistance&quot;:30,&quot;fontSize&quot;:0.6,&quot;opacityScale&quot;:1,&quot;showTags&quot;:true,&quot;removeTags&quot;:[],&quot;focusOnHover&quot;:true}"></div></div></div><div class="toc desktop-only"><button type="button" id="toc" class aria-controls="toc-content" aria-expanded="true"><h3>Table of Contents</h3><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="fold"><polyline points="6 9 12 15 18 9"></polyline></svg></button><div id="toc-content" class><ul class="overflow"><li class="depth-0"><a href="#promptir" data-for="promptir">PromptIR</a></li><li class="depth-1"><a href="#method" data-for="method">Method</a></li><li class="depth-0"><a href="#aim-to-learn-a-single-model-m-to-restore-an-image-i-from-a-degraded-image-i-that-has-been-degraded-using-a-degradation-d-while-having-no-prior-information-about-d" data-for="aim-to-learn-a-single-model-m-to-restore-an-image-i-from-a-degraded-image-i-that-has-been-degraded-using-a-degradation-d-while-having-no-prior-information-about-d">aim to learn a single model M to restore an image I from a degraded image I, that has been degraded using a degradation D, while having no prior information about D.</a></li><li class="depth-0"><a href="#each-level-of-the-encoder-decoder-employs-several-transformer-blocks-with-the-number-of-blocks-gradually-increasing-from-the-top-level-to-the-bottom-level-to-maintain-computational-efficiency" data-for="each-level-of-the-encoder-decoder-employs-several-transformer-blocks-with-the-number-of-blocks-gradually-increasing-from-the-top-level-to-the-bottom-level-to-maintain-computational-efficiency">Each level of the encoder-decoder employs several Transformer blocks, with the number of blocks gradually increasing from the top level to the bottom level to maintain computational efficiency.</a></li><li class="depth-1"><a href="#prompt-block" data-for="prompt-block">Prompt Block</a></li><li class="depth-1"><a href="#prompt-generation-module" data-for="prompt-generation-module">Prompt Generation Module</a></li><li class="depth-0"><a href="#dynamically-predicts-attention-based-weights-from-the-input-features-and-apply-them-to-prompt-components-to-yield-input-conditioned-prompts-p" data-for="dynamically-predicts-attention-based-weights-from-the-input-features-and-apply-them-to-prompt-components-to-yield-input-conditioned-prompts-p">dynamically predicts attention-based weights from the input features and apply them to prompt components to yield input-conditioned prompts P</a></li><li class="depth-1"><a href="#prompt-interaction-module" data-for="prompt-interaction-module">Prompt Interaction Module</a></li><li class="depth-0"><a href="#in-pim-we-concatenate-the-generated-prompts-with-the-input-features-along-the-channel-dimension" data-for="in-pim-we-concatenate-the-generated-prompts-with-the-input-features-along-the-channel-dimension">In PIM, we concatenate the generated prompts with the input features along the channel dimension.</a></li><li class="depth-1"><a href="#" data-for></a></li><li class="depth-0"><a href="#cropped-patches-of-size-128-x-128" data-for="cropped-patches-of-size-128-x-128">cropped patches of size 128 x 128</a></li></ul></div></div><div class="backlinks"><h3>Backlinks</h3><ul class="overflow"><li><a href="../../../../KB/AI/Machine-Learning/Explainability/_Index_of_Explainability" class="internal">_Index_of_Explainability</a></li></ul></div></div></div><footer class><p>Created with <a href="https://quartz.jzhao.xyz/">Quartz v4.3.1</a> © 2025</p><ul><li><a href="https://github.com/SubhadityaMukherjee/AI-knowledge-base/">GitHub</a></li></ul></footer></div></body><script type="application/javascript">function c(){let t=this.parentElement;t.classList.toggle("is-collapsed");let l=t.classList.contains("is-collapsed")?this.scrollHeight:t.scrollHeight;t.style.maxHeight=l+"px";let o=t,e=t.parentElement;for(;e;){if(!e.classList.contains("callout"))return;let n=e.classList.contains("is-collapsed")?e.scrollHeight:e.scrollHeight+o.scrollHeight;e.style.maxHeight=n+"px",o=e,e=e.parentElement}}function i(){let t=document.getElementsByClassName("callout is-collapsible");for(let s of t){let l=s.firstElementChild;if(l){l.addEventListener("click",c),window.addCleanup(()=>l.removeEventListener("click",c));let e=s.classList.contains("is-collapsed")?l.scrollHeight:s.scrollHeight;s.style.maxHeight=e+"px"}}}document.addEventListener("nav",i);window.addEventListener("resize",i);
</script><script type="module">
          let mermaidImport = undefined
          document.addEventListener('nav', async () => {
            if (document.querySelector("code.mermaid")) {
              mermaidImport ||= await import('https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.7.0/mermaid.esm.min.mjs')
              const mermaid = mermaidImport.default
              const darkMode = document.documentElement.getAttribute('saved-theme') === 'dark'
              mermaid.initialize({
                startOnLoad: false,
                securityLevel: 'loose',
                theme: darkMode ? 'dark' : 'default'
              })

              await mermaid.run({
                querySelector: '.mermaid'
              })
            }
          });
          </script><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/contrib/copy-tex.min.js" type="application/javascript"></script><script type="application/javascript">
        const socket = new WebSocket('ws://localhost:3001')
        // reload(true) ensures resources like images and scripts are fetched again in firefox
        socket.addEventListener('message', () => document.location.reload(true))
      </script><script src="../../../../postscript.js" type="module"></script></html>