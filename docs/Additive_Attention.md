---
title: Additive Attention

tags: architecture 
date modified: Monday, October 10th 2022, 2:02:34 pm
date created: Tuesday, July 26th 2022, 8:33:15 pm
---

# Additive [Attention](Attention.md)
- [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf)
- Uses a one layer feedforward network to calculate [Attention_Alignment](Attention_Alignment.md)
- Oh, basically it is the same as [Bahdanau_Attention](Bahdanau_Attention.md)

