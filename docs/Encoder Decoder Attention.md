---
title: Encoder Decoder Attention
tags: architecture 
---

# Encoder Decoder [[Attention]]
- Q comes from prev decoder
- K,V from encoder










































## Backlinks

> - [BEiT](BEiT.md)
>   - Bidirectional [[Encoder Decoder Attention]] representations from [[Vision Transformer]]
>    
> - [Attention](Attention.md)
>   - [[Encoder Decoder Attention]]

_Backlinks last generated 2022-07-26 20:33:15_
