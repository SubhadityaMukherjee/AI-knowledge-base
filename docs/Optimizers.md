# Optimization
- [[Gradient Descent gradients]]

- [[Adagrad]]

- [[Rmsprop]]

- [[Adam]]

- [[Learning Rate Decay tricks]]

- [[Early Stopping tricks]]
## Backlinks
* [[Vanishingexploding gradients]]
	* [[Regularization]] , [[Optimizers]] , [[Architectures]]
* [[DeepLearning]]
	* [[Optimizers]]
* [[Bias Variance Dilemma]]
	* Tune on [[Emperical Risk]] instead using [[Optimizers]] 

