- **08:05** Been a while. Just want to add some stuff to the DB. Some timepass tbh. Maybe for 30 35 mins
- **08:26** I guess it would be nice to upgrade my little website formatter script with the filenames as tags. :/ Idk whats up with me and making this database. I feel like its the most important thing ive done so far in this domain. Idek why. But what to do. its just something I have to do I guess.
	- Okay lets see if I can make this script better.
- **08:35** Yeah idk about that. It seems like all im doing is redifining what Obsidian does already. Maybe I should just let it do its thing and focus on simple formatting only

- [[Benchmark LLM]]
- [[OPT]]
- [[Diffusion LM]]
- [[DeepPERF]]
- [[CTC]]
- [[Joint Factor Analysis]]
- [Speech Recognition](../Speech%20Recognition.md)
- [[Speaker Verification]]
- [[Listen Attend Spell]]
- [[VGGish]]
- **08:47** I guess this whole thing counts as procrastinating? I dunno honestly but well. I do think its not tooo bad
- **08:50** Maybe I should write one bit to automate the file creation.. that is taking the most time right now honestly
- [[X Vectors]]
- [[WaveGlow]]
- **11:05** More now
- **11:19**
- [[Tacotron 2]]
- [[wave2vec]]
- [[SpecAugment]]
- [[Conformer]]
- [[wave2vec 2]]
- [[HiFI-GAN_Denoising]]
- [[HiFI-GAN Synthesis]]
- [[Speech Emotion Recognition]]
- [[XLM-R]]
- [[GE2E]]
- [[Generative Spoken Language Modeling]]
- [[pGLSM]]
- [[Speech Resynthesis]]
- [[S2ST]]
- [[Textless Speech Emotion Conversion]]
- [[dGSLM]]
- [[textless-lib]]
- **11:35** More?

- [“Why Should I Trust You?” Explaining the Predictions of Any Classifier](https://arxiv.org/abs/1602.04938v1)
- LIME
- novel model-agnostic modular and extensible explanation technique that explains the predictions of any classifier in an interpretable and faithful manner
- learning an interpretable model locally around the prediction
- SP-LIME
- method to explain models by selectingrepresentative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem and providing a global view of the model to users
- flexibility of these methods by explaining different models for text (e.g random forests) and image classification (e.g neural networks)
- usefulness of explanations is shown via novel experiments, both simulated and with human subjects
- [A Unified Approach to Interpreting Model Predictions](https://proceedings.neurips.cc/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf)
- help users interpret the predictions of complex models
- unclear how these methods are related and when one method is preferable over another
- unified framework for interpreting predictions
- SHAP
- SHapley Additive exPlanations
- game theoretic approach to explain the output of any machine learning model
- connects optimal credit allocation with local explanations using the classic Shapley values from game theory and their related extensions
- assigns each feature an importance value for a particular prediction
- identification of a new class of additive feature importance measures
- theoretical results showing there is a unique solution in this class with a set of desirable properties
- notable because several recent methods in the class lack the proposed desirable properties
- present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches


