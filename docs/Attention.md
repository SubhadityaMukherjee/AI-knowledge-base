---
title: Attention
tags: architecture
date modified: Thursday, August 11th 2022, 12:32:57 am
date created: Tuesday, July 26th 2022, 8:33:15 pm
---

# Attention
- Model can decide where to look in the input
- [[Self Attention]]
- [[Additive Attention]]
- [[Dot Product Attention]]
- [[Location Aware Attention]]
- [[Relative Multi Head Self Attention]]
- [[Soft Attention]]
- [[Scaled Dot Product Attention]]
- [[Encoder Decoder Attention]]
- [[Multi Head Attention]]
- [[Strided Attention]]
- [[Fixed Factorization Attention]]
- [[Sliding Window Attention]]
- [[Dilated Sliding Window Attention]]
- [[Global and Sliding Window Attention]]
- [[Content Based Attention]]
- [[Location Base Attention]]
- [[Mixed chunk attention]]

