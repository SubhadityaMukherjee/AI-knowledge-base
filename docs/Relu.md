## Relu
- $$ReLU(x) = max(0,x)$$
- $$\frac{d}{d_x}ReLU(X) = \begin{cases}0 & x \geq 0 \\ 1 & otherwise \end{cases}$$
- He init
- MLP, CNN : Hidden
- [[Leaky Relu]]
- [[PRelu]]
<<<<<<< HEAD
- [[Noisy Relu]]



## Backlinks
* [[Noisy Relu]]
	* [[Relu]]
* [[Activation Functions]]
	* [[Relu]]

## ...
=======
- [[Noisy Relu]]## Backlinks
* [[ActivationFunctions]]
	* [[Relu]]

>>>>>>> 1dd38fd29e2ea89a9d6c64b1ecd9e965740dd3c9
