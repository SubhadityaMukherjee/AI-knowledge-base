---
title: Xavier Initialization
tags: regularize 
---

# Xavier [[Initialization]]
- $$\mathrm{a=\sqrt{\frac{6}{\left(\mathrm{d}\mathrm{_{\mathrm{in}}^{\mathrm{ }}}+\mathrm{d}_{\mathrm{out}} \right)}}}$$
- Random values drawn uniformly from $[-a,a]$
- For [[Batch Normalization]] [[Layers]], $\gamma =1$ and $\beta=0$
- For Tanh based activating neural nets




## Backlinks

> - [Initialization](Initialization.md)
>   - [[Xavier Initialization]]

_Backlinks last generated 2022-07-26 20:33:15_
