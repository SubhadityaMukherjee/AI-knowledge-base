---
title: MNIST
tags: dataset 
---

# MNIST
- 10 classes
- 2 channels






























































































## Backlinks

> - [Discrete Cosine Transform](Discrete Cosine Transform.md)
>   - In my opinion, this can be explained by looking at the internals of a convolutional layer. It works as follows. You specify a number of filters which, during training, learn to recognize unique aspects of the image-like data. They can then be used to classify new samples - quite accurately, as we have seen with raw [[MNIST]] data. This means that the convolutional layer already makes your data representation sparser. What's more, this effect gets even stronger when [[Layers|layers]] like [[Pooling]] are applied
>    
> - [Ensemble Distillation](Ensemble Distillation.md)
>   - [[MNIST]]

_Backlinks last generated 2022-07-26 20:33:15_
