---
categories: ['temp']
date modified: Monday, October 10th 2022, 2:02:14 pm
date created: Tuesday, July 26th 2022, 8:33:15 pm
---

# Vision [Transformer](Transformer.qmd)
- @dosovitskiyImageWorth16x162021
- [paper](https://arxiv.org/abs/2010.11929)
- ![vit](images/vit.png)
- [Transformer](Transformer.qmd) applied directly to sequences/patches of images
- Lower computational resources
- [ImageNet](ImageNet.qmd) , [CIFAR](CIFAR.qmd), [VTAB](VTAB.qmd)
- [Do Vision Transformers See Like Convolutional Neural Networks?](https://arxiv.org/abs/2108.08810)
- analyzes the internal representation structure of ViTs and [Conv](Conv.qmd) on image classification benchmarks
- striking differences in the [features](Features.qmd) and internal structures between the two architectures
- ViT having more uniform representations across all [layers](Layers.qmd)
- early aggregation of global information
- spatial localization
- discovering ViTs successfully preserve input spatial information with CLS tokens
- finding larger ViT models develop significantly stronger intermediate representations through larger pretraining datasets
- [MLP-Mixer](MLP-Mixer)



