---
title: Gradient Descent #gradients
---

# Gradient Descent #gradients
- [[Backprop]]
- Gradient Direction ~ Steepest Descent $$\theta = tan^{-1}(\frac{\frac{\partial f}{\partial y}}{\frac{\partial f}{\partial x}})$$
- Gradient Magnitude ~ Edge Strength $$||\triangledown f|| = \sqrt{(\frac{\partial f}{\partial x})^{2} + (\frac{\partial f}{\partial y})^{2}}$$
- Params $$\theta$$
- Minimize loss function $$\mathscr{L}(\theta) = \Sigma^N_{n=1}l_n(\theta)$$
- [[Simple Gradient Descent]]
- [[SGD]]
- [[Mini Batch GD]]
- [[SGD Momentum]]
- [[Adagrad]]
- [[Nesterov Momentum]]
- [[AdaDelta]]
- [Rmsprop](Rmsprop.md)
- [Adam](Adam.md)
## â€¦

## Backlinks
> - [Learning Rate [[Scheduling]]](Learning Rate Scheduling.md)
>   - [[Gradient Descent gradients]]
>
> - [Contrastive Loss](Contrastive Loss.md)
>   - Minimize distance between similar inputs [[Gradient Descent gradients]], maximize between dissimilar [[Gradient Ascent]]
>
> - [Gradient Ascent](Gradient Ascent.md)
>   - To maximize loss function unlike [[Gradient Descent gradients]]
>
> - [Optimization](Optimizers.md)
>   - [[Gradient Descent gradients]]
>
> - [AutoEncoder](Auto Encoders.md)
>   - Learn using [[Gradient Descent gradients]]
>
> - [GAN](Generative Models.md)
>   - G : [[Gradient Descent gradients]]
>
> - [Methods for [Feature Learning](Feature%20Learning.md)](Methods for Feature Learning.md)
>   - [[Gradient Descent gradients]] or [[LinearRegression]]

_Backlinks last generated 2022-06-26 00:07:54_
