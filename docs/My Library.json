[
  {"id":"adadiPeekingBlackBoxSurvey2018","abstract":"At the dawn of the fourth industrial revolution, we are witnessing a fast and widespread adoption of artificial intelligence (AI) in our daily life, which contributes to accelerating the shift towards a more algorithmic society. However, even with such unprecedented advancements, a key impediment to the use of AI-based systems is that they often lack transparency. Indeed, the black-box nature of these systems allows powerful predictions, but it cannot be directly explained. This issue has triggered a new debate on explainable AI (XAI). A research field holds substantial promise for improving trust and transparency of AI-based systems. It is recognized as the sine qua non for AI to continue making steady progress without disruption. This survey provides an entry point for interested researchers and practitioners to learn key aspects of the young and rapidly growing body of research related to XAI. Through the lens of the literature, we review the existing approaches regarding the topic, discuss trends surrounding its sphere, and present major research trajectories.","author":[{"family":"Adadi","given":"Amina"},{"family":"Berrada","given":"Mohammed"}],"citation-key":"adadiPeekingBlackBoxSurvey2018","container-title":"IEEE Access","DOI":"10.1109/ACCESS.2018.2870052","ISSN":"2169-3536","issued":{"date-parts":[[2018]]},"page":"52138-52160","source":"IEEE Xplore","title":"Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI)","title-short":"Peeking Inside the Black-Box","type":"article-journal","volume":"6"},
  {"id":"adebayoSanityChecksSaliency2020","abstract":"Saliency methods have emerged as a popular tool to highlight features in an input deemed relevant for the prediction of a learned model. Several saliency methods have been proposed, often guided by visual appeal on image data. In this work, we propose an actionable methodology to evaluate what kinds of explanations a given method can and cannot provide. We find that reliance, solely, on visual assessment can be misleading. Through extensive experiments we show that some existing saliency methods are independent both of the model and of the data generating process. Consequently, methods that fail the proposed tests are inadequate for tasks that are sensitive to either data or model, such as, finding outliers in the data, explaining the relationship between inputs and outputs that the model learned, and debugging the model. We interpret our findings through an analogy with edge detection in images, a technique that requires neither training data nor model. Theory in the case of a linear model and a single-layer convolutional neural network supports our experimental findings.","accessed":{"date-parts":[[2022,11,28]]},"author":[{"family":"Adebayo","given":"Julius"},{"family":"Gilmer","given":"Justin"},{"family":"Muelly","given":"Michael"},{"family":"Goodfellow","given":"Ian"},{"family":"Hardt","given":"Moritz"},{"family":"Kim","given":"Been"}],"citation-key":"adebayoSanityChecksSaliency2020","DOI":"10.48550/arXiv.1810.03292","issued":{"date-parts":[[2020,11,6]]},"number":"arXiv:1810.03292","publisher":"arXiv","source":"arXiv.org","title":"Sanity Checks for Saliency Maps","type":"article","URL":"http://arxiv.org/abs/1810.03292"},
  {"id":"aghelanUnderwaterImagesSuperResolution2022","abstract":"Single image super-resolution (SISR) methods can enhance the resolution and quality of underwater images. Enhancing the resolution of underwater images leads to better performance of autonomous underwater vehicles. In this work, we fine-tune the Real-Enhanced Super-Resolution Generative Adversarial Network (Real-ESRGAN) model to increase the resolution of underwater images. In our proposed approach, the pre-trained generator and discriminator networks of the Real-ESRGAN model are fine-tuned using underwater image datasets. We used the USR-248 and UFO-120 datasets to fine-tune the Real-ESRGAN model. Our fine-tuned model produces images with better resolution and quality compared to the original model.","accessed":{"date-parts":[[2022,11,19]]},"author":[{"family":"Aghelan","given":"Alireza"}],"citation-key":"aghelanUnderwaterImagesSuperResolution2022","DOI":"10.48550/arXiv.2211.03550","issued":{"date-parts":[[2022,11,7]]},"number":"arXiv:2211.03550","publisher":"arXiv","source":"arXiv.org","title":"Underwater Images Super-Resolution Using Generative Adversarial Network-based Model","type":"article","URL":"http://arxiv.org/abs/2211.03550"},
  {"id":"aharonySocialFMRIInvestigating2011","abstract":"We introduce the Friends and Family study, a longitudinal living laboratory in a residential community. In this study, we employ a ubiquitous computing approach, Social Functional Mechanism-design and Relationship Imaging, or Social fMRI, that combines extremely rich data collection with the ability to conduct targeted experimental interventions with study populations. We present our mobile-phone-based social and behavioral sensing system, deployed in the wild for over 15 months. Finally, we present three investigations performed during the study, looking into the connection between individuals’ social behavior and their financial status, network effects in decision making, and a novel intervention aimed at increasing physical activity in the subject population. Results demonstrate the value of social factors for choice, motivation, and adherence, and enable quantifying the contribution of different incentive mechanisms.","accessed":{"date-parts":[[2022,6,15]]},"author":[{"family":"Aharony","given":"Nadav"},{"family":"Pan","given":"Wei"},{"family":"Ip","given":"Cory"},{"family":"Khayal","given":"Inas"},{"family":"Pentland","given":"Alex"}],"citation-key":"aharonySocialFMRIInvestigating2011","container-title":"Pervasive and Mobile Computing","container-title-short":"Pervasive and Mobile Computing","DOI":"10.1016/j.pmcj.2011.09.004","ISSN":"15741192","issue":"6","issued":{"date-parts":[[2011,12]]},"language":"en","page":"643-659","source":"DOI.org (Crossref)","title":"Social fMRI: Investigating and shaping social mechanisms in the real world","title-short":"Social fMRI","type":"article-journal","URL":"https://linkinghub.elsevier.com/retrieve/pii/S1574119211001246","volume":"7"},
  {"id":"alammarIllustratedTransformer","abstract":"Discussions:\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\n\n\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Russian, Spanish, Vietnamese\n\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\n\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\n\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\n\n2020 Update: I’ve created a “Narrated Transformer” video which is a gentler approach to the topic:\n\n\n\n\nA High-Level Look\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.","accessed":{"date-parts":[[2022,5,25]]},"author":[{"family":"Alammar","given":"Jay"}],"citation-key":"alammarIllustratedTransformer","title":"The Illustrated Transformer","type":"webpage","URL":"https://jalammar.github.io/illustrated-transformer/"},
  {"id":"aliEvaluationLatentSpace2021","accessed":{"date-parts":[[2022,9,6]]},"author":[{"family":"Ali","given":"Sharjeel"},{"family":"Kaick","given":"Oliver","non-dropping-particle":"van"}],"citation-key":"aliEvaluationLatentSpace2021","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[[2021]]},"language":"en","page":"2086-2094","source":"openaccess.thecvf.com","title":"Evaluation of Latent Space Learning With Procedurally-Generated Datasets of Shapes","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2021W/DLGC/html/Ali_Evaluation_of_Latent_Space_Learning_With_Procedurally-Generated_Datasets_of_Shapes_ICCVW_2021_paper.html"},
  {"id":"anconaBetterUnderstandingGradientbased2018","abstract":"Understanding the flow of information in Deep Neural Networks (DNNs) is a challenging problem that has gain increasing attention over the last few years. While several methods have been proposed to explain network predictions, there have been only a few attempts to compare them from a theoretical perspective. What is more, no exhaustive empirical comparison has been performed in the past. In this work, we analyze four gradient-based attribution methods and formally prove conditions of equivalence and approximation between them. By reformulating two of these methods, we construct a unified framework which enables a direct comparison, as well as an easier implementation. Finally, we propose a novel evaluation metric, called Sensitivity-n and test the gradient-based attribution methods alongside with a simple perturbation-based attribution method on several datasets in the domains of image and text classification, using various network architectures.","accessed":{"date-parts":[[2022,12,6]]},"author":[{"family":"Ancona","given":"Marco"},{"family":"Ceolini","given":"Enea"},{"family":"Öztireli","given":"Cengiz"},{"family":"Gross","given":"Markus"}],"citation-key":"anconaBetterUnderstandingGradientbased2018","DOI":"10.48550/arXiv.1711.06104","issued":{"date-parts":[[2018,3,7]]},"number":"arXiv:1711.06104","publisher":"arXiv","source":"arXiv.org","title":"Towards better understanding of gradient-based attribution methods for Deep Neural Networks","type":"article","URL":"http://arxiv.org/abs/1711.06104"},
  {"id":"andrienkoVisualAnalyticsHumanCentered2022","abstract":"We introduce a new research area in Visual Analytics (VA) aiming to bridge existing gaps between methods of interactive Machine Learning (ML) and eXplainable Artificial Intelligence (XAI), on one side, and human minds, on the other side. The gaps are, first, a conceptual mismatch between ML/XAI outputs and human mental models and ways of reasoning, second, a mismatch between the information quantity and level of detail and human capabilities to perceive and understand. A grand challenge is to adapt ML and XAI to human goals, concepts, values, and ways of thinking. Complementing the current efforts in XAI towards solving this challenge, VA can contribute by exploiting the potential of visualization as an effective way of communicating information to humans and a strong trigger of human abstractive perception and thinking. We propose a cross-disciplinary research framework and formulate research directions for VA.","accessed":{"date-parts":[[2023,2,13]]},"author":[{"family":"Andrienko","given":"Natalia"},{"family":"Andrienko","given":"Gennady"},{"family":"Adilova","given":"Linara"},{"family":"Wrobel","given":"Stefan"}],"citation-key":"andrienkoVisualAnalyticsHumanCentered2022","container-title":"IEEE Computer Graphics and Applications","container-title-short":"IEEE Comput. Grap. Appl.","DOI":"10.1109/MCG.2021.3130314","ISSN":"0272-1716, 1558-1756","issue":"1","issued":{"date-parts":[[2022,1,1]]},"language":"en","page":"123-133","source":"DOI.org (Crossref)","title":"Visual Analytics for Human-Centered Machine Learning","type":"article-journal","URL":"https://ieeexplore.ieee.org/document/9693359/","volume":"42"},
  {"id":"arrietaExplainableArtificialIntelligence2019","abstract":"In the last years, Artificial Intelligence (AI) has achieved a notable momentum that may deliver the best of expectations over many application sectors across the field. For this to occur, the entire community stands in front of the barrier of explainability, an inherent problem of AI techniques brought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not present in the last hype of AI. Paradigms underlying this problem fall within the so-called eXplainable AI (XAI) field, which is acknowledged as a crucial feature for the practical deployment of AI models. This overview examines the existing literature in the field of XAI, including a prospect toward what is yet to be reached. We summarize previous efforts to define explainability in Machine Learning, establishing a novel definition that covers prior conceptual propositions with a major focus on the audience for which explainability is sought. We then propose and discuss about a taxonomy of recent contributions related to the explainability of different Machine Learning models, including those aimed at Deep Learning methods for which a second taxonomy is built. This literature analysis serves as the background for a series of challenges faced by XAI, such as the crossroads between data fusion and explainability. Our prospects lead toward the concept of Responsible Artificial Intelligence, namely, a methodology for the large-scale implementation of AI methods in real organizations with fairness, model explainability and accountability at its core. Our ultimate goal is to provide newcomers to XAI with a reference material in order to stimulate future research advances, but also to encourage experts and professionals from other disciplines to embrace the benefits of AI in their activity sectors, without any prior bias for its lack of interpretability.","accessed":{"date-parts":[[2022,11,24]]},"author":[{"family":"Arrieta","given":"Alejandro Barredo"},{"family":"Díaz-Rodríguez","given":"Natalia"},{"family":"Del Ser","given":"Javier"},{"family":"Bennetot","given":"Adrien"},{"family":"Tabik","given":"Siham"},{"family":"Barbado","given":"Alberto"},{"family":"García","given":"Salvador"},{"family":"Gil-López","given":"Sergio"},{"family":"Molina","given":"Daniel"},{"family":"Benjamins","given":"Richard"},{"family":"Chatila","given":"Raja"},{"family":"Herrera","given":"Francisco"}],"citation-key":"arrietaExplainableArtificialIntelligence2019","DOI":"10.48550/arXiv.1910.10045","issued":{"date-parts":[[2019,12,26]]},"number":"arXiv:1910.10045","publisher":"arXiv","source":"arXiv.org","title":"Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI","title-short":"Explainable Artificial Intelligence (XAI)","type":"article","URL":"http://arxiv.org/abs/1910.10045"},
  {"id":"aylingPuttingAIEthics2022","abstract":"Bias, unfairness and lack of transparency and accountability in Artificial Intelligence (AI) systems, and the potential for the misuse of predictive models for decision-making have raised concerns about the ethical impact and unintended consequences of new technologies for society across every sector where data-driven innovation is taking place. This paper reviews the landscape of suggested ethical frameworks with a focus on those which go beyond high-level statements of principles and offer practical tools for application of these principles in the production and deployment of systems. This work provides an assessment of these practical frameworks with the lens of known best practices for impact assessment and audit of technology. We review other historical uses of risk assessments and audits and create a typology that allows us to compare current AI ethics tools to Best Practices found in previous methodologies from technology, environment, privacy, finance and engineering. We analyse current AI ethics tools and their support for diverse stakeholders and components of the AI development and deployment lifecycle as well as the types of tools used to facilitate use. From this, we identify gaps in current AI ethics tools in auditing and risk assessment that should be considered going forward.","accessed":{"date-parts":[[2023,1,17]]},"author":[{"family":"Ayling","given":"Jacqui"},{"family":"Chapman","given":"Adriane"}],"citation-key":"aylingPuttingAIEthics2022","container-title":"AI and Ethics","container-title-short":"AI Ethics","DOI":"10.1007/s43681-021-00084-x","ISSN":"2730-5961","issue":"3","issued":{"date-parts":[[2022,8,1]]},"language":"en","page":"405-429","source":"Springer Link","title":"Putting AI ethics to work: are the tools fit for purpose?","title-short":"Putting AI ethics to work","type":"article-journal","URL":"https://doi.org/10.1007/s43681-021-00084-x","volume":"2"},
  {"id":"babikerIntroductionDeepVisual2018","abstract":"The practical impact of deep learning on complex supervised learning problems has been significant, so much so that almost every Artificial Intelligence problem, or at least a portion thereof, has been somehow recast as a deep learning problem. The applications appeal is significant, but this appeal is increasingly challenged by what some call the challenge of explainability, or more generally the more traditional challenge of debuggability: if the outcomes of a deep learning process produce unexpected results (e.g., less than expected performance of a classifier), then there is little available in the way of theories or tools to help investigate the potential causes of such unexpected behavior, especially when this behavior could impact people's lives. We describe a preliminary framework to help address this issue, which we call \"deep visual explanation\" (DVE). \"Deep,\" because it is the development and performance of deep neural network models that we want to understand. \"Visual,\" because we believe that the most rapid insight into a complex multi-dimensional model is provided by appropriate visualization techniques, and \"Explanation,\" because in the spectrum from instrumentation by inserting print statements to the abductive inference of explanatory hypotheses, we believe that the key to understanding deep learning relies on the identification and exposure of hypotheses about the performance behavior of a learned deep model. In the exposition of our preliminary framework, we use relatively straightforward image classification examples and a variety of choices on initial configuration of a deep model building scenario. By careful but not complicated instrumentation, we expose classification outcomes of deep models using visualization, and also show initial results for one potential application of interpretability.","accessed":{"date-parts":[[2023,2,20]]},"author":[{"family":"Babiker","given":"Housam Khalifa Bashier"},{"family":"Goebel","given":"Randy"}],"citation-key":"babikerIntroductionDeepVisual2018","issued":{"date-parts":[[2018,3,15]]},"number":"arXiv:1711.09482","publisher":"arXiv","source":"arXiv.org","title":"An Introduction to Deep Visual Explanation","type":"article","URL":"http://arxiv.org/abs/1711.09482"},
  {"id":"baiAreTransformersMore2021","abstract":"Transformer emerges as a powerful tool for visual recognition. In addition to demonstrating competitive performance on a broad range of visual benchmarks, recent works also argue that Transformers are much more robust than Convolutions Neural Networks (CNNs). Nonetheless, surprisingly, we find these conclusions are drawn from unfair experimental settings, where Transformers and CNNs are compared at different scales and are applied with distinct training frameworks. In this paper, we aim to provide the first fair & in-depth comparisons between Transformers and CNNs, focusing on robustness evaluations. With our unified training setup, we first challenge the previous belief that Transformers outshine CNNs when measuring adversarial robustness. More surprisingly, we find CNNs can easily be as robust as Transformers on defending against adversarial attacks, if they properly adopt Transformers' training recipes. While regarding generalization on out-of-distribution samples, we show pre-training on (external) large-scale datasets is not a fundamental request for enabling Transformers to achieve better performance than CNNs. Moreover, our ablations suggest such stronger generalization is largely benefited by the Transformer's self-attention-like architectures per se, rather than by other training setups. We hope this work can help the community better understand and benchmark the robustness of Transformers and CNNs. The code and models are publicly available at https://github.com/ytongbai/ViTs-vs-CNNs.","accessed":{"date-parts":[[2023,5,7]]},"author":[{"family":"Bai","given":"Yutong"},{"family":"Mei","given":"Jieru"},{"family":"Yuille","given":"Alan"},{"family":"Xie","given":"Cihang"}],"citation-key":"baiAreTransformersMore2021","DOI":"10.48550/arXiv.2111.05464","issued":{"date-parts":[[2021,11,9]]},"number":"arXiv:2111.05464","publisher":"arXiv","source":"arXiv.org","title":"Are Transformers More Robust Than CNNs?","type":"article","URL":"http://arxiv.org/abs/2111.05464"},
  {"id":"bakryUntanglingObjectviewManifold2014","author":[{"family":"Bakry","given":"Amr"},{"family":"Elgammal","given":"Ahmed"}],"citation-key":"bakryUntanglingObjectviewManifold2014","container-title":"European conference on computer vision","issued":{"date-parts":[[2014]]},"page":"434–449","publisher":"Springer","title":"Untangling object-view manifold for multiview recognition and pose estimation","type":"paper-conference"},
  {"id":"bastingsElephantInterpretabilityRoom2020","abstract":"There is a recent surge of interest in using attention as explanation of model predictions, with mixed evidence on whether attention can be used as such. While attention conveniently gives us one weight per input token and is easily extracted, it is often unclear toward what goal it is used as explanation. We find that often that goal, whether explicitly stated or not, is to find out what input tokens are the most relevant to a prediction, and that the implied user for the explanation is a model developer. For this goal and user, we argue that input saliency methods are better suited, and that there are no compelling reasons to use attention, despite the coincidence that it provides a weight for each input. With this position paper, we hope to shift some of the recent focus on attention to saliency methods, and for authors to clearly state the goal and user for their explanations.","accessed":{"date-parts":[[2023,5,18]]},"author":[{"family":"Bastings","given":"Jasmijn"},{"family":"Filippova","given":"Katja"}],"citation-key":"bastingsElephantInterpretabilityRoom2020","issued":{"date-parts":[[2020,10,12]]},"number":"arXiv:2010.05607","publisher":"arXiv","source":"arXiv.org","title":"The elephant in the interpretability room: Why use attention as explanation when we have saliency methods?","title-short":"The elephant in the interpretability room","type":"article","URL":"http://arxiv.org/abs/2010.05607"},
  {"id":"bauNetworkDissectionQuantifying2017","abstract":"We propose a general framework called Network Dissection for quantifying the interpretability of latent representations of CNNs by evaluating the alignment between individual hidden units and a set of semantic concepts. Given any CNN model, the proposed method draws on a broad data set of visual concepts to score the semantics of hidden units at each intermediate convolutional layer. The units with semantics are given labels across a range of objects, parts, scenes, textures, materials, and colors. We use the proposed method to test the hypothesis that interpretability of units is equivalent to random linear combinations of units, then we apply our method to compare the latent representations of various networks when trained to solve different supervised and self-supervised training tasks. We further analyze the effect of training iterations, compare networks trained with different initializations, examine the impact of network depth and width, and measure the effect of dropout and batch normalization on the interpretability of deep visual representations. We demonstrate that the proposed method can shed light on characteristics of CNN models and training methods that go beyond measurements of their discriminative power.","accessed":{"date-parts":[[2022,11,24]]},"author":[{"family":"Bau","given":"David"},{"family":"Zhou","given":"Bolei"},{"family":"Khosla","given":"Aditya"},{"family":"Oliva","given":"Aude"},{"family":"Torralba","given":"Antonio"}],"citation-key":"bauNetworkDissectionQuantifying2017","DOI":"10.48550/arXiv.1704.05796","issued":{"date-parts":[[2017,4,19]]},"number":"arXiv:1704.05796","publisher":"arXiv","source":"arXiv.org","title":"Network Dissection: Quantifying Interpretability of Deep Visual Representations","title-short":"Network Dissection","type":"article","URL":"http://arxiv.org/abs/1704.05796"},
  {"id":"baySurfSpeededRobust2006","author":[{"family":"Bay","given":"Herbert"},{"family":"Tuytelaars","given":"Tinne"},{"family":"Van Gool","given":"Luc"}],"citation-key":"baySurfSpeededRobust2006","container-title":"European conference on computer vision","issued":{"date-parts":[[2006]]},"page":"404–417","publisher":"Springer","title":"Surf: Speeded up robust features","type":"paper-conference"},
  {"id":"belloAttentionAugmentedConvolutional2019","abstract":"Convolutional networks have been the paradigm of choice in many computer vision applications. The convolution operation however has a signiﬁcant weakness in that it only operates on a local neighborhood, thus missing global information. Self-attention, on the other hand, has emerged as a recent advance to capture long range interactions, but has mostly been applied to sequence modeling and generative modeling tasks. In this paper, we consider the use of self-attention for discriminative visual tasks as an alternative to convolutions. We introduce a novel two-dimensional relative self-attention mechanism that proves competitive in replacing convolutions as a stand-alone computational primitive for image classiﬁcation. We ﬁnd in control experiments that the best results are obtained when combining both convolutions and self-attention. We therefore propose to augment convolutional operators with this self-attention mechanism by concatenating convolutional feature maps with a set of feature maps produced via self-attention. Extensive experiments show that Attention Augmentation leads to consistent improvements in image classiﬁcation on ImageNet and object detection on COCO across many different models and scales, including ResNets and a stateof-the art mobile constrained network, while keeping the number of parameters similar. In particular, our method achieves a 1.3% top-1 accuracy improvement on ImageNet classiﬁcation over a ResNet50 baseline and outperforms other attention mechanisms for images such as Squeezeand-Excitation [17]. It also achieves an improvement of 1.4 mAP in COCO Object Detection on top of a RetinaNet baseline.","accessed":{"date-parts":[[2022,10,3]]},"author":[{"family":"Bello","given":"Irwan"},{"family":"Zoph","given":"Barret"},{"family":"Le","given":"Quoc"},{"family":"Vaswani","given":"Ashish"},{"family":"Shlens","given":"Jonathon"}],"citation-key":"belloAttentionAugmentedConvolutional2019","container-title":"2019 IEEE/CVF International Conference on Computer Vision (ICCV)","DOI":"10.1109/ICCV.2019.00338","event-place":"Seoul, Korea (South)","event-title":"2019 IEEE/CVF International Conference on Computer Vision (ICCV)","ISBN":"978-1-72814-803-8","issued":{"date-parts":[[2019,10]]},"language":"en","page":"3285-3294","publisher":"IEEE","publisher-place":"Seoul, Korea (South)","source":"DOI.org (Crossref)","title":"Attention Augmented Convolutional Networks","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/9010285/"},
  {"id":"birhaneForgottenMarginsAI2022","accessed":{"date-parts":[[2023,1,17]]},"author":[{"family":"Birhane","given":"Abeba"},{"family":"Ruane","given":"Elayne"},{"family":"Laurent","given":"Thomas"},{"family":"S. Brown","given":"Matthew"},{"family":"Flowers","given":"Johnathan"},{"family":"Ventresque","given":"Anthony"},{"family":"L. Dancy","given":"Christopher"}],"citation-key":"birhaneForgottenMarginsAI2022","container-title":"2022 ACM Conference on Fairness, Accountability, and Transparency","DOI":"10.1145/3531146.3533157","event-place":"Seoul Republic of Korea","event-title":"FAccT '22: 2022 ACM Conference on Fairness, Accountability, and Transparency","ISBN":"978-1-4503-9352-2","issued":{"date-parts":[[2022,6,21]]},"language":"en","page":"948-958","publisher":"ACM","publisher-place":"Seoul Republic of Korea","source":"DOI.org (Crossref)","title":"The Forgotten Margins of AI Ethics","type":"paper-conference","URL":"https://dl.acm.org/doi/10.1145/3531146.3533157"},
  {"id":"birhaneLargeImageDatasets2021","abstract":"In this paper we investigate problematic practices and consequences of large scale vision datasets (LSVDs). We examine broad issues such as the question of consent and justice as well as specific concerns such as the inclusion of verifiably pornographic images in datasets. Taking the ImageNet-ILSVRC-2012 dataset as an example, we perform a cross-sectional model-based quantitative census covering factors such as age, gender, NSFW content scoring, class- wise accuracy, human-cardinality-analysis, and the semanticity of the image class information in order to statistically investigate the extent and subtleties of ethical transgressions. We then use the census to help hand-curate a look-up-table of images in the ImageNet-ILSVRC-2012 dataset that fall into the categories of verifiably pornographic: shot in a non-consensual setting (up-skirt), beach voyeuristic, and exposed private parts. We survey the landscape of harm and threats both the society at large and individuals face due to uncritical and ill-considered dataset curation practices. We then propose possible courses of correction and critique their pros and cons. We have duly open-sourced all of the code and the census meta-datasets generated in this endeavor for the computer vision community to build on. By unveiling the severity of the threats, our hope is to motivate the constitution of mandatory Institutional Review Boards (IRB) for large scale dataset curation.","author":[{"family":"Birhane","given":"Abeba"},{"family":"Prabhu","given":"Vinay Uday"}],"citation-key":"birhaneLargeImageDatasets2021","container-title":"2021 IEEE Winter Conference on Applications of Computer Vision (WACV)","DOI":"10.1109/WACV48630.2021.00158","event-title":"2021 IEEE Winter Conference on Applications of Computer Vision (WACV)","ISSN":"2642-9381","issued":{"date-parts":[[2021,1]]},"page":"1536-1546","source":"IEEE Xplore","title":"Large image datasets: A pyrrhic win for computer vision?","title-short":"Large image datasets","type":"paper-conference"},
  {"id":"bommasaniOpportunitiesRisksFoundation2021","abstract":"AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.","accessed":{"date-parts":[[2022,5,25]]},"author":[{"family":"Bommasani","given":"Rishi"},{"family":"Hudson","given":"Drew A."},{"family":"Adeli","given":"Ehsan"},{"family":"Altman","given":"Russ"},{"family":"Arora","given":"Simran"},{"family":"Arx","given":"Sydney","non-dropping-particle":"von"},{"family":"Bernstein","given":"Michael S."},{"family":"Bohg","given":"Jeannette"},{"family":"Bosselut","given":"Antoine"},{"family":"Brunskill","given":"Emma"},{"family":"Brynjolfsson","given":"Erik"},{"family":"Buch","given":"Shyamal"},{"family":"Card","given":"Dallas"},{"family":"Castellon","given":"Rodrigo"},{"family":"Chatterji","given":"Niladri"},{"family":"Chen","given":"Annie"},{"family":"Creel","given":"Kathleen"},{"family":"Davis","given":"Jared Quincy"},{"family":"Demszky","given":"Dora"},{"family":"Donahue","given":"Chris"},{"family":"Doumbouya","given":"Moussa"},{"family":"Durmus","given":"Esin"},{"family":"Ermon","given":"Stefano"},{"family":"Etchemendy","given":"John"},{"family":"Ethayarajh","given":"Kawin"},{"family":"Fei-Fei","given":"Li"},{"family":"Finn","given":"Chelsea"},{"family":"Gale","given":"Trevor"},{"family":"Gillespie","given":"Lauren"},{"family":"Goel","given":"Karan"},{"family":"Goodman","given":"Noah"},{"family":"Grossman","given":"Shelby"},{"family":"Guha","given":"Neel"},{"family":"Hashimoto","given":"Tatsunori"},{"family":"Henderson","given":"Peter"},{"family":"Hewitt","given":"John"},{"family":"Ho","given":"Daniel E."},{"family":"Hong","given":"Jenny"},{"family":"Hsu","given":"Kyle"},{"family":"Huang","given":"Jing"},{"family":"Icard","given":"Thomas"},{"family":"Jain","given":"Saahil"},{"family":"Jurafsky","given":"Dan"},{"family":"Kalluri","given":"Pratyusha"},{"family":"Karamcheti","given":"Siddharth"},{"family":"Keeling","given":"Geoff"},{"family":"Khani","given":"Fereshte"},{"family":"Khattab","given":"Omar"},{"family":"Koh","given":"Pang Wei"},{"family":"Krass","given":"Mark"},{"family":"Krishna","given":"Ranjay"},{"family":"Kuditipudi","given":"Rohith"},{"family":"Kumar","given":"Ananya"},{"family":"Ladhak","given":"Faisal"},{"family":"Lee","given":"Mina"},{"family":"Lee","given":"Tony"},{"family":"Leskovec","given":"Jure"},{"family":"Levent","given":"Isabelle"},{"family":"Li","given":"Xiang Lisa"},{"family":"Li","given":"Xuechen"},{"family":"Ma","given":"Tengyu"},{"family":"Malik","given":"Ali"},{"family":"Manning","given":"Christopher D."},{"family":"Mirchandani","given":"Suvir"},{"family":"Mitchell","given":"Eric"},{"family":"Munyikwa","given":"Zanele"},{"family":"Nair","given":"Suraj"},{"family":"Narayan","given":"Avanika"},{"family":"Narayanan","given":"Deepak"},{"family":"Newman","given":"Ben"},{"family":"Nie","given":"Allen"},{"family":"Niebles","given":"Juan Carlos"},{"family":"Nilforoshan","given":"Hamed"},{"family":"Nyarko","given":"Julian"},{"family":"Ogut","given":"Giray"},{"family":"Orr","given":"Laurel"},{"family":"Papadimitriou","given":"Isabel"},{"family":"Park","given":"Joon Sung"},{"family":"Piech","given":"Chris"},{"family":"Portelance","given":"Eva"},{"family":"Potts","given":"Christopher"},{"family":"Raghunathan","given":"Aditi"},{"family":"Reich","given":"Rob"},{"family":"Ren","given":"Hongyu"},{"family":"Rong","given":"Frieda"},{"family":"Roohani","given":"Yusuf"},{"family":"Ruiz","given":"Camilo"},{"family":"Ryan","given":"Jack"},{"family":"Ré","given":"Christopher"},{"family":"Sadigh","given":"Dorsa"},{"family":"Sagawa","given":"Shiori"},{"family":"Santhanam","given":"Keshav"},{"family":"Shih","given":"Andy"},{"family":"Srinivasan","given":"Krishnan"},{"family":"Tamkin","given":"Alex"},{"family":"Taori","given":"Rohan"},{"family":"Thomas","given":"Armin W."},{"family":"Tramèr","given":"Florian"},{"family":"Wang","given":"Rose E."},{"family":"Wang","given":"William"},{"family":"Wu","given":"Bohan"},{"family":"Wu","given":"Jiajun"},{"family":"Wu","given":"Yuhuai"},{"family":"Xie","given":"Sang Michael"},{"family":"Yasunaga","given":"Michihiro"},{"family":"You","given":"Jiaxuan"},{"family":"Zaharia","given":"Matei"},{"family":"Zhang","given":"Michael"},{"family":"Zhang","given":"Tianyi"},{"family":"Zhang","given":"Xikun"},{"family":"Zhang","given":"Yuhui"},{"family":"Zheng","given":"Lucia"},{"family":"Zhou","given":"Kaitlyn"},{"family":"Liang","given":"Percy"}],"citation-key":"bommasaniOpportunitiesRisksFoundation2021","DOI":"10.48550/arXiv.2108.07258","issued":{"date-parts":[[2021,8,18]]},"number":"arXiv:2108.07258","publisher":"arXiv","source":"arXiv.org","title":"On the Opportunities and Risks of Foundation Models","type":"article","URL":"http://arxiv.org/abs/2108.07258"},
  {"id":"bonaventuraSurveyViewpointSelection2018","abstract":"Viewpoint selection has been an emerging area in computer graphics for some years, and it is now getting maturity with applications in ﬁelds such as scene navigation, scientiﬁc visualization, object recognition, mesh simpliﬁcation, and camera placement. In this survey, we review and compare twenty-two measures to select good views of a polygonal 3D model, classify them using an extension of the categories deﬁned by Secord et al., and evaluate them against the Dutagaci et al. benchmark. Eleven of these measures have not been reviewed in previous surveys. Three out of the ﬁve short-listed best viewpoint measures are directly related to information. We also present in which ﬁelds the different viewpoint measures have been applied. Finally, we provide a publicly available framework where all the viewpoint selection measures are implemented and can be compared against each other.","accessed":{"date-parts":[[2022,5,14]]},"author":[{"family":"Bonaventura","given":"Xavier"},{"family":"Feixas","given":"Miquel"},{"family":"Sbert","given":"Mateu"},{"family":"Chuang","given":"Lewis"},{"family":"Wallraven","given":"Christian"}],"citation-key":"bonaventuraSurveyViewpointSelection2018","container-title":"Entropy","container-title-short":"Entropy","DOI":"10.3390/e20050370","ISSN":"1099-4300","issue":"5","issued":{"date-parts":[[2018,5,16]]},"language":"en","page":"370","source":"DOI.org (Crossref)","title":"A Survey of Viewpoint Selection Methods for Polygonal Models","type":"article-journal","URL":"http://www.mdpi.com/1099-4300/20/5/370","volume":"20"},
  {"id":"borstStepbystepTutorialUsing2017","abstract":"The cognitive architecture ACT-R is at the same time a psychological theory and a modeling framework for constructing cognitive models that adhere to the principles of the theory. ACT-R can be used in combination with fMRI data in two different ways: (1) fMRI data can be used to evaluate and constrain models in ACT-R by means of predefined Region-of-Interest (ROI) analysis, and (2) predictions from ACT-R models can be used to locate neural correlates of model processes and representations by means of modelbased fMRI analysis. In this paper we provide a step-by-step tutorial on both approaches. Note that this tutorial neither teaches the ACT-R theory in any detail, nor fMRI analysis, but explains how ACT-R can be used in combination with fMRI data. To this end, we provide all data and computer code necessary to run the ACT-R model, carry out the analyses, and recreate the figures in the paper. As an example dataset we use a relatively simple algebra task. In the first section, we develop an ACT-R model of this task and fit it to behavioral data. In the second section, we apply a predefined ROI-analysis to evaluate the model using fMRI data. In the third section, we use model-based fMRI analysis to locate the following processes in the brain: retrieval of mathematical facts from memory, working memory updates, motor responses, and visually encoding the problems. After working through this tutorial, the reader will have learned what can be achieved with the two different analysis methods and how they are conducted; the example code can then be adapted to a new dataset.","accessed":{"date-parts":[[2022,5,27]]},"author":[{"family":"Borst","given":"Jelmer P."},{"family":"Anderson","given":"John R."}],"citation-key":"borstStepbystepTutorialUsing2017","container-title":"Journal of Mathematical Psychology","container-title-short":"Journal of Mathematical Psychology","DOI":"10.1016/j.jmp.2016.05.005","ISSN":"00222496","issued":{"date-parts":[[2017,2]]},"language":"en","page":"94-103","source":"DOI.org (Crossref)","title":"A step-by-step tutorial on using the cognitive architecture ACT-R in combination with fMRI data","type":"article-journal","URL":"https://linkinghub.elsevier.com/retrieve/pii/S0022249616300293","volume":"76"},
  {"id":"brandoneChildrenDevelopingIntuitions2015","abstract":"Generic statements express generalizations about categories and present a unique semantic proﬁle that is distinct from quantiﬁed statements. This paper reports two studies examining the development of children’s intuitions about the semantics of generics and how they differ from statements quantiﬁed by all, most, and some. Results reveal that, like adults, preschoolers (a) recognize that generics have ﬂexible truth conditions and are capable of representing a wide range of prevalence levels; and (b) interpret novel generics as having near-universal prevalence implications. Results further show that by age 4, children are beginning to differentiate the meaning of generics and quantiﬁed statements; however, even 7- to 11-year-olds are not adultlike in their intuitions about the meaning of most-quantiﬁed statements. Overall, these studies suggest that by preschool, children interpret generics in much the same way that adults do; however, mastery of the semantics of quantiﬁed statements follows a more protracted course.","accessed":{"date-parts":[[2022,10,24]]},"author":[{"family":"Brandone","given":"Amanda C."},{"family":"Gelman","given":"Susan A."},{"family":"Hedglen","given":"Jenna"}],"citation-key":"brandoneChildrenDevelopingIntuitions2015","container-title":"Cognitive Science","container-title-short":"Cogn Sci","DOI":"10.1111/cogs.12176","ISSN":"03640213","issue":"4","issued":{"date-parts":[[2015,5]]},"language":"en","page":"711-738","source":"DOI.org (Crossref)","title":"Children's Developing Intuitions About the Truth Conditions and Implications of Novel Generics Versus Quantified Statements","type":"article-journal","URL":"https://onlinelibrary.wiley.com/doi/10.1111/cogs.12176","volume":"39"},
  {"id":"brockGenerativeDiscriminativeVoxel2016","author":[{"family":"Brock","given":"Andrew"},{"family":"Lim","given":"Theodore"},{"family":"Ritchie","given":"James M"},{"family":"Weston","given":"Nick"}],"citation-key":"brockGenerativeDiscriminativeVoxel2016","container-title":"arXiv preprint arXiv:1608.04236","issued":{"date-parts":[[2016]]},"title":"Generative and discriminative voxel modeling with convolutional neural networks","type":"article-journal"},
  {"id":"brownLanguageModelsAre2020","abstract":"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.","accessed":{"date-parts":[[2022,10,21]]},"author":[{"family":"Brown","given":"Tom B."},{"family":"Mann","given":"Benjamin"},{"family":"Ryder","given":"Nick"},{"family":"Subbiah","given":"Melanie"},{"family":"Kaplan","given":"Jared"},{"family":"Dhariwal","given":"Prafulla"},{"family":"Neelakantan","given":"Arvind"},{"family":"Shyam","given":"Pranav"},{"family":"Sastry","given":"Girish"},{"family":"Askell","given":"Amanda"},{"family":"Agarwal","given":"Sandhini"},{"family":"Herbert-Voss","given":"Ariel"},{"family":"Krueger","given":"Gretchen"},{"family":"Henighan","given":"Tom"},{"family":"Child","given":"Rewon"},{"family":"Ramesh","given":"Aditya"},{"family":"Ziegler","given":"Daniel M."},{"family":"Wu","given":"Jeffrey"},{"family":"Winter","given":"Clemens"},{"family":"Hesse","given":"Christopher"},{"family":"Chen","given":"Mark"},{"family":"Sigler","given":"Eric"},{"family":"Litwin","given":"Mateusz"},{"family":"Gray","given":"Scott"},{"family":"Chess","given":"Benjamin"},{"family":"Clark","given":"Jack"},{"family":"Berner","given":"Christopher"},{"family":"McCandlish","given":"Sam"},{"family":"Radford","given":"Alec"},{"family":"Sutskever","given":"Ilya"},{"family":"Amodei","given":"Dario"}],"citation-key":"brownLanguageModelsAre2020","DOI":"10.48550/arXiv.2005.14165","issued":{"date-parts":[[2020,7,22]]},"number":"arXiv:2005.14165","publisher":"arXiv","source":"arXiv.org","title":"Language Models are Few-Shot Learners","type":"article","URL":"http://arxiv.org/abs/2005.14165"},
  {"id":"brownUnionManifoldsHypothesis2022","abstract":"Deep learning has had tremendous success at learning low-dimensional representations of high-dimensional data. This success would be impossible if there was no hidden low-dimensional structure in data of interest; this existence is posited by the manifold hypothesis, which states that the data lies on an unknown manifold of low intrinsic dimension. In this paper, we argue that this hypothesis does not properly capture the low-dimensional structure typically present in data. Assuming the data lies on a single manifold implies intrinsic dimension is identical across the entire data space, and does not allow for subregions of this space to have a different number of factors of variation. To address this deﬁciency, we put forth the union of manifolds hypothesis, which accommodates the existence of non-constant intrinsic dimensions. We empirically verify this hypothesis on commonly-used image datasets, ﬁnding that indeed, intrinsic dimension should be allowed to vary. We also show that classes with higher intrinsic dimensions are harder to classify, and how this insight can be used to improve classiﬁcation accuracy. We then turn our attention to the impact of this hypothesis in the context of deep generative models (DGMs). Most current DGMs struggle to model datasets with several connected components and/or varying intrinsic dimensions. To tackle these shortcomings, we propose clustered DGMs, where we ﬁrst cluster the data and then train a DGM on each cluster. We show that clustered DGMs can model multiple connected components with different intrinsic dimensions, and empirically outperform their non-clustered counterparts without increasing computational requirements.","accessed":{"date-parts":[[2022,11,15]]},"author":[{"family":"Brown","given":"Bradley C. A."},{"family":"Caterini","given":"Anthony L."},{"family":"Ross","given":"Brendan Leigh"},{"family":"Cresswell","given":"Jesse C."},{"family":"Loaiza-Ganem","given":"Gabriel"}],"citation-key":"brownUnionManifoldsHypothesis2022","issued":{"date-parts":[[2022,7,6]]},"language":"en","number":"arXiv:2207.02862","publisher":"arXiv","source":"arXiv.org","title":"The Union of Manifolds Hypothesis and its Implications for Deep Generative Modelling","type":"article","URL":"http://arxiv.org/abs/2207.02862"},
  {"id":"buhrmesterAnalysisExplainersBlack2021","abstract":"Deep Learning is a state-of-the-art technique to make inference on extensive or complex data. As a black box model due to their multilayer nonlinear structure, Deep Neural Networks are often criticized as being non-transparent and their predictions not traceable by humans. Furthermore, the models learn from artificially generated datasets, which often do not reflect reality. By basing decision-making algorithms on Deep Neural Networks, prejudice and unfairness may be promoted unknowingly due to a lack of transparency. Hence, several so-called explanators, or explainers, have been developed. Explainers try to give insight into the inner structure of machine learning black boxes by analyzing the connection between the input and output. In this survey, we present the mechanisms and properties of explaining systems for Deep Neural Networks for Computer Vision tasks. We give a comprehensive overview about the taxonomy of related studies and compare several survey papers that deal with explainability in general. We work out the drawbacks and gaps and summarize further research ideas.","accessed":{"date-parts":[[2023,2,20]]},"author":[{"family":"Buhrmester","given":"Vanessa"},{"family":"Münch","given":"David"},{"family":"Arens","given":"Michael"}],"citation-key":"buhrmesterAnalysisExplainersBlack2021","container-title":"Machine Learning and Knowledge Extraction","DOI":"10.3390/make3040048","ISSN":"2504-4990","issue":"4","issued":{"date-parts":[[2021,12]]},"language":"en","license":"http://creativecommons.org/licenses/by/3.0/","number":"4","page":"966-989","publisher":"Multidisciplinary Digital Publishing Institute","source":"www.mdpi.com","title":"Analysis of Explainers of Black Box Deep Neural Networks for Computer Vision: A Survey","title-short":"Analysis of Explainers of Black Box Deep Neural Networks for Computer Vision","type":"article-journal","URL":"https://www.mdpi.com/2504-4990/3/4/48","volume":"3"},
  {"id":"cannyComputationalApproachEdge1986","author":[{"family":"Canny","given":"John"}],"citation-key":"cannyComputationalApproachEdge1986","container-title":"IEEE Transactions on pattern analysis and machine intelligence","issue":"6","issued":{"date-parts":[[1986]]},"page":"679–698","publisher":"Ieee","title":"A computational approach to edge detection","type":"article-journal"},
  {"id":"caoReMixImagetoImageTranslation2021","accessed":{"date-parts":[[2023,3,31]]},"author":[{"family":"Cao","given":"Jie"},{"family":"Hou","given":"Luanxuan"},{"family":"Yang","given":"Ming-Hsuan"},{"family":"He","given":"Ran"},{"family":"Sun","given":"Zhenan"}],"citation-key":"caoReMixImagetoImageTranslation2021","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[[2021]]},"language":"en","page":"15018-15027","source":"openaccess.thecvf.com","title":"ReMix: Towards Image-to-Image Translation With Limited Data","title-short":"ReMix","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2021/html/Cao_ReMix_Towards_Image-to-Image_Translation_With_Limited_Data_CVPR_2021_paper.html"},
  {"id":"caytonAlgorithmsManifoldLearning","abstract":"Manifold learning is a popular recent approach to nonlinear dimensionality reduction. Algorithms for this task are based on the idea that the dimensionality of many data sets is only artiﬁcially high; though each data point consists of perhaps thousands of features, it may be described as a function of only a few underlying parameters. That is, the data points are actually samples from a low-dimensional manifold that is embedded in a high-dimensional space. Manifold learning algorithms attempt to uncover these parameters in order to ﬁnd a low-dimensional representation of the data. In this paper, we discuss the motivation, background, and algorithms proposed for manifold learning. Isomap, Locally Linear Embedding, Laplacian Eigenmaps, Semideﬁnite Embedding, and a host of variants of these algorithms are examined.","author":[{"family":"Cayton","given":"Lawrence"}],"citation-key":"caytonAlgorithmsManifoldLearning","language":"en","page":"17","source":"Zotero","title":"Algorithms for manifold learning","type":"article-journal"},
  {"id":"chakrabortyGeneralizingAdversarialExplanations2022","abstract":"Gradient-weighted Class Activation Mapping (GradCAM), is an example-based explanation method that provides a gradient activation heat map as an explanation for Convolution Neural Network (CNN) models. The drawback of this method is that it cannot be used to generalize CNN behaviour. In this paper, we present a novel method that extends Grad-CAM from example-based explanations to a method for explaining global model behaviour. This is achieved by introducing two new metrics, (i) Mean Observed Dissimilarity (MOD) and (ii) Variation in Dissimilarity (VID), for model generalization. These metrics are computed by comparing a Normalized Inverted Structural Similarity Index (NISSIM) metric of the Grad-CAM generated heatmap for samples from the original test set and samples from the adversarial test set. For our experiment, we study adversarial attacks on deep models such as VGG16, ResNet50, and ResNet101, and wide models such as InceptionNetv3 and XceptionNet using Fast Gradient Sign Method (FGSM). We then compute the metrics MOD and VID for the automatic face recognition (AFR) use case with the VGGFace2 dataset. We observe a consistent shift in the region highlighted in the Grad-CAM heatmap, reflecting its participation to the decision making, across all models under adversarial attacks. The proposed method can be used to understand adversarial attacks and explain the behaviour of black box CNN models for image analysis.","accessed":{"date-parts":[[2022,10,3]]},"author":[{"family":"Chakraborty","given":"Tanmay"},{"family":"Trehan","given":"Utkarsh"},{"family":"Mallat","given":"Khawla"},{"family":"Dugelay","given":"Jean-Luc"}],"citation-key":"chakrabortyGeneralizingAdversarialExplanations2022","container-title":"2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","DOI":"10.1109/CVPRW56347.2022.00031","event-place":"New Orleans, LA, USA","event-title":"2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","ISBN":"978-1-66548-739-9","issued":{"date-parts":[[2022,6]]},"language":"en","page":"186-192","publisher":"IEEE","publisher-place":"New Orleans, LA, USA","source":"DOI.org (Crossref)","title":"Generalizing Adversarial Explanations with Grad-CAM","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/9857321/"},
  {"id":"chattopadhayGradCAMGeneralizedGradientBased2018","accessed":{"date-parts":[[2023,2,20]]},"author":[{"family":"Chattopadhay","given":"Aditya"},{"family":"Sarkar","given":"Anirban"},{"family":"Howlader","given":"Prantik"},{"family":"Balasubramanian","given":"Vineeth N"}],"citation-key":"chattopadhayGradCAMGeneralizedGradientBased2018","container-title":"2018 IEEE Winter Conference on Applications of Computer Vision (WACV)","DOI":"10.1109/WACV.2018.00097","event-place":"Lake Tahoe, NV","event-title":"2018 IEEE Winter Conference on Applications of Computer Vision (WACV)","ISBN":"978-1-5386-4886-5","issued":{"date-parts":[[2018,3]]},"page":"839-847","publisher":"IEEE","publisher-place":"Lake Tahoe, NV","source":"DOI.org (Crossref)","title":"Grad-CAM++: Generalized Gradient-Based Visual Explanations for Deep Convolutional Networks","title-short":"Grad-CAM++","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/8354201/"},
  {"id":"chattopadhyayGradCAMImprovedVisual2018","abstract":"Over the last decade, Convolutional Neural Network (CNN) models have been highly successful in solving complex vision problems. However, these deep models are perceived as \"black box\" methods considering the lack of understanding of their internal functioning. There has been a significant recent interest in developing explainable deep learning models, and this paper is an effort in this direction. Building on a recently proposed method called Grad-CAM, we propose a generalized method called Grad-CAM++ that can provide better visual explanations of CNN model predictions, in terms of better object localization as well as explaining occurrences of multiple object instances in a single image, when compared to state-of-the-art. We provide a mathematical derivation for the proposed method, which uses a weighted combination of the positive partial derivatives of the last convolutional layer feature maps with respect to a specific class score as weights to generate a visual explanation for the corresponding class label. Our extensive experiments and evaluations, both subjective and objective, on standard datasets showed that Grad-CAM++ provides promising human-interpretable visual explanations for a given CNN architecture across multiple tasks including classification, image caption generation and 3D action recognition; as well as in new settings such as knowledge distillation.","accessed":{"date-parts":[[2022,10,3]]},"author":[{"family":"Chattopadhyay","given":"Aditya"},{"family":"Sarkar","given":"Anirban"},{"family":"Howlader","given":"Prantik"},{"family":"Balasubramanian","given":"Vineeth N."}],"citation-key":"chattopadhyayGradCAMImprovedVisual2018","container-title":"2018 IEEE Winter Conference on Applications of Computer Vision (WACV)","DOI":"10.1109/WACV.2018.00097","issued":{"date-parts":[[2018,3]]},"page":"839-847","source":"arXiv.org","title":"Grad-CAM++: Improved Visual Explanations for Deep Convolutional Networks","title-short":"Grad-CAM++","type":"paper-conference","URL":"http://arxiv.org/abs/1710.11063"},
  {"id":"chattopadhyayGradCAMImprovedVisual2018a","abstract":"Over the last decade, Convolutional Neural Network (CNN) models have been highly successful in solving complex vision problems. However, these deep models are perceived as ”black box” methods considering the lack of understanding of their internal functioning. There has been a signiﬁcant recent interest in developing explainable deep learning models, and this paper is an effort in this direction. Building on a recently proposed method called Grad-CAM, we propose a generalized method called Grad-CAM++ that can provide better visual explanations of CNN model predictions, in terms of better object localization as well as explaining occurrences of multiple object instances in a single image, when compared to state-of-the-art. We provide a mathematical derivation for the proposed method, which uses a weighted combination of the positive partial derivatives of the last convolutional layer feature maps with respect to a speciﬁc class score as weights to generate a visual explanation for the corresponding class label. Our extensive experiments and evaluations, both subjective and objective, on standard datasets showed that Grad-CAM++ provides promising human-interpretable visual explanations for a given CNN architecture across multiple tasks including classiﬁcation, image caption generation and 3D action recognition; as well as in new settings such as knowledge distillation.","accessed":{"date-parts":[[2023,2,20]]},"author":[{"family":"Chattopadhyay","given":"Aditya"},{"family":"Sarkar","given":"Anirban"},{"family":"Howlader","given":"Prantik"},{"family":"Balasubramanian","given":"Vineeth N."}],"citation-key":"chattopadhyayGradCAMImprovedVisual2018a","container-title":"2018 IEEE Winter Conference on Applications of Computer Vision (WACV)","DOI":"10.1109/WACV.2018.00097","issued":{"date-parts":[[2018,3]]},"language":"en","page":"839-847","source":"arXiv.org","title":"Grad-CAM++: Improved Visual Explanations for Deep Convolutional Networks","title-short":"Grad-CAM++","type":"paper-conference","URL":"http://arxiv.org/abs/1710.11063"},
  {"id":"chengImproveDeepLearning2022","abstract":"In forestry studies, deep learning models have achieved excellent performance in many application scenarios (e.g., detecting forest damage). However, the unclear model decisions (i.e., black-box) undermine the credibility of the results and hinder their practicality. This study intends to obtain explanations of such models through the use of explainable artificial intelligence methods, and then use feature unlearning methods to improve their performance, which is the first such attempt in the field of forestry. Results of three experiments show that the model training can be guided by expertise to gain specific knowledge, which is reflected by explanations. For all three experiments based on synthetic and real leaf images, the improvement of models is quantified in the classification accuracy (up to 4.6%) and three indicators of explanation assessment (i.e., root-mean-square error, cosine similarity, and the proportion of important pixels). Besides, the introduced expertise in annotation matrix form was automatically created in all experiments. This study emphasizes that studies of deep learning in forestry should not only pursue model performance (e.g., higher classification accuracy) but also focus on the explanations and try to improve models according to the expertise.","accessed":{"date-parts":[[2022,9,26]]},"author":[{"family":"Cheng","given":"Ximeng"},{"family":"Doosthosseini","given":"Ali"},{"family":"Kunkel","given":"Julian"}],"citation-key":"chengImproveDeepLearning2022","container-title":"Frontiers in Plant Science","container-title-short":"Front. Plant Sci.","DOI":"10.3389/fpls.2022.902105","ISSN":"1664-462X","issued":{"date-parts":[[2022,5,19]]},"language":"en","page":"902105","source":"DOI.org (Crossref)","title":"Improve the Deep Learning Models in Forestry Based on Explanations and Expertise","type":"article-journal","URL":"https://www.frontiersin.org/articles/10.3389/fpls.2022.902105/full","volume":"13"},
  {"id":"chenGridMaskDataAugmentation2020","abstract":"We propose a novel data augmentation method `GridMask' in this paper. It utilizes information removal to achieve state-of-the-art results in a variety of computer vision tasks. We analyze the requirement of information dropping. Then we show limitation of existing information dropping algorithms and propose our structured method, which is simple and yet very effective. It is based on the deletion of regions of the input image. Our extensive experiments show that our method outperforms the latest AutoAugment, which is way more computationally expensive due to the use of reinforcement learning to find the best policies. On the ImageNet dataset for recognition, COCO2017 object detection, and on Cityscapes dataset for semantic segmentation, our method all notably improves performance over baselines. The extensive experiments manifest the effectiveness and generality of the new method.","accessed":{"date-parts":[[2023,3,31]]},"author":[{"family":"Chen","given":"Pengguang"},{"family":"Liu","given":"Shu"},{"family":"Zhao","given":"Hengshuang"},{"family":"Jia","given":"Jiaya"}],"citation-key":"chenGridMaskDataAugmentation2020","DOI":"10.48550/arXiv.2001.04086","issued":{"date-parts":[[2020,1,13]]},"number":"arXiv:2001.04086","publisher":"arXiv","source":"arXiv.org","title":"GridMask Data Augmentation","type":"article","URL":"http://arxiv.org/abs/2001.04086"},
  {"id":"chenVisionTransformerAdapter2022","abstract":"This work investigates a simple yet powerful adapter for Vision Transformer (ViT). Unlike recent visual transformers that introduce vision-specific inductive biases into their architectures, ViT achieves inferior performance on dense prediction tasks due to lacking prior information of images. To solve this issue, we propose a Vision Transformer Adapter (ViT-Adapter), which can remedy the defects of ViT and achieve comparable performance to vision-specific models by introducing inductive biases via an additional architecture. Specifically, the backbone in our framework is a vanilla transformer that can be pre-trained with multi-modal data. When fine-tuning on downstream tasks, a modality-specific adapter is used to introduce the data and tasks' prior information into the model, making it suitable for these tasks. We verify the effectiveness of our ViT-Adapter on multiple downstream tasks, including object detection, instance segmentation, and semantic segmentation. Notably, when using HTC++, our ViT-Adapter-L yields 60.1 box AP and 52.1 mask AP on COCO test-dev, surpassing Swin-L by 1.4 box AP and 1.0 mask AP. For semantic segmentation, our ViT-Adapter-L establishes a new state-of-the-art of 60.5 mIoU on ADE20K val, 0.6 points higher than SwinV2-G. We hope that the proposed ViT-Adapter could serve as an alternative for vision-specific transformers and facilitate future research. The code and models will be released at https://github.com/czczup/ViT-Adapter.","accessed":{"date-parts":[[2022,10,23]]},"author":[{"family":"Chen","given":"Zhe"},{"family":"Duan","given":"Yuchen"},{"family":"Wang","given":"Wenhai"},{"family":"He","given":"Junjun"},{"family":"Lu","given":"Tong"},{"family":"Dai","given":"Jifeng"},{"family":"Qiao","given":"Yu"}],"citation-key":"chenVisionTransformerAdapter2022","DOI":"10.48550/arXiv.2205.08534","issued":{"date-parts":[[2022,5,17]]},"number":"arXiv:2205.08534","publisher":"arXiv","source":"arXiv.org","title":"Vision Transformer Adapter for Dense Predictions","type":"article","URL":"http://arxiv.org/abs/2205.08534"},
  {"id":"cheraghian3DCapsuleExtendingCapsule2019","abstract":"This paper introduces the 3DCapsule, which is a 3D extension of the recently introduced Capsule concept that makes it applicable to unordered point sets. The original Capsule relies on the existence of a spatial relationship between the elements in the feature map it is presented with, whereas in point permutation invariant formulations of 3D point set classiﬁcation methods, such relationships are typically lost. Here, a new layer called ComposeCaps is introduced that, in lieu of a spatially relevant feature mapping, learns a new mapping that can be exploited by the 3DCapsule. Previous works in the 3D point set classiﬁcation domain have focused on other parts of the architecture, whereas instead, the 3DCapsule is a drop-in replacement of the commonly used fully connected classiﬁer. It is demonstrated via an ablation study, that when the 3DCapsule is applied to recent 3D point set classiﬁcation architectures, it consistently shows an improvement, in particular when subjected to noisy data. Similarly, the ComposeCaps layer is evaluated and demonstrates an improvement over the baseline. In an apples-to-apples comparison against state-ofthe-art methods, again, better performance is demonstrated by the 3DCapsule.","accessed":{"date-parts":[[2022,5,10]]},"author":[{"family":"Cheraghian","given":"Ali"},{"family":"Petersson","given":"Lars"}],"citation-key":"cheraghian3DCapsuleExtendingCapsule2019","container-title":"2019 IEEE Winter Conference on Applications of Computer Vision (WACV)","DOI":"10.1109/WACV.2019.00132","event-place":"Waikoloa Village, HI, USA","event-title":"2019 IEEE Winter Conference on Applications of Computer Vision (WACV)","ISBN":"978-1-72811-975-5","issued":{"date-parts":[[2019,1]]},"language":"en","page":"1194-1202","publisher":"IEEE","publisher-place":"Waikoloa Village, HI, USA","source":"DOI.org (Crossref)","title":"3DCapsule: Extending the Capsule Architecture to Classify 3D Point Clouds","title-short":"3DCapsule","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/8658405/"},
  {"id":"choiAnalyzingLatentSpace2022","abstract":"The impressive success of style-based GANs (StyleGANs) in high-fidelity image synthesis has motivated research to understand the semantic properties of their latent spaces. Recently, a close relationship was observed between the semantically disentangled local perturbations and the local PCA components in the learned latent space $\\mathcal{W}$. However, understanding the number of disentangled perturbations remains challenging. Building upon this observation, we propose a local dimension estimation algorithm for an arbitrary intermediate layer in a pre-trained GAN model. The estimated intrinsic dimension corresponds to the number of disentangled local perturbations. In this perspective, we analyze the intermediate layers of the mapping network in StyleGANs. Our analysis clarifies the success of $\\mathcal{W}$-space in StyleGAN and suggests an alternative. Moreover, the intrinsic dimension estimation opens the possibility of unsupervised evaluation of global-basis-compatibility and disentanglement for a latent space. Our proposed metric, called Distortion, measures an inconsistency of intrinsic tangent space on the learned latent space. The metric is purely geometric and does not require any additional attribute information. Nevertheless, the metric shows a high correlation with the global-basis-compatibility and supervised disentanglement score. Our findings pave the way towards an unsupervised selection of globally disentangled latent space among the intermediate latent spaces in a GAN.","accessed":{"date-parts":[[2022,9,6]]},"author":[{"family":"Choi","given":"Jaewoong"},{"family":"Hwang","given":"Geonho"},{"family":"Cho","given":"Hyunsoo"},{"family":"Kang","given":"Myungjoo"}],"citation-key":"choiAnalyzingLatentSpace2022","DOI":"10.48550/arXiv.2205.13182","issued":{"date-parts":[[2022,5,26]]},"number":"arXiv:2205.13182","publisher":"arXiv","source":"arXiv.org","title":"Analyzing the Latent Space of GAN through Local Dimension Estimation","type":"article","URL":"http://arxiv.org/abs/2205.13182"},
  {"id":"cholletXceptionDeepLearning2017","abstract":"We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and signiﬁcantly outperforms Inception V3 on a larger image classiﬁcation dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efﬁcient use of model parameters.","accessed":{"date-parts":[[2023,2,20]]},"author":[{"family":"Chollet","given":"Francois"}],"citation-key":"cholletXceptionDeepLearning2017","container-title":"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","DOI":"10.1109/CVPR.2017.195","event-place":"Honolulu, HI","event-title":"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","ISBN":"978-1-5386-0457-1","issued":{"date-parts":[[2017,7]]},"language":"en","page":"1800-1807","publisher":"IEEE","publisher-place":"Honolulu, HI","source":"DOI.org (Crossref)","title":"Xception: Deep Learning with Depthwise Separable Convolutions","title-short":"Xception","type":"paper-conference","URL":"http://ieeexplore.ieee.org/document/8099678/"},
  {"id":"cohenSeparabilityGeometryObject2020","abstract":"Abstract\n            Stimuli are represented in the brain by the collective population responses of sensory neurons, and an object presented under varying conditions gives rise to a collection of neural population responses called an ‘object manifold’. Changes in the object representation along a hierarchical sensory system are associated with changes in the geometry of those manifolds, and recent theoretical progress connects this geometry with ‘classification capacity’, a quantitative measure of the ability to support object classification. Deep neural networks trained on object classification tasks are a natural testbed for the applicability of this relation. We show how classification capacity improves along the hierarchies of deep neural networks with different architectures. We demonstrate that changes in the geometry of the associated object manifolds underlie this improved capacity, and shed light on the functional roles different levels in the hierarchy play to achieve it, through orchestrated reduction of manifolds’ radius, dimensionality and inter-manifold correlations.","accessed":{"date-parts":[[2022,11,15]]},"author":[{"family":"Cohen","given":"Uri"},{"family":"Chung","given":"SueYeon"},{"family":"Lee","given":"Daniel D."},{"family":"Sompolinsky","given":"Haim"}],"citation-key":"cohenSeparabilityGeometryObject2020","container-title":"Nature Communications","container-title-short":"Nat Commun","DOI":"10.1038/s41467-020-14578-5","ISSN":"2041-1723","issue":"1","issued":{"date-parts":[[2020,12]]},"language":"en","page":"746","source":"DOI.org (Crossref)","title":"Separability and geometry of object manifolds in deep neural networks","type":"article-journal","URL":"http://www.nature.com/articles/s41467-020-14578-5","volume":"11"},
  {"id":"corniaPredictingHumanEye2018","abstract":"Data-driven saliency has recently gained a lot of attention thanks to the use of convolutional neural networks for predicting gaze fixations. In this paper, we go beyond standard approaches to saliency prediction, in which gaze maps are computed with a feed-forward network, and present a novel model which can predict accurate saliency maps by incorporating neural attentive mechanisms. The core of our solution is a convolutional long short-term memory that focuses on the most salient regions of the input image to iteratively refine the predicted saliency map. In addition, to tackle the center bias typical of human eye fixations, our model can learn a set of prior maps generated with Gaussian functions. We show, through an extensive evaluation, that the proposed architecture outperforms the current state-of-the-art on public saliency prediction datasets. We further study the contribution of each key component to demonstrate their robustness on different scenarios.","author":[{"family":"Cornia","given":"Marcella"},{"family":"Baraldi","given":"Lorenzo"},{"family":"Serra","given":"Giuseppe"},{"family":"Cucchiara","given":"Rita"}],"citation-key":"corniaPredictingHumanEye2018","container-title":"IEEE Transactions on Image Processing","DOI":"10.1109/TIP.2018.2851672","ISSN":"1941-0042","issue":"10","issued":{"date-parts":[[2018,10]]},"page":"5142-5154","source":"IEEE Xplore","title":"Predicting Human Eye Fixations via an LSTM-Based Saliency Attentive Model","type":"article-journal","volume":"27"},
  {"id":"corniaSAMPushingLimits2018","abstract":"The prediction of human eye ﬁxations has been recently gaining a lot of attention thanks to the improvements shown by deep architectures. In our work, we go beyond classical feed-forward networks to predict saliency maps and propose a Saliency Attentive Model which incorporates neural attention mechanisms to iteratively reﬁne predictions. Experiments demonstrate that the proposed strategy overcomes by a considerable margin the state of the art on the largest dataset available for saliency prediction. Here, we provide experimental results on other popular saliency datasets to conﬁrm the effectiveness and the generalization capabilities of our model, which enable us to reach the state of the art on all considered datasets.","accessed":{"date-parts":[[2023,5,8]]},"author":[{"family":"Cornia","given":"Marcella"},{"family":"Baraldi","given":"Lorenzo"},{"family":"Serra","given":"Giuseppe"},{"family":"Cucchiara","given":"Rita"}],"citation-key":"corniaSAMPushingLimits2018","container-title":"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","DOI":"10.1109/CVPRW.2018.00250","event-place":"Salt Lake City, UT, USA","event-title":"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","ISBN":"978-1-5386-6100-0","issued":{"date-parts":[[2018,6]]},"language":"en","page":"1971-19712","publisher":"IEEE","publisher-place":"Salt Lake City, UT, USA","source":"DOI.org (Crossref)","title":"SAM: Pushing the Limits of Saliency Prediction Models","title-short":"SAM","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/8575416/"},
  {"id":"cubukRandaugmentPracticalAutomated2020","abstract":"Recent work on automated augmentation strategies has led to state-of-the-art results in image classiﬁcation and object detection. An obstacle to a large-scale adoption of these methods is that they require a separate and expensive search phase. A common way to overcome the expense of the search phase was to use a smaller proxy task. However, it was not clear if the optimized hyperparameters found on the proxy task are also optimal for the actual task. In this work, we rethink the process of designing automated augmentation strategies. We ﬁnd that while previous work required a search for both magnitude and probability of each operation independently, it is sufﬁcient to only search for a single distortion magnitude that jointly controls all operations. We hence propose a simpliﬁed search space that vastly reduces the computational expense of automated augmentation, and permits the removal of a separate proxy task.","accessed":{"date-parts":[[2023,1,16]]},"author":[{"family":"Cubuk","given":"Ekin D."},{"family":"Zoph","given":"Barret"},{"family":"Shlens","given":"Jonathon"},{"family":"Le","given":"Quoc V."}],"citation-key":"cubukRandaugmentPracticalAutomated2020","container-title":"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","DOI":"10.1109/CVPRW50498.2020.00359","event-place":"Seattle, WA, USA","event-title":"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","ISBN":"978-1-72819-360-1","issued":{"date-parts":[[2020,6]]},"language":"en","page":"3008-3017","publisher":"IEEE","publisher-place":"Seattle, WA, USA","source":"DOI.org (Crossref)","title":"Randaugment: Practical automated data augmentation with a reduced search space","title-short":"Randaugment","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/9150790/"},
  {"id":"dabkowskiRealTimeImage2017","abstract":"In this work we develop a fast saliency detection method that can be applied to any differentiable image classifier. We train a masking model to manipulate the scores of the classifier by masking salient parts of the input image. Our model generalises well to unseen images and requires a single forward pass to perform saliency detection, therefore suitable for use in real-time systems. We test our approach on CIFAR-10 and ImageNet datasets and show that the produced saliency maps are easily interpretable, sharp, and free of artifacts. We suggest a new metric for saliency and test our method on the ImageNet object localisation task. We achieve results outperforming other weakly supervised methods.","accessed":{"date-parts":[[2022,11,24]]},"author":[{"family":"Dabkowski","given":"Piotr"},{"family":"Gal","given":"Yarin"}],"citation-key":"dabkowskiRealTimeImage2017","DOI":"10.48550/arXiv.1705.07857","issued":{"date-parts":[[2017,5,22]]},"number":"arXiv:1705.07857","publisher":"arXiv","source":"arXiv.org","title":"Real Time Image Saliency for Black Box Classifiers","type":"article","URL":"http://arxiv.org/abs/1705.07857"},
  {"id":"darnaiInternetAddictionFunctional2019","abstract":"A common brain-related feature of addictions is the altered function of higher-order brain networks. Growing evidence suggests that Internet-related addictions are also associated with breakdown of functional brain networks. Taking into consideration the limited number of studies used in previous studies in Internet addiction (IA), our aim was to investigate the functional correlates of IA in the default mode network (DMN) and in the inhibitory control network (ICN). To observe these relationships, task-related fMRI responses to verbal Stroop and non-verbal Stroop-like tasks were measured in 60 healthy university students. The Problematic Internet Use Questionnaire (PIUQ) was used to assess IA. We found significant deactivations in areas related to the DMN (precuneus, posterior cingulate gyrus) and these areas were negatively correlated with PIUQ during incongruent stimuli. In Stroop task the incongruent_minus_congruent contrast showed positive correlation with PIUQ in areas related to the ICN (left inferior frontal gyrus, left frontal pole, left central opercular, left frontal opercular, left frontal orbital and left insular cortex). Altered DMN might explain some comorbid symptoms and might predict treatment outcomes, while altered ICN may be the reason for having difficulties in stopping and controlling overuse.","accessed":{"date-parts":[[2022,6,15]]},"author":[{"family":"Darnai","given":"Gergely"},{"family":"Perlaki","given":"Gábor"},{"family":"Zsidó","given":"András N."},{"family":"Inhóf","given":"Orsolya"},{"family":"Orsi","given":"Gergely"},{"family":"Horváth","given":"Réka"},{"family":"Nagy","given":"Szilvia Anett"},{"family":"Lábadi","given":"Beatrix"},{"family":"Tényi","given":"Dalma"},{"family":"Kovács","given":"Norbert"},{"family":"Dóczi","given":"Tamás"},{"family":"Demetrovics","given":"Zsolt"},{"family":"Janszky","given":"József"}],"citation-key":"darnaiInternetAddictionFunctional2019","container-title":"Scientific Reports","container-title-short":"Sci Rep","DOI":"10.1038/s41598-019-52296-1","ISSN":"2045-2322","issue":"1","issued":{"date-parts":[[2019,10,31]]},"language":"en","license":"2019 The Author(s)","number":"1","page":"15777","publisher":"Nature Publishing Group","source":"www.nature.com","title":"Internet addiction and functional brain networks: task-related fMRI study","title-short":"Internet addiction and functional brain networks","type":"article-journal","URL":"https://www.nature.com/articles/s41598-019-52296-1","volume":"9"},
  {"id":"darnaiNeuralCorrelatesMental2023","abstract":"Increasing time spent on the task (i.e., the time-on-task (ToT) effect) often results in mental fatigue. Typical effects of ToT are decreasing levels of task-related motivation and the deterioration of cognitive performance. However, a massive body of research indicates that the detrimental effects can be reversed by extrinsic motivators, for example, providing rewards to fatigued participants. Although several attempts have been made to identify brain areas involved in mental fatigue and related reward processing, the neural correlates are still less understood. In this study, we used the psychomotor vigilance task to induce mental fatigue and blood oxygen-level-dependent functional magnetic resonance imaging to investigate the neural correlates of the ToT effect and the reward effect (i.e., providing extra monetary reward after fatigue induction) in a healthy young sample. Our results were interpreted in a recently proposed neurocognitive framework. The activation of the right middle frontal gyrus, right insula and right anterior cingulate gyrus decreased as fatigue emerged and the cognitive performance dropped. However, after providing an extra reward, the cognitive performance, as well as activation of these areas, increased. Moreover, the activation levels of all of the mentioned areas were negatively associated with reaction times. Our results confirm that the middle frontal gyrus, insula and anterior cingulate cortex play crucial roles in cost-benefit evaluations, a potential background mechanism underlying fatigue, as suggested by the neurocognitive framework.","accessed":{"date-parts":[[2023,1,25]]},"author":[{"family":"Darnai","given":"Gergely"},{"family":"Matuz","given":"András"},{"family":"Alhour","given":"Husamalddin Ali"},{"family":"Perlaki","given":"Gábor"},{"family":"Orsi","given":"Gergely"},{"family":"Arató","given":"Ákos"},{"family":"Szente","given":"Anna"},{"family":"Áfra","given":"Eszter"},{"family":"Nagy","given":"Szilvia Anett"},{"family":"Janszky","given":"József"},{"family":"Csathó","given":"Árpád"}],"citation-key":"darnaiNeuralCorrelatesMental2023","container-title":"NeuroImage","container-title-short":"NeuroImage","DOI":"10.1016/j.neuroimage.2022.119812","ISSN":"1053-8119","issued":{"date-parts":[[2023,1,1]]},"language":"en","page":"119812","source":"ScienceDirect","title":"The neural correlates of mental fatigue and reward processing: A task-based fMRI study","title-short":"The neural correlates of mental fatigue and reward processing","type":"article-journal","URL":"https://www.sciencedirect.com/science/article/pii/S1053811922009338","volume":"265"},
  {"id":"dasOpportunitiesChallengesExplainable2020","abstract":"Nowadays, deep neural networks are widely used in mission critical systems such as healthcare, self-driving vehicles, and military which have direct impact on human lives. However, the black-box nature of deep neural networks challenges its use in mission critical applications, raising ethical and judicial concerns inducing lack of trust. Explainable Artificial Intelligence (XAI) is a field of Artificial Intelligence (AI) that promotes a set of tools, techniques, and algorithms that can generate high-quality interpretable, intuitive, human-understandable explanations of AI decisions. In addition to providing a holistic view of the current XAI landscape in deep learning, this paper provides mathematical summaries of seminal work. We start by proposing a taxonomy and categorizing the XAI techniques based on their scope of explanations, methodology behind the algorithms, and explanation level or usage which helps build trustworthy, interpretable, and self-explanatory deep learning models. We then describe the main principles used in XAI research and present the historical timeline for landmark studies in XAI from 2007 to 2020. After explaining each category of algorithms and approaches in detail, we then evaluate the explanation maps generated by eight XAI algorithms on image data, discuss the limitations of this approach, and provide potential future directions to improve XAI evaluation.","accessed":{"date-parts":[[2023,1,17]]},"author":[{"family":"Das","given":"Arun"},{"family":"Rad","given":"Paul"}],"citation-key":"dasOpportunitiesChallengesExplainable2020","DOI":"10.48550/arXiv.2006.11371","issued":{"date-parts":[[2020,6,22]]},"number":"arXiv:2006.11371","publisher":"arXiv","source":"arXiv.org","title":"Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey","title-short":"Opportunities and Challenges in Explainable Artificial Intelligence (XAI)","type":"article","URL":"http://arxiv.org/abs/2006.11371"},
  {"id":"dattaWhoThinkingPush2023","abstract":"Deployed artificial intelligence (AI) often impacts humans, and there is no one-size-fits-all metric to evaluate these tools. Human-centered evaluation of AI-based systems combines quantitative and qualitative analysis and human input. It has been explored to some depth in the explainable AI (XAI) and human-computer interaction (HCI) communities. Gaps remain, but the basic understanding that humans interact with AI and accompanying explanations, and that humans' needs -- complete with their cognitive biases and quirks -- should be held front and center, is accepted by the community. In this paper, we draw parallels between the relatively mature field of XAI and the rapidly evolving research boom around large language models (LLMs). Accepted evaluative metrics for LLMs are not human-centered. We argue that many of the same paths tread by the XAI community over the past decade will be retread when discussing LLMs. Specifically, we argue that humans' tendencies -- again, complete with their cognitive biases and quirks -- should rest front and center when evaluating deployed LLMs. We outline three developed focus areas of human-centered evaluation of XAI: mental models, use case utility, and cognitive engagement, and we highlight the importance of exploring each of these concepts for LLMs. Our goal is to jumpstart human-centered LLM evaluation.","accessed":{"date-parts":[[2023,5,19]]},"author":[{"family":"Datta","given":"Teresa"},{"family":"Dickerson","given":"John P."}],"citation-key":"dattaWhoThinkingPush2023","DOI":"10.48550/arXiv.2303.06223","issued":{"date-parts":[[2023,3,10]]},"number":"arXiv:2303.06223","publisher":"arXiv","source":"arXiv.org","title":"Who's Thinking? A Push for Human-Centered Evaluation of LLMs using the XAI Playbook","title-short":"Who's Thinking?","type":"article","URL":"http://arxiv.org/abs/2303.06223"},
  {"id":"dengImageNetLargeScaleHierarchical","abstract":"The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a largescale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 5001000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classiﬁcation and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.","author":[{"family":"Deng","given":"Jia"},{"family":"Dong","given":"Wei"},{"family":"Socher","given":"Richard"},{"family":"Li","given":"Li-Jia"},{"family":"Li","given":"Kai"},{"family":"Fei-Fei","given":"Li"}],"citation-key":"dengImageNetLargeScaleHierarchical","language":"en","source":"Zotero","title":"ImageNet: A Large-Scale Hierarchical Image Database","type":"article-journal"},
  {"id":"dengImageNetLargeScaleHierarchical2009","author":[{"family":"Deng","given":"J."},{"family":"Dong","given":"W."},{"family":"Socher","given":"R."},{"family":"Li","given":"L.-J."},{"family":"Li","given":"K."},{"family":"Fei-Fei","given":"L."}],"citation-key":"dengImageNetLargeScaleHierarchical2009","container-title":"CVPR09","issued":{"date-parts":[[2009]]},"title":"ImageNet: A Large-Scale Hierarchical Image Database","type":"paper-conference"},
  {"id":"devlinBERTPretrainingDeep2019","abstract":"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).","accessed":{"date-parts":[[2022,10,21]]},"author":[{"family":"Devlin","given":"Jacob"},{"family":"Chang","given":"Ming-Wei"},{"family":"Lee","given":"Kenton"},{"family":"Toutanova","given":"Kristina"}],"citation-key":"devlinBERTPretrainingDeep2019","DOI":"10.48550/arXiv.1810.04805","issued":{"date-parts":[[2019,5,24]]},"number":"arXiv:1810.04805","publisher":"arXiv","source":"arXiv.org","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","title-short":"BERT","type":"article","URL":"http://arxiv.org/abs/1810.04805"},
  {"id":"devriesImprovedRegularizationConvolutional2017","abstract":"Convolutional neural networks are capable of learning powerful representational spaces, which are necessary for tackling complex learning tasks. However, due to the model capacity required to capture such representations, they are often susceptible to overfitting and therefore require proper regularization in order to generalize well. In this paper, we show that the simple regularization technique of randomly masking out square regions of input during training, which we call cutout, can be used to improve the robustness and overall performance of convolutional neural networks. Not only is this method extremely easy to implement, but we also demonstrate that it can be used in conjunction with existing forms of data augmentation and other regularizers to further improve model performance. We evaluate this method by applying it to current state-of-the-art architectures on the CIFAR-10, CIFAR-100, and SVHN datasets, yielding new state-of-the-art results of 2.56%, 15.20%, and 1.30% test error respectively. Code is available at https://github.com/uoguelph-mlrg/Cutout","accessed":{"date-parts":[[2023,3,27]]},"author":[{"family":"DeVries","given":"Terrance"},{"family":"Taylor","given":"Graham W."}],"citation-key":"devriesImprovedRegularizationConvolutional2017","DOI":"10.48550/arXiv.1708.04552","issued":{"date-parts":[[2017,11,29]]},"number":"arXiv:1708.04552","publisher":"arXiv","source":"arXiv.org","title":"Improved Regularization of Convolutional Neural Networks with Cutout","type":"article","URL":"http://arxiv.org/abs/1708.04552"},
  {"id":"dhamdhereHowImportantNeuron2018","abstract":"The problem of attributing a deep network's prediction to its \\emph{input/base} features is well-studied. We introduce the notion of \\emph{conductance} to extend the notion of attribution to the understanding the importance of \\emph{hidden} units. Informally, the conductance of a hidden unit of a deep network is the \\emph{flow} of attribution via this hidden unit. We use conductance to understand the importance of a hidden unit to the prediction for a specific input, or over a set of inputs. We evaluate the effectiveness of conductance in multiple ways, including theoretical properties, ablation studies, and a feature selection task. The empirical evaluations are done using the Inception network over ImageNet data, and a sentiment analysis network over reviews. In both cases, we demonstrate the effectiveness of conductance in identifying interesting insights about the internal workings of these networks.","accessed":{"date-parts":[[2022,11,28]]},"author":[{"family":"Dhamdhere","given":"Kedar"},{"family":"Sundararajan","given":"Mukund"},{"family":"Yan","given":"Qiqi"}],"citation-key":"dhamdhereHowImportantNeuron2018","DOI":"10.48550/arXiv.1805.12233","issued":{"date-parts":[[2018,5,30]]},"number":"arXiv:1805.12233","publisher":"arXiv","source":"arXiv.org","title":"How Important Is a Neuron?","type":"article","URL":"http://arxiv.org/abs/1805.12233"},
  {"id":"dieterRoleEmotionalInhibitory2017","accessed":{"date-parts":[[2022,6,15]]},"author":[{"family":"Dieter","given":"Julia"},{"family":"Hoffmann","given":"Sabine"},{"family":"Mier","given":"Daniela"},{"family":"Reinhard","given":"Iris"},{"family":"Beutel","given":"Martin"},{"family":"Vollstädt-Klein","given":"Sabine"},{"family":"Kiefer","given":"Falk"},{"family":"Mann","given":"Karl"},{"family":"Leménager","given":"Tagrid"}],"citation-key":"dieterRoleEmotionalInhibitory2017","container-title":"Behavioural Brain Research","container-title-short":"Behavioural Brain Research","DOI":"10.1016/j.bbr.2017.01.046","ISSN":"01664328","issued":{"date-parts":[[2017,5]]},"language":"en","page":"1-14","source":"DOI.org (Crossref)","title":"The role of emotional inhibitory control in specific internet addiction – an fMRI study","type":"article-journal","URL":"https://linkinghub.elsevier.com/retrieve/pii/S0166432816310130","volume":"324"},
  {"id":"dombrowskiExplanationsCanBe2019","abstract":"Explanation methods aim to make neural networks more trustworthy and interpretable. In this paper, we demonstrate a property of explanation methods which is disconcerting for both of these purposes. Namely, we show that explanations can be manipulated arbitrarily by applying visually hardly perceptible perturbations to the input that keep the network's output approximately constant. We establish theoretically that this phenomenon can be related to certain geometrical properties of neural networks. This allows us to derive an upper bound on the susceptibility of explanations to manipulations. Based on this result, we propose effective mechanisms to enhance the robustness of explanations.","accessed":{"date-parts":[[2022,11,24]]},"author":[{"family":"Dombrowski","given":"Ann-Kathrin"},{"family":"Alber","given":"Maximilian"},{"family":"Anders","given":"Christopher J."},{"family":"Ackermann","given":"Marcel"},{"family":"Müller","given":"Klaus-Robert"},{"family":"Kessel","given":"Pan"}],"citation-key":"dombrowskiExplanationsCanBe2019","DOI":"10.48550/arXiv.1906.07983","issued":{"date-parts":[[2019,9,25]]},"number":"arXiv:1906.07983","publisher":"arXiv","source":"arXiv.org","title":"Explanations can be manipulated and geometry is to blame","type":"article","URL":"http://arxiv.org/abs/1906.07983"},
  {"id":"dongImpairedErrorMonitoringFunction2013","abstract":"<b><i>Background:</i></b> Internet addiction disorder (IAD) is rapidly becoming a prevalent mental health concern around the world. The neurobiological underpinnings of IAD should be studied to unravel the potential heterogeneity. This study was set to investigate the error-monitoring ability in IAD subjects. <b><i>Methods:</i></b> Fifteen IAD subjects and 15 healthy controls (HC) participated in this study. Participants were asked to perform a fast Stroop task that may show error responses. Behavioral and neurobiological results in relation to error responses were compared between IAD subjects and HC. <b><i>Results:</i></b> Compared to HC, IAD subjects showed increased activation in the anterior cingulate cortex (ACC) and decreased activation in the orbitofrontal cortex following error responses. Significant correlation was found between ACC activation and the Internet addiction test scores. <b><i>Conclusions:</i></b> IAD subjects show an impaired error-monitoring ability compared to HC, which can be detected by the hyperactivation in ACC in error responses.","accessed":{"date-parts":[[2022,6,15]]},"author":[{"family":"Dong","given":"Guangheng"},{"family":"Shen","given":"Yue"},{"family":"Huang","given":"Jie"},{"family":"Du","given":"Xiaoxia"}],"citation-key":"dongImpairedErrorMonitoringFunction2013","container-title":"European Addiction Research","container-title-short":"EAR","DOI":"10.1159/000346783","ISSN":"1022-6877, 1421-9891","issue":"5","issued":{"date-parts":[[2013]]},"language":"english","page":"269-275","PMID":"23548798","publisher":"Karger Publishers","source":"www.karger.com","title":"Impaired Error-Monitoring Function in People with Internet Addiction Disorder: An Event-Related fMRI Study","title-short":"Impaired Error-Monitoring Function in People with Internet Addiction Disorder","type":"article-journal","URL":"https://www.karger.com/Article/FullText/346783","volume":"19"},
  {"id":"doshi-velezRigorousScienceInterpretable2017","abstract":"As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.","accessed":{"date-parts":[[2023,3,9]]},"author":[{"family":"Doshi-Velez","given":"Finale"},{"family":"Kim","given":"Been"}],"citation-key":"doshi-velezRigorousScienceInterpretable2017","issued":{"date-parts":[[2017,3,2]]},"number":"arXiv:1702.08608","publisher":"arXiv","source":"arXiv.org","title":"Towards A Rigorous Science of Interpretable Machine Learning","type":"article","URL":"http://arxiv.org/abs/1702.08608"},
  {"id":"dosovitskiyImageWorth16x162021","abstract":"While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.","accessed":{"date-parts":[[2022,10,21]]},"author":[{"family":"Dosovitskiy","given":"Alexey"},{"family":"Beyer","given":"Lucas"},{"family":"Kolesnikov","given":"Alexander"},{"family":"Weissenborn","given":"Dirk"},{"family":"Zhai","given":"Xiaohua"},{"family":"Unterthiner","given":"Thomas"},{"family":"Dehghani","given":"Mostafa"},{"family":"Minderer","given":"Matthias"},{"family":"Heigold","given":"Georg"},{"family":"Gelly","given":"Sylvain"},{"family":"Uszkoreit","given":"Jakob"},{"family":"Houlsby","given":"Neil"}],"citation-key":"dosovitskiyImageWorth16x162021","DOI":"10.48550/arXiv.2010.11929","issued":{"date-parts":[[2021,6,3]]},"number":"arXiv:2010.11929","publisher":"arXiv","source":"arXiv.org","title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale","title-short":"An Image is Worth 16x16 Words","type":"article","URL":"http://arxiv.org/abs/2010.11929"},
  {"id":"dulhantyAuditingImageNetModeldriven2019","abstract":"The ImageNet dataset ushered in a flood of academic and industry interest in deep learning for computer vision applications. Despite its significant impact, there has not been a comprehensive investigation into the demographic attributes of images contained within the dataset. Such a study could lead to new insights on inherent biases within ImageNet, particularly important given it is frequently used to pretrain models for a wide variety of computer vision tasks. In this work, we introduce a model-driven framework for the automatic annotation of apparent age and gender attributes in large-scale image datasets. Using this framework, we conduct the first demographic audit of the 2012 ImageNet Large Scale Visual Recognition Challenge (ILSVRC) subset of ImageNet and the \"person\" hierarchical category of ImageNet. We find that 41.62% of faces in ILSVRC appear as female, 1.71% appear as individuals above the age of 60, and males aged 15 to 29 account for the largest subgroup with 27.11%. We note that the presented model-driven framework is not fair for all intersectional groups, so annotation are subject to bias. We present this work as the starting point for future development of unbiased annotation models and for the study of downstream effects of imbalances in the demographics of ImageNet. Code and annotations are available at: http://bit.ly/ImageNetDemoAudit","accessed":{"date-parts":[[2022,9,6]]},"author":[{"family":"Dulhanty","given":"Chris"},{"family":"Wong","given":"Alexander"}],"citation-key":"dulhantyAuditingImageNetModeldriven2019","DOI":"10.48550/arXiv.1905.01347","issued":{"date-parts":[[2019,6,4]]},"number":"arXiv:1905.01347","publisher":"arXiv","source":"arXiv.org","title":"Auditing ImageNet: Towards a Model-driven Framework for Annotating Demographic Attributes of Large-Scale Image Datasets","title-short":"Auditing ImageNet","type":"article","URL":"http://arxiv.org/abs/1905.01347"},
  {"id":"dutagaciBenchmarkBestView2010","author":[{"family":"Dutagaci","given":"Helin"},{"family":"Cheung","given":"Chun Pan"},{"family":"Godil","given":"Afzal"}],"citation-key":"dutagaciBenchmarkBestView2010","container-title":"Proceedings of the ACM workshop on 3D object retrieval","issued":{"date-parts":[[2010]]},"page":"45–50","title":"A benchmark for best view selection of 3D objects","type":"paper-conference"},
  {"id":"dvornikImportanceVisualContext2019","abstract":"Performing data augmentation for learning deep neural networks is known to be important for training visual recognition systems. By artiﬁcially increasing the number of training examples, it helps reducing overﬁtting and improves generalization. While simple image transformations can already improve predictive performance in most vision tasks, larger gains can be obtained by leveraging task-speciﬁc prior knowledge. In this work, we consider object detection, semantic and instance segmentation and augment the training images by blending objects in existing scenes, using instance segmentation annotations. We observe that randomly pasting objects on images hurts the performance, unless the object is placed in the right context. To resolve this issue, we propose an explicit context model by using a convolutional neural network, which predicts whether an image region is suitable for placing a given object or not. In our experiments, we show that our approach is able to improve object detection, semantic and instance segmentation on the PASCAL VOC12 and COCO datasets, with signiﬁcant gains in a limited annotation scenario, i.e. when only one category is annotated. We also show that the method is not limited to datasets that come with expensive pixel-wise instance annotations and can be used when only bounding boxes are available, by employing weakly-supervised learning for instance masks approximation.","accessed":{"date-parts":[[2022,10,21]]},"author":[{"family":"Dvornik","given":"Nikita"},{"family":"Mairal","given":"Julien"},{"family":"Schmid","given":"Cordelia"}],"citation-key":"dvornikImportanceVisualContext2019","issued":{"date-parts":[[2019,9,19]]},"language":"en","number":"arXiv:1809.02492","publisher":"arXiv","source":"arXiv.org","title":"On the Importance of Visual Context for Data Augmentation in Scene Understanding","type":"article","URL":"http://arxiv.org/abs/1809.02492"},
  {"id":"dvornikModelingVisualContext2018","accessed":{"date-parts":[[2022,10,21]]},"author":[{"family":"Dvornik","given":"Nikita"},{"family":"Mairal","given":"Julien"},{"family":"Schmid","given":"Cordelia"}],"citation-key":"dvornikModelingVisualContext2018","event-title":"Proceedings of the European Conference on Computer Vision (ECCV)","issued":{"date-parts":[[2018]]},"page":"364-380","source":"openaccess.thecvf.com","title":"Modeling Visual Context is Key to Augmenting Object Detection Datasets","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_ECCV_2018/html/NIKITA_DVORNIK_Modeling_Visual_Context_ECCV_2018_paper.html"},
  {"id":"dvornikModelingVisualContext2018a","accessed":{"date-parts":[[2023,3,31]]},"author":[{"family":"Dvornik","given":"Nikita"},{"family":"Mairal","given":"Julien"},{"family":"Schmid","given":"Cordelia"}],"citation-key":"dvornikModelingVisualContext2018a","event-title":"Proceedings of the European Conference on Computer Vision (ECCV)","issued":{"date-parts":[[2018]]},"page":"364-380","source":"openaccess.thecvf.com","title":"Modeling Visual Context is Key to Augmenting Object Detection Datasets","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_ECCV_2018/html/NIKITA_DVORNIK_Modeling_Visual_Context_ECCV_2018_paper.html"},
  {"id":"dwibediCutPasteLearn2017","accessed":{"date-parts":[[2023,3,31]]},"author":[{"family":"Dwibedi","given":"Debidatta"},{"family":"Misra","given":"Ishan"},{"family":"Hebert","given":"Martial"}],"citation-key":"dwibediCutPasteLearn2017","event-title":"Proceedings of the IEEE International Conference on Computer Vision","issued":{"date-parts":[[2017]]},"page":"1301-1310","source":"openaccess.thecvf.com","title":"Cut, Paste and Learn: Surprisingly Easy Synthesis for Instance Detection","title-short":"Cut, Paste and Learn","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_iccv_2017/html/Dwibedi_Cut_Paste_and_ICCV_2017_paper.html"},
  {"id":"e.kissQuantifierSpreadingChildren2017","abstract":"This paper calls attention to a methodological problem of acquisition experiments. It shows that the economy of the stimulus employed in child language experiments may lend an increased ostensive effect to the message communicated to the child. Thus, when the visual stimulus in a sentence-picture matching task is a minimal model abstracting away from the details of the situation, children often regard all the elements of the stimulus as ostensive clues to be represented in the corresponding sentence. The use of such minimal stimuli is mistaken when the experiment aims to test whether or not a certain element of the stimulus is relevant for the linguistic representation or interpretation. The paper illustrates this point by an experiment involving quantifier spreading. It is claimed that children find a universally quantified sentence like Every girl is riding a bicycle to be a false description of a picture showing three girls riding bicycles and a solo bicycle because they are misled to believe that all the elements in the visual stimulus are relevant, hence all of them are to be represented by the corresponding linguistic description. When the iconic drawings were replaced by photos taken in a natural environment rich in accidental details, the occurrence of quantifier spreading was radically reduced. It is shown that an extra object in the visual stimulus can lead to the rejection of the sentence also in the case of sentences involving no quantification, which gives further support to the claim that the source of the problem is not (or not only) the grammatical or cognitive difficulty of quantification but the unintended ostensive effect of the extra object.  This article is part of the special collection: Acquisition of Quantification A correction article related to this research can be found here: http://doi.org/10.5334/gjgl.902","accessed":{"date-parts":[[2022,10,24]]},"author":[{"family":"É. Kiss","given":"Katalin"},{"family":"Zétényi","given":"Tamás"}],"citation-key":"e.kissQuantifierSpreadingChildren2017","container-title":"Glossa: a journal of general linguistics","DOI":"10.5334/gjgl.147","ISSN":"2397-1835","issue":"1","issued":{"date-parts":[[2017,4,26]]},"language":"en","source":"DOI.org (Crossref)","title":"Quantifier spreading: children misled by ostensive cues","title-short":"Quantifier spreading","type":"article-journal","URL":"https://www.glossa-journal.org/article/id/4900/","volume":"2"},
  {"id":"ehsanSocialConstructionXAI2022","abstract":"There is a growing frustration amongst researchers and developers in Explainable AI (XAI) around the lack of consensus around what is meant by 'explainability'. Do we need one definition of explainability to rule them all? In this paper, we argue why a singular definition of XAI is neither feasible nor desirable at this stage of XAI's development. We view XAI through the lenses of Social Construction of Technology (SCOT) to explicate how diverse stakeholders (relevant social groups) have different interpretations (interpretative flexibility) that shape the meaning of XAI. Forcing a standardization (closure) on the pluralistic interpretations too early can stifle innovation and lead to premature conclusions. We share how we can leverage the pluralism to make progress in XAI without having to wait for a definitional consensus.","accessed":{"date-parts":[[2023,5,19]]},"author":[{"family":"Ehsan","given":"Upol"},{"family":"Riedl","given":"Mark O."}],"citation-key":"ehsanSocialConstructionXAI2022","DOI":"10.48550/arXiv.2211.06499","issued":{"date-parts":[[2022,11,11]]},"number":"arXiv:2211.06499","publisher":"arXiv","source":"arXiv.org","title":"Social Construction of XAI: Do We Need One Definition to Rule Them All?","title-short":"Social Construction of XAI","type":"article","URL":"http://arxiv.org/abs/2211.06499"},
  {"id":"eldarInteractionEmotionalState2015","accessed":{"date-parts":[[2022,5,27]]},"author":[{"family":"Eldar","given":"Eran"},{"family":"Niv","given":"Yael"}],"citation-key":"eldarInteractionEmotionalState2015","container-title":"Nature Communications","container-title-short":"Nat Commun","DOI":"10.1038/ncomms7149","ISSN":"2041-1723","issue":"1","issued":{"date-parts":[[2015,5,5]]},"language":"en","page":"6149","source":"DOI.org (Crossref)","title":"Interaction between emotional state and learning underlies mood instability","type":"article-journal","URL":"http://www.nature.com/articles/ncomms7149","volume":"6"},
  {"id":"elhoseinyComparativeAnalysisStudy2016","author":[{"family":"Elhoseiny","given":"Mohamed"},{"family":"El-Gaaly","given":"Tarek"},{"family":"Bakry","given":"Amr"},{"family":"Elgammal","given":"Ahmed"}],"citation-key":"elhoseinyComparativeAnalysisStudy2016","container-title":"International Conference on Machine learning","issued":{"date-parts":[[2016]]},"page":"888–897","publisher":"PMLR","title":"A comparative analysis and study of multiview CNN models for joint object categorization and pose estimation","type":"paper-conference"},
  {"id":"eppelClassifyingSpecificImage","abstract":"Convolutional neural nets (CNN) are the leading computer vision method for classifying images. In some cases, it is desirable to classify only a specific region of the image that corresponds to a certain object. Hence, assuming that the region of the object in the image is known in advance and is given as a binary region of interest (ROI) mask, the goal is to classify the object in this region using a convolutional neural net. This goal is achieved using a standard image classification net with the addition of a side branch, which converts the ROI mask into an attention map. This map is then combined with the image classification net. This allows the net to focus the attention on the object region while still extracting contextual cues from the background. This approach was evaluated using the COCO object dataset and the OpenSurfaces materials dataset. In both cases, it gave superior results to methods that completely ignore the background region. In addition, it was found that combining the attention map at the first layer of the net gave better results than combining it at higher layers of the net. The advantages of this method are most apparent in the classification of small regions which demands a great deal of contextual information from the background.","author":[{"family":"Eppel","given":"Sagi"}],"citation-key":"eppelClassifyingSpecificImage","language":"en","page":"8","source":"Zotero","title":"Classifying a specific image region using convolutional nets with an ROI mask as input","type":"article-journal"},
  {"id":"EthicalPrinciplesGovernance","accessed":{"date-parts":[[2023,1,17]]},"citation-key":"EthicalPrinciplesGovernance","DOI":"10.1016/j.eng.2019.12.015","language":"en","title":"Ethical Principles and Governance Technology Development of AI in China | Elsevier Enhanced Reader","type":"webpage","URL":"https://reader.elsevier.com/reader/sd/pii/S2095809920300011?token=BCF6223A379F2D2FFF8ED1953A89610B058E50DB52EACA96871713C72D2CF4A6B61BE11552799F45A55E826D0A6575EF&originRegion=eu-west-1&originCreation=20230117183919"},
  {"id":"fabbriEnhancingUnderwaterImagery2018","abstract":"Autonomous underwater vehicles (AUVs) rely on a variety of sensors – acoustic, inertial and visual – for intelligent decision making. Due to its non-intrusive, passive nature, and high information content, vision is an attractive sensing modality, particularly at shallower depths. However, factors such as light refraction and absorption, suspended particles in the water, and color distortion aﬀect the quality of visual data, resulting in noisy and distorted images. AUVs that rely on visual sensing thus face diﬃcult challenges, and consequently exhibit poor performance on visiondriven tasks. This paper proposes a method to improve the quality of visual underwater scenes using Generative Adversarial Networks (GANs), with the goal of improving input to vision-driven behaviors further down the autonomy pipeline. Furthermore, we show how recently proposed methods are able to generate a dataset for the purpose of such underwater image restoration. For any visually-guided underwater robots, this improvement can result in increased safety and reliability through robust visual perception. To that eﬀect, we present quantitative and qualitative data which demonstrates that images corrected through the proposed approach generate more visually appealing images, and also provide increased accuracy for a diver tracking algorithm.","accessed":{"date-parts":[[2022,11,19]]},"author":[{"family":"Fabbri","given":"Cameron"},{"family":"Islam","given":"Md Jahidul"},{"family":"Sattar","given":"Junaed"}],"citation-key":"fabbriEnhancingUnderwaterImagery2018","issued":{"date-parts":[[2018,1,11]]},"language":"en","number":"arXiv:1801.04011","publisher":"arXiv","source":"arXiv.org","title":"Enhancing Underwater Imagery using Generative Adversarial Networks","type":"article","URL":"http://arxiv.org/abs/1801.04011"},
  {"id":"fabbriEnhancingUnderwaterImagery2018a","abstract":"Autonomous underwater vehicles (AUVs) rely on a variety of sensors – acoustic, inertial and visual – for intelligent decision making. Due to its non-intrusive, passive nature, and high information content, vision is an attractive sensing modality, particularly at shallower depths. However, factors such as light refraction and absorption, suspended particles in the water, and color distortion aﬀect the quality of visual data, resulting in noisy and distorted images. AUVs that rely on visual sensing thus face diﬃcult challenges, and consequently exhibit poor performance on visiondriven tasks. This paper proposes a method to improve the quality of visual underwater scenes using Generative Adversarial Networks (GANs), with the goal of improving input to vision-driven behaviors further down the autonomy pipeline. Furthermore, we show how recently proposed methods are able to generate a dataset for the purpose of such underwater image restoration. For any visually-guided underwater robots, this improvement can result in increased safety and reliability through robust visual perception. To that eﬀect, we present quantitative and qualitative data which demonstrates that images corrected through the proposed approach generate more visually appealing images, and also provide increased accuracy for a diver tracking algorithm.","accessed":{"date-parts":[[2022,11,19]]},"author":[{"family":"Fabbri","given":"Cameron"},{"family":"Islam","given":"Md Jahidul"},{"family":"Sattar","given":"Junaed"}],"citation-key":"fabbriEnhancingUnderwaterImagery2018a","issued":{"date-parts":[[2018,1,11]]},"language":"en","number":"arXiv:1801.04011","publisher":"arXiv","source":"arXiv.org","title":"Enhancing Underwater Imagery using Generative Adversarial Networks","type":"article","URL":"http://arxiv.org/abs/1801.04011"},
  {"id":"fawziAdaptiveDataAugmentation2016","abstract":"Data augmentation is the process of generating samples by transforming training data, with the target of improving the accuracy and robustness of classifiers. In this paper, we propose a new automatic and adaptive algorithm for choosing the transformations of the samples used in data augmentation. Specifically, for each sample, our main idea is to seek a small transformation that yields maximal classification loss on the transformed sample. We employ a trust-region optimization strategy, which consists of solving a sequence of linear programs. Our data augmentation scheme is then integrated into a Stochastic Gradient Descent algorithm for training deep neural networks. We perform experiments on two datasets, and show that that the proposed scheme outperforms random data augmentation algorithms in terms of accuracy and robustness, while yielding comparable or superior results with respect to existing selective sampling approaches.","author":[{"family":"Fawzi","given":"Alhussein"},{"family":"Samulowitz","given":"Horst"},{"family":"Turaga","given":"Deepak"},{"family":"Frossard","given":"Pascal"}],"citation-key":"fawziAdaptiveDataAugmentation2016","container-title":"2016 IEEE International Conference on Image Processing (ICIP)","DOI":"10.1109/ICIP.2016.7533048","event-title":"2016 IEEE International Conference on Image Processing (ICIP)","ISSN":"2381-8549","issued":{"date-parts":[[2016,9]]},"page":"3688-3692","source":"IEEE Xplore","title":"Adaptive data augmentation for image classification","type":"paper-conference"},
  {"id":"feffermanTestingManifoldHypothesis2016","accessed":{"date-parts":[[2022,11,15]]},"author":[{"family":"Fefferman","given":"Charles"},{"family":"Mitter","given":"Sanjoy"},{"family":"Narayanan","given":"Hariharan"}],"citation-key":"feffermanTestingManifoldHypothesis2016","container-title":"Journal of the American Mathematical Society","container-title-short":"J. Amer. Math. Soc.","DOI":"10.1090/jams/852","ISSN":"0894-0347, 1088-6834","issue":"4","issued":{"date-parts":[[2016,2,9]]},"language":"en","page":"983-1049","source":"DOI.org (Crossref)","title":"Testing the manifold hypothesis","type":"article-journal","URL":"https://www.ams.org/jams/2016-29-04/S0894-0347-2016-00852-4/","volume":"29"},
  {"id":"fongInterpretableExplanationsBlack2017","abstract":"As machine learning algorithms are increasingly applied to high impact yet high risk tasks, such as medical diagnosis or autonomous driving, it is critical that researchers can explain how such algorithms arrived at their predictions. In recent years, a number of image saliency methods have been developed to summarize where highly complex neural networks \"look\" in an image for evidence for their predictions. However, these techniques are limited by their heuristic nature and architectural constraints. In this paper, we make two main contributions: First, we propose a general framework for learning different kinds of explanations for any black box algorithm. Second, we specialise the framework to find the part of an image most responsible for a classifier decision. Unlike previous works, our method is model-agnostic and testable because it is grounded in explicit and interpretable image perturbations.","accessed":{"date-parts":[[2023,3,24]]},"author":[{"family":"Fong","given":"Ruth"},{"family":"Vedaldi","given":"Andrea"}],"citation-key":"fongInterpretableExplanationsBlack2017","container-title":"2017 IEEE International Conference on Computer Vision (ICCV)","DOI":"10.1109/ICCV.2017.371","issued":{"date-parts":[[2017,10]]},"page":"3449-3457","source":"arXiv.org","title":"Interpretable Explanations of Black Boxes by Meaningful Perturbation","type":"paper-conference","URL":"http://arxiv.org/abs/1704.03296"},
  {"id":"fongUnderstandingDeepNetworks2019","abstract":"The problem of attribution is concerned with identifying the parts of an input that are responsible for a model's output. An important family of attribution methods is based on measuring the effect of perturbations applied to the input. In this paper, we discuss some of the shortcomings of existing approaches to perturbation analysis and address them by introducing the concept of extremal perturbations, which are theoretically grounded and interpretable. We also introduce a number of technical innovations to compute extremal perturbations, including a new area constraint and a parametric family of smooth perturbations, which allow us to remove all tunable hyper-parameters from the optimization problem. We analyze the effect of perturbations as a function of their area, demonstrating excellent sensitivity to the spatial properties of the deep neural network under stimulation. We also extend perturbation analysis to the intermediate layers of a network. This application allows us to identify the salient channels necessary for classification, which, when visualized using feature inversion, can be used to elucidate model behavior. Lastly, we introduce TorchRay, an interpretability library built on PyTorch.","accessed":{"date-parts":[[2022,11,24]]},"author":[{"family":"Fong","given":"Ruth"},{"family":"Patrick","given":"Mandela"},{"family":"Vedaldi","given":"Andrea"}],"citation-key":"fongUnderstandingDeepNetworks2019","DOI":"10.48550/arXiv.1910.08485","issued":{"date-parts":[[2019,10,18]]},"number":"arXiv:1910.08485","publisher":"arXiv","source":"arXiv.org","title":"Understanding Deep Networks via Extremal Perturbations and Smooth Masks","type":"article","URL":"http://arxiv.org/abs/1910.08485"},
  {"id":"frenchMilkingCowMaskSemiSupervised2020","abstract":"Consistency regularization is a technique for semi-supervised learning that underlies a number of strong results for classification with few labeled data. It works by encouraging a learned model to be robust to perturbations on unlabeled data. Here, we present a novel mask-based augmentation method called CowMask. Using it to provide perturbations for semi-supervised consistency regularization, we achieve a state-of-the-art result on ImageNet with 10% labeled data, with a top-5 error of 8.76% and top-1 error of 26.06%. Moreover, we do so with a method that is much simpler than many alternatives. We further investigate the behavior of CowMask for semi-supervised learning by running many smaller scale experiments on the SVHN, CIFAR-10 and CIFAR-100 data sets, where we achieve results competitive with the state of the art, indicating that CowMask is widely applicable. We open source our code at https://github.com/google-research/google-research/tree/master/milking_cowmask","accessed":{"date-parts":[[2023,3,31]]},"author":[{"family":"French","given":"Geoff"},{"family":"Oliver","given":"Avital"},{"family":"Salimans","given":"Tim"}],"citation-key":"frenchMilkingCowMaskSemiSupervised2020","DOI":"10.48550/arXiv.2003.12022","issued":{"date-parts":[[2020,6,5]]},"number":"arXiv:2003.12022","publisher":"arXiv","source":"arXiv.org","title":"Milking CowMask for Semi-Supervised Image Classification","type":"article","URL":"http://arxiv.org/abs/2003.12022"},
  {"id":"fristonComputationalPsychiatryBrain2014","accessed":{"date-parts":[[2022,5,27]]},"author":[{"family":"Friston","given":"Karl J"},{"family":"Stephan","given":"Klaas Enno"},{"family":"Montague","given":"Read"},{"family":"Dolan","given":"Raymond J"}],"citation-key":"fristonComputationalPsychiatryBrain2014","container-title":"The Lancet Psychiatry","container-title-short":"The Lancet Psychiatry","DOI":"10.1016/S2215-0366(14)70275-5","ISSN":"22150366","issue":"2","issued":{"date-parts":[[2014,7]]},"language":"en","page":"148-158","source":"DOI.org (Crossref)","title":"Computational psychiatry: the brain as a phantastic organ","title-short":"Computational psychiatry","type":"article-journal","URL":"https://linkinghub.elsevier.com/retrieve/pii/S2215036614702755","volume":"1"},
  {"id":"ganVisionLanguagePretrainingBasics2022","abstract":"This paper surveys vision-language pre-training (VLP) methods for multimodal intelligence that have been developed in the last few years. We group these approaches into three categories: ($i$) VLP for image-text tasks, such as image captioning, image-text retrieval, visual question answering, and visual grounding; ($ii$) VLP for core computer vision tasks, such as (open-set) image classification, object detection, and segmentation; and ($iii$) VLP for video-text tasks, such as video captioning, video-text retrieval, and video question answering. For each category, we present a comprehensive review of state-of-the-art methods, and discuss the progress that has been made and challenges still being faced, using specific systems and models as case studies. In addition, for each category, we discuss advanced topics being actively explored in the research community, such as big foundation models, unified modeling, in-context few-shot learning, knowledge, robustness, and computer vision in the wild, to name a few.","accessed":{"date-parts":[[2022,11,19]]},"author":[{"family":"Gan","given":"Zhe"},{"family":"Li","given":"Linjie"},{"family":"Li","given":"Chunyuan"},{"family":"Wang","given":"Lijuan"},{"family":"Liu","given":"Zicheng"},{"family":"Gao","given":"Jianfeng"}],"citation-key":"ganVisionLanguagePretrainingBasics2022","DOI":"10.48550/arXiv.2210.09263","issued":{"date-parts":[[2022,10,17]]},"number":"arXiv:2210.09263","publisher":"arXiv","source":"arXiv.org","title":"Vision-Language Pre-training: Basics, Recent Advances, and Future Trends","title-short":"Vision-Language Pre-training","type":"article","URL":"http://arxiv.org/abs/2210.09263"},
  {"id":"ganVisionLanguagePretrainingBasics2022a","abstract":"This paper surveys vision-language pre-training (VLP) methods for multimodal intelligence that have been developed in the last few years. We group these approaches into three categories: (i) VLP for image-text tasks, such as image captioning, image-text retrieval, visual question answering, and visual grounding; (ii) VLP for core computer vision tasks, such as (open-set) image classiﬁcation, object detection, and segmentation; and (iii) VLP for video-text tasks, such as video captioning, video-text retrieval, and video question answering. For each category, we present a comprehensive review of state-of-the-art methods, and discuss the progress that has been made and challenges still being faced, using speciﬁc systems and models as case studies. In addition, for each category, we discuss advanced topics being actively explored in the research community, such as big foundation models, uniﬁed modeling, in-context few-shot learning, knowledge, robustness, and computer vision in the wild, to name a few.","accessed":{"date-parts":[[2022,11,19]]},"author":[{"family":"Gan","given":"Zhe"},{"family":"Li","given":"Linjie"},{"family":"Li","given":"Chunyuan"},{"family":"Wang","given":"Lijuan"},{"family":"Liu","given":"Zicheng"},{"family":"Gao","given":"Jianfeng"}],"citation-key":"ganVisionLanguagePretrainingBasics2022a","issued":{"date-parts":[[2022,10,17]]},"language":"en","number":"arXiv:2210.09263","publisher":"arXiv","source":"arXiv.org","title":"Vision-Language Pre-training: Basics, Recent Advances, and Future Trends","title-short":"Vision-Language Pre-training","type":"article","URL":"http://arxiv.org/abs/2210.09263"},
  {"id":"ganVisionLanguagePretrainingBasics2022b","abstract":"This paper surveys vision-language pre-training (VLP) methods for multimodal intelligence that have been developed in the last few years. We group these approaches into three categories: (i) VLP for image-text tasks, such as image captioning, image-text retrieval, visual question answering, and visual grounding; (ii) VLP for core computer vision tasks, such as (open-set) image classiﬁcation, object detection, and segmentation; and (iii) VLP for video-text tasks, such as video captioning, video-text retrieval, and video question answering. For each category, we present a comprehensive review of state-of-the-art methods, and discuss the progress that has been made and challenges still being faced, using speciﬁc systems and models as case studies. In addition, for each category, we discuss advanced topics being actively explored in the research community, such as big foundation models, uniﬁed modeling, in-context few-shot learning, knowledge, robustness, and computer vision in the wild, to name a few.","accessed":{"date-parts":[[2022,11,19]]},"author":[{"family":"Gan","given":"Zhe"},{"family":"Li","given":"Linjie"},{"family":"Li","given":"Chunyuan"},{"family":"Wang","given":"Lijuan"},{"family":"Liu","given":"Zicheng"},{"family":"Gao","given":"Jianfeng"}],"citation-key":"ganVisionLanguagePretrainingBasics2022b","issued":{"date-parts":[[2022,10,17]]},"language":"en","number":"arXiv:2210.09263","publisher":"arXiv","source":"arXiv.org","title":"Vision-Language Pre-training: Basics, Recent Advances, and Future Trends","title-short":"Vision-Language Pre-training","type":"article","URL":"http://arxiv.org/abs/2210.09263"},
  {"id":"gaoGoingXAISystematic2022","abstract":"As the societal impact of Deep Neural Networks (DNNs) grows, the goals for advancing DNNs become more complex and diverse, ranging from improving a conventional model accuracy metric to infusing advanced human virtues such as fairness, accountability, transparency (FaccT), and unbiasedness. Recently, techniques in Explainable Artificial Intelligence (XAI) are attracting considerable attention, and have tremendously helped Machine Learning (ML) engineers in understanding AI models. However, at the same time, we started to witness the emerging need beyond XAI among AI communities; based on the insights learned from XAI, how can we better empower ML engineers in steering their DNNs so that the model's reasonableness and performance can be improved as intended? This article provides a timely and extensive literature overview of the field Explanation-Guided Learning (EGL), a domain of techniques that steer the DNNs' reasoning process by adding regularization, supervision, or intervention on model explanations. In doing so, we first provide a formal definition of EGL and its general learning paradigm. Secondly, an overview of the key factors for EGL evaluation, as well as summarization and categorization of existing evaluation procedures and metrics for EGL are provided. Finally, the current and potential future application areas and directions of EGL are discussed, and an extensive experimental study is presented aiming at providing comprehensive comparative studies among existing EGL models in various popular application domains, such as Computer Vision (CV) and Natural Language Processing (NLP) domains.","accessed":{"date-parts":[[2023,5,19]]},"author":[{"family":"Gao","given":"Yuyang"},{"family":"Gu","given":"Siyi"},{"family":"Jiang","given":"Junji"},{"family":"Hong","given":"Sungsoo Ray"},{"family":"Yu","given":"Dazhou"},{"family":"Zhao","given":"Liang"}],"citation-key":"gaoGoingXAISystematic2022","DOI":"10.48550/arXiv.2212.03954","issued":{"date-parts":[[2022,12,7]]},"number":"arXiv:2212.03954","publisher":"arXiv","source":"arXiv.org","title":"Going Beyond XAI: A Systematic Survey for Explanation-Guided Learning","title-short":"Going Beyond XAI","type":"article","URL":"http://arxiv.org/abs/2212.03954"},
  {"id":"geContributionsShapeTexture2022","abstract":"We investigate the contributions of three important features of the human visual system (HVS)~ -- ~shape, texture, and color ~ -- ~to object classification. We build a humanoid vision engine (HVE) that explicitly and separately computes shape, texture, and color features from images. The resulting feature vectors are then concatenated to support the final classification. We show that HVE can summarize and rank-order the contributions of the three features to object recognition. We use human experiments to confirm that both HVE and humans predominantly use some specific features to support the classification of specific classes (e.g., texture is the dominant feature to distinguish a zebra from other quadrupeds, both for humans and HVE). With the help of HVE, given any environment (dataset), we can summarize the most important features for the whole task (task-specific; e.g., color is the most important feature overall for classification with the CUB dataset), and for each class (class-specific; e.g., shape is the most important feature to recognize boats in the iLab-20M dataset). To demonstrate more usefulness of HVE, we use it to simulate the open-world zero-shot learning ability of humans with no attribute labeling. Finally, we show that HVE can also simulate human imagination ability with the combination of different features. We will open-source the HVE engine and corresponding datasets.","accessed":{"date-parts":[[2023,2,20]]},"author":[{"family":"Ge","given":"Yunhao"},{"family":"Xiao","given":"Yao"},{"family":"Xu","given":"Zhi"},{"family":"Wang","given":"Xingrui"},{"family":"Itti","given":"Laurent"}],"citation-key":"geContributionsShapeTexture2022","issued":{"date-parts":[[2022,7,19]]},"number":"arXiv:2207.09510","publisher":"arXiv","source":"arXiv.org","title":"Contributions of Shape, Texture, and Color in Visual Recognition","type":"article","URL":"http://arxiv.org/abs/2207.09510"},
  {"id":"geirhosImageNettrainedCNNsAre2019","abstract":"Convolutional Neural Networks (CNNs) are commonly thought to recognise objects by learning increasingly complex representations of object shapes. Some recent studies suggest a more important role of image textures. We here put these conflicting hypotheses to a quantitative test by evaluating CNNs and human observers on images with a texture-shape cue conflict. We show that ImageNet-trained CNNs are strongly biased towards recognising textures rather than shapes, which is in stark contrast to human behavioural evidence and reveals fundamentally different classification strategies. We then demonstrate that the same standard architecture (ResNet-50) that learns a texture-based representation on ImageNet is able to learn a shape-based representation instead when trained on \"Stylized-ImageNet\", a stylized version of ImageNet. This provides a much better fit for human behavioural performance in our well-controlled psychophysical lab setting (nine experiments totalling 48,560 psychophysical trials across 97 observers) and comes with a number of unexpected emergent benefits such as improved object detection performance and previously unseen robustness towards a wide range of image distortions, highlighting advantages of a shape-based representation.","accessed":{"date-parts":[[2022,9,6]]},"author":[{"family":"Geirhos","given":"Robert"},{"family":"Rubisch","given":"Patricia"},{"family":"Michaelis","given":"Claudio"},{"family":"Bethge","given":"Matthias"},{"family":"Wichmann","given":"Felix A."},{"family":"Brendel","given":"Wieland"}],"citation-key":"geirhosImageNettrainedCNNsAre2019","DOI":"10.48550/arXiv.1811.12231","issued":{"date-parts":[[2019,1,14]]},"number":"arXiv:1811.12231","publisher":"arXiv","source":"arXiv.org","title":"ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness","type":"article","URL":"http://arxiv.org/abs/1811.12231"},
  {"id":"ghorbaniInterpretationNeuralNetworks2018","abstract":"In order for machine learning to be deployed and trusted in many applications, it is crucial to be able to reliably explain why the machine learning algorithm makes certain predictions. For example, if an algorithm classifies a given pathology image to be a malignant tumor, then the doctor may need to know which parts of the image led the algorithm to this classification. How to interpret black-box predictors is thus an important and active area of research. A fundamental question is: how much can we trust the interpretation itself? In this paper, we show that interpretation of deep learning predictions is extremely fragile in the following sense: two perceptively indistinguishable inputs with the same predicted label can be assigned very different interpretations. We systematically characterize the fragility of several widely-used feature-importance interpretation methods (saliency maps, relevance propagation, and DeepLIFT) on ImageNet and CIFAR-10. Our experiments show that even small random perturbation can change the feature importance and new systematic perturbations can lead to dramatically different interpretations without changing the label. We extend these results to show that interpretations based on exemplars (e.g. influence functions) are similarly fragile. Our analysis of the geometry of the Hessian matrix gives insight on why fragility could be a fundamental challenge to the current interpretation approaches.","accessed":{"date-parts":[[2022,11,18]]},"author":[{"family":"Ghorbani","given":"Amirata"},{"family":"Abid","given":"Abubakar"},{"family":"Zou","given":"James"}],"citation-key":"ghorbaniInterpretationNeuralNetworks2018","DOI":"10.48550/arXiv.1710.10547","issued":{"date-parts":[[2018,11,6]]},"number":"arXiv:1710.10547","publisher":"arXiv","source":"arXiv.org","title":"Interpretation of Neural Networks is Fragile","type":"article","URL":"http://arxiv.org/abs/1710.10547"},
  {"id":"gianferraraCognitiveMotorSkill2021","abstract":"We examined the detailed behavioral characteristics of transfer of skill and the ability of the adaptive control of thought rational (ACT-R) architecture to account for this with its new Controller module. We employed a simple action video game called Auto Orbit and investigated the control tuning of timing skills across speed perturbations of the environment. In Auto Orbit, players needed to learn to alternate turn and shot actions to blow and burst balloons under time constraints imposed by balloon resets and deflations. Cognitive and motor skill transfer was assessed both in terms of game performance and in terms of the details of their motor actions. We found that skill transfer across speeds necessitated the recalibration of action timing skills. In addition, we found that acquiring skill in Auto Orbit involved a progressive decrease in variability of behavior. Finally, we found that players with higher skill levels tended to be less variable in terms of action chunking and action timing. These findings further shed light on the complex cognitive and motor mechanisms of skill transfer across speeds in complex task environments.","accessed":{"date-parts":[[2023,1,25]]},"author":[{"family":"Gianferrara","given":"Pierre Giovanni"},{"family":"Betts","given":"Shawn"},{"family":"Anderson","given":"John Robert"}],"citation-key":"gianferraraCognitiveMotorSkill2021","container-title":"PLOS ONE","container-title-short":"PLoS ONE","DOI":"10.1371/journal.pone.0258242","editor":[{"family":"Goldwater","given":"Micah B."}],"ISSN":"1932-6203","issue":"10","issued":{"date-parts":[[2021,10,12]]},"language":"en","page":"e0258242","source":"DOI.org (Crossref)","title":"Cognitive & motor skill transfer across speeds: A video game study","title-short":"Cognitive & motor skill transfer across speeds","type":"article-journal","URL":"https://dx.plos.org/10.1371/journal.pone.0258242","volume":"16"},
  {"id":"gilpinExplainingExplanationsOverview2019","abstract":"There has recently been a surge of work in explanatory artificial intelligence (XAI). This research area tackles the important problem that complex machines and algorithms often cannot provide insights into their behavior and thought processes. XAI allows users and parts of the internal system to be more transparent, providing explanations of their decisions in some level of detail. These explanations are important to ensure algorithmic fairness, identify potential bias/problems in the training data, and to ensure that the algorithms perform as expected. However, explanations produced by these systems is neither standardized nor systematically assessed. In an effort to create best practices and identify open challenges, we provide our definition of explainability and show how it can be used to classify existing literature. We discuss why current approaches to explanatory methods especially for deep neural networks are insufficient. Finally, based on our survey, we conclude with suggested future research directions for explanatory artificial intelligence.","accessed":{"date-parts":[[2022,11,24]]},"author":[{"family":"Gilpin","given":"Leilani H."},{"family":"Bau","given":"David"},{"family":"Yuan","given":"Ben Z."},{"family":"Bajwa","given":"Ayesha"},{"family":"Specter","given":"Michael"},{"family":"Kagal","given":"Lalana"}],"citation-key":"gilpinExplainingExplanationsOverview2019","DOI":"10.48550/arXiv.1806.00069","issued":{"date-parts":[[2019,2,3]]},"number":"arXiv:1806.00069","publisher":"arXiv","source":"arXiv.org","title":"Explaining Explanations: An Overview of Interpretability of Machine Learning","title-short":"Explaining Explanations","type":"article","URL":"http://arxiv.org/abs/1806.00069"},
  {"id":"gilpinExplanationNotTechnical2022","abstract":"There is broad agreement that Artificial Intelligence (AI) systems, particularly those using Machine Learning (ML), should be able to \"explain\" their behavior. Unfortunately, there is little agreement as to what constitutes an \"explanation.\" This has caused a disconnect between the explanations that systems produce in service of explainable Artificial Intelligence (XAI) and those explanations that users and other audiences actually need, which should be defined by the full spectrum of functional roles, audiences, and capabilities for explanation. In this paper, we explore the features of explanations and how to use those features in evaluating their utility. We focus on the requirements for explanations defined by their functional role, the knowledge states of users who are trying to understand them, and the availability of the information needed to generate them. Further, we discuss the risk of XAI enabling trust in systems without establishing their trustworthiness and define a critical next step for the field of XAI to establish metrics to guide and ground the utility of system-generated explanations.","accessed":{"date-parts":[[2023,5,19]]},"author":[{"family":"Gilpin","given":"Leilani H."},{"family":"Paley","given":"Andrew R."},{"family":"Alam","given":"Mohammed A."},{"family":"Spurlock","given":"Sarah"},{"family":"Hammond","given":"Kristian J."}],"citation-key":"gilpinExplanationNotTechnical2022","DOI":"10.48550/arXiv.2207.00007","issued":{"date-parts":[[2022,6,27]]},"number":"arXiv:2207.00007","publisher":"arXiv","source":"arXiv.org","title":"\"Explanation\" is Not a Technical Term: The Problem of Ambiguity in XAI","title-short":"\"Explanation\" is Not a Technical Term","type":"article","URL":"http://arxiv.org/abs/2207.00007"},
  {"id":"gongKeepAugmentSimpleInformationPreserving2021","abstract":"Data augmentation (DA) is an essential technique for training state-of-the-art deep learning systems. In this paper, we empirically show that the standard data augmentation methods may introduce distribution shift and consequently hurt the performance on unaugmented data during inference. To alleviate this issue, we propose a simple yet effective approach, dubbed KeepAugment, to increase the ﬁdelity of augmented images. The idea is to use the saliency map to detect important regions on the original images and preserve these informative regions during augmentation. This information-preserving strategy allows us to generate more faithful training examples. Empirically, we demonstrate that our method signiﬁcantly improves upon a number of prior art data augmentation schemes, e.g. AutoAugment, Cutout, random erasing, achieving promising results on image classiﬁcation, semi-supervised image classiﬁcation, multi-view multi-camera tracking and object detection.","accessed":{"date-parts":[[2023,5,8]]},"author":[{"family":"Gong","given":"Chengyue"},{"family":"Wang","given":"Dilin"},{"family":"Li","given":"Meng"},{"family":"Chandra","given":"Vikas"},{"family":"Liu","given":"Qiang"}],"citation-key":"gongKeepAugmentSimpleInformationPreserving2021","container-title":"2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","DOI":"10.1109/CVPR46437.2021.00111","event-place":"Nashville, TN, USA","event-title":"2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","ISBN":"978-1-66544-509-2","issued":{"date-parts":[[2021,6]]},"language":"en","page":"1055-1064","publisher":"IEEE","publisher-place":"Nashville, TN, USA","source":"DOI.org (Crossref)","title":"KeepAugment: A Simple Information-Preserving Data Augmentation Approach","title-short":"KeepAugment","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/9578546/"},
  {"id":"gongReshapingVisualDatasets2013","abstract":"In visual recognition problems, the common data distribution mismatches between training and testing make domain adaptation essential. However, image data is difficult to manually divide into the discrete domains required by adaptation algorithms, and the standard practice of equating datasets with domains is a weak proxy for all the real conditions that alter the statistics in complex ways (lighting, pose, background, resolution, etc.) We propose an approach to automatically discover latent domains in image or video datasets. Our formulation imposes two key properties on domains: maximum distinctiveness and maximum learnability. By maximum distinctiveness, we require the underlying distributions of the identified domains to be different from each other; by maximum learnability, we ensure that a strong discriminative model can be learned from the domain. We devise a nonparametric representation and efficient optimization procedure for distinctiveness, which, when coupled with our learnability constraint, can successfully discover domains among both training and test data. We extensively evaluate our approach on object recognition and human activity recognition tasks.","accessed":{"date-parts":[[2022,9,6]]},"author":[{"family":"Gong","given":"Boqing"},{"family":"Grauman","given":"Kristen"},{"family":"Sha","given":"Fei"}],"citation-key":"gongReshapingVisualDatasets2013","container-title":"Advances in Neural Information Processing Systems","issued":{"date-parts":[[2013]]},"publisher":"Curran Associates, Inc.","source":"Neural Information Processing Systems","title":"Reshaping Visual Datasets for Domain Adaptation","type":"paper-conference","URL":"https://proceedings.neurips.cc/paper/2013/hash/2291d2ec3b3048d1a6f86c2c4591b7e0-Abstract.html","volume":"26"},
  {"id":"gozalo-brizuelaChatGPTNotAll2023","abstract":"During the last two years there has been a plethora of large generative models such as ChatGPT or Stable Diﬀusion that have been published. Concretely, these models are able to perform tasks such as being a general question and answering system or automatically creating artistic images that are revolutionizing several sectors. Consequently, the implications that these generative models have in the industry and society are enormous, as several job positions may be transformed. For example, Generative AI is capable of transforming eﬀectively and creatively texts to images, like the DALLE-2 model; text to 3D images, like the Dreamfusion model; images to text, like the Flamingo model; texts to video, like the Phenaki model; texts to audio, like the AudioLM model; texts to other texts, like ChatGPT; texts to code, like the Codex model; texts to scientiﬁc texts, like the Galactica model or even create algorithms like AlphaTensor. This work consists on an attempt to describe in a concise way the main models are sectors that are aﬀected by generative AI and to provide a taxonomy of the main generative models published recently.","accessed":{"date-parts":[[2023,1,23]]},"author":[{"family":"Gozalo-Brizuela","given":"Roberto"},{"family":"Garrido-Merchan","given":"Eduardo C."}],"citation-key":"gozalo-brizuelaChatGPTNotAll2023","issued":{"date-parts":[[2023,1,11]]},"language":"en","number":"arXiv:2301.04655","publisher":"arXiv","source":"arXiv.org","title":"ChatGPT is not all you need. A State of the Art Review of large Generative AI models","type":"article","URL":"http://arxiv.org/abs/2301.04655"},
  {"id":"gozalo-brizuelaChatGPTNotAll2023a","abstract":"During the last two years there has been a plethora of large generative models such as ChatGPT or Stable Diﬀusion that have been published. Concretely, these models are able to perform tasks such as being a general question and answering system or automatically creating artistic images that are revolutionizing several sectors. Consequently, the implications that these generative models have in the industry and society are enormous, as several job positions may be transformed. For example, Generative AI is capable of transforming eﬀectively and creatively texts to images, like the DALLE-2 model; text to 3D images, like the Dreamfusion model; images to text, like the Flamingo model; texts to video, like the Phenaki model; texts to audio, like the AudioLM model; texts to other texts, like ChatGPT; texts to code, like the Codex model; texts to scientiﬁc texts, like the Galactica model or even create algorithms like AlphaTensor. This work consists on an attempt to describe in a concise way the main models are sectors that are aﬀected by generative AI and to provide a taxonomy of the main generative models published recently.","accessed":{"date-parts":[[2023,1,23]]},"author":[{"family":"Gozalo-Brizuela","given":"Roberto"},{"family":"Garrido-Merchan","given":"Eduardo C."}],"citation-key":"gozalo-brizuelaChatGPTNotAll2023a","issued":{"date-parts":[[2023,1,11]]},"language":"en","number":"arXiv:2301.04655","publisher":"arXiv","source":"arXiv.org","title":"ChatGPT is not all you need. A State of the Art Review of large Generative AI models","type":"article","URL":"http://arxiv.org/abs/2301.04655"},
  {"id":"gozalo-brizuelaChatGPTNotAll2023b","abstract":"During the last two years there has been a plethora of large generative models such as ChatGPT or Stable Diﬀusion that have been published. Concretely, these models are able to perform tasks such as being a general question and answering system or automatically creating artistic images that are revolutionizing several sectors. Consequently, the implications that these generative models have in the industry and society are enormous, as several job positions may be transformed. For example, Generative AI is capable of transforming eﬀectively and creatively texts to images, like the DALLE-2 model; text to 3D images, like the Dreamfusion model; images to text, like the Flamingo model; texts to video, like the Phenaki model; texts to audio, like the AudioLM model; texts to other texts, like ChatGPT; texts to code, like the Codex model; texts to scientiﬁc texts, like the Galactica model or even create algorithms like AlphaTensor. This work consists on an attempt to describe in a concise way the main models are sectors that are aﬀected by generative AI and to provide a taxonomy of the main generative models published recently.","accessed":{"date-parts":[[2023,1,23]]},"author":[{"family":"Gozalo-Brizuela","given":"Roberto"},{"family":"Garrido-Merchan","given":"Eduardo C."}],"citation-key":"gozalo-brizuelaChatGPTNotAll2023b","issued":{"date-parts":[[2023,1,11]]},"language":"en","number":"arXiv:2301.04655","publisher":"arXiv","source":"arXiv.org","title":"ChatGPT is not all you need. A State of the Art Review of large Generative AI models","type":"article","URL":"http://arxiv.org/abs/2301.04655"},
  {"id":"graesserRunningHeadINTELLIGENT","author":[{"family":"Graesser","given":"Arthur C"},{"family":"Conley","given":"Mark W"},{"family":"Olney","given":"Andrew"}],"citation-key":"graesserRunningHeadINTELLIGENT","language":"en","page":"57","source":"Zotero","title":"Running head: INTELLIGENT TUTORING SYSTEMS","type":"article-journal"},
  {"id":"gunningExplainableArtificialIntelligence","author":[{"family":"Gunning","given":"David"}],"citation-key":"gunningExplainableArtificialIntelligence","language":"en","source":"Zotero","title":"Explainable Artificial Intelligence (XAI)","type":"article-journal"},
  {"id":"han3D2SeqViewsAggregatingSequential2019","accessed":{"date-parts":[[2022,5,10]]},"author":[{"family":"Han","given":"Zhizhong"},{"family":"Lu","given":"Honglei"},{"family":"Liu","given":"Zhenbao"},{"family":"Vong","given":"Chi-Man"},{"family":"Liu","given":"Yu-Shen"},{"family":"Zwicker","given":"Matthias"},{"family":"Han","given":"Junwei"},{"family":"Chen","given":"C. L. Philip"}],"citation-key":"han3D2SeqViewsAggregatingSequential2019","container-title":"IEEE Transactions on Image Processing","container-title-short":"IEEE Trans. on Image Process.","DOI":"10.1109/TIP.2019.2904460","ISSN":"1057-7149, 1941-0042","issue":"8","issued":{"date-parts":[[2019,8]]},"language":"en","page":"3986-3999","source":"DOI.org (Crossref)","title":"3D2SeqViews: Aggregating Sequential Views for 3D Global Feature Learning by CNN With Hierarchical Attention Aggregation","title-short":"3D2SeqViews","type":"article-journal","URL":"https://ieeexplore.ieee.org/document/8666059/","volume":"28"},
  {"id":"hanChangeVariableForeperiodEffects2022","abstract":"The framework of binding and retrieval in action control (BRAC) by Frings et al. (2020) proposed that repetition of any element in the previous trial triggers the retrieval of other elements in the same event file. Consistent with this framework, Los et al. (2014) argued that the temporal relation between the warning signal and the target stimulus on a trial is stored in a distinct memory trace (or, event file). Retrieval of the preceding memory trace, which is triggered by perceiving the same warning signal, leads to sequential foreperiod (SFP) effect. We modeled the data from four experiments using a Bayesian method to investigate whether the SFP effect changes over time. Results of Experiments 1, 3 and 4 support the multiple trace theory of preparation, which predicts an asymmetric sequential foreperiod effect, whereas those of Experiment 2 (extremely short foreperiods) support the repetition priming account by Capizzi et al. (2015). Moreover, the significance of the parameters showed that the asymmetry in Experiments 1 and 3 (non-aging distribution) developed gradually, whereas in Experiment 4 (uniform distribution), this asymmetry was significant from the beginning and did not change over time. Implications of these findings for temporal preparation models and BRAC framework were discussed.","accessed":{"date-parts":[[2023,1,25]]},"author":[{"family":"Han","given":"Tianfang"},{"family":"Proctor","given":"Robert W."}],"citation-key":"hanChangeVariableForeperiodEffects2022","container-title":"Journal of Cognition","DOI":"10.5334/joc.235","ISSN":"2514-4820","issue":"1","issued":{"date-parts":[[2022,7,13]]},"language":"en","license":"Authors who publish with this journal agree to the following terms:    Authors retain copyright and grant the journal right of first publication with the work simultaneously licensed under a  Creative Commons Attribution License  that allows others to share the work with an acknowledgement of the work's authorship and initial publication in this journal.  Authors are able to enter into separate, additional contractual arrangements for the non-exclusive distribution of the journal's published version of the work (e.g., post it to an institutional repository or publish it in a book), with an acknowledgement of its initial publication in this journal.  Authors are permitted and encouraged to post their work online (e.g., in institutional repositories or on their website) prior to and during the submission process, as it can lead to productive exchanges, as well as earlier and greater citation of published work (See  The Effect of Open Access ).  All third-party images reproduced on this journal are shared under Educational Fair Use. For more information on  Educational Fair Use , please see  this useful checklist prepared by Columbia University Libraries .   All copyright  of third-party content posted here for research purposes belongs to its original owners.  Unless otherwise stated all references to characters and comic art presented on this journal are ©, ® or ™ of their respective owners. No challenge to any owner’s rights is intended or should be inferred.","number":"1","page":"40","publisher":"Ubiquity Press","source":"www.journalofcognition.org","title":"Change of Variable-Foreperiod Effects within an Experiment: A Bayesian Modeling Approach","title-short":"Change of Variable-Foreperiod Effects within an Experiment","type":"article-journal","URL":"http://www.journalofcognition.org/articles/10.5334/joc.235/","volume":"5"},
  {"id":"hanRevisitingVariableforeperiodEffects2022","abstract":"A warning signal preceding an imperative stimulus by a certain foreperiod can accelerate responses (foreperiod effect). When foreperiod is varied within a block, the foreperiod effect on reaction time (RT) is modulated by both the current and the prior foreperiods. Using a non-aging foreperiod distribution in a simple-reaction task, Capizzi et al. (Cognition, 134, 39-49, 2015) found equal sequential effects for different foreperiods, which they credited to repetition priming. The multiple-trace theory of Los et al. (Frontiers in Psychology, 5, Article 1058, 2014) attributes the slope of the foreperiod-RT function to the foreperiod distribution. We conducted three experiments that examined these predicted relations. Experiment 1 tested Capizzi et al.’s prediction in a choice-reaction task and found an increasing foreperiod-RT function but a larger sequential effect at the shorter foreperiod. Experiment 2 used two distinct short foreperiods with the same foreperiod distribution and found a decreasing foreperiod-RT function. By increasing the difference between the foreperiods used in Experiment 2, Experiment 3 yielded a larger sequential effect overall. The experiments provide evidence that, with a non-aging foreperiod distribution, the variable-foreperiod paradigm yields unequal sequential-effect sizes at the different foreperiods, consistent with the multiple-trace theory but contrary to Capizzi et al.’s repetition-priming account. The foreperiod-RT functions are similar to those of the fixed-foreperiod paradigm, which is not predicted by the multiple trace theory.","accessed":{"date-parts":[[2023,1,25]]},"author":[{"family":"Han","given":"Tianfang"},{"family":"Proctor","given":"Robert W."}],"citation-key":"hanRevisitingVariableforeperiodEffects2022","container-title":"Attention, Perception, & Psychophysics","container-title-short":"Atten Percept Psychophys","DOI":"10.3758/s13414-022-02476-5","ISSN":"1943-393X","issue":"4","issued":{"date-parts":[[2022,5,1]]},"language":"en","page":"1193-1207","source":"Springer Link","title":"Revisiting variable-foreperiod effects: evaluating the repetition priming account","title-short":"Revisiting variable-foreperiod effects","type":"article-journal","URL":"https://doi.org/10.3758/s13414-022-02476-5","volume":"84"},
  {"id":"heBagTricksImage2019","abstract":"Much of the recent progress made in image classiﬁcation research can be credited to training procedure reﬁnements, such as changes in data augmentations and optimization methods. In the literature, however, most reﬁnements are either brieﬂy mentioned as implementation details or only visible in source code. In this paper, we will examine a collection of such reﬁnements and empirically evaluate their impact on the ﬁnal model accuracy through ablation study. We will show that, by combining these reﬁnements together, we are able to improve various CNN models signiﬁcantly. For example, we raise ResNet-50’s top-1 validation accuracy from 75.3% to 79.29% on ImageNet. We will also demonstrate that improvement on image classiﬁcation accuracy leads to better transfer learning performance in other application domains such as object detection and semantic segmentation.","accessed":{"date-parts":[[2023,2,28]]},"author":[{"family":"He","given":"Tong"},{"family":"Zhang","given":"Zhi"},{"family":"Zhang","given":"Hang"},{"family":"Zhang","given":"Zhongyue"},{"family":"Xie","given":"Junyuan"},{"family":"Li","given":"Mu"}],"citation-key":"heBagTricksImage2019","container-title":"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","DOI":"10.1109/CVPR.2019.00065","event-place":"Long Beach, CA, USA","event-title":"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","ISBN":"978-1-72813-293-8","issued":{"date-parts":[[2019,6]]},"language":"en","page":"558-567","publisher":"IEEE","publisher-place":"Long Beach, CA, USA","source":"DOI.org (Crossref)","title":"Bag of Tricks for Image Classification with Convolutional Neural Networks","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/8954382/"},
  {"id":"heDeepResidualLearning2016","author":[{"family":"He","given":"Kaiming"},{"family":"Zhang","given":"Xiangyu"},{"family":"Ren","given":"Shaoqing"},{"family":"Sun","given":"Jian"}],"citation-key":"heDeepResidualLearning2016","container-title":"Proceedings of the IEEE conference on computer vision and pattern recognition","issued":{"date-parts":[[2016]]},"page":"770–778","title":"Deep residual learning for image recognition","type":"paper-conference"},
  {"id":"hendrycksAugMixSimpleData2020","abstract":"Modern deep neural networks can achieve high accuracy when the training distribution and test distribution are identically distributed, but this assumption is frequently violated in practice. When the train and test distributions are mismatched, accuracy can plummet. Currently there are few techniques that improve robustness to unforeseen data shifts encountered during deployment. In this work, we propose a technique to improve the robustness and uncertainty estimates of image classiﬁers. We propose AUGMIX, a data processing technique that is simple to implement, adds limited computational overhead, and helps models withstand unforeseen corruptions. AUGMIX signiﬁcantly improves robustness and uncertainty measures on challenging image classiﬁcation benchmarks, closing the gap between previous methods and the best possible performance in some cases by more than half.","accessed":{"date-parts":[[2023,1,16]]},"author":[{"family":"Hendrycks","given":"Dan"},{"family":"Mu","given":"Norman"},{"family":"Cubuk","given":"Ekin D."},{"family":"Zoph","given":"Barret"},{"family":"Gilmer","given":"Justin"},{"family":"Lakshminarayanan","given":"Balaji"}],"citation-key":"hendrycksAugMixSimpleData2020","issued":{"date-parts":[[2020,2,17]]},"language":"en","number":"arXiv:1912.02781","publisher":"arXiv","source":"arXiv.org","title":"AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty","title-short":"AugMix","type":"article","URL":"http://arxiv.org/abs/1912.02781"},
  {"id":"herlambangModelingMotivationUsing2021","abstract":"Motivation can counteract the effects of mental fatigue. However, the underlying mechanism by which motivation affects performance in mentally fatiguing tasks is obscure. In this paper, we propose goal competition as a paradigm to understand the role of motivation and built three models of mental fatigue studies to demonstrate the mechanism in a cognitive architecture named PRIMs. Each of these studies explored the impact of reward and mental fatigue on performance. Overall, performance decreased in nonreward conditions but remained stable in reward conditions. The comparisons between our models and empirical data showed that our models were able to capture human performance. We managed to model changes in performance levels by adjusting the value of the main task goals, which controls the competition with distractions. In all the tasks modeled, the best model fits were obtained by a linear decrease in goal activation, suggesting this is a general pattern. We discuss possible mechanisms for activation decrease, and the potential of goal competition to model motivation.","accessed":{"date-parts":[[2023,1,25]]},"author":[{"family":"Herlambang","given":"Mega B."},{"family":"Taatgen","given":"Niels A."},{"family":"Cnossen","given":"Fokie"}],"citation-key":"herlambangModelingMotivationUsing2021","container-title":"Journal of Mathematical Psychology","container-title-short":"Journal of Mathematical Psychology","DOI":"10.1016/j.jmp.2021.102540","ISSN":"0022-2496","issued":{"date-parts":[[2021,6,1]]},"language":"en","page":"102540","source":"ScienceDirect","title":"Modeling motivation using goal competition in mental fatigue studies","type":"article-journal","URL":"https://www.sciencedirect.com/science/article/pii/S0022249621000316","volume":"102"},
  {"id":"herlambangModelingMotivationUsing2021a","abstract":"Motivation can counteract the effects of mental fatigue. However, the underlying mechanism by which motivation affects performance in mentally fatiguing tasks is obscure. In this paper, we propose goal competition as a paradigm to understand the role of motivation and built three models of mental fatigue studies to demonstrate the mechanism in a cognitive architecture named PRIMs. Each of these studies explored the impact of reward and mental fatigue on performance. Overall, performance decreased in nonreward conditions but remained stable in reward conditions. The comparisons between our models and empirical data showed that our models were able to capture human performance. We managed to model changes in performance levels by adjusting the value of the main task goals, which controls the competition with distractions. In all the tasks modeled, the best model fits were obtained by a linear decrease in goal activation, suggesting this is a general pattern. We discuss possible mechanisms for activation decrease, and the potential of goal competition to model motivation.","accessed":{"date-parts":[[2023,1,25]]},"author":[{"family":"Herlambang","given":"Mega B."},{"family":"Taatgen","given":"Niels A."},{"family":"Cnossen","given":"Fokie"}],"citation-key":"herlambangModelingMotivationUsing2021a","container-title":"Journal of Mathematical Psychology","container-title-short":"Journal of Mathematical Psychology","DOI":"10.1016/j.jmp.2021.102540","ISSN":"0022-2496","issued":{"date-parts":[[2021,6,1]]},"language":"en","page":"102540","source":"ScienceDirect","title":"Modeling motivation using goal competition in mental fatigue studies","type":"article-journal","URL":"https://www.sciencedirect.com/science/article/pii/S0022249621000316","volume":"102"},
  {"id":"hintonDistillingKnowledgeNeural2015","abstract":"A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions [3]. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators [1] have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can signiﬁcantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish ﬁne-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.","accessed":{"date-parts":[[2022,11,18]]},"author":[{"family":"Hinton","given":"Geoffrey"},{"family":"Vinyals","given":"Oriol"},{"family":"Dean","given":"Jeff"}],"citation-key":"hintonDistillingKnowledgeNeural2015","issued":{"date-parts":[[2015,3,9]]},"language":"en","number":"arXiv:1503.02531","publisher":"arXiv","source":"arXiv.org","title":"Distilling the Knowledge in a Neural Network","type":"article","URL":"http://arxiv.org/abs/1503.02531"},
  {"id":"hohmanSummitScalingDeep2019","abstract":"Deep learning is increasingly used in decision-making tasks. However, understanding how neural networks produce final predictions remains a fundamental challenge. Existing work on interpreting neural network predictions for images often focuses on explaining predictions for single images or neurons. As predictions are often computed from millions of weights that are optimized over millions of images, such explanations can easily miss a bigger picture. We present Summit, an interactive system that scalably and systematically summarizes and visualizes what features a deep learning model has learned and how those features interact to make predictions. Summit introduces two new scalable summarization techniques: (1) activation aggregation discovers important neurons, and (2) neuron-influence aggregation identifies relationships among such neurons. Summit combines these techniques to create the novel attribution graph that reveals and summarizes crucial neuron associations and substructures that contribute to a model's outcomes. Summit scales to large data, such as the ImageNet dataset with 1.2M images, and leverages neural network feature visualization and dataset examples to help users distill large, complex neural network models into compact, interactive visualizations. We present neural network exploration scenarios where Summit helps us discover multiple surprising insights into a prevalent, large-scale image classifier's learned representations and informs future neural network architecture design. The Summit visualization runs in modern web browsers and is open-sourced.","accessed":{"date-parts":[[2023,2,20]]},"author":[{"family":"Hohman","given":"Fred"},{"family":"Park","given":"Haekyu"},{"family":"Robinson","given":"Caleb"},{"family":"Chau","given":"Duen Horng"}],"citation-key":"hohmanSummitScalingDeep2019","issued":{"date-parts":[[2019,9,2]]},"language":"en","number":"arXiv:1904.02323","publisher":"arXiv","source":"arXiv.org","title":"Summit: Scaling Deep Learning Interpretability by Visualizing Activation and Attribution Summarizations","title-short":"Summit","type":"article","URL":"http://arxiv.org/abs/1904.02323"},
  {"id":"houDynamicVisualAttention2008","abstract":"A visual attention system should respond placidly when common stimuli are presented, while at the same time keep alert to anomalous visual inputs. In this paper, a dynamic visual attention model based on the rarity of features is proposed. We introduce the Incremental Coding Length (ICL) to measure the perspective entropy gain of each feature. The objective of our model is to maximize the entropy of the sampled visual features. In order to optimize energy consumption, the limit amount of energy of the system is re-distributed amongst features according to their Incremental Coding Length. By selecting features with large coding length increments, the computational system can achieve attention selectivity in both static and dynamic scenes. We demonstrate that the proposed model achieves superior accuracy in comparison to mainstream approaches in static saliency map generation. Moreover, we also show that our model captures several less-reported dynamic visual search behaviors, such as attentional swing and inhibition of return.","accessed":{"date-parts":[[2023,2,20]]},"author":[{"family":"Hou","given":"Xiaodi"},{"family":"Zhang","given":"Liqing"}],"citation-key":"houDynamicVisualAttention2008","container-title":"Advances in Neural Information Processing Systems","issued":{"date-parts":[[2008]]},"publisher":"Curran Associates, Inc.","source":"Neural Information Processing Systems","title":"Dynamic visual attention: searching for coding length increments","title-short":"Dynamic visual attention","type":"paper-conference","URL":"https://proceedings.neurips.cc/paper/2008/hash/a8baa56554f96369ab93e4f3bb068c22-Abstract.html","volume":"21"},
  {"id":"howardMobilenetsEfficientConvolutional2017","author":[{"family":"Howard","given":"Andrew G"},{"family":"Zhu","given":"Menglong"},{"family":"Chen","given":"Bo"},{"family":"Kalenichenko","given":"Dmitry"},{"family":"Wang","given":"Weijun"},{"family":"Weyand","given":"Tobias"},{"family":"Andreetto","given":"Marco"},{"family":"Adam","given":"Hartwig"}],"citation-key":"howardMobilenetsEfficientConvolutional2017","container-title":"arXiv preprint arXiv:1704.04861","issued":{"date-parts":[[2017]]},"title":"Mobilenets: Efficient convolutional neural networks for mobile vision applications","type":"article-journal"},
  {"id":"howardUniversalLanguageModel2018","abstract":"Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100x more data. We open-source our pretrained models and code.","accessed":{"date-parts":[[2023,2,27]]},"author":[{"family":"Howard","given":"Jeremy"},{"family":"Ruder","given":"Sebastian"}],"citation-key":"howardUniversalLanguageModel2018","DOI":"10.48550/arXiv.1801.06146","issued":{"date-parts":[[2018,5,23]]},"number":"arXiv:1801.06146","publisher":"arXiv","source":"arXiv.org","title":"Universal Language Model Fine-tuning for Text Classification","type":"article","URL":"http://arxiv.org/abs/1801.06146"},
  {"id":"huangSnapMixSemanticallyProportional2021","abstract":"Data mixing augmentation has proved effective in training deep models. Recent methods mix labels mainly according to the mixture proportion of image pixels. Due to the major discriminative information of a fine-grained image usually resides in subtle regions, these methods tend to introduce heavy label noise in fine-grained recognition. We propose Semantically Proportional Mixing (SnapMix) that exploits class activation map (CAM) to lessen the label noise in augmenting fine-grained data. SnapMix generates the target label for a mixed image by estimating its intrinsic semantic composition. This strategy can adapt to asymmetric mixing operations and ensure semantic correspondence between synthetic images and target labels. Experiments show that our method consistently outperforms existing mixed-based approaches regardless of different datasets or network depths. Further, by incorporating the mid-level features, the proposed SnapMix achieves top-level performance, demonstrating its potential to serve as a strong baseline for fine-grained recognition.","accessed":{"date-parts":[[2023,3,31]]},"author":[{"family":"Huang","given":"Shaoli"},{"family":"Wang","given":"Xinchao"},{"family":"Tao","given":"Dacheng"}],"citation-key":"huangSnapMixSemanticallyProportional2021","container-title":"Proceedings of the AAAI Conference on Artificial Intelligence","DOI":"10.1609/aaai.v35i2.16255","ISSN":"2374-3468","issue":"2","issued":{"date-parts":[[2021,5,18]]},"language":"en","license":"Copyright (c) 2021 Association for the Advancement of Artificial Intelligence","number":"2","page":"1628-1636","source":"ojs.aaai.org","title":"SnapMix: Semantically Proportional Mixing for Augmenting Fine-grained Data","title-short":"SnapMix","type":"article-journal","URL":"https://ojs.aaai.org/index.php/AAAI/article/view/16255","volume":"35"},
  {"id":"huoSURVEYMANIFOLDBASEDLEARNING","abstract":"We review the ideas, algorithms, and numerical performance of manifold-based machine learning and dimension reduction methods. The representative methods include locally linear embedding (LLE), ISOMAP, Laplacian eigenmaps, Hessian eigenmaps, local tangent space alignment (LTSA), and charting. We describe the insights from these developments, as well as new opportunities for both researchers and practitioners. Potential applications in image and sensor data are illustrated. This chapter is based on an invited survey presentation that was delivered by Huo at the 2004 INFORMS Annual Meeting, which was held in Denver, CO, USA.","author":[{"family":"Huo","given":"Xiaoming"},{"family":"Smith","given":"Andrew K"}],"citation-key":"huoSURVEYMANIFOLDBASEDLEARNING","language":"en","page":"34","source":"Zotero","title":"A SURVEY OF MANIFOLD-BASED LEARNING METHODS","type":"article-journal"},
  {"id":"huPreliminaryStudyData2019","abstract":"Deep learning models have a large number of free parameters that need to be calculated by effective training of the models on a great deal of training data to improve their generalization performance. However, data obtaining and labeling is expensive in practice. Data augmentation is one of the methods to alleviate this problem. In this paper, we conduct a preliminary study on how three variables (augmentation method, augmentation rate and size of basic dataset per label) can affect the accuracy of deep learning for image classiﬁcation. The study provides some guidelines: (1) it is better to use transformations that alter the geometry of the images rather than those just lighting and color. (2) 2-3 times augmentation rate is good enough for training. (3) the smaller amount of data, the more obvious contributions could have.","accessed":{"date-parts":[[2023,2,1]]},"author":[{"family":"Hu","given":"Benlin"},{"family":"Lei","given":"Cheng"},{"family":"Wang","given":"Dong"},{"family":"Zhang","given":"Shu"},{"family":"Chen","given":"Zhenyu"}],"citation-key":"huPreliminaryStudyData2019","issued":{"date-parts":[[2019,6,9]]},"language":"en","number":"arXiv:1906.11887","publisher":"arXiv","source":"arXiv.org","title":"A Preliminary Study on Data Augmentation of Deep Learning for Image Classification","type":"article","URL":"http://arxiv.org/abs/1906.11887"},
  {"id":"inoueDataAugmentationPairing2018","abstract":"Data augmentation is a widely used technique in many machine learning tasks, such as image classification, to virtually enlarge the training dataset size and avoid overfitting. Traditional data augmentation techniques for image classification tasks create new samples from the original training data by, for example, flipping, distorting, adding a small amount of noise to, or cropping a patch from an original image. In this paper, we introduce a simple but surprisingly effective data augmentation technique for image classification tasks. With our technique, named SamplePairing, we synthesize a new sample from one image by overlaying another image randomly chosen from the training data (i.e., taking an average of two images for each pixel). By using two images randomly selected from the training set, we can generate $N^2$ new samples from $N$ training samples. This simple data augmentation technique significantly improved classification accuracy for all the tested datasets; for example, the top-1 error rate was reduced from 33.5% to 29.0% for the ILSVRC 2012 dataset with GoogLeNet and from 8.22% to 6.93% in the CIFAR-10 dataset. We also show that our SamplePairing technique largely improved accuracy when the number of samples in the training set was very small. Therefore, our technique is more valuable for tasks with a limited amount of training data, such as medical imaging tasks.","accessed":{"date-parts":[[2023,3,30]]},"author":[{"family":"Inoue","given":"Hiroshi"}],"citation-key":"inoueDataAugmentationPairing2018","issued":{"date-parts":[[2018,4,11]]},"number":"arXiv:1801.02929","publisher":"arXiv","source":"arXiv.org","title":"Data Augmentation by Pairing Samples for Images Classification","type":"article","URL":"http://arxiv.org/abs/1801.02929"},
  {"id":"IntroductionAdvancedExplainable","accessed":{"date-parts":[[2023,5,7]]},"citation-key":"IntroductionAdvancedExplainable","title":"Introduction: Advanced Explainable AI for computer vision — Advanced AI explainability with pytorch-gradcam","type":"webpage","URL":"https://jacobgil.github.io/pytorch-gradcam-book/introduction.html"},
  {"id":"irieModernSelfReferentialWeight2022","abstract":"The weight matrix (WM) of a neural network (NN) is its program. The programs of many traditional NNs are learned through gradient descent in some error function, then remain fixed. The WM of a self-referential NN, however, can keep rapidly modifying all of itself during runtime. In principle, such NNs can meta-learn to learn, and meta-meta-learn to meta-learn to learn, and so on, in the sense of recursive self-improvement. While NN architectures potentially capable of implementing such behaviour have been proposed since the ’90s, there have been few if any practical studies. Here we revisit such NNs, building upon recent successes of fast weight programmers and closely related linear Transformers. We propose a scalable self-referential WM (SRWM) that learns to use outer products and the delta update rule to modify itself. We evaluate our SRWM in supervised few-shot learning and in multi-task reinforcement learning with procedurally generated game environments. Our experiments demonstrate both practical applicability and competitive performance of the proposed SRWM. Our code is public.","accessed":{"date-parts":[[2023,4,26]]},"author":[{"family":"Irie","given":"Kazuki"},{"family":"Schlag","given":"Imanol"},{"family":"Csordás","given":"Róbert"},{"family":"Schmidhuber","given":"Jürgen"}],"citation-key":"irieModernSelfReferentialWeight2022","container-title":"Proceedings of the 39th International Conference on Machine Learning","event-title":"International Conference on Machine Learning","ISSN":"2640-3498","issued":{"date-parts":[[2022,6,28]]},"language":"en","page":"9660-9677","publisher":"PMLR","source":"proceedings.mlr.press","title":"A Modern Self-Referential Weight Matrix That Learns to Modify Itself","type":"paper-conference","URL":"https://proceedings.mlr.press/v162/irie22b.html"},
  {"id":"izenmanIntroductionManifoldLearning2012","accessed":{"date-parts":[[2022,11,15]]},"author":[{"family":"Izenman","given":"Alan Julian"}],"citation-key":"izenmanIntroductionManifoldLearning2012","container-title":"Wiley Interdisciplinary Reviews: Computational Statistics","container-title-short":"WIREs Comp Stat","DOI":"10.1002/wics.1222","ISSN":"19395108","issue":"5","issued":{"date-parts":[[2012,9]]},"language":"en","page":"439-446","source":"DOI.org (Crossref)","title":"Introduction to manifold learning: Introduction to manifold learning","title-short":"Introduction to manifold learning","type":"article-journal","URL":"https://onlinelibrary.wiley.com/doi/10.1002/wics.1222","volume":"4"},
  {"id":"jacobPyTorchLibraryCAM2021","author":[{"family":"Jacob","given":"Gildenblat"},{"family":"contributors","given":""}],"citation-key":"jacobPyTorchLibraryCAM2021","issued":{"date-parts":[[2021]]},"title":"PyTorch library for CAM methods","type":"software","URL":"https://github.com/jacobgil/pytorch-grad-cam"},
  {"id":"jacoviTrendsExplainableAI2023","abstract":"The XAI literature is decentralized, both in terminology and in publication venues, but recent years saw the community converge around keywords that make it possible to more reliably discover papers automatically. We use keyword search using the SemanticScholar API and manual curation to collect a well-formatted and reasonably comprehensive set of 5199 XAI papers, available at https://github.com/alonjacovi/XAI-Scholar . We use this collection to clarify and visualize trends about the size and scope of the literature, citation trends, cross-field trends, and collaboration trends. Overall, XAI is becoming increasingly multidisciplinary, with relative growth in papers belonging to increasingly diverse (non-CS) scientific fields, increasing cross-field collaborative authorship, increasing cross-field citation activity. The collection can additionally be used as a paper discovery engine, by retrieving XAI literature which is cited according to specific constraints (for example, papers that are influential outside of their field, or influential to non-XAI research).","accessed":{"date-parts":[[2023,5,19]]},"author":[{"family":"Jacovi","given":"Alon"}],"citation-key":"jacoviTrendsExplainableAI2023","DOI":"10.48550/arXiv.2301.05433","issued":{"date-parts":[[2023,1,13]]},"number":"arXiv:2301.05433","publisher":"arXiv","source":"arXiv.org","title":"Trends in Explainable AI (XAI) Literature","type":"article","URL":"http://arxiv.org/abs/2301.05433"},
  {"id":"jaipuriaDeflatingDatasetBias2020","abstract":"Deep Learning has seen an unprecedented increase in vision applications since the publication of large-scale object recognition datasets and introduction of scalable compute hardware. State-of-the-art methods for most vision tasks for Autonomous Vehicles (AVs) rely on supervised learning and often fail to generalize to domain shifts and/or outliers. Dataset diversity is thus key to successful real-world deployment. No matter how big the size of the dataset, capturing long tails of the distribution pertaining to task-speciﬁc environmental factors is impractical. The goal of this paper is to investigate the use of targeted synthetic data augmentation - combining the beneﬁts of gaming engine simulations and sim2real style transfer techniques - for ﬁlling gaps in real datasets for vision tasks. Empirical studies on three different computer vision tasks of practical use to AVs parking slot detection, lane detection and monocular depth estimation - consistently show that having synthetic data in the training mix provides a signiﬁcant boost in cross-dataset generalization performance as compared to training on real data only, for the same size of the training set.","accessed":{"date-parts":[[2023,1,16]]},"author":[{"family":"Jaipuria","given":"Nikita"},{"family":"Zhang","given":"Xianling"},{"family":"Bhasin","given":"Rohan"},{"family":"Arafa","given":"Mayar"},{"family":"Chakravarty","given":"Punarjay"},{"family":"Shrivastava","given":"Shubham"},{"family":"Manglani","given":"Sagar"},{"family":"Murali","given":"Vidya N."}],"citation-key":"jaipuriaDeflatingDatasetBias2020","container-title":"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","DOI":"10.1109/CVPRW50498.2020.00394","event-place":"Seattle, WA, USA","event-title":"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","ISBN":"978-1-72819-360-1","issued":{"date-parts":[[2020,6]]},"language":"en","page":"3344-3353","publisher":"IEEE","publisher-place":"Seattle, WA, USA","source":"DOI.org (Crossref)","title":"Deflating Dataset Bias Using Synthetic Data Augmentation","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/9150612/"},
  {"id":"jiangMLVCNNMultiLoopViewConvolutional2019","abstract":"3D shape retrieval has attracted much attention and become a hot topic in computer vision field recently.With the development of deep learning, 3D shape retrieval has also made great progress and many view-based methods have been introduced in recent years. However, how to represent 3D shapes better is still a challenging problem. At the same time, the intrinsic hierarchical associations among views still have not been well utilized. In order to tackle these problems, in this paper, we propose a multi-loop-view convolutional neural network (MLVCNN) framework for 3D shape retrieval. In this method, multiple groups of views are extracted from different loop directions first. Given these multiple loop views, the proposed MLVCNN framework introduces a hierarchical view-loop-shape architecture, i.e., the view level, the loop level, and the shape level, to conduct 3D shape representation from different scales. In the view-level, a convolutional neural network is first trained to extract view features. Then, the proposed Loop Normalization and LSTM are utilized for each loop of view to generate the loop-level features, which considering the intrinsic associations of the different views in the same loop. Finally, all the loop-level descriptors are combined into a shape-level descriptor for 3D shape representation, which is used for 3D shape retrieval. Our proposed method has been evaluated on the public 3D shape benchmark, i.e., ModelNet40. Experiments and comparisons with the state-of-the-art methods show that the proposed MLVCNN method can achieve significant performance improvement on 3D shape retrieval tasks. Our MLVCNN outperforms the state-of-the-art methods by the mAP of 4.84% in 3D shape retrieval task. We have also evaluated the performance of the proposed method on the 3D shape classification task where MLVCNN also achieves superior performance compared with recent methods.","accessed":{"date-parts":[[2022,5,10]]},"author":[{"family":"Jiang","given":"Jianwen"},{"family":"Bao","given":"Di"},{"family":"Chen","given":"Ziqiang"},{"family":"Zhao","given":"Xibin"},{"family":"Gao","given":"Yue"}],"citation-key":"jiangMLVCNNMultiLoopViewConvolutional2019","container-title":"Proceedings of the AAAI Conference on Artificial Intelligence","container-title-short":"AAAI","DOI":"10.1609/aaai.v33i01.33018513","ISSN":"2374-3468, 2159-5399","issued":{"date-parts":[[2019,7,17]]},"language":"en","page":"8513-8520","source":"DOI.org (Crossref)","title":"MLVCNN: Multi-Loop-View Convolutional Neural Network for 3D Shape Retrieval","title-short":"MLVCNN","type":"article-journal","URL":"http://aaai.org/ojs/index.php/AAAI/article/view/4869","volume":"33"},
  {"id":"jiangMLVCNNMultiloopviewConvolutional2019","author":[{"family":"Jiang","given":"Jianwen"},{"family":"Bao","given":"Di"},{"family":"Chen","given":"Ziqiang"},{"family":"Zhao","given":"Xibin"},{"family":"Gao","given":"Yue"}],"citation-key":"jiangMLVCNNMultiloopviewConvolutional2019","container-title":"Proceedings of the AAAI Conference on Artificial Intelligence","issue":"01","issued":{"date-parts":[[2019]]},"page":"8513–8520","title":"MLVCNN: Multi-loop-view convolutional neural network for 3D shape retrieval","type":"paper-conference","volume":"33"},
  {"id":"kaiserDepthwiseSeparableConvolutions2017","abstract":"Depthwise separable convolutions reduce the number of parameters and computation used in convolutional operations while increasing representational efficiency. They have been shown to be successful in image classification models, both in obtaining better models than previously possible for a given parameter count (the Xception architecture) and considerably reducing the number of parameters required to perform at a given level (the MobileNets family of architectures). Recently, convolutional sequence-to-sequence networks have been applied to machine translation tasks with good results. In this work, we study how depthwise separable convolutions can be applied to neural machine translation. We introduce a new architecture inspired by Xception and ByteNet, called SliceNet, which enables a significant reduction of the parameter count and amount of computation needed to obtain results like ByteNet, and, with a similar parameter count, achieves new state-of-the-art results. In addition to showing that depthwise separable convolutions perform well for machine translation, we investigate the architectural changes that they enable: we observe that thanks to depthwise separability, we can increase the length of convolution windows, removing the need for filter dilation. We also introduce a new \"super-separable\" convolution operation that further reduces the number of parameters and computational cost for obtaining state-of-the-art results.","accessed":{"date-parts":[[2022,5,24]]},"author":[{"family":"Kaiser","given":"Lukasz"},{"family":"Gomez","given":"Aidan N."},{"family":"Chollet","given":"Francois"}],"citation-key":"kaiserDepthwiseSeparableConvolutions2017","DOI":"10.48550/arXiv.1706.03059","issued":{"date-parts":[[2017,6,15]]},"number":"arXiv:1706.03059","publisher":"arXiv","source":"arXiv.org","title":"Depthwise Separable Convolutions for Neural Machine Translation","type":"article","URL":"http://arxiv.org/abs/1706.03059"},
  {"id":"kaiserOneModelLearn2017","abstract":"Deep learning yields great results across many fields, from speech recognition, image classification, to translation. But for each problem, getting a deep model to work well involves research into the architecture and a long period of tuning. We present a single model that yields good results on a number of problems spanning multiple domains. In particular, this single model is trained concurrently on ImageNet, multiple translation tasks, image captioning (COCO dataset), a speech recognition corpus, and an English parsing task. Our model architecture incorporates building blocks from multiple domains. It contains convolutional layers, an attention mechanism, and sparsely-gated layers. Each of these computational blocks is crucial for a subset of the tasks we train on. Interestingly, even if a block is not crucial for a task, we observe that adding it never hurts performance and in most cases improves it on all tasks. We also show that tasks with less data benefit largely from joint training with other tasks, while performance on large tasks degrades only slightly if at all.","accessed":{"date-parts":[[2022,5,24]]},"author":[{"family":"Kaiser","given":"Lukasz"},{"family":"Gomez","given":"Aidan N."},{"family":"Shazeer","given":"Noam"},{"family":"Vaswani","given":"Ashish"},{"family":"Parmar","given":"Niki"},{"family":"Jones","given":"Llion"},{"family":"Uszkoreit","given":"Jakob"}],"citation-key":"kaiserOneModelLearn2017","DOI":"10.48550/arXiv.1706.05137","issued":{"date-parts":[[2017,6,15]]},"number":"arXiv:1706.05137","publisher":"arXiv","source":"arXiv.org","title":"One Model To Learn Them All","type":"article","URL":"http://arxiv.org/abs/1706.05137"},
  {"id":"kaiserOneModelLearn2017a","abstract":"Deep learning yields great results across many fields, from speech recognition, image classification, to translation. But for each problem, getting a deep model to work well involves research into the architecture and a long period of tuning. We present a single model that yields good results on a number of problems spanning multiple domains. In particular, this single model is trained concurrently on ImageNet, multiple translation tasks, image captioning (COCO dataset), a speech recognition corpus, and an English parsing task. Our model architecture incorporates building blocks from multiple domains. It contains convolutional layers, an attention mechanism, and sparsely-gated layers. Each of these computational blocks is crucial for a subset of the tasks we train on. Interestingly, even if a block is not crucial for a task, we observe that adding it never hurts performance and in most cases improves it on all tasks. We also show that tasks with less data benefit largely from joint training with other tasks, while performance on large tasks degrades only slightly if at all.","accessed":{"date-parts":[[2022,5,24]]},"author":[{"family":"Kaiser","given":"Lukasz"},{"family":"Gomez","given":"Aidan N."},{"family":"Shazeer","given":"Noam"},{"family":"Vaswani","given":"Ashish"},{"family":"Parmar","given":"Niki"},{"family":"Jones","given":"Llion"},{"family":"Uszkoreit","given":"Jakob"}],"citation-key":"kaiserOneModelLearn2017a","DOI":"10.48550/arXiv.1706.05137","issued":{"date-parts":[[2017,6,15]]},"number":"arXiv:1706.05137","publisher":"arXiv","source":"arXiv.org","title":"One Model To Learn Them All","type":"article","URL":"http://arxiv.org/abs/1706.05137"},
  {"id":"kanezakiRotationNetJointObject2018","author":[{"family":"Kanezaki","given":"Asako"},{"family":"Matsushita","given":"Yasuyuki"},{"family":"Nishida","given":"Yoshifumi"}],"citation-key":"kanezakiRotationNetJointObject2018","container-title":"Proceedings of IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)","issued":{"date-parts":[[2018]]},"title":"RotationNet: Joint Object Categorization and Pose Estimation Using Multiviews from Unsupervised Viewpoints","type":"paper-conference"},
  {"id":"kanezakiRotationNetJointObject2021","abstract":"We propose a Convolutional Neural Network (CNN)-based model “RotationNet,” which takes multi-view images of an object as input and jointly estimates its pose and object category. Unlike previous approaches that use known viewpoint labels for training, our method treats the viewpoint labels as latent variables, which are learned in an unsupervised manner during the training using an unaligned object dataset. RotationNet uses only a partial set of multi-view images for inference, and this property makes it useful in practical scenarios where only partial views are available. Moreover, our pose alignment strategy enables one to obtain view-speciﬁc feature representations shared across classes, which is important to maintain high accuracy in both object categorization and pose estimation. Effectiveness of RotationNet is demonstrated by its superior performance to the state-of-the-art methods of 3D object classiﬁcation on 10- and 40-class ModelNet datasets. We also show that RotationNet, even trained without known poses, achieves comparable performance to the state-of-the-art methods on an object pose estimation dataset. Furthermore, our object ranking method based on classiﬁcation by RotationNet achieved the ﬁrst prize in two tracks of the 3D Shape Retrieval Contest (SHREC) 2017. Finally, we demonstrate the performance of real-world applications of RotationNet trained with our newly created multi-view image dataset using a moving USB camera.","accessed":{"date-parts":[[2022,5,10]]},"author":[{"family":"Kanezaki","given":"Asako"},{"family":"Matsushita","given":"Yasuyuki"},{"family":"Nishida","given":"Yoshifumi"}],"citation-key":"kanezakiRotationNetJointObject2021","container-title":"IEEE Transactions on Pattern Analysis and Machine Intelligence","container-title-short":"IEEE Trans. Pattern Anal. Mach. Intell.","DOI":"10.1109/TPAMI.2019.2922640","ISSN":"0162-8828, 2160-9292, 1939-3539","issue":"1","issued":{"date-parts":[[2021,1,1]]},"language":"en","page":"269-283","source":"DOI.org (Crossref)","title":"RotationNet for Joint Object Categorization and Unsupervised Pose Estimation from Multi-View Images","type":"article-journal","URL":"https://ieeexplore.ieee.org/document/8736864/","volume":"43"},
  {"id":"kangInterpretingUndesirablePixels2019","abstract":"In an effort to interpret black-box models, researches for developing explanation methods have proceeded in recent years. Most studies have tried to identify input pixels that are crucial to the prediction of a classifier. While this approach is meaningful to analyse the characteristic of blackbox models, it is also important to investigate pixels that interfere with the prediction. To tackle this issue, in this paper, we propose an explanation method that visualizes undesirable regions to classify an image as a target class. To be specific, we divide the concept of undesirable regions into two terms: (1) factors for a target class, which hinder that black-box models identify intrinsic characteristics of a target class and (2) factors for non-target classes that are important regions for an image to be classified as other classes. We visualize such undesirable regions on heatmaps to qualitatively validate the proposed method. Furthermore, we present an evaluation metric to provide quantitative results on ImageNet.","accessed":{"date-parts":[[2022,9,26]]},"author":[{"family":"Kang","given":"Sin-Han"},{"family":"Jung","given":"Hong-Gyu"},{"family":"Lee","given":"Seong-Whan"}],"citation-key":"kangInterpretingUndesirablePixels2019","issued":{"date-parts":[[2019,12,16]]},"number":"arXiv:1909.12446","publisher":"arXiv","source":"arXiv.org","title":"Interpreting Undesirable Pixels for Image Classification on Black-Box Models","type":"article","URL":"http://arxiv.org/abs/1909.12446"},
  {"id":"kasaeiOrthographicNetDeepTransfer2020","abstract":"Nowadays, service robots are appearing more and more in our daily life. For this type of robot, open-ended object category learning and recognition is necessary since no matter how extensive the training data used for batch learning, the robot might be faced with a new object when operating in a real-world environment. In this work, we present OrthographicNet, a Convolutional Neural Network (CNN)-based model, for 3D object recognition in open-ended domains. In particular, OrthographicNet generates a global rotation- and scale-invariant representation for a given 3D object, enabling robots to recognize the same or similar objects seen from different perspectives. Experimental results show that our approach yields significant improvements over the previous state-of-the-art approaches concerning object recognition performance and scalability in open-ended scenarios. Moreover, OrthographicNet demonstrates the capability of learning new categories from very few examples on-site. Regarding real-time performance, three real-world demonstrations validate the promising performance of the proposed architecture.","accessed":{"date-parts":[[2022,5,12]]},"author":[{"family":"Kasaei","given":"Hamidreza"}],"citation-key":"kasaeiOrthographicNetDeepTransfer2020","container-title":"arXiv:1902.03057 [cs]","issued":{"date-parts":[[2020,12,31]]},"source":"arXiv.org","title":"OrthographicNet: A Deep Transfer Learning Approach for 3D Object Recognition in Open-Ended Domains","title-short":"OrthographicNet","type":"article-journal","URL":"http://arxiv.org/abs/1902.03057"},
  {"id":"kasaeiPerceivingLearningRecognizing2018","author":[{"family":"Kasaei","given":"S"},{"family":"Sock","given":"Juil"},{"family":"Lopes","given":"Luis Seabra"},{"family":"Tomé","given":"Ana Maria"},{"family":"Kim","given":"Tae-Kyun"}],"citation-key":"kasaeiPerceivingLearningRecognizing2018","container-title":"Proceedings of the AAAI Conference on Artificial Intelligence","issued":{"date-parts":[[2018]]},"title":"Perceiving, learning, and recognizing 3d objects: An approach to cognitive service robots","type":"paper-conference"},
  {"id":"keaneIfOnlyWe2021","abstract":"In recent years, there has been an explosion of AI research on counterfactual explanations as a solution to the problem of eXplainable AI (XAI). These explanations seem to offer technical, psychological and legal benefits over other explanation techniques. We survey 100 distinct counterfactual explanation methods reported in the literature. This survey addresses the extent to which these methods have been adequately evaluated, both psychologically and computationally, and quantifies the shortfalls occurring. For instance, only 21% of these methods have been user tested. Five key deficits in the evaluation of these methods are detailed and a roadmap, with standardised benchmark evaluations, is proposed to resolve the issues arising; issues, that currently effectively block scientific progress in this field.","accessed":{"date-parts":[[2023,2,13]]},"author":[{"family":"Keane","given":"Mark T."},{"family":"Kenny","given":"Eoin M."},{"family":"Delaney","given":"Eoin"},{"family":"Smyth","given":"Barry"}],"citation-key":"keaneIfOnlyWe2021","issued":{"date-parts":[[2021,2,26]]},"number":"arXiv:2103.01035","publisher":"arXiv","source":"arXiv.org","title":"If Only We Had Better Counterfactual Explanations: Five Key Deficits to Rectify in the Evaluation of Counterfactual XAI Techniques","title-short":"If Only We Had Better Counterfactual Explanations","type":"article","URL":"http://arxiv.org/abs/2103.01035"},
  {"id":"khanUnsupervisedPrimitiveDiscovery2019","accessed":{"date-parts":[[2022,5,10]]},"author":[{"family":"Khan","given":"Salman H."},{"family":"Guo","given":"Yulan"},{"family":"Hayat","given":"Munawar"},{"family":"Barnes","given":"Nick"}],"citation-key":"khanUnsupervisedPrimitiveDiscovery2019","container-title":"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","DOI":"10.1109/CVPR.2019.00997","event-place":"Long Beach, CA, USA","event-title":"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","ISBN":"978-1-72813-293-8","issued":{"date-parts":[[2019,6]]},"language":"en","page":"9731-9740","publisher":"IEEE","publisher-place":"Long Beach, CA, USA","source":"DOI.org (Crossref)","title":"Unsupervised Primitive Discovery for Improved 3D Generative Modeling","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/8954393/"},
  {"id":"khoslaNovelDatasetFineGrained","author":[{"family":"Khosla","given":"Aditya"},{"family":"Jayadevaprakash","given":"Nityananda"},{"family":"Yao","given":"Bangpeng"},{"family":"Li","given":"Fei-Fei"}],"citation-key":"khoslaNovelDatasetFineGrained","language":"en","source":"Zotero","title":"Novel Dataset for Fine-Grained Image Categorization: Stanford Dogs","type":"article-journal"},
  {"id":"kimCoMixupSaliencyGuided2021","abstract":"While deep neural networks show great performance on fitting to the training distribution, improving the networks' generalization performance to the test distribution and robustness to the sensitivity to input perturbations still remain as a challenge. Although a number of mixup based augmentation strategies have been proposed to partially address them, it remains unclear as to how to best utilize the supervisory signal within each input data for mixup from the optimization perspective. We propose a new perspective on batch mixup and formulate the optimal construction of a batch of mixup data maximizing the data saliency measure of each individual mixup data and encouraging the supermodular diversity among the constructed mixup data. This leads to a novel discrete optimization problem minimizing the difference between submodular functions. We also propose an efficient modular approximation based iterative submodular minimization algorithm for efficient mixup computation per each minibatch suitable for minibatch based neural network training. Our experiments show the proposed method achieves the state of the art generalization, calibration, and weakly supervised localization results compared to other mixup methods. The source code is available at https://github.com/snu-mllab/Co-Mixup.","accessed":{"date-parts":[[2023,4,4]]},"author":[{"family":"Kim","given":"Jang-Hyun"},{"family":"Choo","given":"Wonho"},{"family":"Jeong","given":"Hosan"},{"family":"Song","given":"Hyun Oh"}],"citation-key":"kimCoMixupSaliencyGuided2021","DOI":"10.48550/arXiv.2102.03065","issued":{"date-parts":[[2021,2,5]]},"number":"arXiv:2102.03065","publisher":"arXiv","source":"arXiv.org","title":"Co-Mixup: Saliency Guided Joint Mixup with Supermodular Diversity","title-short":"Co-Mixup","type":"article","URL":"http://arxiv.org/abs/2102.03065"},
  {"id":"kimPuzzleMixExploiting2020","abstract":"While deep neural networks achieve great performance on fitting the training distribution, the learned networks are prone to overfitting and are susceptible to adversarial attacks. In this regard, a number of mixup based augmentation methods have been recently proposed. However, these approaches mainly focus on creating previously unseen virtual examples and can sometimes provide misleading supervisory signal to the network. To this end, we propose Puzzle Mix, a mixup method for explicitly utilizing the saliency information and the underlying statistics of the natural examples. This leads to an interesting optimization problem alternating between the multi-label objective for optimal mixing mask and saliency discounted optimal transport objective. Our experiments show Puzzle Mix achieves the state of the art generalization and the adversarial robustness results compared to other mixup methods on CIFAR-100, Tiny-ImageNet, and ImageNet datasets, and the source code is available at https://github.com/snu-mllab/PuzzleMix.","accessed":{"date-parts":[[2023,4,4]]},"author":[{"family":"Kim","given":"Jang-Hyun"},{"family":"Choo","given":"Wonho"},{"family":"Song","given":"Hyun Oh"}],"citation-key":"kimPuzzleMixExploiting2020","container-title":"Proceedings of the 37th International Conference on Machine Learning","event-title":"International Conference on Machine Learning","ISSN":"2640-3498","issued":{"date-parts":[[2020,11,21]]},"language":"en","page":"5275-5285","publisher":"PMLR","source":"proceedings.mlr.press","title":"Puzzle Mix: Exploiting Saliency and Local Statistics for Optimal Mixup","title-short":"Puzzle Mix","type":"paper-conference","URL":"https://proceedings.mlr.press/v119/kim20b.html"},
  {"id":"kindermansReliabilitySaliencyMethods2017","abstract":"Saliency methods aim to explain the predictions of deep neural networks. These methods lack reliability when the explanation is sensitive to factors that do not contribute to the model prediction. We use a simple and common pre-processing step ---adding a constant shift to the input data--- to show that a transformation with no effect on the model can cause numerous methods to incorrectly attribute. In order to guarantee reliability, we posit that methods should fulfill input invariance, the requirement that a saliency method mirror the sensitivity of the model with respect to transformations of the input. We show, through several examples, that saliency methods that do not satisfy input invariance result in misleading attribution.","accessed":{"date-parts":[[2022,11,18]]},"author":[{"family":"Kindermans","given":"Pieter-Jan"},{"family":"Hooker","given":"Sara"},{"family":"Adebayo","given":"Julius"},{"family":"Alber","given":"Maximilian"},{"family":"Schütt","given":"Kristof T."},{"family":"Dähne","given":"Sven"},{"family":"Erhan","given":"Dumitru"},{"family":"Kim","given":"Been"}],"citation-key":"kindermansReliabilitySaliencyMethods2017","DOI":"10.48550/arXiv.1711.00867","issued":{"date-parts":[[2017,11,2]]},"number":"arXiv:1711.00867","publisher":"arXiv","source":"arXiv.org","title":"The (Un)reliability of saliency methods","type":"article","URL":"http://arxiv.org/abs/1711.00867"},
  {"id":"kingmaAdamMethodStochastic2014","author":[{"family":"Kingma","given":"Diederik P"},{"family":"Ba","given":"Jimmy"}],"citation-key":"kingmaAdamMethodStochastic2014","container-title":"arXiv preprint arXiv:1412.6980","issued":{"date-parts":[[2014]]},"title":"Adam: A method for stochastic optimization","type":"article-journal"},
  {"id":"kokhlikyanCaptumUnifiedGeneric2020","abstract":"In this paper we introduce a novel, unified, open-source model interpretability library for PyTorch [12]. The library contains generic implementations of a number of gradient and perturbation-based attribution algorithms, also known as feature, neuron and layer importance algorithms, as well as a set of evaluation metrics for these algorithms. It can be used for both classification and non-classification models including graph-structured models built on Neural Networks (NN). In this paper we give a high-level overview of supported attribution algorithms and show how to perform memory-efficient and scalable computations. We emphasize that the three main characteristics of the library are multimodality, extensibility and ease of use. Multimodality supports different modality of inputs such as image, text, audio or video. Extensibility allows adding new algorithms and features. The library is also designed for easy understanding and use. Besides, we also introduce an interactive visualization tool called Captum Insights that is built on top of Captum library and allows sample-based model debugging and visualization using feature importance metrics.","accessed":{"date-parts":[[2023,4,4]]},"author":[{"family":"Kokhlikyan","given":"Narine"},{"family":"Miglani","given":"Vivek"},{"family":"Martin","given":"Miguel"},{"family":"Wang","given":"Edward"},{"family":"Alsallakh","given":"Bilal"},{"family":"Reynolds","given":"Jonathan"},{"family":"Melnikov","given":"Alexander"},{"family":"Kliushkina","given":"Natalia"},{"family":"Araya","given":"Carlos"},{"family":"Yan","given":"Siqi"},{"family":"Reblitz-Richardson","given":"Orion"}],"citation-key":"kokhlikyanCaptumUnifiedGeneric2020","DOI":"10.48550/arXiv.2009.07896","issued":{"date-parts":[[2020,9,16]]},"number":"arXiv:2009.07896","publisher":"arXiv","source":"arXiv.org","title":"Captum: A unified and generic model interpretability library for PyTorch","title-short":"Captum","type":"article","URL":"http://arxiv.org/abs/2009.07896"},
  {"id":"kononowiczSlowPotentialsTime2011","abstract":"Numerous studies have shown that contingent negative variation (CNV) measured at fronto-central and parietal–central areas is closely related to interval timing. However, the exact nature of the relation between CNV and the underlying timing mechanisms is still a topic of discussion. On the one hand, it has been proposed that the CNV measured at supplementary motor area (SMA) is a direct reflection of the unfolding of time since a perceived onset, whereas other work has suggested that the increased amplitude reflects decision processes involved in interval timing. Strong evidence for the first view has been reported by Macar et al. (1999), who showed that variations in temporal performance were reflected in the measured CNV amplitude. If the CNV measured at SMA is a direct function of the passing of time, habituation effects are not expected. Here we report two replication studies, which both failed to replicate the expected performance-dependent variations. Even more powerful linear-mixed effect analyses failed to find any performance related effects on the CNV amplitude, whereas habituation effects were found. These studies therefore suggest that the CNV amplitude does not directly reflect the unfolding of time.","accessed":{"date-parts":[[2023,1,25]]},"author":[{"family":"Kononowicz","given":"Tadeusz"},{"family":"Van Rijn","given":"Hedderik"}],"citation-key":"kononowiczSlowPotentialsTime2011","container-title":"Frontiers in Integrative Neuroscience","ISSN":"1662-5145","issued":{"date-parts":[[2011]]},"source":"Frontiers","title":"Slow Potentials in Time Estimation: The Role of Temporal Accumulation and Habituation","title-short":"Slow Potentials in Time Estimation","type":"article-journal","URL":"https://www.frontiersin.org/articles/10.3389/fnint.2011.00048","volume":"5"},
  {"id":"kragelSimilarPatternsNeural2017","accessed":{"date-parts":[[2022,5,27]]},"author":[{"family":"Kragel","given":"James E."},{"family":"Ezzyat","given":"Youssef"},{"family":"Sperling","given":"Michael R."},{"family":"Gorniak","given":"Richard"},{"family":"Worrell","given":"Gregory A."},{"family":"Berry","given":"Brent M."},{"family":"Inman","given":"Cory"},{"family":"Lin","given":"Jui-Jui"},{"family":"Davis","given":"Kathryn A."},{"family":"Das","given":"Sandhitsu R."},{"family":"Stein","given":"Joel M."},{"family":"Jobst","given":"Barbara C."},{"family":"Zaghloul","given":"Kareem A."},{"family":"Sheth","given":"Sameer A."},{"family":"Rizzuto","given":"Daniel S."},{"family":"Kahana","given":"Michael J."}],"citation-key":"kragelSimilarPatternsNeural2017","container-title":"NeuroImage","container-title-short":"NeuroImage","DOI":"10.1016/j.neuroimage.2017.03.042","ISSN":"10538119","issued":{"date-parts":[[2017,7]]},"language":"en","page":"60-71","source":"DOI.org (Crossref)","title":"Similar patterns of neural activity predict memory function during encoding and retrieval","type":"article-journal","URL":"https://linkinghub.elsevier.com/retrieve/pii/S1053811917302549","volume":"155"},
  {"id":"kriegeskorteCognitiveComputationalNeuroscience2018","accessed":{"date-parts":[[2022,5,27]]},"author":[{"family":"Kriegeskorte","given":"Nikolaus"},{"family":"Douglas","given":"Pamela K."}],"citation-key":"kriegeskorteCognitiveComputationalNeuroscience2018","container-title":"Nature Neuroscience","container-title-short":"Nat Neurosci","DOI":"10.1038/s41593-018-0210-5","ISSN":"1097-6256, 1546-1726","issue":"9","issued":{"date-parts":[[2018,9]]},"language":"en","page":"1148-1160","source":"DOI.org (Crossref)","title":"Cognitive computational neuroscience","type":"article-journal","URL":"http://www.nature.com/articles/s41593-018-0210-5","volume":"21"},
  {"id":"krizhevskyLearningMultipleLayers","author":[{"family":"Krizhevsky","given":"Alex"}],"citation-key":"krizhevskyLearningMultipleLayers","language":"en","source":"Zotero","title":"Learning Multiple Layers of Features from Tiny Images","type":"article-journal"},
  {"id":"kruijneImplicitlyLearningWhen2022","abstract":"There is growing appreciation for the role of long-term memory in guiding temporal preparation in speeded reaction time tasks. In experiments with variable foreperiods between a warning stimulus (S1) and a target stimulus (S2), preparation is affected by foreperiod distributions experienced in the past, long after the distribution has changed. These effects from memory can shape preparation largely implicitly, outside of participants’ awareness. Recent studies have demonstrated the associative nature of memory-guided preparation. When distinct S1s predict different foreperiods, they can trigger differential preparation accordingly. Here, we propose that memory-guided preparation allows for another key feature of learning: the ability to generalize across acquired associations and apply them to novel situations. Participants completed a variable foreperiod task where S1 was a unique image of either a face or a scene on each trial. Images of either category were paired with different distributions with predominantly shorter versus predominantly longer foreperiods. Participants displayed differential preparation to never-before seen images of either category, without being aware of the predictive nature of these categories. They continued doing so in a subsequent Transfer phase, after they had been informed that these contingencies no longer held. A novel rolling regression analysis revealed at a fine timescale how category-guided preparation gradually developed throughout the task, and that explicit information about these contingencies only briefly disrupted memory-guided preparation. These results offer new insights into temporal preparation as the product of a largely implicit process governed by associative learning from past experiences.","accessed":{"date-parts":[[2023,1,25]]},"author":[{"family":"Kruijne","given":"Wouter"},{"family":"Galli","given":"Riccardo M."},{"family":"Los","given":"Sander A."}],"citation-key":"kruijneImplicitlyLearningWhen2022","container-title":"Psychonomic Bulletin & Review","container-title-short":"Psychon Bull Rev","DOI":"10.3758/s13423-021-02004-w","ISSN":"1531-5320","issue":"2","issued":{"date-parts":[[2022,4,1]]},"language":"en","page":"552-562","source":"Springer Link","title":"Implicitly learning when to be ready: From instances to categories","title-short":"Implicitly learning when to be ready","type":"article-journal","URL":"https://doi.org/10.3758/s13423-021-02004-w","volume":"29"},
  {"id":"kumawatLP3DCNNUnveilingLocal2019","abstract":"Traditional 3D Convolutional Neural Networks (CNNs) are computationally expensive, memory intensive, prone to overﬁt, and most importantly, there is a need to improve their feature learning capabilities. To address these issues, we propose Rectiﬁed Local Phase Volume (ReLPV) block, an efﬁcient alternative to the standard 3D convolutional layer. The ReLPV block extracts the phase in a 3D local neighborhood (e.g., 3 × 3 × 3) of each position of the input map to obtain the feature maps. The phase is extracted by computing 3D Short Term Fourier Transform (STFT) at multiple ﬁxed low frequency points in the 3D local neighborhood of each position. These feature maps at different frequency points are then linearly combined after passing them through an activation function. The ReLPV block provides signiﬁcant parameter savings of at least, 33 to 133 times compared to the standard 3D convolutional layer with the ﬁlter sizes 3 × 3 × 3 to 13 × 13 × 13, respectively. We show that the feature learning capabilities of the ReLPV block are signiﬁcantly better than the standard 3D convolutional layer. Furthermore, it produces consistently better results across different 3D data representations. We achieve state-of-the-art accuracy on the volumetric ModelNet10 and ModelNet40 datasets while utilizing only 11% parameters of the current state-of-theart. We also improve the state-of-the-art on the UCF-101 split-1 action recognition dataset by 5.68% (when trained from scratch) while using only 15% of the parameters of the state-of-the-art. The project webpage is available at https://sites.google.com/view/lp-3dcnn/home.","accessed":{"date-parts":[[2022,5,10]]},"author":[{"family":"Kumawat","given":"Sudhakar"},{"family":"Raman","given":"Shanmuganathan"}],"citation-key":"kumawatLP3DCNNUnveilingLocal2019","container-title":"arXiv:1904.03498 [cs]","issued":{"date-parts":[[2019,4,6]]},"language":"en","source":"arXiv.org","title":"LP-3DCNN: Unveiling Local Phase in 3D Convolutional Neural Networks","title-short":"LP-3DCNN","type":"article-journal","URL":"http://arxiv.org/abs/1904.03498"},
  {"id":"kumawatLp3dcnnUnveilingLocal2019","author":[{"family":"Kumawat","given":"Sudhakar"},{"family":"Raman","given":"Shanmuganathan"}],"citation-key":"kumawatLp3dcnnUnveilingLocal2019","container-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[[2019]]},"page":"4903–4912","title":"Lp-3dcnn: Unveiling local phase in 3d convolutional neural networks","type":"paper-conference"},
  {"id":"kuznetsovaExploitingViewspecificAppearance2016","author":[{"family":"Kuznetsova","given":"Alina"},{"family":"Hwang","given":"Sung Ju"},{"family":"Rosenhahn","given":"Bodo"},{"family":"Sigal","given":"Leonid"}],"citation-key":"kuznetsovaExploitingViewspecificAppearance2016","container-title":"Proceedings of the AAAI Conference on Artificial Intelligence","issue":"1","issued":{"date-parts":[[2016]]},"title":"Exploiting view-specific appearance similarities across classes for zero-shot pose prediction: A metric learning approach","type":"paper-conference","volume":"30"},
  {"id":"laiScalableTreebasedApproach2011","author":[{"family":"Lai","given":"Kevin"},{"family":"Bo","given":"Liefeng"},{"family":"Ren","given":"Xiaofeng"},{"family":"Fox","given":"Dieter"}],"citation-key":"laiScalableTreebasedApproach2011","container-title":"Proceedings of the AAAI Conference on Artificial Intelligence","issued":{"date-parts":[[2011]]},"title":"A scalable tree-based approach for joint object and pose recognition","type":"paper-conference"},
  {"id":"lanCouplformerRethinkingVision2021","abstract":"With the development of the self-attention mechanism, the Transformer model has demonstrated its outstanding performance in the computer vision domain. However, the massive computation brought from the full attention mechanism became a heavy burden for memory consumption. Sequentially, the limitation of memory reduces the possibility of improving the Transformer model. To remedy this problem, we propose a novel memory economy attention mechanism named Couplformer, which decouples the attention map into two sub-matrices and generates the alignment scores from spatial information. A series of different scale image classification tasks are applied to evaluate the effectiveness of our model. The result of experiments shows that on the ImageNet-1k classification task, the Couplformer can significantly decrease 28% memory consumption compared with regular Transformer while accessing sufficient accuracy requirements and outperforming 0.92% on Top-1 accuracy while occupying the same memory footprint. As a result, the Couplformer can serve as an efficient backbone in visual tasks, and provide a novel perspective on the attention mechanism for researchers.","accessed":{"date-parts":[[2022,10,3]]},"author":[{"family":"Lan","given":"Hai"},{"family":"Wang","given":"Xihao"},{"family":"Wei","given":"Xian"}],"citation-key":"lanCouplformerRethinkingVision2021","issued":{"date-parts":[[2021,12,10]]},"number":"arXiv:2112.05425","publisher":"arXiv","source":"arXiv.org","title":"Couplformer:Rethinking Vision Transformer with Coupling Attention Map","title-short":"Couplformer","type":"article","URL":"http://arxiv.org/abs/2112.05425"},
  {"id":"langExplainingStyleTraining2021","abstract":"Image classification models can depend on multiple different semantic attributes of the image. An explanation of the decision of the classifier needs to both discover and visualize these properties. Here we present StylEx, a method for doing this, by training a generative model to specifically explain multiple attributes that underlie classifier decisions. A natural source for such attributes is the StyleSpace of StyleGAN, which is known to generate semantically meaningful dimensions in the image. However, because standard GAN training is not dependent on the classifier, it may not represent these attributes which are important for the classifier decision, and the dimensions of StyleSpace may represent irrelevant attributes. To overcome this, we propose a training procedure for a StyleGAN, which incorporates the classifier model, in order to learn a classifier-specific StyleSpace. Explanatory attributes are then selected from this space. These can be used to visualize the effect of changing multiple attributes per image, thus providing image-specific explanations. We apply StylEx to multiple domains, including animals, leaves, faces and retinal images. For these, we show how an image can be modified in different ways to change its classifier output. Our results show that the method finds attributes that align well with semantic ones, generate meaningful image-specific explanations, and are human-interpretable as measured in user-studies.","accessed":{"date-parts":[[2022,9,6]]},"author":[{"family":"Lang","given":"Oran"},{"family":"Gandelsman","given":"Yossi"},{"family":"Yarom","given":"Michal"},{"family":"Wald","given":"Yoav"},{"family":"Elidan","given":"Gal"},{"family":"Hassidim","given":"Avinatan"},{"family":"Freeman","given":"William T."},{"family":"Isola","given":"Phillip"},{"family":"Globerson","given":"Amir"},{"family":"Irani","given":"Michal"},{"family":"Mosseri","given":"Inbar"}],"citation-key":"langExplainingStyleTraining2021","DOI":"10.48550/arXiv.2104.13369","issued":{"date-parts":[[2021,9,1]]},"number":"arXiv:2104.13369","publisher":"arXiv","source":"arXiv.org","title":"Explaining in Style: Training a GAN to explain a classifier in StyleSpace","title-short":"Explaining in Style","type":"article","URL":"http://arxiv.org/abs/2104.13369"},
  {"id":"leeMeshSaliency2005","author":[{"family":"Lee","given":"Chang Ha"},{"family":"Varshney","given":"Amitabh"},{"family":"Jacobs","given":"David W"}],"citation-key":"leeMeshSaliency2005","container-title":"ACM SIGGRAPH 2005 Papers","issued":{"date-parts":[[2005]]},"page":"659–666","title":"Mesh saliency","type":"chapter"},
  {"id":"leeSmoothMixSimpleEffective2020","accessed":{"date-parts":[[2023,3,29]]},"author":[{"family":"Lee","given":"Jin-Ha"},{"family":"Zaheer","given":"Muhammad Zaigham"},{"family":"Astrid","given":"Marcella"},{"family":"Lee","given":"Seung-Ik"}],"citation-key":"leeSmoothMixSimpleEffective2020","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops","issued":{"date-parts":[[2020]]},"page":"756-757","source":"openaccess.thecvf.com","title":"SmoothMix: A Simple Yet Effective Data Augmentation to Train Robust Classifiers","title-short":"SmoothMix","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_CVPRW_2020/html/w45/Lee_SmoothMix_A_Simple_Yet_Effective_Data_Augmentation_to_Train_Robust_CVPRW_2020_paper.html"},
  {"id":"leinoInfluenceDirectedExplanationsDeep2018","abstract":"We study the problem of explaining a rich class of behavioral properties of deep neural networks. Distinctively, our influence-directed explanations approach this problem by peering inside the network to identify neurons with high influence on a quantity and distribution of interest, using an axiomatically-justified influence measure, and then providing an interpretation for the concepts these neurons represent. We evaluate our approach by demonstrating a number of its unique capabilities on convolutional neural networks trained on ImageNet. Our evaluation demonstrates that influence-directed explanations (1) identify influential concepts that generalize across instances, (2) can be used to extract the \"essence\" of what the network learned about a class, and (3) isolate individual features the network uses to make decisions and distinguish related classes.","accessed":{"date-parts":[[2022,11,28]]},"author":[{"family":"Leino","given":"Klas"},{"family":"Sen","given":"Shayak"},{"family":"Datta","given":"Anupam"},{"family":"Fredrikson","given":"Matt"},{"family":"Li","given":"Linyi"}],"citation-key":"leinoInfluenceDirectedExplanationsDeep2018","DOI":"10.48550/arXiv.1802.03788","issued":{"date-parts":[[2018,11,13]]},"number":"arXiv:1802.03788","publisher":"arXiv","source":"arXiv.org","title":"Influence-Directed Explanations for Deep Convolutional Networks","type":"article","URL":"http://arxiv.org/abs/1802.03788"},
  {"id":"lethamInterpretableClassifiersUsing2015","accessed":{"date-parts":[[2023,2,20]]},"author":[{"family":"Letham","given":"Benjamin"},{"family":"Rudin","given":"Cynthia"},{"family":"McCormick","given":"Tyler H."},{"family":"Madigan","given":"David"}],"citation-key":"lethamInterpretableClassifiersUsing2015","container-title":"The Annals of Applied Statistics","container-title-short":"Ann. Appl. Stat.","DOI":"10.1214/15-AOAS848","ISSN":"1932-6157","issue":"3","issued":{"date-parts":[[2015,9,1]]},"source":"DOI.org (Crossref)","title":"Interpretable classifiers using rules and Bayesian analysis: Building a better stroke prediction model","title-short":"Interpretable classifiers using rules and Bayesian analysis","type":"article-journal","URL":"https://projecteuclid.org/journals/annals-of-applied-statistics/volume-9/issue-3/Interpretable-classifiers-using-rules-and-Bayesian-analysis--Building-a/10.1214/15-AOAS848.full","volume":"9"},
  {"id":"liAttributeMixSemantic2020","abstract":"Collecting fine-grained labels usually requires expert-level domain knowledge and is prohibitive to scale up. In this paper, we propose Attribute Mix, a data augmentation strategy at attribute level to expand the fine-grained samples. The principle lies in that attribute features are shared among fine-grained sub-categories, and can be seamlessly transferred among images. Toward this goal, we propose an automatic attribute mining approach to discover attributes that belong to the same super-category, and Attribute Mix is operated by mixing semantically meaningful attribute features from two images. Attribute Mix is a simple but effective data augmentation strategy that can significantly improve the recognition performance without increasing the inference budgets. Furthermore, since attributes can be shared among images from the same super-category, we further enrich the training samples with attribute level labels using images from the generic domain. Experiments on widely used fine-grained benchmarks demonstrate the effectiveness of our proposed method.","accessed":{"date-parts":[[2023,2,20]]},"author":[{"family":"Li","given":"Hao"},{"family":"Zhang","given":"Xiaopeng"},{"family":"Xiong","given":"Hongkai"},{"family":"Tian","given":"Qi"}],"citation-key":"liAttributeMixSemantic2020","issued":{"date-parts":[[2020,7,9]]},"number":"arXiv:2004.02684","publisher":"arXiv","source":"arXiv.org","title":"Attribute Mix: Semantic Data Augmentation for Fine Grained Recognition","title-short":"Attribute Mix","type":"article","URL":"http://arxiv.org/abs/2004.02684"},
  {"id":"liBrainStructuresFunctional2015","abstract":"Internet addiction (IA) incurs signiﬁcant social and ﬁnancial costs in the form of physical side-effects, academic and occupational impairment, and serious relationship problems. The majority of previous studies on Internet addiction disorders (IAD) have focused on structural and functional abnormalities, while few studies have simultaneously investigated the structural and functional brain alterations underlying individual differences in IA tendencies measured by questionnaires in a healthy sample. Here we combined structural (regional gray matter volume, rGMV) and functional (resting-state functional connectivity, rsFC) information to explore the neural mechanisms underlying IAT in a large sample of 260 healthy young adults. The results showed that IAT scores were signiﬁcantly and positively correlated with rGMV in the right dorsolateral prefrontal cortex (DLPFC, one key node of the cognitive control network, CCN), which might reﬂect reduced functioning of inhibitory control. More interestingly, decreased anticorrelations between the right DLPFC and the medial prefrontal cortex/rostral anterior cingulate cortex (mPFC/rACC, one key node of the default mode network, DMN) were associated with higher IAT scores, which might be associated with reduced efﬁciency of the CCN and DMN (e.g., diminished cognitive control and self-monitoring). Furthermore, the Stroop interference effect was positively associated with the volume of the DLPFC and with the IA scores, as well as with the connectivity between DLPFC and mPFC, which further indicated that rGMV variations in the DLPFC and decreased anticonnections between the DLPFC and mPFC may reﬂect addiction-related reduced inhibitory control and cognitive efﬁciency. These ﬁndings suggest the combination of structural and functional information can provide a valuable basis for further understanding of the mechanisms and pathogenesis of IA.","accessed":{"date-parts":[[2022,6,17]]},"author":[{"family":"Li","given":"Weiwei"},{"family":"Li","given":"Yadan"},{"family":"Yang","given":"Wenjing"},{"family":"Zhang","given":"Qinglin"},{"family":"Wei","given":"Dongtao"},{"family":"Li","given":"Wenfu"},{"family":"Hitchman","given":"Glenn"},{"family":"Qiu","given":"Jiang"}],"citation-key":"liBrainStructuresFunctional2015","container-title":"Neuropsychologia","container-title-short":"Neuropsychologia","DOI":"10.1016/j.neuropsychologia.2015.02.019","ISSN":"00283932","issued":{"date-parts":[[2015,4]]},"language":"en","page":"134-144","source":"DOI.org (Crossref)","title":"Brain structures and functional connectivity associated with individual differences in Internet tendency in healthy young adults","type":"article-journal","URL":"https://linkinghub.elsevier.com/retrieve/pii/S0028393215000809","volume":"70"},
  {"id":"liCaltech101","author":[{"family":"Li","given":"Fei-Fei"},{"family":"Andreetto","given":""},{"family":"Marco","given":"Marc'Aurelio"},{"family":"Ranzato","given":"Marc'Aurelio"}],"citation-key":"liCaltech101","DOI":"10.22002/D1.20086","title":"Caltech 101","type":"dataset"},
  {"id":"liDeepLIFTDeepLabelSpecific2022","accessed":{"date-parts":[[2023,2,20]]},"author":[{"family":"Li","given":"Junbing"},{"family":"Zhang","given":"Changqing"},{"family":"Zhou","given":"Joey Tianyi"},{"family":"Fu","given":"Huazhu"},{"family":"Xia","given":"Shuyin"},{"family":"Hu","given":"Qinghua"}],"citation-key":"liDeepLIFTDeepLabelSpecific2022","container-title":"IEEE Transactions on Cybernetics","container-title-short":"IEEE Trans. Cybern.","DOI":"10.1109/TCYB.2021.3049630","ISSN":"2168-2267, 2168-2275","issue":"8","issued":{"date-parts":[[2022,8]]},"page":"7732-7741","source":"DOI.org (Crossref)","title":"Deep-LIFT: Deep Label-Specific Feature Learning for Image Annotation","title-short":"Deep-LIFT","type":"article-journal","URL":"https://ieeexplore.ieee.org/document/9352498/","volume":"52"},
  {"id":"liDeepLIFTDeepLabelSpecific2022a","accessed":{"date-parts":[[2023,5,7]]},"author":[{"family":"Li","given":"Junbing"},{"family":"Zhang","given":"Changqing"},{"family":"Zhou","given":"Joey Tianyi"},{"family":"Fu","given":"Huazhu"},{"family":"Xia","given":"Shuyin"},{"family":"Hu","given":"Qinghua"}],"citation-key":"liDeepLIFTDeepLabelSpecific2022a","container-title":"IEEE Transactions on Cybernetics","container-title-short":"IEEE Trans. Cybern.","DOI":"10.1109/TCYB.2021.3049630","ISSN":"2168-2267, 2168-2275","issue":"8","issued":{"date-parts":[[2022,8]]},"page":"7732-7741","source":"DOI.org (Crossref)","title":"Deep-LIFT: Deep Label-Specific Feature Learning for Image Annotation","title-short":"Deep-LIFT","type":"article-journal","URL":"https://ieeexplore.ieee.org/document/9352498/","volume":"52"},
  {"id":"liFpnnFieldProbing2016","author":[{"family":"Li","given":"Yangyan"},{"family":"Pirk","given":"Soeren"},{"family":"Su","given":"Hao"},{"family":"Qi","given":"Charles R"},{"family":"Guibas","given":"Leonidas J"}],"citation-key":"liFpnnFieldProbing2016","container-title":"arXiv preprint arXiv:1605.06240","issued":{"date-parts":[[2016]]},"title":"Fpnn: Field probing neural networks for 3D data","type":"article-journal"},
  {"id":"liHowDoesNeural2021","abstract":"Noisy labels are inevitable in large real-world datasets. In this work, we explore an area understudied by previous works -- how the network's architecture impacts its robustness to noisy labels. We provide a formal framework connecting the robustness of a network to the alignments between its architecture and target/noise functions. Our framework measures a network's robustness via the predictive power in its representations -- the test performance of a linear model trained on the learned representations using a small set of clean labels. We hypothesize that a network is more robust to noisy labels if its architecture is more aligned with the target function than the noise. To support our hypothesis, we provide both theoretical and empirical evidence across various neural network architectures and different domains. We also find that when the network is well-aligned with the target function, its predictive power in representations could improve upon state-of-the-art (SOTA) noisy-label-training methods in terms of test accuracy and even outperform sophisticated methods that use clean labels.","accessed":{"date-parts":[[2022,9,6]]},"author":[{"family":"Li","given":"Jingling"},{"family":"Zhang","given":"Mozhi"},{"family":"Xu","given":"Keyulu"},{"family":"Dickerson","given":"John P."},{"family":"Ba","given":"Jimmy"}],"citation-key":"liHowDoesNeural2021","DOI":"10.48550/arXiv.2012.12896","issued":{"date-parts":[[2021,11,27]]},"number":"arXiv:2012.12896","publisher":"arXiv","source":"arXiv.org","title":"How Does a Neural Network's Architecture Impact Its Robustness to Noisy Labels?","type":"article","URL":"http://arxiv.org/abs/2012.12896"},
  {"id":"liHyperbandNovelBanditbased2017","author":[{"family":"Li","given":"Lisha"},{"family":"Jamieson","given":"Kevin"},{"family":"DeSalvo","given":"Giulia"},{"family":"Rostamizadeh","given":"Afshin"},{"family":"Talwalkar","given":"Ameet"}],"citation-key":"liHyperbandNovelBanditbased2017","container-title":"The Journal of Machine Learning Research","issue":"1","issued":{"date-parts":[[2017]]},"page":"6765–6816","publisher":"JMLR.org","title":"Hyperband: A novel bandit-based approach to hyperparameter optimization","type":"article-journal","volume":"18"},
  {"id":"lillywhiteCoverageEthicsArtificial2021","abstract":"Disabled people are often the anticipated users of scientific and technological products and processes advanced and enabled by artificial intelligence (AI) and machine learning (ML). Disabled people are also impacted by societal impacts of AI/ML. Many ethical issues are identified within AI/ML as fields and within individual applications of AI/ML. At the same time, problems have been identified in how ethics discourses engage with disabled people. The aim of our scoping review was to better understand to what extent and how the AI/ML focused academic literature engaged with the ethics of AI/ML in relation to disabled people. Of the n = 1659 abstracts engaging with AI/ML and ethics downloaded from Scopus (which includes all Medline articles) and the 70 databases of EBSCO ALL, we found 54 relevant abstracts using the term “patient” and 11 relevant abstracts mentioning terms linked to “impair*”, “disab*” and “deaf”. Our study suggests a gap in the literature that should be filled given the many AI/ML related ethical issues identified in the literature and their impact on disabled people.","accessed":{"date-parts":[[2023,1,17]]},"author":[{"family":"Lillywhite","given":"Aspen"},{"family":"Wolbring","given":"Gregor"}],"citation-key":"lillywhiteCoverageEthicsArtificial2021","container-title":"Assistive Technology","container-title-short":"Assistive Technology","DOI":"10.1080/10400435.2019.1593259","ISSN":"1040-0435, 1949-3614","issue":"3","issued":{"date-parts":[[2021,5,4]]},"language":"en","page":"129-135","source":"DOI.org (Crossref)","title":"Coverage of ethics within the artificial intelligence and machine learning academic literature: The case of disabled people","title-short":"Coverage of ethics within the artificial intelligence and machine learning academic literature","type":"article-journal","URL":"https://www.tandfonline.com/doi/full/10.1080/10400435.2019.1593259","volume":"33"},
  {"id":"linDivergenceMeasuresBased","abstract":"A new class of information-theoretic divergence measures based on the Shannon entropy is introduced. Unlike the well-known Kullback divergences, the new measures do not require the condition of absolute continuity to be satisfied by the probability distributions involved. More importantly, their close relationship with the variational distance and the probability of misclassification error are established in terms of bounds. These bounds are crucial in many applications of divergence measures. The new measures are also well characterized by the properties of nonnegativity, finiteness, semiboundedness, and boundedness.","author":[{"family":"Lin","given":"Jianhua"}],"citation-key":"linDivergenceMeasuresBased","language":"en","source":"Zotero","title":"Divergence Measures Based on the Shannon Entropy","type":"article-journal"},
  {"id":"linGeometricViewpointManifold2015","abstract":"In many data analysis tasks, one is often confronted with very high dimensional data. The manifold assumption, which states that the data is sampled from a submanifold embedded in much higher dimensional Euclidean space, has been widely adopted by many researchers. In the last 15 years, a large number of manifold learning algorithms have been proposed. Many of them rely on the evaluation of the geometrical and topological of the data manifold. In this paper, we present a review of these methods on a novel geometric perspective. We categorize these methods by three main groups: Laplacian-based, Hessian-based, and parallel field-based methods. We show the connection and difference between these three groups on their continuous and discrete counterparts. The discussion is focused on the problem of dimensionality reduction and semi-supervised learning.","accessed":{"date-parts":[[2022,11,15]]},"author":[{"family":"Lin","given":"Binbin"},{"family":"He","given":"Xiaofei"},{"family":"Ye","given":"Jieping"}],"citation-key":"linGeometricViewpointManifold2015","container-title":"Applied Informatics","container-title-short":"Appl Inform","DOI":"10.1186/s40535-015-0006-6","ISSN":"2196-0089","issue":"1","issued":{"date-parts":[[2015,12]]},"language":"en","page":"3","source":"DOI.org (Crossref)","title":"A geometric viewpoint of manifold learning","type":"article-journal","URL":"https://applied-informatics-j.springeropen.com/articles/10.1186/s40535-015-0006-6","volume":"2"},
  {"id":"linNetworkNetwork2014","abstract":"We propose a novel deep network structure called \"Network In Network\" (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.","accessed":{"date-parts":[[2022,10,21]]},"author":[{"family":"Lin","given":"Min"},{"family":"Chen","given":"Qiang"},{"family":"Yan","given":"Shuicheng"}],"citation-key":"linNetworkNetwork2014","issued":{"date-parts":[[2014,3,4]]},"number":"arXiv:1312.4400","publisher":"arXiv","source":"arXiv.org","title":"Network In Network","type":"article","URL":"http://arxiv.org/abs/1312.4400","version":"3"},
  {"id":"linNetworkNetwork2014a","abstract":"We propose a novel deep network structure called \"Network In Network\" (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.","accessed":{"date-parts":[[2022,11,18]]},"author":[{"family":"Lin","given":"Min"},{"family":"Chen","given":"Qiang"},{"family":"Yan","given":"Shuicheng"}],"citation-key":"linNetworkNetwork2014a","DOI":"10.48550/arXiv.1312.4400","issued":{"date-parts":[[2014,3,4]]},"number":"arXiv:1312.4400","publisher":"arXiv","source":"arXiv.org","title":"Network In Network","type":"article","URL":"http://arxiv.org/abs/1312.4400"},
  {"id":"liuConvNet2020s2022","abstract":"The \"Roaring 20s\" of visual recognition began with the introduction of Vision Transformers (ViTs), which quickly superseded ConvNets as the state-of-the-art image classification model. A vanilla ViT, on the other hand, faces difficulties when applied to general computer vision tasks such as object detection and semantic segmentation. It is the hierarchical Transformers (e.g., Swin Transformers) that reintroduced several ConvNet priors, making Transformers practically viable as a generic vision backbone and demonstrating remarkable performance on a wide variety of vision tasks. However, the effectiveness of such hybrid approaches is still largely credited to the intrinsic superiority of Transformers, rather than the inherent inductive biases of convolutions. In this work, we reexamine the design spaces and test the limits of what a pure ConvNet can achieve. We gradually \"modernize\" a standard ResNet toward the design of a vision Transformer, and discover several key components that contribute to the performance difference along the way. The outcome of this exploration is a family of pure ConvNet models dubbed ConvNeXt. Constructed entirely from standard ConvNet modules, ConvNeXts compete favorably with Transformers in terms of accuracy and scalability, achieving 87.8% ImageNet top-1 accuracy and outperforming Swin Transformers on COCO detection and ADE20K segmentation, while maintaining the simplicity and efficiency of standard ConvNets.","accessed":{"date-parts":[[2022,5,24]]},"author":[{"family":"Liu","given":"Zhuang"},{"family":"Mao","given":"Hanzi"},{"family":"Wu","given":"Chao-Yuan"},{"family":"Feichtenhofer","given":"Christoph"},{"family":"Darrell","given":"Trevor"},{"family":"Xie","given":"Saining"}],"citation-key":"liuConvNet2020s2022","DOI":"10.48550/arXiv.2201.03545","issued":{"date-parts":[[2022,3,2]]},"number":"arXiv:2201.03545","publisher":"arXiv","source":"arXiv.org","title":"A ConvNet for the 2020s","type":"article","URL":"http://arxiv.org/abs/2201.03545"},
  {"id":"liuConvNet2020s2022a","abstract":"The \"Roaring 20s\" of visual recognition began with the introduction of Vision Transformers (ViTs), which quickly superseded ConvNets as the state-of-the-art image classification model. A vanilla ViT, on the other hand, faces difficulties when applied to general computer vision tasks such as object detection and semantic segmentation. It is the hierarchical Transformers (e.g., Swin Transformers) that reintroduced several ConvNet priors, making Transformers practically viable as a generic vision backbone and demonstrating remarkable performance on a wide variety of vision tasks. However, the effectiveness of such hybrid approaches is still largely credited to the intrinsic superiority of Transformers, rather than the inherent inductive biases of convolutions. In this work, we reexamine the design spaces and test the limits of what a pure ConvNet can achieve. We gradually \"modernize\" a standard ResNet toward the design of a vision Transformer, and discover several key components that contribute to the performance difference along the way. The outcome of this exploration is a family of pure ConvNet models dubbed ConvNeXt. Constructed entirely from standard ConvNet modules, ConvNeXts compete favorably with Transformers in terms of accuracy and scalability, achieving 87.8% ImageNet top-1 accuracy and outperforming Swin Transformers on COCO detection and ADE20K segmentation, while maintaining the simplicity and efficiency of standard ConvNets.","accessed":{"date-parts":[[2022,5,24]]},"author":[{"family":"Liu","given":"Zhuang"},{"family":"Mao","given":"Hanzi"},{"family":"Wu","given":"Chao-Yuan"},{"family":"Feichtenhofer","given":"Christoph"},{"family":"Darrell","given":"Trevor"},{"family":"Xie","given":"Saining"}],"citation-key":"liuConvNet2020s2022a","issued":{"date-parts":[[2022,3,2]]},"number":"arXiv:2201.03545","publisher":"arXiv","source":"arXiv.org","title":"A ConvNet for the 2020s","type":"article","URL":"http://arxiv.org/abs/2201.03545"},
  {"id":"liuDataAugmentationLatent2018","abstract":"Effective training of the deep neural networks requires much data to avoid underdetermined and poor generalization. Data Augmentation alleviates this by using existing data more effectively. However standard data augmentation produces only limited plausible alternative data by for example, flipping, distorting, adding noise to, cropping a patch from the original samples. In this paper, we introduce the adversarial autoencoder (AAE) to impose the feature representations with uniform distribution and apply the linear interpolation on latent space, which is potential to generate a much broader set of augmentations for image classification. As a possible “recognition via generation” framework, it has potentials for several other classification tasks. Our experiments on the ILSVRC 2012, CIFAR-10 datasets show that the latent space interpolation (LSI) improves the generalization and performance of state-of-the-art deep neural networks.","accessed":{"date-parts":[[2023,2,1]]},"author":[{"family":"Liu","given":"Xiaofeng"},{"family":"Zou","given":"Yang"},{"family":"Kong","given":"Lingsheng"},{"family":"Diao","given":"Zhihui"},{"family":"Yan","given":"Junliang"},{"family":"Wang","given":"Jun"},{"family":"Li","given":"Site"},{"family":"Jia","given":"Ping"},{"family":"You","given":"Jane"}],"citation-key":"liuDataAugmentationLatent2018","container-title":"2018 24th International Conference on Pattern Recognition (ICPR)","DOI":"10.1109/ICPR.2018.8545506","event-place":"Beijing","event-title":"2018 24th International Conference on Pattern Recognition (ICPR)","ISBN":"978-1-5386-3788-3","issued":{"date-parts":[[2018,8]]},"language":"en","page":"728-733","publisher":"IEEE","publisher-place":"Beijing","source":"DOI.org (Crossref)","title":"Data Augmentation via Latent Space Interpolation for Image Classification","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/8545506/"},
  {"id":"liuDensepointLearningDensely2019","author":[{"family":"Liu","given":"Yongcheng"},{"family":"Fan","given":"Bin"},{"family":"Meng","given":"Gaofeng"},{"family":"Lu","given":"Jiwen"},{"family":"Xiang","given":"Shiming"},{"family":"Pan","given":"Chunhong"}],"citation-key":"liuDensepointLearningDensely2019","container-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[[2019]]},"page":"5239–5248","title":"Densepoint: Learning densely contextual representation for efficient point cloud processing","type":"paper-conference"},
  {"id":"liuMoreConvNets2020s2022","abstract":"Transformers have quickly shined in the computer vision world since the emergence of Vision Transformers (ViTs). The dominant role of convolutional neural networks (CNNs) seems to be challenged by increasingly effective transformer-based models. Very recently, a couple of advanced convolutional models strike back with large kernels motivated by the local-window attention mechanism, showing appealing performance and efficiency. While one of them, i.e. RepLKNet, impressively manages to scale the kernel size to 31x31 with improved performance, the performance starts to saturate as the kernel size continues growing, compared to the scaling trend of advanced ViTs such as Swin Transformer. In this paper, we explore the possibility of training extreme convolutions larger than 31x31 and test whether the performance gap can be eliminated by strategically enlarging convolutions. This study ends up with a recipe for applying extremely large kernels from the perspective of sparsity, which can smoothly scale up kernels to 61x61 with better performance. Built on this recipe, we propose Sparse Large Kernel Network (SLaK), a pure CNN architecture equipped with sparse factorized 51x51 kernels that can perform on par with or better than state-of-the-art hierarchical Transformers and modern ConvNet architectures like ConvNeXt and RepLKNet, on ImageNet classification as well as a wide range of downstream tasks including semantic segmentation on ADE20K, object detection on PASCAL VOC 2007, and object detection/segmentation on MS COCO.","accessed":{"date-parts":[[2022,10,23]]},"author":[{"family":"Liu","given":"Shiwei"},{"family":"Chen","given":"Tianlong"},{"family":"Chen","given":"Xiaohan"},{"family":"Chen","given":"Xuxi"},{"family":"Xiao","given":"Qiao"},{"family":"Wu","given":"Boqian"},{"family":"Pechenizkiy","given":"Mykola"},{"family":"Mocanu","given":"Decebal"},{"family":"Wang","given":"Zhangyang"}],"citation-key":"liuMoreConvNets2020s2022","DOI":"10.48550/arXiv.2207.03620","issued":{"date-parts":[[2022,9,30]]},"number":"arXiv:2207.03620","publisher":"arXiv","source":"arXiv.org","title":"More ConvNets in the 2020s: Scaling up Kernels Beyond 51x51 using Sparsity","title-short":"More ConvNets in the 2020s","type":"article","URL":"http://arxiv.org/abs/2207.03620"},
  {"id":"liuRelationShapeConvolutionalNeural2019","abstract":"Point cloud analysis is very challenging, as the shape implied in irregular points is difficult to capture. In this paper, we propose RS-CNN, namely, Relation-Shape Convolutional Neural Network, which extends regular grid CNN to irregular configuration for point cloud analysis. The key to RS-CNN is learning from relation, i.e., the geometric topology constraint among points. Specifically, the convolutional weight for local point set is forced to learn a high-level relation expression from predefined geometric priors, between a sampled point from this point set and the others. In this way, an inductive local representation with explicit reasoning about the spatial layout of points can be obtained, which leads to much shape awareness and robustness. With this convolution as a basic operator, RS-CNN, a hierarchical architecture can be developed to achieve contextual shape-aware learning for point cloud analysis. Extensive experiments on challenging benchmarks across three tasks verify RS-CNN achieves the state of the arts.","accessed":{"date-parts":[[2022,5,10]]},"author":[{"family":"Liu","given":"Yongcheng"},{"family":"Fan","given":"Bin"},{"family":"Xiang","given":"Shiming"},{"family":"Pan","given":"Chunhong"}],"citation-key":"liuRelationShapeConvolutionalNeural2019","container-title":"arXiv:1904.07601 [cs]","issued":{"date-parts":[[2019,5,25]]},"source":"arXiv.org","title":"Relation-Shape Convolutional Neural Network for Point Cloud Analysis","type":"article-journal","URL":"http://arxiv.org/abs/1904.07601"},
  {"id":"liuRelationShapeConvolutionalNeural2019a","author":[{"family":"Liu","given":"Yongcheng"},{"family":"Fan","given":"Bin"},{"family":"Xiang","given":"Shiming"},{"family":"Pan","given":"Chunhong"}],"citation-key":"liuRelationShapeConvolutionalNeural2019a","issued":{"date-parts":[[2019]]},"title":"Relation-Shape Convolutional Neural Network for Point Cloud Analysis","type":"document"},
  {"id":"liuSwinTransformerHierarchical2021","abstract":"This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with \\textbf{S}hifted \\textbf{win}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at~\\url{https://github.com/microsoft/Swin-Transformer}.","accessed":{"date-parts":[[2022,5,24]]},"author":[{"family":"Liu","given":"Ze"},{"family":"Lin","given":"Yutong"},{"family":"Cao","given":"Yue"},{"family":"Hu","given":"Han"},{"family":"Wei","given":"Yixuan"},{"family":"Zhang","given":"Zheng"},{"family":"Lin","given":"Stephen"},{"family":"Guo","given":"Baining"}],"citation-key":"liuSwinTransformerHierarchical2021","DOI":"10.48550/arXiv.2103.14030","issued":{"date-parts":[[2021,8,17]]},"number":"arXiv:2103.14030","publisher":"arXiv","source":"arXiv.org","title":"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows","title-short":"Swin Transformer","type":"article","URL":"http://arxiv.org/abs/2103.14030"},
  {"id":"liuSwinTransformerV22022","abstract":"Large-scale NLP models have been shown to significantly improve the performance on language tasks with no signs of saturation. They also demonstrate amazing few-shot capabilities like that of human beings. This paper aims to explore large-scale models in computer vision. We tackle three major issues in training and application of large vision models, including training instability, resolution gaps between pre-training and fine-tuning, and hunger on labelled data. Three main techniques are proposed: 1) a residual-post-norm method combined with cosine attention to improve training stability; 2) A log-spaced continuous position bias method to effectively transfer models pre-trained using low-resolution images to downstream tasks with high-resolution inputs; 3) A self-supervised pre-training method, SimMIM, to reduce the needs of vast labeled images. Through these techniques, this paper successfully trained a 3 billion-parameter Swin Transformer V2 model, which is the largest dense vision model to date, and makes it capable of training with images of up to 1,536$\\times$1,536 resolution. It set new performance records on 4 representative vision tasks, including ImageNet-V2 image classification, COCO object detection, ADE20K semantic segmentation, and Kinetics-400 video action classification. Also note our training is much more efficient than that in Google's billion-level visual models, which consumes 40 times less labelled data and 40 times less training time. Code is available at \\url{https://github.com/microsoft/Swin-Transformer}.","accessed":{"date-parts":[[2022,10,21]]},"author":[{"family":"Liu","given":"Ze"},{"family":"Hu","given":"Han"},{"family":"Lin","given":"Yutong"},{"family":"Yao","given":"Zhuliang"},{"family":"Xie","given":"Zhenda"},{"family":"Wei","given":"Yixuan"},{"family":"Ning","given":"Jia"},{"family":"Cao","given":"Yue"},{"family":"Zhang","given":"Zheng"},{"family":"Dong","given":"Li"},{"family":"Wei","given":"Furu"},{"family":"Guo","given":"Baining"}],"citation-key":"liuSwinTransformerV22022","issued":{"date-parts":[[2022,4,11]]},"number":"arXiv:2111.09883","publisher":"arXiv","source":"arXiv.org","title":"Swin Transformer V2: Scaling Up Capacity and Resolution","title-short":"Swin Transformer V2","type":"article","URL":"http://arxiv.org/abs/2111.09883","version":"2"},
  {"id":"liVisualizingLossLandscape","abstract":"Neural network training relies on our ability to ﬁnd “good” minimizers of highly non-convex loss functions. It is well-known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and wellchosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effect on the underlying loss landscape, are not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple “ﬁlter normalization” method that helps us visualize loss function curvature and make meaningful side-by-side comparisons between loss functions. Then, using a variety of visualizations, we explore how network architecture affects the loss landscape, and how training parameters affect the shape of minimizers.","author":[{"family":"Li","given":"Hao"},{"family":"Xu","given":"Zheng"},{"family":"Taylor","given":"Gavin"},{"family":"Studer","given":"Christoph"},{"family":"Goldstein","given":"Tom"}],"citation-key":"liVisualizingLossLandscape","language":"en","source":"Zotero","title":"Visualizing the Loss Landscape of Neural Nets","type":"article-journal"},
  {"id":"liVisualizingLossLandscape2018","abstract":"Neural network training relies on our ability to find \"good\" minimizers of highly non-convex loss functions. It is well known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effect on the underlying loss landscape, is not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple \"filter normalization\" method that helps us visualize loss function curvature, and make meaningful side-by-side comparisons between loss functions. Then, using a variety of visualizations, we explore how network architecture affects the loss landscape, and how training parameters affect the shape of minimizers.","accessed":{"date-parts":[[2023,3,27]]},"author":[{"family":"Li","given":"Hao"},{"family":"Xu","given":"Zheng"},{"family":"Taylor","given":"Gavin"},{"family":"Studer","given":"Christoph"},{"family":"Goldstein","given":"Tom"}],"citation-key":"liVisualizingLossLandscape2018","container-title":"Advances in Neural Information Processing Systems","issued":{"date-parts":[[2018]]},"publisher":"Curran Associates, Inc.","source":"Neural Information Processing Systems","title":"Visualizing the Loss Landscape of Neural Nets","type":"paper-conference","URL":"https://proceedings.neurips.cc/paper/2018/hash/a41b3bb3e6b050b6c9067c67f663b915-Abstract.html","volume":"31"},
  {"id":"loshchilovDecoupledWeightDecay2019","abstract":"L$_2$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is \\emph{not} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L$_2$ regularization (often calling it \"weight decay\" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by \\emph{decoupling} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW","accessed":{"date-parts":[[2023,5,9]]},"author":[{"family":"Loshchilov","given":"Ilya"},{"family":"Hutter","given":"Frank"}],"citation-key":"loshchilovDecoupledWeightDecay2019","DOI":"10.48550/arXiv.1711.05101","issued":{"date-parts":[[2019,1,4]]},"number":"arXiv:1711.05101","publisher":"arXiv","source":"arXiv.org","title":"Decoupled Weight Decay Regularization","type":"article","URL":"http://arxiv.org/abs/1711.05101"},
  {"id":"losWarningStimulusRetrieval2021","abstract":"In a warned reaction time task, the warning stimulus (S1) initiates a process of temporal preparation, which promotes a speeded response to the impending target stimulus (S2). According to the multiple trace theory of temporal preparation (MTP), participants learn the timing of S2 by storing a memory trace on each trial, which contains a temporal profile of the events on that trial. On each new trial, S1 serves as a retrieval cue that implicitly and associatively activates memory traces created on earlier trials, which jointly drive temporal preparation for S2. The idea that S1 assumes this role as a retrieval cue was tested across eight experiments, in which two different S1s were associated with two different distributions of S1-S2 intervals: one with predominantly short and one with predominantly long intervals. Experiments differed regarding the S1 features that made up a pair, ranging from highly distinct (e.g., tone and flash) to more similar (e.g., red and green flash) and verbal (i.e., “short” vs “long”). Exclusively for pairs of highly distinct S1s, the results showed that the S1 cue modified temporal preparation, even in participants who showed no awareness of the contingency. This cueing effect persisted in a subsequent transfer phase, in which the contingency between S1 and the timing of S2 was broken – a fact participants were informed of in advance. Together, these findings support the role of S1 as an implicit retrieval cue, consistent with MTP.","accessed":{"date-parts":[[2022,11,28]]},"author":[{"family":"Los","given":"Sander A."},{"family":"Nieuwenstein","given":"Jurre"},{"family":"Bouharab","given":"Anass"},{"family":"Stephens","given":"David J."},{"family":"Meeter","given":"Martijn"},{"family":"Kruijne","given":"Wouter"}],"citation-key":"losWarningStimulusRetrieval2021","container-title":"Cognitive Psychology","container-title-short":"Cognitive Psychology","DOI":"10.1016/j.cogpsych.2021.101378","ISSN":"0010-0285","issued":{"date-parts":[[2021,3,1]]},"language":"en","page":"101378","source":"ScienceDirect","title":"The warning stimulus as retrieval cue: The role of associative memory in temporal preparation","title-short":"The warning stimulus as retrieval cue","type":"article-journal","URL":"https://www.sciencedirect.com/science/article/pii/S0010028521000013","volume":"125"},
  {"id":"loweObjectRecognitionLocal1999","author":[{"family":"Lowe","given":"David G"}],"citation-key":"loweObjectRecognitionLocal1999","container-title":"Proceedings of the seventh IEEE international conference on computer vision","issued":{"date-parts":[[1999]]},"page":"1150–1157","publisher":"Ieee","title":"Object recognition from local scale-invariant features","type":"paper-conference","volume":"2"},
  {"id":"lundbergUnifiedApproachInterpreting2017","abstract":"Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.","accessed":{"date-parts":[[2022,11,28]]},"author":[{"family":"Lundberg","given":"Scott M"},{"family":"Lee","given":"Su-In"}],"citation-key":"lundbergUnifiedApproachInterpreting2017","container-title":"Advances in Neural Information Processing Systems","issued":{"date-parts":[[2017]]},"publisher":"Curran Associates, Inc.","source":"Neural Information Processing Systems","title":"A Unified Approach to Interpreting Model Predictions","type":"paper-conference","URL":"https://proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html","volume":"30"},
  {"id":"luPretrainedTransformersUniversal2021","abstract":"We investigate the capability of a transformer pretrained on natural language to generalize to other modalities with minimal ﬁnetuning – in particular, without ﬁnetuning of the self-attention and feedforward layers of the residual blocks. We consider such a model, which we call a Frozen Pretrained Transformer (FPT), and study ﬁnetuning it on a variety of sequence classiﬁcation tasks spanning numerical computation, vision, and protein fold prediction. In contrast to prior works which investigate ﬁnetuning on the same modality as the pretraining dataset, we show that pretraining on natural language can improve performance and compute efﬁciency on non-language downstream tasks. Additionally, we perform an analysis of the architecture, comparing the performance of a random initialized transformer to a random LSTM. Combining the two insights, we ﬁnd language-pretrained transformers can obtain strong performance on a variety of non-language tasks1.","accessed":{"date-parts":[[2022,5,24]]},"author":[{"family":"Lu","given":"Kevin"},{"family":"Grover","given":"Aditya"},{"family":"Abbeel","given":"Pieter"},{"family":"Mordatch","given":"Igor"}],"citation-key":"luPretrainedTransformersUniversal2021","issued":{"date-parts":[[2021,6,30]]},"language":"en","number":"arXiv:2103.05247","publisher":"arXiv","source":"arXiv.org","title":"Pretrained Transformers as Universal Computation Engines","type":"article","URL":"http://arxiv.org/abs/2103.05247"},
  {"id":"maci�tRESEARCHSOCIALMEDIA2018","abstract":"Human relationships in societies consisted of face-to-face relationships until the middle of the 20th century. Throughout their lives, people have established social relationships with a limited number of people, sharing their sadness and happiness with them. The great technological developments coming up to date and began with the invention of the transistor in the mid-20th century allowed the technological communication tools to enter through our pockets and caused great changes in the way of communication of the societies. The virtual chat culture, which started with the introduction of the Internet into the houses, has reached the dimension of media sharing with the spread of mobile devices. Everyone in society has got the chance to become mediatic and famous, and many of them have started to make an effort for it. However, statistical and medical researches have made in recent years that this attractive media is addictive. Research has been conducted on changes in chemical movements and physiological behavior of individuals’ brain and nervous system. In this study, statistical information of the year 2018 on the use of internet and social media in the world are presented. In addition, the concept of addiction, addicted behavior and symptoms were examined. Research made about substances which cause biological and psychological addiction like alcohol, cigarettes and pills and chemicals which affect on behaviour of individuals such as dopamine. Symptoms of mania, depression and bipolar disorder were examined, and similarities between behaviors of individuals using social media for a long time and individuals who are addicted and sick were examined.","accessed":{"date-parts":[[2022,6,15]]},"author":[{"family":"Maci̇t","given":"Hüseyin Bilal"},{"family":"Maci̇t","given":"Gamze"},{"family":"Güngör","given":"Orhan"}],"citation-key":"maci�tRESEARCHSOCIALMEDIA2018","container-title":"Mehmet Akif Ersoy Üniversitesi İktisadi ve İdari Bilimler Fakültesi Dergisi","DOI":"10.30798/makuiibf.435845","ISSN":"2149-1658","issue":"3","issued":{"date-parts":[[2018,12,27]]},"language":"en","number":"3","page":"882-897","source":"dergipark.org.tr","title":"A RESEARCH ON SOCIAL MEDIA ADDICTION AND DOPAMINE DRIVEN FEEDBACK","type":"article-journal","URL":"https://dergipark.org.tr/en/pub/makuiibf/issue/41626/435845","volume":"5"},
  {"id":"maLearningMultiViewRepresentation2019","abstract":"Shape representation for 3-D models is an important topic in computer vision, multimedia analysis, and computer graphics. Recent multiview-based methods demonstrate promising performance for 3-D shape recognition and retrieval. However, most multiview-based methods ignore the correlations of multiple views or suffer from high computional cost. In this paper, we propose a novel multiview-based network architecture for 3-D shape recognition and retrieval. Our network combines convolutional neural networks (CNNs) with long short-term memory (LSTM) to exploit the correlative information from multiple views. Well-pretrained CNNs with residual connections are ﬁrst used to extract a low-level feature of each view image rendered from a 3-D shape. Then, a LSTM and a sequence voting layer are employed to aggregate these features into a shape descriptor. The highway network and a three-step training strategy are also adopted to boost the optimization of the deep network. Experimental results on two public datasets demonstrate that the proposed method achieves promising performance for 3-D shape recognition and the state-of-the-art performance for the 3-D shape retrieval.","accessed":{"date-parts":[[2022,5,10]]},"author":[{"family":"Ma","given":"Chao"},{"family":"Guo","given":"Yulan"},{"family":"Yang","given":"Jungang"},{"family":"An","given":"Wei"}],"citation-key":"maLearningMultiViewRepresentation2019","container-title":"IEEE Transactions on Multimedia","container-title-short":"IEEE Trans. Multimedia","DOI":"10.1109/TMM.2018.2875512","ISSN":"1520-9210, 1941-0077","issue":"5","issued":{"date-parts":[[2019,5]]},"language":"en","page":"1169-1182","source":"DOI.org (Crossref)","title":"Learning Multi-View Representation With LSTM for 3-D Shape Recognition and Retrieval","type":"article-journal","URL":"https://ieeexplore.ieee.org/document/8490588/","volume":"21"},
  {"id":"ManifoldHypothesis2022","abstract":"When are gradient-based explanations meaningful? We propose a necessary criterion: explanations need to be aligned with the tangent space of the data manifold. To test this hypothesis, we employ autoencoders to estimate and generate data manifolds. Across a range of different datasets – MNIST, EMNIST, CIFAR10, X-ray pneumonia and Diabetic Retinopathy detection – we demonstrate empirically that the more an explanation is aligned with the tangent space of the data, the more interpretable it tends to be. In particular, popular post-hoc explanation methods such as Integrated Gradients and SmoothGrad tend to align their results with the data manifold. The same is true for the outcome of adversarial training, which has been claimed to lead to more interpretable explanations. Empirically, alignment with the data manifold happens early during training, and to some degree even when training with random labels. However, we theoretically prove that good generalization of neural networks does not imply good or bad alignment of model gradients with the data manifold. This leads to a number of interesting follow-up questions regarding gradient-based explanations.","citation-key":"ManifoldHypothesis2022","issued":{"date-parts":[[2022]]},"language":"en","page":"21","source":"Zotero","title":"Manifold Hypothesis","type":"article-journal"},
  {"id":"marcinkevicsInterpretabilityExplainabilityMachine2023","abstract":"In this review, we examine the problem of designing interpretable and explainable machine learning models. Interpretability and explainability lie at the core of many machine learning and statistical applications in medicine, economics, law, and natural sciences. Although interpretability and explainability have escaped a clear universal definition, many techniques motivated by these properties have been developed over the recent 30 years with the focus currently shifting towards deep learning methods. In this review, we emphasise the divide between interpretability and explainability and illustrate these two different research directions with concrete examples of the state-of-the-art. The review is intended for a general machine learning audience with interest in exploring the problems of interpretation and explanation beyond logistic regression or random forest variable importance. This work is not an exhaustive literature survey, but rather a primer focusing selectively on certain lines of research which the authors found interesting or informative.","accessed":{"date-parts":[[2023,3,9]]},"author":[{"family":"Marcinkevičs","given":"Ričards"},{"family":"Vogt","given":"Julia E."}],"citation-key":"marcinkevicsInterpretabilityExplainabilityMachine2023","issued":{"date-parts":[[2023,3,1]]},"number":"arXiv:2012.01805","publisher":"arXiv","source":"arXiv.org","title":"Interpretability and Explainability: A Machine Learning Zoo Mini-tour","title-short":"Interpretability and Explainability","type":"article","URL":"http://arxiv.org/abs/2012.01805"},
  {"id":"maturanaVoxNet3DConvolutional2015","author":[{"family":"Maturana","given":"Daniel"},{"family":"Scherer","given":"Sebastian"}],"citation-key":"maturanaVoxNet3DConvolutional2015","container-title":"2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","issued":{"date-parts":[[2015]]},"publisher":"IEEE","title":"VoxNet: A 3D convolutional neural network for real-time object recognition","type":"paper-conference"},
  {"id":"mehrabiSurveyBiasFairness2021","abstract":"With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.","accessed":{"date-parts":[[2022,9,6]]},"author":[{"family":"Mehrabi","given":"Ninareh"},{"family":"Morstatter","given":"Fred"},{"family":"Saxena","given":"Nripsuta"},{"family":"Lerman","given":"Kristina"},{"family":"Galstyan","given":"Aram"}],"citation-key":"mehrabiSurveyBiasFairness2021","container-title":"ACM Computing Surveys","container-title-short":"ACM Comput. Surv.","DOI":"10.1145/3457607","ISSN":"0360-0300","issue":"6","issued":{"date-parts":[[2021,7,13]]},"page":"115:1–115:35","source":"July 2022","title":"A Survey on Bias and Fairness in Machine Learning","type":"article-journal","URL":"https://doi.org/10.1145/3457607","volume":"54"},
  {"id":"melas-kyriaziMathematicalFoundationsManifold2020","abstract":"Manifold learning is a popular and quickly-growing subfield of machine learning based on the assumption that one's observed data lie on a low-dimensional manifold embedded in a higher-dimensional space. This thesis presents a mathematical perspective on manifold learning, delving into the intersection of kernel learning, spectral graph theory, and differential geometry. Emphasis is placed on the remarkable interplay between graphs and manifolds, which forms the foundation for the widely-used technique of manifold regularization. This work is written to be accessible to a broad mathematical audience, including machine learning researchers and practitioners interested in understanding the theorems underlying popular manifold learning algorithms and dimensionality reduction techniques.","accessed":{"date-parts":[[2022,11,15]]},"author":[{"family":"Melas-Kyriazi","given":"Luke"}],"citation-key":"melas-kyriaziMathematicalFoundationsManifold2020","issued":{"date-parts":[[2020,10,30]]},"language":"en","number":"arXiv:2011.01307","publisher":"arXiv","source":"arXiv.org","title":"The Mathematical Foundations of Manifold Learning","type":"article","URL":"http://arxiv.org/abs/2011.01307"},
  {"id":"micikeviciusMixedPrecisionTraining2017","author":[{"family":"Micikevicius","given":"Paulius"},{"family":"Narang","given":"Sharan"},{"family":"Alben","given":"Jonah"},{"family":"Diamos","given":"Gregory"},{"family":"Elsen","given":"Erich"},{"family":"Garcia","given":"David"},{"family":"Ginsburg","given":"Boris"},{"family":"Houston","given":"Michael"},{"family":"Kuchaiev","given":"Oleksii"},{"family":"Venkatesh","given":"Ganesh"},{"literal":"others"}],"citation-key":"micikeviciusMixedPrecisionTraining2017","container-title":"arXiv preprint arXiv:1710.03740","issued":{"date-parts":[[2017]]},"title":"Mixed precision training","type":"article-journal"},
  {"id":"micikeviciusMixedPrecisionTraining2018","abstract":"Deep neural networks have enabled progress in a wide variety of applications. Growing the size of the neural network typically results in improved accuracy. As model sizes grow, the memory and compute requirements for training these models also increases. We introduce a technique to train deep neural networks using half precision floating point numbers. In our technique, weights, activations and gradients are stored in IEEE half-precision format. Half-precision floating numbers have limited numerical range compared to single-precision numbers. We propose two techniques to handle this loss of information. Firstly, we recommend maintaining a single-precision copy of the weights that accumulates the gradients after each optimizer step. This single-precision copy is rounded to half-precision format during training. Secondly, we propose scaling the loss appropriately to handle the loss of information with half-precision gradients. We demonstrate that this approach works for a wide variety of models including convolution neural networks, recurrent neural networks and generative adversarial networks. This technique works for large scale models with more than 100 million parameters trained on large datasets. Using this approach, we can reduce the memory consumption of deep learning models by nearly 2x. In future processors, we can also expect a significant computation speedup using half-precision hardware units.","accessed":{"date-parts":[[2023,4,11]]},"author":[{"family":"Micikevicius","given":"Paulius"},{"family":"Narang","given":"Sharan"},{"family":"Alben","given":"Jonah"},{"family":"Diamos","given":"Gregory"},{"family":"Elsen","given":"Erich"},{"family":"Garcia","given":"David"},{"family":"Ginsburg","given":"Boris"},{"family":"Houston","given":"Michael"},{"family":"Kuchaiev","given":"Oleksii"},{"family":"Venkatesh","given":"Ganesh"},{"family":"Wu","given":"Hao"}],"citation-key":"micikeviciusMixedPrecisionTraining2018","issued":{"date-parts":[[2018,2,15]]},"number":"arXiv:1710.03740","publisher":"arXiv","source":"arXiv.org","title":"Mixed Precision Training","type":"article","URL":"http://arxiv.org/abs/1710.03740"},
  {"id":"millerExplainableAIBeware2017","abstract":"In his seminal book `The Inmates are Running the Asylum: Why High-Tech Products Drive Us Crazy And How To Restore The Sanity' [2004, Sams Indianapolis, IN, USA], Alan Cooper argues that a major reason why software is often poorly designed (from a user perspective) is that programmers are in charge of design decisions, rather than interaction designers. As a result, programmers design software for themselves, rather than for their target audience, a phenomenon he refers to as the `inmates running the asylum'. This paper argues that explainable AI risks a similar fate. While the re-emergence of explainable AI is positive, this paper argues most of us as AI researchers are building explanatory agents for ourselves, rather than for the intended users. But explainable AI is more likely to succeed if researchers and practitioners understand, adopt, implement, and improve models from the vast and valuable bodies of research in philosophy, psychology, and cognitive science, and if evaluation of these models is focused more on people than on technology. From a light scan of literature, we demonstrate that there is considerable scope to infuse more results from the social and behavioural sciences into explainable AI, and present some key results from these fields that are relevant to explainable AI.","accessed":{"date-parts":[[2022,11,24]]},"author":[{"family":"Miller","given":"Tim"},{"family":"Howe","given":"Piers"},{"family":"Sonenberg","given":"Liz"}],"citation-key":"millerExplainableAIBeware2017","DOI":"10.48550/arXiv.1712.00547","issued":{"date-parts":[[2017,12,4]]},"number":"arXiv:1712.00547","publisher":"arXiv","source":"arXiv.org","title":"Explainable AI: Beware of Inmates Running the Asylum Or: How I Learnt to Stop Worrying and Love the Social and Behavioural Sciences","title-short":"Explainable AI","type":"article","URL":"http://arxiv.org/abs/1712.00547"},
  {"id":"mitsuharaEmbeddingHumanKnowledge2019","abstract":"In this work, we aim to realize a method for embedding human knowledge into deep neural networks. While the conventional method to embed human knowledge has been applied for non-deep machine learning, it is challenging to apply it for deep learning models due to the enormous number of model parameters. To tackle this problem, we focus on the attention mechanism of an attention branch network (ABN). In this paper, we propose a fine-tuning method that utilizes a single-channel attention map which is manually edited by a human expert. Our fine-tuning method can train a network so that the output attention map corresponds to the edited ones. As a result, the fine-tuned network can output an attention map that takes into account human knowledge. Experimental results with ImageNet, CUB-200-2010, and IDRiD demonstrate that it is possible to obtain a clear attention map for a visual explanation and improve the classification performance. Our findings can be a novel framework for optimizing networks through human intuitive editing via a visual interface and suggest new possibilities for human-machine cooperation in addition to the improvement of visual explanations.","accessed":{"date-parts":[[2022,10,3]]},"author":[{"family":"Mitsuhara","given":"Masahiro"},{"family":"Fukui","given":"Hiroshi"},{"family":"Sakashita","given":"Yusuke"},{"family":"Ogata","given":"Takanori"},{"family":"Hirakawa","given":"Tsubasa"},{"family":"Yamashita","given":"Takayoshi"},{"family":"Fujiyoshi","given":"Hironobu"}],"citation-key":"mitsuharaEmbeddingHumanKnowledge2019","issued":{"date-parts":[[2019,12,19]]},"number":"arXiv:1905.03540","publisher":"arXiv","source":"arXiv.org","title":"Embedding Human Knowledge into Deep Neural Network via Attention Map","type":"article","URL":"http://arxiv.org/abs/1905.03540"},
  {"id":"mitsuharaEmbeddingHumanKnowledge2019a","abstract":"In this work, we aim to realize a method for embedding human knowledge into deep neural networks. While the conventional method to embed human knowledge has been applied for non-deep machine learning, it is challenging to apply it for deep learning models due to the enormous number of model parameters. To tackle this problem, we focus on the attention mechanism of an attention branch network (ABN). In this paper, we propose a fine-tuning method that utilizes a single-channel attention map which is manually edited by a human expert. Our fine-tuning method can train a network so that the output attention map corresponds to the edited ones. As a result, the fine-tuned network can output an attention map that takes into account human knowledge. Experimental results with ImageNet, CUB-200-2010, and IDRiD demonstrate that it is possible to obtain a clear attention map for a visual explanation and improve the classification performance. Our findings can be a novel framework for optimizing networks through human intuitive editing via a visual interface and suggest new possibilities for human-machine cooperation in addition to the improvement of visual explanations.","accessed":{"date-parts":[[2023,5,8]]},"author":[{"family":"Mitsuhara","given":"Masahiro"},{"family":"Fukui","given":"Hiroshi"},{"family":"Sakashita","given":"Yusuke"},{"family":"Ogata","given":"Takanori"},{"family":"Hirakawa","given":"Tsubasa"},{"family":"Yamashita","given":"Takayoshi"},{"family":"Fujiyoshi","given":"Hironobu"}],"citation-key":"mitsuharaEmbeddingHumanKnowledge2019a","DOI":"10.48550/arXiv.1905.03540","issued":{"date-parts":[[2019,12,19]]},"number":"arXiv:1905.03540","publisher":"arXiv","source":"arXiv.org","title":"Embedding Human Knowledge into Deep Neural Network via Attention Map","type":"article","URL":"http://arxiv.org/abs/1905.03540"},
  {"id":"mittelstadtPrinciplesAloneCannot2019","accessed":{"date-parts":[[2023,1,17]]},"author":[{"family":"Mittelstadt","given":"Brent"}],"citation-key":"mittelstadtPrinciplesAloneCannot2019","container-title":"Nature Machine Intelligence","container-title-short":"Nat Mach Intell","DOI":"10.1038/s42256-019-0114-4","ISSN":"2522-5839","issue":"11","issued":{"date-parts":[[2019,11,4]]},"language":"en","page":"501-507","source":"DOI.org (Crossref)","title":"Principles alone cannot guarantee ethical AI","type":"article-journal","URL":"https://www.nature.com/articles/s42256-019-0114-4","volume":"1"},
  {"id":"momennejadOfflineReplaySupports2017","abstract":"Making decisions in sequentially structured tasks requires integrating distally acquired information. The extensive computational cost of such integration challenges planning methods that integrate online, at decision time. Furthermore, it remains unclear whether “offline” integration during replay supports planning, and if so which memories should be replayed. Inspired by machine learning, we propose that (a) offline replay of trajectories facilitates integrating representations that guide decisions, and (b) unsigned prediction errors (uncertainty) trigger such integrative replay. We designed a 2-step revaluation task for fMRI, whereby participants needed to integrate changes in rewards with past knowledge to optimally replan decisions. As predicted, we found that (a) multi-voxel pattern evidence for off-task replay predicts subsequent replanning; (b) neural sensitivity to uncertainty predicts subsequent replay and replanning; (c) off-task hippocampus and anterior cingulate activity increase when revaluation is required. These findings elucidate how the brain leverages offline mechanisms in planning and goaldirected behavior under uncertainty.","accessed":{"date-parts":[[2022,5,27]]},"author":[{"family":"Momennejad","given":"Ida"},{"family":"Ross Otto","given":"A."},{"family":"Daw","given":"Nathaniel D."},{"family":"Norman","given":"Kenneth A."}],"citation-key":"momennejadOfflineReplaySupports2017","DOI":"10.1101/196758","genre":"preprint","issued":{"date-parts":[[2017,10,2]]},"language":"en","publisher":"Neuroscience","source":"DOI.org (Crossref)","title":"Offline Replay Supports Planning: fMRI Evidence from Reward Revaluation","title-short":"Offline Replay Supports Planning","type":"report","URL":"http://biorxiv.org/lookup/doi/10.1101/196758"},
  {"id":"montavonMethodsInterpretingUnderstanding2018","abstract":"This paper provides an entry point to the problem of interpreting a deep neural network model and explaining its predictions. It is based on a tutorial given at ICASSP 2017. It introduces some recently proposed techniques of interpretation, along with theory, tricks and recommendations, to make most efficient use of these techniques on real data. It also discusses a number of practical applications.","accessed":{"date-parts":[[2022,11,24]]},"author":[{"family":"Montavon","given":"Grégoire"},{"family":"Samek","given":"Wojciech"},{"family":"Müller","given":"Klaus-Robert"}],"citation-key":"montavonMethodsInterpretingUnderstanding2018","container-title":"Digital Signal Processing","container-title-short":"Digital Signal Processing","DOI":"10.1016/j.dsp.2017.10.011","ISSN":"10512004","issued":{"date-parts":[[2018,2]]},"page":"1-15","source":"arXiv.org","title":"Methods for Interpreting and Understanding Deep Neural Networks","type":"article-journal","URL":"http://arxiv.org/abs/1706.07979","volume":"73"},
  {"id":"MoodRepresentationMomentum","accessed":{"date-parts":[[2022,5,27]]},"citation-key":"MoodRepresentationMomentum","DOI":"10.1016/j.tics.2015.07.010","language":"en","title":"Mood as Representation of Momentum | Elsevier Enhanced Reader","type":"webpage","URL":"https://reader.elsevier.com/reader/sd/pii/S1364661315001746?token=52D5CDE080CFCCB0A2DB3DF7FBB520945AFD13439A2A5F94287E6E10A3DAF7CE517F685221C4189C3CAD1073A2546694&originRegion=eu-west-1&originCreation=20220527120802"},
  {"id":"moosavi-dezfooliDeepFoolSimpleAccurate2016","accessed":{"date-parts":[[2023,2,20]]},"author":[{"family":"Moosavi-Dezfooli","given":"Seyed-Mohsen"},{"family":"Fawzi","given":"Alhussein"},{"family":"Frossard","given":"Pascal"}],"citation-key":"moosavi-dezfooliDeepFoolSimpleAccurate2016","container-title":"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","DOI":"10.1109/CVPR.2016.282","event-place":"Las Vegas, NV, USA","event-title":"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","ISBN":"978-1-4673-8851-1","issued":{"date-parts":[[2016,6]]},"language":"en","page":"2574-2582","publisher":"IEEE","publisher-place":"Las Vegas, NV, USA","source":"DOI.org (Crossref)","title":"DeepFool: A Simple and Accurate Method to Fool Deep Neural Networks","title-short":"DeepFool","type":"paper-conference","URL":"http://ieeexplore.ieee.org/document/7780651/"},
  {"id":"morleyEthicsAIHealth2020","abstract":"This article presents a mapping review of the literature concerning the ethics of artificial intelligence (AI) in health care. The goal of this review is to summarise current debates and identify open questions for future research. Five literature databases were searched to support the following research question: how can the primary ethical risks presented by AI-health be categorised, and what issues must policymakers, regulators and developers consider in order to be ‘ethically mindful? A series of screening stages were carried out—for example, removing articles that focused on digital health in general (e.g. data sharing, data access, data privacy, surveillance/nudging, consent, ownership of health data, evidence of efficacy)—yielding a total of 156 papers that were included in the review.","accessed":{"date-parts":[[2023,1,17]]},"author":[{"family":"Morley","given":"Jessica"},{"family":"Machado","given":"Caio C.V."},{"family":"Burr","given":"Christopher"},{"family":"Cowls","given":"Josh"},{"family":"Joshi","given":"Indra"},{"family":"Taddeo","given":"Mariarosaria"},{"family":"Floridi","given":"Luciano"}],"citation-key":"morleyEthicsAIHealth2020","container-title":"Social Science & Medicine","container-title-short":"Social Science & Medicine","DOI":"10.1016/j.socscimed.2020.113172","ISSN":"02779536","issued":{"date-parts":[[2020,9]]},"language":"en","page":"113172","source":"DOI.org (Crossref)","title":"The ethics of AI in health care: A mapping review","title-short":"The ethics of AI in health care","type":"article-journal","URL":"https://linkinghub.elsevier.com/retrieve/pii/S0277953620303919","volume":"260"},
  {"id":"morrisonClosingLoopRobotic2018","author":[{"family":"Morrison","given":"Douglas"},{"family":"Corke","given":"Peter"},{"family":"Leitner","given":"Jürgen"}],"citation-key":"morrisonClosingLoopRobotic2018","container-title":"Proc.\\ of Robotics: Science and Systems (RSS)","issued":{"date-parts":[[2018]]},"title":"Closing the Loop for Robotic Grasping: A Real-time, Generative Grasp Synthesis Approach","type":"paper-conference"},
  {"id":"mudrakartaDidModelUnderstand2018","abstract":"We analyze state-of-the-art deep learning models for three tasks: question answering on (1) images, (2) tables, and (3) passages of text. Using the notion of \\emph{attribution} (word importance), we find that these deep networks often ignore important question terms. Leveraging such behavior, we perturb questions to craft a variety of adversarial examples. Our strongest attacks drop the accuracy of a visual question answering model from $61.1\\%$ to $19\\%$, and that of a tabular question answering model from $33.5\\%$ to $3.3\\%$. Additionally, we show how attributions can strengthen attacks proposed by Jia and Liang (2017) on paragraph comprehension models. Our results demonstrate that attributions can augment standard measures of accuracy and empower investigation of model performance. When a model is accurate but for the wrong reasons, attributions can surface erroneous logic in the model that indicates inadequacies in the test data.","accessed":{"date-parts":[[2022,11,28]]},"author":[{"family":"Mudrakarta","given":"Pramod Kaushik"},{"family":"Taly","given":"Ankur"},{"family":"Sundararajan","given":"Mukund"},{"family":"Dhamdhere","given":"Kedar"}],"citation-key":"mudrakartaDidModelUnderstand2018","DOI":"10.48550/arXiv.1805.05492","issued":{"date-parts":[[2018,5,14]]},"number":"arXiv:1805.05492","publisher":"arXiv","source":"arXiv.org","title":"Did the Model Understand the Question?","type":"article","URL":"http://arxiv.org/abs/1805.05492"},
  {"id":"muellerExplanationHumanAISystems2019","abstract":"This is an integrative review that address the question, \"What makes for a good explanation?\" with reference to AI systems. Pertinent literatures are vast. Thus, this review is necessarily selective. That said, most of the key concepts and issues are expressed in this Report. The Report encapsulates the history of computer science efforts to create systems that explain and instruct (intelligent tutoring systems and expert systems). The Report expresses the explainability issues and challenges in modern AI, and presents capsule views of the leading psychological theories of explanation. Certain articles stand out by virtue of their particular relevance to XAI, and their methods, results, and key points are highlighted. It is recommended that AI/XAI researchers be encouraged to include in their research reports fuller details on their empirical or experimental methods, in the fashion of experimental psychology research reports: details on Participants, Instructions, Procedures, Tasks, Dependent Variables (operational definitions of the measures and metrics), Independent Variables (conditions), and Control Conditions.","accessed":{"date-parts":[[2022,11,24]]},"author":[{"family":"Mueller","given":"Shane T."},{"family":"Hoffman","given":"Robert R."},{"family":"Clancey","given":"William"},{"family":"Emrey","given":"Abigail"},{"family":"Klein","given":"Gary"}],"citation-key":"muellerExplanationHumanAISystems2019","DOI":"10.48550/arXiv.1902.01876","issued":{"date-parts":[[2019,2,5]]},"number":"arXiv:1902.01876","publisher":"arXiv","source":"arXiv.org","title":"Explanation in Human-AI Systems: A Literature Meta-Review, Synopsis of Key Ideas and Publications, and Bibliography for Explainable AI","title-short":"Explanation in Human-AI Systems","type":"article","URL":"http://arxiv.org/abs/1902.01876"},
  {"id":"munozExtendingGGCNNAutomated","author":[{"family":"Muñoz","given":"Mario Ríos"},{"family":"Schomaker","given":"Lambert"},{"family":"Kasaei","given":"S Hamidreza"}],"citation-key":"munozExtendingGGCNNAutomated","title":"Extending GG-CNN through Automated Model Space Exploration using Knowledge Transfer","type":"article-journal"},
  {"id":"nasserCueReactivityYoungAdults2020","abstract":"BackgroundProblematic Instagram use (PIGU), a specific type of internet addiction, is prevalent among adolescents and young adults. In certain instances, Instagram acts as a platform for exhibiting photos of risk-taking behavior that the subjects with PIGU upload to gain likes as a surrogate for gaining peer acceptance and popularity.AimsThe primary objective was to evaluate whether addiction-specific cues compared with neutral cues, i.e., negative emotional valence cues vs. positive emotional valence cues, would elicit activation of the dopaminergic reward network (i.e., precuneus, nucleus accumbens, and amygdala) and consecutive deactivation of the executive control network [i.e., medial prefrontal cortex (mPFC) and dorsolateral prefrontal cortex (dlPFC)], in the PIGU subjects.MethodAn fMRI cue-induced reactivity study was performed using negative emotional valence, positive emotional valence, and truly neutral cues, using Instagram themes. Thirty subjects were divided into PIGU and healthy control (HC) groups, based on a set of diagnostic criteria using behavioral tests, including the Modified Instagram Addiction Test (IGAT), to assess the severity of PIGU. In-scanner recordings of the subjects’ responses to the images and regional activity of the neural addiction pathways were recorded.ResultsNegative emotional valence > positive emotional valence cues elicited increased activations in the precuneus in the PIGU group. A negative and moderate correlation was observed between PSC at the right mPFC with the IGAT scores of the PIGU subjects when corrected for multiple comparisons [r = −0.777, (p < 0.004, two-tailed)].ConclusionAddiction-specific Instagram-themed cues identify the neurobiological underpinnings of Instagram addiction. Activations of the dopaminergic reward system and deactivation of the executive control network indicate converging neuropathological pathways between Instagram addiction and other types of addictions.","accessed":{"date-parts":[[2022,6,15]]},"author":[{"family":"Nasser","given":"Nisha Syed"},{"family":"Sharifat","given":"Hamed"},{"family":"Rashid","given":"Aida Abdul"},{"family":"Hamid","given":"Suzana Ab"},{"family":"Rahim","given":"Ezamin Abdul"},{"family":"Loh","given":"Jia Ling"},{"family":"Ching","given":"Siew Mooi"},{"family":"Hoo","given":"Fan Kee"},{"family":"Ismail","given":"Siti Irma Fadillah"},{"family":"Tyagi","given":"Rohit"},{"family":"Mohammad","given":"Mazlyfarina"},{"family":"Suppiah","given":"Subapriya"}],"citation-key":"nasserCueReactivityYoungAdults2020","container-title":"Frontiers in Psychology","ISSN":"1664-1078","issued":{"date-parts":[[2020]]},"source":"Frontiers","title":"Cue-Reactivity Among Young Adults With Problematic Instagram Use in Response to Instagram-Themed Risky Behavior Cues: A Pilot fMRI Study","title-short":"Cue-Reactivity Among Young Adults With Problematic Instagram Use in Response to Instagram-Themed Risky Behavior Cues","type":"article-journal","URL":"https://www.frontiersin.org/article/10.3389/fpsyg.2020.556060","volume":"11"},
  {"id":"nasserValidationEmotionalStimuli2020","abstract":"Problematic Instagram Use (PIGU) is a specific-Internet-addiction disorder observed among the youth of today. fMRI, is able to objectively assess regional brain activation in response to addiction-specific rewards, e.g. viewing picture flashcards. Pictures uploaded onto Instagram by PIGUs have often been associated with risky behaviours in their efforts to gain more ‘Likes’, thus it is hypothesized that PIGUs are more drawn to ‘Negative-Emotional’ cues. To date, there is no local database with addiction-specific cues. Objective: To conduct an out-of-scanner validation study to create a database of pictures using ‘Negative-Emotional’ cues that evoke responses of arousal among PIGUs. Method: Forty-four Malaysian undergraduate students (20 PIGUs, 24 controls) were randomly recruited based on the evaluation using the Smartphone-Addiction-ScaleMalay version (SAS-M) and modified Instagram Addiction Test (IGAT); and fulfilled Lin et al. (2016) definition of addiction. They were administered 200 content-specific pictures that were multidimensional i.e. arousal (excitation/relaxation effects), approach-avoidance (motivational direction) and emotional valence (positive/negative feelings) to describe the PIGUs perception of the psychological properties of the pictures using a 9-point Likert scale. Results: PIGUs viewing ‘Negative-Emotional’ cues demonstrated significant positive correlations between arousal & valence (z = 4.834, p < .001, effect size = 0.69) and arousal & avoidance-approach (z = 4.625, p < .001, effect size= 0.66) as compared to controls and were more frequently aroused by ‘NegativeEmotional’ type of stimuli. Conclusion: A database of validated, addiction-specific pictures can be developed to potentiate any future cue-induced response to reward fMRI studies to assess PIGU.","accessed":{"date-parts":[[2022,6,15]]},"author":[{"family":"Nasser","given":"Nisha Syed"},{"family":"Sharifat","given":"Hamed"},{"family":"Rashid","given":"Aida Abdul"},{"family":"Hamid","given":"Suzana Ab"},{"family":"Rahim","given":"Ezamin Abdul"},{"family":"Mohamad","given":"Mazlyfarina"},{"family":"Tyagi","given":"Rohit"},{"family":"Ismail","given":"Siti Irma Fadhilah"},{"family":"Mooi","given":"Ching Siew"},{"family":"Suppiah","given":"Subapriya"}],"citation-key":"nasserValidationEmotionalStimuli2020","DOI":"10.1101/2020.01.17.20017202","genre":"preprint","issued":{"date-parts":[[2020,1,20]]},"language":"en","publisher":"Psychiatry and Clinical Psychology","source":"DOI.org (Crossref)","title":"Validation of Emotional Stimuli Flashcards for Conducting ‘Response to Reward’ fMRI study among Malaysian students","type":"report","URL":"http://medrxiv.org/lookup/doi/10.1101/2020.01.17.20017202"},
  {"id":"naveedSurveyImageMixing2023","abstract":"Neural networks are prone to overfitting and memorizing data patterns. To avoid over-fitting and enhance their generalization and performance, various methods have been suggested in the literature, including dropout, regularization, label smoothing, etc. One such method is augmentation which introduces different types of corruption in the data to prevent the model from overfitting and to memorize patterns present in the data. A sub-area of data augmentation is image mixing and deleting. This specific type of augmentation either deletes image regions or mixes two images to hide or make particular characteristics of images confusing for the network, forcing it to emphasize the overall structure of the object in an image. Models trained with this approach have proven to perform and generalize well compared to those trained without image mixing or deleting. An added benefit that comes with this method of training is robustness against image corruption. Due to its low computational cost and recent success, researchers have proposed many image mixing and deleting techniques. We furnish an in-depth survey of image mixing and deleting techniques and provide categorization via their most distinguishing features. We initiate our discussion with some fundamental relevant concepts. Next, we present essentials, such as each category's strengths and limitations, describing their working mechanism, basic formulations, and applications. We also discuss the general challenges and recommend possible future research directions for image mixing and deleting data augmentation techniques. Datasets and codes for evaluation are publicly available here.","accessed":{"date-parts":[[2023,3,31]]},"author":[{"family":"Naveed","given":"Humza"},{"family":"Anwar","given":"Saeed"},{"family":"Hayat","given":"Munawar"},{"family":"Javed","given":"Kashif"},{"family":"Mian","given":"Ajmal"}],"citation-key":"naveedSurveyImageMixing2023","DOI":"10.48550/arXiv.2106.07085","issued":{"date-parts":[[2023,2,6]]},"number":"arXiv:2106.07085","publisher":"arXiv","source":"arXiv.org","title":"Survey: Image Mixing and Deleting for Data Augmentation","title-short":"Survey","type":"article","URL":"http://arxiv.org/abs/2106.07085"},
  {"id":"nazarSystematicReviewHuman2021","abstract":"Artificial intelligence (AI) is one of the emerging technologies. In recent decades, artificial intelligence (AI) has gained widespread acceptance in a variety of fields, including virtual support, healthcare, and security. Human-Computer Interaction (HCI) is a field that has been combining AI and human-computer engagement over the past several years in order to create an interactive intelligent system for user interaction. AI, in conjunction with HCI, is being used in a variety of fields by employing various algorithms and employing HCI to provide transparency to the user, allowing them to trust the machine. The comprehensive examination of both the areas of AI and HCI, as well as their subfields, has been explored in this work. The main goal of this article was to discover a point of intersection between the two fields. The understanding of Explainable Artificial Intelligence (XAI), which is a linking point of HCI and XAI, was gained through a literature review conducted in this research. The literature survey encompassed themes identified in the literature (such as XAI and its areas, major XAI aims, and XAI problems and challenges). The study’s other major focus was on the use of AI, HCI, and XAI in healthcare. The poll also addressed the shortcomings in XAI in healthcare, as well as the field’s future potential. As a result, the literature indicates that XAI in healthcare is still a novel subject that has to be explored more in the future.","author":[{"family":"Nazar","given":"Mobeen"},{"family":"Alam","given":"Muhammad Mansoor"},{"family":"Yafi","given":"Eiad"},{"family":"Su’ud","given":"Mazliham Mohd"}],"citation-key":"nazarSystematicReviewHuman2021","container-title":"IEEE Access","DOI":"10.1109/ACCESS.2021.3127881","ISSN":"2169-3536","issued":{"date-parts":[[2021]]},"page":"153316-153348","source":"IEEE Xplore","title":"A Systematic Review of Human–Computer Interaction and Explainable Artificial Intelligence in Healthcare With Artificial Intelligence Techniques","type":"article-journal","volume":"9"},
  {"id":"neyshaburWhatBeingTransferred2020","abstract":"One desired capability for machines is the ability to transfer their understanding of one domain to another domain where data is (usually) scarce. Despite ample adaptation of transfer learning in many deep learning applications, we yet do not understand what enables a successful transfer and which part of the network is responsible for that. In this paper, we provide new tools and analysis to address these fundamental questions. Through a series of analysis on transferring to block-shuffled images, we separate the effect of feature reuse from learning high-level statistics of data and show that some benefit of transfer learning comes from the latter. We present that when training from pre-trained weights, the model stays in the same basin in the loss landscape and different instances of such model are similar in feature space and close in parameter space.","accessed":{"date-parts":[[2023,2,27]]},"author":[{"family":"Neyshabur","given":"Behnam"},{"family":"Sedghi","given":"Hanie"},{"family":"Zhang","given":"Chiyuan"}],"citation-key":"neyshaburWhatBeingTransferred2020","container-title":"Advances in Neural Information Processing Systems","issued":{"date-parts":[[2020]]},"page":"512–523","publisher":"Curran Associates, Inc.","source":"Neural Information Processing Systems","title":"What is being transferred in transfer learning?","type":"paper-conference","URL":"https://proceedings.neurips.cc/paper/2020/hash/0607f4c705595b911a4f3e7a127b44e0-Abstract.html","volume":"33"},
  {"id":"nguyenDeepNeuralNetworks2015","abstract":"Deep neural networks (DNNs) have recently been achieving state-of-the-art performance on a variety of pattern-recognition tasks, most notably visual classiﬁcation problems. Given that DNNs are now able to classify objects in images with near-human-level performance, questions naturally arise as to what differences remain between computer and human vision. A recent study [30] revealed that changing an image (e.g. of a lion) in a way imperceptible to humans can cause a DNN to label the image as something else entirely (e.g. mislabeling a lion a library). Here we show a related result: it is easy to produce images that are completely unrecognizable to humans, but that state-of-theart DNNs believe to be recognizable objects with 99.99% conﬁdence (e.g. labeling with certainty that white noise static is a lion). Speciﬁcally, we take convolutional neural networks trained to perform well on either the ImageNet or MNIST datasets and then ﬁnd images with evolutionary algorithms or gradient ascent that DNNs label with high conﬁdence as belonging to each dataset class. It is possible to produce images totally unrecognizable to human eyes that DNNs believe with near certainty are familiar objects, which we call “fooling images” (more generally, fooling examples). Our results shed light on interesting differences between human vision and current DNNs, and raise questions about the generality of DNN computer vision.","accessed":{"date-parts":[[2023,3,27]]},"author":[{"family":"Nguyen","given":"Anh"},{"family":"Yosinski","given":"Jason"},{"family":"Clune","given":"Jeff"}],"citation-key":"nguyenDeepNeuralNetworks2015","container-title":"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","DOI":"10.1109/CVPR.2015.7298640","event-place":"Boston, MA, USA","event-title":"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","ISBN":"978-1-4673-6964-0","issued":{"date-parts":[[2015,6]]},"language":"en","page":"427-436","publisher":"IEEE","publisher-place":"Boston, MA, USA","source":"DOI.org (Crossref)","title":"Deep neural networks are easily fooled: High confidence predictions for unrecognizable images","title-short":"Deep neural networks are easily fooled","type":"paper-conference","URL":"http://ieeexplore.ieee.org/document/7298640/"},
  {"id":"nunnariOverlapGradCAMSaliency2021","abstract":"Dermatologists recognize melanomas by inspecting images in which they identify human-comprehensible visual features. In this paper, we investigate to what extent such features correspond to the saliency areas identiﬁed on CNNs trained for classiﬁcation. Our experiments, conducted on two neural architectures characterized by different depth and different resolution of the last convolutional layer, quantify to what extent thresholded Grad-CAM saliency maps can be used to identify visual features of skin cancer. We found that the best threshold value, i.e., the threshold at which we can measure the highest Jaccard index, varies signiﬁcantly among features; ranging from 0.3 to 0.7. In addition, we measured Jaccard indices as high as 0.143, which is almost 50% of the performance of state-of-the-art architectures specialized in feature mask prediction at pixel-level, such as U-Net. Finally, a breakdown test between malignancy and classiﬁcation correctness shows that higher resolution saliency maps could help doctors in spotting wrong classiﬁcations.","accessed":{"date-parts":[[2022,10,3]]},"author":[{"family":"Nunnari","given":"Fabrizio"},{"family":"Kadir","given":"Md Abdul"},{"family":"Sonntag","given":"Daniel"}],"citation-key":"nunnariOverlapGradCAMSaliency2021","container-title":"Machine Learning and Knowledge Extraction","DOI":"10.1007/978-3-030-84060-0_16","editor":[{"family":"Holzinger","given":"Andreas"},{"family":"Kieseberg","given":"Peter"},{"family":"Tjoa","given":"A Min"},{"family":"Weippl","given":"Edgar"}],"event-place":"Cham","ISBN":"978-3-030-84059-4 978-3-030-84060-0","issued":{"date-parts":[[2021]]},"language":"en","page":"241-253","publisher":"Springer International Publishing","publisher-place":"Cham","source":"DOI.org (Crossref)","title":"On the Overlap Between Grad-CAM Saliency Maps and Explainable Visual Features in Skin Cancer Images","type":"chapter","URL":"https://link.springer.com/10.1007/978-3-030-84060-0_16","volume":"12844"},
  {"id":"ogaraComparingDataAugmentation","author":[{"family":"O'Gara","given":"Sarah"},{"family":"McGuinness","given":"Kevin"}],"citation-key":"ogaraComparingDataAugmentation","language":"en","source":"Zotero","title":"Comparing Data Augmentation Strategies for Deep Image Classification","type":"article-journal"},
  {"id":"ogaraComparingDataAugmentationa","author":[{"family":"O'Gara","given":"Sarah"},{"family":"McGuinness","given":"Kevin"}],"citation-key":"ogaraComparingDataAugmentationa","language":"en","source":"Zotero","title":"Comparing Data Augmentation Strategies for Deep Image Classification","type":"article-journal"},
  {"id":"oquabObjectLocalizationFree2015","abstract":"Successful methods for visual object recognition typically rely on training datasets containing lots of richly annotated images. Detailed image annotation, e.g. by object bounding boxes, however, is both expensive and often subjective. We describe a weakly supervised convolutional neural network (CNN) for object classiﬁcation that relies only on image-level labels, yet can learn from cluttered scenes containing multiple objects. We quantify its object classiﬁcation and object location prediction performance on the Pascal VOC 2012 (20 object classes) and the much larger Microsoft COCO (80 object classes) datasets. We ﬁnd that the network (i) outputs accurate image-level labels, (ii) predicts approximate locations (but not extents) of objects, and (iii) performs comparably to its fully-supervised counterparts using object bounding box annotation for training.","accessed":{"date-parts":[[2022,10,21]]},"author":[{"family":"Oquab","given":"Maxime"},{"family":"Bottou","given":"Leon"},{"family":"Laptev","given":"Ivan"},{"family":"Sivic","given":"Josef"}],"citation-key":"oquabObjectLocalizationFree2015","container-title":"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","DOI":"10.1109/CVPR.2015.7298668","event-place":"Boston, MA, USA","event-title":"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","ISBN":"978-1-4673-6964-0","issued":{"date-parts":[[2015,6]]},"language":"en","page":"685-694","publisher":"IEEE","publisher-place":"Boston, MA, USA","source":"DOI.org (Crossref)","title":"Is object localization for free? - Weakly-supervised learning with convolutional neural networks","title-short":"Is object localization for free?","type":"paper-conference","URL":"http://ieeexplore.ieee.org/document/7298668/"},
  {"id":"oyamaInfluenceImageClassification2018","abstract":"Saliency map estimation in computer vision aims to estimate the locations where people gaze in images. Since people tend to look at objects in images, the parameters of the model pre-trained on ImageNet for image classification are useful for the saliency map estimation. However, there is no research on the relationship between the image classification accuracy and the performance of the saliency map estimation. In this study, it is shown that there is a strong correlation between image classification accuracy and saliency map estimation accuracy. The authors also investigated the effective architecture based on multi-scale images and the up-sampling layers to refine the saliency-map resolution. The model achieved the state-of-the-art accuracy on the PASCAL-S, OSIE, and MIT1003 datasets. In the MIT saliency benchmark, the model achieved the best performance in some metrics and competitive results in the other metrics.","accessed":{"date-parts":[[2022,10,3]]},"author":[{"family":"Oyama","given":"Taiki"},{"family":"Yamanaka","given":"Takao"}],"citation-key":"oyamaInfluenceImageClassification2018","container-title":"CAAI Transactions on Intelligence Technology","DOI":"10.1049/trit.2018.1012","ISSN":"2468-2322","issue":"3","issued":{"date-parts":[[2018]]},"language":"en","page":"140-152","source":"Wiley Online Library","title":"Influence of image classification accuracy on saliency map estimation","type":"article-journal","URL":"https://onlinelibrary.wiley.com/doi/abs/10.1049/trit.2018.1012","volume":"3"},
  {"id":"pageShapeAnalysisAlgorithm2003","author":[{"family":"Page","given":"David L"},{"family":"Koschan","given":"Andreas F"},{"family":"Sukumar","given":"Sreenivas R"},{"family":"Roui-Abidi","given":"Besma"},{"family":"Abidi","given":"Mongi A"}],"citation-key":"pageShapeAnalysisAlgorithm2003","container-title":"Proceedings 2003 International Conference on Image Processing (Cat. No. 03CH37429)","issued":{"date-parts":[[2003]]},"page":"I–229","publisher":"IEEE","title":"Shape analysis algorithm based on information theory","type":"paper-conference","volume":"1"},
  {"id":"parmarImageTransformer2018","abstract":"Image generation has been successfully cast as an autoregressive sequence generation or transformation problem. Recent work has shown that self-attention is an effective way of modeling textual sequences. In this work, we generalize a recently proposed model architecture based on self-attention, the Transformer, to a sequence modeling formulation of image generation with a tractable likelihood. By restricting the self-attention mechanism to attend to local neighborhoods we significantly increase the size of images the model can process in practice, despite maintaining significantly larger receptive fields per layer than typical convolutional neural networks. While conceptually simple, our generative models significantly outperform the current state of the art in image generation on ImageNet, improving the best published negative log-likelihood on ImageNet from 3.83 to 3.77. We also present results on image super-resolution with a large magnification ratio, applying an encoder-decoder configuration of our architecture. In a human evaluation study, we find that images generated by our super-resolution model fool human observers three times more often than the previous state of the art.","accessed":{"date-parts":[[2022,5,24]]},"author":[{"family":"Parmar","given":"Niki"},{"family":"Vaswani","given":"Ashish"},{"family":"Uszkoreit","given":"Jakob"},{"family":"Kaiser","given":"Łukasz"},{"family":"Shazeer","given":"Noam"},{"family":"Ku","given":"Alexander"},{"family":"Tran","given":"Dustin"}],"citation-key":"parmarImageTransformer2018","DOI":"10.48550/arXiv.1802.05751","issued":{"date-parts":[[2018,6,15]]},"number":"arXiv:1802.05751","publisher":"arXiv","source":"arXiv.org","title":"Image Transformer","type":"article","URL":"http://arxiv.org/abs/1802.05751"},
  {"id":"patersonChildrenInterpretationAmbiguous2006","accessed":{"date-parts":[[2022,10,24]]},"author":[{"family":"Paterson","given":"Kevin B."},{"family":"Liversedge","given":"Simon P."},{"family":"White","given":"Diane"},{"family":"Filik","given":"Ruth"},{"family":"Jaz","given":"Kristina"}],"citation-key":"patersonChildrenInterpretationAmbiguous2006","container-title":"Language Acquisition","container-title-short":"Language Acquisition","DOI":"10.1207/s15327817la1303_4","ISSN":"1048-9223, 1532-7817","issue":"3","issued":{"date-parts":[[2006,7]]},"language":"en","page":"253-284","source":"DOI.org (Crossref)","title":"Children's Interpretation of Ambiguous Focus in Sentences With \"Only\"","type":"article-journal","URL":"http://www.tandfonline.com/doi/abs/10.1207/s15327817la1303_4","volume":"13"},
  {"id":"pepik3dObjectClass2015","author":[{"family":"Pepik","given":"Bojan"},{"family":"Stark","given":"Michael"},{"family":"Gehler","given":"Peter"},{"family":"Ritschel","given":"Tobias"},{"family":"Schiele","given":"Bernt"}],"citation-key":"pepik3dObjectClass2015","container-title":"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops","issued":{"date-parts":[[2015]]},"page":"1–10","title":"3d object class detection in the wild","type":"paper-conference"},
  {"id":"petsiukRISERandomizedInput2018","abstract":"Deep neural networks are being used increasingly to automate data analysis and decision making, yet their decision-making process is largely unclear and is difﬁcult to explain to the end users. In this paper, we address the problem of Explainable AI for deep neural networks that take images as input and output a class probability. We propose an approach called RISE that generates an importance map indicating how salient each pixel is for the model’s prediction. In contrast to white-box approaches that estimate pixel importance using gradients or other internal network state, RISE works on blackbox models. It estimates importance empirically by probing the model with randomly masked versions of the input image and obtaining the corresponding outputs. We compare our approach to state-of-the-art importance extraction methods using both an automatic deletion/insertion metric and a pointing metric based on human-annotated object segments. Extensive experiments on several benchmark datasets show that our approach matches or exceeds the performance of other methods, including white-box approaches.","accessed":{"date-parts":[[2023,2,20]]},"author":[{"family":"Petsiuk","given":"Vitali"},{"family":"Das","given":"Abir"},{"family":"Saenko","given":"Kate"}],"citation-key":"petsiukRISERandomizedInput2018","issued":{"date-parts":[[2018,9,25]]},"language":"en","number":"arXiv:1806.07421","publisher":"arXiv","source":"arXiv.org","title":"RISE: Randomized Input Sampling for Explanation of Black-box Models","title-short":"RISE","type":"article","URL":"http://arxiv.org/abs/1806.07421"},
  {"id":"poliHyenaHierarchyLarger2023","abstract":"Recent advances in deep learning have relied heavily on the use of large Transformers due to their ability to learn at scale. However, the core building block of Transformers, the attention operator, exhibits quadratic cost in sequence length, limiting the amount of context accessible. Existing subquadratic methods based on low-rank and sparse approximations need to be combined with dense attention layers to match Transformers, indicating a gap in capability. In this work, we propose Hyena, a subquadratic drop-in replacement for attention constructed by interleaving implicitly parametrized long convolutions and data-controlled gating. In recall and reasoning tasks on sequences of thousands to hundreds of thousands of tokens, Hyena improves accuracy by more than 50 points over operators relying on state-spaces and other implicit and explicit methods, matching attention-based models. We set a new state-of-the-art for dense-attention-free architectures on language modeling in standard datasets (WikiText103 and The Pile), reaching Transformer quality with a 20% reduction in training compute required at sequence length 2K. Hyena operators are twice as fast as highly optimized attention at sequence length 8K, and 100x faster at sequence length 64K.","accessed":{"date-parts":[[2023,5,11]]},"author":[{"family":"Poli","given":"Michael"},{"family":"Massaroli","given":"Stefano"},{"family":"Nguyen","given":"Eric"},{"family":"Fu","given":"Daniel Y."},{"family":"Dao","given":"Tri"},{"family":"Baccus","given":"Stephen"},{"family":"Bengio","given":"Yoshua"},{"family":"Ermon","given":"Stefano"},{"family":"Ré","given":"Christopher"}],"citation-key":"poliHyenaHierarchyLarger2023","DOI":"10.48550/arXiv.2302.10866","issued":{"date-parts":[[2023,4,19]]},"number":"arXiv:2302.10866","publisher":"arXiv","source":"arXiv.org","title":"Hyena Hierarchy: Towards Larger Convolutional Language Models","title-short":"Hyena Hierarchy","type":"article","URL":"http://arxiv.org/abs/2302.10866"},
  {"id":"polonskyWhatImage2005","author":[{"family":"Polonsky","given":"Oleg"},{"family":"Patané","given":"Giuseppe"},{"family":"Biasotti","given":"Silvia"},{"family":"Gotsman","given":"Craig"},{"family":"Spagnuolo","given":"Michela"}],"citation-key":"polonskyWhatImage2005","container-title":"The Visual Computer","issue":"8","issued":{"date-parts":[[2005]]},"page":"840–847","publisher":"Springer","title":"What’s in an image?","type":"article-journal","volume":"21"},
  {"id":"polynMemorySearchNeural2008","accessed":{"date-parts":[[2022,5,27]]},"author":[{"family":"Polyn","given":"Sean M."},{"family":"Kahana","given":"Michael J."}],"citation-key":"polynMemorySearchNeural2008","container-title":"Trends in Cognitive Sciences","container-title-short":"Trends in Cognitive Sciences","DOI":"10.1016/j.tics.2007.10.010","ISSN":"13646613","issue":"1","issued":{"date-parts":[[2008,1]]},"language":"en","page":"24-30","source":"DOI.org (Crossref)","title":"Memory search and the neural representation of context","type":"article-journal","URL":"https://linkinghub.elsevier.com/retrieve/pii/S1364661307003038","volume":"12"},
  {"id":"popelTrainingTipsTransformer2018","abstract":"This article describes our experiments in neural machine translation using the recent Tensor2Tensor framework and the Transformer sequence-to-sequence model (Vaswani et al., 2017). We examine some of the critical parameters that affect the final translation quality, memory usage, training stability and training time, concluding each experiment with a set of recommendations for fellow researchers. In addition to confirming the general mantra \"more data and larger models\", we address scaling to multiple GPUs and provide practical tips for improved training regarding batch size, learning rate, warmup steps, maximum sentence length and checkpoint averaging. We hope that our observations will allow others to get better results given their particular hardware and data constraints.","accessed":{"date-parts":[[2022,5,24]]},"author":[{"family":"Popel","given":"Martin"},{"family":"Bojar","given":"Ondřej"}],"citation-key":"popelTrainingTipsTransformer2018","container-title":"The Prague Bulletin of Mathematical Linguistics","DOI":"10.2478/pralin-2018-0002","ISSN":"1804-0462","issue":"1","issued":{"date-parts":[[2018,4,1]]},"page":"43-70","source":"arXiv.org","title":"Training Tips for the Transformer Model","type":"article-journal","URL":"http://arxiv.org/abs/1804.00247","volume":"110"},
  {"id":"portolesCharacterizingSynchronyPatterns2018","abstract":"Numerous studies seek to understand the role of oscillatory synchronization in cognition. This problem is particularly challenging in the context of complex cognitive behavior, which consists of a sequence of processing steps with uncertain duration. In this study, we analyzed oscillatory connectivity measures in time windows that previous computational models had associated with a speciﬁc sequence of processing steps in an associative memory recognition task (visual encoding, familiarity, memory retrieval, decision making, and motor response). The timing of these processing steps was estimated on a single-trial basis with a novel hidden semi-Markov model multivariate pattern analysis (HSMM-MVPA) method. We show that different processing stages are associated with speciﬁc patterns of oscillatory connectivity. Visual encoding is characterized by a dense network connecting frontal, posterior, and temporal areas as well as frontal and occipital phase locking in the 4–9 Hz theta band. Familiarity is associated with frontal phase locking in the 9–14 Hz alpha band. Decision making is associated with frontal and temporo-central interhemispheric connections in the alpha band. During decision making, a second network in the theta band that connects left-temporal, central, and occipital areas bears similarity to the neural signature for preparing a motor response. A similar theta band network is also present during the motor response, with additionally alpha band connectivity between righttemporal and posterior areas. This demonstrates that the processing stages discovered with the HSMM-MVPA method are indeed linked to distinct synchronization patterns, leading to a closer understanding of the functional role of oscillations in cognition.","accessed":{"date-parts":[[2022,5,27]]},"author":[{"family":"Portoles","given":"Oscar"},{"family":"Borst","given":"Jelmer P."},{"family":"Vugt","given":"Marieke K.","non-dropping-particle":"van"}],"citation-key":"portolesCharacterizingSynchronyPatterns2018","container-title":"European Journal of Neuroscience","container-title-short":"Eur J Neurosci","DOI":"10.1111/ejn.13817","ISSN":"0953816X","issue":"8","issued":{"date-parts":[[2018,10]]},"language":"en","page":"2759-2769","source":"DOI.org (Crossref)","title":"Characterizing synchrony patterns across cognitive task stages of associative recognition memory","type":"article-journal","URL":"https://onlinelibrary.wiley.com/doi/10.1111/ejn.13817","volume":"48"},
  {"id":"pyntaPowerSocialTelevision2014","abstract":"Marketers everywhere are paying close attention to radical changes in consumer behavior and engagement provoked by the rise of digital technology. In today's household, it is a common occurrence to share viewing experience across at least two screens: the television and secondary Internet-enabled devices. The current study used Steady State Topography (SST), a brain-activity recording methodology to explore this relationship. Participants' neural responses were recorded while they watched a live television broadcast and were allowed to freely interact on social-media platforms Twitter and Fango. The results indicate that engaging in social media while viewing television can significantly enhance neural indicators of viewer engagement in the television program.","accessed":{"date-parts":[[2022,6,15]]},"author":[{"family":"Pynta","given":"Peter"},{"family":"Seixas","given":"Shaun A. S."},{"family":"Nield","given":"Geoffrey E."},{"family":"Hier","given":"James"},{"family":"Millward","given":"Emelia"},{"family":"Silberstein","given":"Richard B."}],"citation-key":"pyntaPowerSocialTelevision2014","container-title":"Journal of Advertising Research","DOI":"10.2501/JAR-54-1-071-080","ISSN":"0021-8499","issue":"1","issued":{"date-parts":[[2014,3,1]]},"language":"en","license":"© Copyright 2014 The ARF. All rights reserved.","page":"71-80","publisher":"Journal of Advertising Research","section":"Tracking the Power of Social Media","source":"www.journalofadvertisingresearch.com","title":"The Power of Social Television: Can Social Media Build Viewer Engagement?: A New Approach to Brain Imaging of Viewer Immersion","title-short":"The Power of Social Television","type":"article-journal","URL":"http://www.journalofadvertisingresearch.com/content/54/1/71","volume":"54"},
  {"id":"qinResizeMixMixingData2020","abstract":"Data augmentation is a powerful technique to increase the diversity of data, which can effectively improve the generalization ability of neural networks in image recognition tasks. Recent data mixing based augmentation strategies have achieved great success. Especially, CutMix uses a simple but effective method to improve the classifiers by randomly cropping a patch from one image and pasting it on another image. To further promote the performance of CutMix, a series of works explore to use the saliency information of the image to guide the mixing. We systematically study the importance of the saliency information for mixing data, and find that the saliency information is not so necessary for promoting the augmentation performance. Furthermore, we find that the cutting based data mixing methods carry two problems of label misallocation and object information missing, which cannot be resolved simultaneously. We propose a more effective but very easily implemented method, namely ResizeMix. We mix the data by directly resizing the source image to a small patch and paste it on another image. The obtained patch preserves more substantial object information compared with conventional cut-based methods. ResizeMix shows evident advantages over CutMix and the saliency-guided methods on both image classification and object detection tasks without additional computation cost, which even outperforms most costly search-based automatic augmentation methods.","accessed":{"date-parts":[[2023,3,29]]},"author":[{"family":"Qin","given":"Jie"},{"family":"Fang","given":"Jiemin"},{"family":"Zhang","given":"Qian"},{"family":"Liu","given":"Wenyu"},{"family":"Wang","given":"Xingang"},{"family":"Wang","given":"Xinggang"}],"citation-key":"qinResizeMixMixingData2020","issued":{"date-parts":[[2020,12,20]]},"number":"arXiv:2012.11101","publisher":"arXiv","source":"arXiv.org","title":"ResizeMix: Mixing Data with Preserved Object Information and True Labels","title-short":"ResizeMix","type":"article","URL":"http://arxiv.org/abs/2012.11101"},
  {"id":"qiVolumetricMultiviewCNNs2016","author":[{"family":"Qi","given":"Charles R"},{"family":"Su","given":"Hao"},{"family":"Nießner","given":"Matthias"},{"family":"Dai","given":"Angela"},{"family":"Yan","given":"Mengyuan"},{"family":"Guibas","given":"Leonidas J"}],"citation-key":"qiVolumetricMultiviewCNNs2016","container-title":"Proceedings of the IEEE conference on computer vision and pattern recognition","issued":{"date-parts":[[2016]]},"page":"5648–5656","title":"Volumetric and multi-view CNNs for object classification on 3D data","type":"paper-conference"},
  {"id":"quSketchXAIFirstLook2023","abstract":"This paper, for the very first time, introduces human sketches to the landscape of XAI (Explainable Artificial Intelligence). We argue that sketch as a ``human-centred'' data form, represents a natural interface to study explainability. We focus on cultivating sketch-specific explainability designs. This starts by identifying strokes as a unique building block that offers a degree of flexibility in object construction and manipulation impossible in photos. Following this, we design a simple explainability-friendly sketch encoder that accommodates the intrinsic properties of strokes: shape, location, and order. We then move on to define the first ever XAI task for sketch, that of stroke location inversion SLI. Just as we have heat maps for photos, and correlation matrices for text, SLI offers an explainability angle to sketch in terms of asking a network how well it can recover stroke locations of an unseen sketch. We offer qualitative results for readers to interpret as snapshots of the SLI process in the paper, and as GIFs on the project page. A minor but interesting note is that thanks to its sketch-specific design, our sketch encoder also yields the best sketch recognition accuracy to date while having the smallest number of parameters. The code is available at \\url{https://sketchxai.github.io}.","accessed":{"date-parts":[[2023,5,19]]},"author":[{"family":"Qu","given":"Zhiyu"},{"family":"Gryaditskaya","given":"Yulia"},{"family":"Li","given":"Ke"},{"family":"Pang","given":"Kaiyue"},{"family":"Xiang","given":"Tao"},{"family":"Song","given":"Yi-Zhe"}],"citation-key":"quSketchXAIFirstLook2023","DOI":"10.48550/arXiv.2304.11744","issued":{"date-parts":[[2023,4,23]]},"number":"arXiv:2304.11744","publisher":"arXiv","source":"arXiv.org","title":"SketchXAI: A First Look at Explainability for Human Sketches","title-short":"SketchXAI","type":"article","URL":"http://arxiv.org/abs/2304.11744"},
  {"id":"radhakrishnanPatchnetInterpretableNeural2018","abstract":"Understanding how a complex machine learning model makes a classification decision is essential for its acceptance in sensitive areas such as health care. Towards this end, we present PatchNet, a method that provides the features indicative of each class in an image using a tradeoff between restricting global image context and classification error. We mathematically analyze this tradeoff, demonstrate Patchnet's ability to construct sharp visual heatmap representations of the learned features, and quantitatively compare these features with features selected by domain experts by applying PatchNet to the classification of benign/malignant skin lesions from the ISBI-ISIC 2017 melanoma classification challenge.","accessed":{"date-parts":[[2022,9,26]]},"author":[{"family":"Radhakrishnan","given":"Adityanarayanan"},{"family":"Durham","given":"Charles"},{"family":"Soylemezoglu","given":"Ali"},{"family":"Uhler","given":"Caroline"}],"citation-key":"radhakrishnanPatchnetInterpretableNeural2018","issued":{"date-parts":[[2018,11,29]]},"number":"arXiv:1705.08078","publisher":"arXiv","source":"arXiv.org","title":"Patchnet: Interpretable Neural Networks for Image Classification","title-short":"Patchnet","type":"article","URL":"http://arxiv.org/abs/1705.08078"},
  {"id":"rebuffiDataAugmentationCan","abstract":"Adversarial training suffers from robust overﬁtting, a phenomenon where the robust test accuracy starts to decrease during training. In this paper, we focus on reducing robust overﬁtting by using common data augmentation schemes. We demonstrate that, contrary to previous ﬁndings, when combined with model weight averaging, data augmentation can signiﬁcantly boost robust accuracy. Furthermore, we compare various data augmentations techniques and observe that spatial composition techniques work best for adversarial training. Finally, we evaluate our approach on CIFAR-10 against ∞ and 2 norm-bounded perturbations of size = 8/255 and = 128/255, respectively. We show large absolute improvements of +2.93% and +2.16% in robust accuracy compared to previous state-of-the-art methods. In particular, against ∞ norm-bounded perturbations of size = 8/255, our model reaches 60.07% robust accuracy without using any external data. We also achieve a signiﬁcant performance boost with this approach while using other architectures and datasets such as CIFAR-100, SVHN and TINYIMAGENET.","author":[{"family":"Rebufﬁ","given":"Sylvestre-Alvise"},{"family":"Gowal","given":"Sven"},{"family":"Calian","given":"Dan"},{"family":"Stimberg","given":"Florian"},{"family":"Wiles","given":"Olivia"},{"family":"Mann","given":"Timothy"}],"citation-key":"rebuffiDataAugmentationCan","language":"en","source":"Zotero","title":"Data Augmentation Can Improve Robustness","type":"article-journal"},
  {"id":"rebuffiFixingDataAugmentation2021","abstract":"Adversarial training suffers from robust overﬁtting, a phenomenon where the robust test accuracy starts to decrease during training. In this paper, we focus on both heuristics-driven and data-driven augmentations as a means to reduce robust overﬁtting. First, we demonstrate that, contrary to previous ﬁndings, when combined with model weight averaging, data augmentation can signiﬁcantly boost robust accuracy. Second, we explore how state-of-the-art generative models can be leveraged to artiﬁcially increase the size of the training set and further improve adversarial robustness. Finally, we evaluate our approach on CIFAR-10 against ∞ and 2 norm-bounded perturbations of size = 8/255 and = 128/255, respectively. We show large absolute improvements of +7.06% and +5.88% in robust accuracy compared to previous state-of-the-art methods. In particular, against ∞ norm-bounded perturbations of size = 8/255, our model reaches 64.20% robust accuracy without using any external data, beating most prior works that use external data. Since its original publication (2 Mar 2021), this paper has been accepted to NeurIPS 2021 as two separate and updated papers (Rebufﬁ et al., 2021; Gowal et al., 2021). The new papers improve results and clarity.","accessed":{"date-parts":[[2023,1,16]]},"author":[{"family":"Rebuffi","given":"Sylvestre-Alvise"},{"family":"Gowal","given":"Sven"},{"family":"Calian","given":"Dan A."},{"family":"Stimberg","given":"Florian"},{"family":"Wiles","given":"Olivia"},{"family":"Mann","given":"Timothy"}],"citation-key":"rebuffiFixingDataAugmentation2021","issued":{"date-parts":[[2021,10,18]]},"language":"en","number":"arXiv:2103.01946","publisher":"arXiv","source":"arXiv.org","title":"Fixing Data Augmentation to Improve Adversarial Robustness","type":"article","URL":"http://arxiv.org/abs/2103.01946"},
  {"id":"rebuffiThereBackAgain2020","abstract":"Saliency methods seek to explain the predictions of a model by producing an importance map across each input sample. A popular class of such methods is based on backpropagating a signal and analyzing the resulting gradient. Despite much research on such methods, relatively little work has been done to clarify the differences between such methods as well as the desiderata of these techniques. Thus, there is a need for rigorously understanding the relationships between different methods as well as their failure modes. In this work, we conduct a thorough analysis of backpropagation-based saliency methods and propose a single framework under which several such methods can be uniﬁed. As a result of our study, we make three additional contributions. First, we use our framework to propose NormGrad, a novel saliency method based on the spatial contribution of gradients of convolutional weights. Second, we combine saliency maps at different layers to test the ability of saliency methods to extract complementary information at different network levels (e.g. trading off spatial resolution and distinctiveness) and we explain why some methods fail at speciﬁc layers (e.g., Grad-CAM anywhere besides the last convolutional layer). Third, we introduce a classsensitivity metric and a meta-learning inspired paradigm applicable to any saliency method for improving sensitivity to the output class being explained.","accessed":{"date-parts":[[2023,2,22]]},"author":[{"family":"Rebuffi","given":"Sylvestre-Alvise"},{"family":"Fong","given":"Ruth"},{"family":"Ji","given":"Xu"},{"family":"Vedaldi","given":"Andrea"}],"citation-key":"rebuffiThereBackAgain2020","container-title":"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","DOI":"10.1109/CVPR42600.2020.00886","event-place":"Seattle, WA, USA","event-title":"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","ISBN":"978-1-72817-168-5","issued":{"date-parts":[[2020,6]]},"language":"en","page":"8836-8845","publisher":"IEEE","publisher-place":"Seattle, WA, USA","source":"DOI.org (Crossref)","title":"There and Back Again: Revisiting Backpropagation Saliency Methods","title-short":"There and Back Again","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/9157775/"},
  {"id":"rhodesDistinctionPerceivedDuration2018","abstract":"Time is a fundamental dimension of human perception, cognition and action, as the processing and cognition of temporal information is essential for everyday activities and survival. Innumerable studies have investigated the perception of time over the last 100 years, but the neural and computational bases for the processing of time remains unknown. Extant models of time perception are discussed before the proposition of a unified model of time perception that relates perceived event timing with perceived duration. The distinction between perceived event timing and perceived duration provides the current for navigating a river of contemporary approaches to time perception. Recent work has advocated a Bayesian approach to time perception. This framework has been applied to both duration and perceived timing, where prior expectations about when a stimulus might occur in the future (prior distribution) are combined with current sensory evidence (likelihood function) in order to generate the perception of temporal properties (posterior distribution). In general, these models predict that the brain uses temporal expectations to bias perception in a way that stimuli are ‘regularized’ i.e. stimuli look more like what has been seen before. As such, the synthesis of perceived timing and duration models is of theoretical importance for the field of timing and time perception.","accessed":{"date-parts":[[2023,1,25]]},"author":[{"family":"Rhodes","given":"Darren"}],"citation-key":"rhodesDistinctionPerceivedDuration2018","container-title":"Timing & Time Perception","DOI":"10.1163/22134468-20181132","ISSN":"2213-445X, 2213-4468","issue":"1","issued":{"date-parts":[[2018,4,10]]},"language":"eng","page":"90-123","publisher":"Brill","source":"brill.com","title":"On the Distinction Between Perceived Duration and Event Timing: Towards a Unified Model of Time Perception","title-short":"On the Distinction Between Perceived Duration and Event Timing","type":"article-journal","URL":"https://brill.com/view/journals/time/6/1/article-p90_90.xml","volume":"6"},
  {"id":"ribeiroWhyShouldTrust2016","abstract":"Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classiﬁer in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the ﬂexibility of these methods by explaining diﬀerent models for text (e.g. random forests) and image classiﬁcation (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classiﬁer, and identifying why a classiﬁer should not be trusted.","accessed":{"date-parts":[[2022,11,18]]},"author":[{"family":"Ribeiro","given":"Marco Tulio"},{"family":"Singh","given":"Sameer"},{"family":"Guestrin","given":"Carlos"}],"citation-key":"ribeiroWhyShouldTrust2016","issued":{"date-parts":[[2016,8,9]]},"language":"en","number":"arXiv:1602.04938","publisher":"arXiv","source":"arXiv.org","title":"\"Why Should I Trust You?\": Explaining the Predictions of Any Classifier","title-short":"\"Why Should I Trust You?","type":"article","URL":"http://arxiv.org/abs/1602.04938"},
  {"id":"ribeiroWhyShouldTrust2016a","abstract":"Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classiﬁer in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the ﬂexibility of these methods by explaining diﬀerent models for text (e.g. random forests) and image classiﬁcation (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classiﬁer, and identifying why a classiﬁer should not be trusted.","accessed":{"date-parts":[[2023,2,20]]},"author":[{"family":"Ribeiro","given":"Marco Tulio"},{"family":"Singh","given":"Sameer"},{"family":"Guestrin","given":"Carlos"}],"citation-key":"ribeiroWhyShouldTrust2016a","issued":{"date-parts":[[2016,8,9]]},"language":"en","number":"arXiv:1602.04938","publisher":"arXiv","source":"arXiv.org","title":"\"Why Should I Trust You?\": Explaining the Predictions of Any Classifier","title-short":"\"Why Should I Trust You?","type":"article","URL":"http://arxiv.org/abs/1602.04938"},
  {"id":"richterVarGradLowVarianceGradient2020","abstract":"We analyse the properties of an unbiased gradient estimator of the ELBO for variational inference, based on the score function method with leave-one-out control variates. We show that this gradient estimator can be obtained using a new loss, defined as the variance of the log-ratio between the exact posterior and the variational approximation, which we call the log-variance loss. Under certain conditions, the gradient of the log-variance loss equals the gradient of the (negative) ELBO. We show theoretically that this gradient estimator, which we call VarGrad due to its connection to the log-variance loss, exhibits lower variance than the score function method in certain settings, and that the leave-one-out control variate coefficients are close to the optimal ones. We empirically demonstrate that VarGrad offers a favourable variance versus computation trade-off compared to other state-of-the-art estimators on a discrete VAE.","accessed":{"date-parts":[[2023,2,20]]},"author":[{"family":"Richter","given":"Lorenz"},{"family":"Boustati","given":"Ayman"},{"family":"Nüsken","given":"Nikolas"},{"family":"Ruiz","given":"Francisco"},{"family":"Akyildiz","given":"Omer Deniz"}],"citation-key":"richterVarGradLowVarianceGradient2020","container-title":"Advances in Neural Information Processing Systems","issued":{"date-parts":[[2020]]},"page":"13481–13492","publisher":"Curran Associates, Inc.","source":"Neural Information Processing Systems","title":"VarGrad: A Low-Variance Gradient Estimator for Variational Inference","title-short":"VarGrad","type":"paper-conference","URL":"https://proceedings.neurips.cc/paper/2020/hash/9c22c0b51b3202246463e986c7e205df-Abstract.html","volume":"33"},
  {"id":"rombachHighResolutionImageSynthesis2022","abstract":"By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion .","accessed":{"date-parts":[[2022,9,26]]},"author":[{"family":"Rombach","given":"Robin"},{"family":"Blattmann","given":"Andreas"},{"family":"Lorenz","given":"Dominik"},{"family":"Esser","given":"Patrick"},{"family":"Ommer","given":"Björn"}],"citation-key":"rombachHighResolutionImageSynthesis2022","issued":{"date-parts":[[2022,4,13]]},"number":"arXiv:2112.10752","publisher":"arXiv","source":"arXiv.org","title":"High-Resolution Image Synthesis with Latent Diffusion Models","type":"article","URL":"http://arxiv.org/abs/2112.10752"},
  {"id":"sahakyanExplainableArtificialIntelligence2021","abstract":"Machine learning techniques are increasingly gaining attention due to their widespread use in various disciplines across academia and industry. Despite their tremendous success, many such techniques suffer from the “black-box” problem, which refers to situations where the data analyst is unable to explain why such techniques arrive at certain decisions. This problem has fuelled interest in Explainable Artificial Intelligence (XAI), which refers to techniques that can easily be interpreted by humans. Unfortunately, many of these techniques are not suitable for tabular data, which is surprising given the importance and widespread use of tabular data in critical applications such as finance, healthcare, and criminal justice. Also surprising is the fact that, despite the vast literature on XAI, there are still no survey articles to date that focus on tabular data. Consequently, despite the existing survey articles that cover a wide range of XAI techniques, it remains challenging for researchers working on tabular data to go through all of these surveys and extract the techniques that are suitable for their analysis. Our article fills this gap by providing a comprehensive and up-to-date survey of the XAI techniques that are relevant to tabular data. Furthermore, we categorize the references covered in our survey, indicating the type of the model being explained, the approach being used to provide the explanation, and the XAI problem being addressed. Our article is the first to provide researchers with a map that helps them navigate the XAI literature in the context of tabular data.","author":[{"family":"Sahakyan","given":"Maria"},{"family":"Aung","given":"Zeyar"},{"family":"Rahwan","given":"Talal"}],"citation-key":"sahakyanExplainableArtificialIntelligence2021","container-title":"IEEE Access","DOI":"10.1109/ACCESS.2021.3116481","ISSN":"2169-3536","issued":{"date-parts":[[2021]]},"page":"135392-135422","source":"IEEE Xplore","title":"Explainable Artificial Intelligence for Tabular Data: A Survey","title-short":"Explainable Artificial Intelligence for Tabular Data","type":"article-journal","volume":"9"},
  {"id":"sandlerMobilenetv2InvertedResiduals2018","author":[{"family":"Sandler","given":"Mark"},{"family":"Howard","given":"Andrew"},{"family":"Zhu","given":"Menglong"},{"family":"Zhmoginov","given":"Andrey"},{"family":"Chen","given":"Liang-Chieh"}],"citation-key":"sandlerMobilenetv2InvertedResiduals2018","container-title":"Proceedings of the IEEE conference on computer vision and pattern recognition","issued":{"date-parts":[[2018]]},"page":"4510–4520","title":"Mobilenetv2: Inverted residuals and linear bottlenecks","type":"paper-conference"},
  {"id":"santurkarHowDoesBatch2019","abstract":"Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm's effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers' input distributions during training to reduce the so-called \"internal covariate shift\". In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training.","accessed":{"date-parts":[[2022,8,25]]},"author":[{"family":"Santurkar","given":"Shibani"},{"family":"Tsipras","given":"Dimitris"},{"family":"Ilyas","given":"Andrew"},{"family":"Madry","given":"Aleksander"}],"citation-key":"santurkarHowDoesBatch2019","DOI":"10.48550/arXiv.1805.11604","issued":{"date-parts":[[2019,4,14]]},"number":"arXiv:1805.11604","publisher":"arXiv","source":"arXiv.org","title":"How Does Batch Normalization Help Optimization?","type":"article","URL":"http://arxiv.org/abs/1805.11604"},
  {"id":"savarese3DGenericObject2007","author":[{"family":"Savarese","given":"Silvio"},{"family":"Fei-Fei","given":"Li"}],"citation-key":"savarese3DGenericObject2007","container-title":"2007 IEEE 11th International Conference on Computer Vision","issued":{"date-parts":[[2007]]},"page":"1–8","publisher":"IEEE","title":"3D generic object categorization, localization and pose estimation","type":"paper-conference"},
  {"id":"schmidhuberSelfReferentialWeightMatrix1993","abstract":"Weight modi cations in traditional neural nets are computed by hard-wired algorithms. Without exception, all previous weight change algorithms have many speci c limitations. Is it (in principle) possible to overcome limitations of hard-wired algorithms by allowing neural nets to run and improve their own weight change algorithms? This paper constructively demonstrates that the answer (in principle) is `yes'. I derive an initial gradientbased sequence learning algorithm for a `self-referential' recurrent network that can `speak' about its own weight matrix in terms of activations. It uses some of its input and output units for observing its own errors and for explicitly analyzing and modifying its own weight matrix, including those parts of the weight matrix responsible for analyzing and modifying the weight matrix. The result is the rst `introspective' neural net with explicit potential control over all of its own adaptive parameters. A disadvantage of the algorithm is its high computational complexity per time step which is independent of the sequence length and equals O(nconnlognconn), where nconn is the number of connections. Another disadvantage is the high number of local minima of the unusually complex error surface. The purpose of this paper, however, is not to come up with the most e cient `introspective' or `self-referential' weight change algorithm, but to show that such algorithms are possible at all.","accessed":{"date-parts":[[2023,4,26]]},"author":[{"family":"Schmidhuber","given":"J."}],"citation-key":"schmidhuberSelfReferentialWeightMatrix1993","container-title":"ICANN ’93","DOI":"10.1007/978-1-4471-2063-6_107","editor":[{"family":"Gielen","given":"Stan"},{"family":"Kappen","given":"Bert"}],"event-place":"London","ISBN":"978-3-540-19839-0 978-1-4471-2063-6","issued":{"date-parts":[[1993]]},"language":"en","page":"446-450","publisher":"Springer London","publisher-place":"London","source":"DOI.org (Crossref)","title":"A ‘Self-Referential’ Weight Matrix","type":"chapter","URL":"http://link.springer.com/10.1007/978-1-4471-2063-6_107"},
  {"id":"selvarajuGradCAMVisualExplanations","abstract":"We propose a technique for producing ‘visual explanations’ for decisions from a large class of Convolutional Neural Network (CNN)-based models, making them more transparent. Our approach – Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept (say logits for ‘dog’ or even a caption), ﬂowing into the ﬁnal convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Unlike previous approaches, GradCAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers (e.g. VGG), (2) CNNs used for structured outputs (e.g. captioning), (3) CNNs used in tasks with multi-modal inputs (e.g. visual question answering) or reinforcement learning, without architectural changes or re-training. We combine Grad-CAM with existing ﬁne-grained visualizations to create a high-resolution class-discriminative visualization, Guided Grad-CAM, and apply it to image classiﬁcation, image captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classiﬁcation models, our visualizations (a) lend insights into failure modes of these models (showing that seemingly unreasonable predictions have reasonable explanations), (b) outperform previous methods on the ILSVRC-15 weakly-supervised localization task, (c) are more faithful to the underlying model, and (d) help achieve model generalization by identifying dataset bias. For image captioning and VQA, our visualizations show even non-attention based models can localize inputs. Finally, we design and conduct human studies to measure if Grad-CAM explanations help users establish appropriate trust in predictions from deep networks and show that Grad-CAM helps untrained users successfully discern a ‘stronger’ deep network from a ‘weaker’ one even when both make identical predictions. Our code is available at https: //github.com/ramprs/grad-cam/ along with a demo on CloudCV [2]1 and video at youtu.be/COjUB9Izk6E.","author":[{"family":"Selvaraju","given":"Ramprasaath R"},{"family":"Cogswell","given":"Michael"},{"family":"Das","given":"Abhishek"},{"family":"Vedantam","given":"Ramakrishna"},{"family":"Parikh","given":"Devi"},{"family":"Batra","given":"Dhruv"}],"citation-key":"selvarajuGradCAMVisualExplanations","language":"en","page":"9","source":"Zotero","title":"Grad-CAM: Visual Explanations From Deep Networks via Gradient-Based Localization","type":"article-journal"},
  {"id":"selvarajuGradCAMWhyDid2017","abstract":"We propose a technique for making Convolutional Neural Network (CNN)-based models more transparent by visualizing input regions that are 'important' for predictions -- or visual explanations. Our approach, called Gradient-weighted Class Activation Mapping (Grad-CAM), uses class-specific gradient information to localize important regions. These localizations are combined with existing pixel-space visualizations to create a novel high-resolution and class-discriminative visualization called Guided Grad-CAM. These methods help better understand CNN-based models, including image captioning and visual question answering (VQA) models. We evaluate our visual explanations by measuring their ability to discriminate between classes, to inspire trust in humans, and their correlation with occlusion maps. Grad-CAM provides a new way to understand CNN-based models. We have released code, an online demo hosted on CloudCV, and a full version of this extended abstract.","accessed":{"date-parts":[[2023,2,20]]},"author":[{"family":"Selvaraju","given":"Ramprasaath R."},{"family":"Das","given":"Abhishek"},{"family":"Vedantam","given":"Ramakrishna"},{"family":"Cogswell","given":"Michael"},{"family":"Parikh","given":"Devi"},{"family":"Batra","given":"Dhruv"}],"citation-key":"selvarajuGradCAMWhyDid2017","issued":{"date-parts":[[2017,1,25]]},"number":"arXiv:1611.07450","publisher":"arXiv","source":"arXiv.org","title":"Grad-CAM: Why did you say that?","title-short":"Grad-CAM","type":"article","URL":"http://arxiv.org/abs/1611.07450"},
  {"id":"shankarOperationalizingMachineLearning2022","abstract":"Organizations rely on machine learning engineers (MLEs) to operationalize ML, i.e., deploy and maintain ML pipelines in production. The process of operationalizing ML, or MLOps, consists of a continual loop of (i) data collection and labeling, (ii) experimentation to improve ML performance, (iii) evaluation throughout a multi-staged deployment process, and (iv) monitoring of performance drops in production. When considered together, these responsibilities seem staggering -- how does anyone do MLOps, what are the unaddressed challenges, and what are the implications for tool builders? We conducted semi-structured ethnographic interviews with 18 MLEs working across many applications, including chatbots, autonomous vehicles, and finance. Our interviews expose three variables that govern success for a production ML deployment: Velocity, Validation, and Versioning. We summarize common practices for successful ML experimentation, deployment, and sustaining production performance. Finally, we discuss interviewees' pain points and anti-patterns, with implications for tool design.","accessed":{"date-parts":[[2022,9,26]]},"author":[{"family":"Shankar","given":"Shreya"},{"family":"Garcia","given":"Rolando"},{"family":"Hellerstein","given":"Joseph M."},{"family":"Parameswaran","given":"Aditya G."}],"citation-key":"shankarOperationalizingMachineLearning2022","issued":{"date-parts":[[2022,9,16]]},"number":"arXiv:2209.09125","publisher":"arXiv","source":"arXiv.org","title":"Operationalizing Machine Learning: An Interview Study","title-short":"Operationalizing Machine Learning","type":"article","URL":"http://arxiv.org/abs/2209.09125"},
  {"id":"shermanPowerAdolescence2016","abstract":"We investigated a unique way in which adolescent peer influence occurs on social\nmedia. We developed a novel functional MRI (fMRI) paradigm to simulate\nInstagram, a popular social photo-sharing tool, and measured adolescents’\nbehavioral and neural responses to likes, a quantifiable form\nof social endorsement and potential source of peer influence. Adolescents\nunderwent fMRI while viewing photos ostensibly submitted to Instagram. They were\nmore likely to like photos depicted with many likes than photos\nwith few likes; this finding showed the influence of virtual peer endorsement\nand held for both neutral photos and photos of risky behaviors (e.g., drinking,\nsmoking). Viewing photos with many (compared with few) likes was associated with\ngreater activity in neural regions implicated in reward processing, social\ncognition, imitation, and attention. Furthermore, when adolescents viewed risky\nphotos (as opposed to neutral photos), activation in the cognitive-control\nnetwork decreased. These findings highlight possible mechanisms underlying peer\ninfluence during adolescence.","accessed":{"date-parts":[[2022,6,15]]},"author":[{"family":"Sherman","given":"Lauren E."},{"family":"Payton","given":"Ashley A."},{"family":"Hernandez","given":"Leanna M."},{"family":"Greenfield","given":"Patricia M."},{"family":"Dapretto","given":"Mirella"}],"citation-key":"shermanPowerAdolescence2016","container-title":"Psychological Science","container-title-short":"Psychol Sci","DOI":"10.1177/0956797616645673","ISSN":"0956-7976","issue":"7","issued":{"date-parts":[[2016,7]]},"page":"1027-1035","PMCID":"PMC5387999","PMID":"27247125","source":"PubMed Central","title":"The Power of the Like in Adolescence","type":"article-journal","URL":"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5387999/","volume":"27"},
  {"id":"shortenSurveyImageData2019","abstract":"Deep convolutional neural networks have performed remarkably well on many Computer Vision tasks. However, these networks are heavily reliant on big data to avoid overfitting. Overfitting refers to the phenomenon when a network learns a function with very high variance such as to perfectly model the training data. Unfortunately, many application domains do not have access to big data, such as medical image analysis. This survey focuses on Data Augmentation, a data-space solution to the problem of limited data. Data Augmentation encompasses a suite of techniques that enhance the size and quality of training datasets such that better Deep Learning models can be built using them. The image augmentation algorithms discussed in this survey include geometric transformations, color space augmentations, kernel filters, mixing images, random erasing, feature space augmentation, adversarial training, generative adversarial networks, neural style transfer, and meta-learning. The application of augmentation methods based on GANs are heavily covered in this survey. In addition to augmentation techniques, this paper will briefly discuss other characteristics of Data Augmentation such as test-time augmentation, resolution impact, final dataset size, and curriculum learning. This survey will present existing methods for Data Augmentation, promising developments, and meta-level decisions for implementing Data Augmentation. Readers will understand how Data Augmentation can improve the performance of their models and expand limited datasets to take advantage of the capabilities of big data.","accessed":{"date-parts":[[2023,5,8]]},"author":[{"family":"Shorten","given":"Connor"},{"family":"Khoshgoftaar","given":"Taghi M."}],"citation-key":"shortenSurveyImageData2019","container-title":"Journal of Big Data","container-title-short":"J Big Data","DOI":"10.1186/s40537-019-0197-0","ISSN":"2196-1115","issue":"1","issued":{"date-parts":[[2019,12]]},"language":"en","page":"60","source":"DOI.org (Crossref)","title":"A survey on Image Data Augmentation for Deep Learning","type":"article-journal","URL":"https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0197-0","volume":"6"},
  {"id":"shrikumarComputationallyEfficientMeasures2018","abstract":"The challenge of assigning importance to individual neurons in a network is of interest when interpreting deep learning models. In recent work, Dhamdhere et al. proposed Total Conductance, a \"natural refinement of Integrated Gradients\" for attributing importance to internal neurons. Unfortunately, the authors found that calculating conductance in tensorflow required the addition of several custom gradient operators and did not scale well. In this work, we show that the formula for Total Conductance is mathematically equivalent to Path Integrated Gradients computed on a hidden layer in the network. We provide a scalable implementation of Total Conductance using standard tensorflow gradient operators that we call Neuron Integrated Gradients. We compare Neuron Integrated Gradients to DeepLIFT, a pre-existing computationally efficient approach that is applicable to calculating internal neuron importance. We find that DeepLIFT produces strong empirical results and is faster to compute, but because it lacks the theoretical properties of Neuron Integrated Gradients, it may not always be preferred in practice. Colab notebook reproducing results: http://bit.ly/neuronintegratedgradients","accessed":{"date-parts":[[2022,11,28]]},"author":[{"family":"Shrikumar","given":"Avanti"},{"family":"Su","given":"Jocelin"},{"family":"Kundaje","given":"Anshul"}],"citation-key":"shrikumarComputationallyEfficientMeasures2018","DOI":"10.48550/arXiv.1807.09946","issued":{"date-parts":[[2018,7,25]]},"number":"arXiv:1807.09946","publisher":"arXiv","source":"arXiv.org","title":"Computationally Efficient Measures of Internal Neuron Importance","type":"article","URL":"http://arxiv.org/abs/1807.09946"},
  {"id":"shrikumarLearningImportantFeatures2019","abstract":"The purported \"black box\" nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLIFT (Deep Learning Important FeaTures), a method for decomposing the output prediction of a neural network on a specific input by backpropagating the contributions of all neurons in the network to every feature of the input. DeepLIFT compares the activation of each neuron to its 'reference activation' and assigns contribution scores according to the difference. By optionally giving separate consideration to positive and negative contributions, DeepLIFT can also reveal dependencies which are missed by other approaches. Scores can be computed efficiently in a single backward pass. We apply DeepLIFT to models trained on MNIST and simulated genomic data, and show significant advantages over gradient-based methods. Video tutorial: http://goo.gl/qKb7pL, ICML slides: bit.ly/deeplifticmlslides, ICML talk: https://vimeo.com/238275076, code: http://goo.gl/RM8jvH.","accessed":{"date-parts":[[2022,11,28]]},"author":[{"family":"Shrikumar","given":"Avanti"},{"family":"Greenside","given":"Peyton"},{"family":"Kundaje","given":"Anshul"}],"citation-key":"shrikumarLearningImportantFeatures2019","DOI":"10.48550/arXiv.1704.02685","issued":{"date-parts":[[2019,10,12]]},"number":"arXiv:1704.02685","publisher":"arXiv","source":"arXiv.org","title":"Learning Important Features Through Propagating Activation Differences","type":"article","URL":"http://arxiv.org/abs/1704.02685"},
  {"id":"sigihaleAtypicalBrainActivation2007","abstract":"Objective: Executive dysfunction in ADHD is well supported. However, recent studies suggest that more fundamental impairments may be contributing. We assessed brain function in adults with ADHD during simple and complex forms of processing. Method: We used functional magnetic resonance imaging with forward and backward digit spans to investigate number repetitions and complex working memory function. If pathology is limited to higher cognitive operations, group differences should be confined to the backward condition. Results: During the forward digit span, ADHD participants exhibited greater activation of LH linguistic processing areas and increased activation of right frontal and parietal cortices. During the backward digit span, they exhibited greater activation of LH linguistic processing areas and failed to activate bilateral parietal regions important for the complex executive operations. Conclusion: Abnormal brain function among adult ADHD participants was not limited to complex executive functions. Abnormal processing of numeric stimuli was indicated during both simple and complex cognitive operations. (J. of Att. Dis. 2007;11(2) 125-140)","accessed":{"date-parts":[[2022,6,15]]},"author":[{"family":"Sigi Hale","given":"T."},{"family":"Bookheimer","given":"Susan"},{"family":"McGough","given":"James J."},{"family":"Phillips","given":"Joseph M."},{"family":"McCracken","given":"James T."}],"citation-key":"sigihaleAtypicalBrainActivation2007","container-title":"Journal of Attention Disorders","container-title-short":"J Atten Disord","DOI":"10.1177/1087054706294101","ISSN":"1087-0547, 1557-1246","issue":"2","issued":{"date-parts":[[2007,9]]},"language":"en","page":"125-139","source":"DOI.org (Crossref)","title":"Atypical Brain Activation During Simple & Complex Levels of Processing in Adult ADHD: An fMRI Study","title-short":"Atypical Brain Activation During Simple & Complex Levels of Processing in Adult ADHD","type":"article-journal","URL":"http://journals.sagepub.com/doi/10.1177/1087054706294101","volume":"11"},
  {"id":"simonyanDeepConvolutionalNetworks2014","abstract":"This paper addresses the visualisation of image classiﬁcation models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The ﬁrst one generates an image, which maximises the class score [5], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, speciﬁc to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classiﬁcation ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [13].","accessed":{"date-parts":[[2022,11,18]]},"author":[{"family":"Simonyan","given":"Karen"},{"family":"Vedaldi","given":"Andrea"},{"family":"Zisserman","given":"Andrew"}],"citation-key":"simonyanDeepConvolutionalNetworks2014","issued":{"date-parts":[[2014,4,19]]},"language":"en","number":"arXiv:1312.6034","publisher":"arXiv","source":"arXiv.org","title":"Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps","title-short":"Deep Inside Convolutional Networks","type":"article","URL":"http://arxiv.org/abs/1312.6034"},
  {"id":"simonyanDeepConvolutionalNetworks2014a","abstract":"This paper addresses the visualisation of image classiﬁcation models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The ﬁrst one generates an image, which maximises the class score [5], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, speciﬁc to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classiﬁcation ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [13].","accessed":{"date-parts":[[2022,11,18]]},"author":[{"family":"Simonyan","given":"Karen"},{"family":"Vedaldi","given":"Andrea"},{"family":"Zisserman","given":"Andrew"}],"citation-key":"simonyanDeepConvolutionalNetworks2014a","issued":{"date-parts":[[2014,4,19]]},"language":"en","number":"arXiv:1312.6034","publisher":"arXiv","source":"arXiv.org","title":"Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps","title-short":"Deep Inside Convolutional Networks","type":"article","URL":"http://arxiv.org/abs/1312.6034"},
  {"id":"simonyanDeepConvolutionalNetworks2014b","abstract":"This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013].","accessed":{"date-parts":[[2022,11,28]]},"author":[{"family":"Simonyan","given":"Karen"},{"family":"Vedaldi","given":"Andrea"},{"family":"Zisserman","given":"Andrew"}],"citation-key":"simonyanDeepConvolutionalNetworks2014b","DOI":"10.48550/arXiv.1312.6034","issued":{"date-parts":[[2014,4,19]]},"number":"arXiv:1312.6034","publisher":"arXiv","source":"arXiv.org","title":"Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps","title-short":"Deep Inside Convolutional Networks","type":"article","URL":"http://arxiv.org/abs/1312.6034"},
  {"id":"simonyanVeryDeepConvolutional2014","author":[{"family":"Simonyan","given":"Karen"},{"family":"Zisserman","given":"Andrew"}],"citation-key":"simonyanVeryDeepConvolutional2014","container-title":"arXiv preprint arXiv:1409.1556","issued":{"date-parts":[[2014]]},"title":"Very deep convolutional networks for large-scale image recognition","type":"article-journal"},
  {"id":"singhHideandSeekDataAugmentation2018","abstract":"We propose 'Hide-and-Seek' a general purpose data augmentation technique, which is complementary to existing data augmentation techniques and is beneficial for various visual recognition tasks. The key idea is to hide patches in a training image randomly, in order to force the network to seek other relevant content when the most discriminative content is hidden. Our approach only needs to modify the input image and can work with any network to improve its performance. During testing, it does not need to hide any patches. The main advantage of Hide-and-Seek over existing data augmentation techniques is its ability to improve object localization accuracy in the weakly-supervised setting, and we therefore use this task to motivate the approach. However, Hide-and-Seek is not tied only to the image localization task, and can generalize to other forms of visual input like videos, as well as other recognition tasks like image classification, temporal action localization, semantic segmentation, emotion recognition, age/gender estimation, and person re-identification. We perform extensive experiments to showcase the advantage of Hide-and-Seek on these various visual recognition problems.","accessed":{"date-parts":[[2023,3,27]]},"author":[{"family":"Singh","given":"Krishna Kumar"},{"family":"Yu","given":"Hao"},{"family":"Sarmasi","given":"Aron"},{"family":"Pradeep","given":"Gautam"},{"family":"Lee","given":"Yong Jae"}],"citation-key":"singhHideandSeekDataAugmentation2018","DOI":"10.48550/arXiv.1811.02545","issued":{"date-parts":[[2018,11,6]]},"number":"arXiv:1811.02545","publisher":"arXiv","source":"arXiv.org","title":"Hide-and-Seek: A Data Augmentation Technique for Weakly-Supervised Localization and Beyond","title-short":"Hide-and-Seek","type":"article","URL":"http://arxiv.org/abs/1811.02545"},
  {"id":"smilkovSmoothGradRemovingNoise2017","abstract":"Explaining the output of a deep network remains a challenge. In the case of an image classifier, one type of explanation is to identify pixels that strongly influence the final decision. A starting point for this strategy is the gradient of the class score function with respect to the input image. This gradient can be interpreted as a sensitivity map, and there are several techniques that elaborate on this basic idea. This paper makes two contributions: it introduces SmoothGrad, a simple method that can help visually sharpen gradient-based sensitivity maps, and it discusses lessons in the visualization of these maps. We publish the code for our experiments and a website with our results.","accessed":{"date-parts":[[2022,11,28]]},"author":[{"family":"Smilkov","given":"Daniel"},{"family":"Thorat","given":"Nikhil"},{"family":"Kim","given":"Been"},{"family":"Viégas","given":"Fernanda"},{"family":"Wattenberg","given":"Martin"}],"citation-key":"smilkovSmoothGradRemovingNoise2017","DOI":"10.48550/arXiv.1706.03825","issued":{"date-parts":[[2017,6,12]]},"number":"arXiv:1706.03825","publisher":"arXiv","source":"arXiv.org","title":"SmoothGrad: removing noise by adding noise","title-short":"SmoothGrad","type":"article","URL":"http://arxiv.org/abs/1706.03825"},
  {"id":"smithSuperConvergenceVeryFast2018","abstract":"In this paper, we describe a phenomenon, which we named \"super-convergence\", where neural networks can be trained an order of magnitude faster than with standard training methods. The existence of super-convergence is relevant to understanding why deep networks generalize well. One of the key elements of super-convergence is training with one learning rate cycle and a large maximum learning rate. A primary insight that allows super-convergence training is that large learning rates regularize the training, hence requiring a reduction of all other forms of regularization in order to preserve an optimal regularization balance. We also derive a simplification of the Hessian Free optimization method to compute an estimate of the optimal learning rate. Experiments demonstrate super-convergence for Cifar-10/100, MNIST and Imagenet datasets, and resnet, wide-resnet, densenet, and inception architectures. In addition, we show that super-convergence provides a greater boost in performance relative to standard training when the amount of labeled training data is limited. The architectures and code to replicate the figures in this paper are available at github.com/lnsmith54/super-convergence. See http://www.fast.ai/2018/04/30/dawnbench-fastai/ for an application of super-convergence to win the DAWNBench challenge (see https://dawn.cs.stanford.edu/benchmark/).","accessed":{"date-parts":[[2023,2,28]]},"author":[{"family":"Smith","given":"Leslie N."},{"family":"Topin","given":"Nicholay"}],"citation-key":"smithSuperConvergenceVeryFast2018","DOI":"10.48550/arXiv.1708.07120","issued":{"date-parts":[[2018,5,17]]},"number":"arXiv:1708.07120","publisher":"arXiv","source":"arXiv.org","title":"Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates","title-short":"Super-Convergence","type":"article","URL":"http://arxiv.org/abs/1708.07120"},
  {"id":"SMOTESyntheticMinority","accessed":{"date-parts":[[2023,3,31]]},"citation-key":"SMOTESyntheticMinority","title":"SMOTE: Synthetic Minority Over-sampling Technique | Journal of Artificial Intelligence Research","type":"webpage","URL":"https://www.jair.org/index.php/jair/article/view/10302"},
  {"id":"sockMultiView6DObject2017","author":[{"family":"Sock","given":"Juil"},{"family":"Hamidreza Kasaei","given":"S."},{"family":"Seabra Lopes","given":"Luis"},{"family":"Kim","given":"Tae-Kyun"}],"citation-key":"sockMultiView6DObject2017","container-title":"Proceedings of the IEEE International Conference on Computer Vision (ICCV) Workshops","issued":{"date-parts":[[2017]]},"title":"Multi-View 6D Object Pose Estimation and Camera Motion Planning Using RGBD Images","type":"paper-conference"},
  {"id":"spinnerExplAInerVisualAnalytics2019","abstract":"We propose a framework for interactive and explainable machine learning that enables users to (1) understand machine learning models; (2) diagnose model limitations using different explainable AI methods; as well as (3) refine and optimize the models. Our framework combines an iterative XAI pipeline with eight global monitoring and steering mechanisms, including quality monitoring, provenance tracking, model comparison, and trust building. To operationalize the framework, we present explAIner, a visual analytics system for interactive and explainable machine learning that instantiates all phases of the suggested pipeline within the commonly used TensorBoard environment. We performed a user-study with nine participants across different expertise levels to examine their perception of our workflow and to collect suggestions to fill the gap between our system and framework. The evaluation confirms that our tightly integrated system leads to an informed machine learning process while disclosing opportunities for further extensions.","accessed":{"date-parts":[[2023,2,13]]},"author":[{"family":"Spinner","given":"Thilo"},{"family":"Schlegel","given":"Udo"},{"family":"Schäfer","given":"Hanna"},{"family":"El-Assady","given":"Mennatallah"}],"citation-key":"spinnerExplAInerVisualAnalytics2019","container-title":"IEEE Transactions on Visualization and Computer Graphics","container-title-short":"IEEE Trans. Visual. Comput. Graphics","DOI":"10.1109/TVCG.2019.2934629","ISSN":"1077-2626, 1941-0506, 2160-9306","issued":{"date-parts":[[2019]]},"page":"1-1","source":"arXiv.org","title":"explAIner: A Visual Analytics Framework for Interactive and Explainable Machine Learning","title-short":"explAIner","type":"article-journal","URL":"http://arxiv.org/abs/1908.00087"},
  {"id":"springenbergStrivingSimplicityAll2015","abstract":"Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. We re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding -- and building on other recent work for finding simple network structures -- we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the \"deconvolution approach\" for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches.","accessed":{"date-parts":[[2022,11,18]]},"author":[{"family":"Springenberg","given":"Jost Tobias"},{"family":"Dosovitskiy","given":"Alexey"},{"family":"Brox","given":"Thomas"},{"family":"Riedmiller","given":"Martin"}],"citation-key":"springenbergStrivingSimplicityAll2015","DOI":"10.48550/arXiv.1412.6806","issued":{"date-parts":[[2015,4,13]]},"number":"arXiv:1412.6806","publisher":"arXiv","source":"arXiv.org","title":"Striving for Simplicity: The All Convolutional Net","title-short":"Striving for Simplicity","type":"article","URL":"http://arxiv.org/abs/1412.6806"},
  {"id":"steedImageRepresentationsLearned2021","abstract":"Recent advances in machine learning leverage massive datasets of unlabeled images from the web to learn general-purpose image representations for tasks from image classification to face recognition. But do unsupervised computer vision models automatically learn implicit patterns and embed social biases that could have harmful downstream effects? We develop a novel method for quantifying biased associations between representations of social concepts and attributes in images. We find that state-of-the-art unsupervised models trained on ImageNet, a popular benchmark image dataset curated from internet images, automatically learn racial, gender, and intersectional biases. We replicate 8 previously documented human biases from social psychology, from the innocuous, as with insects and flowers, to the potentially harmful, as with race and gender. Our results closely match three hypotheses about intersectional bias from social psychology. For the first time in unsupervised computer vision, we also quantify implicit human biases about weight, disabilities, and several ethnicities. When compared with statistical patterns in online image datasets, our findings suggest that machine learning models can automatically learn bias from the way people are stereotypically portrayed on the web.","accessed":{"date-parts":[[2022,9,6]]},"author":[{"family":"Steed","given":"Ryan"},{"family":"Caliskan","given":"Aylin"}],"citation-key":"steedImageRepresentationsLearned2021","collection-title":"FAccT '21","container-title":"Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency","DOI":"10.1145/3442188.3445932","event-place":"New York, NY, USA","ISBN":"978-1-4503-8309-7","issued":{"date-parts":[[2021,3,3]]},"page":"701–713","publisher":"Association for Computing Machinery","publisher-place":"New York, NY, USA","source":"ACM Digital Library","title":"Image Representations Learned With Unsupervised Pre-Training Contain Human-like Biases","type":"paper-conference","URL":"https://doi.org/10.1145/3442188.3445932"},
  {"id":"steinbornSequentialEffectsShort2008","abstract":"Responses to an imperative stimulus (IS) are especially fast when they are preceded by a warning signal (WS). When the interval between WS and IS (the foreperiod, FP) is variable, reaction time (RT) is not only inﬂuenced by the current FP but also by the FP of the preceding trial. These sequential effects have recently been proposed to originate from a trace conditioning process, in which the individuals learn the temporal WS–IS relationship in a trial-by-trial manner. Research has shown that trace conditioning is maximal when the temporal interval between the conditioned and unconditioned stimulus is between 0.25 and 0.60 s. Consequently, one would predict that sequential effects occur especially within short FP contexts. However, this prediction is contradicted by Karlin [Karlin, L. (1959). Reaction time as a function of foreperiod duration and variability. Journal of Experimental Psychology, 58, 185–191] who did not observe the typical sequential effects with short FPs. To investigate temporal preparation for short FPs, three experiments were conducted, examining the sequential FP effect comparably for short and long FP-sets (Experiment 1), assessing the inﬂuence of catch trials (Experiment 2) and the case of a very dense FP-range (Experiment 3) on sequential FP effects. The results provide strong evidence for sequential effects within a short FP context and thus support the trace conditioning account of temporal preparation.","accessed":{"date-parts":[[2023,1,27]]},"author":[{"family":"Steinborn","given":"Michael B."},{"family":"Rolke","given":"Bettina"},{"family":"Bratzke","given":"Daniel"},{"family":"Ulrich","given":"Rolf"}],"citation-key":"steinbornSequentialEffectsShort2008","container-title":"Acta Psychologica","container-title-short":"Acta Psychologica","DOI":"10.1016/j.actpsy.2008.08.005","ISSN":"00016918","issue":"2","issued":{"date-parts":[[2008,10]]},"language":"en","page":"297-307","source":"DOI.org (Crossref)","title":"Sequential effects within a short foreperiod context: Evidence for the conditioning account of temporal preparation","title-short":"Sequential effects within a short foreperiod context","type":"article-journal","URL":"https://linkinghub.elsevier.com/retrieve/pii/S000169180800111X","volume":"129"},
  {"id":"steinerHowTrainYour2022","abstract":"Vision Transformers (ViT) have been shown to attain highly competitive performance for a wide range of vision applications, such as image classiﬁcation, object detection and semantic image segmentation. In comparison to convolutional neural networks, the Vision Transformer’s weaker inductive bias is generally found to cause an increased reliance on model regularization or data augmentation (“AugReg” for short) when training on smaller training datasets. We conduct a systematic empirical study in order to better understand the interplay between the amount of training data, AugReg, model size and compute budget.1 As one result of this study we ﬁnd that the combination of increased compute and AugReg can yield models with the same performance as models trained on an order of magnitude more training data: we train ViT models of various sizes on the public ImageNet-21k dataset which either match or outperform their counterparts trained on the larger, but not publicly available JFT-300M dataset.","accessed":{"date-parts":[[2023,1,16]]},"author":[{"family":"Steiner","given":"Andreas"},{"family":"Kolesnikov","given":"Alexander"},{"family":"Zhai","given":"Xiaohua"},{"family":"Wightman","given":"Ross"},{"family":"Uszkoreit","given":"Jakob"},{"family":"Beyer","given":"Lucas"}],"citation-key":"steinerHowTrainYour2022","issued":{"date-parts":[[2022,6,23]]},"language":"en","number":"arXiv:2106.10270","publisher":"arXiv","source":"arXiv.org","title":"How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers","title-short":"How to train your ViT?","type":"article","URL":"http://arxiv.org/abs/2106.10270"},
  {"id":"sturmfelsVisualizingImpactFeature2020","abstract":"Exploring the baseline input hyperparameter, and how it impacts interpretations of neural network behavior.","accessed":{"date-parts":[[2023,3,24]]},"author":[{"family":"Sturmfels","given":"Pascal"},{"family":"Lundberg","given":"Scott"},{"family":"Lee","given":"Su-In"}],"citation-key":"sturmfelsVisualizingImpactFeature2020","container-title":"Distill","container-title-short":"Distill","DOI":"10.23915/distill.00022","ISSN":"2476-0757","issue":"1","issued":{"date-parts":[[2020,1,10]]},"language":"en","page":"e22","source":"distill.pub","title":"Visualizing the Impact of Feature Attribution Baselines","type":"article-journal","URL":"https://distill.pub/2020/attribution-baselines","volume":"5"},
  {"id":"suMultiviewConvolutionalNeural2015","author":[{"family":"Su","given":"Hang"},{"family":"Maji","given":"Subhransu"},{"family":"Kalogerakis","given":"Evangelos"},{"family":"Learned-Miller","given":"Erik"}],"citation-key":"suMultiviewConvolutionalNeural2015","container-title":"Proceedings of the IEEE international conference on computer vision","issued":{"date-parts":[[2015]]},"title":"Multi-view convolutional neural networks for 3D shape recognition","type":"paper-conference"},
  {"id":"suMultiviewConvolutionalNeural2015a","author":[{"family":"Su","given":"Hang"},{"family":"Maji","given":"Subhransu"},{"family":"Kalogerakis","given":"Evangelos"},{"family":"Learned-Miller","given":"Erik"}],"citation-key":"suMultiviewConvolutionalNeural2015a","container-title":"Proceedings of the IEEE international conference on computer vision","issued":{"date-parts":[[2015]]},"title":"Multi-view convolutional neural networks for 3D shape recognition","type":"paper-conference"},
  {"id":"sundararajanAxiomaticAttributionDeep2017","abstract":"We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms---Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.","accessed":{"date-parts":[[2023,3,24]]},"author":[{"family":"Sundararajan","given":"Mukund"},{"family":"Taly","given":"Ankur"},{"family":"Yan","given":"Qiqi"}],"citation-key":"sundararajanAxiomaticAttributionDeep2017","DOI":"10.48550/arXiv.1703.01365","issued":{"date-parts":[[2017,6,12]]},"number":"arXiv:1703.01365","publisher":"arXiv","source":"arXiv.org","title":"Axiomatic Attribution for Deep Networks","type":"article","URL":"http://arxiv.org/abs/1703.01365"},
  {"id":"suttonAlbertaPlanAI2022","abstract":"Herein we describe our approach to artificial intelligence research, which we call the Alberta Plan. The Alberta Plan is pursued within our research groups in Alberta and by others who are like minded throughout the world. We welcome all who would join us in this pursuit.","accessed":{"date-parts":[[2022,9,20]]},"author":[{"family":"Sutton","given":"Richard S."},{"family":"Bowling","given":"Michael H."},{"family":"Pilarski","given":"Patrick M."}],"citation-key":"suttonAlbertaPlanAI2022","issued":{"date-parts":[[2022,8,23]]},"number":"arXiv:2208.11173","publisher":"arXiv","source":"arXiv.org","title":"The Alberta Plan for AI Research","type":"article","URL":"http://arxiv.org/abs/2208.11173"},
  {"id":"szegedyGoingDeeperConvolutions2014","abstract":"We propose a deep convolutional neural network architecture codenamed \"Inception\", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.","accessed":{"date-parts":[[2023,4,29]]},"author":[{"family":"Szegedy","given":"Christian"},{"family":"Liu","given":"Wei"},{"family":"Jia","given":"Yangqing"},{"family":"Sermanet","given":"Pierre"},{"family":"Reed","given":"Scott"},{"family":"Anguelov","given":"Dragomir"},{"family":"Erhan","given":"Dumitru"},{"family":"Vanhoucke","given":"Vincent"},{"family":"Rabinovich","given":"Andrew"}],"citation-key":"szegedyGoingDeeperConvolutions2014","DOI":"10.48550/arXiv.1409.4842","issued":{"date-parts":[[2014,9,16]]},"number":"arXiv:1409.4842","publisher":"arXiv","source":"arXiv.org","title":"Going Deeper with Convolutions","type":"article","URL":"http://arxiv.org/abs/1409.4842"},
  {"id":"taatgenTracesTimesRepresentations2011","abstract":"Theories of time perception typically assume that some sort of memory represents time intervals. This memory component is typically underdeveloped in theories of time perception. Following earlier work that suggested that representations of different time intervals contaminate each other (Grondin, 2005; Jazayeri & Shadlen, 2010; Jones & Wearden, 2004), an experiment was conducted in which subjects had to alternate in reproducing two intervals. In two conditions of the experiment, the duration of one of the intervals changed over the experiment, forcing subjects to adjust their representation of that interval, while keeping the other constant. The results show that the adjustment of one interval carried over to the other interval, indicating that subjects were not able to completely separate the two representations. We propose a temporal reference memory that is based on existing memory models (Anderson, 1990). Our model assumes that the representation of an interval is based on a pool of recent experiences. In a series of simulations, we show that our pool model fits the data, while two alternative models that have previously been proposed do not.","author":[{"family":"Taatgen","given":"Niels"},{"family":"Rijn","given":"Hedderik"}],"citation-key":"taatgenTracesTimesRepresentations2011","container-title":"Memory & cognition","container-title-short":"Memory & cognition","DOI":"10.3758/s13421-011-0113-0","issued":{"date-parts":[[2011,5,28]]},"page":"1546-60","source":"ResearchGate","title":"Traces of times past: Representations of temporal intervals in memory","title-short":"Traces of times past","type":"article-journal","volume":"39"},
  {"id":"takahashiDataAugmentationUsing2020","abstract":"Deep convolutional neural networks (CNNs) have achieved remarkable results in image processing tasks. However, their high expression ability risks overfitting. Consequently, data augmentation techniques have been proposed to prevent overfitting while enriching datasets. Recent CNN architectures with more parameters are rendering traditional data augmentation techniques insufficient. In this study, we propose a new data augmentation technique called random image cropping and patching (RICAP) which randomly crops four images and patches them to create a new training image. Moreover, RICAP mixes the class labels of the four images, resulting in an advantage similar to label smoothing. We evaluated RICAP with current state-of-the-art CNNs (e.g., the shake-shake regularization model) by comparison with competitive data augmentation techniques such as cutout and mixup. RICAP achieves a new state-of-the-art test error of $2.19\\%$ on CIFAR-10. We also confirmed that deep CNNs with RICAP achieve better results on classification tasks using CIFAR-100 and ImageNet and an image-caption retrieval task using Microsoft COCO.","accessed":{"date-parts":[[2023,3,30]]},"author":[{"family":"Takahashi","given":"Ryo"},{"family":"Matsubara","given":"Takashi"},{"family":"Uehara","given":"Kuniaki"}],"citation-key":"takahashiDataAugmentationUsing2020","container-title":"IEEE Transactions on Circuits and Systems for Video Technology","container-title-short":"IEEE Trans. Circuits Syst. Video Technol.","DOI":"10.1109/TCSVT.2019.2935128","ISSN":"1051-8215, 1558-2205","issue":"9","issued":{"date-parts":[[2020,9]]},"page":"2917-2931","source":"arXiv.org","title":"Data Augmentation using Random Image Cropping and Patching for Deep CNNs","type":"article-journal","URL":"http://arxiv.org/abs/1811.09030","volume":"30"},
  {"id":"tal-perrySpatiotemporalLinkTemporal2022","accessed":{"date-parts":[[2023,1,25]]},"author":[{"family":"Tal-Perry","given":"Noam"},{"family":"Yuval-Greenberg","given":"Shlomit"}],"citation-key":"tal-perrySpatiotemporalLinkTemporal2022","container-title":"The Journal of Neuroscience","container-title-short":"J. Neurosci.","DOI":"10.1523/JNEUROSCI.1555-21.2022","ISSN":"0270-6474, 1529-2401","issue":"12","issued":{"date-parts":[[2022,3,23]]},"language":"en","page":"2516-2523","source":"DOI.org (Crossref)","title":"The Spatiotemporal Link of Temporal Expectations: Contextual Temporal Expectation Is Independent of Spatial Attention","title-short":"The Spatiotemporal Link of Temporal Expectations","type":"article-journal","URL":"https://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.1555-21.2022","volume":"42"},
  {"id":"tanEfficientnetRethinkingModel2019","author":[{"family":"Tan","given":"Mingxing"},{"family":"Le","given":"Quoc"}],"citation-key":"tanEfficientnetRethinkingModel2019","container-title":"International Conference on Machine Learning","issued":{"date-parts":[[2019]]},"page":"6105–6114","publisher":"PMLR","title":"Efficientnet: Rethinking model scaling for convolutional neural networks","type":"paper-conference"},
  {"id":"teneyMultiviewFeatureDistributions2014","author":[{"family":"Teney","given":"Damien"},{"family":"Piater","given":"Justus"}],"citation-key":"teneyMultiviewFeatureDistributions2014","container-title":"Computer Vision and Image Understanding","issued":{"date-parts":[[2014]]},"page":"265–282","publisher":"Elsevier","title":"Multiview feature distributions for object detection and continuous pose estimation","type":"article-journal","volume":"125"},
  {"id":"theprecise4qconsortiumExplainabilityArtificialIntelligence2020","abstract":"Abstract\n            \n              Background\n              Explainability is one of the most heavily debated topics when it comes to the application of artificial intelligence (AI) in healthcare. Even though AI-driven systems have been shown to outperform humans in certain analytical tasks, the lack of explainability continues to spark criticism. Yet, explainability is not a purely technological issue, instead it invokes a host of medical, legal, ethical, and societal questions that require thorough exploration. This paper provides a comprehensive assessment of the role of explainability in medical AI and makes an ethical evaluation of what explainability means for the adoption of AI-driven tools into clinical practice.\n            \n            \n              Methods\n              Taking AI-based clinical decision support systems as a case in point, we adopted a multidisciplinary approach to analyze the relevance of explainability for medical AI from the technological, legal, medical, and patient perspectives. Drawing on the findings of this conceptual analysis, we then conducted an ethical assessment using the “Principles of Biomedical Ethics” by Beauchamp and Childress (autonomy, beneficence, nonmaleficence, and justice) as an analytical framework to determine the need for explainability in medical AI.\n            \n            \n              Results\n              Each of the domains highlights a different set of core considerations and values that are relevant for understanding the role of explainability in clinical practice. From the technological point of view, explainability has to be considered both in terms how it can be achieved and what is beneficial from a development perspective. When looking at the legal perspective we identified informed consent, certification and approval as medical devices, and liability as core touchpoints for explainability. Both the medical and patient perspectives emphasize the importance of considering the interplay between human actors and medical AI. We conclude that omitting explainability in clinical decision support systems poses a threat to core ethical values in medicine and may have detrimental consequences for individual and public health.\n            \n            \n              Conclusions\n              To ensure that medical AI lives up to its promises, there is a need to sensitize developers, healthcare professionals, and legislators to the challenges and limitations of opaque algorithms in medical AI and to foster multidisciplinary collaboration moving forward.","accessed":{"date-parts":[[2023,1,17]]},"author":[{"literal":"the Precise4Q consortium"},{"family":"Amann","given":"Julia"},{"family":"Blasimme","given":"Alessandro"},{"family":"Vayena","given":"Effy"},{"family":"Frey","given":"Dietmar"},{"family":"Madai","given":"Vince I."}],"citation-key":"theprecise4qconsortiumExplainabilityArtificialIntelligence2020","container-title":"BMC Medical Informatics and Decision Making","container-title-short":"BMC Med Inform Decis Mak","DOI":"10.1186/s12911-020-01332-6","ISSN":"1472-6947","issue":"1","issued":{"date-parts":[[2020,12]]},"language":"en","page":"310","source":"DOI.org (Crossref)","title":"Explainability for artificial intelligence in healthcare: a multidisciplinary perspective","title-short":"Explainability for artificial intelligence in healthcare","type":"article-journal","URL":"https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-020-01332-6","volume":"20"},
  {"id":"thrunProbabilisticRobotics2002","author":[{"family":"Thrun","given":"Sebastian"}],"citation-key":"thrunProbabilisticRobotics2002","container-title":"Communications of the ACM","issue":"3","issued":{"date-parts":[[2002]]},"page":"52–57","publisher":"ACM New York, NY, USA","title":"Probabilistic robotics","type":"article-journal","volume":"45"},
  {"id":"tommasiDeeperLookDataset2017","abstract":"The presence of a bias in each image data collection has recently attracted a lot of attention in the computer vision community showing the limits in generalization of any learning method trained on a specific dataset. At the same time, with the rapid development of deep learning architectures, the activation values of Convolutional Neural Networks (CNN) are emerging as reliable and robust image descriptors. In this chapter we propose to verify the potential of the CNN features when facing the dataset biasDataset biasproblem. With this purpose we introduce a large testbed for cross-dataset analysis and we discuss the challenges faced to create two comprehensive experimental setups by aligning twelve existing image databases. We conduct a series of analyses looking at how the datasets differ among each other and verifying the performance of existing debiasing methods under different representations. We learn important lessons on which part of the dataset bias problem can be considered solved and which open questions still need to be tackled.","accessed":{"date-parts":[[2022,9,6]]},"author":[{"family":"Tommasi","given":"Tatiana"},{"family":"Patricia","given":"Novi"},{"family":"Caputo","given":"Barbara"},{"family":"Tuytelaars","given":"Tinne"}],"citation-key":"tommasiDeeperLookDataset2017","collection-title":"Advances in Computer Vision and Pattern Recognition","container-title":"Domain Adaptation in Computer Vision Applications","DOI":"10.1007/978-3-319-58347-1_2","editor":[{"family":"Csurka","given":"Gabriela"}],"event-place":"Cham","ISBN":"978-3-319-58347-1","issued":{"date-parts":[[2017]]},"language":"en","page":"37-55","publisher":"Springer International Publishing","publisher-place":"Cham","source":"Springer Link","title":"A Deeper Look at Dataset Bias","type":"chapter","URL":"https://doi.org/10.1007/978-3-319-58347-1_2"},
  {"id":"torralbaUnbiasedLookDataset2011","accessed":{"date-parts":[[2023,1,18]]},"author":[{"family":"Torralba","given":"Antonio"},{"family":"Efros","given":"Alexei A."}],"citation-key":"torralbaUnbiasedLookDataset2011","container-title":"CVPR 2011","DOI":"10.1109/CVPR.2011.5995347","event-place":"Colorado Springs, CO, USA","event-title":"2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","ISBN":"978-1-4577-0394-2","issued":{"date-parts":[[2011,6]]},"page":"1521-1528","publisher":"IEEE","publisher-place":"Colorado Springs, CO, USA","source":"DOI.org (Crossref)","title":"Unbiased look at dataset bias","type":"paper-conference","URL":"http://ieeexplore.ieee.org/document/5995347/"},
  {"id":"turelTimeDistortionWhen2018","abstract":"Background: There is a growing concern over the addictiveness of Social Media use. Additional representative indicators of impaired control are needed in order to distinguish presumed social media addiction from normal use. Aims: (1) To examine the existence of time distortion during non-social-media use tasks that involve social media cues among those who may be considered at-risk for social media addiction. (2) To examine the usefulness of this distortion for at-risk vs. low/no-risk classification.\nMethod: We used a task that prevented Facebook use and invoked Facebook reflections (survey on self-control strategies) and subsequently measured estimated vs. actual task completion time. We captured the level of addiction using the Bergen Facebook Addiction Scale in the survey, and we used a common cutoff criterion to classify people as at-risk vs. low/no-risk of Facebook addiction.\nResults: The at-risk group presented significant upward time estimate bias and the low/no-risk group presented significant downward time estimate bias. The bias was positively correlated with Facebook addiction scores. It was efficacious, especially when combined with self-reported estimates of extent of Facebook use, in classifying people to the two categories.\nConclusions: Our study points to a novel, easy to obtain, and useful marker of at-risk for social media addiction, which may be considered for inclusion in diagnosis tools and procedures.","accessed":{"date-parts":[[2022,6,15]]},"author":[{"family":"Turel","given":"Ofir"},{"family":"Brevers","given":"Damien"},{"family":"Bechara","given":"Antoine"}],"citation-key":"turelTimeDistortionWhen2018","container-title":"Journal of Psychiatric Research","container-title-short":"Journal of Psychiatric Research","DOI":"10.1016/j.jpsychires.2017.11.014","ISSN":"00223956","issued":{"date-parts":[[2018,2]]},"language":"en","page":"84-88","source":"DOI.org (Crossref)","title":"Time distortion when users at-risk for social media addiction engage in non-social media tasks","type":"article-journal","URL":"https://linkinghub.elsevier.com/retrieve/pii/S0022395617308750","volume":"97"},
  {"id":"uddinSaliencyMixSaliencyGuided2021","abstract":"Advanced data augmentation strategies have widely been studied to improve the generalization ability of deep learning models. Regional dropout is one of the popular solutions that guides the model to focus on less discriminative parts by randomly removing image regions, resulting in improved regularization. However, such information removal is undesirable. On the other hand, recent strategies suggest to randomly cut and mix patches and their labels among training images, to enjoy the advantages of regional dropout without having any pointless pixel in the augmented images. We argue that such random selection strategies of the patches may not necessarily represent sufficient information about the corresponding object and thereby mixing the labels according to that uninformative patch enables the model to learn unexpected feature representation. Therefore, we propose SaliencyMix that carefully selects a representative image patch with the help of a saliency map and mixes this indicative patch with the target image, thus leading the model to learn more appropriate feature representation. SaliencyMix achieves the best known top-1 error of 21.26% and 20.09% for ResNet-50 and ResNet-101 architectures on ImageNet classification, respectively, and also improves the model robustness against adversarial perturbations. Furthermore, models that are trained with SaliencyMix help to improve the object detection performance. Source code is available at https://github.com/SaliencyMix/SaliencyMix.","accessed":{"date-parts":[[2023,4,11]]},"author":[{"family":"Uddin","given":"A. F. M. Shahab"},{"family":"Monira","given":"Mst Sirazam"},{"family":"Shin","given":"Wheemyung"},{"family":"Chung","given":"TaeChoong"},{"family":"Bae","given":"Sung-Ho"}],"citation-key":"uddinSaliencyMixSaliencyGuided2021","issued":{"date-parts":[[2021,7,27]]},"number":"arXiv:2006.01791","publisher":"arXiv","source":"arXiv.org","title":"SaliencyMix: A Saliency Guided Data Augmentation Strategy for Better Regularization","title-short":"SaliencyMix","type":"article","URL":"http://arxiv.org/abs/2006.01791"},
  {"id":"urbachQuantifiersAreIncrementally2015","abstract":"Language interpretation is often assumed to be incremental. However, our studies of quantiﬁer expressions in isolated sentences found N400 event-related brain potential (ERP) evidence for partial but not full immediate quantiﬁer interpretation (Urbach & Kutas, 2010). Here we tested similar quantiﬁer expressions in pragmatically supporting discourse contexts (Alex was an unusual toddler. Most/Few kids prefer sweets/vegetables . . .) while participants made plausibility judgments (Experiment 1) or read for comprehension (Experiment 2). Control Experiments 3A (plausibility) and 3B (comprehension) removed the discourse contexts. Quantiﬁers always modulated typical and/or atypical word N400 amplitudes. However, the real-time N400 effects only in Experiment 2 mirrored ofﬂine quantiﬁer and typicality crossover interaction effects for plausibility ratings and cloze probabilities. We conclude that quantiﬁer expressions can be interpreted fully and immediately, though pragmatic and task variables appear to impact the speed and/or depth of quantiﬁer interpretation.","accessed":{"date-parts":[[2022,10,24]]},"author":[{"family":"Urbach","given":"Thomas P."},{"family":"DeLong","given":"Katherine A."},{"family":"Kutas","given":"Marta"}],"citation-key":"urbachQuantifiersAreIncrementally2015","container-title":"Journal of Memory and Language","container-title-short":"Journal of Memory and Language","DOI":"10.1016/j.jml.2015.03.010","ISSN":"0749596X","issued":{"date-parts":[[2015,8]]},"language":"en","page":"79-96","source":"DOI.org (Crossref)","title":"Quantifiers are incrementally interpreted in context, more than less","type":"article-journal","URL":"https://linkinghub.elsevier.com/retrieve/pii/S0749596X15000431","volume":"83"},
  {"id":"valdenegro-toroMachineLearningStudents2022","abstract":"Overfitting and generalization is an important concept in Machine Learning as only models that generalize are interesting for general applications. Yet some students have trouble learning this important concept through lectures and exercises. In this paper we describe common examples of students misunderstanding overfitting, and provide recommendations for possible solutions. We cover student misconceptions about overfitting, about solutions to overfitting, and implementation mistakes that are commonly confused with overfitting issues. We expect that our paper can contribute to improving student understanding and lectures about this important topic.","accessed":{"date-parts":[[2023,2,7]]},"author":[{"family":"Valdenegro-Toro","given":"Matias"},{"family":"Sabatelli","given":"Matthia"}],"citation-key":"valdenegro-toroMachineLearningStudents2022","DOI":"10.48550/arXiv.2209.03032","issued":{"date-parts":[[2022,9,7]]},"number":"arXiv:2209.03032","publisher":"arXiv","source":"arXiv.org","title":"Machine Learning Students Overfit to Overfitting","type":"article","URL":"http://arxiv.org/abs/2209.03032"},
  {"id":"vanderveldenExplainableArtificialIntelligence2022","abstract":"With an increase in deep learning-based methods, the call for explainability of such methods grows, especially in high-stakes decision making areas such as medical image analysis. This survey presents an overview of explainable artificial intelligence (XAI) used in deep learning-based medical image analysis. A framework of XAI criteria is introduced to classify deep learning-based medical image analysis methods. Papers on XAI techniques in medical image analysis are then surveyed and categorized according to the framework and according to anatomical location. The paper concludes with an outlook of future opportunities for XAI in medical image analysis.","accessed":{"date-parts":[[2023,2,13]]},"author":[{"family":"Velden","given":"Bas H. M.","non-dropping-particle":"van der"},{"family":"Kuijf","given":"Hugo J."},{"family":"Gilhuijs","given":"Kenneth G. A."},{"family":"Viergever","given":"Max A."}],"citation-key":"vanderveldenExplainableArtificialIntelligence2022","container-title":"Medical Image Analysis","container-title-short":"Medical Image Analysis","DOI":"10.1016/j.media.2022.102470","ISSN":"1361-8415","issued":{"date-parts":[[2022,7,1]]},"language":"en","page":"102470","source":"ScienceDirect","title":"Explainable artificial intelligence (XAI) in deep learning-based medical image analysis","type":"article-journal","URL":"https://www.sciencedirect.com/science/article/pii/S1361841522001177","volume":"79"},
  {"id":"vanlehnRelativeEffectivenessHuman2011","accessed":{"date-parts":[[2022,11,13]]},"author":[{"family":"VanLEHN","given":"Kurt"}],"citation-key":"vanlehnRelativeEffectivenessHuman2011","container-title":"Educational Psychologist","container-title-short":"Educational Psychologist","DOI":"10.1080/00461520.2011.611369","ISSN":"0046-1520, 1532-6985","issue":"4","issued":{"date-parts":[[2011,10]]},"language":"en","page":"197-221","source":"DOI.org (Crossref)","title":"The Relative Effectiveness of Human Tutoring, Intelligent Tutoring Systems, and Other Tutoring Systems","type":"article-journal","URL":"http://www.tandfonline.com/doi/abs/10.1080/00461520.2011.611369","volume":"46"},
  {"id":"vaswaniAttentionAllYou2017","abstract":"The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.","accessed":{"date-parts":[[2022,10,21]]},"author":[{"family":"Vaswani","given":"Ashish"},{"family":"Shazeer","given":"Noam"},{"family":"Parmar","given":"Niki"},{"family":"Uszkoreit","given":"Jakob"},{"family":"Jones","given":"Llion"},{"family":"Gomez","given":"Aidan N."},{"family":"Kaiser","given":"Lukasz"},{"family":"Polosukhin","given":"Illia"}],"citation-key":"vaswaniAttentionAllYou2017","DOI":"10.48550/arXiv.1706.03762","issued":{"date-parts":[[2017,12,5]]},"number":"arXiv:1706.03762","publisher":"arXiv","source":"arXiv.org","title":"Attention Is All You Need","type":"article","URL":"http://arxiv.org/abs/1706.03762"},
  {"id":"vazquezViewpointSelectionUsing2001","author":[{"family":"Vázquez","given":"Pere-Pau"},{"family":"Feixas","given":"Miquel"},{"family":"Sbert","given":"Mateu"},{"family":"Heidrich","given":"Wolfgang"}],"citation-key":"vazquezViewpointSelectionUsing2001","container-title":"VMV","issued":{"date-parts":[[2001]]},"page":"273–280","publisher":"Citeseer","title":"Viewpoint selection using viewpoint entropy.","type":"paper-conference","volume":"1"},
  {"id":"venugopalanItEasyFool2020","abstract":"Confounding variables are a well known source of nuisance in biomedical studies. They present an even greater challenge when we combine them with black-box machine learning techniques that operate on raw data. This work presents two case studies. In one, we discovered biases arising from systematic errors in the data generation process. In the other, we found a spurious source of signal unrelated to the prediction task at hand. In both cases, our prediction models performed well but under careful examination hidden confounders and biases were revealed. These are cautionary tales on the limits of using machine learning techniques on raw data from scientific experiments.","accessed":{"date-parts":[[2022,9,6]]},"author":[{"family":"Venugopalan","given":"Subhashini"},{"family":"Narayanaswamy","given":"Arunachalam"},{"family":"Yang","given":"Samuel"},{"family":"Geraschenko","given":"Anton"},{"family":"Lipnick","given":"Scott"},{"family":"Makhortova","given":"Nina"},{"family":"Hawrot","given":"James"},{"family":"Marques","given":"Christine"},{"family":"Pereira","given":"Joao"},{"family":"Brenner","given":"Michael"},{"family":"Rubin","given":"Lee"},{"family":"Wainger","given":"Brian"},{"family":"Berndl","given":"Marc"}],"citation-key":"venugopalanItEasyFool2020","DOI":"10.48550/arXiv.1912.07661","issued":{"date-parts":[[2020,4,6]]},"number":"arXiv:1912.07661","publisher":"arXiv","source":"arXiv.org","title":"It's easy to fool yourself: Case studies on identifying bias and confounding in bio-medical datasets","title-short":"It's easy to fool yourself","type":"article","URL":"http://arxiv.org/abs/1912.07661"},
  {"id":"VisualizingImpactFeature","accessed":{"date-parts":[[2022,7,31]]},"citation-key":"VisualizingImpactFeature","title":"Visualizing the Impact of Feature Attribution Baselines","type":"webpage","URL":"https://distill.pub/2020/attribution-baselines/"},
  {"id":"walawalkarAttentiveCutMixEnhanced2020","abstract":"Convolutional neural networks (CNN) are capable of learning robust representation with different regularization methods and activations as convolutional layers are spatially correlated. Based on this property, a large variety of regional dropout strategies have been proposed, such as Cutout, DropBlock, CutMix, etc. These methods aim to promote the network to generalize better by partially occluding the discriminative parts of objects. However, all of them perform this operation randomly, without capturing the most important region(s) within an object. In this paper, we propose Attentive CutMix, a naturally enhanced augmentation strategy based on CutMix. In each training iteration, we choose the most descriptive regions based on the intermediate attention maps from a feature extractor, which enables searching for the most discriminative parts in an image. Our proposed method is simple yet effective, easy to implement and can boost the baseline significantly. Extensive experiments on CIFAR-10/100, ImageNet datasets with various CNN architectures (in a unified setting) demonstrate the effectiveness of our proposed method, which consistently outperforms the baseline CutMix and other methods by a significant margin.","accessed":{"date-parts":[[2023,3,29]]},"author":[{"family":"Walawalkar","given":"Devesh"},{"family":"Shen","given":"Zhiqiang"},{"family":"Liu","given":"Zechun"},{"family":"Savvides","given":"Marios"}],"citation-key":"walawalkarAttentiveCutMixEnhanced2020","DOI":"10.48550/arXiv.2003.13048","issued":{"date-parts":[[2020,4,5]]},"number":"arXiv:2003.13048","publisher":"arXiv","source":"arXiv.org","title":"Attentive CutMix: An Enhanced Data Augmentation Approach for Deep Learning Based Image Classification","title-short":"Attentive CutMix","type":"article","URL":"http://arxiv.org/abs/2003.13048"},
  {"id":"wangEntailmentFewShotLearner2021","abstract":"Large pre-trained language models (LMs) have demonstrated remarkable ability as few-shot learners. However, their success hinges largely on scaling model parameters to a degree that makes it challenging to train and serve. In this paper, we propose a new approach, named as EFL, that can turn small LMs into better few-shot learners. The key idea of this approach is to reformulate potential NLP task into an entailment one, and then fine-tune the model with as little as 8 examples. We further demonstrate our proposed method can be: (i) naturally combined with an unsupervised contrastive learning-based data augmentation method; (ii) easily extended to multilingual few-shot learning. A systematic evaluation on 18 standard NLP tasks demonstrates that this approach improves the various existing SOTA few-shot learning methods by 12\\%, and yields competitive few-shot performance with 500 times larger models, such as GPT-3.","accessed":{"date-parts":[[2023,4,18]]},"author":[{"family":"Wang","given":"Sinong"},{"family":"Fang","given":"Han"},{"family":"Khabsa","given":"Madian"},{"family":"Mao","given":"Hanzi"},{"family":"Ma","given":"Hao"}],"citation-key":"wangEntailmentFewShotLearner2021","issued":{"date-parts":[[2021,4,29]]},"number":"arXiv:2104.14690","publisher":"arXiv","source":"arXiv.org","title":"Entailment as Few-Shot Learner","type":"article","URL":"http://arxiv.org/abs/2104.14690"},
  {"id":"wangMedicalImageSegmentation2022","abstract":"Deep learning has been widely used for medical image segmentation and a large number of papers has been presented recording the success of deep learning in the field. A comprehensive thematic survey on medical image segmentation using deep learning techniques is presented. This paper makes two original contributions. Firstly, compared to traditional surveys that directly divide literatures of deep learning on medical image segmentation into many groups and introduce literatures in detail for each group, we classify currently popular literatures according to a multi-level structure from coarse to fine. Secondly, this paper focuses on supervised and weakly supervised learning approaches, without including unsupervised approaches since they have been introduced in many old surveys and they are not popular currently. For supervised learning approaches, we analyse literatures in three aspects: the selection of backbone networks, the design of network blocks, and the improvement of loss functions. For weakly supervised learning approaches, we investigate literature according to data augmentation, transfer learning, and interactive segmentation, separately. Compared to existing surveys, this survey classifies the literatures very differently from before and is more convenient for readers to understand the relevant rationale and will guide them to think of appropriate improvements in medical image segmentation based on deep learning approaches.","accessed":{"date-parts":[[2022,8,16]]},"author":[{"family":"Wang","given":"Risheng"},{"family":"Lei","given":"Tao"},{"family":"Cui","given":"Ruixia"},{"family":"Zhang","given":"Bingtao"},{"family":"Meng","given":"Hongying"},{"family":"Nandi","given":"Asoke K."}],"citation-key":"wangMedicalImageSegmentation2022","container-title":"IET Image Processing","DOI":"10.1049/ipr2.12419","ISSN":"1751-9667","issue":"5","issued":{"date-parts":[[2022]]},"language":"en","page":"1243-1267","source":"Wiley Online Library","title":"Medical image segmentation using deep learning: A survey","title-short":"Medical image segmentation using deep learning","type":"article-journal","URL":"https://onlinelibrary.wiley.com/doi/abs/10.1049/ipr2.12419","volume":"16"},
  {"id":"wangNormalNetVoxelbasedCNN2019","accessed":{"date-parts":[[2022,5,10]]},"author":[{"family":"Wang","given":"Cheng"},{"family":"Cheng","given":"Ming"},{"family":"Sohel","given":"Ferdous"},{"family":"Bennamoun","given":"Mohammed"},{"family":"Li","given":"Jonathan"}],"citation-key":"wangNormalNetVoxelbasedCNN2019","container-title":"Neurocomputing","container-title-short":"Neurocomputing","DOI":"10.1016/j.neucom.2018.09.075","ISSN":"09252312","issued":{"date-parts":[[2019,1]]},"language":"en","page":"139-147","source":"DOI.org (Crossref)","title":"NormalNet: A voxel-based CNN for 3D object classification and retrieval","title-short":"NormalNet","type":"article-journal","URL":"https://linkinghub.elsevier.com/retrieve/pii/S0925231218311561","volume":"323"},
  {"id":"wangScoreCAMScoreWeightedVisual2020","abstract":"Recently, increasing attention has been drawn to the internal mechanisms of convolutional neural networks, and the reason why the network makes specific decisions. In this paper, we develop a novel post-hoc visual explanation method called Score-CAM based on class activation mapping. Unlike previous class activation mapping based approaches, Score-CAM gets rid of the dependence on gradients by obtaining the weight of each activation map through its forward passing score on target class, the final result is obtained by a linear combination of weights and activation maps. We demonstrate that Score-CAM achieves better visual performance and fairness for interpreting the decision making process. Our approach outperforms previous methods on both recognition and localization tasks, it also passes the sanity check. We also indicate its application as debugging tools. Official code has been released.","accessed":{"date-parts":[[2023,2,16]]},"author":[{"family":"Wang","given":"Haofan"},{"family":"Wang","given":"Zifan"},{"family":"Du","given":"Mengnan"},{"family":"Yang","given":"Fan"},{"family":"Zhang","given":"Zijian"},{"family":"Ding","given":"Sirui"},{"family":"Mardziel","given":"Piotr"},{"family":"Hu","given":"Xia"}],"citation-key":"wangScoreCAMScoreWeightedVisual2020","issued":{"date-parts":[[2020,4,13]]},"number":"arXiv:1910.01279","publisher":"arXiv","source":"arXiv.org","title":"Score-CAM: Score-Weighted Visual Explanations for Convolutional Neural Networks","title-short":"Score-CAM","type":"article","URL":"http://arxiv.org/abs/1910.01279","version":"2"},
  {"id":"wangScoreCAMScoreWeightedVisual2020a","accessed":{"date-parts":[[2023,2,20]]},"author":[{"family":"Wang","given":"Haofan"},{"family":"Wang","given":"Zifan"},{"family":"Du","given":"Mengnan"},{"family":"Yang","given":"Fan"},{"family":"Zhang","given":"Zijian"},{"family":"Ding","given":"Sirui"},{"family":"Mardziel","given":"Piotr"},{"family":"Hu","given":"Xia"}],"citation-key":"wangScoreCAMScoreWeightedVisual2020a","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops","issued":{"date-parts":[[2020]]},"page":"24-25","source":"openaccess.thecvf.com","title":"Score-CAM: Score-Weighted Visual Explanations for Convolutional Neural Networks","title-short":"Score-CAM","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_CVPRW_2020/html/w1/Wang_Score-CAM_Score-Weighted_Visual_Explanations_for_Convolutional_Neural_Networks_CVPRW_2020_paper.html"},
  {"id":"weilnhammerPredictiveCodingAccount2017","abstract":"In bistable vision, subjective perception wavers between two interpretations of a constant ambiguous stimulus. This dissociation between conscious perception and sensory stimulation has motivated various empirical studies on the neural correlates of bistable perception, but the neurocomputational mechanism behind endogenous perceptual transitions has remained elusive. Here, we recurred to a generic Bayesian framework of predictive coding and devised a model that casts endogenous perceptual transitions as a consequence of prediction errors emerging from residual evidence for the suppressed percept. Data simulations revealed close similarities between the model’s predictions and key temporal characteristics of perceptual bistability, indicating that the model was able to reproduce bistable perception. Fitting the predictive coding model to behavioural data from an fMRI-experiment on bistable perception, we found a correlation across participants between the model parameter encoding perceptual stabilization and the behaviourally measured frequency of perceptual transitions, corroborating that the model successfully accounted for participants’ perception. Formal model comparison with established models of bistable perception based on mutual inhibition and adaptation, noise or a combination of adaptation and noise was used for the validation of the predictive coding model against the established models. Most importantly, model-based analyses of the fMRI data revealed that prediction error timecourses derived from the predictive coding model correlated with neural signal time-courses in bilateral inferior frontal gyri and anterior insulae. Voxel-wise model selection indicated a superiority of the predictive coding model over conventional analysis approaches in explaining neural activity in these frontal areas, suggesting that frontal cortex encodes prediction errors that mediate endogenous perceptual transitions in bistable perception. Taken together, our current work provides a theoretical framework that allows for the analysis of behavioural and neural data using a predictive coding perspective on bistable perception. In this, our approach posits a crucial role of prediction error signalling for the resolution of perceptual ambiguities.","accessed":{"date-parts":[[2022,5,27]]},"author":[{"family":"Weilnhammer","given":"Veith"},{"family":"Stuke","given":"Heiner"},{"family":"Hesselmann","given":"Guido"},{"family":"Sterzer","given":"Philipp"},{"family":"Schmack","given":"Katharina"}],"citation-key":"weilnhammerPredictiveCodingAccount2017","container-title":"PLOS Computational Biology","container-title-short":"PLoS Comput Biol","DOI":"10.1371/journal.pcbi.1005536","editor":[{"family":"Daunizeau","given":"Jean"}],"ISSN":"1553-7358","issue":"5","issued":{"date-parts":[[2017,5,15]]},"language":"en","page":"e1005536","source":"DOI.org (Crossref)","title":"A predictive coding account of bistable perception - a model-based fMRI study","type":"article-journal","URL":"https://dx.plos.org/10.1371/journal.pcbi.1005536","volume":"13"},
  {"id":"weiTransferableAdversarialAttacks2022","abstract":"Vision transformers (ViTs) have demonstrated impressive performance on a series of computer vision tasks, yet they still suffer from adversarial examples. % crafted in a similar fashion as CNNs. In this paper, we posit that adversarial attacks on transformers should be specially tailored for their architecture, jointly considering both patches and self-attention, in order to achieve high transferability. More specifically, we introduce a dual attack framework, which contains a Pay No Attention (PNA) attack and a PatchOut attack, to improve the transferability of adversarial samples across different ViTs. We show that skipping the gradients of attention during backpropagation can generate adversarial examples with high transferability. In addition, adversarial perturbations generated by optimizing randomly sampled subsets of patches at each iteration achieve higher attack success rates than attacks using all patches. We evaluate the transferability of attacks on state-of-the-art ViTs, CNNs and robustly trained CNNs. The results of these experiments demonstrate that the proposed dual attack can greatly boost transferability between ViTs and from ViTs to CNNs. In addition, the proposed method can easily be combined with existing transfer methods to boost performance. Code is available at https://github.com/zhipeng-wei/PNA-PatchOut.","accessed":{"date-parts":[[2022,11,19]]},"author":[{"family":"Wei","given":"Zhipeng"},{"family":"Chen","given":"Jingjing"},{"family":"Goldblum","given":"Micah"},{"family":"Wu","given":"Zuxuan"},{"family":"Goldstein","given":"Tom"},{"family":"Jiang","given":"Yu-Gang"}],"citation-key":"weiTransferableAdversarialAttacks2022","DOI":"10.48550/arXiv.2109.04176","issued":{"date-parts":[[2022,1,2]]},"number":"arXiv:2109.04176","publisher":"arXiv","source":"arXiv.org","title":"Towards Transferable Adversarial Attacks on Vision Transformers","type":"article","URL":"http://arxiv.org/abs/2109.04176"},
  {"id":"wickramanayakeExplanationbasedDataAugmentation","abstract":"Existing works have generated explanations for deep neural network decisions to provide insights into model behavior. We observe that these explanations can also be used to identify concepts that caused misclassiﬁcations. This allows us to understand the possible limitations of the dataset used to train the model, particularly the under-represented regions in the dataset. This work proposes a framework that utilizes concept-based explanations to automatically augment the dataset with new images that can cover these under-represented regions to improve the model performance. The framework is able to use the explanations generated by both interpretable classiﬁers and post-hoc explanations from black-box classiﬁers. Experiment results demonstrate that the proposed approach improves the accuracy of classiﬁers compared to state-of-the-art augmentation strategies.","author":[{"family":"Wickramanayake","given":"Sandareka"},{"family":"Lee","given":"Mong Li"},{"family":"Hsu","given":"Wynne"}],"citation-key":"wickramanayakeExplanationbasedDataAugmentation","language":"en","page":"12","source":"Zotero","title":"Explanation-based Data Augmentation for Image Classification","type":"article-journal"},
  {"id":"wickramanayakeExplanationbasedDataAugmentationa","abstract":"Existing works have generated explanations for deep neural network decisions to provide insights into model behavior. We observe that these explanations can also be used to identify concepts that caused misclassiﬁcations. This allows us to understand the possible limitations of the dataset used to train the model, particularly the under-represented regions in the dataset. This work proposes a framework that utilizes concept-based explanations to automatically augment the dataset with new images that can cover these under-represented regions to improve the model performance. The framework is able to use the explanations generated by both interpretable classiﬁers and post-hoc explanations from black-box classiﬁers. Experiment results demonstrate that the proposed approach improves the accuracy of classiﬁers compared to state-of-the-art augmentation strategies.","author":[{"family":"Wickramanayake","given":"Sandareka"},{"family":"Lee","given":"Mong Li"},{"family":"Hsu","given":"Wynne"}],"citation-key":"wickramanayakeExplanationbasedDataAugmentationa","language":"en","source":"Zotero","title":"Explanation-based Data Augmentation for Image Classification","type":"article-journal"},
  {"id":"wightmanRwightmanPytorchimagemodelsV02023","abstract":"Feb 7, 2023 New inference benchmark numbers added in results folder. Add convnext LAION CLIP trained weights and initial set of in1k fine-tunes convnext_base.clip_laion2b_augreg_ft_in1k - 86.2% @ 256x256 convnext_base.clip_laiona_augreg_ft_in1k_384 - 86.5% @ 384x384 convnext_large_mlp.clip_laion2b_augreg_ft_in1k - 87.3% @ 256x256 convnext_large_mlp.clip_laion2b_augreg_ft_in1k_384 - 87.9% @ 384x384 Add DaViT models. Supports features_only=True. Adapted from https://github.com/dingmyu/davit by Fredo. Use a common NormMlpClassifierHead across MaxViT, ConvNeXt, DaViT Add EfficientFormer-V2 model, update EfficientFormer, and refactor LeViT (closely related architectures). Weights on HF hub. New EfficientFormer-V2 arch, significant refactor from original at (https://github.com/snap-research/EfficientFormer). Supports features_only=True. Minor updates to EfficientFormer. Refactor LeViT models to stages, add features_only=True support to new conv variants, weight remap required. Move ImageNet meta-data (synsets, indices) from /results to timm/data/_info. Add ImageNetInfo / DatasetInfo classes to provide labelling for various ImageNet classifier layouts in timm Update inference.py to use, try: python inference.py /folder/to/images --model convnext_small.in12k --label-type detail --topk 5 Ready for 0.8.10 pypi pre-release (final testing). Jan 20, 2023 Add two convnext 12k -> 1k fine-tunes at 384x384 convnext_tiny.in12k_ft_in1k_384 - 85.1 @ 384 convnext_small.in12k_ft_in1k_384 - 86.2 @ 384 Push all MaxxViT weights to HF hub, and add new ImageNet-12k -> 1k fine-tunes for rw base MaxViT and CoAtNet 1/2 models model top1 top5 samples / sec Params (M) GMAC Act (M) maxvit_xlarge_tf_512.in21k_ft_in1k 88.53 98.64 21.76 475.77 534.14 1413.22 maxvit_xlarge_tf_384.in21k_ft_in1k 88.32 98.54 42.53 475.32 292.78 668.76 maxvit_base_tf_512.in21k_ft_in1k 88.20 98.53 50.87 119.88 138.02 703.99 maxvit_large_tf_512.in21k_ft_in1k 88.04 98.40 36.42 212.33 244.75 942.15 maxvit_large_tf_384.in21k_ft_in1k 87.98 98.56 71.75 212.03 132.55 445.84 maxvit_base_tf_384.in21k_ft_in1k 87.92 98.54 104.71 119.65 73.80 332.90 maxvit_rmlp_base_rw_384.sw_in12k_ft_in1k 87.81 98.37 106.55 116.14 70.97 318.95 maxxvitv2_rmlp_base_rw_384.sw_in12k_ft_in1k 87.47 98.37 149.49 116.09 72.98 213.74 coatnet_rmlp_2_rw_384.sw_in12k_ft_in1k 87.39 98.31 160.80 73.88 47.69 209.43 maxvit_rmlp_base_rw_224.sw_in12k_ft_in1k 86.89 98.02 375.86 116.14 23.15 92.64 maxxvitv2_rmlp_base_rw_224.sw_in12k_ft_in1k 86.64 98.02 501.03 116.09 24.20 62.77 maxvit_base_tf_512.in1k 86.60 97.92 50.75 119.88 138.02 703.99 coatnet_2_rw_224.sw_in12k_ft_in1k 86.57 97.89 631.88 73.87 15.09 49.22 maxvit_large_tf_512.in1k 86.52 97.88 36.04 212.33 244.75 942.15 coatnet_rmlp_2_rw_224.sw_in12k_ft_in1k 86.49 97.90 620.58 73.88 15.18 54.78 maxvit_base_tf_384.in1k 86.29 97.80 101.09 119.65 73.80 332.90 maxvit_large_tf_384.in1k 86.23 97.69 70.56 212.03 132.55 445.84 maxvit_small_tf_512.in1k 86.10 97.76 88.63 69.13 67.26 383.77 maxvit_tiny_tf_512.in1k 85.67 97.58 144.25 31.05 33.49 257.59 maxvit_small_tf_384.in1k 85.54 97.46 188.35 69.02 35.87 183.65 maxvit_tiny_tf_384.in1k 85.11 97.38 293.46 30.98 17.53 123.42 maxvit_large_tf_224.in1k 84.93 96.97 247.71 211.79 43.68 127.35 coatnet_rmlp_1_rw2_224.sw_in12k_ft_in1k 84.90 96.96 1025.45 41.72 8.11 40.13 maxvit_base_tf_224.in1k 84.85 96.99 358.25 119.47 24.04 95.01 maxxvit_rmlp_small_rw_256.sw_in1k 84.63 97.06 575.53 66.01 14.67 58.38 coatnet_rmlp_2_rw_224.sw_in1k 84.61 96.74 625.81 73.88 15.18 54.78 maxvit_rmlp_small_rw_224.sw_in1k 84.49 96.76 693.82 64.90 10.75 49.30 maxvit_small_tf_224.in1k 84.43 96.83 647.96 68.93 11.66 53.17 maxvit_rmlp_tiny_rw_256.sw_in1k 84.23 96.78 807.21 29.15 6.77 46.92 coatnet_1_rw_224.sw_in1k 83.62 96.38 989.59 41.72 8.04 34.60 maxvit_tiny_rw_224.sw_in1k 83.50 96.50 1100.53 29.06 5.11 33.11 maxvit_tiny_tf_224.in1k 83.41 96.59 1004.94 30.92 5.60 35.78 coatnet_rmlp_1_rw_224.sw_in1k 83.36 96.45 1093.03 41.69 7.85 35.47 maxxvitv2_nano_rw_256.sw_in1k 83.11 96.33 1276.88 23.70 6.26 23.05 maxxvit_rmlp_nano_rw_256.sw_in1k 83.03 96.34 1341.24 16.78 4.37 26.05 maxvit_rmlp_nano_rw_256.sw_in1k 82.96 96.26 1283.24 15.50 4.47 31.92 maxvit_nano_rw_256.sw_in1k 82.93 96.23 1218.17 15.45 4.46 30.28 coatnet_bn_0_rw_224.sw_in1k 82.39 96.19 1600.14 27.44 4.67 22.04 coatnet_0_rw_224.sw_in1k 82.39 95.84 1831.21 27.44 4.43 18.73 coatnet_rmlp_nano_rw_224.sw_in1k 82.05 95.87 2109.09 15.15 2.62 20.34 coatnext_nano_rw_224.sw_in1k 81.95 95.92 2525.52 14.70 2.47 12.80 coatnet_nano_rw_224.sw_in1k 81.70 95.64 2344.52 15.14 2.41 15.41 maxvit_rmlp_pico_rw_256.sw_in1k 80.53 95.21 1594.71 7.52 1.85 24.86","accessed":{"date-parts":[[2023,5,7]]},"author":[{"family":"Wightman","given":"Ross"},{"family":"Raw","given":"Nathan"},{"family":"Soare","given":"Alexander"},{"family":"Arora","given":"Aman"},{"family":"Ha","given":"Chris"},{"family":"Reich","given":"Christoph"},{"family":"Guan","given":"Fredo"},{"family":"Kaczmarzyk","given":"Jakub"},{"family":"mrT23","given":""},{"family":"Mike","given":""},{"family":"SeeFun","given":""},{"family":"contrastive","given":""},{"family":"Rizin","given":"Mohammed"},{"family":"Kim","given":"Hyeongchan"},{"family":"Kertész","given":"Csaba"},{"family":"Mehta","given":"Dushyant"},{"family":"Cucurull","given":"Guillem"},{"family":"Singh","given":"Kushajveer"},{"family":"hankyul","given":""},{"family":"Tatsunami","given":"Yuki"},{"family":"Lavin","given":"Andrew"},{"family":"Zhuang","given":"Juntang"},{"family":"Hollemans","given":"Matthijs"},{"family":"Rashad","given":"Mohamed"},{"family":"Sameni","given":"Sepehr"},{"family":"Shults","given":"Vyacheslav"},{"family":"Lucain","given":""},{"family":"Wang","given":"Xiao"},{"family":"Kwon","given":"Yonghye"},{"family":"Uchida","given":"Yusuke"}],"citation-key":"wightmanRwightmanPytorchimagemodelsV02023","DOI":"10.5281/zenodo.7618837","issued":{"date-parts":[[2023,2,7]]},"publisher":"Zenodo","source":"Zenodo","title":"rwightman/pytorch-image-models: v0.8.10dev0 Release","title-short":"rwightman/pytorch-image-models","type":"software","URL":"https://zenodo.org/record/7618837"},
  {"id":"wu3DShapenetsDeep2015","author":[{"family":"Wu","given":"Zhirong"},{"family":"Song","given":"Shuran"},{"family":"Khosla","given":"Aditya"},{"family":"Yu","given":"Fisher"},{"family":"Zhang","given":"Linguang"},{"family":"Tang","given":"Xiaoou"},{"family":"Xiao","given":"Jianxiong"}],"citation-key":"wu3DShapenetsDeep2015","container-title":"Proceedings of the IEEE conference on computer vision and pattern recognition","issued":{"date-parts":[[2015]]},"page":"1912–1920","title":"3D shapenets: A deep representation for volumetric shapes","type":"paper-conference"},
  {"id":"wuLearningProbabilisticLatent2016","author":[{"family":"Wu","given":"Jiajun"},{"family":"Zhang","given":"Chengkai"},{"family":"Xue","given":"Tianfan"},{"family":"Freeman","given":"William T"},{"family":"Tenenbaum","given":"Joshua B"}],"citation-key":"wuLearningProbabilisticLatent2016","container-title":"arXiv preprint arXiv:1610.07584","issued":{"date-parts":[[2016]]},"title":"Learning a probabilistic latent space of object shapes via 3D generative-adversarial modeling","type":"article-journal"},
  {"id":"xiaoOffsiteTuningTransferLearning2023","abstract":"Transfer learning is important for foundation models to adapt to downstream tasks. However, many foundation models are proprietary, so users must share their data with model owners to fine-tune the models, which is costly and raise privacy concerns. Moreover, fine-tuning large foundation models is computation-intensive and impractical for most downstream users. In this paper, we propose Offsite-Tuning, a privacy-preserving and efficient transfer learning framework that can adapt billion-parameter foundation models to downstream data without access to the full model. In offsite-tuning, the model owner sends a light-weight adapter and a lossy compressed emulator to the data owner, who then fine-tunes the adapter on the downstream data with the emulator's assistance. The fine-tuned adapter is then returned to the model owner, who plugs it into the full model to create an adapted foundation model. Offsite-tuning preserves both parties' privacy and is computationally more efficient than the existing fine-tuning methods that require access to the full model weights. We demonstrate the effectiveness of offsite-tuning on various large language and vision foundation models. Offsite-tuning can achieve comparable accuracy as full model fine-tuning while being privacy-preserving and efficient, achieving 6.5x speedup and 5.6x memory reduction. Code is available at https://github.com/mit-han-lab/offsite-tuning.","accessed":{"date-parts":[[2023,2,13]]},"author":[{"family":"Xiao","given":"Guangxuan"},{"family":"Lin","given":"Ji"},{"family":"Han","given":"Song"}],"citation-key":"xiaoOffsiteTuningTransferLearning2023","DOI":"10.48550/arXiv.2302.04870","issued":{"date-parts":[[2023,2,9]]},"number":"arXiv:2302.04870","publisher":"arXiv","source":"arXiv.org","title":"Offsite-Tuning: Transfer Learning without Full Model","title-short":"Offsite-Tuning","type":"article","URL":"http://arxiv.org/abs/2302.04870"},
  {"id":"xuanMVC3DSpatialCorrelated2019","abstract":"As the development of deep neural networks, 3D object recognition is becoming increasingly popular in the computer vision community. Many multi-view-based methods are proposed to improve the category recognition accuracy. These approaches mainly rely on multi-view images that are rendered with the whole circumference. In real-world applications, however, 3D objects are mostly observed from partial viewpoints in a less range. Therefore, we propose a multi-view-based 3D convolutional neural network that takes only part of contiguous multi-view images as input and can still maintain high accuracy. Moreover, our model takes these view images as a joint variable to better learn the spatially correlated features using 3D convolution and 3D max-pooling layers. The experimental results on ModelNet10 and ModelNet40 datasets show that our MV-C3D technique can achieve outstanding performance with multi-view images that are captured from partial angles with less range. The results on 3D-rotated real image dataset MIRO further demonstrate that MV-C3D is more adaptable in real-world scenarios. The classification accuracy can be further improved with the increasing number of view images.","author":[{"family":"Xuan","given":"Qi"},{"family":"Li","given":"Fuxian"},{"family":"Liu","given":"Yi"},{"family":"Xiang","given":"Yun"}],"citation-key":"xuanMVC3DSpatialCorrelated2019","container-title":"IEEE Access","DOI":"10.1109/ACCESS.2019.2923022","ISSN":"2169-3536","issued":{"date-parts":[[2019]]},"page":"92528-92538","source":"IEEE Xplore","title":"MV-C3D: A Spatial Correlated Multi-View 3D Convolutional Neural Networks","title-short":"MV-C3D","type":"article-journal","volume":"7"},
  {"id":"yamadaDoesRobustnessImageNet2022","abstract":"As clean ImageNet accuracy nears its ceiling, the research community is increasingly more concerned about robust accuracy under distributional shifts. While a variety of methods have been proposed to robustify neural networks, these techniques often target models trained on ImageNet classification. At the same time, it is a common practice to use ImageNet pretrained backbones for downstream tasks such as object detection, semantic segmentation, and image classification from different domains. This raises a question: Can these robust image classifiers transfer robustness to downstream tasks? For object detection and semantic segmentation, we find that a vanilla Swin Transformer, a variant of Vision Transformer tailored for dense prediction tasks, transfers robustness better than Convolutional Neural Networks that are trained to be robust to the corrupted version of ImageNet. For CIFAR10 classification, we find that models that are robustified for ImageNet do not retain robustness when fully fine-tuned. These findings suggest that current robustification techniques tend to emphasize ImageNet evaluations. Moreover, network architecture is a strong source of robustness when we consider transfer learning.","accessed":{"date-parts":[[2022,10,23]]},"author":[{"family":"Yamada","given":"Yutaro"},{"family":"Otani","given":"Mayu"}],"citation-key":"yamadaDoesRobustnessImageNet2022","DOI":"10.48550/arXiv.2204.03934","issued":{"date-parts":[[2022,4,8]]},"number":"arXiv:2204.03934","publisher":"arXiv","source":"arXiv.org","title":"Does Robustness on ImageNet Transfer to Downstream Tasks?","type":"article","URL":"http://arxiv.org/abs/2204.03934"},
  {"id":"yangFairerDatasetsFiltering2020","abstract":"Computer vision technology is being used by many but remains representative of only a few. People have reported misbehavior of computer vision models, including offensive prediction results and lower performance for underrepresented groups. Current computer vision models are typically developed using datasets consisting of manually annotated images or videos; the data and label distributions in these datasets are critical to the models' behavior. In this paper, we examine ImageNet, a large-scale ontology of images that has spurred the development of many modern computer vision methods. We consider three key factors within the person subtree of ImageNet that may lead to problematic behavior in downstream computer vision technology: (1) the stagnant concept vocabulary of WordNet, (2) the attempt at exhaustive illustration of all categories with images, and (3) the inequality of representation in the images within concepts. We seek to illuminate the root causes of these concerns and take the first steps to mitigate them constructively.","accessed":{"date-parts":[[2022,9,6]]},"author":[{"family":"Yang","given":"Kaiyu"},{"family":"Qinami","given":"Klint"},{"family":"Fei-Fei","given":"Li"},{"family":"Deng","given":"Jia"},{"family":"Russakovsky","given":"Olga"}],"citation-key":"yangFairerDatasetsFiltering2020","collection-title":"FAT* '20","container-title":"Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency","DOI":"10.1145/3351095.3375709","event-place":"New York, NY, USA","ISBN":"978-1-4503-6936-7","issued":{"date-parts":[[2020,1,27]]},"page":"547–558","publisher":"Association for Computing Machinery","publisher-place":"New York, NY, USA","source":"ACM Digital Library","title":"Towards fairer datasets: filtering and balancing the distribution of the people subtree in the ImageNet hierarchy","title-short":"Towards fairer datasets","type":"paper-conference","URL":"https://doi.org/10.1145/3351095.3375709"},
  {"id":"yangTransferLearningSelfsupervised2020","abstract":"Pretraining has become a standard technique in computer vision and natural language processing, which usually helps to improve performance substantially. Previously, the most dominant pretraining method is transfer learning (TL), which uses labeled data to learn a good representation network. Recently, a new pretraining approach -- self-supervised learning (SSL) -- has demonstrated promising results on a wide range of applications. SSL does not require annotated labels. It is purely conducted on input data by solving auxiliary tasks defined on the input data examples. The current reported results show that in certain applications, SSL outperforms TL and the other way around in other applications. There has not been a clear understanding on what properties of data and tasks render one approach outperforms the other. Without an informed guideline, ML researchers have to try both methods to find out which one is better empirically. It is usually time-consuming to do so. In this work, we aim to address this problem. We perform a comprehensive comparative study between SSL and TL regarding which one works better under different properties of data and tasks, including domain difference between source and target tasks, the amount of pretraining data, class imbalance in source data, and usage of target data for additional pretraining, etc. The insights distilled from our comparative studies can help ML researchers decide which method to use based on the properties of their applications.","accessed":{"date-parts":[[2023,3,27]]},"author":[{"family":"Yang","given":"Xingyi"},{"family":"He","given":"Xuehai"},{"family":"Liang","given":"Yuxiao"},{"family":"Yang","given":"Yue"},{"family":"Zhang","given":"Shanghang"},{"family":"Xie","given":"Pengtao"}],"citation-key":"yangTransferLearningSelfsupervised2020","DOI":"10.36227/techrxiv.12502298.v1","issued":{"date-parts":[[2020,6,22]]},"language":"en","publisher":"TechRxiv","source":"www.techrxiv.org","title":"Transfer Learning or Self-supervised Learning? A Tale of Two Pretraining Paradigms","title-short":"Transfer Learning or Self-supervised Learning?","type":"article","URL":"https://www.techrxiv.org/articles/preprint/Transfer_Learning_or_Self-supervised_Learning_A_Tale_of_Two_Pretraining_Paradigms/12502298/1"},
  {"id":"yeDuFeNetImproveAccuracy2022","abstract":"In image classiﬁcation ﬁeld, existing work tends to modify the network structure to obtain higher accuracy or faster speed. However, some studies have found that the neural network usually has texture bias effect, which means that the neural network is more sensitive to the texture information than the shape information. Based on such phenomenon, we propose a new way to improve network performance by making full use of gradient information. The dual features network (DuFeNet) is proposed in this paper. In DuFeNet, one sub-network is used to learn the information of gradient features, and the other is a traditional neural network with texture bias. The structure of DuFeNet is easy to implement in the original neural network structure. The experimental results clearly show that DuFeNet can achieve better accuracy in image classiﬁcation and detection. It can increase the shape bias of the network adapted to human visual perception. Besides, DuFeNet can be used without modifying the structure of the original network at lower additional parameters cost.","accessed":{"date-parts":[[2022,9,26]]},"author":[{"family":"Ye","given":"Zecong"},{"family":"Gao","given":"Zhiqiang"},{"family":"Cui","given":"Xiaolong"},{"family":"Wang","given":"Yaojie"},{"family":"Shan","given":"Nanliang"}],"citation-key":"yeDuFeNetImproveAccuracy2022","container-title":"Signal, Image and Video Processing","container-title-short":"SIViP","DOI":"10.1007/s11760-021-02065-3","ISSN":"1863-1703, 1863-1711","issue":"5","issued":{"date-parts":[[2022,7]]},"language":"en","page":"1153-1160","source":"DOI.org (Crossref)","title":"DuFeNet: Improve the Accuracy and Increase Shape Bias of Neural Network Models","title-short":"DuFeNet","type":"article-journal","URL":"https://link.springer.com/10.1007/s11760-021-02065-3","volume":"16"},
  {"id":"yehFidelitySensitivityExplanations2019","abstract":"We consider objective evaluation measures of saliency explanations for complex black-box machine learning models. We propose simple robust variants of two notions that have been considered in recent literature: (in)fidelity, and sensitivity. We analyze optimal explanations with respect to both these measures, and while the optimal explanation for sensitivity is a vacuous constant explanation, the optimal explanation for infidelity is a novel combination of two popular explanation methods. By varying the perturbation distribution that defines infidelity, we obtain novel explanations by optimizing infidelity, which we show to out-perform existing explanations in both quantitative and qualitative measurements. Another salient question given these measures is how to modify any given explanation to have better values with respect to these measures. We propose a simple modification based on lowering sensitivity, and moreover show that when done appropriately, we could simultaneously improve both sensitivity as well as fidelity.","accessed":{"date-parts":[[2022,11,28]]},"author":[{"family":"Yeh","given":"Chih-Kuan"},{"family":"Hsieh","given":"Cheng-Yu"},{"family":"Suggala","given":"Arun Sai"},{"family":"Inouye","given":"David I."},{"family":"Ravikumar","given":"Pradeep"}],"citation-key":"yehFidelitySensitivityExplanations2019","DOI":"10.48550/arXiv.1901.09392","issued":{"date-parts":[[2019,11,3]]},"number":"arXiv:1901.09392","publisher":"arXiv","source":"arXiv.org","title":"On the (In)fidelity and Sensitivity for Explanations","type":"article","URL":"http://arxiv.org/abs/1901.09392"},
  {"id":"yeomPruningExplainingNovel2021","abstract":"The success of convolutional neural networks (CNNs) in various applications is accompanied by a significant increase in computation and parameter storage costs. Recent efforts to reduce these overheads involve pruning and compressing the weights of various layers while at the same time aiming to not sacrifice performance. In this paper, we propose a novel criterion for CNN pruning inspired by neural network interpretability: The most relevant units, i.e. weights or filters, are automatically found using their relevance scores obtained from concepts of explainable AI (XAI). By exploring this idea, we connect the lines of interpretability and model compression research. We show that our proposed method can efficiently prune CNN models in transfer-learning setups in which networks pre-trained on large corpora are adapted to specialized tasks. The method is evaluated on a broad range of computer vision datasets. Notably, our novel criterion is not only competitive or better compared to state-of-the-art pruning criteria when successive retraining is performed, but clearly outperforms these previous criteria in the resource-constrained application scenario in which the data of the task to be transferred to is very scarce and one chooses to refrain from fine-tuning. Our method is able to compress the model iteratively while maintaining or even improving accuracy. At the same time, it has a computational cost in the order of gradient computation and is comparatively simple to apply without the need for tuning hyperparameters for pruning.","accessed":{"date-parts":[[2023,2,16]]},"author":[{"family":"Yeom","given":"Seul-Ki"},{"family":"Seegerer","given":"Philipp"},{"family":"Lapuschkin","given":"Sebastian"},{"family":"Binder","given":"Alexander"},{"family":"Wiedemann","given":"Simon"},{"family":"Müller","given":"Klaus-Robert"},{"family":"Samek","given":"Wojciech"}],"citation-key":"yeomPruningExplainingNovel2021","container-title":"Pattern Recognition","container-title-short":"Pattern Recognition","DOI":"10.1016/j.patcog.2021.107899","ISSN":"0031-3203","issued":{"date-parts":[[2021,7,1]]},"language":"en","page":"107899","source":"ScienceDirect","title":"Pruning by explaining: A novel criterion for deep neural network pruning","title-short":"Pruning by explaining","type":"article-journal","URL":"https://www.sciencedirect.com/science/article/pii/S0031320321000868","volume":"115"},
  {"id":"yuBuildingEthicsArtificial2018","abstract":"As artificial intelligence (AI) systems become increasingly ubiquitous, the topic of AI governance for ethical decision-making by AI has captured public imagination. Within the AI research community, this topic remains less familiar to many researchers. In this paper, we complement existing surveys, which largely focused on the psychological, social and legal discussions of the topic, with an analysis of recent advances in technical solutions for AI governance. By reviewing publications in leading AI conferences including AAAI, AAMAS, ECAI and IJCAI, we propose a taxonomy which divides the field into four areas: 1) exploring ethical dilemmas; 2) individual ethical decision frameworks; 3) collective ethical decision frameworks; and 4) ethics in human-AI interactions. We highlight the intuitions and key techniques used in each approach, and discuss promising future research directions towards successful integration of ethical AI systems into human societies.","accessed":{"date-parts":[[2023,1,17]]},"author":[{"family":"Yu","given":"Han"},{"family":"Shen","given":"Zhiqi"},{"family":"Miao","given":"Chunyan"},{"family":"Leung","given":"Cyril"},{"family":"Lesser","given":"Victor R."},{"family":"Yang","given":"Qiang"}],"citation-key":"yuBuildingEthicsArtificial2018","DOI":"10.48550/arXiv.1812.02953","issued":{"date-parts":[[2018,12,7]]},"number":"arXiv:1812.02953","publisher":"arXiv","source":"arXiv.org","title":"Building Ethics into Artificial Intelligence","type":"article","URL":"http://arxiv.org/abs/1812.02953"},
  {"id":"yunCutMixRegularizationStrategy2019","abstract":"Regional dropout strategies have been proposed to enhance the performance of convolutional neural network classiﬁers. They have proved to be effective for guiding the model to attend on less discriminative parts of objects (e.g. leg as opposed to head of a person), thereby letting the network generalize better and have better object localization capabilities. On the other hand, current methods for regional dropout remove informative pixels on training images by overlaying a patch of either black pixels or random noise. Such removal is not desirable because it leads to information loss and inefﬁciency during training. We therefore propose the CutMix augmentation strategy: patches are cut and pasted among training images where the ground truth labels are also mixed proportionally to the area of the patches. By making efﬁcient use of training pixels and retaining the regularization effect of regional dropout, CutMix consistently outperforms the state-of-the-art augmentation strategies on CIFAR and ImageNet classiﬁcation tasks, as well as on the ImageNet weakly-supervised localization task. Moreover, unlike previous augmentation methods, our CutMix-trained ImageNet classiﬁer, when used as a pretrained model, results in consistent performance gains in Pascal detection and MS-COCO image captioning benchmarks. We also show that CutMix improves the model robustness against input corruptions and its out-of-distribution detection performances. Source code and pretrained models are available at https://github.com/clovaai/CutMix-PyTorch.","accessed":{"date-parts":[[2023,2,20]]},"author":[{"family":"Yun","given":"Sangdoo"},{"family":"Han","given":"Dongyoon"},{"family":"Chun","given":"Sanghyuk"},{"family":"Oh","given":"Seong Joon"},{"family":"Yoo","given":"Youngjoon"},{"family":"Choe","given":"Junsuk"}],"citation-key":"yunCutMixRegularizationStrategy2019","container-title":"2019 IEEE/CVF International Conference on Computer Vision (ICCV)","DOI":"10.1109/ICCV.2019.00612","event-place":"Seoul, Korea (South)","event-title":"2019 IEEE/CVF International Conference on Computer Vision (ICCV)","ISBN":"978-1-72814-803-8","issued":{"date-parts":[[2019,10]]},"language":"en","page":"6022-6031","publisher":"IEEE","publisher-place":"Seoul, Korea (South)","source":"DOI.org (Crossref)","title":"CutMix: Regularization Strategy to Train Strong Classifiers With Localizable Features","title-short":"CutMix","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/9008296/"},
  {"id":"yunCutMixRegularizationStrategy2019a","abstract":"Regional dropout strategies have been proposed to enhance the performance of convolutional neural network classifiers. They have proved to be effective for guiding the model to attend on less discriminative parts of objects (e.g. leg as opposed to head of a person), thereby letting the network generalize better and have better object localization capabilities. On the other hand, current methods for regional dropout remove informative pixels on training images by overlaying a patch of either black pixels or random noise. Such removal is not desirable because it leads to information loss and inefficiency during training. We therefore propose the CutMix augmentation strategy: patches are cut and pasted among training images where the ground truth labels are also mixed proportionally to the area of the patches. By making efficient use of training pixels and retaining the regularization effect of regional dropout, CutMix consistently outperforms the state-of-the-art augmentation strategies on CIFAR and ImageNet classification tasks, as well as on the ImageNet weakly-supervised localization task. Moreover, unlike previous augmentation methods, our CutMix-trained ImageNet classifier, when used as a pretrained model, results in consistent performance gains in Pascal detection and MS-COCO image captioning benchmarks. We also show that CutMix improves the model robustness against input corruptions and its out-of-distribution detection performances. Source code and pretrained models are available at https://github.com/clovaai/CutMix-PyTorch .","accessed":{"date-parts":[[2023,3,27]]},"author":[{"family":"Yun","given":"Sangdoo"},{"family":"Han","given":"Dongyoon"},{"family":"Oh","given":"Seong Joon"},{"family":"Chun","given":"Sanghyuk"},{"family":"Choe","given":"Junsuk"},{"family":"Yoo","given":"Youngjoon"}],"citation-key":"yunCutMixRegularizationStrategy2019a","DOI":"10.48550/arXiv.1905.04899","issued":{"date-parts":[[2019,8,7]]},"number":"arXiv:1905.04899","publisher":"arXiv","source":"arXiv.org","title":"CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features","title-short":"CutMix","type":"article","URL":"http://arxiv.org/abs/1905.04899"},
  {"id":"zeilerAdaptiveDeconvolutionalNetworks2011","abstract":"We present a hierarchical model that learns image decompositions via alternating layers of convolutional sparse coding and max pooling. When trained on natural images, the layers of our model capture image information in a variety of forms: low-level edges, mid-level edge junctions, high-level object parts and complete objects. To build our model we rely on a novel inference scheme that ensures each layer reconstructs the input, rather than just the output of the layer directly beneath, as is common with existing hierarchical approaches. This makes it possible to learn multiple layers of representation and we show models with 4 layers, trained on images from the Caltech-101 and 256 datasets. When combined with a standard classiﬁer, features extracted from these models outperform SIFT, as well as representations from other feature learning methods.","accessed":{"date-parts":[[2023,2,20]]},"author":[{"family":"Zeiler","given":"Matthew D."},{"family":"Taylor","given":"Graham W."},{"family":"Fergus","given":"Rob"}],"citation-key":"zeilerAdaptiveDeconvolutionalNetworks2011","container-title":"2011 International Conference on Computer Vision","DOI":"10.1109/ICCV.2011.6126474","event-place":"Barcelona, Spain","event-title":"2011 IEEE International Conference on Computer Vision (ICCV)","ISBN":"978-1-4577-1102-2 978-1-4577-1101-5 978-1-4577-1100-8","issued":{"date-parts":[[2011,11]]},"language":"en","page":"2018-2025","publisher":"IEEE","publisher-place":"Barcelona, Spain","source":"DOI.org (Crossref)","title":"Adaptive deconvolutional networks for mid and high level feature learning","type":"paper-conference","URL":"http://ieeexplore.ieee.org/document/6126474/"},
  {"id":"zeilerVisualizingUnderstandingConvolutional2013","abstract":"Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky \\etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.","accessed":{"date-parts":[[2022,11,28]]},"author":[{"family":"Zeiler","given":"Matthew D."},{"family":"Fergus","given":"Rob"}],"citation-key":"zeilerVisualizingUnderstandingConvolutional2013","DOI":"10.48550/arXiv.1311.2901","issued":{"date-parts":[[2013,11,28]]},"number":"arXiv:1311.2901","publisher":"arXiv","source":"arXiv.org","title":"Visualizing and Understanding Convolutional Networks","type":"article","URL":"http://arxiv.org/abs/1311.2901"},
  {"id":"zeilerVisualizingUnderstandingConvolutional2013a","abstract":"Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky \\etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.","accessed":{"date-parts":[[2022,12,7]]},"author":[{"family":"Zeiler","given":"Matthew D."},{"family":"Fergus","given":"Rob"}],"citation-key":"zeilerVisualizingUnderstandingConvolutional2013a","DOI":"10.48550/arXiv.1311.2901","issued":{"date-parts":[[2013,11,28]]},"number":"arXiv:1311.2901","publisher":"arXiv","source":"arXiv.org","title":"Visualizing and Understanding Convolutional Networks","type":"article","URL":"http://arxiv.org/abs/1311.2901"},
  {"id":"zhangDebiasedCAMMitigateImage2022","abstract":"Model explanations such as saliency maps can improve user trust in AI by highlighting important features for a prediction. However, these become distorted and misleading when explaining predictions of images that are subject to systematic error (bias) by perturbations and corruptions. Furthermore, the distortions persist despite model fine-tuning on images biased by different factors (blur, color temperature, day/night). We present Debiased-CAM to recover explanation faithfulness across various bias types and levels by training a multi-input, multi-task model with auxiliary tasks for explanation and bias level predictions. In simulation studies, the approach not only enhanced prediction accuracy, but also generated highly faithful explanations about these predictions as if the images were unbiased. In user studies, debiased explanations improved user task performance, perceived truthfulness and perceived helpfulness. Debiased training can provide a versatile platform for robust performance and explanation faithfulness for a wide range of applications with data biases.","accessed":{"date-parts":[[2023,2,13]]},"author":[{"family":"Zhang","given":"Wencan"},{"family":"Dimiccoli","given":"Mariella"},{"family":"Lim","given":"Brian Y"}],"citation-key":"zhangDebiasedCAMMitigateImage2022","container-title":"CHI Conference on Human Factors in Computing Systems","DOI":"10.1145/3491102.3517522","event-place":"New Orleans LA USA","event-title":"CHI '22: CHI Conference on Human Factors in Computing Systems","ISBN":"978-1-4503-9157-3","issued":{"date-parts":[[2022,4,29]]},"language":"en","page":"1-32","publisher":"ACM","publisher-place":"New Orleans LA USA","source":"DOI.org (Crossref)","title":"Debiased-CAM to mitigate image perturbations with faithful visual explanations of machine learning","type":"paper-conference","URL":"https://dl.acm.org/doi/10.1145/3491102.3517522"},
  {"id":"zhangExaminingCNNRepresentations2018","abstract":"Given a pre-trained CNN without any testing samples, this paper proposes a simple yet effective method to diagnose feature representations of the CNN. We aim to discover representation flaws caused by potential dataset bias. More specifically, when the CNN is trained to estimate image attributes, we mine latent relationships between representations of different attributes inside the CNN. Then, we compare the mined attribute relationships with ground-truth attribute relationships to discover the CNN's blind spots and failure modes due to dataset bias. In fact, representation flaws caused by dataset bias cannot be examined by conventional evaluation strategies based on testing images, because testing images may also have a similar bias. Experiments have demonstrated the effectiveness of our method.","accessed":{"date-parts":[[2022,9,6]]},"author":[{"family":"Zhang","given":"Quanshi"},{"family":"Wang","given":"Wenguan"},{"family":"Zhu","given":"Song-Chun"}],"citation-key":"zhangExaminingCNNRepresentations2018","container-title":"Proceedings of the AAAI Conference on Artificial Intelligence","DOI":"10.1609/aaai.v32i1.11833","ISSN":"2374-3468","issue":"1","issued":{"date-parts":[[2018,4,29]]},"language":"en","license":"Copyright (c)","number":"1","source":"ojs.aaai.org","title":"Examining CNN Representations With Respect to Dataset Bias","type":"article-journal","URL":"https://ojs.aaai.org/index.php/AAAI/article/view/11833","volume":"32"},
  {"id":"zhangHowDoesMixup2021","abstract":"Mixup is a popular data augmentation technique based on on convex combinations of pairs of examples and their labels. This simple technique has shown to substantially improve both the model's robustness as well as the generalization of the trained model. However, it is not well-understood why such improvement occurs. In this paper, we provide theoretical analysis to demonstrate how using Mixup in training helps model robustness and generalization. For robustness, we show that minimizing the Mixup loss corresponds to approximately minimizing an upper bound of the adversarial loss. This explains why models obtained by Mixup training exhibits robustness to several kinds of adversarial attacks such as Fast Gradient Sign Method (FGSM). For generalization, we prove that Mixup augmentation corresponds to a specific type of data-adaptive regularization which reduces overfitting. Our analysis provides new insights and a framework to understand Mixup.","accessed":{"date-parts":[[2022,11,20]]},"author":[{"family":"Zhang","given":"Linjun"},{"family":"Deng","given":"Zhun"},{"family":"Kawaguchi","given":"Kenji"},{"family":"Ghorbani","given":"Amirata"},{"family":"Zou","given":"James"}],"citation-key":"zhangHowDoesMixup2021","event-title":"International Conference on Learning Representations","issued":{"date-parts":[[2021,3,17]]},"language":"en","source":"openreview.net","title":"How Does Mixup Help With Robustness and Generalization?","type":"paper-conference","URL":"https://openreview.net/forum?id=8yKEo06dKNo"},
  {"id":"zhangInductiveMultiHypergraphLearning2018","abstract":"The wide 3D applications have led to increasing amount of 3D object data, and thus effective 3D object classiﬁcation technique has become urgent requirement. One important and challenging task for 3D object classiﬁcation is how to formulate the 3D data correlation and exploit it. Most of previous works focus on learning optimal pairwise distance metric for object comparison, which may lose the global correlation among 3D objects. Recently, transductive hypergraph learning has been investigated for classiﬁcation, which can jointly explore the correlation among multiple objects, including both the labeled and unlabeled data. Although these methods have shown better performance, they are still limited due to 1) a considerable amount of testing data may not be available in practice and 2) the high computational cost to test new coming data. To handle this problem, considering the multimodal representations of 3D objects in practice, we propose an inductive multi-hypergraph learning algorithm, which targets on learning an optimal projection for the multi-modal training data. In this method, all the training data are formulated in multihypergraph based on the features, and the inductive learning is conducted to learn the projection matrices and the optimal multi-hypergraph combination weights simultaneously. Different from the transductive learning on hypergraph, the high cost training process is off-line, and the testing process is very efﬁcient for the inductive learning on hypergraph. We have conducted experiments on two 3D benchmarks, i.e., the NTU and the ModelNet40 datasets, and compared the proposed algorithm with the state-of-the-art methods and traditional transductive multi-hypergraph learning methods. Experimental results have demonstrated that the proposed method can achieve effective and efﬁcient classiﬁcation performance. We also note that the proposed method is a general framework and has the potential to be applied in other applications in practice.","accessed":{"date-parts":[[2022,5,10]]},"author":[{"family":"Zhang","given":"Zizhao"},{"family":"Lin","given":"Haojie"},{"family":"Zhao","given":"Xibin"},{"family":"Ji","given":"Rongrong"},{"family":"Gao","given":"Yue"}],"citation-key":"zhangInductiveMultiHypergraphLearning2018","container-title":"IEEE Transactions on Image Processing","container-title-short":"IEEE Trans. on Image Process.","DOI":"10.1109/TIP.2018.2862625","ISSN":"1057-7149, 1941-0042","issue":"12","issued":{"date-parts":[[2018,12]]},"language":"en","page":"5957-5968","source":"DOI.org (Crossref)","title":"Inductive Multi-Hypergraph Learning and Its Application on View-Based 3D Object Classification","type":"article-journal","URL":"https://ieeexplore.ieee.org/document/8424480/","volume":"27"},
  {"id":"zhangInductiveMultihypergraphLearning2018","author":[{"family":"Zhang","given":"Zizhao"},{"family":"Lin","given":"Haojie"},{"family":"Zhao","given":"Xibin"},{"family":"Ji","given":"Rongrong"},{"family":"Gao","given":"Yue"}],"citation-key":"zhangInductiveMultihypergraphLearning2018","container-title":"IEEE Transactions on Image Processing","issue":"12","issued":{"date-parts":[[2018]]},"page":"5957–5968","publisher":"IEEE","title":"Inductive multi-hypergraph learning and its application on view-based 3D object classification","type":"article-journal","volume":"27"},
  {"id":"zhangIntraClassPartSwapping2021","accessed":{"date-parts":[[2023,3,30]]},"author":[{"family":"Zhang","given":"Lianbo"},{"family":"Huang","given":"Shaoli"},{"family":"Liu","given":"Wei"}],"citation-key":"zhangIntraClassPartSwapping2021","event-title":"Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision","issued":{"date-parts":[[2021]]},"language":"en","page":"3209-3218","source":"openaccess.thecvf.com","title":"Intra-Class Part Swapping for Fine-Grained Image Classification","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/WACV2021/html/Zhang_Intra-Class_Part_Swapping_for_Fine-Grained_Image_Classification_WACV_2021_paper.html"},
  {"id":"zhangJointObjectPose2013","author":[{"family":"Zhang","given":"Haopeng"},{"family":"El-Gaaly","given":"Tarek"},{"family":"Elgammal","given":"Ahmed"},{"family":"Jiang","given":"Zhiguo"}],"citation-key":"zhangJointObjectPose2013","container-title":"Proceedings of the AAAI Conference on Artificial Intelligence","issued":{"date-parts":[[2013]]},"title":"Joint object and pose recognition using homeomorphic manifold analysis","type":"paper-conference"},
  {"id":"zhangLinkedDynamicGraph2019","abstract":"Learning on point cloud is eagerly in demand because the point cloud is a common type of geometric data and can aid robots to understand environments robustly. However, the point cloud is sparse, unstructured, and unordered, which cannot be recognized accurately by a traditional convolutional neural network (CNN) nor a recurrent neural network (RNN). Fortunately, a graph convolutional neural network (Graph CNN) can process sparse and unordered data. Hence, we propose a linked dynamic graph CNN (LDGCNN) to classify and segment point cloud directly in this paper. We remove the transformation network, link hierarchical features from dynamic graphs, freeze feature extractor, and retrain the classifier to increase the performance of LDGCNN. We explain our network using theoretical analysis and visualization. Through experiments, we show that the proposed LDGCNN achieves state-of-art performance on two standard datasets: ModelNet40 and ShapeNet.","accessed":{"date-parts":[[2022,5,10]]},"author":[{"family":"Zhang","given":"Kuangen"},{"family":"Hao","given":"Ming"},{"family":"Wang","given":"Jing"},{"family":"Silva","given":"Clarence W.","non-dropping-particle":"de"},{"family":"Fu","given":"Chenglong"}],"citation-key":"zhangLinkedDynamicGraph2019","container-title":"arXiv:1904.10014 [cs]","issued":{"date-parts":[[2019,8,5]]},"source":"arXiv.org","title":"Linked Dynamic Graph CNN: Learning on Point Cloud via Linking Hierarchical Features","title-short":"Linked Dynamic Graph CNN","type":"article-journal","URL":"http://arxiv.org/abs/1904.10014"},
  {"id":"zhangMentalizingInformationPropagation2016","abstract":"Microblogs is one of the main social networking channels by which information is spread. Among them, Sina Weibo is one of the largest social networking channels in China. Millions of users repost information from Sina Weibo and share embedded emotion at the same time. The present study investigated participants’ propensity to repost microblog messages of positive, negative, or neutral valence, and studied the neural correlates during resting state with the reposting rate of each type microblog messages. Participants preferred to repost negative messages relative to positive and neutral messages. Reposting rate of negative messages was positively correlated to the functional connectivity of temporoparietal junction (TPJ) with insula, and TPJ with dorsolateral prefrontal cortex. These results indicate that reposting negative messages is related to conflict resolution between the feeling of pain/disgust and the intention to repost significant information. Thus, resposting emotional microblog messages might be attributed to participants’ appraisal of personal and recipient’s interest, as well as their cognitive process for decision making.","accessed":{"date-parts":[[2022,6,15]]},"author":[{"family":"Zhang","given":"Huijun"},{"family":"Mo","given":"Lei"}],"citation-key":"zhangMentalizingInformationPropagation2016","container-title":"Frontiers in Psychology","ISSN":"1664-1078","issued":{"date-parts":[[2016]]},"source":"Frontiers","title":"Mentalizing and Information Propagation through Social Network: Evidence from a Resting-State-fMRI Study","title-short":"Mentalizing and Information Propagation through Social Network","type":"article-journal","URL":"https://www.frontiersin.org/article/10.3389/fpsyg.2016.01716","volume":"7"},
  {"id":"zhangMixupEmpiricalRisk2018","abstract":"Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.","accessed":{"date-parts":[[2023,3,27]]},"author":[{"family":"Zhang","given":"Hongyi"},{"family":"Cisse","given":"Moustapha"},{"family":"Dauphin","given":"Yann N."},{"family":"Lopez-Paz","given":"David"}],"citation-key":"zhangMixupEmpiricalRisk2018","DOI":"10.48550/arXiv.1710.09412","issued":{"date-parts":[[2018,4,27]]},"number":"arXiv:1710.09412","publisher":"arXiv","source":"arXiv.org","title":"mixup: Beyond Empirical Risk Minimization","title-short":"mixup","type":"article","URL":"http://arxiv.org/abs/1710.09412"},
  {"id":"zhangSurveyNeuralNetwork2021","abstract":"Along with the great success of deep neural networks, there is also growing concern about their black-box nature. The interpretability issue affects people's trust on deep learning systems. It is also related to many ethical problems, e.g., algorithmic discrimination. Moreover, interpretability is a desired property for deep networks to become powerful tools in other research fields, e.g., drug discovery and genomics. In this survey, we conduct a comprehensive review of the neural network interpretability research. We first clarify the definition of interpretability as it has been used in many different contexts. Then we elaborate on the importance of interpretability and propose a novel taxonomy organized along three dimensions: type of engagement (passive vs. active interpretation approaches), the type of explanation, and the focus (from local to global interpretability). This taxonomy provides a meaningful 3D view of distribution of papers from the relevant literature as two of the dimensions are not simply categorical but allow ordinal subcategories. Finally, we summarize the existing interpretability evaluation methods and suggest possible research directions inspired by our new taxonomy.","accessed":{"date-parts":[[2022,12,7]]},"author":[{"family":"Zhang","given":"Yu"},{"family":"Tiňo","given":"Peter"},{"family":"Leonardis","given":"Aleš"},{"family":"Tang","given":"Ke"}],"citation-key":"zhangSurveyNeuralNetwork2021","container-title":"IEEE Transactions on Emerging Topics in Computational Intelligence","container-title-short":"IEEE Trans. Emerg. Top. Comput. Intell.","DOI":"10.1109/TETCI.2021.3100641","ISSN":"2471-285X","issue":"5","issued":{"date-parts":[[2021,10]]},"page":"726-742","source":"arXiv.org","title":"A Survey on Neural Network Interpretability","type":"article-journal","URL":"http://arxiv.org/abs/2012.14261","volume":"5"},
  {"id":"zhaoExploitingExplanationsModel2021","abstract":"The successful deployment of artiﬁcial intelligence (AI) in many domains from healthcare to hiring requires their responsible use, particularly in model explanations and privacy. Explainable artiﬁcial intelligence (XAI) provides more information to help users to understand model decisions, yet this additional knowledge exposes additional risks for privacy attacks. Hence, providing explanation harms privacy. We study this risk for image-based model inversion attacks and identiﬁed several attack architectures with increasing performance to reconstruct private image data from model explanations. We have developed several multi-modal transposed CNN architectures that achieve signiﬁcantly higher inversion performance than using the target model prediction only. These XAI-aware inversion models were designed to exploit the spatial knowledge in image explanations. To understand which explanations have higher privacy risk, we analyzed how various explanation types and factors inﬂuence inversion performance. In spite of some models not providing explanations, we further demonstrate increased inversion performance even for nonexplainable target models by exploiting explanations of surrogate models through attention transfer. This method ﬁrst inverts an explanation from the target prediction, then reconstructs the target image. These threats highlight the urgent and signiﬁcant privacy risks of explanations and calls attention for new privacy preservation techniques that balance the dual-requirement for AI explainability and privacy.","accessed":{"date-parts":[[2023,2,13]]},"author":[{"family":"Zhao","given":"Xuejun"},{"family":"Zhang","given":"Wencan"},{"family":"Xiao","given":"Xiaokui"},{"family":"Lim","given":"Brian"}],"citation-key":"zhaoExploitingExplanationsModel2021","container-title":"2021 IEEE/CVF International Conference on Computer Vision (ICCV)","DOI":"10.1109/ICCV48922.2021.00072","event-place":"Montreal, QC, Canada","event-title":"2021 IEEE/CVF International Conference on Computer Vision (ICCV)","ISBN":"978-1-66542-812-5","issued":{"date-parts":[[2021,10]]},"language":"en","page":"662-672","publisher":"IEEE","publisher-place":"Montreal, QC, Canada","source":"DOI.org (Crossref)","title":"Exploiting Explanations for Model Inversion Attacks","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/9709977/"},
  {"id":"zhaoM2DetSingleShotObject2019","abstract":"Feature pyramids are widely exploited by both the state-of-the-art one-stage object detectors (e.g., DSSD, RetinaNet, RefineDet) and the two-stage object detectors (e.g., Mask RCNN, DetNet) to alleviate the problem arising from scale variation across object instances. Although these object detectors with feature pyramids achieve encouraging results, they have some limitations due to that they only simply construct the feature pyramid according to the inherent multiscale, pyramidal architecture of the backbones which are originally designed for object classification task. Newly, in this work, we present Multi-Level Feature Pyramid Network (MLFPN) to construct more effective feature pyramids for detecting objects of different scales. First, we fuse multi-level features (i.e. multiple layers) extracted by backbone as the base feature. Second, we feed the base feature into a block of alternating joint Thinned U-shape Modules and Feature Fusion Modules and exploit the decoder layers of each Ushape module as the features for detecting objects. Finally, we gather up the decoder layers with equivalent scales (sizes) to construct a feature pyramid for object detection, in which every feature map consists of the layers (features) from multiple levels. To evaluate the effectiveness of the proposed MLFPN, we design and train a powerful end-to-end one-stage object detector we call M2Det by integrating it into the architecture of SSD, and achieve better detection performance than state-of-the-art one-stage detectors. Specifically, on MSCOCO benchmark, M2Det achieves AP of 41.0 at speed of 11.8 FPS with single-scale inference strategy and AP of 44.2 with multi-scale inference strategy, which are the new stateof-the-art results among one-stage detectors. The code will be made available on https://github.com/qijiezhao/M2Det.","accessed":{"date-parts":[[2023,5,8]]},"author":[{"family":"Zhao","given":"Qijie"},{"family":"Sheng","given":"Tao"},{"family":"Wang","given":"Yongtao"},{"family":"Tang","given":"Zhi"},{"family":"Chen","given":"Ying"},{"family":"Cai","given":"Ling"},{"family":"Ling","given":"Haibin"}],"citation-key":"zhaoM2DetSingleShotObject2019","container-title":"Proceedings of the AAAI Conference on Artificial Intelligence","DOI":"10.1609/aaai.v33i01.33019259","ISSN":"2374-3468","issue":"01","issued":{"date-parts":[[2019,7,17]]},"language":"en","license":"Copyright (c) 2019 Association for the Advancement of Artificial Intelligence","number":"01","page":"9259-9266","source":"ojs.aaai.org","title":"M2Det: A Single-Shot Object Detector Based on Multi-Level Feature Pyramid Network","title-short":"M2Det","type":"article-journal","URL":"https://ojs.aaai.org/index.php/AAAI/article/view/4962","volume":"33"},
  {"id":"zhaoUnderstandingEvaluatingRacial2021","accessed":{"date-parts":[[2022,9,6]]},"author":[{"family":"Zhao","given":"Dora"},{"family":"Wang","given":"Angelina"},{"family":"Russakovsky","given":"Olga"}],"citation-key":"zhaoUnderstandingEvaluatingRacial2021","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[[2021]]},"language":"en","page":"14830-14840","source":"openaccess.thecvf.com","title":"Understanding and Evaluating Racial Biases in Image Captioning","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2021/html/Zhao_Understanding_and_Evaluating_Racial_Biases_in_Image_Captioning_ICCV_2021_paper.html"},
  {"id":"zhongRandomErasingData2020","abstract":"In this paper, we introduce Random Erasing, a new data augmentation method for training the convolutional neural network (CNN). In training, Random Erasing randomly selects a rectangle region in an image and erases its pixels with random values. In this process, training images with various levels of occlusion are generated, which reduces the risk of over-ﬁtting and makes the model robust to occlusion. Random Erasing is parameter learning free, easy to implement, and can be integrated with most of the CNN-based recognition models. Albeit simple, Random Erasing is complementary to commonly used data augmentation techniques such as random cropping and ﬂipping, and yields consistent improvement over strong baselines in image classiﬁcation, object detection and person re-identiﬁcation. Code is available at: https://github.com/zhunzhong07/Random-Erasing.","accessed":{"date-parts":[[2022,10,21]]},"author":[{"family":"Zhong","given":"Zhun"},{"family":"Zheng","given":"Liang"},{"family":"Kang","given":"Guoliang"},{"family":"Li","given":"Shaozi"},{"family":"Yang","given":"Yi"}],"citation-key":"zhongRandomErasingData2020","container-title":"Proceedings of the AAAI Conference on Artificial Intelligence","container-title-short":"AAAI","DOI":"10.1609/aaai.v34i07.7000","ISSN":"2374-3468, 2159-5399","issue":"07","issued":{"date-parts":[[2020,4,3]]},"language":"en","page":"13001-13008","source":"DOI.org (Crossref)","title":"Random Erasing Data Augmentation","type":"article-journal","URL":"https://aaai.org/ojs/index.php/AAAI/article/view/7000","volume":"34"},
  {"id":"zhongRandomErasingData2020a","abstract":"In this paper, we introduce Random Erasing, a new data augmentation method for training the convolutional neural network (CNN). In training, Random Erasing randomly selects a rectangle region in an image and erases its pixels with random values. In this process, training images with various levels of occlusion are generated, which reduces the risk of over-fitting and makes the model robust to occlusion. Random Erasing is parameter learning free, easy to implement, and can be integrated with most of the CNN-based recognition models. Albeit simple, Random Erasing is complementary to commonly used data augmentation techniques such as random cropping and flipping, and yields consistent improvement over strong baselines in image classification, object detection and person re-identification. Code is available at: https://github.com/zhunzhong07/Random-Erasing.","accessed":{"date-parts":[[2023,3,29]]},"author":[{"family":"Zhong","given":"Zhun"},{"family":"Zheng","given":"Liang"},{"family":"Kang","given":"Guoliang"},{"family":"Li","given":"Shaozi"},{"family":"Yang","given":"Yi"}],"citation-key":"zhongRandomErasingData2020a","container-title":"Proceedings of the AAAI Conference on Artificial Intelligence","DOI":"10.1609/aaai.v34i07.7000","ISSN":"2374-3468","issue":"07","issued":{"date-parts":[[2020,4,3]]},"language":"en","license":"Copyright (c) 2020 Association for the Advancement of Artificial Intelligence","number":"07","page":"13001-13008","source":"ojs.aaai.org","title":"Random Erasing Data Augmentation","type":"article-journal","URL":"https://ojs.aaai.org/index.php/AAAI/article/view/7000","volume":"34"},
  {"id":"zhouConvNetsVsTransformers2021","abstract":"Vision transformers have attracted much attention from computer vision researchers as they are not restricted to the spatial inductive bias of ConvNets. However, although Transformer-based backbones have achieved much progress on ImageNet classification, it is still unclear whether the learned representations are as transferable as or even more transferable than ConvNets' features. To address this point, we systematically investigate the transfer learning ability of ConvNets and vision transformers in 15 single-task and multi-task performance evaluations. Given the strong correlation between the performance of pre-trained models and transfer learning, we include 2 residual ConvNets (i.e., R-101x3 and R-152x4) and 3 Transformer-based visual backbones (i.e., ViT-B, ViT-L and Swin-B), which have close error rates on ImageNet, that indicate similar transfer learning performance on downstream datasets. We observe consistent advantages of Transformer-based backbones on 13 downstream tasks (out of 15), including but not limited to fine-grained classification, scene recognition (classification, segmentation and depth estimation), open-domain classification, face recognition, etc. More specifically, we find that two ViT models heavily rely on whole network fine-tuning to achieve performance gains while Swin Transformer does not have such a requirement. Moreover, vision transformers behave more robustly in multi-task learning, i.e., bringing more improvements when managing mutually beneficial tasks and reducing performance losses when tackling irrelevant tasks. We hope our discoveries can facilitate the exploration and exploitation of vision transformers in the future.","accessed":{"date-parts":[[2022,10,23]]},"author":[{"family":"Zhou","given":"Hong-Yu"},{"family":"Lu","given":"Chixiang"},{"family":"Yang","given":"Sibei"},{"family":"Yu","given":"Yizhou"}],"citation-key":"zhouConvNetsVsTransformers2021","DOI":"10.48550/arXiv.2108.05305","issued":{"date-parts":[[2021,8,17]]},"number":"arXiv:2108.05305","publisher":"arXiv","source":"arXiv.org","title":"ConvNets vs. Transformers: Whose Visual Representations are More Transferable?","title-short":"ConvNets vs. Transformers","type":"article","URL":"http://arxiv.org/abs/2108.05305"},
  {"id":"zhouLearningDeepFeatures2016","abstract":"In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network (CNN) to have remarkable localization ability despite being trained on imagelevel labels. While this technique was previously proposed as a means for regularizing training, we ﬁnd that it actually builds a generic localizable deep representation that exposes the implicit attention of CNNs on an image. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1% top-5 error for object localization on ILSVRC 2014 without training on any bounding box annotation.We demonstrate in a variety of experiments that our network is able to localize the discriminative image regions despite just being trained for solving classiﬁcation task1.","accessed":{"date-parts":[[2023,2,20]]},"author":[{"family":"Zhou","given":"Bolei"},{"family":"Khosla","given":"Aditya"},{"family":"Lapedriza","given":"Agata"},{"family":"Oliva","given":"Aude"},{"family":"Torralba","given":"Antonio"}],"citation-key":"zhouLearningDeepFeatures2016","container-title":"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","DOI":"10.1109/CVPR.2016.319","event-place":"Las Vegas, NV, USA","event-title":"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","ISBN":"978-1-4673-8851-1","issued":{"date-parts":[[2016,6]]},"language":"en","page":"2921-2929","publisher":"IEEE","publisher-place":"Las Vegas, NV, USA","source":"DOI.org (Crossref)","title":"Learning Deep Features for Discriminative Localization","type":"paper-conference","URL":"http://ieeexplore.ieee.org/document/7780688/"},
  {"id":"zhouOpen3DModernLibrary2018","author":[{"family":"Zhou","given":"Qian-Yi"},{"family":"Park","given":"Jaesik"},{"family":"Koltun","given":"Vladlen"}],"citation-key":"zhouOpen3DModernLibrary2018","container-title":"arXiv:1801.09847","issued":{"date-parts":[[2018]]},"title":"Open3D: A Modern Library for 3D Data Processing","type":"article-journal"},
  {"id":"zhouSurveyEthicalPrinciples2020","abstract":"AI has powerful capabilities in prediction, automation, planning, targeting, and personalisation. Generally, it is assumed that AI can enable machines to exhibit human-like intelligence, and is claimed to benefit to different areas of our lives. Since AI is fueled by data and is a distinct form of autonomous and self-learning agency, we are seeing increasing ethical concerns related to AI uses. In order to mitigate various ethical concerns, national and international organisations including governmental organisations, private sectors as well as research institutes have made extensive efforts by drafting ethical principles of AI, and having active discussions on ethics of AI within and beyond the AI community. This paper investigates these efforts with a focus on the identification of fundamental ethical principles of AI and their implementations. The review found that there is a convergence around limited principles and the most prevalent principles are transparency, justice and fairness, responsibility, non-maleficence, and privacy. The investigation suggests that ethical principles need to be combined with every stages of the AI lifecycle in the implementation to ensure that the AI system is designed, implemented and deployed in an ethical manner. Similar to ethical framework used in biomedical and clinical research, this paper suggests checklist-style questionnaires as benchmarks for the implementation of ethical principles of AI.","accessed":{"date-parts":[[2023,1,17]]},"author":[{"family":"Zhou","given":"Jianlong"},{"family":"Chen","given":"Fang"},{"family":"Berry","given":"Adam"},{"family":"Reed","given":"Mike"},{"family":"Zhang","given":"Shujia"},{"family":"Savage","given":"Siobhan"}],"citation-key":"zhouSurveyEthicalPrinciples2020","container-title":"2020 IEEE Symposium Series on Computational Intelligence (SSCI)","DOI":"10.1109/SSCI47803.2020.9308437","event-place":"Canberra, ACT, Australia","event-title":"2020 IEEE Symposium Series on Computational Intelligence (SSCI)","ISBN":"978-1-72812-547-3","issued":{"date-parts":[[2020,12,1]]},"language":"en","page":"3010-3017","publisher":"IEEE","publisher-place":"Canberra, ACT, Australia","source":"DOI.org (Crossref)","title":"A Survey on Ethical Principles of AI and Implementations","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/9308437/"},
  {"id":"zhuangComprehensiveSurveyTransfer2020","abstract":"Transfer learning aims at improving the performance of target learners on target domains by transferring the knowledge contained in different but related source domains. In this way, the dependence on a large number of target domain data can be reduced for constructing target learners. Due to the wide application prospects, transfer learning has become a popular and promising area in machine learning. Although there are already some valuable and impressive surveys on transfer learning, these surveys introduce approaches in a relatively isolated way and lack the recent advances in transfer learning. Due to the rapid expansion of the transfer learning area, it is both necessary and challenging to comprehensively review the relevant studies. This survey attempts to connect and systematize the existing transfer learning researches, as well as to summarize and interpret the mechanisms and the strategies of transfer learning in a comprehensive way, which may help readers have a better understanding of the current research status and ideas. Unlike previous surveys, this survey paper reviews more than forty representative transfer learning approaches, especially homogeneous transfer learning approaches, from the perspectives of data and model. The applications of transfer learning are also briefly introduced. In order to show the performance of different transfer learning models, over twenty representative transfer learning models are used for experiments. The models are performed on three different datasets, i.e., Amazon Reviews, Reuters-21578, and Office-31. And the experimental results demonstrate the importance of selecting appropriate transfer learning models for different applications in practice.","accessed":{"date-parts":[[2023,2,27]]},"author":[{"family":"Zhuang","given":"Fuzhen"},{"family":"Qi","given":"Zhiyuan"},{"family":"Duan","given":"Keyu"},{"family":"Xi","given":"Dongbo"},{"family":"Zhu","given":"Yongchun"},{"family":"Zhu","given":"Hengshu"},{"family":"Xiong","given":"Hui"},{"family":"He","given":"Qing"}],"citation-key":"zhuangComprehensiveSurveyTransfer2020","issued":{"date-parts":[[2020,6,23]]},"number":"arXiv:1911.02685","publisher":"arXiv","source":"arXiv.org","title":"A Comprehensive Survey on Transfer Learning","type":"article","URL":"http://arxiv.org/abs/1911.02685"},
  {"id":"zophNeuralArchitectureSearch2016","author":[{"family":"Zoph","given":"Barret"},{"family":"Le","given":"Quoc V"}],"citation-key":"zophNeuralArchitectureSearch2016","container-title":"arXiv preprint arXiv:1611.01578","issued":{"date-parts":[[2016]]},"title":"Neural architecture search with reinforcement learning","type":"article-journal"}
]
