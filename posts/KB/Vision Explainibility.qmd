---
toc: true
title: Vision Explainibility

categories: ['explainabilityexplainabilityflow']
date modified: Monday, January 16th 2023, 6:58:47 pm
date created: Friday, November 18th 2022, 12:31:29 pm
---

# Vision Explainibility

## Links Useful
- [Captum Algos Comparison](https://captum.ai/docs/algorithms_comparison_matrix)

## Flow
- [DeconvNet](DeconvNet.qmd) (2013)
- [Deep Inside Convolutional Networks](Deep%20Inside%20Convolutional%20Networks.qmd) (2014)
- [Guided BackProp](Guided%20BackProp.qmd) (2015) Aka All Conv net
	- Building up on [Deep Inside Convolutional Networks](Deep%20Inside%20Convolutional%20Networks.qmd) and [DeconvNet](DeconvNet.qmd)
- [Salience Map](Salience%20Map.qmd)
	- Not class discriminative
	- Noise
	- Not appealing
- [CAM](CAM.qmd)
	- less noisy
	- not class discriminative
	- Worked only a restricted set of CNN templates
- [Grad-CAM](Grad-CAM.qmd)
	- class discriminative
	- not high res
	- Works for any arbitrary CNN
- [Occlusion Map](Occlusion%20Map)
	- Same as the next but not very fast
- [Guided GradCAM](Guided%20GradCAM.qmd)
- [DeepLIFT](DeepLIFT.qmd)
- [Noise Tunnel](Noise Tunnel.qmd)
- [Smooth-Grad](Smooth-Grad.qmd)
- [SmoothGrad Square](SmoothGrad Square.qmd)
- [VarGrad](VarGrad.qmd)
- [Integrated Gradients](Integrated Gradients.qmd)
- [Proxy Attention](Proxy Attention.qmd)
- [Conductance](Conductance.qmd)


## Disadvantages
- [The Unreliability of Saliency Methods](The Unreliability of Saliency Methods.qmd)
- [Interpretation of Neural networks is fragile](Interpretation of Neural networks is fragile.qmd)
- Fine grained data

## Links





