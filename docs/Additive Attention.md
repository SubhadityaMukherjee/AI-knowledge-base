---
title: Additive Attention

tags: architecture 
date modified: Thursday, August 11th 2022, 12:32:57 am
date created: Tuesday, July 26th 2022, 8:33:15 pm
---

# Additive [Attention](Attention.md)
- [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf)
- Uses a one layer feedforward network to calculate [Attention Alignment](Attention%20Alignment.md)
- Oh, basically it is the same as [Bahdanau Attention](Bahdanau%20Attention.md)

