---
toc: true
title: Attention
categories: ['architecture']
date modified: Monday, October 10th 2022, 2:02:34 pm
date created: Tuesday, July 26th 2022, 8:33:15 pm
---

# Attention
- Model can decide where to look in the input
- [[Self Attention.md|Self Attention]]
- [[Additive Attention.md|Additive Attention]]
- [[Dot Product Attention.md|Dot Product Attention]]
- [[Location Aware Attention.md|Location Aware Attention]]
- [[Relative Multi Head Self Attention.md|Relative Multi Head Self Attention]]
- [[Soft Attention.md|Soft Attention]]
- [[Scaled Dot Product Attention.md|Scaled Dot Product Attention]]
- [[Encoder Decoder Attention.md|Encoder Decoder Attention]]
- [[Multi Head Attention.md|Multi Head Attention]]
- [[Strided Attention.md|Strided Attention]]
- [[Fixed Factorization Attention.md|Fixed Factorization Attention]]
- [[Sliding Window Attention.md|Sliding Window Attention]]
- [[Dilated Sliding Window Attention.md|Dilated Sliding Window Attention]]
- [[Global and Sliding Window Attention.md|Global and Sliding Window Attention]]
- [[Content Based Attention.md|Content Based Attention]]
- [[Location Base Attention.md|Location Base Attention]]
- [[Mixed chunk attention.md|Mixed chunk attention]]



